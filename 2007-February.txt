From murdoch at stats.uwo.ca  Thu Feb  1 00:01:15 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 31 Jan 2007 18:01:15 -0500
Subject: [R] possible spam alert
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4AD0C@iu-mssg-mbx106.ads.iu.edu>
References: <836F00680EECD340A96AD34ECFF3B534B4AD0C@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <45C11FBB.5080306@stats.uwo.ca>

On 1/31/2007 5:38 PM, Kimpel, Mark William wrote:
> The last two times I have originated message threads on R or
> Bioconductor I have received the message included below from someone
> named Patrick Connolly. Both times I was the originator of the message
> thread and used what I thought was a unique subject line that explained
> as best I could what my question was. Patrick seems to be implying that
> I am abusing the R and BioC help newsgroups in this fashion. 

Your last message (subject "[R] regexpr and parsing question") shows up 
in my reader as a reply to a message by Gabor Grothendieck with subject 
"Re: [R] change plotting symbol for groups in trellis graph", because it 
has this line in the header:

In-Reply-To: <971536df0701301355j77b8d05oabce276b61fa2bf7 at mail.gmail.com>

and his message had this in the header:

Message-ID: <971536df0701301355j77b8d05oabce276b61fa2bf7 at mail.gmail.com>

That's what Patrick was complaining about.  You probably found Gabor's 
message in the group, hit "Reply", and then edited the subject line to 
make it unique.  Your mailer remembered that it was a reply to Gabor's 
message, and told everyone that, even though it wasn't really.

Instead, if you want to write to R-help, just start a new message, and 
send it to r-help at r-project.org (or r-help at stat.math.ethz.ch).

Duncan Murdoch


> 
> When I emailed him to give me a specific example, he did not reply. The
> most recent thread that he seems concerned about was to the R list and
> was entitled "regexpr and parsing question" . I believe the previous
> post of mine that he had problems with was to the BioC list but I can't
> remember its subject.
> 
> Is this spam?
> 
> If I am doing this correctly, you should see the subject "possible spam
> alert" in the subject header of THIS message.
> 
> Would the moderators of the lists please check and see if I am doing
> some wrong and, if not, inform Mr. Connolly that I am not. If others
> have received this message in error, it is possible it is spam and users
> should be alerted.
> 
> Thanks,
> 
> Mark
> 
> Mark W. Kimpel MD 
> 
>  
> 
>  
> 
> Official Business Address:
> 
>  
> 
> Department of Psychiatry
> 
> Indiana University School of Medicine
> 
> PR M116
> 
> Institute of Psychiatric Research
> 
> 791 Union Drive
> 
> Indianapolis, IN 46202
> 
>  
> This is a request to anyone who starts a new subject to begin with a new
> message and NOT reply to an existing one.  If your mail client is any
> good, it's very simple to set up an alias (mine is simply 'r') so that
> the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary and
> it's quicker than scrolling through an address book.
> It's also quicker than deleting the previous subject.
> 
> Most mornings, I have over a screenful of messages mostly from R-help
> and it's very useful to have them threaded.  However, the usefulness of
> threading is lost when posters reply to a message and then change the
> subject instead of creating a new message.
> 
> People who don't have a mail client that can display email in threads
> are probably unaware that this sort of thing can happen in ones that do:
> 
> 
>     37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
>     38 N   25 Jan Uwe Ligges              ( 55) `-> 
>     39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
> histograms
> ->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
>     41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
>     42     26 Jan Qin Xin                 (  9) | `->[R] how could I add
> legends
>     43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
>     44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
> coloured his
>     45 N   26 Jan Damon Wischik           (125) `-> 
>     46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting primatives,
> ellipse
>     47 N   25 Jan Uwe Ligges              ( 19) `->   
> 
> 
> As Martin Maechler explained some time ago, it also screws up the
> archives for a similar reason.
> 
> Your cooperation will be greatly appreciated.
> 
> best
>


From p.dalgaard at biostat.ku.dk  Thu Feb  1 00:24:59 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 01 Feb 2007 00:24:59 +0100
Subject: [R] possible spam alert
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4AD0C@iu-mssg-mbx106.ads.iu.edu>
References: <836F00680EECD340A96AD34ECFF3B534B4AD0C@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <45C1254B.1000307@biostat.ku.dk>

Kimpel, Mark William wrote:
> The last two times I have originated message threads on R or
> Bioconductor I have received the message included below from someone
> named Patrick Connolly. Both times I was the originator of the message
> thread and used what I thought was a unique subject line that explained
> as best I could what my question was. Patrick seems to be implying that
> I am abusing the R and BioC help newsgroups in this fashion. 
>
> When I emailed him to give me a specific example, he did not reply. The
> most recent thread that he seems concerned about was to the R list and
> was entitled "regexpr and parsing question" . I believe the previous
> post of mine that he had problems with was to the BioC list but I can't
> remember its subject.
>
> Is this spam?
>   
No. Breach of netiquette, yes.

The message in question starts a new thread, yet contains an 
In-Reply-To: header line, which presumably means that you started 
writing the message as a reply to something completely unrelated, 
specifically: "Re: [R] change plotting symbol for groups in trellis 
graph". You should not do that, unless you know how to remove the 
In-Reply-To line (and this is not obvious in many mail clients); 
changing the subject is not sufficient.
> If I am doing this correctly, you should see the subject "possible spam
> alert" in the subject header of THIS message.
>
> Would the moderators of the lists please check and see if I am doing
> some wrong and, if not, inform Mr. Connolly that I am not. If others
> have received this message in error, it is possible it is spam and users
> should be alerted.
>
> Thanks,
>
> Mark
>
> Mark W. Kimpel MD 
>
>  
>
>  
>
> Official Business Address:
>
>  
>
> Department of Psychiatry
>
> Indiana University School of Medicine
>
> PR M116
>
> Institute of Psychiatric Research
>
> 791 Union Drive
>
> Indianapolis, IN 46202
>
>  
> This is a request to anyone who starts a new subject to begin with a new
> message and NOT reply to an existing one.  If your mail client is any
> good, it's very simple to set up an alias (mine is simply 'r') so that
> the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary and
> it's quicker than scrolling through an address book.
> It's also quicker than deleting the previous subject.
>
> Most mornings, I have over a screenful of messages mostly from R-help
> and it's very useful to have them threaded.  However, the usefulness of
> threading is lost when posters reply to a message and then change the
> subject instead of creating a new message.
>
> People who don't have a mail client that can display email in threads
> are probably unaware that this sort of thing can happen in ones that do:
>
>
>     37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
>     38 N   25 Jan Uwe Ligges              ( 55) `-> 
>     39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
> histograms
> ->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
>     41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
>     42     26 Jan Qin Xin                 (  9) | `->[R] how could I add
> legends
>     43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
>     44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
> coloured his
>     45 N   26 Jan Damon Wischik           (125) `-> 
>     46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting primatives,
> ellipse
>     47 N   25 Jan Uwe Ligges              ( 19) `->   
>
>
> As Martin Maechler explained some time ago, it also screws up the
> archives for a similar reason.
>
> Your cooperation will be greatly appreciated.
>
> best
>
>


From muenchen at utk.edu  Thu Feb  1 00:31:53 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Wed, 31 Jan 2007 18:31:53 -0500
Subject: [R] R for SAS & SPSS Users Document
In-Reply-To: <87d54vbobn.fsf@ens-lsh.fr>
References: <7270AEC73132194E8BC0EE06B35D93D86D3CE7@UTKFSVS3.utk.tennessee.edu>
	<87d54vbobn.fsf@ens-lsh.fr>
Message-ID: <7270AEC73132194E8BC0EE06B35D93D87335DD@UTKFSVS3.utk.tennessee.edu>

Julien Barnier wrote: "... I think it will be very useful to me, even if
I will use it "the reverse way" : learn how to use SAS from R..."

I hadn't thought of using the document in "reverse" to learn SAS or SPSS
if you already know R. I'll have to reread it from that perspective &
see if there are any changes I can make to help in that direction
without a total rewrite. If anyone has any suggestions along those
lines, please send them my way.

Thanks for the PDF tip. Several people suggested that. I thought cutting
& pasting examples would be important, which is not as easy from PDF.
OpenOffice can open the .doc version on Linux if you use that. I have
added a PDF version at the same link ending in PDF:
http://oit.utk.edu/scc/RforSAS&SPSSusers.pdf

Cheers,
Bob

=========================================================
Bob Muenchen (pronounced Min'-chen), Manager 
Statistical Consulting Center
U of TN Office of Information Technology
200 Stokely Management Center, Knoxville, TN 37996-0520
Voice: (865) 974-5230 
FAX: (865) 974-4810
Email: muenchen at utk.edu
Web: http://oit.utk.edu/scc, 
News: http://listserv.utk.edu/archives/statnews.html
=========================================================


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Julien Barnier
Sent: Wednesday, January 31, 2007 3:20 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] R for SAS & SPSS Users Document

Hi,

> I am pleased to announce the availability of the document, "R for SAS
> and SPSS Users", at 
> http://oit.utk.edu/scc/RforSAS&SPSSusers.doc

I've looked at the document and printed it. I think it will be very
useful to me, even if I will use it "the reverse way" : learn how to
use SAS from R...

As I am far from an R expert, I will not be able to give you good
advices on R code. But maybe you would have had more comments on your
tutorial if you had given the link to the PDF version instead of the
MSWord one :

http://oit.utk.edu/scc/RforSAS&SPSSusers.pdf

Thanks again for your document,

-- 
Julien

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mkimpel at iupui.edu  Thu Feb  1 00:35:58 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Wed, 31 Jan 2007 18:35:58 -0500
Subject: [R] possible spam alert
In-Reply-To: <45C1254B.1000307@biostat.ku.dk>
References: <836F00680EECD340A96AD34ECFF3B534B4AD0C@iu-mssg-mbx106.ads.iu.edu>
	<45C1254B.1000307@biostat.ku.dk>
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4AD10@iu-mssg-mbx106.ads.iu.edu>

Peter,

Thanks you for your explanation, I had taken Mr. Connolly's message to
me to imply that I was not changing the subject line. I use MS Outlook
2007 and, unless I am just not seeing it, Outlook does not normally
display the "in reply to" header, I was under the mistaken impression
that that was what the Subject line was for. See, for example, the
header to your message to me below. Outlook will, however, sort messages
by Subject, and that is what I thought was meant by threading.

Well, I learned something today and apologize for any inconvenience my
posts may have caused.

BTW, I use Outlook because it is supported by my university server and
will synch my appointments and contacts with my PDA, which runs Windows
CE. If anyone has a suggestion for me of a better email program that
will provide proper threading AND work with a MS email server and synch
with Windows CE, I'd love to hear it.

Thanks again,

Mark

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Wednesday, January 31, 2007 6:25 PM
To: Kimpel, Mark William
Cc: bioconductor at stat.math.ethz.ch; r-help at stat.math.ethz.ch
Subject: Re: [R] possible spam alert

Kimpel, Mark William wrote:
> The last two times I have originated message threads on R or
> Bioconductor I have received the message included below from someone
> named Patrick Connolly. Both times I was the originator of the message
> thread and used what I thought was a unique subject line that
explained
> as best I could what my question was. Patrick seems to be implying
that
> I am abusing the R and BioC help newsgroups in this fashion. 
>
> When I emailed him to give me a specific example, he did not reply.
The
> most recent thread that he seems concerned about was to the R list and
> was entitled "regexpr and parsing question" . I believe the previous
> post of mine that he had problems with was to the BioC list but I
can't
> remember its subject.
>
> Is this spam?
>   
No. Breach of netiquette, yes.

The message in question starts a new thread, yet contains an 
In-Reply-To: header line, which presumably means that you started 
writing the message as a reply to something completely unrelated, 
specifically: "Re: [R] change plotting symbol for groups in trellis 
graph". You should not do that, unless you know how to remove the 
In-Reply-To line (and this is not obvious in many mail clients); 
changing the subject is not sufficient.
> If I am doing this correctly, you should see the subject "possible
spam
> alert" in the subject header of THIS message.
>
> Would the moderators of the lists please check and see if I am doing
> some wrong and, if not, inform Mr. Connolly that I am not. If others
> have received this message in error, it is possible it is spam and
users
> should be alerted.
>
> Thanks,
>
> Mark
>
> Mark W. Kimpel MD 
>
>  
>
>  
>
> Official Business Address:
>
>  
>
> Department of Psychiatry
>
> Indiana University School of Medicine
>
> PR M116
>
> Institute of Psychiatric Research
>
> 791 Union Drive
>
> Indianapolis, IN 46202
>
>  
> This is a request to anyone who starts a new subject to begin with a
new
> message and NOT reply to an existing one.  If your mail client is any
> good, it's very simple to set up an alias (mine is simply 'r') so that
> the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary
and
> it's quicker than scrolling through an address book.
> It's also quicker than deleting the previous subject.
>
> Most mornings, I have over a screenful of messages mostly from R-help
> and it's very useful to have them threaded.  However, the usefulness
of
> threading is lost when posters reply to a message and then change the
> subject instead of creating a new message.
>
> People who don't have a mail client that can display email in threads
> are probably unaware that this sort of thing can happen in ones that
do:
>
>
>     37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
>     38 N   25 Jan Uwe Ligges              ( 55) `-> 
>     39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
> histograms
> ->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
>     41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
>     42     26 Jan Qin Xin                 (  9) | `->[R] how could I
add
> legends
>     43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
>     44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
> coloured his
>     45 N   26 Jan Damon Wischik           (125) `-> 
>     46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting
primatives,
> ellipse
>     47 N   25 Jan Uwe Ligges              ( 19) `->   
>
>
> As Martin Maechler explained some time ago, it also screws up the
> archives for a similar reason.
>
> Your cooperation will be greatly appreciated.
>
> best
>
>


From fpepin at cs.mcgill.ca  Thu Feb  1 00:52:39 2007
From: fpepin at cs.mcgill.ca (Francois Pepin)
Date: Wed, 31 Jan 2007 18:52:39 -0500
Subject: [R] [BioC] possible spam alert
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4AD0C@iu-mssg-mbx106.ads.iu.edu>
References: <836F00680EECD340A96AD34ECFF3B534B4AD0C@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <1170287559.3069.117.camel@ybrig.MCB.McGill.CA>

Hi Mark,

I'm sending this off-list because I don't want to unnecessarily fill
people's inbox if I'm wrong.

I think you are misunderstanding Patrick's problem. The point is kind of
subtle and requires a bit of knowledge of how e-mail works.

What he is complaining about is that people take a random message on the
mailing list, hit reply and then change the subject, as opposed to
creating a new message.

For most people this ends up being the same: a new message appears with
a new subject heading.

In some cases, which include the archiving system, the e-mail client
tries to organize the e-mails by threads. Because some people tend to
change the subject name inside of a thread, there is an additional
(hidden) identifier that is used. This means that if you replied to a
random message, you are now associated to an unrelated thread, which is
annoying at best and can cause your message to be ignored at worst. This
is especially true in a high-volume list like R-help, as most people
don't want to jump all over to follow a conversation and having a
completely irrelevant message inside a conversation is confusing.

Hopefully this clarifies the situation. You're not being accused of
spamming, but of not following one of the finer points of mailing list
etiquette. Given the volume of the lists involved and the time and
effort that many people spend in reading and replying to the list, I
think it is a good idea to make the extra effort to make sure that
everything runs smoothly.

Francois

On Wed, 2007-01-31 at 17:38 -0500, Kimpel, Mark William wrote:
> The last two times I have originated message threads on R or
> Bioconductor I have received the message included below from someone
> named Patrick Connolly. Both times I was the originator of the message
> thread and used what I thought was a unique subject line that explained
> as best I could what my question was. Patrick seems to be implying that
> I am abusing the R and BioC help newsgroups in this fashion. 
> 
> When I emailed him to give me a specific example, he did not reply. The
> most recent thread that he seems concerned about was to the R list and
> was entitled "regexpr and parsing question" . I believe the previous
> post of mine that he had problems with was to the BioC list but I can't
> remember its subject.
> 
> Is this spam?
> 
> If I am doing this correctly, you should see the subject "possible spam
> alert" in the subject header of THIS message.
> 
> Would the moderators of the lists please check and see if I am doing
> some wrong and, if not, inform Mr. Connolly that I am not. If others
> have received this message in error, it is possible it is spam and users
> should be alerted.
> 
> Thanks,
> 
> Mark
> 
> Mark W. Kimpel MD 
> 
>  
> 
> 
> 
> Official Business Address:
> 
>  
> 
> Department of Psychiatry
> 
> Indiana University School of Medicine
> 
> PR M116
> 
> Institute of Psychiatric Research
> 
> 791 Union Drive
> 
> Indianapolis, IN 46202
> 
>  
> This is a request to anyone who starts a new subject to begin with a new
> message and NOT reply to an existing one.  If your mail client is any
> good, it's very simple to set up an alias (mine is simply 'r') so that
> the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary and
> it's quicker than scrolling through an address book.
> It's also quicker than deleting the previous subject.
> 
> Most mornings, I have over a screenful of messages mostly from R-help
> and it's very useful to have them threaded.  However, the usefulness of
> threading is lost when posters reply to a message and then change the
> subject instead of creating a new message.
> 
> People who don't have a mail client that can display email in threads
> are probably unaware that this sort of thing can happen in ones that do:
> 
> 
>     37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
>     38 N   25 Jan Uwe Ligges              ( 55) `-> 
>     39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
> histograms
> ->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
>     41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
>     42     26 Jan Qin Xin                 (  9) | `->[R] how could I add
> legends
>     43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
>     44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
> coloured his
>     45 N   26 Jan Damon Wischik           (125) `-> 
>     46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting primatives,
> ellipse
>     47 N   25 Jan Uwe Ligges              ( 19) `->   
> 
> 
> As Martin Maechler explained some time ago, it also screws up the
> archives for a similar reason.
> 
> Your cooperation will be greatly appreciated.
> 
> best
>


From georgia.chan at gmail.com  Thu Feb  1 00:58:03 2007
From: georgia.chan at gmail.com (Georgia Chan)
Date: Wed, 31 Jan 2007 23:58:03 +0000
Subject: [R] 3D histograms
Message-ID: <c30d61d30701311558oea84f0v66340f553f796756@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070131/c5c9dc6c/attachment.pl 

From mkimpel at iupui.edu  Thu Feb  1 01:25:27 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Wed, 31 Jan 2007 19:25:27 -0500
Subject: [R] Outlook does threading
In-Reply-To: <002a01c74594$5ce58310$4d908980@gne.windows.gene.com>
References: <836F00680EECD340A96AD34ECFF3B534B4AD10@iu-mssg-mbx106.ads.iu.edu>
	<002a01c74594$5ce58310$4d908980@gne.windows.gene.com>
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4AD1B@iu-mssg-mbx106.ads.iu.edu>

See below for Bert Gunter's off list reply to me (which I do
appreciate). I'm putting it back on the list because it seems there is
still confusion regarding the difference between threading and sorting
by subject. I thought the example I will give below will serve as
instructional for other Outlook users who may be similarly confused as I
was (am?). 

Per Bert's instructions, I just set up my inbox to sort by subject. I
sent one email to myself with the subject "test1" and then replied to it
without changing the subject. The reply correctly went to "test1" in the
inbox sorter. I then changed the subject heading in the test1 reply to
"test2" and sent it to myself. This time Outlook re-categorized it and
put it in a separate compartment in the view called "test2".

If Outlook can do threading the way the R mail server does, I don't
think this is the way to do it.

Unless someone has an idea of how to correctly set up Outlook to do
threading in the manner that the R mail server does, I think the message
for us Outlook users is to just create, from scratch, a new message when
initiating a new subject.

Thanks for all your help. 

Mark

-----Original Message-----
From: Bert Gunter [mailto:gunter.berton at gene.com] 
Sent: Wednesday, January 31, 2007 7:03 PM
To: Kimpel, Mark William
Subject: Outlook does threading

 Mark:

No need to bother the R list with this. Outlook does threading. Just
sort on
Subject in the viewer.

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kimpel, Mark
William
Sent: Wednesday, January 31, 2007 3:36 PM
To: Peter Dalgaard
Cc: r-help at stat.math.ethz.ch; bioconductor at stat.math.ethz.ch
Subject: Re: [R] possible spam alert

Peter,

Thanks you for your explanation, I had taken Mr. Connolly's message to
me to imply that I was not changing the subject line. I use MS Outlook
2007 and, unless I am just not seeing it, Outlook does not normally
display the "in reply to" header, I was under the mistaken impression
that that was what the Subject line was for. See, for example, the
header to your message to me below. Outlook will, however, sort messages
by Subject, and that is what I thought was meant by threading.

Well, I learned something today and apologize for any inconvenience my
posts may have caused.

BTW, I use Outlook because it is supported by my university server and
will synch my appointments and contacts with my PDA, which runs Windows
CE. If anyone has a suggestion for me of a better email program that
will provide proper threading AND work with a MS email server and synch
with Windows CE, I'd love to hear it.

Thanks again,

Mark

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Wednesday, January 31, 2007 6:25 PM
To: Kimpel, Mark William
Cc: bioconductor at stat.math.ethz.ch; r-help at stat.math.ethz.ch
Subject: Re: [R] possible spam alert

Kimpel, Mark William wrote:
> The last two times I have originated message threads on R or
> Bioconductor I have received the message included below from someone
> named Patrick Connolly. Both times I was the originator of the message
> thread and used what I thought was a unique subject line that
explained
> as best I could what my question was. Patrick seems to be implying
that
> I am abusing the R and BioC help newsgroups in this fashion. 
>
> When I emailed him to give me a specific example, he did not reply.
The
> most recent thread that he seems concerned about was to the R list and
> was entitled "regexpr and parsing question" . I believe the previous
> post of mine that he had problems with was to the BioC list but I
can't
> remember its subject.
>
> Is this spam?
>   
No. Breach of netiquette, yes.

The message in question starts a new thread, yet contains an 
In-Reply-To: header line, which presumably means that you started 
writing the message as a reply to something completely unrelated, 
specifically: "Re: [R] change plotting symbol for groups in trellis 
graph". You should not do that, unless you know how to remove the 
In-Reply-To line (and this is not obvious in many mail clients); 
changing the subject is not sufficient.
> If I am doing this correctly, you should see the subject "possible
spam
> alert" in the subject header of THIS message.
>
> Would the moderators of the lists please check and see if I am doing
> some wrong and, if not, inform Mr. Connolly that I am not. If others
> have received this message in error, it is possible it is spam and
users
> should be alerted.
>
> Thanks,
>
> Mark
>
> Mark W. Kimpel MD 
>
>  
>
>  
>
> Official Business Address:
>
>  
>
> Department of Psychiatry
>
> Indiana University School of Medicine
>
> PR M116
>
> Institute of Psychiatric Research
>
> 791 Union Drive
>
> Indianapolis, IN 46202
>
>  
> This is a request to anyone who starts a new subject to begin with a
new
> message and NOT reply to an existing one.  If your mail client is any
> good, it's very simple to set up an alias (mine is simply 'r') so that
> the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary
and
> it's quicker than scrolling through an address book.
> It's also quicker than deleting the previous subject.
>
> Most mornings, I have over a screenful of messages mostly from R-help
> and it's very useful to have them threaded.  However, the usefulness
of
> threading is lost when posters reply to a message and then change the
> subject instead of creating a new message.
>
> People who don't have a mail client that can display email in threads
> are probably unaware that this sort of thing can happen in ones that
do:
>
>
>     37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
>     38 N   25 Jan Uwe Ligges              ( 55) `-> 
>     39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
> histograms
> ->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
>     41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
>     42     26 Jan Qin Xin                 (  9) | `->[R] how could I
add
> legends
>     43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
>     44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
> coloured his
>     45 N   26 Jan Damon Wischik           (125) `-> 
>     46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting
primatives,
> ellipse
>     47 N   25 Jan Uwe Ligges              ( 19) `->   
>
>
> As Martin Maechler explained some time ago, it also screws up the
> archives for a similar reason.
>
> Your cooperation will be greatly appreciated.
>
> best
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From tplate at acm.org  Thu Feb  1 02:07:49 2007
From: tplate at acm.org (Tony Plate)
Date: Wed, 31 Jan 2007 18:07:49 -0700
Subject: [R] Outlook does threading
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4AD1B@iu-mssg-mbx106.ads.iu.edu>
References: <836F00680EECD340A96AD34ECFF3B534B4AD10@iu-mssg-mbx106.ads.iu.edu>	<002a01c74594$5ce58310$4d908980@gne.windows.gene.com>
	<836F00680EECD340A96AD34ECFF3B534B4AD1B@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <45C13D65.8000602@acm.org>

Your final paragraph has the take-home message for everyone (not just MS 
Outlook users): "just create, from scratch, a new message when 
initiating a new subject."

Viewing threads can be completely different to sorting based on the 
subject line.  Your initial post with the subject "regexpr and parsing 
question" was in fact a reply to the message from Gabor Grothendick in 
the thread "Re: [R] change plotting symbol for groups in trellis graph." 
   (I can see this by looking at the header information: I see a 
"In-reply-to:" header item.)

When I view threads in the Thunderbird mail reader, your post and 
replies with the subject "regexpr and parsing question" do in fact show 
up under the thread in which Gabor's message appeared, not in their own 
thread.

According to 
http://office.microsoft.com/en-us/outlook/HA011356671033.aspx, one can 
view threads in Outlook by selecting "View->Arrange By->Conversation".

Hope this helps (in case the horse was not thoroughly dead already.)

-- Tony Plate

Kimpel, Mark William wrote:
> See below for Bert Gunter's off list reply to me (which I do
> appreciate). I'm putting it back on the list because it seems there is
> still confusion regarding the difference between threading and sorting
> by subject. I thought the example I will give below will serve as
> instructional for other Outlook users who may be similarly confused as I
> was (am?). 
> 
> Per Bert's instructions, I just set up my inbox to sort by subject. I
> sent one email to myself with the subject "test1" and then replied to it
> without changing the subject. The reply correctly went to "test1" in the
> inbox sorter. I then changed the subject heading in the test1 reply to
> "test2" and sent it to myself. This time Outlook re-categorized it and
> put it in a separate compartment in the view called "test2".
> 
> If Outlook can do threading the way the R mail server does, I don't
> think this is the way to do it.
> 
> Unless someone has an idea of how to correctly set up Outlook to do
> threading in the manner that the R mail server does, I think the message
> for us Outlook users is to just create, from scratch, a new message when
> initiating a new subject.
> 
> Thanks for all your help. 
> 
> Mark
> 
> -----Original Message-----
> From: Bert Gunter [mailto:gunter.berton at gene.com] 
> Sent: Wednesday, January 31, 2007 7:03 PM
> To: Kimpel, Mark William
> Subject: Outlook does threading
> 
>  Mark:
> 
> No need to bother the R list with this. Outlook does threading. Just
> sort on
> Subject in the viewer.
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kimpel, Mark
> William
> Sent: Wednesday, January 31, 2007 3:36 PM
> To: Peter Dalgaard
> Cc: r-help at stat.math.ethz.ch; bioconductor at stat.math.ethz.ch
> Subject: Re: [R] possible spam alert
> 
> Peter,
> 
> Thanks you for your explanation, I had taken Mr. Connolly's message to
> me to imply that I was not changing the subject line. I use MS Outlook
> 2007 and, unless I am just not seeing it, Outlook does not normally
> display the "in reply to" header, I was under the mistaken impression
> that that was what the Subject line was for. See, for example, the
> header to your message to me below. Outlook will, however, sort messages
> by Subject, and that is what I thought was meant by threading.
> 
> Well, I learned something today and apologize for any inconvenience my
> posts may have caused.
> 
> BTW, I use Outlook because it is supported by my university server and
> will synch my appointments and contacts with my PDA, which runs Windows
> CE. If anyone has a suggestion for me of a better email program that
> will provide proper threading AND work with a MS email server and synch
> with Windows CE, I'd love to hear it.
> 
> Thanks again,
> 
> Mark
> 
> Mark W. Kimpel MD 
> 
>  
> 
> (317) 490-5129 Work, & Mobile
> 
>  
> 
> (317) 663-0513 Home (no voice mail please)
> 
> 1-(317)-536-2730 FAX
> 
> 
> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> Sent: Wednesday, January 31, 2007 6:25 PM
> To: Kimpel, Mark William
> Cc: bioconductor at stat.math.ethz.ch; r-help at stat.math.ethz.ch
> Subject: Re: [R] possible spam alert
> 
> Kimpel, Mark William wrote:
> 
>>The last two times I have originated message threads on R or
>>Bioconductor I have received the message included below from someone
>>named Patrick Connolly. Both times I was the originator of the message
>>thread and used what I thought was a unique subject line that
> 
> explained
> 
>>as best I could what my question was. Patrick seems to be implying
> 
> that
> 
>>I am abusing the R and BioC help newsgroups in this fashion. 
>>
>>When I emailed him to give me a specific example, he did not reply.
> 
> The
> 
>>most recent thread that he seems concerned about was to the R list and
>>was entitled "regexpr and parsing question" . I believe the previous
>>post of mine that he had problems with was to the BioC list but I
> 
> can't
> 
>>remember its subject.
>>
>>Is this spam?
>>  
> 
> No. Breach of netiquette, yes.
> 
> The message in question starts a new thread, yet contains an 
> In-Reply-To: header line, which presumably means that you started 
> writing the message as a reply to something completely unrelated, 
> specifically: "Re: [R] change plotting symbol for groups in trellis 
> graph". You should not do that, unless you know how to remove the 
> In-Reply-To line (and this is not obvious in many mail clients); 
> changing the subject is not sufficient.
> 
>>If I am doing this correctly, you should see the subject "possible
> 
> spam
> 
>>alert" in the subject header of THIS message.
>>
>>Would the moderators of the lists please check and see if I am doing
>>some wrong and, if not, inform Mr. Connolly that I am not. If others
>>have received this message in error, it is possible it is spam and
> 
> users
> 
>>should be alerted.
>>
>>Thanks,
>>
>>Mark
>>
>>Mark W. Kimpel MD 
>>
>> 
>>
>> 
>>
>>Official Business Address:
>>
>> 
>>
>>Department of Psychiatry
>>
>>Indiana University School of Medicine
>>
>>PR M116
>>
>>Institute of Psychiatric Research
>>
>>791 Union Drive
>>
>>Indianapolis, IN 46202
>>
>> 
>>This is a request to anyone who starts a new subject to begin with a
> 
> new
> 
>>message and NOT reply to an existing one.  If your mail client is any
>>good, it's very simple to set up an alias (mine is simply 'r') so that
>>the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary
> 
> and
> 
>>it's quicker than scrolling through an address book.
>>It's also quicker than deleting the previous subject.
>>
>>Most mornings, I have over a screenful of messages mostly from R-help
>>and it's very useful to have them threaded.  However, the usefulness
> 
> of
> 
>>threading is lost when posters reply to a message and then change the
>>subject instead of creating a new message.
>>
>>People who don't have a mail client that can display email in threads
>>are probably unaware that this sort of thing can happen in ones that
> 
> do:
> 
>>
>>    37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
>>    38 N   25 Jan Uwe Ligges              ( 55) `-> 
>>    39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
>>histograms
>>->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
>>    41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
>>    42     26 Jan Qin Xin                 (  9) | `->[R] how could I
> 
> add
> 
>>legends
>>    43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
>>    44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
>>coloured his
>>    45 N   26 Jan Damon Wischik           (125) `-> 
>>    46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting
> 
> primatives,
> 
>>ellipse
>>    47 N   25 Jan Uwe Ligges              ( 19) `->   
>>
>>
>>As Martin Maechler explained some time ago, it also screws up the
>>archives for a similar reason.
>>
>>Your cooperation will be greatly appreciated.
>>
>>best
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Feb  1 02:14:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 31 Jan 2007 20:14:41 -0500
Subject: [R] Outlook does threading
In-Reply-To: <45C13D65.8000602@acm.org>
References: <836F00680EECD340A96AD34ECFF3B534B4AD10@iu-mssg-mbx106.ads.iu.edu>
	<002a01c74594$5ce58310$4d908980@gne.windows.gene.com>
	<836F00680EECD340A96AD34ECFF3B534B4AD1B@iu-mssg-mbx106.ads.iu.edu>
	<45C13D65.8000602@acm.org>
Message-ID: <971536df0701311714o3216eb99g93e38f809abaa8f4@mail.gmail.com>

You can see how it looks to most readers by viewing it on gmane:

  http://thread.gmane.org/gmane.comp.lang.r.general/78065

Note that even though the subject has been changed its still
listed as a child of another message rather than the start
of a new thread.

On 1/31/07, Tony Plate <tplate at acm.org> wrote:
> Your final paragraph has the take-home message for everyone (not just MS
> Outlook users): "just create, from scratch, a new message when
> initiating a new subject."
>
> Viewing threads can be completely different to sorting based on the
> subject line.  Your initial post with the subject "regexpr and parsing
> question" was in fact a reply to the message from Gabor Grothendick in
> the thread "Re: [R] change plotting symbol for groups in trellis graph."
>   (I can see this by looking at the header information: I see a
> "In-reply-to:" header item.)
>
> When I view threads in the Thunderbird mail reader, your post and
> replies with the subject "regexpr and parsing question" do in fact show
> up under the thread in which Gabor's message appeared, not in their own
> thread.
>
> According to
> http://office.microsoft.com/en-us/outlook/HA011356671033.aspx, one can
> view threads in Outlook by selecting "View->Arrange By->Conversation".
>
> Hope this helps (in case the horse was not thoroughly dead already.)
>
> -- Tony Plate
>
> Kimpel, Mark William wrote:
> > See below for Bert Gunter's off list reply to me (which I do
> > appreciate). I'm putting it back on the list because it seems there is
> > still confusion regarding the difference between threading and sorting
> > by subject. I thought the example I will give below will serve as
> > instructional for other Outlook users who may be similarly confused as I
> > was (am?).
> >
> > Per Bert's instructions, I just set up my inbox to sort by subject. I
> > sent one email to myself with the subject "test1" and then replied to it
> > without changing the subject. The reply correctly went to "test1" in the
> > inbox sorter. I then changed the subject heading in the test1 reply to
> > "test2" and sent it to myself. This time Outlook re-categorized it and
> > put it in a separate compartment in the view called "test2".
> >
> > If Outlook can do threading the way the R mail server does, I don't
> > think this is the way to do it.
> >
> > Unless someone has an idea of how to correctly set up Outlook to do
> > threading in the manner that the R mail server does, I think the message
> > for us Outlook users is to just create, from scratch, a new message when
> > initiating a new subject.
> >
> > Thanks for all your help.
> >
> > Mark
> >
> > -----Original Message-----
> > From: Bert Gunter [mailto:gunter.berton at gene.com]
> > Sent: Wednesday, January 31, 2007 7:03 PM
> > To: Kimpel, Mark William
> > Subject: Outlook does threading
> >
> >  Mark:
> >
> > No need to bother the R list with this. Outlook does threading. Just
> > sort on
> > Subject in the viewer.
> >
> > Bert Gunter
> > Genentech Nonclinical Statistics
> > South San Francisco, CA 94404
> > 650-467-7374
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kimpel, Mark
> > William
> > Sent: Wednesday, January 31, 2007 3:36 PM
> > To: Peter Dalgaard
> > Cc: r-help at stat.math.ethz.ch; bioconductor at stat.math.ethz.ch
> > Subject: Re: [R] possible spam alert
> >
> > Peter,
> >
> > Thanks you for your explanation, I had taken Mr. Connolly's message to
> > me to imply that I was not changing the subject line. I use MS Outlook
> > 2007 and, unless I am just not seeing it, Outlook does not normally
> > display the "in reply to" header, I was under the mistaken impression
> > that that was what the Subject line was for. See, for example, the
> > header to your message to me below. Outlook will, however, sort messages
> > by Subject, and that is what I thought was meant by threading.
> >
> > Well, I learned something today and apologize for any inconvenience my
> > posts may have caused.
> >
> > BTW, I use Outlook because it is supported by my university server and
> > will synch my appointments and contacts with my PDA, which runs Windows
> > CE. If anyone has a suggestion for me of a better email program that
> > will provide proper threading AND work with a MS email server and synch
> > with Windows CE, I'd love to hear it.
> >
> > Thanks again,
> >
> > Mark
> >
> > Mark W. Kimpel MD
> >
> >
> >
> > (317) 490-5129 Work, & Mobile
> >
> >
> >
> > (317) 663-0513 Home (no voice mail please)
> >
> > 1-(317)-536-2730 FAX
> >
> >
> > -----Original Message-----
> > From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
> > Sent: Wednesday, January 31, 2007 6:25 PM
> > To: Kimpel, Mark William
> > Cc: bioconductor at stat.math.ethz.ch; r-help at stat.math.ethz.ch
> > Subject: Re: [R] possible spam alert
> >
> > Kimpel, Mark William wrote:
> >
> >>The last two times I have originated message threads on R or
> >>Bioconductor I have received the message included below from someone
> >>named Patrick Connolly. Both times I was the originator of the message
> >>thread and used what I thought was a unique subject line that
> >
> > explained
> >
> >>as best I could what my question was. Patrick seems to be implying
> >
> > that
> >
> >>I am abusing the R and BioC help newsgroups in this fashion.
> >>
> >>When I emailed him to give me a specific example, he did not reply.
> >
> > The
> >
> >>most recent thread that he seems concerned about was to the R list and
> >>was entitled "regexpr and parsing question" . I believe the previous
> >>post of mine that he had problems with was to the BioC list but I
> >
> > can't
> >
> >>remember its subject.
> >>
> >>Is this spam?
> >>
> >
> > No. Breach of netiquette, yes.
> >
> > The message in question starts a new thread, yet contains an
> > In-Reply-To: header line, which presumably means that you started
> > writing the message as a reply to something completely unrelated,
> > specifically: "Re: [R] change plotting symbol for groups in trellis
> > graph". You should not do that, unless you know how to remove the
> > In-Reply-To line (and this is not obvious in many mail clients);
> > changing the subject is not sufficient.
> >
> >>If I am doing this correctly, you should see the subject "possible
> >
> > spam
> >
> >>alert" in the subject header of THIS message.
> >>
> >>Would the moderators of the lists please check and see if I am doing
> >>some wrong and, if not, inform Mr. Connolly that I am not. If others
> >>have received this message in error, it is possible it is spam and
> >
> > users
> >
> >>should be alerted.
> >>
> >>Thanks,
> >>
> >>Mark
> >>
> >>Mark W. Kimpel MD
> >>
> >>
> >>
> >>
> >>
> >>Official Business Address:
> >>
> >>
> >>
> >>Department of Psychiatry
> >>
> >>Indiana University School of Medicine
> >>
> >>PR M116
> >>
> >>Institute of Psychiatric Research
> >>
> >>791 Union Drive
> >>
> >>Indianapolis, IN 46202
> >>
> >>
> >>This is a request to anyone who starts a new subject to begin with a
> >
> > new
> >
> >>message and NOT reply to an existing one.  If your mail client is any
> >>good, it's very simple to set up an alias (mine is simply 'r') so that
> >>the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary
> >
> > and
> >
> >>it's quicker than scrolling through an address book.
> >>It's also quicker than deleting the previous subject.
> >>
> >>Most mornings, I have over a screenful of messages mostly from R-help
> >>and it's very useful to have them threaded.  However, the usefulness
> >
> > of
> >
> >>threading is lost when posters reply to a message and then change the
> >>subject instead of creating a new message.
> >>
> >>People who don't have a mail client that can display email in threads
> >>are probably unaware that this sort of thing can happen in ones that
> >
> > do:
> >
> >>
> >>    37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
> >>    38 N   25 Jan Uwe Ligges              ( 55) `->
> >>    39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
> >>histograms
> >>->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
> >>    41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
> >>    42     26 Jan Qin Xin                 (  9) | `->[R] how could I
> >
> > add
> >
> >>legends
> >>    43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
> >>    44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
> >>coloured his
> >>    45 N   26 Jan Damon Wischik           (125) `->
> >>    46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting
> >
> > primatives,
> >
> >>ellipse
> >>    47 N   25 Jan Uwe Ligges              ( 19) `->
> >>
> >>
> >>As Martin Maechler explained some time ago, it also screws up the
> >>archives for a similar reason.
> >>
> >>Your cooperation will be greatly appreciated.
> >>
> >>best
> >>
> >>
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Alexander.Herr at csiro.au  Thu Feb  1 02:15:01 2007
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Thu, 1 Feb 2007 11:15:01 +1000
Subject: [R] rgl.postscript{rgl} crashes R
Message-ID: <062AE320EF971E40ACD0F6C93391D769C6C835@exqld1-tsv.nexus.csiro.au>

Hi list,

I am trying to print a rgl scene. I can do this with
rgl.snapshot("test.pnt",fmt="png"), but
rgl.postscript("test.pdf",fmt="pdf") does crash R and returns to the
linux shell after extented time period.

I am running R 2.4.0 on i686 linux mandrake 10.2. Do I need any other R
external program to make this work?

Thanks
Herry


Dr Alexander Herr - Herry
Spatial and statistical analyst
CSIRO, Sustainable Ecosystems
Davies Laboratory,
University Drive, Douglas, QLD 4814 
Private Mail Bag, Aitkenvale, QLD 4814
 
Phone/www 
(07) 4753 8510; 4753 8650(fax)
Home: http://herry.ausbats.org.au
Webadmin ABS: http://ausbats.org.au
Sustainable Ecosystems: http://www.cse.csiro.au/


From mkimpel at iupui.edu  Thu Feb  1 02:40:05 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Wed, 31 Jan 2007 20:40:05 -0500
Subject: [R] Outlook does threading
In-Reply-To: <45C13D65.8000602@acm.org>
References: <836F00680EECD340A96AD34ECFF3B534B4AD10@iu-mssg-mbx106.ads.iu.edu>	<002a01c74594$5ce58310$4d908980@gne.windows.gene.com>
	<836F00680EECD340A96AD34ECFF3B534B4AD1B@iu-mssg-mbx106.ads.iu.edu>
	<45C13D65.8000602@acm.org>
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4AD22@iu-mssg-mbx106.ads.iu.edu>

Tony,

I went to the MS link that you suggested (see below) and it indeed says
that "The Arrange by Conversation arrangement shows your e-mail items
grouped by message subject or 'thread.'" Instead of arranging by
subject, I arranged my view by "conversation" and got exactly the same
result that I had gotten when viewing by subject, i.e. MS Outlook looks
only at the subject line when deciding on threads, conversations,
subjects, or whatever you want to call it. I am, BTW, using Outlook 2007
on Windows XP SP2 and cannot vouch for Outlooks behavior in other
versions or configurations.

So, no matter what I do, it seems impossible for me to duplicate in
Outlook what Gabor pointed out to me when he said,

" You can see how it looks to most readers by viewing it on gmane:

  http://thread.gmane.org/gmane.comp.lang.r.general/78065

Note that even though the subject has been changed its still listed as a
child of another message rather than the start of a new thread." I did
check and of course Gabor is correct.

This subject does need to be put to bed. I have reread the posting guide
for R-help at http://www.r-project.org/
and it does indeed say "Do please create a new email message when
posting to the list rather than replying to a previous message and
simply changing the subject line! This allows sensible threading in the
mailing list archives (and many users e-mail readers)."

To be honest, I probably read this 3 years ago when I subscribed to the
list but, because my email reader doesn't behave this way, I just forgot
about it. I email so many people during the day that I frequently hit
reply to a previous message and then change the subject if appropriate.

So, not to justify my behavior, but would it be possible for the R mail
server to somehow check and see if the subject heading on a thread has
been changed and then return-to-sender with a standard message
explaining everything we have been through tonight? If Patrick Connolly
sees this enough to have a standard message he sends out and Martin
Maechler commented on it in the past, perhaps other Windows users of
Outlook are doing the same thing I did. Rest assured that I have learned
my lesson and won't repeat the same mistake, but if such a filter was
put in place at the R mail server level, perhaps it would save the
non-Outlook users a lot of aggravation.

These exchanges have been edifying and I thank all for their patience
and explanations.

Mark

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


-----Original Message-----
From: Tony Plate [mailto:tplate at acm.org] 
Sent: Wednesday, January 31, 2007 8:08 PM
To: Kimpel, Mark William
Cc: r-help at stat.math.ethz.ch; bioconductor at stat.math.ethz.ch
Subject: Re: [R] Outlook does threading

Your final paragraph has the take-home message for everyone (not just MS

Outlook users): "just create, from scratch, a new message when 
initiating a new subject."

Viewing threads can be completely different to sorting based on the 
subject line.  Your initial post with the subject "regexpr and parsing 
question" was in fact a reply to the message from Gabor Grothendick in 
the thread "Re: [R] change plotting symbol for groups in trellis graph."

   (I can see this by looking at the header information: I see a 
"In-reply-to:" header item.)

When I view threads in the Thunderbird mail reader, your post and 
replies with the subject "regexpr and parsing question" do in fact show 
up under the thread in which Gabor's message appeared, not in their own 
thread.

According to 
http://office.microsoft.com/en-us/outlook/HA011356671033.aspx, one can 
view threads in Outlook by selecting "View->Arrange By->Conversation".

Hope this helps (in case the horse was not thoroughly dead already.)

-- Tony Plate

Kimpel, Mark William wrote:
> See below for Bert Gunter's off list reply to me (which I do
> appreciate). I'm putting it back on the list because it seems there is
> still confusion regarding the difference between threading and sorting
> by subject. I thought the example I will give below will serve as
> instructional for other Outlook users who may be similarly confused as
I
> was (am?). 
> 
> Per Bert's instructions, I just set up my inbox to sort by subject. I
> sent one email to myself with the subject "test1" and then replied to
it
> without changing the subject. The reply correctly went to "test1" in
the
> inbox sorter. I then changed the subject heading in the test1 reply to
> "test2" and sent it to myself. This time Outlook re-categorized it and
> put it in a separate compartment in the view called "test2".
> 
> If Outlook can do threading the way the R mail server does, I don't
> think this is the way to do it.
> 
> Unless someone has an idea of how to correctly set up Outlook to do
> threading in the manner that the R mail server does, I think the
message
> for us Outlook users is to just create, from scratch, a new message
when
> initiating a new subject.
> 
> Thanks for all your help. 
> 
> Mark
> 
> -----Original Message-----
> From: Bert Gunter [mailto:gunter.berton at gene.com] 
> Sent: Wednesday, January 31, 2007 7:03 PM
> To: Kimpel, Mark William
> Subject: Outlook does threading
> 
>  Mark:
> 
> No need to bother the R list with this. Outlook does threading. Just
> sort on
> Subject in the viewer.
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kimpel, Mark
> William
> Sent: Wednesday, January 31, 2007 3:36 PM
> To: Peter Dalgaard
> Cc: r-help at stat.math.ethz.ch; bioconductor at stat.math.ethz.ch
> Subject: Re: [R] possible spam alert
> 
> Peter,
> 
> Thanks you for your explanation, I had taken Mr. Connolly's message to
> me to imply that I was not changing the subject line. I use MS Outlook
> 2007 and, unless I am just not seeing it, Outlook does not normally
> display the "in reply to" header, I was under the mistaken impression
> that that was what the Subject line was for. See, for example, the
> header to your message to me below. Outlook will, however, sort
messages
> by Subject, and that is what I thought was meant by threading.
> 
> Well, I learned something today and apologize for any inconvenience my
> posts may have caused.
> 
> BTW, I use Outlook because it is supported by my university server and
> will synch my appointments and contacts with my PDA, which runs
Windows
> CE. If anyone has a suggestion for me of a better email program that
> will provide proper threading AND work with a MS email server and
synch
> with Windows CE, I'd love to hear it.
> 
> Thanks again,
> 
> Mark
> 
> Mark W. Kimpel MD 
> 
>  
> 
> (317) 490-5129 Work, & Mobile
> 
>  
> 
> (317) 663-0513 Home (no voice mail please)
> 
> 1-(317)-536-2730 FAX
> 
> 
> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> Sent: Wednesday, January 31, 2007 6:25 PM
> To: Kimpel, Mark William
> Cc: bioconductor at stat.math.ethz.ch; r-help at stat.math.ethz.ch
> Subject: Re: [R] possible spam alert
> 
> Kimpel, Mark William wrote:
> 
>>The last two times I have originated message threads on R or
>>Bioconductor I have received the message included below from someone
>>named Patrick Connolly. Both times I was the originator of the message
>>thread and used what I thought was a unique subject line that
> 
> explained
> 
>>as best I could what my question was. Patrick seems to be implying
> 
> that
> 
>>I am abusing the R and BioC help newsgroups in this fashion. 
>>
>>When I emailed him to give me a specific example, he did not reply.
> 
> The
> 
>>most recent thread that he seems concerned about was to the R list and
>>was entitled "regexpr and parsing question" . I believe the previous
>>post of mine that he had problems with was to the BioC list but I
> 
> can't
> 
>>remember its subject.
>>
>>Is this spam?
>>  
> 
> No. Breach of netiquette, yes.
> 
> The message in question starts a new thread, yet contains an 
> In-Reply-To: header line, which presumably means that you started 
> writing the message as a reply to something completely unrelated, 
> specifically: "Re: [R] change plotting symbol for groups in trellis 
> graph". You should not do that, unless you know how to remove the 
> In-Reply-To line (and this is not obvious in many mail clients); 
> changing the subject is not sufficient.
> 
>>If I am doing this correctly, you should see the subject "possible
> 
> spam
> 
>>alert" in the subject header of THIS message.
>>
>>Would the moderators of the lists please check and see if I am doing
>>some wrong and, if not, inform Mr. Connolly that I am not. If others
>>have received this message in error, it is possible it is spam and
> 
> users
> 
>>should be alerted.
>>
>>Thanks,
>>
>>Mark
>>
>>Mark W. Kimpel MD 
>>
>> 
>>
>> 
>>
>>Official Business Address:
>>
>> 
>>
>>Department of Psychiatry
>>
>>Indiana University School of Medicine
>>
>>PR M116
>>
>>Institute of Psychiatric Research
>>
>>791 Union Drive
>>
>>Indianapolis, IN 46202
>>
>> 
>>This is a request to anyone who starts a new subject to begin with a
> 
> new
> 
>>message and NOT reply to an existing one.  If your mail client is any
>>good, it's very simple to set up an alias (mine is simply 'r') so that
>>the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary
> 
> and
> 
>>it's quicker than scrolling through an address book.
>>It's also quicker than deleting the previous subject.
>>
>>Most mornings, I have over a screenful of messages mostly from R-help
>>and it's very useful to have them threaded.  However, the usefulness
> 
> of
> 
>>threading is lost when posters reply to a message and then change the
>>subject instead of creating a new message.
>>
>>People who don't have a mail client that can display email in threads
>>are probably unaware that this sort of thing can happen in ones that
> 
> do:
> 
>>
>>    37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
>>    38 N   25 Jan Uwe Ligges              ( 55) `-> 
>>    39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
>>histograms
>>->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
>>    41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
>>    42     26 Jan Qin Xin                 (  9) | `->[R] how could I
> 
> add
> 
>>legends
>>    43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
>>    44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
>>coloured his
>>    45 N   26 Jan Damon Wischik           (125) `-> 
>>    46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting
> 
> primatives,
> 
>>ellipse
>>    47 N   25 Jan Uwe Ligges              ( 19) `->   
>>
>>
>>As Martin Maechler explained some time ago, it also screws up the
>>archives for a similar reason.
>>
>>Your cooperation will be greatly appreciated.
>>
>>best
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrstear at sandia.gov  Thu Feb  1 02:59:30 2007
From: jrstear at sandia.gov (Jon Stearley)
Date: Wed, 31 Jan 2007 18:59:30 -0700
Subject: [R] memory-efficient column aggregation of a sparse matrix
Message-ID: <6D0C7829-2699-4D85-9D8D-A8696EF1AE6E@sandia.gov>

I need to sum the columns of a sparse matrix according to a factor -  
ie given a sparse matrix X and a factor fac of length ncol(X), sum  
the elements by column factors and return the sparse matrix Y of size  
nrow(X) by nlevels(f).  The appended code does the job, but is  
unacceptably memory-bound because tapply() uses a non-sparse  
representation.  Can anyone suggest a more memory and cpu efficient  
approach?  Eg, a sparse matrix tapply method?  Thanks.

-- 
+--------------------------------------------------------------+
| Jon Stearley                  (505) 845-7571  (FAX 844-9297) |
| Sandia National Laboratories  Scalable Systems Integration   |
+--------------------------------------------------------------+


# x and y are of SparseM class matrix.csr
"aggregate.csr" <-
function(x, fac) {
         # make a vector indicating the row of each nonzero
         rows <- integer(length=length(x at ra))
         rows[x at ia[1:nrow(x)]] <- 1 # put a 1 at start of each row
         rows <- as.integer(cumsum(rows)) # and finish with a cumsum

         # make a vector indicating the column factor of each nonzero
         f <- fac[x at ja]

         # aggregate by row,f
         y <- tapply(x at ra, list(rows,f), sum)

         # sparsify it
         y[is.na(y)] <- 0  # change tapply NAs to as.matrix.csr 0s
         y <- as.matrix.csr(y)

         y
}


From adam_6242 at yahoo.com  Thu Feb  1 03:39:21 2007
From: adam_6242 at yahoo.com (aat)
Date: Wed, 31 Jan 2007 18:39:21 -0800 (PST)
Subject: [R] traverse through many columns of a matrix in a function
Message-ID: <8742152.post@talk.nabble.com>


Hello everyone,

Here is the setup.

z is a 119 x 15 matrix, m_index is a 119 x 5 matrix

What I am trying to do is return the results from fitCopula by sequentially
binding all 15 columns of z to the first column of m_index,
(cbind(z[,1],m_index[,1]),(cbind(z[,2],m_index[,1]), etc.

Unfortunately, my code below only binds z[,1] and m_index[,1] and return the
same result 14 times.

Any ideas on how to implement my idea would be greatly appreciated. Also,
why am I getting the same result 14 times, shouldn't it be 15 since there
are 15 columns in z?


my.cop <- normalCopula(param = 0.5, dim = 2) 
answer <- apply(z[,-1], 2, function(m_index[,1],my.cop)
fitCopula(cbind(z[,-1], m_index[,1]), my.cop, start=0.3), my.cop = my.cop)

Thanks for help.

Adam (aat)

-- 
View this message in context: http://www.nabble.com/traverse-through-many-columns-of-a-matrix-in-a-function-tf3152747.html#a8742152
Sent from the R help mailing list archive at Nabble.com.


From murdoch at stats.uwo.ca  Thu Feb  1 04:01:06 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 31 Jan 2007 22:01:06 -0500
Subject: [R] rgl.postscript{rgl} crashes R
In-Reply-To: <062AE320EF971E40ACD0F6C93391D769C6C835@exqld1-tsv.nexus.csiro.au>
References: <062AE320EF971E40ACD0F6C93391D769C6C835@exqld1-tsv.nexus.csiro.au>
Message-ID: <45C157F2.5070309@stats.uwo.ca>

On 1/31/2007 8:15 PM, Alexander.Herr at csiro.au wrote:
> Hi list,
> 
> I am trying to print a rgl scene. I can do this with
> rgl.snapshot("test.pnt",fmt="png"), but
> rgl.postscript("test.pdf",fmt="pdf") does crash R and returns to the
> linux shell after extented time period.
> 
> I am running R 2.4.0 on i686 linux mandrake 10.2. Do I need any other R
> external program to make this work?

You aren't using the latest R, and probably aren't using the latest rgl. 
  Try updating both:  R to 2.4.1, rgl to 0.70.  The rgl ChangeLog shows 
this for 0.70:

   - fixed bug in rgl.postscript in Linux, added text support to it


From forestfloor at comcast.net  Thu Feb  1 04:41:00 2007
From: forestfloor at comcast.net (Forest Floor)
Date: Wed, 31 Jan 2007 22:41:00 -0500
Subject: [R] Loading functions in R
Message-ID: <45C1614C.5090206@comcast.net>

Hi all,

This information must be out there, but I can't seem to find it.  What I 
want to do is to store functions I've created (as .R files or in 
whatever form) and then load them when I need them (or on startup) so 
that I can access without cluttering my program with the function code.  
This seems like it should be easy, but....

Thanks!

Jeff


From andy_liaw at merck.com  Thu Feb  1 04:48:17 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 31 Jan 2007 22:48:17 -0500
Subject: [R] [BioC] Outlook does threading  [Broadcast]
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4AD1B@iu-mssg-mbx106.ads.iu.edu>
References: <836F00680EECD340A96AD34ECFF3B534B4AD10@iu-mssg-mbx106.ads.iu.edu>
	<002a01c74594$5ce58310$4d908980@gne.windows.gene.com>
	<836F00680EECD340A96AD34ECFF3B534B4AD1B@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03A20C97@usctmx1106.merck.com>

This is really off-topic for both BioC and R-help, so I'll 
keep it short. 


From: Kimpel, Mark William
> 
> See below for Bert Gunter's off list reply to me (which I do 
> appreciate). I'm putting it back on the list because it seems 
> there is still confusion regarding the difference between 
> threading and sorting by subject. I thought the example I 
> will give below will serve as instructional for other Outlook 
> users who may be similarly confused as I was (am?). 
> 
> Per Bert's instructions, I just set up my inbox to sort by 
> subject. I sent one email to myself with the subject "test1" 
> and then replied to it without changing the subject. The 
> reply correctly went to "test1" in the inbox sorter. I then 
> changed the subject heading in the test1 reply to "test2" and 
> sent it to myself. This time Outlook re-categorized it and 
> put it in a separate compartment in the view called "test2".
> 
> If Outlook can do threading the way the R mail server does, I 
> don't think this is the way to do it.

AFAIK there's no proper way to get the correct threading in 
Outlook.  What I do is group by conversation topic, but that
doesn't solve the problem.  This is only problem on your
(and all Outlook users'?) end, though.  The bigger problem
that affects the lists is that some versions of MS Exchange 
Server do not include the "In-reply-to" header field that
many mailing lists rely on for proper threading.  As a result,
when I reply to other people's post, it may show up in Outlook
as having been threaded properly (because the subject is fine),
but it throws everything else that does proper threading off.
 
> Unless someone has an idea of how to correctly set up Outlook 
> to do threading in the manner that the R mail server does,

Maybe some VBA coding can be done to get it right, but short
of that, I very much doubt it.

> I 
> think the message for us Outlook users is to just create, 
> from scratch, a new message when initiating a new subject.

That message ought to be clear for everyone.  You should
never reply to a message when you really mean to start
a new topic, regardless what you are using.

Andy
 
> Thanks for all your help. 
> 
> Mark
> 
> -----Original Message-----
> From: Bert Gunter [mailto:gunter.berton at gene.com]
> Sent: Wednesday, January 31, 2007 7:03 PM
> To: Kimpel, Mark William
> Subject: Outlook does threading
> 
>  Mark:
> 
> No need to bother the R list with this. Outlook does 
> threading. Just sort on Subject in the viewer.
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Kimpel, Mark William
> Sent: Wednesday, January 31, 2007 3:36 PM
> To: Peter Dalgaard
> Cc: r-help at stat.math.ethz.ch; bioconductor at stat.math.ethz.ch
> Subject: Re: [R] possible spam alert
> 
> Peter,
> 
> Thanks you for your explanation, I had taken Mr. Connolly's 
> message to me to imply that I was not changing the subject 
> line. I use MS Outlook
> 2007 and, unless I am just not seeing it, Outlook does not 
> normally display the "in reply to" header, I was under the 
> mistaken impression that that was what the Subject line was 
> for. See, for example, the header to your message to me 
> below. Outlook will, however, sort messages by Subject, and 
> that is what I thought was meant by threading.
> 
> Well, I learned something today and apologize for any 
> inconvenience my posts may have caused.
> 
> BTW, I use Outlook because it is supported by my university 
> server and will synch my appointments and contacts with my 
> PDA, which runs Windows CE. If anyone has a suggestion for me 
> of a better email program that will provide proper threading 
> AND work with a MS email server and synch with Windows CE, 
> I'd love to hear it.
> 
> Thanks again,
> 
> Mark
> 
> Mark W. Kimpel MD 
> 
>  
> 
> (317) 490-5129 Work, & Mobile
> 
>  
> 
> (317) 663-0513 Home (no voice mail please)
> 
> 1-(317)-536-2730 FAX
> 
> 
> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
> Sent: Wednesday, January 31, 2007 6:25 PM
> To: Kimpel, Mark William
> Cc: bioconductor at stat.math.ethz.ch; r-help at stat.math.ethz.ch
> Subject: Re: [R] possible spam alert
> 
> Kimpel, Mark William wrote:
> > The last two times I have originated message threads on R or 
> > Bioconductor I have received the message included below 
> from someone 
> > named Patrick Connolly. Both times I was the originator of 
> the message 
> > thread and used what I thought was a unique subject line that
> explained
> > as best I could what my question was. Patrick seems to be implying
> that
> > I am abusing the R and BioC help newsgroups in this fashion. 
> >
> > When I emailed him to give me a specific example, he did not reply.
> The
> > most recent thread that he seems concerned about was to the 
> R list and 
> > was entitled "regexpr and parsing question" . I believe the 
> previous 
> > post of mine that he had problems with was to the BioC list but I
> can't
> > remember its subject.
> >
> > Is this spam?
> >   
> No. Breach of netiquette, yes.
> 
> The message in question starts a new thread, yet contains an
> In-Reply-To: header line, which presumably means that you 
> started writing the message as a reply to something 
> completely unrelated,
> specifically: "Re: [R] change plotting symbol for groups in 
> trellis graph". You should not do that, unless you know how 
> to remove the In-Reply-To line (and this is not obvious in 
> many mail clients); changing the subject is not sufficient.
> > If I am doing this correctly, you should see the subject "possible
> spam
> > alert" in the subject header of THIS message.
> >
> > Would the moderators of the lists please check and see if I 
> am doing 
> > some wrong and, if not, inform Mr. Connolly that I am not. 
> If others 
> > have received this message in error, it is possible it is spam and
> users
> > should be alerted.
> >
> > Thanks,
> >
> > Mark
> >
> > Mark W. Kimpel MD
> >
> >  
> >
> >  
> >
> > Official Business Address:
> >
> >  
> >
> > Department of Psychiatry
> >
> > Indiana University School of Medicine
> >
> > PR M116
> >
> > Institute of Psychiatric Research
> >
> > 791 Union Drive
> >
> > Indianapolis, IN 46202
> >
> >  
> > This is a request to anyone who starts a new subject to begin with a
> new
> > message and NOT reply to an existing one.  If your mail 
> client is any 
> > good, it's very simple to set up an alias (mine is simply 
> 'r') so that 
> > the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary
> and
> > it's quicker than scrolling through an address book.
> > It's also quicker than deleting the previous subject.
> >
> > Most mornings, I have over a screenful of messages mostly 
> from R-help 
> > and it's very useful to have them threaded.  However, the usefulness
> of
> > threading is lost when posters reply to a message and then 
> change the 
> > subject instead of creating a new message.
> >
> > People who don't have a mail client that can display email 
> in threads 
> > are probably unaware that this sort of thing can happen in ones that
> do:
> >
> >
> >     37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
> >     38 N   25 Jan Uwe Ligges              ( 55) `-> 
> >     39 N   25 Jan Fernando Henrique Ferra ( 20) [R] 
> Plotting coloured
> > histograms
> > ->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] 
> Distributions.
> >     41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
> >     42     26 Jan Qin Xin                 (  9) | `->[R] how could I
> add
> > legends
> >     43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
> >     44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
> > coloured his
> >     45 N   26 Jan Damon Wischik           (125) `-> 
> >     46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting
> primatives,
> > ellipse
> >     47 N   25 Jan Uwe Ligges              ( 19) `->   
> >
> >
> > As Martin Maechler explained some time ago, it also screws up the 
> > archives for a similar reason.
> >
> > Your cooperation will be greatly appreciated.
> >
> > best
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> _______________________________________________
> Bioconductor mailing list
> Bioconductor at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/bioconductor
> Search the archives: 
> http://news.gmane.org/gmane.science.biology.informatics.conductor
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From arun.kumar.saha at gmail.com  Thu Feb  1 05:44:49 2007
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Thu, 1 Feb 2007 10:14:49 +0530
Subject: [R] Combining two datasets
Message-ID: <d4c57560701312044l72e6531dy4de5475b9a41023@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/66748b3d/attachment.pl 

From christos at nuverabio.com  Thu Feb  1 05:46:54 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 31 Jan 2007 23:46:54 -0500
Subject: [R] Loading functions in R
In-Reply-To: <45C1614C.5090206@comcast.net>
References: <45C1614C.5090206@comcast.net>
Message-ID: <001701c745bb$ffffbee0$0202a8c0@headquarters.silicoinsights>

The recommended approach is to make a package for your functions that will
include documentation, error checks etc.
Another way to accomplish what you want is to start a new R session and
'source' your .R files and then to save the workspace in a .RData file, e.g.
myFunctions.RData.

Finally

attach(myFunctions.RData)  

should do the trick without cluttering your workspace.

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Forest Floor
Sent: Wednesday, January 31, 2007 10:41 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Loading functions in R

Hi all,

This information must be out there, but I can't seem to find it.  What I
want to do is to store functions I've created (as .R files or in whatever
form) and then load them when I need them (or on startup) so that I can
access without cluttering my program with the function code.  
This seems like it should be easy, but....

Thanks!

Jeff

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From xmeng at capitalbio.com  Thu Feb  1 07:21:34 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Thu, 01 Feb 2007 14:21:34 +0800
Subject: [R] line plot
Message-ID: <370310894.11100@capitalbio.com>

Hello sir: 
I wanna get such kind of plot: a line whose start point is(1,10),end point is(5,10) 

In other words: 

How can I draw a line if I only know the coordinate of the start point and end point? Thanks! My best


From lobry at biomserv.univ-lyon1.fr  Thu Feb  1 07:22:41 2007
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Thu, 1 Feb 2007 07:22:41 +0100
Subject: [R] prop.test() references
Message-ID: <p06002012c1e7360ad1bb@[134.214.236.113]>

Dear R-help,

I'm using prop.test() to compute a confidence interval for a proportion
under R version 2.4.1, as in:

prop.test(x = 340, n = 400)$conf
[1] 0.8103309 0.8827749

I have two questions:

1) from the source code my understanding is that the confidence
interval is computed according to Wilson, E.B. (1927) Probable
inference, the law of succession, and statistical inference.
J. Am. Stat. Assoc., 22:209-212.
Is it correct?

2) The doc says "Continuity correction is used only if it does not exceed
the difference between sample and null proportions in absolute value."
Does someone has a reference in which this point is discussed?

Best,
-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From petr.pikal at precheza.cz  Thu Feb  1 08:14:52 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 01 Feb 2007 08:14:52 +0100
Subject: [R] line plot
In-Reply-To: <370310894.11100@capitalbio.com>
Message-ID: <45C1A17C.26755.722E58@localhost>

Hi

see ?segments
segments(1,10,5,10)

HTH
Petr


On 1 Feb 2007 at 14:21, XinMeng wrote:

From:           	"XinMeng" <xmeng at capitalbio.com>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Thu, 01 Feb 2007 14:21:34 +0800
Subject:        	[R] line plot
Send reply to:  	XinMeng <xmeng at capitalbio.com>
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hello sir: 
> I wanna get such kind of plot: a line whose start point is(1,10),end
> point is(5,10) 
> 
> In other words: 
> 
> How can I draw a line if I only know the coordinate of the start point
> and end point? Thanks! My best
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From mpiktas at gmail.com  Thu Feb  1 08:43:54 2007
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Thu, 1 Feb 2007 09:43:54 +0200
Subject: [R] features of save and save.image (unexpected file sizes)
In-Reply-To: <45C0DA8C.2020902@stats.ox.ac.uk>
References: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com>
	<45C0DA8C.2020902@stats.ox.ac.uk>
Message-ID: <e47808320701312343i6fd6a2d1le55175dafeba26c6@mail.gmail.com>

Hi,

On 1/31/07, Professor Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Two comments:
>
> 1) ls() does not list all the objects: it has all.names argument.
>
Yes, I tried it with all.names, but the effect was the same, I forgot
to mention it in a letter.

> 2) save.image() does not just save the objects in the workspace, it also
> saves any environments they may have.  Having a function with a
> large environment is the usual cause of a large saved image.

I have little experience dealing with environments, so is there a
quick way to discard the environments of the functions? When saving
the session I really do not need them.

>
> (And finally, a compressed binary representation from save.image is
> nor comparable sizewise with an ASCII version from dump.)
>
I know, but thus I found out that I am saving something besides my large object.

Thanks very much for your answer!


Vaidotas Zemlys
--
Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
Vilnius University


From justin_bem at yahoo.fr  Thu Feb  1 11:17:01 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Thu, 1 Feb 2007 10:17:01 +0000 (GMT)
Subject: [R] Re :  Combining two datasets
Message-ID: <434975.45456.qm@web23011.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070201/000025ab/attachment.pl 

From shubhak at ambaresearch.com  Thu Feb  1 11:08:07 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Thu, 1 Feb 2007 15:38:07 +0530
Subject: [R] Sleep Function
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3EB9F08@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/4d2016c6/attachment.pl 

From debmidya at yahoo.com  Thu Feb  1 11:34:39 2007
From: debmidya at yahoo.com (Deb Midya)
Date: Thu, 1 Feb 2007 02:34:39 -0800 (PST)
Subject: [R] Calling C code from R
Message-ID: <835531.21036.qm@web50407.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/4f8be0ff/attachment.pl 

From anil_rohilla at rediffmail.com  Thu Feb  1 08:33:52 2007
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 1 Feb 2007 07:33:52 -0000
Subject: [R] Wavlet filter using morlet mother wavelet
Message-ID: <1168576702.S.2831.29254.webmail61.rediffmail.com.old.1170315232.1375@webmail.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/f57c070b/attachment.pl 

From stat700004 at yahoo.co.in  Thu Feb  1 08:37:09 2007
From: stat700004 at yahoo.co.in (stat stat)
Date: Thu, 1 Feb 2007 07:37:09 +0000 (GMT)
Subject: [R] Extracting part of date variable
Message-ID: <890150.12203.qm@web7605.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/dad9a124/attachment.pl 

From wwwhsd at gmail.com  Thu Feb  1 12:00:28 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 1 Feb 2007 09:00:28 -0200
Subject: [R] Extracting part of date variable
In-Reply-To: <890150.12203.qm@web7605.mail.in.yahoo.com>
References: <890150.12203.qm@web7605.mail.in.yahoo.com>
Message-ID: <da79af330702010300w2f1e0c3fy7f44d81d860700d1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/d968c369/attachment.pl 

From jgarcia at ija.csic.es  Thu Feb  1 12:05:00 2007
From: jgarcia at ija.csic.es (javier garcia-pintado)
Date: Thu, 01 Feb 2007 12:05:00 +0100
Subject: [R] indexing
Message-ID: <45C1C95C.8090605@ija.csic.es>

Hello,
In a nutshell, I've got a data.frame like this:

> assignation <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
> assignation
  value class
1   6.5     1
2   7.5     3
3   8.5     5
4  12.0     2
>   

and a long vector of classes like this:

> x <- c(1,1,2,7,6,5,4,3,2,2,2...)

And would like to obtain  a vector of length = length(x), with the
corresponding values extracted from assignation table. Like this:
> x.value
 [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0

Could you help me with an elegant way to do this ?
(I just can do it with looping for each class in the assignation table,
what a think is not perfect in R's sense)

Wishes,
Javier
-- 

Javier Garc?a-Pintado
Institute of Earth Sciences Jaume Almera (CSIC)
Lluis Sole Sabaris s/n, 08028 Barcelona
Phone: +34 934095410
Fax:   +34 934110012
e-mail:jgarcia at ija.csic.es 


From P.Dalgaard at biostat.ku.dk  Thu Feb  1 12:04:27 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 01 Feb 2007 12:04:27 +0100
Subject: [R] Calling C code from R
In-Reply-To: <835531.21036.qm@web50407.mail.yahoo.com>
References: <835531.21036.qm@web50407.mail.yahoo.com>
Message-ID: <45C1C93B.2050806@biostat.ku.dk>

Deb Midya wrote:
> Hi!
>    
>   Thanks in advance.
>    
>   I am using R-2.4.0 on Windows XP. I am trying to create dll file.
>    
>   My C code:
>    
>   /* useC1.c */
>   void useC(int *i) {
>     i[6] = 100;
> }
>    
>   I have tried to create useC1.dll. 
>    
>   C:\R-2.4.0\bin>R CMD SHLIB useC1.c
>    
>   'perl' is not recognized as an internal or external command, operable program or batch file.
>    
>   Then I have tried:
>    
>   C:\R-2.4.0\bin>Rcmd SHLIB useC1.c
>    
>   'perl' is not recognized as an internal or external command, operable program or batch file.
>    
>   I am looking forward for your reply.
>    
>   
Did you install Perl? and did you read
http://cran.r-project.org/doc/manuals/R-admin.html#The-Windows-toolset
and http://cran.r-project.org/doc/manuals/R-exts.html#Creating-R-packages?

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From wl2776 at gmail.com  Thu Feb  1 12:21:47 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Thu, 1 Feb 2007 03:21:47 -0800 (PST)
Subject: [R] Calling C code from R
In-Reply-To: <835531.21036.qm@web50407.mail.yahoo.com>
References: <835531.21036.qm@web50407.mail.yahoo.com>
Message-ID: <8746593.post@talk.nabble.com>


You need to install perl and MinGW, at least.
If you have them installed, then you need to properly set PATH environment
variable and, probably, restart your command line session.

See chapter 5 of the manual "Writing R extensions" (installed in
R_HOME/doc/manual)
and these two links

http://www.murdoch-sutherland.com/Rtools/
http://www.stats.uwo.ca/faculty/murdoch/software/debuggingR/

Also, it would be great to upgrade R to 2.4.1


Deb Midya wrote:
> 
>   I am using R-2.4.0 on Windows XP. I am trying to create dll file.
>   My C code:
>   /* useC1.c */
>   void useC(int *i) {
>     i[6] = 100;
> }
>    
>   I have tried to create useC1.dll. 
>   C:\R-2.4.0\bin>R CMD SHLIB useC1.c
>   'perl' is not recognized as an internal or external command, operable
> program or batch file.
> 
>   Then I have tried:
>   C:\R-2.4.0\bin>Rcmd SHLIB useC1.c
>   'perl' is not recognized as an internal or external command, operable
> program or batch file.
> 

-- 
View this message in context: http://www.nabble.com/-R--Calling-C-code-from-R-tf3154058.html#a8746593
Sent from the R help mailing list archive at Nabble.com.


From P.Dalgaard at biostat.ku.dk  Thu Feb  1 12:48:15 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 01 Feb 2007 12:48:15 +0100
Subject: [R] Extracting part of date variable
In-Reply-To: <890150.12203.qm@web7605.mail.in.yahoo.com>
References: <890150.12203.qm@web7605.mail.in.yahoo.com>
Message-ID: <45C1D37F.8070009@biostat.ku.dk>

stat stat wrote:
> Dear all,
>    
>   Suppose I have a date variable:
>    
>   c = "99/05/12"
>    
>   I want to extract the parts of this date like month number, year and day. I can do it in SPSS. Is it possible to do this in R as well?
>    
>   Rgd,
>
>   
Yes. One way is to use substr(), e.g.:

> substr(c,1,2)
[1] "99"
> as.numeric(substr(c,1,2))
[1] 99

This also nicely sidesteps the ambiguity issue: 1999 or 1899? May or
December? On the other hand, you'll get in trouble if leading zeros are
sometimes absent (strsplit() or gsub() if you want to pursue that route
further).

For a more principled approach, use the time and date handling tools.
Assuming that you can live with the system defaults for 2-digit years,

> strptime(c,format="%y/%m/%d")
[1] "1999-05-12"
> strptime(c,format="%y/%m/%d")$year
[1] 99
> strptime(c,format="%y/%m/%d")$mon
[1] 4
> strptime(c,format="%y/%m/%d")$mday
[1] 12

Beware the peculiarities of the entries defined by POSIX standard, see
?DateTimeClasses, and also:

     '%y' Year without century (00-99). If you use this on input, which
          century you get is system-specific.  So don't!  Often values
          up to 69 (or 68) are prefixed by 20 and 70(or 69) to 99 by
          19.

(I'm at a bit of a loss as to fixing up two digit years once the damage
has been done. Presumably, you can just diddle the year field, but I'm a
bit uneasy about the fact that  2000 was a leap year and 1900 was not.)



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From fabascal at cnb.uam.es  Thu Feb  1 12:56:59 2007
From: fabascal at cnb.uam.es (Federico Abascal)
Date: Thu, 01 Feb 2007 12:56:59 +0100
Subject: [R] matrix of matrices
Message-ID: <45C1D58B.4080301@cnb.uam.es>

Dear all,

it is likely a stupid question but I cannot solve it.

I want to have a matrix of 100 elements.
Each element must be a vector of 500 elements.

If I do:
    imp<-array(dim=100)
    imp[1]<-vector(length=500)
it does not work. Warning message: "number of items to replace is not a
multiple of replacement length"

If I do:
    imp <- array(dim=c(100,500))   
and then fill imp:
    for(i in c(1:500)) {
        imp[i,] <- im[1:500,]
        #im[1:500,] is a vector of length 500, of class numeric. IT
CONTAINS NAMES!
    }

Now it works, but I loose the labels (names) associated to the original
"im" variable.
If I just do:
    j<- im[1:500,]
I do not loose the labels.

names(j) => list of labels
names(imp[1,]) => NULL

Any clue?

Thanks in advance!
Federico


From ggrothendieck at gmail.com  Thu Feb  1 12:57:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Feb 2007 06:57:57 -0500
Subject: [R] Extracting part of date variable
In-Reply-To: <890150.12203.qm@web7605.mail.in.yahoo.com>
References: <890150.12203.qm@web7605.mail.in.yahoo.com>
Message-ID: <971536df0702010357t4105f7cfi54eb08a9ac43064a@mail.gmail.com>

Read the help desk article in R News 4/1 about dates and note the table at the
end of it, in particular.

On 2/1/07, stat stat <stat700004 at yahoo.co.in> wrote:
> Dear all,
>
>  Suppose I have a date variable:
>
>  c = "99/05/12"
>
>  I want to extract the parts of this date like month number, year and day. I can do it in SPSS. Is it possible to do this in R as well?
>
>  Rgd,
>
>
> ---------------------------------
>  Here's a new way to find what you're looking for - Yahoo! Answers
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From jholtman at gmail.com  Thu Feb  1 13:13:00 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 1 Feb 2007 07:13:00 -0500
Subject: [R] indexing
In-Reply-To: <45C1C95C.8090605@ija.csic.es>
References: <45C1C95C.8090605@ija.csic.es>
Message-ID: <644e1f320702010413v19ebd8ccw59901576e6259221@mail.gmail.com>

> assignation$value[match(x,assignation$class)]
 [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0


On 2/1/07, javier garcia-pintado <jgarcia at ija.csic.es> wrote:
> Hello,
> In a nutshell, I've got a data.frame like this:
>
> > assignation <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
> > assignation
>  value class
> 1   6.5     1
> 2   7.5     3
> 3   8.5     5
> 4  12.0     2
> >
>
> and a long vector of classes like this:
>
> > x <- c(1,1,2,7,6,5,4,3,2,2,2...)
>
> And would like to obtain  a vector of length = length(x), with the
> corresponding values extracted from assignation table. Like this:
> > x.value
>  [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0
>
> Could you help me with an elegant way to do this ?
> (I just can do it with looping for each class in the assignation table,
> what a think is not perfect in R's sense)
>
> Wishes,
> Javier
> --
>
> Javier Garc?a-Pintado
> Institute of Earth Sciences Jaume Almera (CSIC)
> Lluis Sole Sabaris s/n, 08028 Barcelona
> Phone: +34 934095410
> Fax:   +34 934110012
> e-mail:jgarcia at ija.csic.es
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From dimitris.rizopoulos at med.kuleuven.be  Thu Feb  1 13:29:13 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 1 Feb 2007 13:29:13 +0100
Subject: [R] indexing
References: <45C1C95C.8090605@ija.csic.es>
Message-ID: <00a901c745fc$94ec2e90$0540210a@www.domain>

one way is the following:

assignation$value[match(x, assignation$class)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "javier garcia-pintado" <jgarcia at ija.csic.es>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 01, 2007 12:05 PM
Subject: [R] indexing


> Hello,
> In a nutshell, I've got a data.frame like this:
>
>> assignation <- 
>> data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
>> assignation
>  value class
> 1   6.5     1
> 2   7.5     3
> 3   8.5     5
> 4  12.0     2
>>
>
> and a long vector of classes like this:
>
>> x <- c(1,1,2,7,6,5,4,3,2,2,2...)
>
> And would like to obtain  a vector of length = length(x), with the
> corresponding values extracted from assignation table. Like this:
>> x.value
> [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0
>
> Could you help me with an elegant way to do this ?
> (I just can do it with looping for each class in the assignation 
> table,
> what a think is not perfect in R's sense)
>
> Wishes,
> Javier
> -- 
>
> Javier Garc?a-Pintado
> Institute of Earth Sciences Jaume Almera (CSIC)
> Lluis Sole Sabaris s/n, 08028 Barcelona
> Phone: +34 934095410
> Fax:   +34 934110012
> e-mail:jgarcia at ija.csic.es
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Feb  1 13:52:48 2007
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 1 Feb 2007 13:52:48 +0100
Subject: [R] read.spss and encodings
Message-ID: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>

Hi!

I'm having trouble with importing spss files containing non-ascii characters 
(R 2.4.1, debian linux, i386). To reproduce:

Download the following file: 
http://statmath.wu-wien.ac.at/data/spss/de/comphomeneu.sav

require (foreign)
Sys.setlocale (locale="C")
read.spss("comphomeneu.sav")$ARBEIT[1]
# prints:
# [1] im B\374ro
# Levels: im B\374ro zuhause

\374 of course is actually a u-umlaut. However, I guess in the C locale it's 
not expected to print as such. But now try this (use any UTF-8 locale you may 
have installed):

Sys.setlocale (locale="de_DE.UTF-8")
read.spss("comphomeneu.sav")$ARBEIT[1]
# prints:
# [1]Error in print.default(xx, quote = quote, ...) :
#        invalid multibyte string

To me it looks, like read.spss () would probably need an encoding parameter, 
and / or some iconv () magic. Now, locale conversion always makes my head 
spin, so I thought I'd better post here, before calling this to be a bug in 
R. Two questions:

1) Is there some way to work around this, i.e. make sure it is converted to 
proper UTF-8 while importing? Am I missing something obvious?
2) Should I submit this as a bug report?

Thanks!
Thomas Friedrichsmeier
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070201/e419601f/attachment.bin 

From majid.iravani at wsl.ch  Thu Feb  1 14:11:09 2007
From: majid.iravani at wsl.ch (Majid Iravani)
Date: Thu, 01 Feb 2007 14:11:09 +0100
Subject: [R] How can I calculate conditional mean in a large dataset
 including date data
Message-ID: <5.2.1.1.1.20070201130111.0dd558f0@mailbi.wsl.ch>

Dear R users,

I have a dataframe with two columns: first column is date data (e.g. 
1/1/2000 with character format: daily data from 1/1/1970 till 31/12/2003) 
and second column is temperature value. Now I'd like to calculate mean for 
each month in a year (i.e. May 2001, June 1997) and mean for each month in 
all of years. As the number of days in some months is different from others 
I could not write appreciate command for this. Therefore I would greatly 
appreciate if somebody can help me in this case

Thank you
Majid
--------------------------------------------------------------------------------
  Majid Iravani
  PhD Student
  Swiss Federal Research Institute WSL
  Research Group of Vegetation Ecology
  Z?rcherstrasse 111  CH-8903 Birmensdorf  Switzerland
  Phone: +41-1-739-2693
  Fax: +41-1-739-2215
  Email: Majid.iravani at wsl.ch
http://www.wsl.ch/staff/majid.iravani/


From P.Dalgaard at biostat.ku.dk  Thu Feb  1 14:18:30 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 01 Feb 2007 14:18:30 +0100
Subject: [R] read.spss and encodings
In-Reply-To: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>
References: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>
Message-ID: <45C1E8A6.2090308@biostat.ku.dk>

Thomas Friedrichsmeier wrote:
> Hi!
>
> I'm having trouble with importing spss files containing non-ascii characters 
> (R 2.4.1, debian linux, i386). To reproduce:
>
> Download the following file: 
> http://statmath.wu-wien.ac.at/data/spss/de/comphomeneu.sav
>
> require (foreign)
> Sys.setlocale (locale="C")
> read.spss("comphomeneu.sav")$ARBEIT[1]
> # prints:
> # [1] im B\374ro
> # Levels: im B\374ro zuhause
>
> \374 of course is actually a u-umlaut. However, I guess in the C locale it's 
> not expected to print as such. But now try this (use any UTF-8 locale you may 
> have installed):
>
> Sys.setlocale (locale="de_DE.UTF-8")
> read.spss("comphomeneu.sav")$ARBEIT[1]
> # prints:
> # [1]Error in print.default(xx, quote = quote, ...) :
> #        invalid multibyte string
>
> To me it looks, like read.spss () would probably need an encoding parameter, 
> and / or some iconv () magic. Now, locale conversion always makes my head 
> spin, so I thought I'd better post here, before calling this to be a bug in 
> R. Two questions:
>
> 1) Is there some way to work around this, i.e. make sure it is converted to 
> proper UTF-8 while importing? Am I missing something obvious
>   
> 2) Should I submit this as a bug report?
>   
1) Yes, 2) No

This is really not in read.spss, but in R itself. The short version is
that in released versions, we have

> "Im B\374ro"
[1]Error: invalid multibyte string

which is indeed a buglet, since it is not good if you cannot output what
you can input (notice that there is no problem until you try to print).
In r-devel, this has become

> "Im B\374ro"
[1] "Im B\xfcro"

so that invalid multibytes at least do not cause error. However, the
real issue is that the string  is in the wrong encoding for your locale,
so you should convert it:

> iconv("Im B\xfcro", from="latin1", to="UTF-8")
[1] "Im B?ro"
> iconv("Im B\374ro",from="latin1", to="UTF-8")
[1] "Im B?ro"


-p

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bates at stat.wisc.edu  Thu Feb  1 14:22:07 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 1 Feb 2007 07:22:07 -0600
Subject: [R] memory-efficient column aggregation of a sparse matrix
In-Reply-To: <6D0C7829-2699-4D85-9D8D-A8696EF1AE6E@sandia.gov>
References: <6D0C7829-2699-4D85-9D8D-A8696EF1AE6E@sandia.gov>
Message-ID: <40e66e0b0702010522w2106a0d4i4b993366e92ed911@mail.gmail.com>

On 1/31/07, Jon Stearley <jrstear at sandia.gov> wrote:
> I need to sum the columns of a sparse matrix according to a factor -
> ie given a sparse matrix X and a factor fac of length ncol(X), sum
> the elements by column factors and return the sparse matrix Y of size
> nrow(X) by nlevels(f).  The appended code does the job, but is
> unacceptably memory-bound because tapply() uses a non-sparse
> representation.  Can anyone suggest a more memory and cpu efficient
> approach?  Eg, a sparse matrix tapply method?  Thanks.

This is the sort of operation that is much more easily performed in
the triplet representation of a sparse matrix where each nonzero
element is represented by its row index, column index and value.
Using that representation you could map the column indices according
to the factor then convert back to one of the other representations.
The only question would be what to do about nonzeros in different
columns of the original matrix that get mapped to the same element in
the result.  It turns out that in the sparse matrix code used by the
Matrix package the triplet representation allows for duplicate index
positions with the convention that the resulting value at a position
is the sum of the values of any triplets with that index pair.

If you decide to use this approach please be aware that the indices
for the triplet representation in the Matrix package are 0-based (as
in C code) not 1-based (as in R code).  (I imagine that Martin is
thinking "we really should change that" as he reads this part.)

>
> --
> +--------------------------------------------------------------+
> | Jon Stearley                  (505) 845-7571  (FAX 844-9297) |
> | Sandia National Laboratories  Scalable Systems Integration   |
> +--------------------------------------------------------------+
>
>
> # x and y are of SparseM class matrix.csr
> "aggregate.csr" <-
> function(x, fac) {
>          # make a vector indicating the row of each nonzero
>          rows <- integer(length=length(x at ra))
>          rows[x at ia[1:nrow(x)]] <- 1 # put a 1 at start of each row
>          rows <- as.integer(cumsum(rows)) # and finish with a cumsum
>
>          # make a vector indicating the column factor of each nonzero
>          f <- fac[x at ja]
>
>          # aggregate by row,f
>          y <- tapply(x at ra, list(rows,f), sum)
>
>          # sparsify it
>          y[is.na(y)] <- 0  # change tapply NAs to as.matrix.csr 0s
>          y <- as.matrix.csr(y)
>
>          y
> }
>


From fabascal at cnb.uam.es  Thu Feb  1 14:24:15 2007
From: fabascal at cnb.uam.es (Federico Abascal)
Date: Thu, 01 Feb 2007 14:24:15 +0100
Subject: [R] matrix of matrices
In-Reply-To: <45C1D58B.4080301@cnb.uam.es>
References: <45C1D58B.4080301@cnb.uam.es>
Message-ID: <45C1E9FF.2090908@cnb.uam.es>

For the case someone is interested in it, here it is the solution
somebody suggested me: to use a list.

imp <- vector("list", 100)
imp[[1]] <- im[1:500,]
names(imp[[1]]) => the list of labels of imp[1:500,]

Thanks!
Federico



Federico Abascal wrote:
> Dear all,
>
> it is likely a stupid question but I cannot solve it.
>
> I want to have a matrix of 100 elements.
> Each element must be a vector of 500 elements.
>
> If I do:
>     imp<-array(dim=100)
>     imp[1]<-vector(length=500)
> it does not work. Warning message: "number of items to replace is not a
> multiple of replacement length"
>
> If I do:
>     imp <- array(dim=c(100,500))   
> and then fill imp:
>     for(i in c(1:500)) {
>         imp[i,] <- im[1:500,]
>         #im[1:500,] is a vector of length 500, of class numeric. IT
> CONTAINS NAMES!
>     }
>
> Now it works, but I loose the labels (names) associated to the original
> "im" variable.
> If I just do:
>     j<- im[1:500,]
> I do not loose the labels.
>
> names(j) => list of labels
> names(imp[1,]) => NULL
>
> Any clue?
>
> Thanks in advance!
> Federico
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From mpiktas at gmail.com  Thu Feb  1 14:30:14 2007
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Thu, 1 Feb 2007 15:30:14 +0200
Subject: [R] features of save and save.image (unexpected file sizes)
In-Reply-To: <Pine.LNX.4.64.0702010747090.9746@gannet.stats.ox.ac.uk>
References: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com>
	<45C0DA8C.2020902@stats.ox.ac.uk>
	<e47808320701312334y5f181f15ic8fb952b756a0461@mail.gmail.com>
	<Pine.LNX.4.64.0702010747090.9746@gannet.stats.ox.ac.uk>
Message-ID: <e47808320702010530y1044427dw2815dda86a29ba3f@mail.gmail.com>

Hi,


On 2/1/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Thu, 1 Feb 2007, Vaidotas Zemlys wrote:
>
> > Hi,
> >
> > On 1/31/07, Professor Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> >> Two comments:
> >>
> >> 1) ls() does not list all the objects: it has all.names argument.
> >>
> > Yes, I tried it with all.names, but the effect was the same, I forgot
> > to mention it in a letter.
> >
> >> 2) save.image() does not just save the objects in the workspace, it also
> >> saves any environments they may have.  Having a function with a
> >> large environment is the usual cause of a large saved image.
> >
> > I have little experience dealing with enivronments, so is there a
> > quick way to discard the environments of the functions? When saving
> > the session I really do not need them.
>
> Change, not discard.  E.g. environment(f) <- .GlobalEnv.  If environments
> are not mentioned by anything saved, they will not be saved.
>

I found the culprit. I was parsing formulas in my code, and I saved
them in that large object. So the environment came with saved
formulas. Is there a nice way to say R: "please do not save the
environments with the formulas, I do not need them?"

This is what I was doing (I am discarding irrelevant code)

testf<- function(formula) {
   mainform <- formula
   if(deparse(mainform[[3]][[1]])!="|") pandterm("invalid conditioning
for main regression")
    mmodel <- substitute(y~x,list(y=mainform[[2]],x=mainform[[3]][[2]]))
    mmodel <- as.formula(mmodel)
   list(formula=list(main=mmodel))
}

when called
bu <- testf(lnp~I(CE/12000)+hhs|Country)

I get

ls(env=environment(bu$formula$main))
[1] "formula"  "mainform" "mmodel"

or in actual case, a lot of more objects, which I do not need, but
which take a lot of place. For the moment I solved the problem with

environment(mmodel) <- NULL

but is this correct R way?

Vaidotas Zemlys
--
Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
Vilnius University


From rkoenker at uiuc.edu  Thu Feb  1 14:30:48 2007
From: rkoenker at uiuc.edu (roger koenker)
Date: Thu, 1 Feb 2007 07:30:48 -0600
Subject: [R] memory-efficient column aggregation of a sparse matrix
In-Reply-To: <40e66e0b0702010522w2106a0d4i4b993366e92ed911@mail.gmail.com>
References: <6D0C7829-2699-4D85-9D8D-A8696EF1AE6E@sandia.gov>
	<40e66e0b0702010522w2106a0d4i4b993366e92ed911@mail.gmail.com>
Message-ID: <8BE5CD08-E3F7-4B55-B76A-6098F81FBF76@uiuc.edu>

Doug is right, I think, that this would be easier with full indexing
using the  matrix.coo classe, if you want to use SparseM.  But
then the tapply seems to be the way to go.

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Feb 1, 2007, at 7:22 AM, Douglas Bates wrote:

> On 1/31/07, Jon Stearley <jrstear at sandia.gov> wrote:
>> I need to sum the columns of a sparse matrix according to a factor -
>> ie given a sparse matrix X and a factor fac of length ncol(X), sum
>> the elements by column factors and return the sparse matrix Y of size
>> nrow(X) by nlevels(f).  The appended code does the job, but is
>> unacceptably memory-bound because tapply() uses a non-sparse
>> representation.  Can anyone suggest a more memory and cpu efficient
>> approach?  Eg, a sparse matrix tapply method?  Thanks.
>
> This is the sort of operation that is much more easily performed in
> the triplet representation of a sparse matrix where each nonzero
> element is represented by its row index, column index and value.
> Using that representation you could map the column indices according
> to the factor then convert back to one of the other representations.
> The only question would be what to do about nonzeros in different
> columns of the original matrix that get mapped to the same element in
> the result.  It turns out that in the sparse matrix code used by the
> Matrix package the triplet representation allows for duplicate index
> positions with the convention that the resulting value at a position
> is the sum of the values of any triplets with that index pair.
>
> If you decide to use this approach please be aware that the indices
> for the triplet representation in the Matrix package are 0-based (as
> in C code) not 1-based (as in R code).  (I imagine that Martin is
> thinking "we really should change that" as he reads this part.)
>
>>
>> --
>> +--------------------------------------------------------------+
>> | Jon Stearley                  (505) 845-7571  (FAX 844-9297) |
>> | Sandia National Laboratories  Scalable Systems Integration   |
>> +--------------------------------------------------------------+
>>
>>
>> # x and y are of SparseM class matrix.csr
>> "aggregate.csr" <-
>> function(x, fac) {
>>          # make a vector indicating the row of each nonzero
>>          rows <- integer(length=length(x at ra))
>>          rows[x at ia[1:nrow(x)]] <- 1 # put a 1 at start of each row
>>          rows <- as.integer(cumsum(rows)) # and finish with a cumsum
>>
>>          # make a vector indicating the column factor of each nonzero
>>          f <- fac[x at ja]
>>
>>          # aggregate by row,f
>>          y <- tapply(x at ra, list(rows,f), sum)
>>
>>          # sparsify it
>>          y[is.na(y)] <- 0  # change tapply NAs to as.matrix.csr 0s
>>          y <- as.matrix.csr(y)
>>
>>          y
>> }
>>


From demi.anderson at gmx.net  Thu Feb  1 14:34:21 2007
From: demi.anderson at gmx.net (Demi Anderson)
Date: Thu, 01 Feb 2007 14:34:21 +0100
Subject: [R] Index mapping on arrays
Message-ID: <20070201133421.275440@gmx.net>

Dear R-community,

I have some trouble with index mappings for arrays. If, for example,
I have the array

R> A <- array(1:9, c(3,3,2))

and two index mappings both of same size

R> x <- c(2, 3)
R> y <- c(1, 2)

Now I want to access the elements (A[1, x[i], y[i]])_i of A, i.e.
A[1, x[1], y[1]] = A[1, 2, 1] and A[1, x[2], y[2]] = A[1, 3, 2].

If I use

R> A[1, x, y]

I would get every combinations of indices of all elements of x and y
i.e. A[1, x[1], y[1]], A[1, x[1], y[2]], A[1, x[2], y[1]] and A[1,
x[2], y[2]]. But how can I access the elements (A[1, x[i], y[i]])_i.
My arrays dimensions are actually large in the second
component (for example the dimension might be 10*10000*10) so I'm
looking for a method avoiding loops.

The question is probably trivial for you, but I just could not
figure it out. So sorry for bugging you and many thanks in advance
for any help.

Best wishes, Demi Anderson.
-- 
"Feel free" - 10 GB Mailbox, 100 FreeSMS/Monat ...


From aa2007r at gmail.com  Thu Feb  1 14:35:58 2007
From: aa2007r at gmail.com (AA)
Date: Thu, 1 Feb 2007 08:35:58 -0500
Subject: [R] any implementations for adaptive modeling of time series?
In-Reply-To: <c5d31fc20701300509l777edf2dp5a8e50cf6bd8c6e0@mail.gmail.com>
References: <c5d31fc20701300509l777edf2dp5a8e50cf6bd8c6e0@mail.gmail.com>
Message-ID: <55dcc5de0702010535x78c9fd0dv781a4e2c016f7cb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/a436bf45/attachment.pl 

From bolker at zoo.ufl.edu  Thu Feb  1 14:42:29 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Thu, 1 Feb 2007 13:42:29 +0000 (UTC)
Subject: [R] Wiki for Graphics tips for MacOS X
References: <E9E8F95E-7610-4D61-ABF5-B8CEA952F1A3@MUOhio.edu>
	<971536df0701310911g597d73afiede5947437e35498@mail.gmail.com>
Message-ID: <loom.20070201T143915-738@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:

> 
> To get the best results you need to transfer it using vector
> graphics rather than bitmapped graphics:
> 
> http://www.stc-saz.org/resources/0203_graphics.pdf
> 
> There are a number of variations described here (see
> entire thread).  Its for UNIX and Windows but I think
> it would likely work similarly on Mac and Windows:
> 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32297.html
> 
   
  yes, but:

  the whole point of this discussion was that there _is_ no
vector format that anyone knows of that (1) can be reliably
created on MacOS using R/open source tools and (2) can be
reliably imported into MS Word (with working preview etc.).
The thread you reference assumes that one has a Windows machine
handy (with or without R installed) for creating WMF
graphics.
  Hence the advice to create a high-resolution PNG, which
seems to work well enough even if it is not optimal.

  cheers
    Ben Bolker


From dimitris.rizopoulos at med.kuleuven.be  Thu Feb  1 14:48:25 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 1 Feb 2007 14:48:25 +0100
Subject: [R] Index mapping on arrays
References: <20070201133421.275440@gmx.net>
Message-ID: <011d01c74607$a51f0f20$0540210a@www.domain>

probably you want something like the following:

A[cbind(rep(1, length(x)), x, y)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Demi Anderson" <demi.anderson at gmx.net>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 01, 2007 2:34 PM
Subject: [R] Index mapping on arrays


> Dear R-community,
>
> I have some trouble with index mappings for arrays. If, for example,
> I have the array
>
> R> A <- array(1:9, c(3,3,2))
>
> and two index mappings both of same size
>
> R> x <- c(2, 3)
> R> y <- c(1, 2)
>
> Now I want to access the elements (A[1, x[i], y[i]])_i of A, i.e.
> A[1, x[1], y[1]] = A[1, 2, 1] and A[1, x[2], y[2]] = A[1, 3, 2].
>
> If I use
>
> R> A[1, x, y]
>
> I would get every combinations of indices of all elements of x and y
> i.e. A[1, x[1], y[1]], A[1, x[1], y[2]], A[1, x[2], y[1]] and A[1,
> x[2], y[2]]. But how can I access the elements (A[1, x[i], y[i]])_i.
> My arrays dimensions are actually large in the second
> component (for example the dimension might be 10*10000*10) so I'm
> looking for a method avoiding loops.
>
> The question is probably trivial for you, but I just could not
> figure it out. So sorry for bugging you and many thanks in advance
> for any help.
>
> Best wishes, Demi Anderson.
> -- 
> "Feel free" - 10 GB Mailbox, 100 FreeSMS/Monat ...
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rolf at math.unb.ca  Thu Feb  1 14:48:36 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Thu, 1 Feb 2007 09:48:36 -0400 (AST)
Subject: [R] Estimation of discrete unimodal density
Message-ID: <200702011348.l11Dmai0011024@weisner.math.unb.ca>

Wessel van Wieringen wrote:

> A method for the estimation is univariate unimodal densities (with
> unknown mode) is described in "Statistical Inference under Order
> Restrictions" by Barlow et al.. Would anyone know whether there is an
> R-implementation (preferably with reference) for the estimation of
> univariate discrete unimodal densities (with unknown mode)? Thanks in
> advance for your help.

You could have a look at my ``isotonic'' package.  Go to:

	http://www.math.unb.ca/~rolf/Research/Packages/

Click on ``gzipped tar file for R'' under ``isotonic''.

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From marc_schwartz at comcast.net  Thu Feb  1 14:50:11 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 07:50:11 -0600
Subject: [R] prop.test() references
In-Reply-To: <p06002012c1e7360ad1bb@[134.214.236.113]>
References: <p06002012c1e7360ad1bb@[134.214.236.113]>
Message-ID: <1170337811.4970.6.camel@localhost.localdomain>

On Thu, 2007-02-01 at 07:22 +0100, Jean lobry wrote:
> Dear R-help,
> 
> I'm using prop.test() to compute a confidence interval for a proportion
> under R version 2.4.1, as in:
> 
> prop.test(x = 340, n = 400)$conf
> [1] 0.8103309 0.8827749
> 
> I have two questions:
> 
> 1) from the source code my understanding is that the confidence
> interval is computed according to Wilson, E.B. (1927) Probable
> inference, the law of succession, and statistical inference.
> J. Am. Stat. Assoc., 22:209-212.
> Is it correct?

Yes.

> 2) The doc says "Continuity correction is used only if it does not exceed
> the difference between sample and null proportions in absolute value."
> Does someone has a reference in which this point is discussed?

I believe that this is a modification by Newcombe. See:

Newcombe RG: Two-Sided Confidence Intervals for the Single Proportion:
Comparison of Seven Methods. Statistics in Medicine 1998;17:857-872.

Newcombe RG: Interval Estimation for the Difference Between Independent
Proportions: Comparison of Eleven Methods. Statistics in Medicine
1998;17:873-890.


HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Thu Feb  1 14:56:36 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Feb 2007 08:56:36 -0500
Subject: [R] Wiki for Graphics tips for MacOS X
In-Reply-To: <loom.20070201T143915-738@post.gmane.org>
References: <E9E8F95E-7610-4D61-ABF5-B8CEA952F1A3@MUOhio.edu>
	<971536df0701310911g597d73afiede5947437e35498@mail.gmail.com>
	<loom.20070201T143915-738@post.gmane.org>
Message-ID: <971536df0702010556x25bc2292pb2b3d4bd7f33a43b@mail.gmail.com>

On 2/1/07, Ben Bolker <bolker at zoo.ufl.edu> wrote:
> Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
>
> >
> > To get the best results you need to transfer it using vector
> > graphics rather than bitmapped graphics:
> >
> > http://www.stc-saz.org/resources/0203_graphics.pdf
> >
> > There are a number of variations described here (see
> > entire thread).  Its for UNIX and Windows but I think
> > it would likely work similarly on Mac and Windows:
> >
> > http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32297.html
> >
>
>  yes, but:
>
>  the whole point of this discussion was that there _is_ no
> vector format that anyone knows of that (1) can be reliably
> created on MacOS using R/open source tools and (2) can be
> reliably imported into MS Word (with working preview etc.).
> The thread you reference assumes that one has a Windows machine
> handy (with or without R installed) for creating WMF
> graphics.
>  Hence the advice to create a high-resolution PNG, which
> seems to work well enough even if it is not optimal.

AFAIK there do exist tools for the Mac for fig graphics and that was one of the
several solutions proposed there.


From candrews at buffalo.edu  Thu Feb  1 15:00:35 2007
From: candrews at buffalo.edu (Chris Andrews)
Date: Thu, 01 Feb 2007 09:00:35 -0500
Subject: [R] what is the purpose of an error message in uniroot?
Message-ID: <45C1F283.9000002@buffalo.edu>


Matt,

Some time back I didn't like the uniroot restriction either so I wrote a short function "manyroots" that breaks an interval into many shorter intervals and looks for a single root in each of them.  This function is NOT guaranteed to find all roots in an interval even if you specify many subintervals.  Graphing is always a good idea.  There is always a chance the function dips to or below the axis and back up in an arbitrarily short interval.  For example, f(x) = x^2.  If one of your subintervals doesn't happen to end at 0, you're out of luck.  (This is in contrast to uniroot working on a continuous function that is positive at one end of an interval and negative at the other end:  at least one root is guaranteed and uniroot will find it given enough iterations.)  Furthermore, if you are working with a polynomial, just use polyroot.

Chris

(Sorry for the dearth of code comments in the following)

manyroots <- function(f, interval, ints=1, maxlen=NULL,
  lower = min(interval), upper = max(interval),
  tol = .Machine$double.eps^0.25, maxiter = 1000, ...)
{
    if (!is.numeric(lower) || !is.numeric(upper) || lower >=
        upper)
        stop("lower < upper  is not fulfilled")
    if (is.infinite(lower) || is.infinite(upper))
        stop("Interval must have finite length")
    if (!is.null(maxlen))
        ints <- ceiling((upper-lower)/maxlen)
    if (!is.numeric(ints) || length(ints)>1 || floor(ints)!=ints || ints<1)
        stop("ints must be positive integer")

    ends <- seq(lower, upper, length=ints+1)
    fends <- numeric(length(ends))
    for (i in seq(along=ends)) fends[i] <- f(ends[i], ...)

    zeros <- iters <- prec <- rep(NA, ints)

    for (i in seq(ints)) {
        cat(i, ends[i], ends[i+1], fends[i], fends[i+1], "\n")
        if (fends[i] * fends[i+1] > 0) {
#            cat("f() values at end points not of opposite sign\n")
            next;
        }
        if (fends[i] == 0 & i>1) {
#            cat("this was found in previous iteration\n")
            next;
        }
        
        val <- .Internal(zeroin(function(arg) f(arg, ...), ends[i],
            ends[i+1], tol, as.integer(maxiter)))
        if (as.integer(val[2]) == maxiter) {
            warning("Iteration limit (", maxiter, ") reached in interval (",
              ends[i], ",", ends[i+1], ").")
        }
        zeros[i] <- val[1]
        iters[i] <- val[2]
        prec[i] <- val[3]
    }
    zeros <- as.vector(na.omit(zeros))
    fzeros <- numeric(length(zeros))
    for (i in seq(along=zeros)) fzeros[i] <- f(zeros[i], ...)

    list(root = zeros, f.root = fzeros,
        iter = as.vector(na.omit(iters)),
        estim.prec = as.vector(na.omit(prec)))
}

gg <- function(x) x*(x-1)*(x+1)
manyroots(gg, c(-4,4), 13, maxiter=200, tol=10^-10)

hh <- function(x,x2) x^2-x2
manyroots(hh, c(-10, 10), maxlen=.178, x2=9)

manyroots(sin, c(-4,20), maxlen=.01)
#but
ss <- function(x) sin(x)^2
manyroots(ss, c(-4,20), maxlen=.01)
plot(ss, -4,20)
abline(h=0)



From: rolf at math.unb.ca

mckellercran at gmail.com wrote:

> > This is probably a blindingly obvious question:

        Yes, it is.

> > Why does it matter in the uniroot function whether the f() values at
> > the end points that you supply are of the same sign?

        Plot some graphs.

        Think about the *name* of the function --- *uni*root.

        Does that ring any bells?

        And how do you know there *is* a root in the interval
        in question?  Try your ``uniroot2'' on f(x) = 1+x2
        and the interval [-5,5].

        To belabour the point --- if the f() values are of the
        same sign, then there are 0, or 2, or 4, or ....
        roots in the interval in question.

Rolf,
Only if f is continuous.... (of course finding roots of discontinuous functions is a greater challenge)

        The ***only chance*** you have of there being a unique
        root is if the f() values are of opposite sign.

        The algorithm used and the precision estimates returned
        presumably depend on the change of sign.  You can get
        answers --- sometimes --- if the change of sign is not
        present, but the results could be seriously misleading.

        Without the opposite sign requirement the user will often
        wind up trying to do something impossible or getting
        results about which he/she is deluded.

                                cheers,

                                        Rolf Turner
                                        rolf at math.unb.ca

P. S.  If the f() values are of the same sign, uniroot() DOES
NOT give a warning!  It gives an error.

                                        R. T.




-- 
Christopher Andrews, PhD
SUNY Buffalo, Department of Biostatistics
242 Farber Hall, candrews at buffalo.edu, 716 829 2756


From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Feb  1 14:59:51 2007
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 1 Feb 2007 14:59:51 +0100
Subject: [R] read.spss and encodings
In-Reply-To: <45C1E8A6.2090308@biostat.ku.dk>
References: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<45C1E8A6.2090308@biostat.ku.dk>
Message-ID: <200702011459.54733.thomas.friedrichsmeier@ruhr-uni-bochum.de>

On Thursday 01 February 2007 14:18, Peter Dalgaard wrote:
> so you should convert it:
> > iconv("Im B\xfcro", from="latin1", to="UTF-8")
>
> [1] "Im B?ro"
>
> > iconv("Im B\374ro",from="latin1", to="UTF-8")
>
> [1] "Im B?ro"

I see. Thanks!

Any chances of adding something like this to read.spss()?
read.spss <- function([...], encoding=NULL) {
	[...]
	if (!is.null(encoding)) {
		iconv.recursive <- function(x, from) {
			attribs <- attributes(x);
			if (is.character(x)) {
				x <- iconv(x, from=from, to="", sub="")
			} else if (is.list(x)) {
				x <- lapply(x, function(sub) iconv.recursive(sub, from))
			}
			# convert factor levels and all other attributes
			attributes(x) <- lapply(attribs, function(sub) iconv.recursive(sub, from))
			x
		}

		convert.recursive(rval, from=encoding)
	} else {
		rval
	}
}

Now that I've written this iconv.recursive() function once, I'm fine. But I 
guess something like this might be useful to others as well.

Regards
Thomas
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070201/f7fb10f4/attachment.bin 

From wl2776 at gmail.com  Thu Feb  1 15:07:42 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Thu, 1 Feb 2007 06:07:42 -0800 (PST)
Subject: [R] How can I calculate conditional mean in a large dataset
 including date data
In-Reply-To: <5.2.1.1.1.20070201130111.0dd558f0@mailbi.wsl.ch>
References: <5.2.1.1.1.20070201130111.0dd558f0@mailbi.wsl.ch>
Message-ID: <8748821.post@talk.nabble.com>


>
dfr<-data.frame(day=c("1/1/1970","5/1/1970","5/12/2003","31/12/2003"),temperature=c(1,-1,2,0.5))
> dfr
        day temperature
1   1/1/1970         1.0
2   5/1/1970        -1.0
3  5/12/2003         2.0
4 31/12/2003         0.5

> aggregate(dfr["temperature"],by=list(format(as.Date(dfr$day,format="%d/%m/%Y"),"%m-%Y")),mean,na.rm=TRUE)
  Group.1 temperature
1 01-1970        0.00
2 12-2003        1.25

> aggregate(dfr["temperature"],by=list(format(as.Date(dfr$Dt,format="%d/%m/%Y"),"%m")),mean,na.rm=TRUE)
  Group.1 temperature
1      01        0.00
2      12        1.25


Majid Iravani wrote:
> 
> Dear R users,
> 
> I have a dataframe with two columns: first column is date data (e.g. 
> 1/1/2000 with character format: daily data from 1/1/1970 till 31/12/2003) 
> and second column is temperature value. Now I'd like to calculate mean for 
> each month in a year (i.e. May 2001, June 1997) and mean for each month in 
> all of years. As the number of days in some months is different from
> others 
> I could not write appreciate command for this. Therefore I would greatly 
> appreciate if somebody can help me in this case
> 
> Thank you
> Majid
> 
> 

-- 
View this message in context: http://www.nabble.com/-R--How-can-I-calculate-conditional-mean-in-a-large-dataset-including-date-data-tf3154751.html#a8748821
Sent from the R help mailing list archive at Nabble.com.


From jrkrideau at yahoo.ca  Thu Feb  1 15:07:50 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 1 Feb 2007 09:07:50 -0500 (EST)
Subject: [R] Fwd: Re:  read.spss and encodings
Message-ID: <20070201140750.969.qmail@web32801.mail.mud.yahoo.com>


--- John Kane <jrkrideau at yahoo.ca> wrote:

> Date: Thu, 1 Feb 2007 09:07:11 -0500 (EST)
> From: John Kane <jrkrideau at yahoo.ca>
> Subject: Re: [R] read.spss and encodings
> To: Thomas Friedrichsmeier
> <thomas.friedrichsmeier at ruhr-uni-bochum.de>
> 
> Hi Thomas,
> 
> I am using R 2.4.1 on WindowsXP and I don't seem to
> be
> having any problem, im B?ro, and zuhause are coming
> in
>  just fine in a 200 line dataset. 
> 
> I have imported it with both read.spss and spss.get
> (package Hmisc) with no problems.
> 
> I am afraid I have no idea what the problem is but
> it
> does not seem to be specifically an R problem
> 
> --- Thomas Friedrichsmeier
> <thomas.friedrichsmeier at ruhr-uni-bochum.de> wrote:
> 
> > Hi!
> > 
> > I'm having trouble with importing spss files
> > containing non-ascii characters 
> > (R 2.4.1, debian linux, i386). To reproduce:
> > 
> > Download the following file: 
> >
>
http://statmath.wu-wien.ac.at/data/spss/de/comphomeneu.sav
> > 
> > require (foreign)
> > Sys.setlocale (locale="C")
> > read.spss("comphomeneu.sav")$ARBEIT[1]
> > # prints:
> > # [1] im B\374ro
> > # Levels: im B\374ro zuhause
> > 
> > \374 of course is actually a u-umlaut. However, I
> > guess in the C locale it's 
> > not expected to print as such. But now try this
> (use
> > any UTF-8 locale you may 
> > have installed):
> > 
> > Sys.setlocale (locale="de_DE.UTF-8")
> > read.spss("comphomeneu.sav")$ARBEIT[1]
> > # prints:
> > # [1]Error in print.default(xx, quote = quote,
> ...)
> > :
> > #        invalid multibyte string
> > 
> > To me it looks, like read.spss () would probably
> > need an encoding parameter, 
> > and / or some iconv () magic. Now, locale
> conversion
> > always makes my head 
> > spin, so I thought I'd better post here, before
> > calling this to be a bug in 
> > R. Two questions:
> > 
> > 1) Is there some way to work around this, i.e.
> make
> > sure it is converted to 
> > proper UTF-8 while importing? Am I missing
> something
> > obvious?
> > 2) Should I submit this as a bug report?
> > 
> > Thanks!
> > Thomas Friedrichsmeier
> > > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> > reproducible code.
> > 
> 
> 
> __________________________________________________
> Do You Yahoo!?

> protection around 
> http://mail.yahoo.com 
>


From br44114 at gmail.com  Thu Feb  1 15:12:45 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 1 Feb 2007 09:12:45 -0500
Subject: [R] How can I calculate conditional mean in a large dataset
	including date data
Message-ID: <8d5a36350702010612u53c37568q1b3d1caea51b8d8c@mail.gmail.com>

days <- seq(as.Date("1970/1/1"), as.Date("2003/12/31"), "days")
temp <- rnorm(length(days), mean=10, sd=8)
tapply(temp, format(days,"%Y-%m"), mean)
tapply(temp, format(days,"%b"), mean)


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Majid Iravani
> Sent: Thursday, February 01, 2007 8:11 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How can I calculate conditional mean in a large
> dataset including date data
>
> Dear R users,
>
> I have a dataframe with two columns: first column is date data (e.g.
> 1/1/2000 with character format: daily data from 1/1/1970 till
> 31/12/2003)
> and second column is temperature value. Now I'd like to
> calculate mean for
> each month in a year (i.e. May 2001, June 1997) and mean for
> each month in
> all of years. As the number of days in some months is
> different from others
> I could not write appreciate command for this. Therefore I
> would greatly
> appreciate if somebody can help me in this case
>
> Thank you
> Majid
> --------------------------------------------------------------
> ------------------
>   Majid Iravani
>   PhD Student
>   Swiss Federal Research Institute WSL
>   Research Group of Vegetation Ecology
>   Z?rcherstrasse 111  CH-8903 Birmensdorf  Switzerland
>   Phone: +41-1-739-2693
>   Fax: +41-1-739-2215
>   Email: Majid.iravani at wsl.ch
> http://www.wsl.ch/staff/majid.iravani/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Thu Feb  1 15:18:52 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 1 Feb 2007 09:18:52 -0500 (EST)
Subject: [R] Extracting part of date variable
In-Reply-To: <45C1D37F.8070009@biostat.ku.dk>
Message-ID: <20070201141852.33983.qmail@web32811.mail.mud.yahoo.com>

Thank you Peter.

It was not my question but I was just about to start
the morning's work by searching Help and RSiteSearch()
for this exact question. 

--- Peter Dalgaard <P.Dalgaard at biostat.ku.dk> wrote:

> stat stat wrote:
> > Dear all,
> >    
> >   Suppose I have a date variable:
> >    
> >   c = "99/05/12"
> >    
> >   I want to extract the parts of this date like
> month number, year and day. I can do it in SPSS. Is
> it possible to do this in R as well?
> >    
> >   Rgd,
> >
> >   
> Yes. One way is to use substr(), e.g.:
> 
> > substr(c,1,2)
> [1] "99"
> > as.numeric(substr(c,1,2))
> [1] 99
> 
> This also nicely sidesteps the ambiguity issue: 1999
> or 1899? May or
> December? On the other hand, you'll get in trouble
> if leading zeros are
> sometimes absent (strsplit() or gsub() if you want
> to pursue that route
> further).
> 
> For a more principled approach, use the time and
> date handling tools.
> Assuming that you can live with the system defaults
> for 2-digit years,
> 
> > strptime(c,format="%y/%m/%d")
> [1] "1999-05-12"
> > strptime(c,format="%y/%m/%d")$year
> [1] 99
> > strptime(c,format="%y/%m/%d")$mon
> [1] 4
> > strptime(c,format="%y/%m/%d")$mday
> [1] 12
> 
> Beware the peculiarities of the entries defined by
> POSIX standard, see
> ?DateTimeClasses, and also:
> 
>      '%y' Year without century (00-99). If you use
> this on input, which
>           century you get is system-specific.  So
> don't!  Often values
>           up to 69 (or 68) are prefixed by 20 and
> 70(or 69) to 99 by
>           19.
> 
> (I'm at a bit of a loss as to fixing up two digit
> years once the damage
> has been done. Presumably, you can just diddle the
> year field, but I'm a
> bit uneasy about the fact that  2000 was a leap year
> and 1900 was not.)
> 
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster
> Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099,
> 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark     
>     Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             
>     FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From r.hankin at noc.soton.ac.uk  Thu Feb  1 15:37:31 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 1 Feb 2007 14:37:31 +0000
Subject: [R] Index mapping on arrays
In-Reply-To: <20070201133421.275440@gmx.net>
References: <20070201133421.275440@gmx.net>
Message-ID: <81FF486D-7F2F-4B35-9858-18B2B8898B16@soc.soton.ac.uk>

Hello Demi


The trick for array indexing
on an array A, where length(dim(A))==n,
is to use an n-column matrix M to extract the elements
by rows of M.

If I understand correctly, the following should help:


 > A <- array(1:18,c(3,3,2))
 > x <- 2:3
 > y <- 1:2
 > A[cbind(x,y,1)]
[1] 2 6




You may find the following useful too:

 > A[as.matrix(cbind(1,expand.grid(x,y)))]
[1]  4  7 13 16
 >


best

rksh



On 1 Feb 2007, at 13:34, Demi Anderson wrote:

> Dear R-community,
>
> I have some trouble with index mappings for arrays. If, for example,
> I have the array
>
> R> A <- array(1:9, c(3,3,2))
>
> and two index mappings both of same size
>
> R> x <- c(2, 3)
> R> y <- c(1, 2)
>
> Now I want to access the elements (A[1, x[i], y[i]])_i of A, i.e.
> A[1, x[1], y[1]] = A[1, 2, 1] and A[1, x[2], y[2]] = A[1, 3, 2].
>
> If I use
>
> R> A[1, x, y]
>
> I would get every combinations of indices of all elements of x and y
> i.e. A[1, x[1], y[1]], A[1, x[1], y[2]], A[1, x[2], y[1]] and A[1,
> x[2], y[2]]. But how can I access the elements (A[1, x[i], y[i]])_i.
> My arrays dimensions are actually large in the second
> component (for example the dimension might be 10*10000*10) so I'm
> looking for a method avoiding loops.
>
> The question is probably trivial for you, but I just could not
> figure it out. So sorry for bugging you and many thanks in advance
> for any help.
>
> Best wishes, Demi Anderson.
> --  
> "Feel free" - 10 GB Mailbox, 100 FreeSMS/Monat ...
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


-- 
This e-mail (and any attachments) is confidential and intend...{{dropped}}


From ggrothendieck at gmail.com  Thu Feb  1 15:43:29 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Feb 2007 09:43:29 -0500
Subject: [R] How can I calculate conditional mean in a large dataset
	including date data
In-Reply-To: <8748821.post@talk.nabble.com>
References: <5.2.1.1.1.20070201130111.0dd558f0@mailbi.wsl.ch>
	<8748821.post@talk.nabble.com>
Message-ID: <971536df0702010643g24502712q4bdabd7bdf1e7430@mail.gmail.com>

You could also use aggregate with the zoo package.  Using the same
input data that Vladimir used, create a zoo variable and aggregate it:

> library(zoo)
> z <- zoo(dfr[,2], as.Date(dfr[,1], "%d/%m/%Y"))
> aggregate(z, as.yearmon, mean)
Jan 1970 Dec 2003
    0.00     1.25

zoo is described in the vignette:

library(zoo)
vignette("zoo")


On 2/1/07, Vladimir Eremeev <wl2776 at gmail.com> wrote:
>
> >
> dfr<-data.frame(day=c("1/1/1970","5/1/1970","5/12/2003","31/12/2003"),temperature=c(1,-1,2,0.5))
> > dfr
>        day temperature
> 1   1/1/1970         1.0
> 2   5/1/1970        -1.0
> 3  5/12/2003         2.0
> 4 31/12/2003         0.5
>
> > aggregate(dfr["temperature"],by=list(format(as.Date(dfr$day,format="%d/%m/%Y"),"%m-%Y")),mean,na.rm=TRUE)
>  Group.1 temperature
> 1 01-1970        0.00
> 2 12-2003        1.25
>
> > aggregate(dfr["temperature"],by=list(format(as.Date(dfr$Dt,format="%d/%m/%Y"),"%m")),mean,na.rm=TRUE)
>  Group.1 temperature
> 1      01        0.00
> 2      12        1.25
>
>
> Majid Iravani wrote:
> >
> > Dear R users,
> >
> > I have a dataframe with two columns: first column is date data (e.g.
> > 1/1/2000 with character format: daily data from 1/1/1970 till 31/12/2003)
> > and second column is temperature value. Now I'd like to calculate mean for
> > each month in a year (i.e. May 2001, June 1997) and mean for each month in
> > all of years. As the number of days in some months is different from
> > others
> > I could not write appreciate command for this. Therefore I would greatly
> > appreciate if somebody can help me in this case
> >
> > Thank you
> > Majid
> >
> >
>
> --
> View this message in context: http://www.nabble.com/-R--How-can-I-calculate-conditional-mean-in-a-large-dataset-including-date-data-tf3154751.html#a8748821
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.nieuwenhuis at student.ru.nl  Thu Feb  1 15:47:48 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Thu, 1 Feb 2007 15:47:48 +0100
Subject: [R] line plot
In-Reply-To: <45C1A17C.26755.722E58@localhost>
References: <45C1A17C.26755.722E58@localhost>
Message-ID: <5415C981-FC05-4A19-99A4-D1D91D1E7F6A@student.ru.nl>

Hi,

or otherwise you may try:

  plot(c(1,5), c(1,10),type="l")

kindest regard, Rense


On Feb 1, 2007, at 8:14 , Petr Pikal wrote:

> Hi
>
> see ?segments
> segments(1,10,5,10)
>
> HTH
> Petr
>
>
> On 1 Feb 2007 at 14:21, XinMeng wrote:
>
> From:           	"XinMeng" <xmeng at capitalbio.com>
> To:             	r-help at stat.math.ethz.ch
> Date sent:      	Thu, 01 Feb 2007 14:21:34 +0800
> Subject:        	[R] line plot
> Send reply to:  	XinMeng <xmeng at capitalbio.com>
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
>
>> Hello sir:
>> I wanna get such kind of plot: a line whose start point is(1,10),end
>> point is(5,10)
>>
>> In other words:
>>
>> How can I draw a line if I only know the coordinate of the start  
>> point
>> and end point? Thanks! My best
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ian_kennedy at hc-sc.gc.ca  Thu Feb  1 15:52:02 2007
From: ian_kennedy at hc-sc.gc.ca (Ian Kennedy)
Date: Thu, 1 Feb 2007 09:52:02 -0500
Subject: [R] xtable and column headings
Message-ID: <OFC7ED8B1C.B97B822A-ON85257275.004DBBC5-85257275.0051AB8F@hc-sc.gc.ca>


When I generate a LaTeX table using xtable I have been setting column names
to strings with LaTeX code in order to get features like subscripts in the
column headings. I recently had to reinstall xtable and discovered that all
my LaTeX column headings were printing out in LaTeX code rather than with
LaTeX formatting. For example, with the older xtable I could give my column
a name something like "$A_b$" to get printed column heading of "A" with the
subscript "b." Now my printed column heading is "$A_b$" and the LaTeX code
in the .tex file generated by Sweave is "\$A\_b\$". It seems that the
newest version of print.xtable takes all my LaTeX special characters and
inserts backslashes, making LaTeX print the special characters rather than
interpreting them.

Is there a way to keep xtable from "fixing" my column names like this? Is
there another (maybe better) way to get nicely LaTeX formatted column
headings from xtable?

Thanks,
Ian


From Reinecke at consultic.com  Thu Feb  1 16:08:30 2007
From: Reinecke at consultic.com (Michael Reinecke)
Date: Thu, 1 Feb 2007 16:08:30 +0100
Subject: [R] mca-graphics: all elements overlapping in the help-example
	for multiple correspondence analysis
Message-ID: <D1A363788EC8F946A56DAF95C0FBE7CF29928B@sbs2003.CMI.local>

Thank you very much, that works fine!

I now realize that I should have looked up not only the help pages for plot and mca but also for plot.mca, which I did not think possible (unfortunately I am a too sporadic user to know where to get the appropriate information).

Best regards,

Michael
 

> -----Urspr?ngliche Nachricht-----
> Von: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Gesendet: Mittwoch, 31. Januar 2007 10:19
> An: Michael Reinecke
> Cc: r-help at stat.math.ethz.ch
> Betreff: Re: [R] mca-graphics: all elements overlapping in 
> the help-example for multiple correspondence analysis
> 
> On Wed, 31 Jan 2007, Michael Reinecke wrote:
> 
> > Dear all,
> >
> > I tried out the example in the help document for mca (the 
> multiple correspondence analysis of the MASS package):
> >
> > farms.mca <- mca(farms, abbrev=TRUE)
> > farms.mca
> > plot(farms.mca)
> >
> > But the graphic that I get seems unfeasible to me: I cannot 
> recognize 
> > the numbers (printed in black) because they are all overlapping and 
> > concealing each other. I don ?t dare using my own data, 
> which consist 
> > of several hundred cases - I guess I won ?t see anything.
> >
> > How can I solve this? Thank you for any idea!
> 
> Some levels do overplot, as they are identical (this is an 
> unusual example).  But as you see in the book, not many, and 
> you can adjust pointsize of your device or 'cex' to mitigate 
> the problem.
> 
> Plotting the rows is optional: see the help page.  I would 
> not recommend plotting rows for several hundred cases.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From rvaradhan at jhmi.edu  Thu Feb  1 16:10:41 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 1 Feb 2007 10:10:41 -0500
Subject: [R] Need help writing a faster code
Message-ID: <000301c74613$249cd790$7c94100a@win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/405fe621/attachment.pl 

From mckellercran at gmail.com  Thu Feb  1 16:16:03 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Thu, 1 Feb 2007 10:16:03 -0500
Subject: [R] Loading functions in R
In-Reply-To: <001701c745bb$ffffbee0$0202a8c0@headquarters.silicoinsights>
References: <45C1614C.5090206@comcast.net>
	<001701c745bb$ffffbee0$0202a8c0@headquarters.silicoinsights>
Message-ID: <3f547caa0702010716g26af724fo6edd8cfabb171838@mail.gmail.com>

Hi Jeff,

The way I do this is to place all the options that I want, along with
functions I've written that I always want available, into the
"Rprofile.site" file. R always loads this file upon startup. That file
is located in the etc/ folder. E.g., on my computer, it is at:
C:\Program Files\R\R-2.2.1\etc\Rprofile.site . This is explained in
section 10.8 of the R-intro.pdf manual that comes with R.

If you only want the functions available sometimes, then use Christos'
suggestions.

-- Matt


On 1/31/07, Christos Hatzis <christos at nuverabio.com> wrote:
> The recommended approach is to make a package for your functions that will
> include documentation, error checks etc.
> Another way to accomplish what you want is to start a new R session and
> 'source' your .R files and then to save the workspace in a .RData file, e.g.
> myFunctions.RData.
>
> Finally
>
> attach(myFunctions.RData)
>
> should do the trick without cluttering your workspace.
>
> -Christos
>
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com
>
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Forest Floor
> Sent: Wednesday, January 31, 2007 10:41 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Loading functions in R
>
> Hi all,
>
> This information must be out there, but I can't seem to find it.  What I
> want to do is to store functions I've created (as .R files or in whatever
> form) and then load them when I need them (or on startup) so that I can
> access without cluttering my program with the function code.
> This seems like it should be easy, but....
>
> Thanks!
>
> Jeff
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.hankin at noc.soton.ac.uk  Thu Feb  1 16:27:16 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 1 Feb 2007 15:27:16 +0000
Subject: [R] Need help writing a faster code
In-Reply-To: <000301c74613$249cd790$7c94100a@win.ad.jhu.edu>
References: <000301c74613$249cd790$7c94100a@win.ad.jhu.edu>
Message-ID: <F316361E-D8B3-48C0-AF18-A27162812B5C@soc.soton.ac.uk>

Hi



 > A <- matrix(runif(10),ncol=2)
 > B <- matrix(runif(10),ncol=2)
 > g <- function(i4){theta <- atan2( (i4[4]-i4[2]),(i4[3]-i4[1]))
+ return(theta + 2*pi*(theta<0))}
 > apply(A,1,function(x){apply(B,1,function(y){g(c(x,y))})})
           [,1]      [,2]     [,3]        [,4]      [,5]
[1,] 1.1709326 2.6521457 3.857477 0.219274562 1.2948374
[2,] 1.1770919 4.2109056 4.057313 4.918552967 1.9733967
[3,] 0.9171661 0.6721475 4.193675 0.434253839 0.9781060
[4,] 0.9181475 0.6911804 4.213295 0.455127422 0.9771797
[5,] 1.0467449 4.9263243 3.983248 0.004371504 1.1693707
 >



HTH

rksh


On 1 Feb 2007, at 15:10, Ravi Varadhan wrote:

> Hi,
>
>
>
> I apologize for this repeat posting, which I first posted  
> yesterday. I would
> appreciate any hints on solving this problem:
>
>
>
> I have two matrices A (m x 2) and B (n x 2), where m and n are large
> integers (on the order of 10^4).  I am looking for an efficient way to
> create another matrix, W (m x n), which can be defined as follows:
>
>
>
> for (i in 1:m){
>
> for (j in 1:n) {
>
> W[i,j] <- g(A[i,], B[j,])
>
> } }
>
> where g(x,y) is a function that takes two vectors and returns a  
> scalar.
>
>
>
> The following works okay, but is not fast enough for my purpose.  I  
> am sure
> that I can do better:
>
>
>
> for (i in 1:m) {
>
> W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
>
> }
>
>
>
> How can I do this in a faster manner? I attempted "outer",  
> "kronecker",
> "expand.grid", etc, but with no success.
>
>
>
> Here is an example:
>
>
>
> m <- 2000
>
> n <- 5000
>
> A <- matrix(rnorm(2*m),ncol=2)
>
> B <- matrix(rnorm(2*n),ncol=2)
>
> W <- matrix(NA, m, n)
>
>
>
> for (i in 1:m) {
>
> W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
>
> }
>
>
>
> g <- function(x,y){
>
> theta <- atan((y[2]-x[2]) / (y[1] - x[1]))
>
> theta + 2*pi*(theta < 0)
>
> }
>
>
>
> Thanks for any suggestions.
>
>
>
> Best,
>
> Ravi.
>
>
>
>
>
>
>
> ---------------------------------------------------------------------- 
> ------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/ 
> Varadhan.html
>
>
>
> ---------------------------------------------------------------------- 
> ------
> --------
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


-- 
This e-mail (and any attachments) is confidential and intend...{{dropped}}


From dimitris.rizopoulos at med.kuleuven.be  Thu Feb  1 16:33:18 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 1 Feb 2007 16:33:18 +0100
Subject: [R] Need help writing a faster code
References: <000301c74613$249cd790$7c94100a@win.ad.jhu.edu>
Message-ID: <017401c74616$4c6c76b0$0540210a@www.domain>

the following seems to be a first improvement:

m <- 2000
n <- 5000
A <- matrix(rnorm(2*m), ncol=2)
B <- matrix(rnorm(2*n), ncol=2)
W1 <- W2 <- matrix(0, m, n)

##############################
##############################

g1 <- function(x, y){
    theta <- atan((y[2] - x[2]) / (y[1] - x[1]))
    theta + 2*pi*(theta < 0)
}

invisible({gc(); gc()})
system.time(for (i in 1:m) {
    W1[i, ] <- apply(B, 1, y = A[i,], function(x, y) g1(y, x))
})

##############################

g2 <- function(x){
    out <- tB - x
    theta <- atan(out[2, ] / out[1, ])
    theta + 2*pi*(theta < 0)
}

tB <- t(B)
invisible({gc(); gc()})
system.time(for (i in 1:m) {
    W2[i, ] <- g2(A[i, ])
})

## or

invisible({gc(); gc()})
system.time(W3 <- t(apply(A, 1, g2)))

all.equal(W1, W2)
all.equal(W1, W3)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ravi Varadhan" <rvaradhan at jhmi.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 01, 2007 4:10 PM
Subject: [R] Need help writing a faster code


> Hi,
>
>
>
> I apologize for this repeat posting, which I first posted yesterday. 
> I would
> appreciate any hints on solving this problem:
>
>
>
> I have two matrices A (m x 2) and B (n x 2), where m and n are large
> integers (on the order of 10^4).  I am looking for an efficient way 
> to
> create another matrix, W (m x n), which can be defined as follows:
>
>
>
> for (i in 1:m){
>
> for (j in 1:n) {
>
> W[i,j] <- g(A[i,], B[j,])
>
> } }
>
> where g(x,y) is a function that takes two vectors and returns a 
> scalar.
>
>
>
> The following works okay, but is not fast enough for my purpose.  I 
> am sure
> that I can do better:
>
>
>
> for (i in 1:m) {
>
> W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
>
> }
>
>
>
> How can I do this in a faster manner? I attempted "outer", 
> "kronecker",
> "expand.grid", etc, but with no success.
>
>
>
> Here is an example:
>
>
>
> m <- 2000
>
> n <- 5000
>
> A <- matrix(rnorm(2*m),ncol=2)
>
> B <- matrix(rnorm(2*n),ncol=2)
>
> W <- matrix(NA, m, n)
>
>
>
> for (i in 1:m) {
>
> W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
>
> }
>
>
>
> g <- function(x,y){
>
> theta <- atan((y[2]-x[2]) / (y[1] - x[1]))
>
> theta + 2*pi*(theta < 0)
>
> }
>
>
>
> Thanks for any suggestions.
>
>
>
> Best,
>
> Ravi.
>
>
>
>
>
>
>
> ----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage: 
> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>
>
> ----------------------------------------------------------------------------
> --------
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From tlumley at u.washington.edu  Thu Feb  1 16:45:11 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 1 Feb 2007 07:45:11 -0800 (PST)
Subject: [R] read.spss and encodings
In-Reply-To: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>
References: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>
Message-ID: <Pine.LNX.4.64.0702010738170.21547@homer21.u.washington.edu>

On Thu, 1 Feb 2007, Thomas Friedrichsmeier wrote:

> Hi!
>
> I'm having trouble with importing spss files containing non-ascii characters

Peter has explained what is going on.  It would be ideal for read.spss() 
to do the translation to the current locale. This would require knowing 
what encoding the SPSS file is using.  I think it is always a one-byte 
encoding and in your case it is apparently Latin-1, but I don't know if 
this is always the case, or how to tell which encoding it uses.

 	-thomas


From rmh at temple.edu  Thu Feb  1 16:58:01 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu,  1 Feb 2007 10:58:01 -0500 (EST)
Subject: [R] xtable and column headings
Message-ID: <20070201105801.BTJ78125@po-d.temple.edu>

I would use latex() in the Hmsic package.  Here is a short example

tmp <- matrix(1:12,4)
library(Hmisc)
tmp.latex <- latex(tmp, colheads=c("abc$_1$","def$^{12}_4$","$g\\times h$"))
## note the escaped "\" in the above colheads vector
print.default(tmp.latex)

Copy the contents of the file referenced in tmp.latex to your
real myfile.tex file.

There are about a zillion optional arguments to latex() that give you very fine
control over the appearance of the typeset object.  See ?latex


From sebastian.weber at physik.tu-darmstadt.de  Thu Feb  1 17:01:19 2007
From: sebastian.weber at physik.tu-darmstadt.de (Sebastian Weber)
Date: Thu, 01 Feb 2007 17:01:19 +0100
Subject: [R] [lattice] levelplot for 2D density plots
Message-ID: <1170345679.7176.33.camel@rock.kraft.de>

Hello all,

I'm trying to use the levelplot lattice function and can not adapt it to
my tastes concering colors:

dens <- data.frame(x=c(), y=c(), z=c(), run=c())
for(l in levels(degCorrel$run)) {
  ind <- degCorrel$run == l
  dk <- kde2d(log10(degCorrel$correlFunc[ind]), log10(degCorrel
$correlFunc.ref[ind]), n=50)
  dt <- cbind(con2tr(dk), run=l)
  dt$z <- dt$z/sum(dt$z)
  dens <- rbind(dens, dt)
}
dens$run <- factor(dens$run)

levelplot(z ~ x *y | run, data=dens)

However, I need to adjust the cuts for every panel differently since the
scales are very different. I know, that this is not a very good
practice, but anyway, how can I do it?

Any help is greatly appreciated. Thanks in advance,

Sebastian


From jgarcia at ija.csic.es  Thu Feb  1 17:05:06 2007
From: jgarcia at ija.csic.es (javier garcia-pintado)
Date: Thu, 01 Feb 2007 17:05:06 +0100
Subject: [R] indexing without looping
Message-ID: <45C20FB2.2070800@ija.csic.es>

Hello,
I've got a data.frame like this:

> > assignation <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
> > assignation
>   
  value class
1   6.5     1
2   7.5     3
3   8.5     5
4  12.0     2

> >   
>   

and a long vector of classes like this:


> > x <- c(1,1,2,7,6,5,4,3,2,2,2...)
>   

And would like to obtain  a vector of length = length(x), with the
corresponding values extracted from assignation table. Like this:

> > x.value
>   
 [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0

Could you help me with an elegant way to do this ?
(I just can do it with looping for each class in the assignation table,
what a think is not perfect in R's sense)

Wishes,
Javier

-- 
Javier Garc?a-Pintado
Institute of Earth Sciences Jaume Almera (CSIC)
Lluis Sole Sabaris s/n, 08028 Barcelona
Phone: +34 934095410
Fax:   +34 934110012
e-mail:jgarcia at ija.csic.es 


From rvaradhan at jhmi.edu  Thu Feb  1 17:08:00 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 1 Feb 2007 11:08:00 -0500
Subject: [R] Need help writing a faster code
In-Reply-To: <017401c74616$4c6c76b0$0540210a@www.domain>
References: <000301c74613$249cd790$7c94100a@win.ad.jhu.edu>
	<017401c74616$4c6c76b0$0540210a@www.domain>
Message-ID: <001501c7461b$2548ae50$7c94100a@win.ad.jhu.edu>

Thank you, Dimitris and Robin.  

Dimitris - your solution(s) works very well.  Although my "g" function is a
lot more complicated than that in the simple example that I gave, I think
that I can use your idea of taking the whole matrix inside the function and
working directly with it.

Robin - using two applys doesn't make the code any faster, it just produces
a compact one-liner.

Best,
Ravi.
----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: Dimitris Rizopoulos [mailto:dimitris.rizopoulos at med.kuleuven.be] 
Sent: Thursday, February 01, 2007 10:33 AM
To: Ravi Varadhan
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Need help writing a faster code

the following seems to be a first improvement:

m <- 2000
n <- 5000
A <- matrix(rnorm(2*m), ncol=2)
B <- matrix(rnorm(2*n), ncol=2)
W1 <- W2 <- matrix(0, m, n)

##############################
##############################

g1 <- function(x, y){
    theta <- atan((y[2] - x[2]) / (y[1] - x[1]))
    theta + 2*pi*(theta < 0)
}

invisible({gc(); gc()})
system.time(for (i in 1:m) {
    W1[i, ] <- apply(B, 1, y = A[i,], function(x, y) g1(y, x))
})

##############################

g2 <- function(x){
    out <- tB - x
    theta <- atan(out[2, ] / out[1, ])
    theta + 2*pi*(theta < 0)
}

tB <- t(B)
invisible({gc(); gc()})
system.time(for (i in 1:m) {
    W2[i, ] <- g2(A[i, ])
})

## or

invisible({gc(); gc()})
system.time(W3 <- t(apply(A, 1, g2)))

all.equal(W1, W2)
all.equal(W1, W3)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ravi Varadhan" <rvaradhan at jhmi.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 01, 2007 4:10 PM
Subject: [R] Need help writing a faster code


> Hi,
>
>
>
> I apologize for this repeat posting, which I first posted yesterday. 
> I would
> appreciate any hints on solving this problem:
>
>
>
> I have two matrices A (m x 2) and B (n x 2), where m and n are large
> integers (on the order of 10^4).  I am looking for an efficient way 
> to
> create another matrix, W (m x n), which can be defined as follows:
>
>
>
> for (i in 1:m){
>
> for (j in 1:n) {
>
> W[i,j] <- g(A[i,], B[j,])
>
> } }
>
> where g(x,y) is a function that takes two vectors and returns a 
> scalar.
>
>
>
> The following works okay, but is not fast enough for my purpose.  I 
> am sure
> that I can do better:
>
>
>
> for (i in 1:m) {
>
> W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
>
> }
>
>
>
> How can I do this in a faster manner? I attempted "outer", 
> "kronecker",
> "expand.grid", etc, but with no success.
>
>
>
> Here is an example:
>
>
>
> m <- 2000
>
> n <- 5000
>
> A <- matrix(rnorm(2*m),ncol=2)
>
> B <- matrix(rnorm(2*n),ncol=2)
>
> W <- matrix(NA, m, n)
>
>
>
> for (i in 1:m) {
>
> W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
>
> }
>
>
>
> g <- function(x,y){
>
> theta <- atan((y[2]-x[2]) / (y[1] - x[1]))
>
> theta + 2*pi*(theta < 0)
>
> }
>
>
>
> Thanks for any suggestions.
>
>
>
> Best,
>
> Ravi.
>
>
>
>
>
>
>
>
----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage: 
> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>
>
>
----------------------------------------------------------------------------
> --------
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From huxiaopengstat at gmail.com  Thu Feb  1 17:09:46 2007
From: huxiaopengstat at gmail.com (xiaopeng hu)
Date: Fri, 2 Feb 2007 00:09:46 +0800
Subject: [R] when i configure the R 2.4.1,i meet the problem
Message-ID: <ffe0539f0702010809w28fe3549t729a5f2d5c26b1fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070202/74c3c3b1/attachment.pl 

From mothsailor at googlemail.com  Thu Feb  1 17:18:53 2007
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 1 Feb 2007 16:18:53 +0000
Subject: [R] indexing without looping
In-Reply-To: <45C20FB2.2070800@ija.csic.es>
References: <45C20FB2.2070800@ija.csic.es>
Message-ID: <815b70590702010818s1a4a733bm9895a37a45bd8b55@mail.gmail.com>

One way would be to use merge, like this:

merge(assignation,data.frame(class=x),all.y=TRUE)

There might well be better ways...

On 01/02/07, javier garcia-pintado <jgarcia at ija.csic.es> wrote:
> Hello,
> I've got a data.frame like this:
>
> > > assignation <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
> > > assignation
> >
>   value class
> 1   6.5     1
> 2   7.5     3
> 3   8.5     5
> 4  12.0     2
>
> > >
> >
>
> and a long vector of classes like this:
>
>
> > > x <- c(1,1,2,7,6,5,4,3,2,2,2...)
> >
>
> And would like to obtain  a vector of length = length(x), with the
> corresponding values extracted from assignation table. Like this:
>
> > > x.value
> >
>  [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0
>
> Could you help me with an elegant way to do this ?
> (I just can do it with looping for each class in the assignation table,
> what a think is not perfect in R's sense)
>
> Wishes,
> Javier
>
> --
> Javier Garc?a-Pintado
> Institute of Earth Sciences Jaume Almera (CSIC)
> Lluis Sole Sabaris s/n, 08028 Barcelona
> Phone: +34 934095410
> Fax:   +34 934110012
> e-mail:jgarcia at ija.csic.es
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From tamir at imp.univie.ac.at  Thu Feb  1 17:40:11 2007
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Thu, 1 Feb 2007 17:40:11 +0100
Subject: [R] indexing without looping
Message-ID: <200702011740.11464.tamir@imp.univie.ac.at>

Hi,

xt <- assignation$value[match(x,assignation$class)]

HTH
ido


From tplate at acm.org  Thu Feb  1 17:44:28 2007
From: tplate at acm.org (Tony Plate)
Date: Thu, 01 Feb 2007 09:44:28 -0700
Subject: [R] indexing
In-Reply-To: <45C1C95C.8090605@ija.csic.es>
References: <45C1C95C.8090605@ija.csic.es>
Message-ID: <45C218EC.3060508@acm.org>

 > a <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
 > x <- c(1,1,2,7,6,5,4,3,2,2,2)
 > match(x, a$class)
  [1]  1  1  4 NA NA  3 NA  2  4  4  4
 > a[match(x, a$class), "value"]
  [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0
 >

-- Tony Plate

javier garcia-pintado wrote:
> Hello,
> In a nutshell, I've got a data.frame like this:
> 
> 
>>assignation <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
>>assignation
> 
>   value class
> 1   6.5     1
> 2   7.5     3
> 3   8.5     5
> 4  12.0     2
> 
>>  
> 
> 
> and a long vector of classes like this:
> 
> 
>>x <- c(1,1,2,7,6,5,4,3,2,2,2...)
> 
> 
> And would like to obtain  a vector of length = length(x), with the
> corresponding values extracted from assignation table. Like this:
> 
>>x.value
> 
>  [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0
> 
> Could you help me with an elegant way to do this ?
> (I just can do it with looping for each class in the assignation table,
> what a think is not perfect in R's sense)
> 
> Wishes,
> Javier
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rvaradhan at jhmi.edu  Thu Feb  1 18:01:21 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 1 Feb 2007 12:01:21 -0500
Subject: [R] Need help writing a faster code
In-Reply-To: <017401c74616$4c6c76b0$0540210a@www.domain>
References: <000301c74613$249cd790$7c94100a@win.ad.jhu.edu>
	<017401c74616$4c6c76b0$0540210a@www.domain>
Message-ID: <003801c74622$98db4dd0$7c94100a@win.ad.jhu.edu>

Dear Dimitris,

I implemented your solution on my actual problem.  I was able to generate my
large transition matrix in 56 seconds, compared to the previous time of
around 27 minutes.  Wow!!!

I thank you very much for the help.  R and the R-user group are truly
amazing!

Best regards,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: Dimitris Rizopoulos [mailto:dimitris.rizopoulos at med.kuleuven.be] 
Sent: Thursday, February 01, 2007 10:33 AM
To: Ravi Varadhan
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Need help writing a faster code

the following seems to be a first improvement:

m <- 2000
n <- 5000
A <- matrix(rnorm(2*m), ncol=2)
B <- matrix(rnorm(2*n), ncol=2)
W1 <- W2 <- matrix(0, m, n)

##############################
##############################

g1 <- function(x, y){
    theta <- atan((y[2] - x[2]) / (y[1] - x[1]))
    theta + 2*pi*(theta < 0)
}

invisible({gc(); gc()})
system.time(for (i in 1:m) {
    W1[i, ] <- apply(B, 1, y = A[i,], function(x, y) g1(y, x))
})

##############################

g2 <- function(x){
    out <- tB - x
    theta <- atan(out[2, ] / out[1, ])
    theta + 2*pi*(theta < 0)
}

tB <- t(B)
invisible({gc(); gc()})
system.time(for (i in 1:m) {
    W2[i, ] <- g2(A[i, ])
})

## or

invisible({gc(); gc()})
system.time(W3 <- t(apply(A, 1, g2)))

all.equal(W1, W2)
all.equal(W1, W3)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ravi Varadhan" <rvaradhan at jhmi.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 01, 2007 4:10 PM
Subject: [R] Need help writing a faster code


> Hi,
>
>
>
> I apologize for this repeat posting, which I first posted yesterday. 
> I would
> appreciate any hints on solving this problem:
>
>
>
> I have two matrices A (m x 2) and B (n x 2), where m and n are large
> integers (on the order of 10^4).  I am looking for an efficient way 
> to
> create another matrix, W (m x n), which can be defined as follows:
>
>
>
> for (i in 1:m){
>
> for (j in 1:n) {
>
> W[i,j] <- g(A[i,], B[j,])
>
> } }
>
> where g(x,y) is a function that takes two vectors and returns a 
> scalar.
>
>
>
> The following works okay, but is not fast enough for my purpose.  I 
> am sure
> that I can do better:
>
>
>
> for (i in 1:m) {
>
> W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
>
> }
>
>
>
> How can I do this in a faster manner? I attempted "outer", 
> "kronecker",
> "expand.grid", etc, but with no success.
>
>
>
> Here is an example:
>
>
>
> m <- 2000
>
> n <- 5000
>
> A <- matrix(rnorm(2*m),ncol=2)
>
> B <- matrix(rnorm(2*n),ncol=2)
>
> W <- matrix(NA, m, n)
>
>
>
> for (i in 1:m) {
>
> W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
>
> }
>
>
>
> g <- function(x,y){
>
> theta <- atan((y[2]-x[2]) / (y[1] - x[1]))
>
> theta + 2*pi*(theta < 0)
>
> }
>
>
>
> Thanks for any suggestions.
>
>
>
> Best,
>
> Ravi.
>
>
>
>
>
>
>
>
----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage: 
> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>
>
>
----------------------------------------------------------------------------
> --------
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From roland.rproject at gmail.com  Thu Feb  1 18:07:27 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Thu, 1 Feb 2007 12:07:27 -0500
Subject: [R] when i configure the R 2.4.1,i meet the problem
In-Reply-To: <ffe0539f0702010809w28fe3549t729a5f2d5c26b1fe@mail.gmail.com>
References: <ffe0539f0702010809w28fe3549t729a5f2d5c26b1fe@mail.gmail.com>
Message-ID: <47c7c59e0702010907o19b274d9t2c6ef9e5c5a38ac6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/8f84fd89/attachment.pl 

From a.eslami at Mail.sbu.ac.ir  Thu Feb  1 18:10:23 2007
From: a.eslami at Mail.sbu.ac.ir (a.eslami)
Date: Thu, 1 Feb 2007 20:40:23 +0330
Subject: [R] would you please navigate me?
References: <368305.31709.qm@web56103.mail.re3.yahoo.com>
Message-ID: <4AC03A6244C3C34BB52A7EC60B799C4C03944B@m-pdc.sbu.ac.ir>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/8e1dfa1a/attachment.pl 

From mrennie at utm.utoronto.ca  Thu Feb  1 18:13:41 2007
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Thu, 01 Feb 2007 12:13:41 -0500
Subject: [R] Losing factor levels when moving variables from one context to
 another
Message-ID: <6.1.0.6.0.20070201120612.01babc38@mail.utm.utoronto.ca>


Hi, there

I'm currently trying to figure out how to keep my "factor" levels for a 
variable when moving it from one data frame or matrix to another.

Example below:

vec1<-(rep("10",5))
vec2<-(rep("30",5))
vec3<-(rep("80",5))
vecs<-c(vec1, vec2, vec3)

resp<-rnorm(2,15)

dat<-as.data.frame(cbind(resp, vecs))
dat$vecs<-factor(dat$vecs)
dat

R returns:
                   resp  vecs
1     1.57606068767956   10
2     2.30271782269308   10
3     2.39874788444542   10
4    0.963987738423353   10
5     2.03620782454740   10
6  -0.0706713324725649   30
7     1.49001721222926   30
8     2.00587718501980   30
9    0.450576585429981   30
10    2.87120375367357   30
11    2.25575058079324   80
12    2.03471288724508   80
13    2.67432066972984   80
14    1.74102136279177   80
15    2.29827581276955   80

and now:

newvar<-(rnorm(15,4))
newdat<-as.data.frame(cbind(newvar, dat$vecs))
newdat

R returns:

       newvar V2
1  4.300788  1
2  5.295951  1
3  5.099849  1
4  3.211045  1
5  3.703554  1
6  3.693826  2
7  5.314679  2
8  4.222270  2
9  3.534515  2
10 4.037401  2
11 4.476808  3
12 4.842449  3
13 3.109677  3
14 4.752961  3
15 4.445216  3
 >

I seem to have lost everything I once has associated with "vecs", and it's 
turned my actual values into arbitrary groupings.

I assume this has something to do with the behaviour of factors? Does 
anyone have any suggestions on how to get my original levels, etc., back?

Cheers,

Mike

Michael Rennie
Ph.D. Candidate, University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792
www.utm.utoronto.ca/~w3rennie


From ccleland at optonline.net  Thu Feb  1 18:34:53 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 01 Feb 2007 12:34:53 -0500
Subject: [R] Losing factor levels when moving variables from one context
 to another
In-Reply-To: <6.1.0.6.0.20070201120612.01babc38@mail.utm.utoronto.ca>
References: <6.1.0.6.0.20070201120612.01babc38@mail.utm.utoronto.ca>
Message-ID: <45C224BD.6030400@optonline.net>

Michael Rennie wrote:
> Hi, there
> 
> I'm currently trying to figure out how to keep my "factor" levels for a 
> variable when moving it from one data frame or matrix to another.
> 
> Example below:
> 
> vec1<-(rep("10",5))
> vec2<-(rep("30",5))
> vec3<-(rep("80",5))
> vecs<-c(vec1, vec2, vec3)
> 
> resp<-rnorm(2,15)
> 
> dat<-as.data.frame(cbind(resp, vecs))
> dat$vecs<-factor(dat$vecs)
> dat
> 
> R returns:
>                    resp  vecs
> 1     1.57606068767956   10
> 2     2.30271782269308   10
> 3     2.39874788444542   10
> 4    0.963987738423353   10
> 5     2.03620782454740   10
> 6  -0.0706713324725649   30
> 7     1.49001721222926   30
> 8     2.00587718501980   30
> 9    0.450576585429981   30
> 10    2.87120375367357   30
> 11    2.25575058079324   80
> 12    2.03471288724508   80
> 13    2.67432066972984   80
> 14    1.74102136279177   80
> 15    2.29827581276955   80
> 
> and now:
> 
> newvar<-(rnorm(15,4))
> newdat<-as.data.frame(cbind(newvar, dat$vecs))
> newdat
> 
> R returns:
> 
>        newvar V2
> 1  4.300788  1
> 2  5.295951  1
> 3  5.099849  1
> 4  3.211045  1
> 5  3.703554  1
> 6  3.693826  2
> 7  5.314679  2
> 8  4.222270  2
> 9  3.534515  2
> 10 4.037401  2
> 11 4.476808  3
> 12 4.842449  3
> 13 3.109677  3
> 14 4.752961  3
> 15 4.445216  3
>  >
> 
> I seem to have lost everything I once has associated with "vecs", and it's 
> turned my actual values into arbitrary groupings.
> 
> I assume this has something to do with the behaviour of factors? Does 
> anyone have any suggestions on how to get my original levels, etc., back?

  It has more to do with the behavior of cbind().  Construct the data
frame with data.frame() rather than the combination of as.data.frame()
and cbind().  For example:

vec1 <- (rep("10",2))
vec2 <- (rep("30",2))
vec3 <- (rep("80",2))
vecs <- c(vec1, vec2, vec3)
resp <- rnorm(6,2)

dat <- data.frame(resp, vecs)
dat$vecs <- factor(dat$vecs)
dat
      resp vecs
1 2.795851   10
2 3.673296   10
3 1.731921   30
4 1.172945   30
5 2.427164   80
6 1.470758   80

newvar <- (rnorm(6,4))
newdat <- data.frame(newvar, dat$vecs)
newdat
    newvar dat.vecs
1 6.389386       10
2 3.453535       10
3 3.807821       30
4 6.067712       30
5 4.978724       80
6 3.015975       80

?data.frame

> Cheers,
> 
> Mike
> 
> Michael Rennie
> Ph.D. Candidate, University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> www.utm.utoronto.ca/~w3rennie
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From marc_schwartz at comcast.net  Thu Feb  1 18:51:10 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 11:51:10 -0600
Subject: [R] Losing factor levels when moving variables from one	context
	to another
In-Reply-To: <6.1.0.6.0.20070201120612.01babc38@mail.utm.utoronto.ca>
References: <6.1.0.6.0.20070201120612.01babc38@mail.utm.utoronto.ca>
Message-ID: <1170352270.9954.44.camel@localhost.localdomain>

On Thu, 2007-02-01 at 12:13 -0500, Michael Rennie wrote:
> Hi, there
> 
> I'm currently trying to figure out how to keep my "factor" levels for a 
> variable when moving it from one data frame or matrix to another.
> 
> Example below:
> 
> vec1<-(rep("10",5))
> vec2<-(rep("30",5))
> vec3<-(rep("80",5))
> vecs<-c(vec1, vec2, vec3)
> 
> resp<-rnorm(2,15)
> 
> dat<-as.data.frame(cbind(resp, vecs))
> dat$vecs<-factor(dat$vecs)
> dat
> 
> R returns:
>                    resp  vecs
> 1     1.57606068767956   10
> 2     2.30271782269308   10
> 3     2.39874788444542   10
> 4    0.963987738423353   10
> 5     2.03620782454740   10
> 6  -0.0706713324725649   30
> 7     1.49001721222926   30
> 8     2.00587718501980   30
> 9    0.450576585429981   30
> 10    2.87120375367357   30
> 11    2.25575058079324   80
> 12    2.03471288724508   80
> 13    2.67432066972984   80
> 14    1.74102136279177   80
> 15    2.29827581276955   80
> 
> and now:
> 
> newvar<-(rnorm(15,4))
> newdat<-as.data.frame(cbind(newvar, dat$vecs))
> newdat
> 
> R returns:
> 
>        newvar V2
> 1  4.300788  1
> 2  5.295951  1
> 3  5.099849  1
> 4  3.211045  1
> 5  3.703554  1
> 6  3.693826  2
> 7  5.314679  2
> 8  4.222270  2
> 9  3.534515  2
> 10 4.037401  2
> 11 4.476808  3
> 12 4.842449  3
> 13 3.109677  3
> 14 4.752961  3
> 15 4.445216  3
>  >
> 
> I seem to have lost everything I once has associated with "vecs", and it's 
> turned my actual values into arbitrary groupings.
> 
> I assume this has something to do with the behaviour of factors? Does 
> anyone have any suggestions on how to get my original levels, etc., back?
> 
> Cheers,
> 
> Mike

Mike,

The problem (specific to your example) is that you are using
as.data.frame() and cbind(), which will first coerce the columns to a
common data type, create a matrix and then coerce the matrix to a
dataframe.

Thus, in the second case, your factor dat$vecs is first being coerced to
its numeric equivalent values, rather then being retained as a factor,
since a matrix can contain only one data type and the first column is
numeric.

Try this instead:

vec1<-(rep("10", 5))
vec2<-(rep("30", 5))
vec3<-(rep("80", 5))
vecs<-c(vec1, vec2, vec3)

set.seed(1)
resp<-rnorm(15, 2)

dat <- data.frame(resp, vecs)

> str(dat)
'data.frame':	15 obs. of  2 variables:
 $ resp: num  1.37 2.18 1.16 3.60 2.33 ...
 $ vecs: Factor w/ 3 levels "10","30","80": 1 1 1 1 1 2 2 2 2 2 ..


set.seed(2)
newvar <- rnorm(15, 4)
newdat <- data.frame(newvar, dat$vecs)

> str(newdat)
'data.frame':	15 obs. of  2 variables:
 $ newvar  : num  3.10 4.18 5.59 2.87 3.92 ...
 $ dat.vecs: Factor w/ 3 levels "10","30","80": 1 1 1 1 1 2 2 2 2 2 ...

> all(levels(newdat$dat.vecs) == levels(dat$vecs))
[1] TRUE


BTW, there may very well be times when you are combining two factors
together and need to ensure that the factor levels either are
intentionally different or need to "relevel" the combined factors into
common levels. See the Warning and other information in ?factor. This
would be critical, for example, if you are combining data sets to then
run modeling functions on the combined data sets.

HTH,

Marc Schwartz


From bolker at zoo.ufl.edu  Thu Feb  1 19:07:35 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Thu, 1 Feb 2007 18:07:35 +0000 (UTC)
Subject: [R] would you please navigate me?
References: <368305.31709.qm@web56103.mail.re3.yahoo.com>
	<4AC03A6244C3C34BB52A7EC60B799C4C03944B@m-pdc.sbu.ac.ir>
Message-ID: <loom.20070201T190531-783@post.gmane.org>

a.eslami <a.eslami <at> Mail.sbu.ac.ir> writes:

> 
> 
> Hello 
>    My name is Aida Eslami. I am a M.S.c student of statistics at Shahid
Beheshti University , Tehran, Iran. The
> subject of my thesis is Analysis of Masked Data. I have some problems in
writing of my program
> (optimization). Would you please navigate me and introduce some to help me? 
> Thank you. 
> 
>  
>                                                                              
                         Yours sincerely
>                                                                              
                          Aida Eslami


  I'm sorry, but we can only answer _specific_ questions about R on
this mailing list; there are too many deserving students all over the
world for us to help them all.  You should try to get enough help
from someone at your local institution to get you to the point where
you can formulate a specific question about R; failing that, you will
have to struggle with the R documentation on your own until you can
get to that point.  As it says at the bottom of every e-mail to the
list, please read the posting guide as well ...


From racinej at mcmaster.ca  Thu Feb  1 19:18:03 2007
From: racinej at mcmaster.ca (Jeffrey Racine)
Date: Thu, 01 Feb 2007 13:18:03 -0500
Subject: [R] Help with efficient double sum of max (X_i, Y_i) (X & Y vectors)
Message-ID: <1170353883.961.15.camel@pc-racine1.economics.mcmaster.ca>

Greetings.

For R gurus this may be a no brainer, but I could not find pointers to
efficient computation of this beast in past help files.

Background - I wish to implement a Cramer-von Mises type test statistic
which involves double sums of max(X_i,Y_j) where X and Y are vectors of
differing length.

I am currently using ifelse pointwise in a vector, but have a nagging
suspicion that there is a more efficient way to do this. Basically, I
require three sums:

sum1: \sum_i\sum_j max(X_i,X_j)
sum2: \sum_i\sum_j max(Y_i,Y_j)
sum3: \sum_i\sum_j max(X_i,Y_j)

Here is my current implementation - any pointers to more efficient
computation greatly appreciated.

  nx <- length(x)
  ny <- length(y)

  sum1 <- 0
  sum3 <- 0
    
  for(i in 1:nx) {
    sum1 <- sum1 + sum(ifelse(x[i]>x,x[i],x))
    sum3 <- sum3 + sum(ifelse(x[i]>y,x[i],y))
  }

  sum2 <- 0
  sum4 <- sum3 # symmetric and identical
    
  for(i in 1:ny) {
    sum2 <- sum2 + sum(ifelse(y[i]>y,y[i],y))
  }

Thanks in advance for your help.

-- Jeff

-- 
Professor J. S. Racine         Phone:  (905) 525 9140 x 23825
Department of Economics        FAX:    (905) 521-8232
McMaster University            e-mail: racinej at mcmaster.ca
1280 Main St. W.,Hamilton,     URL:
http://www.economics.mcmaster.ca/racine/
Ontario, Canada. L8S 4M4

`The generation of random numbers is too important to be left to chance'


From bcarvalh at jhsph.edu  Thu Feb  1 19:37:01 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Thu, 1 Feb 2007 13:37:01 -0500
Subject: [R] Help with efficient double sum of max (X_i,
	Y_i) (X & Y vectors)
In-Reply-To: <1170353883.961.15.camel@pc-racine1.economics.mcmaster.ca>
References: <1170353883.961.15.camel@pc-racine1.economics.mcmaster.ca>
Message-ID: <809DB795-782B-4C78-84F3-9CB7869D02C1@jhsph.edu>

Well, a reproducible example would be nice =)

not tested:

x = rnorm(10)
y = rnorm(20)
mymax <- function(t1, t2) apply(cbind(t1, t2), 1, max)
sum(outer(x, y, mymax))

is this sth like what you need?

b

On Feb 1, 2007, at 1:18 PM, Jeffrey Racine wrote:

> Greetings.
>
> For R gurus this may be a no brainer, but I could not find pointers to
> efficient computation of this beast in past help files.
>
> Background - I wish to implement a Cramer-von Mises type test  
> statistic
> which involves double sums of max(X_i,Y_j) where X and Y are  
> vectors of
> differing length.
>
> I am currently using ifelse pointwise in a vector, but have a nagging
> suspicion that there is a more efficient way to do this. Basically, I
> require three sums:
>
> sum1: \sum_i\sum_j max(X_i,X_j)
> sum2: \sum_i\sum_j max(Y_i,Y_j)
> sum3: \sum_i\sum_j max(X_i,Y_j)
>
> Here is my current implementation - any pointers to more efficient
> computation greatly appreciated.
>
>   nx <- length(x)
>   ny <- length(y)
>
>   sum1 <- 0
>   sum3 <- 0
>
>   for(i in 1:nx) {
>     sum1 <- sum1 + sum(ifelse(x[i]>x,x[i],x))
>     sum3 <- sum3 + sum(ifelse(x[i]>y,x[i],y))
>   }
>
>   sum2 <- 0
>   sum4 <- sum3 # symmetric and identical
>
>   for(i in 1:ny) {
>     sum2 <- sum2 + sum(ifelse(y[i]>y,y[i],y))
>   }
>
> Thanks in advance for your help.
>
> -- Jeff
>
> -- 
> Professor J. S. Racine         Phone:  (905) 525 9140 x 23825
> Department of Economics        FAX:    (905) 521-8232
> McMaster University            e-mail: racinej at mcmaster.ca
> 1280 Main St. W.,Hamilton,     URL:
> http://www.economics.mcmaster.ca/racine/
> Ontario, Canada. L8S 4M4
>
> `The generation of random numbers is too important to be left to  
> chance'
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rvaradhan at jhmi.edu  Thu Feb  1 19:37:01 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 1 Feb 2007 13:37:01 -0500
Subject: [R] Help with efficient double sum of max (X_i,
	Y_i) (X & Y vectors)
In-Reply-To: <1170353883.961.15.camel@pc-racine1.economics.mcmaster.ca>
References: <1170353883.961.15.camel@pc-racine1.economics.mcmaster.ca>
Message-ID: <000001c7462f$f6aa6d80$7c94100a@win.ad.jhu.edu>

Jeff,

Here is something which is a little faster:

sum1 <- sum(outer(x, x, FUN="pmax"))
sum3 <- sum(outer(x, y, FUN="pmax"))

Best,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeffrey Racine
Sent: Thursday, February 01, 2007 1:18 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Help with efficient double sum of max (X_i, Y_i) (X & Y
vectors)

Greetings.

For R gurus this may be a no brainer, but I could not find pointers to
efficient computation of this beast in past help files.

Background - I wish to implement a Cramer-von Mises type test statistic
which involves double sums of max(X_i,Y_j) where X and Y are vectors of
differing length.

I am currently using ifelse pointwise in a vector, but have a nagging
suspicion that there is a more efficient way to do this. Basically, I
require three sums:

sum1: \sum_i\sum_j max(X_i,X_j)
sum2: \sum_i\sum_j max(Y_i,Y_j)
sum3: \sum_i\sum_j max(X_i,Y_j)

Here is my current implementation - any pointers to more efficient
computation greatly appreciated.

  nx <- length(x)
  ny <- length(y)

  sum1 <- 0
  sum3 <- 0
    
  for(i in 1:nx) {
    sum1 <- sum1 + sum(ifelse(x[i]>x,x[i],x))
    sum3 <- sum3 + sum(ifelse(x[i]>y,x[i],y))
  }

  sum2 <- 0
  sum4 <- sum3 # symmetric and identical
    
  for(i in 1:ny) {
    sum2 <- sum2 + sum(ifelse(y[i]>y,y[i],y))
  }

Thanks in advance for your help.

-- Jeff

-- 
Professor J. S. Racine         Phone:  (905) 525 9140 x 23825
Department of Economics        FAX:    (905) 521-8232
McMaster University            e-mail: racinej at mcmaster.ca
1280 Main St. W.,Hamilton,     URL:
http://www.economics.mcmaster.ca/racine/
Ontario, Canada. L8S 4M4

`The generation of random numbers is too important to be left to chance'

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From elvis at xlsolutions-corp.com  Thu Feb  1 19:38:13 2007
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Thu, 01 Feb 2007 11:38:13 -0700
Subject: [R] Upcoming Course**** R/Splus Fundamentals and Programming
	Techniques**** In Washington DC, San Francisco and Princeton
Message-ID: <20070201113813.9f08cc34deb45d78e54b3b5664e21546.c8276bc567.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce our "R/S-plus Fundamentals and Programming Techniques" :
www.xlsolutions-corp.com/Rfund.htm

*** Washington DC / March 1-2, 2007
*** San Francisco / March 15-16, 2007
*** Princeton / Week of Feb 26  (dates coming soon)

Should we bring this course to your city? please let us know!

Interested in R/Splus Advanced course? email us.

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com


From sfalcon at fhcrc.org  Thu Feb  1 19:45:16 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 01 Feb 2007 10:45:16 -0800
Subject: [R] R for bioinformatics
In-Reply-To: <45C0C4D2.9080908@ebi.ac.uk> (Benoit Ballester's message of "Wed,
	31 Jan 2007 16:33:22 +0000")
References: <45C0C4D2.9080908@ebi.ac.uk>
Message-ID: <m2hcu5zphf.fsf@fhcrc.org>

Benoit Ballester <benoit at ebi.ac.uk> writes:

> Hi,
>
> I was wondering if someone could tell me more about this book, (if it's 
> a good or bad one).
> I can't find it, as it seems that O'Reilly doesn't publish any more.

I've never seen a copy so I can't comment about its quality (has
anyone seen a copy?).

You might want to take a look at _Bioinformatics and Computational
Biology Solutions Using R and Bioconductor_.

http://www.bioconductor.org/pub/docs/mogr/

+ seth


From greg at nosnhoj.org  Thu Feb  1 17:28:45 2007
From: greg at nosnhoj.org (Greg Johnson)
Date: Thu, 1 Feb 2007 16:28:45 +0000 (UTC)
Subject: [R] indexing without looping
References: <45C20FB2.2070800@ija.csic.es>
Message-ID: <loom.20070201T172756-204@post.gmane.org>

javier garcia-pintado <jgarcia <at> ija.csic.es> writes:

> 
> Hello,
> I've got a data.frame like this:
> 
> > > assignation <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
> > > assignation
> >   
>   value class
> 1   6.5     1
> 2   7.5     3
> 3   8.5     5
> 4  12.0     2
> 
> > >   
> >   
> 
> and a long vector of classes like this:
> 
> > > x <- c(1,1,2,7,6,5,4,3,2,2,2...)
> >   
> 
> And would like to obtain  a vector of length = length(x), with the
> corresponding values extracted from assignation table. Like this:
> 
> > > x.value
> >   
>  [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0
> 
> Could you help me with an elegant way to do this ?
> (I just can do it with looping for each class in the assignation table,
> what a think is not perfect in R's sense)
> 
> Wishes,
> Javier
> 

Javier,

you might try this:
> assignation <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
> assignation
  value class
1   6.5     1
2   7.5     3
3   8.5     5
4  12.0     2

> x <- c(1,1,2,7,6,5,4,3,2,2,2)
> x
 [1] 1 1 2 7 6 5 4 3 2 2 2

> merge( x, assignation, by.x=1, by.y=2, all.x=T )
   x value
1  1   6.5
2  1   6.5
3  2  12.0
4  2  12.0
5  2  12.0
6  2  12.0
7  3   7.5
8  4    NA
9  5   8.5
10 6    NA
11 7    NA


From marc_schwartz at comcast.net  Thu Feb  1 20:16:57 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 13:16:57 -0600
Subject: [R] R for bioinformatics
In-Reply-To: <m2hcu5zphf.fsf@fhcrc.org>
References: <45C0C4D2.9080908@ebi.ac.uk>  <m2hcu5zphf.fsf@fhcrc.org>
Message-ID: <1170357417.9954.73.camel@localhost.localdomain>

On Thu, 2007-02-01 at 10:45 -0800, Seth Falcon wrote:
> Benoit Ballester <benoit at ebi.ac.uk> writes:
> 
> > Hi,
> >
> > I was wondering if someone could tell me more about this book, (if it's 
> > a good or bad one).
> > I can't find it, as it seems that O'Reilly doesn't publish any more.
> 
> I've never seen a copy so I can't comment about its quality (has
> anyone seen a copy?).
> 
> You might want to take a look at _Bioinformatics and Computational
> Biology Solutions Using R and Bioconductor_.
> 
> http://www.bioconductor.org/pub/docs/mogr/

I'll stand (or sit) to be corrected on this as I cannot find the source,
but I have a recollection from seeing something quite some time ago that
the book may have never been published.

It is no longer listed on Amazon.com (USA), but here is a listing on UK:

http://www.amazon.co.uk/R-Bioinformatics-Kimberley-Seefeld/dp/059600544X

I located a posting from Kim Seefeld (one of the authors) on a usenet
group from back in 2003. Her e-mail then was listed as:

  kseefeld1 at aol.com

You might want to drop her a line if the e-mail is still valid.

HTH,

Marc Schwartz


From Achim.Zeileis at wu-wien.ac.at  Thu Feb  1 20:20:24 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 1 Feb 2007 20:20:24 +0100 (CET)
Subject: [R] Help with efficient double sum of max (X_i,
 Y_i) (X & Y vectors)
In-Reply-To: <1170353883.961.15.camel@pc-racine1.economics.mcmaster.ca>
Message-ID: <Pine.LNX.4.44.0702012018060.10150-100000@disco.wu-wien.ac.at>

Jeff,

you can do

> sum1: \sum_i\sum_j max(X_i,X_j)
> sum2: \sum_i\sum_j max(Y_i,Y_j)

sum(x * (2 * rank(x) - 1))

> sum3: \sum_i\sum_j max(X_i,Y_j)

sum(outer(x, y, pmax))

Probably, the latter can be speeded up even more...
Z


From kapatp at gmail.com  Thu Feb  1 20:43:10 2007
From: kapatp at gmail.com (Prasenjit Kapat)
Date: Thu, 1 Feb 2007 14:43:10 -0500
Subject: [R] plot.function with xlim, bug?
Message-ID: <de8c7cb40702011143o69790fe3ne7c25621cb0ea23d@mail.gmail.com>

Consider the following lines of code:

     plot(function(x) sin(cos(x)*exp(-x/2)), from=-8,to=7,xlim=c(-5,5))
Uses integral points (integers from -5 to 5) to draw the plot, instead
of the usual default of n= 101 "equally spaced points" (from
?plot.function).

    plot(function(x) sin(cos(x)*exp(-x/2)), from=-8,to=7,n=101,xlim=c(-5,5))
Gives the following error:
Error in add && par("xlog") : invalid 'x' type in 'x && y'

Any explanations? The following modification, in the plot.R by NOT
passing 'y' to plot.function, seems to fix both of the above problems
! I am sure to be missing something!

"plot2" <- function (x, y, ...)
{
    if (is.null(attr(x, "class")) && is.function(x)) {
        nms <- names(list(...))
        ## need to pass 'y' to plot.function() when positionally matched "????"
        if(missing(y)) # set to defaults {could use formals(plot.default)}:
            y <- { if (!"from" %in% nms) 0 else
                   if (!"to"   %in% nms) 1 else
                   if (!"xlim" %in% nms) NULL }
        if ("ylab" %in% nms)
            plot.function(x, ...)
        else
            plot.function(x, ylab=paste(deparse(substitute(x)),"(x)"), ...)
    }
    else UseMethod("plot")
}
-------------------------------------------------------------------------------------------------------------------------------
version.string R version 2.4.1 (2006-12-18)
platform   i486-pc-gnu-linux


From christos at nuverabio.com  Thu Feb  1 21:05:22 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 1 Feb 2007 15:05:22 -0500
Subject: [R] Lining up x-y datasets based on values of x
Message-ID: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>

Hi,

I was wondering if there is a direct approach for lining up 2-column
matrices according to the values of the first column.  An example and a
brute-force approach is given below:

x <- cbind(1:10, runif(10))
y <- cbind(5:14, runif(10))
z <- cbind((-4):5, runif(10))

xx <- seq( min(c(x[,1],y[,1],z[,1])), max(c(x[,1],y[,1],z[,1])), 1)
w <- cbind(xx, matrix(rep(0, 3*length(xx)), ncol=3)) 

w[ xx >= x[1,1] & xx <= x[10,1], 2 ] <- x[,2]
w[ xx >= y[1,1] & xx <= y[10,1], 3 ] <- y[,2]
w[ xx >= z[1,1] & xx <= z[10,1], 4 ] <- z[,2]

w 

I appreciate any pointers.

Thanks.
 
Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com


From marc_schwartz at comcast.net  Thu Feb  1 21:28:56 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 14:28:56 -0600
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
Message-ID: <1170361736.9954.96.camel@localhost.localdomain>

On Thu, 2007-02-01 at 15:05 -0500, Christos Hatzis wrote:
> Hi,
> 
> I was wondering if there is a direct approach for lining up 2-column
> matrices according to the values of the first column.  An example and a
> brute-force approach is given below:
> 
> x <- cbind(1:10, runif(10))
> y <- cbind(5:14, runif(10))
> z <- cbind((-4):5, runif(10))
> 
> xx <- seq( min(c(x[,1],y[,1],z[,1])), max(c(x[,1],y[,1],z[,1])), 1)
> w <- cbind(xx, matrix(rep(0, 3*length(xx)), ncol=3)) 
> 
> w[ xx >= x[1,1] & xx <= x[10,1], 2 ] <- x[,2]
> w[ xx >= y[1,1] & xx <= y[10,1], 3 ] <- y[,2]
> w[ xx >= z[1,1] & xx <= z[10,1], 4 ] <- z[,2]
> 
> w 
> 
> I appreciate any pointers.
> 
> Thanks.

How about this:

x <- cbind(1:10, runif(10))
y <- cbind(5:14, runif(10))
z <- cbind((-4):5, runif(10))

colnames(x) <- c("X", "Y")
colnames(y) <- c("X", "Y")
colnames(z) <- c("X", "Y")

xy <- merge(x, y, by = "X", all = TRUE)
xyz <- merge(xy, z, by = "X", all = TRUE)

xyz[is.na(xyz)] <- 0

> xyz
    X       Y.x       Y.y         Y
1  -4 0.0000000 0.0000000 0.3969099
2  -3 0.0000000 0.0000000 0.8943127
3  -2 0.0000000 0.0000000 0.4882819
4  -1 0.0000000 0.0000000 0.0275787
5   0 0.0000000 0.0000000 0.7562341
6   1 0.6873130 0.0000000 0.6185218
7   2 0.1930880 0.0000000 0.2318025
8   3 0.1164783 0.0000000 0.7336057
9   4 0.7408532 0.0000000 0.3006347
10  5 0.7112887 0.6383823 0.8515126
11  6 0.2719079 0.5952721 0.0000000
12  7 0.2067017 0.8178048 0.0000000
13  8 0.2085043 0.5714917 0.0000000
14  9 0.2251435 0.4032660 0.0000000
15 10 0.3471888 0.5247478 0.0000000
16 11 0.0000000 0.6899197 0.0000000
17 12 0.0000000 0.7188912 0.0000000
18 13 0.0000000 0.9133252 0.0000000
19 14 0.0000000 0.9186001 0.0000000

Note that 'xyz' will be a data frame, so just use as.matrix(xyz) to
coerce back to a numeric matrix if needed.

See ?merge

HTH,

Marc Schwartz


From p.dalgaard at biostat.ku.dk  Thu Feb  1 21:32:20 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 01 Feb 2007 21:32:20 +0100
Subject: [R] R for bioinformatics
In-Reply-To: <1170357417.9954.73.camel@localhost.localdomain>
References: <45C0C4D2.9080908@ebi.ac.uk> <m2hcu5zphf.fsf@fhcrc.org>
	<1170357417.9954.73.camel@localhost.localdomain>
Message-ID: <45C24E54.1080904@biostat.ku.dk>

Marc Schwartz wrote:
> On Thu, 2007-02-01 at 10:45 -0800, Seth Falcon wrote:
>   
>> Benoit Ballester <benoit at ebi.ac.uk> writes:
>>
>>     
>>> Hi,
>>>
>>> I was wondering if someone could tell me more about this book, (if it's 
>>> a good or bad one).
>>> I can't find it, as it seems that O'Reilly doesn't publish any more.
>>>       
>> I've never seen a copy so I can't comment about its quality (has
>> anyone seen a copy?).
>>
>> You might want to take a look at _Bioinformatics and Computational
>> Biology Solutions Using R and Bioconductor_.
>>
>> http://www.bioconductor.org/pub/docs/mogr/
>>     
>
> I'll stand (or sit) to be corrected on this as I cannot find the source,
> but I have a recollection from seeing something quite some time ago that
> the book may have never been published.
>   
It's been a while since the status was something along the lines that 
"the authors may or may not complete it". Subject matter moving faster 
than pen, I suspect....

-p


From topkatz at msn.com  Thu Feb  1 21:31:59 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 01 Feb 2007 15:31:59 -0500
Subject: [R] Can this loop be delooped?
Message-ID: <BAY132-F3067AE9564C5C073BCC2E2AAA40@phx.gbl>

Hi.

I have the following code in a loop.  It splits a vector into subvectors of 
equal size.  But if the size of the original vector is not an exact multiple 
of the desired subvector size, then the first few subvectors have one more 
element than the last few.  I know that the cut function could be used to 
determine where to break up the vector, but it doesn't seem to provide 
control over where to put the larger and smaller subvectors.

numgp1_v=sidect_v%/%compmin
numgroup_v[small]=max(1,numgp1_v[small])
sidemin_v=sidect_v%/%numgroup_v
nummax_v=sidect_v%%sidemin_v
eix=0
smallindexlist<-list(NULL)
for(i in 1:numgroup_v[small])	{
	bix=eix+1
	eix=bix+sidemin_v[small]+(i<=nummax_v[small])-1
	smallindexlist[[i]]<-dlpo_sm_v[bix:eix]
}

The key fact is that smallindexlist is a list, each list element is a 
subvector of dlpo_sm_v of the proper size.  The sizes may be different.


I tried to see whether I could eliminate the loop, as follows.  First I 
defined a function:

intgpi	<-	function(totalength,numgroups,groupnum,place="LEFT")	{
#	function to split the integer sequence, 1:totalength, into the groupnum 
group out of numgroups groups of equal size, totalength%/%numgroups
#	there are totalength%%numgroups number of groups of length 
1+totalength%/%numgroups, with the large groups all to one side, left if 
place=LEFT
#	totalength >= numgroups >= groupnum all integers, or it won't work right
	if(charmatch(toupper(place),"RIGHT",nomatch=FALSE)==1){
		extra1_1=max((groupnum-1)+((totalength%%numgroups)-numgroups),0)
		extra1_2=(groupnum>numgroups-totalength%%numgroups)
	}
	else{
		extra1_1=min(totalength%%numgroups,groupnum-1)
		extra1_2=(groupnum<=totalength%%numgroups)
	}
	gsize=totalength%/%numgroups
	gleft=((groupnum-1)*gsize)+extra1_1+1
	gright=gleft+gsize+extra1_2-1
	gleft:gright
}


The function appears to work okay.  Then I used it as follows:

numgp1_v=sidect_v%/%compmin
numgroup_v[small]=max(1,numgp1_v[small])
smallindexlist<-list(NULL)
smallindexlist=sapply(1:numgroup_v[small],function(i){dlpo_sm_v[intgpi(sidect_v[small],numgroup_v[small],i)]})

In this case, smallindexlist will be a list like I had before if the 
subvectors are not all the same size, but if the subvectors are all the same 
size, it appears that I get an array.  Can I force this operation to give me 
a list the way I want it in all cases?  Or is there a better way to deloop 
my original code?

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From christos at nuverabio.com  Thu Feb  1 21:45:05 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 1 Feb 2007 15:45:05 -0500
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <1170361736.9954.96.camel@localhost.localdomain>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
Message-ID: <007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>

Thanks Marc and Phil.

My dataset actually consists of 50+ individual files, so I will have to do
this one column at a time in a loop...
I might look into SQL and outer joints as an alternative to avoid looping.

Thanks again.
-Christos 

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Thursday, February 01, 2007 3:29 PM
To: christos at nuverabio.com
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Lining up x-y datasets based on values of x

On Thu, 2007-02-01 at 15:05 -0500, Christos Hatzis wrote:
> Hi,
> 
> I was wondering if there is a direct approach for lining up 2-column 
> matrices according to the values of the first column.  An example and 
> a brute-force approach is given below:
> 
> x <- cbind(1:10, runif(10))
> y <- cbind(5:14, runif(10))
> z <- cbind((-4):5, runif(10))
> 
> xx <- seq( min(c(x[,1],y[,1],z[,1])), max(c(x[,1],y[,1],z[,1])), 1) w 
> <- cbind(xx, matrix(rep(0, 3*length(xx)), ncol=3))
> 
> w[ xx >= x[1,1] & xx <= x[10,1], 2 ] <- x[,2] w[ xx >= y[1,1] & xx <= 
> y[10,1], 3 ] <- y[,2] w[ xx >= z[1,1] & xx <= z[10,1], 4 ] <- z[,2]
> 
> w
> 
> I appreciate any pointers.
> 
> Thanks.

How about this:

x <- cbind(1:10, runif(10))
y <- cbind(5:14, runif(10))
z <- cbind((-4):5, runif(10))

colnames(x) <- c("X", "Y")
colnames(y) <- c("X", "Y")
colnames(z) <- c("X", "Y")

xy <- merge(x, y, by = "X", all = TRUE)
xyz <- merge(xy, z, by = "X", all = TRUE)

xyz[is.na(xyz)] <- 0

> xyz
    X       Y.x       Y.y         Y
1  -4 0.0000000 0.0000000 0.3969099
2  -3 0.0000000 0.0000000 0.8943127
3  -2 0.0000000 0.0000000 0.4882819
4  -1 0.0000000 0.0000000 0.0275787
5   0 0.0000000 0.0000000 0.7562341
6   1 0.6873130 0.0000000 0.6185218
7   2 0.1930880 0.0000000 0.2318025
8   3 0.1164783 0.0000000 0.7336057
9   4 0.7408532 0.0000000 0.3006347
10  5 0.7112887 0.6383823 0.8515126
11  6 0.2719079 0.5952721 0.0000000
12  7 0.2067017 0.8178048 0.0000000
13  8 0.2085043 0.5714917 0.0000000
14  9 0.2251435 0.4032660 0.0000000
15 10 0.3471888 0.5247478 0.0000000
16 11 0.0000000 0.6899197 0.0000000
17 12 0.0000000 0.7188912 0.0000000
18 13 0.0000000 0.9133252 0.0000000
19 14 0.0000000 0.9186001 0.0000000

Note that 'xyz' will be a data frame, so just use as.matrix(xyz) to coerce
back to a numeric matrix if needed.

See ?merge

HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Thu Feb  1 21:39:11 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Feb 2007 20:39:11 +0000 (GMT)
Subject: [R] Problems installing R-2.4.1 on Solaris 11 x-86 from source:
 error in "gmake" after successful "configure"
In-Reply-To: <45C10B91.7020804@ipea.gov.br>
References: <45C10B91.7020804@ipea.gov.br>
Message-ID: <Pine.LNX.4.64.0702012031040.25244@gannet.stats.ox.ac.uk>

What is 'Solaris 11'?  According to www.sun.com, the latest Solaris 
version is 10, and my sysadmins have not heard of Solaris 11.

You seem to be missing the Solaris compilation tools, ar in this case.
In Solaris <= 10 they are in /usr/ccs/bin, not in the path by default.

On Wed, 31 Jan 2007, Octavio Tourinho wrote:

> Dear friends,
> I am trying to install R-2.4.1 from source on Solaris 11 x-86. 64 bits,

There is 32-bit x86 and 64-bit amd64 or x86_64.

> running on Sun Ultra-20 workstation, and using the SunStudio 11 compilers.
> I was able to "configure" R correctly, but received an error in "gmake", 
> aparently related to bzip2 which I have been unable to debug.
> The messages are listed below.
> The configure.log and configure.status files are attached.
>
> Any help would be sincerely appreciated.
>
> Octavio Tourinho
>
> =============================================
> R is now configured for i386-pc-solaris2.11
>
> Source directory:          .
> Installation directory:    /usr/local
>
> C compiler:                gcc -std=gnu99 -D__NO_MATH_INLINES -g -O2
> Fortran 77 compiler:       g77  -g -O2
>
> C++ compiler:              g++  -g -O2
> Fortran 90/95 compiler:    f95 -g
>
> Interfaces supported:      X11, tcltk
> External libraries:        readline
> Additional capabilities:   PNG, JPEG, NLS
> Options enabled:           shared BLAS, R profiling
>
> Recommended packages:      yes
>
> configure: WARNING: you cannot build DVI versions of the R manuals
> configure: WARNING: you cannot build PDF versions of the R manuals
> # gmake
> gmake[1]: Entering directory `/usr/local/R-2.4.1/m4'
> gmake[1]: Nothing to be done for `R'.
> gmake[1]: Leaving directory `/usr/local/R-2.4.1/m4'
> gmake[1]: Entering directory `/usr/local/R-2.4.1/tools'
> gmake[1]: Nothing to be done for `R'.
> gmake[1]: Leaving directory `/usr/local/R-2.4.1/tools'
> gmake[1]: Entering directory `/usr/local/R-2.4.1/doc'
> gmake[2]: Entering directory `/usr/local/R-2.4.1/doc/html'
> gmake[3]: Entering directory `/usr/local/R-2.4.1/doc/html/search'
> gmake[3]: Leaving directory `/usr/local/R-2.4.1/doc/html/search'
> gmake[2]: Leaving directory `/usr/local/R-2.4.1/doc/html'
> gmake[2]: Entering directory `/usr/local/R-2.4.1/doc/manual'
> gmake[2]: Nothing to be done for `R'.
> gmake[2]: Leaving directory `/usr/local/R-2.4.1/doc/manual'
> gmake[1]: Leaving directory `/usr/local/R-2.4.1/doc'
> gmake[1]: Entering directory `/usr/local/R-2.4.1/etc'
> gmake[1]: Leaving directory `/usr/local/R-2.4.1/etc'
> gmake[1]: Entering directory `/usr/local/R-2.4.1/share'
> gmake[1]: Leaving directory `/usr/local/R-2.4.1/share'
> gmake[1]: Entering directory `/usr/local/R-2.4.1/src'
> gmake[2]: Entering directory `/usr/local/R-2.4.1/src/scripts'
> creating src/scripts/R.fe
> gmake[3]: Entering directory `/usr/local/R-2.4.1/src/scripts'
> gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/scripts'
> gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/scripts'
> gmake[2]: Entering directory `/usr/local/R-2.4.1/src/include'
> config.status: creating src/include/config.h
> config.status: src/include/config.h is unchanged
> Rmath.h is unchanged
> gmake[3]: Entering directory `/usr/local/R-2.4.1/src/include/R_ext'
> gmake[3]: Nothing to be done for `R'.
> gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/include/R_ext'
> gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/include'
> gmake[2]: Entering directory `/usr/local/R-2.4.1/src/extra'
> gmake[3]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
> gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
> gmake[4]: `libRblas.so' is up to date.
> gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
> gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
> /usr/local/R-2.4.1/lib/libRblas.so is unchanged
> gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
> gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
> gmake[3]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
> gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
> gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
> gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
> rm -f libbz2.a
> false cr libbz2.a blocksort.o bzlib.o compress.o crctable.o decompress.o 
> huffman.o randtable.o
> gmake[4]: *** [libbz2.a] Error 1
> gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
> gmake[3]: *** [R] Error 2
> gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
> gmake[2]: *** [R] Error 1
> gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/extra'
> gmake[1]: *** [R] Error 1
> gmake[1]: Leaving directory `/usr/local/R-2.4.1/src'
> gmake: *** [R] Error 1
> ============================================================
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Thu Feb  1 21:49:43 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 14:49:43 -0600
Subject: [R] R for bioinformatics
In-Reply-To: <45C24E54.1080904@biostat.ku.dk>
References: <45C0C4D2.9080908@ebi.ac.uk> <m2hcu5zphf.fsf@fhcrc.org>
	<1170357417.9954.73.camel@localhost.localdomain>
	<45C24E54.1080904@biostat.ku.dk>
Message-ID: <1170362983.9954.103.camel@localhost.localdomain>

On Thu, 2007-02-01 at 21:32 +0100, Peter Dalgaard wrote:
> Marc Schwartz wrote:
> > On Thu, 2007-02-01 at 10:45 -0800, Seth Falcon wrote:
> >   
> >> Benoit Ballester <benoit at ebi.ac.uk> writes:
> >>
> >>     
> >>> Hi,
> >>>
> >>> I was wondering if someone could tell me more about this book, (if it's 
> >>> a good or bad one).
> >>> I can't find it, as it seems that O'Reilly doesn't publish any more.
> >>>       
> >> I've never seen a copy so I can't comment about its quality (has
> >> anyone seen a copy?).
> >>
> >> You might want to take a look at _Bioinformatics and Computational
> >> Biology Solutions Using R and Bioconductor_.
> >>
> >> http://www.bioconductor.org/pub/docs/mogr/
> >>     
> >
> > I'll stand (or sit) to be corrected on this as I cannot find the source,
> > but I have a recollection from seeing something quite some time ago that
> > the book may have never been published.
> >   
> It's been a while since the status was something along the lines that 
> "the authors may or may not complete it". Subject matter moving faster 
> than pen, I suspect....

Peter, that wording does seem familiar, just cannot recall where I saw
it. Perhaps on the O'Reilly web site, where it is no longer listed.

For confirmation, I called O'Reilly's customer service in Cambridge, MA.
They confirm that the book was indeed cancelled and never published.

No reasons were given.

Regards,

Marc


From lamac_k at hotmail.com  Thu Feb  1 22:12:15 2007
From: lamac_k at hotmail.com (lamack lamack)
Date: Thu, 01 Feb 2007 21:12:15 +0000
Subject: [R] time series analysis
Message-ID: <BAY113-F13DE1113C78074C2D8565499A40@phx.gbl>

Does anyone know a good introductory book or tutorial about time series 
analysis? (time
series for a beginner).

Thank you so much.

John Lamak

_________________________________________________________________
Descubra como mandar Torpedos SMS do seu Messenger para o celular dos seus 
amigos. http://mobile.msn.com/


From marc_schwartz at comcast.net  Thu Feb  1 22:20:50 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 15:20:50 -0600
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
Message-ID: <1170364850.9954.110.camel@localhost.localdomain>

On Thu, 2007-02-01 at 15:45 -0500, Christos Hatzis wrote:
> Thanks Marc and Phil.
> 
> My dataset actually consists of 50+ individual files, so I will have to do
> this one column at a time in a loop...
> I might look into SQL and outer joints as an alternative to avoid looping.
> 
> Thanks again.
> -Christos 

If the files conform to some naming convention and/or are all located in
a common sub-directory, you can use list.files() to get the file names
into a vector.  If not, you could use file.choose() interactively.

Then use either a for() loop or sapply() to loop over the filenames,
read them in to data frames using read.table() and merge them together
in the same loop.

When it comes to basic data manipulation like this, loops are not a bad
thing. The overhead of a loop is typically outweighed by the file I/O
and related considerations.

HTH,

Marc


From BEN at SSANET.COM  Thu Feb  1 22:31:17 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Thu, 1 Feb 2007 15:31:17 -0600
Subject: [R] time series analysis
References: <BAY113-F13DE1113C78074C2D8565499A40@phx.gbl>
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05DE1FAC@hercules.ssainfo>

John --

Well, as a start, have a look at "Modern Applied Statistics with S," by
Venables and Ripley, both of which names you will recognize if you read
this list often.  There is a 30-page chapter on time series (with
suggestions for other readings), obviously geared to S and R, that is a
good jumping-off place.

Ben Fairbank


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of lamack lamack
Sent: Thursday, February 01, 2007 3:12 PM
To: R-help at stat.math.ethz.ch
Subject: [R] time series analysis

Does anyone know a good introductory book or tutorial about time series 
analysis? (time
series for a beginner).

Thank you so much.

John Lamak

_________________________________________________________________
Descubra como mandar Torpedos SMS do seu Messenger para o celular dos
seus 
amigos. http://mobile.msn.com/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rab45+ at pitt.edu  Thu Feb  1 22:38:24 2007
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Thu, 01 Feb 2007 16:38:24 -0500
Subject: [R] Autocorrelated Binomial
Message-ID: <1170365904.6015.35.camel@localhost.localdomain>

I need to generate autocorrelated binary data. I've found references to
the IEKS package but none of the web pages currently exist. Does anyone
know where I can find this package or suggest another package?

Rick B.


From christos at nuverabio.com  Thu Feb  1 22:48:42 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 1 Feb 2007 16:48:42 -0500
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <1170364850.9954.110.camel@localhost.localdomain>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
	<1170364850.9954.110.camel@localhost.localdomain>
Message-ID: <007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>

[Sorry I meant to reply to the list]

Thanks, Marc.

That's what I have done.
However, there seems to be a penalty from using merge repeatedly as it
appears to internally re-sort the datasets.  In my case the datasets are
long (~35K rows) and already sorted so this step adds considerable and
unnecessary overhead.  There doesn't seem to be an option for disabling
sorting. Setting 'sort=F' only affects sorting of the final data.frame.

> system.time(merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], 
> by="V1", all=T, sort=T))
[1] 6.96 0.00 7.24   NA   NA
> system.time(merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], 
> by="V1", all=T, sort=F))
[1] 6.82 0.00 7.14   NA   NA
> 

I was wondering if perhaps there is a parallel between this problem and
methods for linining up time-series data, since such data are also usually
sorted on the time dimension. 

-Christos  

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Thursday, February 01, 2007 4:21 PM
To: christos at nuverabio.com
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Lining up x-y datasets based on values of x

On Thu, 2007-02-01 at 15:45 -0500, Christos Hatzis wrote:
> Thanks Marc and Phil.
> 
> My dataset actually consists of 50+ individual files, so I will have 
> to do this one column at a time in a loop...
> I might look into SQL and outer joints as an alternative to avoid looping.
> 
> Thanks again.
> -Christos

If the files conform to some naming convention and/or are all located in a
common sub-directory, you can use list.files() to get the file names into a
vector.  If not, you could use file.choose() interactively.

Then use either a for() loop or sapply() to loop over the filenames, read
them in to data frames using read.table() and merge them together in the
same loop.

When it comes to basic data manipulation like this, loops are not a bad
thing. The overhead of a loop is typically outweighed by the file I/O and
related considerations.

HTH,

Marc


From marc_schwartz at comcast.net  Thu Feb  1 23:00:11 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 16:00:11 -0600
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
	<1170364850.9954.110.camel@localhost.localdomain>
	<007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>
Message-ID: <1170367211.9954.138.camel@localhost.localdomain>

Christos,

Hmmmm....according to the Value section in ?merge:

A data frame. The rows are by default lexicographically sorted on the
common columns, but for sort=FALSE are in an unspecified order.


Looking at the code, while there is a lot of time spent on matching
things, the key sort() code seems to be near the end of the function:

          if (sort) 
            res <- res[if (all.x || all.y) 
                do.call("order", x[, 1:l.b, drop = FALSE])
            else sort.list(bx[m$xi]), , drop = FALSE]

I wonder if you could create a local version of merge(), say my.merge(),
without that code and without breaking things. A quick glance suggests
that as long as you are not merging on the rownames, I think that you
might be OK. You would want to test that hypothesis however.

HTH,

Marc

On Thu, 2007-02-01 at 16:48 -0500, Christos Hatzis wrote:
> [Sorry I meant to reply to the list]
> 
> Thanks, Marc.
> 
> That's what I have done.
> However, there seems to be a penalty from using merge repeatedly as it
> appears to internally re-sort the datasets.  In my case the datasets are
> long (~35K rows) and already sorted so this step adds considerable and
> unnecessary overhead.  There doesn't seem to be an option for disabling
> sorting. Setting 'sort=F' only affects sorting of the final data.frame.
> 
> > system.time(merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], 
> > by="V1", all=T, sort=T))
> [1] 6.96 0.00 7.24   NA   NA
> > system.time(merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], 
> > by="V1", all=T, sort=F))
> [1] 6.82 0.00 7.14   NA   NA
> > 
> 
> I was wondering if perhaps there is a parallel between this problem and
> methods for linining up time-series data, since such data are also usually
> sorted on the time dimension. 
> 
> -Christos  
> 
> -----Original Message-----
> From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
> Sent: Thursday, February 01, 2007 4:21 PM
> To: christos at nuverabio.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Lining up x-y datasets based on values of x
> 
> On Thu, 2007-02-01 at 15:45 -0500, Christos Hatzis wrote:
> > Thanks Marc and Phil.
> > 
> > My dataset actually consists of 50+ individual files, so I will have 
> > to do this one column at a time in a loop...
> > I might look into SQL and outer joints as an alternative to avoid looping.
> > 
> > Thanks again.
> > -Christos
> 
> If the files conform to some naming convention and/or are all located in a
> common sub-directory, you can use list.files() to get the file names into a
> vector.  If not, you could use file.choose() interactively.
> 
> Then use either a for() loop or sapply() to loop over the filenames, read
> them in to data frames using read.table() and merge them together in the
> same loop.
> 
> When it comes to basic data manipulation like this, loops are not a bad
> thing. The overhead of a loop is typically outweighed by the file I/O and
> related considerations.
> 
> HTH,
> 
> Marc


From tcoram at mail.wsu.edu  Thu Feb  1 23:46:42 2007
From: tcoram at mail.wsu.edu (Tristan Coram)
Date: Thu, 1 Feb 2007 14:46:42 -0800
Subject: [R] Affymetrix data analysis
Message-ID: <000301c74652$db16b6f0$b8e810ac@KateTristan>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/b9ff982c/attachment.pl 

From jholtman at gmail.com  Fri Feb  2 00:04:14 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 1 Feb 2007 18:04:14 -0500
Subject: [R] Can this loop be delooped?
In-Reply-To: <BAY132-F3067AE9564C5C073BCC2E2AAA40@phx.gbl>
References: <BAY132-F3067AE9564C5C073BCC2E2AAA40@phx.gbl>
Message-ID: <644e1f320702011504v46c9e2b2lbaab62f23daade23@mail.gmail.com>

This might do what you want:

> # test data
> x <- 1:43
> nb <- 5  # number of subsets
> # create vector of lengths of subsets
> ns <- rep(length(x) %/% nb, nb)
> # see if we have to adjust counts of initial subsets
> if ((.offset <- length(x) %% nb) != 0) ns[1:.offset] = ns[1:.offset] + 1
> # create the subsets
> split(x, rep(1:nb,ns))
$`1`
[1] 1 2 3 4 5 6 7 8 9

$`2`
[1] 10 11 12 13 14 15 16 17 18

$`3`
[1] 19 20 21 22 23 24 25 26 27

$`4`
[1] 28 29 30 31 32 33 34 35

$`5`
[1] 36 37 38 39 40 41 42 43



On 2/1/07, Talbot Katz <topkatz at msn.com> wrote:
> Hi.
>
> I have the following code in a loop.  It splits a vector into subvectors of
> equal size.  But if the size of the original vector is not an exact multiple
> of the desired subvector size, then the first few subvectors have one more
> element than the last few.  I know that the cut function could be used to
> determine where to break up the vector, but it doesn't seem to provide
> control over where to put the larger and smaller subvectors.
>
> numgp1_v=sidect_v%/%compmin
> numgroup_v[small]=max(1,numgp1_v[small])
> sidemin_v=sidect_v%/%numgroup_v
> nummax_v=sidect_v%%sidemin_v
> eix=0
> smallindexlist<-list(NULL)
> for(i in 1:numgroup_v[small])   {
>        bix=eix+1
>        eix=bix+sidemin_v[small]+(i<=nummax_v[small])-1
>        smallindexlist[[i]]<-dlpo_sm_v[bix:eix]
> }
>
> The key fact is that smallindexlist is a list, each list element is a
> subvector of dlpo_sm_v of the proper size.  The sizes may be different.
>
>
> I tried to see whether I could eliminate the loop, as follows.  First I
> defined a function:
>
> intgpi  <-      function(totalength,numgroups,groupnum,place="LEFT")    {
> #       function to split the integer sequence, 1:totalength, into the groupnum
> group out of numgroups groups of equal size, totalength%/%numgroups
> #       there are totalength%%numgroups number of groups of length
> 1+totalength%/%numgroups, with the large groups all to one side, left if
> place=LEFT
> #       totalength >= numgroups >= groupnum all integers, or it won't work right
>        if(charmatch(toupper(place),"RIGHT",nomatch=FALSE)==1){
>                extra1_1=max((groupnum-1)+((totalength%%numgroups)-numgroups),0)
>                extra1_2=(groupnum>numgroups-totalength%%numgroups)
>        }
>        else{
>                extra1_1=min(totalength%%numgroups,groupnum-1)
>                extra1_2=(groupnum<=totalength%%numgroups)
>        }
>        gsize=totalength%/%numgroups
>        gleft=((groupnum-1)*gsize)+extra1_1+1
>        gright=gleft+gsize+extra1_2-1
>        gleft:gright
> }
>
>
> The function appears to work okay.  Then I used it as follows:
>
> numgp1_v=sidect_v%/%compmin
> numgroup_v[small]=max(1,numgp1_v[small])
> smallindexlist<-list(NULL)
> smallindexlist=sapply(1:numgroup_v[small],function(i){dlpo_sm_v[intgpi(sidect_v[small],numgroup_v[small],i)]})
>
> In this case, smallindexlist will be a list like I had before if the
> subvectors are not all the same size, but if the subvectors are all the same
> size, it appears that I get an array.  Can I force this operation to give me
> a list the way I want it in all cases?  Or is there a better way to deloop
> my original code?
>
> Thanks!
>
> --  TMK  --
> 212-460-5430    home
> 917-656-5351    cell
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From rdporto1 at terra.com.br  Fri Feb  2 00:24:08 2007
From: rdporto1 at terra.com.br (rdporto1)
Date: Thu,  1 Feb 2007 20:24:08 -0300
Subject: [R] Wavlet filter using morlet mother wavelet
Message-ID: <JCT508$48CD3C45E9F787EB435739AEF0EED6D2@terra.com.br>

Anil Kumar,

it seems there isn't packages for continuous wavelet 
transforms in R. Anyway, take a look at the packages
waveslim, wavethresh, wavelets or rwt. Maybe one of
them can be useful to you.

Rogerio.

---------- Cabe?alho original -----------

De: r-help-bounces at stat.math.ethz.ch
Para: r-help at stat.math.ethz.ch
C?pia: 
Data: 1 Feb 2007 07:33:52 -0000
Assunto: [R] Wavlet filter using morlet mother wavelet

> &nbsp;   Hi, List ,I am searching any package on R which can do wavelet filtering for mother wavelet morlet ,is anybody having any script for the same ?    I am new to the RwAVELET ANALSSIS..    THANKS IN ADVANCE  ANIL KUMAR          ANIL KUMAR(&nbsp;METEOROLOGIST)    LRF SECTION&nbsp;    NATIONAL CLIMATE&nbsp;CENTER     ADGM(RESEARCH)    INDIA METEOROLOGICAL&nbsp;DEPARTMENT    SHIVIJI NAGAR    PUNE-411005 INDIA    MOBILE +919422023277    anilkumar at imdpune.gov.in              
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From baron at psych.upenn.edu  Fri Feb  2 00:34:19 2007
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 1 Feb 2007 18:34:19 -0500
Subject: [R] RSiteSearch() etc. - speed improvement
Message-ID: <20070201233419.GA20314@psych.upenn.edu>

My R search page at http://finzi.psych.upenn.edu/
which is also what you get with RSiteSearch()
has been slowing down the last few months (years?).

I thought this was because the archives were just getting too big.
But I discovered a simple fix.  The technical term for the problem is
"garbage."  By cleaning up the garbage, I increased the speed to the
point where now most searches - even those with three search terms -
are instantaneous.

Thus, it is probably good for a few more years, before I have to think
about a different search engine or a faster computer (which I should get
anyway).

If you have given up on it because of its slow response, do try again.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron


From ripley at stats.ox.ac.uk  Fri Feb  2 00:34:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Feb 2007 23:34:44 +0000 (GMT)
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <1170367211.9954.138.camel@localhost.localdomain>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
	<1170364850.9954.110.camel@localhost.localdomain>
	<007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>
	<1170367211.9954.138.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0702012324250.20563@gannet.stats.ox.ac.uk>

On Thu, 1 Feb 2007, Marc Schwartz wrote:

> Christos,
>
> Hmmmm....according to the Value section in ?merge:
>
> A data frame. The rows are by default lexicographically sorted on the
> common columns, but for sort=FALSE are in an unspecified order.

There is also a sort in the .Internal code.  But I am not buying 
that this is a major part of the time without detailed evidence from 
profiling.  Sorting 35k numbers should take a few milliseconds, and 
less if they are already sorted.

> x <- rnorm(35000)
> system.time(y <- sort(x, method="quick"))
[1] 0.003 0.001 0.004 0.000 0.000
> system.time(sort(y, method="quick"))
[1] 0.002 0.000 0.001 0.000 0.000



> Looking at the code, while there is a lot of time spent on matching
> things, the key sort() code seems to be near the end of the function:
>
>          if (sort)
>            res <- res[if (all.x || all.y)
>                do.call("order", x[, 1:l.b, drop = FALSE])
>            else sort.list(bx[m$xi]), , drop = FALSE]
>
> I wonder if you could create a local version of merge(), say my.merge(),
> without that code and without breaking things. A quick glance suggests
> that as long as you are not merging on the rownames, I think that you
> might be OK. You would want to test that hypothesis however.
>
> HTH,
>
> Marc
>
> On Thu, 2007-02-01 at 16:48 -0500, Christos Hatzis wrote:
>> [Sorry I meant to reply to the list]
>>
>> Thanks, Marc.
>>
>> That's what I have done.
>> However, there seems to be a penalty from using merge repeatedly as it
>> appears to internally re-sort the datasets.  In my case the datasets are
>> long (~35K rows) and already sorted so this step adds considerable and
>> unnecessary overhead.  There doesn't seem to be an option for disabling
>> sorting. Setting 'sort=F' only affects sorting of the final data.frame.
>>
>>> system.time(merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]],
>>> by="V1", all=T, sort=T))
>> [1] 6.96 0.00 7.24   NA   NA
>>> system.time(merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]],
>>> by="V1", all=T, sort=F))
>> [1] 6.82 0.00 7.14   NA   NA
>>>
>>
>> I was wondering if perhaps there is a parallel between this problem and
>> methods for linining up time-series data, since such data are also usually
>> sorted on the time dimension.
>>
>> -Christos
>>
>> -----Original Message-----
>> From: Marc Schwartz [mailto:marc_schwartz at comcast.net]
>> Sent: Thursday, February 01, 2007 4:21 PM
>> To: christos at nuverabio.com
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Lining up x-y datasets based on values of x
>>
>> On Thu, 2007-02-01 at 15:45 -0500, Christos Hatzis wrote:
>>> Thanks Marc and Phil.
>>>
>>> My dataset actually consists of 50+ individual files, so I will have
>>> to do this one column at a time in a loop...
>>> I might look into SQL and outer joints as an alternative to avoid looping.
>>>
>>> Thanks again.
>>> -Christos
>>
>> If the files conform to some naming convention and/or are all located in a
>> common sub-directory, you can use list.files() to get the file names into a
>> vector.  If not, you could use file.choose() interactively.
>>
>> Then use either a for() loop or sapply() to loop over the filenames, read
>> them in to data frames using read.table() and merge them together in the
>> same loop.
>>
>> When it comes to basic data manipulation like this, loops are not a bad
>> thing. The overhead of a loop is typically outweighed by the file I/O and
>> related considerations.
>>
>> HTH,
>>
>> Marc
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jrstear at sandia.gov  Fri Feb  2 00:42:47 2007
From: jrstear at sandia.gov (Jon Stearley)
Date: Thu, 1 Feb 2007 16:42:47 -0700
Subject: [R] memory-efficient column aggregation of a sparse matrix
In-Reply-To: <40e66e0b0702010522w2106a0d4i4b993366e92ed911@mail.gmail.com>
References: <6D0C7829-2699-4D85-9D8D-A8696EF1AE6E@sandia.gov>
	<40e66e0b0702010522w2106a0d4i4b993366e92ed911@mail.gmail.com>
Message-ID: <3DE986B5-0CCD-464C-A608-37B219750DC4@sandia.gov>


On Feb 1, 2007, at 6:22 AM, Douglas Bates wrote:

> It turns out that in the sparse matrix code used by the
> Matrix package the triplet representation allows for duplicate index
> positions with the convention that the resulting value at a position
> is the sum of the values of any triplets with that index pair.

Very handy!  I suggest adding this nugget near the "(possibly  
redundant) triplets" phrase in Matrix.pdf.

> If you decide to use this approach please be aware that the indices
> for the triplet representation in the Matrix package are 0-based (as
> in C code) not 1-based (as in R code).  (I imagine that Martin is
> thinking "we really should change that" as he reads this part.)

The Value of the appended function is equivalent to my previous  
version, but it runs in 1/10'th the time, uses vastly less memory,  
and is fewer lines of code to boot!  Sure it's tricky, but it does  
the trick.

THANK YOU SO MUCH!

-jon

NEWaggregate.csr <- function(x,fac) {
         # cast into handy Matrix sparse Triplet form
         x.T <- as(as(x, "dgRMatrix"), "dgTMatrix")

         # factor column indexes (compensating for 0 vs 1 indexing)
         x.T at j <- as.integer(as.integer(fac[x.T at j+1])-1)

         # cast back, magically computing factor sums along the way :)
         y <- as(x.T, "matrix.csr")

         # and fix the dimension (doing this on x.T bus errors!)
         y at dimension <- as.integer(c(nrow(y),nlevels(fac)))
         y
}


From ggrothendieck at gmail.com  Fri Feb  2 01:24:49 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Feb 2007 19:24:49 -0500
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
Message-ID: <971536df0702011624l769bebb8pd012c95c15a263e7@mail.gmail.com>

The zoo package has a multiway merge with optional zero fill.
Here are two ways:

library(zoo)
merge(x = zoo(x[,2], x[,1]),
      y = zoo(y[,2], y[,1]),
      z = zoo(z[,2], z[,1]),
      fill = 0)

# or

library(zoo)
X <- list(x = x, y = y, z = z)
merge0 <- function(..., fill = 0) merge(..., fill = fill)
do.call("merge0", lapply(X, function(x) zoo(x[,2], x[,1])))

To get more info on zoo try:

vignette("zoo")

On 2/1/07, Christos Hatzis <christos at nuverabio.com> wrote:
> Hi,
>
> I was wondering if there is a direct approach for lining up 2-column
> matrices according to the values of the first column.  An example and a
> brute-force approach is given below:
>
> x <- cbind(1:10, runif(10))
> y <- cbind(5:14, runif(10))
> z <- cbind((-4):5, runif(10))
>
> xx <- seq( min(c(x[,1],y[,1],z[,1])), max(c(x[,1],y[,1],z[,1])), 1)
> w <- cbind(xx, matrix(rep(0, 3*length(xx)), ncol=3))
>
> w[ xx >= x[1,1] & xx <= x[10,1], 2 ] <- x[,2]
> w[ xx >= y[1,1] & xx <= y[10,1], 3 ] <- y[,2]
> w[ xx >= z[1,1] & xx <= z[10,1], 4 ] <- z[,2]
>
> w
>
> I appreciate any pointers.
>
> Thanks.
>
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Feb  2 01:39:13 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Feb 2007 19:39:13 -0500
Subject: [R] Wiki for Graphics tips for MacOS X
In-Reply-To: <20070201194405.GG15607@ihug.co.nz>
References: <E9E8F95E-7610-4D61-ABF5-B8CEA952F1A3@MUOhio.edu>
	<971536df0701310911g597d73afiede5947437e35498@mail.gmail.com>
	<20070201194405.GG15607@ihug.co.nz>
Message-ID: <971536df0702011639m2e69dcd3u1470d60e88bddecb@mail.gmail.com>

I don't have a Linux system to try it with but
omitting both dev.control statements it worked for me
between two Windows XP sessions on the same
machine using this version of R:

> R.version.string # Windows XP
[1] "R version 2.4.1 Patched (2006-12-30 r40331)"

It also successfully worked with:

> R.version.string # Windows XP
[1] "R version 2.5.0 Under development (unstable) (2007-01-31 r40623)"



On 2/1/07, Patrick Connolly <p_connolly at ihug.co.nz> wrote:
> On Wed, 31-Jan-2007 at 12:11PM -0500, Gabor Grothendieck wrote:
>
> |> To get the best results you need to transfer it using vector
> |> graphics rather than bitmapped graphics:
> |>
> |> http://www.stc-saz.org/resources/0203_graphics.pdf
> |>
> |> There are a number of variations described here (see
> |> entire thread).  Its for UNIX and Windows but I think
> |> it would likely work similarly on Mac and Windows:
> |>
> |> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32297.html
>
> I found that interesting, particularly this part:
>
> For example, on Linux do this:
>
>   dev.control(displaylist="enable") # enable display list
>   plot(1:10)
>   myplot <- recordPlot() # load displaylist into variable
>   save(myplot, file="myplot", ascii=TRUE)
>
> Send the ascii file, myplot, to the Windows machine and on Windows do this:
>
>   dev.control(displaylist="enable") # enable display list
>   load("myplot")
>   myplot # displays the plot
>   savePlot("myplot", type="wmf") # saves current plot as wmf
>
> I tried that, but I was never able to load the myplot in the Windows
> R.  I always got a message about a syntax error to do with ' ' but I
> was unable to work out what the problem was.  I thought it was because
> the transfer to Windows wasn't binary, but that wasn't the problem.
>
> I was unable to get the thread view at that archive to function so I
> was unable to see if there were any follow ups which offered an
> explanation.
>
> R has changed quite a bit in the years since then, so it might be that
> something needs to be done differently with more recent versions.
>
> Has anyone done this recently?
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>   ___    Patrick Connolly
>  {~._.~}                         Great minds discuss ideas
>  _( Y )_                        Middle minds discuss events
> (:_~*~_:)                        Small minds discuss people
>  (_)-(_)                                   ..... Anon
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>


From bcarvalh at jhsph.edu  Fri Feb  2 01:46:33 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Thu, 1 Feb 2007 19:46:33 -0500
Subject: [R] Affymetrix data analysis
In-Reply-To: <000301c74652$db16b6f0$b8e810ac@KateTristan>
References: <000301c74652$db16b6f0$b8e810ac@KateTristan>
Message-ID: <696C6E48-0EDB-416C-B840-A03390F1781E@jhsph.edu>

The bioconductor mailing list is probably a better place to ask this  
type of question.

bioconductor at stat.math.ethz.ch

But we also need to know what arrays are you working with, what the  
errors are, what your sessionInfo() is....

Let us know, ok?

b

On Feb 1, 2007, at 5:46 PM, Tristan Coram wrote:

> Hi,
>
>
>
> I am trying to read in my Affymetrix CEL files (48 files, total  
> ~600 MB) but
> I keep getting memory errors.  Can somebody please help me with  
> this.  Or is
> therea remote server I can send my data to for computation?
>
>
>
> Any help is much appreciated.
>
>
>
> Thanks
>
>
>
>
>
> Dr. Tristan Coram
>
> Postdoctoral Research Associate
>
> Research Plant Pathologist/Geneticist
>
>
>
> United States Department of Agriculture
>
> Agricultural Research Service
>
> Wheat Genetics, Quality Physiology & Disease Research
>
>
>
> 209 Johnson Hall
>
> Washington State University
>
> Pullman, WA 99163
>
>
>
> Office: +1 509 335-1596  Fax: +1 509 335-2553
>
> Email: tcoram at mail.wsu.edu
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Sicotte.Hugues at mayo.edu  Fri Feb  2 01:54:27 2007
From: Sicotte.Hugues at mayo.edu (Sicotte, Hugues   Ph.D.)
Date: Thu, 1 Feb 2007 18:54:27 -0600
Subject: [R] Affymetrix data analysis
References: <000301c74652$db16b6f0$b8e810ac@KateTristan>
	<696C6E48-0EDB-416C-B840-A03390F1781E@jhsph.edu>
Message-ID: <2E17292A64E6ED418A60BE89326B1AAB3225E3@msgebe11.mfad.mfroot.org>

Tristan, 
I have a soft spot for problems analyzing microarrays with R..

for the memory issue, there have been previous posts to this list..
But here is the answer I gave a few weeks ago.
If you need more memory, you have to move to linux or recompile R for
windows yourself..
.. But you'll still need a computer with more memory.
The long term solution, which we are implementing, is to rewrite the
normalization code so it doesn't
Need to load all those arrays at once.

-- cut previous part of message--
The defaults in R is to play nice and limit your allocation to half
the available RAM. Make sure you have a lot of disk swap space (at least
1G with 2G of RAM) and you can set your memory limit to 2G for R.

See help(memory.size)  and use the memory.limit function


Hugues 


P.s. Someone let me use their 16Gig of RAM linux
And I was able to run R-64 bits with "top" showing 6Gigs of RAM
allocated (with suitable --max-mem-size command line parameters at
startup for R). 
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Benilton Carvalho
Sent: Thursday, February 01, 2007 6:47 PM
To: Tristan Coram
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Affymetrix data analysis

The bioconductor mailing list is probably a better place to ask this  
type of question.

bioconductor at stat.math.ethz.ch

But we also need to know what arrays are you working with, what the  
errors are, what your sessionInfo() is....

Let us know, ok?

b

On Feb 1, 2007, at 5:46 PM, Tristan Coram wrote:

> Hi,
>
>
>
> I am trying to read in my Affymetrix CEL files (48 files, total  
> ~600 MB) but
> I keep getting memory errors.  Can somebody please help me with  
> this.  Or is
> therea remote server I can send my data to for computation?
>
>
>
> Any help is much appreciated.
>
>
>
> Thanks
>
>
>
>
>
> Dr. Tristan Coram
>
> Postdoctoral Research Associate
>
> Research Plant Pathologist/Geneticist
>
>
>
> United States Department of Agriculture
>
> Agricultural Research Service
>
> Wheat Genetics, Quality Physiology & Disease Research
>
>
>
> 209 Johnson Hall
>
> Washington State University
>
> Pullman, WA 99163
>
>
>
> Office: +1 509 335-1596  Fax: +1 509 335-2553
>
> Email: tcoram at mail.wsu.edu
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Fri Feb  2 03:58:48 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 20:58:48 -0600
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <Pine.LNX.4.64.0702012324250.20563@gannet.stats.ox.ac.uk>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
	<1170364850.9954.110.camel@localhost.localdomain>
	<007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>
	<1170367211.9954.138.camel@localhost.localdomain>
	<Pine.LNX.4.64.0702012324250.20563@gannet.stats.ox.ac.uk>
Message-ID: <1170385128.5033.43.camel@localhost.localdomain>

On Thu, 2007-02-01 at 23:34 +0000, Prof Brian Ripley wrote:
> On Thu, 1 Feb 2007, Marc Schwartz wrote:
> 
> > Christos,
> >
> > Hmmmm....according to the Value section in ?merge:
> >
> > A data frame. The rows are by default lexicographically sorted on the
> > common columns, but for sort=FALSE are in an unspecified order.
> 
> There is also a sort in the .Internal code.  But I am not buying 
> that this is a major part of the time without detailed evidence from 
> profiling.  Sorting 35k numbers should take a few milliseconds, and 
> less if they are already sorted.
> 
> > x <- rnorm(35000)
> > system.time(y <- sort(x, method="quick"))
> [1] 0.003 0.001 0.004 0.000 0.000
> > system.time(sort(y, method="quick"))
> [1] 0.002 0.000 0.001 0.000 0.000

Having had a chance to mock up some examples, I would have to agree with
Prof. Ripley on this point.

Presuming that we are not missing something about the nature of
Christos' data sets, here are 4 examples, with rows sorted in ascending
order, descending order, reversed sort order and random order. In
theory, the descending order example should, I believe, represent a
worst cast scenario, since reverse sorting a sorted list is typically
slowest. However, note that there is not much time variation below and
running each of the examples several times resulted in material
differences across runs.


1. Ascending order

DF.X <- data.frame(X = 1:35000, Y = runif(35000))
DF.Y <- data.frame(X = 1:35000, Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.249 0.004 0.264 0.000 0.000


2. Descending order

DF.X <- data.frame(X = 35000:1, Y = runif(35000))
DF.Y <- data.frame(X = 35000:1, Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.300 0.007 0.309 0.000 0.000


3. Reversed sort order

DF.X <- data.frame(X = 35000:1, Y = runif(35000))
DF.Y <- data.frame(X = 1:35000, Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.236 0.008 0.245 0.000 0.000


4. Random order

DF.X <- data.frame(X = sample(35000), Y = runif(35000))
DF.Y <- data.frame(X = sample(35000), Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.339 0.016 0.357 0.000 0.000



Spending some time looking at profiling the descending order example, we
get:

> summaryRprof()
$by.self
                         self.time self.pct total.time total.pct
"duplicated.default"          0.16     38.1       0.16      38.1
"match"                       0.08     19.0       0.08      19.0
"sort.list"                   0.08     19.0       0.08      19.0
"[.data.frame"                0.04      9.5       0.24      57.1
"merge.data.frame"            0.02      4.8       0.42     100.0
"names.default"               0.02      4.8       0.02       4.8
"seq_len"                     0.02      4.8       0.02       4.8
"merge"                       0.00      0.0       0.42     100.0
"["                           0.00      0.0       0.24      57.1
"any"                         0.00      0.0       0.18      42.9
"duplicated"                  0.00      0.0       0.18      42.9
"cbind"                       0.00      0.0       0.04       9.5
"data.frame"                  0.00      0.0       0.04       9.5
"data.row.names"              0.00      0.0       0.02       4.8
"names"                       0.00      0.0       0.02       4.8
"row.names<-"                 0.00      0.0       0.02       4.8
"row.names<-.data.frame"      0.00      0.0       0.02       4.8

$by.total
                         total.time total.pct self.time self.pct
"merge.data.frame"             0.42     100.0      0.02      4.8
"merge"                        0.42     100.0      0.00      0.0
"[.data.frame"                 0.24      57.1      0.04      9.5
"["                            0.24      57.1      0.00      0.0
"any"                          0.18      42.9      0.00      0.0
"duplicated"                   0.18      42.9      0.00      0.0
"duplicated.default"           0.16      38.1      0.16     38.1
"match"                        0.08      19.0      0.08     19.0
"sort.list"                    0.08      19.0      0.08     19.0
"cbind"                        0.04       9.5      0.00      0.0
"data.frame"                   0.04       9.5      0.00      0.0
"names.default"                0.02       4.8      0.02      4.8
"seq_len"                      0.02       4.8      0.02      4.8
"data.row.names"               0.02       4.8      0.00      0.0
"names"                        0.02       4.8      0.00      0.0
"row.names<-"                  0.02       4.8      0.00      0.0
"row.names<-.data.frame"       0.02       4.8      0.00      0.0

$sampling.time
[1] 0.42



The above suggests that a meaningful amount of time is spent in checking
for and dealing with duplicates in the common ('by') columns. To that
end:

DF.X <- data.frame(X = sample(10000, 35000, replace = TRUE), Y = runif(35000))
DF.Y <- data.frame(X = sample(10000, 35000, replace = TRUE), Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 3.316 0.148 3.502 0.000 0.000


So, it would seem that introducing duplicate values in the same sized
vector space does indeed materially affect the time required:

> summaryRprof()
$by.self
                         self.time self.pct total.time total.pct
"duplicated.default"          0.86     27.6       0.86      27.6
"make.unique"                 0.76     24.4       0.76      24.4
"[.data.frame"                0.72     23.1       1.70      54.5
"data.frame"                  0.18      5.8       0.38      12.2
"row.names<-.data.frame"      0.14      4.5       0.36      11.5
"names<-.default"             0.14      4.5       0.14       4.5
"merge.data.frame"            0.08      2.6       3.12     100.0
"order"                       0.06      1.9       0.06       1.9
"rbind"                       0.04      1.3       0.44      14.1
"[[<-.data.frame"             0.04      1.3       0.04       1.3
"match"                       0.04      1.3       0.04       1.3
"unclass"                     0.04      1.3       0.04       1.3
"unlist"                      0.02      0.6       0.02       0.6
"merge"                       0.00      0.0       3.12     100.0
"["                           0.00      0.0       1.70      54.5
"any"                         0.00      0.0       0.86      27.6
"duplicated"                  0.00      0.0       0.86      27.6
"cbind"                       0.00      0.0       0.38      12.2
"row.names<-"                 0.00      0.0       0.36      11.5
"do.call"                     0.00      0.0       0.18       5.8
"names<-"                     0.00      0.0       0.14       4.5
"data.row.names"              0.00      0.0       0.10       3.2
"[[<-"                        0.00      0.0       0.04       1.3

$by.total
                         total.time total.pct self.time self.pct
"merge.data.frame"             3.12     100.0      0.08      2.6
"merge"                        3.12     100.0      0.00      0.0
"[.data.frame"                 1.70      54.5      0.72     23.1
"["                            1.70      54.5      0.00      0.0
"duplicated.default"           0.86      27.6      0.86     27.6
"any"                          0.86      27.6      0.00      0.0
"duplicated"                   0.86      27.6      0.00      0.0
"make.unique"                  0.76      24.4      0.76     24.4
"rbind"                        0.44      14.1      0.04      1.3
"data.frame"                   0.38      12.2      0.18      5.8
"cbind"                        0.38      12.2      0.00      0.0
"row.names<-.data.frame"       0.36      11.5      0.14      4.5
"row.names<-"                  0.36      11.5      0.00      0.0
"do.call"                      0.18       5.8      0.00      0.0
"names<-.default"              0.14       4.5      0.14      4.5
"names<-"                      0.14       4.5      0.00      0.0
"data.row.names"               0.10       3.2      0.00      0.0
"order"                        0.06       1.9      0.06      1.9
"[[<-.data.frame"              0.04       1.3      0.04      1.3
"match"                        0.04       1.3      0.04      1.3
"unclass"                      0.04       1.3      0.04      1.3
"[[<-"                         0.04       1.3      0.00      0.0
"unlist"                       0.02       0.6      0.02      0.6

$sampling.time
[1] 3.12



I would also point out the following:

> str(DF.XY)
'data.frame':	124704 obs. of  3 variables:
 $ X  : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Y.x: num  0.7233 0.7233 0.7233 0.0577 0.0577 ...
 $ Y.y: num  0.805 0.742 0.324 0.805 0.742 ...


Note that the resultant data frame is NOT 35,000 rows as a consequence
of the multiple matches.  Presumably, this gets worse with each
subsequent merge back to the prior result. For example:


> system.time(DF.XY2 <- merge(DF.XY, DF.Y, by = "X", all = TRUE))
[1] 12.270  1.173 13.624  0.000  0.000

> str(DF.XY2)
'data.frame':	557116 obs. of  4 variables:
 $ X  : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Y.x: num  0.00213 0.00213 0.00213 0.85017 0.85017 ...
 $ Y.y: num  0.324 0.324 0.324 0.324 0.324 ...
 $ Y  : num  0.742 0.324 0.805 0.742 0.324 ...



I may be extrapolating beyond known data here, but Christos, the above
suggests that your data sets have some proportion of duplicate values in
the columns that you are using for matching.

HTH,

Marc Schwartz


From christos at nuverabio.com  Fri Feb  2 04:46:27 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 1 Feb 2007 22:46:27 -0500
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <1170385128.5033.43.camel@localhost.localdomain>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
	<1170364850.9954.110.camel@localhost.localdomain>
	<007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>
	<1170367211.9954.138.camel@localhost.localdomain>
	<Pine.LNX.4.64.0702012324250.20563@gannet.stats.ox.ac.uk>
	<1170385128.5033.43.camel@localhost.localdomain>
Message-ID: <001601c7467c$b85d4dc0$0202a8c0@headquarters.silicoinsights>

Marc,

I don't think the issue is duplicates in the matching columns.  The data
were generated by an instrument (NMR spectrometer), processed by the
instrument's software through an FFT transform and other transformations and
finally reported as a sequence of chemical shift (x) vs intensity (y) pairs.
So all x values are unique.  For the example that I reported earlier:

> length(nmr.spectra.serum[[1]]$V1)
[1] 32768
> length(unique(nmr.spectra.serum[[1]]$V1))
[1] 32768
> length(nmr.spectra.serum[[2]]$V1)
[1] 32768
> length(unique(nmr.spectra.serum[[2]]$V1))
[1] 32768

And most of the x-values are common
> sum(nmr.spectra.serum[[1]]$V1 %in% nmr.spectra.serum[[2]]$V1)
[1] 32625

For this reason, merge is probably an overkill for this problem and my
initial thought was to align the datasets through some simple index-shifting
operation. 

Profiling of the merge code in my case shows that most of the time is spent
on data frame subsetting operations and on internal merge and rbind calls
secondarily (if I read the summary output correctly).  So even if most of
the time in the internal merge function is spent on sorting (haven't checked
the source code), this is in the worst case a rather minor effect, as
suggested by Prof. Ripley.
  
> Rprof("merge.out")
> zz <- merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], by="V1",
all=T, sort=T)
> Rprof(NULL)
> summaryRprof("merge.out")

$by.self
                       self.time self.pct total.time total.pct
merge.data.frame            6.56     50.0      11.84      90.2
[.data.frame                2.42     18.4       3.68      28.0
merge                       1.28      9.8      13.12     100.0
rbind                       1.24      9.5       1.36      10.4
names<-.default             1.16      8.8       1.16       8.8
row.names<-.data.frame      0.12      0.9       0.18       1.4
duplicated.default          0.12      0.9       0.12       0.9
make.unique                 0.10      0.8       0.10       0.8
data.frame                  0.02      0.2       0.04       0.3
*                           0.02      0.2       0.02       0.2
is.na                       0.02      0.2       0.02       0.2
match                       0.02      0.2       0.02       0.2
order                       0.02      0.2       0.02       0.2
unclass                     0.02      0.2       0.02       0.2
[                           0.00      0.0       3.68      28.0
do.call                     0.00      0.0       1.18       9.0
names<-                     0.00      0.0       1.16       8.8
row.names<-                 0.00      0.0       0.18       1.4
any                         0.00      0.0       0.14       1.1
duplicated                  0.00      0.0       0.12       0.9
cbind                       0.00      0.0       0.04       0.3
as.vector                   0.00      0.0       0.02       0.2
seq                         0.00      0.0       0.02       0.2
seq.default                 0.00      0.0       0.02       0.2

$by.total
                       total.time total.pct self.time self.pct
merge                       13.12     100.0      1.28      9.8
merge.data.frame            11.84      90.2      6.56     50.0
[.data.frame                 3.68      28.0      2.42     18.4
[                            3.68      28.0      0.00      0.0
rbind                        1.36      10.4      1.24      9.5
do.call                      1.18       9.0      0.00      0.0
names<-.default              1.16       8.8      1.16      8.8
names<-                      1.16       8.8      0.00      0.0
row.names<-.data.frame       0.18       1.4      0.12      0.9
row.names<-                  0.18       1.4      0.00      0.0
any                          0.14       1.1      0.00      0.0
duplicated.default           0.12       0.9      0.12      0.9
duplicated                   0.12       0.9      0.00      0.0
make.unique                  0.10       0.8      0.10      0.8
data.frame                   0.04       0.3      0.02      0.2
cbind                        0.04       0.3      0.00      0.0
*                            0.02       0.2      0.02      0.2
is.na                        0.02       0.2      0.02      0.2
match                        0.02       0.2      0.02      0.2
order                        0.02       0.2      0.02      0.2
unclass                      0.02       0.2      0.02      0.2
as.vector                    0.02       0.2      0.00      0.0
seq                          0.02       0.2      0.00      0.0
seq.default                  0.02       0.2      0.00      0.0

$sampling.time
[1] 13.12


Thanks again for your time in looking into this.
-Christos

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Thursday, February 01, 2007 9:59 PM
To: Prof Brian 

Ripley
Cc: r-help at stat.math.ethz.ch; christos at nuverabio.com
Subject: Re: [R] Lining up x-y datasets based on values of x

On Thu, 2007-02-01 at 23:34 +0000, Prof Brian Ripley wrote:
> On Thu, 1 Feb 2007, Marc Schwartz wrote:
> 
> > Christos,
> >
> > Hmmmm....according to the Value section in ?merge:
> >
> > A data frame. The rows are by default lexicographically sorted on 
> > the common columns, but for sort=FALSE are in an unspecified order.
> 
> There is also a sort in the .Internal code.  But I am not buying that 
> this is a major part of the time without detailed evidence from 
> profiling.  Sorting 35k numbers should take a few milliseconds, and 
> less if they are already sorted.
> 
> > x <- rnorm(35000)
> > system.time(y <- sort(x, method="quick"))
> [1] 0.003 0.001 0.004 0.000 0.000
> > system.time(sort(y, method="quick"))
> [1] 0.002 0.000 0.001 0.000 0.000

Having had a chance to mock up some examples, I would have to agree with
Prof. Ripley on this point.

Presuming that we are not missing something about the nature of Christos'
data sets, here are 4 examples, with rows sorted in ascending order,
descending order, reversed sort order and random order. In theory, the
descending order example should, I believe, represent a worst cast scenario,
since reverse sorting a sorted list is typically slowest. However, note that
there is not much time variation below and running each of the examples
several times resulted in material differences across runs.


1. Ascending order

DF.X <- data.frame(X = 1:35000, Y = runif(35000)) DF.Y <- data.frame(X =
1:35000, Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.249 0.004 0.264 0.000 0.000


2. Descending order

DF.X <- data.frame(X = 35000:1, Y = runif(35000)) DF.Y <- data.frame(X =
35000:1, Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.300 0.007 0.309 0.000 0.000


3. Reversed sort order

DF.X <- data.frame(X = 35000:1, Y = runif(35000)) DF.Y <- data.frame(X =
1:35000, Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.236 0.008 0.245 0.000 0.000


4. Random order

DF.X <- data.frame(X = sample(35000), Y = runif(35000)) DF.Y <- data.frame(X
= sample(35000), Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.339 0.016 0.357 0.000 0.000



Spending some time looking at profiling the descending order example, we
get:

> summaryRprof()
$by.self
                         self.time self.pct total.time total.pct
"duplicated.default"          0.16     38.1       0.16      38.1
"match"                       0.08     19.0       0.08      19.0
"sort.list"                   0.08     19.0       0.08      19.0
"[.data.frame"                0.04      9.5       0.24      57.1
"merge.data.frame"            0.02      4.8       0.42     100.0
"names.default"               0.02      4.8       0.02       4.8
"seq_len"                     0.02      4.8       0.02       4.8
"merge"                       0.00      0.0       0.42     100.0
"["                           0.00      0.0       0.24      57.1
"any"                         0.00      0.0       0.18      42.9
"duplicated"                  0.00      0.0       0.18      42.9
"cbind"                       0.00      0.0       0.04       9.5
"data.frame"                  0.00      0.0       0.04       9.5
"data.row.names"              0.00      0.0       0.02       4.8
"names"                       0.00      0.0       0.02       4.8
"row.names<-"                 0.00      0.0       0.02       4.8
"row.names<-.data.frame"      0.00      0.0       0.02       4.8

$by.total
                         total.time total.pct self.time self.pct
"merge.data.frame"             0.42     100.0      0.02      4.8
"merge"                        0.42     100.0      0.00      0.0
"[.data.frame"                 0.24      57.1      0.04      9.5
"["                            0.24      57.1      0.00      0.0
"any"                          0.18      42.9      0.00      0.0
"duplicated"                   0.18      42.9      0.00      0.0
"duplicated.default"           0.16      38.1      0.16     38.1
"match"                        0.08      19.0      0.08     19.0
"sort.list"                    0.08      19.0      0.08     19.0
"cbind"                        0.04       9.5      0.00      0.0
"data.frame"                   0.04       9.5      0.00      0.0
"names.default"                0.02       4.8      0.02      4.8
"seq_len"                      0.02       4.8      0.02      4.8
"data.row.names"               0.02       4.8      0.00      0.0
"names"                        0.02       4.8      0.00      0.0
"row.names<-"                  0.02       4.8      0.00      0.0
"row.names<-.data.frame"       0.02       4.8      0.00      0.0

$sampling.time
[1] 0.42



The above suggests that a meaningful amount of time is spent in checking for
and dealing with duplicates in the common ('by') columns. To that
end:

DF.X <- data.frame(X = sample(10000, 35000, replace = TRUE), Y =
runif(35000)) DF.Y <- data.frame(X = sample(10000, 35000, replace = TRUE), Y
= runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 3.316 0.148 3.502 0.000 0.000


So, it would seem that introducing duplicate values in the same sized vector
space does indeed materially affect the time required:

> summaryRprof()
$by.self
                         self.time self.pct total.time total.pct
"duplicated.default"          0.86     27.6       0.86      27.6
"make.unique"                 0.76     24.4       0.76      24.4
"[.data.frame"                0.72     23.1       1.70      54.5
"data.frame"                  0.18      5.8       0.38      12.2
"row.names<-.data.frame"      0.14      4.5       0.36      11.5
"names<-.default"             0.14      4.5       0.14       4.5
"merge.data.frame"            0.08      2.6       3.12     100.0
"order"                       0.06      1.9       0.06       1.9
"rbind"                       0.04      1.3       0.44      14.1
"[[<-.data.frame"             0.04      1.3       0.04       1.3
"match"                       0.04      1.3       0.04       1.3
"unclass"                     0.04      1.3       0.04       1.3
"unlist"                      0.02      0.6       0.02       0.6
"merge"                       0.00      0.0       3.12     100.0
"["                           0.00      0.0       1.70      54.5
"any"                         0.00      0.0       0.86      27.6
"duplicated"                  0.00      0.0       0.86      27.6
"cbind"                       0.00      0.0       0.38      12.2
"row.names<-"                 0.00      0.0       0.36      11.5
"do.call"                     0.00      0.0       0.18       5.8
"names<-"                     0.00      0.0       0.14       4.5
"data.row.names"              0.00      0.0       0.10       3.2
"[[<-"                        0.00      0.0       0.04       1.3

$by.total
                         total.time total.pct self.time self.pct
"merge.data.frame"             3.12     100.0      0.08      2.6
"merge"                        3.12     100.0      0.00      0.0
"[.data.frame"                 1.70      54.5      0.72     23.1
"["                            1.70      54.5      0.00      0.0
"duplicated.default"           0.86      27.6      0.86     27.6
"any"                          0.86      27.6      0.00      0.0
"duplicated"                   0.86      27.6      0.00      0.0
"make.unique"                  0.76      24.4      0.76     24.4
"rbind"                        0.44      14.1      0.04      1.3
"data.frame"                   0.38      12.2      0.18      5.8
"cbind"                        0.38      12.2      0.00      0.0
"row.names<-.data.frame"       0.36      11.5      0.14      4.5
"row.names<-"                  0.36      11.5      0.00      0.0
"do.call"                      0.18       5.8      0.00      0.0
"names<-.default"              0.14       4.5      0.14      4.5
"names<-"                      0.14       4.5      0.00      0.0
"data.row.names"               0.10       3.2      0.00      0.0
"order"                        0.06       1.9      0.06      1.9
"[[<-.data.frame"              0.04       1.3      0.04      1.3
"match"                        0.04       1.3      0.04      1.3
"unclass"                      0.04       1.3      0.04      1.3
"[[<-"                         0.04       1.3      0.00      0.0
"unlist"                       0.02       0.6      0.02      0.6

$sampling.time
[1] 3.12



I would also point out the following:

> str(DF.XY)
'data.frame':	124704 obs. of  3 variables:
 $ X  : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Y.x: num  0.7233 0.7233 0.7233 0.0577 0.0577 ...
 $ Y.y: num  0.805 0.742 0.324 0.805 0.742 ...


Note that the resultant data frame is NOT 35,000 rows as a consequence of
the multiple matches.  Presumably, this gets worse with each subsequent
merge back to the prior result. For example:


> system.time(DF.XY2 <- merge(DF.XY, DF.Y, by = "X", all = TRUE))
[1] 12.270  1.173 13.624  0.000  0.000

> str(DF.XY2)
'data.frame':	557116 obs. of  4 variables:
 $ X  : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Y.x: num  0.00213 0.00213 0.00213 0.85017 0.85017 ...
 $ Y.y: num  0.324 0.324 0.324 0.324 0.324 ...
 $ Y  : num  0.742 0.324 0.805 0.742 0.324 ...



I may be extrapolating beyond known data here, but Christos, the above
suggests that your data sets have some proportion of duplicate values in the
columns that you are using for matching.

HTH,

Marc Schwartz


From gongyi.liao at msa.hinet.net  Fri Feb  2 04:58:22 2007
From: gongyi.liao at msa.hinet.net (=?UTF-8?Q?=E5=BB=96=E5=AE=AE=E6=AF=85?=)
Date: Fri, 02 Feb 2007 11:58:22 +0800
Subject: [R] Problems installing R-2.4.1 on Solaris 11 x-86 from	source:
	error in "gmake" after successful "configure"
In-Reply-To: <Pine.LNX.4.64.0702012031040.25244@gannet.stats.ox.ac.uk>
References: <45C10B91.7020804@ipea.gov.br>
	<Pine.LNX.4.64.0702012031040.25244@gannet.stats.ox.ac.uk>
Message-ID: <1170388702.5790.5.camel@LiaoLianFa>

On Thu, 2007-02-01 at 20:39 +0000, Prof Brian Ripley wrote:
> What is 'Solaris 11'?  According to www.sun.com, the latest Solaris 
> version is 10, and my sysadmins have not heard of Solaris 11.
> 
That is the solaris express community release, or the pre-release of the
upcoming OpenSolaris
http://www.opensolaris.org

> You seem to be missing the Solaris compilation tools, ar in this case.
> In Solaris <= 10 they are in /usr/ccs/bin, not in the path by default.
> 
> On Wed, 31 Jan 2007, Octavio Tourinho wrote:
> 
> > Dear friends,
> > I am trying to install R-2.4.1 from source on Solaris 11 x-86. 64 bits,
> 
> There is 32-bit x86 and 64-bit amd64 or x86_64.
> 
> > running on Sun Ultra-20 workstation, and using the SunStudio 11 compilers.
> > I was able to "configure" R correctly, but received an error in "gmake", 
> > aparently related to bzip2 which I have been unable to debug.
> > The messages are listed below.
> > The configure.log and configure.status files are attached.
> >
> > Any help would be sincerely appreciated.
> >
> > Octavio Tourinho
> >
> > =============================================
> > R is now configured for i386-pc-solaris2.11
> >
> > Source directory:          .
> > Installation directory:    /usr/local
> >
> > C compiler:                gcc -std=gnu99 -D__NO_MATH_INLINES -g -O2
> > Fortran 77 compiler:       g77  -g -O2
> >
> > C++ compiler:              g++  -g -O2
> > Fortran 90/95 compiler:    f95 -g
> >
> > Interfaces supported:      X11, tcltk
> > External libraries:        readline
> > Additional capabilities:   PNG, JPEG, NLS
> > Options enabled:           shared BLAS, R profiling
> >
> > Recommended packages:      yes
> >
> > configure: WARNING: you cannot build DVI versions of the R manuals
> > configure: WARNING: you cannot build PDF versions of the R manuals
> > # gmake
> > gmake[1]: Entering directory `/usr/local/R-2.4.1/m4'
> > gmake[1]: Nothing to be done for `R'.
> > gmake[1]: Leaving directory `/usr/local/R-2.4.1/m4'
> > gmake[1]: Entering directory `/usr/local/R-2.4.1/tools'
> > gmake[1]: Nothing to be done for `R'.
> > gmake[1]: Leaving directory `/usr/local/R-2.4.1/tools'
> > gmake[1]: Entering directory `/usr/local/R-2.4.1/doc'
> > gmake[2]: Entering directory `/usr/local/R-2.4.1/doc/html'
> > gmake[3]: Entering directory `/usr/local/R-2.4.1/doc/html/search'
> > gmake[3]: Leaving directory `/usr/local/R-2.4.1/doc/html/search'
> > gmake[2]: Leaving directory `/usr/local/R-2.4.1/doc/html'
> > gmake[2]: Entering directory `/usr/local/R-2.4.1/doc/manual'
> > gmake[2]: Nothing to be done for `R'.
> > gmake[2]: Leaving directory `/usr/local/R-2.4.1/doc/manual'
> > gmake[1]: Leaving directory `/usr/local/R-2.4.1/doc'
> > gmake[1]: Entering directory `/usr/local/R-2.4.1/etc'
> > gmake[1]: Leaving directory `/usr/local/R-2.4.1/etc'
> > gmake[1]: Entering directory `/usr/local/R-2.4.1/share'
> > gmake[1]: Leaving directory `/usr/local/R-2.4.1/share'
> > gmake[1]: Entering directory `/usr/local/R-2.4.1/src'
> > gmake[2]: Entering directory `/usr/local/R-2.4.1/src/scripts'
> > creating src/scripts/R.fe
> > gmake[3]: Entering directory `/usr/local/R-2.4.1/src/scripts'
> > gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/scripts'
> > gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/scripts'
> > gmake[2]: Entering directory `/usr/local/R-2.4.1/src/include'
> > config.status: creating src/include/config.h
> > config.status: src/include/config.h is unchanged
> > Rmath.h is unchanged
> > gmake[3]: Entering directory `/usr/local/R-2.4.1/src/include/R_ext'
> > gmake[3]: Nothing to be done for `R'.
> > gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/include/R_ext'
> > gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/include'
> > gmake[2]: Entering directory `/usr/local/R-2.4.1/src/extra'
> > gmake[3]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
> > gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
> > gmake[4]: `libRblas.so' is up to date.
> > gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
> > gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
> > /usr/local/R-2.4.1/lib/libRblas.so is unchanged
> > gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
> > gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
> > gmake[3]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
> > gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
> > gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
> > gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
> > rm -f libbz2.a
> > false cr libbz2.a blocksort.o bzlib.o compress.o crctable.o decompress.o 
> > huffman.o randtable.o
> > gmake[4]: *** [libbz2.a] Error 1
> > gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
> > gmake[3]: *** [R] Error 2
> > gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
> > gmake[2]: *** [R] Error 1
> > gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/extra'
> > gmake[1]: *** [R] Error 1
> > gmake[1]: Leaving directory `/usr/local/R-2.4.1/src'
> > gmake: *** [R] Error 1
> > ============================================================
> >
> 
I have build R-2.4.1 on my Solaris 11 box (which is, the Nevada build
55) which Sun studio 11's CC, but the configure file cannot detect the
malloc of SUN's, my installed R is using GNU malloc.


From christos at nuverabio.com  Fri Feb  2 05:01:38 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 1 Feb 2007 23:01:38 -0500
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <971536df0702011624l769bebb8pd012c95c15a263e7@mail.gmail.com>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<971536df0702011624l769bebb8pd012c95c15a263e7@mail.gmail.com>
Message-ID: <001701c7467e$d711f390$0202a8c0@headquarters.silicoinsights>

Thanks Gabor.

This is along the lines of what I was looking for.  In fact the merge
function for zoo objects (ordered) turns out to be almost an order of
magnitude faster than the generic merge function for my problem:

> system.time(
+ zz <- merge( spec.1 = zoo(nmr.spectra.serum[[1]]$V2,
nmr.spectra.serum[[1]]$V1),
+        spec.2 = zoo(nmr.spectra.serum[[2]]$V2, nmr.spectra.serum[[2]]$V1),
fill=NA )
+ )
[1] 0.74 0.07 0.82   NA   NA
> system.time(
+ ww <- merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], by="V1",
all=T, sort=T)
+ )
[1] 6.85 0.05 6.94   NA   NA
> head(zz)
        spec.1 spec.2
-1322.2 -0.651     NA
-1321.9 -0.266     NA
-1321.7 -0.962     NA
-1321.4 -0.602     NA
-1321.2  0.753     NA
-1320.9  1.212     NA
> head(ww)
       V1   V2.x V2.y
1 -1322.2 -0.651   NA
2 -1321.9 -0.266   NA
3 -1321.7 -0.962   NA
4 -1321.4 -0.602   NA
5 -1321.2  0.753   NA
6 -1320.9  1.212   NA
> 

Thanks again.
-Christos 

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Thursday, February 01, 2007 7:25 PM
To: christos at nuverabio.com
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Lining up x-y datasets based on values of x

The zoo package has a multiway merge with optional zero fill.
Here are two ways:

library(zoo)
merge(x = zoo(x[,2], x[,1]),
      y = zoo(y[,2], y[,1]),
      z = zoo(z[,2], z[,1]),
      fill = 0)

# or

library(zoo)
X <- list(x = x, y = y, z = z)
merge0 <- function(..., fill = 0) merge(..., fill = fill) do.call("merge0",
lapply(X, function(x) zoo(x[,2], x[,1])))

To get more info on zoo try:

vignette("zoo")

On 2/1/07, Christos Hatzis <christos at nuverabio.com> wrote:
> Hi,
>
> I was wondering if there is a direct approach for lining up 2-column 
> matrices according to the values of the first column.  An example and 
> a brute-force approach is given below:
>
> x <- cbind(1:10, runif(10))
> y <- cbind(5:14, runif(10))
> z <- cbind((-4):5, runif(10))
>
> xx <- seq( min(c(x[,1],y[,1],z[,1])), max(c(x[,1],y[,1],z[,1])), 1) w 
> <- cbind(xx, matrix(rep(0, 3*length(xx)), ncol=3))
>
> w[ xx >= x[1,1] & xx <= x[10,1], 2 ] <- x[,2] w[ xx >= y[1,1] & xx <= 
> y[10,1], 3 ] <- y[,2] w[ xx >= z[1,1] & xx <= z[10,1], 4 ] <- z[,2]
>
> w
>
> I appreciate any pointers.
>
> Thanks.
>
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Fri Feb  2 05:06:07 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 22:06:07 -0600
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <001601c7467c$b85d4dc0$0202a8c0@headquarters.silicoinsights>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
	<1170364850.9954.110.camel@localhost.localdomain>
	<007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>
	<1170367211.9954.138.camel@localhost.localdomain>
	<Pine.LNX.4.64.0702012324250.20563@gannet.stats.ox.ac.uk>
	<1170385128.5033.43.camel@localhost.localdomain>
	<001601c7467c$b85d4dc0$0202a8c0@headquarters.silicoinsights>
Message-ID: <1170389167.5033.50.camel@localhost.localdomain>

On Thu, 2007-02-01 at 22:46 -0500, Christos Hatzis wrote:
> Marc,
> 
> I don't think the issue is duplicates in the matching columns.  The data
> were generated by an instrument (NMR spectrometer), processed by the
> instrument's software through an FFT transform and other transformations and
> finally reported as a sequence of chemical shift (x) vs intensity (y) pairs.
> So all x values are unique.  For the example that I reported earlier:
> 
> > length(nmr.spectra.serum[[1]]$V1)
> [1] 32768
> > length(unique(nmr.spectra.serum[[1]]$V1))
> [1] 32768
> > length(nmr.spectra.serum[[2]]$V1)
> [1] 32768
> > length(unique(nmr.spectra.serum[[2]]$V1))
> [1] 32768
> 
> And most of the x-values are common
> > sum(nmr.spectra.serum[[1]]$V1 %in% nmr.spectra.serum[[2]]$V1)
> [1] 32625
> 
> For this reason, merge is probably an overkill for this problem and my
> initial thought was to align the datasets through some simple index-shifting
> operation. 
> 
> Profiling of the merge code in my case shows that most of the time is spent
> on data frame subsetting operations and on internal merge and rbind calls
> secondarily (if I read the summary output correctly).  So even if most of
> the time in the internal merge function is spent on sorting (haven't checked
> the source code), this is in the worst case a rather minor effect, as
> suggested by Prof. Ripley.
>   
> > Rprof("merge.out")
> > zz <- merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], by="V1",
> all=T, sort=T)
> > Rprof(NULL)
> > summaryRprof("merge.out")
> 
> $by.self
>                        self.time self.pct total.time total.pct
> merge.data.frame            6.56     50.0      11.84      90.2
> [.data.frame                2.42     18.4       3.68      28.0
> merge                       1.28      9.8      13.12     100.0
> rbind                       1.24      9.5       1.36      10.4
> names<-.default             1.16      8.8       1.16       8.8
> row.names<-.data.frame      0.12      0.9       0.18       1.4
> duplicated.default          0.12      0.9       0.12       0.9
> make.unique                 0.10      0.8       0.10       0.8
> data.frame                  0.02      0.2       0.04       0.3
> *                           0.02      0.2       0.02       0.2
> is.na                       0.02      0.2       0.02       0.2
> match                       0.02      0.2       0.02       0.2
> order                       0.02      0.2       0.02       0.2
> unclass                     0.02      0.2       0.02       0.2
> [                           0.00      0.0       3.68      28.0
> do.call                     0.00      0.0       1.18       9.0
> names<-                     0.00      0.0       1.16       8.8
> row.names<-                 0.00      0.0       0.18       1.4
> any                         0.00      0.0       0.14       1.1
> duplicated                  0.00      0.0       0.12       0.9
> cbind                       0.00      0.0       0.04       0.3
> as.vector                   0.00      0.0       0.02       0.2
> seq                         0.00      0.0       0.02       0.2
> seq.default                 0.00      0.0       0.02       0.2
> 
> $by.total
>                        total.time total.pct self.time self.pct
> merge                       13.12     100.0      1.28      9.8
> merge.data.frame            11.84      90.2      6.56     50.0
> [.data.frame                 3.68      28.0      2.42     18.4
> [                            3.68      28.0      0.00      0.0
> rbind                        1.36      10.4      1.24      9.5
> do.call                      1.18       9.0      0.00      0.0
> names<-.default              1.16       8.8      1.16      8.8
> names<-                      1.16       8.8      0.00      0.0
> row.names<-.data.frame       0.18       1.4      0.12      0.9
> row.names<-                  0.18       1.4      0.00      0.0
> any                          0.14       1.1      0.00      0.0
> duplicated.default           0.12       0.9      0.12      0.9
> duplicated                   0.12       0.9      0.00      0.0
> make.unique                  0.10       0.8      0.10      0.8
> data.frame                   0.04       0.3      0.02      0.2
> cbind                        0.04       0.3      0.00      0.0
> *                            0.02       0.2      0.02      0.2
> is.na                        0.02       0.2      0.02      0.2
> match                        0.02       0.2      0.02      0.2
> order                        0.02       0.2      0.02      0.2
> unclass                      0.02       0.2      0.02      0.2
> as.vector                    0.02       0.2      0.00      0.0
> seq                          0.02       0.2      0.00      0.0
> seq.default                  0.02       0.2      0.00      0.0
> 
> $sampling.time
> [1] 13.12
> 
> 
> Thanks again for your time in looking into this.
> -Christos

Christos,

Thanks for the follow up.  Thought I had something, but apparently not.

Question: What is the actual structure of the nmr.spectra.serum objects?
The indexing approach that you have suggests they are not simple two
column objects, which may be at least partially the source of the
[.data.frame overhead.

Thanks,

Marc


From christos at nuverabio.com  Fri Feb  2 05:36:47 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 1 Feb 2007 23:36:47 -0500
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <1170389167.5033.50.camel@localhost.localdomain>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
	<1170364850.9954.110.camel@localhost.localdomain>
	<007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>
	<1170367211.9954.138.camel@localhost.localdomain>
	<Pine.LNX.4.64.0702012324250.20563@gannet.stats.ox.ac.uk>
	<1170385128.5033.43.camel@localhost.localdomain>
	<001601c7467c$b85d4dc0$0202a8c0@headquarters.silicoinsights>
	<1170389167.5033.50.camel@localhost.localdomain>
Message-ID: <001801c74683$c0500c00$0202a8c0@headquarters.silicoinsights>

Marc,

The data structure is a list of data frames generated from read.table:

> class(nmr.spectra.serum)
[1] "list"
> class(nmr.spectra.serum[[1]])
[1] "data.frame" 
> dim(nmr.spectra.serum[[1]])
[1] 32768     2

Converting the data.frames to matrices does not have much of an effect on
timing.

-Christos

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Thursday, February 01, 2007 11:06 PM
To: christos at nuverabio.com
Cc: 'Prof Brian Ripley'; r-help at stat.math.ethz.ch
Subject: Re: [R] Lining up x-y datasets based on values of x

On Thu, 2007-02-01 at 22:46 -0500, Christos Hatzis wrote:
> Marc,
> 
> I don't think the issue is duplicates in the matching columns.  The 
> data were generated by an instrument (NMR spectrometer), processed by 
> the instrument's software through an FFT transform and other 
> transformations and finally reported as a sequence of chemical shift (x)
vs intensity (y) pairs.
> So all x values are unique.  For the example that I reported earlier:
> 
> > length(nmr.spectra.serum[[1]]$V1)
> [1] 32768
> > length(unique(nmr.spectra.serum[[1]]$V1))
> [1] 32768
> > length(nmr.spectra.serum[[2]]$V1)
> [1] 32768
> > length(unique(nmr.spectra.serum[[2]]$V1))
> [1] 32768
> 
> And most of the x-values are common
> > sum(nmr.spectra.serum[[1]]$V1 %in% nmr.spectra.serum[[2]]$V1)
> [1] 32625
> 
> For this reason, merge is probably an overkill for this problem and my 
> initial thought was to align the datasets through some simple 
> index-shifting operation.
> 
> Profiling of the merge code in my case shows that most of the time is 
> spent on data frame subsetting operations and on internal merge and 
> rbind calls secondarily (if I read the summary output correctly).  So 
> even if most of the time in the internal merge function is spent on 
> sorting (haven't checked the source code), this is in the worst case a 
> rather minor effect, as suggested by Prof. Ripley.
>   
> > Rprof("merge.out")
> > zz <- merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], by="V1",
> all=T, sort=T)
> > Rprof(NULL)
> > summaryRprof("merge.out")
> 
> $by.self
>                        self.time self.pct total.time total.pct
> merge.data.frame            6.56     50.0      11.84      90.2
> [.data.frame                2.42     18.4       3.68      28.0
> merge                       1.28      9.8      13.12     100.0
> rbind                       1.24      9.5       1.36      10.4
> names<-.default             1.16      8.8       1.16       8.8
> row.names<-.data.frame      0.12      0.9       0.18       1.4
> duplicated.default          0.12      0.9       0.12       0.9
> make.unique                 0.10      0.8       0.10       0.8
> data.frame                  0.02      0.2       0.04       0.3
> *                           0.02      0.2       0.02       0.2
> is.na                       0.02      0.2       0.02       0.2
> match                       0.02      0.2       0.02       0.2
> order                       0.02      0.2       0.02       0.2
> unclass                     0.02      0.2       0.02       0.2
> [                           0.00      0.0       3.68      28.0
> do.call                     0.00      0.0       1.18       9.0
> names<-                     0.00      0.0       1.16       8.8
> row.names<-                 0.00      0.0       0.18       1.4
> any                         0.00      0.0       0.14       1.1
> duplicated                  0.00      0.0       0.12       0.9
> cbind                       0.00      0.0       0.04       0.3
> as.vector                   0.00      0.0       0.02       0.2
> seq                         0.00      0.0       0.02       0.2
> seq.default                 0.00      0.0       0.02       0.2
> 
> $by.total
>                        total.time total.pct self.time self.pct
> merge                       13.12     100.0      1.28      9.8
> merge.data.frame            11.84      90.2      6.56     50.0
> [.data.frame                 3.68      28.0      2.42     18.4
> [                            3.68      28.0      0.00      0.0
> rbind                        1.36      10.4      1.24      9.5
> do.call                      1.18       9.0      0.00      0.0
> names<-.default              1.16       8.8      1.16      8.8
> names<-                      1.16       8.8      0.00      0.0
> row.names<-.data.frame       0.18       1.4      0.12      0.9
> row.names<-                  0.18       1.4      0.00      0.0
> any                          0.14       1.1      0.00      0.0
> duplicated.default           0.12       0.9      0.12      0.9
> duplicated                   0.12       0.9      0.00      0.0
> make.unique                  0.10       0.8      0.10      0.8
> data.frame                   0.04       0.3      0.02      0.2
> cbind                        0.04       0.3      0.00      0.0
> *                            0.02       0.2      0.02      0.2
> is.na                        0.02       0.2      0.02      0.2
> match                        0.02       0.2      0.02      0.2
> order                        0.02       0.2      0.02      0.2
> unclass                      0.02       0.2      0.02      0.2
> as.vector                    0.02       0.2      0.00      0.0
> seq                          0.02       0.2      0.00      0.0
> seq.default                  0.02       0.2      0.00      0.0
> 
> $sampling.time
> [1] 13.12
> 
> 
> Thanks again for your time in looking into this.
> -Christos

Christos,

Thanks for the follow up.  Thought I had something, but apparently not.

Question: What is the actual structure of the nmr.spectra.serum objects?
The indexing approach that you have suggests they are not simple two column
objects, which may be at least partially the source of the [.data.frame
overhead.

Thanks,

Marc


From buckelew at biology.ucsc.edu  Fri Feb  2 06:03:50 2007
From: buckelew at biology.ucsc.edu (Stacey Buckelew)
Date: Thu, 1 Feb 2007 21:03:50 -0800 (PST)
Subject: [R] Regression trees with an ordinal response variable
Message-ID: <3671.150.229.227.99.1170392630.squirrel@mail.acg.ucsc.edu>

Hi,

I am working on a regression tree in Rpart that uses a continuous response
variable that is ordered.  I read a previous response by Pfr. Ripley to a
inquiry regarding the ability of rpart to handle ordinal responses in
2003.  At that time rpart was unable to implement an algorithm to handle
ordinal responses.  Has there been any effort to rectify this in recent
years?

Thanks!

Stacey



On Mon, 2 Jun 2003, Andreas Christmann wrote:
> >>> 1. RE: Ordinal data - Regression Trees & Proportional Odds
> (Liaw, Andy)
>
> > AFAIK there's no implementation (or description) of tree algorithm
> > that handles ordinal response.
> >
>
> Regression trees with an ordinal response variable can be computed with
> SPSS Answer Tree 3.0.
They *can* be handled by tree or rpart in R.
I think Andy's point was that there is no consensus as to the right way to
handle them: certainly using the codes of categories works and may often
be reasonable, and treating ordinal responses as categorical is also very
often perfectly adequate.
Note that rpart is user-extensible, so it would be reasonably easy to write
an extension for a proportional-odds logistic regression model, if that is
thought appropriate (and it seems strange to me to impose such strong
structure on the model with such a general `linear predictor': POLR
models are often in my experience a poor reflection of real problems).
-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Fri Feb  2 06:35:29 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Feb 2007 23:35:29 -0600
Subject: [R] Lining up x-y datasets based on values of x
In-Reply-To: <001801c74683$c0500c00$0202a8c0@headquarters.silicoinsights>
References: <006d01c7463c$4f0a0fb0$0e010a0a@headquarters.silicoinsights>
	<1170361736.9954.96.camel@localhost.localdomain>
	<007101c74641$db68de50$0e010a0a@headquarters.silicoinsights>
	<1170364850.9954.110.camel@localhost.localdomain>
	<007301c7464a$bdf06100$0e010a0a@headquarters.silicoinsights>
	<1170367211.9954.138.camel@localhost.localdomain>
	<Pine.LNX.4.64.0702012324250.20563@gannet.stats.ox.ac.uk>
	<1170385128.5033.43.camel@localhost.localdomain>
	<001601c7467c$b85d4dc0$0202a8c0@headquarters.silicoinsights>
	<1170389167.5033.50.camel@localhost.localdomain>
	<001801c74683$c0500c00$0202a8c0@headquarters.silicoinsights>
Message-ID: <1170394529.5033.93.camel@localhost.localdomain>

Christos,

At least on my system, this does not appear to increase timing:

DF.X <- data.frame(X = 35000:1, Y = runif(35000))
DF.Y <- data.frame(X = 35000:1, Y = runif(35000))

> system.time(DF.XY <- merge(DF.X, DF.Y, by = "X", all = TRUE))
[1] 0.238 0.012 0.256 0.000 0.000


compared to:

DF.list <- list(DF.X, DF.Y)

> str(DF.list)
List of 2
 $ :'data.frame':	35000 obs. of  2 variables:
  ..$ X: int [1:35000] 35000 34999 34998 34997 34996 34995 34994 34993 34992 34991 ...
  ..$ Y: num [1:35000] 0.720 0.855 0.216 0.817 0.534 ...
 $ :'data.frame':	35000 obs. of  2 variables:
  ..$ X: int [1:35000] 35000 34999 34998 34997 34996 34995 34994 34993 34992 34991 ...
  ..$ Y: num [1:35000] 0.68090 0.00694 0.64235 0.15728 0.27436 ...


> system.time(DF.XY.L <- merge(DF.list[[1]], DF.list[[2]], by = "X", all = TRUE))
[1] 0.251 0.005 0.262 0.000 0.000


So I am still confuzzled as to why it is taking 13 seconds on your
system.  I am missing something here.

However, I did note that using merge.zoo() appears to be helpful.

Regards,

Marc

On Thu, 2007-02-01 at 23:36 -0500, Christos Hatzis wrote:
> Marc,
> 
> The data structure is a list of data frames generated from read.table:
> 
> > class(nmr.spectra.serum)
> [1] "list"
> > class(nmr.spectra.serum[[1]])
> [1] "data.frame" 
> > dim(nmr.spectra.serum[[1]])
> [1] 32768     2
> 
> Converting the data.frames to matrices does not have much of an effect on
> timing.
> 
> -Christos
> 
> -----Original Message-----
> From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
> Sent: Thursday, February 01, 2007 11:06 PM
> To: christos at nuverabio.com
> Cc: 'Prof Brian Ripley'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Lining up x-y datasets based on values of x
> 
> On Thu, 2007-02-01 at 22:46 -0500, Christos Hatzis wrote:
> > Marc,
> > 
> > I don't think the issue is duplicates in the matching columns.  The 
> > data were generated by an instrument (NMR spectrometer), processed by 
> > the instrument's software through an FFT transform and other 
> > transformations and finally reported as a sequence of chemical shift (x)
> vs intensity (y) pairs.
> > So all x values are unique.  For the example that I reported earlier:
> > 
> > > length(nmr.spectra.serum[[1]]$V1)
> > [1] 32768
> > > length(unique(nmr.spectra.serum[[1]]$V1))
> > [1] 32768
> > > length(nmr.spectra.serum[[2]]$V1)
> > [1] 32768
> > > length(unique(nmr.spectra.serum[[2]]$V1))
> > [1] 32768
> > 
> > And most of the x-values are common
> > > sum(nmr.spectra.serum[[1]]$V1 %in% nmr.spectra.serum[[2]]$V1)
> > [1] 32625
> > 
> > For this reason, merge is probably an overkill for this problem and my 
> > initial thought was to align the datasets through some simple 
> > index-shifting operation.
> > 
> > Profiling of the merge code in my case shows that most of the time is 
> > spent on data frame subsetting operations and on internal merge and 
> > rbind calls secondarily (if I read the summary output correctly).  So 
> > even if most of the time in the internal merge function is spent on 
> > sorting (haven't checked the source code), this is in the worst case a 
> > rather minor effect, as suggested by Prof. Ripley.
> >   
> > > Rprof("merge.out")
> > > zz <- merge(nmr.spectra.serum[[1]], nmr.spectra.serum[[2]], by="V1",
> > all=T, sort=T)
> > > Rprof(NULL)
> > > summaryRprof("merge.out")
> > 

<snip>


From upsattar at yahoo.com  Fri Feb  2 06:43:09 2007
From: upsattar at yahoo.com (Abdus Sattar)
Date: Thu, 1 Feb 2007 21:43:09 -0800 (PST)
Subject: [R] Fitting Weighted Estimating Equations
Message-ID: <855169.25206.qm@web58101.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/a9a4c277/attachment.pl 

From wangtong at usc.edu  Fri Feb  2 08:38:19 2007
From: wangtong at usc.edu (Tong Wang)
Date: Thu, 01 Feb 2007 23:38:19 -0800
Subject: [R] A question about dput
Message-ID: <dde9f68ec96.45c279eb@usc.edu>

Hi, 
   I am trying to output a R data set for use in WinBugs,  I used
         dput(list(x=rnorm(100),N=100),file="bug.dat") 
But I can't get the intended format:    list(x=c(.......),N=100), instead, I got 
something like this (copied the first two lines):

[00000000]???73?74?72?75??63?74?75?72??65?28?6C?69??73?74?28?78????    structure(list(x
[00000010]???20?3D?20?63??28?2D?30?2E??33?36?33?31??36?31?30?33    ?????=?c(-0.36316103

Did I do something wrong here ?  Thanks a lot for any help

tong


From nilsson.henric at gmail.com  Fri Feb  2 08:51:54 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Fri, 2 Feb 2007 08:51:54 +0100 (CET)
Subject: [R] Fitting Weighted Estimating Equations
In-Reply-To: <855169.25206.qm@web58101.mail.re3.yahoo.com>
References: <855169.25206.qm@web58101.mail.re3.yahoo.com>
Message-ID: <11202.212.209.13.15.1170402714.squirrel@www.sorch.se>

Den Fr, 2007-02-02, 06:43 skrev Abdus Sattar:
> Hello Everybody:
>
> I am searching for an R package for fitting Generalized Estimating
> Equations (GEE) with weights (i.e. Weighted Estimating Equations). From
> the R documentation I found "geese(geepack)" for fitting Generalized
> Estimating Equations. In this documentation, under the paragraph ?weights?
> it has been written, ?an optional vector of weights to be used in the
> fitting process. The length of weights should be the same as the number of
> observations. This weights is not (yet) the weight as in sas proc genmod,
> and hence is not recommended to use.? Now my question is, is there any
> other package you know that might allow me to fit GEE with weight option
> and the results would be same as SAS PROC GENMOD with weights? Any help

The `yags' function in the `yags' package (available from Prof Ripley's
CRAN extras) has got a `weights' argument, though I've never used it and
I've got no idea whether it corresponds to GENMOD's weights statement.


HTH,
Henric



> would be sincerely appreciated.
>
> Thank you very much.
>
> Sincerely,
>
> Abdus Sattar
> University of Pittsburgh
> Email: mas196 at pitt.edu
>
>
>
> ____________________________________________________________________________________
> It's here! Your new message!
> Get new email alerts with the free Yahoo! Toolbar.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Feb  2 09:27:47 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Feb 2007 03:27:47 -0500
Subject: [R] A question about dput
In-Reply-To: <dde9f68ec96.45c279eb@usc.edu>
References: <dde9f68ec96.45c279eb@usc.edu>
Message-ID: <971536df0702020027q7e5a8533q2e4b2282b6e081ae@mail.gmail.com>

Try control=NULL :

>  dput(list(x=rnorm(3),N=3), control = NULL)
list(x = c(-0.254393363810571, -0.650328028909466, -1.20888767858120
), N = 3)

On 2/2/07, Tong Wang <wangtong at usc.edu> wrote:
> Hi,
>   I am trying to output a R data set for use in WinBugs,  I used
>         dput(list(x=rnorm(100),N=100),file="bug.dat")
> But I can't get the intended format:    list(x=c(.......),N=100), instead, I got
> something like this (copied the first two lines):
>
> [00000000]???73?74?72?75??63?74?75?72??65?28?6C?69??73?74?28?78????    structure(list(x
> [00000010]???20?3D?20?63??28?2D?30?2E??33?36?33?31??36?31?30?33    ?????=?c(-0.36316103
>
> Did I do something wrong here ?  Thanks a lot for any help
>
> tong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From nilsson.henric at gmail.com  Fri Feb  2 09:30:33 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Fri, 2 Feb 2007 09:30:33 +0100 (CET)
Subject: [R] Regression trees with an ordinal response variable
In-Reply-To: <3671.150.229.227.99.1170392630.squirrel@mail.acg.ucsc.edu>
References: <3671.150.229.227.99.1170392630.squirrel@mail.acg.ucsc.edu>
Message-ID: <34208.212.209.13.15.1170405033.squirrel@www.sorch.se>

Den Fr, 2007-02-02, 06:03 skrev Stacey Buckelew:
> Hi,
>
> I am working on a regression tree in Rpart that uses a continuous response
> variable that is ordered.  I read a previous response by Pfr. Ripley to a
> inquiry regarding the ability of rpart to handle ordinal responses in
> 2003.  At that time rpart was unable to implement an algorithm to handle
> ordinal responses.  Has there been any effort to rectify this in recent
> years?

The `ctree' function in the `party' package is able to handle ordered
responses, but note that there are fundamental differences between the
former and `rpart'. Reading the package vignette and the relevant
references will help.

However, at the moment there seems to be a problem related to the ordinal
case (predicted probabilities > 1) and I've CC:ed the package's maintainer
(Torsten Hothorn).


HTH,
Henric

- - - - -

Torsten, consider the following:

> ### ordinal regression
> mammoct <- ctree(ME ~ ., data = mammoexp)
Warning message:
no admissible split found
> ### estimated class probabilities
> treeresponse(mammoct, newdata = mammoexp[1:5, ])
[[1]]
[1] 1.822115

[[2]]
[1] 1.265487

[[3]]
[1] 1.822115

[[4]]
[1] 1.560440

[[5]]
[1] 1.822115

> sessionInfo()
R version 2.4.1 Patched (2007-01-06 r40399)
i386-pc-mingw32

locale:
LC_COLLATE=Swedish_Sweden.1252;LC_CTYPE=Swedish_Sweden.1252;LC_MONETARY=Swedish_Sweden.1252;LC_NUMERIC=C;LC_TIME=Swedish_Sweden.1252

attached base packages:
 [1] "stats4"    "grid"      "splines"   "stats"     "graphics"  "grDevices"
 [7] "utils"     "datasets"  "methods"   "base"

other attached packages:
      party         vcd  colorspace        MASS strucchange    sandwich
    "0.9-8"     "1.0-2"      "0.95"    "7.2-31"     "1.3-1"     "2.0-1"
        zoo        coin     mvtnorm  modeltools    survival
    "1.2-2"     "0.5-2"     "0.7-5"    "0.2-10"      "2.30"
>






>
> Thanks!
>
> Stacey
>
>
>
> On Mon, 2 Jun 2003, Andreas Christmann wrote:
>> >>> 1. RE: Ordinal data - Regression Trees & Proportional Odds
>> (Liaw, Andy)
>>
>> > AFAIK there's no implementation (or description) of tree algorithm
>> > that handles ordinal response.
>> >
>>
>> Regression trees with an ordinal response variable can be computed with
>> SPSS Answer Tree 3.0.
> They *can* be handled by tree or rpart in R.
> I think Andy's point was that there is no consensus as to the right way to
> handle them: certainly using the codes of categories works and may often
> be reasonable, and treating ordinal responses as categorical is also very
> often perfectly adequate.
> Note that rpart is user-extensible, so it would be reasonably easy to
> write
> an extension for a proportional-odds logistic regression model, if that is
> thought appropriate (and it seems strange to me to impose such strong
> structure on the model with such a general `linear predictor': POLR
> models are often in my experience a poor reflection of real problems).
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272860 (secr)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From nilsson.henric at gmail.com  Fri Feb  2 09:34:48 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Fri, 2 Feb 2007 09:34:48 +0100 (CET)
Subject: [R] Autocorrelated Binomial
In-Reply-To: <1170365904.6015.35.camel@localhost.localdomain>
References: <1170365904.6015.35.camel@localhost.localdomain>
Message-ID: <13430.212.209.13.15.1170405288.squirrel@www.sorch.se>

Den To, 2007-02-01, 22:38 skrev Rick Bilonick:
> I need to generate autocorrelated binary data. I've found references to
> the IEKS package but none of the web pages currently exist. Does anyone
> know where I can find this package or suggest another package?

The `bindata' package can generate correlated binary data.


HTH,
Henric



>
> Rick B.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Feb  2 09:45:59 2007
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 2 Feb 2007 09:45:59 +0100 (CET)
Subject: [R] Regression trees with an ordinal response variable
In-Reply-To: <34208.212.209.13.15.1170405033.squirrel@www.sorch.se>
References: <3671.150.229.227.99.1170392630.squirrel@mail.acg.ucsc.edu>
	<34208.212.209.13.15.1170405033.squirrel@www.sorch.se>
Message-ID: <Pine.LNX.4.64.0702020944170.23380@imbe153.imbe.med.uni-erlangen.de>


On Fri, 2 Feb 2007, Henric Nilsson (Public) wrote:

> Den Fr, 2007-02-02, 06:03 skrev Stacey Buckelew:
>> Hi,
>>
>> I am working on a regression tree in Rpart that uses a continuous response
>> variable that is ordered.  I read a previous response by Pfr. Ripley to a
>> inquiry regarding the ability of rpart to handle ordinal responses in
>> 2003.  At that time rpart was unable to implement an algorithm to handle
>> ordinal responses.  Has there been any effort to rectify this in recent
>> years?
>
> The `ctree' function in the `party' package is able to handle ordered
> responses, but note that there are fundamental differences between the
> former and `rpart'. Reading the package vignette and the relevant
> references will help.
>

Hi Henric,

> However, at the moment there seems to be a problem related to the ordinal
> case (predicted probabilities > 1) and I've CC:ed the package's maintainer
> (Torsten Hothorn).
>

yep, you are right (as always)-- a bug introduced by a fix, grrr. Its a 
little bit more complicated, but I'll make correct predictions available 
again *asap*.

Thanks!

Torsten

>
> HTH,
> Henric
>
> - - - - -
>
> Torsten, consider the following:
>
>> ### ordinal regression
>> mammoct <- ctree(ME ~ ., data = mammoexp)
> Warning message:
> no admissible split found
>> ### estimated class probabilities
>> treeresponse(mammoct, newdata = mammoexp[1:5, ])
> [[1]]
> [1] 1.822115
>
> [[2]]
> [1] 1.265487
>
> [[3]]
> [1] 1.822115
>
> [[4]]
> [1] 1.560440
>
> [[5]]
> [1] 1.822115
>
>> sessionInfo()
> R version 2.4.1 Patched (2007-01-06 r40399)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=Swedish_Sweden.1252;LC_CTYPE=Swedish_Sweden.1252;LC_MONETARY=Swedish_Sweden.1252;LC_NUMERIC=C;LC_TIME=Swedish_Sweden.1252
>
> attached base packages:
> [1] "stats4"    "grid"      "splines"   "stats"     "graphics"  "grDevices"
> [7] "utils"     "datasets"  "methods"   "base"
>
> other attached packages:
>      party         vcd  colorspace        MASS strucchange    sandwich
>    "0.9-8"     "1.0-2"      "0.95"    "7.2-31"     "1.3-1"     "2.0-1"
>        zoo        coin     mvtnorm  modeltools    survival
>    "1.2-2"     "0.5-2"     "0.7-5"    "0.2-10"      "2.30"
>>
>
>
>
>
>
>
>>
>> Thanks!
>>
>> Stacey
>>
>>
>>
>> On Mon, 2 Jun 2003, Andreas Christmann wrote:
>>>>>> 1. RE: Ordinal data - Regression Trees & Proportional Odds
>>> (Liaw, Andy)
>>>
>>>> AFAIK there's no implementation (or description) of tree algorithm
>>>> that handles ordinal response.
>>>>
>>>
>>> Regression trees with an ordinal response variable can be computed with
>>> SPSS Answer Tree 3.0.
>> They *can* be handled by tree or rpart in R.
>> I think Andy's point was that there is no consensus as to the right way to
>> handle them: certainly using the codes of categories works and may often
>> be reasonable, and treating ordinal responses as categorical is also very
>> often perfectly adequate.
>> Note that rpart is user-extensible, so it would be reasonably easy to
>> write
>> an extension for a proportional-odds logistic regression model, if that is
>> thought appropriate (and it seems strange to me to impose such strong
>> structure on the model with such a general `linear predictor': POLR
>> models are often in my experience a poor reflection of real problems).
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272860 (secr)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>


From ripley at stats.ox.ac.uk  Fri Feb  2 10:05:01 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 09:05:01 +0000 (GMT)
Subject: [R] Affymetrix data analysis
In-Reply-To: <2E17292A64E6ED418A60BE89326B1AAB3225E3@msgebe11.mfad.mfroot.org>
References: <000301c74652$db16b6f0$b8e810ac@KateTristan>
	<696C6E48-0EDB-416C-B840-A03390F1781E@jhsph.edu>
	<2E17292A64E6ED418A60BE89326B1AAB3225E3@msgebe11.mfad.mfroot.org>
Message-ID: <Pine.LNX.4.64.0702020900540.19324@gannet.stats.ox.ac.uk>

On Thu, 1 Feb 2007, Sicotte, Hugues   Ph.D. wrote:

> Tristan,
> I have a soft spot for problems analyzing microarrays with R..
>
> for the memory issue, there have been previous posts to this list..
> But here is the answer I gave a few weeks ago.
> If you need more memory, you have to move to linux or recompile R for
> windows yourself..
> .. But you'll still need a computer with more memory.
> The long term solution, which we are implementing, is to rewrite the
> normalization code so it doesn't
> Need to load all those arrays at once.
>
> -- cut previous part of message--
> The defaults in R is to play nice and limit your allocation to half
> the available RAM. Make sure you have a lot of disk swap space (at least
> 1G with 2G of RAM) and you can set your memory limit to 2G for R.

That just isn't true (R uses as much of the RAM as is reasonable, all for 
up to 1.5Gb installed).  Please consult the rw-FAQ for the whole truth.

> See help(memory.size)  and use the memory.limit function

[Please follow the advice you quote.]

> Hugues
>
>
> P.s. Someone let me use their 16Gig of RAM linux
> And I was able to run R-64 bits with "top" showing 6Gigs of RAM
> allocated (with suitable --max-mem-size command line parameters at
> startup for R).

There is no such 'command' for R under Linux.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Feb  2 10:12:29 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 09:12:29 +0000 (GMT)
Subject: [R] features of save and save.image (unexpected file sizes)
In-Reply-To: <e47808320702010530y1044427dw2815dda86a29ba3f@mail.gmail.com>
References: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com>
	<45C0DA8C.2020902@stats.ox.ac.uk>
	<e47808320701312334y5f181f15ic8fb952b756a0461@mail.gmail.com>
	<Pine.LNX.4.64.0702010747090.9746@gannet.stats.ox.ac.uk>
	<e47808320702010530y1044427dw2815dda86a29ba3f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702020906500.19324@gannet.stats.ox.ac.uk>

On Thu, 1 Feb 2007, Vaidotas Zemlys wrote:

> Hi,
>
>
> On 2/1/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>> On Thu, 1 Feb 2007, Vaidotas Zemlys wrote:
>>
>>> Hi,
>>>
>>> On 1/31/07, Professor Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>>> Two comments:
>>>>
>>>> 1) ls() does not list all the objects: it has all.names argument.
>>>>
>>> Yes, I tried it with all.names, but the effect was the same, I forgot
>>> to mention it in a letter.
>>>
>>>> 2) save.image() does not just save the objects in the workspace, it also
>>>> saves any environments they may have.  Having a function with a
>>>> large environment is the usual cause of a large saved image.
>>>
>>> I have little experience dealing with enivronments, so is there a
>>> quick way to discard the environments of the functions? When saving
>>> the session I really do not need them.
>>
>> Change, not discard.  E.g. environment(f) <- .GlobalEnv.  If environments
>> are not mentioned by anything saved, they will not be saved.
>>
>
> I found the culprit. I was parsing formulas in my code, and I saved
> them in that large object. So the environment came with saved
> formulas. Is there a nice way to say R: "please do not save the
> environments with the formulas, I do not need them?"

No, but why create them that way?  You could do

mmodel <- as.formula(mmodel, env=.GlobalEnv)

The R way is to create what you want, not fix up afterwards.

(I find your code unreadable--spaces help a great deal, so am not sure if 
I have understood it correctly.)

>
> This is what I was doing (I am discarding irrelevant code)
>
> testf<- function(formula) {
>   mainform <- formula
>   if(deparse(mainform[[3]][[1]])!="|") pandterm("invalid conditioning
> for main regression")
>    mmodel <- substitute(y~x,list(y=mainform[[2]],x=mainform[[3]][[2]]))
>    mmodel <- as.formula(mmodel)
>   list(formula=list(main=mmodel))
> }
>
> when called
> bu <- testf(lnp~I(CE/12000)+hhs|Country)
>
> I get
>
> ls(env=environment(bu$formula$main))
> [1] "formula"  "mainform" "mmodel"
>
> or in actual case, a lot of more objects, which I do not need, but
> which take a lot of place. For the moment I solved the problem with
>
> environment(mmodel) <- NULL
>
> but is this correct R way?
>
> Vaidotas Zemlys
> --
> Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
> Vilnius University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From benoit at ebi.ac.uk  Fri Feb  2 10:31:15 2007
From: benoit at ebi.ac.uk (Benoit Ballester)
Date: Fri, 02 Feb 2007 09:31:15 +0000
Subject: [R] R for bioinformatics
In-Reply-To: <1170362983.9954.103.camel@localhost.localdomain>
References: <45C0C4D2.9080908@ebi.ac.uk> <m2hcu5zphf.fsf@fhcrc.org>	
	<1170357417.9954.73.camel@localhost.localdomain>	
	<45C24E54.1080904@biostat.ku.dk>
	<1170362983.9954.103.camel@localhost.localdomain>
Message-ID: <45C304E3.70804@ebi.ac.uk>

Marc Schwartz wrote:
> On Thu, 2007-02-01 at 21:32 +0100, Peter Dalgaard wrote:
>> Marc Schwartz wrote:
>>> On Thu, 2007-02-01 at 10:45 -0800, Seth Falcon wrote:
>>>   
>>>> Benoit Ballester <benoit at ebi.ac.uk> writes:
>>>>
>>>>     
>>>>> Hi,
>>>>>
>>>>> I was wondering if someone could tell me more about this book, (if it's 
>>>>> a good or bad one).
>>>>> I can't find it, as it seems that O'Reilly doesn't publish any more.
>>>>>       
>>>> I've never seen a copy so I can't comment about its quality (has
>>>> anyone seen a copy?).
>>>>
>>>> You might want to take a look at _Bioinformatics and Computational
>>>> Biology Solutions Using R and Bioconductor_.
>>>>
>>>> http://www.bioconductor.org/pub/docs/mogr/
>>>>     
>>> I'll stand (or sit) to be corrected on this as I cannot find the source,
>>> but I have a recollection from seeing something quite some time ago that
>>> the book may have never been published.
>>>   
>> It's been a while since the status was something along the lines that 
>> "the authors may or may not complete it". Subject matter moving faster 
>> than pen, I suspect....
> 
> Peter, that wording does seem familiar, just cannot recall where I saw
> it. Perhaps on the O'Reilly web site, where it is no longer listed.
> 
> For confirmation, I called O'Reilly's customer service in Cambridge, MA.
> They confirm that the book was indeed cancelled and never published.
> 
> No reasons were given.

Thanks for those replies.
I did also contacted the O'reilly offices in UK, and they told me the 
same thing.  The book was never published. I just wanted to compare the 
"R for bioinformatics" with the "Bioinformatics and Computational 
Biology Solutions Using R and Bioconductor", and see which one suit me 
more - But guess I don't have the choice now :-)

Ben

-- 
Benoit Ballester
Ensembl Team


From Bernhard_Pfaff at fra.invesco.com  Fri Feb  2 10:41:30 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Fri, 2 Feb 2007 09:41:30 -0000
Subject: [R] time series analysis
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05DE1FAC@hercules.ssainfo>
References: <BAY113-F13DE1113C78074C2D8565499A40@phx.gbl>
	<CA612484A337C6479EA341DF9EEE14AC05DE1FAC@hercules.ssainfo>
Message-ID: <E4A9111DA23BA048B9A46686BF727CF461BF1A@DEFRAXMB01.corp.amvescap.net>

Hello John,

as a starting point you might also want to have a look at:

@book{BOOK,
author={Robert S Pindyck and Daniel L Rubinfeld},
title={Econometric Models and Economic Forecasts},
year={1997},
publisher={McGraw-Hill/Irwin},
isbn={0079132928}
}

The monographies of Hamilton and L?tkepohl might then be taken into focus.

Best,
Bernhard 

>John --
>
>Well, as a start, have a look at "Modern Applied Statistics with S," by
>Venables and Ripley, both of which names you will recognize if you read
>this list often.  There is a 30-page chapter on time series (with
>suggestions for other readings), obviously geared to S and R, that is a
>good jumping-off place.
>
>Ben Fairbank
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of lamack lamack
>Sent: Thursday, February 01, 2007 3:12 PM
>To: R-help at stat.math.ethz.ch
>Subject: [R] time series analysis
>
>Does anyone know a good introductory book or tutorial about 
>time series 
>analysis? (time
>series for a beginner).
>
>Thank you so much.
>
>John Lamak
>
>_________________________________________________________________
>Descubra como mandar Torpedos SMS do seu Messenger para o celular dos
>seus 
>amigos. http://mobile.msn.com/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From jgarcia at ija.csic.es  Fri Feb  2 11:09:06 2007
From: jgarcia at ija.csic.es (javier garcia-pintado)
Date: Fri, 02 Feb 2007 11:09:06 +0100
Subject: [R] indexing
In-Reply-To: <45C218EC.3060508@acm.org>
References: <45C1C95C.8090605@ija.csic.es> <45C218EC.3060508@acm.org>
Message-ID: <45C30DC2.70609@ija.csic.es>

Thanks a lot
match() function does the task (of course this a a simple example of my
long datasets)
Just for the record, merge() function does not seem to work properly for
this (even with the sort=FALSE argument) as the order of the results
does not match the corresponding order of classes in the x vector.

Wishes,
Javier
-------------------------------------------------------

Tony Plate wrote:
> > a <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
> > x <- c(1,1,2,7,6,5,4,3,2,2,2)
> > match(x, a$class)
>  [1]  1  1  4 NA NA  3 NA  2  4  4  4
> > a[match(x, a$class), "value"]
>  [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0
> >
>
> -- Tony Plate
>
> javier garcia-pintado wrote:
>> Hello,
>> In a nutshell, I've got a data.frame like this:
>>
>>
>>> assignation <- data.frame(value=c(6.5,7.5,8.5,12.0),class=c(1,3,5,2))
>>> assignation
>>
>>   value class
>> 1   6.5     1
>> 2   7.5     3
>> 3   8.5     5
>> 4  12.0     2
>>
>>>  
>>
>>
>> and a long vector of classes like this:
>>
>>
>>> x <- c(1,1,2,7,6,5,4,3,2,2,2...)
>>
>>
>> And would like to obtain  a vector of length = length(x), with the
>> corresponding values extracted from assignation table. Like this:
>>
>>> x.value
>>
>>  [1]  6.5  6.5 12.0   NA   NA  8.5   NA  7.5 12.0 12.0 12.0
>>
>> Could you help me with an elegant way to do this ?
>> (I just can do it with looping for each class in the assignation table,
>> what a think is not perfect in R's sense)
>>
>> Wishes,
>> Javier
>>
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Javier Garc?a-Pintado
Institute of Earth Sciences Jaume Almera (CSIC)
Lluis Sole Sabaris s/n, 08028 Barcelona
Phone: +34 934095410
Fax:   +34 934110012
e-mail:jgarcia at ija.csic.es 


From f.falciano at cineca.it  Fri Feb  2 11:55:06 2007
From: f.falciano at cineca.it (Francesco Falciano)
Date: Fri,  2 Feb 2007 11:55:06 +0100 (MET)
Subject: [R] Problem installing R-2.4.1 on AIX 5.3
Message-ID: <003101c746b8$99967650$c840a8c0@int.cineca.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070202/b61d889e/attachment.pl 

From Sicotte.Hugues at mayo.edu  Fri Feb  2 12:03:43 2007
From: Sicotte.Hugues at mayo.edu (Sicotte, Hugues   Ph.D.)
Date: Fri, 2 Feb 2007 05:03:43 -0600
Subject: [R] Affymetrix data analysis
References: <Pine.LNX.4.64.0702020900540.19324@gannet.stats.ox.ac.uk>
Message-ID: <2E17292A64E6ED418A60BE89326B1AAB3225E5@msgebe11.mfad.mfroot.org>

Of course, you would know best, so can you tell us if the help pages I
pull using

help(Memory)

is wrong?
That help page says (2nd paragraph)

"(On Windows the --max-mem-size option sets the maximum memory
allocation: it has a minimum allowed value of 16M. This is intended to
catch attempts to allocate excessive amounts of memory which may cause
other processes to run out of resources. The default is the smaller of
the amount of physical RAM in the machine and 1024Mb. See also
memory.limit.) "



Hugues

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Friday, February 02, 2007 3:05 AM
To: Sicotte, Hugues Ph.D.
Cc: Tristan Coram; R-help at stat.math.ethz.ch
Subject: Re: [R] Affymetrix data analysis

On Thu, 1 Feb 2007, Sicotte, Hugues   Ph.D. wrote:

> Tristan,
> I have a soft spot for problems analyzing microarrays with R..
>
> for the memory issue, there have been previous posts to this list..
> But here is the answer I gave a few weeks ago.
> If you need more memory, you have to move to linux or recompile R for
> windows yourself..
> .. But you'll still need a computer with more memory.
> The long term solution, which we are implementing, is to rewrite the
> normalization code so it doesn't
> Need to load all those arrays at once.
>
> -- cut previous part of message--
> The defaults in R is to play nice and limit your allocation to half
> the available RAM. Make sure you have a lot of disk swap space (at
least
> 1G with 2G of RAM) and you can set your memory limit to 2G for R.

That just isn't true (R uses as much of the RAM as is reasonable, all
for 
up to 1.5Gb installed).  Please consult the rw-FAQ for the whole truth.

> See help(memory.size)  and use the memory.limit function

[Please follow the advice you quote.]

> Hugues
>
>
> P.s. Someone let me use their 16Gig of RAM linux
> And I was able to run R-64 bits with "top" showing 6Gigs of RAM
> allocated (with suitable --max-mem-size command line parameters at
> startup for R).

There is no such 'command' for R under Linux.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Feb  2 12:37:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 11:37:07 +0000 (GMT)
Subject: [R] Affymetrix data analysis
In-Reply-To: <2E17292A64E6ED418A60BE89326B1AAB3225E5@msgebe11.mfad.mfroot.org>
References: <Pine.LNX.4.64.0702020900540.19324@gannet.stats.ox.ac.uk>
	<2E17292A64E6ED418A60BE89326B1AAB3225E5@msgebe11.mfad.mfroot.org>
Message-ID: <Pine.LNX.4.64.0702021128140.24105@gannet.stats.ox.ac.uk>

On Fri, 2 Feb 2007, Sicotte, Hugues   Ph.D. wrote:

> Of course, you would know best, so can you tell us if the help pages I
> pull using
>
> help(Memory)
>
> is wrong?
> That help page says (2nd paragraph)
>
> "(On Windows the --max-mem-size option sets the maximum memory
> allocation: it has a minimum allowed value of 16M. This is intended to
> catch attempts to allocate excessive amounts of memory which may cause
> other processes to run out of resources. The default is the smaller of
> the amount of physical RAM in the machine and 1024Mb. See also
> memory.limit.) "

It says nothing about 'half' does it?

Depending on your version of R and Windows, the default is 1Gb, 1.5Gb or 
2.5Gb, and the rw-FAQ gives the whole truth. The current version of that 
help page is different:

https://svn.r-project.org/R/trunk/src/library/base/man/Memory.Rd

it looks like in 2.4.1 it had not been updated yet.

>
>
> Hugues
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Friday, February 02, 2007 3:05 AM
> To: Sicotte, Hugues Ph.D.
> Cc: Tristan Coram; R-help at stat.math.ethz.ch
> Subject: Re: [R] Affymetrix data analysis
>
> On Thu, 1 Feb 2007, Sicotte, Hugues   Ph.D. wrote:
>
>> Tristan,
>> I have a soft spot for problems analyzing microarrays with R..
>>
>> for the memory issue, there have been previous posts to this list..
>> But here is the answer I gave a few weeks ago.
>> If you need more memory, you have to move to linux or recompile R for
>> windows yourself..
>> .. But you'll still need a computer with more memory.
>> The long term solution, which we are implementing, is to rewrite the
>> normalization code so it doesn't
>> Need to load all those arrays at once.
>>
>> -- cut previous part of message--
>> The defaults in R is to play nice and limit your allocation to half
>> the available RAM. Make sure you have a lot of disk swap space (at
> least
>> 1G with 2G of RAM) and you can set your memory limit to 2G for R.
>
> That just isn't true (R uses as much of the RAM as is reasonable, all
> for
> up to 1.5Gb installed).  Please consult the rw-FAQ for the whole truth.
>
>> See help(memory.size)  and use the memory.limit function
>
> [Please follow the advice you quote.]
>
>> Hugues
>>
>>
>> P.s. Someone let me use their 16Gig of RAM linux
>> And I was able to run R-64 bits with "top" showing 6Gigs of RAM
>> allocated (with suitable --max-mem-size command line parameters at
>> startup for R).
>
> There is no such 'command' for R under Linux.
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Feb  2 12:37:50 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 11:37:50 +0000 (GMT)
Subject: [R] Problem installing R-2.4.1 on AIX 5.3
In-Reply-To: <003101c746b8$99967650$c840a8c0@int.cineca.it>
References: <003101c746b8$99967650$c840a8c0@int.cineca.it>
Message-ID: <Pine.LNX.4.64.0702021137270.24105@gannet.stats.ox.ac.uk>

Please check the archives: these problems are known, and await help from 
an AIX expert.

On Fri, 2 Feb 2007, Francesco Falciano wrote:

> Dear all,
> I have some problems to install R-2.4.1 on AIX 5.3.
>
> Configure string:
>
> ./configure  --with-readline=no LDFLAGS='-bshared' --with-jpeglib=no --with-libpng=no --with-lapack=no --prefix=/cineca/prod/Bioinf/R-2.4.1
>
> configure .site=
> #! /bin/sh
> CC=xlc
> F77=xlf
> MAIN_LDFLAGS=-Wl,-brtl
> SHLIB_LDFLAGS=-Wl,-G
> CXX=xlc
> CXXFLAGS=' -g -O'
> SHLIB_LDFLAGS=-W1, -G
> MAKE=gmake
>
> Configure ends successfully, but the make fails:
>
> ---cut
> de -DHAVE_CONFIG_H -I/usr/local/include      -g -c init.c -o init.o
>        xlc -qlanglvl=extc99 -Wl,-G -Wl,-G -Wl,-bexpall -Wl,-bnoentry -bshared -o grDevices.so chull.o devNull.o devPicTeX.o devPS.o devQuartz.o init.o  -lm
> Error in solve.default(rgb) : lapack routines cannot be loaded
> In addition: Warning message:
> unable to load shared library '/cineca/prod/Bioinf/R-2.4.1/modules//lapack.so':
>  rtld: 0712-001 Symbol _xldipow/cineca/prod/Bioinf/R-2.4.1/lib/libRlapack.so was referenced
>      from module /cineca/prod/Bioinf/R-2.4.1/lib/libRlapack.so(), but a runtime definition
>      of the symbol was not found.
> rtld: 0712-001 Symbol _log was referenced
>      from module /cineca/prod/Bioinf/R-2.4.1/lib/libRlapack.so(), but a runtime definition
>      of the symbol was not found.
> rtld: 0712-001 Symbol _sqrt was referenced
>      from module /cineca/prod/Bioinf/R-2.4.1/lib/libRlapack.so(), but a runtime definition
>      of the symbol was not found.
> rtld: 0712-001 Symbol idamax was referenced
>      from module /cineca/prod/Bioinf/R-2.4.1/lib/libRlapack.so(), but a runtime definition
>      of the symbol was not found.
> rtld: 0712-001 Symbol dger was referenced
>      from module /cineca/prod/Bioinf/R-2.4.1/lib/libRlapack.so(), but a runtime definition
>      of the symbol was not found.
> rtld: 0712-001 Symbo
> Error: unable to load R code in package 'grDevices'
> Execution halted
> make: 1254-004 The error code from the last command is 1.
>
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
>
> Stop.
> ---------------------------------
>
> Can you help me?
>
> Thank you in advance.
> FF
>
> -----------------------------------------------------------
> Dr Francesco Falciano
> CINECA
> (High Performance Systems)
> via Magnanelli, 6/3
> 40033 Casalecchio di Reno (BO)-ITALY
> tel: +39-051-6171724
> fax: +39-051-6132198
> e-mail: f.falciano at cineca.it
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mpiktas at gmail.com  Fri Feb  2 13:33:01 2007
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Fri, 2 Feb 2007 14:33:01 +0200
Subject: [R] features of save and save.image (unexpected file sizes)
In-Reply-To: <Pine.LNX.4.64.0702020906500.19324@gannet.stats.ox.ac.uk>
References: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com>
	<45C0DA8C.2020902@stats.ox.ac.uk>
	<e47808320701312334y5f181f15ic8fb952b756a0461@mail.gmail.com>
	<Pine.LNX.4.64.0702010747090.9746@gannet.stats.ox.ac.uk>
	<e47808320702010530y1044427dw2815dda86a29ba3f@mail.gmail.com>
	<Pine.LNX.4.64.0702020906500.19324@gannet.stats.ox.ac.uk>
Message-ID: <e47808320702020433k4781ae2ci3bdcc7abc69d066c@mail.gmail.com>

Hi,

On 2/2/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> > I found the culprit. I was parsing formulas in my code, and I saved
> > them in that large object. So the environment came with saved
> > formulas. Is there a nice way to say R: "please do not save the
> > environments with the formulas, I do not need them?"
>
> No, but why create them that way?  You could do
>
> mmodel <- as.formula(mmodel, env=.GlobalEnv)
>
Hm, but say I have some large object in .GlobalEnv, and I generate
mmodel  10 different times and save the result as a list with length
10. Now if I try to save this list, R will save 10 different copies of
.GlobalEnv together with aforementioned large object?

> The R way is to create what you want, not fix up afterwards.
>
> (I find your code unreadable--spaces help a great deal, so am not sure if
> I have understood it correctly.)
>
Hm, I copied this code directly from Emacs+ESS, maybe the mailer
mangled something. What I want to do with this piece of code (I will
repaste it here)

testf<- function(formula) {
    mainform <- formula
    if(deparse(mainform[[3]][[1]])!="|") stop("invalid conditioning")
    mmodel <- substitute(y~x,list(y=mainform[[2]],x=mainform[[3]][[2]]))
    mmodel <- as.formula(mmodel)
    list(formula=list(main=mmodel))
}

is to read formula with condition:

formula(y~x|z)

and construct formula

formula(y~x)

I looked for examples in code of coplot in library graphics and
latticeParseFormula in library lattice.

Vaidotas Zemlys
--
Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
Vilnius University


From jrkrideau at yahoo.ca  Fri Feb  2 14:00:40 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 2 Feb 2007 08:00:40 -0500 (EST)
Subject: [R] Assigning labels to a list created with apply
Message-ID: <832456.45326.qm@web32804.mail.mud.yahoo.com>

I have a simple data base and I want to produce tables
for each variable.  I wrote a simple function
 fn1 <- function(x) {table(x)}  where x is a matrix or
data.frame. and used apply to produce a list of
tables. Example below.

How do I apply the colnames from the matrix or names
from the data.frame to label the tables in the results
in the list.  I know that I can do this individually
but there should be a way to do something like an
apply but I am missing it

cata <- c( 1,1,6,1,1,NA)
catb <- c( 1,2,3,4,5,6)
doga <- c(3,5,3,6,4, 0)
dogb <- c(2,4,6,8,10, 12)
rata <- c (NA, 9, 9, 8, 9, 8)
ratb <- c( 1,2,3,4,5,6)
bata <- c( 12, 42,NA, 45, 32, 54)
batb <- c( 13, 15, 17,19,21,23)
id <- c('a', 'b', 'b', 'c', 'a', 'b')
site <- c(1,1,4,4,1,4)
mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
bata, batb)

fn1 <- function(x) {table(x)}
jj <-apply(mat1, 1, fn1) ; jj

##  Slow way to label a list ###
label(jj[[1]]) <- "cata"
label(jj[[2]]) <- "catb"

#  and so on ...


From lorenzo.isella at gmail.com  Fri Feb  2 14:07:40 2007
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 2 Feb 2007 14:07:40 +0100
Subject: [R] Double labels and scales
Message-ID: <a2b3004b0702020507k68c31730t60051e2dbd7a4c68@mail.gmail.com>

Dear All,
Say you want to plot, on the same figure two quantities, concentration
and temperature, both as function of the same variable.
I'd like to be able to put a certain label and scale on the y axis on
the left of the figure (referring to the temperature) and another
label and scale for the concentration on the right.
Any suggestion about what to do?
I am sure it is trivial, but I could not find what I needed on the
net. I found some reference about a plot.double function by Anne York,
but I do not think that I need anything so elaborate.
Many thanks

Lorenzo


From ripley at stats.ox.ac.uk  Fri Feb  2 14:14:34 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 13:14:34 +0000 (GMT)
Subject: [R] features of save and save.image (unexpected file sizes)
In-Reply-To: <e47808320702020433k4781ae2ci3bdcc7abc69d066c@mail.gmail.com>
References: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com> 
	<45C0DA8C.2020902@stats.ox.ac.uk>
	<e47808320701312334y5f181f15ic8fb952b756a0461@mail.gmail.com>
	<Pine.LNX.4.64.0702010747090.9746@gannet.stats.ox.ac.uk> 
	<e47808320702010530y1044427dw2815dda86a29ba3f@mail.gmail.com> 
	<Pine.LNX.4.64.0702020906500.19324@gannet.stats.ox.ac.uk>
	<e47808320702020433k4781ae2ci3bdcc7abc69d066c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702021307200.26643@gannet.stats.ox.ac.uk>

On Fri, 2 Feb 2007, Vaidotas Zemlys wrote:

> Hi,
>
> On 2/2/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>
>> > I found the culprit. I was parsing formulas in my code, and I saved
>> > them in that large object. So the environment came with saved
>> > formulas. Is there a nice way to say R: "please do not save the
>> > environments with the formulas, I do not need them?"
>> 
>> No, but why create them that way?  You could do
>> 
>> mmodel <- as.formula(mmodel, env=.GlobalEnv)
>> 
> Hm, but say I have some large object in .GlobalEnv, and I generate
> mmodel  10 different times and save the result as a list with length
> 10. Now if I try to save this list, R will save 10 different copies of
> .GlobalEnv together with aforementioned large object?

No, it saves the environment (here .GlobalEnv), not objects, and there can 
be many shared references.

>> The R way is to create what you want, not fix up afterwards.
>> 
>> (I find your code unreadable--spaces help a great deal, so am not sure if
>> I have understood it correctly.)
>> 
> Hm, I copied this code directly from Emacs+ESS, maybe the mailer
> mangled something. What I want to do with this piece of code (I will
> repaste it here)
>
> testf<- function(formula) {
>   mainform <- formula
>   if(deparse(mainform[[3]][[1]])!="|") stop("invalid conditioning")
>   mmodel <- substitute(y~x,list(y=mainform[[2]],x=mainform[[3]][[2]]))
>   mmodel <- as.formula(mmodel)
>   list(formula=list(main=mmodel))
> }

You use no spaces around your operators or after commas.  R does when 
deparsing:

> testf
function (formula)
{
     mainform <- formula
     if (deparse(mainform[[3]][[1]]) != "|")
         stop("invalid conditioning")
     mmodel <- substitute(y ~ x, list(y = mainform[[2]], x = mainform[[3]][[2]]))
     mmodel <- as.formula(mmodel)
     list(formula = list(main = mmodel))
}

because it is (at least to old hands) much easier to read.

IcanreadEnglishtextwithoutanyspacesbutIchoosenotto.Similarly,Rcode.Occasional
spacesare evenharderto parse.

> is to read formula with condition:
>
> formula(y~x|z)
>
> and construct formula
>
> formula(y~x)
>
> I looked for examples in code of coplot in library graphics and
> latticeParseFormula in library lattice.
>
> Vaidotas Zemlys
> --
> Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
> Vilnius University
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Sicotte.Hugues at mayo.edu  Fri Feb  2 14:20:46 2007
From: Sicotte.Hugues at mayo.edu (Sicotte, Hugues   Ph.D.)
Date: Fri, 2 Feb 2007 07:20:46 -0600
Subject: [R] Affymetrix data analysis
References: <Pine.LNX.4.64.0702020900540.19324@gannet.stats.ox.ac.uk>
	<2E17292A64E6ED418A60BE89326B1AAB3225E5@msgebe11.mfad.mfroot.org>
	<Pine.LNX.4.64.0702021128140.24105@gannet.stats.ox.ac.uk>
Message-ID: <2E17292A64E6ED418A60BE89326B1AAB3225E6@msgebe11.mfad.mfroot.org>

I stand corrected, the rule was not 1/2. (I have 2 Gigs.. So the rule
was for my office's PC's).
Still, R doesn't always use all available memory on Windows, and one may
be able to set the options in order to get more.

Hugues
Ps. By the way Prof. Ripley, Thanks for all your efforts for R. 

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Friday, February 02, 2007 5:37 AM
To: Sicotte, Hugues Ph.D.
Cc: Tristan Coram; R-help at stat.math.ethz.ch
Subject: RE: [R] Affymetrix data analysis

On Fri, 2 Feb 2007, Sicotte, Hugues   Ph.D. wrote:

> Of course, you would know best, so can you tell us if the help pages I
> pull using
>
> help(Memory)
>
> is wrong?
> That help page says (2nd paragraph)
>
> "(On Windows the --max-mem-size option sets the maximum memory
> allocation: it has a minimum allowed value of 16M. This is intended to
> catch attempts to allocate excessive amounts of memory which may cause
> other processes to run out of resources. The default is the smaller of
> the amount of physical RAM in the machine and 1024Mb. See also
> memory.limit.) "

It says nothing about 'half' does it?

Depending on your version of R and Windows, the default is 1Gb, 1.5Gb or

2.5Gb, and the rw-FAQ gives the whole truth. The current version of that

help page is different:

https://svn.r-project.org/R/trunk/src/library/base/man/Memory.Rd

it looks like in 2.4.1 it had not been updated yet.

>
>
> Hugues
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Friday, February 02, 2007 3:05 AM
> To: Sicotte, Hugues Ph.D.
> Cc: Tristan Coram; R-help at stat.math.ethz.ch
> Subject: Re: [R] Affymetrix data analysis
>
> On Thu, 1 Feb 2007, Sicotte, Hugues   Ph.D. wrote:
>
>> Tristan,
>> I have a soft spot for problems analyzing microarrays with R..
>>
>> for the memory issue, there have been previous posts to this list..
>> But here is the answer I gave a few weeks ago.
>> If you need more memory, you have to move to linux or recompile R for
>> windows yourself..
>> .. But you'll still need a computer with more memory.
>> The long term solution, which we are implementing, is to rewrite the
>> normalization code so it doesn't
>> Need to load all those arrays at once.
>>
>> -- cut previous part of message--
>> The defaults in R is to play nice and limit your allocation to half
>> the available RAM. Make sure you have a lot of disk swap space (at
> least
>> 1G with 2G of RAM) and you can set your memory limit to 2G for R.
>
> That just isn't true (R uses as much of the RAM as is reasonable, all
> for
> up to 1.5Gb installed).  Please consult the rw-FAQ for the whole
truth.
>
>> See help(memory.size)  and use the memory.limit function
>
> [Please follow the advice you quote.]
>
>> Hugues
>>
>>
>> P.s. Someone let me use their 16Gig of RAM linux
>> And I was able to run R-64 bits with "top" showing 6Gigs of RAM
>> allocated (with suitable --max-mem-size command line parameters at
>> startup for R).
>
> There is no such 'command' for R under Linux.
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Feb  2 14:21:46 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 13:21:46 +0000 (GMT)
Subject: [R] Assigning labels to a list created with apply
In-Reply-To: <832456.45326.qm@web32804.mail.mud.yahoo.com>
References: <832456.45326.qm@web32804.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0702021315170.26643@gannet.stats.ox.ac.uk>

On Fri, 2 Feb 2007, John Kane wrote:

> I have a simple data base and I want to produce tables
> for each variable.  I wrote a simple function
> fn1 <- function(x) {table(x)}  where x is a matrix or
> data.frame. and used apply to produce a list of
> tables. Example below.
>
> How do I apply the colnames from the matrix or names
> from the data.frame to label the tables in the results
> in the list.  I know that I can do this individually
> but there should be a way to do something like an
> apply but I am missing it
>
> cata <- c( 1,1,6,1,1,NA)
> catb <- c( 1,2,3,4,5,6)
> doga <- c(3,5,3,6,4, 0)
> dogb <- c(2,4,6,8,10, 12)
> rata <- c (NA, 9, 9, 8, 9, 8)
> ratb <- c( 1,2,3,4,5,6)
> bata <- c( 12, 42,NA, 45, 32, 54)
> batb <- c( 13, 15, 17,19,21,23)
> id <- c('a', 'b', 'b', 'c', 'a', 'b')
> site <- c(1,1,4,4,1,4)
> mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> bata, batb)
>
> fn1 <- function(x) {table(x)}
> jj <-apply(mat1, 1, fn1) ; jj
>
> ##  Slow way to label a list ###
> label(jj[[1]]) <- "cata"
> label(jj[[2]]) <- "catb"

That does not work in vanilla R.  There is no function label<- (or label). 
Have you a package attached you did not tell us about?  (E.g. are you 
using the Hmisc system, not the R system?)

You have applied fn1 to the rows and not the columns, and

jj <-apply(mat1, 2, fn1)

would give you the column labels as they do make sense.  The way to add 
names to a list is names().


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jrkrideau at yahoo.ca  Fri Feb  2 14:34:26 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 2 Feb 2007 08:34:26 -0500 (EST)
Subject: [R] Assigning labels to a list created with apply
In-Reply-To: <Pine.LNX.4.64.0702021315170.26643@gannet.stats.ox.ac.uk>
Message-ID: <20070202133426.72915.qmail@web32801.mail.mud.yahoo.com>


--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Fri, 2 Feb 2007, John Kane wrote:
> 
> > I have a simple data base and I want to produce
> tables
> > for each variable.  I wrote a simple function
> > fn1 <- function(x) {table(x)}  where x is a matrix
> or
> > data.frame. and used apply to produce a list of
> > tables. Example below.
> >
> > How do I apply the colnames from the matrix or
> names
> > from the data.frame to label the tables in the
> results
> > in the list.  I know that I can do this
> individually
> > but there should be a way to do something like an
> > apply but I am missing it
> >
> > cata <- c( 1,1,6,1,1,NA)
> > catb <- c( 1,2,3,4,5,6)
> > doga <- c(3,5,3,6,4, 0)
> > dogb <- c(2,4,6,8,10, 12)
> > rata <- c (NA, 9, 9, 8, 9, 8)
> > ratb <- c( 1,2,3,4,5,6)
> > bata <- c( 12, 42,NA, 45, 32, 54)
> > batb <- c( 13, 15, 17,19,21,23)
> > id <- c('a', 'b', 'b', 'c', 'a', 'b')
> > site <- c(1,1,4,4,1,4)
> > mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> > bata, batb)
> >
> > fn1 <- function(x) {table(x)}
> > jj <-apply(mat1, 1, fn1) ; jj
> >
> > ##  Slow way to label a list ###
> > label(jj[[1]]) <- "cata"
> > label(jj[[2]]) <- "catb"
> 
> That does not work in vanilla R.  There is no
> function label<- (or label). 
> Have you a package attached you did not tell us
> about?  (E.g. are you 
> using the Hmisc system, not the R system?)

Yes I am.  I usually load it automatically and forgot
that it is not part of the package. My appologies
> 
> You have applied fn1 to the rows and not the
> columns, and
> 
> jj <-apply(mat1, 2, fn1)
> 
> would give you the column labels as they do make
> sense.  The way to add 
> names to a list is names().

Now that makes me feel stupid. I knew I was
overlooking something blindingly obvious.

Thanks very much.


From murdoch at stats.uwo.ca  Fri Feb  2 14:48:39 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 02 Feb 2007 08:48:39 -0500
Subject: [R] Double labels and scales
In-Reply-To: <a2b3004b0702020507k68c31730t60051e2dbd7a4c68@mail.gmail.com>
References: <a2b3004b0702020507k68c31730t60051e2dbd7a4c68@mail.gmail.com>
Message-ID: <45C34137.7050809@stats.uwo.ca>

On 2/2/2007 8:07 AM, Lorenzo Isella wrote:
 > Dear All,
 > Say you want to plot, on the same figure two quantities, concentration
 > and temperature, both as function of the same variable.
 > I'd like to be able to put a certain label and scale on the y axis on
 > the left of the figure (referring to the temperature) and another
 > label and scale for the concentration on the right.
 > Any suggestion about what to do?
 > I am sure it is trivial, but I could not find what I needed on the
 > net. I found some reference about a plot.double function by Anne York,
 > but I do not think that I need anything so elaborate.
 > Many thanks

One way is to work out the linear transformation that puts the other one 
in the scale of the one you want on the left axis, then plot both and 
manually apply an axis to the right side.

For example:

x <- 1:10
temp <- rnorm(10, mean=5-20*x)
conc <- rnorm(10, mean=x)

b <- diff(range(temp))/diff(range(conc))
a <- min(temp) - b*min(conc)

par(mar=c(5,5,5,5))

plot(x, temp, ylim = range(c(temp, a + b*conc)), type='l')
lines(x, a + b*conc, lty=2)

ticks <- pretty(conc)
axis(4, at=a + b*ticks, labels=ticks)
mtext("conc", 4, 3)
legend("top", c("temp", "conc"), lty=1:2)


From p_connolly at ihug.co.nz  Thu Feb  1 20:44:05 2007
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Fri, 2 Feb 2007 08:44:05 +1300
Subject: [R] Wiki for Graphics tips for MacOS X
In-Reply-To: <971536df0701310911g597d73afiede5947437e35498@mail.gmail.com>
References: <E9E8F95E-7610-4D61-ABF5-B8CEA952F1A3@MUOhio.edu>
	<971536df0701310911g597d73afiede5947437e35498@mail.gmail.com>
Message-ID: <20070201194405.GG15607@ihug.co.nz>

On Wed, 31-Jan-2007 at 12:11PM -0500, Gabor Grothendieck wrote:

|> To get the best results you need to transfer it using vector
|> graphics rather than bitmapped graphics:
|> 
|> http://www.stc-saz.org/resources/0203_graphics.pdf
|> 
|> There are a number of variations described here (see
|> entire thread).  Its for UNIX and Windows but I think
|> it would likely work similarly on Mac and Windows:
|> 
|> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32297.html

I found that interesting, particularly this part:

For example, on Linux do this:

   dev.control(displaylist="enable") # enable display list
   plot(1:10)
   myplot <- recordPlot() # load displaylist into variable
   save(myplot, file="myplot", ascii=TRUE)

Send the ascii file, myplot, to the Windows machine and on Windows do this:

   dev.control(displaylist="enable") # enable display list
   load("myplot")
   myplot # displays the plot
   savePlot("myplot", type="wmf") # saves current plot as wmf 

I tried that, but I was never able to load the myplot in the Windows
R.  I always got a message about a syntax error to do with ' ' but I
was unable to work out what the problem was.  I thought it was because
the transfer to Windows wasn't binary, but that wasn't the problem.

I was unable to get the thread view at that archive to function so I
was unable to see if there were any follow ups which offered an
explanation.

R has changed quite a bit in the years since then, so it might be that
something needs to be done differently with more recent versions.

Has anyone done this recently?

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From xmeng at capitalbio.com  Fri Feb  2 03:25:37 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Fri, 02 Feb 2007 10:25:37 +0800
Subject: [R] 2 axes in an coordinate system
Message-ID: <370383137.16333@capitalbio.com>

Hello sir: 
I wanna get such kind of plot: 

x : location of chromosome
y_left: p value 
y_right: logratio 

In other words,On the left side of plot,y is p value;On the right side of plot,y is logratio How can I do it via R command? 

Thanks a lot! My best!


From tlumley at u.washington.edu  Fri Feb  2 15:05:38 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 2 Feb 2007 06:05:38 -0800 (PST)
Subject: [R] Help with efficient double sum of max (X_i,
 Y_i) (X & Y vectors)
In-Reply-To: <000001c7462f$f6aa6d80$7c94100a@win.ad.jhu.edu>
References: <1170353883.961.15.camel@pc-racine1.economics.mcmaster.ca>
	<000001c7462f$f6aa6d80$7c94100a@win.ad.jhu.edu>
Message-ID: <Pine.LNX.4.64.0702020601120.4457@homer21.u.washington.edu>

On Thu, 1 Feb 2007, Ravi Varadhan wrote:

> Jeff,
>
> Here is something which is a little faster:
>
> sum1 <- sum(outer(x, x, FUN="pmax"))
> sum3 <- sum(outer(x, y, FUN="pmax"))

This is the sort of problem where profiling can be useful.  My experience 
with pmax() is that it is surprisingly slow, presumably because it handles 
recycling and NAs

In the example I profiled (an MCMC calculation) it was measurably faster 
to use
    function(x,y) {i<- x<y; x[i]<-y[i]; x}

 	-thomas

>
> Best,
> Ravi.
>
> ----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>
>
> ----------------------------------------------------------------------------
> --------
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeffrey Racine
> Sent: Thursday, February 01, 2007 1:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help with efficient double sum of max (X_i, Y_i) (X & Y
> vectors)
>
> Greetings.
>
> For R gurus this may be a no brainer, but I could not find pointers to
> efficient computation of this beast in past help files.
>
> Background - I wish to implement a Cramer-von Mises type test statistic
> which involves double sums of max(X_i,Y_j) where X and Y are vectors of
> differing length.
>
> I am currently using ifelse pointwise in a vector, but have a nagging
> suspicion that there is a more efficient way to do this. Basically, I
> require three sums:
>
> sum1: \sum_i\sum_j max(X_i,X_j)
> sum2: \sum_i\sum_j max(Y_i,Y_j)
> sum3: \sum_i\sum_j max(X_i,Y_j)
>
> Here is my current implementation - any pointers to more efficient
> computation greatly appreciated.
>
>  nx <- length(x)
>  ny <- length(y)
>
>  sum1 <- 0
>  sum3 <- 0
>
>  for(i in 1:nx) {
>    sum1 <- sum1 + sum(ifelse(x[i]>x,x[i],x))
>    sum3 <- sum3 + sum(ifelse(x[i]>y,x[i],y))
>  }
>
>  sum2 <- 0
>  sum4 <- sum3 # symmetric and identical
>
>  for(i in 1:ny) {
>    sum2 <- sum2 + sum(ifelse(y[i]>y,y[i],y))
>  }
>
> Thanks in advance for your help.
>
> -- Jeff
>
> -- 
> Professor J. S. Racine         Phone:  (905) 525 9140 x 23825
> Department of Economics        FAX:    (905) 521-8232
> McMaster University            e-mail: racinej at mcmaster.ca
> 1280 Main St. W.,Hamilton,     URL:
> http://www.economics.mcmaster.ca/racine/
> Ontario, Canada. L8S 4M4
>
> `The generation of random numbers is too important to be left to chance'
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From kashei at sip-oy.com  Fri Feb  2 15:06:51 2007
From: kashei at sip-oy.com (Kaskelma, Heikki)
Date: Fri, 2 Feb 2007 16:06:51 +0200
Subject: [R] Can this loop be delooped?
Message-ID: <58FDC30CAA92594996B65C93926950AF1E040A@epont3.mas-oy.com>

Consider

na=43; nb=5; x=1:na
ns=rep(na %/% nb, nb) + (1:nb <= na %% nb)
split(x, rep(1:nb, ns))


Heikki Kaskelma

On Fri, 2 Feb 2007, jim holtman <jholtman at gmail.com> wrote:
>This might do what you want:
>
> # test data
> x <- 1:43
> nb <- 5  # number of subsets
> # create vector of lengths of subsets
> ns <- rep(length(x) %/% nb, nb)
> # see if we have to adjust counts of initial subsets
> if ((.offset <- length(x) %% nb) != 0) ns[1:.offset] = ns[1:.offset] +
1
> # create the subsets
> split(x, rep(1:nb,ns))


From skiadas at hanover.edu  Fri Feb  2 15:18:43 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Fri, 2 Feb 2007 09:18:43 -0500
Subject: [R] Double labels and scales
In-Reply-To: <a2b3004b0702020507k68c31730t60051e2dbd7a4c68@mail.gmail.com>
References: <a2b3004b0702020507k68c31730t60051e2dbd7a4c68@mail.gmail.com>
Message-ID: <2D6F5184-D157-42FE-A2FA-C7F7D64086E8@hanover.edu>

On Feb 2, 2007, at 8:07 AM, Lorenzo Isella wrote:

> Dear All,
> Say you want to plot, on the same figure two quantities, concentration
> and temperature, both as function of the same variable.
> I'd like to be able to put a certain label and scale on the y axis on
> the left of the figure (referring to the temperature) and another
> label and scale for the concentration on the right.
> Any suggestion about what to do?
> I am sure it is trivial, but I could not find what I needed on the
> net. I found some reference about a plot.double function by Anne York,
> but I do not think that I need anything so elaborate.
> Many thanks
>
Sounds like you just need the following two commands probably:

?axis
?mtext

> Lorenzo

Haris


From vdemart1 at tin.it  Fri Feb  2 15:57:04 2007
From: vdemart1 at tin.it (Vittorio)
Date: Fri, 2 Feb 2007 15:57:04 +0100 (GMT+01:00)
Subject: [R] New RODBC: a problem with sqlSave
Message-ID: <11082f564fb.vdemart1@tin.it>

Till yesterday I was more than satisfied by RODBC, specifically by 
sqlSave which I had been happily using in a daily crontab R batch job 
regularly updating a postgresql db for as long as one year. In this R-
batch I was able to save records into indexed tables *** even though 
*** they already existed. in this case sqlSave simply neglected those 
rows of the input table that were offending the constraints of existing 
rows of a table, only appending the *** very new *** rows.

Yesterday I 
updated the R packages in my unix box including RODBC and running the R 
batch the following error popped up ((I only added the verbose option 
to sqlSave!)
====================================================
 
sqlSave(canale, tabella1, tablename="tminmax", rownames=FALSE,
varTypes=tipicampitminmax, append=TRUE,fast=TRUE,verbose=T)
Query: 
INSERT INTO tminmax ( data, cod_wmo, tipo, t ) VALUES ( ?,?,?,? )
Binding: data: DataType 9
Binding: cod_wmo: DataType 1
Binding: tipo: 
DataType 1
Binding: t: DataType 2
Parameters:
no: 1: data 2007-01-
26/***/no: 2: cod_wmo 16045/***/no: 3: tipo MIN/***/no: 4: t -2.5/***/
sqlwrite returned 
[RODBC] Failed exec in Update
01000 -1 [unixODBC]
Error while executing the query (non-fatal);
ERROR:  duplicate key 
violates unique constraint "data_cod_tipo"

Query: DROP TABLE tminmax
Errore in sqlSave(canale, tabella1, tablename = "tminmax", rownames = 
FALSE,  : 
        unable to append to table 'tminmax'
===========================================
It seems to say that the 
violation caused a non-fatal error nevertheless it stopped the batch 
job without going on!

What should I do?
Ciao
Vittorio

PS Eliminating 
from the original postgresql the offending records, of course, the 
above mentioned sqlSave command works like a charme!


From bill.shipley at usherbrooke.ca  Fri Feb  2 16:11:31 2007
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Fri, 2 Feb 2007 10:11:31 -0500
Subject: [R] dynamic loading error with Open Watcom object file
Message-ID: <000d01c746dc$6c82f660$7a1ad284@BIO041>

Hello.  I am trying to use a FORTRAN subroutine from within R (Windows
version).  This fortran subroutine is compiled using the Open Watcom Fortran
compiler and the compiled object file is called ritscale.obj.  Following the
explanation on pages 193-194 of "The New S language" I use the dyn.load
command:

> dyn.load("f:/maxent/ritscale.obj")
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library 'f:/maxent/ritscale.obj':
  LoadLibrary failure:  %1 n'est pas une application Win32 valide. 

The error message says:  LoadLibrary failure: %1 is not a valid Win32
application

I do not know what this means.  Can someone help?

Bill Shipley


From johannes_graumann at web.de  Fri Feb  2 16:32:03 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 02 Feb 2007 16:32:03 +0100
Subject: [R] Dealing with Duplicates - How to count instances?
Message-ID: <epvli8$778$1@sea.gmane.org>

Hi there,

given a data.frame 'data' I managed to filter out entries (rows) that are
identical with respect to one column like so:

duplicity <- duplicated(data[column])
data_unique <- subset(data,duplicity!=TRUE)

But I'm trying to extract how many duplicates each of the remaining rows
had.

Can someone please send me down the right path for this?

Joh


From mpiktas at gmail.com  Fri Feb  2 16:39:18 2007
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Fri, 2 Feb 2007 17:39:18 +0200
Subject: [R] features of save and save.image (unexpected file sizes)
In-Reply-To: <Pine.LNX.4.64.0702021307200.26643@gannet.stats.ox.ac.uk>
References: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com>
	<45C0DA8C.2020902@stats.ox.ac.uk>
	<e47808320701312334y5f181f15ic8fb952b756a0461@mail.gmail.com>
	<Pine.LNX.4.64.0702010747090.9746@gannet.stats.ox.ac.uk>
	<e47808320702010530y1044427dw2815dda86a29ba3f@mail.gmail.com>
	<Pine.LNX.4.64.0702020906500.19324@gannet.stats.ox.ac.uk>
	<e47808320702020433k4781ae2ci3bdcc7abc69d066c@mail.gmail.com>
	<Pine.LNX.4.64.0702021307200.26643@gannet.stats.ox.ac.uk>
Message-ID: <e47808320702020739s73f41ea6w82bd0280408ba4a0@mail.gmail.com>

Hi,

On 2/2/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Fri, 2 Feb 2007, Vaidotas Zemlys wrote:
>
> > Hm, I copied this code directly from Emacs+ESS, maybe the mailer
> > mangled something. What I want to do with this piece of code (I will
> > repaste it here)
> >
> > testf<- function(formula) {
> >   mainform <- formula
> >   if(deparse(mainform[[3]][[1]])!="|") stop("invalid conditioning")
> >   mmodel <- substitute(y~x,list(y=mainform[[2]],x=mainform[[3]][[2]]))
> >   mmodel <- as.formula(mmodel)
> >   list(formula=list(main=mmodel))
> > }
>
> You use no spaces around your operators or after commas.  R does when
> deparsing:
>
> > testf
> function (formula)
> {
>      mainform <- formula
>      if (deparse(mainform[[3]][[1]]) != "|")
>          stop("invalid conditioning")
>      mmodel <- substitute(y ~ x, list(y = mainform[[2]], x = mainform[[3]][[2]]))
>      mmodel <- as.formula(mmodel)
>      list(formula = list(main = mmodel))
> }
>
> because it is (at least to old hands) much easier to read.
>
> IcanreadEnglishtextwithoutanyspacesbutIchoosenotto.Similarly,Rcode.Occasional
> spacesare evenharderto parse.
>

Sorry for that, it is an old bad habit of mine. I'll try to get rid of
it. Does anybody know, if ESS can do this automatically, besides
automatical identing?

Vaidotas Zemlys
--
Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
Vilnius University


From dittmann at in.tum.de  Fri Feb  2 16:41:14 2007
From: dittmann at in.tum.de (dittmann at in.tum.de)
Date: Fri, 2 Feb 2007 16:41:14 +0100 (MET)
Subject: [R] Problem with party and ordered factors
Message-ID: <26942.62.180.31.65.1170430874.squirrel@home.in.tum.de>

Hi All,

i've got a problem using the ctree function of the party package. I've
searched around a lot but couldn't manage it.
I've got an ordered factor as response. As far as i know i have to use
scores to be able to use this ordered factor. But if i do so i get a tree
which predicts all observations as the first level of my ordered factor.

In order to test if i did anything wrong i tried the example of Torsten
Hothorn, Kurt Hornik and Achim Zeileis described in the documentation
party: A Laboratory for Recursive Part(y)titioning

There i got the same problem. I execute the following code:
> data("mammoexp", package = "party")
> mtree <- ctree(ME ~ ., data = mammoexp, scores = list(ME = 1:3, SYMPT =
1:4, DECT = 1:3))
> plot(mtree)

Beside getting one waring everthing's ok. But when i excute
summary(predict(mtree)) the result is:
 Never Within a Year   Over a Year
  412             0             0

So now i'm stuck. Am i doing anything wrong? I'm using R 2.4.1 and all
packages are uptodate.

Thanks in advance for your help.

Sincerely yours
Christoph


From aiminy at iastate.edu  Fri Feb  2 16:45:07 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 02 Feb 2007 09:45:07 -0600
Subject: [R] CGIwithR question
Message-ID: <6.1.2.0.2.20070202094153.01cf2aa8@aiminy.mail.iastate.edu>

I use svm in R to do some prediction.
Now I want to get input from web, do prediction use svm, then output 
prediction results to web.
I know CGIwithR could do this, but I don't know the details.Could someone 
give me some advice for this?
thanks,

Aimin Yan


From ripley at stats.ox.ac.uk  Fri Feb  2 16:50:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 15:50:07 +0000 (GMT)
Subject: [R] dynamic loading error with Open Watcom object file
In-Reply-To: <000d01c746dc$6c82f660$7a1ad284@BIO041>
References: <000d01c746dc$6c82f660$7a1ad284@BIO041>
Message-ID: <Pine.LNX.4.64.0702021544300.31903@gannet.stats.ox.ac.uk>

On Fri, 2 Feb 2007, Bill Shipley wrote:

> Hello.  I am trying to use a FORTRAN subroutine from within R (Windows
> version).  This fortran subroutine is compiled using the Open Watcom Fortran
> compiler and the compiled object file is called ritscale.obj.  Following the
> explanation on pages 193-194 of "The New S language" I use the dyn.load
> command:
>
>> dyn.load("f:/maxent/ritscale.obj")
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library 'f:/maxent/ritscale.obj':
>  LoadLibrary failure:  %1 n'est pas une application Win32 valide.
>
> The error message says:  LoadLibrary failure: %1 is not a valid Win32
> application
>
> I do not know what this means.  Can someone help?

Yes.

Unlike versions of S from the 1980s and 1990s, dyn.load() in R loads a DLL 
and not a compiled object.  As the help page says:

      dyn.load(x, local = TRUE, now = TRUE)

        x: a character string giving the pathname to a shared library or
           DLL.

'shared library' is a Unix name for what Windows calls a DLL.

So, you need to make a DLL from ritscale.obj.  I used to know how to do 
that under Watcom, but (S Programming p.245) 'it is fraught with 
difficulties'.  It would be much easier to use the recommended compiler 
(MinGW's g77).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Freddie at shaw.ca  Fri Feb  2 16:50:43 2007
From: Freddie at shaw.ca (Andrew Malkovich)
Date: Sat, 3 Feb 2007 00:50:43 +0900
Subject: [R] Easy Earnings with Flextime
Message-ID: <D8A85800.9285635@shaw.ca>

Hello,

We hope you are reading this message in a fine mood.

I'd like to welcome you on a very interesting opportunity.
We supppose you will be very interested in a home job in which you
could get about AUD4000 per month.

This job will not affect your present career, it will only take a small
part of your free time. The only things you will need to have to start
running your business with our company are reliable Internet/E-mail
access and checking/savings bank account. And your decency, of course!

Your part of the job is to receive the funds which we will send directly
to you through one of our money transfer methods from our company and/or
our partners. After that you should re-send the money (less your commission)
to us/our customers via one of chosen money transfer agencies. The job is
rather simple and you won't need any special knowledge to become our partner!
You will also stand the chances of being a part of our future and the
excellence of a team in which you will be highly respected - just think about
this amazing opportunity! We will be hoping to hear from you soon.

Please fill our application form. No fees are asked, just leave your
contact details: mail at sharksdiving.bz

We will contact you soon. Thank you!


From rvaradhan at jhmi.edu  Fri Feb  2 17:06:17 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 2 Feb 2007 11:06:17 -0500
Subject: [R] Help with efficient double sum of max (X_i,
	Y_i) (X & Y vectors)
In-Reply-To: <Pine.LNX.4.64.0702020601120.4457@homer21.u.washington.edu>
References: <1170353883.961.15.camel@pc-racine1.economics.mcmaster.ca>
	<000001c7462f$f6aa6d80$7c94100a@win.ad.jhu.edu>
	<Pine.LNX.4.64.0702020601120.4457@homer21.u.washington.edu>
Message-ID: <000301c746e4$12fb10c0$7c94100a@win.ad.jhu.edu>

Thomas, you are absolutely correct.  

Here are some results comparing Jeff's original implementation, my
suggestion using outer and pmax, and your clever trick using "fast.pmax".
Your fast.pmax function is more than 3 times faster than pmax.  Thanks for
this wonderful insight. 

Best,
Ravi.

> x <- rnorm(2000)
> y <- runif(500)
>   nx <- length(x)
>   ny <- length(y)
>   sum1 <- 0
>   sum3 <- 0
>     
> # Here is the straightforward way
> system.time(
+   for(i in 1:nx) {
+     sum1 <- sum1 + sum(ifelse(x[i]>x,x[i],x))
+     sum3 <- sum3 + sum(ifelse(x[i]>y,x[i],y))
+   }
+ )
[1] 3.83 0.00 3.83   NA   NA
> 
> # Here is a faster way using "outer" and "pmax"
> system.time({
+ sum11 <- sum(outer(x,x,FUN="pmax"))
+ sum33 <- sum(outer(x,y,FUN="pmax"))
+ })
[1] 2.55 0.48 3.04   NA   NA
> 
> # Here is an even faster method using Tom Lumley's suggestion:
> fast.pmax <- function(x,y) {i<- x<y; x[i]<-y[i]; x}
> 
> system.time({
+ sum111 <- sum(outer(x,x,FUN="fast.pmax"))
+ sum333 <- sum(outer(x,y,FUN="fast.pmax"))
+ })
[1] 0.78 0.08 0.86   NA   NA
> 
> 
> all.equal(sum1,sum11,sum111)
[1] TRUE
> all.equal(sum3,sum33,sum333)
[1] TRUE
> 
>

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
Sent: Friday, February 02, 2007 9:06 AM
To: Ravi Varadhan
Cc: racinej at mcmaster.ca; r-help at stat.math.ethz.ch
Subject: Re: [R] Help with efficient double sum of max (X_i, Y_i) (X & Y
vectors)

On Thu, 1 Feb 2007, Ravi Varadhan wrote:

> Jeff,
>
> Here is something which is a little faster:
>
> sum1 <- sum(outer(x, x, FUN="pmax"))
> sum3 <- sum(outer(x, y, FUN="pmax"))

This is the sort of problem where profiling can be useful.  My experience 
with pmax() is that it is surprisingly slow, presumably because it handles 
recycling and NAs

In the example I profiled (an MCMC calculation) it was measurably faster 
to use
    function(x,y) {i<- x<y; x[i]<-y[i]; x}

 	-thomas

>
> Best,
> Ravi.
>
>
----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>
>
>
----------------------------------------------------------------------------
> --------
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeffrey Racine
> Sent: Thursday, February 01, 2007 1:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help with efficient double sum of max (X_i, Y_i) (X & Y
> vectors)
>
> Greetings.
>
> For R gurus this may be a no brainer, but I could not find pointers to
> efficient computation of this beast in past help files.
>
> Background - I wish to implement a Cramer-von Mises type test statistic
> which involves double sums of max(X_i,Y_j) where X and Y are vectors of
> differing length.
>
> I am currently using ifelse pointwise in a vector, but have a nagging
> suspicion that there is a more efficient way to do this. Basically, I
> require three sums:
>
> sum1: \sum_i\sum_j max(X_i,X_j)
> sum2: \sum_i\sum_j max(Y_i,Y_j)
> sum3: \sum_i\sum_j max(X_i,Y_j)
>
> Here is my current implementation - any pointers to more efficient
> computation greatly appreciated.
>
>  nx <- length(x)
>  ny <- length(y)
>
>  sum1 <- 0
>  sum3 <- 0
>
>  for(i in 1:nx) {
>    sum1 <- sum1 + sum(ifelse(x[i]>x,x[i],x))
>    sum3 <- sum3 + sum(ifelse(x[i]>y,x[i],y))
>  }
>
>  sum2 <- 0
>  sum4 <- sum3 # symmetric and identical
>
>  for(i in 1:ny) {
>    sum2 <- sum2 + sum(ifelse(y[i]>y,y[i],y))
>  }
>
> Thanks in advance for your help.
>
> -- Jeff
>
> -- 
> Professor J. S. Racine         Phone:  (905) 525 9140 x 23825
> Department of Economics        FAX:    (905) 521-8232
> McMaster University            e-mail: racinej at mcmaster.ca
> 1280 Main St. W.,Hamilton,     URL:
> http://www.economics.mcmaster.ca/racine/
> Ontario, Canada. L8S 4M4
>
> `The generation of random numbers is too important to be left to chance'
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From topkatz at msn.com  Fri Feb  2 17:13:33 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 02 Feb 2007 11:13:33 -0500
Subject: [R] Can this loop be delooped?
In-Reply-To: <58FDC30CAA92594996B65C93926950AF1E040A@epont3.mas-oy.com>
Message-ID: <BAY132-F28B137A8EDB6A9DDE65E13AA9B0@phx.gbl>

Jim and Heikki,

Thank you so much for your inventive solutions, much better and more 
efficient than my approach!  As a footnote to my original post, it turns out 
that using lapply rather than sapply consistently returns a list, and not an 
array (which sapply gives when the number of points is an exact multiple of 
the number of groups).

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: "Kaskelma, Heikki" <kashei at sip-oy.com>
>To: "jim holtman" <jholtman at gmail.com>,"Talbot Katz" <topkatz at msn.com>
>CC: <r-help at stat.math.ethz.ch>
>Subject: RE: [R] Can this loop be delooped?
>Date: Fri, 2 Feb 2007 16:06:51 +0200
>
>Consider
>
>na=43; nb=5; x=1:na
>ns=rep(na %/% nb, nb) + (1:nb <= na %% nb)
>split(x, rep(1:nb, ns))
>
>
>Heikki Kaskelma
>
>On Fri, 2 Feb 2007, jim holtman <jholtman at gmail.com> wrote:
> >This might do what you want:
> >
> > # test data
> > x <- 1:43
> > nb <- 5  # number of subsets
> > # create vector of lengths of subsets
> > ns <- rep(length(x) %/% nb, nb)
> > # see if we have to adjust counts of initial subsets
> > if ((.offset <- length(x) %% nb) != 0) ns[1:.offset] = ns[1:.offset] +
>1
> > # create the subsets
> > split(x, rep(1:nb,ns))


From martinol at ensam.inra.fr  Fri Feb  2 17:53:10 2007
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Fri, 02 Feb 2007 17:53:10 +0100
Subject: [R] R syntaxe
Message-ID: <45C36C76.7090503@ensam.inra.fr>

Hi all,

Suppose I have a vector x with numerical values.
In y, I have a categorial variable : y<-c(1,1,..2,2,...30,30,30)
x and y have the same length.
I would like to compute the mean for x for the modality 1 to 30 in y.
mean(x[y==1]),...,mean(x[y==30])
I do not want to use an iterative procedure such that for (i in 1:30)..
Thanks for your help,

Regards.
Olivier.


-- 

-------------------------------------------------------------
Martin Olivier
INRA - Unit? Biostatistique & Processus Spatiaux
Domaine St Paul, Site Agroparc
84914 Avignon Cedex 9, France
Tel : 04 32 72 21 57


From Greg.Snow at intermountainmail.org  Fri Feb  2 17:54:55 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 2 Feb 2007 09:54:55 -0700
Subject: [R] A question about dput
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7FC461@LP-EXCHVS07.CO.IHC.COM>

Have you looked at the BRugs and R2WinBUGS packages?  They have functions for automatically converting R lists and other objects into WinBugs files (you can even run winbugs from within R).

Hope this helps, 

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tong Wang
> Sent: Friday, February 02, 2007 12:38 AM
> To: R help
> Subject: [R] A question about dput
> 
> Hi, 
>    I am trying to output a R data set for use in WinBugs,  I used
>          dput(list(x=rnorm(100),N=100),file="bug.dat") 
> But I can't get the intended format:    
> list(x=c(.......),N=100), instead, I got 
> something like this (copied the first two lines):
> 
> [00000000]???73?74?72?75??63?74?75?72??65?28?6C?69??73?74?28?7
> 8????    structure(list(x
> [00000010]???20?3D?20?63??28?2D?30?2E??33?36?33?31??36?31?30?3
> 3    ?????=?c(-0.36316103
> 
> Did I do something wrong here ?  Thanks a lot for any help
> 
> tong
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From roland.rproject at gmail.com  Fri Feb  2 18:13:38 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Fri, 2 Feb 2007 12:13:38 -0500
Subject: [R] R syntaxe
In-Reply-To: <45C36C76.7090503@ensam.inra.fr>
References: <45C36C76.7090503@ensam.inra.fr>
Message-ID: <47c7c59e0702020913x13063fa5pbdb3097102025482@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070202/cdd8bb7d/attachment.pl 

From pausas at gmail.com  Fri Feb  2 18:40:27 2007
From: pausas at gmail.com (juli g. pausas)
Date: Fri, 2 Feb 2007 18:40:27 +0100
Subject: [R] reading very large files
Message-ID: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070202/2d2eb65b/attachment.pl 

From xinxia.peng at sbri.org  Fri Feb  2 19:13:43 2007
From: xinxia.peng at sbri.org (Xinxia Peng)
Date: Fri, 2 Feb 2007 10:13:43 -0800
Subject: [R] Make error for R-devel package under linux
Message-ID: <89E37888628D0244B25B4105152963CC368D41@mail01.sbri.org>

Hi,

I got an error while compiling the R-devel (R-2.5.0) package under
Redhat Linux, but the R-patch package (R-2.4.1) was compiled perfectly
fine. The following is what I got from the R-devel package downloaed on
Jan. 30th., 2007:


-----------------------------------------------------------
>./configure

....

R is now configured for x86_64-unknown-linux-gnu

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                gcc -std=gnu99  -g -O2
  Fortran 77 compiler:       g77  -g -O2

  C++ compiler:              g++  -g -O2
  Fortran 90/95 compiler:    gfortran -g -O2
  Obj-C compiler:

  Interfaces supported:      X11
  External libraries:        readline
  Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
  Options enabled:           shared BLAS, R profiling, Java

  Recommended packages:      yes

configure: WARNING: you cannot build DVI versions of the R manuals
configure: WARNING: you cannot build PDF versions of the R manuals

>make

.....

mkdir -p -- ../../../../library/tools/libs
make[5]: Leaving directory
`/nethome/xpeng/linux/tools/R-devel/src/library/tools/src'
make[4]: Leaving directory
`/nethome/xpeng/linux/tools/R-devel/src/library/tools/src'
Error in loadNamespace(name) : there is no package called 'tools'
In addition: Warning messages:
1: Line starting 'Package: tools ...' is malformed!
2: Line starting 'Version: 2.5.0 ...' is malformed!
3: Line starting 'Priority: base ...' is malformed!
4: Line starting 'Title: Tools for Pac ...' is malformed!
5: Line starting 'Author: Kurt Hornik  ...' is malformed!
6: Line starting 'Maintainer: R Core T ...' is malformed!
7: Line starting 'Description: Tools f ...' is malformed!
8: Line starting 'License: GPL Version ...' is malformed!
9: Line starting 'Built: R 2.5.0; x86_ ...' is malformed!
Execution halted
make[3]: *** [all] Error 1
make[3]: Leaving directory
`/nethome/xpeng/linux/tools/R-devel/src/library/tools'
make[2]: *** [R] Error 1
make[2]: Leaving directory
`/nethome/xpeng/linux/tools/R-devel/src/library'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/nethome/xpeng/linux/tools/R-devel/src'
make: *** [R] Error 1

-----------------------------------------------------------

I got the same error after I went into the directoy
'R-devel/src/library/tools' and run 'make' again. Any suggestions?

Thanks,
Xinxia


From hb at stat.berkeley.edu  Fri Feb  2 19:19:19 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Fri, 2 Feb 2007 10:19:19 -0800
Subject: [R] reading very large files
In-Reply-To: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
Message-ID: <59d7961d0702021019t58132bbeoaac2da2d6ea9d471@mail.gmail.com>

Hi.

General idea:

1. Open your file as a connection, i.e. con <- file(pathname, open="r")

2. Generate a "row to (file offset, row length) map of your text file,
i.e. a numeric vector 'fileOffsets' and 'rowLengths'.  Use readBin()
for this. You build this up as you go by reading the file in chunks
meaning you can handles files of any size.  You can store this lookup
map to file for your future R sessions.

3. Sample a set of rows r = (r1, r2, ..., rR), i.e. rows =
sample(length(fileOffsets)).

4. Look up the file offsets and row lengths for these rows, i.e.
offsets = fileOffsets[rows].  lengths = rowLengths[rows].

5. In case your subset of rows is not ordered, it is wise to order
them first to speed up things.  If order is important, keep track of
the ordering and re-order them at then end.

6. For each row r, use seek(con=con, where=offsets[r]) to jump to the
start of the row.  Use readBin(..., n=lengths[r]) to read the data.

7. Repeat from (3).

/Henrik

On 2/2/07, juli g. pausas <pausas at gmail.com> wrote:
> Hi all,
> I have a large file (1.8 GB) with 900,000 lines that I would like to read.
> Each line is a string characters. Specifically I would like to randomly
> select 3000 lines. For smaller files, what I'm doing is:
>
> trs <- scan("myfile", what= character(), sep = "\n")
> trs<- trs[sample(length(trs), 3000)]
>
> And this works OK; however my computer seems not able to handle the 1.8 G
> file.
> I thought of an alternative way that not require to read the whole file:
>
> sel <- sample(1:900000, 3000)
> for (i in 1:3000)  {
> un <- scan("myfile", what= character(), sep = "\n", skip=sel[i], nlines=1)
> write(un, "myfile_short", append=TRUE)
> }
>
> This works on my computer; however it is extremely slow; it read one line
> each time. It is been running for 25 hours and I think it has done less than
> half of the file (Yes, probably I do not have a very good computer and I'm
> working under Windows ...).
> So my question is: do you know any other faster way to do this?
> Thanks in advance
>
> Juli
>
> --
> http://www.ceam.es/pausas
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From hb at stat.berkeley.edu  Fri Feb  2 19:22:41 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Fri, 2 Feb 2007 10:22:41 -0800
Subject: [R] reading very large files
In-Reply-To: <59d7961d0702021019t58132bbeoaac2da2d6ea9d471@mail.gmail.com>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
	<59d7961d0702021019t58132bbeoaac2da2d6ea9d471@mail.gmail.com>
Message-ID: <59d7961d0702021022j58a87819k15e628b0c104ecc@mail.gmail.com>

Forgot to say, in your script you're reading the rows unordered
meaning you're jumping around in the file and there is no way the
hardware or the file caching system can optimize that.  I'm pretty
sure you would see a substantial speedup if you did:

sel <- sort(sel);

/H

On 2/2/07, Henrik Bengtsson <hb at stat.berkeley.edu> wrote:
> Hi.
>
> General idea:
>
> 1. Open your file as a connection, i.e. con <- file(pathname, open="r")
>
> 2. Generate a "row to (file offset, row length) map of your text file,
> i.e. a numeric vector 'fileOffsets' and 'rowLengths'.  Use readBin()
> for this. You build this up as you go by reading the file in chunks
> meaning you can handles files of any size.  You can store this lookup
> map to file for your future R sessions.
>
> 3. Sample a set of rows r = (r1, r2, ..., rR), i.e. rows =
> sample(length(fileOffsets)).
>
> 4. Look up the file offsets and row lengths for these rows, i.e.
> offsets = fileOffsets[rows].  lengths = rowLengths[rows].
>
> 5. In case your subset of rows is not ordered, it is wise to order
> them first to speed up things.  If order is important, keep track of
> the ordering and re-order them at then end.
>
> 6. For each row r, use seek(con=con, where=offsets[r]) to jump to the
> start of the row.  Use readBin(..., n=lengths[r]) to read the data.
>
> 7. Repeat from (3).
>
> /Henrik
>
> On 2/2/07, juli g. pausas <pausas at gmail.com> wrote:
> > Hi all,
> > I have a large file (1.8 GB) with 900,000 lines that I would like to read.
> > Each line is a string characters. Specifically I would like to randomly
> > select 3000 lines. For smaller files, what I'm doing is:
> >
> > trs <- scan("myfile", what= character(), sep = "\n")
> > trs<- trs[sample(length(trs), 3000)]
> >
> > And this works OK; however my computer seems not able to handle the 1.8 G
> > file.
> > I thought of an alternative way that not require to read the whole file:
> >
> > sel <- sample(1:900000, 3000)
> > for (i in 1:3000)  {
> > un <- scan("myfile", what= character(), sep = "\n", skip=sel[i], nlines=1)
> > write(un, "myfile_short", append=TRUE)
> > }
> >
> > This works on my computer; however it is extremely slow; it read one line
> > each time. It is been running for 25 hours and I think it has done less than
> > half of the file (Yes, probably I do not have a very good computer and I'm
> > working under Windows ...).
> > So my question is: do you know any other faster way to do this?
> > Thanks in advance
> >
> > Juli
> >
> > --
> > http://www.ceam.es/pausas
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From marc_schwartz at comcast.net  Fri Feb  2 19:32:15 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 02 Feb 2007 12:32:15 -0600
Subject: [R] reading very large files
In-Reply-To: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
Message-ID: <1170441135.6498.23.camel@localhost.localdomain>

On Fri, 2007-02-02 at 18:40 +0100, juli g. pausas wrote:
> Hi all,
> I have a large file (1.8 GB) with 900,000 lines that I would like to read.
> Each line is a string characters. Specifically I would like to randomly
> select 3000 lines. For smaller files, what I'm doing is:
> 
> trs <- scan("myfile", what= character(), sep = "\n")
> trs<- trs[sample(length(trs), 3000)]
> 
> And this works OK; however my computer seems not able to handle the 1.8 G
> file.
> I thought of an alternative way that not require to read the whole file:
> 
> sel <- sample(1:900000, 3000)
> for (i in 1:3000)  {
> un <- scan("myfile", what= character(), sep = "\n", skip=sel[i], nlines=1)
> write(un, "myfile_short", append=TRUE)
> }
> 
> This works on my computer; however it is extremely slow; it read one line
> each time. It is been running for 25 hours and I think it has done less than
> half of the file (Yes, probably I do not have a very good computer and I'm
> working under Windows ...).
> So my question is: do you know any other faster way to do this?
> Thanks in advance
> 
> Juli


Juli,

I don't have a file to test this on, so caveat emptor.

The problem with the approach above, is that you are re-reading the
source file, once per line, or 3000 times.  In addition, each read is
likely going through half the file on average to locate the randomly
selected line. Thus, the reality is that you are probably reading on the
order of:

> 3000 * 450000
[1] 1.35e+09

lines in the file, which of course if going to be quite slow.

In addition, you are also writing to the target file 3000 times.

The basic premise with this approach below, is that you are in effect
creating a sequential file cache in an R object. Reading large chunks of
the source file into the cache. Then randomly selecting rows within the
cache and then writing out the selected rows.

Thus, if you can read 100,000 rows at once, you would have 9 reads of
the source file, and 9 writes of the target file.

The key thing here is to ensure that the offsets within the cache and
the corresponding random row values are properly set.

Here's the code:

# Generate the random values
sel <- sample(1:900000, 3000)

# Set up a sequence for the cache chunks
# Presume you can read 100,000 rows at once
Cuts <- seq(0, 900000, 100000)

# Loop over the length of Cuts, less 1
for (i in seq(along = Cuts[-1]))
{
  # Get a 100,000 row chunk, skipping rows
  # as appropriate for each subsequent chunk
  Chunk <- scan("myfile", what = character(), sep = "\n", 
                 skip = Cuts[i], nlines = 100000)

  # set up a row sequence for the current 
  # chunk
  Rows <- (Cuts[i] + 1):(Cuts[i + 1])

  # Are any of the random values in the 
  # current chunk?
  Chunk.Sel <- sel[which(sel %in% Rows)]

  # If so, get them 
  if (length(Chunk.Sel) > 0)
  {
    Write.Rows <- Chunk[sel - Cuts[i]]

    # Now write them out
    write(Write.Rows, "myfile_short", append = TRUE)
  }
}


As noted, I have not tested this, so there may yet be additional ways to
save time with file seeks, etc.

HTH,

Marc Schwartz


From jholtman at gmail.com  Fri Feb  2 19:33:17 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 2 Feb 2007 13:33:17 -0500
Subject: [R] reading very large files
In-Reply-To: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
Message-ID: <644e1f320702021033v4a63e5b0nedc05433dd51dde3@mail.gmail.com>

I had a file with 200,000 lines in it and it took 1 second to select
3000 sample lines out of it.  One of the things is to use a connection
so that the file stays opens and then just 'skip' to the next record
to read:



> input <- file("/tempxx.txt", "r")
> sel <- 3000
> remaining <- 200000
> # get the records numbers to select
> recs <- sort(sample(1:remaining, sel))
> # compute number to skip on each read; account for the record just read
> skip <- diff(c(1, recs)) - 1
> # allocate my data
> mysel <- vector('character', sel)
> system.time({
+ for (i in 1:sel){
+     mysel[i] <- scan(input, what="", sep="\n", skip=skip[i], n=1, quiet=TRUE)
+ }
+ })
[1] 0.97 0.02 1.00   NA   NA
>
>


On 2/2/07, juli g. pausas <pausas at gmail.com> wrote:
> Hi all,
> I have a large file (1.8 GB) with 900,000 lines that I would like to read.
> Each line is a string characters. Specifically I would like to randomly
> select 3000 lines. For smaller files, what I'm doing is:
>
> trs <- scan("myfile", what= character(), sep = "\n")
> trs<- trs[sample(length(trs), 3000)]
>
> And this works OK; however my computer seems not able to handle the 1.8 G
> file.
> I thought of an alternative way that not require to read the whole file:
>
> sel <- sample(1:900000, 3000)
> for (i in 1:3000)  {
> un <- scan("myfile", what= character(), sep = "\n", skip=sel[i], nlines=1)
> write(un, "myfile_short", append=TRUE)
> }
>
> This works on my computer; however it is extremely slow; it read one line
> each time. It is been running for 25 hours and I think it has done less than
> half of the file (Yes, probably I do not have a very good computer and I'm
> working under Windows ...).
> So my question is: do you know any other faster way to do this?
> Thanks in advance
>
> Juli
>
> --
> http://www.ceam.es/pausas
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ripley at stats.ox.ac.uk  Fri Feb  2 19:34:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 18:34:58 +0000 (GMT)
Subject: [R] reading very large files
In-Reply-To: <59d7961d0702021019t58132bbeoaac2da2d6ea9d471@mail.gmail.com>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
	<59d7961d0702021019t58132bbeoaac2da2d6ea9d471@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702021832090.9145@gannet.stats.ox.ac.uk>

I suspect that reading from a connection in chunks of say 10,000 rows and 
discarding those you do not want would be simpler and at least as quick.
Not least because seek() on Windows is so unreliable.

On Fri, 2 Feb 2007, Henrik Bengtsson wrote:

> Hi.
>
> General idea:
>
> 1. Open your file as a connection, i.e. con <- file(pathname, open="r")
>
> 2. Generate a "row to (file offset, row length) map of your text file,
> i.e. a numeric vector 'fileOffsets' and 'rowLengths'.  Use readBin()
> for this. You build this up as you go by reading the file in chunks
> meaning you can handles files of any size.  You can store this lookup
> map to file for your future R sessions.
>
> 3. Sample a set of rows r = (r1, r2, ..., rR), i.e. rows =
> sample(length(fileOffsets)).
>
> 4. Look up the file offsets and row lengths for these rows, i.e.
> offsets = fileOffsets[rows].  lengths = rowLengths[rows].
>
> 5. In case your subset of rows is not ordered, it is wise to order
> them first to speed up things.  If order is important, keep track of
> the ordering and re-order them at then end.
>
> 6. For each row r, use seek(con=con, where=offsets[r]) to jump to the
> start of the row.  Use readBin(..., n=lengths[r]) to read the data.
>
> 7. Repeat from (3).
>
> /Henrik
>
> On 2/2/07, juli g. pausas <pausas at gmail.com> wrote:
>> Hi all,
>> I have a large file (1.8 GB) with 900,000 lines that I would like to read.
>> Each line is a string characters. Specifically I would like to randomly
>> select 3000 lines. For smaller files, what I'm doing is:
>>
>> trs <- scan("myfile", what= character(), sep = "\n")
>> trs<- trs[sample(length(trs), 3000)]
>>
>> And this works OK; however my computer seems not able to handle the 1.8 G
>> file.
>> I thought of an alternative way that not require to read the whole file:
>>
>> sel <- sample(1:900000, 3000)
>> for (i in 1:3000)  {
>> un <- scan("myfile", what= character(), sep = "\n", skip=sel[i], nlines=1)
>> write(un, "myfile_short", append=TRUE)
>> }
>>
>> This works on my computer; however it is extremely slow; it read one line
>> each time. It is been running for 25 hours and I think it has done less than
>> half of the file (Yes, probably I do not have a very good computer and I'm
>> working under Windows ...).
>> So my question is: do you know any other faster way to do this?
>> Thanks in advance
>>
>> Juli
>>
>> --
>> http://www.ceam.es/pausas
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Fri Feb  2 19:42:03 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 02 Feb 2007 12:42:03 -0600
Subject: [R] reading very large files
In-Reply-To: <1170441135.6498.23.camel@localhost.localdomain>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
	<1170441135.6498.23.camel@localhost.localdomain>
Message-ID: <1170441723.6498.28.camel@localhost.localdomain>

On Fri, 2007-02-02 at 12:32 -0600, Marc Schwartz wrote:
> On Fri, 2007-02-02 at 18:40 +0100, juli g. pausas wrote:
> > Hi all,
> > I have a large file (1.8 GB) with 900,000 lines that I would like to read.
> > Each line is a string characters. Specifically I would like to randomly
> > select 3000 lines. For smaller files, what I'm doing is:
> > 
> > trs <- scan("myfile", what= character(), sep = "\n")
> > trs<- trs[sample(length(trs), 3000)]
> > 
> > And this works OK; however my computer seems not able to handle the 1.8 G
> > file.
> > I thought of an alternative way that not require to read the whole file:
> > 
> > sel <- sample(1:900000, 3000)
> > for (i in 1:3000)  {
> > un <- scan("myfile", what= character(), sep = "\n", skip=sel[i], nlines=1)
> > write(un, "myfile_short", append=TRUE)
> > }
> > 
> > This works on my computer; however it is extremely slow; it read one line
> > each time. It is been running for 25 hours and I think it has done less than
> > half of the file (Yes, probably I do not have a very good computer and I'm
> > working under Windows ...).
> > So my question is: do you know any other faster way to do this?
> > Thanks in advance
> > 
> > Juli
> 
> 
> Juli,
> 
> I don't have a file to test this on, so caveat emptor.
> 
> The problem with the approach above, is that you are re-reading the
> source file, once per line, or 3000 times.  In addition, each read is
> likely going through half the file on average to locate the randomly
> selected line. Thus, the reality is that you are probably reading on the
> order of:
> 
> > 3000 * 450000
> [1] 1.35e+09
> 
> lines in the file, which of course if going to be quite slow.
> 
> In addition, you are also writing to the target file 3000 times.
> 
> The basic premise with this approach below, is that you are in effect
> creating a sequential file cache in an R object. Reading large chunks of
> the source file into the cache. Then randomly selecting rows within the
> cache and then writing out the selected rows.
> 
> Thus, if you can read 100,000 rows at once, you would have 9 reads of
> the source file, and 9 writes of the target file.
> 
> The key thing here is to ensure that the offsets within the cache and
> the corresponding random row values are properly set.
> 
> Here's the code:
> 
> # Generate the random values
> sel <- sample(1:900000, 3000)
> 
> # Set up a sequence for the cache chunks
> # Presume you can read 100,000 rows at once
> Cuts <- seq(0, 900000, 100000)
> 
> # Loop over the length of Cuts, less 1
> for (i in seq(along = Cuts[-1]))
> {
>   # Get a 100,000 row chunk, skipping rows
>   # as appropriate for each subsequent chunk
>   Chunk <- scan("myfile", what = character(), sep = "\n", 
>                  skip = Cuts[i], nlines = 100000)
> 
>   # set up a row sequence for the current 
>   # chunk
>   Rows <- (Cuts[i] + 1):(Cuts[i + 1])
> 
>   # Are any of the random values in the 
>   # current chunk?
>   Chunk.Sel <- sel[which(sel %in% Rows)]
> 
>   # If so, get them 
>   if (length(Chunk.Sel) > 0)
>   {
>     Write.Rows <- Chunk[sel - Cuts[i]]


Quick typo correction:

The last line above should be:

      Write.Rows <- Chunk[sel - Cuts[i], ]


>     # Now write them out
>     write(Write.Rows, "myfile_short", append = TRUE)
>   }
> }
> 
> 
> As noted, I have not tested this, so there may yet be additional ways to
> save time with file seeks, etc.

If that's the only error in the code...   :-)

Marc


From luke at stat.uiowa.edu  Fri Feb  2 19:48:47 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 2 Feb 2007 12:48:47 -0600 (CST)
Subject: [R] features of save and save.image (unexpected file sizes)
In-Reply-To: <Pine.LNX.4.64.0702021307200.26643@gannet.stats.ox.ac.uk>
References: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com> 
	<45C0DA8C.2020902@stats.ox.ac.uk>
	<e47808320701312334y5f181f15ic8fb952b756a0461@mail.gmail.com>
	<Pine.LNX.4.64.0702010747090.9746@gannet.stats.ox.ac.uk> 
	<e47808320702010530y1044427dw2815dda86a29ba3f@mail.gmail.com> 
	<Pine.LNX.4.64.0702020906500.19324@gannet.stats.ox.ac.uk>
	<e47808320702020433k4781ae2ci3bdcc7abc69d066c@mail.gmail.com>
	<Pine.LNX.4.64.0702021307200.26643@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0702021247160.10424@nokomis.stat.uiowa.edu>

On Fri, 2 Feb 2007, Prof Brian Ripley wrote:

> On Fri, 2 Feb 2007, Vaidotas Zemlys wrote:
>
>> Hi,
>>
>> On 2/2/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>
>>>> I found the culprit. I was parsing formulas in my code, and I saved
>>>> them in that large object. So the environment came with saved
>>>> formulas. Is there a nice way to say R: "please do not save the
>>>> environments with the formulas, I do not need them?"
>>>
>>> No, but why create them that way?  You could do
>>>
>>> mmodel <- as.formula(mmodel, env=.GlobalEnv)
>>>
>> Hm, but say I have some large object in .GlobalEnv, and I generate
>> mmodel  10 different times and save the result as a list with length
>> 10. Now if I try to save this list, R will save 10 different copies of
>> .GlobalEnv together with aforementioned large object?
>
> No, it saves the environment (here .GlobalEnv), not objects, and there can
> be many shared references.

Just to amplify this point: Only a marker representing .GlobalEnv is
saved; on load into a new session that marker becomes the .GlobalEnv
of the new session.

Best,

luke


>
>>> The R way is to create what you want, not fix up afterwards.
>>>
>>> (I find your code unreadable--spaces help a great deal, so am not sure if
>>> I have understood it correctly.)
>>>
>> Hm, I copied this code directly from Emacs+ESS, maybe the mailer
>> mangled something. What I want to do with this piece of code (I will
>> repaste it here)
>>
>> testf<- function(formula) {
>>   mainform <- formula
>>   if(deparse(mainform[[3]][[1]])!="|") stop("invalid conditioning")
>>   mmodel <- substitute(y~x,list(y=mainform[[2]],x=mainform[[3]][[2]]))
>>   mmodel <- as.formula(mmodel)
>>   list(formula=list(main=mmodel))
>> }
>
> You use no spaces around your operators or after commas.  R does when
> deparsing:
>
>> testf
> function (formula)
> {
>     mainform <- formula
>     if (deparse(mainform[[3]][[1]]) != "|")
>         stop("invalid conditioning")
>     mmodel <- substitute(y ~ x, list(y = mainform[[2]], x = mainform[[3]][[2]]))
>     mmodel <- as.formula(mmodel)
>     list(formula = list(main = mmodel))
> }
>
> because it is (at least to old hands) much easier to read.
>
> IcanreadEnglishtextwithoutanyspacesbutIchoosenotto.Similarly,Rcode.Occasional
> spacesare evenharderto parse.
>
>> is to read formula with condition:
>>
>> formula(y~x|z)
>>
>> and construct formula
>>
>> formula(y~x)
>>
>> I looked for examples in code of coplot in library graphics and
>> latticeParseFormula in library lattice.
>>
>> Vaidotas Zemlys
>> --
>> Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
>> Vilnius University
>>
>
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From dreamportrait at gmail.com  Fri Feb  2 19:50:26 2007
From: dreamportrait at gmail.com (Angie Hernandez)
Date: Fri, 02 Feb 2007 10:50:26 -0800
Subject: [R] Inquiry
Message-ID: <3436212538928161177684@VAIO>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070202/2b69e92f/attachment.pl 

From milton_ruser at yahoo.com.br  Fri Feb  2 19:54:44 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Fri, 2 Feb 2007 15:54:44 -0300 (ART)
Subject: [R] good brochure about matrix operations
Message-ID: <820572.73066.qm@web56608.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070202/910772a6/attachment.pl 

From jholtman at gmail.com  Fri Feb  2 19:55:55 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 2 Feb 2007 13:55:55 -0500
Subject: [R] Dealing with Duplicates - How to count instances?
In-Reply-To: <epvli8$778$1@sea.gmane.org>
References: <epvli8$778$1@sea.gmane.org>
Message-ID: <644e1f320702021055m3e63fc3ahdd5bcb54770df9e6@mail.gmail.com>

table(data[column])

will give you the number of items in each subgroup; that would be the
count you are after.

On 2/2/07, Johannes Graumann <johannes_graumann at web.de> wrote:
> Hi there,
>
> given a data.frame 'data' I managed to filter out entries (rows) that are
> identical with respect to one column like so:
>
> duplicity <- duplicated(data[column])
> data_unique <- subset(data,duplicity!=TRUE)
>
> But I'm trying to extract how many duplicates each of the remaining rows
> had.
>
> Can someone please send me down the right path for this?
>
> Joh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ripley at stats.ox.ac.uk  Fri Feb  2 19:55:59 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Feb 2007 18:55:59 +0000 (GMT)
Subject: [R] Make error for R-devel package under linux
In-Reply-To: <89E37888628D0244B25B4105152963CC368D41@mail01.sbri.org>
References: <89E37888628D0244B25B4105152963CC368D41@mail01.sbri.org>
Message-ID: <Pine.LNX.4.64.0702021835240.9145@gannet.stats.ox.ac.uk>

It is Feb 2 today: this was fixed 60 hours ago.

In any case, R-help is not the list to discuss any aspect of

R version 2.5.0 Under development (unstable) (2007-02-01 r40632)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
which cannot be expected to build in all locales and architectures at all 
times.  (This was a UTF-8 x 64-bit problem, and you omitted to tell us 
your locale, crucial here.)

'R-devel' is not a package: it is both a series of versions of R and a 
mailing to discuss R developments (amongst other things).

On Fri, 2 Feb 2007, Xinxia Peng wrote:

> Hi,
>
> I got an error while compiling the R-devel (R-2.5.0) package under
> Redhat Linux, but the R-patch package (R-2.4.1) was compiled perfectly
> fine. The following is what I got from the R-devel package downloaed on
> Jan. 30th., 2007:
>
>
> -----------------------------------------------------------
>> ./configure
>
> ....
>
> R is now configured for x86_64-unknown-linux-gnu
>
>  Source directory:          .
>  Installation directory:    /usr/local
>
>  C compiler:                gcc -std=gnu99  -g -O2
>  Fortran 77 compiler:       g77  -g -O2
>
>  C++ compiler:              g++  -g -O2
>  Fortran 90/95 compiler:    gfortran -g -O2
>  Obj-C compiler:
>
>  Interfaces supported:      X11
>  External libraries:        readline
>  Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
>  Options enabled:           shared BLAS, R profiling, Java
>
>  Recommended packages:      yes
>
> configure: WARNING: you cannot build DVI versions of the R manuals
> configure: WARNING: you cannot build PDF versions of the R manuals
>
>> make
>
> .....
>
> mkdir -p -- ../../../../library/tools/libs
> make[5]: Leaving directory
> `/nethome/xpeng/linux/tools/R-devel/src/library/tools/src'
> make[4]: Leaving directory
> `/nethome/xpeng/linux/tools/R-devel/src/library/tools/src'
> Error in loadNamespace(name) : there is no package called 'tools'
> In addition: Warning messages:
> 1: Line starting 'Package: tools ...' is malformed!
> 2: Line starting 'Version: 2.5.0 ...' is malformed!
> 3: Line starting 'Priority: base ...' is malformed!
> 4: Line starting 'Title: Tools for Pac ...' is malformed!
> 5: Line starting 'Author: Kurt Hornik  ...' is malformed!
> 6: Line starting 'Maintainer: R Core T ...' is malformed!
> 7: Line starting 'Description: Tools f ...' is malformed!
> 8: Line starting 'License: GPL Version ...' is malformed!
> 9: Line starting 'Built: R 2.5.0; x86_ ...' is malformed!
> Execution halted
> make[3]: *** [all] Error 1
> make[3]: Leaving directory
> `/nethome/xpeng/linux/tools/R-devel/src/library/tools'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory
> `/nethome/xpeng/linux/tools/R-devel/src/library'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/nethome/xpeng/linux/tools/R-devel/src'
> make: *** [R] Error 1
>
> -----------------------------------------------------------
>
> I got the same error after I went into the directoy
> 'R-devel/src/library/tools' and run 'make' again. Any suggestions?
>
> Thanks,
> Xinxia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Achim.Zeileis at wu-wien.ac.at  Fri Feb  2 20:00:14 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 2 Feb 2007 20:00:14 +0100 (CET)
Subject: [R] Problem with party and ordered factors
In-Reply-To: <26942.62.180.31.65.1170430874.squirrel@home.in.tum.de>
Message-ID: <Pine.LNX.4.44.0702021951530.10150-100000@disco.wu-wien.ac.at>

Christoph:

> I've got an ordered factor as response. As far as i know i have to use
> scores to be able to use this ordered factor.

If you want to exploit the ordering in the statistical tests (used for
variable selection in CTree), a natural approach is to use a
linear-by-linear test with scores assigned to the ordered levels of your
factor. That's what the example below does.

> But if i do so i get a tree
> which predicts all observations as the first level of my ordered factor.

That is not due to the factor being ordered. It results simply from the
fact that more than half of the observations have "Never" in the variable
ME.

> There i got the same problem. I execute the following code:
> > data("mammoexp", package = "party")
> > mtree <- ctree(ME ~ ., data = mammoexp, scores = list(ME = 1:3, SYMPT =
> 1:4, DECT = 1:3))
> > plot(mtree)

If you look at this picture, you can see that majority voting in each node
will result in the prediction "Never".

> So now i'm stuck. Am i doing anything wrong?

Nothing.
If you want to see how the distribution in each node changes, you can
look at
  treeresponse(mtree)

> I'm using R 2.4.1 and all packages are uptodate.

Not anymore, I just uploaded a new "party" version to CRAN ;-))

Best wishes,
Z


From xinxia.peng at sbri.org  Fri Feb  2 20:03:56 2007
From: xinxia.peng at sbri.org (Xinxia Peng)
Date: Fri, 2 Feb 2007 11:03:56 -0800
Subject: [R] Make error for R-devel package under linux
Message-ID: <89E37888628D0244B25B4105152963CC368D45@mail01.sbri.org>

Thanks a lot for the quick help. That's the first message I sent since I
joined the list last night. :)

Best,
Xinxia


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Friday, February 02, 2007 10:56 AM
To: Xinxia Peng
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Make error for R-devel package under linux

It is Feb 2 today: this was fixed 60 hours ago.

In any case, R-help is not the list to discuss any aspect of

R version 2.5.0 Under development (unstable) (2007-02-01 r40632)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ which cannot be expected
to build in all locales and architectures at all times.  (This was a
UTF-8 x 64-bit problem, and you omitted to tell us your locale, crucial
here.)

'R-devel' is not a package: it is both a series of versions of R and a
mailing to discuss R developments (amongst other things).

On Fri, 2 Feb 2007, Xinxia Peng wrote:

> Hi,
>
> I got an error while compiling the R-devel (R-2.5.0) package under 
> Redhat Linux, but the R-patch package (R-2.4.1) was compiled perfectly

> fine. The following is what I got from the R-devel package downloaed 
> on Jan. 30th., 2007:
>
>
> -----------------------------------------------------------
>> ./configure
>
> ....
>
> R is now configured for x86_64-unknown-linux-gnu
>
>  Source directory:          .
>  Installation directory:    /usr/local
>
>  C compiler:                gcc -std=gnu99  -g -O2
>  Fortran 77 compiler:       g77  -g -O2
>
>  C++ compiler:              g++  -g -O2
>  Fortran 90/95 compiler:    gfortran -g -O2
>  Obj-C compiler:
>
>  Interfaces supported:      X11
>  External libraries:        readline
>  Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
>  Options enabled:           shared BLAS, R profiling, Java
>
>  Recommended packages:      yes
>
> configure: WARNING: you cannot build DVI versions of the R manuals
> configure: WARNING: you cannot build PDF versions of the R manuals
>
>> make
>
> .....
>
> mkdir -p -- ../../../../library/tools/libs
> make[5]: Leaving directory
> `/nethome/xpeng/linux/tools/R-devel/src/library/tools/src'
> make[4]: Leaving directory
> `/nethome/xpeng/linux/tools/R-devel/src/library/tools/src'
> Error in loadNamespace(name) : there is no package called 'tools'
> In addition: Warning messages:
> 1: Line starting 'Package: tools ...' is malformed!
> 2: Line starting 'Version: 2.5.0 ...' is malformed!
> 3: Line starting 'Priority: base ...' is malformed!
> 4: Line starting 'Title: Tools for Pac ...' is malformed!
> 5: Line starting 'Author: Kurt Hornik  ...' is malformed!
> 6: Line starting 'Maintainer: R Core T ...' is malformed!
> 7: Line starting 'Description: Tools f ...' is malformed!
> 8: Line starting 'License: GPL Version ...' is malformed!
> 9: Line starting 'Built: R 2.5.0; x86_ ...' is malformed!
> Execution halted
> make[3]: *** [all] Error 1
> make[3]: Leaving directory
> `/nethome/xpeng/linux/tools/R-devel/src/library/tools'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory
> `/nethome/xpeng/linux/tools/R-devel/src/library'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/nethome/xpeng/linux/tools/R-devel/src'
> make: *** [R] Error 1
>
> -----------------------------------------------------------
>
> I got the same error after I went into the directoy 
> 'R-devel/src/library/tools' and run 'make' again. Any suggestions?
>
> Thanks,
> Xinxia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Fri Feb  2 20:04:04 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 02 Feb 2007 13:04:04 -0600
Subject: [R] reading very large files
In-Reply-To: <1170441723.6498.28.camel@localhost.localdomain>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
	<1170441135.6498.23.camel@localhost.localdomain>
	<1170441723.6498.28.camel@localhost.localdomain>
Message-ID: <1170443044.6498.35.camel@localhost.localdomain>

On Fri, 2007-02-02 at 12:42 -0600, Marc Schwartz wrote:
> On Fri, 2007-02-02 at 12:32 -0600, Marc Schwartz wrote:

> > Juli,
> > 
> > I don't have a file to test this on, so caveat emptor.
> > 
> > The problem with the approach above, is that you are re-reading the
> > source file, once per line, or 3000 times.  In addition, each read is
> > likely going through half the file on average to locate the randomly
> > selected line. Thus, the reality is that you are probably reading on the
> > order of:
> > 
> > > 3000 * 450000
> > [1] 1.35e+09
> > 
> > lines in the file, which of course if going to be quite slow.
> > 
> > In addition, you are also writing to the target file 3000 times.
> > 
> > The basic premise with this approach below, is that you are in effect
> > creating a sequential file cache in an R object. Reading large chunks of
> > the source file into the cache. Then randomly selecting rows within the
> > cache and then writing out the selected rows.
> > 
> > Thus, if you can read 100,000 rows at once, you would have 9 reads of
> > the source file, and 9 writes of the target file.
> > 
> > The key thing here is to ensure that the offsets within the cache and
> > the corresponding random row values are properly set.
> > 
> > Here's the code:
> > 
> > # Generate the random values
> > sel <- sample(1:900000, 3000)
> > 
> > # Set up a sequence for the cache chunks
> > # Presume you can read 100,000 rows at once
> > Cuts <- seq(0, 900000, 100000)
> > 
> > # Loop over the length of Cuts, less 1
> > for (i in seq(along = Cuts[-1]))
> > {
> >   # Get a 100,000 row chunk, skipping rows
> >   # as appropriate for each subsequent chunk
> >   Chunk <- scan("myfile", what = character(), sep = "\n", 
> >                  skip = Cuts[i], nlines = 100000)
> > 
> >   # set up a row sequence for the current 
> >   # chunk
> >   Rows <- (Cuts[i] + 1):(Cuts[i + 1])
> > 
> >   # Are any of the random values in the 
> >   # current chunk?
> >   Chunk.Sel <- sel[which(sel %in% Rows)]
> > 
> >   # If so, get them 
> >   if (length(Chunk.Sel) > 0)
> >   {
> >     Write.Rows <- Chunk[sel - Cuts[i]]
> 
> 
> Quick typo correction:
> 
> The last line above should be:
> 
>       Write.Rows <- Chunk[sel - Cuts[i], ]
> 
> 
> >     # Now write them out
> >     write(Write.Rows, "myfile_short", append = TRUE)
> >   }
> > }
> > 

OK, I knew it was too good to be true...

One more correction on that same line:

   Write.Rows <- Chunk[Chunk.Sel - Cuts[i], ]


For clarity, here is the full set of code:

# Generate the random values
sel <- sample(900000, 3000)

# Set up a sequence for the cache chunks
# Presume you can read 100,000 rows at once
Cuts <- seq(0, 900000, 100000)

# Loop over the length of Cuts, less 1
for (i in seq(along = Cuts[-1]))
{
  # Get a 100,000 row chunk, skipping rows
  # as appropriate for each subsequent chunk
  Chunk <- scan("myfile", what = character(), sep = "\n", 
                 skip = Cuts[i], nlines = 100000)

  # set up a row sequence for the current 
  # chunk
  Rows <- (Cuts[i] + 1):(Cuts[i + 1])

  # Are any of the random values in the 
  # current chunk?
  Chunk.Sel <- sel[which(sel %in% Rows)]

  # If so, get them 
  if (length(Chunk.Sel) > 0)
  {
    Write.Rows <- Chunk[Chunk.Sel - Cuts[i], ]

    # Now write them out
    write(Write.Rows, "myfile_short", append = TRUE)
  }
}


Regards,

Marc


From b.rowlingson at lancaster.ac.uk  Fri Feb  2 20:07:25 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 02 Feb 2007 19:07:25 +0000
Subject: [R] Inquiry
In-Reply-To: <3436212538928161177684@VAIO>
References: <3436212538928161177684@VAIO>
Message-ID: <45C38BED.1040403@lancaster.ac.uk>

Angie Hernandez wrote:

> I came across your website and thought it would be a great resource
> to have listed on my friends page.  Would you be interested in
> exchanging links with my new site?

  Well, I don't see why we cant make CRAN more like MySpace?

Barry

[joke]


From rlevy at ucsd.edu  Fri Feb  2 20:18:40 2007
From: rlevy at ucsd.edu (Roger Levy)
Date: Fri, 02 Feb 2007 11:18:40 -0800
Subject: [R] multinomial logistic regression with equality constraints?
Message-ID: <45C38E90.9070709@ucsd.edu>

I'm interested in doing multinomial logistic regression with equality 
constraints on some of the parameter values.  For example, with 
categorical outcomes Y_1 (baseline), Y_2, and Y_3, and covariates X_1 
and X_2, I might want to impose the equality constraint that

   \beta_{2,1} = \beta_{3,2}

that is, that the effect of X_1 on the logit of Y_2 is the same as the 
effect of X_2 on the logit of Y_3.

Is there an existing facility or package in R for doing this?  Would 
multinomRob fit the bill?

Many thanks,

Roger


-- 

Roger Levy                      Email: rlevy at ucsd.edu
Assistant Professor             Phone: 858-534-7219
Department of Linguistics       Fax:   858-534-4789
UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy


From ggrothendieck at gmail.com  Fri Feb  2 20:21:36 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Feb 2007 14:21:36 -0500
Subject: [R] good brochure about matrix operations
In-Reply-To: <820572.73066.qm@web56608.mail.re3.yahoo.com>
References: <820572.73066.qm@web56608.mail.re3.yahoo.com>
Message-ID: <971536df0702021121i76d607d1u8ec6a021d0906ea1@mail.gmail.com>

There are some reference cards and other contributed documents
here:
http://cran.r-project.org/other-docs.html

On 2/2/07, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
> Hi there,
>
>  I would like if is there a good PDF available about matrix operations in R.
>
>  Kind regards,
>
>  miltinho
>  Brazil
>
>  __________________________________________________
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mcnpf748 at mncn.csic.es  Fri Feb  2 18:57:05 2007
From: mcnpf748 at mncn.csic.es (mcnpf748 at mncn.csic.es)
Date: Fri, 02 Feb 2007 17:57:05 +0000
Subject: [R] Access to column names stored in a vector in lm procedure
Message-ID: <20070202175705.A5B669A1B35@ciruelo2.csic.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070202/eca4a12e/attachment.pl 

From robert.robinson at maine.edu  Fri Feb  2 17:23:27 2007
From: robert.robinson at maine.edu (robert.robinson at maine.edu)
Date: Fri,  2 Feb 2007 16:23:27 +0000
Subject: [R] Snow Package and R: Exported Variable Problem
Message-ID: <20070202162327.4hmordptwks04kco@mail1.maine.edu>

Hello and thanks in advance for your time.

I've created a simulation on my cluster which uses a custom package  
developed by me for different functions and also the snow package.   
Right now I'm using LAM to communicate between nodes and am currently  
only testing my code on 3 nodes for simplicity, though I plan on  
expanding to 16 later.  My problem is this error:

"Error in fn(par, ..) : object \"x1\" not found \n"
attr(,"class")
"try-error"

In my simulation I need to run a function several times with an  
different variable each time.  All the invocations on the functions  
are independent of the others.  I start the simulation on one node,  
create a cluster of several nodes, load my custom package and snow on  
all of them, use  clusterExport(cl, "x1") to export the variable  
x1(among other variable I need), then I call my simulation on the  
cluster using clusterApplyLB(cl, 2:S, simClust)  where cl is the  
cluster and S is a constant defined above as 500.  Using print  
statements (since snow, or R for that matter, has next to no ability  
to debug) I found that the error cropped up in this statement:

theta6 = optim(c(0,0,0,0,0,0,.2), loglikelihood, scrore6, method =  
"CG", control=list(fnscale=-1,reltol=1e-8,maxit=2000))$par

Both the functions loglikelelihood and score6 use x1, but I know that  
it is getting exported to the node correctly since it gets assigned  
earlier in the simulation:

x1 = rep(0,n1)

The error I stated above happens fo every itteration of the simulation  
(499 times) and I'm really at a loss as to why its happening and what  
I can do to determine what it is.  I'm wondering at this point if  
exporting the variable makes it unavailable to certain other packages,  
though that doesn't really make any sense.

If anyone can help me with this problem, or let me know how I can  
debug this, or even a clue as to why it might be happening I would  
greatly appreciate it.  I've been wrestling with this for some time  
and no online documentation can help.  Thank you for your time and help.

Just so you know I'm a Computer Scientist not a Statistician, though I  
will be able to give any information about the statistics involved in  
this program.  I am reluctant to give away all source code since it is  
not my work but rather code I'm converting from standard code to  
parallelized code for a professor of mine.


From chrysopa at gmail.com  Fri Feb  2 21:02:22 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Fri, 2 Feb 2007 18:02:22 -0200
Subject: [R] problem with survreg and anova function
Message-ID: <200702021802.22285.chrysopa@gmail.com>

Hi,

I make a weibull survival regression using suvreg function. Bu when I try to 
get the P values from anova, it give me NAs:

I'm using R version 2.4.0 Patched (2006-11-25 r39997) and  survival 2.30 
library

Look:

m <- survreg(Surv(tempo,censor)~grupo*peso)
 anova(m)
           Df   Deviance Resid. Df    -2*LL P(>|Chi|)
NULL       NA         NA       148 966.6416        NA
grupo      -2 25.6334407       146 941.0081        NA
peso       -1  0.7088892       145 940.2992        NA
grupo:peso -2  1.0731248       143 939.2261        NA

Older version of survival dont have this problem, look this printed result:

> anova(glex8.m1)
                                        -2LL
            Df    Deviance Resid. Df             P(>|Chi|)
NULL       NA          NA         2 966.6416           NA
grupo       2 25.6334408          4 941.0081 2.714995e-06
peso        1 0.7088892           5 940.2992 3.998128e-01
grupo:peso 2 1.0731248            7 939.2261 5.847550e-01

This a bug or a change in package?

The data from crawley book:

structure(list(tempo = c(20, 34, 1, 2, 3, 3, 50, 26, 1, 50, 21, 
3, 13, 11, 22, 50, 50, 1, 50, 9, 50, 1, 13, 50, 50, 1, 6, 50, 
50, 50, 36, 3, 46, 10, 50, 1, 18, 3, 36, 37, 50, 7, 1, 1, 7, 
24, 4, 50, 12, 17, 1, 1, 1, 21, 50, 50, 1, 46, 50, 1, 8, 2, 12, 
3, 2, 1, 5, 50, 1, 2, 2, 4, 17, 5, 1, 11, 8, 1, 5, 2, 41, 5, 
21, 1, 38, 50, 3, 19, 4, 7, 1, 46, 2, 5, 40, 4, 50, 2, 1, 17, 
7, 1, 5, 1, 1, 5, 6, 2, 24, 1, 1, 1, 1, 7, 13, 6, 11, 46, 5, 
14, 2, 1, 20, 2, 20, 1, 23, 11, 1, 1, 20, 9, 1, 1, 1, 1, 7, 11, 
1, 3, 1, 5, 9, 21, 10, 11, 30, 1, 1, 17), censor = c(1, 1, 1, 
1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 
0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 
1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 
1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
), peso = c(5.385, 7.413, 9.266, 6.228, 5.229, 9.699, 1.973, 
5.838, 2.088, 0.237, 6.814, 5.502, 1.137, 6.323, 7.384, 8.713, 
7.458, 1.424, 1.312, 5.162, 7.187, 4.677, 6.548, 5.903, 2.113, 
7.617, 3.737, 8.972, 6.523, 2.165, 4.895, 6.538, 1.674, 6.726, 
2.671, 4.949, 4.819, 5.08, 3.532, 4.406, 6.286, 5.529, 2.27, 
5.245, 9.675, 5.61, 4.297, 3.179, 6.776, 0.466, 0.626, 1.221, 
0.124, 0.32, 2.282, 0.287, 3.468, 7.314, 4.901, 5.418, 6.344, 
1.163, 12.126, 11.561, 8.333, 0.055, 10.583, 9.534, 13.182, 10.156, 
16.881, 15.452, 16.831, 18.947, 19.099, 19, 9.652, 1.544, 10.786, 
4.13, 2.2, 7.567, 14.581, 26.259, 0.44, 18.188, 6.789, 16.669, 
38.177, 29.154, 14.578, 1.569, 0.345, 33.929, 28.958, 38.139, 
26.822, 39.501, 9.264, 22.88, 27.48, 35.069, 4.974, 41.521, 42.09, 
25.037, 9.509, 23.682, 0.352, 19.589, 7.426, 7.913, 2.37, 5.533, 
18.8, 18.508, 3.343, 26.926, 2.388, 21.567, 5.594, 17.15, 15.986, 
1.588, 2.055, 16.074, 12.086, 20.524, 6.493, 7.258, 16.635, 10.324, 
5.228, 0.784, 5.587, 5.011, 7.441, 3.69, 4.708, 9.207, 1.4, 6.309, 
1.784, 0.767, 1.993, 1.03, 2.875, 1.82, 0.974, 0.1), grupo = structure(c(1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
3, 3), .Label = c("g1", "g2", "g3"), class = "factor")), .Names = c("tempo", 
"censor", "peso", "grupo"), class = "data.frame", row.names = c("1", 
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", 
"25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", 
"36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", 
"47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", 
"58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68", 
"69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79", 
"80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90", 
"91", "92", "93", "94", "95", "96", "97", "98", "99", "100", 
"101", "102", "103", "104", "105", "106", "107", "108", "109", 
"110", "111", "112", "113", "114", "115", "116", "117", "118", 
"119", "120", "121", "122", "123", "124", "125", "126", "127", 
"128", "129", "130", "131", "132", "133", "134", "135", "136", 
"137", "138", "139", "140", "141", "142", "143", "144", "145", 
"146", "147", "148", "149", "150"))

Thanks
Inte
Ronaldo
-- 
O cigarro disse ao fumante: Hoje voc? me acende, amanh? eu te apago.
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From topkatz at msn.com  Fri Feb  2 21:03:32 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 02 Feb 2007 15:03:32 -0500
Subject: [R] Another loop - deloop question
Message-ID: <BAY132-F27D3EEC7A1AA614A19A928AA9B0@phx.gbl>

Hi.

You folks are so clever, I thought perhaps you could help me make another 
procedure more efficient.

Right now I have the following setup:

p is a vector of length m
g is a list of length n, g[[i]] is a vector whose elements are indices of p, 
i.e., integers between 1 and m inclusive); the g[[i]] cover the full set 
1:m, but they don't have to constitute an exact partition, theycan overlap 
members.
y is another list of length n, each y[[i]] is a vector of the same length as 
g[[i]].

Now I build up the vector p as follows:

p=rep(0,m)
for(i in 1:n){p[g[[i]]]=p[g[[i]]]+y[[i]]}

Can this loop be vectorized?

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From johannes_graumann at web.de  Fri Feb  2 21:37:42 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 02 Feb 2007 21:37:42 +0100
Subject: [R] Dealing with Duplicates - How to count instances?
References: <epvli8$778$1@sea.gmane.org>
	<644e1f320702021055m3e63fc3ahdd5bcb54770df9e6@mail.gmail.com>
Message-ID: <eq07em$b2v$1@sea.gmane.org>

jim holtman wrote:

> table(data[column])
> 
> will give you the number of items in each subgroup; that would be the
> count you are after.

Thanks for your Help! That rocks! I can do 

copynum <- table(data_6plus["Accession.number"])
data_6plus$"Repeats" <- sapply(data_6plus[["Accession.number"]], function(x)         
   copynum[x][[1]])

now!

But how about this:
- do something along the lines of 

duplicity <- duplicated(data_6plus["Accession.number"])
data_6plus_unique <- subset(data_6plus,duplicity!=TRUE)

- BUT: retain from each deleted row one field, append it to a vector and
fill that into a new field of the remaining row of the set sharing
data_6plus["Accession.number"]?

How would you do something like that?

Joh


From mkimpel at iupui.edu  Fri Feb  2 23:00:19 2007
From: mkimpel at iupui.edu (Mark W Kimpel)
Date: Fri, 02 Feb 2007 17:00:19 -0500
Subject: [R] [BioC] Outlook does threading  [Broadcast]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03A20C97@usctmx1106.merck.com>
References: <836F00680EECD340A96AD34ECFF3B534B4AD10@iu-mssg-mbx106.ads.iu.edu>
	<002a01c74594$5ce58310$4d908980@gne.windows.gene.com>
	<836F00680EECD340A96AD34ECFF3B534B4AD1B@iu-mssg-mbx106.ads.iu.edu>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03A20C97@usctmx1106.merck.com>
Message-ID: <45C3B473.7000103@iupui.edu>

To close out this thread... I have solved my problem with Outlook not 
displaying threads by creating a Gmail account for all my R and BioC 
needs and am viewing that with Mozilla Thunderbird. Seems to be working 
nicely and I now have the benefit of viewing by threads like to many of 
you have been doing all along. I continue to use Outlook for my other 
needs. I thought I would share this in case other Outlook users search 
the archives and wonder how the problem was ever solved.

Thanks again,

Mark Kimpel
IU School of Medicine

Liaw, Andy wrote:
> This is really off-topic for both BioC and R-help, so I'll 
> keep it short. 
>
>
> From: Kimpel, Mark William
>   
>> See below for Bert Gunter's off list reply to me (which I do 
>> appreciate). I'm putting it back on the list because it seems 
>> there is still confusion regarding the difference between 
>> threading and sorting by subject. I thought the example I 
>> will give below will serve as instructional for other Outlook 
>> users who may be similarly confused as I was (am?). 
>>
>> Per Bert's instructions, I just set up my inbox to sort by 
>> subject. I sent one email to myself with the subject "test1" 
>> and then replied to it without changing the subject. The 
>> reply correctly went to "test1" in the inbox sorter. I then 
>> changed the subject heading in the test1 reply to "test2" and 
>> sent it to myself. This time Outlook re-categorized it and 
>> put it in a separate compartment in the view called "test2".
>>
>> If Outlook can do threading the way the R mail server does, I 
>> don't think this is the way to do it.
>>     
>
> AFAIK there's no proper way to get the correct threading in 
> Outlook.  What I do is group by conversation topic, but that
> doesn't solve the problem.  This is only problem on your
> (and all Outlook users'?) end, though.  The bigger problem
> that affects the lists is that some versions of MS Exchange 
> Server do not include the "In-reply-to" header field that
> many mailing lists rely on for proper threading.  As a result,
> when I reply to other people's post, it may show up in Outlook
> as having been threaded properly (because the subject is fine),
> but it throws everything else that does proper threading off.
>  
>   
>> Unless someone has an idea of how to correctly set up Outlook 
>> to do threading in the manner that the R mail server does,
>>     
>
> Maybe some VBA coding can be done to get it right, but short
> of that, I very much doubt it.
>
>   
>> I 
>> think the message for us Outlook users is to just create, 
>> from scratch, a new message when initiating a new subject.
>>     
>
> That message ought to be clear for everyone.  You should
> never reply to a message when you really mean to start
> a new topic, regardless what you are using.
>
> Andy
>  
>   
>> Thanks for all your help. 
>>
>> Mark
>>
>> -----Original Message-----
>> From: Bert Gunter [mailto:gunter.berton at gene.com]
>> Sent: Wednesday, January 31, 2007 7:03 PM
>> To: Kimpel, Mark William
>> Subject: Outlook does threading
>>
>>  Mark:
>>
>> No need to bother the R list with this. Outlook does 
>> threading. Just sort on Subject in the viewer.
>>
>> Bert Gunter
>> Genentech Nonclinical Statistics
>> South San Francisco, CA 94404
>> 650-467-7374
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>> Kimpel, Mark William
>> Sent: Wednesday, January 31, 2007 3:36 PM
>> To: Peter Dalgaard
>> Cc: r-help at stat.math.ethz.ch; bioconductor at stat.math.ethz.ch
>> Subject: Re: [R] possible spam alert
>>
>> Peter,
>>
>> Thanks you for your explanation, I had taken Mr. Connolly's 
>> message to me to imply that I was not changing the subject 
>> line. I use MS Outlook
>> 2007 and, unless I am just not seeing it, Outlook does not 
>> normally display the "in reply to" header, I was under the 
>> mistaken impression that that was what the Subject line was 
>> for. See, for example, the header to your message to me 
>> below. Outlook will, however, sort messages by Subject, and 
>> that is what I thought was meant by threading.
>>
>> Well, I learned something today and apologize for any 
>> inconvenience my posts may have caused.
>>
>> BTW, I use Outlook because it is supported by my university 
>> server and will synch my appointments and contacts with my 
>> PDA, which runs Windows CE. If anyone has a suggestion for me 
>> of a better email program that will provide proper threading 
>> AND work with a MS email server and synch with Windows CE, 
>> I'd love to hear it.
>>
>> Thanks again,
>>
>> Mark
>>
>> Mark W. Kimpel MD 
>>
>>  
>>
>> (317) 490-5129 Work, & Mobile
>>
>>  
>>
>> (317) 663-0513 Home (no voice mail please)
>>
>> 1-(317)-536-2730 FAX
>>
>>
>> -----Original Message-----
>> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
>> Sent: Wednesday, January 31, 2007 6:25 PM
>> To: Kimpel, Mark William
>> Cc: bioconductor at stat.math.ethz.ch; r-help at stat.math.ethz.ch
>> Subject: Re: [R] possible spam alert
>>
>> Kimpel, Mark William wrote:
>>     
>>> The last two times I have originated message threads on R or 
>>> Bioconductor I have received the message included below 
>>>       
>> from someone 
>>     
>>> named Patrick Connolly. Both times I was the originator of 
>>>       
>> the message 
>>     
>>> thread and used what I thought was a unique subject line that
>>>       
>> explained
>>     
>>> as best I could what my question was. Patrick seems to be implying
>>>       
>> that
>>     
>>> I am abusing the R and BioC help newsgroups in this fashion. 
>>>
>>> When I emailed him to give me a specific example, he did not reply.
>>>       
>> The
>>     
>>> most recent thread that he seems concerned about was to the 
>>>       
>> R list and 
>>     
>>> was entitled "regexpr and parsing question" . I believe the 
>>>       
>> previous 
>>     
>>> post of mine that he had problems with was to the BioC list but I
>>>       
>> can't
>>     
>>> remember its subject.
>>>
>>> Is this spam?
>>>   
>>>       
>> No. Breach of netiquette, yes.
>>
>> The message in question starts a new thread, yet contains an
>> In-Reply-To: header line, which presumably means that you 
>> started writing the message as a reply to something 
>> completely unrelated,
>> specifically: "Re: [R] change plotting symbol for groups in 
>> trellis graph". You should not do that, unless you know how 
>> to remove the In-Reply-To line (and this is not obvious in 
>> many mail clients); changing the subject is not sufficient.
>>     
>>> If I am doing this correctly, you should see the subject "possible
>>>       
>> spam
>>     
>>> alert" in the subject header of THIS message.
>>>
>>> Would the moderators of the lists please check and see if I 
>>>       
>> am doing 
>>     
>>> some wrong and, if not, inform Mr. Connolly that I am not. 
>>>       
>> If others 
>>     
>>> have received this message in error, it is possible it is spam and
>>>       
>> users
>>     
>>> should be alerted.
>>>
>>> Thanks,
>>>
>>> Mark
>>>
>>> Mark W. Kimpel MD
>>>
>>>  
>>>
>>>  
>>>
>>> Official Business Address:
>>>
>>>  
>>>
>>> Department of Psychiatry
>>>
>>> Indiana University School of Medicine
>>>
>>> PR M116
>>>
>>> Institute of Psychiatric Research
>>>
>>> 791 Union Drive
>>>
>>> Indianapolis, IN 46202
>>>
>>>  
>>> This is a request to anyone who starts a new subject to begin with a
>>>       
>> new
>>     
>>> message and NOT reply to an existing one.  If your mail 
>>>       
>> client is any 
>>     
>>> good, it's very simple to set up an alias (mine is simply 
>>>       
>> 'r') so that 
>>     
>>> the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary
>>>       
>> and
>>     
>>> it's quicker than scrolling through an address book.
>>> It's also quicker than deleting the previous subject.
>>>
>>> Most mornings, I have over a screenful of messages mostly 
>>>       
>> from R-help 
>>     
>>> and it's very useful to have them threaded.  However, the usefulness
>>>       
>> of
>>     
>>> threading is lost when posters reply to a message and then 
>>>       
>> change the 
>>     
>>> subject instead of creating a new message.
>>>
>>> People who don't have a mail client that can display email 
>>>       
>> in threads 
>>     
>>> are probably unaware that this sort of thing can happen in ones that
>>>       
>> do:
>>     
>>>     37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
>>>     38 N   25 Jan Uwe Ligges              ( 55) `-> 
>>>     39 N   25 Jan Fernando Henrique Ferra ( 20) [R] 
>>>       
>> Plotting coloured
>>     
>>> histograms
>>> ->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] 
>>>       
>> Distributions.
>>     
>>>     41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
>>>     42     26 Jan Qin Xin                 (  9) | `->[R] how could I
>>>       
>> add
>>     
>>> legends
>>>     43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
>>>     44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
>>> coloured his
>>>     45 N   26 Jan Damon Wischik           (125) `-> 
>>>     46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting
>>>       
>> primatives,
>>     
>>> ellipse
>>>     47 N   25 Jan Uwe Ligges              ( 19) `->   
>>>
>>>
>>> As Martin Maechler explained some time ago, it also screws up the 
>>> archives for a similar reason.
>>>
>>> Your cooperation will be greatly appreciated.
>>>
>>> best
>>>
>>>
>>>       
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> _______________________________________________
>> Bioconductor mailing list
>> Bioconductor at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/bioconductor
>> Search the archives: 
>> http://news.gmane.org/gmane.science.biology.informatics.conductor
>>
>>
>>
>>     
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From aiminy at iastate.edu  Fri Feb  2 23:02:56 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 02 Feb 2007 16:02:56 -0600
Subject: [R] CGIwithR
Message-ID: <6.1.2.0.2.20070202155914.01cb4e58@aiminy.mail.iastate.edu>

I try a example
http://omega.psi.iastate.edu/bootstrapFile.html

it doesn't give me output. I don't why.

For another example

http://omega.psi.iastate.edu/trivial.html

it seems works except it doesn't display figure.

Does anyone know how to figure it out?

Aimin


From jzelner at umich.edu  Fri Feb  2 23:06:44 2007
From: jzelner at umich.edu (Jonathan Zelner)
Date: Fri, 2 Feb 2007 17:06:44 -0500 (EST)
Subject: [R] Adding Histograms to Leaves of Rpart Tree or other Dendrogram
Message-ID: <Pine.LNX.4.64.0702021702070.21687@rygar.gpcc.itd.umich.edu>

Hi - I'm trying to append simple density histograms of a continuous 
variable to the leaves of an rpart tree.

The splits in the tree are all levels of a factor and I'm hoping to make 
the histograms out of the subsets of the dataframe corresponding to the 
splits and for them to be attached to the appropriate leaf of the final 
tree.

Any help would be much appreciated,
thanks,
Jon Zelner

University of Michigan
Gerald R. Ford School of Public Policy


From christos at nuverabio.com  Fri Feb  2 23:19:11 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Fri, 2 Feb 2007 17:19:11 -0500
Subject: [R] Adding Histograms to Leaves of Rpart Tree or other
	Dendrogram
In-Reply-To: <Pine.LNX.4.64.0702021702070.21687@rygar.gpcc.itd.umich.edu>
References: <Pine.LNX.4.64.0702021702070.21687@rygar.gpcc.itd.umich.edu>
Message-ID: <005a01c74718$2b3a80b0$0e010a0a@headquarters.silicoinsights>

Hi Jon,

Take a look at this graph

http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=85

I think it is very close to what you need (source code provided at the
site).

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 
  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathan Zelner
> Sent: Friday, February 02, 2007 5:07 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Adding Histograms to Leaves of Rpart Tree or 
> other Dendrogram
> 
> Hi - I'm trying to append simple density histograms of a 
> continuous variable to the leaves of an rpart tree.
> 
> The splits in the tree are all levels of a factor and I'm 
> hoping to make the histograms out of the subsets of the 
> dataframe corresponding to the splits and for them to be 
> attached to the appropriate leaf of the final tree.
> 
> Any help would be much appreciated,
> thanks,
> Jon Zelner
> 
> University of Michigan
> Gerald R. Ford School of Public Policy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From johanfaux at yahoo.com  Fri Feb  2 23:51:13 2007
From: johanfaux at yahoo.com (johan Faux)
Date: Fri, 2 Feb 2007 14:51:13 -0800 (PST)
Subject: [R] silent loading of packages
Message-ID: <991971.71191.qm@web56204.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070202/d33802af/attachment.pl 

From oliverfaulhaber at gmx.de  Sat Feb  3 00:14:49 2007
From: oliverfaulhaber at gmx.de (Oliver Faulhaber)
Date: Sat, 03 Feb 2007 00:14:49 +0100
Subject: [R] Newbie problem: Vectorizing a minimum function with constraints
Message-ID: <45C3C5E9.3010905@gmx.de>

Sorry, if this question is rather basic, but being a newbie I still have 
problems to think in the "R way".

My problem is as follows:

- I have a data frame X with stock prices X$Price and corresponding 
dates X$Date.

- Now I want to get for each date x in X$Date the index z, such that
      z = min (a | X$Date(a)>x and X$Price(a)>Price(x)

To put it simple, I am looking for the first time in the future at which 
the stock price is higher than today's stock price. If such a time does 
not exist the output should be N.A.

An example input:

Index X$Price X$Date
1     20      2000-01-01
2     25      2000-01-02
3     15      2000-01-03
4     20      2000-01-04

The output should be

Index z
1     2
2     N.A.
3     4
4     N.A.

I don't have any idea how to program this in an elegant way in R. Any 
help is thus highly appreciated.

Thanks in advance
Oliver

-- 
Oliver Faulhaber (Dipl.-Math. oec., Aktuar DAV)
oliverfaulhaber at gmx.de ? www.oliverfaulhaber.de


From ggrothendieck at gmail.com  Sat Feb  3 01:46:29 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Feb 2007 19:46:29 -0500
Subject: [R] Newbie problem: Vectorizing a minimum function with
	constraints
In-Reply-To: <45C3C5E9.3010905@gmx.de>
References: <45C3C5E9.3010905@gmx.de>
Message-ID: <971536df0702021646i5c4f579au3139f232499f7518@mail.gmail.com>

Try this:

> x <- c(20, 25, 15, 20)
>
> f <- function(i, x) {
+   out <- which.max(x > x[i] & seq_along(x) > i)
+   if (out == 1) NA else out
+ }
> sapply(seq_along(x), f, x)
[1]  2 NA  4 NA

On 2/2/07, Oliver Faulhaber <oliverfaulhaber at gmx.de> wrote:
> Sorry, if this question is rather basic, but being a newbie I still have
> problems to think in the "R way".
>
> My problem is as follows:
>
> - I have a data frame X with stock prices X$Price and corresponding
> dates X$Date.
>
> - Now I want to get for each date x in X$Date the index z, such that
>      z = min (a | X$Date(a)>x and X$Price(a)>Price(x)
>
> To put it simple, I am looking for the first time in the future at which
> the stock price is higher than today's stock price. If such a time does
> not exist the output should be N.A.
>
> An example input:
>
> Index X$Price X$Date
> 1     20      2000-01-01
> 2     25      2000-01-02
> 3     15      2000-01-03
> 4     20      2000-01-04
>
> The output should be
>
> Index z
> 1     2
> 2     N.A.
> 3     4
> 4     N.A.
>
> I don't have any idea how to program this in an elegant way in R. Any
> help is thus highly appreciated.
>
> Thanks in advance
> Oliver
>
> --
> Oliver Faulhaber (Dipl.-Math. oec., Aktuar DAV)
> oliverfaulhaber at gmx.de ? www.oliverfaulhaber.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cberry at tajo.ucsd.edu  Sat Feb  3 02:04:22 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 2 Feb 2007 17:04:22 -0800
Subject: [R] Another loop - deloop question
In-Reply-To: <BAY132-F27D3EEC7A1AA614A19A928AA9B0@phx.gbl>
References: <BAY132-F27D3EEC7A1AA614A19A928AA9B0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0702021629090.16185@tajo.ucsd.edu>


Talbot,

Vectorization is not panacea.

For n == 100, m ==1000:

> system.time( for( i in 1:n ){ p[ g[[i]] ] <- p[ g[[i]] ] + y[[i]] })
[1]  0  0  0 NA NA
> system.time( p2 <- tapply( unlist(y), unlist(g), sum ))
[1] 0.16 0.00 0.16   NA   NA
> all.equal(p,as.vector(p2))
[1] TRUE
> system.time( p3 <- xtabs( unlist(y) ~ unlist(g) ) )
[1] 0.08 0.00 0.08   NA   NA
> all.equal(p,as.vector(p3))
[1] TRUE
> system.time( p4 <- unlist(y) %*% diag(m)[ unlist(g), ] )
[1] 4.16 0.20 4.36   NA   NA
> all.equal(p,as.vector(p4))
[1] TRUE


Vectorization has had no victory, Grasshopper.

---

For n == 10000, m == 10, the slowest method above becomes the fastest, and 
the fastest above becomes the slowest. So, you need to consider the 
applications to which you will apply this.

Read up on profiling if you really 'feel the need for speed'. (Writing R 
Extensions 3.2 Profiling R code for speed.)

Chuck

p.s. Please read "Writing R Extensions 3.1 Tidying R code" and follow the 
wisdom therein.


On Fri, 2 Feb 2007, Talbot Katz wrote:

> Hi.
>
> You folks are so clever, I thought perhaps you could help me make another
> procedure more efficient.
>
> Right now I have the following setup:
>
> p is a vector of length m
> g is a list of length n, g[[i]] is a vector whose elements are indices of p,
> i.e., integers between 1 and m inclusive); the g[[i]] cover the full set
> 1:m, but they don't have to constitute an exact partition, theycan overlap
> members.
> y is another list of length n, each y[[i]] is a vector of the same length as
> g[[i]].
>
> Now I build up the vector p as follows:
>
> p=rep(0,m)
> for(i in 1:n){p[g[[i]]]=p[g[[i]]]+y[[i]]}
>
> Can this loop be vectorized?
>
> Thanks!
>
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From duncan at wald.ucdavis.edu  Sat Feb  3 03:52:37 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Fri, 02 Feb 2007 18:52:37 -0800
Subject: [R] CGIwithR
In-Reply-To: <6.1.2.0.2.20070202155914.01cb4e58@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070202155914.01cb4e58@aiminy.mail.iastate.edu>
Message-ID: <45C3F8F5.3080008@wald.ucdavis.edu>



Aimin Yan wrote:
> I try a example
> http://omega.psi.iastate.edu/bootstrapFile.html
> 
> it doesn't give me output. I don't why.
> 
> For another example
> 
> http://omega.psi.iastate.edu/trivial.html
> 
> it seems works except it doesn't display figure.

But it gives you an error that states why there is no
figure.

One has to install and configure CGIwithR for
use with the Apache server.
It is described in a paper that accompanies the
package.  Please read that and if you have specific
questions, then we can address those.

That is where the details are described, so there
is little point in cutting and pasting them here too!

  D.

> 
> Does anyone know how to figure it out?
> 
> Aimin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pchen at uni-bielefeld.de  Sat Feb  3 11:17:25 2007
From: pchen at uni-bielefeld.de (pu chen)
Date: Sat, 03 Feb 2007 11:17:25 +0100
Subject: [R] precision of zero in R
Message-ID: <f9fee63473ab.45c46f45@uni-bielefeld.de>

hello,

I have to calculate the determinants of near singular matrices. Presently R just stop at the precision of  2.16 e-16.  

Can some one tell me how can I set the precision of zero in R ?


thanks


chen pu


From Torsten.Hothorn at rzmail.uni-erlangen.de  Sat Feb  3 11:25:21 2007
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Sat, 3 Feb 2007 11:25:21 +0100 (CET)
Subject: [R] Regression trees with an ordinal response variable
In-Reply-To: <34208.212.209.13.15.1170405033.squirrel@www.sorch.se>
References: <3671.150.229.227.99.1170392630.squirrel@mail.acg.ucsc.edu>
	<34208.212.209.13.15.1170405033.squirrel@www.sorch.se>
Message-ID: <Pine.LNX.4.64.0702031124110.32202@imbe153.imbe.med.uni-erlangen.de>


On Fri, 2 Feb 2007, Henric Nilsson (Public) wrote:

> Torsten, consider the following:
>
>> ### ordinal regression
>> mammoct <- ctree(ME ~ ., data = mammoexp)
> Warning message:
> no admissible split found
>> ### estimated class probabilities
>> treeresponse(mammoct, newdata = mammoexp[1:5, ])
> [[1]]
> [1] 1.822115
>
> [[2]]
> [1] 1.265487
>
> [[3]]
> [1] 1.822115
>
> [[4]]
> [1] 1.560440
>
> [[5]]
> [1] 1.822115
>

works again as advertised in party 0.9-9. Thanks for pointing this out!

Best wishes,

Torsten


From rdiaz02 at gmail.com  Sat Feb  3 13:02:57 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 3 Feb 2007 13:02:57 +0100
Subject: [R] Snow Package and R: Exported Variable Problem
In-Reply-To: <20070202162327.4hmordptwks04kco@mail1.maine.edu>
References: <20070202162327.4hmordptwks04kco@mail1.maine.edu>
Message-ID: <624934630702030402l2b7f5abby6a4fa3d7b9837568@mail.gmail.com>

Dear Robert,


On 2/2/07, robert.robinson at maine.edu <robert.robinson at maine.edu> wrote:
> Hello and thanks in advance for your time.
>
> I've created a simulation on my cluster which uses a custom package
> developed by me for different functions and also the snow package.
> Right now I'm using LAM to communicate between nodes and am currently
> only testing my code on 3 nodes for simplicity, though I plan on
> expanding to 16 later.  My problem is this error:
>
> "Error in fn(par, ..) : object \"x1\" not found \n"
> attr(,"class")
> "try-error"
>
> In my simulation I need to run a function several times with an
> different variable each time.  All the invocations on the functions
> are independent of the others.  I start the simulation on one node,
> create a cluster of several nodes, load my custom package and snow on
> all of them, use  clusterExport(cl, "x1") to export the variable
> x1(among other variable I need), then I call my simulation on the
> cluster using clusterApplyLB(cl, 2:S, simClust)  where cl is the
> cluster and S is a constant defined above as 500.  Using print
> statements (since snow, or R for that matter, has next to no ability
> to debug) I found that the error cropped up in this statement:
>
> theta6 = optim(c(0,0,0,0,0,0,.2), loglikelihood, scrore6, method =
> "CG", control=list(fnscale=-1,reltol=1e-8,maxit=2000))$par
>
> Both the functions loglikelelihood and score6 use x1, but I know that
> it is getting exported to the node correctly since it gets assigned
> earlier in the simulation:
>
> x1 = rep(0,n1)
>
> The error I stated above happens fo every itteration of the simulation
> (499 times) and I'm really at a loss as to why its happening and what
> I can do to determine what it is.  I'm wondering at this point if
> exporting the variable makes it unavailable to certain other packages,
> though that doesn't really make any sense.
>

>From reading quickly through your description, I do not see anything
obviously wrong.


> If anyone can help me with this problem, or let me know how I can
> debug this, or even a clue as to why it might be happening I would
> greatly appreciate it.  I've been wrestling with this for some time
> and no online documentation can help.  Thank you for your time and help.
>

When I was feeling really lost, I've resorted to assigning
intermediate output from commands such as ls, search, etc, to
variables (i.e., something like "this.ls <<- ls()" from inside you
function call, e.g., simClust) and then, e.g., from mpi.remote.exec,
looking at the value of those variables.

And, for over a year now, I've been doing most of my MPI stuff with
papply; the one nice thing of papply is that, if you have no LAM/MPI
universe, it will use a serial (not a parallel) version, so it is
much, much, much easier to debug, because you see the warnings and the
errors. So most of the frustration of things like launching something
and seeing it never return, etc, is gone.



Best,

R.

> Just so you know I'm a Computer Scientist not a Statistician, though I
> will be able to give any information about the statistics involved in
> this program.  I am reluctant to give away all source code since it is
> not my work but rather code I'm converting from standard code to
> parallelized code for a professor of mine.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From ggrothendieck at gmail.com  Sat Feb  3 13:44:30 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 3 Feb 2007 07:44:30 -0500
Subject: [R] Access to column names stored in a vector in lm procedure
In-Reply-To: <20070202175705.A5B669A1B35@ciruelo2.csic.es>
References: <20070202175705.A5B669A1B35@ciruelo2.csic.es>
Message-ID: <971536df0702030444v12ebca2fhae663fdef1ff6d38@mail.gmail.com>

You can regress multiple dependent variables at once by placing them
in a matrix.   For example, this regresses each of the first 4 columns of
the builtin iris dataset against Species, the last column:

iris4 <- as.matrix(iris[,-5])
summary(lm(iris4 ~ Species, iris))


On 2/2/07, mcnpf748 at mncn.csic.es <mcnpf748 at mncn.csic.es> wrote:
> Hello everybody
>
> I have to run many statistical tests that are identical, with the
> exception of the dependent variable. Is there a possibility to store the
> dependent variable names e.g. in a vector (in the below mentioned example
> called "variable") and to use the content of this vector in a simple
> statistical test (e.g. a regression). I would like to write the
> statistical procedure only once?
>
>
> For example: I would like to store 100 dependent variable names in a
> vector called "variable" and then I would like to do a simple regression
> with each of theses variables and the variable "length" using e.g. the
> while function. I would then extract e.g. the t-value and add it to a
> vector ("result") that contains all results. Something like that:
>
>
>
> "variable" should contain the names of the 100 dependent variables (Var1,
> Var2, ? Var100)
>
> while(i<101){
> result<-c(result,coef(summary(lm(variable[i] ~ length, data = data2)))[2,4]);
> i<-i+1
> }
>
> This example does not work since the lm function does not recognize the
> dependent variable's name.
>
> Does somebody know how to store the names of the dependent variables in
> e.g. a vector and to make them available for the lm function?
>
> Many thanks
>
> Patrick
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From jrkrideau at yahoo.ca  Sat Feb  3 15:11:37 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sat, 3 Feb 2007 09:11:37 -0500 (EST)
Subject: [R] LyX 1.4.3-5 problem generating PDF documents
Message-ID: <866007.72903.qm@web32812.mail.mud.yahoo.com>

I just changed machines and, at first, could not get
LyX 1.4.3-4 to even load.  I tried 1.5 which loaded
nicely but would not give me PDF output.  Removed 1.5
tried using 1.4.3-5.exe. Result : On loading LyX : 
  lyx: Disabling socket
  cont class std::basic_string<char struct std:
char::char_traits ...

Removed and installed from LyXWin143Complete-2-9.exe. 
Loads okay but no PDF generation. My PDF reader, Foxit
Reader loads but nothing is produced.  

I have managed to produce a postscript doc and convert
it using Ghostscript. 

Could this just be a Foxit problem? As soon as I
figure out how to reset the Adobe to default pdf
reader I'll try it out.


From bates at stat.wisc.edu  Sat Feb  3 16:59:15 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 3 Feb 2007 09:59:15 -0600
Subject: [R] precision of zero in R
In-Reply-To: <f9fee63473ab.45c46f45@uni-bielefeld.de>
References: <f9fee63473ab.45c46f45@uni-bielefeld.de>
Message-ID: <40e66e0b0702030759x5e13cbder4c5631c14e833c6a@mail.gmail.com>

On 2/3/07, pu chen <pchen at uni-bielefeld.de> wrote:
> hello,

> I have to calculate the determinants of near singular matrices. Presently R just stop at the precision of  2.16 e-16.

I'm not sure what you mean by that.  Can you explain how you
determined that number, perhaps including some code?

It happens that on many systems that number is close to the relative
machine precision.  For example, on my version of R

> .Machine$double.eps
[1] 2.220446e-16

> Can some one tell me how can I set the precision of zero in R ?

If you mean how could you change R so that it would give infinitely
precise floating point representations for which the relative machine
precision is zero I think the only answer is "by magic".  In the real
world we need to work with the floating point representations on the
computers that we have access to.  Perhaps you could rethink the
calculation you are attempting to perform.


From evaiannario at libero.it  Sat Feb  3 12:50:15 2007
From: evaiannario at libero.it (evaiannario)
Date: Sat,  3 Feb 2007 12:50:15 +0100
Subject: [R] residual GLMM
Message-ID: <JCVY7R$A842BB653027A59990F953E34B592AA1@libero.it>

Hi! 
can I effect a graphic analysis of the residual in GLMM? 
The data that I have are all of binary nature and I'm trying to eliminate the overdispersion with the GLMM.
Thanks for attention.
Eva 



------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada03feb07


From amrahmed76 at gmail.com  Sat Feb  3 17:50:57 2007
From: amrahmed76 at gmail.com (Amr Ahmed)
Date: Sat, 3 Feb 2007 11:50:57 -0500
Subject: [R] Installation Help
Message-ID: <652e09cf0702030850r36de61a9k5342dc389cec35cc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070203/02bb7be2/attachment.pl 

From dorcas at baconsulting.it  Sat Feb  3 17:58:51 2007
From: dorcas at baconsulting.it (Ionel Cardella)
Date: Sat, 3 Feb 2007 17:58:51 +0100
Subject: [R] new incriminat
Message-ID: <01c747b4$94969390$0300a8c0@ISABELLE>

Hi,

Vinagra - 3. 35
Vanlium - 1. 25
Cinalis - 3. 75
Amnbien - 2. 90

http://lodrx!com

Important, Replace "!" with "."

--

Oh yes, very easy to forget Myrtles dead,  said Myrtle, gulping,
looking at him out of swollen eyes. Nobody missed me even when I was
alive. Took them hours and hours to find my body I know, I was sitting


From ggrothendieck at gmail.com  Sat Feb  3 18:05:31 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 3 Feb 2007 12:05:31 -0500
Subject: [R] precision of zero in R
In-Reply-To: <f9fee63473ab.45c46f45@uni-bielefeld.de>
References: <f9fee63473ab.45c46f45@uni-bielefeld.de>
Message-ID: <971536df0702030905r665fcc4an38e345c045f1116@mail.gmail.com>

The Ryacas package can do exact arithmetic:
   http://code.google.com/p/ryacas/
See the vignette for examples.

On 2/3/07, pu chen <pchen at uni-bielefeld.de> wrote:
> hello,
>
> I have to calculate the determinants of near singular matrices. Presently R just stop at the precision of  2.16 e-16.
>
> Can some one tell me how can I set the precision of zero in R ?
>
>
> thanks
>
>
> chen pu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From amrahmed76 at gmail.com  Sat Feb  3 19:30:13 2007
From: amrahmed76 at gmail.com (Amr Ahmed)
Date: Sat, 3 Feb 2007 13:30:13 -0500
Subject: [R] Installation Help [Ignore]
Message-ID: <652e09cf0702031030n2ae156b8u9dcb7849cda5b3cd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070203/8e8c7d0c/attachment.pl 

From mail at xesoftware.com.au  Sat Feb  3 20:46:01 2007
From: mail at xesoftware.com.au (stephenc)
Date: Sun, 4 Feb 2007 06:46:01 +1100
Subject: [R] futures, investment
Message-ID: <000001c747cb$f02de020$6501a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070204/3033cdde/attachment.pl 

From ripley at stats.ox.ac.uk  Sat Feb  3 21:15:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 3 Feb 2007 20:15:23 +0000 (GMT)
Subject: [R] Installation Help
In-Reply-To: <652e09cf0702030850r36de61a9k5342dc389cec35cc@mail.gmail.com>
References: <652e09cf0702030850r36de61a9k5342dc389cec35cc@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702032011530.13651@gannet.stats.ox.ac.uk>

You have an incompatible libz.a in /usr/local/lib64, without the 
corresponding libz.so.  That is *not* the default on any 'linux machine' I 
have ever seen.  Please remove it, and make sure you have the system 
shared libz installed.

Also, why are you installing the obsolete R-2.4.0?

On Sat, 3 Feb 2007, Amr Ahmed wrote:

> Hi,
>
> I am trying to install R on a linux machine. The output of running
> ./configure is
>
> ==============================
> R is now configured for x86_64-unknown-linux-gnu
>
>  Source directory:          .
>  Installation directory:    /usr/local
>
>  C compiler:                gcc  -g -O2 -std=gnu99
>  Fortran 77 compiler:       g77  -g -O2
>
>  C++ compiler:              g++  -g -O2
>  Fortran 90/95 compiler:    gfortran -g -O2
>
>  Interfaces supported:      X11, tcltk
>  External libraries:        readline
>  Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
>  Options enabled:           shared BLAS, R profiling
>
>  Recommended packages:      yes
> ================================
>
>
> however when I run "make" I got the following error:
>
>
> ================================
> gcc -shared -L/usr/local/lib64 -o R_X11.so dataentry.o devX11.o rotated.o
> rbitmap.o  -lSM -lICE -L/usr/X11R6/lib64 -lX11 -lXt  -ljpeg -lpng -lz  -lm
> /usr/bin/ld: /usr/local/lib64/libz.a(crc32.o): relocation R_X86_64_32
> against `a local symbol' can not be used when making a shared object;
> recompile with -fPIC
> /usr/local/lib64/libz.a: could not read symbols: Bad value
> collect2: ld returned 1 exit status
> make[4]: *** [R_X11.so] Error 1
> make[4]: Leaving directory `/afs/cs.cmu.edu/user/amahmed/R-2.4.0
> /src/modules/X11'
> make[3]: *** [R] Error 2
> make[3]: Leaving directory `/afs/cs.cmu.edu/user/amahmed/R-2.4.0
> /src/modules/X11'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/afs/cs.cmu.edu/user/amahmed/R-2.4.0
> /src/modules'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/afs/cs.cmu.edu/user/amahmed/R-2.4.0/src'
> make: *** [R] Error 1
> ================================
>
> I think the error is critical as when I want to run R from the command line
> I got the following error:
>
> =======================
> $./R
> ./R: error while loading shared libraries: libRblas.so: cannot open shared
> object file: No such file or directory[
> =======================
>
> Any idea? I just want to be able to run R from command line, no need for gui
> interface.
>
> Thanks
> Amr
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sekhon at berkeley.edu  Sat Feb  3 21:22:16 2007
From: sekhon at berkeley.edu (Jasjeet Singh Sekhon)
Date: Sat, 3 Feb 2007 12:22:16 -0800
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <45C38E90.9070709@ucsd.edu>
References: <45C38E90.9070709@ucsd.edu>
Message-ID: <17860.61176.520222.602660@lapo.berkeley.edu>


Hi Roger,

Yes, multinomRob can handle equality constraints of this type---see
the 'equality' option.  But the function assumes that the outcomes are
multinomial counts and it estimates overdispersed multinomial logistic
models via MLE, a robust redescending-M estimator, and LQD which is
another high breakdown point estimator.  It would be a simple matter
to edit the 'multinomMLE' function to work without counts and to do
straight MNL instead, but right now it estimates an overdispersed MNL
model.

Cheers,
Jas.

=======================================
Jasjeet S. Sekhon                     
                                      
Associate Professor             
Travers Department of Political Science
Survey Research Center          
UC Berkeley                     

http://sekhon.berkeley.edu/
V: 510-642-9974  F: 617-507-5524
=======================================



Roger Levy writes:
 > I'm interested in doing multinomial logistic regression with equality 
 > constraints on some of the parameter values.  For example, with 
 > categorical outcomes Y_1 (baseline), Y_2, and Y_3, and covariates X_1 
 > and X_2, I might want to impose the equality constraint that
 > 
 >    \beta_{2,1} = \beta_{3,2}
 > 
 > that is, that the effect of X_1 on the logit of Y_2 is the same as the 
 > effect of X_2 on the logit of Y_3.
 > 
 > Is there an existing facility or package in R for doing this?  Would 
 > multinomRob fit the bill?
 > 
 > Many thanks,
 > 
 > Roger
 > 
 > 
 > -- 
 > 
 > Roger Levy                      Email: rlevy at ucsd.edu
 > Assistant Professor             Phone: 858-534-7219
 > Department of Linguistics       Fax:   858-534-4789
 > UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy
 > 
 >


From marc_schwartz at comcast.net  Sat Feb  3 21:47:57 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 03 Feb 2007 14:47:57 -0600
Subject: [R] reading very large files
In-Reply-To: <a17009720702031006n56b6b780wbb40301f599d68e2@mail.gmail.com>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
	<a17009720702031006n56b6b780wbb40301f599d68e2@mail.gmail.com>
Message-ID: <1170535678.4890.2.camel@localhost.localdomain>

On Sat, 2007-02-03 at 19:06 +0100, juli g. pausas wrote:
> Thank so much for your help and comments.
> The approach proposed by Jim Holtman was the simplest and fastest. The
> approach by Marc Schwartz also worked (after a very small
> modification).
> 
> It is clear that a good knowledge of R save a lot of time!! I've been
> able to do in few minutes a process that was only 1/4th done after 25
> h!
> 
> Many thanks
> 
> Juli

Juli,

Just out of curiosity, what change did you make?

Also, what were the running times for the solutions?

Regards,

Marc


From deleeuw at cuddyvalley.org  Sat Feb  3 22:43:03 2007
From: deleeuw at cuddyvalley.org (Jan de Leeuw)
Date: Sat, 3 Feb 2007 13:43:03 -0800
Subject: [R] JSS Volume 18 (www.jstatsoft.org)
Message-ID: <82A88369-0966-487E-B441-4379F32FC33C@cuddyvalley.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070203/8704d9a6/attachment.pl 

From pengyu.ut at gmail.com  Sat Feb  3 22:55:54 2007
From: pengyu.ut at gmail.com (Peng Yu)
Date: Sat, 3 Feb 2007 15:55:54 -0600
Subject: [R] metapost output
Message-ID: <366c6f340702031355l238442a1l7201ce4c72aea1ce@mail.gmail.com>

Hi,

I know the R can make eps output. But metapost sees to be an even
better choice for me. I've been using gnuplot for a while, it seems it
has excellent support with its metapost latex terminal. So I can
insert any latex equation in the figure. Meanwhile, I can make the
font in the figure will be consistent with font used in the main text.

I searched R help, but I can not find metapost support.

I'm wondering it metapost output supported by R. If it is not, will
this function be developed in the future?

Thanks,
Peng


From aiminy at iastate.edu  Sat Feb  3 23:50:02 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sat, 03 Feb 2007 16:50:02 -0600
Subject: [R] CGIwithR
In-Reply-To: <45C3F8F5.3080008@wald.ucdavis.edu>
References: <6.1.2.0.2.20070202155914.01cb4e58@aiminy.mail.iastate.edu>
	<45C3F8F5.3080008@wald.ucdavis.edu>
Message-ID: <6.1.2.0.2.20070203164109.01c4fb30@aiminy.mail.iastate.edu>


Yes, I already read it. I want to try those examples out.
The question for bootstarpFile.html is:

when I give several number , and press Submit Query.
I didn't get any output.

here is the location of my file

[root at omega cgi-bin]# pwd
/var/www/cgi-bin
[root at omega cgi-bin]# ls -al
total 68
drwxr-xr-x  2 root root 4096 Feb  3 16:38 .
drwxrwxrwx  8 root root 4096 Jan 10 12:24 ..
-rw-r--r--  1 root root 2219 Feb  2 15:09 bootstrap.R
-rw-r--r--  1 root root  479 Feb  2 15:48 data.R
-rw-r--r--  1 root root 3339 Feb  2 15:15 qvweb.R
-rwxr-xr-x  1 root root 4641 Feb  2 15:42 R.cgi
-rw-r--r--  1 root root   41 Feb  2 15:43 .Rprofile
-rw-r--r--  1 root root 1334 Feb  2 15:46 trivial.R

[root at omega html]# pwd
/var/www/html
[root at omega html]# ls -al
total 48
drwxrwxrwx  2 root   root   4096 Feb  2 15:52 .
drwxrwxrwx  8 root   root   4096 Jan 10 12:24 ..
-rw-rw-r--  1 aiminy aiminy 1737 Feb  2 15:52 bootstrapFile.html
-rw-rw-r--  1 aiminy aiminy 1682 Feb  2 14:24 bootstrap.html
-rw-rw-r--  1 aiminy aiminy  904 Jan 10 12:41 course.html
-rw-rw-r--  1 aiminy aiminy  548 Feb  2 15:46 trivial.html


From wrm1 at macht.arts.cornell.edu  Sat Feb  3 23:54:19 2007
From: wrm1 at macht.arts.cornell.edu (Walter Mebane)
Date: Sat, 3 Feb 2007 17:54:19 -0500
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <17860.61176.520222.602660@lapo.berkeley.edu>
References: <45C38E90.9070709@ucsd.edu>
	<17860.61176.520222.602660@lapo.berkeley.edu>
Message-ID: <17861.4763.359422.817825@macht.arts.cornell.edu>

By default, with print.level=0 or greater, the multinomRob program
prints the maximum likelihood estimates with conventional standard
errors before going on to compute the robust estimates.

Walter Mebane

Jasjeet Singh Sekhon writes:
 > 
 > Hi Roger,
 > 
 > Yes, multinomRob can handle equality constraints of this type---see
 > the 'equality' option.  But the function assumes that the outcomes are
 > multinomial counts and it estimates overdispersed multinomial logistic
 > models via MLE, a robust redescending-M estimator, and LQD which is
 > another high breakdown point estimator.  It would be a simple matter
 > to edit the 'multinomMLE' function to work without counts and to do
 > straight MNL instead, but right now it estimates an overdispersed MNL
 > model.
 > 
 > Cheers,
 > Jas.
 > 
 > =======================================
 > Jasjeet S. Sekhon                     
 >                                       
 > Associate Professor             
 > Travers Department of Political Science
 > Survey Research Center          
 > UC Berkeley                     
 > 
 > http://sekhon.berkeley.edu/
 > V: 510-642-9974  F: 617-507-5524
 > =======================================
 > 
 > 
 > 
 > Roger Levy writes:
 >  > I'm interested in doing multinomial logistic regression with equality 
 >  > constraints on some of the parameter values.  For example, with 
 >  > categorical outcomes Y_1 (baseline), Y_2, and Y_3, and covariates X_1 
 >  > and X_2, I might want to impose the equality constraint that
 >  > 
 >  >    \beta_{2,1} = \beta_{3,2}
 >  > 
 >  > that is, that the effect of X_1 on the logit of Y_2 is the same as the 
 >  > effect of X_2 on the logit of Y_3.
 >  > 
 >  > Is there an existing facility or package in R for doing this?  Would 
 >  > multinomRob fit the bill?
 >  > 
 >  > Many thanks,
 >  > 
 >  > Roger
 >  > 
 >  > 
 >  > -- 
 >  > 
 >  > Roger Levy                      Email: rlevy at ucsd.edu
 >  > Assistant Professor             Phone: 858-534-7219
 >  > Department of Linguistics       Fax:   858-534-4789
 >  > UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy
 >  > 
 >  > 

-- 
* - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - *
Walter R. Mebane, Jr.                        email:  wrm1 at cornell.edu
Professor                             office voice:  607/255-3868    
Department of Government                      cell:  607/592-0546
Cornell University                             fax:  607/255-4530    
217 White Hall              WWW:  http://macht.arts.cornell.edu/wrm1/
Ithaca, NY 14853-7901
* - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - *


From rlevy at ucsd.edu  Sun Feb  4 01:55:51 2007
From: rlevy at ucsd.edu (Roger Levy)
Date: Sat, 03 Feb 2007 16:55:51 -0800
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <17861.4763.359422.817825@macht.arts.cornell.edu>
References: <45C38E90.9070709@ucsd.edu>	<17860.61176.520222.602660@lapo.berkeley.edu>
	<17861.4763.359422.817825@macht.arts.cornell.edu>
Message-ID: <45C52F17.1030601@ucsd.edu>

Many thanks for pointing this out to me!

I'm still a bit confused, however, as to how to use multinomRob.  For 
example I tried to translate the following example using nnet:


x1 <- c(1,1,1,1,0,0,0,0,0,0,0,0)
x2 <- c(0,0,0,0,1,1,1,1,0,0,0,0)
y <- factor(c("a","b","b","c","a","b","c","c","a","a","b","c"))
library(nnet)
d <- data.frame(x1,x2,y)
summary(multinom(y ~ x1 + x2, data=d))


into multinomRob as follows:


x1 <- c(1,1,1,1,0,0,0,0,0,0,0,0)
x2 <- c(0,0,0,0,1,1,1,1,0,0,0,0)
y <- factor(c("a","b","b","c","a","b","c","c","a","a","b","c"))
y1 <- ifelse(y=="a",1, 0)
y2 <- ifelse(y=="b", 1, 0)
y3 <- ifelse(y=="c", 1, 0)
d <- data.frame(x1,x2,y,y1,y2,y3)
summary(multinomRob(list(y1 ~ x1 + x2,y2 ~ x1 + x2, y3 ~ x1 + x2),data=d))

but the last command gives me the error message:


[1] "multinomMLE: Hessian is not positive definite"
Error in obsformation %*% opg : non-conformable arguments


though it's not obvious to me why.  I also tried a couple other variants:


 > summary(multinomRob(list(y1 ~ 0,y2 ~ x1 + x2,y3 ~ x1 + x2),data=d))
Error in multinomT(Yp = Yp, Xarray = X, xvec = xvec, jacstack = 
jacstack,  :
         (multinomT): invalid specification of Xarray (regressors not 
allowed for last category
 > summary(multinomRob(list(y1 ~ 0,y2 ~ x1 ,y3 ~ x2),data=d))
Error in multinomT(Yp = Yp, Xarray = X, xvec = xvec, jacstack = 
jacstack,  :
         (multinomT): invalid specification of Xarray (regressors not 
allowed for last category


Any advice would be much appreciated!


Many thanks,

Roger

Walter Mebane wrote:
> By default, with print.level=0 or greater, the multinomRob program
> prints the maximum likelihood estimates with conventional standard
> errors before going on to compute the robust estimates.
> 
> Walter Mebane
> 
> Jasjeet Singh Sekhon writes:
>  > 
>  > Hi Roger,
>  > 
>  > Yes, multinomRob can handle equality constraints of this type---see
>  > the 'equality' option.  But the function assumes that the outcomes are
>  > multinomial counts and it estimates overdispersed multinomial logistic
>  > models via MLE, a robust redescending-M estimator, and LQD which is
>  > another high breakdown point estimator.  It would be a simple matter
>  > to edit the 'multinomMLE' function to work without counts and to do
>  > straight MNL instead, but right now it estimates an overdispersed MNL
>  > model.
>  > 
>  > Cheers,
>  > Jas.
>  > 
>  > =======================================
>  > Jasjeet S. Sekhon                     
>  >                                       
>  > Associate Professor             
>  > Travers Department of Political Science
>  > Survey Research Center          
>  > UC Berkeley                     
>  > 
>  > http://sekhon.berkeley.edu/
>  > V: 510-642-9974  F: 617-507-5524
>  > =======================================
>  > 
>  > 
>  > 
>  > Roger Levy writes:
>  >  > I'm interested in doing multinomial logistic regression with equality 
>  >  > constraints on some of the parameter values.  For example, with 
>  >  > categorical outcomes Y_1 (baseline), Y_2, and Y_3, and covariates X_1 
>  >  > and X_2, I might want to impose the equality constraint that
>  >  > 
>  >  >    \beta_{2,1} = \beta_{3,2}
>  >  > 
>  >  > that is, that the effect of X_1 on the logit of Y_2 is the same as the 
>  >  > effect of X_2 on the logit of Y_3.
>  >  > 
>  >  > Is there an existing facility or package in R for doing this?  Would 
>  >  > multinomRob fit the bill?
>  >  > 
>  >  > Many thanks,
>  >  > 
>  >  > Roger
>  >  > 
>  >  > 
>  >  > -- 
>  >  > 
>  >  > Roger Levy                      Email: rlevy at ucsd.edu
>  >  > Assistant Professor             Phone: 858-534-7219
>  >  > Department of Linguistics       Fax:   858-534-4789
>  >  > UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy
>  >  > 
>  >  > 
>


From wrm1 at macht.arts.cornell.edu  Sun Feb  4 02:17:36 2007
From: wrm1 at macht.arts.cornell.edu (Walter Mebane)
Date: Sat, 3 Feb 2007 20:17:36 -0500
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <45C52F17.1030601@ucsd.edu>
References: <45C38E90.9070709@ucsd.edu>
	<17860.61176.520222.602660@lapo.berkeley.edu>
	<17861.4763.359422.817825@macht.arts.cornell.edu>
	<45C52F17.1030601@ucsd.edu>
Message-ID: <17861.13360.697951.302800@macht.arts.cornell.edu>

Roger,

summary(multinomRob(list(y1 ~ x1 + x2,y2 ~ x1 + x2, y3 ~ 0),data=d,
  print.level=1))

Walter Mebane

Roger Levy writes:
 > Many thanks for pointing this out to me!
 > 
 > I'm still a bit confused, however, as to how to use multinomRob.  For 
 > example I tried to translate the following example using nnet:
 > 
 > 
 > x1 <- c(1,1,1,1,0,0,0,0,0,0,0,0)
 > x2 <- c(0,0,0,0,1,1,1,1,0,0,0,0)
 > y <- factor(c("a","b","b","c","a","b","c","c","a","a","b","c"))
 > library(nnet)
 > d <- data.frame(x1,x2,y)
 > summary(multinom(y ~ x1 + x2, data=d))
 > 
 > 
 > into multinomRob as follows:
 > 
 > 
 > x1 <- c(1,1,1,1,0,0,0,0,0,0,0,0)
 > x2 <- c(0,0,0,0,1,1,1,1,0,0,0,0)
 > y <- factor(c("a","b","b","c","a","b","c","c","a","a","b","c"))
 > y1 <- ifelse(y=="a",1, 0)
 > y2 <- ifelse(y=="b", 1, 0)
 > y3 <- ifelse(y=="c", 1, 0)
 > d <- data.frame(x1,x2,y,y1,y2,y3)
 > summary(multinomRob(list(y1 ~ x1 + x2,y2 ~ x1 + x2, y3 ~ x1 + x2),data=d))
 > 
 > but the last command gives me the error message:
 > 
 > 
 > [1] "multinomMLE: Hessian is not positive definite"
 > Error in obsformation %*% opg : non-conformable arguments
 > 
 > 
 > though it's not obvious to me why.  I also tried a couple other variants:
 > 
 > 
 >  > summary(multinomRob(list(y1 ~ 0,y2 ~ x1 + x2,y3 ~ x1 + x2),data=d))
 > Error in multinomT(Yp = Yp, Xarray = X, xvec = xvec, jacstack = 
 > jacstack,  :
 >          (multinomT): invalid specification of Xarray (regressors not 
 > allowed for last category
 >  > summary(multinomRob(list(y1 ~ 0,y2 ~ x1 ,y3 ~ x2),data=d))
 > Error in multinomT(Yp = Yp, Xarray = X, xvec = xvec, jacstack = 
 > jacstack,  :
 >          (multinomT): invalid specification of Xarray (regressors not 
 > allowed for last category
 > 
 > 
 > Any advice would be much appreciated!
 > 
 > 
 > Many thanks,
 > 
 > Roger
 > 
 > Walter Mebane wrote:
 > > By default, with print.level=0 or greater, the multinomRob program
 > > prints the maximum likelihood estimates with conventional standard
 > > errors before going on to compute the robust estimates.
 > > 
 > > Walter Mebane
 > > 
 > > Jasjeet Singh Sekhon writes:
 > >  > 
 > >  > Hi Roger,
 > >  > 
 > >  > Yes, multinomRob can handle equality constraints of this type---see
 > >  > the 'equality' option.  But the function assumes that the outcomes are
 > >  > multinomial counts and it estimates overdispersed multinomial logistic
 > >  > models via MLE, a robust redescending-M estimator, and LQD which is
 > >  > another high breakdown point estimator.  It would be a simple matter
 > >  > to edit the 'multinomMLE' function to work without counts and to do
 > >  > straight MNL instead, but right now it estimates an overdispersed MNL
 > >  > model.
 > >  > 
 > >  > Cheers,
 > >  > Jas.
 > >  > 
 > >  > =======================================
 > >  > Jasjeet S. Sekhon                     
 > >  >                                       
 > >  > Associate Professor             
 > >  > Travers Department of Political Science
 > >  > Survey Research Center          
 > >  > UC Berkeley                     
 > >  > 
 > >  > http://sekhon.berkeley.edu/
 > >  > V: 510-642-9974  F: 617-507-5524
 > >  > =======================================
 > >  > 
 > >  > 
 > >  > 
 > >  > Roger Levy writes:
 > >  >  > I'm interested in doing multinomial logistic regression with equality 
 > >  >  > constraints on some of the parameter values.  For example, with 
 > >  >  > categorical outcomes Y_1 (baseline), Y_2, and Y_3, and covariates X_1 
 > >  >  > and X_2, I might want to impose the equality constraint that
 > >  >  > 
 > >  >  >    \beta_{2,1} = \beta_{3,2}
 > >  >  > 
 > >  >  > that is, that the effect of X_1 on the logit of Y_2 is the same as the 
 > >  >  > effect of X_2 on the logit of Y_3.
 > >  >  > 
 > >  >  > Is there an existing facility or package in R for doing this?  Would 
 > >  >  > multinomRob fit the bill?
 > >  >  > 
 > >  >  > Many thanks,
 > >  >  > 
 > >  >  > Roger
 > >  >  > 
 > >  >  > 
 > >  >  > -- 
 > >  >  > 
 > >  >  > Roger Levy                      Email: rlevy at ucsd.edu
 > >  >  > Assistant Professor             Phone: 858-534-7219
 > >  >  > Department of Linguistics       Fax:   858-534-4789
 > >  >  > UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy
 > >  >  > 
 > >  >  > 
 > > 

-- 
* - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - *
Walter R. Mebane, Jr.                        email:  wrm1 at cornell.edu
Professor                             office voice:  607/255-3868    
Department of Government                      cell:  607/592-0546
Cornell University                             fax:  607/255-4530    
217 White Hall              WWW:  http://macht.arts.cornell.edu/wrm1/
Ithaca, NY 14853-7901
* - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - *


From sashag at stanford.edu  Sun Feb  4 05:14:49 2007
From: sashag at stanford.edu (Sasha Goodman)
Date: Sat, 3 Feb 2007 20:14:49 -0800
Subject: [R] Announcing another R search engine
Message-ID: <e6820af20702032014j62996da2q8da4594d3729113f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070203/09326544/attachment.pl 

From sashag at stanford.edu  Sun Feb  4 05:15:53 2007
From: sashag at stanford.edu (Sasha Goodman)
Date: Sat, 3 Feb 2007 20:15:53 -0800
Subject: [R] Announcing another R search engine
In-Reply-To: <e6820af20702032014j62996da2q8da4594d3729113f@mail.gmail.com>
References: <e6820af20702032014j62996da2q8da4594d3729113f@mail.gmail.com>
Message-ID: <e6820af20702032015x6d6bdd85x5a9882a11b68f369@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070203/07bb5eb6/attachment.pl 

From sekhon at berkeley.edu  Sun Feb  4 05:42:13 2007
From: sekhon at berkeley.edu (Jasjeet Singh Sekhon)
Date: Sat, 3 Feb 2007 20:42:13 -0800
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <17861.13360.697951.302800@macht.arts.cornell.edu>
References: <45C38E90.9070709@ucsd.edu>
	<17860.61176.520222.602660@lapo.berkeley.edu>
	<17861.4763.359422.817825@macht.arts.cornell.edu>
	<45C52F17.1030601@ucsd.edu>
	<17861.13360.697951.302800@macht.arts.cornell.edu>
Message-ID: <17861.25637.324856.683429@lapo.berkeley.edu>


Hi Roger,

Walter's command is correct.  To match the exact normalization used by
nnet's multinom(), however, you would need to make the coefficients
zero for the first class (i.e., y1) and not the last (i.e., y3).

mr <- multinomRob(list(y2 ~ x1 + x2, y3 ~ x1 + x2, y1~0),data=d,
print.level=1)

The results are:

MNL Estimates:
                           y1         y2         y3
NA/(Intercept)/(Intercept)  0 -0.6931462 -0.6931462
NA/x1/x1                    0  1.3862936  0.6931474
NA/x2/x2                    0  0.6931474  1.3862936

Compare to the output from nnet's multinom:

> summary(m1)
Call:
multinom(formula = y ~ x1 + x2, data = d)

Coefficients:
  (Intercept)        x1        x2
b  -0.6931475 1.3862975 0.6931499
c  -0.6931475 0.6931499 1.3862975

Also, the MLE MNL objects are in:

mr$mnl

To constrain the x1 coeffs to be equal do:

emr <- multinomRob(list(y2 ~ x1 + x2,y3 ~ x1 + x2, y1~0),data=d, print.level=1,
                  equality=list(list(y2~x1+0,y3~x1+0)))

To constrain y2's x1 to be equal to y3's x2:
emr2 <- multinomRob(list(y2 ~ x1 + x2,y3 ~ x1 + x2, y1~0),data=d, print.level=1,
                  equality=list(list(y2~x1+0,y3~x2+0)))


See the multinomRob help file for more details:
http://sekhon.berkeley.edu/robust/multinomRob.html

Any BFGS warnings can be ignored because you are not interested in the
robust estimates (they are comming from LQD estimation and require
changing 'genoud.parms').

Cheers,
Jas.

=======================================
Jasjeet S. Sekhon                     
                                      
Associate Professor             
Travers Department of Political Science
Survey Research Center          
UC Berkeley                     

http://sekhon.berkeley.edu/
V: 510-642-9974  F: 617-507-5524
=======================================

Walter Mebane writes:
 > Roger,
 > 
 > summary(multinomRob(list(y1 ~ x1 + x2,y2 ~ x1 + x2, y3 ~ 0),data=d,
 >   print.level=1))
 > 
 > Walter Mebane
 > 
 > Roger Levy writes:
 >  > Many thanks for pointing this out to me!
 >  > 
 >  > I'm still a bit confused, however, as to how to use multinomRob.  For 
 >  > example I tried to translate the following example using nnet:
 >  > 
 >  > 
 >  > x1 <- c(1,1,1,1,0,0,0,0,0,0,0,0)
 >  > x2 <- c(0,0,0,0,1,1,1,1,0,0,0,0)
 >  > y <- factor(c("a","b","b","c","a","b","c","c","a","a","b","c"))
 >  > library(nnet)
 >  > d <- data.frame(x1,x2,y)
 >  > summary(multinom(y ~ x1 + x2, data=d))
 >  > 
 >  > 
 >  > into multinomRob as follows:
 >  > 
 >  > 
 >  > x1 <- c(1,1,1,1,0,0,0,0,0,0,0,0)
 >  > x2 <- c(0,0,0,0,1,1,1,1,0,0,0,0)
 >  > y <- factor(c("a","b","b","c","a","b","c","c","a","a","b","c"))
 >  > y1 <- ifelse(y=="a",1, 0)
 >  > y2 <- ifelse(y=="b", 1, 0)
 >  > y3 <- ifelse(y=="c", 1, 0)
 >  > d <- data.frame(x1,x2,y,y1,y2,y3)
 >  > summary(multinomRob(list(y1 ~ x1 + x2,y2 ~ x1 + x2, y3 ~ x1 + x2),data=d))
 >  > 
 >  > but the last command gives me the error message:
 >  > 
 >  > 
 >  > [1] "multinomMLE: Hessian is not positive definite"
 >  > Error in obsformation %*% opg : non-conformable arguments
 >  > 
 >  > 
 >  > though it's not obvious to me why.  I also tried a couple other variants:
 >  > 
 >  > 
 >  >  > summary(multinomRob(list(y1 ~ 0,y2 ~ x1 + x2,y3 ~ x1 + x2),data=d))
 >  > Error in multinomT(Yp = Yp, Xarray = X, xvec = xvec, jacstack = 
 >  > jacstack,  :
 >  >          (multinomT): invalid specification of Xarray (regressors not 
 >  > allowed for last category
 >  >  > summary(multinomRob(list(y1 ~ 0,y2 ~ x1 ,y3 ~ x2),data=d))
 >  > Error in multinomT(Yp = Yp, Xarray = X, xvec = xvec, jacstack = 
 >  > jacstack,  :
 >  >          (multinomT): invalid specification of Xarray (regressors not 
 >  > allowed for last category
 >  > 
 >  > 
 >  > Any advice would be much appreciated!
 >  > 
 >  > 
 >  > Many thanks,
 >  > 
 >  > Roger
 >  > 
 >  > Walter Mebane wrote:
 >  > > By default, with print.level=0 or greater, the multinomRob program
 >  > > prints the maximum likelihood estimates with conventional standard
 >  > > errors before going on to compute the robust estimates.
 >  > > 
 >  > > Walter Mebane
 >  > > 
 >  > > Jasjeet Singh Sekhon writes:
 >  > >  > 
 >  > >  > Hi Roger,
 >  > >  > 
 >  > >  > Yes, multinomRob can handle equality constraints of this type---see
 >  > >  > the 'equality' option.  But the function assumes that the outcomes are
 >  > >  > multinomial counts and it estimates overdispersed multinomial logistic
 >  > >  > models via MLE, a robust redescending-M estimator, and LQD which is
 >  > >  > another high breakdown point estimator.  It would be a simple matter
 >  > >  > to edit the 'multinomMLE' function to work without counts and to do
 >  > >  > straight MNL instead, but right now it estimates an overdispersed MNL
 >  > >  > model.
 >  > >  > 
 >  > >  > Cheers,
 >  > >  > Jas.
 >  > >  > 
 >  > >  > =======================================
 >  > >  > Jasjeet S. Sekhon                     
 >  > >  >                                       
 >  > >  > Associate Professor             
 >  > >  > Travers Department of Political Science
 >  > >  > Survey Research Center          
 >  > >  > UC Berkeley                     
 >  > >  > 
 >  > >  > http://sekhon.berkeley.edu/
 >  > >  > V: 510-642-9974  F: 617-507-5524
 >  > >  > =======================================
 >  > >  > 
 >  > >  > 
 >  > >  > 
 >  > >  > Roger Levy writes:
 >  > >  >  > I'm interested in doing multinomial logistic regression with equality 
 >  > >  >  > constraints on some of the parameter values.  For example, with 
 >  > >  >  > categorical outcomes Y_1 (baseline), Y_2, and Y_3, and covariates X_1 
 >  > >  >  > and X_2, I might want to impose the equality constraint that
 >  > >  >  > 
 >  > >  >  >    \beta_{2,1} = \beta_{3,2}
 >  > >  >  > 
 >  > >  >  > that is, that the effect of X_1 on the logit of Y_2 is the same as the 
 >  > >  >  > effect of X_2 on the logit of Y_3.
 >  > >  >  > 
 >  > >  >  > Is there an existing facility or package in R for doing this?  Would 
 >  > >  >  > multinomRob fit the bill?
 >  > >  >  > 
 >  > >  >  > Many thanks,
 >  > >  >  > 
 >  > >  >  > Roger
 >  > >  >  > 
 >  > >  >  > 
 >  > >  >  > -- 
 >  > >  >  > 
 >  > >  >  > Roger Levy                      Email: rlevy at ucsd.edu
 >  > >  >  > Assistant Professor             Phone: 858-534-7219
 >  > >  >  > Department of Linguistics       Fax:   858-534-4789
 >  > >  >  > UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy
 >  > >  >  > 
 >  > >  >  > 
 >  > > 
 > 
 > -- 
 > * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - *
 > Walter R. Mebane, Jr.                        email:  wrm1 at cornell.edu
 > Professor                             office voice:  607/255-3868    
 > Department of Government                      cell:  607/592-0546
 > Cornell University                             fax:  607/255-4530    
 > 217 White Hall              WWW:  http://macht.arts.cornell.edu/wrm1/
 > Ithaca, NY 14853-7901
 > * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - *


From debmidya at yahoo.com  Sun Feb  4 07:23:11 2007
From: debmidya at yahoo.com (Deb Midya)
Date: Sat, 3 Feb 2007 22:23:11 -0800 (PST)
Subject: [R] Fwd: Re:  Calling C code from R
Message-ID: <337396.94029.qm@web50405.mail.yahoo.com>

Hi!
   
  Thanks in advance.
   
  Thanks to all of you who have responded to me on above. This is one of the responses I received on above.
   
  I have installed perl (with path C:\Perl\bin\;) an MinGW (with path C:\MinGW\bin; C:\MinGW;).
   
  At the Command Prompt (C:\R-2.4.0\bin) I have typed:
   
  C:\R-2.4.0\bin>Rcmd SHLIB useC1.c 
  (No error and useC1.dll file has not been created)
   
  C:\R-2.4.0\bin>R CMD SHLIB useC1.c 
  (No error and useC1.dll file has not been created)
   
  May I request you where I am going wrong.
   
  Regards,
   
 Deb
   
 Statistician
NSW Department of Commerce
Sydney Australia.

  
Note: forwarded message attached.

 
---------------------------------
Need Mail bonding?
-------------- next part --------------
An embedded message was scrubbed...
From: unknown sender
Subject: no subject
Date: no date
Size: 3633
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070203/5bb68cf8/attachment.mht 

From ripley at stats.ox.ac.uk  Sun Feb  4 09:20:16 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 4 Feb 2007 08:20:16 +0000 (GMT)
Subject: [R] metapost output
In-Reply-To: <366c6f340702031355l238442a1l7201ce4c72aea1ce@mail.gmail.com>
References: <366c6f340702031355l238442a1l7201ce4c72aea1ce@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702040757420.19217@gannet.stats.ox.ac.uk>

On Sat, 3 Feb 2007, Peng Yu wrote:

> Hi,
>
> I know the R can make eps output. But metapost sees to be an even
> better choice for me. I've been using gnuplot for a while, it seems it
> has excellent support with its metapost latex terminal. So I can
> insert any latex equation in the figure. Meanwhile, I can make the
> font in the figure will be consistent with font used in the main text.
>
> I searched R help, but I can not find metapost support.
>
> I'm wondering it metapost output supported by R. If it is not, will
> this function be developed in the future?

Please search R-help: this has come up several times.  It seems quite a 
few people are quite prepared to volunteer others to do this, but not 
prepared to contribute to writing such a device.

RSiteSearch("metapaost")

will get you many references.  (I am not posting the results, as it seems 
they are not permanent URLs.)  Note that you can convert xfig output to 
metapost, and R has an xfig driver (since someone was generous enough to 
actually contribute that).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Sun Feb  4 12:23:59 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 04 Feb 2007 06:23:59 -0500
Subject: [R] Fwd: Re:  Calling C code from R
In-Reply-To: <337396.94029.qm@web50405.mail.yahoo.com>
References: <337396.94029.qm@web50405.mail.yahoo.com>
Message-ID: <45C5C24F.5090207@stats.uwo.ca>

On 2/4/2007 1:23 AM, Deb Midya wrote:
> Hi!
>    
>   Thanks in advance.
>    
>   Thanks to all of you who have responded to me on above. This is one of the responses I received on above.
>    
>   I have installed perl (with path C:\Perl\bin\;) an MinGW (with path C:\MinGW\bin; C:\MinGW;).
>    
>   At the Command Prompt (C:\R-2.4.0\bin) I have typed:
>    
>   C:\R-2.4.0\bin>Rcmd SHLIB useC1.c 
>   (No error and useC1.dll file has not been created)
>    
>   C:\R-2.4.0\bin>R CMD SHLIB useC1.c 
>   (No error and useC1.dll file has not been created)
>    
>   May I request you where I am going wrong.

It looks as though you didn't install the Rtools collection.  Take a 
look in the Installation and Administration manual, Appendix E:  the 
Windows toolset.  For compiling code you need the first 3 items:  "the 
command line tools" (which you seem to be missing), Perl, and the MinGW 
compilers.  Be careful to get your path in the right order, as described 
there.

Duncan Murdoch

>    
>   Regards,
>    
>  Deb
>    
>  Statistician
> NSW Department of Commerce
> Sydney Australia.
> 
>   
> Note: forwarded message attached.
> 
>  
> ---------------------------------
> Need Mail bonding?
> 
> 
> ------------------------------------------------------------------------
> 
> 
> X-Originating-IP: [129.132.145.15]
> Authentication-Results: mta305.mail.mud.yahoo.com  from=stat.math.ethz.ch; domainkeys=neutral (no sig)
> Received: from 129.132.145.15  (EHLO hypatia.math.ethz.ch) (129.132.145.15)
>   by mta305.mail.mud.yahoo.com with SMTP; Thu, 01 Feb 2007 09:06:15 -0800
> Received: from hypatia.math.ethz.ch (hypatia [129.132.145.15])
> 	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l11Bhc1w009865;
> 	Thu, 1 Feb 2007 12:44:51 +0100
> X-Spam-Checker-Version: SpamAssassin 3.1.7 (2006-10-05) on hypatia.math.ethz.ch
> X-Spam-Level: 
> X-Spam-Status: No, score=0.3 required=5.0 tests=AWL,
> 	SPF_HELO_PASS autolearn=no version=3.1.7
> Received: from talk.nabble.com (www.nabble.com [72.21.53.35])
> 	by hypatia.math.ethz.ch (8.13.6/8.13.6) with ESMTP id l11BLmS8001133
> 	(version=TLSv1/SSLv3 cipher=AES256-SHA bits=256 verify=NO)
> 	for <r-help at stat.math.ethz.ch>; Thu, 1 Feb 2007 12:21:49 +0100
> Received: from [72.21.53.38] (helo=jubjub.nabble.com)
> 	by talk.nabble.com with esmtp (Exim 4.50) id 1HCa0h-0007kZ-Th
> 	for r-help at stat.math.ethz.ch; Thu, 01 Feb 2007 03:21:47 -0800
> Date: Thu, 1 Feb 2007 03:21:47 -0800 (PST)
> From: Vladimir Eremeev <wl2776 at gmail.com>
> To: r-help at stat.math.ethz.ch
> In-Reply-To: <835531.21036.qm at web50407.mail.yahoo.com>
> MIME-Version: 1.0
> X-Nabble-From: wl2776 at gmail.com
> References: <835531.21036.qm at web50407.mail.yahoo.com>
> X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
> Subject: Re: [R] Calling C code from R
> X-BeenThere: r-help at stat.math.ethz.ch
> X-Mailman-Version: 2.1.9
> Precedence: list
> List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
> List-Unsubscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
> List-Archive: <https://stat.ethz.ch/pipermail/r-help>
> List-Post: <mailto:r-help at stat.math.ethz.ch>
> List-Help: <mailto:r-help-request at stat.math.ethz.ch?subject=help>
> List-Subscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
> Content-Type: text/plain; charset="us-ascii"
> Content-Transfer-Encoding: 7bit
> Sender: r-help-bounces at stat.math.ethz.ch
> Errors-To: r-help-bounces at stat.math.ethz.ch
> Content-Length: 762
> 
> 
> You need to install perl and MinGW, at least.
> If you have them installed, then you need to properly set PATH environment
> variable and, probably, restart your command line session.
> 
> See chapter 5 of the manual "Writing R extensions" (installed in
> R_HOME/doc/manual)
> and these two links
> 
> http://www.murdoch-sutherland.com/Rtools/
> http://www.stats.uwo.ca/faculty/murdoch/software/debuggingR/
> 
> Also, it would be great to upgrade R to 2.4.1
> 
> 
> Deb Midya wrote:
>>   I am using R-2.4.0 on Windows XP. I am trying to create dll file.
>>   My C code:
>>   /* useC1.c */
>>   void useC(int *i) {
>>     i[6] = 100;
>> }
>>    
>>   I have tried to create useC1.dll. 
>>   C:\R-2.4.0\bin>R CMD SHLIB useC1.c
>>   'perl' is not recognized as an internal or external command, operable
>> program or batch file.
>>
>>   Then I have tried:
>>   C:\R-2.4.0\bin>Rcmd SHLIB useC1.c
>>   'perl' is not recognized as an internal or external command, operable
>> program or batch file.
>>
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From phhs80 at gmail.com  Sun Feb  4 12:41:53 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Sun, 4 Feb 2007 11:41:53 +0000
Subject: [R] Announcing another R search engine
In-Reply-To: <e6820af20702032015x6d6bdd85x5a9882a11b68f369@mail.gmail.com>
References: <e6820af20702032014j62996da2q8da4594d3729113f@mail.gmail.com>
	<e6820af20702032015x6d6bdd85x5a9882a11b68f369@mail.gmail.com>
Message-ID: <6ade6f6c0702040341r7a813718i42bf0720119814d@mail.gmail.com>

On 2/4/07, Sasha Goodman <sashag at stanford.edu> wrote:
> Oh, I almost forgot:
>
> The search engine is at:
> http://www.rseek.org
>
> --Sasha
>
> On 2/3/07, Sasha Goodman <sashag at stanford.edu> wrote:
> >
> > Hello all,
> >
> > I wanted to announce a new R search engine I made that covers all the
> > major R mailing lists, CRAN, r-project.org, and more. Results are tabbed,
> > so you can refine the search. Current refinements include searching just the
> > mailing lists, searching just introductions, and searching the web for
> > source files ending in .R.
> >
> > Please send comments and suggestions. If you want to add sites to the
> > search, there is no need to contact me. Just hit the volunteer link at the
> > bottom and you can use Google Coop to add links.

Thanks a lot for this very useful search tool, Sasha!

Paul


From duncan at wald.ucdavis.edu  Sun Feb  4 14:47:01 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Sun, 04 Feb 2007 05:47:01 -0800
Subject: [R] CGIwithR
In-Reply-To: <6.1.2.0.2.20070203164109.01c4fb30@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070202155914.01cb4e58@aiminy.mail.iastate.edu>
	<45C3F8F5.3080008@wald.ucdavis.edu>
	<6.1.2.0.2.20070203164109.01c4fb30@aiminy.mail.iastate.edu>
Message-ID: <45C5E3D5.20002@wald.ucdavis.edu>


You should look in the log files from apache for error messages,
specifically the error.log file (typically).


If you are getting no output, the code in the
R script is most likely generating an error
before it creates any output.

So you can have it output information about what it is doing.
There is a variable  verbose  that you can set to
TRUE to have information displayed about the computations
and inputs which might help.

The better way to debug is to use dump.frames.
In the script, put near the top
options(error =
   quote({dump.frames("cgi_error", to.file = TRUE);q()}))

Then, after the script has been run,
start R in the cgi-bin directory and use

load("cgi_error.rda")
debugger(cgi_error)


  D

Aimin Yan wrote:
> 
> Yes, I already read it. I want to try those examples out.
> The question for bootstarpFile.html is:
> 
> when I give several number , and press Submit Query.
> I didn't get any output.
> 
> here is the location of my file
> 
> [root at omega cgi-bin]# pwd
> /var/www/cgi-bin
> [root at omega cgi-bin]# ls -al
> total 68
> drwxr-xr-x  2 root root 4096 Feb  3 16:38 .
> drwxrwxrwx  8 root root 4096 Jan 10 12:24 ..
> -rw-r--r--  1 root root 2219 Feb  2 15:09 bootstrap.R
> -rw-r--r--  1 root root  479 Feb  2 15:48 data.R
> -rw-r--r--  1 root root 3339 Feb  2 15:15 qvweb.R
> -rwxr-xr-x  1 root root 4641 Feb  2 15:42 R.cgi
> -rw-r--r--  1 root root   41 Feb  2 15:43 .Rprofile
> -rw-r--r--  1 root root 1334 Feb  2 15:46 trivial.R
> 
> [root at omega html]# pwd
> /var/www/html
> [root at omega html]# ls -al
> total 48
> drwxrwxrwx  2 root   root   4096 Feb  2 15:52 .
> drwxrwxrwx  8 root   root   4096 Jan 10 12:24 ..
> -rw-rw-r--  1 aiminy aiminy 1737 Feb  2 15:52 bootstrapFile.html
> -rw-rw-r--  1 aiminy aiminy 1682 Feb  2 14:24 bootstrap.html
> -rw-rw-r--  1 aiminy aiminy  904 Jan 10 12:41 course.html
> -rw-rw-r--  1 aiminy aiminy  548 Feb  2 15:46 trivial.html


From mihainica at yahoo.com  Sun Feb  4 15:24:48 2007
From: mihainica at yahoo.com (Mihai Nica)
Date: Sun, 4 Feb 2007 06:24:48 -0800 (PST)
Subject: [R] Download stock prices
Message-ID: <566367.74161.qm@web50811.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070204/55f53f3d/attachment.pl 

From pausas at gmail.com  Sun Feb  4 15:27:34 2007
From: pausas at gmail.com (juli g. pausas)
Date: Sun, 4 Feb 2007 15:27:34 +0100
Subject: [R] reading very large files
In-Reply-To: <1170535678.4890.2.camel@localhost.localdomain>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
	<a17009720702031006n56b6b780wbb40301f599d68e2@mail.gmail.com>
	<1170535678.4890.2.camel@localhost.localdomain>
Message-ID: <a17009720702040627h159f36e0j6872fd6dfb351faa@mail.gmail.com>

Hi all,
The small modification was replacing
Write.Rows <- Chunk[Chunk.Sel - Cuts[i], ]   # (2nd line from the end)
by
Write.Rows <- Chunk[Chunk.Sel - Cuts[i] ]    # Chunk has one dimension only

Running times:
- For the Jim Holtman solution (reading once, using diff and skiping
from one record to the other)
[1] 49.80  0.27 50.96    NA    NA

- For Marc Schwartz solution (reading in chunks of 100000)
[1] 1203.94    9.12 1279.04      NA      NA

Both in R2.4.1, under Windows:
> R.version
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          4.1
year           2006
month          12
day            18
svn rev        40228
language       R
version.string R version 2.4.1 (2006-12-18)
>

Juli




On 03/02/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Sat, 2007-02-03 at 19:06 +0100, juli g. pausas wrote:
> > Thank so much for your help and comments.
> > The approach proposed by Jim Holtman was the simplest and fastest. The
> > approach by Marc Schwartz also worked (after a very small
> > modification).
> >
> > It is clear that a good knowledge of R save a lot of time!! I've been
> > able to do in few minutes a process that was only 1/4th done after 25
> > h!
> >
> > Many thanks
> >
> > Juli
>
> Juli,
>
> Just out of curiosity, what change did you make?
>
> Also, what were the running times for the solutions?
>
> Regards,
>
> Marc
>
>
>


-- 
http://www.ceam.es/pausas


From rcyranek at smail.uni-koeln.de  Sun Feb  4 16:39:51 2007
From: rcyranek at smail.uni-koeln.de (=?iso-8859-1?Q?Ren=E9_Cyranek?=)
Date: Sun, 4 Feb 2007 16:39:51 +0100
Subject: [R] x-axis labeling between thickpoints
Message-ID: <200702041540.l14Fe0Aj005038@smtp.uni-koeln.de>

Hello!

I am trying to plot a histogram, which has the labels of the x-axis not
directly at the tick marks but between them. I tried a lot so far, but did
not find the way to do it. Does somebody know this?

Thanks,
Ren?


From skiadas at hanover.edu  Sun Feb  4 16:49:34 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sun, 4 Feb 2007 10:49:34 -0500
Subject: [R] Reading expressions from character vectors
Message-ID: <DD75F08D-9A1C-42A4-82CD-1EEDD2CE076E@hanover.edu>

Greetings,

I have a problem that I am sure is very straightforward, but I just  
can't wrap my head around it. I've read the help pages on text,  
plotmath, expression, substitute, but somehow I can't find the answer  
to this simple question.

Basically consider the following example:

plot( NULL, xlim = c(0,2), ylim = c(0,2) )
expressions <-  expression( -infinity, infinity )
text( c(0.5,1.5), 1.5, expressions )
labels <- c( "-infinity", "infinity" )
text( c(0.5,1.5), 0.5, as.expression(labels) )

I want the character vector "labels" to be interpreted as  an  
expression vector, and so to appear just like the expressions vector.  
Is this possible? I mean yes, it is probably possible, but how?

I suppose the problem is that the result of as.expression(labels) is  
expression("-infinity", "infinity") instead of expression(-infinity,  
infinity), as I would have liked.  I just can't figure out how to  
convert it to the right thing.

Haris


From h.wickham at gmail.com  Sun Feb  4 17:19:16 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 4 Feb 2007 10:19:16 -0600
Subject: [R] Reading expressions from character vectors
In-Reply-To: <DD75F08D-9A1C-42A4-82CD-1EEDD2CE076E@hanover.edu>
References: <DD75F08D-9A1C-42A4-82CD-1EEDD2CE076E@hanover.edu>
Message-ID: <f8e6ff050702040819r1f6ce5n3fc36e4477edb27d@mail.gmail.com>

On 2/4/07, Charilaos Skiadas <skiadas at hanover.edu> wrote:
> Greetings,
>
> I have a problem that I am sure is very straightforward, but I just
> can't wrap my head around it. I've read the help pages on text,
> plotmath, expression, substitute, but somehow I can't find the answer
> to this simple question.
>
> Basically consider the following example:
>
> plot( NULL, xlim = c(0,2), ylim = c(0,2) )
> expressions <-  expression( -infinity, infinity )
> text( c(0.5,1.5), 1.5, expressions )
> labels <- c( "-infinity", "infinity" )
> text( c(0.5,1.5), 0.5, as.expression(labels) )
>
> I want the character vector "labels" to be interpreted as  an
> expression vector, and so to appear just like the expressions vector.
> Is this possible? I mean yes, it is probably possible, but how?

text( c(0.5,1.5), 0.5, parse(text=labels))  ?

You need to parse the text to get to the expression

Hadley


From ggrothendieck at gmail.com  Sun Feb  4 17:20:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 4 Feb 2007 11:20:22 -0500
Subject: [R] Reading expressions from character vectors
In-Reply-To: <DD75F08D-9A1C-42A4-82CD-1EEDD2CE076E@hanover.edu>
References: <DD75F08D-9A1C-42A4-82CD-1EEDD2CE076E@hanover.edu>
Message-ID: <971536df0702040820s184d8b18kabbc842c5fd30898@mail.gmail.com>

Try

text( c(0.5,1.5), 0.5, parse(text = labels))


On 2/4/07, Charilaos Skiadas <skiadas at hanover.edu> wrote:
> Greetings,
>
> I have a problem that I am sure is very straightforward, but I just
> can't wrap my head around it. I've read the help pages on text,
> plotmath, expression, substitute, but somehow I can't find the answer
> to this simple question.
>
> Basically consider the following example:
>
> plot( NULL, xlim = c(0,2), ylim = c(0,2) )
> expressions <-  expression( -infinity, infinity )
> text( c(0.5,1.5), 1.5, expressions )
> labels <- c( "-infinity", "infinity" )
> text( c(0.5,1.5), 0.5, as.expression(labels) )
>
> I want the character vector "labels" to be interpreted as  an
> expression vector, and so to appear just like the expressions vector.
> Is this possible? I mean yes, it is probably possible, but how?
>
> I suppose the problem is that the result of as.expression(labels) is
> expression("-infinity", "infinity") instead of expression(-infinity,
> infinity), as I would have liked.  I just can't figure out how to
> convert it to the right thing.
>
> Haris
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Sun Feb  4 17:30:52 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 4 Feb 2007 16:30:52 +0000 (GMT)
Subject: [R] Reading expressions from character vectors
In-Reply-To: <DD75F08D-9A1C-42A4-82CD-1EEDD2CE076E@hanover.edu>
References: <DD75F08D-9A1C-42A4-82CD-1EEDD2CE076E@hanover.edu>
Message-ID: <Pine.LNX.4.64.0702041618250.14720@gannet.stats.ox.ac.uk>

On Sun, 4 Feb 2007, Charilaos Skiadas wrote:

> Greetings,
>
> I have a problem that I am sure is very straightforward, but I just
> can't wrap my head around it. I've read the help pages on text,
> plotmath, expression, substitute, but somehow I can't find the answer
> to this simple question.
>
> Basically consider the following example:
>
> plot( NULL, xlim = c(0,2), ylim = c(0,2) )
> expressions <-  expression( -infinity, infinity )
> text( c(0.5,1.5), 1.5, expressions )
> labels <- c( "-infinity", "infinity" )
> text( c(0.5,1.5), 0.5, as.expression(labels) )
>
> I want the character vector "labels" to be interpreted as  an
> expression vector, and so to appear just like the expressions vector.
> Is this possible? I mean yes, it is probably possible, but how?
>
> I suppose the problem is that the result of as.expression(labels) is
> expression("-infinity", "infinity") instead of expression(-infinity,
> infinity), as I would have liked.  I just can't figure out how to
> convert it to the right thing.

The simplest way is almost certainly parse(text=labels).
It is complicated by the fact that -infinity is a call and infinity is a 
symbol.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From skiadas at hanover.edu  Sun Feb  4 17:41:33 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sun, 4 Feb 2007 11:41:33 -0500
Subject: [R] Reading expressions from character vectors
In-Reply-To: <5AA00A9C-21A7-4FE5-AE5E-0DB5A75EECA2@hanover.edu>
References: <DD75F08D-9A1C-42A4-82CD-1EEDD2CE076E@hanover.edu>
	<f8e6ff050702040819r1f6ce5n3fc36e4477edb27d@mail.gmail.com>
	<5AA00A9C-21A7-4FE5-AE5E-0DB5A75EECA2@hanover.edu>
Message-ID: <AD65A02C-D003-4E00-99EA-7D8980AA5138@hanover.edu>

I keep forgetting that this list doesn't default to reply-to-all ;).  
Sorry hadley, you'll get this twice.

On Feb 4, 2007, at 11:39 AM, Charilaos Skiadas wrote:

> On Feb 4, 2007, at 11:19 AM, hadley wickham wrote:
>
>> text( c(0.5,1.5), 0.5, parse(text=labels))  ?
>>
>> You need to parse the text to get to the expression
>
> I just love the response rate and speed of this list, it's one of  
> the best lists I am subscribed to. Thank you all for your  
> responses, it makes more sense now (though I'll probably still want  
> to digest the whole thing in my head for a couple of days, to  
> understand exactly what is going on under the hood ;) ).
>
>> Hadley
>
> Haris

Haris


From nono.231 at gmail.com  Sun Feb  4 18:20:18 2007
From: nono.231 at gmail.com (I. Soumpasis)
Date: Sun, 4 Feb 2007 19:20:18 +0200
Subject: [R] read.spss and encodings
In-Reply-To: <Pine.LNX.4.64.0702010738170.21547@homer21.u.washington.edu>
References: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<Pine.LNX.4.64.0702010738170.21547@homer21.u.washington.edu>
Message-ID: <3ff92a550702040920y731c13a0hae611c84ae74b249@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070204/ee7d06f5/attachment.pl 

From ripley at stats.ox.ac.uk  Sun Feb  4 18:53:04 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 4 Feb 2007 17:53:04 +0000 (GMT)
Subject: [R] read.spss and encodings
In-Reply-To: <3ff92a550702040920y731c13a0hae611c84ae74b249@mail.gmail.com>
References: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<Pine.LNX.4.64.0702010738170.21547@homer21.u.washington.edu>
	<3ff92a550702040920y731c13a0hae611c84ae74b249@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702041733020.15010@gannet.stats.ox.ac.uk>

Most of package 'foreign' was written to support only single-byte 
character sets.  Since CP1253 is not an encoding in use on Linux and your 
value labels are not valid in el_GR.iso88597 (I tried doing this in that 
locale: had you?), I think you are expecting far too much.

That R is unable to read binary files encoded in a charset not supported 
on your own system seems perfectly reasonable for any system, let alone a 
volunteer project.  You are very welcome to contribute a package to read 
such files, of course (and that people did is how package 'foreign' came 
into existence).

On Sun, 4 Feb 2007, I. Soumpasis wrote:

> HI!
>
> This mail is related to Thomas mail so I follow up.
>
> I use Greek language and the spss files with value labels containing greek
> characters can not be imported with read.spss.
>
> I am on:
>
>> sessionInfo()
> R version 2.5.0 Under development (unstable) (2007-02-01 r40632)
> i686-pc-linux-gnu
>
> locale:
> LC_CTYPE=el_GR.UTF-8;LC_NUMERIC=C;LC_TIME=el_GR.UTF-8;LC_COLLATE=el_GR.UTF-8;LC_MONETARY=el_GR.UTF-8;LC_MESSAGES=el_GR.UTF-8;LC_PAPER=el_GR.UTF-8;LC_NAME=el_GR.UTF-8;LC_ADDRESS=el_GR.UTF-8;LC_TELEPHONE=el_GR.UTF-8;LC_MEASUREMENT=el_GR.UTF-8;LC_IDENTIFICATION=el_GR.UTF-8
>
>
> The following files are small examples used below:
> http://users.forthnet.gr/the/isoumpasis/data/1.sav
> http://users.forthnet.gr/the/isoumpasis/data/12.sav<http://users.forthnet.gr/the/isoumpasis/data/12.RData>
>
> The first file has english value labels and can be read:
>> read.spss("~/Desktop/1.sav")
> $VAR1
> [1] "\xf3\xf0\xdf\xf4\xe9     "       "\xf3\xf0\xdf\xf4\xe9     "
> [3] "\xf3\xf0\xdf\xf4\xe9     "       "\xf3\xf0\xdf\xf4\xe9     "
> [5] "\xf3\xf0\xdf\xf4\xe9     "       "\xe3\xf1\xe1\xf6\xe5\xdf\xef   "
> [7] "\xe3\xf1\xe1\xf6\xe5\xdf\xef   " "\xe3\xf1\xe1\xf6\xe5\xdf\xef   "
> [9] "\xe3\xf1\xe1\xf6\xe5\xdf\xef   " "\xf3\xf0\xdf\xf4\xe9     "
> [11] "\xe3\xf1\xe1\xf6\xe5\xdf\xef   "
>
> $VAR2
> [1] 5 6 7 7 5 7 3 5 6 7 8
>
> attr(,"label.table ")
> attr(,"label.table")$VAR1
> NULL
>
> attr(,"label.table")$VAR2
> NULL
>
> I can then convert the characters to greek using Thomas' code, so there is
> no problem here.
>
> In file 12.sav the value labels are greek. The problem is that the file
> cannot be read.
>
>> read.spss("~/Desktop/12.sav")
> Error in read.spss("~/Desktop/12.sav") : error reading system-file header
> In addition: Warning message:
> ~/Desktop/12.sav: position 0: Variable name begins with invalid character
>
> I also tried using use.value.labels=FALSE having the same message.
>
>> read.spss("~/Desktop/12.sav", use.value.labels=FALSE)
> Error in read.spss("~/Desktop/12.sav", use.value.labels = FALSE) :
>    error reading system-file header
> In addition: Warning message:
> ~/Desktop/12.sav: position 0: Variable name begins with invalid character
>
> The encoding of the spss files is windows-1253 (greek). The problem should
> be with other non-ascii characters too. Is there any workaround for this?
>
> Thanks in advance
> I.Soumpasis
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pausas at gmail.com  Sat Feb  3 19:06:01 2007
From: pausas at gmail.com (juli g. pausas)
Date: Sat, 3 Feb 2007 19:06:01 +0100
Subject: [R] reading very large files
In-Reply-To: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
References: <a17009720702020940r30bc8ae3qb6e4676ee5962693@mail.gmail.com>
Message-ID: <a17009720702031006n56b6b780wbb40301f599d68e2@mail.gmail.com>

Thank so much for your help and comments.
The approach proposed by Jim Holtman was the simplest and fastest. The
approach by Marc Schwartz also worked (after a very small
modification).

It is clear that a good knowledge of R save a lot of time!! I've been
able to do in few minutes a process that was only 1/4th done after 25
h!

Many thanks

Juli


On 02/02/07, juli g. pausas <pausas at gmail.com> wrote:
> Hi all,
> I have a large file (1.8 GB) with 900,000 lines that I would like to read.
> Each line is a string characters. Specifically I would like to randomly
> select 3000 lines. For smaller files, what I'm doing is:
>
> trs <- scan("myfile", what= character(), sep = "\n")
>  trs<- trs[sample(length(trs), 3000)]
>
> And this works OK; however my computer seems not able to handle the 1.8 G
> file.
> I thought of an alternative way that not require to read the whole file:
>
> sel <- sample(1:900000, 3000)
> for (i in 1:3000)  {
> un <- scan("myfile", what= character(), sep = "\n", skip=sel[i], nlines=1)
>  write(un, "myfile_short", append=TRUE)
> }
>
> This works on my computer; however it is extremely slow; it read one line
> each time. It is been running for 25 hours and I think it has done less than
> half of the file (Yes, probably I do not have a very good computer and I'm
> working under Windows ...).
> So my question is: do you know any other faster way to do this?
> Thanks in advance
>
> Juli
>
> --
>  http://www.ceam.es/pausas
>


-- 
http://www.ceam.es/pausas


From ericthompso at gmail.com  Sun Feb  4 19:45:39 2007
From: ericthompso at gmail.com (Eric Thompson)
Date: Sun, 4 Feb 2007 13:45:39 -0500
Subject: [R] x-axis labeling between thickpoints
In-Reply-To: <200702041540.l14Fe0Aj005038@smtp.uni-koeln.de>
References: <200702041540.l14Fe0Aj005038@smtp.uni-koeln.de>
Message-ID: <e603d4040702041045s10162688p64831f8f7c5ab1f3@mail.gmail.com>

I think you want the 'at', 'labels', and 'tick' arguments of axis().
This should essentially get you what you want:

> hist(rnorm(100), axes = F)
> axis(side = 1, at = -3:3, lab = F)
> axis(side = 1, at = (-3:3)+0.5, tick = F)



On 2/4/07, Ren? Cyranek <rcyranek at smail.uni-koeln.de> wrote:
> Hello!
>
> I am trying to plot a histogram, which has the labels of the x-axis not
> directly at the tick marks but between them. I tried a lot so far, but did
> not find the way to do it. Does somebody know this?
>
> Thanks,
> Ren?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Sun Feb  4 21:09:20 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 4 Feb 2007 15:09:20 -0500 (EST)
Subject: [R] Announcing another R search engine
In-Reply-To: <e6820af20702032015x6d6bdd85x5a9882a11b68f369@mail.gmail.com>
Message-ID: <998673.92913.qm@web32804.mail.mud.yahoo.com>


--- Sasha Goodman <sashag at stanford.edu> wrote:

> Oh, I almost forgot:
> 
> The search engine is at:
> http://www.rseek.org
> 
> --Sasha

Thanks it looks very useful

> 
> On 2/3/07, Sasha Goodman <sashag at stanford.edu>
> wrote:
> >
> > Hello all,
> >
> > I wanted to announce a new R search engine I made
> that covers all the
> > major R mailing lists, CRAN, r-project.org, and
> more. Results are tabbed,
> > so you can refine the search. Current refinements
> include searching just the
> > mailing lists, searching just introductions, and
> searching the web for
> > source files ending in .R.
> >
> > Please send comments and suggestions. If you want
> to add sites to the
> > search, there is no need to contact me. Just hit
> the volunteer link at the
> > bottom and you can use Google Coop to add links.
> >
> > --Sasha Goodman
> > PhD student in Organizational Behavior
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From RMan54 at cox.net  Sun Feb  4 21:42:46 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Sun, 4 Feb 2007 12:42:46 -0800
Subject: [R] Reference to dataframe and contents
Message-ID: <005301c7489d$069b60e0$0900a8c0@rman>

This is probably easy for experienced users but I could not find a solution.
 
I have several R scripts that process several columns of a dataframe
(several dataframes and columns actually, but simplified for my question).

References such as:
 
myDF$myCol
 
are all over. I like to automate this for other dataframes and columns by
defining a reference only once in the beginning of the script.

One idea is to use strings:

s1 <- "myDF"
S2 <- "myCol"

My question is how to construct the equivalent of myDF$myCol that can be
used as such. Or is there a better solution?

Thanks. The help and discussions on this forum are the best.
Rene
-----------------------------------------
Rene Braeckman, PhD
Clinical Pharmacology & Pharmacometrics
Irvine, California, USA


From jfox at mcmaster.ca  Sun Feb  4 21:54:14 2007
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 4 Feb 2007 15:54:14 -0500
Subject: [R] Reference to dataframe and contents
In-Reply-To: <005301c7489d$069b60e0$0900a8c0@rman>
Message-ID: <20070204205413.JCYX6280.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Rene,

There are several ways to do what you want, including get(s1)[,S2].

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rene Braeckman
> Sent: Sunday, February 04, 2007 3:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Reference to dataframe and contents
> 
> This is probably easy for experienced users but I could not 
> find a solution.
>  
> I have several R scripts that process several columns of a 
> dataframe (several dataframes and columns actually, but 
> simplified for my question).
> 
> References such as:
>  
> myDF$myCol
>  
> are all over. I like to automate this for other dataframes 
> and columns by defining a reference only once in the 
> beginning of the script.
> 
> One idea is to use strings:
> 
> s1 <- "myDF"
> S2 <- "myCol"
> 
> My question is how to construct the equivalent of myDF$myCol 
> that can be used as such. Or is there a better solution?
> 
> Thanks. The help and discussions on this forum are the best.
> Rene
> -----------------------------------------
> Rene Braeckman, PhD
> Clinical Pharmacology & Pharmacometrics
> Irvine, California, USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From skiadas at hanover.edu  Sun Feb  4 22:12:10 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sun, 4 Feb 2007 16:12:10 -0500
Subject: [R] Reference to dataframe and contents
In-Reply-To: <005301c7489d$069b60e0$0900a8c0@rman>
References: <005301c7489d$069b60e0$0900a8c0@rman>
Message-ID: <1FF9B13A-7B10-4AEA-94B0-D6289DDFDDDA@hanover.edu>

On Feb 4, 2007, at 3:42 PM, Rene Braeckman wrote:

> My question is how to construct the equivalent of myDF$myCol that  
> can be
> used as such. Or is there a better solution?
>

I think what you want is ?with and wrapping the whole work you want  
to do in a function.

> Thanks. The help and discussions on this forum are the best.
> Rene

Haris


From murdoch at stats.uwo.ca  Sun Feb  4 23:05:16 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 04 Feb 2007 17:05:16 -0500
Subject: [R] Reference to dataframe and contents
In-Reply-To: <005301c7489d$069b60e0$0900a8c0@rman>
References: <005301c7489d$069b60e0$0900a8c0@rman>
Message-ID: <45C6589C.7040905@stats.uwo.ca>

On 2/4/2007 3:42 PM, Rene Braeckman wrote:
> This is probably easy for experienced users but I could not find a solution.
>  
> I have several R scripts that process several columns of a dataframe
> (several dataframes and columns actually, but simplified for my question).
> 
> References such as:
>  
> myDF$myCol
>  
> are all over. I like to automate this for other dataframes and columns by
> defining a reference only once in the beginning of the script.
> 
> One idea is to use strings:
> 
> s1 <- "myDF"
> S2 <- "myCol"
> 
> My question is how to construct the equivalent of myDF$myCol that can be
> used as such. Or is there a better solution?

I'd pass myDF and the name of the column as a parameters to the 
function, e.g.

myfun <- function( DF, column ) {
   do something now with DF[, column]
}

then call it as

myfun(myDF, "myCol")

Duncan Murdoch

> Thanks. The help and discussions on this forum are the best.
> Rene
> -----------------------------------------
> Rene Braeckman, PhD
> Clinical Pharmacology & Pharmacometrics
> Irvine, California, USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fjbuch at gmail.com  Mon Feb  5 01:20:39 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sun, 4 Feb 2007 19:20:39 -0500
Subject: [R] RSNPper SNPinfo and making it handle a vector
Message-ID: <bd93cdad0702041620o6ef96599i73faef29aefd5f5b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070204/5edd49bb/attachment.pl 

From stvjc at channing.harvard.edu  Mon Feb  5 02:23:55 2007
From: stvjc at channing.harvard.edu (Vincent Carey 525-2265)
Date: Sun, 4 Feb 2007 20:23:55 -0500 (EST)
Subject: [R] RSNPper SNPinfo and making it handle a vector
In-Reply-To: <bd93cdad0702041620o6ef96599i73faef29aefd5f5b@mail.gmail.com>
References: <bd93cdad0702041620o6ef96599i73faef29aefd5f5b@mail.gmail.com>
Message-ID: <Pine.GSO.4.58.0702042015440.17325@capecod.bwh.harvard.edu>


> If I run an analysis which generates statistical tests on many SNPs I would
> naturally want to get more details on the most significant SNPs. Directly
> from within R one can get the information by loading RSNPer (from
> Bioconductor) and simply issuing a command SNPinfo(2073285). Unfortunately,
> the command cannot handle a vector and therefore only wants to do one at a
> time.
> I tried the lapply and sapply functions but was stymied temporarily
> lapply(best.snp,SNPinfo) #where best.snp is a vector of SNPs
> Error in FREQ[[1]] : subscript out of bounds [I do not know what that means]
> Nevertheless, I found that one of the SNPs was causing this and could bypass
> it by using the try function.
>
> lapply(best.snp,function(x) try(SNPinfo(x))) which let R continue
> One can then extract numbers or values out of the resultant output but this
> method lacks finesse and is not easy.

looks pretty easy to me.  there is the nuisance of digging through
the output but i have no use cases or -- to my knowledge, until you
wrote -- any active users other than myself.  so little motivation
to push further.

additionally, snpper is not particularly well maintained (builds seem
pretty old) and i think it is going to be replaced.  so i don't plan
to put much more effort into it until i learn more about snpper.chip.org
durability.

>
> Do you know of another method that would read the data and then write the
> resultant data to a dataframe?

i think you'll be able to do this with your lapply -- and contribute
the code?

>
> Another very useful feature for which I would like to use the same treatment
> is to use the SNPinfo as a way to get the gene information
> lapply(best.snp,function(x) try(geneDetails(SNPinfo(x))))

as above


From fjbuch at gmail.com  Mon Feb  5 03:04:54 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sun, 4 Feb 2007 21:04:54 -0500
Subject: [R] RSNPper SNPinfo and making it handle a vector
In-Reply-To: <Pine.GSO.4.58.0702042015440.17325@capecod.bwh.harvard.edu>
References: <bd93cdad0702041620o6ef96599i73faef29aefd5f5b@mail.gmail.com>
	<Pine.GSO.4.58.0702042015440.17325@capecod.bwh.harvard.edu>
Message-ID: <bd93cdad0702041804u7e0d83fcr7dd2d0e0a7681ab5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070204/997dfb57/attachment.pl 

From mckellercran at gmail.com  Mon Feb  5 05:06:46 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Sun, 4 Feb 2007 23:06:46 -0500
Subject: [R] Download stock prices
In-Reply-To: <566367.74161.qm@web50811.mail.yahoo.com>
References: <566367.74161.qm@web50811.mail.yahoo.com>
Message-ID: <3f547caa0702042006sd187f91wbab10aa6de0423da@mail.gmail.com>

Hi Mihai,

You might check out the Rmetrics bundle, available on the cran
website. I've used its fBasics library to download stock prices. Try
the yahooImport() function and the keystats() function for downloading
specific stock prices. I had to fiddle with the keystats function to
get it to work properly, but I wrote to the writer of the library and
it may have been fixed by now.

Best of luck,

Matt

On 2/4/07, Mihai Nica <mihainica at yahoo.com> wrote:
> gReetings:
>
> Is there any way to download a (or a sample of a) crossection of stock market prices? Or is it possible to use get.hist.quote with a *wild card*?
>
> Thanks,
>
> mihai
>
> Mihai Nica
> 170 East Griffith St. G5
> Jackson, MS 39201
> 601-914-0361
>
>
>
>
>
> ____________________________________________________________________________________
> 8:00? 8:25? 8:40? Find a flick in no time
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics


From newruser at yahoo.com  Mon Feb  5 06:27:39 2007
From: newruser at yahoo.com (new ruser)
Date: Sun, 4 Feb 2007 21:27:39 -0800 (PST)
Subject: [R] novice/beginner's reading list for non-programmers learning R?
Message-ID: <362962.61868.qm@web62204.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070204/1720e6eb/attachment.pl 

From manhthe at yahoo.com  Mon Feb  5 06:52:31 2007
From: manhthe at yahoo.com (Nguyen Manh The)
Date: Sun, 4 Feb 2007 21:52:31 -0800 (PST)
Subject: [R]  Using boot.ci to find lower confident bound
In-Reply-To: <3f547caa0702042006sd187f91wbab10aa6de0423da@mail.gmail.com>
Message-ID: <962027.28971.qm@web38207.mail.mud.yahoo.com>


Dear R-users,
I am trying to find lower confident bound (one side
CI) for a variable of interest using "boot.ci" command
in bootstrap method. In the boot.ci, it provides 5
methods for finding CI (two-side CI). Anyone can help
me with that? Or any suggestions? Do I have to use a
different commands?
Many thanks,
The 

Nguyen Manh The
Department of Statistics
University of Glasgow


 
____________________________________________________________________________________
Don't pick lemons.


From blomsp at ozemail.com.au  Mon Feb  5 07:02:39 2007
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Mon, 05 Feb 2007 17:02:39 +1100
Subject: [R] novice/beginner's reading list for non-programmers learning
 R?
In-Reply-To: <362962.61868.qm@web62204.mail.re1.yahoo.com>
References: <362962.61868.qm@web62204.mail.re1.yahoo.com>
Message-ID: <45C6C87F.2040804@ozemail.com.au>

You will find a lot of information on CRAN, from simple "cheat sheets", 
through to advanced books. Perhaps start with the freely-downloadable 
documents until you find your feet?

Cheers,

Simon.

new ruser wrote:
> Can someone please recommend a novice/beginner's reading list for non-programmers learning R?
> 
>  
> ---------------------------------
> 8:00? 8:25? 8:40?  Find a flick in no time
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for
an answer does not ensure that a reasonable answer
can be extracted from a given body of data.
- John Tukey.


From blomsp at ozemail.com.au  Mon Feb  5 07:06:57 2007
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Mon, 05 Feb 2007 17:06:57 +1100
Subject: [R] Using boot.ci to find lower confident bound
In-Reply-To: <962027.28971.qm@web38207.mail.mud.yahoo.com>
References: <962027.28971.qm@web38207.mail.mud.yahoo.com>
Message-ID: <45C6C981.4050504@ozemail.com.au>

Start by reading the references in ?boot.ci, to understand the different 
methods. Then you pays your money and you takes your choice. (Hint: R is 
free.)

Cheers,

Simon.

Nguyen Manh The wrote:
> Dear R-users,
> I am trying to find lower confident bound (one side
> CI) for a variable of interest using "boot.ci" command
> in bootstrap method. In the boot.ci, it provides 5
> methods for finding CI (two-side CI). Anyone can help
> me with that? Or any suggestions? Do I have to use a
> different commands?
> Many thanks,
> The 
> 
> Nguyen Manh The
> Department of Statistics
> University of Glasgow
> 
> 
>  
> ____________________________________________________________________________________
> Don't pick lemons.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for
an answer does not ensure that a reasonable answer
can be extracted from a given body of data.
- John Tukey.


From ajsaez at ujaen.es  Mon Feb  5 08:23:20 2007
From: ajsaez at ujaen.es (=?ISO-8859-1?Q?Antonio_Jos=E9_S=E1ez_Castillo?=)
Date: Mon, 05 Feb 2007 08:23:20 +0100
Subject: [R] Generalized hypergeometric function 2F1
Message-ID: <45C6DB68.4000405@ujaen.es>


From ripley at stats.ox.ac.uk  Mon Feb  5 08:19:12 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Feb 2007 07:19:12 +0000 (GMT)
Subject: [R] Using boot.ci to find lower confident bound
In-Reply-To: <45C6C981.4050504@ozemail.com.au>
References: <962027.28971.qm@web38207.mail.mud.yahoo.com>
	<45C6C981.4050504@ozemail.com.au>
Message-ID: <Pine.LNX.4.64.0702050707180.13833@gannet.stats.ox.ac.uk>

I think the question was 'how do I get one-sided confidence limits from 
package boot?'   As far as I know you would have to rewrite the code to do 
so, and this looks pretty simple.

Simon is right to point out that you need to understand the methods: some 
of these methods are much worse for one-sided limits, especially the 
so-called 'basic' method.

On Mon, 5 Feb 2007, Simon Blomberg wrote:

> Start by reading the references in ?boot.ci, to understand the different
> methods. Then you pays your money and you takes your choice. (Hint: R is
> free.)
>
> Cheers,
>
> Simon.
>
> Nguyen Manh The wrote:
>> Dear R-users,
>> I am trying to find lower confident bound (one side
>> CI) for a variable of interest using "boot.ci" command
>> in bootstrap method. In the boot.ci, it provides 5
>> methods for finding CI (two-side CI). Anyone can help
>> me with that? Or any suggestions? Do I have to use a
>> different commands?
>> Many thanks,
>> The
>>
>> Nguyen Manh The
>> Department of Statistics
>> University of Glasgow
>>
>>
>>
>> ____________________________________________________________________________________
>> Don't pick lemons.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wht_crl at yahoo.com  Mon Feb  5 08:33:12 2007
From: wht_crl at yahoo.com (carol white)
Date: Sun, 4 Feb 2007 23:33:12 -0800 (PST)
Subject: [R] ran out of iteration in coxph
Message-ID: <31793.56123.qm@web62001.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070204/2c664c62/attachment.pl 

From mel at altk.com  Mon Feb  5 08:44:57 2007
From: mel at altk.com (mel)
Date: Mon, 05 Feb 2007 08:44:57 +0100
Subject: [R] Announcing another R search engine
In-Reply-To: <e6820af20702032015x6d6bdd85x5a9882a11b68f369@mail.gmail.com>
References: <e6820af20702032014j62996da2q8da4594d3729113f@mail.gmail.com>
	<e6820af20702032015x6d6bdd85x5a9882a11b68f369@mail.gmail.com>
Message-ID: <45C6E079.1070100@altk.com>

Sasha Goodman a ?crit :

> http://www.rseek.org

Very nice tool. Great thanks.


From Thomas.Petzoldt at tu-dresden.de  Mon Feb  5 12:01:34 2007
From: Thomas.Petzoldt at tu-dresden.de (Thomas Petzoldt)
Date: Mon, 05 Feb 2007 12:01:34 +0100
Subject: [R] JSS Volume 18 (www.jstatsoft.org)
In-Reply-To: <82A88369-0966-487E-B441-4379F32FC33C@cuddyvalley.org>
References: <82A88369-0966-487E-B441-4379F32FC33C@cuddyvalley.org>
Message-ID: <45C70E8E.4020001@tu-dresden.de>

Jan de Leeuw wrote:
> Special Volume on Spectroscopy and Chemometrics in R
> Guest Edited by Katharine M. Mullen and Ivo H.M. van Stokkum
> 11 contributions, with software packages and examples
> 
> Similar volumes on R in Psychometrics (guest edited by
> Jan de Leeuw) and R in Political Methodology (guest edited
> by Micah Altman and Simon Jackman) are close to being finished.
> 
> Additional volumes are being worked on. Proposals for
> volumes are welcome.

[...]

Dear useRs and developeRs,

one of the additional volumes is

   "Ecology and Ecological Modelling in R"

Please note that the deadline will end next week (2007-02-15). We have 
already a handful interesting papers in review and some more 
announcements, and a few more high-quality contributions are, of course, 
welcome.

More information, see:

http://wiki.r-project.org/rwiki/doku.php?id=misc:r_in_ecology_and_ecological_modelling


Thomas Petzoldt and Thomas Kneib, guest editors



-- 
Thomas Petzoldt
Technische Universitaet Dresden
Institut fuer Hydrobiologie        thomas.petzoldt at tu-dresden.de
01062 Dresden                      http://tu-dresden.de/hydrobiologie/
GERMANY


From milton_ruser at yahoo.com.br  Mon Feb  5 12:41:05 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Mon, 5 Feb 2007 03:41:05 -0800 (PST)
Subject: [R] writing a fomated matrix
Message-ID: <632481.84036.qm@web56603.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/55d5838a/attachment.pl 

From wl2776 at gmail.com  Mon Feb  5 12:57:05 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 5 Feb 2007 03:57:05 -0800 (PST)
Subject: [R] writing a fomated matrix
In-Reply-To: <632481.84036.qm@web56603.mail.re3.yahoo.com>
References: <632481.84036.qm@web56603.mail.re3.yahoo.com>
Message-ID: <8804977.post@talk.nabble.com>



Milton Cezar Ribeiro wrote:
> 
> I have a 1000x1000 matrix and I would like to write it in a ASC file,
> where each row from my matrix 
> are written in a separated line. I tryed write() function, but it don?t
> work fine to me.
> any idea?
> Kind regards, Miltinho
> 

What was unsatisfying?

You could also try 
?write.table 
and 
library(MASS); 
?write.matrix

-- 
View this message in context: http://www.nabble.com/-R--writing-a-fomated-matrix-tf3173848.html#a8804977
Sent from the R help mailing list archive at Nabble.com.


From Roger.Bivand at nhh.no  Mon Feb  5 13:01:34 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 5 Feb 2007 13:01:34 +0100 (CET)
Subject: [R] writing a fomated matrix
In-Reply-To: <632481.84036.qm@web56603.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.44.0702051259120.12369-100000@reclus.nhh.no>

On Mon, 5 Feb 2007, Milton Cezar Ribeiro wrote:

> Hi R-gurus,
> 
> I have a 1000x1000 matrix and I would like to write it in a ASC file,
> where each row from my matrix are written in a separated line. I tryed
> write() function, but it don?t work fine to me.

RSiteSearch("write matrix", restrict = c("functions", "docs"))

takes you to:

library(MASS)
?write.matrix

which ought to get you there. But:

?write

would also have shown that there is an ncolumns argument - using that 
would probably also help.

> 
> any idea?
> 
> Kind regards,
> 
> Miltinho
> 
> __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From stvjc at channing.harvard.edu  Mon Feb  5 13:36:26 2007
From: stvjc at channing.harvard.edu (Vincent Carey 525-2265)
Date: Mon, 5 Feb 2007 07:36:26 -0500 (EST)
Subject: [R] RSNPper SNPinfo and making it handle a vector
In-Reply-To: <bd93cdad0702041804u7e0d83fcr7dd2d0e0a7681ab5@mail.gmail.com>
References: <bd93cdad0702041620o6ef96599i73faef29aefd5f5b@mail.gmail.com> 
	<Pine.GSO.4.58.0702042015440.17325@capecod.bwh.harvard.edu>
	<bd93cdad0702041804u7e0d83fcr7dd2d0e0a7681ab5@mail.gmail.com>
Message-ID: <Pine.GSO.4.58.0702050729030.25170@capecod.bwh.harvard.edu>


> I am not a card-carrying Bioinformatician or Biostatistician. At the risk of
> demonstrating naivete let me ask "if you have reservations about snpper and
> its durability why not query dbSNP?" :
> http://www.ncbi.nlm.nih.gov/projects/SNP/
>
>
> That may be easy for me to "say" since I don't have the skills to do the
> programming, I am a pediatric otolaryngologist.

another good question.  dbSNP did not, at the time RSNPper was created,
provide easy programmatic access to such nicely curated/amalgamated data from various
sources.  i suspect that is still the case.  but there may be other web services
providing information on SNPs, where a clear specification exists regarding what you
issue and what you get back, and what you get back tells you things like SNP
location, role, relation to genes, population frequency, etc..  if you find
one and let me know about it i will consider writing another package to retrieve
SNP-related metadata.

you might look at the biomaRt package in Bioconductor and see if its snp query resolution
facilities meet your needs.


From dimitri.mahieux at student.uclouvain.be  Mon Feb  5 13:37:38 2007
From: dimitri.mahieux at student.uclouvain.be (Mahieux Dimitri)
Date: Mon, 05 Feb 2007 13:37:38 +0100
Subject: [R] Exact matching with grep
Message-ID: <45C72512.4000101@student.uclouvain.be>

Hello,

I would know if it is possible with grep to match a exact string. For 
example, I want to match the string "DP2" (and only this) and grep match 
"DP2BS" too.
I have sought in the grep help but I didn't find what I want.


From P.Dalgaard at biostat.ku.dk  Mon Feb  5 13:55:36 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 05 Feb 2007 13:55:36 +0100
Subject: [R] Exact matching with grep
In-Reply-To: <45C72512.4000101@student.uclouvain.be>
References: <45C72512.4000101@student.uclouvain.be>
Message-ID: <45C72948.4080802@biostat.ku.dk>

Mahieux Dimitri wrote:
> Hello,
>
> I would know if it is possible with grep to match a exact string. For 
> example, I want to match the string "DP2" (and only this) and grep match 
> "DP2BS" too.
> I have sought in the grep help but I didn't find what I want.
>   
"^DP2$" could be what you are looking for, although I wonder why you
didn't just test for equality with == in the first place. Also, word
matching as in "\\<DP2\\>" could be relevant.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From wl2776 at gmail.com  Mon Feb  5 14:19:33 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 5 Feb 2007 05:19:33 -0800 (PST)
Subject: [R] Exact matching with grep
In-Reply-To: <45C72512.4000101@student.uclouvain.be>
References: <45C72512.4000101@student.uclouvain.be>
Message-ID: <8805966.post@talk.nabble.com>



Mahieux Dimitri wrote:
> 
> I would know if it is possible with grep to match a exact string. For 
> example, I want to match the string "DP2" (and only this) and grep match 
> "DP2BS" too.
> I have sought in the grep help but I didn't find what I want.
> 

grep("DP2", {other arguments}, fixed=TRUE)

?grep says that "fixed is logical. If TRUE, pattern is a string to be
matched as is. Overrides all conflicting arguments."
-- 
View this message in context: http://www.nabble.com/-R--Exact-matching-with-grep-tf3174076.html#a8805966
Sent from the R help mailing list archive at Nabble.com.


From P.Dalgaard at biostat.ku.dk  Mon Feb  5 14:36:40 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 05 Feb 2007 14:36:40 +0100
Subject: [R] Exact matching with grep
In-Reply-To: <8805966.post@talk.nabble.com>
References: <45C72512.4000101@student.uclouvain.be>
	<8805966.post@talk.nabble.com>
Message-ID: <45C732E8.90505@biostat.ku.dk>

Vladimir Eremeev wrote:
> Mahieux Dimitri wrote:
>   
>> I would know if it is possible with grep to match a exact string. For 
>> example, I want to match the string "DP2" (and only this) and grep match 
>> "DP2BS" too.
>> I have sought in the grep help but I didn't find what I want.
>>
>>     
>
> grep("DP2", {other arguments}, fixed=TRUE)
>
> ?grep says that "fixed is logical. If TRUE, pattern is a string to be
> matched as is. Overrides all conflicting arguments."
>   
That's not the issue here:

> grep("DP2", c("DP2","x", "DP2BS","y"))

[1] 1 3

> grep("DP2", c("DP2","x", "DP2BS","y"), fixed=TRUE)

[1] 1 3
> grep("^DP2$", c("DP2","x", "DP2BS","y"))
[1] 1
> which("DP2"==c("DP2","x", "DP2BS","y"))
[1] 1



"fixed=TRUE" avoids special interpretation of metacharacters, but that
doesn't solve the problem of matching the entire string.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From kajla at bioinfo.pl  Mon Feb  5 14:48:03 2007
From: kajla at bioinfo.pl (laszlo kajan)
Date: Mon, 05 Feb 2007 14:48:03 +0100
Subject: [R] rgl.snapshot "failed"
Message-ID: <45C73593.8060607@bioinfo.pl>

Dear Roger Koenker,

I have had just the same problem (rgl.snapshot returns "failed") on my 
Fedora Core 6 system.

It took me quite a while to figure out the solution, so I would like to 
post it here to make others' lives easier.

My configuration:

Linux zachariasz.dns6.org 2.6.18-1.2869.fc6 #1 SMP Wed Dec 20 14:51:19 
EST 2006 i686 athlon i386 GNU/Linux

Fedora Core 6

R version 2.4.0 Patched (2006-11-03 r39789)

Package:              rgl
Version:              0.70
Date:                 2007-01-06

What I had to do:

Download the rgl package, install libX11, libX11-devel, libXt, 
libXt-devel, and then run:

--------------------------
R CMD INSTALL --configure-args='--x-includes=/usr/include/X11
  --x-libraries=/usr/lib' path_to_rgl_0.70.tar.gz
--------------------------

The problem is that the ./configure script in the rgl_0.70 package does 
not seem to be able to locate the relevant X include and library paths. 
These have to be given explicitely, as shown.

Hope this will help some in a similar situation.

Best regards,

Laszlo Kajan


From mikewhite.diu at btconnect.com  Mon Feb  5 14:47:53 2007
From: mikewhite.diu at btconnect.com (Mike White)
Date: Mon, 5 Feb 2007 13:47:53 -0000
Subject: [R] Confidence intervals of quantiles
Message-ID: <000701c7492c$3c6bb9b0$6201a8c0@FSSFQCV7BGDVED>

Can anyone please tell me if there is a function to calculate confidence
intervals for the results of the quantile function.
Some of my data is normally distributed but some is also a squewed
distribution or a capped normal distribution. Some of the data sets contain
about 700 values whereas others are smaller with about 100-150 values, so I
would like to see how the confidence intervals change for the different
distributions and different data sizes.

Thanks
Mike White


From tom.hatfield at ae.ctscorp.com  Mon Feb  5 14:50:34 2007
From: tom.hatfield at ae.ctscorp.com (Tom H.)
Date: Mon, 5 Feb 2007 05:50:34 -0800 (PST)
Subject: [R] Beginner Question on Persp()
Message-ID: <8806483.post@talk.nabble.com>


I recently downloaded R for Windows, running on Win XP.  I'm trying to create
a perspective plot but not having any luck after reading the R manual and
several examples found on the Internet.

I have a 100 x 100 matrix of Z data as a tab-delimited text file exported
from Minitab.  I read this in to R using read.delim; this seemed to go ok. 
I created  X and Y using seq() to get 100 divisions for the X and Y axes. 
Again, everything looked good, when I typed X or Y I got a string of 100
numbers.

However, when I try to do persp(x,y,z) I get an error something like: (list)
cannot be coerced to double.  I know the basic data (matrix of z values) is
ok (no hidden non-numeric values) because Minitab will graph it just fine. 
Any suggestions on how to proceed?  Thanks in advanced for your help.
-- 
View this message in context: http://www.nabble.com/Beginner-Question-on-Persp%28%29-tf3174399.html#a8806483
Sent from the R help mailing list archive at Nabble.com.


From jsorkin at grecc.umaryland.edu  Mon Feb  5 14:52:14 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 05 Feb 2007 08:52:14 -0500
Subject: [R] Two ways to deal with age in Cox model
Message-ID: <45C6F03F020000CB000085C0@medicine.umaryland.edu>

I hope one and all will allow a stats question:

When running a cox proportional hazards model ,there are two ways to
deal with age, 
including age as a covariate, or to include age as part of the
follow-up time, viz,

Age as a covariate:

tetest1 <- list(time=  c(4, 3,1,1,2,2,3),
                     status=c(1,NA,1,0,1,1,0),
                     age= c(0, 2,1,1,1,0,0),
                     riskfactor=   c(0, 0,0,0,1,1,1))
 fitagecovariate<-coxph( Surv(time, status) ~ age +riskfactor, test1) 
 fitagecovariate

Age included as part of follow-up time:

 test2<-test1
 test2$timeplusage<-test2$time+test2$age
 fitagefollowup<-coxph( Surv(timeplusage, status) ~ riskfactor, test2)
 fitagefollowup

I would appreciate any thoughts about the differences in the
interpretation of the two models.
One obvious difference is that in the first model (fitagecovariate) one
can make inferences about age and in the second one cannot. I think a
second
difference may be that in the first model the riskfactor is assumed to
have values measured at the values of age where as in the second model
riskfactor is assumed to have given values throughout the subject's
life.

Your thoughts please.

Thanks,
John

R 2.1.1
windows XP

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu

Confidentiality Statement:
This email message, including any attachments, is for the so...{{dropped}}


From P.Dalgaard at biostat.ku.dk  Mon Feb  5 14:58:45 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 05 Feb 2007 14:58:45 +0100
Subject: [R] Beginner Question on Persp()
In-Reply-To: <8806483.post@talk.nabble.com>
References: <8806483.post@talk.nabble.com>
Message-ID: <45C73815.6070909@biostat.ku.dk>

Tom H. wrote:
> I recently downloaded R for Windows, running on Win XP.  I'm trying to create
> a perspective plot but not having any luck after reading the R manual and
> several examples found on the Internet.
>
> I have a 100 x 100 matrix of Z data as a tab-delimited text file exported
> from Minitab.  I read this in to R using read.delim; this seemed to go ok. 
> I created  X and Y using seq() to get 100 divisions for the X and Y axes. 
> Again, everything looked good, when I typed X or Y I got a string of 100
> numbers.
>
> However, when I try to do persp(x,y,z) I get an error something like: (list)
> cannot be coerced to double.  I know the basic data (matrix of z values) is
> ok (no hidden non-numeric values) because Minitab will graph it just fine. 
> Any suggestions on how to proceed?  Thanks in advanced for your help.
>   
Data frames are not matrices. Presumably, you want as.matrix(z).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From rolf at math.unb.ca  Mon Feb  5 15:00:38 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Mon, 5 Feb 2007 10:00:38 -0400 (AST)
Subject: [R] Beginner Question on Persp()
Message-ID: <200702051400.l15E0c4I025906@weisner.math.unb.ca>


Your ``z'' which was read in by read.delim() is a *data frame*
not a matrix.

Do z <- as.matrix(z), then persp(x,y,z) will work.

				cheers,

					Rolf Turner
					rolf at math.unb.ca

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

Original message:

> I recently downloaded R for Windows, running on Win XP.  I'm trying to create
> a perspective plot but not having any luck after reading the R manual and
> several examples found on the Internet.
> 
> I have a 100 x 100 matrix of Z data as a tab-delimited text file exported
> from Minitab.  I read this in to R using read.delim; this seemed to go ok. 
> I created  X and Y using seq() to get 100 divisions for the X and Y axes. 
> Again, everything looked good, when I typed X or Y I got a string of 100
> numbers.
> 
> However, when I try to do persp(x,y,z) I get an error something like: (list)
> cannot be coerced to double.  I know the basic data (matrix of z values) is
> ok (no hidden non-numeric values) because Minitab will graph it just fine. 
> Any suggestions on how to proceed?  Thanks in advanced for your help.


From maechler at stat.math.ethz.ch  Mon Feb  5 15:01:04 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 5 Feb 2007 15:01:04 +0100
Subject: [R] Download stock prices
In-Reply-To: <3f547caa0702042006sd187f91wbab10aa6de0423da@mail.gmail.com>
References: <566367.74161.qm@web50811.mail.yahoo.com>
	<3f547caa0702042006sd187f91wbab10aa6de0423da@mail.gmail.com>
Message-ID: <17863.14496.24861.272428@stat.math.ethz.ch>

Hi Matthew,

>>>>> "Matthew" == Matthew Keller <mckellercran at gmail.com>
>>>>>     on Sun, 4 Feb 2007 23:06:46 -0500 writes:

    Matthew> Hi Mihai, You might check out the Rmetrics bundle,
    Matthew> available on the cran website. I've used its
    Matthew> fBasics library

it's the fBasics *package*; a library is something (actually
more than one thing!) different!

    Matthew> to download stock prices. Try the
    Matthew> yahooImport() function and the keystats() function
    Matthew> for downloading specific stock prices. I had to
    Matthew> fiddle with the keystats function to get it to work
    Matthew> properly, but I wrote to the writer of the library

you mean the maintainer of the *package* 

    Matthew> and it may have been fixed by now.

the function is (now) called  keystatsImport() and is part of
'fCalendar' -- which is automatically required from package
'fBasics'.
help(keystatsImport) contains several examples, unfortunately
explicitly not available through example(), but I can
successfully execute all of them -- and they do work, including
the yahooImport() one.

Martin Maechler, ETH Zurich

    Matthew> Best of luck,

    Matthew> Matt

    Matthew> On 2/4/07, Mihai Nica <mihainica at yahoo.com> wrote:
    >> gReetings:
    >> 
    >> Is there any way to download a (or a sample of a)
    >> crossection of stock market prices? Or is it possible to
    >> use get.hist.quote with a *wild card*?
    >> 
    >> Thanks,
    >> 
    >> mihai
    >> 
    >> Mihai Nica 170 East Griffith St. G5 Jackson, MS 39201
    >> 601-914-0361


    Matthew> -- Matthew C Keller Postdoctoral Fellow Virginia
    Matthew> Institute for Psychiatric and Behavioral Genetics


From mothsailor at googlemail.com  Mon Feb  5 15:01:33 2007
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 5 Feb 2007 14:01:33 +0000
Subject: [R] Beginner Question on Persp()
In-Reply-To: <8806483.post@talk.nabble.com>
References: <8806483.post@talk.nabble.com>
Message-ID: <815b70590702050601p1d9824c8v14a0870c5c01e71e@mail.gmail.com>

read.delim creates a data frame (which is a type of list), whereas
persp requires that the z argument be a matrix.  These are different
classes of object in R -- you might want to read the Introduction to R
that ships with the programme.  Try

persp(x,y,as.matrix(z))

On 05/02/07, Tom H. <tom.hatfield at ae.ctscorp.com> wrote:
>
> I recently downloaded R for Windows, running on Win XP.  I'm trying to create
> a perspective plot but not having any luck after reading the R manual and
> several examples found on the Internet.
>
> I have a 100 x 100 matrix of Z data as a tab-delimited text file exported
> from Minitab.  I read this in to R using read.delim; this seemed to go ok.
> I created  X and Y using seq() to get 100 divisions for the X and Y axes.
> Again, everything looked good, when I typed X or Y I got a string of 100
> numbers.
>
> However, when I try to do persp(x,y,z) I get an error something like: (list)
> cannot be coerced to double.  I know the basic data (matrix of z values) is
> ok (no hidden non-numeric values) because Minitab will graph it just fine.
> Any suggestions on how to proceed?  Thanks in advanced for your help.
> --
> View this message in context: http://www.nabble.com/Beginner-Question-on-Persp%28%29-tf3174399.html#a8806483
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From wl2776 at gmail.com  Mon Feb  5 15:13:19 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 5 Feb 2007 06:13:19 -0800 (PST)
Subject: [R] Exact matching with grep
In-Reply-To: <45C732E8.90505@biostat.ku.dk>
References: <45C72512.4000101@student.uclouvain.be>
	<8805966.post@talk.nabble.com> <45C732E8.90505@biostat.ku.dk>
Message-ID: <8806901.post@talk.nabble.com>


Thank you.
There was my misunderstanding of the documentation.
(Un)fortunately, I haven't met the cases, when my code worked incorrectly.


Peter Dalgaard wrote:
> 
> Vladimir Eremeev wrote:
>> Mahieux Dimitri wrote:
>>> I would know if it is possible with grep to match a exact string. For 
>>> example, I want to match the string "DP2" (and only this) and grep match 
>>> "DP2BS" too.
>>> I have sought in the grep help but I didn't find what I want.
>> grep("DP2", {other arguments}, fixed=TRUE)
>>
>> ?grep says that "fixed is logical. If TRUE, pattern is a string to be
>> matched as is. Overrides all conflicting arguments."
>>   
> That's not the issue here:
>> grep("DP2", c("DP2","x", "DP2BS","y"))
> [1] 1 3
>> grep("DP2", c("DP2","x", "DP2BS","y"), fixed=TRUE)
> [1] 1 3
>> grep("^DP2$", c("DP2","x", "DP2BS","y"))
> [1] 1
>> which("DP2"==c("DP2","x", "DP2BS","y"))
> [1] 1
> 
> "fixed=TRUE" avoids special interpretation of metacharacters, but that
> doesn't solve the problem of matching the entire string.
> 

-- 
View this message in context: http://www.nabble.com/-R--Exact-matching-with-grep-tf3174076.html#a8806901
Sent from the R help mailing list archive at Nabble.com.


From P.Dalgaard at biostat.ku.dk  Mon Feb  5 15:16:44 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 05 Feb 2007 15:16:44 +0100
Subject: [R] Two ways to deal with age in Cox model
In-Reply-To: <45C6F03F020000CB000085C0@medicine.umaryland.edu>
References: <45C6F03F020000CB000085C0@medicine.umaryland.edu>
Message-ID: <45C73C4C.3030708@biostat.ku.dk>

John Sorkin wrote:
> I hope one and all will allow a stats question:
>
> When running a cox proportional hazards model ,there are two ways to
> deal with age, 
> including age as a covariate, or to include age as part of the
> follow-up time, viz,
>
> Age as a covariate:
>
> tetest1 <- list(time=  c(4, 3,1,1,2,2,3),
>                      status=c(1,NA,1,0,1,1,0),
>                      age= c(0, 2,1,1,1,0,0),
>                      riskfactor=   c(0, 0,0,0,1,1,1))
>  fitagecovariate<-coxph( Surv(time, status) ~ age +riskfactor, test1) 
>  fitagecovariate
>
> Age included as part of follow-up time:
>
>  test2<-test1
>  test2$timeplusage<-test2$time+test2$age
>  fitagefollowup<-coxph( Surv(timeplusage, status) ~ riskfactor, test2)
>  fitagefollowup
>
> I would appreciate any thoughts about the differences in the
> interpretation of the two models.
> One obvious difference is that in the first model (fitagecovariate) one
> can make inferences about age and in the second one cannot. I think a
> second
> difference may be that in the first model the riskfactor is assumed to
> have values measured at the values of age where as in the second model
> riskfactor is assumed to have given values throughout the subject's
> life.
>
>   
Model2 is plainly wrong, unless your times can be negative it represents
long stretches of immortality (more obvious if all ages are about
80...)! Presumably, age is the age at entry, so a delayed-entry model
could be appropriate (Surv(age,timeplusage,status)). If this
modification is made, the main difference is that the time-since-entry
scale can not (easily) have a separate effect in the delayed-entry
model. If time is really is time since diagnosis or operation, then that
could be badly wrong.

> Your thoughts please.
>
> Thanks,
> John
>
> R 2.1.1
> windows XP
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> jsorkin at grecc.umaryland.edu
>
> Confidentiality Statement:
> This email message, including any attachments, is for the so...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From mihainica at yahoo.com  Mon Feb  5 15:20:15 2007
From: mihainica at yahoo.com (Mihai Nica)
Date: Mon, 5 Feb 2007 06:20:15 -0800 (PST)
Subject: [R] Download stock prices
Message-ID: <947422.90915.qm@web50801.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/085e25e7/attachment.pl 

From petr.pikal at precheza.cz  Mon Feb  5 15:23:33 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 05 Feb 2007 15:23:33 +0100
Subject: [R] Beginner Question on Persp()
In-Reply-To: <8806483.post@talk.nabble.com>
Message-ID: <45C74BF5.16353.1B9AB90@localhost>

Hi

> x<-1:100
> y<-1:100
> z<-rnorm(100*100)
> dim(z)<-c(100,100)
> persp(x,y,z)

works ok
so problem is with your data. "Z" being OK in Minitab does not mean 
it is read OK to R.


On 5 Feb 2007 at 5:50, Tom H. wrote:

Date sent:      	Mon, 5 Feb 2007 05:50:34 -0800 (PST)
From:           	"Tom H." <tom.hatfield at ae.ctscorp.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Beginner Question on Persp()

> 
> I recently downloaded R for Windows, running on Win XP.  I'm trying to
> create a perspective plot but not having any luck after reading the R
> manual and several examples found on the Internet.
> 
> I have a 100 x 100 matrix of Z data as a tab-delimited text file
> exported from Minitab.  I read this in to R using read.delim; this
> seemed to go ok. I created  X and Y using seq() to get 100 divisions

what does str(Z) says about your data?
z<-list(z)
> persp(x,y,z)
Error in persp.default(x, y, z) : (list) object cannot be coerced to 
'double'

so although z looks like matrix it is actually list

 z1<-z[[1]]
 persp(x,y,z1)

works again OK.

Based on structure of your data you need to extract a matrix for 
using it as z. See ?persp

HTH
Petr

BTW try to transfer it to R just by selection of data list in 
Minitab, pressing Ctrl-C, and using

z <-  read.delim("clipboard")

in R. It works for me.

> for the X and Y axes. Again, everything looked good, when I typed X or
> Y I got a string of 100 numbers.
> 
> However, when I try to do persp(x,y,z) I get an error something like:
> (list) cannot be coerced to double.  I know the basic data (matrix of
> z values) is ok (no hidden non-numeric values) because Minitab will
> graph it just fine. Any suggestions on how to proceed?  Thanks in
> advanced for your help. -- View this message in context:
> http://www.nabble.com/Beginner-Question-on-Persp%28%29-tf3174399.html#
> a8806483 Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From aa2007r at gmail.com  Mon Feb  5 15:24:04 2007
From: aa2007r at gmail.com (AA)
Date: Mon, 5 Feb 2007 09:24:04 -0500
Subject: [R] futures, investment
In-Reply-To: <000001c747cb$f02de020$6501a8c0@tablet>
References: <000001c747cb$f02de020$6501a8c0@tablet>
Message-ID: <55dcc5de0702050624l6c6940e8v8cc96ee4a71656f4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/12e19158/attachment.pl 

From murdoch at stats.uwo.ca  Mon Feb  5 15:33:58 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 05 Feb 2007 09:33:58 -0500
Subject: [R] rgl.snapshot "failed"
In-Reply-To: <45C73593.8060607@bioinfo.pl>
References: <45C73593.8060607@bioinfo.pl>
Message-ID: <45C74056.9080405@stats.uwo.ca>

On 2/5/2007 8:48 AM, laszlo kajan wrote:
> Dear Roger Koenker,
> 
> I have had just the same problem (rgl.snapshot returns "failed") on my 
> Fedora Core 6 system.
> 
> It took me quite a while to figure out the solution, so I would like to 
> post it here to make others' lives easier.
> 
> My configuration:
> 
> Linux zachariasz.dns6.org 2.6.18-1.2869.fc6 #1 SMP Wed Dec 20 14:51:19 
> EST 2006 i686 athlon i386 GNU/Linux
> 
> Fedora Core 6
> 
> R version 2.4.0 Patched (2006-11-03 r39789)
> 
> Package:              rgl
> Version:              0.70
> Date:                 2007-01-06
> 
> What I had to do:
> 
> Download the rgl package, install libX11, libX11-devel, libXt, 
> libXt-devel, and then run:
> 
> --------------------------
> R CMD INSTALL --configure-args='--x-includes=/usr/include/X11
>   --x-libraries=/usr/lib' path_to_rgl_0.70.tar.gz
> --------------------------
> 
> The problem is that the ./configure script in the rgl_0.70 package does 
> not seem to be able to locate the relevant X include and library paths. 
> These have to be given explicitely, as shown.
> 
> Hope this will help some in a similar situation.

The configure script in rgl 0.70 uses some very old autoconf material. 
The next release will replace it with newer versions.  Not sure when 
that will happen...

If you'd like to test the new script you can get a copy from

http://www.stats.uwo.ca/faculty/murdoch/temp/rgl_0.70.552.tar.gz

I don't see the error you solved, so it's hard for me to know if this 
version fixes it, but I would hope you could leave out the extra install 
args, and just use the regular

R CMD INSTALL path_to/rgl_0.70.552.tar.gz

on a wider variety of systems.

Duncan Murdoch


From dimitris.rizopoulos at med.kuleuven.be  Mon Feb  5 15:43:46 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 5 Feb 2007 15:43:46 +0100
Subject: [R] Confidence intervals of quantiles
References: <000701c7492c$3c6bb9b0$6201a8c0@FSSFQCV7BGDVED>
Message-ID: <005f01c74934$0a811ff0$0540210a@www.domain>

you could use the Bootstrap method, using package 'boot', e.g.,

library(boot)

f.quantile <- function(x, ind, ...){
    quantile(x[ind], ...)
}

###########

x <- rgamma(750, 2)
quant.boot <- boot(x, f.quantile, R = 1000, probs = c(0.025, 0.25, 
0.5, 0.75, 0.975))
lapply(1:5, function(i) boot.ci(quant.boot, c(0.90, 0.95), type = 
c("perc", "bca"), index = i))

y <- rgamma(150, 2)
quant.boot <- boot(y, f.quantile, R = 1000, probs = c(0.025, 0.25, 
0.5, 0.75, 0.975))
lapply(1:5, function(i) boot.ci(quant.boot, c(0.90, 0.95), type = 
c("perc", "bca"), index = i))


However, you should be a little bit careful with Bootstrap if you wish 
to obtain CIs for extreme quantiles in small samples.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Mike White" <mikewhite.diu at btconnect.com>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, February 05, 2007 2:47 PM
Subject: [R] Confidence intervals of quantiles


> Can anyone please tell me if there is a function to calculate 
> confidence
> intervals for the results of the quantile function.
> Some of my data is normally distributed but some is also a squewed
> distribution or a capped normal distribution. Some of the data sets 
> contain
> about 700 values whereas others are smaller with about 100-150 
> values, so I
> would like to see how the confidence intervals change for the 
> different
> distributions and different data sizes.
>
> Thanks
> Mike White
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From tom.hatfield at ae.ctscorp.com  Mon Feb  5 15:44:20 2007
From: tom.hatfield at ae.ctscorp.com (Tom H.)
Date: Mon, 5 Feb 2007 06:44:20 -0800 (PST)
Subject: [R] Beginner Question on Persp()
In-Reply-To: <8806483.post@talk.nabble.com>
References: <8806483.post@talk.nabble.com>
Message-ID: <8807458.post@talk.nabble.com>




Tom H. wrote:
> 
> I recently downloaded R for Windows, running on Win XP.  I'm trying to
> create a perspective plot but not having any luck after reading the R
> manual and several examples found on the Internet.
> 
> I have a 100 x 100 matrix of Z data as a tab-delimited text file exported
> from Minitab.  I read this in to R using read.delim; this seemed to go ok. 
> I created  X and Y using seq() to get 100 divisions for the X and Y axes. 
> Again, everything looked good, when I typed X or Y I got a string of 100
> numbers.
> 
> However, when I try to do persp(x,y,z) I get an error something like:
> (list) cannot be coerced to double.  I know the basic data (matrix of z
> values) is ok (no hidden non-numeric values) because Minitab will graph it
> just fine.  Any suggestions on how to proceed?  Thanks in advanced for
> your help.
> 

-- 
View this message in context: http://www.nabble.com/Beginner-Question-on-Persp%28%29-tf3174399.html#a8807458
Sent from the R help mailing list archive at Nabble.com.


From jsorkin at grecc.umaryland.edu  Mon Feb  5 15:44:01 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 05 Feb 2007 09:44:01 -0500
Subject: [R] Two ways to deal with age in Cox model
In-Reply-To: <45C73C4C.3030708@biostat.ku.dk>
References: <45C6F03F020000CB000085C0@medicine.umaryland.edu>
	<45C73C4C.3030708@biostat.ku.dk>
Message-ID: <45C6FC53.A712.00CB.0@grecc.umaryland.edu>

Peter,
Many thanks for your prompt reply.

I think you may have been too quick to dismiss model2; there is no need for time to be negative. The time parameter is Surv represents survival, i.e. follow-up time. We usually start the follow-up clock at the time a subject is enrolled into a study, but this is not the only measure of survival time. One might argue that the clock should start at birth because the subject has survived to birth to plus the time represented by the ususal follow-up clock. 
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu

>>> Peter Dalgaard <P.Dalgaard at biostat.ku.dk> 2/5/2007 9:16 AM >>>
John Sorkin wrote:
> I hope one and all will allow a stats question:
>
> When running a cox proportional hazards model ,there are two ways to
> deal with age, 
> including age as a covariate, or to include age as part of the
> follow-up time, viz,
>
> Age as a covariate:
>
> tetest1 <- list(time=  c(4, 3,1,1,2,2,3),
>                      status=c(1,NA,1,0,1,1,0),
>                      age= c(0, 2,1,1,1,0,0),
>                      riskfactor=   c(0, 0,0,0,1,1,1))
>  fitagecovariate<-coxph( Surv(time, status) ~ age +riskfactor, test1) 
>  fitagecovariate
>
> Age included as part of follow-up time:
>
>  test2<-test1
>  test2$timeplusage<-test2$time+test2$age
>  fitagefollowup<-coxph( Surv(timeplusage, status) ~ riskfactor, test2)
>  fitagefollowup
>
> I would appreciate any thoughts about the differences in the
> interpretation of the two models.
> One obvious difference is that in the first model (fitagecovariate) one
> can make inferences about age and in the second one cannot. I think a
> second
> difference may be that in the first model the riskfactor is assumed to
> have values measured at the values of age where as in the second model
> riskfactor is assumed to have given values throughout the subject's
> life.
>
>   
Model2 is plainly wrong, unless your times can be negative it represents
long stretches of immortality (more obvious if all ages are about
80...)! Presumably, age is the age at entry, so a delayed-entry model
could be appropriate (Surv(age,timeplusage,status)). If this
modification is made, the main difference is that the time-since-entry
scale can not (easily) have a separate effect in the delayed-entry
model. If time is really is time since diagnosis or operation, then that
could be badly wrong.

> Your thoughts please.
>
> Thanks,
> John
>
> R 2.1.1
> windows XP
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> jsorkin at grecc.umaryland.edu 
>
> Confidentiality Statement:
> This email message, including any attachments, is for the so...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Statement:
This email message, including any attachments, is for the so...{{dropped}}


From tom.hatfield at ae.ctscorp.com  Mon Feb  5 15:46:33 2007
From: tom.hatfield at ae.ctscorp.com (Tom H.)
Date: Mon, 5 Feb 2007 06:46:33 -0800 (PST)
Subject: [R] Beginner Question on Persp()
In-Reply-To: <8807458.post@talk.nabble.com>
References: <8806483.post@talk.nabble.com> <8807458.post@talk.nabble.com>
Message-ID: <8807536.post@talk.nabble.com>


Thank you all for taking the time to point out that a data frame is not the
same as a matrix!  I'll give it another shot when I get home tonight.


-- 
View this message in context: http://www.nabble.com/Beginner-Question-on-Persp%28%29-tf3174399.html#a8807536
Sent from the R help mailing list archive at Nabble.com.


From justin_bem at yahoo.fr  Mon Feb  5 15:53:09 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Mon, 5 Feb 2007 14:53:09 +0000 (GMT)
Subject: [R] The ordinal Package
Message-ID: <933234.45524.qm@web23008.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070205/b3f6a914/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Mon Feb  5 16:07:39 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 05 Feb 2007 16:07:39 +0100
Subject: [R] Two ways to deal with age in Cox model
In-Reply-To: <45C6FC53.A712.00CB.0@grecc.umaryland.edu>
References: <45C6F03F020000CB000085C0@medicine.umaryland.edu>	<45C73C4C.3030708@biostat.ku.dk>
	<45C6FC53.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <45C7483B.2050904@biostat.ku.dk>

John Sorkin wrote:
> Peter,
> Many thanks for your prompt reply.
>
> I think you may have been too quick to dismiss model2; there is no need for time to be negative. The time parameter is Surv represents survival, i.e. follow-up time. We usually start the follow-up clock at the time a subject is enrolled into a study, but this is not the only measure of survival time. One might argue that the clock should start at birth because the subject has survived to birth to plus the time represented by the ususal follow-up clock. 
>   

Yes, but your subjects logically cannot die before their recorded age if
I understand the setup correctly. I.e. you have left truncation --
people who die before enrolment are not recorded at all. This is at odds
with the proportional hazards assumption and it is a source of
(potential) grave error if the length of the "immortal" period is
related to the presence of the risk factor.

> John
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> jsorkin at grecc.umaryland.edu
>
>   
>>>> Peter Dalgaard <P.Dalgaard at biostat.ku.dk> 2/5/2007 9:16 AM >>>
>>>>         
> John Sorkin wrote:
>   
>> I hope one and all will allow a stats question:
>>
>> When running a cox proportional hazards model ,there are two ways to
>> deal with age, 
>> including age as a covariate, or to include age as part of the
>> follow-up time, viz,
>>
>> Age as a covariate:
>>
>> tetest1 <- list(time=  c(4, 3,1,1,2,2,3),
>>                      status=c(1,NA,1,0,1,1,0),
>>                      age= c(0, 2,1,1,1,0,0),
>>                      riskfactor=   c(0, 0,0,0,1,1,1))
>>  fitagecovariate<-coxph( Surv(time, status) ~ age +riskfactor, test1) 
>>  fitagecovariate
>>
>> Age included as part of follow-up time:
>>
>>  test2<-test1
>>  test2$timeplusage<-test2$time+test2$age
>>  fitagefollowup<-coxph( Surv(timeplusage, status) ~ riskfactor, test2)
>>  fitagefollowup
>>
>> I would appreciate any thoughts about the differences in the
>> interpretation of the two models.
>> One obvious difference is that in the first model (fitagecovariate) one
>> can make inferences about age and in the second one cannot. I think a
>> second
>> difference may be that in the first model the riskfactor is assumed to
>> have values measured at the values of age where as in the second model
>> riskfactor is assumed to have given values throughout the subject's
>> life.
>>
>>   
>>     
> Model2 is plainly wrong, unless your times can be negative it represents
> long stretches of immortality (more obvious if all ages are about
> 80...)! Presumably, age is the age at entry, so a delayed-entry model
> could be appropriate (Surv(age,timeplusage,status)). If this
> modification is made, the main difference is that the time-since-entry
> scale can not (easily) have a separate effect in the delayed-entry
> model. If time is really is time since diagnosis or operation, then that
> could be badly wrong.
>
>   
>> Your thoughts please.
>>
>> Thanks,
>> John
>>
>> R 2.1.1
>> windows XP
>>
>> John Sorkin M.D., Ph.D.
>> Chief, Biostatistics and Informatics
>> Baltimore VA Medical Center GRECC,
>> University of Maryland School of Medicine Claude D. Pepper OAIC,
>> University of Maryland Clinical Nutrition Research Unit, and
>> Baltimore VA Center Stroke of Excellence
>>
>> University of Maryland School of Medicine
>> Division of Gerontology
>> Baltimore VA Medical Center
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>>
>> (Phone) 410-605-7119
>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>> jsorkin at grecc.umaryland.edu 
>>
>> Confidentiality Statement:
>> This email message, including any attachments, is for the so...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
>> and provide commented, minimal, self-contained, reproducible code.
>>   
>>     
>
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From romi at aon.at  Mon Feb  5 12:09:57 2007
From: romi at aon.at (romi at aon.at)
Date: Mon, 05 Feb 2007 12:09:57 +0100 (MET)
Subject: [R]  vgam cao
Message-ID: <1170673797.45c7108588836@webmail.aon.at>

Hi!
I would like to calculate a constrained additive ordination with the package VGAM. I get the error message: 
Error in ncol(xbig.save) : object "xbig.save" not found

My data consists of 90 samples, 10 species and 3 environmental variables. The species data is continous biomass data, with values from 0 to 3000 micrograms per litre. 

I use the function:
cao (name.of.species.matrix~Temp+LF+Ptot, data=name.of.environmental.data.frame, family=gaussianff, df1.nl=3.5, Bestof=4)

After fitting model 1, I get an error message. Here is the output of model 1:

Obtaining initial values

Using initial values
    Temp    LF  Ptot
lv 0.408 0.189 -0.77

Iteration 1 
initial  value 88934920.105286 
iter  10 value 83029461.332581
iter  20 value 83028498.211367
final  value 83028488.241532 
converged
Error in ncol(xbig.save) : object "xbig.save" not found

I have tried it with fewer or more sites, fewer and more species, df1.nl from 0.5 to 5.5. I always get the same error message.
It only works when I use family=poissonff. But as I have continuous data, I think I can't use poissonff.

Can anybody help me?
Thanks,
Romana




-------------------------------------------
Versendet durch aonWebmail (webmail.aon.at)


From stephenc at ics.mq.edu.au  Sun Feb  4 21:33:45 2007
From: stephenc at ics.mq.edu.au (stephenc at ics.mq.edu.au)
Date: Mon, 5 Feb 2007 07:33:45 +1100 (EST)
Subject: [R] futures, investment, etc
Message-ID: <1846.60.241.49.44.1170621225.squirrel@webmail.ics.mq.edu.au>

Hi



I am just starting to look at R and trading in futures, stock, etc



Can anyone point me to useful background material?


From john.gavin at ubs.com  Mon Feb  5 16:09:59 2007
From: john.gavin at ubs.com (john.gavin at ubs.com)
Date: Mon, 5 Feb 2007 15:09:59 -0000
Subject: [R] Rconsole - setting the size and location of Windows help files
	(Rgui)
Message-ID: <182544D7A3144B42994EEA5662C54E010506FFF5@NLDNC105PEX1.ubsw.net>

Hi,

Using the Rconsole file I can specify the size and location of the Rgui
windows on NT.
e.g.
# Dimensions (in characters) of the console.
rows = 51
columns = 100

How can I specify the size of the help windows that popups 
when I ask for help? e.g. '?help' 
I would like the popup window to have say rows = 51 and columns = 100,
just like the main window but a different location on the screen.

In my .Rprofile file I have set 'options(winhelp=FALSE)'.
                         
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Commodities, FIRC,
UBS Investment Bank, 2nd floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
This communication is issued by UBS AG and/or affiliates to
institutional investors; it is not for private persons. This is a
product of a sales or trading desk and not the Research Dept.
Opinions expressed may differ from those of other divisions of UBS,
including Research.  UBS may trade as principal in instruments
identified herein and may accumulate/have accumulated a long or short
position in instruments or derivatives thereof.  UBS has policies
designed to negate conflicts of interest.  This e-mail is not an
official confirmation of terms and unless stated, is not a
recommendation, offer or solicitation to buy or sell.  Any prices or
quotations contained herein are indicative only.  Communications
may be monitored.

 ? 2006 UBS.  All rights reserved. 
Intended for recipient only and not for further distribution without
the consent of UBS.

UBS Limited is a company registered in England & Wales under company
number 2035362, whose registered office is at 1 Finsbury Avenue,
London, EC2M 2PP, United Kingdom.

UBS AG (London Branch) is registered as a branch of a foreign company
under number BR004507, whose registered office is at
1 Finsbury Avenue, London, EC2M 2PP, United Kingdom.

UBS Clearing and Execution Services Limited is a company registered
in England & Wales under company number 03123037, whose registered
office is at 1 Finsbury Avenue, London, EC2M 2PP, United Kingdom.


From kajla at bioinfo.pl  Mon Feb  5 16:28:43 2007
From: kajla at bioinfo.pl (laszlo kajan)
Date: Mon, 05 Feb 2007 16:28:43 +0100
Subject: [R] rgl.snapshot "failed"
In-Reply-To: <45C74056.9080405@stats.uwo.ca>
References: <45C73593.8060607@bioinfo.pl> <45C74056.9080405@stats.uwo.ca>
Message-ID: <45C74D2B.9040403@bioinfo.pl>

Hello Duncan,

no, it unfortunately does not find the paths. Here's what happens:

---------------------------
$ R CMD INSTALL rgl_0.70.552.tar.gz<ENTER>
...
checking for X... libraries , headers
checking for libpng-config... yes
...
...
g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H 
-I/usr/include/libpng12...
...
---------------------------

As you can see, it does not find the X libraries and headers, 
consequently it fails to construct the correct g++ command (note the 
-I<space>-DHAVE_PNG_H - incorrect definition of HAVE_PNG_H). This latter 
leads to the error, because if I modify the autoconf.ac with the 
following patch:

---------------------------
--- rgl/configure.ac    2006-12-11 12:37:13.000000000 +0100
+++ ../rgl/configure.ac 2007-02-02 23:03:27.800030897 +0100
@@ -29,7 +29,9 @@
    if test x$no_x == xyes ; then
      AC_MSG_ERROR([X11 not found but required, configure aborted.])
    fi
-  CPPFLAGS="${CPPFLAGS} -I${x_includes}"
+  #CPPFLAGS="${CPPFLAGS} -I${x_includes}"
+  # kajla
+  if test "x${x_includes}" != "x" ; then CPPFLAGS="${CPPFLAGS} 
-I${x_includes}"; fi
    LIBS="${LIBS} -L${x_libraries} -lX11 -lXext"
    if test `uname` = "Darwin" ; then
      CPPFLAGS="${CPPFLAGS} -DDarwin"
---------------------------

, run autoconf, re-create the tar.gz and R CMD INSTALL that, then it 
works (rgl.snapshot generates pngs) even in the absence of the paths for X!

I wouldn't be surprised if this turned out to be a problem with Fedora 
Core 6 and the installed packages though, rather than rgl.

Laszlo Kajan




Duncan Murdoch wrote:
> On 2/5/2007 8:48 AM, laszlo kajan wrote:
> 
>> Dear Roger Koenker,
>>
>> I have had just the same problem (rgl.snapshot returns "failed") on my 
>> Fedora Core 6 system.
>>
>> It took me quite a while to figure out the solution, so I would like 
>> to post it here to make others' lives easier.
>>
>> My configuration:
>>
>> Linux zachariasz.dns6.org 2.6.18-1.2869.fc6 #1 SMP Wed Dec 20 14:51:19 
>> EST 2006 i686 athlon i386 GNU/Linux
>>
>> Fedora Core 6
>>
>> R version 2.4.0 Patched (2006-11-03 r39789)
>>
>> Package:              rgl
>> Version:              0.70
>> Date:                 2007-01-06
>>
>> What I had to do:
>>
>> Download the rgl package, install libX11, libX11-devel, libXt, 
>> libXt-devel, and then run:
>>
>> --------------------------
>> R CMD INSTALL --configure-args='--x-includes=/usr/include/X11
>>   --x-libraries=/usr/lib' path_to_rgl_0.70.tar.gz
>> --------------------------
>>
>> The problem is that the ./configure script in the rgl_0.70 package 
>> does not seem to be able to locate the relevant X include and library 
>> paths. These have to be given explicitely, as shown.
>>
>> Hope this will help some in a similar situation.
> 
> 
> The configure script in rgl 0.70 uses some very old autoconf material. 
> The next release will replace it with newer versions.  Not sure when 
> that will happen...
> 
> If you'd like to test the new script you can get a copy from
> 
> http://www.stats.uwo.ca/faculty/murdoch/temp/rgl_0.70.552.tar.gz
> 
> I don't see the error you solved, so it's hard for me to know if this 
> version fixes it, but I would hope you could leave out the extra install 
> args, and just use the regular
> 
> R CMD INSTALL path_to/rgl_0.70.552.tar.gz
> 
> on a wider variety of systems.
> 
> Duncan Murdoch


From jmb at mssl.ucl.ac.uk  Mon Feb  5 16:25:39 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Mon, 5 Feb 2007 15:25:39 +0000 (GMT)
Subject: [R] futures, investment, etc
Message-ID: <200702051525.l15FPdKg019564@msslhb.mssl.ucl.ac.uk>

Hi -----> you didn't give your name!

http://finzi.psych.upenn.edu/search.html is a useful search engine from the help 
archives of R

http://life.bio.sunysb.edu/~dstoebel/R/index.html is a mini-course in using R 
for statistical analysis

http://www.stat.tamu.edu/~ljin/Finance/stat689-R.htm may be of more use to you 
as it is called "Statistics and Finance: An Introduction" in R

Hope some of these links help you - R seems to be mostly about searching around 
on the internet (especially on that first search website) and finding your own 
way of doing things - it's so diverse (for example I am using R for climate and 
statistical analysis and producing graphs and maps).

http://www.r-project.org/posting-guide.html This link is very useful (and I 
don't mean any offence) but it shows you how to ask good questions that are 
likely to prompt useful answers from this R-help mailing list.....people on the 
mailing list are all very busy and involved in their own work, if you need help 
it can be very useful to write to this mailing list, but it pays to spend time 
writing the email using this posting guide - to get the best responses!

Good luck, it's a steep learning curve :-)

Jenny


>X-Spam-Checker-Version: SpamAssassin 3.1.7 (2006-10-05) on hypatia.math.ethz.ch
>X-Spam-Level: ***
>X-Spam-Status: No, score=3.8 required=5.0 tests=BAYES_60, BLANK_LINES_70_80, 
NO_REAL_NAME autolearn=no version=3.1.7
>Date: Mon, 5 Feb 2007 07:33:45 +1100 (EST)
>From: stephenc at ics.mq.edu.au
>To: r-help at stat.math.ethz.ch
>User-Agent: SquirrelMail/1.4.9a
>MIME-Version: 1.0
>X-Priority: 3 (Normal)
>Importance: Normal
>X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
>X-Mailman-Approved-At: Mon, 05 Feb 2007 16:13:51 +0100
>Subject: [R] futures, investment, etc
>X-BeenThere: r-help at stat.math.ethz.ch
>X-Mailman-Version: 2.1.9
>List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
>List-Unsubscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>, 
<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
>List-Archive: <https://stat.ethz.ch/pipermail/r-help>
>List-Post: <mailto:r-help at stat.math.ethz.ch>
>List-Help: <mailto:r-help-request at stat.math.ethz.ch?subject=help>
>List-Subscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>, 
<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
>Content-Transfer-Encoding: 7bit
>X-MSSL-MailScanner-Information: Please contact the ISP for more information
>X-MSSL-MailScanner: No virus found
>X-MSSL-MailScanner-SpamCheck: not spam, SpamAssassin (score=-0.152, required 5, 
BAYES_01 -1.52, NO_REAL_NAME 0.16, PRIORITY_NO_NAME 1.21)
>
>Hi
>
>
>
>I am just starting to look at R and trading in futures, stock, etc
>
>
>
>Can anyone point me to useful background material?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student: long range drought prediction 
Climate Extremes Group
Department of Space and Climate Physics
University College London
Holmbury St Mary 
Dorking, Surrey, RH5 6NT
Tel: 01483 204149
Mob: 07916 139187
Web: http://climate.mssl.ucl.ac.uk


From ripley at stats.ox.ac.uk  Mon Feb  5 17:01:18 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Feb 2007 16:01:18 +0000 (GMT)
Subject: [R] ran out of iteration in coxph
In-Reply-To: <31793.56123.qm@web62001.mail.re1.yahoo.com>
References: <31793.56123.qm@web62001.mail.re1.yahoo.com>
Message-ID: <Pine.LNX.4.64.0702051554240.27564@gannet.stats.ox.ac.uk>

It is quite possible that you are trying to find an MPLE where none 
exists, in which case non-convergence is a good thing.

Using only 300 cases for 188 or 215 variables is asking a lot, especially 
when 26% of them are only partially observed. Since you did not sign your 
message we have no idea of your background, but it looks like the problem 
here is with statistical expectations and not R usage.

On Sun, 4 Feb 2007, carol white wrote:

> hi,
> I applied coxph to my matrix of 300 samples and 215 variables and got the following error
>
> Error in fitter(X, Y, strats, offset, init, control, weights = weights,  :
>        NA/NaN/Inf in foreign function call (arg 6)
> In addition: Warning message:
> Ran out of iterations and did not converge in: fitter(X, Y, strats, offset, init, control, weights = weights,
>
> 26% of time data is censored and here is the result of density of the matrix
>
>       x                y
> Min.   :-6.943   Min.   :-5.258e-18
> 1st Qu.:-4.086   1st Qu.: 2.341e-04
> Median :-1.228   Median : 4.584e-03
> Mean   :-1.228   Mean   : 8.740e-02
> 3rd Qu.: 1.630   3rd Qu.: 5.857e-02
> Max.   :      4.487   Max.   : 7.099e-01
> ------------------------------------------------------------------------------
> in another trial of the same matrix, the number of var is reduced to 188. the variables were normalized (mean = 0 and standard dev = 1 for each variable) and here is the result of density
>       x                 y
> Min.   :-8.1532   Min.   :4.571e-16
> 1st Qu.:-4.3334   1st Qu.:1.043e-04
> Median :-0.5136   Median :1.102e-03
> Mean   :-0.5136   Mean   :6.538e-02
> 3rd Qu.: 3.3062   3rd Qu.:5.373e-02
> Max.   : 7.1260   Max.   :4.337e-01
>
> I get still the warning
> Warning message:
> Ran out of iterations and did not converge in: fitter(X, Y, strats, offset,  init,     control, weights = weights,
> but not error in fitter ....
>
> the number of iterations is 20 and didn't converge. saw from coefficient values and their p-value
>
> Why do I get this message?
>
> Look forward to your reply
>
> carol
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From topkatz at msn.com  Mon Feb  5 17:31:26 2007
From: topkatz at msn.com (Talbot Katz)
Date: Mon, 05 Feb 2007 11:31:26 -0500
Subject: [R] Another loop - deloop question
In-Reply-To: <Pine.LNX.4.64.0702021629090.16185@tajo.ucsd.edu>
Message-ID: <BAY132-F319C9FE8DBE00515812B09AA980@phx.gbl>

Thank you so much!  This is super helpful, even if it turns out that 
vectorization doesn't improve performance in my situation.

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: "Charles C. Berry" <cberry at tajo.ucsd.edu>
>To: Talbot Katz <topkatz at msn.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Another loop - deloop question
>Date: Fri, 2 Feb 2007 17:04:22 -0800
>
>
>Talbot,
>
>Vectorization is not panacea.
>
>For n == 100, m ==1000:
>
>>system.time( for( i in 1:n ){ p[ g[[i]] ] <- p[ g[[i]] ] + y[[i]] })
>[1]  0  0  0 NA NA
>>system.time( p2 <- tapply( unlist(y), unlist(g), sum ))
>[1] 0.16 0.00 0.16   NA   NA
>>all.equal(p,as.vector(p2))
>[1] TRUE
>>system.time( p3 <- xtabs( unlist(y) ~ unlist(g) ) )
>[1] 0.08 0.00 0.08   NA   NA
>>all.equal(p,as.vector(p3))
>[1] TRUE
>>system.time( p4 <- unlist(y) %*% diag(m)[ unlist(g), ] )
>[1] 4.16 0.20 4.36   NA   NA
>>all.equal(p,as.vector(p4))
>[1] TRUE
>
>
>Vectorization has had no victory, Grasshopper.
>
>---
>
>For n == 10000, m == 10, the slowest method above becomes the fastest, and 
>the fastest above becomes the slowest. So, you need to consider the 
>applications to which you will apply this.
>
>Read up on profiling if you really 'feel the need for speed'. (Writing R 
>Extensions 3.2 Profiling R code for speed.)
>
>Chuck
>
>p.s. Please read "Writing R Extensions 3.1 Tidying R code" and follow the 
>wisdom therein.
>
>
>On Fri, 2 Feb 2007, Talbot Katz wrote:
>
>>Hi.
>>
>>You folks are so clever, I thought perhaps you could help me make another
>>procedure more efficient.
>>
>>Right now I have the following setup:
>>
>>p is a vector of length m
>>g is a list of length n, g[[i]] is a vector whose elements are indices of 
>>p,
>>i.e., integers between 1 and m inclusive); the g[[i]] cover the full set
>>1:m, but they don't have to constitute an exact partition, theycan overlap
>>members.
>>y is another list of length n, each y[[i]] is a vector of the same length 
>>as
>>g[[i]].
>>
>>Now I build up the vector p as follows:
>>
>>p=rep(0,m)
>>for(i in 1:n){p[g[[i]]]=p[g[[i]]]+y[[i]]}
>>
>>Can this loop be vectorized?
>>
>>Thanks!
>>
>>--  TMK  --
>>212-460-5430	home
>>917-656-5351	cell
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>
>Charles C. Berry                        (858) 534-2098
>                                          Dept of Family/Preventive 
>Medicine
>E mailto:cberry at tajo.ucsd.edu	         UC San Diego
>http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901
>
>


From tlumley at u.washington.edu  Mon Feb  5 17:39:45 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Feb 2007 08:39:45 -0800 (PST)
Subject: [R] Two ways to deal with age in Cox model
In-Reply-To: <45C6F03F020000CB000085C0@medicine.umaryland.edu>
References: <45C6F03F020000CB000085C0@medicine.umaryland.edu>
Message-ID: <Pine.LNX.4.64.0702050832580.4298@homer21.u.washington.edu>

On Mon, 5 Feb 2007, John Sorkin wrote:

> When running a cox proportional hazards model ,there are two ways to
> deal with age,
> including age as a covariate, or to include age as part of the
> follow-up time, viz,
<snip>
> I would appreciate any thoughts about the differences in the
> interpretation of the two models.
> One obvious difference is that in the first model (fitagecovariate) one
> can make inferences about age and in the second one cannot. I think a
> second
> difference may be that in the first model the riskfactor is assumed to
> have values measured at the values of age where as in the second model
> riskfactor is assumed to have given values throughout the subject's
> life.

There are even more possibilities (a nice example and discussion is in 
Breslow & Day, the example being occupational exposure to nickel and 
later cancer).

The Cox model works by comparing covariates for the observation that 
failed and other observations at risk at the same time, so the comparisons 
are entirely within time-point.

If you use time since start of study you are comparing people with 
different covariates at the same time since start of study.

If you use calendar time you are comparing people with different 
covariates at the same calendar time

If you use age you are comparing people with different covariates at the 
same age.


In an observational study it often is more important to control for age or 
for calendar time than for time since the study started, so these might be 
better time scales.  A disadvantage in some studies with longitudinal data 
is that on the study time scale everyone may have measurements at the same 
time but on other time scales everyone may have measurements at different 
times.


 	-thomas


From tlumley at u.washington.edu  Mon Feb  5 17:45:38 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Feb 2007 08:45:38 -0800 (PST)
Subject: [R] read.spss and encodings
In-Reply-To: <3ff92a550702040920y731c13a0hae611c84ae74b249@mail.gmail.com>
References: <200702011352.51946.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<Pine.LNX.4.64.0702010738170.21547@homer21.u.washington.edu>
	<3ff92a550702040920y731c13a0hae611c84ae74b249@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702050840230.4298@homer21.u.washington.edu>



Thanks for the example file.  I'm trying to work out what, if anything, is 
easy to support reliably for encodings in SPSS.

 	-thomas


On Sun, 4 Feb 2007, I. Soumpasis wrote:

> HI!
>
> This mail is related to Thomas mail so I follow up.
>
> I use Greek language and the spss files with value labels containing greek
> characters can not be imported with read.spss.
>
> I am on:
>
>> sessionInfo()
> R version 2.5.0 Under development (unstable) (2007-02-01 r40632)
> i686-pc-linux-gnu
>
> locale:
> LC_CTYPE=el_GR.UTF-8;LC_NUMERIC=C;LC_TIME=el_GR.UTF-8;LC_COLLATE=el_GR.UTF-8;LC_MONETARY=el_GR.UTF-8;LC_MESSAGES=el_GR.UTF-8;LC_PAPER=el_GR.UTF-8;LC_NAME=el_GR.UTF-8;LC_ADDRESS=el_GR.UTF-8;LC_TELEPHONE=el_GR.UTF-8;LC_MEASUREMENT=el_GR.UTF-8;LC_IDENTIFICATION=el_GR.UTF-8
>
>
> The following files are small examples used below:
> http://users.forthnet.gr/the/isoumpasis/data/1.sav
> http://users.forthnet.gr/the/isoumpasis/data/12.sav<http://users.forthnet.gr/the/isoumpasis/data/12.RData>
>
> The first file has english value labels and can be read:
>> read.spss("~/Desktop/1.sav")
> $VAR1
> [1] "\xf3\xf0\xdf\xf4\xe9     "       "\xf3\xf0\xdf\xf4\xe9     "
> [3] "\xf3\xf0\xdf\xf4\xe9     "       "\xf3\xf0\xdf\xf4\xe9     "
> [5] "\xf3\xf0\xdf\xf4\xe9     "       "\xe3\xf1\xe1\xf6\xe5\xdf\xef   "
> [7] "\xe3\xf1\xe1\xf6\xe5\xdf\xef   " "\xe3\xf1\xe1\xf6\xe5\xdf\xef   "
> [9] "\xe3\xf1\xe1\xf6\xe5\xdf\xef   " "\xf3\xf0\xdf\xf4\xe9     "
> [11] "\xe3\xf1\xe1\xf6\xe5\xdf\xef   "
>
> $VAR2
> [1] 5 6 7 7 5 7 3 5 6 7 8
>
> attr(,"label.table ")
> attr(,"label.table")$VAR1
> NULL
>
> attr(,"label.table")$VAR2
> NULL
>
> I can then convert the characters to greek using Thomas' code, so there is
> no problem here.
>
> In file 12.sav the value labels are greek. The problem is that the file
> cannot be read.
>
>> read.spss("~/Desktop/12.sav")
> Error in read.spss("~/Desktop/12.sav") : error reading system-file header
> In addition: Warning message:
> ~/Desktop/12.sav: position 0: Variable name begins with invalid character
>
> I also tried using use.value.labels=FALSE having the same message.
>
>> read.spss("~/Desktop/12.sav", use.value.labels=FALSE)
> Error in read.spss("~/Desktop/12.sav", use.value.labels = FALSE) :
>    error reading system-file header
> In addition: Warning message:
> ~/Desktop/12.sav: position 0: Variable name begins with invalid character
>
> The encoding of the spss files is windows-1253 (greek). The problem should
> be with other non-ascii characters too. Is there any workaround for this?
>
> Thanks in advance
> I.Soumpasis
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From singularitaet at gmx.net  Mon Feb  5 18:00:57 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Mon, 05 Feb 2007 18:00:57 +0100
Subject: [R] LyX 1.4.3-5 problem generating PDF documents
In-Reply-To: <866007.72903.qm@web32812.mail.mud.yahoo.com>
References: <866007.72903.qm@web32812.mail.mud.yahoo.com>
Message-ID: <45C762C9.2070906@gmx.net>

You are posting in the wrong mailing list, mail your question to the lyx
users list: http://www.lyx.org/internet/mailing.php


John Kane wrote:
> I just changed machines and, at first, could not get
> LyX 1.4.3-4 to even load.  I tried 1.5 which loaded
> nicely but would not give me PDF output.  Removed 1.5
> tried using 1.4.3-5.exe. Result : On loading LyX : 
>   lyx: Disabling socket
>   cont class std::basic_string<char struct std:
> char::char_traits ...
>
> Removed and installed from LyXWin143Complete-2-9.exe. 
> Loads okay but no PDF generation. My PDF reader, Foxit
> Reader loads but nothing is produced.  
>
> I have managed to produce a postscript doc and convert
> it using Ghostscript. 
>
> Could this just be a Foxit problem? As soon as I
> figure out how to reset the Adobe to default pdf
> reader I'll try it out.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From eduarmasrs at yahoo.com.br  Mon Feb  5 19:37:43 2007
From: eduarmasrs at yahoo.com.br (Eduardo Dutra de Armas)
Date: Mon, 5 Feb 2007 15:37:43 -0300
Subject: [R] RdbiPgSQL in R 2.4.1
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAJZDfeLVO6RNtveDzx6TiALCgAAAEAAAAGhIfGxDMK9Jg1YaS0GgIgkBAAAAAA==@yahoo.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/8f16b3f7/attachment.pl 

From murdoch at stats.uwo.ca  Mon Feb  5 19:06:49 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 05 Feb 2007 13:06:49 -0500
Subject: [R] rgl.snapshot "failed"
In-Reply-To: <45C74D2B.9040403@bioinfo.pl>
References: <45C73593.8060607@bioinfo.pl> <45C74056.9080405@stats.uwo.ca>
	<45C74D2B.9040403@bioinfo.pl>
Message-ID: <45C77239.1000802@stats.uwo.ca>

On 2/5/2007 10:28 AM, laszlo kajan wrote:
> Hello Duncan,
> 
> no, it unfortunately does not find the paths. Here's what happens:

Thanks for your patch.  It looks like it would be good on any system, so 
I'll put it in place.

Duncan Murdoch

> 
> ---------------------------
> $ R CMD INSTALL rgl_0.70.552.tar.gz<ENTER>
> ...
> checking for X... libraries , headers
> checking for libpng-config... yes
> ...
> ...
> g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H 
> -I/usr/include/libpng12...
> ...
> ---------------------------
> 
> As you can see, it does not find the X libraries and headers, 
> consequently it fails to construct the correct g++ command (note the 
> -I<space>-DHAVE_PNG_H - incorrect definition of HAVE_PNG_H). This latter 
> leads to the error, because if I modify the autoconf.ac with the 
> following patch:
> 
> ---------------------------
> --- rgl/configure.ac    2006-12-11 12:37:13.000000000 +0100
> +++ ../rgl/configure.ac 2007-02-02 23:03:27.800030897 +0100
> @@ -29,7 +29,9 @@
>     if test x$no_x == xyes ; then
>       AC_MSG_ERROR([X11 not found but required, configure aborted.])
>     fi
> -  CPPFLAGS="${CPPFLAGS} -I${x_includes}"
> +  #CPPFLAGS="${CPPFLAGS} -I${x_includes}"
> +  # kajla
> +  if test "x${x_includes}" != "x" ; then CPPFLAGS="${CPPFLAGS} 
> -I${x_includes}"; fi
>     LIBS="${LIBS} -L${x_libraries} -lX11 -lXext"
>     if test `uname` = "Darwin" ; then
>       CPPFLAGS="${CPPFLAGS} -DDarwin"
> ---------------------------
> 
> , run autoconf, re-create the tar.gz and R CMD INSTALL that, then it 
> works (rgl.snapshot generates pngs) even in the absence of the paths for X!
> 
> I wouldn't be surprised if this turned out to be a problem with Fedora 
> Core 6 and the installed packages though, rather than rgl.
> 
> Laszlo Kajan
> 
> 
> 
> 
> Duncan Murdoch wrote:
>> On 2/5/2007 8:48 AM, laszlo kajan wrote:
>> 
>>> Dear Roger Koenker,
>>>
>>> I have had just the same problem (rgl.snapshot returns "failed") on my 
>>> Fedora Core 6 system.
>>>
>>> It took me quite a while to figure out the solution, so I would like 
>>> to post it here to make others' lives easier.
>>>
>>> My configuration:
>>>
>>> Linux zachariasz.dns6.org 2.6.18-1.2869.fc6 #1 SMP Wed Dec 20 14:51:19 
>>> EST 2006 i686 athlon i386 GNU/Linux
>>>
>>> Fedora Core 6
>>>
>>> R version 2.4.0 Patched (2006-11-03 r39789)
>>>
>>> Package:              rgl
>>> Version:              0.70
>>> Date:                 2007-01-06
>>>
>>> What I had to do:
>>>
>>> Download the rgl package, install libX11, libX11-devel, libXt, 
>>> libXt-devel, and then run:
>>>
>>> --------------------------
>>> R CMD INSTALL --configure-args='--x-includes=/usr/include/X11
>>>   --x-libraries=/usr/lib' path_to_rgl_0.70.tar.gz
>>> --------------------------
>>>
>>> The problem is that the ./configure script in the rgl_0.70 package 
>>> does not seem to be able to locate the relevant X include and library 
>>> paths. These have to be given explicitely, as shown.
>>>
>>> Hope this will help some in a similar situation.
>> 
>> 
>> The configure script in rgl 0.70 uses some very old autoconf material. 
>> The next release will replace it with newer versions.  Not sure when 
>> that will happen...
>> 
>> If you'd like to test the new script you can get a copy from
>> 
>> http://www.stats.uwo.ca/faculty/murdoch/temp/rgl_0.70.552.tar.gz
>> 
>> I don't see the error you solved, so it's hard for me to know if this 
>> version fixes it, but I would hope you could leave out the extra install 
>> args, and just use the regular
>> 
>> R CMD INSTALL path_to/rgl_0.70.552.tar.gz
>> 
>> on a wider variety of systems.
>> 
>> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sfalcon at fhcrc.org  Mon Feb  5 19:07:15 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 05 Feb 2007 10:07:15 -0800
Subject: [R] RdbiPgSQL in R 2.4.1
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAJZDfeLVO6RNtveDzx6TiALCgAAAEAAAAGhIfGxDMK9Jg1YaS0GgIgkBAAAAAA==@yahoo.com.br>
	(Eduardo Dutra de Armas's message of "Mon,
	5 Feb 2007 15:37:43 -0300")
References: <!&!AAAAAAAAAAAYAAAAAAAAAJZDfeLVO6RNtveDzx6TiALCgAAAEAAAAGhIfGxDMK9Jg1YaS0GgIgkBAAAAAA==@yahoo.com.br>
Message-ID: <m2bqk832cs.fsf@ziti.local>

Hi Eduardo,

It would probably be best to send question regarding Bioconductor
packages to the bioconductor email list.

"Eduardo Dutra de Armas" <eduarmasrs at yahoo.com.br> writes:

> Hi R-users
>
> I recently downloaded RdbiPgSQL 1.8.0 and Rdbi 1.8.0 from Bioconductor to be
> installed under R 2.4.1.
>
> When requiring RdbiPgSQL an error message is showed as follows:
>
>  
>
>> require(RdbiPgSQL)
>
> Loading required package: RdbiPgSQL
>
> Error in library(package, lib.loc = lib.loc, character.only = TRUE, logical
> = TRUE,   :
>
>              RdbiPgSQL is not a valid package  installed < 2.0.0?
>
>  
>
> There is no issue about version restriction in Description file.

What OS are you running on?  How did you install the packages?  I'm
not able to reproduce the message you posted.

Perhaps try reinstalling using the biocLite install script.  Like
this:

    source("http://bioconductor.org/biocLite.R")
    biocLite(c("Rdbi", "RdbiPgSQL))

NB: normally, you don't need to specify dependencies, but this way it
will reinstall Rdbi as well in case you have a bogus install...

+ seth


From fugate at lanl.gov  Mon Feb  5 19:30:37 2007
From: fugate at lanl.gov (Mike Fugate)
Date: Mon, 5 Feb 2007 11:30:37 -0700
Subject: [R] random forest proximities
Message-ID: <8ad46ff056ca89d6c8788ce3220877a0@lanl.gov>

Good Day,

I'm using the randomForest package to perform a classification.  If I 
supply weights to the optional classwt argument are proximity values 
computed as a weighted average?  I understand that the forest will 
possibly change as a function of the particular weights I supply.

Thanks in advance.
Mike


Michael Fugate
Los Alamos National Laboratory
Mail Stop MS-F600,
Los Alamos, NM
87545
(505) 667-0398


From octou at ipea.gov.br  Mon Feb  5 20:06:42 2007
From: octou at ipea.gov.br (Octavio Tourinho)
Date: Mon, 05 Feb 2007 17:06:42 -0200
Subject: [R] Problems installing R-2.4.1 on Solaris 11 x-86 from source:
 error in "gmake" after successful "configure"]
Message-ID: <45C78042.2080506@ipea.gov.br>

Dear friends,
I am trying to install R-2.4.1 from source on Solaris 11 x-86_64. 
running on Sun Ultra-20 workstation, and using the SunStudio 11 compilers.

I was able to "configure" R correctly, but received an error in "make", 
aparently related to bzip2 which I have been unable to debug.
The messages are listed below.
The configure.log and configure.site and teh terminal output with the 
error message files are attached.

Any help would be sincerely appreciated, as I have exausted my debugging 
resources.  I believe that there is a problem with the configure script 
in dealing with my particular setup.

Octavio Tourinho

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: config.site
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/d9bfd230/attachment.pl 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Terminal output
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/d9bfd230/attachment-0001.pl 

From adik at ilovebacon.org  Mon Feb  5 20:07:14 2007
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Mon, 5 Feb 2007 11:07:14 -0800 (PST)
Subject: [R] manova discriminant functions?
In-Reply-To: <mailman.7.1170673203.28439.r-help@stat.math.ethz.ch>
References: <mailman.7.1170673203.28439.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0702051102180.11755@parser.ilovebacon.org>

Hello,

 	I've been playing with the manova() function to do some pretty
straightforward multivariate analyses, and I can't for the life of me figure
out how to get at the discriminant functions used. When predicting several
variables simultaneously, it's important to be able to gauge how much each
variable is contributing to the analysis...a simple p-value isn't really
enough. I find examination of the discriminant function loadings to be a
good indicator of this.

Thanks,
Adam Kramer


From zhliur at yahoo.com  Mon Feb  5 20:24:54 2007
From: zhliur at yahoo.com (yyan liu)
Date: Mon, 5 Feb 2007 11:24:54 -0800 (PST)
Subject: [R] "lme" in R and Splus-7
Message-ID: <895233.54498.qm@web35814.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/59cd9eae/attachment.pl 

From fjbuch at gmail.com  Mon Feb  5 20:35:27 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Mon, 5 Feb 2007 14:35:27 -0500
Subject: [R] RSNPper SNPinfo and making it handle a vector
References: <bd93cdad0702041620o6ef96599i73faef29aefd5f5b@mail.gmail.com>
	<Pine.GSO.4.58.0702042015440.17325@capecod.bwh.harvard.edu><bd93cdad0702041804u7e0d83fcr7dd2d0e0a7681ab5@mail.gmail.com>
	<Pine.GSO.4.58.0702050729030.25170@capecod.bwh.harvard.edu>
Message-ID: <eq80u2$cgh$1@sea.gmane.org>

Doesn't this provide a programmer with the information that he would need.
http://www.ncbi.nlm.nih.gov/projects/SNP/SNPeutils.htm

"Vincent Carey 525-2265" <stvjc at channing.harvard.edu> wrote in message 
news:Pine.GSO.4.58.0702050729030.25170 at capecod.bwh.harvard.edu...
> another good question.  dbSNP did not, at the time RSNPper was created,
> provide easy programmatic access to such nicely curated/amalgamated data 
> from various
> sources.  i suspect that is still the case.  but there may be other web 
> services
> providing information on SNPs, where a clear specification exists 
> regarding what you
> issue and what you get back, and what you get back tells you things like 
> SNP
> location, role, relation to genes, population frequency, etc..  if you 
> find
> one and let me know about it i will consider writing another package to 
> retrieve
> SNP-related metadata.
>
> you might look at the biomaRt package in Bioconductor and see if its snp 
> query resolution
> facilities meet your needs.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Roger.Bivand at nhh.no  Mon Feb  5 20:37:17 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 5 Feb 2007 20:37:17 +0100 (CET)
Subject: [R] Problems installing R-2.4.1 on Solaris 11 x-86 from source:
 error in "gmake" after successful "configure"]
In-Reply-To: <45C78042.2080506@ipea.gov.br>
Message-ID: <Pine.LNX.4.44.0702052030250.12369-100000@reclus.nhh.no>

On Mon, 5 Feb 2007, Octavio Tourinho wrote:

> Dear friends,
> I am trying to install R-2.4.1 from source on Solaris 11 x-86_64. 
> running on Sun Ultra-20 workstation, and using the SunStudio 11 compilers.
> 
> I was able to "configure" R correctly, but received an error in "make", 
> aparently related to bzip2 which I have been unable to debug.
> The messages are listed below.
> The configure.log and configure.site and teh terminal output with the 
> error message files are attached.

If you read your site settings *carefully*, you will see that at least 
some are not prefaced by <tag>=. They will then not be recognised and 
inserted where needed, leading to chaos. Please go through your site 
settings once again and correct them. Here is an example that just will 
not work:

## Header file search directory ('-IDIR') and any other miscellaneous
## options (such as defines) for the C preprocessor and compiler.
## If unset defaults to '-I/usr/local/include', with '-I/sw/include'
## prepended on systems using Fink with root '/sw'.
## CPPFLAGS="-I/usr/local/include -I/usr/local/include/readline
-I/usr/sfw/include" 

but you also have:

## The command which runs the C++ compiler.  It not specified, configure
## uses the values of the environment variables 'CXX' or 'CCC' if set,
## and then looks under the names 'c++', 'g++', 'gcc', 'CC', 'cxx', and
## 'cc++' (in that order).
CXX="/opt/SUNWspro/bin/CC -xarch=amd64"



> 
> Any help would be sincerely appreciated, as I have exausted my debugging 
> resources.  I believe that there is a problem with the configure script 
> in dealing with my particular setup.
> 
> Octavio Tourinho
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From mail at friedrich-schuster.de  Mon Feb  5 20:43:11 2007
From: mail at friedrich-schuster.de (Friedrich Schuster)
Date: Mon, 5 Feb 2007 20:43:11 +0100
Subject: [R] The ordinal Package
Message-ID: <200702052043.11928.mail@friedrich-schuster.de>


Hi Justin, 

there seems to be an answer in this post from Peter Dalgaard:

http://tolstoy.newcastle.edu.au/R/help/05/03/1472.html


-- 

Friedrich Schuster
mail at friedrich-schuster.de
Tel.: +49 6221 737474
Tel.: +49 163 7374744


From gunter.berton at gene.com  Mon Feb  5 20:45:32 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 5 Feb 2007 11:45:32 -0800
Subject: [R] "lme" in R and Splus-7
In-Reply-To: <895233.54498.qm@web35814.mail.mud.yahoo.com>
Message-ID: <006301c7495e$32d56db0$4d908980@gne.windows.gene.com>



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of yyan liu
Sent: Monday, February 05, 2007 11:25 AM
To: r-help at stat.math.ethz.ch
Subject: [R] "lme" in R and Splus-7

Hi:
  I used the function "lme" in R and Splus-7. With the same dataset and same
argument for the function, I got quite different estimation results from
these two software. Anyone has this experience before?


> Why don't you try searching the archives yourself to see?

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


From michael.conklin at markettools.com  Mon Feb  5 21:05:13 2007
From: michael.conklin at markettools.com (Michael Conklin)
Date: Mon, 5 Feb 2007 14:05:13 -0600
Subject: [R]  Help with party package
Message-ID: <8EA061E48306894180DB020B0C6907A170DE06@MNMAIL02.markettools.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/f02b1a0b/attachment.pl 

From Roger.Bivand at nhh.no  Mon Feb  5 21:20:09 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 5 Feb 2007 21:20:09 +0100 (CET)
Subject: [R] Problems installing R-2.4.1 on Solaris 11 x-86 from source:
 error in "gmake" after successful "configure"]
In-Reply-To: <Pine.LNX.4.44.0702052030250.12369-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.44.0702052112520.12369-100000@reclus.nhh.no>

On Mon, 5 Feb 2007, Roger Bivand wrote:

> On Mon, 5 Feb 2007, Octavio Tourinho wrote:
> 
> > Dear friends,
> > I am trying to install R-2.4.1 from source on Solaris 11 x-86_64. 
> > running on Sun Ultra-20 workstation, and using the SunStudio 11 compilers.
> > 
> > I was able to "configure" R correctly, but received an error in "make", 
> > aparently related to bzip2 which I have been unable to debug.
> > The messages are listed below.
> > The configure.log and configure.site and teh terminal output with the 
> > error message files are attached.
> 
> If you read your site settings *carefully*, you will see that at least 
> some are not prefaced by <tag>=. They will then not be recognised and 
> inserted where needed, leading to chaos. Please go through your site 
> settings once again and correct them. Here is an example that just will 
> not work:

Sorry, my mistake - the line breaks were in my mail client, not your 
original. Do you want to mix gcc and g77 with the SunStudio compilers? You 
are at the moment (from your configure output), not setting CC or F77 but 
setting CXX and FC. Is this an appropriate list for your question?

> 
> ## Header file search directory ('-IDIR') and any other miscellaneous
> ## options (such as defines) for the C preprocessor and compiler.
> ## If unset defaults to '-I/usr/local/include', with '-I/sw/include'
> ## prepended on systems using Fink with root '/sw'.
> ## CPPFLAGS="-I/usr/local/include -I/usr/local/include/readline
> -I/usr/sfw/include" 
> 
> but you also have:
> 
> ## The command which runs the C++ compiler.  It not specified, configure
> ## uses the values of the environment variables 'CXX' or 'CCC' if set,
> ## and then looks under the names 'c++', 'g++', 'gcc', 'CC', 'cxx', and
> ## 'cc++' (in that order).
> CXX="/opt/SUNWspro/bin/CC -xarch=amd64"
> 
> 
> 
> > 
> > Any help would be sincerely appreciated, as I have exausted my debugging 
> > resources.  I believe that there is a problem with the configure script 
> > in dealing with my particular setup.
> > 
> > Octavio Tourinho
> > 
> > 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From aiminy at iastate.edu  Mon Feb  5 21:59:47 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Mon, 05 Feb 2007 14:59:47 -0600
Subject: [R] rpart
Message-ID: <6.2.3.4.2.20070205145753.03b28a00@aiminy.mail.iastate.edu>

Hello,
I have a question for rpart,
I try to use it to do prediction for a continuous variable.
But I get the different prediction accuracy for same training set, 
anyone know why?

Aimin


From liuwensui at gmail.com  Mon Feb  5 22:11:50 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 5 Feb 2007 16:11:50 -0500
Subject: [R] rpart
In-Reply-To: <6.2.3.4.2.20070205145753.03b28a00@aiminy.mail.iastate.edu>
References: <6.2.3.4.2.20070205145753.03b28a00@aiminy.mail.iastate.edu>
Message-ID: <1115a2b00702051311s128eecddm44e708cde19b24fd@mail.gmail.com>

are you sure you are using the same setting,  tree size, and so on?

On 2/5/07, Aimin Yan <aiminy at iastate.edu> wrote:
> Hello,
> I have a question for rpart,
> I try to use it to do prediction for a continuous variable.
> But I get the different prediction accuracy for same training set,
> anyone know why?
>
> Aimin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From p.dalgaard at biostat.ku.dk  Mon Feb  5 22:15:15 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 05 Feb 2007 22:15:15 +0100
Subject: [R] The ordinal Package
In-Reply-To: <200702052043.11928.mail@friedrich-schuster.de>
References: <200702052043.11928.mail@friedrich-schuster.de>
Message-ID: <45C79E63.8010001@biostat.ku.dk>

Friedrich Schuster wrote:
> Hi Justin, 
>
> there seems to be an answer in this post from Peter Dalgaard:
>
> http://tolstoy.newcastle.edu.au/R/help/05/03/1472.html
>   
(Including a sarcastic remark from yours truly. I'll stand by it -- he's 
moved again, it seems, and the software is now listed as "available on 
request" at

http://popgen.unimaas.nl/~plindsey/rlibs.html )


From David.Duffy at qimr.edu.au  Mon Feb  5 22:37:45 2007
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Tue, 6 Feb 2007 07:37:45 +1000 (EST)
Subject: [R] RSNPper SNPinfo and making it handle a vector
In-Reply-To: <mailman.7.1170673203.28439.r-help@stat.math.ethz.ch>
References: <mailman.7.1170673203.28439.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0702060707400.11528@orpheus.qimr.edu.au>


Farrel Buchinsky <fjbuch at gmail.com> wrote:

> If I run an analysis which generates statistical tests on many SNPs I would
> naturally want to get more details on the most significant SNPs. Directly
> from within R one can get the information by loading RSNPer (from
> Bioconductor) and simply issuing a command SNPinfo(2073285). Unfortunately,
> the command cannot handle a vector and therefore only wants to do one at a
> time.
> 
> Another very useful feature for which I would like to use the same treatment
> is to use the SNPinfo as a way to get the gene information
> lapply(best.snp,function(x) try(geneDetails(SNPinfo(x))))

And Vincent Carey 525-2265 <stvjc at channing.harvard.edu> wrote:

> additionally, snpper is not particularly well maintained (builds seem
> pretty old) and i think it is going to be replaced.  so i don't plan
> to put much more effort into it until i learn more about snpper.chip.org
> durability.


You might instead look at Ensembl via the biomaRt package  -- the HapMap
Biomart interface does not (yet) implement the XML Mart interface that BiomaRt
uses (boo hiss).  These _are_ vectorized.  I have made a couple of 
wrappers specific to SNPs, which you can email me about if they sound 
useful ("snps.by.name", "snps.by.region", "annotate.snps", "maf": they 
probably need to be updated before I show them off ;)).

David Duffy


-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v


From aiminy at iastate.edu  Mon Feb  5 23:08:48 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Mon, 05 Feb 2007 16:08:48 -0600
Subject: [R] rpart
In-Reply-To: <1115a2b00702051311s128eecddm44e708cde19b24fd@mail.gmail.co
 m>
References: <6.2.3.4.2.20070205145753.03b28a00@aiminy.mail.iastate.edu>
	<1115a2b00702051311s128eecddm44e708cde19b24fd@mail.gmail.com>
Message-ID: <6.2.3.4.2.20070205151610.03b34ba0@aiminy.mail.iastate.edu>

Yes, I use the same setting, and I calculate MSE and CC as
prediction accuracy measure.
Someone told me  I should not trust one tree and should do bagging.
Is this correct?
Aimin

At 03:11 PM 2/5/2007, Wensui Liu wrote:
>are you sure you are using the same setting,  tree size, and so on?
>
>On 2/5/07, Aimin Yan <aiminy at iastate.edu> wrote:
>>Hello,
>>I have a question for rpart,
>>I try to use it to do prediction for a continuous variable.
>>But I get the different prediction accuracy for same training set,
>>anyone know why?
>>
>>Aimin
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>--
>WenSui Liu
>A lousy statistician who happens to know a little programming
>(http://spaces.msn.com/statcompute/blog)


From pisicandru at hotmail.com  Mon Feb  5 23:33:59 2007
From: pisicandru at hotmail.com (Monica Pisica)
Date: Mon, 05 Feb 2007 22:33:59 +0000
Subject: [R] strange error in "robust" package
In-Reply-To: <mailman.7.1170673203.28439.r-help@stat.math.ethz.ch>
Message-ID: <BAY125-F2889BDC5C9C7AFC16E8CFCC3980@phx.gbl>

Hi everybody,

I am using quite frequently the "robust" package and until now i never had 
any problems. Actually last time i used it was last Friday very 
successfully.

Anyway, today anytime i want to use the function "fit.models" i get the 
following error even if i use the example form the help file:

data(woodmod.dat)
woodmod.fm <- fit.models(list(Robust = "covRob", Classical = "cov"), data = 
woodmod.dat)

Error in donostah(data, control) : object ".Random.seed" not found
Error in model.list[[i]] : subscript out of bounds

Does anybody know what is wrong?

Thanks,

Monica Palaseanu-Lovejoy
USGS / ETI Pro
St. Petersburg, FL

_________________________________________________________________

Spaces


From lobry at biomserv.univ-lyon1.fr  Mon Feb  5 23:45:41 2007
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Mon, 5 Feb 2007 23:45:41 +0100
Subject: [R] prop.test() references
Message-ID: <p06002003c1ed51c33e70@[192.168.1.12]>

Dear Marc,

>  I believe that this is a modification by Newcombe. See:
>
>  Newcombe RG: Two-Sided Confidence Intervals for the Single Proportion:
>  Comparison of Seven Methods. Statistics in Medicine 1998;17:857-872.
>
>  Newcombe RG: Interval Estimation for the Difference Between Independent
>  Proportions: Comparison of Eleven Methods. Statistics in Medicine
>  1998;17:873-890.

thank you very much for the references. The paragraph with number 4
page 859 in Newcombe is as follows in LaTeX:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
\begin{document}
Score method incorporating continuity corection\cite{six,nineteen}. 
The interval
consists of all $\theta$ such that $\left| p - \theta \right| - \frac{1}{2n}
\le z \sqrt{\frac{\theta(1 - \theta)}{n}}$. Expressions for the lower and
upper limits $L$ and $U$ in closed forms are available:

$$
L = \frac{2np + z^2 -1 -z \sqrt{z^2 -2 -\frac1n + 4p(nq +1)}}{2 (n + z^2)}
$$

$$
U = \frac{2np + z^2 +1 +z \sqrt{z^2 -2 -\frac1n + 4p(nq +1)}}{2 (n + z^2)}
$$

However, if $p = 0$, $L$ must be taken as $0$; if $p = 1$, $U$ is then $1$.

\begin{thebibliography}{99}
\bibitem{six} Blyth, C.R., Still, H.A. (1983) Binomial confidence intervals.
\textit{Journal of the American Statistical Association},\textbf{78}:108-116.
\bibitem{nineteen} Fleiss, J.L. (1981) \textit{Statistical Methods for Rates
and Proportions}, 2nd edn, Wiley, New York, USA.
\end{thebibliography}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After some rearangements, the formulas for the lower (L) and upper (U)
bounds are consistent with the code used in prop.test(). Note that
Newcombe credits them to earlier works.

If I understand well the paper by Newcombe, the continuity correction
in the confidence interval according to Wilson (1927) is always used.

Could it be that it is the same when I'm using prop.test() just to
get the confidence interval as in prop.test(x = 340, n = 400) ?

the code :

YATES <- min(YATES, abs(x - n * p))

seems to have no effect because p is set to 0.5 by

if (is.null(p) && (k == 1))
         p <- 0.5

so that abs(x - n * p)) is at least 0.5

When correct == TRUE, YATES is initially set to 0.5, so
that min(0.5, >= 0.5) is always 0.5.

On an other hand, the doc says that "Continuity
correction is used only if it does not exceed the difference
between sample and null proportions in absolute value", so
that shouldn't it be something like abs(x/n - p) to have
two proportions ?

Sorry if it is obvious, but I'm completely lost here.

Best,
-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From gunter.berton at gene.com  Mon Feb  5 23:45:52 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 5 Feb 2007 14:45:52 -0800
Subject: [R] strange error in "robust" package
In-Reply-To: <BAY125-F2889BDC5C9C7AFC16E8CFCC3980@phx.gbl>
Message-ID: <00b201c74977$642fd260$4d908980@gne.windows.gene.com>

Probably not worth the effort to try and figure out. Try reinstalling the
latest version of the package and repeating. Maybe something got corrupted.
Also, while you're at it, make sure you have the latest version or R
installed and all your other packages are up to date (robust uses some of
them).


Bert Gunter
Nonclinical Statistics
7-7374

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Monica Pisica
Sent: Monday, February 05, 2007 2:34 PM
To: r-help at stat.math.ethz.ch
Subject: [R] strange error in "robust" package
Importance: High

Hi everybody,

I am using quite frequently the "robust" package and until now i never had 
any problems. Actually last time i used it was last Friday very 
successfully.

Anyway, today anytime i want to use the function "fit.models" i get the 
following error even if i use the example form the help file:

data(woodmod.dat)
woodmod.fm <- fit.models(list(Robust = "covRob", Classical = "cov"), data = 
woodmod.dat)

Error in donostah(data, control) : object ".Random.seed" not found
Error in model.list[[i]] : subscript out of bounds

Does anybody know what is wrong?

Thanks,

Monica Palaseanu-Lovejoy
USGS / ETI Pro
St. Petersburg, FL

_________________________________________________________________

Spaces

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From adik at ilovebacon.org  Tue Feb  6 00:12:32 2007
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Mon, 5 Feb 2007 15:12:32 -0800 (PST)
Subject: [R] manova discriminant functions: Addendum
In-Reply-To: <Pine.LNX.4.64.0702051102180.11755@parser.ilovebacon.org>
References: <mailman.7.1170673203.28439.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.64.0702051102180.11755@parser.ilovebacon.org>
Message-ID: <Pine.LNX.4.64.0702051423270.11755@parser.ilovebacon.org>

As an addendum to my earlier post, I am having another difficulty with
getting manova() to behave as I would like: when I specify contrasts for my
independent variable(s), I am unsure of how to test them. This is a
contrived example of both of my questions:

Assume three alertness measurements, "alert1" "alert2" "alert3", a
within-subjects variable measuring their alertness at three timepoints,
minutes after taking the drug (alert1), one hour (alert2), and two hours
(alert3). The between-subjects variable is dosage, with dose==0 when
subjects have had no drug, dose==1 when they have had a single dose, dose==2
when they have had a double dose.

My intuition says to do the following:

alert <- cbind(alert1,alert2,alert3) %*% contr.poly(3)
contrasts(dose) <- matrix(c(2,-1,-1,0,1,-1),3,2)
m <- manova(alert ~ dose)

...what I want is two main effects (dose and alert) and one interaction
(dose by alert), but also "main effect" and "interaction" for the two
individual contrasts for dose. For the main effect for alert, and all of the
dose*alert interactions, I need the discriminant function loadings of my two
alertness contrasts in order to interpret the manner in which alertness is
varying (e.g., is it varying in a linear or quadratic way).

m2 <- manova (alert ~ dose)
summary(m2)
...gives me a test for the dose * alertness interaction. Good! But I can't
seem to find the contrasts I asked for for dose. In univariate ANOVA, I
usually just call summary.lm() which gives me t-test coefficients for each
level of the dose contrast...but calling summary.lm on a manova object
returns t-tests on three unnamed coefficients, with 27 error degrees of
freedom (when it should be 26, as I am intending to compute both dosage
contrasts simultaneously). Also, I cannot tell whether it is the linear or
quadratic contrast that is contributing to the differentiation of dosage
levels--this is why I need the discriminant function loadings.

m2 <- lm( apply(cbind(alert1,alert2,alert3),1,mean) ~ dose)
summary(m2)
...gives me a test for the two contrasts, which can be pooled to get a main
effect. Excellent!

...finally, for the main effect of alertness, I'm more or less at a loss.
The question is whether the three alertness conditions differ from each
other, or whether some linear combination of the linear and quadratic
contrast columns is significantly different from zero...and then the
relative weightings of the linear and quadratic contrasts.

Any suggestions?

Thanks,
Adam

On Mon, 5 Feb 2007, Adam D. I. Kramer wrote:

> Hello,
>
> 	I've been playing with the manova() function to do some pretty
> straightforward multivariate analyses, and I can't for the life of me figure
> out how to get at the discriminant functions used. When predicting several
> variables simultaneously, it's important to be able to gauge how much each
> variable is contributing to the analysis...a simple p-value isn't really
> enough. I find examination of the discriminant function loadings to be a
> good indicator of this.
>
> Thanks,
> Adam Kramer
>


From Mark.Leeds at morganstanley.com  Tue Feb  6 00:21:49 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Mon, 5 Feb 2007 18:21:49 -0500
Subject: [R] ar function in stats
Message-ID: <D3AEEDA31E57474B840BEBC25A8A83440135DB9E@NYWEXMB23.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070205/e17fac9b/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Tue Feb  6 00:41:18 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Feb 2007 23:41:18 -0000 (GMT)
Subject: [R] Confidence intervals of quantiles
In-Reply-To: <000701c7492c$3c6bb9b0$6201a8c0@FSSFQCV7BGDVED>
Message-ID: <XFMail.070205234118.ted.harding@nessie.mcc.ac.uk>

On 05-Feb-07 Mike White wrote:
> Can anyone please tell me if there is a function to calculate
> confidence intervals for the results of the quantile function.
> Some of my data is normally distributed but some is also a
> squewed distribution or a capped normal distribution. Some of
> the data sets contain about 700 values whereas others are smaller
> with about 100-150 values, so I would like to see how the confidence
> intervals change for the different distributions and different data
> sizes.

As well as the bootstrap suggestion (which may do what you want)
I'd like to suggest calcualting exact CIs for the distribution
quantiles. I don't know of an R function which does this, though
the principle is straightforward enough (I once implemented it
for octave/matlab).

I'm assuming you're after a truly distribution-free CI (as your
query suggests). In that case, the sample quantiles provide the
CI limits for the distribution quantiles, with the proviso that
you will not in general get an exact match for the confidence
level P that you desire for your confidence interval, so you
need to use P as a lower bound for the confidence level.

I'll illustrate with an equi-tailed  95% CI.

Notation:

x[r] (r = 1:n) is the r-th order statistic of a sample of n
values of a random variable X, drawn from a continuous distribution.
X[r] is the corresponding random variable.

X(p) is the p-th quantile of the distribution in question,
i.e. the value of X such that P(X <= X[p]) = p. 0 < p < 1.

Objective: You want a 95% CI (X(p)L,X(p).R) for the quantile X(p)
for given p.

Principle:

  P( X[r] <= X(p) ) = pbinom(r, n, p)      [*]

(i.e. the probability that you get at least r of the x's
below X(p)).

Now, for the upper limit X(p).R of the CI, you want the
smallest value of X(p) such that the probability [*] is not
less than 0.925 (i.e. 1 - (1-P)/2 = 1 - (1-0.95)/2 ).

This is achieved by X(p).R = x[s] where

  s = min(which(pbinom((0:n),n,p) >= 0.025))

and, similalry, for the lower limit, X(p).L = x[r] where

  r = max(which(pbinom((0:n),n,p) <= 0.025))

(Note that the "which" counts the binomial case "r=0" as number 1)

If I've got my head around the above right, the following
function does the above job, and caould be fairly easily
modified for asymmetric confidence intervals (e.g. a 95%
confidence interval with 96% for inclusion below the upper
limit and 99% for inclusion above the lower limit).

q.CI<-function(x,p,P){
# x is the sample, p (0<p<1) the quantile, P the confidence level
  x<-sort(x)
  n<-length(x)
  s <- min(which(pbinom((0:n),n,p) >= 1-(1-P)/2))
  r <- max(which(pbinom((0:n),n,p) <= (1-P)/2))
  c(x[r],x[s])
# x[r] is the lower limit, x[s] the upper limit, of the CI
}

Note that it gives a confidence level P at least equal to P,
not in general exactly equal to P.

I've tested it as follows (10000 simulations with samples
of size 101 from a Normal distribution -- why not, after
all? No loss of generality!):

p<-0.5; Xq<-qnorm(p); P<-0.95
N<-10000; incls<-numeric(N)
for(i in (1:N)){
  x<-rnorm(101)
  CI<-q.CI(x,p,P)
  if((CI[1]<=Xq)&(CI[2]>=Xq)){ incls[i]<-1 }
}; sum(incls)/N
# [1] 0.9564
# [1] 0.9548

p<-0.75; Xq<-qnorm(p); P<-0.95
[etc. ... ]
# [1] 0.9631
# [1] 0.9596

p<-0.90; Xq<-qnorm(p); P<-0.95
[etc. ... ]
# [1] 0.9540
# [1] 0.9533

p<-0.95; Xq<-qnorm(p); P<-0.95
[etc. ... ]
# [1] 0.9840
# [1] 0.9826

p<-0.99; Xq<-qnorm(p); P<-0.95
[etc. ... ]
Error in if ((CI[1] <= Xq) & (CI[2] >= Xq)) { : 
        missing value where TRUE/FALSE needed

showing that for extreme values of p relative to the
sample size, trouble occurs (as is to be expected)!

The solution is to test for "NA" in CI[1] and/or CI[2],
so I modified my test routine as follows, to give notional
lower confidence limit -Inf if CI[1] is NA, and/or upper
confidence limit +Inf if CI[2] is NA (of course this
could be done in the function q.CI() itself, but perhaps
it offers more flexibility for the user to do something
else with it, if it is left as NA).

p<-0.99; Xq<-qnorm(p); P<-0.95
N<-10000; incls<-numeric(N)
for(i in (1:N)){
  x<-rnorm(101)
  CI<-q.CI(x,p,P)
  if(is.na(CI[1])){ CI[1]<-(-Inf) }
  if(is.na(CI[2])){ CI[2]<-(+Inf) }
  if((CI[1]<=Xq)&(CI[2]>=Xq)){ incls[i]<-1 }
}; sum(incls)/N
# [1] 0.9841
# [1] 0.9821

Comments welcome!!!

Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Feb-07                                       Time: 23:40:23
------------------------------ XFMail ------------------------------


From ggrothendieck at gmail.com  Tue Feb  6 00:42:12 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Feb 2007 18:42:12 -0500
Subject: [R] ar function in stats
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A83440135DB9E@NYWEXMB23.msad.ms.com>
References: <D3AEEDA31E57474B840BEBC25A8A83440135DB9E@NYWEXMB23.msad.ms.com>
Message-ID: <971536df0702051542q444673q2ac65601ad9f822f@mail.gmail.com>

Try:

> ar(testSeries, demean = FALSE, method = "burg", var.method = 2)

Call:
ar(x = testSeries, method = "burg", demean = FALSE, var.method = 2)

Coefficients:
      1        2        3        4        5        6
 0.0888   0.0063   0.0654  -0.0169   0.0215   0.0950

Order selected 6  sigma^2 estimated as  5.194e-09


On 2/5/07, Leeds, Mark (IED) <Mark.Leeds at morganstanley.com> wrote:
> I had a couple of questions about the ar function that i was hoping
> someone could answer.
>
> I have the structure below
>
> testSeries<-structure(c(-3.88613620955214e-05, 0, -7.77272551011343e-05,
>
> 0, -0.000194344573539562, -0.000116624876218163, -3.88779814601281e-05,
> 0, 3.88779814601281e-05, -0.000155520995647807, -0.000116656621367561,
> -3.88885648225368e-05, -3.88900772017586e-05, 7.77786420242954e-05,
> 0, 7.77725929772544e-05, 0, 0, -3.88855404165889e-05,
> -0.000155557284283514,
> 0.000116670231722849, 7.77725929772544e-05, 0, 0, 3.88840283903069e-05,
> -0.000116656621367561, -7.77786420242954e-05, 0.000233317779873343,
> -0.000155539137849048, 0, 0, -3.88885648225368e-05, 0, 0,
> 0.000116661157799791,
> 3.88840283903069e-05, 0, -3.88840283903069e-05, 7.77665448717935e-05,
> 7.77604977061919e-05, 0.000155502857678291, 0, 0, 7.77423618523176e-05,
> -7.77423618523176e-05, 3.88719364105006e-05, 3.88704254418171e-05,
> -3.88704254418171e-05, 3.88704254418171e-05, 0.0002331908288839,
> -7.77242344552342e-05, 0, -7.7730275981791e-05, -7.77363184468749e-05,
> -7.77423618523176e-05, 0, -0.000116624876218163, -3.88779814601281e-05,
> -0.000233299635555240, 7.77725929772544e-05, 7.77665448717935e-05,
> 0.00011663847916632, 7.7751428721684e-05, 0, 3.88734474964791e-05,
> 3.88719364105006e-05, 7.77393400321347e-05, 3.88674038565573e-05,
> -3.88674038565573e-05, 0, 7.77332970969269e-05, -3.88658932403696e-05,
> -3.88674038565573e-05, 0, 0, 3.88674038565573e-05, 7.7730275981791e-05,
> -7.7730275981791e-05, 0, 7.7730275981791e-05, 0, 0,
> 0.000233154582543582,
> 0.000194253968248181, 3.88462659076660e-05, 7.76880050110673e-05,
> -7.76880050110673e-05, 0, 7.76880050110673e-05, -3.88432480773471e-05,
> 0, 3.88432480773471e-05, 0, 0, 0, -3.88432480773471e-05,
> -0.000194238875579067,
> 0, -3.88523029751786e-05, -3.88538125353222e-05, -0.000116570496139390,
> -3.8859851948958e-05, 0, 0, 0.000155430348088348, 0,
> 3.88538125353222e-05,
> 0, 3.88523029751786e-05, 0, 3.88507935322746e-05, 3.88492842067212e-05,
> 0, 0, -7.77000777389958e-05, 0, 0, -3.88523029751786e-05,
> -3.88538125353222e-05,
> 0, 0.000155406193249497, 0, -0.000155406193249497,
> -0.000116570496139390,
> -3.8859851948958e-05, -7.77242344552342e-05, -3.88643827414215e-05,
> 3.88643827414215e-05, 3.88628723597129e-05, -7.77272551011343e-05,
> -3.88658932403696e-05, 0.000116593148341504, 0, 7.77212140444794e-05,
> 0, 0, 0, 3.88583419195232e-05, 7.77121542198667e-05, 0, 0,
> 7.77061155105008e-05,
> 0, 7.77000777389958e-05, 0, 0, -7.77000777389958e-05,
> 7.77000777389958e-05,
> 0, -7.77000777389958e-05, 3.88507935322746e-05, 3.88492842067212e-05,
> -7.77000777389958e-05, 7.77000777389958e-05, -7.77000777389958e-05,
> 7.77000777389958e-05, -3.88492842067212e-05, 3.88492842067212e-05,
> 0, 0, 0, 3.88477749986849e-05, -3.88477749986849e-05, 0, 0,
> -3.88492842067212e-05,
> 0, -3.88507935322746e-05, 0, 0, -0.000155418269730367, 0,
> 3.88568320072724e-05,
> -3.88568320072724e-05, 0, 0, 0, 0, 0, -7.77181938684812e-05,
> -7.77242344552342e-05, 7.77242344552342e-05, 0, 7.77181938684812e-05,
> 0, -7.77181938684812e-05, 3.8859851948958e-05, 0, 0,
> -3.8859851948958e-05,
> 0, 0, 0, -7.77242344552342e-05, -0.000233208956280984,
> -0.000155502857678291,
> 0.000155502857678291, -3.88734474964791e-05, 0, 3.88734474964791e-05
> ), index = structure(c(1115056920, 1115056980, 1115057040, 1115057100,
> 1115057160, 1115057220, 1115057280, 1115057340, 1115057400, 1115057460,
> 1115057520, 1115057580, 1115057640, 1115057700, 1115057760, 1115057820,
> 1115057880, 1115057940, 1115058000, 1115058060, 1115058120, 1115058180,
> 1115058240, 1115058300, 1115058360, 1115058420, 1115058480, 1115058540,
> 1115058600, 1115058660, 1115058720, 1115058780, 1115058840, 1115058900,
> 1115058960, 1115059020, 1115059080, 1115059140, 1115059200, 1115059260,
> 1115059320, 1115059380, 1115059440, 1115059500, 1115059560, 1115059620,
> 1115059680, 1115059740, 1115059800, 1115059860, 1115059920, 1115059980,
> 1115060040, 1115060100, 1115060160, 1115060220, 1115060280, 1115060340,
> 1115060400, 1115060460, 1115060520, 1115060580, 1115060640, 1115060700,
> 1115060760, 1115060820, 1115060880, 1115060940, 1115061000, 1115061060,
> 1115061120, 1115061180, 1115061240, 1115061300, 1115061360, 1115061420,
> 1115061480, 1115061540, 1115061600, 1115061660, 1115061720, 1115061780,
> 1115061840, 1115061900, 1115061960, 1115062020, 1115062080, 1115062140,
> 1115062200, 1115062260, 1115062320, 1115062380, 1115062440, 1115062500,
> 1115062560, 1115062620, 1115062680, 1115062740, 1115062800, 1115062860,
> 1115062920, 1115062980, 1115063040, 1115063100, 1115063160, 1115063220,
> 1115063280, 1115063340, 1115063400, 1115063460, 1115063520, 1115063580,
> 1115063640, 1115063700, 1115063760, 1115063820, 1115063880, 1115063940,
> 1115064000, 1115064060, 1115064120, 1115064180, 1115064240, 1115064300,
> 1115064360, 1115064420, 1115064480, 1115064540, 1115064600, 1115064660,
> 1115064720, 1115064780, 1115064840, 1115064900, 1115064960, 1115065020,
> 1115065080, 1115065140, 1115065200, 1115065260, 1115065320, 1115065380,
> 1115065440, 1115065500, 1115065560, 1115065620, 1115065680, 1115065740,
> 1115065800, 1115065860, 1115065920, 1115065980, 1115066040, 1115066100,
> 1115066160, 1115066220, 1115066280, 1115066340, 1115066400, 1115066460,
> 1115066520, 1115066580, 1115066640, 1115066700, 1115066760, 1115066820,
> 1115066880, 1115066940, 1115067000, 1115067060, 1115067120, 1115067180,
> 1115067240, 1115067300, 1115067360, 1115067420, 1115067480, 1115067540,
> 1115067600, 1115067660, 1115067720, 1115067780, 1115067840, 1115067900,
> 1115067960, 1115068020, 1115068080, 1115068140, 1115068200, 1115068260,
> 1115068320, 1115068380, 1115068440, 1115068500, 1115068560, 1115068620,
> 1115068680, 1115068740, 1115068800, 1115068860), class = c("POSIXt",
> "POSIXct")), class = "zoo")
>
> I call the ar function with aic=TRUE as below so that it picks the order
> of the ar model based on BIC. Yet it returns with no coefficients as the
> best model.
> I do the same call using many, series in a loop ( besides just the one
> above )  and it returns zero coefficients quite a bit of the time.
>
> 1) can this be possible ? because i don't see how zero coefficients
> could be better than one or some ?
>
> ARest<-ar(testSeries,aic=TRUE,demean=FALSE,order.max=4,method="ols")
>
> 2) also, is there a way to calculate the t-stats for the coefficients
> that come back ?
>
> it would probably be easier for me to just call arima over and over in a
> loop while decreasing the number of lags each time by 1 but
> i am not clever enough in R to do this ? i imagine it requires some form
> of the use of embed but i looked at the code for ar and it
> was pretty beyond me. if anyone happens to have code for doing this type
> of thing, that would be great also. thank you very much.
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From liuwensui at gmail.com  Tue Feb  6 00:42:28 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 5 Feb 2007 18:42:28 -0500
Subject: [R] rpart
In-Reply-To: <6.2.3.4.2.20070205151610.03b34ba0@aiminy.mail.iastate.edu>
References: <6.2.3.4.2.20070205145753.03b28a00@aiminy.mail.iastate.edu>
	<1115a2b00702051311s128eecddm44e708cde19b24fd@mail.gmail.com>
	<6.2.3.4.2.20070205151610.03b34ba0@aiminy.mail.iastate.edu>
Message-ID: <1115a2b00702051542w7be0cb77ya4c9117df5179fde@mail.gmail.com>

man, oh, man
Surely you can use bagging, or probably boosting. But that doesn't
answer your question, does it?
Believe me, even you use bagging, the result will vary, depending on set.seed().

On 2/5/07, Aimin Yan <aiminy at iastate.edu> wrote:
> Yes, I use the same setting, and I calculate MSE and CC as
> prediction accuracy measure.
> Someone told me  I should not trust one tree and should do bagging.
> Is this correct?
> Aimin
>
> At 03:11 PM 2/5/2007, Wensui Liu wrote:
> >are you sure you are using the same setting,  tree size, and so on?
> >
> >On 2/5/07, Aimin Yan <aiminy at iastate.edu> wrote:
> >>Hello,
> >>I have a question for rpart,
> >>I try to use it to do prediction for a continuous variable.
> >>But I get the different prediction accuracy for same training set,
> >>anyone know why?
> >>
> >>Aimin
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >--
> >WenSui Liu
> >A lousy statistician who happens to know a little programming
> >(http://spaces.msn.com/statcompute/blog)
>
>
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From geoffrey.russell at gmail.com  Tue Feb  6 00:53:00 2007
From: geoffrey.russell at gmail.com (Geoff Russell)
Date: Tue, 6 Feb 2007 10:23:00 +1030
Subject: [R] Metapost device driver
Message-ID: <93c3eada0702051553x5375d70br8d9f8ca73199d2c6@mail.gmail.com>

Hi All,

I've started work on a MetaPost device driver (please don't hold your
breath).

I've copied the XFig driver and renamed everything and this works, I can
open the new metapost() and it works exactly like the xfig
driver. Now all I have to do is the actual work!

Just one question. There is a magic number in ExtEntries as follows:

static const R_ExternalMethodDef ExtEntries[] = {
       EXTDEF(PicTeX, 6),
       EXTDEF(PostScript, 16),        EXTDEF(XFig, 11),
       EXTDEF(MetaPost, 12),    /* Is 12 is OK */
       EXTDEF(PDF, 13),

I just picked 12, is this Ok, or does it have some special significance?

Cheers,
Geoff Russell.


From upsattar at yahoo.com  Tue Feb  6 00:54:47 2007
From: upsattar at yahoo.com (Abdus Sattar)
Date: Mon, 5 Feb 2007 15:54:47 -0800 (PST)
Subject: [R] Reshaping wide form to long form of repeated measure data
Message-ID: <666445.96716.qm@web58105.mail.re3.yahoo.com>

Hello R-users:
 
I am new to the R-world. I am trying to reshape the "wide" longitudinal data into the "long" form. I have used the following codes for the attached data file "xyw1.rda":
 
long=reshape(xyw1, idvar="Subject", varying=list(names(xyw1)[9:15]), v.names="Ddimer", timevar="time", direction="long") 
 
I was expecting 10 clusters but when I run the geeglm it is analyzing many more clusters(70) with cluster size =1.  How can I modify this code so that there would be only ten (10) clusters and size of the each clusters would be seven (7)?  Any help would be sincerely appreciated. 
 
Best Regards, 
 
Sattar


 
____________________________________________________________________________________
The fish are biting. 
Get more visitors on your site using Yahoo! Search Marketing.
http://searchmarketing.yahoo.com/arp/sponsoredsearch_v2.php

From dylan.beaudette at gmail.com  Tue Feb  6 02:13:06 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Mon, 5 Feb 2007 17:13:06 -0800
Subject: [R] sample size calculations
Message-ID: <200702051713.06519.dylan.beaudette@gmail.com>

Greetings,

I have experimented with the MBESS and pwr packages for the estimation of 
sample size for a given CV, precision, and confidence interval. 

Thus far I have found the ss.aipe.cv {MBESS} (Sample size planning for the 
coefficient of variation given the goal of Accuracy in Parameter Estimation 
approach to sample size planning.) function to be best suited for my needs.

However, the data from which I am calculating my CV is approximately 
log-normally distributed- and thus has a large CV (1.4). Using this CV, 
precision (20% within the pop mean) and confidence interval (95%) parameters 
I obviously get a suggested sample size that is very large (n = 1182). By 
reducing my precision and confidence interval requirements to something like:
ss.aipe.cv(C.of.V=1.4, width=0.5, conf.level=0.9)
... the function still suggests about 230 samples which is near the upper 
limit of feasibility. 

I would like to deduce an optimal number of samples, however the log-normal 
distribution of this data suggests that the above approach is not well suited 
to this task. 

Are there any better approaches or references which might send me in the right 
direction?

Thanks in advance,


-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341


From rlevy at ucsd.edu  Tue Feb  6 02:23:29 2007
From: rlevy at ucsd.edu (Roger Levy)
Date: Mon, 05 Feb 2007 17:23:29 -0800
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <17861.25637.324856.683429@lapo.berkeley.edu>
References: <45C38E90.9070709@ucsd.edu>	<17860.61176.520222.602660@lapo.berkeley.edu>	<17861.4763.359422.817825@macht.arts.cornell.edu>	<45C52F17.1030601@ucsd.edu>	<17861.13360.697951.302800@macht.arts.cornell.edu>
	<17861.25637.324856.683429@lapo.berkeley.edu>
Message-ID: <45C7D891.9060204@ucsd.edu>

Thank you both, this is terrific!  I understand much better now.

Best

Roger

Jasjeet Singh Sekhon wrote:
> Hi Roger,
> 
> Walter's command is correct.  To match the exact normalization used by
> nnet's multinom(), however, you would need to make the coefficients
> zero for the first class (i.e., y1) and not the last (i.e., y3).
> 
> mr <- multinomRob(list(y2 ~ x1 + x2, y3 ~ x1 + x2, y1~0),data=d,
> print.level=1)
> 
> The results are:
> 
> MNL Estimates:
>                            y1         y2         y3
> NA/(Intercept)/(Intercept)  0 -0.6931462 -0.6931462
> NA/x1/x1                    0  1.3862936  0.6931474
> NA/x2/x2                    0  0.6931474  1.3862936
> 
> Compare to the output from nnet's multinom:
> 
>> summary(m1)
> Call:
> multinom(formula = y ~ x1 + x2, data = d)
> 
> Coefficients:
>   (Intercept)        x1        x2
> b  -0.6931475 1.3862975 0.6931499
> c  -0.6931475 0.6931499 1.3862975
> 
> Also, the MLE MNL objects are in:
> 
> mr$mnl
> 
> To constrain the x1 coeffs to be equal do:
> 
> emr <- multinomRob(list(y2 ~ x1 + x2,y3 ~ x1 + x2, y1~0),data=d, print.level=1,
>                   equality=list(list(y2~x1+0,y3~x1+0)))
> 
> To constrain y2's x1 to be equal to y3's x2:
> emr2 <- multinomRob(list(y2 ~ x1 + x2,y3 ~ x1 + x2, y1~0),data=d, print.level=1,
>                   equality=list(list(y2~x1+0,y3~x2+0)))
> 
> 
> See the multinomRob help file for more details:
> http://sekhon.berkeley.edu/robust/multinomRob.html
> 
> Any BFGS warnings can be ignored because you are not interested in the
> robust estimates (they are comming from LQD estimation and require
> changing 'genoud.parms').
> 
> Cheers,
> Jas.
> 
> =======================================
> Jasjeet S. Sekhon                     
>                                       
> Associate Professor             
> Travers Department of Political Science
> Survey Research Center          
> UC Berkeley                     
> 
> http://sekhon.berkeley.edu/
> V: 510-642-9974  F: 617-507-5524
> =======================================
> 
> Walter Mebane writes:
>  > Roger,
>  > 
>  > summary(multinomRob(list(y1 ~ x1 + x2,y2 ~ x1 + x2, y3 ~ 0),data=d,
>  >   print.level=1))
>  > 
>  > Walter Mebane
>  > 
>  > Roger Levy writes:
>  >  > Many thanks for pointing this out to me!
>  >  > 
>  >  > I'm still a bit confused, however, as to how to use multinomRob.  For 
>  >  > example I tried to translate the following example using nnet:
>  >  > 
>  >  > 
>  >  > x1 <- c(1,1,1,1,0,0,0,0,0,0,0,0)
>  >  > x2 <- c(0,0,0,0,1,1,1,1,0,0,0,0)
>  >  > y <- factor(c("a","b","b","c","a","b","c","c","a","a","b","c"))
>  >  > library(nnet)
>  >  > d <- data.frame(x1,x2,y)
>  >  > summary(multinom(y ~ x1 + x2, data=d))
>  >  > 
>  >  > 
>  >  > into multinomRob as follows:
>  >  > 
>  >  > 
>  >  > x1 <- c(1,1,1,1,0,0,0,0,0,0,0,0)
>  >  > x2 <- c(0,0,0,0,1,1,1,1,0,0,0,0)
>  >  > y <- factor(c("a","b","b","c","a","b","c","c","a","a","b","c"))
>  >  > y1 <- ifelse(y=="a",1, 0)
>  >  > y2 <- ifelse(y=="b", 1, 0)
>  >  > y3 <- ifelse(y=="c", 1, 0)
>  >  > d <- data.frame(x1,x2,y,y1,y2,y3)
>  >  > summary(multinomRob(list(y1 ~ x1 + x2,y2 ~ x1 + x2, y3 ~ x1 + x2),data=d))
>  >  > 
>  >  > but the last command gives me the error message:
>  >  > 
>  >  > 
>  >  > [1] "multinomMLE: Hessian is not positive definite"
>  >  > Error in obsformation %*% opg : non-conformable arguments
>  >  > 
>  >  > 
>  >  > though it's not obvious to me why.  I also tried a couple other variants:
>  >  > 
>  >  > 
>  >  >  > summary(multinomRob(list(y1 ~ 0,y2 ~ x1 + x2,y3 ~ x1 + x2),data=d))
>  >  > Error in multinomT(Yp = Yp, Xarray = X, xvec = xvec, jacstack = 
>  >  > jacstack,  :
>  >  >          (multinomT): invalid specification of Xarray (regressors not 
>  >  > allowed for last category
>  >  >  > summary(multinomRob(list(y1 ~ 0,y2 ~ x1 ,y3 ~ x2),data=d))
>  >  > Error in multinomT(Yp = Yp, Xarray = X, xvec = xvec, jacstack = 
>  >  > jacstack,  :
>  >  >          (multinomT): invalid specification of Xarray (regressors not 
>  >  > allowed for last category
>  >  > 
>  >  > 
>  >  > Any advice would be much appreciated!
>  >  > 
>  >  > 
>  >  > Many thanks,
>  >  > 
>  >  > Roger
>  >  > 
>  >  > Walter Mebane wrote:
>  >  > > By default, with print.level=0 or greater, the multinomRob program
>  >  > > prints the maximum likelihood estimates with conventional standard
>  >  > > errors before going on to compute the robust estimates.
>  >  > > 
>  >  > > Walter Mebane
>  >  > > 
>  >  > > Jasjeet Singh Sekhon writes:
>  >  > >  > 
>  >  > >  > Hi Roger,
>  >  > >  > 
>  >  > >  > Yes, multinomRob can handle equality constraints of this type---see
>  >  > >  > the 'equality' option.  But the function assumes that the outcomes are
>  >  > >  > multinomial counts and it estimates overdispersed multinomial logistic
>  >  > >  > models via MLE, a robust redescending-M estimator, and LQD which is
>  >  > >  > another high breakdown point estimator.  It would be a simple matter
>  >  > >  > to edit the 'multinomMLE' function to work without counts and to do
>  >  > >  > straight MNL instead, but right now it estimates an overdispersed MNL
>  >  > >  > model.
>  >  > >  > 
>  >  > >  > Cheers,
>  >  > >  > Jas.
>  >  > >  > 
>  >  > >  > =======================================
>  >  > >  > Jasjeet S. Sekhon                     
>  >  > >  >                                       
>  >  > >  > Associate Professor             
>  >  > >  > Travers Department of Political Science
>  >  > >  > Survey Research Center          
>  >  > >  > UC Berkeley                     
>  >  > >  > 
>  >  > >  > http://sekhon.berkeley.edu/
>  >  > >  > V: 510-642-9974  F: 617-507-5524
>  >  > >  > =======================================
>  >  > >  > 
>  >  > >  > 
>  >  > >  > 
>  >  > >  > Roger Levy writes:
>  >  > >  >  > I'm interested in doing multinomial logistic regression with equality 
>  >  > >  >  > constraints on some of the parameter values.  For example, with 
>  >  > >  >  > categorical outcomes Y_1 (baseline), Y_2, and Y_3, and covariates X_1 
>  >  > >  >  > and X_2, I might want to impose the equality constraint that
>  >  > >  >  > 
>  >  > >  >  >    \beta_{2,1} = \beta_{3,2}
>  >  > >  >  > 
>  >  > >  >  > that is, that the effect of X_1 on the logit of Y_2 is the same as the 
>  >  > >  >  > effect of X_2 on the logit of Y_3.
>  >  > >  >  > 
>  >  > >  >  > Is there an existing facility or package in R for doing this?  Would 
>  >  > >  >  > multinomRob fit the bill?
>  >  > >  >  > 
>  >  > >  >  > Many thanks,
>  >  > >  >  > 
>  >  > >  >  > Roger
>  >  > >  >  > 
>  >  > >  >  > 
>  >  > >  >  > -- 
>  >  > >  >  > 
>  >  > >  >  > Roger Levy                      Email: rlevy at ucsd.edu
>  >  > >  >  > Assistant Professor             Phone: 858-534-7219
>  >  > >  >  > Department of Linguistics       Fax:   858-534-4789
>  >  > >  >  > UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy
>  >  > >  >  > 
>  >  > >  >  > 
>  >  > > 
>  > 
>  > -- 
>  > * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - *
>  > Walter R. Mebane, Jr.                        email:  wrm1 at cornell.edu
>  > Professor                             office voice:  607/255-3868    
>  > Department of Government                      cell:  607/592-0546
>  > Cornell University                             fax:  607/255-4530    
>  > 217 White Hall              WWW:  http://macht.arts.cornell.edu/wrm1/
>  > Ithaca, NY 14853-7901
>  > * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - * - *
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 

Roger Levy                      Email: rlevy at ucsd.edu
Assistant Professor             Phone: 858-534-7219
Department of Linguistics       Fax:   858-534-4789
UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy


From xmeng at capitalbio.com  Tue Feb  6 05:02:27 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Tue, 06 Feb 2007 12:02:27 +0800
Subject: [R] sort dataframe
Message-ID: <370734547.07402@capitalbio.com>

Hello sir:
How can I sort a dataframe by sorting one of its column?

e.g.

dataframe:

id  x   y
a  0.1  3
b  0.5  1
c  0.2  9
d  0    5

I want the dataframe sorted according to y accending,the result is:

id  x    y
b   0.5  1
a   0.1  3
d   0    5
c   0.2  9



Thanks a lot!

My best


From wangtong at usc.edu  Tue Feb  6 05:24:08 2007
From: wangtong at usc.edu (Tong Wang)
Date: Mon, 05 Feb 2007 20:24:08 -0800
Subject: [R] Question on computing mv-norm density
Message-ID: <f684b45ad281.45c79268@usc.edu>

Hi, 
    I am trying to figure out how to evaluate the density function of multivariate normal efficiently for a large data set, the 
data set should look like this (take d=2 for example): 
          
           data                                        mean                                      sigma

       2.131, 3.000                       1.000,1.000                                     1   0
                                                                                                                0   1

       1.231,5.141                        2.000,2.000                                      .5  -.1
            .............                                 .............                                          -.1 .4
                                                                                                                 .........

that is , I need   SUM_i (log(dmvnorm(d[i], mu[i], sigma[i])).     tried to rewrite the dmvnorm function from mvtnorm 
package, but could not find a satisfactory method to avoid using loops.  ( using those "apply" functions seem to be slower
than loops) .  May I get some suggestion on this ? Thanks a lot in advance. 

best


From uniace at mac.com  Tue Feb  6 08:33:54 2007
From: uniace at mac.com (Jason R. Finley)
Date: Tue, 6 Feb 2007 01:33:54 -0600
Subject: [R] ANOVA Table for Full Linear Model?
Message-ID: <6ADFCA9A-EEE3-4243-9879-C91EB082FFC9@mac.com>

Hello,
I have spent a good deal of time searching for an answer to this but  
have come up empty-handed; I apologize if I missed something that is  
common knowledge.

I am trying to figure out how to get an ANOVA table that shows the  
sum of squares. degrees of freedom, etc, for the full model versus  
the error (aka residuals).

Here is an example of the kind of table I'd like to get:

Analysis of Variance
Source       DF          SS          MS         F        P
Regression    1      8654.7      8654.7    102.35    0.000
Error        75      6342.1        84.6
Total        76     14996.8

This kind of table is prevalent throughout my statistics textbook,  
and can apparently be easily obtained in other statistical software  
tools.  I'm not saying this as a gripe, but just as evidence that I'm  
not trying to do something obviously bizarre.


Here is an example of the only kind of ANOVA table for a single  
linear model that I've been able to get using R:

 > regression9 <- lm(y ~ x1 + x2 + x3, data=data9)
 > anova.lm(regression9)
Analysis of Variance Table

Response: y
           Df Sum Sq Mean Sq F value    Pr(>F)
x1         1 8275.4  8275.4 81.8026 2.059e-11 ***
x2         1  480.9   480.9  4.7539   0.03489 *
x3         1  364.2   364.2  3.5997   0.06468 .
Residuals 42 4248.8   101.2
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


Is there a way to get an ANOVA table with the full linear regression  
model considered as a whole rather than broken down into each  
additional predictor variable?  In other words, is there a way to get  
the former kind of table?

Again, apologies if I'm missing something basic.
thanks very much,
~jason


PS - I am on Mac OSX 10.4.8 using R 2.4.1 GUI 1.18 (4038)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jason R. Finley
Graduate Student, Department of Psychology
Cognitive Division
University of Illinois, Urbana-Champaign

uniace at mac.com
jrfinley at uiuc.edu
http://www.jasonfinley.com/
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From john.gavin at ubs.com  Tue Feb  6 09:04:09 2007
From: john.gavin at ubs.com (john.gavin at ubs.com)
Date: Tue, 6 Feb 2007 08:04:09 -0000
Subject: [R] Rconsole - setting the size and location of Windows help
	files (Rgui)
In-Reply-To: <3948d9e50702052237l2dbdd5e3ub870d95e4063cc13@mail.gmail.com>
References: <182544D7A3144B42994EEA5662C54E010506FFF5@NLDNC105PEX1.ubsw.net>
	<3948d9e50702052237l2dbdd5e3ub870d95e4063cc13@mail.gmail.com>
Message-ID: <182544D7A3144B42994EEA5662C54E0105100AE3@NLDNC105PEX1.ubsw.net>

Hi,

> # Dimensions(in characters) of the internal pager.
> pgrows = 48
> pgcolumns = 128

Thanks, that worked.

Regards,

John. 

> -----Original Message-----
> From: talepanda [mailto:talepanda at gmail.com] 
> Sent: 06 February 2007 06:37
> To: Gavin, John
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Rconsole - setting the size and location of 
> Windows help files (Rgui)
> 
> For size, maybe:
> 
> # Dimensions(in characters) of the internal pager.
> #pgrows = 25
> #pgcolumns = 80
> pgrows = 48
> pgcolumns = 128
> 
> in Rconsole, but location cannot be handled.
> 
> On 2/6/07, john.gavin at ubs.com <john.gavin at ubs.com> wrote:
> > Hi,
> >
> > Using the Rconsole file I can specify the size and location 
> of the Rgui
> > windows on NT.
> > e.g.
> > # Dimensions (in characters) of the console.
> > rows = 51
> > columns = 100
> >
> > How can I specify the size of the help windows that popups
> > when I ask for help? e.g. '?help'
> > I would like the popup window to have say rows = 51 and 
> columns = 100,
> > just like the main window but a different location on the screen.
> >
> > In my .Rprofile file I have set 'options(winhelp=FALSE)'.
> >
> > platform       i386-pc-mingw32
> > arch           i386
> > os             mingw32
> > system         i386, mingw32
> > status
> > major          2
> > minor          4.1
> > year           2006
> > month          12
> > day            18
> > svn rev        40228
> > language       R
> > version.string R version 2.4.1 (2006-12-18)
> >
> > Regards,
> >
> > John.
> >
> > John Gavin <john.gavin at ubs.com>,
> > Commodities, FIRC,
> > UBS Investment Bank, 2nd floor,
> > 100 Liverpool St., London EC2M 2RH, UK.
> > Phone +44 (0) 207 567 4289
This communication is issued by UBS AG and/or affiliates to\...{{dropped}}


From p.dalgaard at biostat.ku.dk  Tue Feb  6 09:11:42 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 06 Feb 2007 09:11:42 +0100
Subject: [R] ANOVA Table for Full Linear Model?
In-Reply-To: <6ADFCA9A-EEE3-4243-9879-C91EB082FFC9@mac.com>
References: <6ADFCA9A-EEE3-4243-9879-C91EB082FFC9@mac.com>
Message-ID: <45C8383E.1080606@biostat.ku.dk>

Jason R. Finley wrote:
> Hello,
> I have spent a good deal of time searching for an answer to this but  
> have come up empty-handed; I apologize if I missed something that is  
> common knowledge.
>
> I am trying to figure out how to get an ANOVA table that shows the  
> sum of squares. degrees of freedom, etc, for the full model versus  
> the error (aka residuals).
>
> Here is an example of the kind of table I'd like to get:
>
> Analysis of Variance
> Source       DF          SS          MS         F        P
> Regression    1      8654.7      8654.7    102.35    0.000
> Error        75      6342.1        84.6
> Total        76     14996.8
>
> This kind of table is prevalent throughout my statistics textbook,  
> and can apparently be easily obtained in other statistical software  
> tools.  I'm not saying this as a gripe, but just as evidence that I'm  
> not trying to do something obviously bizarre.
>
>
> Here is an example of the only kind of ANOVA table for a single  
> linear model that I've been able to get using R:
>
>  > regression9 <- lm(y ~ x1 + x2 + x3, data=data9)
>  > anova.lm(regression9)
> Analysis of Variance Table
>
> Response: y
>            Df Sum Sq Mean Sq F value    Pr(>F)
> x1         1 8275.4  8275.4 81.8026 2.059e-11 ***
> x2         1  480.9   480.9  4.7539   0.03489 *
> x3         1  364.2   364.2  3.5997   0.06468 .
> Residuals 42 4248.8   101.2
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>
> Is there a way to get an ANOVA table with the full linear regression  
> model considered as a whole rather than broken down into each  
> additional predictor variable?  In other words, is there a way to get  
> the former kind of table?
>   
The logical thing to do would be (please avoid calling methods like 
anova.lm directly)

# following example(anova.lm):

> anova(update(fit,~1),fit)

Analysis of Variance Table

Model 1: sr ~ 1

Model 2: sr ~ pop15 + pop75 + dpi + ddpi

  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    

1     49 983.63                                  

2     45 650.71  4    332.92 5.7557 0.0007904 ***


If you really want the column of MS, you have a little extra work to do. 
Notice also that the F test is part of the standard summary(fit)


> Again, apologies if I'm missing something basic.
> thanks very much,
> ~jason
>
>
> PS - I am on Mac OSX 10.4.8 using R 2.4.1 GUI 1.18 (4038)
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Jason R. Finley
> Graduate Student, Department of Psychology
> Cognitive Division
> University of Illinois, Urbana-Champaign
>
> uniace at mac.com
> jrfinley at uiuc.edu
> http://www.jasonfinley.com/
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Tue Feb  6 09:35:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Feb 2007 08:35:44 +0000 (GMT)
Subject: [R] Metapost device driver
In-Reply-To: <93c3eada0702051553x5375d70br8d9f8ca73199d2c6@mail.gmail.com>
References: <93c3eada0702051553x5375d70br8d9f8ca73199d2c6@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702060834150.19359@gannet.stats.ox.ac.uk>

Please discuss this on R-devel: it is way off-topic for R-help.
'11' is the number of arguments, it is not 'magic'.

On Tue, 6 Feb 2007, Geoff Russell wrote:

> Hi All,
>
> I've started work on a MetaPost device driver (please don't hold your
> breath).
>
> I've copied the XFig driver and renamed everything and this works, I can
> open the new metapost() and it works exactly like the xfig
> driver. Now all I have to do is the actual work!
>
> Just one question. There is a magic number in ExtEntries as follows:
>
> static const R_ExternalMethodDef ExtEntries[] = {
>       EXTDEF(PicTeX, 6),
>       EXTDEF(PostScript, 16),        EXTDEF(XFig, 11),
>       EXTDEF(MetaPost, 12),    /* Is 12 is OK */
>       EXTDEF(PDF, 13),
>
> I just picked 12, is this Ok, or does it have some special significance?
>
> Cheers,
> Geoff Russell.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Feb  6 09:39:34 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Feb 2007 08:39:34 +0000 (GMT)
Subject: [R] Rconsole - setting the size and location of Windows help
 files (Rgui)
In-Reply-To: <3948d9e50702052237l2dbdd5e3ub870d95e4063cc13@mail.gmail.com>
References: <182544D7A3144B42994EEA5662C54E010506FFF5@NLDNC105PEX1.ubsw.net>
	<3948d9e50702052237l2dbdd5e3ub870d95e4063cc13@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702060837300.19359@gannet.stats.ox.ac.uk>

On Tue, 6 Feb 2007, talepanda wrote:

> For size, maybe:
>
> # Dimensions(in characters) of the internal pager.
> #pgrows = 25
> #pgcolumns = 80
> pgrows = 48
> pgcolumns = 128
>
> in Rconsole, but location cannot be handled.

For the very good reason that you can have multiple pagers and I at least 
do not want them exactly on top of one another.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Feb  6 09:55:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Feb 2007 08:55:24 +0000 (GMT)
Subject: [R] ar function in stats
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A83440135DB9E@NYWEXMB23.msad.ms.com>
References: <D3AEEDA31E57474B840BEBC25A8A83440135DB9E@NYWEXMB23.msad.ms.com>
Message-ID: <Pine.LNX.4.64.0702060840220.19359@gannet.stats.ox.ac.uk>

On Mon, 5 Feb 2007, Leeds, Mark (IED) wrote:

> I had a couple of questions about the ar function that i was hoping
> someone could answer.
>
[...]

> I call the ar function with aic=TRUE as below so that it picks the order
> of the ar model based on BIC.

It actually does as it says it does (BIC != AIC).

> Yet it returns with no coefficients as the best model.

You mean order 0?

> I do the same call using many, series in a loop ( besides just the one
> above )  and it returns zero coefficients quite a bit of the time.
>
> 1) can this be possible ? because i don't see how zero coefficients
> could be better than one or some ?

Then you don't understand AIC (or BIC), so please study the concepts.
It is not only possible, it happens (including for you).

> 2) also, is there a way to calculate the t-stats for the coefficients
> that come back ?

What are the 't-stats'?  The estimated standard errors are not calculated 
in ar(), if you mean (estimated coefficient)/(estimated se).  There is of 
course *a* way to calculate them!  From the Value section on the help 
page:

asy.var.coef: (univariate case, 'order > 0'.) The asymptotic-theory
           variance matrix of the coefficient estimates.

The question is, what could you validly do with the 't-stats'?

> it would probably be easier for me to just call arima over and over in a
> loop while decreasing the number of lags each time by 1 but
> i am not clever enough in R to do this ? i imagine it requires some form
> of the use of embed but i looked at the code for ar and it
> was pretty beyond me. if anyone happens to have code for doing this type
> of thing, that would be great also. thank you very much.

Hmm, R itself does have examples.  See e.g. ?WWWusage.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From alistair.campbell at jcu.edu.au  Tue Feb  6 12:07:06 2007
From: alistair.campbell at jcu.edu.au (Alistair Campbell)
Date: Tue,  6 Feb 2007 21:07:06 +1000 (EST)
Subject: [R] How-To construct a cov list to use a covariance matrix in
 factanal?
Message-ID: <20070206210706.BHR67222@mirapoint-ms1.jcu.edu.au>

Hi,

I have a set of covariance matrices but not the original data. I want to carry out some exploratory factor analysis. So, I am trying to construct a covariance matrix list as the input for factanal. I can construct a list which includes the cov, the centers, and the n.obs. But it doesn't work. I get an error that says "Error in sqrt(diag(cv)) : Non-numeric argument to mathematical function". So, obviously I am doing something wrong.

Two questions occur. Can someone either tell me how to construct a proper covmat list object or point me to a description of how to do it? The other question is whether it is possible to simply use the covariance matrix as the argument for covmat in factanal? The description implies that it is but I really have no idea of how to do this. I have tried simply making covmat the covariance matrix but it doesn't wotk. I just get the message "'covmat' is not a valid covariance list"

Anyway, thanks for any thoughts you might have on this.

Alistair Campbell
-- 
Dr Alistair Campbell, PhD
Senior Lecturer in Clinical Psychology
School of Psychology
James Cook University
Townsville Queensland Australia

Ph: +61 7 47816879


From jzhang04 at 163.com  Tue Feb  6 13:36:19 2007
From: jzhang04 at 163.com (Jian Zhang)
Date: Tue, 6 Feb 2007 20:36:19 +0800
Subject: [R] How to do "moran's I test"?
Message-ID: <45C8764D.027B73.20649@m5-83.163.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070206/0b37a3c0/attachment.pl 

From p.nimda at gmail.com  Tue Feb  6 13:39:49 2007
From: p.nimda at gmail.com (Peter Nimda)
Date: Tue, 6 Feb 2007 13:39:49 +0100
Subject: [R] any implementations for adaptive modeling of time series?
In-Reply-To: <55dcc5de0702010535x78c9fd0dv781a4e2c016f7cb@mail.gmail.com>
References: <c5d31fc20701300509l777edf2dp5a8e50cf6bd8c6e0@mail.gmail.com>
	<55dcc5de0702010535x78c9fd0dv781a4e2c016f7cb@mail.gmail.com>
Message-ID: <c5d31fc20702060439s6ce3541dvc64c6e740c782d7b@mail.gmail.com>

Hi Ansel,

thank you for the response

> generally speaking, wavelets are known to be good at
> extracting signal from noisy data and are adaptive but
> I am not familiar with any R implementation of wavelets.

I used wavelets before.

1. wavelets is just a particular case of orthogonal function
systems. playing with different orthogonal function
systems one can find some "high effective" basis.
However this approach is hardly something what one
can adapt on fly while processing the real data.

2. wavelets are not stochastic and adapting a noise
model as well as random changes in trend is also
quite essential in my case.


> A simple way of looking at changes would be to use CUSUM (strucchange
> package).
> I hope this helps.
> Ansel.

thank you Ansel, i will look at this package right away.

kind regards
--
P.



> On 1/30/07, Peter Nimda <p.nimda at gmail.com> wrote:
> >
> > Hallo,
> >
> > my noisy time series represent a fading signal comprising of long
> > enough parts with a simple trend inside of each such a part.
> > Transition from one part into another is always a non-smooth
> > and very sharp/acute. In other words I have a piecewise
> > polynomial noisy curve asymptotically converging to the
> > biased constant, points between pieces are non-differentiable.
> >
> > I am looking for implementations of models adequate for such a
> > data. Are there any possibilities to adapt the ARIMA or
> > MCMC?
> >
> > Many thanks in advance for any help/URLs
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>


From xrysoflis at gmail.com  Tue Feb  6 14:02:56 2007
From: xrysoflis at gmail.com (Maria Vatapitakapha)
Date: Tue, 6 Feb 2007 13:02:56 +0000
Subject: [R] R equivalent to Matlab spline(x,y,xx)
Message-ID: <bd3d127c0702060502y6cd3165do59e156e1b34f3485@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070206/f0716e68/attachment.pl 

From jrkrideau at yahoo.ca  Tue Feb  6 14:11:45 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 6 Feb 2007 08:11:45 -0500 (EST)
Subject: [R] LyX 1.4.3-5 problem generating PDF documents
In-Reply-To: <45C762C9.2070906@gmx.net>
Message-ID: <413575.28199.qm@web32809.mail.mud.yahoo.com>


--- Stefan Grosse <singularitaet at gmx.net> wrote:

> You are posting in the wrong mailing list, mail your
> question to the lyx
> users list: http://www.lyx.org/internet/mailing.php

OOPs  I wondered why I wasn't getting a reply.  I use
the same account for both lists.  My appologies to the
list

> 
> John Kane wrote:
> > I just changed machines and, at first, could not
> get
> > LyX 1.4.3-4 to even load.  I tried 1.5 which
> loaded
> > nicely but would not give me PDF output.  Removed
> 1.5
> > tried using 1.4.3-5.exe. Result : On loading LyX :
> 
> >   lyx: Disabling socket
> >   cont class std::basic_string<char struct std:
> > char::char_traits ...
> >
> > Removed and installed from
> LyXWin143Complete-2-9.exe. 
> > Loads okay but no PDF generation. My PDF reader,
> Foxit
> > Reader loads but nothing is produced.  
> >
> > I have managed to produce a postscript doc and
> convert
> > it using Ghostscript. 
> >
> > Could this just be a Foxit problem? As soon as I
> > figure out how to reset the Adobe to default pdf
> > reader I'll try it out.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> >   
> 
>


From f.harrell at vanderbilt.edu  Tue Feb  6 14:37:05 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 06 Feb 2007 07:37:05 -0600
Subject: [R] ANOVA Table for Full Linear Model?
In-Reply-To: <45C8383E.1080606@biostat.ku.dk>
References: <6ADFCA9A-EEE3-4243-9879-C91EB082FFC9@mac.com>
	<45C8383E.1080606@biostat.ku.dk>
Message-ID: <45C88481.9000901@vanderbilt.edu>

Peter Dalgaard wrote:
> Jason R. Finley wrote:
>> Hello,
>> I have spent a good deal of time searching for an answer to this but  
>> have come up empty-handed; I apologize if I missed something that is  
>> common knowledge.
>>
>> I am trying to figure out how to get an ANOVA table that shows the  
>> sum of squares. degrees of freedom, etc, for the full model versus  
>> the error (aka residuals).
>>
>> Here is an example of the kind of table I'd like to get:
>>
>> Analysis of Variance
>> Source       DF          SS          MS         F        P
>> Regression    1      8654.7      8654.7    102.35    0.000
>> Error        75      6342.1        84.6
>> Total        76     14996.8
>>
>> This kind of table is prevalent throughout my statistics textbook,  
>> and can apparently be easily obtained in other statistical software  
>> tools.  I'm not saying this as a gripe, but just as evidence that I'm  
>> not trying to do something obviously bizarre.
>>
>>
>> Here is an example of the only kind of ANOVA table for a single  
>> linear model that I've been able to get using R:
>>
>>  > regression9 <- lm(y ~ x1 + x2 + x3, data=data9)
>>  > anova.lm(regression9)
>> Analysis of Variance Table
>>
>> Response: y
>>            Df Sum Sq Mean Sq F value    Pr(>F)
>> x1         1 8275.4  8275.4 81.8026 2.059e-11 ***
>> x2         1  480.9   480.9  4.7539   0.03489 *
>> x3         1  364.2   364.2  3.5997   0.06468 .
>> Residuals 42 4248.8   101.2
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>>
>> Is there a way to get an ANOVA table with the full linear regression  
>> model considered as a whole rather than broken down into each  
>> additional predictor variable?  In other words, is there a way to get  
>> the former kind of table?
>>   
> The logical thing to do would be (please avoid calling methods like 
> anova.lm directly)
> 
> # following example(anova.lm):
> 
>> anova(update(fit,~1),fit)
> 
> Analysis of Variance Table
> 
> Model 1: sr ~ 1
> 
> Model 2: sr ~ pop15 + pop75 + dpi + ddpi
> 
>   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
> 
> 1     49 983.63                                  
> 
> 2     45 650.71  4    332.92 5.7557 0.0007904 ***
> 
> 
> If you really want the column of MS, you have a little extra work to do. 
> Notice also that the F test is part of the standard summary(fit)
> 
> 
>> Again, apologies if I'm missing something basic.
>> thanks very much,
>> ~jason

You can also do

library(Design)   # which also requires the Hmisc package
f <- ols( . . .)
anova(f)          # also includes many interesting composite hypothesis 
tests  involving linearity, pooling main effects with interactions, etc.

Frank Harrell
>>
>>
>> PS - I am on Mac OSX 10.4.8 using R 2.4.1 GUI 1.18 (4038)
>>
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> Jason R. Finley
>> Graduate Student, Department of Psychology
>> Cognitive Division
>> University of Illinois, Urbana-Champaign
>>
>> uniace at mac.com
>> jrfinley at uiuc.edu
>> http://www.jasonfinley.com/
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From lyndsey.stanfill at fcesc.org  Tue Feb  6 14:48:45 2007
From: lyndsey.stanfill at fcesc.org (Lyndsey Stanfill)
Date: Tue, 06 Feb 2007 08:48:45 -0500
Subject: [R] How to do "moran's I test"?
In-Reply-To: <45C8764D.027B73.20649@m5-83.163.com>
Message-ID: <C1EDF16D.2B8%lyndsey.stanfill@fcesc.org>

First, I use lattice data rather than point pattern data, so this may not be
applicable. 
The following is an example of the Moran's I test using the "south" dataset,
analyzing the variable HR60 for dependence.
I  use GeoDa to create my spatial weight (https://www.geoda.uiuc.edu/), and
I call the weight "south.gal"
In R, load package:spdep
Load and attach data:
> south <- read.table("south.txt", header=TRUE)
> attach(south)
Create my .nb weight:
> south.gal.nb <- ("south.gal")
Read the .gal file into a neighbors object:
> south.gal.nb <- read.gal ("south.GAL", override.id=TRUE)
Create my weight?s matrix:
> south.listw <- nb2listw(south.gal.nb, style="W")
Name my variable as a spatially named vector:
> HR60 <- spNamedVec("HR60", south)
Perform test:
> str(moran(HR60, south.listw, length(south.gal.nb), Szero(south.listw)))
List of 2
 $ I: num 0.216
 $ K: num 31
For a monte carlo simulation of Moran's I (with randomization):
> nsim<-999
> set.seed(1234)
> simHR60<-moran.mc(HR60, listw=south.listw, nsim=nsim)
> simHR60
 
     Monte-Carlo simulation of Moran's I
 
data:  HR60 
weights: south.listw
number of simulations + 1: 1000
 
statistic = 0.2159, observed rank = 1000, p-value = 0.001
alternative hypothesis: greater

I hope this helps!
Lyndsey Stanfill
Research Design and Data Specialist
Central Ohio Regional School Improvement Team (CORSIT)
lyndsey.stanfill at fcesc.org
http://www.cositpd.org/index.html

On 2/6/07 7:36 AM, "Jian Zhang" <jzhang04 at 163.com> wrote:

> I want to do "moran's I test" in R language. I try to use "gearymoran" in
> Package "ade4","moran" in Package "spdep", and Moran.I in Package "ape". But
> I do not know how to do it because data format is different.
> 
> My data:
> 
>     x        y       dbh
> 
>   111.03    10.7       7
> 
>   118.11    0.28     1.2
> 
>   165.36    0.36     8.4
> 
>    282.9     0.3     7.5
> 
>   303.29   13.32    12.2
> 
>   319.28    3.88     6.2
> 
>      447       9       6
> 
>    445.5    18.3    13.8
> 
>    445.5    12.1     7.1
> 
>   467.64     1.2     4.7
> 
>    485.4    14.1     4.4
> 
>     2.98   23.95    11.7
> 
>       15   35.78    23.5
> 
>   130.21    23.6    14.1
> 
>    213.5   23.22    21.5
> 
>   233.57   28.76    35.4
> 
>    482.3    20.5       6
> 
>    69.73   45.21     7.5
> 
>     69.8   50.49    10.2
> 
>    76.65    45.5    21.5
> 
>  
> 
> "x","y" are spatial coordinate;
> 
> "dbh" is variable
> 
> I want to test the spatial autocorrelation of "dbh" in different distances
> by "x" and "y".
> 
>  
> 
> Thanks,
> 
>                                 Jian Zhang
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From trevor.graham at ucl.ac.uk  Tue Feb  6 15:38:44 2007
From: trevor.graham at ucl.ac.uk (Trevor Graham)
Date: Tue, 6 Feb 2007 14:38:44 +0000
Subject: [R] heatmap from xyz data
Message-ID: <939F40FC-743C-4F61-BEF0-31457DEDC03A@ucl.ac.uk>

Hi,

I've got some data in a data frame arranged like this:
x	y	z	othervariables	....
0.1	0.2	1.7	0.01	....
0.2	0.2	1.3	0.23	....
0.2	0.3	1.1	0.43	....
etc

I'd like to plot a heatmap of this data, with x and y as the (x,y) co- 
ords and z defining the colour at each point.

I'm having difficulty finding the correct functions to use.  Can  
anyone make any recommendations?

It seems straightforward to make the plot using the image function if  
the data is in matrix form, but I'm not sure how to transform my xyz  
data into matrix form.

Many thanks in advance for your help.

Best wishes,
Trevor


-------------------------------------------------------------
Trevor Graham
Centre for Mathematics and Physics in the Life Sciences and  
Experimental Biology (CoMPLEX)
Cancer Research UK

CoMPLEX, Wolfson House
University College London, Gower Street
London WC1E 6BT, UK
Tel: +44 (0)20 7679 5088
Internal: 25088
http://trevor.complex.ucl.ac.uk


From Serguei.Kaniovski at wifo.ac.at  Tue Feb  6 15:54:49 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Tue, 6 Feb 2007 15:54:49 +0100
Subject: [R] Questions on counts by case
Message-ID: <OFE18B467D.8FF269E3-ONC125727A.0051EC30-C125727A.0051EC48@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070206/052ddfda/attachment.pl 

From dieter.menne at menne-biomed.de  Tue Feb  6 16:03:12 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 6 Feb 2007 15:03:12 +0000 (UTC)
Subject: [R] "lme" in R and Splus-7
References: <895233.54498.qm@web35814.mail.mud.yahoo.com>
Message-ID: <loom.20070206T160245-290@post.gmane.org>

yyan liu <zhliur <at> yahoo.com> writes:

>   I used the function "lme" in R and Splus-7. With the same dataset and same
argument for the function, I got
> quite different estimation results from these two software. Anyone has this
experience before?

>From the FAQ:

R by default uses treatment contrasts in the unordered case, whereas S uses the
Helmert ones. This is a deliberate difference reflecting the opinion that
treatment contrasts are more natural.

Dietr


From ripley at stats.ox.ac.uk  Tue Feb  6 16:04:48 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Feb 2007 15:04:48 +0000 (GMT)
Subject: [R] How-To construct a cov list to use a covariance matrix in
 factanal?
In-Reply-To: <20070206210706.BHR67222@mirapoint-ms1.jcu.edu.au>
References: <20070206210706.BHR67222@mirapoint-ms1.jcu.edu.au>
Message-ID: <Pine.LNX.4.64.0702061459420.15524@auk.stats>

The help page says

   covmat: A covariance matrix, or a covariance list as returned by
           'cov.wt'.  Of course, correlation matrices are covariance
           matrices.

and there is an example of a covariance list (ability.cov).

> factanal(factors = 2, covmat = ability.cov)
> factanal(factors = 2, covmat = ability.cov$cov)

both work.  See

> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

as we have no idea what you tried.

On Tue, 6 Feb 2007, Alistair Campbell wrote:

> Hi,
>
> I have a set of covariance matrices but not the original data. I want to 
> carry out some exploratory factor analysis. So, I am trying to construct 
> a covariance matrix list as the input for factanal. I can construct a 
> list which includes the cov, the centers, and the n.obs. But it doesn't 
> work. I get an error that says "Error in sqrt(diag(cv)) : Non-numeric 
> argument to mathematical function". So, obviously I am doing something 
> wrong.
>
> Two questions occur. Can someone either tell me how to construct a 
> proper covmat list object or point me to a description of how to do it? 
> The other question is whether it is possible to simply use the 
> covariance matrix as the argument for covmat in factanal? The 
> description implies that it is but I really have no idea of how to do 
> this. I have tried simply making covmat the covariance matrix but it 
> doesn't wotk. I just get the message "'covmat' is not a valid covariance 
> list"
>
> Anyway, thanks for any thoughts you might have on this.
>
> Alistair Campbell
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wl2776 at gmail.com  Tue Feb  6 16:12:19 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 6 Feb 2007 07:12:19 -0800 (PST)
Subject: [R] heatmap from xyz data
In-Reply-To: <939F40FC-743C-4F61-BEF0-31457DEDC03A@ucl.ac.uk>
References: <939F40FC-743C-4F61-BEF0-31457DEDC03A@ucl.ac.uk>
Message-ID: <8827597.post@talk.nabble.com>


You can use 

plot(y~x,col=color.index.in.palette.defined.from(z),pch=20,type="p")

where 

color.index.in.palette.defined.from(z)

is a function or an expression, returning either a color index in a
predefined palette or any other color representation, suitable for R. This
is described in ?par. 
I have successfuly used col=z*10 with your sample data.

type="p" specifies, that points will be plotted.
pch=20 says to plot them as small filled circles, sometimes I needed smaller
circles, and pch=183 worked for me.

The package fields and several other packages related to the spatial
statistics are able to plot irregularly distributed point data and
interpolate them to a regular grid (produce a 'contiguous' map).

Here are some links:

http://sal.uiuc.edu/csiss/Rgeo/
http://r-spatial.sourceforge.net/

You can also try
RSiteSearch("spatial")
or
help.search("spatial")
from the R console


Trevor Graham wrote:
> 
> I've got some data in a data frame arranged like this:
> x	y	z	othervariables	....
> 0.1	0.2	1.7	0.01	....
> 0.2	0.2	1.3	0.23	....
> 0.2	0.3	1.1	0.43	....
> etc
> 
> I'd like to plot a heatmap of this data, with x and y as the (x,y) coords
> and z defining the colour at each point.
> 
> I'm having difficulty finding the correct functions to use.  Can anyone
> make any recommendations?
> 
> It seems straightforward to make the plot using the image function if  
> the data is in matrix form, but I'm not sure how to transform my xyz  
> data into matrix form.
> 
> 

-- 
View this message in context: http://www.nabble.com/-R--heatmap-from-xyz-data-tf3180966.html#a8827597
Sent from the R help mailing list archive at Nabble.com.


From lalithaviswanath at yahoo.com  Tue Feb  6 16:15:08 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Tue, 6 Feb 2007 07:15:08 -0800 (PST)
Subject: [R] Query about merging two tables
Message-ID: <287125.38310.qm@web43127.mail.sp1.yahoo.com>

Hi
I have table1 which has the foll. columns
id age rate

and table2 which has the foll. columns
id count

I wish to get data from table1 for all the ids which
are persent in table2 and where the rate is not equal
to 999.
The ids in table2 are a subset of those in table1 and
every id in table2 has an entry in table1.

I would appreciate your input regarding the above.

Thanks in advance
Lalitha


From dan.oshea at dnr.state.mn.us  Tue Feb  6 16:33:48 2007
From: dan.oshea at dnr.state.mn.us (Daniel O'Shea)
Date: Tue, 06 Feb 2007 09:33:48 -0600
Subject: [R] installing packages and windows vista
Message-ID: <45C84B7C0200005A0000374F@co5.dnr.state.mn.us>

I installed  R  (R-2.4.1-win32.exe) on a new computer with Windows Vista
and a 64 bit operating system (hp dv9000 with intel core t7200).  The
base R runs fine, but I can not get any of the packages to load.  From
within R I choose install packages choose a site then a package.  I
tried installing 2 packages and get similar errors (see below), I just
copied and pasted lines from R.

Can anyone offer any suggestions?  Thank you.

Dan O'Shea

> utils:::menuInstallPkgs()
--- Please select a CRAN mirror for use in this session ---
also installing the dependencies 'scatterplot3d', 'rgl', 'ellipse'
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/scatterplot3d_0.3-24.zip'
Content type 'application/zip' length 540328 bytes
opened URL
downloaded 527Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
Content type 'application/zip' length 838137 bytes
opened URL
downloaded 818Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/ellipse_0.3-4.zip'
Content type 'application/zip' length 91877 bytes
opened URL
downloaded 89Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/vegan_1.8-5.zip'
Content type 'application/zip' length 1176434 bytes
opened URL
downloaded 1148Kb
Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
(x86)/R/R-2.4.1/library/file60bf5753/scatterplot3d/chtml/scatterplot3d.chm'


> utils:::menuInstallPkgs()
also installing the dependencies 'akima', 'gam', 'RColorBrewer', 'sm',
'deldir', 'sp', 'maps', 'spatstat', 'PBSmapping', 'gpclib', 'RArcInfo',
'tkrplot', 'maptools', 'mapproj', 'rgl', 'qcc', 'sgeostat', 'acepack',
'TeachingDemos', 'chron', 'Hmisc'
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/akima_0.5-1.zip'
Content type 'application/zip' length 128809 bytes
opened URL
downloaded 125Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/gam_0.98.zip'
Content type 'application/zip' length 238008 bytes
opened URL
downloaded 232Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/RColorBrewer_0.2-3.zip'
Content type 'application/zip' length 39787 bytes
opened URL
downloaded 38Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/sm_2.1-0.zip'
Content type 'application/zip' length 400621 bytes
opened URL
downloaded 391Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/deldir_0.0-5.zip'
Content type 'application/zip' length 108656 bytes
opened URL
downloaded 106Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/sp_0.9-4.zip'
Content type 'application/zip' length 747542 bytes
opened URL
downloaded 730Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/maps_2.0-33.zip'
Content type 'application/zip' length 2219136 bytes
opened URL
downloaded 2167Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/spatstat_1.11-0.zip'
Content type 'application/zip' length 4558460 bytes
opened URL
downloaded 4451Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/PBSmapping_2.09.zip'
Content type 'application/zip' length 6725596 bytes
opened URL
downloaded 6567Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/gpclib_1.3-3.zip'
Content type 'application/zip' length 95120 bytes
opened URL
downloaded 92Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/RArcInfo_0.4-7.zip'
Content type 'application/zip' length 374375 bytes
opened URL
downloaded 365Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/tkrplot_0.0-16.zip'
Content type 'application/zip' length 24119 bytes
opened URL
downloaded 23Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/maptools_0.6-6.zip'
Content type 'application/zip' length 679963 bytes
opened URL
downloaded 664Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/mapproj_1.1-7.1.zip'
Content type 'application/zip' length 64188 bytes
opened URL
downloaded 62Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
Content type 'application/zip' length 838137 bytes
opened URL
downloaded 818Kb
trying URL 'http://cran.wustl.edu/bin/windows/contrib/2.4/qcc_1.2.zip'
Content type 'application/zip' length 314782 bytes
opened URL
downloaded 307Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/sgeostat_1.0-20.zip'
Content type 'application/zip' length 140584 bytes
opened URL
downloaded 137Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/acepack_1.3-2.2.zip'
Content type 'application/zip' length 55675 bytes
opened URL
downloaded 54Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/TeachingDemos_1.4.zip'
Content type 'application/zip' length 383599 bytes
opened URL
downloaded 374Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/chron_2.3-9.zip'
Content type 'application/zip' length 112830 bytes
opened URL
downloaded 110Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/Hmisc_3.1-2.zip'
Content type 'application/zip' length 2079032 bytes
opened URL
downloaded 2030Kb
trying URL
'http://cran.wustl.edu/bin/windows/contrib/2.4/Design_2.0-12.zip'
Content type 'application/zip' length 1410543 bytes
opened URL
downloaded 1377Kb
Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
(x86)/R/R-2.4.1/library/fileceda26/akima/chtml/akima.chm'


From lalithaviswanath at yahoo.com  Tue Feb  6 16:34:35 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Tue, 6 Feb 2007 07:34:35 -0800 (PST)
Subject: [R] Query about merging two tables
Message-ID: <859283.56618.qm@web43120.mail.sp1.yahoo.com>

Hi
I have table1 which has the foll. columns
id age rate

and table2 which has the foll. columns
id count

I wish to get data from table1 for all the ids which
are persent in table2 and where the rate is not equal
to 999.
The ids in table2 are a subset of those in table1 and
every id in table2 has an entry in table1.

I would appreciate your input regarding the above.

Thanks in advance
Lalitha


 
____________________________________________________________________________________
No need to miss a message. Get email on-the-go


From liuwensui at gmail.com  Tue Feb  6 16:50:58 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 6 Feb 2007 10:50:58 -0500
Subject: [R] Query about merging two tables
In-Reply-To: <287125.38310.qm@web43127.mail.sp1.yahoo.com>
References: <287125.38310.qm@web43127.mail.sp1.yahoo.com>
Message-ID: <1115a2b00702060750v6c4ef9ebm73a402a092b01357@mail.gmail.com>

subset(table1, rate != 999&id == table2$id)

On 2/6/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
> Hi
> I have table1 which has the foll. columns
> id age rate
>
> and table2 which has the foll. columns
> id count
>
> I wish to get data from table1 for all the ids which
> are persent in table2 and where the rate is not equal
> to 999.
> The ids in table2 are a subset of those in table1 and
> every id in table2 has an entry in table1.
>
> I would appreciate your input regarding the above.
>
> Thanks in advance
> Lalitha
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From wl2776 at gmail.com  Tue Feb  6 16:57:24 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 6 Feb 2007 07:57:24 -0800 (PST)
Subject: [R] Query about merging two tables
In-Reply-To: <1115a2b00702060750v6c4ef9ebm73a402a092b01357@mail.gmail.com>
References: <287125.38310.qm@web43127.mail.sp1.yahoo.com>
	<1115a2b00702060750v6c4ef9ebm73a402a092b01357@mail.gmail.com>
Message-ID: <8828460.post@talk.nabble.com>



Wensui Liu wrote:
> 
> subset(table1, rate != 999&id == table2$id)
> 
> On 2/6/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
>> Hi
>> I have table1 which has the foll. columns
>> id age rate
>>
>> and table2 which has the foll. columns
>> id count
>>
>> I wish to get data from table1 for all the ids which
>> are persent in table2 and where the rate is not equal
>> to 999.
>> The ids in table2 are a subset of those in table1 and
>> every id in table2 has an entry in table1.
>>
>> I would appreciate your input regarding the above.
> 
 %in%, not ==

subset(table1, rate != 999&id %in% table2$id)

-- 
View this message in context: http://www.nabble.com/-R--Query-about-merging-two-tables-tf3181164.html#a8828460
Sent from the R help mailing list archive at Nabble.com.


From b.jacobs at pandora.be  Tue Feb  6 16:59:42 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Tue, 6 Feb 2007 16:59:42 +0100
Subject: [R] Conditional Sum
In-Reply-To: AAAAAOXvfE/tUWVMujI1J0cWacvk9ycA
Message-ID: <20070206155942.2670512405E@hoboe2bl1.telenet-ops.be>


Hi,

I would like to make a conditional sum for certain columns in a dataframe.

This is how my dataframe looks like:

Clinic Rep ColM1 ColM2 ColM3 ... ColM40
A      1     1    0      0        1
B      1     0    -1     0        0
C      1     0    -1     1        -1 
A      2     1    1      -1       0
B      2     -1   0      0        1 
C      2     1    0      1       -1

I would like to have two new dataframes so that
Dataframe1: with the count of all 1

Clinic  ColM1  ColM2 ColM3 .. ColM40
A        2      1    0         1
B        0      0    0         1
C        1      0    2         0

Dataframe2: with the count of all -1

Clinic  ColM1  ColM2 ColM3 .. ColM40
A        0      0    1         0
B        1      1    0         0
C        0      1    0         2


Is there an easy way to achieve this.
Thx,
Bert


From murdoch at stats.uwo.ca  Tue Feb  6 17:08:00 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 06 Feb 2007 11:08:00 -0500
Subject: [R] installing packages and windows vista
In-Reply-To: <45C84B7C0200005A0000374F@co5.dnr.state.mn.us>
References: <45C84B7C0200005A0000374F@co5.dnr.state.mn.us>
Message-ID: <45C8A7E0.9030004@stats.uwo.ca>

On 2/6/2007 10:33 AM, Daniel O'Shea wrote:
> I installed  R  (R-2.4.1-win32.exe) on a new computer with Windows Vista
> and a 64 bit operating system (hp dv9000 with intel core t7200).  The
> base R runs fine, but I can not get any of the packages to load.  From
> within R I choose install packages choose a site then a package.  I
> tried installing 2 packages and get similar errors (see below), I just
> copied and pasted lines from R.
> 
> Can anyone offer any suggestions?  Thank you.

I believe that on Vista you need to do like other OS's, and run package 
installs at a higher security level than the default.  I don't have 
Vista so I've never done this, but I've been told you do it by right 
clicking on the R icon and choosing "Run as administrator".

I'd be interested in hearing if this is true of all package installs, or 
only installs to C:/Program files.  Can you have a local library for 
your user, with only user permissions needed to modify packages there?
You'd test this by creating a library directory in your own file space, 
then using .libPaths() to add it to the library location list.  By 
default new installs would go there.

Duncan Murdoch

> 
> Dan O'Shea
> 
>> utils:::menuInstallPkgs()
> --- Please select a CRAN mirror for use in this session ---
> also installing the dependencies 'scatterplot3d', 'rgl', 'ellipse'
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/scatterplot3d_0.3-24.zip'
> Content type 'application/zip' length 540328 bytes
> opened URL
> downloaded 527Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
> Content type 'application/zip' length 838137 bytes
> opened URL
> downloaded 818Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/ellipse_0.3-4.zip'
> Content type 'application/zip' length 91877 bytes
> opened URL
> downloaded 89Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/vegan_1.8-5.zip'
> Content type 'application/zip' length 1176434 bytes
> opened URL
> downloaded 1148Kb
> Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
> (x86)/R/R-2.4.1/library/file60bf5753/scatterplot3d/chtml/scatterplot3d.chm'
> 
> 
>> utils:::menuInstallPkgs()
> also installing the dependencies 'akima', 'gam', 'RColorBrewer', 'sm',
> 'deldir', 'sp', 'maps', 'spatstat', 'PBSmapping', 'gpclib', 'RArcInfo',
> 'tkrplot', 'maptools', 'mapproj', 'rgl', 'qcc', 'sgeostat', 'acepack',
> 'TeachingDemos', 'chron', 'Hmisc'
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/akima_0.5-1.zip'
> Content type 'application/zip' length 128809 bytes
> opened URL
> downloaded 125Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/gam_0.98.zip'
> Content type 'application/zip' length 238008 bytes
> opened URL
> downloaded 232Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/RColorBrewer_0.2-3.zip'
> Content type 'application/zip' length 39787 bytes
> opened URL
> downloaded 38Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/sm_2.1-0.zip'
> Content type 'application/zip' length 400621 bytes
> opened URL
> downloaded 391Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/deldir_0.0-5.zip'
> Content type 'application/zip' length 108656 bytes
> opened URL
> downloaded 106Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/sp_0.9-4.zip'
> Content type 'application/zip' length 747542 bytes
> opened URL
> downloaded 730Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/maps_2.0-33.zip'
> Content type 'application/zip' length 2219136 bytes
> opened URL
> downloaded 2167Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/spatstat_1.11-0.zip'
> Content type 'application/zip' length 4558460 bytes
> opened URL
> downloaded 4451Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/PBSmapping_2.09.zip'
> Content type 'application/zip' length 6725596 bytes
> opened URL
> downloaded 6567Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/gpclib_1.3-3.zip'
> Content type 'application/zip' length 95120 bytes
> opened URL
> downloaded 92Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/RArcInfo_0.4-7.zip'
> Content type 'application/zip' length 374375 bytes
> opened URL
> downloaded 365Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/tkrplot_0.0-16.zip'
> Content type 'application/zip' length 24119 bytes
> opened URL
> downloaded 23Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/maptools_0.6-6.zip'
> Content type 'application/zip' length 679963 bytes
> opened URL
> downloaded 664Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/mapproj_1.1-7.1.zip'
> Content type 'application/zip' length 64188 bytes
> opened URL
> downloaded 62Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
> Content type 'application/zip' length 838137 bytes
> opened URL
> downloaded 818Kb
> trying URL 'http://cran.wustl.edu/bin/windows/contrib/2.4/qcc_1.2.zip'
> Content type 'application/zip' length 314782 bytes
> opened URL
> downloaded 307Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/sgeostat_1.0-20.zip'
> Content type 'application/zip' length 140584 bytes
> opened URL
> downloaded 137Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/acepack_1.3-2.2.zip'
> Content type 'application/zip' length 55675 bytes
> opened URL
> downloaded 54Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/TeachingDemos_1.4.zip'
> Content type 'application/zip' length 383599 bytes
> opened URL
> downloaded 374Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/chron_2.3-9.zip'
> Content type 'application/zip' length 112830 bytes
> opened URL
> downloaded 110Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/Hmisc_3.1-2.zip'
> Content type 'application/zip' length 2079032 bytes
> opened URL
> downloaded 2030Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/Design_2.0-12.zip'
> Content type 'application/zip' length 1410543 bytes
> opened URL
> downloaded 1377Kb
> Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
> (x86)/R/R-2.4.1/library/fileceda26/akima/chtml/akima.chm'
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From P.Dalgaard at biostat.ku.dk  Tue Feb  6 17:17:10 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 06 Feb 2007 17:17:10 +0100
Subject: [R] installing packages and windows vista
In-Reply-To: <45C84B7C0200005A0000374F@co5.dnr.state.mn.us>
References: <45C84B7C0200005A0000374F@co5.dnr.state.mn.us>
Message-ID: <45C8AA06.9030602@biostat.ku.dk>

Daniel O'Shea wrote:
> I installed  R  (R-2.4.1-win32.exe) on a new computer with Windows Vista
> and a 64 bit operating system (hp dv9000 with intel core t7200).  The
> base R runs fine, but I can not get any of the packages to load.  From
> within R I choose install packages choose a site then a package.  I
> tried installing 2 packages and get similar errors (see below), I just
> copied and pasted lines from R.
>
> Can anyone offer any suggestions?  Thank you.
>
>   
Hmm. As you might realize, there is as yet very little experience with
Vista to go around. Prepare to be quizzed...

The immediate suspicions would be permission problems. You might
consider changing directory to "My Documents/R" or whatever the
equivalent is on Vista, before you start installation (make sure it
exists, of course). You might also try something like

install.packages("ISwR", lib="mylib")

Again, first make sure that the directory exists. (Reason for suggesting
ISwR is that I know that that package has no uninstalled dependencies,
so it simplifies the issue.) One more thing to try would be to download
the package as a local Zip file and install from that.

    -pd
> Dan O'Shea
>
>   
>> utils:::menuInstallPkgs()
>>     
> --- Please select a CRAN mirror for use in this session ---
> also installing the dependencies 'scatterplot3d', 'rgl', 'ellipse'
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/scatterplot3d_0.3-24.zip'
> Content type 'application/zip' length 540328 bytes
> opened URL
> downloaded 527Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
> Content type 'application/zip' length 838137 bytes
> opened URL
> downloaded 818Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/ellipse_0.3-4.zip'
> Content type 'application/zip' length 91877 bytes
> opened URL
> downloaded 89Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/vegan_1.8-5.zip'
> Content type 'application/zip' length 1176434 bytes
> opened URL
> downloaded 1148Kb
> Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
> (x86)/R/R-2.4.1/library/file60bf5753/scatterplot3d/chtml/scatterplot3d.chm'
>
>
>   
>> utils:::menuInstallPkgs()
>>     
> also installing the dependencies 'akima', 'gam', 'RColorBrewer', 'sm',
> 'deldir', 'sp', 'maps', 'spatstat', 'PBSmapping', 'gpclib', 'RArcInfo',
> 'tkrplot', 'maptools', 'mapproj', 'rgl', 'qcc', 'sgeostat', 'acepack',
> 'TeachingDemos', 'chron', 'Hmisc'
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/akima_0.5-1.zip'
> Content type 'application/zip' length 128809 bytes
> opened URL
> downloaded 125Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/gam_0.98.zip'
> Content type 'application/zip' length 238008 bytes
> opened URL
> downloaded 232Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/RColorBrewer_0.2-3.zip'
> Content type 'application/zip' length 39787 bytes
> opened URL
> downloaded 38Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/sm_2.1-0.zip'
> Content type 'application/zip' length 400621 bytes
> opened URL
> downloaded 391Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/deldir_0.0-5.zip'
> Content type 'application/zip' length 108656 bytes
> opened URL
> downloaded 106Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/sp_0.9-4.zip'
> Content type 'application/zip' length 747542 bytes
> opened URL
> downloaded 730Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/maps_2.0-33.zip'
> Content type 'application/zip' length 2219136 bytes
> opened URL
> downloaded 2167Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/spatstat_1.11-0.zip'
> Content type 'application/zip' length 4558460 bytes
> opened URL
> downloaded 4451Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/PBSmapping_2.09.zip'
> Content type 'application/zip' length 6725596 bytes
> opened URL
> downloaded 6567Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/gpclib_1.3-3.zip'
> Content type 'application/zip' length 95120 bytes
> opened URL
> downloaded 92Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/RArcInfo_0.4-7.zip'
> Content type 'application/zip' length 374375 bytes
> opened URL
> downloaded 365Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/tkrplot_0.0-16.zip'
> Content type 'application/zip' length 24119 bytes
> opened URL
> downloaded 23Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/maptools_0.6-6.zip'
> Content type 'application/zip' length 679963 bytes
> opened URL
> downloaded 664Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/mapproj_1.1-7.1.zip'
> Content type 'application/zip' length 64188 bytes
> opened URL
> downloaded 62Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
> Content type 'application/zip' length 838137 bytes
> opened URL
> downloaded 818Kb
> trying URL 'http://cran.wustl.edu/bin/windows/contrib/2.4/qcc_1.2.zip'
> Content type 'application/zip' length 314782 bytes
> opened URL
> downloaded 307Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/sgeostat_1.0-20.zip'
> Content type 'application/zip' length 140584 bytes
> opened URL
> downloaded 137Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/acepack_1.3-2.2.zip'
> Content type 'application/zip' length 55675 bytes
> opened URL
> downloaded 54Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/TeachingDemos_1.4.zip'
> Content type 'application/zip' length 383599 bytes
> opened URL
> downloaded 374Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/chron_2.3-9.zip'
> Content type 'application/zip' length 112830 bytes
> opened URL
> downloaded 110Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/Hmisc_3.1-2.zip'
> Content type 'application/zip' length 2079032 bytes
> opened URL
> downloaded 2030Kb
> trying URL
> 'http://cran.wustl.edu/bin/windows/contrib/2.4/Design_2.0-12.zip'
> Content type 'application/zip' length 1410543 bytes
> opened URL
> downloaded 1377Kb
> Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
> (x86)/R/R-2.4.1/library/fileceda26/akima/chtml/akima.chm'
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From kashei at sip-oy.com  Tue Feb  6 18:58:07 2007
From: kashei at sip-oy.com (Kaskelma, Heikki)
Date: Tue, 6 Feb 2007 19:58:07 +0200
Subject: [R] Conditional Sum
Message-ID: <58FDC30CAA92594996B65C93926950AF1E0496@epont3.mas-oy.com>

Yes, consider:

df=data.frame(Clinic=rep(c("A", "B", "C"), 2),
              Rep=c(1, 1, 1, 2, 2, 2),
              colM1=c(1, 0, 0, 1, -1, 1),
              ColM2=c(0, -1, -1, 1, 0, 0),
              ColM3=c(0, 0, 1, -1, 0, 1),
              ColM40=c(1, 0, -1, 0, 1, -1)
             )
Clinic=1; Rep=2
ff=function(x, v) sum(x == v)
Dataframe1=aggregate(df[, -c(Clinic, Rep)], df[Clinic], ff, 1)
names(Dataframe1)[1]="Clinic"
Dataframe2=aggregate(df[, -c(Clinic, Rep)], df[Clinic], ff, -1)
names(Dataframe2)[1]="Clinic"
Dataframe1
Dataframe2


Heikki Kaskelma

-------------------------------------- 
Bert Jacobs kirjoitti jokin aikaa sitten:
> I would like to make a conditional sum for certain columns in 
> a dataframe.
> 
> This is how my dataframe looks like:
> 
> Clinic Rep ColM1 ColM2 ColM3 ... ColM40
> A      1     1    0      0        1
> B      1     0    -1     0        0
> C      1     0    -1     1        -1 
> A      2     1    1      -1       0
> B      2     -1   0      0        1 
> C      2     1    0      1       -1
> 
> I would like to have two new dataframes so that
> Dataframe1: with the count of all 1
> 
> Clinic  ColM1  ColM2 ColM3 .. ColM40
> A        2      1    0         1
> B        0      0    0         1
> C        1      0    2         0
> 
> Dataframe2: with the count of all -1
> 
> Clinic  ColM1  ColM2 ColM3 .. ColM40
> A        0      0    1         0
> B        1      1    0         0
> C        0      1    0         2
> 
> 
> Is there an easy way to achieve this.
> Thx,
> Bert


From eduarmasrs at yahoo.com.br  Tue Feb  6 21:10:40 2007
From: eduarmasrs at yahoo.com.br (Eduardo Dutra de Armas)
Date: Tue, 6 Feb 2007 17:10:40 -0300
Subject: [R] RES:  RdbiPgSQL in R 2.4.1
In-Reply-To: <m2bqk832cs.fsf@ziti.local>
References: <!&!AAAAAAAAAAAYAAAAAAAAAJZDfeLVO6RNtveDzx6TiALCgAAAEAAAAGhIfGxDMK9Jg1YaS0GgIgkBAAAAAA==@yahoo.com.br>
	<m2bqk832cs.fsf@ziti.local>
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAJZDfeLVO6RNtveDzx6TiALCgAAAEAAAAFq3kH7nb3hCm3lhtS0fCtQBAAAAAA==@yahoo.com.br>

Hi Seth
I'm running R on WinXP. On the first time I've installed Rdbi and RdbiPgSQL
from zip files not downloaded from Bioconductor. No problems during
installation.
Now I tried to install from biocLite and a message was showed for RdbiPgSQL.

> biocLite(c("Rdbi", "RdbiPgSQL"))
Running getBioC version 0.1.8 with R version 2.4.1
Running biocinstall version 1.9.9 with R version 2.4.1
Your version of R requires version 1.9 of Bioconductor.
Dependency ''RdbiPgSQL'' is not available
...

I gave a look at Bioconductor site, where I saw an Error report during
checking of BioC 1.9/RdbiPgSQL for Windows Server 2003. The same was
reported for another BioC releases.
Conclusion: Isn't possible access postgreSQL databases from R?

Thanks


-----Mensagem original-----
De: Seth Falcon [mailto:sfalcon at fhcrc.org] 
Enviada em: segunda-feira, 5 de fevereiro de 2007 15:07
Para: Eduardo Dutra de Armas
Cc: r-help at stat.math.ethz.ch
Assunto: Re: [R] RdbiPgSQL in R 2.4.1

Hi Eduardo,

It would probably be best to send question regarding Bioconductor
packages to the bioconductor email list.

"Eduardo Dutra de Armas" <eduarmasrs at yahoo.com.br> writes:

> Hi R-users
>
> I recently downloaded RdbiPgSQL 1.8.0 and Rdbi 1.8.0 from Bioconductor to
be
> installed under R 2.4.1.
>
> When requiring RdbiPgSQL an error message is showed as follows:
>
>  
>
>> require(RdbiPgSQL)
>
> Loading required package: RdbiPgSQL
>
> Error in library(package, lib.loc = lib.loc, character.only = TRUE,
logical
> = TRUE,   :
>
>              RdbiPgSQL is not a valid package  installed < 2.0.0?
>
>  
>
> There is no issue about version restriction in Description file.

What OS are you running on?  How did you install the packages?  I'm
not able to reproduce the message you posted.

Perhaps try reinstalling using the biocLite install script.  Like
this:

    source("http://bioconductor.org/biocLite.R")
    biocLite(c("Rdbi", "RdbiPgSQL))

NB: normally, you don't need to specify dependencies, but this way it
will reinstall Rdbi as well in case you have a bogus install...

+ seth

-- 
No virus found in this incoming message.


14:04
 

-- 



16:48


From Roger.Bivand at nhh.no  Tue Feb  6 20:01:24 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 6 Feb 2007 20:01:24 +0100 (CET)
Subject: [R] How to do "moran's I test"?
In-Reply-To: <45C8764D.027B73.20649@m5-83.163.com>
Message-ID: <Pine.LNX.4.44.0702061955370.13521-100000@reclus.nhh.no>

On Tue, 6 Feb 2007, Jian Zhang wrote:

> I want to do "moran's I test" in R language. I try to use "gearymoran" in
> Package "ade4","moran" in Package "spdep", and Moran.I in Package "ape". But
> I do not know how to do it because data format is different.

Please consider posting on the R-sig-geo mailing list. If you are an 
ecologist, you may find the off-CRAN ncf package by Ottar Bj?rnstad more 
intuitive - see the correlog function:

http://asi23.ent.psu.edu/onb1/software.html

> 
> My data:
> 
>     x        y       dbh   
> 
>   111.03    10.7       7
> 
>   118.11    0.28     1.2
> 
>   165.36    0.36     8.4
> 
>    282.9     0.3     7.5
> 
>   303.29   13.32    12.2
> 
>   319.28    3.88     6.2
> 
>      447       9       6
> 
>    445.5    18.3    13.8
> 
>    445.5    12.1     7.1
> 
>   467.64     1.2     4.7
> 
>    485.4    14.1     4.4
> 
>     2.98   23.95    11.7
> 
>       15   35.78    23.5
> 
>   130.21    23.6    14.1
> 
>    213.5   23.22    21.5
> 
>   233.57   28.76    35.4
> 
>    482.3    20.5       6
> 
>    69.73   45.21     7.5
> 
>     69.8   50.49    10.2
> 
>    76.65    45.5    21.5 
> 
>  
> 
> "x","y" are spatial coordinate;
> 
> "dbh" is variable
> 
> I want to test the spatial autocorrelation of "dbh" in different distances
> by "x" and "y".
> 
>  
> 
> Thanks,
> 
>                                 Jian Zhang
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From HDoran at air.org  Tue Feb  6 20:08:01 2007
From: HDoran at air.org (Doran, Harold)
Date: Tue, 6 Feb 2007 14:08:01 -0500
Subject: [R] R in Industry
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070206/6b73cae1/attachment.pl 

From sfalcon at fhcrc.org  Tue Feb  6 20:24:15 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 06 Feb 2007 11:24:15 -0800
Subject: [R] RES:  RdbiPgSQL in R 2.4.1
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAJZDfeLVO6RNtveDzx6TiALCgAAAEAAAAFq3kH7nb3hCm3lhtS0fCtQBAAAAAA==@yahoo.com.br>
	(Eduardo Dutra de Armas's message of "Tue,
	6 Feb 2007 17:10:40 -0300")
References: <!&!AAAAAAAAAAAYAAAAAAAAAJZDfeLVO6RNtveDzx6TiALCgAAAEAAAAGhIfGxDMK9Jg1YaS0GgIgkBAAAAAA==@yahoo.com.br>
	<m2bqk832cs.fsf@ziti.local>
	<!&!AAAAAAAAAAAYAAAAAAAAAJZDfeLVO6RNtveDzx6TiALCgAAAEAAAAFq3kH7nb3hCm3lhtS0fCtQBAAAAAA==@yahoo.com.br>
Message-ID: <m27iuvw0m8.fsf@ziti.local>

"Eduardo Dutra de Armas" <eduarmasrs at yahoo.com.br> writes:

> Hi Seth
> I'm running R on WinXP. On the first time I've installed Rdbi and RdbiPgSQL
> from zip files not downloaded from Bioconductor. No problems during
> installation.
> Now I tried to install from biocLite and a message was showed for RdbiPgSQL.
>
>> biocLite(c("Rdbi", "RdbiPgSQL"))
> Running getBioC version 0.1.8 with R version 2.4.1
> Running biocinstall version 1.9.9 with R version 2.4.1
> Your version of R requires version 1.9 of Bioconductor.
> Dependency ''RdbiPgSQL'' is not available
> ...
>
> I gave a look at Bioconductor site, where I saw an Error report during
> checking of BioC 1.9/RdbiPgSQL for Windows Server 2003. The same was
> reported for another BioC releases.
> Conclusion: Isn't possible access postgreSQL databases from R?

You are partly right: We don't have Windows binary packages available
for the RdbiPgSQL package.  The source package works on Linux and OS X
(if you have the appropriate tools to build source packages).

You might be able to connect using the RODBC package.

+ seth


From sebastiendurand at videotron.ca  Tue Feb  6 20:36:05 2007
From: sebastiendurand at videotron.ca (Sebastien Durand)
Date: Tue, 06 Feb 2007 14:36:05 -0500
Subject: [R] Naming bring to top graphical Device
Message-ID: <p06230919c1ee8345bab2@[192.168.2.3]>

Dear all,

Here is my questions:

1- Under a WINDOWS installation of R-2.4.1,  can 
we change the naming of a new ploting device open 
by the command "windows()"?. Instead of the 
default name e.g.: "Device 2" I would like to use 
something like "Density plot" or whatever!


2- Under a MAC OS X installation of R-2.4.1, 
using quartz devices, is there a way to perform 
bringToTop operation like the one available under 
WINDOWS using bringToTop function.

Thank you very much for your time.

Cheers

S?bastien
--


From JILWIL at SAFECO.com  Tue Feb  6 20:42:49 2007
From: JILWIL at SAFECO.com (WILLIE, JILL)
Date: Tue, 6 Feb 2007 11:42:49 -0800
Subject: [R] glm gamma scale parameter
Message-ID: <916551423F01504BA339BF69CBA3BE72016E7ADF@psmrdcex18.psm.pin.safeco.com>

I would like the option to specify alternative scale parameters when
using the gamma family, log link glm.  In particular I would like the
option to specify any of the following:

1.  maximum likelihood estimate
2.  moment estimator/Pearson's 
3.  total deviance estimator

Is this easy?  Possible?  

In addition, I would like to know what estimation process (maximum
likelihood?) R is using to estimate the parameter if somebody knows that
off the top of their head or can point me to something to read?  

I did read the help & search the archives but I'm a bit confused trying
to reconcile the terminology I'm used to w/R terminology as we're
transitioning to R, so if I missed an obvious way to do this, or stated
this question in a way that's incomprehensible, my apologies.

Jill Willie 
Open Seas
Safeco Insurance
jilwil at safeco.com


From services13ab at aim.com  Tue Feb  6 20:59:55 2007
From: services13ab at aim.com (Angela Frenz)
Date: Tue, 06 Feb 2007 14:59:55 -0500
Subject: [R] Confirmation Of Results For Electronic Email Winners
Message-ID: <E1HEWTr-0006PP-RL@server.frozenmidnightnetwork.com>

Confirmation Of Results For Electronic Email Winners
MICROWORLD INTERNATIONAL PROMOTIONS
EUROPEAN CENTRAL OFFICE
162 ZUIDOOST 1109
AMSTERDARM THE NETHERLANDS

We happily announce to you the draw  of the  Microworld International promotions organized for email users all over the World  as part of our international promotional program to encourage email Users.

Your e-mail address, attached to ticket number NL1267OPLT, 
with serial Number WO-170Z-X and lucky number 12,34,90,00,63,10 consequently won in The Second Category. You have therefore been approved to receive the Sum of 1,000,000 (One Million Euros), which is the winning payout  for Second Category winners. 

Do not have doubt over Your participation in this Promotion, as all Participants were selected Through an electronic computer  ballot system for all email users drawn From over  2,500,000 email addresses of individuals and companies  from Africa, America, Asia, Australia, 
Europe, Middle East,  Oceania, North American, and South American.

Contact the Paying Bank with the following information 

1.Name:
2.Telephone numbers and fax:
3.Age:
4.Address:
5.Country:
6.Occupation:
7.REF Number: LST/006/NL/LT1
8.BATCH No: BAT/LTNL/1236/0TY

***********************************************
Contact person: Mr JIM BENSON
laagste Hypotheekofferte Nl
Amsterdarm, Netherlands
E-mail:cutservices at aim.com
Tel: 0031-626-413-226
***********************************************

Yours Sincerely, 
Mrs.Angela Frenz


From ripley at stats.ox.ac.uk  Tue Feb  6 21:05:56 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Feb 2007 20:05:56 +0000 (GMT)
Subject: [R] glm gamma scale parameter
In-Reply-To: <916551423F01504BA339BF69CBA3BE72016E7ADF@psmrdcex18.psm.pin.safeco.com>
References: <916551423F01504BA339BF69CBA3BE72016E7ADF@psmrdcex18.psm.pin.safeco.com>
Message-ID: <Pine.LNX.4.64.0702062002310.24377@auk.stats>

I think you mean 'shape parameter'.  If so, see the MASS package and 
?gamma.shape.

glm() _is_ providing you with the MLE of the scale parameter, but really 
no estimate of the shape (although summary.glm makes use of one).


On Tue, 6 Feb 2007, WILLIE, JILL wrote:

> I would like the option to specify alternative scale parameters when
> using the gamma family, log link glm.  In particular I would like the
> option to specify any of the following:
>
> 1.  maximum likelihood estimate
> 2.  moment estimator/Pearson's
> 3.  total deviance estimator
>
> Is this easy?  Possible?
>
> In addition, I would like to know what estimation process (maximum
> likelihood?) R is using to estimate the parameter if somebody knows that
> off the top of their head or can point me to something to read?
>
> I did read the help & search the archives but I'm a bit confused trying
> to reconcile the terminology I'm used to w/R terminology as we're
> transitioning to R, so if I missed an obvious way to do this, or stated
> this question in a way that's incomprehensible, my apologies.
>
> Jill Willie
> Open Seas
> Safeco Insurance
> jilwil at safeco.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sapsi at pobox.com  Tue Feb  6 21:23:15 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 6 Feb 2007 15:23:15 -0500
Subject: [R] A question regarding cex and pch="." in lattice
Message-ID: <EDEDB491-7FA8-4EEB-9645-B5EB65C505B9@pobox.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070206/52bf7d7f/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Feb  6 21:24:45 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Feb 2007 20:24:45 +0000 (GMT)
Subject: [R] glm gamma scale parameter
In-Reply-To: <Pine.LNX.4.64.0702062002310.24377@auk.stats>
References: <916551423F01504BA339BF69CBA3BE72016E7ADF@psmrdcex18.psm.pin.safeco.com>
	<Pine.LNX.4.64.0702062002310.24377@auk.stats>
Message-ID: <Pine.LNX.4.64.0702062022260.13310@gannet.stats.ox.ac.uk>

On Tue, 6 Feb 2007, Prof Brian Ripley wrote:

> I think you mean 'shape parameter'.  If so, see the MASS package and 
> ?gamma.shape.

Also http://www.stats.ox.ac.uk/pub/MASS4/#Complements
leads to several pages of discussion.

>
> glm() _is_ providing you with the MLE of the scale parameter, but really no 
> estimate of the shape (although summary.glm makes use of one).
>
>
> On Tue, 6 Feb 2007, WILLIE, JILL wrote:
>
>> I would like the option to specify alternative scale parameters when
>> using the gamma family, log link glm.  In particular I would like the
>> option to specify any of the following:
>> 
>> 1.  maximum likelihood estimate
>> 2.  moment estimator/Pearson's
>> 3.  total deviance estimator
>> 
>> Is this easy?  Possible?
>> 
>> In addition, I would like to know what estimation process (maximum
>> likelihood?) R is using to estimate the parameter if somebody knows that
>> off the top of their head or can point me to something to read?
>> 
>> I did read the help & search the archives but I'm a bit confused trying
>> to reconcile the terminology I'm used to w/R terminology as we're
>> transitioning to R, so if I missed an obvious way to do this, or stated
>> this question in a way that's incomprehensible, my apologies.
>> 
>> Jill Willie
>> Open Seas
>> Safeco Insurance
>> jilwil at safeco.com
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Feb  6 22:05:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Feb 2007 21:05:47 +0000 (GMT)
Subject: [R] A question regarding cex and pch="." in lattice
In-Reply-To: <EDEDB491-7FA8-4EEB-9645-B5EB65C505B9@pobox.com>
References: <EDEDB491-7FA8-4EEB-9645-B5EB65C505B9@pobox.com>
Message-ID: <Pine.LNX.4.64.0702062044120.10526@gannet.stats.ox.ac.uk>

How do you know the points are not being plotted?

All you can tell is that your graphics card / monitor / X11 driver is not
displaying them, not at all the same thing.

My guess is that the rectangle is being displayed only if the differences 
in its discretized x and y coordinates are both positive.  R tries hard to 
ensure that pch='.', cex=1 does always display a point, but not for 
smaller cex.

On Tue, 6 Feb 2007, Saptarshi Guha wrote:

> Hello,
> 	I'm using lattice and opened an X11 device with the following call
> 	X11(width=5,height=5,pointsize=1)
> 	I then ran the following code
>
> 	library(lattice)
> 	x<-rnorm(30,sd=2)
> 	y<-runif(30)
> 	xyplot(y~x,pch=".",col="black",cex=1)
>
> 	If i remove "cex=1", not all the points are plotted. From ?X11, i
> read,"pch='.' with cex=1 corresponds to a rectangle of sides the
> larger of one pixel and 0.01 inch"
>
> 	By way of information, i'm using a Dell 30", where the the pixel
> pitch is 0.250mm(0.0098 inch).
>
> 	My question, why is that some of the points are not being plotted
> when remove cex=1 ?

The default cex for this plot is 0.8. See trellis.par.get("plot.symbol").

> 	Regards
> 	Saptarshi
>
> Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sapsi at pobox.com  Tue Feb  6 22:12:32 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 6 Feb 2007 16:12:32 -0500
Subject: [R] A question regarding cex and pch="." in lattice
In-Reply-To: <Pine.LNX.4.64.0702062044120.10526@gannet.stats.ox.ac.uk>
References: <EDEDB491-7FA8-4EEB-9645-B5EB65C505B9@pobox.com>
	<Pine.LNX.4.64.0702062044120.10526@gannet.stats.ox.ac.uk>
Message-ID: <BC397E9B-EBD5-4B60-A0D3-CEF2E426B55B@pobox.com>


On Feb 6, 2007, at 4:05 PM, Prof Brian Ripley wrote:

> How do you know the points are not being plotted?
>
> All you can tell is that your graphics card / monitor / X11 driver  
> is not
> displaying them, not at all the same thing.
>

Yes, that is what I meant but I suppose I conveyed it wrongly.



> My guess is that the rectangle is being displayed only if the  
> differences in its discretized x and y coordinates are both  
> positive.  R tries hard to ensure that pch='.', cex=1 does always  
> display a point, but not for smaller cex.

This could be the case.
Thanks
Saptarshi


>
> On Tue, 6 Feb 2007, Saptarshi Guha wrote:
>
>> Hello,
>> 	I'm using lattice and opened an X11 device with the following call
>> 	X11(width=5,height=5,pointsize=1)
>> 	I then ran the following code
>>
>> 	library(lattice)
>> 	x<-rnorm(30,sd=2)
>> 	y<-runif(30)
>> 	xyplot(y~x,pch=".",col="black",cex=1)
>>
>> 	If i remove "cex=1", not all the points are plotted. From ?X11, i
>> read,"pch='.' with cex=1 corresponds to a rectangle of sides the
>> larger of one pixel and 0.01 inch"
>>
>> 	By way of information, i'm using a Dell 30", where the the pixel
>> pitch is 0.250mm(0.0098 inch).
>>
>> 	My question, why is that some of the points are not being plotted
>> when remove cex=1 ?
>
> The default cex for this plot is 0.8. See trellis.par.get 
> ("plot.symbol").
>
>> 	Regards
>> 	Saptarshi
>>
>> Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From JILWIL at SAFECO.com  Tue Feb  6 22:13:56 2007
From: JILWIL at SAFECO.com (WILLIE, JILL)
Date: Tue, 6 Feb 2007 13:13:56 -0800
Subject: [R] glm gamma scale parameter
In-Reply-To: <Pine.LNX.4.64.0702062022260.13310@gannet.stats.ox.ac.uk>
References: <916551423F01504BA339BF69CBA3BE72016E7ADF@psmrdcex18.psm.pin.safeco.com>
	<Pine.LNX.4.64.0702062002310.24377@auk.stats>
	<Pine.LNX.4.64.0702062022260.13310@gannet.stats.ox.ac.uk>
Message-ID: <916551423F01504BA339BF69CBA3BE72016E7AE3@psmrdcex18.psm.pin.safeco.com>

Thank you.  You are correct, the shape parameter is what I need to
change & I think I see how to use the MASS package to do it...or if not,
at least I have enough now to figure it out.

A question to reconcile terminology which will speed me up, if you have
time to help me a bit more: phi = 'scale parameter' vs. 'dispersion
parameter' vs. 'shape parameter'?  Excerpt below from the R intro.
manual defining phi & the stats compliment discussion.

R intro:

distribution of y is of the form 
f_Y(y; mu, phi) = 
 exp((A/phi) * (y lambda(mu) - gamma(lambda(mu))) + tau(y, phi))
     
where phi is a scale parameter (possibly known), and is constant for all
observations, A represents a prior weight, assumed known but possibly
varying with the observations, and $\mu$ is the mean of y. So it is
assumed that the distribution of y is determined by its mean and
possibly a scale parameter as well. 


Statistics Complements to Modern Applied Statistics with S, Fourth
edition By W. N. Venables and B. D. Ripley Springer: 

7.6 Gamma models
The role of dispersion parameter for the Gamma family is rather
different. This is a parametric family which can be fitted by maximum
likelihood, including its shape parameter 


Jill Willie 
Open Seas
Safeco Insurance
jilwil at safeco.com 


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Tuesday, February 06, 2007 12:25 PM
To: WILLIE, JILL
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] glm gamma scale parameter

On Tue, 6 Feb 2007, Prof Brian Ripley wrote:

> I think you mean 'shape parameter'.  If so, see the MASS package and 
> ?gamma.shape.

Also http://www.stats.ox.ac.uk/pub/MASS4/#Complements
leads to several pages of discussion.

>
> glm() _is_ providing you with the MLE of the scale parameter, but
really no 
> estimate of the shape (although summary.glm makes use of one).
>
>
> On Tue, 6 Feb 2007, WILLIE, JILL wrote:
>
>> I would like the option to specify alternative scale parameters when
>> using the gamma family, log link glm.  In particular I would like the
>> option to specify any of the following:
>> 
>> 1.  maximum likelihood estimate
>> 2.  moment estimator/Pearson's
>> 3.  total deviance estimator
>> 
>> Is this easy?  Possible?
>> 
>> In addition, I would like to know what estimation process (maximum
>> likelihood?) R is using to estimate the parameter if somebody knows
that
>> off the top of their head or can point me to something to read?
>> 
>> I did read the help & search the archives but I'm a bit confused
trying
>> to reconcile the terminology I'm used to w/R terminology as we're
>> transitioning to R, so if I missed an obvious way to do this, or
stated
>> this question in a way that's incomprehensible, my apologies.
>> 
>> Jill Willie
>> Open Seas
>> Safeco Insurance
>> jilwil at safeco.com
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Feb  6 22:47:41 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Feb 2007 21:47:41 +0000 (GMT)
Subject: [R] glm gamma scale parameter
In-Reply-To: <916551423F01504BA339BF69CBA3BE72016E7AE3@psmrdcex18.psm.pin.safeco.com>
References: <916551423F01504BA339BF69CBA3BE72016E7ADF@psmrdcex18.psm.pin.safeco.com>
	<Pine.LNX.4.64.0702062002310.24377@auk.stats>
	<Pine.LNX.4.64.0702062022260.13310@gannet.stats.ox.ac.uk>
	<916551423F01504BA339BF69CBA3BE72016E7AE3@psmrdcex18.psm.pin.safeco.com>
Message-ID: <Pine.LNX.4.64.0702062139250.3534@gannet.stats.ox.ac.uk>

On Tue, 6 Feb 2007, WILLIE, JILL wrote:

> Thank you.  You are correct, the shape parameter is what I need to
> change & I think I see how to use the MASS package to do it...or if not,
> at least I have enough now to figure it out.
>
> A question to reconcile terminology which will speed me up, if you have
> time to help me a bit more: phi = 'scale parameter' vs. 'dispersion
> parameter' vs. 'shape parameter'?  Excerpt below from the R intro.
> manual defining phi & the stats compliment discussion.

You need to compare the complement with MASS the book.  The dispersion is 
1/shape in the conventional parametrization of the gamma.  See 
?gamma.dispersion.

>
> R intro:
>
> distribution of y is of the form
> f_Y(y; mu, phi) =
> exp((A/phi) * (y lambda(mu) - gamma(lambda(mu))) + tau(y, phi))
>
> where phi is a scale parameter (possibly known), and is constant for all
> observations, A represents a prior weight, assumed known but possibly
> varying with the observations, and $\mu$ is the mean of y. So it is
> assumed that the distribution of y is determined by its mean and
> possibly a scale parameter as well.

Bill Venables wrote that and the equivalent part of MASS.  'dispersion' is 
a more common name for phi.

>
> Statistics Complements to Modern Applied Statistics with S, Fourth
> edition By W. N. Venables and B. D. Ripley Springer:
>
> 7.6 Gamma models
> The role of dispersion parameter for the Gamma family is rather
> different. This is a parametric family which can be fitted by maximum
> likelihood, including its shape parameter
>
>
> Jill Willie
> Open Seas
> Safeco Insurance
> jilwil at safeco.com
>
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Tuesday, February 06, 2007 12:25 PM
> To: WILLIE, JILL
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] glm gamma scale parameter
>
> On Tue, 6 Feb 2007, Prof Brian Ripley wrote:
>
>> I think you mean 'shape parameter'.  If so, see the MASS package and
>> ?gamma.shape.
>
> Also http://www.stats.ox.ac.uk/pub/MASS4/#Complements
> leads to several pages of discussion.
>
>>
>> glm() _is_ providing you with the MLE of the scale parameter, but
> really no
>> estimate of the shape (although summary.glm makes use of one).
>>
>>
>> On Tue, 6 Feb 2007, WILLIE, JILL wrote:
>>
>>> I would like the option to specify alternative scale parameters when
>>> using the gamma family, log link glm.  In particular I would like the
>>> option to specify any of the following:
>>>
>>> 1.  maximum likelihood estimate
>>> 2.  moment estimator/Pearson's
>>> 3.  total deviance estimator
>>>
>>> Is this easy?  Possible?
>>>
>>> In addition, I would like to know what estimation process (maximum
>>> likelihood?) R is using to estimate the parameter if somebody knows
> that
>>> off the top of their head or can point me to something to read?
>>>
>>> I did read the help & search the archives but I'm a bit confused
> trying
>>> to reconcile the terminology I'm used to w/R terminology as we're
>>> transitioning to R, so if I missed an obvious way to do this, or
> stated
>>> this question in a way that's incomprehensible, my apologies.
>>>
>>> Jill Willie
>>> Open Seas
>>> Safeco Insurance
>>> jilwil at safeco.com
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Max.Kuhn at pfizer.com  Tue Feb  6 23:10:25 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Tue, 6 Feb 2007 17:10:25 -0500
Subject: [R] R in Industry
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3076E6F3D@groamrexm03.amer.pfizer.com>

As someone who has (reluctantly) sent job postings to R Help, I think
that a SIG would be a good idea.

Max 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Tuesday, February 06, 2007 2:08 PM
To: R-help at stat.math.ethz.ch
Subject: [R] R in Industry

The other day, CNN had a story on working at Google. Out of curiosity, I
went to the Google employment web site (I'm not looking, but just
curious). In perusing their job posts for statisticians, preference is
given to those who use R and python. Other languages, S-Plus and
something called SAS were listed as lower priorities.

When I started using Python, I noted they have a portion of the web site
with job postings. CRAN does not have something similar, but think it
might be useful. I think R is becoming more widely used in industry and
I wonder if helping it move along a bit, the maintainer of CRAN could
create a section of the web site devoted to jobs where R is a
requirement.

Hence, we could have our own little "monster.com" kind of thing going
on. Of the multitude of ways the gospel can be spread, this is small.
But, I think every small step forward is good.

Anyone think this is useful? 

Harold


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From tariq.khan at gmail.com  Tue Feb  6 23:23:34 2007
From: tariq.khan at gmail.com (=?ISO-8859-1?Q?=A8Tariq_Khan?=)
Date: Tue, 6 Feb 2007 22:23:34 +0000
Subject: [R] R in Industry
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3076E6F3D@groamrexm03.amer.pfizer.com>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
	<71257D09F114DA4A8E134DEAC70F25D3076E6F3D@groamrexm03.amer.pfizer.com>
Message-ID: <2310043c0702061423r176a6f14m1d01fdf719f32940@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070206/36dc0147/attachment.pl 

From gunter.berton at gene.com  Tue Feb  6 23:34:54 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Tue, 6 Feb 2007 14:34:54 -0800
Subject: [R] R in Industry
In-Reply-To: <2310043c0702061423r176a6f14m1d01fdf719f32940@mail.gmail.com>
Message-ID: <009801c74a3f$0604ee50$4d908980@gne.windows.gene.com>

"... two main drawbacks of R at our firm (as viewed by our IT dept) are lack
of
guaranteed support as well as the difficulty in finding candidates.


-- Just an aside: "lack of guaranteed support" -- absolutely true in theory,
absolutely false in practice. I doubt that the voluntary support found on
r-help and other R lists can be matched by the "guaranteed" support of any
commercial software product. Not that this makes a difference to the IT
group's requirements, of course...

Cheers,
Bert


From Mark.Leeds at morganstanley.com  Tue Feb  6 23:49:46 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Tue, 6 Feb 2007 17:49:46 -0500
Subject: [R] formula for test statistics of arima coefficients
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401401602@NYWEXMB23.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070206/34972c7d/attachment.pl 

From jholtman at gmail.com  Wed Feb  7 00:23:39 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 6 Feb 2007 18:23:39 -0500
Subject: [R] Questions on counts by case
In-Reply-To: <OFE18B467D.8FF269E3-ONC125727A.0051EC30-C125727A.0051EC48@wsr.ac.at>
References: <OFE18B467D.8FF269E3-ONC125727A.0051EC30-C125727A.0051EC48@wsr.ac.at>
Message-ID: <644e1f320702061523r5c71f48biffaea64dbc7fc98c@mail.gmail.com>

On 2/6/07, Serguei Kaniovski <Serguei.Kaniovski at wifo.ac.at> wrote:
>
>
> Hi all,
>
>
> for the data below I would like to
>
> 1. generate a dummy variable for each group "gr" of the same composition by
> people, then save each portion in a separate file,
This gives back a list that you can go through and write out each
element to a different file:

> split(x, x$gr)
$gr1
  person  gr x
1   mike gr1 1
2   jane gr1 0
3   bill gr1 0

$gr2
  person  gr x
4   jack gr2 1
5   mike gr2 1
6   jane gr2 0
7   bill gr2 0
8   alex gr2 1
9  james gr2 1

$gr3
   person  gr x
10   mike gr3 0
11   bill gr3 1
12   jane gr3 1


>
> 2. compute the frequency of "1"'s in "x" for each person by group
> "gr". So, "mike" will have freq=2/3, as he has two "1" and one "0" in 3
> groups.
Is this what you want?

> x
   person  gr x
1    mike gr1 1
2    jane gr1 0
3    bill gr1 0
4    jack gr2 1
5    mike gr2 1
6    jane gr2 0
7    bill gr2 0
8    alex gr2 1
9   james gr2 1
10   mike gr3 0
11   bill gr3 1
12   jane gr3 1
> do.call(rbind, by(x, list(x$person), function(z)c("0"=sum(z$x == 0), "1"=sum(z$x == 1))))
      0 1
alex  0 1
bill  2 1
jack  0 1
james 0 1
jane  2 1
mike  1 2

>
>
> Thanks a lot,
>
> Serguei Kaniovski
>
>
> The data looks like:
>
> person;gr;x
>
> mike;gr1;1
>
> jane;gr1;0
>
> bill;gr1;0
>
> jack;gr2;1
>
> mike;gr2;1
>
> jane;gr2;0
>
> bill;gr2;0
>
> alex;gr2;1
>
> james;gr2;1
>
> mike;gr3;0
>
> bill;gr3;1
>
> jane;gr3;1
> ___________________________________________________________________
>
> AustrianInstituteofEconomicResearch(WIFO)
>
> Name:SergueiKaniovski       P.O.Box91
> Tel.:+43-1-7982601-231             ArsenalObjekt20
> Fax:+43-1-7989386                  1103Vienna,Austria
> Mail:Serguei.Kaniovski at wifo.ac.at  A-1030Wien
>
> http://www.wifo.ac.at/Serguei.Kaniovski
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From stubben at lanl.gov  Wed Feb  7 00:38:35 2007
From: stubben at lanl.gov (stubben)
Date: Tue, 6 Feb 2007 16:38:35 -0700
Subject: [R] abbreviate dataframe for Sweave output
Message-ID: <905a1292a615999b45179fff33da716c@lanl.gov>

I wanted to print the first and last rows of some dataframes in Sweave 
using dots in columns to separate the two parts.  Head and tail almost 
work, but I have problems with factors and row names.

z<-data.frame(id=letters[1:26], x=sample(1:26,26))

rbind(head(z,3), ".", tail(z,1))

      id  x
1     a 18
2     b  8
3     c 14
4  <NA>  .
26    z 10
Warning message:
invalid factor level, NAs generated in...


I would like something like this if possible.  Any ideas?

      id  x
1     a 18
2     b  8
3     c 14
.     .  .
.     .  .
26    z 10


Thanks,

Chris Stubben



-- 
-----------------

Los Alamos National Lab
BioScience Division
MS M888
Los Alamos, NM 87545


From muenchen at utk.edu  Wed Feb  7 01:11:52 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Tue, 6 Feb 2007 19:11:52 -0500
Subject: [R] R in Industry
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
Message-ID: <7270AEC73132194E8BC0EE06B35D93D879B707@UTKFSVS3.utk.tennessee.edu>

That sounds like a good idea. The name R makes it especially hard to
find job postings, resumes or do any other type of search. Googling
resume+sas or "job opening"+sas is quick and fairly effective (less a
few airline jobs). Doing that with R is of course futile. At the risk of
getting flamed, it's too bad it's not called something more unique such
as Rpackage, Rlanguage, etc.

Cheers,
Bob

=========================================================
Bob Muenchen (pronounced Min'-chen), Manager 
Statistical Consulting Center
U of TN Office of Information Technology
200 Stokely Management Center, Knoxville, TN 37996-0520
Voice: (865) 974-5230 
FAX: (865) 974-4810
Email: muenchen at utk.edu
Web: http://oit.utk.edu/scc, 
News: http://listserv.utk.edu/archives/statnews.html
=========================================================


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Tuesday, February 06, 2007 2:08 PM
To: R-help at stat.math.ethz.ch
Subject: [R] R in Industry

The other day, CNN had a story on working at Google. Out of curiosity, I
went to the Google employment web site (I'm not looking, but just
curious). In perusing their job posts for statisticians, preference is
given to those who use R and python. Other languages, S-Plus and
something called SAS were listed as lower priorities.

When I started using Python, I noted they have a portion of the web site
with job postings. CRAN does not have something similar, but think it
might be useful. I think R is becoming more widely used in industry and
I wonder if helping it move along a bit, the maintainer of CRAN could
create a section of the web site devoted to jobs where R is a
requirement.

Hence, we could have our own little "monster.com" kind of thing going
on. Of the multitude of ways the gospel can be spread, this is small.
But, I think every small step forward is good.

Anyone think this is useful? 

Harold


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From sue at xlsolutions-corp.com  Wed Feb  7 01:24:29 2007
From: sue at xlsolutions-corp.com (Sue Turner)
Date: Tue, 06 Feb 2007 17:24:29 -0700
Subject: [R] 2 Courses: (1) R/Splus Advanced Programming - San Francisco -
	(2) Data Mining: Practical Tools and Techniques in R/Splus -
	Salt Lake City
Message-ID: <20070206172429.9f08cc34deb45d78e54b3b5664e21546.42a1bd0eb9.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce
our*** R/S Advanced Programming ***course in San Francisco on March
15-16 and
*** Data Mining: Practical Tools and Techniques in R/Splus *** course in
Salt Lake City on March 26-27


Ask for group discount and reserve your seat Now - Earlybird Rates.
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com


(1) R/Splus Advanced Programming  - San Francisco, March 15-16
http://www.xlsolutions-corp.com/Radv.htm  

Course Outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function

Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578

(2) Data Mining: Practical Tools and Techniques in R/Splus - Salt Lake
City, March 26-27
http://www.xlsolutions-corp.com/RSMining.htm



Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat!

Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com


From alistair.campbell at jcu.edu.au  Wed Feb  7 02:00:33 2007
From: alistair.campbell at jcu.edu.au (Alistair Campbell)
Date: Wed,  7 Feb 2007 11:00:33 +1000 (EST)
Subject: [R] How-To construct a cov list to use a covariance matrix in
 factanal?
Message-ID: <20070207110033.BHR85871@mirapoint-ms1.jcu.edu.au>

Thanks for that Brian,

I have worked through the examples. They work because the covmat were produced by the cov.wt which provides output as a list object. I am trying to construct my own list object to use as the covmat. There are no obvious instructions on how to do this.

So, here is what I have done so far.

I reconstructed the covariance matrix in the example and created a dataframe:

 > testmatrix
  general picture  blocks   maze reading   vocab
1  24.641   5.991  33.520  6.023  20.755  29.701
2   5.991   6.700  18.137  1.782   4.936   7.204
3  33.520  18.137 149.831 19.424  31.430  50.753
4   6.023   1.782  19.424 12.711   4.757   9.075
5  20.755   4.936  31.430  4.757  52.604  66.762
6  29.701   7.204  50.753  9.075  66.762 135.292

and then used this to construct a list object like the output from the example;

> tstcov<- list(cov=testmatrix, center=c(0,0,0,0,0), n.obs=112)

I tested to see whether my list object looked like the examples

> tstcov
$cov
  general picture  blocks   maze reading   vocab
1  24.641   5.991  33.520  6.023  20.755  29.701
2   5.991   6.700  18.137  1.782   4.936   7.204
3  33.520  18.137 149.831 19.424  31.430  50.753
4   6.023   1.782  19.424 12.711   4.757   9.075
5  20.755   4.936  31.430  4.757  52.604  66.762
6  29.701   7.204  50.753  9.075  66.762 135.292

$centers
[1] 0 0 0 0 0

$n.obs
[1] 112

It looks the same. So I then used this list as the argument in factanal and get the error message.

> factanal(factors=2, covmat=tstcov, rotation="varimax")
Error in sqrt(diag(cv)) : Non-numeric argument to mathematical function

I know that what you see of a list is not necessarily all that is there. So, I figure I am missing some part of the object that makes this list suitable for use by factanal.

So, I hope this is enough detail. Any thoughts would be appreciated.
-- 
Dr Alistair Campbell, PhD
Senior Lecturer in Clinical Psychology
School of Psychology
James Cook University
Townsville Queensland Australia

Ph: +61 7 47816879


From cberry at tajo.ucsd.edu  Wed Feb  7 02:11:55 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 6 Feb 2007 17:11:55 -0800
Subject: [R] abbreviate dataframe for Sweave output
In-Reply-To: <905a1292a615999b45179fff33da716c@lanl.gov>
References: <905a1292a615999b45179fff33da716c@lanl.gov>
Message-ID: <Pine.LNX.4.64.0702061707430.6043@tajo.ucsd.edu>


Chris,

Not precisely what you want (does both rows and columns), but can be 
specialized to your case easily.

Use with xtable in a chunk with 'results=tex' as in

 	xtable( dot.matrix( diag(10) ) )

Chuck

dot.matrix <- function(x , rws=NULL, cls=NULL, in.math=FALSE )
{


   ## Purpose: prepare a matrix like object for xtable printing with
   ## embedded cdots and vdots to stand in for omitted rows and columns
   ## 
----------------------------------------------------------------------
   ## Arguments: x - a data.frame or matrix,
   ##            rws - rows to include , cls - cols to include ,
   ##            in.math - in math mode? dont use $
   ## 
----------------------------------------------------------------------
   ## Author: CCB, Date: 13 Jan 2007, 12:54

   if (length(rws)==1) {def.rows <- rws;rws <- NULL} else def.rows <- 2
   if (length(cls)==1) {def.cols <- cls;cls <- NULL} else def.cols <- 2

   if (in.math) {
     cdot <- "\\cdots"
     vdot <- "\\vdots"
     ddot <- "\\ddots"
   } else {
     cdot <- "$\\cdots$"
     vdot <- "$\\vdots$"
     ddot <- "$\\ddots$"
   }

   x <- as.data.frame(x)

   if (is.null(rws)) {
     rws <- rownames(x)
     if (is.null(rws)) rws <- seq(nrow(x))
     rws <- list(head=head(rws,def.rows),tail=tail(rws,def.rows))
   }
   if (is.null(cls)) {
     cls <- colnames(x)
     if (is.null(cls)) cls <- seq(ncol(x))
     cls <- list(head=head(cls,def.cols),tail=tail(cls,def.cols))
   }

   safe.dots <- structure(list(factor(cdot,c(cdot,ddot))),names=cdot,row.names=as.integer(1),class="data.frame")

   head.rows <- cbind( x[ rws$head, cls$head ,drop=FALSE ], safe.dots,
                      x[ rws$head, cls$tail ,drop=FALSE ])

   mid.row <- structure(matrix(rep(c(vdot,ddot,vdot),c(length(cls$head),1,length(cls$tail))),nr=1),
                        dimnames=list(vdot,colnames(head.rows)))

   tail.rows <- cbind( x[ rws$tail, cls$head ,drop=FALSE ], safe.dots, x[ rws$tail, cls$tail ,drop=FALSE ] )

   tab <- rbind(head.rows,mid.row,tail.rows)

   tab
}




On Tue, 6 Feb 2007, stubben wrote:

> I wanted to print the first and last rows of some dataframes in Sweave
> using dots in columns to separate the two parts.  Head and tail almost
> work, but I have problems with factors and row names.
>
> z<-data.frame(id=letters[1:26], x=sample(1:26,26))
>
> rbind(head(z,3), ".", tail(z,1))
>
>      id  x
> 1     a 18
> 2     b  8
> 3     c 14
> 4  <NA>  .
> 26    z 10
> Warning message:
> invalid factor level, NAs generated in...
>
>
> I would like something like this if possible.  Any ideas?
>
>      id  x
> 1     a 18
> 2     b  8
> 3     c 14
> .     .  .
> .     .  .
> 26    z 10
>
>
> Thanks,
>
> Chris Stubben
>
>
>
> -- 
> -----------------
>
> Los Alamos National Lab
> BioScience Division
> MS M888
> Los Alamos, NM 87545
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From dale.w.steele at gmail.com  Wed Feb  7 02:34:10 2007
From: dale.w.steele at gmail.com (Dale Steele)
Date: Tue, 6 Feb 2007 20:34:10 -0500
Subject: [R] How to reorder rows in dataframe by text flag
Message-ID: <72e8303a0702061734m2078ce1bh55b6baebdc11612e@mail.gmail.com>

Given two columns of type character in a dataframe of the form:

col1    col2
31*      66
0         0*
102*    66
71*      80
31       2*
66       31*
47       38*

How do I generate the following dataframe?  Ie. col1 contains row item
 with "*" and col2 contains row member without "*"

col1    col2
31       66
0         0
102     66
71       80
2        31
31      66
38      47

Partial ideas thus far....
grep("*",col1,fixed=T)
as.numeric(gsub("*","",col1))

Thanks.  --Dale


From liuwensui at gmail.com  Wed Feb  7 02:53:24 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 6 Feb 2007 20:53:24 -0500
Subject: [R] R in Industry
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
Message-ID: <1115a2b00702061753w541fc16vd25f3c3fa43b78a4@mail.gmail.com>

I've been looking for job that allows me to use R/S+ since I got out
of graduate school 2 years ago but with no success. I am wondering if
there is something that can be done to promote the use of R in
industry.

It's been very frustrating to see people doing statistics using
excel/spss and even more frustrating to see people paying $$$ for
something much inferior to R.


On 2/6/07, Doran, Harold <HDoran at air.org> wrote:
> The other day, CNN had a story on working at Google. Out of curiosity, I
> went to the Google employment web site (I'm not looking, but just
> curious). In perusing their job posts for statisticians, preference is
> given to those who use R and python. Other languages, S-Plus and
> something called SAS were listed as lower priorities.
>
> When I started using Python, I noted they have a portion of the web site
> with job postings. CRAN does not have something similar, but think it
> might be useful. I think R is becoming more widely used in industry and
> I wonder if helping it move along a bit, the maintainer of CRAN could
> create a section of the web site devoted to jobs where R is a
> requirement.
>
> Hence, we could have our own little "monster.com" kind of thing going
> on. Of the multitude of ways the gospel can be spread, this is small.
> But, I think every small step forward is good.
>
> Anyone think this is useful?
>
> Harold
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From lists at revelle.net  Wed Feb  7 02:59:22 2007
From: lists at revelle.net (William Revelle)
Date: Tue, 6 Feb 2007 19:59:22 -0600
Subject: [R] How-To construct a cov list to use a covariance matrix in
 factanal?
In-Reply-To: <20070207110033.BHR85871@mirapoint-ms1.jcu.edu.au>
References: <20070207110033.BHR85871@mirapoint-ms1.jcu.edu.au>
Message-ID: <p06240811c1eee24a1bcd@[165.124.160.87]>

Alistair,


>
>I have worked through the examples. They work because the covmat 
>were produced by the cov.wt which provides output as a list object. 
>I am trying to construct my own list object to use as the covmat. 
>There are no obvious instructions on how to do this.
>
>So, here is what I have done so far.
>
>I reconstructed the covariance matrix in the example and created a dataframe:
>
>  > testmatrix
>   general picture  blocks   maze reading   vocab
>1  24.641   5.991  33.520  6.023  20.755  29.701
>2   5.991   6.700  18.137  1.782   4.936   7.204
>3  33.520  18.137 149.831 19.424  31.430  50.753
>4   6.023   1.782  19.424 12.711   4.757   9.075
>5  20.755   4.936  31.430  4.757  52.604  66.762
>6  29.701   7.204  50.753  9.075  66.762 135.292
>
>and then used this to construct a list object like the output from 
>the example;
>
>>  tstcov<- list(cov=testmatrix, center=c(0,0,0,0,0), n.obs=112)
>
>I tested to see whether my list object looked like the examples
>
>>  tstcov
>$cov
>   general picture  blocks   maze reading   vocab
>1  24.641   5.991  33.520  6.023  20.755  29.701
>2   5.991   6.700  18.137  1.782   4.936   7.204
>3  33.520  18.137 149.831 19.424  31.430  50.753
>4   6.023   1.782  19.424 12.711   4.757   9.075
>5  20.755   4.936  31.430  4.757  52.604  66.762
>6  29.701   7.204  50.753  9.075  66.762 135.292
>
>$centers
>[1] 0 0 0 0 0
>
>$n.obs
>[1] 112
>
>It looks the same. So I then used this list as the argument in 
>factanal and get the error message.
>
>>  factanal(factors=2, covmat=tstcov, rotation="varimax")
>Error in sqrt(diag(cv)) : Non-numeric argument to mathematical function
>
>I know that what you see of a list is not necessarily all that is 
>there. So, I figure I am missing some part of the object that makes 
>this list suitable for use by factanal.
>
So, I hope this is enough detail. Any thoughts would be appreciated.
>--
>Dr Alistair Campbell, PhD
>Senior Lecturer in Clinical Psychology
>School of Psychology
>James Cook University
Townsville Queensland Australia



The call to factanal requires you to specify that you have a 
covariance matrix (covmat).  Using your matrix and specifying two 
factors:


test.df
   general picture  blocks   maze reading   vocab
1  24.641   5.991  33.520  6.023  20.755  29.701
2   5.991   6.700  18.137  1.782   4.936   7.204
3  33.520  18.137 149.831 19.424  31.430  50.753
4   6.023   1.782  19.424 12.711   4.757   9.075
5  20.755   4.936  31.430  4.757  52.604  66.762
6  29.701   7.204  50.753  9.075  66.762 135.292
>  test.mat <- as.matrix(test.df)
>  tf <- factanal(factors=2,covmat=test.mat)
>  tf

Call:
factanal(factors = 2, covmat = test.mat)

Uniquenesses:
general picture  blocks    maze reading   vocab
   0.455   0.589   0.218   0.769   0.052   0.334

Loadings:
   Factor1 Factor2
1 0.499   0.543 
2 0.156   0.622 
3 0.206   0.860 
4 0.109   0.468 
5 0.956   0.182 
6 0.785   0.225 

                Factor1 Factor2
SS loadings      1.858   1.724
Proportion Var   0.310   0.287
Cumulative Var   0.310   0.597

The degrees of freedom for the model is 4 and the fit was 0.0572

Bill

>

-- 
William Revelle		http://pmc.psych.northwestern.edu/revelle.html   
Professor			http://personality-project.org/personality.html
Department of Psychology       http://www.wcas.northwestern.edu/psych/
Northwestern University	http://www.northwestern.edu/


From rlevy at ucsd.edu  Wed Feb  7 03:52:31 2007
From: rlevy at ucsd.edu (Roger Levy)
Date: Tue, 06 Feb 2007 18:52:31 -0800
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <17861.25637.324856.683429@lapo.berkeley.edu>
References: <45C38E90.9070709@ucsd.edu>	<17860.61176.520222.602660@lapo.berkeley.edu>	<17861.4763.359422.817825@macht.arts.cornell.edu>	<45C52F17.1030601@ucsd.edu>	<17861.13360.697951.302800@macht.arts.cornell.edu>
	<17861.25637.324856.683429@lapo.berkeley.edu>
Message-ID: <45C93EEF.9080509@ucsd.edu>

Hi again Jasjeet, Walter,

I have a further question about an error message I get when running 
multinomRob.  I am simulating a dataset where I look at the effect of 
making a previous categorical choice on the probability of making the 
same choice later on.  Given the following code:


n <- 20
choice <- c("A","B","C")
intercepts <- c(0.5,0.3,0.2)
prime.strength <- rep(0.4,length(intercepts))
counts <- c()
for(i in 1:length(choice)) {
   u <- intercepts[1:length(choice)]
   u[i] <- u[i] + prime.strength[i]
   counts <- c(counts,rmultinomial(n = n, pr = multinomLogis(u)))
}
dim(counts) <- c(length(choice),length(choice))
counts <- t(counts)
row.names(counts) <- choice
colnames(counts) <- choice
data <- data.frame(Prev.Choice=choice,counts)

for(i in 1:length(choice)) {
   data[[paste("last",choice[i],sep=".")]] <- 
ifelse(data$Prev.Choice==choice[i],1,0)
}

multinomRob(list(A ~ last.A ,
                  B ~ last.B ,
                  C ~ last.C - 1 ,
                  ),
             data=data,
             print.level=1)


I get the following error:


multinomRob(): Grouped MNL Estimation
Error in if (logliklambda < loglik) bvec <- blambda :
	missing value where TRUE/FALSE needed
In addition: Warning message:
NaNs produced in: sqrt(sigma2GN)


It's not clear to me what this error means, or why the resulting dataset 
would be problematic for MLE estimation.

Many thanks once again!

Roger


From ggrothendieck at gmail.com  Wed Feb  7 03:55:46 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Feb 2007 21:55:46 -0500
Subject: [R] How to reorder rows in dataframe by text flag
In-Reply-To: <72e8303a0702061734m2078ce1bh55b6baebdc11612e@mail.gmail.com>
References: <72e8303a0702061734m2078ce1bh55b6baebdc11612e@mail.gmail.com>
Message-ID: <971536df0702061855r312dc21au2637c3f6cb2bbbd3@mail.gmail.com>

Try this:

DF <- data.frame(
        col1 = factor(c("31*", "0", "102*", "71*", "31", "66", "47")),
        col2 = factor(c("66", "0*", "66", "80", "2*", "31*", "38"))
)

replace(DF, TRUE, as.numeric(sub("*", "", as.matrix(DF), fixed = TRUE)))


On 2/6/07, Dale Steele <dale.w.steele at gmail.com> wrote:
> Given two columns of type character in a dataframe of the form:
>
> col1    col2
> 31*      66
> 0         0*
> 102*    66
> 71*      80
> 31       2*
> 66       31*
> 47       38*
>
> How do I generate the following dataframe?  Ie. col1 contains row item
>  with "*" and col2 contains row member without "*"
>
> col1    col2
> 31       66
> 0         0
> 102     66
> 71       80
> 2        31
> 31      66
> 38      47
>
> Partial ideas thus far....
> grep("*",col1,fixed=T)
> as.numeric(gsub("*","",col1))
>
> Thanks.  --Dale
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mckellercran at gmail.com  Wed Feb  7 04:03:02 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Tue, 6 Feb 2007 22:03:02 -0500
Subject: [R] R in Industry
In-Reply-To: <1115a2b00702061753w541fc16vd25f3c3fa43b78a4@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
	<1115a2b00702061753w541fc16vd25f3c3fa43b78a4@mail.gmail.com>
Message-ID: <3f547caa0702061903k23b041d3j1110ffd37715aa4b@mail.gmail.com>

Bob,

Far from flaming you, I think you made a good point - one that I
imagine most people who use R have come across. The name "R" is a big
impediment to effective online searches. As a check, I entered "R
software", "SAS software", SPSS software", and "S+ software" into
google. The R 'hit rate' was only ten out of the first 20 results (I
didn't look any further). For the other three software packages, the
hit rates were all 100% (20/20).

I do wonder if anything can/should be done about this. I generally
search using the term "CRAN" but of course, that omits lots of stuff
relevant to R. Any ideas about how to do effective online searches for
"R" related materials?

Matt


On 2/6/07, Wensui Liu <liuwensui at gmail.com> wrote:
> I've been looking for job that allows me to use R/S+ since I got out
> of graduate school 2 years ago but with no success. I am wondering if
> there is something that can be done to promote the use of R in
> industry.
>
> It's been very frustrating to see people doing statistics using
> excel/spss and even more frustrating to see people paying $$$ for
> something much inferior to R.
>
>
> On 2/6/07, Doran, Harold <HDoran at air.org> wrote:
> > The other day, CNN had a story on working at Google. Out of curiosity, I
> > went to the Google employment web site (I'm not looking, but just
> > curious). In perusing their job posts for statisticians, preference is
> > given to those who use R and python. Other languages, S-Plus and
> > something called SAS were listed as lower priorities.
> >
> > When I started using Python, I noted they have a portion of the web site
> > with job postings. CRAN does not have something similar, but think it
> > might be useful. I think R is becoming more widely used in industry and
> > I wonder if helping it move along a bit, the maintainer of CRAN could
> > create a section of the web site devoted to jobs where R is a
> > requirement.
> >
> > Hence, we could have our own little "monster.com" kind of thing going
> > on. Of the multitude of ways the gospel can be spread, this is small.
> > But, I think every small step forward is good.
> >
> > Anyone think this is useful?
> >
> > Harold
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> WenSui Liu
> A lousy statistician who happens to know a little programming
> (http://spaces.msn.com/statcompute/blog)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics


From wrm1 at macht.arts.cornell.edu  Wed Feb  7 04:38:50 2007
From: wrm1 at macht.arts.cornell.edu (Walter Mebane)
Date: Tue, 6 Feb 2007 22:38:50 -0500
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <45C93EEF.9080509@ucsd.edu>
References: <45C38E90.9070709@ucsd.edu>
	<17860.61176.520222.602660@lapo.berkeley.edu>
	<17861.4763.359422.817825@macht.arts.cornell.edu>
	<45C52F17.1030601@ucsd.edu>
	<17861.13360.697951.302800@macht.arts.cornell.edu>
	<17861.25637.324856.683429@lapo.berkeley.edu>
	<45C93EEF.9080509@ucsd.edu>
Message-ID: <17865.18890.134246.794371@macht.arts.cornell.edu>

Roger,

 > Error in if (logliklambda < loglik) bvec <- blambda :
 > 	missing value where TRUE/FALSE needed
 > In addition: Warning message:
 > NaNs produced in: sqrt(sigma2GN)

That message comes from the Newton algorithm (defined in source file
multinomMLE.R).  It would be better if we bullet-proofed it a bit
more.  The first thing is to check the data.  I don't have the
multinomLogis() function, so I can't run your code.  But do you really
mean

 > for(i in 1:length(choice)) {
and
 > dim(counts) <- c(length(choice),length(choice))

Should that be

  for(i in 1:n) {
and
  dim(counts) <- c(n, length(choice))

or instead of n, some number m > length(choice).  As it is it seems to
me you have three observations for three categories, which isn't going
to work (there are five coefficient parameters, plus sigma for the
dispersion).

Otherwise, you can get more diagnostic messages by setting a
print.level greater than 32.  To have a bad value of sigma2GN is
unusual.

I should mention that supplying starting values won't help you,
because for multinomRob's primary purpose the ML estimates are
themselves no more than starting values.  So if you supply starting
values it will skip the ML step.

Walter

Roger Levy writes:
 > Hi again Jasjeet, Walter,
 > 
 > I have a further question about an error message I get when running 
 > multinomRob.  I am simulating a dataset where I look at the effect of 
 > making a previous categorical choice on the probability of making the 
 > same choice later on.  Given the following code:
 > 
 > 
 > n <- 20
 > choice <- c("A","B","C")
 > intercepts <- c(0.5,0.3,0.2)
 > prime.strength <- rep(0.4,length(intercepts))
 > counts <- c()
 > for(i in 1:length(choice)) {
 >    u <- intercepts[1:length(choice)]
 >    u[i] <- u[i] + prime.strength[i]
 >    counts <- c(counts,rmultinomial(n = n, pr = multinomLogis(u)))
 > }
 > dim(counts) <- c(length(choice),length(choice))
 > counts <- t(counts)
 > row.names(counts) <- choice
 > colnames(counts) <- choice
 > data <- data.frame(Prev.Choice=choice,counts)
 > 
 > for(i in 1:length(choice)) {
 >    data[[paste("last",choice[i],sep=".")]] <- 
 > ifelse(data$Prev.Choice==choice[i],1,0)
 > }
 > 
 > multinomRob(list(A ~ last.A ,
 >                   B ~ last.B ,
 >                   C ~ last.C - 1 ,
 >                   ),
 >              data=data,
 >              print.level=1)
 > 
 > 
 > I get the following error:
 > 
 > 
 > multinomRob(): Grouped MNL Estimation
 > Error in if (logliklambda < loglik) bvec <- blambda :
 > 	missing value where TRUE/FALSE needed
 > In addition: Warning message:
 > NaNs produced in: sqrt(sigma2GN)
 > 
 > 
 > It's not clear to me what this error means, or why the resulting dataset 
 > would be problematic for MLE estimation.
 > 
 > Many thanks once again!
 > 
 > Roger


From r.arecibo at gmail.com  Wed Feb  7 07:31:11 2007
From: r.arecibo at gmail.com (Dong H. Oh)
Date: Wed, 7 Feb 2007 15:31:11 +0900
Subject: [R] Histogram of ones.
Message-ID: <c5dd223c0702062231y2a806519oea68dcfe819e5d81@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/fa34d88f/attachment.pl 

From vikasrawal at gmail.com  Wed Feb  7 08:06:40 2007
From: vikasrawal at gmail.com (Vikas Rawal)
Date: Wed, 7 Feb 2007 12:36:40 +0530
Subject: [R] boxplot statistics in ggplot
Message-ID: <20070207070640.GA30339@shireen.jnu.ac.in>

I need to make weighted boxplots. I found that ggplot makes them. I
would however like to label them with the boxplot statistics (the
median, q1 and q3). In the boxplot function in r-base, I could output
boxplot statistics and then write a text on the plot to place the
labels. How would one do it with ggplot?

Vikas


From ibrahimmutlay at gmail.com  Wed Feb  7 08:44:24 2007
From: ibrahimmutlay at gmail.com (=?ISO-8859-9?Q?=DDbrahim_Mutlay?=)
Date: Wed, 7 Feb 2007 02:44:24 -0500
Subject: [R] R Search
Message-ID: <eb21cbcd0702062344o67a47c91x5c2d46b6c1a8ce76@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/6936a606/attachment.pl 

From jim at bitwrit.com.au  Wed Feb  7 09:07:56 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 07 Feb 2007 19:07:56 +1100
Subject: [R] R in Industry
Message-ID: <45C988DC.20006@bitwrit.com.au>

Matthew Keller wrote:
 > Far from flaming you, I think you made a good point - one that I
 > imagine most people who use R have come across. The name "R" is a big
 > impediment to effective online searches. As a check, I entered "R
 > software", "SAS software", SPSS software", and "S+ software" into
 > google. The R 'hit rate' was only ten out of the first 20 results (I
 > didn't look any further). For the other three software packages, the
 > hit rates were all 100% (20/20).
 >
 > I do wonder if anything can/should be done about this. I generally
 > search using the term "CRAN" but of course, that omits lots of stuff
 > relevant to R. Any ideas about how to do effective online searches for
 > "R" related materials?
 >
Try "r stats". I get 18/20 on Google with that.

Jim


From ripley at stats.ox.ac.uk  Wed Feb  7 09:07:59 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Feb 2007 08:07:59 +0000 (GMT)
Subject: [R] How-To construct a cov list to use a covariance matrix in
 factanal?
In-Reply-To: <20070207110033.BHR85871@mirapoint-ms1.jcu.edu.au>
References: <20070207110033.BHR85871@mirapoint-ms1.jcu.edu.au>
Message-ID: <Pine.LNX.4.64.0702070805250.10403@gannet.stats.ox.ac.uk>

We still do not have reproducible code, but a 'dataframe' is not a matrix.
And I would expect a covariance matrix to have the same row and column 
names: the examples do.

On Wed, 7 Feb 2007, Alistair Campbell wrote:

> Thanks for that Brian,
>
> I have worked through the examples. They work because the covmat were 
> produced by the cov.wt which provides output as a list object. I am 
> trying to construct my own list object to use as the covmat. There are 
> no obvious instructions on how to do this.

Well, the instructions to follow cov.wt seem obvious to me, and as I have 
said before, covariance matrices also work.

> So, here is what I have done so far.
>
> I reconstructed the covariance matrix in the example and created a dataframe:
>
> > testmatrix
>  general picture  blocks   maze reading   vocab
> 1  24.641   5.991  33.520  6.023  20.755  29.701
> 2   5.991   6.700  18.137  1.782   4.936   7.204
> 3  33.520  18.137 149.831 19.424  31.430  50.753
> 4   6.023   1.782  19.424 12.711   4.757   9.075
> 5  20.755   4.936  31.430  4.757  52.604  66.762
> 6  29.701   7.204  50.753  9.075  66.762 135.292
>
> and then used this to construct a list object like the output from the example;
>
>> tstcov<- list(cov=testmatrix, center=c(0,0,0,0,0), n.obs=112)
>
> I tested to see whether my list object looked like the examples
>
>> tstcov
> $cov
>  general picture  blocks   maze reading   vocab
> 1  24.641   5.991  33.520  6.023  20.755  29.701
> 2   5.991   6.700  18.137  1.782   4.936   7.204
> 3  33.520  18.137 149.831 19.424  31.430  50.753
> 4   6.023   1.782  19.424 12.711   4.757   9.075
> 5  20.755   4.936  31.430  4.757  52.604  66.762
> 6  29.701   7.204  50.753  9.075  66.762 135.292
>
> $centers
> [1] 0 0 0 0 0
>
> $n.obs
> [1] 112
>
> It looks the same. So I then used this list as the argument in factanal and get the error message.
>
>> factanal(factors=2, covmat=tstcov, rotation="varimax")
> Error in sqrt(diag(cv)) : Non-numeric argument to mathematical function
>
> I know that what you see of a list is not necessarily all that is there. So, I figure I am missing some part of the object that makes this list suitable for use by factanal.
>
> So, I hope this is enough detail. Any thoughts would be appreciated.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tagett at ipsogen.com  Wed Feb  7 09:18:37 2007
From: tagett at ipsogen.com (Rebecca)
Date: Wed, 7 Feb 2007 09:18:37 +0100
Subject: [R] problems installing R on Linux
Message-ID: <200702070818.l178IwIF032402@hypatia.math.ethz.ch>

Hi everyone,

I am having installation problems, but this is how it all started:
I had some errors running the bioconductor package affyPLM that uses
LAPACK/Blas

	> Pset <- fitPLM(Data)
	Background correcting PM
	Normalizing PM
	Fitting models
	/usr/local/lib/R/bin/exec/R: relocation error:
/usr/local/lib/R/lib/libRlapack.so: undefined symbol: s_copy
	# thrown out of R ....

I was using R version 2.4.0, so I decided to upgrade to 2.4.1 (on
i686-pc-linux-gnu) and to try various configuration options : default,
'--with -lapack' and '--with-blas="lacml"', as described in "appendix A of
the Installation and Admin" manual.

Everytime I configure and make, the message streams seem clean. But the
'make check' is always a disaster (see below), and if I make install
regardless of the make check errors, I get the same "relocation error" from
fitPLM as above (of course the appropriate BioC packages were installed
too).

I believe my problems are dealt with in Appendix A of the Installation and
Administration Guide, but I can't seem to resolve them. Indeed, I have never
been able to use png() for graphics because of some unresolved issues
concerning access to the X11 graphics device, which I suspected had to do
with the libpng and zlib programs (also mentioned in Appendix A).

Some of the postings that I have read on this forum seem to imply that
installation problems are sometimes due to old versions of zlib. I don't
understand this since the recent zlib (1.2.3) is in
"/usr/local/lib/R-2.4.1/src/extra/zlib". Appendix A says, referring to zlib,
that "the versions in the R sources will be compiled in".
But just to be sure, I verified that my system's version is old :
 rpm -q zlib
zlib-1.1.3-25.7
I downloaded and tried to install zlib-1.2.3 using tar, configure, make,
make test, and make install. But when I type "rpm -q zlib", I am still
informed that my version is "zlib-1.1.3-25.7". So, still hopeful, I thought
that I instead would use rpm to install zlib; I found an intermediate rpm
zlib version 1.2.1.2-1.2 and tried "rpm -Uhv zlib-1.2.1.2-1.2.src.rpm". No
diagnostics appear, but when I type "rpm -q zlib", my version is still
"zlib-1.1.3-25.7". I tried to uninstall the old zlib using "rpm -e zlib",
but rpm refuses, saying that there are too many dependencies...

Does anyone have any suggestions?
Thanks!



The 'make check' disaster : 
	make[1]: Entering directory `/usr/local/lib/R-2.4.1/tests'
	make[2]: Entering directory `/usr/local/lib/R-2.4.1/tests'
	make[3]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
	make[4]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
	make[4]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
	make[4]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
	collecting examples for package 'base' ...
	make[5]: Entering directory `/usr/local/lib/R-2.4.1/src/library'
	 >>> Building/Updating help pages for package 'base'
	     Formats: text html latex example
	make[5]: Leaving directory `/usr/local/lib/R-2.4.1/src/library'
	running code in 'base-Ex.R' ...make[4]: *** [base-Ex.Rout] Error 1
	make[4]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
	make[3]: *** [test-Examples-Base] Error 2
	make[3]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
	make[2]: *** [test-Examples] Error 2
	make[2]: Leaving directory `/usr/local/lib/R-2.4.1/tests'
	make[1]: *** [test-all-basics] Error 1
	make[1]: Leaving directory `/usr/local/lib/R-2.4.1/tests'
	make: *** [check] Error 2


I don't really *know* if the configure results are "clean". Here is a subset
of the configure results (that may be suspect) :

	checking build system type... i686-pc-linux-gnu	
	checking host system type... i686-pc-linux-gnu 
	. . . 
	checking for cblas_cdotu_sub in vecLib framework... no
	checking iconv.h usability... yes
	checking iconv.h presence... yes
	checking for iconv.h... yes
	checking for iconv... yes
	checking whether iconv() accepts "UTF-8", "latin1" and "UCS-*"...
yes
	checking for iconvlist... no
	. . .
	checking for g77... g77
	checking whether we are using the GNU Fortran 77 compiler... yes
	checking whether g77 accepts -g... yes
	checking for g++... g++
	checking whether we are using the GNU C++ compiler... yes
	checking whether g++ accepts -g... yes
	checking how to run the C++ preprocessor... g++ -E
	checking whether __attribute__((visibility())) is supported... no
	checking whether gcc accepts -fvisibility... no
	checking whether g77 accepts -fvisibility... no
	. . .
	checking if libtool supports shared libraries... yes
	checking whether to build shared libraries... yes
	checking whether to build static libraries... no
	. . .
	checking floatingpoint.h usability... no
	checking floatingpoint.h presence... no
	checking for floatingpoint.h... no
	. . .
	checking ieee754.h usability... yes
	checking ieee754.h presence... yes
	checking for ieee754.h... yes
	checking ieeefp.h usability... no
	checking ieeefp.h presence... no
	checking for ieeefp.h... no
	. . .
	checking for dummy main to link with Fortran libraries... none
	. . .
	checking whether isfinite is declared... no
	. . .
	checking for mkdtemp... no
	. . .
	checking whether mkdtemp is declared... no
	. . .
	checking for special C compiler options needed for large files... no
	checking for _FILE_OFFSET_BITS value needed for large files... 64
	checking for _LARGE_FILES value needed for large files... no
	checking for _LARGEFILE_SOURCE value needed for large files... no
	checking for fseeko... (cached) yes
	checking whether KERN_USRSTACK sysctl is supported... no
	checking for visible __lib_stack_end... yes
	. . .
	
	checking for g77... g77
	checking whether we are using the GNU Fortran compiler... yes
	checking whether g77 accepts -g... yes
	. . .
	checking for bison... no
	checking for CFPreferencesCopyAppValue... (cached) no
	checking for CFLocaleCopyCurrent... (cached) no
	checking whether NLS is requested... yes
	checking whether included gettext is requested... no
	checking for GNU gettext in libc... no
	checking for GNU gettext in libintl... no
	. . .
	R is now configured for i686-pc-linux-gnu
  	Source directory:          .
  	Installation directory:    /usr/local
  	C compiler:                gcc  -g -O2
  	Fortran 77 compiler:       g77  -g -O2
  	C++ compiler:              g++  -g -O2
  	Fortran 90/95 compiler:    g77 -g -O2
  	Interfaces supported:      X11, tcltk
  	External libraries:        readline
  	Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
  	Options enabled:           shared BLAS, R profiling
  	Recommended packages:      yes


From shubhak at ambaresearch.com  Wed Feb  7 09:42:39 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 7 Feb 2007 14:12:39 +0530
Subject: [R] Any Limitations for the dataframes?
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3F189D7@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/5078a02c/attachment.pl 

From simon.kempf at web.de  Wed Feb  7 09:45:02 2007
From: simon.kempf at web.de (Simon P. Kempf)
Date: Wed, 7 Feb 2007 09:45:02 +0100
Subject: [R] Convert Class "numeric" to class "lm
Message-ID: <E1HEiQV-0001NH-00@smtp06.web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/82dbe111/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Feb  7 10:07:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Feb 2007 09:07:24 +0000 (GMT)
Subject: [R] problems installing R on Linux
In-Reply-To: <200702070818.l178IwIF032402@hypatia.math.ethz.ch>
References: <200702070818.l178IwIF032402@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0702070901230.2418@auk.stats>

We don't know what 'Linux' is here.  What Linux distribution, what are 
your C and Fortran compilers (in detail, e.g. from gcc --version and g77 
--version)?

We need to see the tail of tests/Examples/base-Ex.Rout.fail to know what 
went wrong.

If you can supply those pieces of information we can begin to help.
(But zlib is a red herring: at most it affects the png() device.)


On Wed, 7 Feb 2007, Rebecca wrote:

> Hi everyone,
>
> I am having installation problems, but this is how it all started:
> I had some errors running the bioconductor package affyPLM that uses
> LAPACK/Blas
>
> 	> Pset <- fitPLM(Data)
> 	Background correcting PM
> 	Normalizing PM
> 	Fitting models
> 	/usr/local/lib/R/bin/exec/R: relocation error:
> /usr/local/lib/R/lib/libRlapack.so: undefined symbol: s_copy
> 	# thrown out of R ....
>
> I was using R version 2.4.0, so I decided to upgrade to 2.4.1 (on
> i686-pc-linux-gnu) and to try various configuration options : default,
> '--with -lapack' and '--with-blas="lacml"', as described in "appendix A of
> the Installation and Admin" manual.
>
> Everytime I configure and make, the message streams seem clean. But the
> 'make check' is always a disaster (see below), and if I make install
> regardless of the make check errors, I get the same "relocation error" from
> fitPLM as above (of course the appropriate BioC packages were installed
> too).
>
> I believe my problems are dealt with in Appendix A of the Installation and
> Administration Guide, but I can't seem to resolve them. Indeed, I have never
> been able to use png() for graphics because of some unresolved issues
> concerning access to the X11 graphics device, which I suspected had to do
> with the libpng and zlib programs (also mentioned in Appendix A).
>
> Some of the postings that I have read on this forum seem to imply that
> installation problems are sometimes due to old versions of zlib. I don't
> understand this since the recent zlib (1.2.3) is in
> "/usr/local/lib/R-2.4.1/src/extra/zlib". Appendix A says, referring to zlib,
> that "the versions in the R sources will be compiled in".
> But just to be sure, I verified that my system's version is old :
> rpm -q zlib
> zlib-1.1.3-25.7
> I downloaded and tried to install zlib-1.2.3 using tar, configure, make,
> make test, and make install. But when I type "rpm -q zlib", I am still
> informed that my version is "zlib-1.1.3-25.7". So, still hopeful, I thought
> that I instead would use rpm to install zlib; I found an intermediate rpm
> zlib version 1.2.1.2-1.2 and tried "rpm -Uhv zlib-1.2.1.2-1.2.src.rpm". No
> diagnostics appear, but when I type "rpm -q zlib", my version is still
> "zlib-1.1.3-25.7". I tried to uninstall the old zlib using "rpm -e zlib",
> but rpm refuses, saying that there are too many dependencies...
>
> Does anyone have any suggestions?
> Thanks!
>
>
>
> The 'make check' disaster :
> 	make[1]: Entering directory `/usr/local/lib/R-2.4.1/tests'
> 	make[2]: Entering directory `/usr/local/lib/R-2.4.1/tests'
> 	make[3]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
> 	make[4]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
> 	make[4]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
> 	make[4]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
> 	collecting examples for package 'base' ...
> 	make[5]: Entering directory `/usr/local/lib/R-2.4.1/src/library'
> 	 >>> Building/Updating help pages for package 'base'
> 	     Formats: text html latex example
> 	make[5]: Leaving directory `/usr/local/lib/R-2.4.1/src/library'
> 	running code in 'base-Ex.R' ...make[4]: *** [base-Ex.Rout] Error 1
> 	make[4]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
> 	make[3]: *** [test-Examples-Base] Error 2
> 	make[3]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
> 	make[2]: *** [test-Examples] Error 2
> 	make[2]: Leaving directory `/usr/local/lib/R-2.4.1/tests'
> 	make[1]: *** [test-all-basics] Error 1
> 	make[1]: Leaving directory `/usr/local/lib/R-2.4.1/tests'
> 	make: *** [check] Error 2
>
>
> I don't really *know* if the configure results are "clean". Here is a subset
> of the configure results (that may be suspect) :
>
> 	checking build system type... i686-pc-linux-gnu
> 	checking host system type... i686-pc-linux-gnu
> 	. . .
> 	checking for cblas_cdotu_sub in vecLib framework... no
> 	checking iconv.h usability... yes
> 	checking iconv.h presence... yes
> 	checking for iconv.h... yes
> 	checking for iconv... yes
> 	checking whether iconv() accepts "UTF-8", "latin1" and "UCS-*"...
> yes
> 	checking for iconvlist... no
> 	. . .
> 	checking for g77... g77
> 	checking whether we are using the GNU Fortran 77 compiler... yes
> 	checking whether g77 accepts -g... yes
> 	checking for g++... g++
> 	checking whether we are using the GNU C++ compiler... yes
> 	checking whether g++ accepts -g... yes
> 	checking how to run the C++ preprocessor... g++ -E
> 	checking whether __attribute__((visibility())) is supported... no
> 	checking whether gcc accepts -fvisibility... no
> 	checking whether g77 accepts -fvisibility... no
> 	. . .
> 	checking if libtool supports shared libraries... yes
> 	checking whether to build shared libraries... yes
> 	checking whether to build static libraries... no
> 	. . .
> 	checking floatingpoint.h usability... no
> 	checking floatingpoint.h presence... no
> 	checking for floatingpoint.h... no
> 	. . .
> 	checking ieee754.h usability... yes
> 	checking ieee754.h presence... yes
> 	checking for ieee754.h... yes
> 	checking ieeefp.h usability... no
> 	checking ieeefp.h presence... no
> 	checking for ieeefp.h... no
> 	. . .
> 	checking for dummy main to link with Fortran libraries... none
> 	. . .
> 	checking whether isfinite is declared... no
> 	. . .
> 	checking for mkdtemp... no
> 	. . .
> 	checking whether mkdtemp is declared... no
> 	. . .
> 	checking for special C compiler options needed for large files... no
> 	checking for _FILE_OFFSET_BITS value needed for large files... 64
> 	checking for _LARGE_FILES value needed for large files... no
> 	checking for _LARGEFILE_SOURCE value needed for large files... no
> 	checking for fseeko... (cached) yes
> 	checking whether KERN_USRSTACK sysctl is supported... no
> 	checking for visible __lib_stack_end... yes
> 	. . .
>
> 	checking for g77... g77
> 	checking whether we are using the GNU Fortran compiler... yes
> 	checking whether g77 accepts -g... yes
> 	. . .
> 	checking for bison... no
> 	checking for CFPreferencesCopyAppValue... (cached) no
> 	checking for CFLocaleCopyCurrent... (cached) no
> 	checking whether NLS is requested... yes
> 	checking whether included gettext is requested... no
> 	checking for GNU gettext in libc... no
> 	checking for GNU gettext in libintl... no
> 	. . .
> 	R is now configured for i686-pc-linux-gnu
>  	Source directory:          .
>  	Installation directory:    /usr/local
>  	C compiler:                gcc  -g -O2
>  	Fortran 77 compiler:       g77  -g -O2
>  	C++ compiler:              g++  -g -O2
>  	Fortran 90/95 compiler:    g77 -g -O2
>  	Interfaces supported:      X11, tcltk
>  	External libraries:        readline
>  	Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
>  	Options enabled:           shared BLAS, R profiling
>  	Recommended packages:      yes
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From huxiaopengstat at gmail.com  Wed Feb  7 10:30:58 2007
From: huxiaopengstat at gmail.com (xiaopeng hu)
Date: Wed, 7 Feb 2007 17:30:58 +0800
Subject: [R] about compile the R 2.4.1 sources
Message-ID: <ffe0539f0702070130h5d42669fg4fd69902f8fe9f5b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/39037741/attachment.pl 

From rfrancois at mango-solutions.com  Wed Feb  7 10:40:16 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Wed, 07 Feb 2007 09:40:16 +0000
Subject: [R] R in Industry
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3076E6F3D@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D3076E6F3D@groamrexm03.amer.pfizer.com>
Message-ID: <45C99E80.6050009@mango-solutions.com>

Kuhn, Max wrote:
> As someone who has (reluctantly) sent job postings to R Help, I think
> that a SIG would be a good idea.
>
> Max 
>   
Hi all,

My personnal experience also shows that it is difficult to find a job 
where R is a key component, find R related material, or find companies 
that would do commercial support for R or R consulting. (BTW, we [1] do).

A R-sig-job list has been proposed on the past, and I still think it is 
a good idea. An other point is that if it is too hard finding material 
using the regular tools (google, ...), let's just not use them, last 
week a new R-focused search engine [2] was created, let's just make sure 
it searches on the right places. Alternatively, the R wiki [3] can be 
(and is already) use to advertise for jobs [4].

[1] http://www.mango-solutions.com
[2] http://www.rseek.org
[3] http://wiki.r-project.org
[4] http://wiki.r-project.org?id=links:jobs

Cheers,

Romain

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
> Sent: Tuesday, February 06, 2007 2:08 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] R in Industry
>
> The other day, CNN had a story on working at Google. Out of curiosity, I
> went to the Google employment web site (I'm not looking, but just
> curious). In perusing their job posts for statisticians, preference is
> given to those who use R and python. Other languages, S-Plus and
> something called SAS were listed as lower priorities.
>
> When I started using Python, I noted they have a portion of the web site
> with job postings. CRAN does not have something similar, but think it
> might be useful. I think R is becoming more widely used in industry and
> I wonder if helping it move along a bit, the maintainer of CRAN could
> create a section of the web site devoted to jobs where R is a
> requirement.
>
> Hence, we could have our own little "monster.com" kind of thing going
> on. Of the multitude of ways the gospel can be spread, this is small.
> But, I think every small step forward is good.
>
> Anyone think this is useful? 
>
> Harold
>   

-- 
Mango Solutions
Tel  +44 1249 467 467
Fax  +44 1249 467 468
Mob  +44 7813 526 123
data analysis that delivers

R Site Search extension for firefox
http://addictedtor.free.fr/rsitesearch


From Roger.Bivand at nhh.no  Wed Feb  7 10:45:07 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 7 Feb 2007 10:45:07 +0100 (CET)
Subject: [R] about compile the R 2.4.1 sources
In-Reply-To: <ffe0539f0702070130h5d42669fg4fd69902f8fe9f5b@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0702071040160.14157-100000@reclus.nhh.no>

On Wed, 7 Feb 2007, xiaopeng hu wrote:

> When I run ./configure ,I got the message:
> configure: WARNING: you cannot build info or html versions of the R manuals.
> 
> What's the matter?

Search for "info" in the R Installation and Administration manual:

http://cran.r-project.org/doc/manuals/R-admin.html

"You will not be able to build the info files unless you have makeinfo 
version 4.7 or later installed."

Reading the manual usually helps - saying what platform you are trying to 
build on also helps, here the platform will guide you to the appropriate 
sections of the manual. I expect that you will usually want access to 
these formats of the help pages.



> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From wl2776 at gmail.com  Wed Feb  7 10:52:54 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Wed, 7 Feb 2007 01:52:54 -0800 (PST)
Subject: [R] Convert Class "numeric" to class "lm
In-Reply-To: <E1HEiQV-0001NH-00@smtp06.web.de>
References: <E1HEiQV-0001NH-00@smtp06.web.de>
Message-ID: <8842534.post@talk.nabble.com>



Simon P. Kempf wrote:
> 
> Background: 
> I have five multiple imputed datasets. For each datasets I have run a
> regression analysis and combined the regression coefficients according to
> Rubin (1987) rule. 
> 

So, now you have two numeric values: slope and offset. Right?


Simon P. Kempf wrote:
> 
> Now I want to use these combined regression coefficients on a different
> dataset (with the same variable names but different values) and check how
> good they can predict my dependent variable. Normally, it would use the
> predict.lm function which requires an object of class "lm". But my
> combined
> regression coefficients are an object of class "numeric". Therefore, I
> need
> to know how to convert an object of class "numeric" to a class "lm". 
> 

Given numeric values above, I would use multiplication and addition
operations on them and new data.

Try to explore the structure of instances of these classes with str().
You will see that an object of class "lm" is a list with several components
(12 afair). 
One of them is "coefficients".

You can also construct this object manually from scratch and use
predict.lm(), however, you must correctly create all list elements used by
this function. 
Typing predict.lm in R console will show you the body of this function.
Don't forget to assign the attribute "class".

RSiteSearch("construct lm") will take you here: 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32782.html

-- 
View this message in context: http://www.nabble.com/-R--Convert-Class-%22numeric%22-to-class-%22lm-tf3185677.html#a8842534
Sent from the R help mailing list archive at Nabble.com.


From maechler at stat.math.ethz.ch  Wed Feb  7 11:18:14 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 7 Feb 2007 11:18:14 +0100
Subject: [R] R Search
In-Reply-To: <eb21cbcd0702062344o67a47c91x5c2d46b6c1a8ce76@mail.gmail.com>
References: <eb21cbcd0702062344o67a47c91x5c2d46b6c1a8ce76@mail.gmail.com>
Message-ID: <17865.42854.750681.213839@stat.math.ethz.ch>

The official R Search place has been

    http://search.R-project.org/

for quite a while now.
It does mention others including 'rseek' below.
BTW: It's main "fault" for me is that it does not include the
R-devel mailing list archives (hint hint :-)

Martin Maechler, ETH Zurich

>>>>> "IM" == ??brahim Mutlay <ibrahimmutlay at gmail.com>
>>>>>     on Wed, 7 Feb 2007 02:44:24 -0500 writes:

    IM> I know that two of the search engine for R is available:
    IM> http://www.rseek.org/

    IM> http://www.dangoldstein.com/search_r.html


    IM> -- ?brahim Mutlay


From huxiaopengstat at gmail.com  Tue Feb  6 14:36:14 2007
From: huxiaopengstat at gmail.com (xiaopeng hu)
Date: Tue, 6 Feb 2007 21:36:14 +0800
Subject: [R] when i run ./configure,i meet a problem
Message-ID: <ffe0539f0702060536g52f0c80dm6c4af827c4e3a7a8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070206/354b928c/attachment.pl 

From rmailbox at justemail.net  Wed Feb  7 01:02:41 2007
From: rmailbox at justemail.net (Eric)
Date: Tue, 06 Feb 2007 16:02:41 -0800
Subject: [R] R in Industry
In-Reply-To: <009801c74a3f$0604ee50$4d908980@gne.windows.gene.com>
References: <009801c74a3f$0604ee50$4d908980@gne.windows.gene.com>
Message-ID: <45C91721.9020701@justemail.net>


Conversely, unqualified(*) candidates are nearly guaranteed to find 
support scarce here.

More seriously, free job boards, highly targeted like the one proposed 
do seem to get enough traffic to make it worth the effort to post there. 
One example serving the US market for market research is here: 
http://quirks.com/jobmart/search.asp

Heck, it could even become a revenue resource for the R Project 
foundation if members and supporting institutions got little gold stars 
with their postings or some such.

Eric

* Defined as "Unable to follow posting guidelines."

Bert Gunter wrote:
> "... two main drawbacks of R at our firm (as viewed by our IT dept) are lack
> of
> guaranteed support as well as the difficulty in finding candidates.
>
>
> -- Just an aside: "lack of guaranteed support" -- absolutely true in theory,
> absolutely false in practice. I doubt that the voluntary support found on
> r-help and other R lists can be matched by the "guaranteed" support of any
> commercial software product. Not that this makes a difference to the IT
> group's requirements, of course...
>
> Cheers,
> Bert
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Dale_Steele at brown.EDU  Wed Feb  7 01:40:11 2007
From: Dale_Steele at brown.EDU (Dale Steele)
Date: Tue, 06 Feb 2007 19:40:11 -0500
Subject: [R] Data management problem (reorder rows in dataframe by text flag)
Message-ID: <45C91FEB.9050409@brown.EDU>

Given two columns of type character in a dataframe of the form:

col1    col2
31*	66
0	0*
102*	66
71*	80
31	2*
66	31*
47	38*

How do I generate the following dataframe?  Ie. col1 contains row item 
with "*" and col2 contains row member without "*"

col1    col2
31	66
0	0
102	66
71	80
2	31
31	66
38	47

Partial ideas thus far....
grep("*",col1,fixed=T)
as.numeric(gsub("*","",col1))

Thanks.  --Dale


From f.harrell at vanderbilt.edu  Wed Feb  7 04:59:45 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 06 Feb 2007 21:59:45 -0600
Subject: [R] R in Industry
In-Reply-To: <3f547caa0702061903k23b041d3j1110ffd37715aa4b@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>	<1115a2b00702061753w541fc16vd25f3c3fa43b78a4@mail.gmail.com>
	<3f547caa0702061903k23b041d3j1110ffd37715aa4b@mail.gmail.com>
Message-ID: <45C94EB1.9010209@vanderbilt.edu>

Matthew Keller wrote:
> Bob,
> 
> Far from flaming you, I think you made a good point - one that I
> imagine most people who use R have come across. The name "R" is a big
> impediment to effective online searches. As a check, I entered "R
> software", "SAS software", SPSS software", and "S+ software" into
> google. The R 'hit rate' was only ten out of the first 20 results (I
> didn't look any further). For the other three software packages, the
> hit rates were all 100% (20/20).
> 
> I do wonder if anything can/should be done about this. I generally
> search using the term "CRAN" but of course, that omits lots of stuff
> relevant to R. Any ideas about how to do effective online searches for
> "R" related materials?
> 
> Matt

I just googled for "R" and www.r-project.org was the first hit.  Don't 
see a problem at present.

Frank

> 
> 
> On 2/6/07, Wensui Liu <liuwensui at gmail.com> wrote:
>> I've been looking for job that allows me to use R/S+ since I got out
>> of graduate school 2 years ago but with no success. I am wondering if
>> there is something that can be done to promote the use of R in
>> industry.
>>
>> It's been very frustrating to see people doing statistics using
>> excel/spss and even more frustrating to see people paying $$$ for
>> something much inferior to R.
>>
>>
>> On 2/6/07, Doran, Harold <HDoran at air.org> wrote:
>>> The other day, CNN had a story on working at Google. Out of curiosity, I
>>> went to the Google employment web site (I'm not looking, but just
>>> curious). In perusing their job posts for statisticians, preference is
>>> given to those who use R and python. Other languages, S-Plus and
>>> something called SAS were listed as lower priorities.
>>>
>>> When I started using Python, I noted they have a portion of the web site
>>> with job postings. CRAN does not have something similar, but think it
>>> might be useful. I think R is becoming more widely used in industry and
>>> I wonder if helping it move along a bit, the maintainer of CRAN could
>>> create a section of the web site devoted to jobs where R is a
>>> requirement.
>>>
>>> Hence, we could have our own little "monster.com" kind of thing going
>>> on. Of the multitude of ways the gospel can be spread, this is small.
>>> But, I think every small step forward is good.
>>>
>>> Anyone think this is useful?
>>>
>>> Harold
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> --
>> WenSui Liu
>> A lousy statistician who happens to know a little programming
>> (http://spaces.msn.com/statcompute/blog)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From osklyar at ebi.ac.uk  Wed Feb  7 11:56:32 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Wed, 07 Feb 2007 10:56:32 +0000
Subject: [R] problems installing R on Linux
In-Reply-To: <200702070818.l178IwIF032402@hypatia.math.ethz.ch>
References: <200702070818.l178IwIF032402@hypatia.math.ethz.ch>
Message-ID: <45C9B060.5000301@ebi.ac.uk>

Hi,

in general Prof. Ripley is right that more information is needed, but 
here's a hint that you might try first.

> 	/usr/local/lib/R/bin/exec/R: relocation error:
> /usr/local/lib/R/lib/libRlapack.so: undefined symbol: s_copy
> 	# thrown out of R ....

Could simply mean that the /usr/local/lib/R/lib/libRlapack.so is not 
found, and considering that you say make was alright, then maybe it was 
make install that did not copy it. But what you can try, try running R 
from the directory where you compiled it in:

get a fresh R tarball, untar it. configure it with "./configure 
--prefix=`pwd`" and do make. Do not do make install, simply run it from 
the bin dir here. You might need to install the packages though if they 
are not in R_LIBS. This will ensure that all files are there, nothing 
was left behind by make install.

>  rpm -q zlib
> zlib-1.1.3-25.7
After you installed zlib with ./configure && make && make install, your 
rpm request will not give you the just installed zlib version because it 
refers to a different source. If by doing make install you overwrite the 
files from zlib*.rpm, it will still report the version written in the 
rpm database. And you cannot uninstall this rpm without braking half of 
your system dependencies.

Best,
Oleg
--
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


From ibrahimmutlay at gmail.com  Wed Feb  7 11:58:24 2007
From: ibrahimmutlay at gmail.com (=?ISO-8859-9?Q?=DDbrahim_Mutlay?=)
Date: Wed, 7 Feb 2007 05:58:24 -0500
Subject: [R] R Search
In-Reply-To: <17865.42854.750681.213839@stat.math.ethz.ch>
References: <eb21cbcd0702062344o67a47c91x5c2d46b6c1a8ce76@mail.gmail.com>
	<17865.42854.750681.213839@stat.math.ethz.ch>
Message-ID: <eb21cbcd0702070258g7c0d37dep6e4ab7f936126e2e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/48e8586a/attachment.pl 

From christian.bieli at unibas.ch  Wed Feb  7 13:39:46 2007
From: christian.bieli at unibas.ch (Christian Bieli)
Date: Wed, 07 Feb 2007 13:39:46 +0100
Subject: [R] Data management problem (reorder rows in dataframe by text flag)
Message-ID: <45C9C892.1030509@unibas.ch>

how about:

t.d <- data.frame(col1=c("31*","0","102*","71*","31","66","47"),
                  col2=c("66","0*","66","80","2*","31*","38*"),
                  stringsAsFactors = FALSE)
t.x <- apply(t.d,1,function(x) x[order(unlist(x)==grep("\\*$",
                                        unlist(x),value=TRUE))])
t.d2 <- data.frame(col1=t.x[1,],col2=sub("\\*$","",t.x[2,]))

greets
christian

-- 
Christian Bieli, project assistant
Institute of Social and Preventive Medicine
University of Basel, Switzerland
Steinengraben 49
CH-4051 Basel
Tel.: +41 61 270 22 12
Fax:  +41 61 270 22 25
christian.bieli at unibas.ch
www.ispm-unibasel.ch


From p.dalgaard at biostat.ku.dk  Wed Feb  7 13:53:12 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 07 Feb 2007 13:53:12 +0100
Subject: [R] when i run ./configure,i meet a problem
References: <ffe0539f0702060536g52f0c80dm6c4af827c4e3a7a8@mail.gmail.com>
Message-ID: <x2mz3qw2mf.fsf@viggo.kubism.ku.dk>

"xiaopeng hu" <huxiaopengstat at gmail.com> writes:

> i get a message:
> configure: WARNING: you cannot build info or html versions of the R manuals
>
> what should i do ?

In principle:

* Use R-devel not R-help
* Read the Installation and Administration manual (sec.2.2)

However, the gist is that you are missing the "makeinfo" program, so
either install it (usually, it is part of the texinfo package) or live
without manuals in those formats...


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Wed Feb  7 13:54:18 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 07 Feb 2007 13:54:18 +0100
Subject: [R] when i run ./configure,i meet a problem
References: <ffe0539f0702060536g52f0c80dm6c4af827c4e3a7a8@mail.gmail.com>
Message-ID: <x2lkjaw2kl.fsf@viggo.kubism.ku.dk>

"xiaopeng hu" <huxiaopengstat at gmail.com> writes:

> i get a message:
> configure: WARNING: you cannot build info or html versions of the R manuals
>
> what should i do ?

In principle:

* Use R-devel not R-help
* Read the Installation and Administration manual (sec.2.2)

However, the gist is that you are missing the "makeinfo" program, so
either install it (usually, it is part of the texinfo package) or live
without manuals in those formats...


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From h.wickham at gmail.com  Wed Feb  7 14:12:26 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 7 Feb 2007 07:12:26 -0600
Subject: [R] boxplot statistics in ggplot
In-Reply-To: <20070207070640.GA30339@shireen.jnu.ac.in>
References: <20070207070640.GA30339@shireen.jnu.ac.in>
Message-ID: <f8e6ff050702070512m4bdfe5dbi1a77fd97b7681fdb@mail.gmail.com>

Hi Vikas,

Exactly what do you want to label them with?  Generally the purpose of
the plot is to avoid having explicit labels - you can just read the
numbers of the axes.  If you want the exact numbers, presenting them
in a table might be more appropriate.

I'm not at my development computer at the moment, so I can't give you
the exact details, but you will have to calculate the statistics
yourself (using the weighted boxplot function in ggplot) and add them
to the plot in some way.  This should be a bit easier in the next
version of ggplot, where the calculation and display are a little more
distinct.

Hadley

On 2/7/07, Vikas Rawal <vikasrawal at gmail.com> wrote:
> I need to make weighted boxplots. I found that ggplot makes them. I
> would however like to label them with the boxplot statistics (the
> median, q1 and q3). In the boxplot function in r-base, I could output
> boxplot statistics and then write a text on the plot to place the
> labels. How would one do it with ggplot?
>
> Vikas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bernarduse1 at yahoo.fr  Wed Feb  7 14:20:55 2007
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Wed, 7 Feb 2007 14:20:55 +0100 (CET)
Subject: [R] generate Binomial (not Binary) data
Message-ID: <426059.35923.qm@web23404.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/2afbac07/attachment.pl 

From vdemart1 at tin.it  Wed Feb  7 14:23:45 2007
From: vdemart1 at tin.it (Vittorio)
Date: Wed, 7 Feb 2007 14:23:45 +0100 (GMT+01:00)
Subject: [R] Finding not-matching rows in tables
Message-ID: <1109c5fc275.vdemart1@tin.it>

I have these two dataframes in which 'id' is the key field
> tabella
   
id          nome
1   1      PIEMONTE
2   2  VALLED'AOSTA
3   3     
LOMBARDIA
4   4      TRENTINO
5   5        VENETO
6   6        FRIULI

AND

> tab
   id          nome
1   1      PIEMONTE
2   2  VALLED'AOSTA
3   3     LOMBARDIA
4   4      TRENTINO
5  25     CAMPANIA
6  28       
LAZIO

Is there any R-one-command able to select the only rows of tab 
that are not present in tabella, matching the two tables on 'id'?  The 
result should be

5  25     CAMPANIA
6  28       LAZIO

I was able to 
obtain this results by means of the merge command merging the two 
tables and selecting according to the <NA> fields, therefore in two 
steps....

Ciao
Vittorio


From portnoy at supereva.it  Wed Feb  7 14:43:36 2007
From: portnoy at supereva.it (portnoy at supereva.it)
Date: Wed, 07 Feb 2007 14:43:36 +0100
Subject: [R] sorting a matrix  by a different colnames order
Message-ID: <45C9D788.6060601@supereva.it>

Hi R users,
I would like to know how to sort a matrix according a different order of 
colnames (or rownames) ,e.g.,
mx = matrix(rnorm(1:20),5,4)
colnames(mx) = letters[1:4]
rownames(mx) = letters[1:5]
mx
             a          b          c           d
a  0.02362598 -0.7033460  0.8106089 -1.03456219
b -0.45021522 -1.5769522  0.1770634  0.27997249
c  1.34732392  0.2956623  2.0027231 -0.85321627
d  0.82314457  1.2698347 -0.5468151  0.05806375
e -0.18668401 -0.4210383  0.6263465 -0.18889031

(new.col.names = sample(letters[1:4]))
[1] "a" "b" "d" "c"

so the new matrix has to be ordered according the new.col.names,i.e.,
             a          b           d          c
a  0.02362598 -0.7033460	-1.03456219  0.8106089
b -0.45021522 -1.5769522	 0.27997249  0.1770634
c  1.34732392  0.2956623	-0.85321627  2.0027231
d  0.82314457  1.2698347	 0.05806375 -0.5468151
e -0.18668401 -0.4210383	-0.18889031  0.6263465

Hope this is not a "basic question"

Thank you for your help.

Paolo


From maechler at stat.math.ethz.ch  Wed Feb  7 14:50:11 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 7 Feb 2007 14:50:11 +0100
Subject: [R] R in Industry
In-Reply-To: <45C94EB1.9010209@vanderbilt.edu>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
	<1115a2b00702061753w541fc16vd25f3c3fa43b78a4@mail.gmail.com>
	<3f547caa0702061903k23b041d3j1110ffd37715aa4b@mail.gmail.com>
	<45C94EB1.9010209@vanderbilt.edu>
Message-ID: <17865.55571.63498.559308@stat.math.ethz.ch>

>>>>> "Frank" == Frank E Harrell <f.harrell at vanderbilt.edu>
>>>>>     on Tue, 06 Feb 2007 21:59:45 -0600 writes:

    Frank> Matthew Keller wrote:
    >> Bob,
    >> 
    >> Far from flaming you, I think you made a good point - one
    >> that I imagine most people who use R have come
    >> across. The name "R" is a big impediment to effective
    >> online searches. As a check, I entered "R software", "SAS
    >> software", SPSS software", and "S+ software" into
    >> google. The R 'hit rate' was only ten out of the first 20
    >> results (I didn't look any further). For the other three
    >> software packages, the hit rates were all 100% (20/20).
    >> 
    >> I do wonder if anything can/should be done about this. I
    >> generally search using the term "CRAN" but of course,
    >> that omits lots of stuff relevant to R. Any ideas about
    >> how to do effective online searches for "R" related
    >> materials?

I don't think we (the R foundation) will ever change away from
"R"..

    >> 
    >> Matt

    Frank> I just googled for "R" and www.r-project.org was the
    Frank> first hit.  Don't see a problem at present.

We are getting really off-topic, but that's interesting:

We all know that Google is helping the Chinese government to
censor their own people, so searches there can lead to
completely different results.  But even here in Zurich
Switzerland, I get quite a different hitlist :

 1) stat.ethz.ch/~statsoft/stat.programme/R.html [in German]

 2) Our local CRAN mirror:  stat.ethz.ch/CRAN/

 3) R - (German-language) Wikipedia about letter "R": de.wikipedia.org/wiki/R

 4) DVD-R - (German-language) Wikipedia  de.wikipedia.org/wiki/DVD-R

 5) The R Project for Statistical Computing http://www.r-project.org/

So 3/5 are related to R which sounds good, but actually these 3
are all from the first twenty: 3/20.

Martin


    >> On 2/6/07, Wensui Liu <liuwensui at gmail.com> wrote:
    >>> I've been looking for job that allows me to use R/S+
    >>> since I got out of graduate school 2 years ago but with
    >>> no success. I am wondering if there is something that
    >>> can be done to promote the use of R in industry.
    >>> 
    >>> It's been very frustrating to see people doing
    >>> statistics using excel/spss and even more frustrating to
    >>> see people paying $$$ for something much inferior to R.
    >>> 
    >>> 
    >>> On 2/6/07, Doran, Harold <HDoran at air.org> wrote:
    >>>> The other day, CNN had a story on working at
    >>>> Google. Out of curiosity, I went to the Google
    >>>> employment web site (I'm not looking, but just
    >>>> curious). In perusing their job posts for
    >>>> statisticians, preference is given to those who use R
    >>>> and python. Other languages, S-Plus and something
    >>>> called SAS were listed as lower priorities.
    >>>> 
    >>>> When I started using Python, I noted they have a
    >>>> portion of the web site with job postings. CRAN does
    >>>> not have something similar, but think it might be
    >>>> useful. I think R is becoming more widely used in
    >>>> industry and I wonder if helping it move along a bit,
    >>>> the maintainer of CRAN could create a section of the
    >>>> web site devoted to jobs where R is a requirement.
    >>>> 
    >>>> Hence, we could have our own little "monster.com" kind
    >>>> of thing going on. Of the multitude of ways the gospel
    >>>> can be spread, this is small.  But, I think every small
    >>>> step forward is good.
    >>>> 
    >>>> Anyone think this is useful?
    >>>> 
    >>>> Harold
    >>>> 
    >>>> 
    >>>> [[alternative HTML version deleted]]
    >>>> 
    >>>> ______________________________________________
    >>>> R-help at stat.math.ethz.ch mailing list
    >>>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    >>>> read the posting guide
    >>>> http://www.R-project.org/posting-guide.html and provide
    >>>> commented, minimal, self-contained, reproducible code.
    >>>> 
    >>> 
    >>> --
    >>> WenSui Liu A lousy statistician who happens to know a
    >>> little programming
    >>> (http://spaces.msn.com/statcompute/blog)
    >>> 
    >>> ______________________________________________
    >>> R-help at stat.math.ethz.ch mailing list
    >>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    >>> read the posting guide
    >>> http://www.R-project.org/posting-guide.html and provide
    >>> commented, minimal, self-contained, reproducible code.
    >>> 
    >>


    Frank> -- Frank E Harrell Jr Professor and Chair School of
    Frank> Medicine Department of Biostatistics Vanderbilt
    Frank> University

    Frank> ______________________________________________
    Frank> R-help at stat.math.ethz.ch mailing list
    Frank> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Frank> do read the posting guide
    Frank> http://www.R-project.org/posting-guide.html and
    Frank> provide commented, minimal, self-contained,
    Frank> reproducible code.


From Thierry.ONKELINX at inbo.be  Wed Feb  7 14:54:40 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 7 Feb 2007 14:54:40 +0100
Subject: [R] Finding not-matching rows in tables
In-Reply-To: <1109c5fc275.vdemart1@tin.it>
Message-ID: <2E9C414912813E4EB981326983E0A104028F1A24@inboexch.inbo.be>

That's something for the %in% command. Try this (untested!)

tab[(tab$id %in% tabella$id) == FALSE, ]

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Vittorio
Verzonden: woensdag 7 februari 2007 14:24
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Finding not-matching rows in tables

I have these two dataframes in which 'id' is the key field
> tabella
   
id          nome
1   1      PIEMONTE
2   2  VALLED'AOSTA
3   3     
LOMBARDIA
4   4      TRENTINO
5   5        VENETO
6   6        FRIULI

AND

> tab
   id          nome
1   1      PIEMONTE
2   2  VALLED'AOSTA
3   3     LOMBARDIA
4   4      TRENTINO
5  25     CAMPANIA
6  28       
LAZIO

Is there any R-one-command able to select the only rows of tab 
that are not present in tabella, matching the two tables on 'id'?  The 
result should be

5  25     CAMPANIA
6  28       LAZIO

I was able to 
obtain this results by means of the merge command merging the two 
tables and selecting according to the <NA> fields, therefore in two 
steps....

Ciao
Vittorio

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Max.Kuhn at pfizer.com  Wed Feb  7 14:54:48 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Wed, 7 Feb 2007 08:54:48 -0500
Subject: [R] abbreviate dataframe for Sweave output
In-Reply-To: <905a1292a615999b45179fff33da716c@lanl.gov>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307731554@groamrexm03.amer.pfizer.com>

Chris,

You might be able to get it using format. You would have to convert the
data frame to a matrix if you want the row names to be dots too.

foo <- function(x, top = 3, ...)
{
   if(dim(x)[1] < top + 3) stop("not enough rows")
   charX <- format(x, ...)
   charX <- charX[c(1:(top+2), dim(charX)[1]), ]
   charX[(top + 1):(top + 2),] <- "."
   charX <- as.matrix(charX)
   rownames(charX)[(top + 1):(top + 2)] <- "."
   charX
}

> library(MASS)
> foo(crabs)
    sp  sex index FL     RW     CL     CW     BD    
1   "B" "M" " 1"  " 8.1" " 6.7" "16.1" "19.0" " 7.0"
2   "B" "M" " 2"  " 8.8" " 7.7" "18.1" "20.8" " 7.4"
3   "B" "M" " 3"  " 9.2" " 7.8" "19.0" "22.4" " 7.7"
.   "." "." "."   "."    "."    "."    "."    "."   
.   "." "." "."   "."    "."    "."    "."    "."   
200 "O" "F" "50"  "23.1" "20.2" "46.2" "52.5" "21.1"

Max
 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of stubben
Sent: Tuesday, February 06, 2007 6:39 PM
To: r-help at stat.math.ethz.ch
Subject: [R] abbreviate dataframe for Sweave output

I wanted to print the first and last rows of some dataframes in Sweave 
using dots in columns to separate the two parts.  Head and tail almost 
work, but I have problems with factors and row names.

z<-data.frame(id=letters[1:26], x=sample(1:26,26))

rbind(head(z,3), ".", tail(z,1))

      id  x
1     a 18
2     b  8
3     c 14
4  <NA>  .
26    z 10
Warning message:
invalid factor level, NAs generated in...


I would like something like this if possible.  Any ideas?

      id  x
1     a 18
2     b  8
3     c 14
.     .  .
.     .  .
26    z 10


Thanks,

Chris Stubben



-- 
-----------------

Los Alamos National Lab
BioScience Division
MS M888
Los Alamos, NM 87545

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From petr.pikal at precheza.cz  Wed Feb  7 15:10:01 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 07 Feb 2007 15:10:01 +0100
Subject: [R] Finding not-matching rows in tables
In-Reply-To: <1109c5fc275.vdemart1@tin.it>
Message-ID: <45C9EBC9.13005.1ADA8FB@localhost>

Hi

you can use
%in%

tabella[tabella$x %in% tab$x,]

to select rows which are in both and

tabella[!(tabella$x %in% tab$x),]
to select only non matching ones


HTH
Petr


On 7 Feb 2007 at 0:00, Vittorio wrote:

Date sent:      	Wed, 7 Feb 2007 14:23:45 +0100 (GMT+01:00)
From:           	Vittorio <vdemart1 at tin.it>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Finding not-matching rows in tables
Send reply to:  	Vittorio <vdemart1 at tin.it>
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> I have these two dataframes in which 'id' is the key field
> > tabella
> 
> id          nome
> 1   1      PIEMONTE
> 2   2  VALLED'AOSTA
> 3   3     
> LOMBARDIA
> 4   4      TRENTINO
> 5   5        VENETO
> 6   6        FRIULI
> 
> AND
> 
> > tab
>    id          nome
> 1   1      PIEMONTE
> 2   2  VALLED'AOSTA
> 3   3     LOMBARDIA
> 4   4      TRENTINO
> 5  25     CAMPANIA
> 6  28       
> LAZIO
> 
> Is there any R-one-command able to select the only rows of tab 
> that are not present in tabella, matching the two tables on 'id'?  The
> result should be
> 
> 5  25     CAMPANIA
> 6  28       LAZIO
> 
> I was able to 
> obtain this results by means of the merge command merging the two
> tables and selecting according to the <NA> fields, therefore in two
> steps....
> 
> Ciao
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From jholtman at gmail.com  Wed Feb  7 15:16:02 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 7 Feb 2007 09:16:02 -0500
Subject: [R] sorting a matrix by a different colnames order
In-Reply-To: <45C9D788.6060601@supereva.it>
References: <45C9D788.6060601@supereva.it>
Message-ID: <644e1f320702070616h5af6a74mf1ebefb4c26eef19@mail.gmail.com>

> mx = matrix(rnorm(1:20),5,4)
> colnames(mx) = letters[1:4]
> rownames(mx) = letters[1:5]
> mx
           a          b          c           d
a -0.6264538 -0.8204684  1.5117812 -0.04493361
b  0.1836433  0.4874291  0.3898432 -0.01619026
c -0.8356286  0.7383247 -0.6212406  0.94383621
d  1.5952808  0.5757814 -2.2146999  0.82122120
e  0.3295078 -0.3053884  1.1249309  0.59390132
> (new.col.names = sample(letters[1:4]))
[1] "d" "b" "c" "a"
> mx[, new.col.names]
            d          b          c          a
a -0.04493361 -0.8204684  1.5117812 -0.6264538
b -0.01619026  0.4874291  0.3898432  0.1836433
c  0.94383621  0.7383247 -0.6212406 -0.8356286
d  0.82122120  0.5757814 -2.2146999  1.5952808
e  0.59390132 -0.3053884  1.1249309  0.3295078
>


On 2/7/07, portnoy at supereva.it <portnoy at supereva.it> wrote:
> Hi R users,
> I would like to know how to sort a matrix according a different order of
> colnames (or rownames) ,e.g.,
> mx = matrix(rnorm(1:20),5,4)
> colnames(mx) = letters[1:4]
> rownames(mx) = letters[1:5]
> mx
>             a          b          c           d
> a  0.02362598 -0.7033460  0.8106089 -1.03456219
> b -0.45021522 -1.5769522  0.1770634  0.27997249
> c  1.34732392  0.2956623  2.0027231 -0.85321627
> d  0.82314457  1.2698347 -0.5468151  0.05806375
> e -0.18668401 -0.4210383  0.6263465 -0.18889031
>
> (new.col.names = sample(letters[1:4]))
> [1] "a" "b" "d" "c"
>
> so the new matrix has to be ordered according the new.col.names,i.e.,
>             a          b           d          c
> a  0.02362598 -0.7033460        -1.03456219  0.8106089
> b -0.45021522 -1.5769522         0.27997249  0.1770634
> c  1.34732392  0.2956623        -0.85321627  2.0027231
> d  0.82314457  1.2698347         0.05806375 -0.5468151
> e -0.18668401 -0.4210383        -0.18889031  0.6263465
>
> Hope this is not a "basic question"
>
> Thank you for your help.
>
> Paolo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ted.harding at nessie.mcc.ac.uk  Wed Feb  7 15:18:12 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 07 Feb 2007 14:18:12 -0000 (GMT)
Subject: [R] generate Binomial (not Binary) data
In-Reply-To: <426059.35923.qm@web23404.mail.ird.yahoo.com>
Message-ID: <XFMail.070207141724.Ted.Harding@manchester.ac.uk>

On 07-Feb-07 Marc Bernard wrote:
> Dear All,
>    
>   I am looking for an R function or any other reference to generate a
> series of correlated Binomial (not a Bernoulli) data. The "bindata"
> library can do this for the binary not the binomial case.
>    
>   Thank you,
>    
>   Bernard

How do you want your series of binomial datato be "correlated"?
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Feb-07                                       Time: 14:17:15
------------------------------ XFMail ------------------------------


From simon.kempf at web.de  Wed Feb  7 15:27:20 2007
From: simon.kempf at web.de (Simon P. Kempf)
Date: Wed, 7 Feb 2007 15:27:20 +0100
Subject: [R] enhanced question / standardized coefficients
Message-ID: <E1HEnlc-0008Ug-00@smtp07.web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/b944720a/attachment.pl 

From jfox at mcmaster.ca  Wed Feb  7 15:49:20 2007
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 7 Feb 2007 09:49:20 -0500
Subject: [R] enhanced question / standardized coefficients
In-Reply-To: <E1HEnlc-0008Ug-00@smtp07.web.de>
Message-ID: <20070207144918.LNXW1750.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Simon,

In my opinion, standardized coefficients only offer the illusion of
comparison for quantitative explanatory variables, since there's no deep
reason that the standard deviation of one variable has the same meaning as
the standard deviation of another. Indeed, if the variables are in the same
units of measurement in the first place, permitting direct comparison of
unstandardized coefficients, then separate standardization of each X is like
using a rubber ruler.

That said, as you point out, it makes no sense to standardize the dummy
regressors for a factor, so you can just standardize the quantitative
variables (Y and X's) in the regression equation.

I hope that this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Simon P. Kempf
> Sent: Wednesday, February 07, 2007 9:27 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] enhanced question / standardized coefficients
> 
> Hello,
> 
>  
> 
> I would like to repost the question of Joerg:
> 
>  
> 
> Hello everybody, 
> 
> a question that connect to the question of Frederik Karlsons 
> about 'how to stand. betas' 
> With the stand. betas i can compare the influence of the 
> different explaning variables. What do i with the betas of 
> factors? I can't use the solution of JohnFox, because there 
> is no sd of an factor. How can i compare the influence of the 
> factor with the influence of the numeric variables? 
> 
> I got the same problem. In my regression equation there are 
> several categorical variables and  I would like to compute 
> the standard coefficients. How can I do this?
> 
>  
> 
> Simon
> 
>  
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From musche1 at comcast.net  Wed Feb  7 15:49:08 2007
From: musche1 at comcast.net (musche1 at comcast.net)
Date: Wed, 07 Feb 2007 14:49:08 +0000
Subject: [R] installing packages and windows vista
Message-ID: <020720071449.24394.45C9E6E40004638F00005F4A2205884484CE0A080C9C9A03@comcast.net>

Opening R by right clicking and choosing "run as administrator' worked.  Was able to run install packages without a problem.  I have not tested the other methods suggested. thank you.
Dan O'Shea
 -------------- Original message ----------------------
From: Duncan Murdoch <murdoch at stats.uwo.ca>
> On 2/6/2007 10:33 AM, Daniel O'Shea wrote:
> > I installed  R  (R-2.4.1-win32.exe) on a new computer with Windows Vista
> > and a 64 bit operating system (hp dv9000 with intel core t7200).  The
> > base R runs fine, but I can not get any of the packages to load.  From
> > within R I choose install packages choose a site then a package.  I
> > tried installing 2 packages and get similar errors (see below), I just
> > copied and pasted lines from R.
> > 
> > Can anyone offer any suggestions?  Thank you.
> 
> I believe that on Vista you need to do like other OS's, and run package 
> installs at a higher security level than the default.  I don't have 
> Vista so I've never done this, but I've been told you do it by right 
> clicking on the R icon and choosing "Run as administrator".
> 
> I'd be interested in hearing if this is true of all package installs, or 
> only installs to C:/Program files.  Can you have a local library for 
> your user, with only user permissions needed to modify packages there?
> You'd test this by creating a library directory in your own file space, 
> then using .libPaths() to add it to the library location list.  By 
> default new installs would go there.
> 
> Duncan Murdoch
> 
> > 
> > Dan O'Shea
> > 
> >> utils:::menuInstallPkgs()
> > --- Please select a CRAN mirror for use in this session ---
> > also installing the dependencies 'scatterplot3d', 'rgl', 'ellipse'
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/scatterplot3d_0.3-24.zip'
> > Content type 'application/zip' length 540328 bytes
> > opened URL
> > downloaded 527Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
> > Content type 'application/zip' length 838137 bytes
> > opened URL
> > downloaded 818Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/ellipse_0.3-4.zip'
> > Content type 'application/zip' length 91877 bytes
> > opened URL
> > downloaded 89Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/vegan_1.8-5.zip'
> > Content type 'application/zip' length 1176434 bytes
> > opened URL
> > downloaded 1148Kb
> > Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
> > (x86)/R/R-2.4.1/library/file60bf5753/scatterplot3d/chtml/scatterplot3d.chm'
> > 
> > 
> >> utils:::menuInstallPkgs()
> > also installing the dependencies 'akima', 'gam', 'RColorBrewer', 'sm',
> > 'deldir', 'sp', 'maps', 'spatstat', 'PBSmapping', 'gpclib', 'RArcInfo',
> > 'tkrplot', 'maptools', 'mapproj', 'rgl', 'qcc', 'sgeostat', 'acepack',
> > 'TeachingDemos', 'chron', 'Hmisc'
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/akima_0.5-1.zip'
> > Content type 'application/zip' length 128809 bytes
> > opened URL
> > downloaded 125Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/gam_0.98.zip'
> > Content type 'application/zip' length 238008 bytes
> > opened URL
> > downloaded 232Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/RColorBrewer_0.2-3.zip'
> > Content type 'application/zip' length 39787 bytes
> > opened URL
> > downloaded 38Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/sm_2.1-0.zip'
> > Content type 'application/zip' length 400621 bytes
> > opened URL
> > downloaded 391Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/deldir_0.0-5.zip'
> > Content type 'application/zip' length 108656 bytes
> > opened URL
> > downloaded 106Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/sp_0.9-4.zip'
> > Content type 'application/zip' length 747542 bytes
> > opened URL
> > downloaded 730Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/maps_2.0-33.zip'
> > Content type 'application/zip' length 2219136 bytes
> > opened URL
> > downloaded 2167Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/spatstat_1.11-0.zip'
> > Content type 'application/zip' length 4558460 bytes
> > opened URL
> > downloaded 4451Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/PBSmapping_2.09.zip'
> > Content type 'application/zip' length 6725596 bytes
> > opened URL
> > downloaded 6567Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/gpclib_1.3-3.zip'
> > Content type 'application/zip' length 95120 bytes
> > opened URL
> > downloaded 92Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/RArcInfo_0.4-7.zip'
> > Content type 'application/zip' length 374375 bytes
> > opened URL
> > downloaded 365Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/tkrplot_0.0-16.zip'
> > Content type 'application/zip' length 24119 bytes
> > opened URL
> > downloaded 23Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/maptools_0.6-6.zip'
> > Content type 'application/zip' length 679963 bytes
> > opened URL
> > downloaded 664Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/mapproj_1.1-7.1.zip'
> > Content type 'application/zip' length 64188 bytes
> > opened URL
> > downloaded 62Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
> > Content type 'application/zip' length 838137 bytes
> > opened URL
> > downloaded 818Kb
> > trying URL 'http://cran.wustl.edu/bin/windows/contrib/2.4/qcc_1.2.zip'
> > Content type 'application/zip' length 314782 bytes
> > opened URL
> > downloaded 307Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/sgeostat_1.0-20.zip'
> > Content type 'application/zip' length 140584 bytes
> > opened URL
> > downloaded 137Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/acepack_1.3-2.2.zip'
> > Content type 'application/zip' length 55675 bytes
> > opened URL
> > downloaded 54Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/TeachingDemos_1.4.zip'
> > Content type 'application/zip' length 383599 bytes
> > opened URL
> > downloaded 374Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/chron_2.3-9.zip'
> > Content type 'application/zip' length 112830 bytes
> > opened URL
> > downloaded 110Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/Hmisc_3.1-2.zip'
> > Content type 'application/zip' length 2079032 bytes
> > opened URL
> > downloaded 2030Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/Design_2.0-12.zip'
> > Content type 'application/zip' length 1410543 bytes
> > opened URL
> > downloaded 1377Kb
> > Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
> > (x86)/R/R-2.4.1/library/fileceda26/akima/chtml/akima.chm'
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gmerino at icm.csic.es  Wed Feb  7 16:24:47 2007
From: gmerino at icm.csic.es (Gorka Merino)
Date: Wed, 07 Feb 2007 16:24:47 +0100
Subject: [R] Detrended Fluctuation Analysis
Message-ID: <6.2.1.2.1.20070207162159.046dca18@cucafera.icm.csic.es>


Good afternoon, my name is Gorka Merino and i am a scientist working in the 
Marine Science Institune in Barcelone.

I'm interested in the application of "Detrended Fluctuation Analysis" (DFA) 
with the R packages.
I've tried to obtain some information related to DFA from the 'Help' 
options but failed.
Could somebody inform me about the use of these techniques in R language?
Thank you very much.

Gorka Merino

Gorka Merino	
Institut de Ci?ncies del Mar, CMIMA-CSIC
Psg. Mar?tim de la Barceloneta 37-49
08003-BARCELONA (Spain)

Tel.: (34) 932 30 95 48
e-mail: gmerino at icm.csic.es

CMIMA:
Tel.: (34) 932 30 95 00
Fax:  (34) 932 30 95 55


From vikasrawal at gmail.com  Wed Feb  7 12:51:39 2007
From: vikasrawal at gmail.com (Vikas Rawal)
Date: Wed, 7 Feb 2007 17:21:39 +0530
Subject: [R] boxplot statistics in ggplot
In-Reply-To: <20070207070640.GA30339@shireen.jnu.ac.in>
References: <20070207070640.GA30339@shireen.jnu.ac.in>
Message-ID: <20070207115139.GA18014@shireen.jnu.ac.in>

How can I superimpose some text labels on ggplot?  I could get
weighted quantiles using wtd.quantiles function in Hmisc package. But
I can't plot these as labels on the boxplot.

My code is as follows.

list(c(1:3),c(1:3),c(1:3))->t
library(Hmisc)
for (i in 1:3)
  {
    wtd.quantile(crop.2.list[[i]]$aggincome,weights=crop.2.list[[i]]$Multiplier,probs=c(0.25,0.5,0.75))->t[[i]]
   }
data.frame(y=round(unlist(t)),x=c(1,1,1,2,2,2,3,3,3))->levels
library(ggplot)
ggplot(crop.2.cast,aesthetics=list(x=Tenurial.status,y=aggincome, weight=Multiplier),labels=c("ts","in"))->p
ggtext(ggboxplot(p),aesthetics=list(x=x,y=y,labels=y),data=levels)

I am unable to combine ggtext with ggboxplot. Will be grateful if
someone could help.

Finally, a relatively minor issue at the moment, is it possible to
change the x-axis and y-axis labels?

Vikas


From pbulian at cro.it  Wed Feb  7 15:54:49 2007
From: pbulian at cro.it (Pietro Bulian)
Date: Wed, 7 Feb 2007 15:54:49 +0100
Subject: [R] step in a model with strata
Message-ID: <016f01c74ac7$f404efa0$2bb411ac@cro.sanita.fvg.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/a9a44966/attachment.pl 

From b.rowlingson at lancaster.ac.uk  Wed Feb  7 11:52:20 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 07 Feb 2007 10:52:20 +0000
Subject: [R] R in Industry
In-Reply-To: <3f547caa0702061903k23b041d3j1110ffd37715aa4b@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>	<1115a2b00702061753w541fc16vd25f3c3fa43b78a4@mail.gmail.com>
	<3f547caa0702061903k23b041d3j1110ffd37715aa4b@mail.gmail.com>
Message-ID: <45C9AF64.5040708@lancaster.ac.uk>

Matthew Keller wrote:

> I do wonder if anything can/should be done about this. I generally
> search using the term "CRAN" but of course, that omits lots of stuff
> relevant to R. 

  Change the name in the next major version to 'Rplus'?

Barry


From Ted.Harding at manchester.ac.uk  Wed Feb  7 15:17:24 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Wed, 07 Feb 2007 14:17:24 -0000 (GMT)
Subject: [R] generate Binomial (not Binary) data
In-Reply-To: <426059.35923.qm@web23404.mail.ird.yahoo.com>
Message-ID: <XFMail.070207141724.Ted.Harding@manchester.ac.uk>

On 07-Feb-07 Marc Bernard wrote:
> Dear All,
>    
>   I am looking for an R function or any other reference to generate a
> series of correlated Binomial (not a Bernoulli) data. The "bindata"
> library can do this for the binary not the binomial case.
>    
>   Thank you,
>    
>   Bernard

How do you want your series of binomial datato be "correlated"?
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Feb-07                                       Time: 14:17:15
------------------------------ XFMail ------------------------------


From Serguei.Kaniovski at wifo.ac.at  Wed Feb  7 15:52:52 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Wed, 7 Feb 2007 15:52:52 +0100
Subject: [R] fill-in a table of pairs
Message-ID: <OF4A4C0845.402C1A07-ONC125727B.0051BEE1-C125727B.0051BEF1@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/7bcf66ab/attachment.pl 

From vikasrawal at gmail.com  Wed Feb  7 16:44:54 2007
From: vikasrawal at gmail.com (Vikas Rawal)
Date: Wed, 7 Feb 2007 21:14:54 +0530
Subject: [R] boxplot statistics in ggplot
In-Reply-To: <f8e6ff050702070512m4bdfe5dbi1a77fd97b7681fdb@mail.gmail.com>
References: <20070207070640.GA30339@shireen.jnu.ac.in>
	<f8e6ff050702070512m4bdfe5dbi1a77fd97b7681fdb@mail.gmail.com>
Message-ID: <20070207154453.GA5684@shireen.jnu.ac.in>

Hi,

Let me first congratulate you for having written the reshape
package. It is very nice and I use it all the time. I wish the
documentation was a bit easier. It took me quite some time to find my
way through it!! But once I got the hang of how it worked, I just
loved it.

With ggplot, this is my first encounter!!

I actually find the labels on the plots quite useful. For example, to
put identifiers for the outlier observations so that my co-researchers
can dig deeper into data to see if something is wrong. Similarly, it
is useful to be able to put the exact values on the plots. It is much
nicer to be able to see the spread of groups of your data while you
compare the medians/means!!

I will see if I can manage the boxplot.weighted myself. But how does
one add the values!! Is the next version round the corner? Is there a
way one could do it with the present version?

Again, congratulations for all the good stuff you have written!!

Vikas


From tlumley at u.washington.edu  Wed Feb  7 16:50:50 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 7 Feb 2007 07:50:50 -0800 (PST)
Subject: [R] R in Industry
In-Reply-To: <7270AEC73132194E8BC0EE06B35D93D879B707@UTKFSVS3.utk.tennessee.edu>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>
	<7270AEC73132194E8BC0EE06B35D93D879B707@UTKFSVS3.utk.tennessee.edu>
Message-ID: <Pine.LNX.4.64.0702070742360.31275@homer22.u.washington.edu>

On Tue, 6 Feb 2007, Muenchen, Robert A (Bob) wrote:

> That sounds like a good idea. The name R makes it especially hard to
> find job postings, resumes or do any other type of search. Googling
> resume+sas or "job opening"+sas is quick and fairly effective (less a
> few airline jobs). Doing that with R is of course futile. At the risk of
> getting flamed, it's too bad it's not called something more unique such
> as Rpackage, Rlanguage, etc.

For all sorts of reasons I don't think Googling for jobs using R was high 
on Ross & Robert's list of use cases when they chose the name ...

It might be better to have an archived list rather than a CRAN page -- 
I've just noticed that cran.us last updated on Jan 12, which would be a 
long delay for job ads.

 	-thomas


From cadeb at usgs.gov  Wed Feb  7 16:51:44 2007
From: cadeb at usgs.gov (Brian S Cade)
Date: Wed, 7 Feb 2007 08:51:44 -0700
Subject: [R] enhanced question / standardized coefficients
In-Reply-To: <20070207144918.LNXW1750.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <OF70F7CAB8.E870EA54-ON8725727B.00546962-8725727B.00576C7A@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/cae0bfed/attachment.pl 

From f.calboli at imperial.ac.uk  Wed Feb  7 17:16:14 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 7 Feb 2007 16:16:14 +0000
Subject: [R] spss file import
Message-ID: <90AAFBD4-FA98-4F76-B6E7-882C6B7E0686@imperial.ac.uk>

Hi All,

does anyone ever import old SPSS files in a sl3 format?

read.spss('file.sl3') does not seem to work... it's not recognised as  
a supported SPSS format at all.

Best,

Fede

--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From Charles.Annis at StatisticalEngineering.com  Wed Feb  7 17:27:23 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 7 Feb 2007 11:27:23 -0500
Subject: [R] installing packages and windows vista
In-Reply-To: <020720071449.24394.45C9E6E40004638F00005F4A2205884484CE0A080C9C9A03@comcast.net>
References: <020720071449.24394.45C9E6E40004638F00005F4A2205884484CE0A080C9C9A03@comcast.net>
Message-ID: <047501c74ad4$d987db30$6400a8c0@DD4XFW31>

Thank you Dan!

I dunno if I would have ever found that.

Thanks!

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of musche1 at comcast.net
Sent: Wednesday, February 07, 2007 9:49 AM
To: Duncan Murdoch; P.Dalgaard at biostat.ku.dk
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] installing packages and windows vista

Opening R by right clicking and choosing "run as administrator' worked.  Was
able to run install packages without a problem.  I have not tested the other
methods suggested. thank you.
Dan O'Shea
 -------------- Original message ----------------------
From: Duncan Murdoch <murdoch at stats.uwo.ca>
> On 2/6/2007 10:33 AM, Daniel O'Shea wrote:
> > I installed  R  (R-2.4.1-win32.exe) on a new computer with Windows Vista
> > and a 64 bit operating system (hp dv9000 with intel core t7200).  The
> > base R runs fine, but I can not get any of the packages to load.  From
> > within R I choose install packages choose a site then a package.  I
> > tried installing 2 packages and get similar errors (see below), I just
> > copied and pasted lines from R.
> > 
> > Can anyone offer any suggestions?  Thank you.
> 
> I believe that on Vista you need to do like other OS's, and run package 
> installs at a higher security level than the default.  I don't have 
> Vista so I've never done this, but I've been told you do it by right 
> clicking on the R icon and choosing "Run as administrator".
> 
> I'd be interested in hearing if this is true of all package installs, or 
> only installs to C:/Program files.  Can you have a local library for 
> your user, with only user permissions needed to modify packages there?
> You'd test this by creating a library directory in your own file space, 
> then using .libPaths() to add it to the library location list.  By 
> default new installs would go there.
> 
> Duncan Murdoch
> 
> > 
> > Dan O'Shea
> > 
> >> utils:::menuInstallPkgs()
> > --- Please select a CRAN mirror for use in this session ---
> > also installing the dependencies 'scatterplot3d', 'rgl', 'ellipse'
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/scatterplot3d_0.3-24.zip'
> > Content type 'application/zip' length 540328 bytes
> > opened URL
> > downloaded 527Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
> > Content type 'application/zip' length 838137 bytes
> > opened URL
> > downloaded 818Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/ellipse_0.3-4.zip'
> > Content type 'application/zip' length 91877 bytes
> > opened URL
> > downloaded 89Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/vegan_1.8-5.zip'
> > Content type 'application/zip' length 1176434 bytes
> > opened URL
> > downloaded 1148Kb
> > Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
> >
(x86)/R/R-2.4.1/library/file60bf5753/scatterplot3d/chtml/scatterplot3d.chm'
> > 
> > 
> >> utils:::menuInstallPkgs()
> > also installing the dependencies 'akima', 'gam', 'RColorBrewer', 'sm',
> > 'deldir', 'sp', 'maps', 'spatstat', 'PBSmapping', 'gpclib', 'RArcInfo',
> > 'tkrplot', 'maptools', 'mapproj', 'rgl', 'qcc', 'sgeostat', 'acepack',
> > 'TeachingDemos', 'chron', 'Hmisc'
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/akima_0.5-1.zip'
> > Content type 'application/zip' length 128809 bytes
> > opened URL
> > downloaded 125Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/gam_0.98.zip'
> > Content type 'application/zip' length 238008 bytes
> > opened URL
> > downloaded 232Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/RColorBrewer_0.2-3.zip'
> > Content type 'application/zip' length 39787 bytes
> > opened URL
> > downloaded 38Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/sm_2.1-0.zip'
> > Content type 'application/zip' length 400621 bytes
> > opened URL
> > downloaded 391Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/deldir_0.0-5.zip'
> > Content type 'application/zip' length 108656 bytes
> > opened URL
> > downloaded 106Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/sp_0.9-4.zip'
> > Content type 'application/zip' length 747542 bytes
> > opened URL
> > downloaded 730Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/maps_2.0-33.zip'
> > Content type 'application/zip' length 2219136 bytes
> > opened URL
> > downloaded 2167Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/spatstat_1.11-0.zip'
> > Content type 'application/zip' length 4558460 bytes
> > opened URL
> > downloaded 4451Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/PBSmapping_2.09.zip'
> > Content type 'application/zip' length 6725596 bytes
> > opened URL
> > downloaded 6567Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/gpclib_1.3-3.zip'
> > Content type 'application/zip' length 95120 bytes
> > opened URL
> > downloaded 92Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/RArcInfo_0.4-7.zip'
> > Content type 'application/zip' length 374375 bytes
> > opened URL
> > downloaded 365Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/tkrplot_0.0-16.zip'
> > Content type 'application/zip' length 24119 bytes
> > opened URL
> > downloaded 23Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/maptools_0.6-6.zip'
> > Content type 'application/zip' length 679963 bytes
> > opened URL
> > downloaded 664Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/mapproj_1.1-7.1.zip'
> > Content type 'application/zip' length 64188 bytes
> > opened URL
> > downloaded 62Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/rgl_0.70.zip'
> > Content type 'application/zip' length 838137 bytes
> > opened URL
> > downloaded 818Kb
> > trying URL 'http://cran.wustl.edu/bin/windows/contrib/2.4/qcc_1.2.zip'
> > Content type 'application/zip' length 314782 bytes
> > opened URL
> > downloaded 307Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/sgeostat_1.0-20.zip'
> > Content type 'application/zip' length 140584 bytes
> > opened URL
> > downloaded 137Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/acepack_1.3-2.2.zip'
> > Content type 'application/zip' length 55675 bytes
> > opened URL
> > downloaded 54Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/TeachingDemos_1.4.zip'
> > Content type 'application/zip' length 383599 bytes
> > opened URL
> > downloaded 374Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/chron_2.3-9.zip'
> > Content type 'application/zip' length 112830 bytes
> > opened URL
> > downloaded 110Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/Hmisc_3.1-2.zip'
> > Content type 'application/zip' length 2079032 bytes
> > opened URL
> > downloaded 2030Kb
> > trying URL
> > 'http://cran.wustl.edu/bin/windows/contrib/2.4/Design_2.0-12.zip'
> > Content type 'application/zip' length 1410543 bytes
> > opened URL
> > downloaded 1377Kb
> > Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files
> > (x86)/R/R-2.4.1/library/fileceda26/akima/chtml/akima.chm'
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bolker at zoo.ufl.edu  Wed Feb  7 17:37:27 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 07 Feb 2007 11:37:27 -0500
Subject: [R] Any Limitations for the dataframes?
Message-ID: <45CA0047.8000100@zoo.ufl.edu>


Shubha Vishwanath Karanth <shubhak <at> ambaresearch.com> writes:

>
> Hi R,
>
> Are there any limitations on the capacity of the data to hold for R data
> frames or zoo objects? I mean to ask are there any restrictions on the
> number of rows or column in the R data frames or the zoo objects?
>
> Thank you,
>
> Shubha

  Pretty much only memory size (the number of elements in
a vector, which probably (?) translates to the number of rows,
is limited to 2^31-1, but if you really have more than
2 billion observations you'll probably run out of memory
first ...

?Memory
help("Memory-limits")

  If you give more specifics on the problem (approx.
number of observations and elements per observation, and
what you plan to try to do with them) you may get
more useful feedback about whether what you plan to
do is feasible and/or sensible.

  Ben Bolker


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 254 bytes
Desc: OpenPGP digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070207/ae6aa244/attachment.bin 

From ford at signal.QinetiQ.com  Wed Feb  7 18:18:56 2007
From: ford at signal.QinetiQ.com (Ashley Ford)
Date: Wed, 07 Feb 2007 17:18:56 +0000
Subject: [R] XML and str
Message-ID: <1170868736.27202.159.camel@fordpc.signal.qinetiq.com>

If I read in an .xml file eg with 
> xeg <- xmlTreeParse(system.file("exampleData", "test.xml",
package="XML"))

It appears to be OK however examining it with str() gives an apparent
error

> str(xeg,2)
List of 2
 $ doc:List of 3
  ..$ file    : list()
  .. ..- attr(*, "class")= chr [1:2] "XMLComment" "XMLNode"
  ..$ version :List of 4
  .. ..- attr(*, "class")= chr "XMLNode"
  ..$ children:Error in obj$children[[...]] : subscript out of bounds

I am unsure if this is a feature or a bug and if the latter whether it
is in XML or str, it is not causing a problem but I would like to
understand what is happening, any ideas ?

examining components eg 
> str(xeg$doc$children,2)

List of 2
 $ comment: list()
  ..- attr(*, "class")= chr [1:2] "XMLComment" "XMLNode"
etc 

is OK.

XML Version 1.4-1, 
same behaviour on Windows and Linux, R version 2.4.1 (2006-12-18)




The information contained in this E-Mail and any subsequent
correspondence is private and is intended solely for the intended
recipient(s).  The information in this communication may be confidential
and/or legally privileged.  Nothing in this e-mail is intended to
conclude a contract on behalf of QinetiQ or make QinetiQ subject to any
other legally binding commitments, unless the e-mail contains an express
statement to the contrary or incorporates a formal Purchase Order.

For those other than the recipient any disclosure, copying,
distribution, or any action taken or omitted to be taken in reliance on
such information is prohibited and may be unlawful.

Emails and other electronic communication with QinetiQ may be monitored
and recorded for business purposes including security, audit and
archival purposes.  Any response to this email indicates consent to
this.

Telephone calls to QinetiQ may be monitored or recorded for quality
control, security and other business purposes.

QinetiQ Group plc,

Company Registration No: 4586941,  

Registered office: 85 Buckingham Gate, London SW1E 6PD


From Mark.Leeds at morganstanley.com  Wed Feb  7 18:28:11 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Wed, 7 Feb 2007 12:28:11 -0500
Subject: [R] ncdf library
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401401828@NYWEXMB23.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/5b44b876/attachment.pl 

From adam_6242 at yahoo.com  Wed Feb  7 18:42:56 2007
From: adam_6242 at yahoo.com (aat)
Date: Wed, 7 Feb 2007 09:42:56 -0800 (PST)
Subject: [R] tCopula package question
Message-ID: <8850378.post@talk.nabble.com>


Hello everyone,

I am having issues with the R copula package.  Just trying to run the
example I found at this site gives me an error.

When I implement this code:

mycop <- tCopula(param=0.5, dim=8, dispstr = "ex", df =5)
myfit <- fitCopula(x, mycop, c(0.6,10),
optim.control=list(trace=1),method="Nelder-Mead") 
myfit

I get the following information on the search method and an error:

Error in chol(x, pivot = FALSE) : the leading minor of order 2 is not
positive definite
> myfit
Error: object "myfit" not found

Any ideas on why this is happening are appreciated. Thanks.

Adam (aat)
-- 
View this message in context: http://www.nabble.com/tCopula-package-question-tf3188503.html#a8850378
Sent from the R help mailing list archive at Nabble.com.


From h.wickham at gmail.com  Wed Feb  7 18:44:42 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 7 Feb 2007 11:44:42 -0600
Subject: [R] boxplot statistics in ggplot
In-Reply-To: <20070207115139.GA18014@shireen.jnu.ac.in>
References: <20070207070640.GA30339@shireen.jnu.ac.in>
	<20070207115139.GA18014@shireen.jnu.ac.in>
Message-ID: <f8e6ff050702070944l3988e440lc74f06205d238c4e@mail.gmail.com>

On 2/7/07, Vikas Rawal <vikasrawal at gmail.com> wrote:
> How can I superimpose some text labels on ggplot?  I could get
> weighted quantiles using wtd.quantiles function in Hmisc package. But
> I can't plot these as labels on the boxplot.
>
> My code is as follows.
>
> list(c(1:3),c(1:3),c(1:3))->t
> library(Hmisc)
> for (i in 1:3)
>   {
>     wtd.quantile(crop.2.list[[i]]$aggincome,weights=crop.2.list[[i]]$Multiplier,probs=c(0.25,0.5,0.75))->t[[i]]
>    }
> data.frame(y=round(unlist(t)),x=c(1,1,1,2,2,2,3,3,3))->levels
> library(ggplot)
> ggplot(crop.2.cast,aesthetics=list(x=Tenurial.status,y=aggincome, weight=Multiplier),labels=c("ts","in"))->p
> ggtext(ggboxplot(p),aesthetics=list(x=x,y=y,labels=y),data=levels)

That should work.  Are you able to send me the data so I can check it myself?

> Finally, a relatively minor issue at the moment, is it possible to
> change the x-axis and y-axis labels?

Yes:  p$xlabel <- "X label"; p$ylabel <- "Y label"

Hadley


From spencer.tapia at ntlworld.com  Wed Feb  7 19:02:32 2007
From: spencer.tapia at ntlworld.com (Thor)
Date: Wed, 7 Feb 2007 10:02:32 -0800 (PST)
Subject: [R] Sample Poisson Distribution
Message-ID: <8850775.post@talk.nabble.com>


Hi,
 I'm completely new to R, I am all at sea with the interface and the
confusing help files, so would appreciate some help to do a simple task.

Need to present the mean and variance of 100 different samples of poisson
distributions (N=1000, with fixed lambda) in a file in two columnns, and
then produce histograms.

So far I have figured out:

> N <- 1000
>  x <- rpois(N, 3.1) ,

and 
> var(x) 
and 
> mean(x)
, and I've seen the hist command, just need to tie it all together.  I read
that loops aren't really used in R, so what do i need to do?

thanks
-- 
View this message in context: http://www.nabble.com/Sample-Poisson-Distribution-tf3188621.html#a8850775
Sent from the R help mailing list archive at Nabble.com.


From Abhijit.Dasgupta at mail.jci.tju.edu  Wed Feb  7 19:10:16 2007
From: Abhijit.Dasgupta at mail.jci.tju.edu (Abhijit Dasgupta)
Date: Wed, 07 Feb 2007 13:10:16 -0500
Subject: [R] Sample Poisson Distribution
In-Reply-To: <8850775.post@talk.nabble.com>
References: <8850775.post@talk.nabble.com>
Message-ID: <45CA1608.9030200@mail.jci.tju.edu>

Do you mean that you have 100 samples, each of size 1000. If this is so, 
you can perhaps do:

N = 1000
n = 100
x = matrix(rpois(N*n, 3.1), ncol=100) # Generate the appropriate no. of 
Poisson samples and rearrange into 100 columns of 1000

output = cbind(means=apply(x,2,mean), vars=apply(x,2,var)) # 
apply(x,2,mean) runs "mean" on each column of x

What do you want to draw the histograms of? Histograms of the means 
and/or variances can be done by
hist(output$means)
hist(output$vars)

Hope this helps

Abhijit

Thor wrote:
> Hi,
>  I'm completely new to R, I am all at sea with the interface and the
> confusing help files, so would appreciate some help to do a simple task.
>
> Need to present the mean and variance of 100 different samples of poisson
> distributions (N=1000, with fixed lambda) in a file in two columnns, and
> then produce histograms.
>
> So far I have figured out:
>
>   
>> N <- 1000
>>  x <- rpois(N, 3.1) ,
>>     
>
> and 
>   
>> var(x) 
>>     
> and 
>   
>> mean(x)
>>     
> , and I've seen the hist command, just need to tie it all together.  I read
> that loops aren't really used in R, so what do i need to do?
>
> thanks
>


From sapsi at pobox.com  Wed Feb  7 19:16:48 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Wed, 7 Feb 2007 13:16:48 -0500
Subject: [R] Filling the window in  lattice plot
Message-ID: <45EC2513-21C0-45A0-B53E-1DB5891364C8@pobox.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/88635fc2/attachment.pl 

From jyan at stat.uiowa.edu  Wed Feb  7 19:17:15 2007
From: jyan at stat.uiowa.edu (Jun Yan)
Date: Wed, 7 Feb 2007 12:17:15 -0600
Subject: [R] tCopula package question
In-Reply-To: <8850378.post@talk.nabble.com>
References: <8850378.post@talk.nabble.com>
Message-ID: <591fc1b70702071017s6373d32cq53cc6ca5427e136e@mail.gmail.com>

That may indicate lack of fit. If the data is generated from the
t-copula, this worked for me:

set.seed(123)
mycop <- tCopula(param=0.5, dim=8, dispstr = "ex", df =5)
x <- rcopula(mycop, 1000)
myfit <- fitCopula(x, mycop, c(0.6,10),
optim.control=list(trace=1),method="Nelder-Mead")
myfit

On 2/7/07, aat <adam_6242 at yahoo.com> wrote:
>
> Hello everyone,
>
> I am having issues with the R copula package.  Just trying to run the
> example I found at this site gives me an error.
>
> When I implement this code:
>
> mycop <- tCopula(param=0.5, dim=8, dispstr = "ex", df =5)
> myfit <- fitCopula(x, mycop, c(0.6,10),
> optim.control=list(trace=1),method="Nelder-Mead")
> myfit
>
> I get the following information on the search method and an error:
>
> Error in chol(x, pivot = FALSE) : the leading minor of order 2 is not
> positive definite
> > myfit
> Error: object "myfit" not found
>
> Any ideas on why this is happening are appreciated. Thanks.
>
> Adam (aat)
> --
> View this message in context: http://www.nabble.com/tCopula-package-question-tf3188503.html#a8850378
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jun Yan
Assistant Professor
Department of Statistics and
 Actuarial Science                             Voice: 319-335-0824
University of Iowa                               Fax: 319-335-3017
219 Schaeffer Hall                          Email: j-yan at uiowa.edu
Iowa City, IA 52242           Web: http://www.stat.uiowa.edu/~jyan


From pgilbert at bank-banque-canada.ca  Wed Feb  7 19:16:30 2007
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 07 Feb 2007 13:16:30 -0500
Subject: [R] R in Industry
In-Reply-To: <17865.55571.63498.559308@stat.math.ethz.ch>
References: <2323A6D37908A847A7C32F1E3662C80E8D490E@dc1ex01.air.org>	<1115a2b00702061753w541fc16vd25f3c3fa43b78a4@mail.gmail.com>	<3f547caa0702061903k23b041d3j1110ffd37715aa4b@mail.gmail.com>	<45C94EB1.9010209@vanderbilt.edu>
	<17865.55571.63498.559308@stat.math.ethz.ch>
Message-ID: <45CA177E.6080304@bank-banque-canada.ca>



Martin Maechler wrote:
>>>>>> "Frank" == Frank E Harrell <f.harrell at vanderbilt.edu>
>>>>>>     on Tue, 06 Feb 2007 21:59:45 -0600 writes:
> 
>     Frank> Matthew Keller wrote:
>     >> Bob,
>     >> 
>     >> Far from flaming you, I think you made a good point - one
>     >> that I imagine most people who use R have come
>     >> across. The name "R" is a big impediment to effective
>     >> online searches. As a check, I entered "R software", "SAS
>     >> software", SPSS software", and "S+ software" into
>     >> google. The R 'hit rate' was only ten out of the first 20
>     >> results (I didn't look any further). For the other three
>     >> software packages, the hit rates were all 100% (20/20).
>     >> 
>     >> I do wonder if anything can/should be done about this. I
>     >> generally search using the term "CRAN" but of course,
>     >> that omits lots of stuff relevant to R. Any ideas about
>     >> how to do effective online searches for "R" related
>     >> materials?
> 
> I don't think we (the R foundation) will ever change away from
> "R"..
> 
>     >> 
>     >> Matt
> 
>     Frank> I just googled for "R" and www.r-project.org was the
>     Frank> first hit.  Don't see a problem at present.
> 
> We are getting really off-topic, but that's interesting:
> 
> We all know that Google is helping the Chinese government to
> censor their own people, so searches there can lead to
> completely different results.  But even here in Zurich
> Switzerland, I get quite a different hitlist :
> 
>  1) stat.ethz.ch/~statsoft/stat.programme/R.html [in German]
> 
>  2) Our local CRAN mirror:  stat.ethz.ch/CRAN/
> 
>  3) R - (German-language) Wikipedia about letter "R": de.wikipedia.org/wiki/R
> 
>  4) DVD-R - (German-language) Wikipedia  de.wikipedia.org/wiki/DVD-R
> 
>  5) The R Project for Statistical Computing http://www.r-project.org/
> 
> So 3/5 are related to R which sounds good, but actually these 3
> are all from the first twenty: 3/20.
> 
> Martin

Interesting. I just tried www.google.ca and got

1) The R Project for Statistical Computing http://www.r-project.org/

2) Wikipedia on the letter R (which also does mention the R software).

3) CRAN:  cran.r-project.org

4) R Commander

with a pretty respectable number of hits in the next 50, and the 
sponsored link is "Mango Solutions provide training and consulting on 
the R language."

Paul

> 
> 
>     >> On 2/6/07, Wensui Liu <liuwensui at gmail.com> wrote:
>     >>> I've been looking for job that allows me to use R/S+
>     >>> since I got out of graduate school 2 years ago but with
>     >>> no success. I am wondering if there is something that
>     >>> can be done to promote the use of R in industry.
>     >>> 
>     >>> It's been very frustrating to see people doing
>     >>> statistics using excel/spss and even more frustrating to
>     >>> see people paying $$$ for something much inferior to R.
>     >>> 
>     >>> 
>     >>> On 2/6/07, Doran, Harold <HDoran at air.org> wrote:
>     >>>> The other day, CNN had a story on working at
>     >>>> Google. Out of curiosity, I went to the Google
>     >>>> employment web site (I'm not looking, but just
>     >>>> curious). In perusing their job posts for
>     >>>> statisticians, preference is given to those who use R
>     >>>> and python. Other languages, S-Plus and something
>     >>>> called SAS were listed as lower priorities.
>     >>>> 
>     >>>> When I started using Python, I noted they have a
>     >>>> portion of the web site with job postings. CRAN does
>     >>>> not have something similar, but think it might be
>     >>>> useful. I think R is becoming more widely used in
>     >>>> industry and I wonder if helping it move along a bit,
>     >>>> the maintainer of CRAN could create a section of the
>     >>>> web site devoted to jobs where R is a requirement.
>     >>>> 
>     >>>> Hence, we could have our own little "monster.com" kind
>     >>>> of thing going on. Of the multitude of ways the gospel
>     >>>> can be spread, this is small.  But, I think every small
>     >>>> step forward is good.
>     >>>> 
>     >>>> Anyone think this is useful?
>     >>>> 
>     >>>> Harold
>     >>>> 
>     >>>> 
>     >>>> [[alternative HTML version deleted]]
>     >>>> 
>     >>>> ______________________________________________
>     >>>> R-help at stat.math.ethz.ch mailing list
>     >>>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
>     >>>> read the posting guide
>     >>>> http://www.R-project.org/posting-guide.html and provide
>     >>>> commented, minimal, self-contained, reproducible code.
>     >>>> 
>     >>> 
>     >>> --
>     >>> WenSui Liu A lousy statistician who happens to know a
>     >>> little programming
>     >>> (http://spaces.msn.com/statcompute/blog)
>     >>> 
>     >>> ______________________________________________
>     >>> R-help at stat.math.ethz.ch mailing list
>     >>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
>     >>> read the posting guide
>     >>> http://www.R-project.org/posting-guide.html and provide
>     >>> commented, minimal, self-contained, reproducible code.
>     >>> 
>     >>
> 
> 
>     Frank> -- Frank E Harrell Jr Professor and Chair School of
>     Frank> Medicine Department of Biostatistics Vanderbilt
>     Frank> University
> 
>     Frank> ______________________________________________
>     Frank> R-help at stat.math.ethz.ch mailing list
>     Frank> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
>     Frank> do read the posting guide
>     Frank> http://www.R-project.org/posting-guide.html and
>     Frank> provide commented, minimal, self-contained,
>     Frank> reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential inform...{{dropped}}


From leog at anicca-vijja.de  Wed Feb  7 19:35:52 2007
From: leog at anicca-vijja.de (=?ISO-8859-15?Q?Leo_G=FCrtler?=)
Date: Wed, 07 Feb 2007 19:35:52 +0100
Subject: [R] blank upper or lower triangle of cor-matrix
Message-ID: <45CA1C08.2000908@anicca-vijja.de>

Dear altogether,

I want to blank the lower (or upper) part of a correlation matrix as it 
is done by dist()

example:

( d <- cor(matrix(runif(12),nrow=4)) )

If I do the following

d[lower.tri(d)] <- ""

of course everything is changed to character - that's not what should be.
Additionally, it does not work to assign "0" or anything else. The same 
is true for assigning "NA".

However, what I want is like the following:

( dist(matrix(runif(12),nrow=4)) )

Looking into dist(), it seems that the calculation and the matrix are 
done in C and not in plain R.

How can I realize it?

thanks!

best,

leo


From ted.harding at nessie.mcc.ac.uk  Wed Feb  7 19:51:55 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 07 Feb 2007 18:51:55 -0000 (GMT)
Subject: [R] Sample Poisson Distribution
In-Reply-To: <8850775.post@talk.nabble.com>
Message-ID: <XFMail.070207185155.ted.harding@nessie.mcc.ac.uk>

On 07-Feb-07 Thor wrote:
> Hi,
>  I'm completely new to R, I am all at sea with the interface
> and the confusing help files, so would appreciate some help
> to do a simple task.
> 
> Need to present the mean and variance of 100 different samples
> of poisson distributions (N=1000, with fixed lambda) in a file
> in two columnns, and then produce histograms.
> 
> So far I have figured out:
> 
>> N <- 1000
>>  x <- rpois(N, 3.1) ,

Comment: The Poisson distribution has only one parameter, lambda,
so it should be rpois(N, lambda), e.g. rpois(N, 3). You will get
an error with your second parameter "1".

> and 
>> var(x) 
> and 
>> mean(x)
> , and I've seen the hist command, just need to tie it all together.
> I read that loops aren't really used in R, so what do i need to do?

Since you're completely new, there are features of how R handles
things in its data structures which are very useful for this kind
of thing.

In this case, the trick is that if you construct a matrix out of
a single vector with many elements in it, R will fill in the
columns from the vector working down each column anf then from
left to right. For example:

> matrix(c(1,2,3,4,5,6),ncol=2)
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6

So you can get all 100 samples into 100 columns of a matrix A
with N rows as

N<-1000; Nsamp<-100
A <- matrix(rpois(N*Nsamp, 3),ncol=Nsamp)

See ?matrix for a summary of the above.

Then (though here it's not quite clear what you really want) you
can put the mean of each of the 100 columns into one column of
your results, and the variance of each column into the next column
of results, obtaining a matrix with 100 rows and 2 columns:

So now you need to get the mean and varuance of each column of A.
If you just try mean(A) you will get one number, because R will
simply calculate the mean of all the numbers in A. The function
to use here is apply():

means<-apply(A,2,mean)
vars <-apply(A,2,var)

since this works along the "2nd dimension" of A (i.e. the columns)
and calculates the mean for each one, and the variance fo each one.

You can tie it all together in one operation by using cbind(),
which assembles a collection of vectors (all theaame length)
into columns side by side and makes a matrix ofthem:

Result <- cbind(means, vars)

or, without the intermediate calculation,

Result <- cbind(apply(A,2,mean), apply(A,2,var))

(However, it will be useful later to have the separate intermediate
results).

At this stage I'm really not sure whatyou exactly want, since you
don't say what you want the histograms of. But I'm going to guess
that you want the histograms of the 100 means, and the 100 variances.

You can do this either with

hist(means)
hist(vars)

or equivalently with

hist(Result[,1])
hist(Result[,2])

In R there are many possibilities for neat manoevres of this kind,
and I tend to agree that they are not always easily found by people
new to R. It's well worth reading the introductory documentation
for R, under "Documentation" on the CRAN website, especially
"An Introduction to R" and (under "Contributed Documentation")
"Using R for Data Analysis and Graphics - Introduction, Examples and
Commentary", "Simple R", "Practical Regression and Anova using R"
and "R for Beginners". You will find several examples of data
manipulation techniques in these. Once you get used to R you will
be using them all the time.

Best wishes, and good luck with R!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Feb-07                                       Time: 18:51:52
------------------------------ XFMail ------------------------------


From jholtman at gmail.com  Wed Feb  7 20:04:09 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 7 Feb 2007 14:04:09 -0500
Subject: [R] fill-in a table of pairs
In-Reply-To: <OF4A4C0845.402C1A07-ONC125727B.0051BEE1-C125727B.0051BEF1@wsr.ac.at>
References: <OF4A4C0845.402C1A07-ONC125727B.0051BEE1-C125727B.0051BEF1@wsr.ac.at>
Message-ID: <644e1f320702071104u3811ba0t72fc99c0fecd068f@mail.gmail.com>

How about this:

> x
     1    2
1  joe 0.45
2 mike 0.34
3  jim 0.25
> combine <- combn(3, 2)
> combine
     [,1] [,2] [,3]
[1,]    1    1    2
[2,]    2    3    3
> ans <- cbind(x[combine[1,],2], x[combine[2,], 2])
> rownames(ans) <- paste(x[combine[1,], 1], x[combine[2,], 1], sep='.')
> ans
         [,1] [,2]
joe.mike 0.45 0.34
joe.jim  0.45 0.25
mike.jim 0.34 0.25
>
>


On 2/7/07, Serguei Kaniovski <Serguei.Kaniovski at wifo.ac.at> wrote:
>
> Hallo,
>
> I have a table of names and values:
> joe 0.45
> mike 0.34
> jim 0.25
>
> I would like to fill-in a table of all pairs of names (which I aleady have)
> joe.mike NA NA
> joe.jim NA NA
> mike.jim NA NA
>
> with the values from the first table in the order of the pairs. The outcome
> looks like
> joe.mike 0.45 0.34
> joe.jim 0.45 0.25
> mike.jim 0.34 0.25
>
> Thanks a lot,
> Serguei
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ted.harding at nessie.mcc.ac.uk  Wed Feb  7 20:10:21 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 07 Feb 2007 19:10:21 -0000 (GMT)
Subject: [R] Sample Poisson Distribution
In-Reply-To: <XFMail.070207185155.ted.harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.070207191021.ted.harding@nessie.mcc.ac.uk>

On 07-Feb-07 Ted Harding wrote:
> On 07-Feb-07 Thor wrote:
>> [...]
>> So far I have figured out:
>> 
>>> N <- 1000
>>>  x <- rpois(N, 3.1) ,
> 
> Comment: The Poisson distribution has only one parameter, lambda,
> so it should be rpois(N, lambda), e.g. rpois(N, 3). You will get
> an error with your second parameter "1".

OOOPS!! My eyesight let me down here, or else it was a speck on my
computer screen! Of course I now see it is "3.1" and not "3,1", so
wherever I wrote "3" before it should be "3.1"!

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Feb-07                                       Time: 19:10:18
------------------------------ XFMail ------------------------------


From this.wiederkehr at gmail.com  Wed Feb  7 20:13:31 2007
From: this.wiederkehr at gmail.com (This Wiederkehr)
Date: Wed, 7 Feb 2007 20:13:31 +0100
Subject: [R] Singular Gradient
Message-ID: <285191030702071113p201bdf5dra688d64e05220249@mail.gmail.com>

I tried to fit data with the following function:

fit<-nls(y~ Is*(1-exp(-l*x))+Iph,start=list(Is=-2e-5,l=2.3,Iph=-0.3
),control=list(maxiter=500,minFactor=1/10000,tol=10e-05),trace=TRUE)
But I get only a singular Gradient warning...
the data can by found attached(there are two sampels of data col 1/2 and
3/4).

I tried to fix it by chanching the start parameters but that didn't solve
the problem.
Would it be a possibiliti to use the selfstart Model? How?

Thanks for any answers

This

From christos at nuverabio.com  Wed Feb  7 20:16:10 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 7 Feb 2007 14:16:10 -0500
Subject: [R] blank upper or lower triangle of cor-matrix
In-Reply-To: <45CA1C08.2000908@anicca-vijja.de>
References: <45CA1C08.2000908@anicca-vijja.de>
Message-ID: <005601c74aec$6d6ce9a0$0e010a0a@headquarters.silicoinsights>

You can try

as.dist(d)


Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leo G?rtler
> Sent: Wednesday, February 07, 2007 1:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] blank upper or lower triangle of cor-matrix
> 
> Dear altogether,
> 
> I want to blank the lower (or upper) part of a correlation 
> matrix as it is done by dist()
> 
> example:
> 
> ( d <- cor(matrix(runif(12),nrow=4)) )
> 
> If I do the following
> 
> d[lower.tri(d)] <- ""
> 
> of course everything is changed to character - that's not 
> what should be.
> Additionally, it does not work to assign "0" or anything 
> else. The same is true for assigning "NA".
> 
> However, what I want is like the following:
> 
> ( dist(matrix(runif(12),nrow=4)) )
> 
> Looking into dist(), it seems that the calculation and the 
> matrix are done in C and not in plain R.
> 
> How can I realize it?
> 
> thanks!
> 
> best,
> 
> leo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From christos at nuverabio.com  Wed Feb  7 20:20:18 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 7 Feb 2007 14:20:18 -0500
Subject: [R] blank upper or lower triangle of cor-matrix
References: <45CA1C08.2000908@anicca-vijja.de> 
Message-ID: <005701c74aed$01709b10$0e010a0a@headquarters.silicoinsights>

And if you want to know how it is done, take a look at 

stats:::print.dist

-Christos 

> -----Original Message-----
> From: Christos Hatzis [mailto:christos at nuverabio.com] 
> Sent: Wednesday, February 07, 2007 2:16 PM
> To: 'Leo G?rtler'; 'r-help at stat.math.ethz.ch'
> Subject: RE: [R] blank upper or lower triangle of cor-matrix
> 
> You can try
> 
> as.dist(d)
> 
> 
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leo G?rtler
> > Sent: Wednesday, February 07, 2007 1:36 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] blank upper or lower triangle of cor-matrix
> > 
> > Dear altogether,
> > 
> > I want to blank the lower (or upper) part of a correlation 
> matrix as 
> > it is done by dist()
> > 
> > example:
> > 
> > ( d <- cor(matrix(runif(12),nrow=4)) )
> > 
> > If I do the following
> > 
> > d[lower.tri(d)] <- ""
> > 
> > of course everything is changed to character - that's not 
> what should 
> > be.
> > Additionally, it does not work to assign "0" or anything else. The 
> > same is true for assigning "NA".
> > 
> > However, what I want is like the following:
> > 
> > ( dist(matrix(runif(12),nrow=4)) )
> > 
> > Looking into dist(), it seems that the calculation and the 
> matrix are 
> > done in C and not in plain R.
> > 
> > How can I realize it?
> > 
> > thanks!
> > 
> > best,
> > 
> > leo
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> >


From philip.leifeld at uni-konstanz.de  Wed Feb  7 20:20:13 2007
From: philip.leifeld at uni-konstanz.de (Philip Leifeld)
Date: Wed, 07 Feb 2007 20:20:13 +0100
Subject: [R] isoMDS - high stress value and strange configuration
Message-ID: <45CA347D.29005.869EF3@philip.leifeld.uni-konstanz.de>

Dear R users,

I have a specific question about isoMDS. Imagine the following (fake) 
distance table:

        hamburg bremen berlin munich cologne
hamburg       0    911    982    677     424
bremen      911      0    293    547     513
berlin      982    293      0    785     875
munich      677    547    785      0     375
cologne     424    513    875    375       0

Now if I try a non-metric multidimensional scaling on these 
dissimilarities using isoMDS (or metaMDS), the stress value is 6.34. 
Nevertheless, other programs (e.g. the Minissa routine implemented in 
UCINet) yield a stress value of 0.00, and the configuration looks 
completely different. I tried this with multiple distance matrices: 
One time UCINet computed a stress value of 0.21 while isoMDS produced 
a stress of 0.33, and again the configuration was completely 
different and apparently random (while the configuration in UCINet 
still made sense). Here is what I tried:

isoMDS(cities, y = cmdscale(cities, k = 2), k = 2, maxit = 50)

Please give me a hint on how to improve the results. I suppose the 
above command is not complete, or something is wrong with it, or 
maybe the input distances are not in the right format.

Btw, the problem does not occur when I use the real distances between 
these cities, not some other numbers, so apparently three-digit 
numbers should be fine as input values?

Thanks!

Phil


From kkthird at yahoo.com  Wed Feb  7 20:50:28 2007
From: kkthird at yahoo.com (KKThird@Yahoo.Com)
Date: Wed, 7 Feb 2007 11:50:28 -0800 (PST)
Subject: [R] Plotting groupedData objects
Message-ID: <452035.79445.qm@web52512.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/1f231b69/attachment.pl 

From maechler at stat.math.ethz.ch  Wed Feb  7 21:02:02 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 7 Feb 2007 21:02:02 +0100
Subject: [R] XML and str
In-Reply-To: <1170868736.27202.159.camel@fordpc.signal.qinetiq.com>
References: <1170868736.27202.159.camel@fordpc.signal.qinetiq.com>
Message-ID: <17866.12346.900084.986303@stat.math.ethz.ch>

>>>>> "Ashley" == Ashley Ford <ford at signal.QinetiQ.com>
>>>>>     on Wed, 07 Feb 2007 17:18:56 +0000 writes:

    Ashley> If I read in an .xml file eg with 

    >> xeg <- xmlTreeParse(system.file("exampleData", "test.xml",
                                       package="XML"))

    Ashley> It appears to be OK however examining it with str() gives an apparent
    Ashley> error

    >> str(xeg, 2)
    Ashley> List of 2
    Ashley> $ doc:List of 3
    Ashley> ..$ file    : list()
    Ashley> .. ..- attr(*, "class")= chr [1:2] "XMLComment" "XMLNode"
    Ashley> ..$ version :List of 4
    Ashley> .. ..- attr(*, "class")= chr "XMLNode"
    Ashley> ..$ children:Error in obj$children[[...]] : subscript out of bounds

    Ashley> I am unsure if this is a feature or a bug and if the latter whether it
    Ashley> is in XML or str, it is not causing a problem but I would like to
    Ashley> understand what is happening, any ideas ?

Yes -  thank you for providing a well-reproducible example.
After setting  
      options(error = recover)

I do

   > obj <- xeg$doc
   > mode(obj)     # "list"
   [1] "list"
   > is.list(obj)  # TRUE
   [1] TRUE
   > length(obj)   # 3
   [1] 3
   > obj[[3]]      # ---> the error you see above.
   Error in obj$children[[...]] : subscript out of bounds

   Enter a frame number, or 0 to exit   

   1: obj[[3]]
   2: `[[.XMLDocumentContent`(obj, 3)

   Selection: 0

   > obj$children  # works, should be identical to obj[[3]]
   $comment
   <!--A comment-->

   $foo
   <foo x="1">
    <element attrib1="my value"/>
   ......

This shows that the XML package implements the "[[" method
wrongly IMHO and also inconsistently with the "$" method.

>From a strict OOP view, the XML author could argue that
this is not a bug in XML but rather str() which assumes that
x[[length(x)]] works for objects of mode "list" even when they
are not of *class* "list", but I hope he would still rather
consider changing [[.XMLDocumentContent ...

Martin

    Ashley> examining components eg 
    >> str(xeg$doc$children,2)

    Ashley> List of 2
    Ashley> $ comment: list()
    Ashley> ..- attr(*, "class")= chr [1:2] "XMLComment" "XMLNode"
    Ashley> etc 

    Ashley> is OK.

    Ashley> XML Version 1.4-1, 
    Ashley> same behaviour on Windows and Linux, R version 2.4.1 (2006-12-18)




    Ashley> The information contained in this E-Mail and any subsequent
    Ashley> correspondence is private and is intended solely for the intended
    Ashley> recipient(s).  The information in this communication may be confidential
    Ashley> and/or legally privileged.  Nothing in this e-mail is intended to
    Ashley> conclude a contract on behalf of QinetiQ or make QinetiQ subject to any
    Ashley> other legally binding commitments, unless the e-mail contains an express
    Ashley> statement to the contrary or incorporates a formal Purchase Order.

    Ashley> For those other than the recipient any disclosure, copying,
    Ashley> distribution, or any action taken or omitted to be taken in reliance on
    Ashley> such information is prohibited and may be unlawful.

    Ashley> Emails and other electronic communication with QinetiQ may be monitored
    Ashley> and recorded for business purposes including security, audit and
    Ashley> archival purposes.  Any response to this email indicates consent to
    Ashley> this.

    Ashley> Telephone calls to QinetiQ may be monitored or recorded for quality
    Ashley> control, security and other business purposes.

    Ashley> QinetiQ Group plc,

    Ashley> Company Registration No: 4586941,  

    Ashley> Registered office: 85 Buckingham Gate, London SW1E 6PD

    Ashley> ______________________________________________
    Ashley> R-help at stat.math.ethz.ch mailing list
    Ashley> https://stat.ethz.ch/mailman/listinfo/r-help
    Ashley> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    Ashley> and provide commented, minimal, self-contained, reproducible code.


From Flom at ndri.org  Wed Feb  7 21:10:03 2007
From: Flom at ndri.org (Peter Flom)
Date: Wed, 07 Feb 2007 15:10:03 -0500
Subject: [R] Problem with subsets and xyplot
Message-ID: <45C9EBCA.B875.00C9.0@ndri.org>

Hello

I have a dataframe that looks like this

     MSA                          CITY HIVEST YEAR   YR CAT
1   0200  Albuquerque                     0.50 1996 1996   5
2   0520  Atlanta                        13.00 1997 1997   5
3   0720  Baltimore                      29.10 1994 1994   1
4   0720  Baltimore                      13.00 1995 1995   5
5   0720  Baltimore                       3.68 1996 1996   3
6   0720  Baltimore                       9.00 1997 1997   5
7   0720  Baltimore                      11.00 1998 1998   5
8   0875  Bergen-Passaic                 51.80 1990 1990   5


many more rows....

I would like to create some xyplots, but separately for MSAs that are
high, moderate or low on HIVEST.  Here's what I tried

#### READ IN DATA AND RECODE SOME VARIABLES
attach(hivest)

cat <- CAT
cat[cat > 5] <- 6


msa <- as.numeric(MSA)
msa[msa == 7361] <- 7360
msa[msa == 7362] <- 7360
msa[msa == 7363] <- 7360

msa[msa == 5601] <- 5600
msa[msa == 5602] <- 5600

msa[msa == 6484] <- 6483


####   FIND MEANS FOR EACH MSA, FOR SUBSETTING LATER
meanbymsa <- aggregate(HIVEST, by = list(msa), FUN = mean, na.rm = T)

#### meanbymsa[,2] gives me the column I want; the 25%tile of this
column is about 3.1.

but when I try

plot1 <- xyplot(HIVEST~YEAR|as.factor(msa),  pch = LETTERS[cat], subset
= (meanbymsa[,2] < 3.1))
plot1


I don't get what I expect.  No errors, and it is a subset, but the
subset is NOT MSAs with low values of HIVEST.


Any help appreciated.


Peter




Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
http://cduhr.ndri.org
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)


From gunter.berton at gene.com  Wed Feb  7 22:03:02 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 7 Feb 2007 13:03:02 -0800
Subject: [R] Problem with subsets and xyplot
In-Reply-To: <45C9EBCA.B875.00C9.0@ndri.org>
Message-ID: <004f01c74afb$5b21b190$4d908980@gne.windows.gene.com>

?aggregate says:

"... the result is reformatted into a data frame containing the variables in
by and x. The ones arising from by contain the unique combinations of
grouping values used for determining the subsets, and the ones arising from
x the corresponding summary statistics for the subset of the respective
variables in x. "

so meansbymsa does not have the same number of rows as your original data
frame, which it must for subsetting to work properly (meansbymsa[,2] was
recycled to be of the right length by default, which produces the nonsense
you got. See ?xyplot)


Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Flom
Sent: Wednesday, February 07, 2007 12:10 PM
To: r-help at r-project.org
Subject: [R] Problem with subsets and xyplot

Hello

I have a dataframe that looks like this

     MSA                          CITY HIVEST YEAR   YR CAT
1   0200  Albuquerque                     0.50 1996 1996   5
2   0520  Atlanta                        13.00 1997 1997   5
3   0720  Baltimore                      29.10 1994 1994   1
4   0720  Baltimore                      13.00 1995 1995   5
5   0720  Baltimore                       3.68 1996 1996   3
6   0720  Baltimore                       9.00 1997 1997   5
7   0720  Baltimore                      11.00 1998 1998   5
8   0875  Bergen-Passaic                 51.80 1990 1990   5


many more rows....

I would like to create some xyplots, but separately for MSAs that are
high, moderate or low on HIVEST.  Here's what I tried

#### READ IN DATA AND RECODE SOME VARIABLES
attach(hivest)

cat <- CAT
cat[cat > 5] <- 6


msa <- as.numeric(MSA)
msa[msa == 7361] <- 7360
msa[msa == 7362] <- 7360
msa[msa == 7363] <- 7360

msa[msa == 5601] <- 5600
msa[msa == 5602] <- 5600

msa[msa == 6484] <- 6483


####   FIND MEANS FOR EACH MSA, FOR SUBSETTING LATER
meanbymsa <- aggregate(HIVEST, by = list(msa), FUN = mean, na.rm = T)

#### meanbymsa[,2] gives me the column I want; the 25%tile of this
column is about 3.1.

but when I try

plot1 <- xyplot(HIVEST~YEAR|as.factor(msa),  pch = LETTERS[cat], subset
= (meanbymsa[,2] < 3.1))
plot1


I don't get what I expect.  No errors, and it is a subset, but the
subset is NOT MSAs with low values of HIVEST.


Any help appreciated.


Peter




Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
http://cduhr.ndri.org
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From robert.ptacnik at niva.no  Wed Feb  7 21:52:06 2007
From: robert.ptacnik at niva.no (robert.ptacnik at niva.no)
Date: Wed, 7 Feb 2007 21:52:06 +0100
Subject: [R] heteroscedasticity problem
Message-ID: <OFC35CD7ED.1647DA21-ONC125727A.00451E9B-C125727B.0072A233@niva.no>






Dear Listers,

I have a regression problem (x->y) with biological data, where x influences
y in two ways, (1) y increases with x and (2) the variation around the mean
(residuals) decreases with increasing x, i.e. y becomes more 'predictable'
as x increases.
The relationship is saturating, y~a + bx + cx^2, gives a very good fit.

I know basically how to test for heteroscedasticity. My question is if
there is an elegant regression method, which captures both, the mean and
the (non-constant) variation around the mean. Such a method would ideally
yield an estimate of the mean and its variation, both as a function of x.

The pattern corresponds very well to some established ecological theory
(each x is the species richness of a community of primary producers, y is
the productivity of each community; productivity and its predictability
both increase with increasing species richness).

Apologies for the probably clumsy decription of my problem - I am
ecologist, not statistician (but a big fan of R).

Cheers,
Robert


Robert Ptacnik
Norwegian Institute for Water Research (NIVA)
Gaustadall?en 21
NO-0349 Oslo
 FON +47 982 277 81
FAX +47 221 852 00


From rkoenker at uiuc.edu  Wed Feb  7 22:39:38 2007
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 7 Feb 2007 15:39:38 -0600
Subject: [R] heteroscedasticity problem
In-Reply-To: <OFC35CD7ED.1647DA21-ONC125727A.00451E9B-C125727B.0072A233@niva.no>
References: <OFC35CD7ED.1647DA21-ONC125727A.00451E9B-C125727B.0072A233@niva.no>
Message-ID: <9D791E6B-1B21-466B-850D-649106D4F1A0@uiuc.edu>

If you haven't already you might want to take a look at:

	http://www.econ.uiuc.edu/~roger/research/rq/QReco.pdf

which is written by and for ecologists.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Feb 7, 2007, at 2:52 PM, robert.ptacnik at niva.no wrote:

>
>
>
>
>
> Dear Listers,
>
> I have a regression problem (x->y) with biological data, where x  
> influences
> y in two ways, (1) y increases with x and (2) the variation around  
> the mean
> (residuals) decreases with increasing x, i.e. y becomes more  
> 'predictable'
> as x increases.
> The relationship is saturating, y~a + bx + cx^2, gives a very good  
> fit.
>
> I know basically how to test for heteroscedasticity. My question is if
> there is an elegant regression method, which captures both, the  
> mean and
> the (non-constant) variation around the mean. Such a method would  
> ideally
> yield an estimate of the mean and its variation, both as a function  
> of x.
>
> The pattern corresponds very well to some established ecological  
> theory
> (each x is the species richness of a community of primary  
> producers, y is
> the productivity of each community; productivity and its  
> predictability
> both increase with increasing species richness).
>
> Apologies for the probably clumsy decription of my problem - I am
> ecologist, not statistician (but a big fan of R).
>
> Cheers,
> Robert
>
>
> Robert Ptacnik
> Norwegian Institute for Water Research (NIVA)
> Gaustadall?en 21
> NO-0349 Oslo
>  FON +47 982 277 81
> FAX +47 221 852 00
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at yahoo.ca  Wed Feb  7 23:08:52 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 7 Feb 2007 17:08:52 -0500 (EST)
Subject: [R] setting a number of values to NA over  a data.frame.
Message-ID: <27739.27117.qm@web32804.mail.mud.yahoo.com>

This is probably a simple problem but I don't see a
solution.

I have a data.frame with a number of columns where I
would like 0 <- NA

thus I have df1[,144:157] <- NA if df1[, 144: 157] ==0
and df1[, 190:198] <- NA if df1[, 190:198] ==0

but I cannot figure out a way do this.  

cata <- c( 1,1,6,1,1,NA)
catb <- c( 1,2,3,4,5,6)
doga <- c(3,5,3,6,4, 0)
dogb <- c(2,4,6,8,10, 12)
rata <- c (NA, 9, 9, 8, 9, 8)
ratb <- c( 1,2,3,4,5,6)
bata <- c( 12, 42,NA, 45, 32, 54)
batb <- c( 13, 15, 17,19,21,23)
id <- c('a', 'b', 'b', 'c', 'a', 'b')
site <- c(1,1,4,4,1,4)
mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
bata, batb)

data1 <- data.frame(site, id, mat1)
data1

 # Obviously this works fine for one column

data1$site[data1$site ==1] <- NA  ; data1

but I cannot see how to do this with indices that
would allow me to do more than one column in the
data.frame.

At one point I even tried something like this
a <- c("site")
data1$a[data1$a ==1] <- NA

which seems to produce a corrupt data.frame.

I am sure it is simple but I don't see it.  

Any help would be much appreciated.


From cadeb at usgs.gov  Wed Feb  7 23:24:16 2007
From: cadeb at usgs.gov (Brian S Cade)
Date: Wed, 7 Feb 2007 15:24:16 -0700
Subject: [R] heteroscedasticity problem
In-Reply-To: <9D791E6B-1B21-466B-850D-649106D4F1A0@uiuc.edu>
Message-ID: <OF29A434C9.F65455E8-ON8725727B.007A41ED-8725727B.007B5C8E@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/548fb8db/attachment.pl 

From iverson at biostat.wisc.edu  Wed Feb  7 23:57:40 2007
From: iverson at biostat.wisc.edu (Erik Iverson)
Date: Wed, 07 Feb 2007 16:57:40 -0600
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <27739.27117.qm@web32804.mail.mud.yahoo.com>
References: <27739.27117.qm@web32804.mail.mud.yahoo.com>
Message-ID: <45CA5964.1080803@biostat.wisc.edu>

John -

Your initial problem uses 0, but the example uses 1 for the value that 
gets an NA.  My solution uses 1 to fit with your example.  There may be 
a better way, but try something like

data1[3:5] <- data.frame(lapply(data1[3:5], function(x) ifelse(x==1, NA, 
x)))

The data1[3:5] is just a test subset  of columns I chose from your data1 
example.  Notice it appears twice, once on each side of the assignment 
operator.

In English, apply to each column of the data frame (which is a list) a 
function that will return NA if the element is 1, and the value 
otherwise, and then turn the modified lists into a data.frame, and save 
it as data1.



See the help files for lapply and ifelse if you haven't seen those before.

Maybe someone has a better way?

Erik

John Kane wrote:
> This is probably a simple problem but I don't see a
> solution.
> 
> I have a data.frame with a number of columns where I
> would like 0 <- NA
> 
> thus I have df1[,144:157] <- NA if df1[, 144: 157] ==0
> and df1[, 190:198] <- NA if df1[, 190:198] ==0
> 
> but I cannot figure out a way do this.  
> 
> cata <- c( 1,1,6,1,1,NA)
> catb <- c( 1,2,3,4,5,6)
> doga <- c(3,5,3,6,4, 0)
> dogb <- c(2,4,6,8,10, 12)
> rata <- c (NA, 9, 9, 8, 9, 8)
> ratb <- c( 1,2,3,4,5,6)
> bata <- c( 12, 42,NA, 45, 32, 54)
> batb <- c( 13, 15, 17,19,21,23)
> id <- c('a', 'b', 'b', 'c', 'a', 'b')
> site <- c(1,1,4,4,1,4)
> mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> bata, batb)
> 
> data1 <- data.frame(site, id, mat1)
> data1
> 
>  # Obviously this works fine for one column
> 
> data1$site[data1$site ==1] <- NA  ; data1
> 
> but I cannot see how to do this with indices that
> would allow me to do more than one column in the
> data.frame.
> 
> At one point I even tried something like this
> a <- c("site")
> data1$a[data1$a ==1] <- NA
> 
> which seems to produce a corrupt data.frame.
> 
> I am sure it is simple but I don't see it.  
> 
> Any help would be much appreciated.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bates at stat.wisc.edu  Thu Feb  8 00:24:40 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 7 Feb 2007 17:24:40 -0600
Subject: [R] Singular Gradient
In-Reply-To: <285191030702071113p201bdf5dra688d64e05220249@mail.gmail.com>
References: <285191030702071113p201bdf5dra688d64e05220249@mail.gmail.com>
Message-ID: <40e66e0b0702071524o51c7746fk6dde12cf35015670@mail.gmail.com>

On 2/7/07, This Wiederkehr <this.wiederkehr at gmail.com> wrote:
> I tried to fit data with the following function:
>
> fit<-nls(y~ Is*(1-exp(-l*x))+Iph,start=list(Is=-2e-5,l=2.3,Iph=-0.3
> ),control=list(maxiter=500,minFactor=1/10000,tol=10e-05),trace=TRUE)
> But I get only a singular Gradient warning...

Did you get any trace output at all?  It is not clear if you got the
singular gradient warning before the first iteration completed, which
means there is a problem at the starting estimates, or after a few
iterations.  Without the data it is difficult to decide.

> the data can by found attached(there are two sampels of data col 1/2 and
> 3/4).

Thanks for offering to include the data.  My copy of your message did
not have the data enclosed.  Did you perhaps forget to attach the
file?

 > I tried to fix it by chanching the start parameters but that didn't solve
> the problem.

> Would it be a possibiliti to use the selfstart Model? How?

Yes.  Try SSasymp.  I believe that model is equivalent to your model
but in a different parameterization.


> Thanks for any answers
>
> This
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From jrkrideau at yahoo.ca  Thu Feb  8 01:06:36 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 7 Feb 2007 19:06:36 -0500 (EST)
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <45CA5964.1080803@biostat.wisc.edu>
Message-ID: <64382.84831.qm@web32814.mail.mud.yahoo.com>

Works beautifully. I modified it a bit to handle the
discontinous ranges to:  

a <- c(3:4, 8)

data1[a] <- data.frame(lapply(data1[a], function(x)
ifelse(x==1,
NA,
x)))

There may be a prettier way to handle the disconituity
but this works so it looks like I'm in good shape.  

I had looked at ifelse and at apply (not lapply) but
did not think to put them together.  

Thanks 
Thanks very muc


--- Erik Iverson <iverson at biostat.wisc.edu> wrote:

> John -
> 
> Your initial problem uses 0, but the example uses 1
> for the value that 
> gets an NA.  My solution uses 1 to fit with your
> example.  There may be 
> a better way, but try something like
> 
> data1[3:5] <- data.frame(lapply(data1[3:5],
> function(x) ifelse(x==1, NA, 
> x)))
> 
> The data1[3:5] is just a test subset  of columns I
> chose from your data1 
> example.  Notice it appears twice, once on each side
> of the assignment 
> operator.
> 
> In English, apply to each column of the data frame
> (which is a list) a 
> function that will return NA if the element is 1,
> and the value 
> otherwise, and then turn the modified lists into a
> data.frame, and save 
> it as data1.
> 
> 
> 
> See the help files for lapply and ifelse if you
> haven't seen those before.
> 
> Maybe someone has a better way?
> 
> Erik
> 
> John Kane wrote:
> > This is probably a simple problem but I don't see
> a
> > solution.
> > 
> > I have a data.frame with a number of columns where
> I
> > would like 0 <- NA
> > 
> > thus I have df1[,144:157] <- NA if df1[, 144: 157]
> ==0
> > and df1[, 190:198] <- NA if df1[, 190:198] ==0
> > 
> > but I cannot figure out a way do this.  
> > 
> > cata <- c( 1,1,6,1,1,NA)
> > catb <- c( 1,2,3,4,5,6)
> > doga <- c(3,5,3,6,4, 0)
> > dogb <- c(2,4,6,8,10, 12)
> > rata <- c (NA, 9, 9, 8, 9, 8)
> > ratb <- c( 1,2,3,4,5,6)
> > bata <- c( 12, 42,NA, 45, 32, 54)
> > batb <- c( 13, 15, 17,19,21,23)
> > id <- c('a', 'b', 'b', 'c', 'a', 'b')
> > site <- c(1,1,4,4,1,4)
> > mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> > bata, batb)
> > 
> > data1 <- data.frame(site, id, mat1)
> > data1
> > 
> >  # Obviously this works fine for one column
> > 
> > data1$site[data1$site ==1] <- NA  ; data1
> > 
> > but I cannot see how to do this with indices that
> > would allow me to do more than one column in the
> > data.frame.
> > 
> > At one point I even tried something like this
> > a <- c("site")
> > data1$a[data1$a ==1] <- NA
> > 
> > which seems to produce a corrupt data.frame.
> > 
> > I am sure it is simple but I don't see it.  
> > 
> > Any help would be much appreciated.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
>


From colliera at ukzn.ac.za  Thu Feb  8 08:10:04 2007
From: colliera at ukzn.ac.za (colliera at ukzn.ac.za)
Date: Thu, 8 Feb 2007 09:10:04 +0200
Subject: [R] path for source()
Message-ID: <20070208071004.GS10041@humboldt>

hello,

i have a couple of .R files distributed about my file system. i commonly source() these from other files, but i have to include the full file path. this is not always convenient if you move files around. is there a way of setting the search path for source()?

thanks a lot!

cheers,
andrew.

-- 
Andrew B. Collier

Space Physics Group
Hermanus Magnetic Observatory

Honorary Research Fellow                                    tel: +27 31 2601157
Space Physics Research Institute                            fax: +27 31 2616550
University of KwaZulu-Natal, Durban, 4041, South Africa     gsm: +27 83 3813655


From jarioksa at sun3.oulu.fi  Thu Feb  8 08:51:43 2007
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Thu, 08 Feb 2007 09:51:43 +0200
Subject: [R]  isoMDS - high stress value and strange configuration
Message-ID: <1170921104.485.6.camel@biol102145.oulu.fi>


> I have a specific question about isoMDS. Imagine the following (fake) 
> distance table:
> 
>         hamburg bremen berlin munich cologne
> hamburg       0    911    982    677     424
> bremen      911      0    293    547     513
> berlin      982    293      0    785     875
> munich      677    547    785      0     375
> cologne     424    513    875    375       0
> 
> Now if I try a non-metric multidimensional scaling on these 
> dissimilarities using isoMDS (or metaMDS), the stress value is 6.34. 
> Nevertheless, other programs (e.g. the Minissa routine implemented in 
> UCINet) yield a stress value of 0.00, and the configuration looks 
> completely different. 

This indeed seems to be a case where NMDS is trapped in its starting
configuration. Metric scaling (cmdscale) produces a cute "horseshoe",
but the best NMDS solutions looks completely different. Any small change
from the initial solution leads into a worse configuration, and you need
a bigger change in the beginning. Using a random configuration seems to
help:

> isoMDS(dis, initMDS(dis))
initial  value 36.383132 
iter   5 value 28.671652
iter  10 value 16.711327
iter  15 value 6.392572
iter  20 value 3.007208
final  value 0.000000 
converged
$points
              [,1]      [,2]
hamburg  29.428121 -36.07858
bremen    2.740499  32.38745
berlin    1.984215  35.35429
munich  -16.910941 -14.13750
cologne -13.844187 -15.24468

$stress
[1] 1.56159e-14

In this case I generated the random configuration using function initMDS
of vegan, but you can do that quite well by any other way.

Another point (which does not matter here so much) is that isoMDS
multiplies stress by 100, so that your stress of 6 would corresponde
0.06 in some other software (assuming they use the same stress).

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>


From felwert at uni-bremen.de  Wed Feb  7 22:56:39 2007
From: felwert at uni-bremen.de (Frederik Elwert)
Date: Wed, 07 Feb 2007 22:56:39 +0100
Subject: [R] Icon proposal
Message-ID: <1170885399.5955.48.camel@localhost.localdomain>

Hello!

In my eyes, the R logo looks somewhat old-fashioned and I don't like
that 3D-ish style. Therefor, I played a bit around with Inkscape and
made an R logo according to the Tango Icon Theme Guidelines
<http://tango.freedesktop.org/Tango_Icon_Theme_Guidelines>. It is in SVG
format, and therefor scalable, but optimized for 48x48 and larger.
Smaller sizes might need some retouching to stay sharp.

Not everybody might feel about the R logo as I do, so this is in no way
an attempt to replace the current logo. But maybe somebody likes the
look of my new logo and wants to use it as an icon or something.

The SVG file:
http://www-user.uni-bremen.de/~felwert/R/R.svg

The logo in about the size of the R's website's logo:
http://www-user.uni-bremen.de/~felwert/R/Rlogo-neu.png

The logo as 48x48 icon:
http://www-user.uni-bremen.de/~felwert/R/scientific-r-48.png

The files themselves are Public Domain, the logo is property of the R
Foundation, I assume - but I couldn't find anything specific on the web
site, so I welcome information on that topic.

I'd like to hear what you think about the logo and if it is of any use
for you.

Regards,
Frederik


From fengfeng99 at gmail.com  Thu Feb  8 06:08:31 2007
From: fengfeng99 at gmail.com (fengfeng)
Date: Wed, 7 Feb 2007 23:08:31 -0600
Subject: [R] the plotting position of theoretical quantile for qqnorm
Message-ID: <481f56810702072108k37a9e7e1u7ff1168f821d818b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070207/0118fad8/attachment.pl 

From mini_ghosh at yahoo.co.in  Thu Feb  8 06:15:57 2007
From: mini_ghosh at yahoo.co.in (MINI GHOSH)
Date: Thu, 8 Feb 2007 05:15:57 +0000 (GMT)
Subject: [R] circle fill problem
Message-ID: <169357.86689.qm@web7904.mail.in.yahoo.com>

Dear R user,

I want to know is there a way to find the minimum
number of circles (of given radius) required to fill a
given area (say rectangular) where overlapping of
circles is allowed.

Thanks,
Regards,
Mini Ghosh


From maechler at stat.math.ethz.ch  Thu Feb  8 09:39:30 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 8 Feb 2007 09:39:30 +0100
Subject: [R] Attachments to R-help postings (Re:  Singular Gradient)
In-Reply-To: <40e66e0b0702071524o51c7746fk6dde12cf35015670@mail.gmail.com>
References: <285191030702071113p201bdf5dra688d64e05220249@mail.gmail.com>
	<40e66e0b0702071524o51c7746fk6dde12cf35015670@mail.gmail.com>
Message-ID: <17866.57794.343878.76952@stat.math.ethz.ch>

>>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
>>>>>     on Wed, 7 Feb 2007 17:24:40 -0600 writes:

    DB> On 2/7/07, This Wiederkehr <this.wiederkehr at gmail.com> wrote:
    >> I tried to fit data with the following function:
    >> 
    >> fit<-nls(y~ Is*(1-exp(-l*x))+Iph,start=list(Is=-2e-5,l=2.3,Iph=-0.3
    >> ),control=list(maxiter=500,minFactor=1/10000,tol=10e-05),trace=TRUE)
    >> But I get only a singular Gradient warning...

    DB> Did you get any trace output at all?  It is not clear if you got the
    DB> singular gradient warning before the first iteration completed, which
    DB> means there is a problem at the starting estimates, or after a few
    DB> iterations.  Without the data it is difficult to decide.

    >> the data can by found attached(there are two sampels of data col 1/2 and
    >> 3/4).

    DB> Thanks for offering to include the data.  My copy of your message did
    DB> not have the data enclosed.  Did you perhaps forget to attach the
    DB> file?

More probably he did not attach them with mime type "text/plain".
Many e-mail clients nowadays attach everything and notably text
as unspecified binary ("application/octet-stream").
For security (and anti-spam) reasons, such attachments are
eliminated from postings.

I've now slightly modified this content-filtering option for
R-help, such that (I think) such e-mails will be *rejected* instead 
of just the attachment removed -- I'm just trying that now,
attaching a 2 line text file

-------------- next part --------------

Martin





    >> I tried to fix it by chanching the start parameters but that didn't solve
    >> the problem.

    >> Would it be a possibiliti to use the selfstart Model? How?

    DB> Yes.  Try SSasymp.  I believe that model is equivalent to your model
    DB> but in a different parameterization.


From wl2776 at gmail.com  Thu Feb  8 09:45:51 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Thu, 8 Feb 2007 00:45:51 -0800 (PST)
Subject: [R] path for source()
In-Reply-To: <20070208071004.GS10041@humboldt>
References: <20070208071004.GS10041@humboldt>
Message-ID: <8861457.post@talk.nabble.com>


Here is the discussion about the function search.path()
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/34411.html
which searches the PATH variable for your script and returns the full path.

You also can put your script contents in functions, then create a package
and load it with library(). 
package.skeleton() can help you.

Anyway, source()'ing every time does not seem to be a good idea, because
every time you do this you obtain one more copy of your functions and
variables in the newly created workspace.
If you don't want to create a package, you can save your functions in a file
with the .Rdata extension and attach that file.

Try going here
https://www.stat.math.ethz.ch/pipermail/r-help/2004-July/thread.html and
searching "source()" in the page (i.e. in message headers). I believe,
discussions could provide you with several more good ideas.


colliera wrote:
> 
> i have a couple of .R files distributed about my file system. i commonly
> source() these from other files, but i have to include the full file path.
> this is not always convenient if you move files around. is there a way of
> setting the search path for source()?
> 

-- 
View this message in context: http://www.nabble.com/-R--path-for-source%28%29-tf3191709.html#a8861457
Sent from the R help mailing list archive at Nabble.com.


From wraff at titus.u-strasbg.fr  Thu Feb  8 09:50:49 2007
From: wraff at titus.u-strasbg.fr (Wolfgang Raffelsberger)
Date: Thu, 08 Feb 2007 09:50:49 +0100
Subject: [R] path for source()
In-Reply-To: <20070208071004.GS10041@humboldt>
References: <20070208071004.GS10041@humboldt>
Message-ID: <45CAE469.9080301@igbmc.u-strasbg.fr>

Hi,
an easy way to address this is to change directory within R before 
calling source() :
setwd("D:/Projects/yourProject")
source("yourCode.R")

Of course you need to know where your .R files are.
Using getwd() you can always check where you are and using dir() you can 
check the files in your directory (which you could combine with grep() 
to search for ".R") ..

Wolfgang

colliera at ukzn.ac.za a ?crit :
> hello,
>
> i have a couple of .R files distributed about my file system. i commonly source() these from other files, but i have to include the full file path. this is not always convenient if you move files around. is there a way of setting the search path for source()?
>
> thanks a lot!
>
> cheers,
> andrew.
>
>   


-- 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . .

Wolfgang Raffelsberger, PhD
Laboratoire de BioInformatique et G?nomique Int?grative
IGBMC
1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
Tel (+33) 388 65 3300         Fax (+33) 388 65 3276
wolfgang.raffelsberger at igbmc.u-strasbg.fr


From r.hankin at noc.soton.ac.uk  Thu Feb  8 09:52:48 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 8 Feb 2007 08:52:48 +0000
Subject: [R] circle fill problem
In-Reply-To: <169357.86689.qm@web7904.mail.in.yahoo.com>
References: <169357.86689.qm@web7904.mail.in.yahoo.com>
Message-ID: <D7D2C1D3-8FE6-435D-9B8C-8BA3F4DC5594@soc.soton.ac.uk>

Mini

This is a hard problem in general.

Recreational mathematics has wrestled with
this and similar problems over the years; the
general field is the "set cover problem" but
in your case the sets are uncountably infinite
(and there are uncountably many of them).

I would be surprised if your problem were not NP complete.


HTH


Robin


On 8 Feb 2007, at 05:15, MINI GHOSH wrote:

> Dear R user,
>
> I want to know is there a way to find the minimum
> number of circles (of given radius) required to fill a
> given area (say rectangular) where overlapping of
> circles is allowed.
>
> Thanks,
> Regards,
> Mini Ghosh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From maechler at stat.math.ethz.ch  Thu Feb  8 09:57:48 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 8 Feb 2007 09:57:48 +0100
Subject: [R] the plotting position of theoretical quantile for qqnorm
In-Reply-To: <481f56810702072108k37a9e7e1u7ff1168f821d818b@mail.gmail.com>
References: <481f56810702072108k37a9e7e1u7ff1168f821d818b@mail.gmail.com>
Message-ID: <17866.58892.968007.975127@stat.math.ethz.ch>

>>>>> "LizF" == fengfeng  <fengfeng99 at gmail.com>
>>>>>     on Wed, 7 Feb 2007 23:08:31 -0600 writes:

    LizF> Hello,
    LizF> I have a doubt about the plotting position of the theoretical quantile for
    LizF> the qqnorm
    LizF> command in R.

    LizF> Let F be the theoretical distribution of Y, we observed a sample of size n,
    LizF> y1,y2, ...,
    LizF> yn. We then sort it and comspare these empirical quantiles to the expected
    LizF> ones
    LizF> from F. For the plotting poition, there are several options:

    LizF> 1. i/(n+1)
    LizF> 2. (i-.375)/(n+.25)
    LizF> 3. (i- .3175)/ (n + .365)
    LizF> etc.

yes, particularly "etc"  ;-)

    LizF> Which one is "qqnorm" used?

It's right in front of you if you read  help(qqnorm)  carefully :

  >> See Also:
  >> 
  >>    'ppoints', used by 'qqnorm' to generate approximations to expected
  >>    order statistics for a normal distribution.
  >> 

So it uses ppoints()  and  help(ppoints) tells you what's going
on: The formula used depends on (n <= 10) but see that help page.

    LizF> Thx a lot!

You're welcome,
Martin Maechler, ETH Zurich

    LizF> Liz


From i.visser at uva.nl  Thu Feb  8 10:21:23 2007
From: i.visser at uva.nl (Ingmar Visser)
Date: Thu, 8 Feb 2007 10:21:23 +0100
Subject: [R] circle fill problem
In-Reply-To: <D7D2C1D3-8FE6-435D-9B8C-8BA3F4DC5594@soc.soton.ac.uk>
References: <169357.86689.qm@web7904.mail.in.yahoo.com>
	<D7D2C1D3-8FE6-435D-9B8C-8BA3F4DC5594@soc.soton.ac.uk>
Message-ID: <550A9A50-DDD1-4B76-A740-9B1EFE72DC8F@uva.nl>

Robin & Mini,
For those interested, googling for the 'orange packing problem' as it  
is known, or more officially the sphere packing problems gives you  
quite a few hits on these and similar problems.
So at least the 3-d case the problem has been solved (I imagine the  
problem is easier in 2-d ...)
hth, Ingmar

On 8 Feb 2007, at 09:52, Robin Hankin wrote:

> Mini
>
> This is a hard problem in general.
>
> Recreational mathematics has wrestled with
> this and similar problems over the years; the
> general field is the "set cover problem" but
> in your case the sets are uncountably infinite
> (and there are uncountably many of them).
>
> I would be surprised if your problem were not NP complete.
>
>
> HTH
>
>
> Robin
>
>
> On 8 Feb 2007, at 05:15, MINI GHOSH wrote:
>
>> Dear R user,
>>
>> I want to know is there a way to find the minimum
>> number of circles (of given radius) required to fill a
>> given area (say rectangular) where overlapping of
>> circles is allowed.
>>
>> Thanks,
>> Regards,
>> Mini Ghosh
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jim at bitwrit.com.au  Thu Feb  8 10:29:25 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 08 Feb 2007 20:29:25 +1100
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <27739.27117.qm@web32804.mail.mud.yahoo.com>
References: <27739.27117.qm@web32804.mail.mud.yahoo.com>
Message-ID: <45CAED75.6060401@bitwrit.com.au>

John Kane wrote:
> This is probably a simple problem but I don't see a
> solution.
> 
> I have a data.frame with a number of columns where I
> would like 0 <- NA
> 
Hi John,
You might have a look at "toNA" in the prettyR package. Wait for version 
1.0-4, just uploaded, as I have fixed a bug in that function.

Jim


From mini_ghosh at yahoo.co.in  Thu Feb  8 10:39:08 2007
From: mini_ghosh at yahoo.co.in (MINI GHOSH)
Date: Thu, 8 Feb 2007 09:39:08 +0000 (GMT)
Subject: [R] circle fill problem
In-Reply-To: <550A9A50-DDD1-4B76-A740-9B1EFE72DC8F@uva.nl>
Message-ID: <748153.10654.qm@web7905.mail.in.yahoo.com>

Dear Ingmar and Robin,

Thanks for you suggestions. I will see to it.

Regards,
Mini
--- Ingmar Visser <i.visser at uva.nl> wrote:

> Robin & Mini,
> For those interested, googling for the 'orange
> packing problem' as it  
> is known, or more officially the sphere packing
> problems gives you  
> quite a few hits on these and similar problems.
> So at least the 3-d case the problem has been solved
> (I imagine the  
> problem is easier in 2-d ...)
> hth, Ingmar
> 
> On 8 Feb 2007, at 09:52, Robin Hankin wrote:
> 
> > Mini
> >
> > This is a hard problem in general.
> >
> > Recreational mathematics has wrestled with
> > this and similar problems over the years; the
> > general field is the "set cover problem" but
> > in your case the sets are uncountably infinite
> > (and there are uncountably many of them).
> >
> > I would be surprised if your problem were not NP
> complete.
> >
> >
> > HTH
> >
> >
> > Robin
> >
> >
> > On 8 Feb 2007, at 05:15, MINI GHOSH wrote:
> >
> >> Dear R user,
> >>
> >> I want to know is there a way to find the minimum
> >> number of circles (of given radius) required to
> fill a
> >> given area (say rectangular) where overlapping of
> >> circles is allowed.
> >>
> >> Thanks,
> >> Regards,
> >> Mini Ghosh
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-
> >> guide.html
> >> and provide commented, minimal, self-contained,
> reproducible code.
> >
> > --
> > Robin Hankin
> > Uncertainty Analyst
> > National Oceanography Centre, Southampton
> > European Way, Southampton SO14 3ZH, UK
> >   tel  023-8059-7743
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting- 
> > guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> 
>


From r.hankin at noc.soton.ac.uk  Thu Feb  8 10:49:30 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 8 Feb 2007 09:49:30 +0000
Subject: [R] circle fill problem
In-Reply-To: <550A9A50-DDD1-4B76-A740-9B1EFE72DC8F@uva.nl>
References: <169357.86689.qm@web7904.mail.in.yahoo.com>
	<D7D2C1D3-8FE6-435D-9B8C-8BA3F4DC5594@soc.soton.ac.uk>
	<550A9A50-DDD1-4B76-A740-9B1EFE72DC8F@uva.nl>
Message-ID: <7A28B109-5DC1-43E4-A482-AFE3C235AD82@soc.soton.ac.uk>

Hello Ingmar

In Kepler's sphere packing problem the
spheres are not allowed to overlap, which
would suggest to me that different tactics should
perhaps be used.

Mini's problem is formally a "cover problem", and
the sphere packing problem is a "packing problem".
The two are related, but tend not to have direct
relevance to one another.

Also be aware that Kepler's conjecture refers to the
packing fraction limit as  the space available tends to
infinity.

My understanding is that Kepler's conjecture has now been
proved beyond all reasonable doubt, but how to use this in
Mini's problem is not clear to me.


rksh


On 8 Feb 2007, at 09:21, Ingmar Visser wrote:

> Robin & Mini,
> For those interested, googling for the 'orange packing problem' as it
> is known, or more officially the sphere packing problems gives you
> quite a few hits on these and similar problems.
> So at least the 3-d case the problem has been solved (I imagine the
> problem is easier in 2-d ...)
> hth, Ingmar
>
> On 8 Feb 2007, at 09:52, Robin Hankin wrote:
>
>> Mini
>>
>> This is a hard problem in general.
>>
>> Recreational mathematics has wrestled with
>> this and similar problems over the years; the
>> general field is the "set cover problem" but
>> in your case the sets are uncountably infinite
>> (and there are uncountably many of them).
>>
>> I would be surprised if your problem were not NP complete.
>>
>>
>> HTH
>>
>>
>> Robin
>>
>>
>> On 8 Feb 2007, at 05:15, MINI GHOSH wrote:
>>
>>> Dear R user,
>>>
>>> I want to know is there a way to find the minimum
>>> number of circles (of given radius) required to fill a
>>> given area (say rectangular) where overlapping of
>>> circles is allowed.
>>>
>>> Thanks,
>>> Regards,
>>> Mini Ghosh
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Robin Hankin
>> Uncertainty Analyst
>> National Oceanography Centre, Southampton
>> European Way, Southampton SO14 3ZH, UK
>>   tel  023-8059-7743
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ripley at stats.ox.ac.uk  Thu Feb  8 10:53:25 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Feb 2007 09:53:25 +0000 (GMT)
Subject: [R] path for source()
In-Reply-To: <45CAE469.9080301@igbmc.u-strasbg.fr>
References: <20070208071004.GS10041@humboldt>
	<45CAE469.9080301@igbmc.u-strasbg.fr>
Message-ID: <Pine.LNX.4.64.0702080946280.5972@gannet.stats.ox.ac.uk>

I don't think anyone have answered the actual question yet.

The answer is simple: the search path for search is '.', the current 
directory.  Just as it is for almost all the software on your system 
except binaries and package addons (including, e.g. R's search path for 
packages).

But R is a progamming language and it takes less time to add a search than 
to post a message here.  Something like (untested)

search.source <- function(file, path=".", ...)
{
    for(p in path) {
        fp <- file.path(p, f)
        if(file.exists(fp)) return(source(fp, ...))
   }
   stop("file ", sQuote(file), " not found")
}

On Thu, 8 Feb 2007, Wolfgang Raffelsberger wrote:

> Hi,
> an easy way to address this is to change directory within R before
> calling source() :
> setwd("D:/Projects/yourProject")
> source("yourCode.R")
>
> Of course you need to know where your .R files are.
> Using getwd() you can always check where you are and using dir() you can
> check the files in your directory (which you could combine with grep()
> to search for ".R") ..
>
> Wolfgang
>
> colliera at ukzn.ac.za a ?crit :
>> hello,
>>
>> i have a couple of .R files distributed about my file system. i 
>> commonly source() these from other files, but i have to include the 
>> full file path. this is not always convenient if you move files around. 
>> is there a way of setting the search path for source()?
>>
>> thanks a lot!
>>
>> cheers,
>> andrew.
>>
>>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From pbulian at cro.it  Thu Feb  8 10:47:02 2007
From: pbulian at cro.it (Pietro Bulian)
Date: Thu, 8 Feb 2007 10:47:02 +0100
Subject: [R] step in a model with strata
Message-ID: <000001c74b67$0a578110$2bb411ac@cro.sanita.fvg.it>

Dear experts,
when I call the step function for a coxph model with n covariates and a 
dicotomous variable included as strata, the first term removed by step is 
always the strata variable. This is not what I want and then I do a manual 
step updating the model minus the least significant covariate and testing 
with anova, until I have minimized the model. Is there a package were this 
can be done? or am I doing something wrong ? (I'm not a statistician).

Thanks for hints

Pietro Bulian

Clinical and Experimental Hematology Research Unit
Centro di Riferimento Oncologico, I.R.C.C.S.
Via Pedemontana, 12
I-33081 Aviano (PN) - Italy

phone: +39 0434 659 412
fax: +39 0434 659 409
e-mail: pbulian at cro.it


From b.otto at uke.uni-hamburg.de  Thu Feb  8 11:29:26 2007
From: b.otto at uke.uni-hamburg.de (Benjamin Otto)
Date: Thu, 8 Feb 2007 11:29:26 +0100
Subject: [R] Diffrerence in "%in%" function to boundry setting via <>
Message-ID: <005b01c74b6c$048f7490$336f12ac@matrix.com>

Hi,

There is a point which is irritating me currently quite a bit and that is an
aspect of different behaviour between the %in% function and the
smaller/bigger than signs (<>). Here is are two examples to demonstrate what
I mean:

Example1:
> c(1,1,2,2,3,4,4,6,7) %in% c(1,2,3)
[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE

Right, that is what I expect.

Example2:
> ps <- seq(-0.502,0.378,by=0.001)
> ps[494]
[1] -0.009

> class(ps[494])
[1] "numeric"
> class(-0.009)
[1] "numeric"
> class(ps[494])
[1] "numeric"

> ps[494] == -0.009
[1] FALSE
> ps[494] %in% -0.009
[1] FALSE
> ps[494] == c(-0.009)
[1] FALSE
> ps[494] %in% c(-0.009)
[1] FALSE
> ps[494] <= -0.008
[1] TRUE
> ps[494] >= -0.010
[1] TRUE
> -0.009 == -0.009
[1] TRUE

BUT: 
> ps[249]
[1] -0.254
> class(ps[249])
[1] "numeric"
> ps[249] %in% -0.254
[1] TRUE

OK! Can sombody explain to me what is happening here? Honestly? I don't
understand where the difference but it's critical! Because obviuosly when I
have a set of numeric values (ALL have three digits) and to boundry values
lb/up, a lower and an upper boundry, I could (from what I thought until now)
chosse between:

Version1:
> small.set <- set[set %in% seq(lb,up,by=0.001)]

Version2:
> small.set <- set[set >= lb & set <= up]

Unfortunately with my data I used I got around 8000 values from my set with
version1 but about 24000 with version2. IS there some main diffrence I
didn't take into account or is my system just behaving irrational (that's
what I think if you look at Example2)?

I checked the behaviour under R-2.4.1 (Windows) and under 2.2.1 (Linux). The
result was the same.

Sincere regards

Benjamin Otto 

-- 
Benjamin Otto
Universitaetsklinikum Eppendorf Hamburg
Institut fuer Klinische Chemie
Martinistrasse 52
20246 Hamburg


From vikasrawal at gmail.com  Thu Feb  8 11:45:24 2007
From: vikasrawal at gmail.com (Vikas Rawal)
Date: Thu, 8 Feb 2007 16:15:24 +0530
Subject: [R] boxplot statistics in ggplot
In-Reply-To: <f8e6ff050702070512m4bdfe5dbi1a77fd97b7681fdb@mail.gmail.com>
References: <20070207070640.GA30339@shireen.jnu.ac.in>
	<f8e6ff050702070512m4bdfe5dbi1a77fd97b7681fdb@mail.gmail.com>
Message-ID: <20070208104524.GA17163@shireen.jnu.ac.in>

It will be useful if you could explain the how to use the weighted boxplot
function. The manual does not give details. I have not been able to
make it work. Specifically, how does one write the function?

Vikas



On Wed, Feb 07, 2007 at 07:12:26AM -0600, hadley wickham wrote:
> Hi Vikas,
> 
> Exactly what do you want to label them with?  Generally the purpose of
> the plot is to avoid having explicit labels - you can just read the
> numbers of the axes.  If you want the exact numbers, presenting them
> in a table might be more appropriate.
> 
> I'm not at my development computer at the moment, so I can't give you
> the exact details, but you will have to calculate the statistics
> yourself (using the weighted boxplot function in ggplot) and add them
> to the plot in some way.  This should be a bit easier in the next
> version of ggplot, where the calculation and display are a little more
> distinct.
> 
> Hadley
> 
> On 2/7/07, Vikas Rawal <vikasrawal at gmail.com> wrote:
> >I need to make weighted boxplots. I found that ggplot makes them. I
> >would however like to label them with the boxplot statistics (the
> >median, q1 and q3). In the boxplot function in r-base, I could output
> >boxplot statistics and then write a text on the plot to place the
> >labels. How would one do it with ggplot?
> >
> >Vikas
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide 
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >
>


From Roger.Bivand at nhh.no  Thu Feb  8 11:51:48 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 8 Feb 2007 11:51:48 +0100 (CET)
Subject: [R] Diffrerence in "%in%" function to boundry setting via <>
In-Reply-To: <005b01c74b6c$048f7490$336f12ac@matrix.com>
Message-ID: <Pine.LNX.4.44.0702081149590.15143-100000@reclus.nhh.no>

On Thu, 8 Feb 2007, Benjamin Otto wrote:

A version of FAQ 7.31 "Why doesn't R think these numbers are equal?"

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

> ps <- seq(-0.502,0.378,by=0.001)
> ps[494]
[1] -0.009
> print(ps[494], digits=16)
[1] -0.009000000000000008
> all.equal(ps[494], -0.009)
[1] TRUE


> Hi,
> 
> There is a point which is irritating me currently quite a bit and that is an
> aspect of different behaviour between the %in% function and the
> smaller/bigger than signs (<>). Here is are two examples to demonstrate what
> I mean:
> 
> Example1:
> > c(1,1,2,2,3,4,4,6,7) %in% c(1,2,3)
> [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE
> 
> Right, that is what I expect.
> 
> Example2:
> > ps <- seq(-0.502,0.378,by=0.001)
> > ps[494]
> [1] -0.009
> 
> > class(ps[494])
> [1] "numeric"
> > class(-0.009)
> [1] "numeric"
> > class(ps[494])
> [1] "numeric"
> 
> > ps[494] == -0.009
> [1] FALSE
> > ps[494] %in% -0.009
> [1] FALSE
> > ps[494] == c(-0.009)
> [1] FALSE
> > ps[494] %in% c(-0.009)
> [1] FALSE
> > ps[494] <= -0.008
> [1] TRUE
> > ps[494] >= -0.010
> [1] TRUE
> > -0.009 == -0.009
> [1] TRUE
> 
> BUT: 
> > ps[249]
> [1] -0.254
> > class(ps[249])
> [1] "numeric"
> > ps[249] %in% -0.254
> [1] TRUE
> 
> OK! Can sombody explain to me what is happening here? Honestly? I don't
> understand where the difference but it's critical! Because obviuosly when I
> have a set of numeric values (ALL have three digits) and to boundry values
> lb/up, a lower and an upper boundry, I could (from what I thought until now)
> chosse between:
> 
> Version1:
> > small.set <- set[set %in% seq(lb,up,by=0.001)]
> 
> Version2:
> > small.set <- set[set >= lb & set <= up]
> 
> Unfortunately with my data I used I got around 8000 values from my set with
> version1 but about 24000 with version2. IS there some main diffrence I
> didn't take into account or is my system just behaving irrational (that's
> what I think if you look at Example2)?
> 
> I checked the behaviour under R-2.4.1 (Windows) and under 2.2.1 (Linux). The
> result was the same.
> 
> Sincere regards
> 
> Benjamin Otto 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From Mauro.Rossi at irpi.cnr.it  Thu Feb  8 11:54:55 2007
From: Mauro.Rossi at irpi.cnr.it (Mauro Rossi)
Date: Thu, 08 Feb 2007 11:54:55 +0100
Subject: [R]  Zeta and Zipf distribution
Message-ID: <45CB017F.8070909@irpi.cnr.it>

Dear R user,
 I want to estimate the parameter of ZETA or/and ZIPF distributions 
using R, given a series of integer values. Do you know a package 
(similar to MASS) or a function (similar to fitdistr) I can use to 
estimate the parameter of these distributions using MLE method? 
Otherwise do you know a function (which use MLE method to estimate 
distribution parameters) that allow me to specify a PDF or PMF?
Thanks,
Regards
Mauro Rossi

-- 
Mauro Rossi
Istituto di Ricerca per la Protezione Idrogeologica
Consiglio Nazionale delle Ricerche
Via della Madonna Alta, 126
06128 Perugia
Italia
Tel. +39 075 5014421
Fax +39 075 5014420

From mothsailor at googlemail.com  Thu Feb  8 12:14:18 2007
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 8 Feb 2007 11:14:18 +0000
Subject: [R] Zeta and Zipf distribution
In-Reply-To: <45CB017F.8070909@irpi.cnr.it>
References: <45CB017F.8070909@irpi.cnr.it>
Message-ID: <815b70590702080314t1c5580d4wc44305bc4c0b11f2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/4eefd84e/attachment.pl 

From jpressnell at westnet.com.au  Thu Feb  8 12:51:46 2007
From: jpressnell at westnet.com.au (Jerry Pressnell)
Date: Thu, 8 Feb 2007 22:51:46 +1100
Subject: [R] Partial file name
Message-ID: <001401c74b77$8309d6c0$6401a8c0@DESKTOP>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/03aa25c4/attachment.pl 

From murdoch at stats.uwo.ca  Thu Feb  8 13:03:58 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Feb 2007 07:03:58 -0500
Subject: [R] Partial file name
In-Reply-To: <001401c74b77$8309d6c0$6401a8c0@DESKTOP>
References: <001401c74b77$8309d6c0$6401a8c0@DESKTOP>
Message-ID: <45CB11AE.20205@stats.uwo.ca>

On 2/8/2007 6:51 AM, Jerry Pressnell wrote:
> Hello all,
> 
>  
> 
> I wish to write an R script to read a specific .txt file each day. The file
> is downloaded from a external source and the file name is the same three
> letters followed by the date and time of download.
> 
>  
> 
> For example
> 
>  
> 
> Day 1
> 
>  
> 
> AAA_08_02_2007_06_18_98.txt
> 
>  
> 
> Day 2
> 
>  
> 
> AAA_09_02_2007_10_12_03.txt
> 
>  
> 
> Is it possible to use read.table() in such a way that it only needs the
> start of the file name?

read.table() would need the full name, but other functions could give 
that to you.  For example, this should work, assuming there's always 
just one file that will match:

read.table( list.files( pattern = "^AAA_" ) )

or if you like file system wildcards better than regular expressions,

read.table( list.files( pattern = glob2rx("AAA_*") ) )

Duncan Murdoch


From olivier.eterradossi at ema.fr  Thu Feb  8 13:21:47 2007
From: olivier.eterradossi at ema.fr (Olivier ETERRADOSSI)
Date: Thu, 08 Feb 2007 13:21:47 +0100
Subject: [R] Re : Re: setting a number of values to NA over a data.frame.
Message-ID: <45CB15DB.6000003@ema.fr>

Hi John,

Unless I miss a point, why dont you try something like :

# some fake data
 > fake<-as.data.frame(cbind(seq(1,10,by=1),c(rep(1,4),rep(0,4),rep(2,2))))
      V1 V2
 1    1    1
 2    2    1
 3    3    1
 4    4    1
 5    5    0
 6    6    0
 7    7    0
 8    8    0
 9    9    2
10   10    2

# change 0 by NA
 > fake[fake==0]<-NA  # or fake$V2[fake$V2==0]<-NA if you don't want all 
0 in the dataframe to be changed to NA
# test
 > is.na(fake$V2)
[1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE

Sorry if I did not understand the issue. Hope this helps. Olivier


Jim Lemon wrote :

> John Kane wrote:
>   
>> > This is probably a simple problem but I don't see a
>> > solution.
>> > 
>> > I have a data.frame with a number of columns where I
>> > would like 0 <- NA
>> > 
>>     
> Hi John,
> You might have a look at "toNA" in the prettyR package. Wait for version 
> 1.0-4, just uploaded, as I have fixed a bug in that function.
>
> Jim

-- 
Olivier ETERRADOSSI
Ma?tre-Assistant
CMGD / Equipe "Propri?t?s Psycho-Sensorielles des Mat?riaux"
Ecole des Mines d'Al?s
H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
tel std: +33 (0)5.59.30.54.25
tel direct: +33 (0)5.59.30.90.35 
fax: +33 (0)5.59.30.63.68
http://www.ema.fr


From olivier.eterradossi at ema.fr  Thu Feb  8 13:50:50 2007
From: olivier.eterradossi at ema.fr (Olivier ETERRADOSSI)
Date: Thu, 08 Feb 2007 13:50:50 +0100
Subject: [R] Re : Re: setting a number of values to NA over a data.frame.
Message-ID: <45CB1CAA.6030400@ema.fr>

Hi again,

Awfully sorry John, I should have been sleeping and did not see your 
full post....

here is a way, unless I miss the point again :

fake<-as.data.frame(cbind(seq(1,10,by=1),c(rep(1,4),rep(0,4),rep(2,2)))) 
# from my previous post

# one moree column this time !
fake3<-cbind(fake,fake$V2)
index<-c(2,3)
fake3[,index][fake3[,index]==0]<-NA

not nice, but seems to do the job.
Hope this helps... this time :-)
Olivier

-- 
Olivier ETERRADOSSI
Ma?tre-Assistant
CMGD / Equipe "Propri?t?s Psycho-Sensorielles des Mat?riaux"
Ecole des Mines d'Al?s
H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
tel std: +33 (0)5.59.30.54.25
tel direct: +33 (0)5.59.30.90.35 
fax: +33 (0)5.59.30.63.68
http://www.ema.fr

-------------- next part --------------
An embedded message was scrubbed...
From: Olivier ETERRADOSSI <olivier.eterradossi at ema.fr>
Subject: Re : Re: [R] setting a number of values to NA over  a data.frame.
Date: Thu, 08 Feb 2007 13:21:47 +0100
Size: 1685
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/7faa5eef/attachment.mht 

From ripley at stats.ox.ac.uk  Thu Feb  8 14:26:04 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Feb 2007 13:26:04 +0000 (GMT)
Subject: [R] Re : Re: setting a number of values to NA over a data.frame.
In-Reply-To: <45CB15DB.6000003@ema.fr>
References: <45CB15DB.6000003@ema.fr>
Message-ID: <Pine.LNX.4.64.0702081313340.1988@gannet.stats.ox.ac.uk>

I think you do understand it. Indexing a data frame by a logical matrix is 
provided just for applications like this and the reverse,

mydf[is.na(mydf)]  <- something

prettyR's toNA seems to believe that data frames can be other than 2D, 
which is surprising.

On Thu, 8 Feb 2007, Olivier ETERRADOSSI wrote:

> Hi John,
>
> Unless I miss a point, why dont you try something like :
>
> # some fake data
> > fake<-as.data.frame(cbind(seq(1,10,by=1),c(rep(1,4),rep(0,4),rep(2,2))))
>      V1 V2
> 1    1    1
> 2    2    1
> 3    3    1
> 4    4    1
> 5    5    0
> 6    6    0
> 7    7    0
> 8    8    0
> 9    9    2
> 10   10    2
>
> # change 0 by NA
> > fake[fake==0]<-NA  # or fake$V2[fake$V2==0]<-NA if you don't want all
> 0 in the dataframe to be changed to NA
> # test
> > is.na(fake$V2)
> [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE
>
> Sorry if I did not understand the issue. Hope this helps. Olivier
>
>
> Jim Lemon wrote :
>
>> John Kane wrote:
>>
>>>> This is probably a simple problem but I don't see a
>>>> solution.
>>>>
>>>> I have a data.frame with a number of columns where I
>>>> would like 0 <- NA
>>>>
>>>
>> Hi John,
>> You might have a look at "toNA" in the prettyR package. Wait for version
>> 1.0-4, just uploaded, as I have fixed a bug in that function.
>>
>> Jim
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From kubovy at virginia.edu  Thu Feb  8 14:41:04 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 8 Feb 2007 08:41:04 -0500
Subject: [R] smartpred depends on fitted() in flexmix?
Message-ID: <DF9C572D-E32C-4B44-BFF7-649D8687B159@virginia.edu>

Hi,

I was going through the examples in smartpred. It seems there's an  
unstated dependency on the fitted() function in package flexmix.

n = 20
set.seed(86)
x = sort(runif(n))
y = sort(runif(n))
library(splines)
fit = lm(y ~ ns(x, df=5))
plot(x, y)
lines(x, fitted(fit)) # won't work w/o prior loading of the flexmix  
package.
newx = seq(0, 1, len=n)
points(newx, predict(fit, data.frame(x=newx)), type="b", col=2, err=-1)
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From martin at martinpercossi.com  Thu Feb  8 15:41:40 2007
From: martin at martinpercossi.com (Martin Percossi)
Date: Thu, 08 Feb 2007 14:41:40 +0000
Subject: [R] Newbie: Acf function
Message-ID: <45CB36A4.4090601@martinpercossi.com>

Hi, I would like to use acf.plot on a correlogram that is computed 
externally. In other words, I would like to "fake out" the acf object. 
Is this possible?-- any help would be appreciated.

TIA
Martin


From vincent.goulet at act.ulaval.ca  Thu Feb  8 16:00:11 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Thu, 8 Feb 2007 10:00:11 -0500
Subject: [R] Newbie: Acf function
In-Reply-To: <45CB36A4.4090601@martinpercossi.com>
References: <45CB36A4.4090601@martinpercossi.com>
Message-ID: <200702081000.12035.vincent.goulet@act.ulaval.ca>

Le Jeudi 8 F?vrier 2007 09:41, Martin Percossi a ?crit?:
> Hi, I would like to use acf.plot on a correlogram that is computed
> externally. In other words, I would like to "fake out" the acf object.
> Is this possible?-- any help would be appreciated.

Well, essentially plot.acf() makes a plot with 'type = "h"'. Playing around 
with that so give you the desired output.

Now wait a little bit... There, I already wrote such a function:

tacf <- function(x, ...)
{
    plot(x, type = "h", ylab = "ACF", xlab = "Lag",
         ylim = c(-1, 1), ...)
    abline(h = 0)
}

('x' contains the autocorrelations.)

HTH

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From nilsson.henric at gmail.com  Thu Feb  8 16:00:42 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Thu, 8 Feb 2007 16:00:42 +0100 (CET)
Subject: [R] step in a model with strata
In-Reply-To: <000001c74b67$0a578110$2bb411ac@cro.sanita.fvg.it>
References: <000001c74b67$0a578110$2bb411ac@cro.sanita.fvg.it>
Message-ID: <31495.212.209.13.15.1170946842.squirrel@www.sorch.se>

Den To, 2007-02-08, 10:47 skrev Pietro Bulian:
> Dear experts,
> when I call the step function for a coxph model with n covariates and a
> dicotomous variable included as strata, the first term removed by step is
> always the strata variable. This is not what I want

So, what do you want exactly? (You didn't tell.)

I'm just guessing here, but it sounds like you'd always want the strata
to stay in the model. In that case, use the `scope' argument i.e.
something like `step(fit, scope = list(lower = ~ strata(x)))' if your
fitted model object is called `fit' and your stratification variable is
called `x' -- see ?step.

> and then I do a manual step updating the model minus the least
> significant covariate and testing with anova, until I have minimized
> the model.

So, let me see if I understand this correctly, you have a two-stage
procedure where you first minimize the AIC criterion and then remove
non-significant predictors in a stepwise fashion?

> Is there a package were this can be done?

If you're referring to the procedure above, I'm not aware of any such
package.

> or am I doing something wrong ? (I'm not a statistician).

Well, it depends... If you want some guidance on model selection, see e.g.

@BOOK{R:Harrell:2001,
  AUTHOR = {Frank E. Harrell},
  TITLE = {Regression Modeling Strategies, with Applications to
                  Linear Models, Survival Analysis and Logistic
                  Regression},
  PUBLISHER = {Springer},
  YEAR = 2001,
  NOTE = {ISBN 0-387-95232-2},
  URL = {http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS}
}


HTH,
Henric



>
> Thanks for hints
>
> Pietro Bulian
>
> Clinical and Experimental Hematology Research Unit
> Centro di Riferimento Oncologico, I.R.C.C.S.
> Via Pedemontana, 12
> I-33081 Aviano (PN) - Italy
>
> phone: +39 0434 659 412
> fax: +39 0434 659 409
> e-mail: pbulian at cro.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From gruen at ci.tuwien.ac.at  Thu Feb  8 16:01:30 2007
From: gruen at ci.tuwien.ac.at (Bettina Gruen)
Date: Thu, 08 Feb 2007 16:01:30 +0100
Subject: [R] smartpred depends on fitted() in flexmix?
In-Reply-To: <DF9C572D-E32C-4B44-BFF7-649D8687B159@virginia.edu>
References: <DF9C572D-E32C-4B44-BFF7-649D8687B159@virginia.edu>
Message-ID: <45CB3B4A.6030700@ci.tuwien.ac.at>

Hi,

which packages do you have attached? Look for example at the output of 
sessionInfo().

For your code you need the function "fitted" and its S3 methods from 
package "stats". So there is no reason why "flexmix" should be 
necessary. However, in "flexmix" S4 methods for "fitted" are provided 
and the S3 methods from "stats" are imported using the following in the 
NAMESPACE:

importFrom(stats, fitted)

I can only guess that the problem might occur because you have for 
example "VGAM" attached. I can run your code if "VGAM" is not attached. 
The code does not work after loading "VGAM" and works again after also 
loading "flexmix":

 > n = 20
 > set.seed(86)
 > x = sort(runif(n))
 > y = sort(runif(n))
 > library(splines)
 > fit = lm(y ~ ns(x, df=5))
 > plot(x, y)
 > lines(x, fitted(fit))
 > library(VGAM)
 > lines(x, fitted(fit))
Error in function (classes, fdef, mtable)  :
         unable to find an inherited method for function "fitted", for 
signature "lm"
 > library(flexmix)
 > lines(x, fitted(fit))

I think this should be possible to solve by also adding 
importFrom(stats, fitted) to the NAMESPACE of "VGAM".

Best,
Bettina

Michael Kubovy wrote:
> Hi,
> 
> I was going through the examples in smartpred. It seems there's an  
> unstated dependency on the fitted() function in package flexmix.
> 
> n = 20
> set.seed(86)
> x = sort(runif(n))
> y = sort(runif(n))
> library(splines)
> fit = lm(y ~ ns(x, df=5))
> plot(x, y)
> lines(x, fitted(fit)) # won't work w/o prior loading of the flexmix  
> package.
> newx = seq(0, 1, len=n)
> points(newx, predict(fit, data.frame(x=newx)), type="b", col=2, err=-1)
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From DempseyC at kssg.com  Thu Feb  8 15:59:23 2007
From: DempseyC at kssg.com (Catherine Dempsey)
Date: Thu, 8 Feb 2007 14:59:23 -0000 
Subject: [R] (no subject)
Message-ID: <AA73B273C6D13F4A953BE8AC1B5CBEB601EB70FB@internuntio.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/c370f3c3/attachment.pl 

From rolf at math.unb.ca  Thu Feb  8 16:20:59 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Thu, 8 Feb 2007 11:20:59 -0400 (AST)
Subject: [R] Newbie: Acf function
Message-ID: <200702081520.l18FKxwq001607@weisner.math.unb.ca>

Martin Percossi wrote:

> Hi, I would like to use acf.plot on a correlogram that is computed 
> externally. In other words, I would like to "fake out" the acf object. 
> Is this possible?-- any help would be appreciated.

        (a) Note that it's ``plot.acf'' NOT acf.plot.

        (b) This is R --- ***ANYTHING*** is possible.

        (c) Create an object, say ``y'' of class ``acf'',
        having components with the right names.  You could
        build a dummy acf object by

                > dum <- acf(rnorm(100),plot=FALSE)

        and the examine ``dum'' to see what it should consist of.

        (d) Something like:

                y <- list(acf=array(ecc,dim=c(length(ecc),1,1)),
                          type="correlation",
                          n.used=n.ecc,lag=array(0:(length(ecc)-1),
                                                 dim=c(length(ecc),1,1)),
                          series="ecc",snames=NULL)
                class(y) <- "acf"
                plot(y)

        where ``ecc'' is a vector comprising your ``externally
        created correlogram'' and ``n.ecc'' is the length of the series
        from which ecc was created.  Note that the first entry of ecc
        should be 1; it corresponds to lag 0.

                                        cheers,

                                                Rolf Turner
                                                rolf at math.unb.ca


From stefan.albrecht at apep.com  Thu Feb  8 16:38:18 2007
From: stefan.albrecht at apep.com (Albrecht,
	Dr. Stefan (AZ Private Equity Partner))
Date: Thu, 8 Feb 2007 16:38:18 +0100
Subject: [R]  R in Industry
Message-ID: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/9e79afb4/attachment.pl 

From martin at martinpercossi.com  Thu Feb  8 16:39:58 2007
From: martin at martinpercossi.com (Martin Percossi)
Date: Thu, 08 Feb 2007 15:39:58 +0000
Subject: [R] Newbie: Acf function
In-Reply-To: <200702081520.l18FKxwq001607@weisner.math.unb.ca>
References: <200702081520.l18FKxwq001607@weisner.math.unb.ca>
Message-ID: <45CB444E.2030708@martinpercossi.com>

Funny enough, but by accident I typed unclass(acf) (I had meant to 
unclass the *data* obtained as a result of applying this function), and 
I saw the source code! From there I managed to reproduce your steps 
below... In any case, many thanks to all for your help.

Martin

rolf at math.unb.ca wrote:

>Martin Percossi wrote:
>
>  
>
>>Hi, I would like to use acf.plot on a correlogram that is computed 
>>externally. In other words, I would like to "fake out" the acf object. 
>>Is this possible?-- any help would be appreciated.
>>    
>>
>
>        (a) Note that it's ``plot.acf'' NOT acf.plot.
>
>        (b) This is R --- ***ANYTHING*** is possible.
>
>        (c) Create an object, say ``y'' of class ``acf'',
>        having components with the right names.  You could
>        build a dummy acf object by
>
>                > dum <- acf(rnorm(100),plot=FALSE)
>
>        and the examine ``dum'' to see what it should consist of.
>
>        (d) Something like:
>
>                y <- list(acf=array(ecc,dim=c(length(ecc),1,1)),
>                          type="correlation",
>                          n.used=n.ecc,lag=array(0:(length(ecc)-1),
>                                                 dim=c(length(ecc),1,1)),
>                          series="ecc",snames=NULL)
>                class(y) <- "acf"
>                plot(y)
>
>        where ``ecc'' is a vector comprising your ``externally
>        created correlogram'' and ``n.ecc'' is the length of the series
>        from which ecc was created.  Note that the first entry of ecc
>        should be 1; it corresponds to lag 0.
>
>                                        cheers,
>
>                                                Rolf Turner
>                                                rolf at math.unb.ca
>  
>


From maechler at stat.math.ethz.ch  Thu Feb  8 16:53:39 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 8 Feb 2007 16:53:39 +0100
Subject: [R] R vs Matlab {Re: R in Industry}
In-Reply-To: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
Message-ID: <17867.18307.453231.153044@stat.math.ethz.ch>

>>>>> "Albr" == Albrecht, Dr Stefan (AZ Private Equity Partner) <stefan.albrecht at apep.com>
>>>>>     on Thu, 8 Feb 2007 16:38:18 +0100 writes:

    Albr> Dear all,
    Albr> I was reading with great interest your comments about the use of R in
    Albr> the industry. Personally, I use R as scripting language in the financial
    Albr> industry, not so much for its statistical capabilities (which are
    Albr> great), but more for programming. I once switched from S-Plus to R,
    Albr> because I liked R more, it had a better and easier to use documentation
    Albr> and it is faster (especially with loops).
 
    Albr> Now some colleagues of mine are (finally) eager to join me in my
    Albr> quantitative efforts, but they feel that they are more at ease with
    Albr> Matlab. I can understand this. Matlab has a real IDE with symbolic
    Albr> debugger, integrated editor and profiling, etc. The help files are
    Albr> great, very comprehensive and coherent. It also could be easier to
    Albr> learn.
 
    Albr> And, I was very astonished to realise, Matlab is very, very much faster
    Albr> with simple "for" loops, which would speed up simulations considerably.
Can you give some evidence for this statement, please?

At the moment, I'd bet that you use forgot to pre-allocate a
result array in R and do something like the "notorious horrible" (:-)
1-dimensional

  r <- NULL
  for(i in 1:10000) {
	r[i] <- verycomplicatedsimulation(i)
  }

instead of the "correct"

  r <- numeric(10000)
  for(i in 1:10000) {
	r[i] <- verycomplicatedsimulation(i)
  }

If r is a matrix or even higher array, and you are using rbind()
or cbind() inside the loop to build up the result,
the problem will become even worse.

    Albr> So I have trouble to argue for a use of R (which I like) instead of
    Albr> Matlab. The price of Matlab is high, but certainly not prohibitive. R is
    Albr> great and free, but maybe less comfortable to use than Matlab.
 
    Albr> Finally, after all, I have the impression that in many job offerings in
    Albr> the financial industry R is much less often mentioned than Matlab.
 
    Albr> I would very much appreciate any comments on my above remarks. I know
    Albr> there has been some discussions of R vs. Matlab on R-help, but these
    Albr> could be somewhat out-dated, since both languages are evolving quite
    Albr> quickly.
 
    Albr> With many thanks and best regards,
    Albr> Stefan Albrecht


From Mark.Leeds at morganstanley.com  Thu Feb  8 16:54:25 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 8 Feb 2007 10:54:25 -0500
Subject: [R] (no subject)
In-Reply-To: <AA73B273C6D13F4A953BE8AC1B5CBEB601EB70FB@internuntio.middleearth.kssg.com>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401401B65@NYWEXMB23.msad.ms.com>

You're not doing anything wrong. You fit an arima(0,1,2) so it doesn't
know the epsilon terms going
forward are after the first step so it assumes them to be zero so you
get the same forecast every day.  I think you would have to 
re-estimate each day if you want different forecasts every day.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Catherine Dempsey
Sent: Thursday, February 08, 2007 9:59 AM
To: 'r-help at lists.R-project.org'
Subject: [R] (no subject)

Hi.

I hope you can help me...

 

I have fitted the following ARIMA model:
arima1<-arima(bigspring$log.volume, order=c(0,1,2))

I need to predict 30 days ahead.  I used following code
predict(arima1,n.ahead=30,se=T)

 

However I get 30 predictions, but from predictions 2:30 I get the same
predictions.  Why is this?  What am I doing wrong

 

Thanks

Catherine



KSS Ltd
Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1
6SS  England Company Registration Number 2800886 
Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
mailto:kssg at kssg.com		http://www.kssg.com


The information in this Internet email is confidential and\ ...{{dropped}}


From Mauro.Rossi at irpi.cnr.it  Thu Feb  8 16:55:57 2007
From: Mauro.Rossi at irpi.cnr.it (Mauro Rossi)
Date: Thu, 08 Feb 2007 16:55:57 +0100
Subject: [R] Zeta and Zipf distribution
In-Reply-To: <815b70590702080314t1c5580d4wc44305bc4c0b11f2@mail.gmail.com>
References: <45CB017F.8070909@irpi.cnr.it>
	<815b70590702080314t1c5580d4wc44305bc4c0b11f2@mail.gmail.com>
Message-ID: <45CB480D.8050109@irpi.cnr.it>

Dear David,
	thank you for your reply.
I tried to use the package VGAM, the function "zipf" and also the 
function "zetaff", but these functions don't allow me to estimate 
parameters directly, I have to use a Gerneralized Linear Model or a 
Generalized Additive Model (vgam or vglm functions) and I don't want to 
use those. Don't you know a way to apply these tools to my data?
At the end my PMF has to be Y=f(X) where f(X) is a zeta or a zipf 
distribution, while using VGAM the PMF is Y = b0 + b1*f(X1)+ ... 
+bn*f(Xn). Do you know how I can write the script using the VGAM 
function for the PMF I need?

Thank you in advance,

Mauro Rossi
	

David Barron ha scritto:
> Does the zipf function in the VGAM package do what you want?
> 
> On 08/02/07, *Mauro Rossi* <Mauro.Rossi a irpi.cnr.it 
> <mailto:Mauro.Rossi a irpi.cnr.it>> wrote:
> 
>     Dear R user,
>     I want to estimate the parameter of ZETA or/and ZIPF distributions
>     using R, given a series of integer values. Do you know a package
>     (similar to MASS) or a function (similar to fitdistr) I can use to
>     estimate the parameter of these distributions using MLE method?
>     Otherwise do you know a function (which use MLE method to estimate
>     distribution parameters) that allow me to specify a PDF or PMF?
>     Thanks,
>     Regards
>     Mauro Rossi
> 
>     --
>     Mauro Rossi
>     Istituto di Ricerca per la Protezione Idrogeologica
>     Consiglio Nazionale delle Ricerche
>     Via della Madonna Alta, 126
>     06128 Perugia
>     Italia
>     Tel. +39 075 5014421
>     Fax +39 075 5014420
> 
>     ______________________________________________
>     R-help a stat.math.ethz.ch <mailto:R-help a stat.math.ethz.ch> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 
> -- 
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP

-- 
Mauro Rossi

Istituto di Ricerca per la Protezione Idrogeologica

Consiglio Nazionale delle Ricerche

Via della Madonna Alta, 126

06128 Perugia

Italia

Tel. +39 075 5014421

Fax +39 075 5014420


From kzembowe at jhuccp.org  Thu Feb  8 17:07:40 2007
From: kzembowe at jhuccp.org (Zembower, Kevin)
Date: Thu, 8 Feb 2007 11:07:40 -0500
Subject: [R] NEWBIE: @BOOK help?
Message-ID: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>

In Henric's recent post, he included this output:

@BOOK{R:Harrell:2001,
  AUTHOR = {Frank E. Harrell},
  TITLE = {Regression Modeling Strategies, with Applications to
                  Linear Models, Survival Analysis and Logistic
                  Regression},
  PUBLISHER = {Springer},
  YEAR = 2001,
  NOTE = {ISBN 0-387-95232-2},
  URL = {http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS}
}

Can someone tell me how this is generated? I've noticed this in a few
recent posts. I attempted:

RSiteSearch("@BOOK")
?BOOK
?book

but it didn't return anything useful.

Thanks.

-Kevin

Kevin Zembower
Internet Services Group manager
Center for Communication Programs
Bloomberg School of Public Health
Johns Hopkins University
111 Market Place, Suite 310
Baltimore, Maryland  21202
410-659-6139


From wvanwie at few.vu.nl  Thu Feb  8 17:12:29 2007
From: wvanwie at few.vu.nl (wvanwie at few.vu.nl)
Date: Thu, 08 Feb 2007 17:12:29 +0100
Subject: [R] Dendrogram plot without y-axis
In-Reply-To: <AA73B273C6D13F4A953BE8AC1B5CBEB601EB70FB@internuntio.middleearth.kssg.com>
References: <AA73B273C6D13F4A953BE8AC1B5CBEB601EB70FB@internuntio.middleearth.kssg.com>
Message-ID: <1170951149.45cb4bed0ff5c@www.few.vu.nl>

Hi,

I wish to plot a dendrogram, but without the y-axis. Simply setting yaxt="n"
does not work. Does anyone know what would work?

Kind regards,

Wessel van Wieringen


From ssj1364 at gmail.com  Thu Feb  8 17:12:42 2007
From: ssj1364 at gmail.com (sj)
Date: Thu, 8 Feb 2007 09:12:42 -0700
Subject: [R] Measures of forecast ERROR seasonal ARIMA model -- general
	question
Message-ID: <1c6126db0702080812n437b7a13h6d6b70397447e168@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/cdf08388/attachment.pl 

From kubovy at virginia.edu  Thu Feb  8 17:16:11 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 8 Feb 2007 11:16:11 -0500
Subject: [R] Strange behavior of abc.ci() in boot
Message-ID: <F9E188FB-14A4-4D3B-A384-2E853BD76640@virginia.edu>

require(boot)
# [1] TRUE
set.seed(1)
summary(rn <- rnorm(30))
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
# -2.21500 -0.43500  0.25660  0.08246  0.70870  1.59500
abc.ci(rnorm(30), mean)
# [1] 0.95000000 0.08245817 0.08245817

*************************************
sessionInfo()
R version 2.4.1 (2006-12-18)
i386-apple-darwin8.8.1

locale:
C

attached base packages:
[1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"  
"utils"     "methods"   "base"

other attached packages:
         boot       xtable latticeExtra      lattice      
gridBase         MASS          JGR       iplots       JavaGD
     "1.2-27"      "1.4-3"      "0.1-4"    "0.14-16"      "0.4-3"      
"7.2-31"     "1.4-15"      "1.0-5"      "0.3-5"
        rJava
     "0.4-13"

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From pburns at pburns.seanet.com  Thu Feb  8 17:23:35 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 08 Feb 2007 16:23:35 +0000
Subject: [R] R in Industry
In-Reply-To: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
Message-ID: <45CB4E87.4040605@pburns.seanet.com>

 From what I know Matlab is much more popular in
fixed income than R, but R is vastly more popular in
equities.  R seems to be making quite a lot of headway
in finance, even in fixed income to some degree.

At least to some extent, this is probably logical behavior --
fixed income is more mathematical, and equities is more
statistical.

Matlab is easier to learn mainly because it has much simpler
data structures.  However, once you are doing something
where a complex data structure is natural, then R is going to
be easier to use and you are likely to have a more complete
implementation of what you want.

If speed becomes a limiting factor, then moving the heavy
computing to C is a natural thing to do, and very easy with R.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Albrecht, Dr. Stefan (AZ Private Equity Partner) wrote:

>Dear all,
> 
>I was reading with great interest your comments about the use of R in
>the industry. Personally, I use R as scripting language in the financial
>industry, not so much for its statistical capabilities (which are
>great), but more for programming. I once switched from S-Plus to R,
>because I liked R more, it had a better and easier to use documentation
>and it is faster (especially with loops).
> 
>Now some colleagues of mine are (finally) eager to join me in my
>quantitative efforts, but they feel that they are more at ease with
>Matlab. I can understand this. Matlab has a real IDE with symbolic
>debugger, integrated editor and profiling, etc. The help files are
>great, very comprehensive and coherent. It also could be easier to
>learn.
> 
>And, I was very astonished to realise, Matlab is very, very much faster
>with simple "for" loops, which would speed up simulations considerably.
>So I have trouble to argue for a use of R (which I like) instead of
>Matlab. The price of Matlab is high, but certainly not prohibitive. R is
>great and free, but maybe less comfortable to use than Matlab.
> 
>Finally, after all, I have the impression that in many job offerings in
>the financial industry R is much less often mentioned than Matlab.
> 
>I would very much appreciate any comments on my above remarks. I know
>there has been some discussions of R vs. Matlab on R-help, but these
>could be somewhat out-dated, since both languages are evolving quite
>quickly.
> 
>With many thanks and best regards,
>Stefan Albrecht
> 
> 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From flyhyena at yahoo.com  Thu Feb  8 17:34:35 2007
From: flyhyena at yahoo.com (sun)
Date: Thu, 8 Feb 2007 17:34:35 +0100
Subject: [R] manipulate group data using column name
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
Message-ID: <001601c74b9f$048e8200$04879b83@campus.tue.nl>

Hi,

 Maybe this is a trivial question but I can not figure out a good solution.

I have a data frame fa and want to add a new column "sum" with the sum value 
of fa$X1 grouped by fa$X3.

> fa
   X1 X2 X3
1   1 11  1
2   2 12  1
3   3 13  1
4   4 14  2
5   5 15  2
6   6 16  2
7   7 17  3
8   8 18  3
9   9 19  3
10 10 20  3

fa$X3 is the index of group

i can

>aggregate(fa[,"X1"],list(fa$X3),sum)
  Group.1  x
1       1  6
2       2 15
3       3 34

then I want to add a new column "sum" in fa and assign the aggregated result 
to the new column. Is there a solution without using loops?
or maybe there is some way can even avoid aggregate operation?

Thanks.


From ggrothendieck at gmail.com  Thu Feb  8 17:41:07 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Feb 2007 11:41:07 -0500
Subject: [R] manipulate group data using column name
In-Reply-To: <001601c74b9f$048e8200$04879b83@campus.tue.nl>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<001601c74b9f$048e8200$04879b83@campus.tue.nl>
Message-ID: <971536df0702080841g3e9b5af6rf0ddc0129adb9d43@mail.gmail.com>

Try this:

fa$sum <- ave(fa$X1, fa$X3, FUN = sum)


On 2/8/07, sun <flyhyena at yahoo.com> wrote:
> Hi,
>
>  Maybe this is a trivial question but I can not figure out a good solution.
>
> I have a data frame fa and want to add a new column "sum" with the sum value
> of fa$X1 grouped by fa$X3.
>
> > fa
>   X1 X2 X3
> 1   1 11  1
> 2   2 12  1
> 3   3 13  1
> 4   4 14  2
> 5   5 15  2
> 6   6 16  2
> 7   7 17  3
> 8   8 18  3
> 9   9 19  3
> 10 10 20  3
>
> fa$X3 is the index of group
>
> i can
>
> >aggregate(fa[,"X1"],list(fa$X3),sum)
>  Group.1  x
> 1       1  6
> 2       2 15
> 3       3 34
>
> then I want to add a new column "sum" in fa and assign the aggregated result
> to the new column. Is there a solution without using loops?
> or maybe there is some way can even avoid aggregate operation?
>
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ecjbosu at aol.com  Thu Feb  8 17:41:22 2007
From: ecjbosu at aol.com (Joe Byers)
Date: Thu, 08 Feb 2007 10:41:22 -0600
Subject: [R] DRMRGL
Message-ID: <45CB52B2.8030606@aol.com>

I have tried to install the drmrgl package and receive the following error:

 > library(djmrgl)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
         unable to load shared library 
'D:/PROGRA~1/R/library/djmrgl/libs/djmrgl.
dll':
   LoadLibrary failure:  The specified procedure could not be found.
Error in library(djmrgl) : .First.lib failed for 'djmrgl'

When I try and load the library from an rterm windows I get a pop up 
error that "the procedure entry point ismdi could not be located in the 
dynamic link library R.dll"

Has anyone seen this problem before? and if so, can you help me?

Thank you
Joe


From clists at perrin.socsci.unc.edu  Thu Feb  8 17:49:28 2007
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Thu, 8 Feb 2007 11:49:28 -0500 (EST)
Subject: [R] NEWBIE: @BOOK help?
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <Pine.LNX.4.64.0702081148390.12331@perrin.socsci.unc.edu>

It's BibTeX source -- used for the BibTeX bibliography management system 
that integrates with LaTeX.

http://www.ecst.csuchico.edu/~jacobsd/bib/formats/bibtex.html
http://www.ctan.org

----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
Assistant Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
New Book: http://www.press.uchicago.edu/cgi-bin/hfs.cgi/00/178592.ctl



On Thu, 8 Feb 2007, Zembower, Kevin wrote:

> In Henric's recent post, he included this output:
>
> @BOOK{R:Harrell:2001,
>  AUTHOR = {Frank E. Harrell},
>  TITLE = {Regression Modeling Strategies, with Applications to
>                  Linear Models, Survival Analysis and Logistic
>                  Regression},
>  PUBLISHER = {Springer},
>  YEAR = 2001,
>  NOTE = {ISBN 0-387-95232-2},
>  URL = {http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS}
> }
>
> Can someone tell me how this is generated? I've noticed this in a few
> recent posts. I attempted:
>
> RSiteSearch("@BOOK")
> ?BOOK
> ?book
>
> but it didn't return anything useful.
>
> Thanks.
>
> -Kevin
>
> Kevin Zembower
> Internet Services Group manager
> Center for Communication Programs
> Bloomberg School of Public Health
> Johns Hopkins University
> 111 Market Place, Suite 310
> Baltimore, Maryland  21202
> 410-659-6139
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dimitri.mahieux at student.uclouvain.be  Thu Feb  8 17:50:49 2007
From: dimitri.mahieux at student.uclouvain.be (Mahieux Dimitri)
Date: Thu, 08 Feb 2007 17:50:49 +0100
Subject: [R] Impossible to get jpeg or png output
Message-ID: <45CB54E9.1070406@student.uclouvain.be>

Hi,

When I want to output a png file, I have the following error message :

Error dans X11(paste("jpeg::", quality, ":", filename, sep = ""), width,  :
        inpossible de d?marrer le p?riph?rique JPEG
De plus : Warning message:
impossible d'ouvrir le fichier JPEG 'Test.jpeg'

or in english

Error in X11(paste("jpeg::", quality, ":", filename, sep = ""), width,  :
        inpossible to start the JPEG peripheral
Warning message:
impossible to open the file JPEG 'Test.jpeg'

I've checked the capabilities which give :

 > capabilities()
    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
    TRUE     TRUE    FALSE     TRUE     TRUE     TRUE     TRUE     TRUE
  cledit    iconv      NLS
    TRUE     TRUE     TRUE

So I don't understand why I can't have a jpeg file ( or png file because 
I've the same problem to)

Any Idea ?

Thx a lot


From Manuel.A.Morales at williams.edu  Thu Feb  8 17:51:57 2007
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Thu, 08 Feb 2007 11:51:57 -0500
Subject: [R] R vs Matlab {Re: R in Industry}
In-Reply-To: <17867.18307.453231.153044@stat.math.ethz.ch>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<17867.18307.453231.153044@stat.math.ethz.ch>
Message-ID: <1170953517.2088.6.camel@solidago.localdomain>

On Thu, 2007-02-08 at 16:53 +0100, Martin Maechler wrote:
> >>>>> "Albr" == Albrecht, Dr Stefan (AZ Private Equity Partner) <stefan.albrecht at apep.com>
> >>>>>     on Thu, 8 Feb 2007 16:38:18 +0100 writes:
<snip> 
>     Albr> And, I was very astonished to realise, Matlab is very, very much faster
>     Albr> with simple "for" loops, which would speed up simulations considerably.
> Can you give some evidence for this statement, please?
> 
> At the moment, I'd bet that you use forgot to pre-allocate a
> result array in R and do something like the "notorious horrible" (:-)
> 1-dimensional
> 
>   r <- NULL
>   for(i in 1:10000) {
> 	r[i] <- verycomplicatedsimulation(i)
>   }
> 
> instead of the "correct"
> 
>   r <- numeric(10000)
>   for(i in 1:10000) {
> 	r[i] <- verycomplicatedsimulation(i)
>   }

Would a similar speed issue arise for the construction:
r <- vector()
...

-- 
Manuel A. Morales
http://mutualism.williams.edu

From martin at martinpercossi.com  Thu Feb  8 18:00:55 2007
From: martin at martinpercossi.com (Martin Percossi)
Date: Thu, 08 Feb 2007 17:00:55 +0000
Subject: [R] Defining functions in separate file...
Message-ID: <45CB5747.30505@martinpercossi.com>

Hello, is it possible to define functions in a file, say, myfunctions.R, 
and import them into R -- into the top-level namespace? I've seen in the 
documentation that you can create packages, but this seems very 
heavy-duty, as it requires me to createa subdirectory, and various other 
files.

TIA
Martin


From mothsailor at googlemail.com  Thu Feb  8 18:03:44 2007
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 8 Feb 2007 17:03:44 +0000
Subject: [R] Zeta and Zipf distribution
In-Reply-To: <45CB480D.8050109@irpi.cnr.it>
References: <45CB017F.8070909@irpi.cnr.it>
	<815b70590702080314t1c5580d4wc44305bc4c0b11f2@mail.gmail.com>
	<45CB480D.8050109@irpi.cnr.it>
Message-ID: <815b70590702080903vf629811kd8c8331e42e26579@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/913a9892/attachment.pl 

From Abhijit.Dasgupta at mail.jci.tju.edu  Thu Feb  8 18:05:53 2007
From: Abhijit.Dasgupta at mail.jci.tju.edu (Abhijit Dasgupta)
Date: Thu, 08 Feb 2007 12:05:53 -0500
Subject: [R] NEWBIE: @BOOK help?
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <45CB5871.3020704@mail.jci.tju.edu>

This is a BibTeX entry for Frank Harrell's book, which can be generated 
using a variety of software (I use JabRef or emacs/RefTeX or WinEdt, as 
needed). It is not generated from R, I believe. BibTeX is the 
bibliography management and citation system that is used within the 
TeX/LaTeX framework of producing documents, which is commonly used in 
the sciences, engineering and statistics. Many of us have BibTeX files 
which hold all the citations we wish to cite, and the BibTeX program 
extracts ones referred to in an article we write, formats it and inserts 
it in the text and References of the article.

For more information, google BibTeX or LaTeX bibliography

Abhijit

Zembower, Kevin wrote:
> In Henric's recent post, he included this output:
>
> @BOOK{R:Harrell:2001,
>   AUTHOR = {Frank E. Harrell},
>   TITLE = {Regression Modeling Strategies, with Applications to
>                   Linear Models, Survival Analysis and Logistic
>                   Regression},
>   PUBLISHER = {Springer},
>   YEAR = 2001,
>   NOTE = {ISBN 0-387-95232-2},
>   URL = {http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS}
> }
>
> Can someone tell me how this is generated? I've noticed this in a few
> recent posts. I attempted:
>
> RSiteSearch("@BOOK")
> ?BOOK
> ?book
>
> but it didn't return anything useful.
>
> Thanks.
>
> -Kevin
>
> Kevin Zembower
> Internet Services Group manager
> Center for Communication Programs
> Bloomberg School of Public Health
> Johns Hopkins University
> 111 Market Place, Suite 310
> Baltimore, Maryland  21202
> 410-659-6139
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From singularitaet at gmx.net  Thu Feb  8 18:09:25 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Thu, 08 Feb 2007 18:09:25 +0100
Subject: [R] R in Industry
In-Reply-To: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
Message-ID: <45CB5945.10403@gmx.net>

I just ran on my Windows PC the benchmark from
http://www.sciviews.org/benchmark/index.html which is pretty old now.
Thats probably the reason for the errors which I did not correct. As
you see R has some advantages but Matlab has also some advantages.
However the differences are not to big. There is only one big
difference which indeed includes loops (Creation of a 220x220 Toeplitz
matrix) where Matlab is much faster. But maybe a simple change in the
programmation can change that...

Has someone in the list an updated script?

Stefan Grosse

The benchmarks:

R 2.4.1
   R Benchmark 2.3
   ===============

   I. Matrix calculation

   ---------------------
Creation, transp., deformation of a 1500x1500 matrix (sec):
0.863333333333335
800x800 normal distributed random matrix ^1000______ (sec):
0.136666666666666
Sorting of 2,000,000 random values__________________ (sec):
0.616666666666665
700x700 cross-product matrix (b = a' * a)___________ (sec):
0.559999999999997
Linear regression over a 600x600 matrix (c = a \ b') (sec):  0 #ERROR


   II. Matrix functions
   --------------------
FFT over 800,000 random values______________________ (sec):
0.559999999999997
Eigenvalues of a 320x320 random matrix______________ (sec):
0.493333333333335
Determinant of a 650x650 random matrix______________ (sec):
0.276666666666666
Cholesky decomposition of a 900x900 matrix__________ (sec):  0 #ERROR
Inverse of a 400x400 random matrix__________________ (sec):  0 #ERROR

   III. Programmation
   ------------------
750,000 Fibonacci numbers calculation (vector calc)_ (sec):
0.466666666666669
Creation of a 2250x2250 Hilbert matrix (matrix calc) (sec):
1.01666666666667
Grand common divisors of 70,000 pairs (recursion)___ (sec):
0.396666666666671
Creation of a 220x220 Toeplitz matrix (loops)_______ (sec):
0.553333333333332
Escoufier's method on a 37x37 matrix (mixed)________ (sec):
2.66999999999999

                      --- End of test ---

Matlab 7.0.4

  Matlab Benchmark 2
   ==================
Number of times each test is run__________________________: 3
 
   I. Matrix calculation
   ---------------------
Creation, transp., deformation of a 1500x1500 matrix (sec): 0.29047
800x800 normal distributed random matrix ^1000______ (sec): 0.42967
Sorting of 2,000,000 random values__________________ (sec): 0.71432
700x700 cross-product matrix (b = a' * a)___________ (sec): 0.14748
Linear regression over a 600x600 matrix (c = a \ b') (sec): 0.12831
                  ------------------------------------------------------
                Trimmed geom. mean (2 extremes eliminated): 0.26403
 
   II. Matrix functions
   --------------------
FFT over 800,000 random values______________________ (sec): 0.24591
Eigenvalues of a 320x320 random matrix______________ (sec): 0.38507
Determinant of a 650x650 random matrix______________ (sec): 0.091612
Cholesky decomposition of a 900x900 matrix__________ (sec): 0.11059
Inverse of a 400x400 random matrix__________________ (sec): 0.069414
                  ------------------------------------------------------
                Trimmed geom. mean (2 extremes eliminated): 0.13556
 
   III. Programmation
   ------------------
750,000 Fibonacci numbers calculation (vector calc)_ (sec): 1.2386
Creation of a 2250x2250 Hilbert matrix (matrix calc) (sec): 3.0541
Grand common divisors of 70,000 pairs (recursion)___ (sec): 1.7637
Creation of a 220x220 Toeplitz matrix (loops)_______ (sec): 0.0045972
Escoufier's method on a 37x37 matrix (mixed)________ (sec): 0.50481
                  ------------------------------------------------------
                Trimmed geom. mean (2 extremes eliminated): 1.0331
 
 
Total time for all 15 tests_________________________ (sec): 9.1786
Overall mean (sum of I, II and III trimmed means/3)_ (sec): 0.33316
                      --- End of test ---

-- 
-------------------------------------------
lic. oec. Stefan Grosse

University of Erfurt
Microeconomics
Nordh?user Str. 63
99089 Erfurt
Germany

phone  +49-361 - 737 45 23
fax    +49-361 - 737 45 29
mobile +49-1609- 760 33 01

web http://www.uni-erfurt.de/mikrooekonomie
mail stefan.grosse at uni-erfurt.de


From stubben at lanl.gov  Thu Feb  8 18:16:49 2007
From: stubben at lanl.gov (stubben)
Date: Thu, 8 Feb 2007 10:16:49 -0700
Subject: [R] abbreviate dataframe for Sweave output
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D307731554@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D307731554@groamrexm03.amer.pfizer.com>
Message-ID: <436f1eeac150a5f6d930603cef1509e5@lanl.gov>

Thanks Charles and Max, both functions work great.  I also used a hack 
to replace row names with '.', '. ', '.  ', etc.

Chris

dot.df <- function(x, head = 3, tail=1, dotrows=2)
{
    x <- format(rbind(head(x,head + dotrows), tail(x,tail)))
    if(dotrows>0)
    {
       x[(head + 1):(head + dotrows),] <- "."
       for(i in 1:dotrows){ rownames(x)[head+i]<-paste(".", substring("  
         ", 1, i-1))}
    }
    x
}

dot.df(crabs)
     sp sex index   FL   RW   CL   CW   BD
1    B   M     1  8.1  6.7 16.1 19.0  7.0
2    B   M     2  8.8  7.7 18.1 20.8  7.4
3    B   M     3  9.2  7.8 19.0 22.4  7.7
.    .   .     .    .    .    .    .    .
.    .   .     .    .    .    .    .    .
200  O   F    50 23.1 20.2 46.2 52.5 21.1



## or dot.matrix (replacing latex commands for display here)

dot.matrix( crabs)
     sp sex ...   CW   BD
1    B   M ...   19    7
2    B   M ... 20.8  7.4
.    .   .  .     .    .
199  O   F ... 48.7 19.8
200  O   F ... 52.5 21.1


>> foo(crabs)
>     sp  sex index FL     RW     CL     CW     BD
> 1   "B" "M" " 1"  " 8.1" " 6.7" "16.1" "19.0" " 7.0"
> 2   "B" "M" " 2"  " 8.8" " 7.7" "18.1" "20.8" " 7.4"
> 3   "B" "M" " 3"  " 9.2" " 7.8" "19.0" "22.4" " 7.7"
> .   "." "." "."   "."    "."    "."    "."    "."
> .   "." "." "."   "."    "."    "."    "."    "."
> 200 "O" "F" "50"  "23.1" "20.2" "46.2" "52.5" "21.1"
>


From jhorn at bu.edu  Thu Feb  8 18:30:58 2007
From: jhorn at bu.edu (Jason Horn)
Date: Thu, 8 Feb 2007 12:30:58 -0500 (EST)
Subject: [R] remove component from list or data frame
Message-ID: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>

Sorry to ask such a simple question, but I can't find the answer after 
extensive searching the docs and the web.

How do you remove a component from a list?  For example say you have:

lst<-c(5,6,7,8,9)

How do you remove, for example, the third component in the list?

lst[[3]]]<-NULL     generates an error:  "Error: more elements supplied 
than there are to replace"



Also, how do you remove a row from a data frame?  For example, say you 
have:

lst1<-c(1,2,3,4,5)
lst2<-c(6,7,8,9,10)
frame<-data.frame(lst1,lst2)

How do you remove, for example, the second row of frame?

Thanks,

- Jason


From bates at stat.wisc.edu  Thu Feb  8 18:31:46 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 8 Feb 2007 11:31:46 -0600
Subject: [R] NEWBIE: @BOOK help?
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <40e66e0b0702080931v544bea9an7edc8e3951f72050@mail.gmail.com>

On 2/8/07, Zembower, Kevin <kzembowe at jhuccp.org> wrote:
> In Henric's recent post, he included this output:
>
> @BOOK{R:Harrell:2001,
>   AUTHOR = {Frank E. Harrell},
>   TITLE = {Regression Modeling Strategies, with Applications to
>                   Linear Models, Survival Analysis and Logistic
>                   Regression},
>   PUBLISHER = {Springer},
>   YEAR = 2001,
>   NOTE = {ISBN 0-387-95232-2},
>   URL = {http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS}
> }
>
> Can someone tell me how this is generated? I've noticed this in a few
> recent posts. I attempted:

I'm not sure what you mean by "how this is generated".  The format is
for a bibliographic reference system called BibTeX associated with the
LaTeX text processing language.

Most BibTeX users have built up a reference database by adding the
entries by hand.  Editors like emacs have special modes to facilitate
entering this information.

Searching on the CTAN.org (Comprehensive TeX Archive Network) web site
may give some links to systems that can generate BibTeX reference
databases automatically.  On Linux the tellico bibliographic database
manager can search commercial sites like amazon.com and download
information about specific books from there, then export it in BibTeX
format.  I haven't tried it myself for books so I can't say how well
it works.  I have used it for extracting information on movies from
imdb.com and it does a good job on that.


From skiadas at hanover.edu  Thu Feb  8 18:34:30 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 8 Feb 2007 12:34:30 -0500
Subject: [R] NEWBIE: @BOOK help?
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <4AD5505B-0F7A-4EC5-93A1-27AD031EC602@hanover.edu>

On Feb 8, 2007, at 11:07 AM, Zembower, Kevin wrote:

> In Henric's recent post, he included this output:
>
> @BOOK{R:Harrell:2001,
>   AUTHOR = {Frank E. Harrell},
>   TITLE = {Regression Modeling Strategies, with Applications to
>                   Linear Models, Survival Analysis and Logistic
>                   Regression},
>   PUBLISHER = {Springer},
>   YEAR = 2001,
>   NOTE = {ISBN 0-387-95232-2},
>   URL = {http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS}
> }
>
> Can someone tell me how this is generated? I've noticed this in a few
> recent posts. I attempted:

It is BibTeX:

http://www.bibtex.org/
http://en.wikipedia.org/wiki/BibTeX

Haris


From whitede at onid.orst.edu  Thu Feb  8 17:28:52 2007
From: whitede at onid.orst.edu (Denis White)
Date: Thu, 08 Feb 2007 08:28:52 -0800
Subject: [R] [R-pkgs] new contributed package 'stream.net'
Message-ID: <1170952132.45cb4fc4dc5dc@webmail.oregonstate.edu>

New contributed package 'stream.net' is available on CRAN.

Description:   Functions with example data for creating, importing,
               attributing, analyzing, and displaying stream networks
               represented as binary trees.  Capabilities include
               importing network topology and attributes from GIS data,
               upstream and downstream distance matrices, stochastic
               network generation, segmentation of network into
               reaches, adding attributes to reaches with specified
               statistical distributions, interpolating reach
               attributes from sparse data, analyzing autocorrelation
               of reach attributes, and creating maps with legends of
               attribute data.  Target applications include dynamic
               fish modeling.

Denis White
US EPA
Corvallis, Oregon, USA

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From roland.rproject at gmail.com  Thu Feb  8 18:40:14 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Thu, 8 Feb 2007 12:40:14 -0500
Subject: [R] R in Industry
In-Reply-To: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
Message-ID: <47c7c59e0702080940rb6c4c2x20b15e06c988b517@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/97fbd21a/attachment.pl 

From BEN at SSANET.COM  Thu Feb  8 18:48:05 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Thu, 8 Feb 2007 11:48:05 -0600
Subject: [R] R in Industry
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<45CB4E87.4040605@pburns.seanet.com>
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>

To those following this thRead:

There was a thread on this topic a year or so ago on this list, in which
contributors mentioned reasons that corporate powers-that-be were
reluctant to commit to R as a corporate statistical platform.  (My
favorite was "There is no one to sue if something goes wrong.")

One reason that I do not think was discussed then, nor have I seen
discussed since, is the issue of the continuity of support.  If one
person has contributed disproportionately heavily to the development and
maintenance of a package, and then retires or follows other interests,
and the package needs maintenance (perhaps as a consequence of new
operating systems or a new version of R), is there any assurance that it
will be available?  With a commercial package such as, say, SPSS, the
corporate memory and continuance makes such continued maintenance
likely, but is there such a commitment with R packages?  If my company
came to depend heavily on a fairly obscure R package (as we are
contemplating doing), what guarantee is there that it will be available
next month/year/decade?  I know of none, nor would I expect one.

As R says when it starts up, "R is free software and comes with
ABSOLUTELY NO WARRANTY."

Ben Fairbank


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
Sent: Thursday, February 08, 2007 10:24 AM
To: Albrecht,Dr. Stefan (AZ Private Equity Partner)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R in Industry

 From what I know Matlab is much more popular in
fixed income than R, but R is vastly more popular in
equities.  R seems to be making quite a lot of headway
in finance, even in fixed income to some degree.

At least to some extent, this is probably logical behavior --
fixed income is more mathematical, and equities is more
statistical.

Matlab is easier to learn mainly because it has much simpler
data structures.  However, once you are doing something
where a complex data structure is natural, then R is going to
be easier to use and you are likely to have a more complete
implementation of what you want.

If speed becomes a limiting factor, then moving the heavy
computing to C is a natural thing to do, and very easy with R.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Albrecht, Dr. Stefan (AZ Private Equity Partner) wrote:

>Dear all,
> 
>I was reading with great interest your comments about the use of R in
>the industry. Personally, I use R as scripting language in the
financial
>industry, not so much for its statistical capabilities (which are
>great), but more for programming. I once switched from S-Plus to R,
>because I liked R more, it had a better and easier to use documentation
>and it is faster (especially with loops).
> 
>Now some colleagues of mine are (finally) eager to join me in my
>quantitative efforts, but they feel that they are more at ease with
>Matlab. I can understand this. Matlab has a real IDE with symbolic
>debugger, integrated editor and profiling, etc. The help files are
>great, very comprehensive and coherent. It also could be easier to
>learn.
> 
>And, I was very astonished to realise, Matlab is very, very much faster
>with simple "for" loops, which would speed up simulations considerably.
>So I have trouble to argue for a use of R (which I like) instead of
>Matlab. The price of Matlab is high, but certainly not prohibitive. R
is
>great and free, but maybe less comfortable to use than Matlab.
> 
>Finally, after all, I have the impression that in many job offerings in
>the financial industry R is much less often mentioned than Matlab.
> 
>I would very much appreciate any comments on my above remarks. I know
>there has been some discussions of R vs. Matlab on R-help, but these
>could be somewhat out-dated, since both languages are evolving quite
>quickly.
> 
>With many thanks and best regards,
>Stefan Albrecht
> 
> 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bates at stat.wisc.edu  Thu Feb  8 18:55:03 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 8 Feb 2007 11:55:03 -0600
Subject: [R] R vs Matlab {Re: R in Industry}
In-Reply-To: <1170953517.2088.6.camel@solidago.localdomain>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<17867.18307.453231.153044@stat.math.ethz.ch>
	<1170953517.2088.6.camel@solidago.localdomain>
Message-ID: <40e66e0b0702080955w2247c64foe213f4a6a837d495@mail.gmail.com>

On 2/8/07, Manuel Morales <Manuel.A.Morales at williams.edu> wrote:
> On Thu, 2007-02-08 at 16:53 +0100, Martin Maechler wrote:
> > >>>>> "Albr" == Albrecht, Dr Stefan (AZ Private Equity Partner) <stefan.albrecht at apep.com>
> > >>>>>     on Thu, 8 Feb 2007 16:38:18 +0100 writes:
> <snip>
> >     Albr> And, I was very astonished to realise, Matlab is very, very much faster
> >     Albr> with simple "for" loops, which would speed up simulations considerably.
> > Can you give some evidence for this statement, please?
> >
> > At the moment, I'd bet that you use forgot to pre-allocate a
> > result array in R and do something like the "notorious horrible" (:-)
> > 1-dimensional
> >
> >   r <- NULL
> >   for(i in 1:10000) {
> >       r[i] <- verycomplicatedsimulation(i)
> >   }
> >
> > instead of the "correct"
> >
> >   r <- numeric(10000)
> >   for(i in 1:10000) {
> >       r[i] <- verycomplicatedsimulation(i)
> >   }
>
> Would a similar speed issue arise for the construction:
> r <- vector()
> ...

Why not try it and find out?

(The answer is yes.  As Martin indicated the issue is whether the
space for the entire result is allocated before inserting individual
elements of the result.  It is possible to extend a vector beyond its
current length but doing so involves allocating space for the new
vector, copying the current contents and then inserting the new
values.  Doing that tens of thousands of times is slow and wasteful.)


From lauri.nikkinen at iki.fi  Thu Feb  8 19:01:49 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Thu, 8 Feb 2007 20:01:49 +0200
Subject: [R] Data.frame columns in R console
Message-ID: <ba8c09910702081001o3a759e7bk73bc0313f752fd9b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/1f9c80f6/attachment.pl 

From mothsailor at googlemail.com  Thu Feb  8 19:02:15 2007
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 8 Feb 2007 18:02:15 +0000
Subject: [R] remove component from list or data frame
In-Reply-To: <815b70590702081001v3d07cef8ha8029566ebaa63bf@mail.gmail.com>
References: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
	<815b70590702081001v3d07cef8ha8029566ebaa63bf@mail.gmail.com>
Message-ID: <815b70590702081002y75473546tcc9f17cda2f7ff7c@mail.gmail.com>

 The first example you provide is a vector, not a list.  You can
remove the third element with:

 > lst[-3]
 [1] 5 6 8 9

 The same thing works for rows of data frames:
 > frame[-3,]
   lst1 lst2
 1    1    6
 2    2    7
 4    4    9
 5    5   10


>
>
> On 08/02/07, Jason Horn <jhorn at bu.edu> wrote:
> >  Sorry to ask such a simple question, but I can't find the answer after
> > extensive searching the docs and the web.
> >
> > How do you remove a component from a list?  For example say you have:
> >
> > lst<-c(5,6,7,8,9)
> >
> > How do you remove, for example, the third component in the list?
> >
> > lst[[3]]]<-NULL     generates an error:  "Error: more elements supplied
> > than there are to replace"
> >
> >
> >
> > Also, how do you remove a row from a data frame?  For example, say you
> > have:
> >
> > lst1<-c(1,2,3,4,5)
> > lst2<-c(6,7,8,9,10)
> > frame<-data.frame(lst1,lst2)
> >
> > How do you remove, for example, the second row of frame?
> >
> > Thanks,
> >
> > - Jason
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide  http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP



-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From sarah.goslee at gmail.com  Thu Feb  8 19:03:18 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 8 Feb 2007 13:03:18 -0500
Subject: [R] remove component from list or data frame
In-Reply-To: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
References: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
Message-ID: <efb536d50702081003s34279289s172214968ab06339@mail.gmail.com>

Hi Jason,

On 2/8/07, Jason Horn <jhorn at bu.edu> wrote:
> Sorry to ask such a simple question, but I can't find the answer after
> extensive searching the docs and the web.
>
> How do you remove a component from a list?  For example say you have:


You use the - operator for both your vector and data frame
examples.

> lst <- c(5,6,7,8,9)
# which by the way isn't a list
> is.list(lst)
[1] FALSE
> lst
[1] 5 6 7 8 9
> lst <- lst[-3]
> lst
[1] 5 6 8 9

> lst1<-c(1,2,3,4,5)
> lst2<-c(6,7,8,9,10)
> frame<-data.frame(lst1,lst2)
> frame
  lst1 lst2
1    1    6
2    2    7
3    3    8
4    4    9
5    5   10
> frame[-2,]
  lst1 lst2
1    1    6
3    3    8
4    4    9
5    5   10


-- 
Sarah Goslee
http://www.functionaldiversity.org


From csardi at rmki.kfki.hu  Thu Feb  8 19:03:41 2007
From: csardi at rmki.kfki.hu (Csardi Gabor)
Date: Thu, 8 Feb 2007 19:03:41 +0100
Subject: [R] Defining functions in separate file...
In-Reply-To: <45CB5747.30505@martinpercossi.com>
References: <45CB5747.30505@martinpercossi.com>
Message-ID: <20070208180341.GF18084@bifur.rmki.kfki.hu>

See ?source and also the list archive, this is a quite frequently asked 
question.

Gabor

On Thu, Feb 08, 2007 at 05:00:55PM +0000, Martin Percossi wrote:
> Hello, is it possible to define functions in a file, say, myfunctions.R, 
> and import them into R -- into the top-level namespace? I've seen in the 
> documentation that you can create packages, but this seems very 
> heavy-duty, as it requires me to createa subdirectory, and various other 
> files.
> 
> TIA
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From murdoch at stats.uwo.ca  Thu Feb  8 19:09:46 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Feb 2007 13:09:46 -0500
Subject: [R] remove component from list or data frame
In-Reply-To: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
References: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
Message-ID: <45CB676A.4060109@stats.uwo.ca>

On 2/8/2007 12:30 PM, Jason Horn wrote:
> Sorry to ask such a simple question, but I can't find the answer after 
> extensive searching the docs and the web.
> 
> How do you remove a component from a list?  For example say you have:
> 
> lst<-c(5,6,7,8,9)

In R jargon, that's a vector, not a list.
> 
> How do you remove, for example, the third component in the list?

lst[-3] will do it.

> 
> lst[[3]]]<-NULL     generates an error:  "Error: more elements supplied 
> than there are to replace"

The [[ index ]] syntax only works on true lists.
> 
> 
> 
> Also, how do you remove a row from a data frame?  For example, say you 
> have:
> 
> lst1<-c(1,2,3,4,5)
> lst2<-c(6,7,8,9,10)
> frame<-data.frame(lst1,lst2)
> 
> How do you remove, for example, the second row of frame?

Same idea:

frame <- frame[-2, ]

Duncan Murdoch
> 
> Thanks,
> 
> - Jason
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Abhijit.Dasgupta at mail.jci.tju.edu  Thu Feb  8 19:15:22 2007
From: Abhijit.Dasgupta at mail.jci.tju.edu (Abhijit Dasgupta)
Date: Thu, 08 Feb 2007 13:15:22 -0500
Subject: [R] NEWBIE: @BOOK help?
In-Reply-To: <40e66e0b0702080931v544bea9an7edc8e3951f72050@mail.gmail.com>
References: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
	<40e66e0b0702080931v544bea9an7edc8e3951f72050@mail.gmail.com>
Message-ID: <45CB68BA.40003@mail.jci.tju.edu>

As an addition, JabRef (which is a Java application) can automatically 
download citation information from CiteSeer and PubMed and store it in 
BibTeX format, albeit once you know the appropriate reference number

Douglas Bates wrote:
> On 2/8/07, Zembower, Kevin <kzembowe at jhuccp.org> wrote:
>   
>> In Henric's recent post, he included this output:
>>
>> @BOOK{R:Harrell:2001,
>>   AUTHOR = {Frank E. Harrell},
>>   TITLE = {Regression Modeling Strategies, with Applications to
>>                   Linear Models, Survival Analysis and Logistic
>>                   Regression},
>>   PUBLISHER = {Springer},
>>   YEAR = 2001,
>>   NOTE = {ISBN 0-387-95232-2},
>>   URL = {http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS}
>> }
>>
>> Can someone tell me how this is generated? I've noticed this in a few
>> recent posts. I attempted:
>>     
>
> I'm not sure what you mean by "how this is generated".  The format is
> for a bibliographic reference system called BibTeX associated with the
> LaTeX text processing language.
>
> Most BibTeX users have built up a reference database by adding the
> entries by hand.  Editors like emacs have special modes to facilitate
> entering this information.
>
> Searching on the CTAN.org (Comprehensive TeX Archive Network) web site
> may give some links to systems that can generate BibTeX reference
> databases automatically.  On Linux the tellico bibliographic database
> manager can search commercial sites like amazon.com and download
> information about specific books from there, then export it in BibTeX
> format.  I haven't tried it myself for books so I can't say how well
> it works.  I have used it for extracting information on movies from
> imdb.com and it does a good job on that.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wwwhsd at gmail.com  Thu Feb  8 19:16:23 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 8 Feb 2007 16:16:23 -0200
Subject: [R] remove component from list or data frame
In-Reply-To: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
References: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
Message-ID: <da79af330702081016v409d4236s301e572f2062e215@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/3249d3ce/attachment.pl 

From Abhijit.Dasgupta at mail.jci.tju.edu  Thu Feb  8 19:19:42 2007
From: Abhijit.Dasgupta at mail.jci.tju.edu (Abhijit Dasgupta)
Date: Thu, 08 Feb 2007 13:19:42 -0500
Subject: [R] Defining functions in separate file...
In-Reply-To: <45CB5747.30505@martinpercossi.com>
References: <45CB5747.30505@martinpercossi.com>
Message-ID: <45CB69BE.3090408@mail.jci.tju.edu>

Look at

help(.First)

Martin Percossi wrote:
> Hello, is it possible to define functions in a file, say, myfunctions.R, 
> and import them into R -- into the top-level namespace? I've seen in the 
> documentation that you can create packages, but this seems very 
> heavy-duty, as it requires me to createa subdirectory, and various other 
> files.
>
> TIA
> Martin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From iverson at biostat.wisc.edu  Thu Feb  8 19:31:18 2007
From: iverson at biostat.wisc.edu (Erik Iverson)
Date: Thu, 08 Feb 2007 12:31:18 -0600
Subject: [R] remove component from list or data frame
In-Reply-To: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
References: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
Message-ID: <45CB6C76.1030303@biostat.wisc.edu>



Jason Horn wrote:
> Sorry to ask such a simple question, but I can't find the answer after 
> extensive searching the docs and the web.
> 
> How do you remove a component from a list?  For example say you have:
> 
> lst<-c(5,6,7,8,9)
> 
> How do you remove, for example, the third component in the list?

Is the object lst really a list?  Try is.list(lst) to check.
To remove an element from a vector, use for example, lst[-3]

> 
> lst[[3]]]<-NULL     generates an error:  "Error: more elements supplied 
> than there are to replace"
> 
> 

If lst were actually a list, that command would work with the obvious 
syntax fix.  So would lst[-3] though.

> 
> Also, how do you remove a row from a data frame?  For example, say you 
> have:
> 
> lst1<-c(1,2,3,4,5)
> lst2<-c(6,7,8,9,10)
> frame<-data.frame(lst1,lst2)
> 
> How do you remove, for example, the second row of frame?

You use

frame[-2, ] #remove second row, keep all columns.

> 
> Thanks,
> 
> - Jason
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rlevy at ucsd.edu  Thu Feb  8 19:34:59 2007
From: rlevy at ucsd.edu (Roger Levy)
Date: Thu, 08 Feb 2007 10:34:59 -0800
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <17865.18890.134246.794371@macht.arts.cornell.edu>
References: <45C38E90.9070709@ucsd.edu>	<17860.61176.520222.602660@lapo.berkeley.edu>	<17861.4763.359422.817825@macht.arts.cornell.edu>	<45C52F17.1030601@ucsd.edu>	<17861.13360.697951.302800@macht.arts.cornell.edu>	<17861.25637.324856.683429@lapo.berkeley.edu>	<45C93EEF.9080509@ucsd.edu>
	<17865.18890.134246.794371@macht.arts.cornell.edu>
Message-ID: <45CB6D53.5000002@ucsd.edu>

Walter Mebane wrote:
> Roger,
> 
>  > Error in if (logliklambda < loglik) bvec <- blambda :
>  > 	missing value where TRUE/FALSE needed
>  > In addition: Warning message:
>  > NaNs produced in: sqrt(sigma2GN)
> 
> That message comes from the Newton algorithm (defined in source file
> multinomMLE.R).  It would be better if we bullet-proofed it a bit
> more.  The first thing is to check the data.  I don't have the
> multinomLogis() function, so I can't run your code.  

Whoops, sorry about that -- I'm putting revised code at the end of the 
message.

> But do you really
> mean
> 
>  > for(i in 1:length(choice)) {
> and
>  > dim(counts) <- c(length(choice),length(choice))
> 
> Should that be
> 
>   for(i in 1:n) {
> and
>   dim(counts) <- c(n, length(choice))
> 
> or instead of n, some number m > length(choice).  As it is it seems to
> me you have three observations for three categories, which isn't going
> to work (there are five coefficient parameters, plus sigma for the
> dispersion).

I really did mean for(i in 1:length(choice)) -- once again, the proper 
code is at the end of this message.

Also, I notice that I get the same error with another kind of data, 
which works for multinom from nnet:


library(nnet)
library(multinomRob)
dtf <- data.frame(y1=c(1,1),y2=c(2,1),y3=c(1,2),x=c(0,1))
summary(multinom(as.matrix(dtf[,1:3]) ~ x, data=dtf))
summary(multinomRob(list(y1 ~ 0, y2 ~ x, y3 ~ x), data=dtf,print.level=128))


The call to multinom fits the following coefficients:

Coefficients:
     (Intercept)          x
y2 0.6933809622 -0.6936052
y3 0.0001928603  0.6928327

but the call to multinomRob gives me the following error:

multinomRob(): Grouped MNL Estimation
[1] "multinomMLE: -loglik initial: 9.48247391895106"
Error in if (logliklambda < loglik) bvec <- blambda :
	missing value where TRUE/FALSE needed
In addition: Warning message:
NaNs produced in: sqrt(sigma2GN)


Does this shed any light on things?


Thanks again,

Roger





***

set.seed(10)
library(multinomRob)
multinomLogis <- function(vector) {
   x <- exp(vector)
   z <- sum(x)
   x/z
}

n <- 20
choice <- c("A","B","C")
intercepts <- c(0.5,0.3,0.2)
prime.strength <- rep(0.4,length(intercepts))
counts <- c()
for(i in 1:length(choice)) {
   u <- intercepts[1:length(choice)]
   u[i] <- u[i] + prime.strength[i]
   counts <- c(counts,rmultinomial(n = n, pr = multinomLogis(u)))
}
dim(counts) <- c(length(choice),length(choice))
counts <- t(counts)
row.names(counts) <- choice
colnames(counts) <- choice
data <- data.frame(Prev.Choice=choice,counts)

for(i in 1:length(choice)) {
   data[[paste("last",choice[i],sep=".")]] <- 
ifelse(data$Prev.Choice==choice[i],1,0)
}

multinomRob(list(A ~ last.A ,
                  B ~ last.B ,
                  C ~ last.C - 1 ,
                  ),
             data=data,
             print.level=128)



I obtained this output:


Your Model (xvec):
                                A B C
(Intercept)/(Intercept)/last.C 1 1 1
last.A/last.B/NA               1 1 0

[1] "multinomRob:  WARNING.  Limited regressor variation..."
[1] "WARNING.  ... A regressor has a distinct value for only one 
observation."
[1] "WARNING.  ... I'm using a modified estimation algorithm (i.e., 
preventing LQD"
[1] "WARNING.  ... from modifying starting values for the affected 
parameters)."
[1] "WARNING.  ... Affected parameters are TRUE in the following table."

                                    A     B     C
(Intercept)/(Intercept)/last.C FALSE FALSE  TRUE
last.A/last.B/NA                TRUE  TRUE FALSE



multinomRob(): Grouped MNL Estimation
[1] "multinomMLE: -loglik initial: 70.2764843511374"
Error in if (logliklambda < loglik) bvec <- blambda :
	missing value where TRUE/FALSE needed
In addition: Warning message:
NaNs produced in: sqrt(sigma2GN)


From murdoch at stats.uwo.ca  Thu Feb  8 19:47:23 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Feb 2007 13:47:23 -0500
Subject: [R] R in Industry
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>	<45CB4E87.4040605@pburns.seanet.com>
	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
Message-ID: <45CB703B.4010003@stats.uwo.ca>

On 2/8/2007 12:48 PM, Ben Fairbank wrote:
> To those following this thRead:
> 
> There was a thread on this topic a year or so ago on this list, in which
> contributors mentioned reasons that corporate powers-that-be were
> reluctant to commit to R as a corporate statistical platform.  (My
> favorite was "There is no one to sue if something goes wrong.")
> 
> One reason that I do not think was discussed then, nor have I seen
> discussed since, is the issue of the continuity of support.  If one
> person has contributed disproportionately heavily to the development and
> maintenance of a package, and then retires or follows other interests,
> and the package needs maintenance (perhaps as a consequence of new
> operating systems or a new version of R), is there any assurance that it
> will be available?  With a commercial package such as, say, SPSS, the
> corporate memory and continuance makes such continued maintenance
> likely, but is there such a commitment with R packages?  If my company
> came to depend heavily on a fairly obscure R package (as we are
> contemplating doing), what guarantee is there that it will be available
> next month/year/decade?  I know of none, nor would I expect one.

There's no guarantee of support, but the majority of R packages are 
licensed under the GPL, so there is a guarantee of availability of the 
source code, which means that contracting with someone expert in the 
field to provide you with support will be a possibility.  If it's an 
obscure package as you say, your company may represent a majority of 
users, and it may well be that the expert you need is already someone in 
your company, who contributed patches to the package while the original 
maintainer was still active.

If a commercial vendor were to withdraw support for a package there is 
really no hope of putting together your own support service.  You would 
have to live with the bugs and without updates, or start from scratch to 
replace it yourself.  For example, this happened to me about 10 years 
ago when Intel withdrew support for 3DR.  As it happens OpenGL is a 
better replacement, but I wasn't too happy at the time.

Duncan Murdoch

> 
> As R says when it starts up, "R is free software and comes with
> ABSOLUTELY NO WARRANTY."
> 
> Ben Fairbank
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
> Sent: Thursday, February 08, 2007 10:24 AM
> To: Albrecht,Dr. Stefan (AZ Private Equity Partner)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R in Industry
> 
>  From what I know Matlab is much more popular in
> fixed income than R, but R is vastly more popular in
> equities.  R seems to be making quite a lot of headway
> in finance, even in fixed income to some degree.
> 
> At least to some extent, this is probably logical behavior --
> fixed income is more mathematical, and equities is more
> statistical.
> 
> Matlab is easier to learn mainly because it has much simpler
> data structures.  However, once you are doing something
> where a complex data structure is natural, then R is going to
> be easier to use and you are likely to have a more complete
> implementation of what you want.
> 
> If speed becomes a limiting factor, then moving the heavy
> computing to C is a natural thing to do, and very easy with R.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Albrecht, Dr. Stefan (AZ Private Equity Partner) wrote:
> 
>>Dear all,
>> 
>>I was reading with great interest your comments about the use of R in
>>the industry. Personally, I use R as scripting language in the
> financial
>>industry, not so much for its statistical capabilities (which are
>>great), but more for programming. I once switched from S-Plus to R,
>>because I liked R more, it had a better and easier to use documentation
>>and it is faster (especially with loops).
>> 
>>Now some colleagues of mine are (finally) eager to join me in my
>>quantitative efforts, but they feel that they are more at ease with
>>Matlab. I can understand this. Matlab has a real IDE with symbolic
>>debugger, integrated editor and profiling, etc. The help files are
>>great, very comprehensive and coherent. It also could be easier to
>>learn.
>> 
>>And, I was very astonished to realise, Matlab is very, very much faster
>>with simple "for" loops, which would speed up simulations considerably.
>>So I have trouble to argue for a use of R (which I like) instead of
>>Matlab. The price of Matlab is high, but certainly not prohibitive. R
> is
>>great and free, but maybe less comfortable to use than Matlab.
>> 
>>Finally, after all, I have the impression that in many job offerings in
>>the financial industry R is much less often mentioned than Matlab.
>> 
>>I would very much appreciate any comments on my above remarks. I know
>>there has been some discussions of R vs. Matlab on R-help, but these
>>could be somewhat out-dated, since both languages are evolving quite
>>quickly.
>> 
>>With many thanks and best regards,
>>Stefan Albrecht
>> 
>> 
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From skiadas at hanover.edu  Thu Feb  8 19:49:15 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 8 Feb 2007 13:49:15 -0500
Subject: [R] R in Industry
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<45CB4E87.4040605@pburns.seanet.com>
	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
Message-ID: <88F28D35-F6E3-449F-A9B4-D39BF7785E50@hanover.edu>

On Feb 8, 2007, at 12:48 PM, Ben Fairbank wrote:
>  If my company
> came to depend heavily on a fairly obscure R package (as we are
> contemplating doing), what guarantee is there that it will be  
> available
> next month/year/decade?  I know of none, nor would I expect one.

I would imagine that if there was a package that really needed  
updating, then your company could hire an R programmer for a short  
time to fix whatever needs fixing, and that would be a much smaller  
expense than licensing an expensive package like those other ones out  
there.

But perhaps I am completely wrong in this, I am relatively far from  
the industry world.

Haris


From iverson at biostat.wisc.edu  Thu Feb  8 19:40:25 2007
From: iverson at biostat.wisc.edu (Erik Iverson)
Date: Thu, 08 Feb 2007 12:40:25 -0600
Subject: [R] R in Industry
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>	<45CB4E87.4040605@pburns.seanet.com>
	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
Message-ID: <45CB6E99.8010505@biostat.wisc.edu>

Ben -

Ben Fairbank wrote:
> To those following this thRead:
> 
> There was a thread on this topic a year or so ago on this list, in which
> contributors mentioned reasons that corporate powers-that-be were
> reluctant to commit to R as a corporate statistical platform.  (My
> favorite was "There is no one to sue if something goes wrong.")
> 
> One reason that I do not think was discussed then, nor have I seen
> discussed since, is the issue of the continuity of support.  If one
> person has contributed disproportionately heavily to the development and
> maintenance of a package, and then retires or follows other interests,
> and the package needs maintenance (perhaps as a consequence of new
> operating systems or a new version of R), is there any assurance that it
> will be available?  With a commercial package such as, say, SPSS, the
> corporate memory and continuance makes such continued maintenance
> likely, but is there such a commitment with R packages?  If my company
> came to depend heavily on a fairly obscure R package (as we are
> contemplating doing), what guarantee is there that it will be available
> next month/year/decade?  I know of none, nor would I expect one.

But you would have the source code, so as long as someone knew R, you 
could maintain it, expand it, customize it, patch it yourselves, even if 
the original maintainer left the project.  You can't say the same with a 
commercial package likely.


> 
> As R says when it starts up, "R is free software and comes with
> ABSOLUTELY NO WARRANTY."
> 
> Ben Fairbank
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
> Sent: Thursday, February 08, 2007 10:24 AM
> To: Albrecht,Dr. Stefan (AZ Private Equity Partner)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R in Industry
> 
>  From what I know Matlab is much more popular in
> fixed income than R, but R is vastly more popular in
> equities.  R seems to be making quite a lot of headway
> in finance, even in fixed income to some degree.
> 
> At least to some extent, this is probably logical behavior --
> fixed income is more mathematical, and equities is more
> statistical.
> 
> Matlab is easier to learn mainly because it has much simpler
> data structures.  However, once you are doing something
> where a complex data structure is natural, then R is going to
> be easier to use and you are likely to have a more complete
> implementation of what you want.
> 
> If speed becomes a limiting factor, then moving the heavy
> computing to C is a natural thing to do, and very easy with R.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Albrecht, Dr. Stefan (AZ Private Equity Partner) wrote:
> 
> 
>>Dear all,
>>
>>I was reading with great interest your comments about the use of R in
>>the industry. Personally, I use R as scripting language in the
> 
> financial
> 
>>industry, not so much for its statistical capabilities (which are
>>great), but more for programming. I once switched from S-Plus to R,
>>because I liked R more, it had a better and easier to use documentation
>>and it is faster (especially with loops).
>>
>>Now some colleagues of mine are (finally) eager to join me in my
>>quantitative efforts, but they feel that they are more at ease with
>>Matlab. I can understand this. Matlab has a real IDE with symbolic
>>debugger, integrated editor and profiling, etc. The help files are
>>great, very comprehensive and coherent. It also could be easier to
>>learn.
>>
>>And, I was very astonished to realise, Matlab is very, very much faster
>>with simple "for" loops, which would speed up simulations considerably.
>>So I have trouble to argue for a use of R (which I like) instead of
>>Matlab. The price of Matlab is high, but certainly not prohibitive. R
> 
> is
> 
>>great and free, but maybe less comfortable to use than Matlab.
>>
>>Finally, after all, I have the impression that in many job offerings in
>>the financial industry R is much less often mentioned than Matlab.
>>
>>I would very much appreciate any comments on my above remarks. I know
>>there has been some discussions of R vs. Matlab on R-help, but these
>>could be somewhat out-dated, since both languages are evolving quite
>>quickly.
>>
>>With many thanks and best regards,
>>Stefan Albrecht
>>
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
> 
> http://www.R-project.org/posting-guide.html
> 
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Max.Kuhn at pfizer.com  Thu Feb  8 20:02:52 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Thu, 8 Feb 2007 14:02:52 -0500
Subject: [R] R in Industry - new SIG
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3076E6F3D@groamrexm03.amer.pfizer.com>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307779FF5@groamrexm03.amer.pfizer.com>

Martin Maechler called my bluff on this suggestion. I'm now the admin
for the new special interest group for R related job postings:

   https://stat.ethz.ch/mailman/listinfo/r-sig-jobs 

Please send appropriate emails to this list. There are some simple rules
for postings (e.g. no attachments etc).

Max

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kuhn, Max
Sent: Tuesday, February 06, 2007 5:10 PM
To: Doran, Harold; R-help at stat.math.ethz.ch
Subject: Re: [R] R in Industry

As someone who has (reluctantly) sent job postings to R Help, I think
that a SIG would be a good idea.

Max 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Tuesday, February 06, 2007 2:08 PM
To: R-help at stat.math.ethz.ch
Subject: [R] R in Industry

The other day, CNN had a story on working at Google. Out of curiosity, I
went to the Google employment web site (I'm not looking, but just
curious). In perusing their job posts for statisticians, preference is
given to those who use R and python. Other languages, S-Plus and
something called SAS were listed as lower priorities.

When I started using Python, I noted they have a portion of the web site
with job postings. CRAN does not have something similar, but think it
might be useful. I think R is becoming more widely used in industry and
I wonder if helping it move along a bit, the maintainer of CRAN could
create a section of the web site devoted to jobs where R is a
requirement.

Hence, we could have our own little "monster.com" kind of thing going
on. Of the multitude of ways the gospel can be spread, this is small.
But, I think every small step forward is good.

Anyone think this is useful? 

Harold


----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From laura at env.leeds.ac.uk  Thu Feb  8 20:09:00 2007
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Thu, 8 Feb 2007 19:09:00 +0000 (GMT)
Subject: [R] Point estimate from loess contour plot
Message-ID: <Pine.LNX.4.44.0702081904400.1670-100000@gw.env.leeds.ac.uk>

Hi,

I was wondering if anyone knows of a way by which one can estimate values
from a contour plot created by using the loess function? I am hoping to
use the loess contour plot as a means of interpolation to identify
the loess created values at points at pre-defined (x,y) locations.

Could anyone point me in the right direction please?

Thanks.

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk


From ecjbosu at aol.com  Thu Feb  8 20:15:10 2007
From: ecjbosu at aol.com (Joe Byers)
Date: Thu, 08 Feb 2007 13:15:10 -0600
Subject: [R] R in Industry
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>	<45CB4E87.4040605@pburns.seanet.com>
	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
Message-ID: <45CB76BE.2060001@aol.com>

Ben Fairbank wrote:
> To those following this thRead:
> 
> There was a thread on this topic a year or so ago on this list, in which
> contributors mentioned reasons that corporate powers-that-be were
> reluctant to commit to R as a corporate statistical platform.  (My
> favorite was "There is no one to sue if something goes wrong.")
> 
> One reason that I do not think was discussed then, nor have I seen
> discussed since, is the issue of the continuity of support.  If one
> person has contributed disproportionately heavily to the development and
> maintenance of a package, and then retires or follows other interests,
> and the package needs maintenance (perhaps as a consequence of new
> operating systems or a new version of R), is there any assurance that it
> will be available?  With a commercial package such as, say, SPSS, the
> corporate memory and continuance makes such continued maintenance
> likely, but is there such a commitment with R packages?  If my company
> came to depend heavily on a fairly obscure R package (as we are
> contemplating doing), what guarantee is there that it will be available
> next month/year/decade?  I know of none, nor would I expect one.
> 
I would add that if you find a package that performs for your company, 
you have done a couple of things.  One reviewed and benchmarked the 
packages results against others or at least makes sure it passes a 
reasonable economic test.  If this is not done, one is assuming the 
BLACK BOX is always correct.  A sin for many quants.

Over time the requirements of the company will change so some 
modifications will be requested from the lead developer or performed in 
house.  This will lead to a level of expertise in the package that new 
developers or maintainers can keep the continuity of the package going 
long after the lead developer retires.  Especially if the company is 
willing to allocate some resources to this endeavor in leiu of license fees.

For example, recently several of us needed the package RMYsql recompiled 
for windows xp.  We went through the mailing list items related to 
RmySQL for windows, built the binary zip file, and have posted it in 
several places.  We needed the package functionality and took care of 
the problem.  Total time was about 1 day for initial discovery and 
research and now about 30 minutes for upgrading the RmySQL for windows 
after a new version for linux is released.

> As R says when it starts up, "R is free software and comes with
> ABSOLUTELY NO WARRANTY."
> 
> Ben Fairbank
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
> Sent: Thursday, February 08, 2007 10:24 AM
> To: Albrecht,Dr. Stefan (AZ Private Equity Partner)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R in Industry
> 
>  From what I know Matlab is much more popular in
> fixed income than R, but R is vastly more popular in
> equities.  R seems to be making quite a lot of headway
> in finance, even in fixed income to some degree.
> 
> At least to some extent, this is probably logical behavior --
> fixed income is more mathematical, and equities is more
> statistical.
> 
> Matlab is easier to learn mainly because it has much simpler
> data structures.  However, once you are doing something
> where a complex data structure is natural, then R is going to
> be easier to use and you are likely to have a more complete
> implementation of what you want.
> 
> If speed becomes a limiting factor, then moving the heavy
> computing to C is a natural thing to do, and very easy with R.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Albrecht, Dr. Stefan (AZ Private Equity Partner) wrote:
> 
>> Dear all,
>>
>> I was reading with great interest your comments about the use of R in
>> the industry. Personally, I use R as scripting language in the
> financial
>> industry, not so much for its statistical capabilities (which are
>> great), but more for programming. I once switched from S-Plus to R,
>> because I liked R more, it had a better and easier to use documentation
>> and it is faster (especially with loops).
>>
>> Now some colleagues of mine are (finally) eager to join me in my
>> quantitative efforts, but they feel that they are more at ease with
>> Matlab. I can understand this. Matlab has a real IDE with symbolic
>> debugger, integrated editor and profiling, etc. The help files are
>> great, very comprehensive and coherent. It also could be easier to
>> learn.
>>
>> And, I was very astonished to realise, Matlab is very, very much faster
>> with simple "for" loops, which would speed up simulations considerably.
>> So I have trouble to argue for a use of R (which I like) instead of
>> Matlab. The price of Matlab is high, but certainly not prohibitive. R
> is
>> great and free, but maybe less comfortable to use than Matlab.
>>
>> Finally, after all, I have the impression that in many job offerings in
>> the financial industry R is much less often mentioned than Matlab.
>>
>> I would very much appreciate any comments on my above remarks. I know
>> there has been some discussions of R vs. Matlab on R-help, but these
>> could be somewhat out-dated, since both languages are evolving quite
>> quickly.
>>
>> With many thanks and best regards,
>> Stefan Albrecht
>>
>>
>>
>>  [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From joe-byers at utulsa.edu  Thu Feb  8 20:12:57 2007
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 08 Feb 2007 13:12:57 -0600
Subject: [R] R in Industry
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>	<45CB4E87.4040605@pburns.seanet.com>
	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
Message-ID: <eqfsnp$go5$1@sea.gmane.org>

Ben Fairbank wrote:
> To those following this thRead:
> 
> There was a thread on this topic a year or so ago on this list, in which
> contributors mentioned reasons that corporate powers-that-be were
> reluctant to commit to R as a corporate statistical platform.  (My
> favorite was "There is no one to sue if something goes wrong.")
> 
> One reason that I do not think was discussed then, nor have I seen
> discussed since, is the issue of the continuity of support.  If one
> person has contributed disproportionately heavily to the development and
> maintenance of a package, and then retires or follows other interests,
> and the package needs maintenance (perhaps as a consequence of new
> operating systems or a new version of R), is there any assurance that it
> will be available?  With a commercial package such as, say, SPSS, the
> corporate memory and continuance makes such continued maintenance
> likely, but is there such a commitment with R packages?  If my company
> came to depend heavily on a fairly obscure R package (as we are
> contemplating doing), what guarantee is there that it will be available
> next month/year/decade?  I know of none, nor would I expect one.
> 
I would add that if you find a package that performs for your company, 
you have done a couple of things.  One reviewed and benchmarked the 
packages results against others or at least makes sure it passes a 
reasonable economic test.  If this is not done, one is assuming the 
BLACK BOX is always correct.  A sin for many quants.

Over time the requirements of the company will change so some 
modifications will be requested from the lead developer or performed in 
house.  This will lead to a level of expertise in the package that new 
developers or maintainers can keep the continuity of the package going 
long after the lead developer retires.  Especially if the company is 
willing to allocate some resources to this endeavor in leiu of license fees.

For example, recently several of us needed the package RMYsql recompiled 
for windows xp.  We went through the mailing list items related to 
RmySQL for windows, built the binary zip file, and have posted it in 
several places.  We needed the package functionality and took care of 
the problem.  Total time was about 1 day for initial discovery and 
research and now about 30 minutes for upgrading the RmySQL for windows 
after a new version for linux is released.

> As R says when it starts up, "R is free software and comes with
> ABSOLUTELY NO WARRANTY."
> 
> Ben Fairbank
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
> Sent: Thursday, February 08, 2007 10:24 AM
> To: Albrecht,Dr. Stefan (AZ Private Equity Partner)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R in Industry
> 
>  From what I know Matlab is much more popular in
> fixed income than R, but R is vastly more popular in
> equities.  R seems to be making quite a lot of headway
> in finance, even in fixed income to some degree.
> 
> At least to some extent, this is probably logical behavior --
> fixed income is more mathematical, and equities is more
> statistical.
> 
> Matlab is easier to learn mainly because it has much simpler
> data structures.  However, once you are doing something
> where a complex data structure is natural, then R is going to
> be easier to use and you are likely to have a more complete
> implementation of what you want.
> 
> If speed becomes a limiting factor, then moving the heavy
> computing to C is a natural thing to do, and very easy with R.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Albrecht, Dr. Stefan (AZ Private Equity Partner) wrote:
> 
>> Dear all,
>>
>> I was reading with great interest your comments about the use of R in
>> the industry. Personally, I use R as scripting language in the
> financial
>> industry, not so much for its statistical capabilities (which are
>> great), but more for programming. I once switched from S-Plus to R,
>> because I liked R more, it had a better and easier to use documentation
>> and it is faster (especially with loops).
>>
>> Now some colleagues of mine are (finally) eager to join me in my
>> quantitative efforts, but they feel that they are more at ease with
>> Matlab. I can understand this. Matlab has a real IDE with symbolic
>> debugger, integrated editor and profiling, etc. The help files are
>> great, very comprehensive and coherent. It also could be easier to
>> learn.
>>
>> And, I was very astonished to realise, Matlab is very, very much faster
>> with simple "for" loops, which would speed up simulations considerably.
>> So I have trouble to argue for a use of R (which I like) instead of
>> Matlab. The price of Matlab is high, but certainly not prohibitive. R
> is
>> great and free, but maybe less comfortable to use than Matlab.
>>
>> Finally, after all, I have the impression that in many job offerings in
>> the financial industry R is much less often mentioned than Matlab.
>>
>> I would very much appreciate any comments on my above remarks. I know
>> there has been some discussions of R vs. Matlab on R-help, but these
>> could be somewhat out-dated, since both languages are evolving quite
>> quickly.
>>
>> With many thanks and best regards,
>> Stefan Albrecht
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Thu Feb  8 20:22:05 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Feb 2007 14:22:05 -0500
Subject: [R] remove component from list or data frame
In-Reply-To: <45CB676A.4060109@stats.uwo.ca>
References: <Pine.A41.4.63.0702081230270.230358@acsrs3.bu.edu>
	<45CB676A.4060109@stats.uwo.ca>
Message-ID: <45CB785D.9020309@stats.uwo.ca>

On 2/8/2007 1:09 PM, Duncan Murdoch wrote:
> On 2/8/2007 12:30 PM, Jason Horn wrote:
>> Sorry to ask such a simple question, but I can't find the answer after 
>> extensive searching the docs and the web.
>> 
>> How do you remove a component from a list?  For example say you have:
>> 
>> lst<-c(5,6,7,8,9)
> 
> In R jargon, that's a vector, not a list.
>> 
>> How do you remove, for example, the third component in the list?
> 
> lst[-3] will do it.
> 
>> 
>> lst[[3]]]<-NULL     generates an error:  "Error: more elements supplied 
>> than there are to replace"
> 
> The [[ index ]] syntax only works on true lists.

Sigh.  This is just my wishful thinking.  It works on numeric vectors 
too, sometimes.  Just not here.

Duncan Murdoch

>> 
>> 
>> 
>> Also, how do you remove a row from a data frame?  For example, say you 
>> have:
>> 
>> lst1<-c(1,2,3,4,5)
>> lst2<-c(6,7,8,9,10)
>> frame<-data.frame(lst1,lst2)
>> 
>> How do you remove, for example, the second row of frame?
> 
> Same idea:
> 
> frame <- frame[-2, ]
> 
> Duncan Murdoch
>> 
>> Thanks,
>> 
>> - Jason
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jporzak at gmail.com  Thu Feb  8 20:28:10 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Thu, 8 Feb 2007 11:28:10 -0800
Subject: [R] R in Industry - new SIG
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D307779FF5@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D3076E6F3D@groamrexm03.amer.pfizer.com>
	<71257D09F114DA4A8E134DEAC70F25D307779FF5@groamrexm03.amer.pfizer.com>
Message-ID: <2a9c000c0702081128l6531af87t18a60a73b1b07af4@mail.gmail.com>

Thanks Max (& Martin)!

I was about to encourage this. Once the head hunters get wind of this,
I expect a lot of activity - hopefully most will be relevant.

Max, I'd be willing to chip in if you need admin help.

-- 
Best,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA
http://www.linkedin.com/in/jimporzak


On 2/8/07, Kuhn, Max <Max.Kuhn at pfizer.com> wrote:
> Martin Maechler called my bluff on this suggestion. I'm now the admin
> for the new special interest group for R related job postings:
>
>    https://stat.ethz.ch/mailman/listinfo/r-sig-jobs
>
> Please send appropriate emails to this list. There are some simple rules
> for postings (e.g. no attachments etc).
>
> Max
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kuhn, Max
> Sent: Tuesday, February 06, 2007 5:10 PM
> To: Doran, Harold; R-help at stat.math.ethz.ch
> Subject: Re: [R] R in Industry
>
> As someone who has (reluctantly) sent job postings to R Help, I think
> that a SIG would be a good idea.
>
> Max
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
> Sent: Tuesday, February 06, 2007 2:08 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] R in Industry
>
> The other day, CNN had a story on working at Google. Out of curiosity, I
> went to the Google employment web site (I'm not looking, but just
> curious). In perusing their job posts for statisticians, preference is
> given to those who use R and python. Other languages, S-Plus and
> something called SAS were listed as lower priorities.
>
> When I started using Python, I noted they have a portion of the web site
> with job postings. CRAN does not have something similar, but think it
> might be useful. I think R is becoming more widely used in industry and
> I wonder if helping it move along a bit, the maintainer of CRAN could
> create a section of the web site devoted to jobs where R is a
> requirement.
>
> Hence, we could have our own little "monster.com" kind of thing going
> on. Of the multitude of ways the gospel can be spread, this is small.
> But, I think every small step forward is good.
>
> Anyone think this is useful?
>
> Harold
>
>
> ----------------------------------------------------------------------
> LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rvaradhan at jhmi.edu  Thu Feb  8 20:39:41 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 8 Feb 2007 14:39:41 -0500
Subject: [R] R in Industry
In-Reply-To: <45CB5945.10403@gmx.net>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<45CB5945.10403@gmx.net>
Message-ID: <000601c74bb8$e07750d0$7c94100a@win.ad.jhu.edu>

Here is a function to create a Toeplitz matrix of any size, and an example
of a 220 x 220 toeplitz matrix, which was created in almost no time:

################################################################
# Given a vector x, forms a Toeplitz matrix
#
toeplitz <- function (x, sym=T) {
   if (!is.vector(x)) 
       stop("x is not a vector")
   n <- length(x)
   if (!sym) { 
   if (!n%%2) stop("length of vector must be odd")
   n2 <- (n+1)/2
   A <- matrix(NA, n2, n2)
   mat <- matrix(x[col(A) - row(A) + n2], n2, n2)
   }
   else {
   A <- matrix(NA, n, n)
   mat <- matrix(x[abs(col(A) - row(A)) + 1], n, n)
   }
   mat
}
###########################################

> system.time(top.mat <- toeplitz(runif(220)))
[1] 0.00 0.01 0.02   NA   NA

Hope this is fast enough!

Best,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stefan Grosse
Sent: Thursday, February 08, 2007 12:09 PM
To: Albrecht, Dr. Stefan (AZ Private Equity Partner)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R in Industry

I just ran on my Windows PC the benchmark from
http://www.sciviews.org/benchmark/index.html which is pretty old now.
Thats probably the reason for the errors which I did not correct. As
you see R has some advantages but Matlab has also some advantages.
However the differences are not to big. There is only one big
difference which indeed includes loops (Creation of a 220x220 Toeplitz
matrix) where Matlab is much faster. But maybe a simple change in the
programmation can change that...

Has someone in the list an updated script?

Stefan Grosse

The benchmarks:

R 2.4.1
   R Benchmark 2.3
   ===============

   I. Matrix calculation

   ---------------------
Creation, transp., deformation of a 1500x1500 matrix (sec):
0.863333333333335
800x800 normal distributed random matrix ^1000______ (sec):
0.136666666666666
Sorting of 2,000,000 random values__________________ (sec):
0.616666666666665
700x700 cross-product matrix (b = a' * a)___________ (sec):
0.559999999999997
Linear regression over a 600x600 matrix (c = a \ b') (sec):  0 #ERROR


   II. Matrix functions
   --------------------
FFT over 800,000 random values______________________ (sec):
0.559999999999997
Eigenvalues of a 320x320 random matrix______________ (sec):
0.493333333333335
Determinant of a 650x650 random matrix______________ (sec):
0.276666666666666
Cholesky decomposition of a 900x900 matrix__________ (sec):  0 #ERROR
Inverse of a 400x400 random matrix__________________ (sec):  0 #ERROR

   III. Programmation
   ------------------
750,000 Fibonacci numbers calculation (vector calc)_ (sec):
0.466666666666669
Creation of a 2250x2250 Hilbert matrix (matrix calc) (sec):
1.01666666666667
Grand common divisors of 70,000 pairs (recursion)___ (sec):
0.396666666666671
Creation of a 220x220 Toeplitz matrix (loops)_______ (sec):
0.553333333333332
Escoufier's method on a 37x37 matrix (mixed)________ (sec):
2.66999999999999

                      --- End of test ---

Matlab 7.0.4

  Matlab Benchmark 2
   ==================
Number of times each test is run__________________________: 3
 
   I. Matrix calculation
   ---------------------
Creation, transp., deformation of a 1500x1500 matrix (sec): 0.29047
800x800 normal distributed random matrix ^1000______ (sec): 0.42967
Sorting of 2,000,000 random values__________________ (sec): 0.71432
700x700 cross-product matrix (b = a' * a)___________ (sec): 0.14748
Linear regression over a 600x600 matrix (c = a \ b') (sec): 0.12831
                  ------------------------------------------------------
                Trimmed geom. mean (2 extremes eliminated): 0.26403
 
   II. Matrix functions
   --------------------
FFT over 800,000 random values______________________ (sec): 0.24591
Eigenvalues of a 320x320 random matrix______________ (sec): 0.38507
Determinant of a 650x650 random matrix______________ (sec): 0.091612
Cholesky decomposition of a 900x900 matrix__________ (sec): 0.11059
Inverse of a 400x400 random matrix__________________ (sec): 0.069414
                  ------------------------------------------------------
                Trimmed geom. mean (2 extremes eliminated): 0.13556
 
   III. Programmation
   ------------------
750,000 Fibonacci numbers calculation (vector calc)_ (sec): 1.2386
Creation of a 2250x2250 Hilbert matrix (matrix calc) (sec): 3.0541
Grand common divisors of 70,000 pairs (recursion)___ (sec): 1.7637
Creation of a 220x220 Toeplitz matrix (loops)_______ (sec): 0.0045972
Escoufier's method on a 37x37 matrix (mixed)________ (sec): 0.50481
                  ------------------------------------------------------
                Trimmed geom. mean (2 extremes eliminated): 1.0331
 
 
Total time for all 15 tests_________________________ (sec): 9.1786
Overall mean (sum of I, II and III trimmed means/3)_ (sec): 0.33316
                      --- End of test ---

-- 
-------------------------------------------
lic. oec. Stefan Grosse

University of Erfurt
Microeconomics
Nordh?user Str. 63
99089 Erfurt
Germany

phone  +49-361 - 737 45 23
fax    +49-361 - 737 45 29
mobile +49-1609- 760 33 01

web http://www.uni-erfurt.de/mikrooekonomie
mail stefan.grosse at uni-erfurt.de

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Thu Feb  8 20:46:41 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Feb 2007 19:46:41 +0000 (GMT)
Subject: [R] Impossible to get jpeg or png output
In-Reply-To: <45CB54E9.1070406@student.uclouvain.be>
References: <45CB54E9.1070406@student.uclouvain.be>
Message-ID: <Pine.LNX.4.64.0702081945020.10800@gannet.stats.ox.ac.uk>

Probably you have no write permission in the current directory.  That 
message means (as it says) that the process cannot open the file for 
writing, and that is not an R issue.

On Thu, 8 Feb 2007, Mahieux Dimitri wrote:

> Hi,
>
> When I want to output a png file, I have the following error message :
>
> Error dans X11(paste("jpeg::", quality, ":", filename, sep = ""), width,  :
>        inpossible de d?marrer le p?riph?rique JPEG
> De plus : Warning message:
> impossible d'ouvrir le fichier JPEG 'Test.jpeg'
>
> or in english
>
> Error in X11(paste("jpeg::", quality, ":", filename, sep = ""), width,  :
>        inpossible to start the JPEG peripheral
> Warning message:
> impossible to open the file JPEG 'Test.jpeg'
>
> I've checked the capabilities which give :
>
> > capabilities()
>    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
>    TRUE     TRUE    FALSE     TRUE     TRUE     TRUE     TRUE     TRUE
>  cledit    iconv      NLS
>    TRUE     TRUE     TRUE
>
> So I don't understand why I can't have a jpeg file ( or png file because
> I've the same problem to)
>
> Any Idea ?
>
> Thx a lot
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Thu Feb  8 20:49:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Feb 2007 19:49:44 +0000 (GMT)
Subject: [R] Data.frame columns in R console
In-Reply-To: <ba8c09910702081001o3a759e7bk73bc0313f752fd9b@mail.gmail.com>
References: <ba8c09910702081001o3a759e7bk73bc0313f752fd9b@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>

?options, look for 'width'.

I don't know what OS this in: the Windows Rgui has an option to set the
width to the width of the console, but you can override it.

On Thu, 8 Feb 2007, Lauri Nikkinen wrote:

> Hi R-users,
>
>
>
> A newbie question: assume that I have for example 30 columns in my
> data.frame named DF. When I print DF in R console I get columns that don't
> fit on the same row underneath each other. So how do I change the R console
> preferences so that the console does not wrap my data.frame columns? I want
> the columns to be printed next to each other, as in a normal table.
>
>
>
> Cheers,
>
> Lauri
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Thu Feb  8 20:51:45 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 08 Feb 2007 11:51:45 -0800
Subject: [R] R in Industry
In-Reply-To: <88F28D35-F6E3-449F-A9B4-D39BF7785E50@hanover.edu>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>	<45CB4E87.4040605@pburns.seanet.com>	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
	<88F28D35-F6E3-449F-A9B4-D39BF7785E50@hanover.edu>
Message-ID: <45CB7F51.4060402@pdf.com>

      As Duncan indicated, I think R wins overwhelmingly  on this point: 

      What should you do if a key software vendor decides to increase 
their license fees beyond reason or obsolete a key product that burdens 
you with excessive transition costs?  Similarly, what do you do if you 
want to migrate a special application from some obscure operating system 
onto Windows or Linux, or you need some enhancements that should be 
minor but your vendor wants an excessive fee for that service?  If they 
see you as the only customer for a certain modification, their fees may 
be reasonable from their perspective. 

      With R, you can get the source code, so adapting it, modifying it, 
etc., should rarely be a problem.  With commercial software, you almost 
never get the source code, and you should consult attorneys before 
attempting to code something required to escape from a vendor whose fee 
structure is becoming prohibitive.  In many situations, just analyzing 
the legal issues could cost you more than paying someone to modify R 
code to support your changing needs. 

      Spencer Graves

Charilaos Skiadas wrote:
> On Feb 8, 2007, at 12:48 PM, Ben Fairbank wrote:
>   
>>  If my company
>> came to depend heavily on a fairly obscure R package (as we are
>> contemplating doing), what guarantee is there that it will be  
>> available
>> next month/year/decade?  I know of none, nor would I expect one.
>>     
>
> I would imagine that if there was a package that really needed  
> updating, then your company could hire an R programmer for a short  
> time to fix whatever needs fixing, and that would be a much smaller  
> expense than licensing an expensive package like those other ones out  
> there.
>
> But perhaps I am completely wrong in this, I am relatively far from  
> the industry world.
>
> Haris
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From magdahaggag at yahoo.com  Thu Feb  8 20:55:12 2007
From: magdahaggag at yahoo.com (magda haggag)
Date: Thu, 8 Feb 2007 11:55:12 -0800 (PST)
Subject: [R] R
Message-ID: <708862.428.qm@web55412.mail.re4.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/766c3a23/attachment.pl 

From andy1983 at excite.com  Thu Feb  8 21:28:39 2007
From: andy1983 at excite.com (andy1983)
Date: Thu, 8 Feb 2007 12:28:39 -0800 (PST)
Subject: [R] loop issues (r.squared)
Message-ID: <8873580.post@talk.nabble.com>


I would like to compare every column in my matrix with every other column and
get the r-squared. I have been using the following formula and loops:
summary(lm(matrix[,x]~matrix[,y]))$r.squared
where x and y are the looping column numbers

If I have 100 columns (10,000 iterations), the loops give me results in a
reasonable time.
If I try 10,000 columns, the loops take forever even if there is no formula
inside. I am guessing I can vectorize my code so that I could eliminate one
or both loops. Unfortunately, I can't figure out how to.

Any suggestions?

Thanks.
-- 
View this message in context: http://www.nabble.com/loop-issues-%28r.squared%29-tf3195843.html#a8873580
Sent from the R help mailing list archive at Nabble.com.


From aldi at wustl.edu  Thu Feb  8 21:33:11 2007
From: aldi at wustl.edu (Aldi Kraja)
Date: Thu, 08 Feb 2007 14:33:11 -0600
Subject: [R] How to protect two jobs running on the same directory at the
 same time not to corrupt each other results:
Message-ID: <45CB8907.50302@wustl.edu>

Hi,

I have a large group of jobs, some of them are running on the same 
directory.  All of them in batch mode.
What are the best ways to protect from corrupting the results two or 
more jobs running on the same directory.
One, I would think can be to run each job in a separate directory, 
collect the results and after remove the directories. But I have 
thousands of jobs that will run in parallel and I have placed about 100 
of them in each directory. They all do the same process, but on 
different variables, replications etc.

Is there any other solution better than creating separate directories in 
R? I am thinking if there is any option in R to create a unique id which 
has its own unique .Rdata, although in the same directory?

SAS for example to each batch job it assigns a different ID and a 
separate temp space, and does not mix it with another job running in 
parallel.

Thanks,

Aldi

--


From gzhu at peak6.com  Thu Feb  8 21:49:43 2007
From: gzhu at peak6.com (Geoffrey Zhu)
Date: Thu, 8 Feb 2007 14:49:43 -0600
Subject: [R] Scope
Message-ID: <99F81FFD0EA54E4DA8D4F1BFE272F3410362AE7D@ppi-mail1.chicago.peak6.net>

Hi all,

When I write a script, I'd like to create a main() function so that I
only need to type main() t re-run it. However, I'd like all the
variables in main() to be global so that when the function terminates, I
still have access to the variables in the environment. Does anyone know
how to do that?

Best regards,
Geoffrey



_______________________________________________________=0A=
=0A=
=0A=
The information in this email or in any file attached hereto...{{dropped}}


From tim_smith_666 at yahoo.com  Thu Feb  8 21:59:31 2007
From: tim_smith_666 at yahoo.com (Tim Smith)
Date: Thu, 8 Feb 2007 12:59:31 -0800 (PST)
Subject: [R] Help with interfacing C & R
Message-ID: <435724.95394.qm@web58415.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/34539089/attachment.pl 

From d.scott at auckland.ac.nz  Thu Feb  8 22:01:54 2007
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 9 Feb 2007 10:01:54 +1300 (NZDT)
Subject: [R] NEWBIE: @BOOK help?
In-Reply-To: <Pine.LNX.4.64.0702081148390.12331@perrin.socsci.unc.edu>
References: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>
	<Pine.LNX.4.64.0702081148390.12331@perrin.socsci.unc.edu>
Message-ID: <Pine.LNX.4.64.0702090955200.13771@stat12.stat.auckland.ac.nz>

On Thu, 8 Feb 2007, Andrew Perrin wrote:

> It's BibTeX source -- used for the BibTeX bibliography management system
> that integrates with LaTeX.
>
> http://www.ecst.csuchico.edu/~jacobsd/bib/formats/bibtex.html
> http://www.ctan.org
>

A further point is that mathematically oriented databases including the 
Current Index to Statistics (http://www.statindex.org/CIS/) can output 
bibliographic details in BibTeX format. You can obtain the reference in 
BibTeX form from the database and easily incorporate it into your document 
or private BibTeX database of references.

David Scott


_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland 1142,    NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz

Graduate Officer, Department of Statistics


From amirherm at tau.ac.il  Thu Feb  8 22:03:44 2007
From: amirherm at tau.ac.il (Amir Herman)
Date: Thu, 8 Feb 2007 13:03:44 -0800 (PST)
Subject: [R] Running source from Unix line
Message-ID: <8874226.post@talk.nabble.com>


How can I run something like source("filename") from the Unix command line?
Maybe somthing like ./R CMD source("filename") - this does not work.

I need to run an R source code file with a command from the Unix command
line.
assuming that I have R installed on my system.

Thank you all
Amir.

-- 
View this message in context: http://www.nabble.com/Running-source-from-Unix-line-tf3196035.html#a8874226
Sent from the R help mailing list archive at Nabble.com.


From amirherm at tau.ac.il  Thu Feb  8 22:04:26 2007
From: amirherm at tau.ac.il (Amir Herman)
Date: Thu, 8 Feb 2007 13:04:26 -0800 (PST)
Subject: [R] Running source from Unix line
Message-ID: <8874240.post@talk.nabble.com>


How can I run something like source("filename") from the Unix command line?
Maybe somthing like ./R CMD source("filename") - this does not work.

I need to run an R source code file with a command from the Unix command
line.
assuming that I have R installed on my system.

Thank you all
Amir.

-- 
View this message in context: http://www.nabble.com/Running-source-from-Unix-line-tf3196037.html#a8874240
Sent from the R help mailing list archive at Nabble.com.


From matthew_wiener at merck.com  Thu Feb  8 22:03:51 2007
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 8 Feb 2007 16:03:51 -0500
Subject: [R] R  [Broadcast]
In-Reply-To: <708862.428.qm@web55412.mail.re4.yahoo.com>
References: <708862.428.qm@web55412.mail.re4.yahoo.com>
Message-ID: <4E9A692D8755DF478B56A2892388EE1F017B0429@usctmx1118.merck.com>

Hi.  The package you are looking for is not a standard package (that is,
one that gets installed automatically with R).

There is documentation available, though.  From cran
(http://cran.r-project.org), go to manuals, look at "R Installation and
Administration", particularly Section 6, which talks about installing
packages.

Briefly:
Install.packages("KernGPLM") might work (in unix or windows).  And on
Windows, you can also download a zip file from cran and use "Install
package from local zip file" in the menu.

Hope this helps.

Regards,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of magda haggag
Sent: Thursday, February 08, 2007 2:55 PM
To: r-help at stat.math.ethz.ch
Subject: [R] R [Broadcast]

Dear Professor,
   
  I am preparing for a Ph.D in semiparametric regression at Cairo
university in Egypt. Referring to R package KernGPLM, I obtained R
version 2.4.1 but I did not find package KernGPLM. Please, help me how
can I obtain this package. Thanks in advance. 
   
   
  Name: Magda Haggag
  E-mail:  magdahaggag at yahoo.com
  Address: 27, Notrdam Desion st., Gleem, Alexandria, Egypt.

 
---------------------------------
8:00? 8:25? 8:40?  Find a flick in no time

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From Greg.Snow at intermountainmail.org  Thu Feb  8 22:12:14 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 8 Feb 2007 14:12:14 -0700
Subject: [R] loop issues (r.squared)
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7FCA93@LP-EXCHVS07.CO.IHC.COM>

The most straight forward way that I can think of is just:

> cor(my.mat)^2 # assuming my.mat is the matrix with your data in the
columns

That will give you all the R^2 values for regressing 1 column on 1
column (it is called R-squared for a reason).

If you want the R^2 values for regressing one column on all other
columns in the matrix, then a short-cut is:

> 1-1/diag(solve(cor(my.mat)))

Both should be much faster than looping, the 2nd may give problems in
trying to invert a very large matrix.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of andy1983
> Sent: Thursday, February 08, 2007 1:29 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] loop issues (r.squared)
> 
> 
> I would like to compare every column in my matrix with every 
> other column and get the r-squared. I have been using the 
> following formula and loops:
> summary(lm(matrix[,x]~matrix[,y]))$r.squared
> where x and y are the looping column numbers
> 
> If I have 100 columns (10,000 iterations), the loops give me 
> results in a reasonable time.
> If I try 10,000 columns, the loops take forever even if there 
> is no formula inside. I am guessing I can vectorize my code 
> so that I could eliminate one or both loops. Unfortunately, I 
> can't figure out how to.
> 
> Any suggestions?
> 
> Thanks.
> --
> View this message in context: 
> http://www.nabble.com/loop-issues-%28r.squared%29-tf3195843.ht
> ml#a8873580
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Feb  8 22:15:43 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Feb 2007 16:15:43 -0500
Subject: [R] Scope
In-Reply-To: <99F81FFD0EA54E4DA8D4F1BFE272F3410362AE7D@ppi-mail1.chicago.peak6.net>
References: <99F81FFD0EA54E4DA8D4F1BFE272F3410362AE7D@ppi-mail1.chicago.peak6.net>
Message-ID: <971536df0702081315p28cdfbdfned1334a181da48b@mail.gmail.com>

You can assign the environment within main to a variable in
the global environment and that will make it accessible even
after main terminates:

main <- function() {
 assign("main.env", environment(), .GlobalEnv)
 x <- 1; y <- 2
}

main()
main.env$x
main.env$y
# or
attach(main.env)
search()  # note that main.env is on search path
x
y
detach()  # remove from path now that we are finished


Alternately you could assign each variable within main
that you want to save:

main <- function() {
 x <- 1; y <- 2
 assign("x", x, .GlobalEnv)
 assign("y", y, .GlobalEnv)
}

main()
x
y



On 2/8/07, Geoffrey Zhu <gzhu at peak6.com> wrote:
> Hi all,
>
> When I write a script, I'd like to create a main() function so that I
> only need to type main() t re-run it. However, I'd like all the
> variables in main() to be global so that when the function terminates, I
> still have access to the variables in the environment. Does anyone know
> how to do that?
>
> Best regards,
> Geoffrey
>
>
>
> _______________________________________________________=0A=
> =0A=
> =0A=
> The information in this email or in any file attached hereto...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From stefan.albrecht at apep.com  Thu Feb  8 22:18:05 2007
From: stefan.albrecht at apep.com (Albrecht,
	Dr. Stefan (AZ Private Equity Partner))
Date: Thu, 8 Feb 2007 22:18:05 +0100
Subject: [R] R in Industry
Message-ID: <B3E803F92F909741B050C9FA4DDEDE7543A189@naimucog.allianzde.rootdom.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/a9b8beef/attachment.pl 

From osklyar at ebi.ac.uk  Thu Feb  8 22:20:21 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Thu, 08 Feb 2007 21:20:21 +0000
Subject: [R] Help with interfacing C & R
In-Reply-To: <435724.95394.qm@web58415.mail.re3.yahoo.com>
References: <435724.95394.qm@web58415.mail.re3.yahoo.com>
Message-ID: <45CB9415.7080502@ebi.ac.uk>

On Windows you need:
  - download and install Cygwin (cygwin.com) with default options,
    supposedly you install into c:\cygwin. Add path to
    c:\cygwin\bin;c:\cygwin\lib to your system PATH
  - download and unpack Rtools
    (http://www.murdoch-sutherland.com/Rtools/tools.zip)
    Assuming you have them in C:\Rtools, add c:\RTools\bin
    to your PATH _in front of_ cygwin
  - download and install MinGW, you will want to get MinGW-5.1.3.exe,
    which will download and install the rest. You will want to select
    at least gcc and make. Add the path to c:\MinGW\bin to your system
    PATH, right in front of Rtools
    (http://sourceforge.net/projects/mingw/)
  - download and install ActivePerl from (activestate.com), ensure path
    is added to your PATH

For help files:
  - get MS hhc, comes as part of htmlhelp.exe from here:
http://www.microsoft.com/downloads/details.aspx?FamilyID=00535334-c8a6-452f-9aa0-d597d16580cc&DisplayLang=en
    this is Microsoft HTML Help Compiler, add path to it to your PATH
  - you might want to consider MikTex, dowload, install, add to path if
    you have a package and a help system a should be built

Be sure that when installing R you included sources for compilation! You 
might need to reinstall R. When this done, you can try executing R CMD 
SHLIB or R CMD build --binary if you have a package.

Please refer to "Writing R Extensions" (CRAN) for complete reference and 
to this guide for step-by-step description:

http://www.ebi.ac.uk/~osklyar/kb/CtoRinterfacingPrimer.pdf

Regards,
Oleg

--
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


Tim Smith wrote:
> Hi all,
> 
> I was trying to set up an interface for using C functions in R. For this, my R file hello2.r is:
> 
> ---------------------------------------------------------------------------------
> 
> hello2 <- function(n) {
> .C("hello", as.integer(n))
> }
> 
> hello2(3)
> --------------------------------------------------------------------------------
> 
> and my hello.c file is:
> 
> ------------------------------------------------------------------------------
> #include <R.h>
> void hello(int *n)
> {
>     int i;
>     for(i=0; i < *n; i++) {
>     Rprintf("Hello, world!\n");
>     }
> }
> ---------------------------------------------------------------------------
> 
>>From my windows command line, I execute:
> 
>> R CMD SHLIB hello.c
> 
> but I get the error message:
> Error: syntax error in "R CMD". I am trying to look up information on the web page at:  http://cran.r-project.org/doc/manuals/R-exts.html#dyn_002eload-and-dyn_002eunload
> 
> As I understand it, I need to load some files, but I don't understand which commands I need to execute to compile & execute my 'hello world' code.
> 
> I am running R 2.4.0 on Windows XP machine.
> 
> Any help would be highly appreciated.
> 
> thanks!
> 
>  
> ---------------------------------
> Food fight? Enjoy some healthy debate
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From aa2007r at gmail.com  Thu Feb  8 22:23:07 2007
From: aa2007r at gmail.com (AA)
Date: Thu, 8 Feb 2007 16:23:07 -0500
Subject: [R] R in Industry
In-Reply-To: <45CB703B.4010003@stats.uwo.ca>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<45CB4E87.4040605@pburns.seanet.com>
	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
	<45CB703B.4010003@stats.uwo.ca>
Message-ID: <55dcc5de0702081323v2f715c13p1c142ae3252637cd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/17407b84/attachment.pl 

From Abhijit.Dasgupta at mail.jci.tju.edu  Thu Feb  8 22:22:43 2007
From: Abhijit.Dasgupta at mail.jci.tju.edu (Abhijit Dasgupta)
Date: Thu, 08 Feb 2007 16:22:43 -0500
Subject: [R] NEWBIE: @BOOK help?
In-Reply-To: <Pine.LNX.4.64.0702090955200.13771@stat12.stat.auckland.ac.nz>
References: <2E8AE992B157C0409B18D0225D0B476304C57850@XCH-VN01.sph.ad.jhsph.edu>	<Pine.LNX.4.64.0702081148390.12331@perrin.socsci.unc.edu>
	<Pine.LNX.4.64.0702090955200.13771@stat12.stat.auckland.ac.nz>
Message-ID: <45CB94A3.2030706@mail.jci.tju.edu>

as can Google Scholar, which isn't as mathematically oriented. I've 
seen, though, that it isn't quite as accurate as CIS

Abhijit
David Scott wrote:
> On Thu, 8 Feb 2007, Andrew Perrin wrote:
>
>   
>> It's BibTeX source -- used for the BibTeX bibliography management system
>> that integrates with LaTeX.
>>
>> http://www.ecst.csuchico.edu/~jacobsd/bib/formats/bibtex.html
>> http://www.ctan.org
>>
>>     
>
> A further point is that mathematically oriented databases including the 
> Current Index to Statistics (http://www.statindex.org/CIS/) can output 
> bibliographic details in BibTeX format. You can obtain the reference in 
> BibTeX form from the database and easily incorporate it into your document 
> or private BibTeX database of references.
>
> David Scott
>
>
> _________________________________________________________________
> David Scott	Department of Statistics, Tamaki Campus
>  		The University of Auckland, PB 92019
>  		Auckland 1142,    NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> Email:	d.scott at auckland.ac.nz
>
> Graduate Officer, Department of Statistics
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Roger.Bivand at nhh.no  Thu Feb  8 22:41:57 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 8 Feb 2007 22:41:57 +0100 (CET)
Subject: [R] Running source from Unix line
In-Reply-To: <8874226.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.44.0702082235270.15143-100000@reclus.nhh.no>

On Thu, 8 Feb 2007, Amir Herman wrote:

> 
> How can I run something like source("filename") from the Unix command line?
> Maybe somthing like ./R CMD source("filename") - this does not work.
> 
> I need to run an R source code file with a command from the Unix command
> line.
> assuming that I have R installed on my system.

Appendix B of manual An Introduction to R:

http://cran.r-project.org/doc/manuals/R-intro.html#Invoking-R-from-the-command-line

is helpful, or see ?Startup, or google littler R script. For the time 
being, think echo 'source("filename")' | R [options], or cat filename | R

> 
> Thank you all
> Amir.
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From HDoran at air.org  Thu Feb  8 22:43:53 2007
From: HDoran at air.org (Doran, Harold)
Date: Thu, 8 Feb 2007 16:43:53 -0500
Subject: [R] R in Industry
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D49C4@dc1ex01.air.org>

It's been an interseting game of "telephone". Actually, the thread
started with a recommendation to have a place on CRAN for prospective
employers to place job adverts that require R as a skill.

I think the sig is a good idea. But, I think it would be much easier to
have something akin to what Python has on their web site. I should
clarify that I *do not* think this would be an additional responsibility
of the CRAN maintainer(s). But, that prospective employers can populate
this portion of the site on their own. Then, maybe that advert would
expire and disappear from the web site at that time or when it is
removed by the person that created it.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of AA
> Sent: Thursday, February 08, 2007 4:23 PM
> To: Duncan Murdoch
> Cc: r-help at stat.math.ethz.ch; Ben Fairbank
> Subject: Re: [R] R in Industry
> 
> I think we began this thread by comparing Matlab to R. In 
> Matlab one has access to the source code except for some 
> internal functions (There are not that many). The same thing 
> is valid for Splus. The choice of the programming language, 
> beside personal preference has a lot to do with the quality 
> and the number of people using it in the  community. In my 
> own experience Matlab is very good in signal processing while 
> R is good in statistics and both benefit from a solid user's 
> community. What I found though is that the documentation in 
> Matlab is much more user-freindly and practical than R. And 
> that is important in industry.
> 
> Ansel.
> 
> On 2/8/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >
> > On 2/8/2007 12:48 PM, Ben Fairbank wrote:
> > > To those following this thRead:
> > >
> > > There was a thread on this topic a year or so ago on this 
> list, in 
> > > which contributors mentioned reasons that corporate 
> powers-that-be 
> > > were reluctant to commit to R as a corporate statistical 
> platform.  
> > > (My favorite was "There is no one to sue if something 
> goes wrong.")
> > >
> > > One reason that I do not think was discussed then, nor 
> have I seen 
> > > discussed since, is the issue of the continuity of 
> support.  If one 
> > > person has contributed disproportionately heavily to the 
> development 
> > > and maintenance of a package, and then retires or follows other 
> > > interests, and the package needs maintenance (perhaps as a 
> > > consequence of new operating systems or a new version of R), is 
> > > there any assurance that it will be available?  With a commercial 
> > > package such as, say, SPSS, the corporate memory and continuance 
> > > makes such continued maintenance likely, but is there such a 
> > > commitment with R packages?  If my company came to depend 
> heavily on 
> > > a fairly obscure R package (as we are contemplating doing), what 
> > > guarantee is there that it will be available next 
> month/year/decade?  I know of none, nor would I expect one.
> >
> > There's no guarantee of support, but the majority of R packages are 
> > licensed under the GPL, so there is a guarantee of 
> availability of the 
> > source code, which means that contracting with someone 
> expert in the 
> > field to provide you with support will be a possibility.  
> If it's an 
> > obscure package as you say, your company may represent a 
> majority of 
> > users, and it may well be that the expert you need is 
> already someone 
> > in your company, who contributed patches to the package while the 
> > original maintainer was still active.
> >
> > If a commercial vendor were to withdraw support for a 
> package there is 
> > really no hope of putting together your own support service.  You 
> > would have to live with the bugs and without updates, or start from 
> > scratch to replace it yourself.  For example, this happened to me 
> > about 10 years ago when Intel withdrew support for 3DR.  As 
> it happens 
> > OpenGL is a better replacement, but I wasn't too happy at the time.
> >
> > Duncan Murdoch
> >
> > >
> > > As R says when it starts up, "R is free software and comes with 
> > > ABSOLUTELY NO WARRANTY."
> > >
> > > Ben Fairbank
> > >
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Patrick Burns
> > > Sent: Thursday, February 08, 2007 10:24 AM
> > > To: Albrecht,Dr. Stefan (AZ Private Equity Partner)
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: Re: [R] R in Industry
> > >
> > >  From what I know Matlab is much more popular in fixed 
> income than 
> > > R, but R is vastly more popular in equities.  R seems to 
> be making 
> > > quite a lot of headway in finance, even in fixed income to some 
> > > degree.
> > >
> > > At least to some extent, this is probably logical 
> behavior -- fixed 
> > > income is more mathematical, and equities is more statistical.
> > >
> > > Matlab is easier to learn mainly because it has much simpler data 
> > > structures.  However, once you are doing something where 
> a complex 
> > > data structure is natural, then R is going to be easier 
> to use and 
> > > you are likely to have a more complete implementation of what you 
> > > want.
> > >
> > > If speed becomes a limiting factor, then moving the heavy 
> computing 
> > > to C is a natural thing to do, and very easy with R.
> > >
> > > Patrick Burns
> > > patrick at burns-stat.com
> > > +44 (0)20 8525 0696
> > > http://www.burns-stat.com
> > > (home of S Poetry and "A Guide for the Unwilling S User")
> > >
> > > Albrecht, Dr. Stefan (AZ Private Equity Partner) wrote:
> > >
> > >>Dear all,
> > >>
> > >>I was reading with great interest your comments about the 
> use of R 
> > >>in the industry. Personally, I use R as scripting language in the
> > > financial
> > >>industry, not so much for its statistical capabilities (which are 
> > >>great), but more for programming. I once switched from 
> S-Plus to R, 
> > >>because I liked R more, it had a better and easier to use 
> > >>documentation and it is faster (especially with loops).
> > >>
> > >>Now some colleagues of mine are (finally) eager to join me in my 
> > >>quantitative efforts, but they feel that they are more at 
> ease with 
> > >>Matlab. I can understand this. Matlab has a real IDE with 
> symbolic 
> > >>debugger, integrated editor and profiling, etc. The help 
> files are 
> > >>great, very comprehensive and coherent. It also could be 
> easier to 
> > >>learn.
> > >>
> > >>And, I was very astonished to realise, Matlab is very, very much 
> > >>faster with simple "for" loops, which would speed up 
> simulations considerably.
> > >>So I have trouble to argue for a use of R (which I like) 
> instead of 
> > >>Matlab. The price of Matlab is high, but certainly not 
> prohibitive. 
> > >>R
> > > is
> > >>great and free, but maybe less comfortable to use than Matlab.
> > >>
> > >>Finally, after all, I have the impression that in many 
> job offerings 
> > >>in the financial industry R is much less often mentioned 
> than Matlab.
> > >>
> > >>I would very much appreciate any comments on my above remarks. I 
> > >>know there has been some discussions of R vs. Matlab on 
> R-help, but 
> > >>these could be somewhat out-dated, since both languages 
> are evolving 
> > >>quite quickly.
> > >>
> > >>With many thanks and best regards,
> > >>Stefan Albrecht
> > >>
> > >>
> > >>
> > >>      [[alternative HTML version deleted]]
> > >>
> > >>______________________________________________
> > >>R-help at stat.math.ethz.ch mailing list 
> > >>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > >>and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >>
> > >>
> > >>
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From amrahmed76 at gmail.com  Thu Feb  8 22:44:20 2007
From: amrahmed76 at gmail.com (Amr Ahmed)
Date: Thu, 8 Feb 2007 16:44:20 -0500
Subject: [R] R
In-Reply-To: <708862.428.qm@web55412.mail.re4.yahoo.com>
References: <708862.428.qm@web55412.mail.re4.yahoo.com>
Message-ID: <652e09cf0702081344n6622575cha9d6fff849a46216@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/2f1c2fa9/attachment.pl 

From upsattar at yahoo.com  Thu Feb  8 22:47:26 2007
From: upsattar at yahoo.com (Abdus Sattar)
Date: Thu, 8 Feb 2007 13:47:26 -0800 (PST)
Subject: [R] How to get p-values,
	seperate vectors of regression coefficients and their s.e. from the
	"yags" output?
Message-ID: <770988.66524.qm@web58110.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/dc87e2e4/attachment.pl 

From michellang at gmail.com  Thu Feb  8 22:47:59 2007
From: michellang at gmail.com (Michel Lang)
Date: Thu, 8 Feb 2007 22:47:59 +0100
Subject: [R] How to protect two jobs running on the same directory at
	the same time not to corrupt each other results:
In-Reply-To: <45CB8907.50302@wustl.edu>
References: <45CB8907.50302@wustl.edu>
Message-ID: <200702082247.59589.michellang@gmail.com>

Am Donnerstag, 8. Februar 2007 21:33 schrieb Aldi Kraja:
> Is there any other solution better than creating separate directories in
> R? I am thinking if there is any option in R to create a unique id which
> has its own unique .Rdata, although in the same directory?

You could try using tempfile() in combination with basename(). From the 
helppage on tempfile():
    The names are very likely to be unique among calls to 'tempfile'
     in an R session and across simultaneous R sessions.  The filenames
     are guaranteed not to be currently in use.

So this is my suggestion for the generation of a useful, unique id in your 
working dir:

prefix <- paste("var_", var, sep = "")
while(file.exists(id <- basename(tempfile(pattern = prefix)))) {}
save(result, file = paste(id, ".Rdata", sep = ""))

Hope this helps.

Michel Lang


From Roger.Bivand at nhh.no  Thu Feb  8 22:52:06 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 8 Feb 2007 22:52:06 +0100 (CET)
Subject: [R] Scope
In-Reply-To: <971536df0702081315p28cdfbdfned1334a181da48b@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0702082242550.15143-100000@reclus.nhh.no>

On Thu, 8 Feb 2007, Gabor Grothendieck wrote:

> You can assign the environment within main to a variable in
> the global environment and that will make it accessible even
> after main terminates:

Indeed, it can be done, but unless the original questioner is already 
expert at handling scopes and environments, this is an easy way to get 
muddled. Functions take arguments and return values, so putting the output 
into a suitable object and assigning that object:

cars_lm <- lm(dist ~ speed, data=cars)

is robust, easy to grasp, and in particular easy to debug. Thinking
through the design of the output object is usually helpful. Assigning to
the global environment will overwrite objects unless one is careful, and
even with years of experience only seems worth considering when no
feasible alternative exists; on consideration, alternatives usually
appear.

> 
> main <- function() {
>  assign("main.env", environment(), .GlobalEnv)
>  x <- 1; y <- 2
> }
> 
> main()
> main.env$x
> main.env$y
> # or
> attach(main.env)
> search()  # note that main.env is on search path
> x
> y
> detach()  # remove from path now that we are finished
> 
> 
> Alternately you could assign each variable within main
> that you want to save:
> 
> main <- function() {
>  x <- 1; y <- 2
>  assign("x", x, .GlobalEnv)
>  assign("y", y, .GlobalEnv)
> }
> 
> main()
> x
> y
> 
> 
> 
> On 2/8/07, Geoffrey Zhu <gzhu at peak6.com> wrote:
> > Hi all,
> >
> > When I write a script, I'd like to create a main() function so that I
> > only need to type main() t re-run it. However, I'd like all the
> > variables in main() to be global so that when the function terminates, I
> > still have access to the variables in the environment. Does anyone know
> > how to do that?
> >
> > Best regards,
> > Geoffrey
> >
> >
> >
> > _______________________________________________________=0A=
> > =0A=
> > =0A=
> > The information in this email or in any file attached hereto...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From rleigh at whinlatter.ukfsn.org  Thu Feb  8 22:51:39 2007
From: rleigh at whinlatter.ukfsn.org (Roger Leigh)
Date: Thu, 08 Feb 2007 21:51:39 +0000
Subject: [R] Problem with factor state when subset()ing a data.frame
Message-ID: <87d54kuxlg.fsf@hardknott.home>

Hi folks,

I am running into a problem when calling subset() on a large
data.frame.  One of the columns contains strings which are used as
factors.  R seems to automatically factor the column when the
data.frame is contstructed, and this appears to not get updated when I
create a subset of the table.

A minimal testcase to demonstrate the problem follows:


sample <- data.frame(c("A", "A", "A", "A", "B", "B", "B", "C", "C", "C"),
                     c(5,3,5,3,6,7,8,3,2,6))
names(sample) <- c("ID", "Value")

print(sample)

sample.filtered <- subset(sample, ID != "B", select=c(ID, Value))
# Or sample.filtered <- subset(sample, ID != "B", select=c(ID, Value), drop=T)

print(sample.filtered)

plot(sample.filtered)
plot(sample.filtered$Value ~ sample.filtered$ID)

print(levels(sample.filtered$ID))
print(levels(factor(sample.filtered$ID)))

plot(sample.filtered$Value ~ factor(sample.filtered$ID))


Am I doing something wrong here, or is this an R bug?  How can I get
the new data.frame to update the factors, so I don't get redundant
"empty" factors on the plot by eliminating the "phantom" factors?  (I
also need to remove the unused factors for other analyses, and
factoring them "by hand" seems a little redundant.)


Kind regards,
Roger

-- 
  .''`.  Roger Leigh
 : :' :  Debian GNU/Linux             http://people.debian.org/~rleigh/
 `. `'   Printing on GNU/Linux?       http://gutenprint.sourceforge.net/
   `-    GPG Public Key: 0x25BFB848   Please GPG sign your mail.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 188 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070208/3566d3e5/attachment.bin 

From ggrothendieck at gmail.com  Thu Feb  8 23:07:28 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Feb 2007 17:07:28 -0500
Subject: [R] R in Industry
In-Reply-To: <B3E803F92F909741B050C9FA4DDEDE7543A189@naimucog.allianzde.rootdom.net>
References: <B3E803F92F909741B050C9FA4DDEDE7543A189@naimucog.allianzde.rootdom.net>
Message-ID: <971536df0702081407y4f6c2bd7x12319a147c5048c5@mail.gmail.com>

On 2/8/07, Albrecht, Dr. Stefan (AZ Private Equity Partner)
<stefan.albrecht at apep.com> wrote:
> Dear all,
>
> thanks a lot for your comments.
>
> You raise several important points. I also think that depending on a certain
> person maintaining a package can be dangerous, since this person might stop
> working on the package. Even if the package is handed over to a second one,
> the other guy may be less skilled and, e.g.. add errors to an excellent package.

>
> I do not think this is a real threat for the often used and mature packages on CRAN, but there might be the one or the other exception. Still, you cannot blame people doing work for free to the benefit for others, especially, if it is of such a high quality. Many thanks to all for their contributions.
>
> Having access to the source code is not really a solution, unless you have the time to study, maintain and correct it. Normally this is not the case. And do not forget that also for commercial packages, like Matlab, you might have access to at least a large part of the source code.
>
> Another point is that I expect commercial packages to be more coherent and
> concise. The parts should fit better together. In R, e.g.., I use several time series
> packages and classes (and I admire the people having done such marvellous jobs):
> Date, its, zoo. Still one for all would be far better.

Regarding the specific packages you mention:

- 'zoo' has two developers so even if one were not able to continue there
would still be one person left.

- 'its' has had two people working on it sequentially so its history is that
maintenance continued even through turnover of personnel.

In the case of a single developer it certainly would be a good idea if each
package developer were to have a backup person assigned who could
either take it over or agree that to find someone to provide continuity were
it to be necessary.  (I have already done this with those packages of
mine for which I have sole authorship.)


From osklyar at ebi.ac.uk  Thu Feb  8 22:53:33 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Thu, 08 Feb 2007 21:53:33 +0000
Subject: [R] Running source from Unix line
In-Reply-To: <8874226.post@talk.nabble.com>
References: <8874226.post@talk.nabble.com>
Message-ID: <45CB9BDD.3000000@ebi.ac.uk>

See the previous reply, but also if you want to run a shell script and 
need to pass arguments like file names or dir names or anything else, 
you cane you the following (${1} -- 1st argument to the shell script, 
${2} -- second etc):

#!/bin/sh
echo 'a="${1}";b="${2}"; source("codeWith_a_and_b.R")" | R --vanilla --quiet

The from shell (e.g. to process smth in this dir, a, with b=2):

./thisScript `pwd` 2

Oleg

--
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


Amir Herman wrote:
> How can I run something like source("filename") from the Unix command line?
> Maybe somthing like ./R CMD source("filename") - this does not work.
> 
> I need to run an R source code file with a command from the Unix command
> line.
> assuming that I have R installed on my system.
> 
> Thank you all
> Amir.
>


From roland.rproject at gmail.com  Thu Feb  8 23:22:23 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Thu, 8 Feb 2007 17:22:23 -0500
Subject: [R] Help with interfacing C & R
In-Reply-To: <45CB9415.7080502@ebi.ac.uk>
References: <435724.95394.qm@web58415.mail.re3.yahoo.com>
	<45CB9415.7080502@ebi.ac.uk>
Message-ID: <47c7c59e0702081422r3accd518i75d2eb4a23e5b85c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/6eda8b62/attachment.pl 

From efg at stowers-institute.org  Thu Feb  8 23:34:31 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 8 Feb 2007 16:34:31 -0600
Subject: [R] Suggestion about "R equivalent of Splus peaks() function"
Message-ID: <eqg8hn$t77$1@sea.gmane.org>

In 2004 there was this R-Help posting from Jan 2004:

    http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html
    R equivalent of Splus peaks() function?

The peaks function there has worked well for me on a couple of projects, but 
some code using "peaks" failed today, which had worked fine in the past.

I was looking for a peak in a test case that was a sine curve over one 
cycle, so there should have been only one peak.  My unexpected surprise was 
to sometimes get one peak, or two adjoining peaks (a tie), but the no peaks 
case cause subsequent code to fail.  I wanted to eliminate this "no peak" 
case when there was an obvious peak.

I thought it was odd that the peak failure could be controlled by the random 
number seed.

# R equivalent of Splus peaks() function
# http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html

peaks <- function(series,span=3)
{
  z <- embed(series, span)
  s <- span%/%2
  v <- max.col(z) == 1 + s
  result <- c(rep(FALSE,s),v)
  result <- result[1:(length(result)-s)]
  result
}

> set.seed(19)
> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1] FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE
> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE
> peaks(c(1,4,4,1,6,1,5,1,1),3)
[1] FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE


Above, the "4" peak at positions 2 and 3 is shown by the TRUE and FALSE in 
positions 2 and 3 above.  Case 4 of FALSE, FALSE was most unexpected -- no 
peak.


I studied the peaks code and found the problem seems to be in max.col:
> z
     [,1] [,2] [,3]
[1,]    4    4    1
[2,]    1    4    4
[3,]    6    1    4
[4,]    1    6    1
[5,]    5    1    6
[6,]    1    5    1
[7,]    1    1    5

> max.col(z)
[1] 2 3 1 2 3 2 3
> max.col(z)
[1] 2 2 1 2 3 2 3
> max.col(z)
[1] 1 2 1 2 3 2 3
> max.col(z)
[1] 2 2 1 2 3 2 3
> max.col(z)
[1] 1 3 1 2 3 2 3
> max.col(z)
[1] 2 2 1 2 3 2 3

The ?max.col help shows that it has a ties.method that defaults to "random". 
I want a peak, any peak if there is a tie, but I don't want the case that a 
tie is treated as "no peak".  For now, I added a "first" parameter to 
max.col in peaks:

# Break ties by using "first"

peaks <- function(series,span=3)
{
  z <- embed(series, span)
  s <- span%/%2
  v <- max.col(z, "first") == 1 + s
  result <- c(rep(FALSE,s),v)
  result <- result[1:(length(result)-s)]
  result
}

A better solution might be a ties.method parameter to peaks, which can be 
passed to max.col.

I did all of this in R 2.4.1, but the problem seems to be in earlier 
versions too.

Just in case anyone else is using this "peaks" function.

efg

Earl F. Glynn
Stowers Institute for Medical Research


From p.dalgaard at biostat.ku.dk  Thu Feb  8 23:40:43 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 08 Feb 2007 23:40:43 +0100
Subject: [R] Help with interfacing C & R
In-Reply-To: <45CB9415.7080502@ebi.ac.uk> (Oleg Sklyar's message of "Thu, 08
	Feb 2007 21:20:21 +0000")
References: <435724.95394.qm@web58415.mail.re3.yahoo.com>
	<45CB9415.7080502@ebi.ac.uk>
Message-ID: <x2wt2suvbo.fsf@viggo.kubism.ku.dk>

Oleg Sklyar <osklyar at ebi.ac.uk> writes:

> On Windows you need:
>   - download and install Cygwin (cygwin.com) with default options,
>     supposedly you install into c:\cygwin. Add path to
>     c:\cygwin\bin;c:\cygwin\lib to your system PATH
>   - download and unpack Rtools
>     (http://www.murdoch-sutherland.com/Rtools/tools.zip)
>     Assuming you have them in C:\Rtools, add c:\RTools\bin
>     to your PATH _in front of_ cygwin
>   - download and install MinGW, you will want to get MinGW-5.1.3.exe,
>     which will download and install the rest. You will want to select
>     at least gcc and make. Add the path to c:\MinGW\bin to your system
>     PATH, right in front of Rtools
>     (http://sourceforge.net/projects/mingw/)
>   - download and install ActivePerl from (activestate.com), ensure path
>     is added to your PATH

Er, why Cygwin? I don't think that is actually needed for anything
(you may want it for other reasons of course).


Also, this passage in the original post made me suspect that perhaps
the poster didn't know of the Command Prompt ("DOS box") in Windows
and tried typing shell level commands to R itself.

>> ---------------------------------------------------------------------------
>> >>>From my windows command line, I execute:
>> 
>>> R CMD SHLIB hello.c
>> 
>> but I get the error message:
>> Error: syntax error in "R CMD". 


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From andy1983 at excite.com  Thu Feb  8 23:45:16 2007
From: andy1983 at excite.com (andy1983)
Date: Thu, 8 Feb 2007 14:45:16 -0800 (PST)
Subject: [R] loop issues (r.squared)
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB7FCA93@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB7FCA93@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <8875897.post@talk.nabble.com>


That was a neat trick. However, it created a new problem.

Before, it took way too long for a 10,000 columns to finish.

Now, I test the memory limit. With 10,000 columns, I use up about 1.5 GBs.

Assuming memory is not the issue, I still end up with a huge matrix that is
difficult to export. Is there a way to convert it to 3 columns (1 for row, 1
for column, 1 for value)?

Thanks.



Greg Snow wrote:
> 
> The most straight forward way that I can think of is just:
> 
>> cor(my.mat)^2 # assuming my.mat is the matrix with your data in the
> columns
> 
> That will give you all the R^2 values for regressing 1 column on 1
> column (it is called R-squared for a reason).
> 
> 
>> I would like to compare every column in my matrix with every 
>> other column and get the r-squared. I have been using the 
>> following formula and loops:
>> summary(lm(matrix[,x]~matrix[,y]))$r.squared
>> where x and y are the looping column numbers
>> 
>> If I have 100 columns (10,000 iterations), the loops give me 
>> results in a reasonable time.
>> If I try 10,000 columns, the loops take forever even if there 
>> is no formula inside. I am guessing I can vectorize my code 
>> so that I could eliminate one or both loops. Unfortunately, I 
>> can't figure out how to.
> 
> 

-- 
View this message in context: http://www.nabble.com/Re%3A--R--loop-issues-%28r.squared%29-tf3196163.html#a8875897
Sent from the R help mailing list archive at Nabble.com.


From osklyar at ebi.ac.uk  Thu Feb  8 23:49:02 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Thu, 08 Feb 2007 22:49:02 +0000
Subject: [R] Help with interfacing C & R
In-Reply-To: <47c7c59e0702081422r3accd518i75d2eb4a23e5b85c@mail.gmail.com>
References: <435724.95394.qm@web58415.mail.re3.yahoo.com>	
	<45CB9415.7080502@ebi.ac.uk>
	<47c7c59e0702081422r3accd518i75d2eb4a23e5b85c@mail.gmail.com>
Message-ID: <45CBA8DE.4000507@ebi.ac.uk>

Roland,

Sure, this code also compiles on my Linux box straight away, it also 
compiles on my Windows box -- because everything is _already_ set up 
there. The question was not whether the code was wrong -- such a "huge" 
chunk of code could hardly be wrong and it was apparent it was written 
just to try if one can _at all_ compile something for R. The question 
was that something was missing to compile it and Tim did not specify 
anything about his system except that it was Windows!

One might not need Cygwin if the code is like Tim's, i.e. has no 
configuration whatsoever, that is right, but one in any case needs at 
least MinGW (for gcc, visible in your output) and ActivePerl (for R CMD) 
and RTools and R includes (visible in your output). You do have them 
installed, does Tim also have them installed? If one adds a simple 
config file (it will rather not work with a full UNIX-like configure) 
then one might want to have Cygwin or MSYS as well.

Tim, when you get all those, add paths to <smth>\bin and <smth>\lib (as 
there might be dlls and Windows searches for dlls in the PATH) to your 
PATH. This includes R! One requirement: MinGW must be added in front of 
Cygwin (for MinGW make) and RTools in front of both MinGW and Cygwin 
(for all RTools to be used instead of those).

And the pdf file pointed to in my previous post was exactly written for 
someone to start doing these things! It was written for Linux, but the 
only difference on Windows is that you need those tools I mentioned above!

Oleg

--
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


Roland Rau wrote:
> Hi,
> 
> On 2/8/07, *Oleg Sklyar* <osklyar at ebi.ac.uk <mailto:osklyar at ebi.ac.uk>> 
> wrote:
> 
>     On Windows you need:
>       - download and install Cygwin (cygwin.com <http://cygwin.com>)
>     with default options,
>         supposedly you install into c:\cygwin. Add path to
>         c:\cygwin\bin;c:\cygwin\lib to your system PATH 
> 
> 
> No. You don't need Cygwin. I don't have it and I can compile Tim's code 
> without any problem.
> Please see bottom of my message for which I used Tim's code without any 
> modification.
> 
> My assumption is rather that the path is not set correctly so he can 
> call R from the command line.
> Maybe you could check, Tim, if you can start Rterm.exe or Rgui.exe from 
> the command line.
> If not, this should be the first thing to fix.
> 
> 
>  
> 
>     For help files:
>       - get MS hhc, comes as part of htmlhelp.exe from here:
>     http://www.microsoft.com/downloads/details.aspx?FamilyID=00535334-c8a6-452f-9aa0-d597d16580cc&DisplayLang=en
>     <http://www.microsoft.com/downloads/details.aspx?FamilyID=00535334-c8a6-452f-9aa0-d597d16580cc&DisplayLang=en>
>         this is Microsoft HTML Help Compiler, add path to it to your PATH
>       - you might want to consider MikTex, dowload, install, add to path if
>         you have a package and a help system a should be built
> 
> Yes, as you write for the help files. But I think it is not necessary if 
> someone wants to make his/her first steps to interface to C from R.
> 
> 
> Best,
> Roland
> 
> P.S.
> Here is the transcript of my shell session in emacs using Tim's code:
> 
> Microsoft Windows XP [Version 5.1.2600]
> (C) Copyright 1985-2001 Microsoft Corp.
> 
> c:\deletefolder\Rsandbox>R CMD SHLIB hello.c
> R CMD SHLIB hello.c
> making hello.d from hello.c
> gcc   -IC:/rolandprogs/R- 2.3.1/include -Wall -O2   -c hello.c -o hello.o
> gcc  -shared -s  -o hello.dll hello.def hello.o  
> -LC:/rolandprogs/R-2.3.1/bin   -lR
> 
> c:\deletefolder\Rsandbox>Rterm --no-save
> Rterm --no-save
> 
> R : Copyright 2006, The R Foundation for Statistical Computing
> Version 2.3.1 (2006-06-01)
> ISBN 3-900051-07-0
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
> 
>  > source("hello2.r")
> source("hello2.r")
>  > dyn.load("hello")
> dyn.load("hello")
>  > hello2(3)
> hello2(3)
> Hello, world!
> Hello, world!
> Hello, world!
> [[1]]
> [1] 3
> 
>  >


From roger at ysidro.econ.uiuc.edu  Thu Feb  8 23:56:59 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Thu, 8 Feb 2007 16:56:59 -0600
Subject: [R] loop issues (r.squared)
In-Reply-To: <8875897.post@talk.nabble.com>
References: <07E228A5BE53C24CAD490193A7381BBB7FCA93@LP-EXCHVS07.CO.IHC.COM>
	<8875897.post@talk.nabble.com>
Message-ID: <019225AC-5145-4D94-B47F-45C3219262F7@ysidro.econ.uiuc.edu>

both Matrix and SparseM have formats of this type.

url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Feb 8, 2007, at 4:45 PM, andy1983 wrote:

>
> That was a neat trick. However, it created a new problem.
>
> Before, it took way too long for a 10,000 columns to finish.
>
> Now, I test the memory limit. With 10,000 columns, I use up about  
> 1.5 GBs.
>
> Assuming memory is not the issue, I still end up with a huge matrix  
> that is
> difficult to export. Is there a way to convert it to 3 columns (1  
> for row, 1
> for column, 1 for value)?
>
> Thanks.
>
>
>
> Greg Snow wrote:
>>
>> The most straight forward way that I can think of is just:
>>
>>> cor(my.mat)^2 # assuming my.mat is the matrix with your data in the
>> columns
>>
>> That will give you all the R^2 values for regressing 1 column on 1
>> column (it is called R-squared for a reason).
>>
>>
>>> I would like to compare every column in my matrix with every
>>> other column and get the r-squared. I have been using the
>>> following formula and loops:
>>> summary(lm(matrix[,x]~matrix[,y]))$r.squared
>>> where x and y are the looping column numbers
>>>
>>> If I have 100 columns (10,000 iterations), the loops give me
>>> results in a reasonable time.
>>> If I try 10,000 columns, the loops take forever even if there
>>> is no formula inside. I am guessing I can vectorize my code
>>> so that I could eliminate one or both loops. Unfortunately, I
>>> can't figure out how to.
>>
>>
>
> -- 
> View this message in context: http://www.nabble.com/Re%3A--R--loop- 
> issues-%28r.squared%29-tf3196163.html#a8875897
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bates at stat.wisc.edu  Fri Feb  9 00:00:15 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 8 Feb 2007 17:00:15 -0600
Subject: [R] Timings of function execution in R [was Re:  R in Industry]
In-Reply-To: <40e66e0b0702081424u5c25420cp5d915d99552ba568@mail.gmail.com>
References: <40e66e0b0702081424u5c25420cp5d915d99552ba568@mail.gmail.com>
Message-ID: <40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>

On 2/8/07, Albrecht, Dr. Stefan (AZ Private Equity Partner)
<stefan.albrecht at apep.com> wrote:
> Dear all,
>
> Thanks a lot for your comments.
>
> I very well agree with you that writing efficient code is about optimisation. The most important rules I know would be:
> - vectorization
> - pre-definition of vectors, etc.
> - use matrix instead of data.frame
> - do not use named objects
> - use pure matrix instead of involved S4 (perhaps also S3) objects (can have enormous effects)
> - use function instead of expression
> - use compiled code
> - I guess indexing with numbers (better variables) is also much faster than with text (names) (see also above)
> - I even made, for example, my own min, max, since they are slow, e.g.,
>
> greaterOf <- function(x, y){
> # Returns for each element of x and y (numeric)
> # x or y may be a multiple of the other
>   z <- x > y
>   z*x + (!z)*y

That's an interesting function.  I initially was tempted to respond
that you have managed to reinvent a specialized form of the ifelse
function but then I decided to do the timings just to check (always a
good idea).  The enclosed timings show that your function is indeed
faster than a call to ifelse.  A couple of comments:

- I needed to make the number of components in the vectors x and y
quite large before I could  get reliable timings on the system I am
using.

- The recommended way of doing timings is with system.time function,
which makes an effort to minimize the effects of garbage collection on
the timings.

- Even when using system.time there is often a big difference in
timing between the first execution of a function call that generates a
large object and subsequent executions of the same function call.

[additional parts of the original message not relevant to this
discussion have been removed]
-------------- next part --------------
> x <- rnorm(1000000)
> y <- rnorm(1000000)
> system.time(r1 <- greaterOf(x, y))
   user  system elapsed 
  0.255   0.023   0.278 
> system.time(r1 <- greaterOf(x, y))
   user  system elapsed 
  0.054   0.029   0.084 
> system.time(r1 <- greaterOf(x, y))
   user  system elapsed 
  0.057   0.028   0.086 
> system.time(r1 <- greaterOf(x, y))
   user  system elapsed 
  0.083   0.040   0.124 
> system.time(r1 <- greaterOf(x, y))
   user  system elapsed 
  0.099   0.026   0.124 
> system.time(r2 <- ifelse(x > y, x, y))
   user  system elapsed 
  0.805   0.109   0.913 
> system.time(r2 <- ifelse(x > y, x, y))
   user  system elapsed 
  0.723   0.113   0.835 
> system.time(r2 <- ifelse(x > y, x, y))
   user  system elapsed 
  0.641   0.116   0.757 
> system.time(r2 <- ifelse(x > y, x, y))
   user  system elapsed 
  0.647   0.111   0.757 
> all.equal(r1,r2)
[1] TRUE

From roland.rproject at gmail.com  Fri Feb  9 00:06:35 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Thu, 8 Feb 2007 18:06:35 -0500
Subject: [R] Help with interfacing C & R
In-Reply-To: <45CBA8DE.4000507@ebi.ac.uk>
References: <435724.95394.qm@web58415.mail.re3.yahoo.com>
	<45CB9415.7080502@ebi.ac.uk>
	<47c7c59e0702081422r3accd518i75d2eb4a23e5b85c@mail.gmail.com>
	<45CBA8DE.4000507@ebi.ac.uk>
Message-ID: <47c7c59e0702081506u67d016e0jc37b1cbd0f4e1912@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/9bcd76ca/attachment.pl 

From roland.rproject at gmail.com  Fri Feb  9 00:17:02 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Thu, 8 Feb 2007 18:17:02 -0500
Subject: [R] loop issues (r.squared)
In-Reply-To: <8875897.post@talk.nabble.com>
References: <07E228A5BE53C24CAD490193A7381BBB7FCA93@LP-EXCHVS07.CO.IHC.COM>
	<8875897.post@talk.nabble.com>
Message-ID: <47c7c59e0702081517x2fa4ffefubdc125d8aeba862a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/20372820/attachment.pl 

From tim_smith_666 at yahoo.com  Fri Feb  9 00:25:31 2007
From: tim_smith_666 at yahoo.com (Tim Smith)
Date: Thu, 8 Feb 2007 15:25:31 -0800 (PST)
Subject: [R] Help with interfacing C & R
In-Reply-To: <47c7c59e0702081506u67d016e0jc37b1cbd0f4e1912@mail.gmail.com>
Message-ID: <878823.82942.qm@web58401.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/f172e185/attachment.pl 

From ggrothendieck at gmail.com  Fri Feb  9 00:29:49 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Feb 2007 18:29:49 -0500
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>
References: <40e66e0b0702081424u5c25420cp5d915d99552ba568@mail.gmail.com>
	<40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>
Message-ID: <971536df0702081529t1cb67d13j4f8be7eef3d9a7eb@mail.gmail.com>

This may not be exactly the same to the last decimal but is nearly
twice as fast again:

> set.seed(1)
> n <- 1000000
> x <- rnorm(n)
> y <- rnorm(n)
> system.time({z <- x > y; z*x+(!z)*y})
   user  system elapsed
   0.64    0.08    0.72
> system.time({z <- x > y; z * (x-y) + y})
   user  system elapsed
   0.35    0.04    0.39

On 2/8/07, Douglas Bates <bates at stat.wisc.edu> wrote:
> On 2/8/07, Albrecht, Dr. Stefan (AZ Private Equity Partner)
> <stefan.albrecht at apep.com> wrote:
> > Dear all,
> >
> > Thanks a lot for your comments.
> >
> > I very well agree with you that writing efficient code is about optimisation. The most important rules I know would be:
> > - vectorization
> > - pre-definition of vectors, etc.
> > - use matrix instead of data.frame
> > - do not use named objects
> > - use pure matrix instead of involved S4 (perhaps also S3) objects (can have enormous effects)
> > - use function instead of expression
> > - use compiled code
> > - I guess indexing with numbers (better variables) is also much faster than with text (names) (see also above)
> > - I even made, for example, my own min, max, since they are slow, e.g.,
> >
> > greaterOf <- function(x, y){
> > # Returns for each element of x and y (numeric)
> > # x or y may be a multiple of the other
> >   z <- x > y
> >   z*x + (!z)*y
>
> That's an interesting function.  I initially was tempted to respond
> that you have managed to reinvent a specialized form of the ifelse
> function but then I decided to do the timings just to check (always a
> good idea).  The enclosed timings show that your function is indeed
> faster than a call to ifelse.  A couple of comments:
>
> - I needed to make the number of components in the vectors x and y
> quite large before I could  get reliable timings on the system I am
> using.
>
> - The recommended way of doing timings is with system.time function,
> which makes an effort to minimize the effects of garbage collection on
> the timings.
>
> - Even when using system.time there is often a big difference in
> timing between the first execution of a function call that generates a
> large object and subsequent executions of the same function call.
>
> [additional parts of the original message not relevant to this
> discussion have been removed]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From osklyar at ebi.ac.uk  Fri Feb  9 00:38:19 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Thu, 08 Feb 2007 23:38:19 +0000
Subject: [R] Help with interfacing C & R
In-Reply-To: <878823.82942.qm@web58401.mail.re3.yahoo.com>
References: <878823.82942.qm@web58401.mail.re3.yahoo.com>
Message-ID: <45CBB46B.4060901@ebi.ac.uk>

Try the following:

Let's assume you put your R and C code into: E:\MyProgram

I assume R is in E:\prg_files\R\R-2.4.1\bin. I also assume MinGW is 
present and ActivePerl is present. I also assume the following 
directories are present (and full of files): 
E:\prg_files\R\R-2.4.1\include; E:\prg_files\R\R-2.4.1\src\gnuwin32. If 
they are not present -- you probably need to reinstall R with 
development files.

Now click on Start in Windows, run "Run As..." or anything that allows 
to run programs by name, type in the dialog "cmd" and hit OK. This will 
start a black DOS box!
Type:

E: <Enter> it should prompt that you are in E: now
cd MyProgram <Enter> now you are in E:\MyProgram
R CMD SHLIB hello.c <Enter>

If this does not work, type in the same box:

echo %PATH% <Enter>

and send us the contents of this printout (PATH).

Oleg

--
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


Tim Smith wrote:
> Hi All,
> 
> Thanks so much for all the responses. I think I don't know enough about 
> R, and I don't know what other information would be required to diagnose 
> what I may be doing wrong.
> 
> I have installed all the software, and reinstalled R as per Olegs 
> earlier email. I have all programs installed on my E:\ partition (R on 
> it's own works fine from here). When I had executed the code:
> 
>> R CMD SHLIB hello.c
> 
> it was clicking on the 'Rterm.exe' (R for windows front end, at E:\prg_files\R\R-2.4.1\bin). I had put my hello.c and the hell.r file in the bin directory too.
> 
> So, my question is where should I put my files, and which "DOS box" do I have to use? I started cygwin and got a bash command window - what do I do from here?
> 
> Thanks again for all the help, and I do apologize if I'm not giving the 'right' information.
> 
> Tim
> 
> 
> 
> 
> 
> 
> 
> */Roland Rau <roland.rproject at gmail.com>/* wrote:
> 
>     Dear Oleg,
> 
>     to avoid any misunderstandings: My intention was not to offend you.
>     I just thought that the original poster's idea was to compile simply
>     some C-code and interface it from R.
> 
>     I am sorry if my previous email sounded impolite.
> 
>     Best,
>     Roland
> 
> 
>     On 2/8/07, *Oleg Sklyar* <osklyar at ebi.ac.uk
>     <mailto:osklyar at ebi.ac.uk>> wrote:
> 
>         Roland,
> 
>         Sure, this code also compiles on my Linux box straight away, it also
>         compiles on my Windows box -- because everything is _already_ set up
>         there. The question was not whether the code was wrong -- such a
>         "huge"
>         chunk of code could hardly be wrong and it was apparent it was
>         written
>         just to try if one can _at all_ compile something for R. The
>         question
>         was that something was missing to compile it and Tim did not specify
>         anything about his system except that it was Windows!
> 
>         One might not need Cygwin if the code is like Tim's, i.e. has no
>         configuration whatsoever, that is right, but one in any case
>         needs at
>         least MinGW (for gcc, visible in your output) and ActivePerl
>         (for R CMD)
>         and RTools and R includes (visible in your output). You do have them
>         installed, does Tim also have them installed? If one adds a simple
>         config file (it will rather not work with a full UNIX-like
>         configure)
>         then one might want to have Cygwin or MSYS as well.
> 
>         Tim, when you get all those, add paths to <smth>\bin and
>         <smth>\lib (as
>         there might be dlls and Windows searches for dlls in the PATH)
>         to your
>         PATH. This includes R! One requirement: MinGW must be added in
>         front of
>         Cygwin (for MinGW make) and RTools in front of both MinGW and Cygwin
>         (for all RTools to be used instead of those).
> 
>         And the pdf file pointed to in my previous post was exactly
>         written for
>         someone to start doing these things! It was written for Linux,
>         but the
>         only difference on Windows is that you need those tools I
>         mentioned above!
> 
>         Oleg
> 
>         --
>         Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466
> 
> 
>         Roland Rau wrote:
>          > Hi,
>          >
>          > On 2/8/07, *Oleg Sklyar* <osklyar at ebi.ac.uk
>         <mailto:osklyar at ebi.ac.uk> <mailto:osklyar at ebi.ac.uk
>         <mailto:osklyar at ebi.ac.uk>>>
>          > wrote:
>          >
>          >     On Windows you need:
>          >       - download and install Cygwin (cygwin.com
>         <http://cygwin.com> <http://cygwin.com>)
>          >     with default options,
>          >         supposedly you install into c:\cygwin. Add path to
>          >         c:\cygwin\bin;c:\cygwin\lib to your system PATH
>          >
>          >
>          > No. You don't need Cygwin. I don't have it and I can compile
>         Tim's code
>          > without any problem.
>          > Please see bottom of my message for which I used Tim's code
>         without any
>          > modification.
>          >
>          > My assumption is rather that the path is not set correctly so
>         he can
>          > call R from the command line.
>          > Maybe you could check, Tim, if you can start Rterm.exe or
>         Rgui.exe from
>          > the command line.
>          > If not, this should be the first thing to fix.
>          >
>          >
>          >
>          >
>          >     For help files:
>          >       - get MS hhc, comes as part of htmlhelp.exe from here:
>          >    
>         http://www.microsoft.com/downloads/details.aspx?FamilyID=00535334-c8a6-452f-9aa0-d597d16580cc&DisplayLang=en
>         <http://www.microsoft.com/downloads/details.aspx?FamilyID=00535334-c8a6-452f-9aa0-d597d16580cc&DisplayLang=en>
>          >    
>         <http://www.microsoft.com/downloads/details.aspx?FamilyID=00535334-c8a6-452f-9aa0-d597d16580cc&DisplayLang=en
>         <http://www.microsoft.com/downloads/details.aspx?FamilyID=00535334-c8a6-452f-9aa0-d597d16580cc&DisplayLang=en>>
>          >         this is Microsoft HTML Help Compiler, add path to it
>         to your PATH
>          >       - you might want to consider MikTex, dowload, install,
>         add to path if
>          >         you have a package and a help system a should be built
>          >
>          > Yes, as you write for the help files. But I think it is not
>         necessary if
>          > someone wants to make his/her first steps to interface to C
>         from R.
>          >
>          >
>          > Best,
>          > Roland
>          >
>          > P.S.
>          > Here is the transcript of my shell session in emacs using
>         Tim's code:
>          >
>          > Microsoft Windows XP [Version 5.1.2600]
>          > (C) Copyright 1985-2001 Microsoft Corp.
>          >
>          > c:\deletefolder\Rsandbox>R CMD SHLIB hello.c
>          > R CMD SHLIB hello.c
>          > making hello.d from hello.c
>          > gcc   -IC:/rolandprogs/R- 2.3.1/include -Wall -O2   -c
>         hello.c -o hello.o
>          > gcc  -shared -s  -o hello.dll hello.def hello.o
>          > -LC:/rolandprogs/R- 2.3.1/bin   -lR
>          >
>          > c:\deletefolder\Rsandbox>Rterm --no-save
>          > Rterm --no-save
>          >
>          > R : Copyright 2006, The R Foundation for Statistical Computing
>          > Version 2.3.1 (2006-06-01)
>          > ISBN 3-900051-07-0
>          >
>          > R is free software and comes with ABSOLUTELY NO WARRANTY.
>          > You are welcome to redistribute it under certain conditions.
>          > Type 'license()' or 'licence()' for distribution details.
>          >
>          > R is a collaborative project with many contributors.
>          > Type 'contributors()' for more information and
>          > 'citation()' on how to cite R or R packages in publications.
>          >
>          > Type 'demo()' for some demos, 'help()' for on-line help, or
>          > 'help.start()' for an HTML browser interface to help.
>          > Type 'q()' to quit R.
>          >
>          >  > source(" hello2.r")
>          > source("hello2.r")
>          >  > dyn.load("hello")
>          > dyn.load("hello")
>          >  > hello2(3)
>          > hello2(3)
>          > Hello, world!
>          > Hello, world!
>          > Hello, world!
>          > [[1]]
>          > [1] 3
>          >
>          >  >
> 
> 
> 
> ------------------------------------------------------------------------
> TV dinner still cooling?
> Check out "Tonight's Picks" 
> <http://us.rd.yahoo.com/evt=49979/*http://tv.yahoo.com/> on Yahoo! TV.


From rvaradhan at jhmi.edu  Fri Feb  9 00:41:38 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 8 Feb 2007 18:41:38 -0500
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>
References: <40e66e0b0702081424u5c25420cp5d915d99552ba568@mail.gmail.com>
	<40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>
Message-ID: <004801c74bda$b1e30f30$7c94100a@win.ad.jhu.edu>

Hi,

"greaterOf" is indeed an interesting function.  It is much faster than the
equivalent R function, "pmax", because pmax does a lot of checking for
missing data and for recycling.  Tom Lumley suggested a simple function to
replace pmax, without these checks, that is analogous to greaterOf, which I
call fast.pmax.  

fast.pmax <- function(x,y) {i<- x<y; x[i]<-y[i]; x}

Interestingly, greaterOf is even faster than fast.pmax, although you have to
be dealing with very large vectors (O(10^6)) to see any real difference.

> n <- 2000000
> 
>  x1 <- runif(n)
>  x2 <- rnorm(n)
> system.time( ans1 <- greaterOf(x1,x2) )
[1] 0.17 0.06 0.23   NA   NA
> system.time( ans2 <- pmax(x1,x2) )
[1] 0.72 0.19 0.94   NA   NA
> system.time( ans3 <- fast.pmax(x1,x2) )
[1] 0.29 0.05 0.35   NA   NA
>  
> all.equal(ans1,ans2,ans3)
[1] TRUE


Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Douglas Bates
Sent: Thursday, February 08, 2007 6:00 PM
To: R-Help
Subject: [R] Timings of function execution in R [was Re: R in Industry]

On 2/8/07, Albrecht, Dr. Stefan (AZ Private Equity Partner)
<stefan.albrecht at apep.com> wrote:
> Dear all,
>
> Thanks a lot for your comments.
>
> I very well agree with you that writing efficient code is about
optimisation. The most important rules I know would be:
> - vectorization
> - pre-definition of vectors, etc.
> - use matrix instead of data.frame
> - do not use named objects
> - use pure matrix instead of involved S4 (perhaps also S3) objects (can
have enormous effects)
> - use function instead of expression
> - use compiled code
> - I guess indexing with numbers (better variables) is also much faster
than with text (names) (see also above)
> - I even made, for example, my own min, max, since they are slow, e.g.,
>
> greaterOf <- function(x, y){
> # Returns for each element of x and y (numeric)
> # x or y may be a multiple of the other
>   z <- x > y
>   z*x + (!z)*y

That's an interesting function.  I initially was tempted to respond
that you have managed to reinvent a specialized form of the ifelse
function but then I decided to do the timings just to check (always a
good idea).  The enclosed timings show that your function is indeed
faster than a call to ifelse.  A couple of comments:

- I needed to make the number of components in the vectors x and y
quite large before I could  get reliable timings on the system I am
using.

- The recommended way of doing timings is with system.time function,
which makes an effort to minimize the effects of garbage collection on
the timings.

- Even when using system.time there is often a big difference in
timing between the first execution of a function call that generates a
large object and subsequent executions of the same function call.

[additional parts of the original message not relevant to this
discussion have been removed]


From tim_smith_666 at yahoo.com  Fri Feb  9 01:05:49 2007
From: tim_smith_666 at yahoo.com (Tim Smith)
Date: Thu, 8 Feb 2007 16:05:49 -0800 (PST)
Subject: [R] Help with interfacing C & R
In-Reply-To: <45CBB46B.4060901@ebi.ac.uk>
Message-ID: <913272.91054.qm@web58401.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/be69f10a/attachment.pl 

From osklyar at ebi.ac.uk  Fri Feb  9 01:20:11 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Fri, 09 Feb 2007 00:20:11 +0000
Subject: [R] Help with interfacing C & R
In-Reply-To: <913272.91054.qm@web58401.mail.re3.yahoo.com>
References: <913272.91054.qm@web58401.mail.re3.yahoo.com>
Message-ID: <45CBBE3B.40902@ebi.ac.uk>

 >     E:\My Documents\work\projects\landmarks\tryc>ls
 >     hello.c  hello2.r

It is always advisable not to have spaces in such directories (like the 
ones you want to include into a system variable), but fine.

 >     E:\My Documents\work\projects\landmarks\tryc>R CMD SHLIB hello.c
 >     'R' is not recognized as an internal or external command,
 >     operable program or batch file.

Your R is not in the PATH! You need to add it, see also below!

 >     E:\My Documents\work\projects\landmarks\tryc>echo %PATH%
 > 
C:\texmf\miktex\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\P
 >     rogram Files\ATI Technologies\ATI Control
 >     Panel;E:\PROGRA~1\F-Secure\Ssh;E:\Prog
 >     ram Files\ESTsoft\ALZip\;C:\Program
 >     Files\QuickTime\QTSystem\;E:\prg_files\tools
 > 
\bin;E:\prg_files\cygwin\bin;E:\prg_files\cygwin\lib;E:\prg_files\ActivePerl\Act
 > 
ivePerl\Perl\bin;E:\prg_files\ActivePerl\ActivePerl\Perl\lib;E:\Program
 >     Files\ES
 >     Tsoft\ALZip\

There are at least 2 mistakes that I see in your PATH above:
  - you need to add: E:\prg_files\R\R-2.4.1\bin;
  - you need to add path to wherever MinGW\bin is and add it before 
cygwin! if you do not have MinGW installed, download and install it.

I guess E:\prg_files\tools\bin are RTools, if so, this is fine. Your own 
program must not necessarily be in the PATH

--
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


From tim_smith_666 at yahoo.com  Fri Feb  9 01:38:53 2007
From: tim_smith_666 at yahoo.com (Tim Smith)
Date: Thu, 8 Feb 2007 16:38:53 -0800 (PST)
Subject: [R] Help with interfacing C & R
In-Reply-To: <45CBBE3B.40902@ebi.ac.uk>
Message-ID: <663114.18136.qm@web58409.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070208/2b297584/attachment.pl 

From osklyar at ebi.ac.uk  Fri Feb  9 01:48:16 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Fri, 09 Feb 2007 00:48:16 +0000
Subject: [R] Help with interfacing C & R
In-Reply-To: <663114.18136.qm@web58409.mail.re3.yahoo.com>
References: <663114.18136.qm@web58409.mail.re3.yahoo.com>
Message-ID: <45CBC4D0.8020604@ebi.ac.uk>

Glad it worked for you. Unfortunately I will not help you with dyn.load, 
I do not use it and am not well familiar with the technicalities about 
its use. Why?.. please read the pdf file I mentioned in my first post 
and you will see why. Anyway as it compiles now it might probably be the 
best time time to read it to prevent further questions that could be 
prevented. It does not cover Windows, but the part it does not cover you 
have just mastered :) Again here is the address:

http://www.ebi.ac.uk/~osklyar/kb/CtoRinterfacingPrimer.pdf

Best,
Oleg

--
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


Tim Smith wrote:
> Hi Oleg,
> 
> Thanks !! I did as you said, and it works from the Rterm window now.
> 
> My only other question is, that now I have the C code compiled, how do I 
> make 'hello2.r' run from the RGui? I tried to execute the r code from 
> this after I was able to run the code in the Rterm and got the following 
> errors:
> 
>  > source("tryc/hello2.r")
> Error in .C("hello", as.integer(n)) : C symbol name "hello" not in load 
> table
> 
> I also tried with putting 'dyn.load("hello")' at the begining of the r 
> code block, but got the following error:
> 
> ---------------------------------------------------------------------------------------
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library 'E:/My 
> Documents/work/projects/landmarks/hello':
>   LoadLibrary failure:  The specified module could not be found.
> ---------------------------------------------------------------------------------------
> 
> 
> Thanks again for all your help. I guess I'll try to figure out the RGui 
> error tomorrow.
> 
> 
> take care,
> 
> Tim
> 
> 
> */Oleg Sklyar <osklyar at ebi.ac.uk>/* wrote:
> 
>      > E:\My Documents\work\projects\landmarks\tryc>ls
>      > hello.c hello2.r
> 
>     It is always advisable not to have spaces in such directories (like the
>     ones you want to include into a system variable), but fine.
> 
>      > E:\My Documents\work\projects\landmarks\tryc>R CMD SHLIB hello.c
>      > 'R' is not recognized as an internal or external command,
>      > operable program or batch file.
> 
>     Your R is not in the PATH! You need to add it, see also below!
> 
>      > E:\My Documents\work\projects\landmarks\tryc>echo %PATH%
>      >
>     C:\texmf\miktex\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\P
>      > rogram Files\ATI Technologies\ATI Control
>      > Panel;E:\PROGRA~1\F-Secure\Ssh;E:\Prog
>      > ram Files\ESTsoft\ALZip\;C:\Program
>      > Files\QuickTime\QTSystem\;E:\prg_files\tools
>      >
>     \bin;E:\prg_files\cygwin\bin;E:\prg_files\cygwin\lib;E:\prg_files\ActivePerl\Act
>      >
>     ivePerl\Perl\bin;E:\prg_files\ActivePerl\ActivePerl\Perl\lib;E:\Program
>      > Files\ES
>      > Tsoft\ALZip\
> 
>     There are at least 2 mistakes that I see in your PATH above:
>     - you need to add: E:\prg_files\R\R-2.4.1\bin;
>     - you need to add path to wherever MinGW\bin is and add it before
>     cygwin! if you do not have MinGW installed, download and install it.
> 
>     I guess E:\prg_files\tools\bin are RTools, if so, this is fine. Your
>     own
>     program must not necessarily be in the PATH
> 
>     --
>     Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466
> 
> 
> ------------------------------------------------------------------------
> Be a PS3 game guru.
> Get your game face on with the latest PS3 news and previews at Yahoo! 
> Games. <http://us.rd.yahoo.com/evt=49936/*http://videogames.yahoo.com>


From Ray.Brownrigg at mcs.vuw.ac.nz  Fri Feb  9 02:33:51 2007
From: Ray.Brownrigg at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Fri, 9 Feb 2007 14:33:51 +1300
Subject: [R] circle fill problem
In-Reply-To: <748153.10654.qm@web7905.mail.in.yahoo.com>
References: <748153.10654.qm@web7905.mail.in.yahoo.com>
Message-ID: <200702091433.51446.Ray.Brownrigg@mcs.vuw.ac.nz>

I suspect the number is:
trunc((x + 3*r)/(2*r)) * trunc((y + 3*r)/(2*r)) + trunc((x +
    2*r)/(2*r)) * trunc((y + 2*r)/(2*r))
where x and y are the dimensions of the rectangle and r is the radius of the 
circle.

I have some code which graphs the overlapping circles if you want visually to 
check the algorithm (but although it is only a page, I won't post it here).

HTH
Ray Brownrigg

On Thursday 08 February 2007 22:39, MINI GHOSH wrote:
> Dear Ingmar and Robin,
>
> Thanks for you suggestions. I will see to it.
>
> Regards,
> Mini
>
> --- Ingmar Visser <i.visser at uva.nl> wrote:
> > Robin & Mini,
> > For those interested, googling for the 'orange
> > packing problem' as it
> > is known, or more officially the sphere packing
> > problems gives you
> > quite a few hits on these and similar problems.
> > So at least the 3-d case the problem has been solved
> > (I imagine the
> > problem is easier in 2-d ...)
> > hth, Ingmar
> >
> > On 8 Feb 2007, at 09:52, Robin Hankin wrote:
> > > Mini
> > >
> > > This is a hard problem in general.
> > >
> > > Recreational mathematics has wrestled with
> > > this and similar problems over the years; the
> > > general field is the "set cover problem" but
> > > in your case the sets are uncountably infinite
> > > (and there are uncountably many of them).
> > >
> > > I would be surprised if your problem were not NP
> >
> > complete.
> >
> > > HTH
> > >
> > >
> > > Robin
> > >
> > > On 8 Feb 2007, at 05:15, MINI GHOSH wrote:
> > >> Dear R user,
> > >>
> > >> I want to know is there a way to find the minimum
> > >> number of circles (of given radius) required to
> >
> > fill a
> >
> > >> given area (say rectangular) where overlapping of
> > >> circles is allowed.
> > >>
> > >> Thanks,
> > >> Regards,
> > >> Mini Ghosh
> > >>
> > >> ______________________________________________
> > >> R-help at stat.math.ethz.ch mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> >
> > http://www.R-project.org/posting-
> >
> > >> guide.html
> > >> and provide commented, minimal, self-contained,
> >
> > reproducible code.
> >
> > > --
> > > Robin Hankin
> > > Uncertainty Analyst
> > > National Oceanography Centre, Southampton
> > > European Way, Southampton SO14 3ZH, UK
> > >   tel  023-8059-7743
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> >
> > http://www.R-project.org/posting-
> >
> > > guide.html
> > > and provide commented, minimal, self-contained,
> >
> > reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From wangtong at usc.edu  Fri Feb  9 02:59:35 2007
From: wangtong at usc.edu (Tong Wang)
Date: Thu, 08 Feb 2007 17:59:35 -0800
Subject: [R] slice sampler
Message-ID: <ddc6eaa3db37.45cb6507@usc.edu>

Hi there,  
     I wrote a slice sampler (one dimension) ,  After testing it, I found it working well on some standard distributions, but problematic with others.  I have thought about it but really can't figure out how to improve it, so I attached the code here
and hope that someone here is willing to try it out and give me some feedback. I would really appreciate it. 
     I basicly followed neal 03 to write the code. The code might looks a little strange as I allowed sampling a vector of parameters  ,   that is , it could update multiple targets with different parameter but same form of distribution. A example
of usage is as follows:    
      output[i] <- slice1d(dnorm(x,1,1,log=TRUE), x0=output[i-1],w=2,m=6,trunc=c(-Inf,Inf),prec=1e-4) 
 things to notice here are: expression has to be on LOG scale, w is the expanding wideth, m is the # of expanding trials,
prec is the precision, this is used to control against those distributions with huge kurtosis (with very sharp peak),  so that the sampler wouldn't spend a lot of time approaching the right value. 

One example that it doesn't work well on is    dgamma(x,.1,.1,log=TRUE)

Thanks a lot in advance. 
best,

CODE: 

slice1d <- function (expr,x0=NULL,w,m,trunc=c(-Inf,Inf),prec=.0001){
    sexpr <- substitute(expr)
    if (!(is.call(sexpr) && match("x", all.vars(sexpr), nomatch = 0))) 
      stop("'expr' must be a function or an expression containing 'x'")
    expr <- sexpr
    
    y.log <- eval(expr, envir=list(x=x0),enclos = if(is.null(list(...)$env)) parent.frame() else list(...)$env)-rexp(1)
    k <- length(y.log)
    if(k!=length(x0))
      stop("The dimension of y.log and x0 have to agree")

    U <- runif(k)
    L <- x0-w*U
    R <- L+w
    U <- runif(k)
    J <- floor(m*U)
    K <- m-1-J

    temp <- (L<trunc[1])   
    L <- as.vector(L)
    L[temp] <- trunc[1]
    vec.cond <- rep(!temp,k)
    
    while(sum(vec.cond)!=0)
      {
        vec.cond <- ((J>0)&(y.log<eval(expr,list(x=L),enclos = if(is.null(list(...)$env)) parent.frame() else list(...)$env)))
        L[vec.cond] <- L[vec.cond]-w
        temp <- (L<trunc[1])
        L[temp] <- trunc[1]
        J[temp] <- 0
        J[vec.cond] <- J[vec.cond]-1}

    temp <- (R>trunc[2])   
    R <- as.vector(R)
    R[temp] <- trunc[2]
    vec.cond <- rep(!temp,k)

    while(sum(vec.cond)!=0)
      {vec.cond <- ((K>0)&(y.log<eval(expr,list(x=R),enclos = if(is.null(list(...)$env)) parent.frame() else list(...)$env)))
       R[vec.cond] <- R[vec.cond]+w
       temp <- (R>trunc[2])
       R[temp] <- trunc[2]
       K[temp] <- 0
       K[vec.cond] <- K[vec.cond]-1}

    vec.cond <- rep(FALSE,k)
    x1 <- rep(NA,k)
    count <- 0
    repeat{
      U <- runif((k-sum(vec.cond)))
      x1[!vec.cond] <- L[!vec.cond]+U*(R[!vec.cond]-L[!vec.cond])
      vec.cond <- ((y.log<eval(expr,list(x=x1),enclos = if(is.null(list(...)$env)) parent.frame() else list(...)$env))|(abs(x1-x0)<prec)) 
      if(sum(vec.cond)==k) {return(x1)}
      L[x1<=x0] <- x1[x1<=x0]
      R[x1>x0] <- x1[x1>x0]
      count <- count+1
      if (count>=10000) stop("dead loop")
    }
  }


From kubovy at virginia.edu  Fri Feb  9 03:38:23 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 8 Feb 2007 21:38:23 -0500
Subject: [R] How to count the number of NAs in each column of a df?
Message-ID: <1EDE1BF8-26F8-495E-A838-9C8EB7E8C23E@virginia.edu>

I would like to remove columns of a df which have too many NAs.

I think that summary() should give me the information, I just don't  
know how to access it.

Advice?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From jgerald at berkeley.edu  Fri Feb  9 03:58:45 2007
From: jgerald at berkeley.edu (jgerald at berkeley.edu)
Date: Thu, 8 Feb 2007 18:58:45 -0800 (PST)
Subject: [R] using weights with survfit
Message-ID: <41692.71.198.247.111.1170989925.squirrel@calmail.berkeley.edu>

Hi,

I am trying to do a weighted cox regression, specifically I need a
weighted estimate of the baseline survival function.  I'm getting an
'unused argument' error when I try to use the weights option in survfit. 
Looking at the code, it looks as though the documentation may be wrong and
you actually can't use weights with survfit at this time where it says

if (!is.null((object$call)$weights))
        stop("Survfit cannot (yet) compute the result for a weighted model")

Are there any alternatives at this time for getting a weighted estimate of
the  baseline survival function from a cox ph model?  I noticed someone
had the same question in 2002 and there were no responses but thought it
was worth a shot!

Thanks,
Jessica


From rmh at temple.edu  Fri Feb  9 04:16:59 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu,  8 Feb 2007 22:16:59 -0500 (EST)
Subject: [R] How to count the number of NAs in each column of a df?
Message-ID: <20070208221659.BTZ03190@po-d.temple.edu>

drop.col.kna <- function(mydf, k)
 mydf[sapply(mydf, function(x) sum(is.na(x))) < k]


tmp <- data.frame(matrix(1:24, 6,4, dimnames=list(letters[1:6], LETTERS[1:4])))
tmp[1:3,1] <- NA
tmp[2:5,2] <- NA
tmp[6,3] <- NA

drop.col.kna(tmp, 0)
drop.col.kna(tmp, 1)
drop.col.kna(tmp, 2)
drop.col.kna(tmp, 3)
drop.col.kna(tmp, 4)
drop.col.kna(tmp, 5)
drop.col.kna(tmp, 6)


From gfmagnus at uchicago.edu  Fri Feb  9 04:20:20 2007
From: gfmagnus at uchicago.edu (Gideon Magnus)
Date: Thu, 8 Feb 2007 21:20:20 -0600
Subject: [R] question about loading shared objects into R
Message-ID: <AC551964-52C8-4710-A025-48EF29A105B4@uchicago.edu>

Hi

I am running the latest version of R on MacOS X. I have the following  
problem:

I create a .so file (using C code) with R CMD SHLIB and then load it  
into R using dyn.load. So far no problem at all. However, if I want  
to change the content of the library this is difficult. If I first  
unload the library in R, then change the code, delete the old .so  
file, then recompile, then reload the library in R, I am still  
loading the old library! The only way I have been able to solve this  
is by restarting R, and then loading the new .so file.  Is there a  
remedy for this problem?

thanks a lot,

Gideon


From sekhon at berkeley.edu  Fri Feb  9 05:10:46 2007
From: sekhon at berkeley.edu (Jasjeet Singh Sekhon)
Date: Thu, 8 Feb 2007 20:10:46 -0800
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <45CB6D53.5000002@ucsd.edu>
References: <45C38E90.9070709@ucsd.edu>
	<17860.61176.520222.602660@lapo.berkeley.edu>
	<17861.4763.359422.817825@macht.arts.cornell.edu>
	<45C52F17.1030601@ucsd.edu>
	<17861.13360.697951.302800@macht.arts.cornell.edu>
	<17861.25637.324856.683429@lapo.berkeley.edu>
	<45C93EEF.9080509@ucsd.edu>
	<17865.18890.134246.794371@macht.arts.cornell.edu>
	<45CB6D53.5000002@ucsd.edu>
Message-ID: <17867.62534.447496.543187@lapo.berkeley.edu>


As we noted earlier and as is clearly stated in the docs, multinomRob
is estimating an OVERDISPERSED multinomial model.  And in your models
here the overdispersion parameter is not identified; you need more
observations.  Walter pointed out using the print.level trick to get
the coefs for the standard MNL model, but when the model the function
is actually trying to estimate is not identified, that trick will not
work.

As I also previously noted, it is a simple matter to change the
multinomMLE function to estimate the standard MNL model.  Since you
don't want to change that file and since nnet's multinom function
doesn't have some functionality people need, we'll add a "MLEonly"
function to multinomRob which will allow you to do what you want.
We'll post a new version on my webpage later tonight:
http://sekhon.berkeley.edu/robust.  And after some testing, we'll
forward the new version to CRAN.

Jas.

=======================================
Jasjeet S. Sekhon                     
                                      
Associate Professor             
Travers Department of Political Science
Survey Research Center          
UC Berkeley                     

http://sekhon.berkeley.edu/
V: 510-642-9974  F: 617-507-5524
=======================================




Roger Levy writes:
 > Walter Mebane wrote:
 > > Roger,
 > > 
 > >  > Error in if (logliklambda < loglik) bvec <- blambda :
 > >  > 	missing value where TRUE/FALSE needed
 > >  > In addition: Warning message:
 > >  > NaNs produced in: sqrt(sigma2GN)
 > > 
 > > That message comes from the Newton algorithm (defined in source file
 > > multinomMLE.R).  It would be better if we bullet-proofed it a bit
 > > more.  The first thing is to check the data.  I don't have the
 > > multinomLogis() function, so I can't run your code.  
 > 
 > Whoops, sorry about that -- I'm putting revised code at the end of the 
 > message.
 > 
 > > But do you really
 > > mean
 > > 
 > >  > for(i in 1:length(choice)) {
 > > and
 > >  > dim(counts) <- c(length(choice),length(choice))
 > > 
 > > Should that be
 > > 
 > >   for(i in 1:n) {
 > > and
 > >   dim(counts) <- c(n, length(choice))
 > > 
 > > or instead of n, some number m > length(choice).  As it is it seems to
 > > me you have three observations for three categories, which isn't going
 > > to work (there are five coefficient parameters, plus sigma for the
 > > dispersion).
 > 
 > I really did mean for(i in 1:length(choice)) -- once again, the proper 
 > code is at the end of this message.
 > 
 > Also, I notice that I get the same error with another kind of data, 
 > which works for multinom from nnet:
 > 
 > 
 > library(nnet)
 > library(multinomRob)
 > dtf <- data.frame(y1=c(1,1),y2=c(2,1),y3=c(1,2),x=c(0,1))
 > summary(multinom(as.matrix(dtf[,1:3]) ~ x, data=dtf))
 > summary(multinomRob(list(y1 ~ 0, y2 ~ x, y3 ~ x), data=dtf,print.level=128))
 > 
 > 
 > The call to multinom fits the following coefficients:
 > 
 > Coefficients:
 >      (Intercept)          x
 > y2 0.6933809622 -0.6936052
 > y3 0.0001928603  0.6928327
 > 
 > but the call to multinomRob gives me the following error:
 > 
 > multinomRob(): Grouped MNL Estimation
 > [1] "multinomMLE: -loglik initial: 9.48247391895106"
 > Error in if (logliklambda < loglik) bvec <- blambda :
 > 	missing value where TRUE/FALSE needed
 > In addition: Warning message:
 > NaNs produced in: sqrt(sigma2GN)
 > 
 > 
 > Does this shed any light on things?
 > 
 > 
 > Thanks again,
 > 
 > Roger
 > 
 > 
 > 
 > 
 > 
 > ***
 > 
 > set.seed(10)
 > library(multinomRob)
 > multinomLogis <- function(vector) {
 >    x <- exp(vector)
 >    z <- sum(x)
 >    x/z
 > }
 > 
 > n <- 20
 > choice <- c("A","B","C")
 > intercepts <- c(0.5,0.3,0.2)
 > prime.strength <- rep(0.4,length(intercepts))
 > counts <- c()
 > for(i in 1:length(choice)) {
 >    u <- intercepts[1:length(choice)]
 >    u[i] <- u[i] + prime.strength[i]
 >    counts <- c(counts,rmultinomial(n = n, pr = multinomLogis(u)))
 > }
 > dim(counts) <- c(length(choice),length(choice))
 > counts <- t(counts)
 > row.names(counts) <- choice
 > colnames(counts) <- choice
 > data <- data.frame(Prev.Choice=choice,counts)
 > 
 > for(i in 1:length(choice)) {
 >    data[[paste("last",choice[i],sep=".")]] <- 
 > ifelse(data$Prev.Choice==choice[i],1,0)
 > }
 > 
 > multinomRob(list(A ~ last.A ,
 >                   B ~ last.B ,
 >                   C ~ last.C - 1 ,
 >                   ),
 >              data=data,
 >              print.level=128)
 > 
 > 
 > 
 > I obtained this output:
 > 
 > 
 > Your Model (xvec):
 >                                 A B C
 > (Intercept)/(Intercept)/last.C 1 1 1
 > last.A/last.B/NA               1 1 0
 > 
 > [1] "multinomRob:  WARNING.  Limited regressor variation..."
 > [1] "WARNING.  ... A regressor has a distinct value for only one 
 > observation."
 > [1] "WARNING.  ... I'm using a modified estimation algorithm (i.e., 
 > preventing LQD"
 > [1] "WARNING.  ... from modifying starting values for the affected 
 > parameters)."
 > [1] "WARNING.  ... Affected parameters are TRUE in the following table."
 > 
 >                                     A     B     C
 > (Intercept)/(Intercept)/last.C FALSE FALSE  TRUE
 > last.A/last.B/NA                TRUE  TRUE FALSE
 > 
 > 
 > 
 > multinomRob(): Grouped MNL Estimation
 > [1] "multinomMLE: -loglik initial: 70.2764843511374"
 > Error in if (logliklambda < loglik) bvec <- blambda :
 > 	missing value where TRUE/FALSE needed
 > In addition: Warning message:
 > NaNs produced in: sqrt(sigma2GN)


From gfmagnus at uchicago.edu  Fri Feb  9 04:20:20 2007
From: gfmagnus at uchicago.edu (Gideon Magnus)
Date: Thu, 8 Feb 2007 21:20:20 -0600
Subject: [R] question about loading shared objects into R
Message-ID: <AC551964-52C8-4710-A025-48EF29A105B4@uchicago.edu>

Hi

I am running the latest version of R on MacOS X. I have the following  
problem:

I create a .so file (using C code) with R CMD SHLIB and then load it  
into R using dyn.load. So far no problem at all. However, if I want  
to change the content of the library this is difficult. If I first  
unload the library in R, then change the code, delete the old .so  
file, then recompile, then reload the library in R, I am still  
loading the old library! The only way I have been able to solve this  
is by restarting R, and then loading the new .so file.  Is there a  
remedy for this problem?

thanks a lot,

Gideon


From aiminy at iastate.edu  Fri Feb  9 06:15:28 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Thu, 08 Feb 2007 23:15:28 -0600
Subject: [R] Help with interfacing C & R
In-Reply-To: <47c7c59e0702081422r3accd518i75d2eb4a23e5b85c@mail.gmail.co
 m>
References: <435724.95394.qm@web58415.mail.re3.yahoo.com>
	<45CB9415.7080502@ebi.ac.uk>
	<47c7c59e0702081422r3accd518i75d2eb4a23e5b85c@mail.gmail.com>
Message-ID: <6.1.2.0.2.20070208231002.01cf4e90@aiminy.mail.iastate.edu>

in linux machine

it should be

 > source("hello2.r")
 > dyn.load("hello.so")
 > hello2(4)
My first try for C-R interface
My first try for C-R interface
My first try for C-R interface
My first try for C-R interface
[[1]]
[1] 4


>On 2/8/07, Oleg Sklyar <osklyar at ebi.ac.uk> wrote:
> >
> > On Windows you need:
> >   - download and install Cygwin (cygwin.com) with default options,
> >     supposedly you install into c:\cygwin. Add path to
> >     c:\cygwin\bin;c:\cygwin\lib to your system PATH
>
>
>No. You don't need Cygwin. I don't have it and I can compile Tim's code
>without any problem.
>Please see bottom of my message for which I used Tim's code without any
>modification.
>
>My assumption is rather that the path is not set correctly so he can call R
>from the command line.
>Maybe you could check, Tim, if you can start Rterm.exe or Rgui.exe from the
>command line.
>If not, this should be the first thing to fix.
>
>
>
>
>For help files:
> >   - get MS hhc, comes as part of htmlhelp.exe from here:
> >
> > 
> http://www.microsoft.com/downloads/details.aspx?FamilyID=00535334-c8a6-452f-9aa0-d597d16580cc&DisplayLang=en
> >     this is Microsoft HTML Help Compiler, add path to it to your PATH
> >   - you might want to consider MikTex, dowload, install, add to path if
> >     you have a package and a help system a should be built
>
>Yes, as you write for the help files. But I think it is not necessary if
>someone wants to make his/her first steps to interface to C from R.
>
>
>Best,
>Roland
>
>P.S.
>Here is the transcript of my shell session in emacs using Tim's code:
>
>Microsoft Windows XP [Version 5.1.2600]
>(C) Copyright 1985-2001 Microsoft Corp.
>
>c:\deletefolder\Rsandbox>R CMD SHLIB hello.c
>R CMD SHLIB hello.c
>making hello.d from hello.c
>gcc   -IC:/rolandprogs/R-2.3.1/include -Wall -O2   -c hello.c -o hello.o
>gcc  -shared -s  -o hello.dll hello.def hello.o  -LC:/rolandprogs/R-2.3.1/bin
>-lR
>
>c:\deletefolder\Rsandbox>Rterm --no-save
>Rterm --no-save
>
>R : Copyright 2006, The R Foundation for Statistical Computing
>Version 2.3.1 (2006-06-01)
>ISBN 3-900051-07-0
>
>R is free software and comes with ABSOLUTELY NO WARRANTY.
>You are welcome to redistribute it under certain conditions.
>Type 'license()' or 'licence()' for distribution details.
>
>R is a collaborative project with many contributors.
>Type 'contributors()' for more information and
>'citation()' on how to cite R or R packages in publications.
>
>Type 'demo()' for some demos, 'help()' for on-line help, or
>'help.start()' for an HTML browser interface to help.
>Type 'q()' to quit R.
>
> > source("hello2.r")
>source("hello2.r")
> > dyn.load("hello")
>dyn.load("hello")
> > hello2(3)
>hello2(3)
>Hello, world!
>Hello, world!
>Hello, world!
>[[1]]
>[1] 3
>
> >
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From skiadas at hanover.edu  Fri Feb  9 07:18:31 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Fri, 9 Feb 2007 01:18:31 -0500
Subject: [R] Scope
In-Reply-To: <Pine.LNX.4.44.0702082242550.15143-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0702082242550.15143-100000@reclus.nhh.no>
Message-ID: <AF25FD84-A214-4164-90F3-148DF641F676@hanover.edu>

On Feb 8, 2007, at 4:52 PM, Roger Bivand wrote:
> Assigning to
> the global environment will overwrite objects unless one is  
> careful, and
> even with years of experience only seems worth considering when no
> feasible alternative exists; on consideration, alternatives usually
> appear.

Or to paraphrase fortune(106):

	If the answer is global variables, then you should usually rethink  
the question.

Haris


From simon.kempf at web.de  Fri Feb  9 07:21:00 2007
From: simon.kempf at web.de (Simon P. Kempf)
Date: Fri, 9 Feb 2007 07:21:00 +0100
Subject: [R] subset function
Message-ID: <E1HFP8C-0003DS-00@smtp08.web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/d93ea4a2/attachment.pl 

From xmeng at capitalbio.com  Fri Feb  9 07:48:12 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Fri, 09 Feb 2007 14:48:12 +0800
Subject: [R] "class" package
Message-ID: <371003692.09420@capitalbio.com>

Hello sir:
Where can I download the package "class"?
What I can find is only:
http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/class/html/00Index.html

But I can't find where I can download it.

Thanks!


From simon.kempf at web.de  Fri Feb  9 07:36:20 2007
From: simon.kempf at web.de (Simon P. Kempf)
Date: Fri, 9 Feb 2007 07:36:20 +0100
Subject: [R] LM Model
Message-ID: <E1HFPMy-00018e-00@smtp07.web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/ebc0c689/attachment.pl 

From petr.pikal at precheza.cz  Fri Feb  9 08:21:53 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Feb 2007 08:21:53 +0100
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <45CA5964.1080803@biostat.wisc.edu>
References: <27739.27117.qm@web32804.mail.mud.yahoo.com>
Message-ID: <45CC2F21.19905.16F64B@localhost>

Hi

Strange. It works for me without any problem.

> zeta
  tepl tio2 al2o3  iep
1   60    1   3.5 5.65
2   60    1   2.0 5.00
3   60    1   1.0 5.30
4   60    0   2.0 4.65
5   40    1   3.5 5.20
6   40    1   2.0 4.85
7   40    0   3.5 5.70
8   40    0   2.0 5.25
> zeta[zeta==1]<-NA
> zeta
  tepl tio2 al2o3  iep
1   60   NA   3.5 5.65
2   60   NA   2.0 5.00
3   60   NA    NA 5.30
4   60    0   2.0 4.65
5   40   NA   3.5 5.20
6   40   NA   2.0 4.85
7   40    0   3.5 5.70
8   40    0   2.0 5.25

> str(zeta)
'data.frame':   8 obs. of  4 variables:
 $ tepl : int  60 60 60 60 40 40 40 40
 $ tio2 : num  NA NA NA 0 NA NA 0 0
 $ al2o3: num  3.5 2 NA 2 3.5 2 3.5 2
 $ iep  : num  5.65 5 5.3 4.65 5.2 4.85 5.7 5.25
>

HTH
Petr



On 7 Feb 2007 at 16:57, Erik Iverson wrote:

Date sent:      	Wed, 07 Feb 2007 16:57:40 -0600
From:           	Erik Iverson <iverson at biostat.wisc.edu>
To:             	John Kane <jrkrideau at yahoo.ca>
Copies to:      	R R-help <r-help at stat.math.ethz.ch>
Subject:        	Re: [R] setting a number of values to NA over  a data.frame.

> John -
> 
> Your initial problem uses 0, but the example uses 1 for the value that
> gets an NA.  My solution uses 1 to fit with your example.  There may
> be a better way, but try something like
> 
> data1[3:5] <- data.frame(lapply(data1[3:5], function(x) ifelse(x==1,
> NA, x)))
> 
> The data1[3:5] is just a test subset  of columns I chose from your
> data1 example.  Notice it appears twice, once on each side of the
> assignment operator.
> 
> In English, apply to each column of the data frame (which is a list) a
> function that will return NA if the element is 1, and the value
> otherwise, and then turn the modified lists into a data.frame, and
> save it as data1.
> 
> 
> 
> See the help files for lapply and ifelse if you haven't seen those
> before.
> 
> Maybe someone has a better way?
> 
> Erik
> 
> John Kane wrote:
> > This is probably a simple problem but I don't see a
> > solution.
> > 
> > I have a data.frame with a number of columns where I
> > would like 0 <- NA
> > 
> > thus I have df1[,144:157] <- NA if df1[, 144: 157] ==0
> > and df1[, 190:198] <- NA if df1[, 190:198] ==0
> > 
> > but I cannot figure out a way do this.  
> > 
> > cata <- c( 1,1,6,1,1,NA)
> > catb <- c( 1,2,3,4,5,6)
> > doga <- c(3,5,3,6,4, 0)
> > dogb <- c(2,4,6,8,10, 12)
> > rata <- c (NA, 9, 9, 8, 9, 8)
> > ratb <- c( 1,2,3,4,5,6)
> > bata <- c( 12, 42,NA, 45, 32, 54)
> > batb <- c( 13, 15, 17,19,21,23)
> > id <- c('a', 'b', 'b', 'c', 'a', 'b')
> > site <- c(1,1,4,4,1,4)
> > mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> > bata, batb)
> > 
> > data1 <- data.frame(site, id, mat1)
> > data1
> > 
> >  # Obviously this works fine for one column
> > 
> > data1$site[data1$site ==1] <- NA  ; data1
> > 
> > but I cannot see how to do this with indices that
> > would allow me to do more than one column in the
> > data.frame.
> > 
> > At one point I even tried something like this
> > a <- c("site")
> > data1$a[data1$a ==1] <- NA
> > 
> > which seems to produce a corrupt data.frame.
> > 
> > I am sure it is simple but I don't see it.  
> > 
> > Any help would be much appreciated.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Fri Feb  9 08:44:15 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Feb 2007 08:44:15 +0100
Subject: [R] Suggestion about "R equivalent of Splus peaks() function"
In-Reply-To: <eqg8hn$t77$1@sea.gmane.org>
Message-ID: <45CC345F.1202.2B707B@localhost>

Hi

On 8 Feb 2007 at 16:34, Earl F. Glynn wrote:

To:             	r-help at stat.math.ethz.ch
From:           	"Earl F. Glynn" <efg at stowers-institute.org>
Date sent:      	Thu, 8 Feb 2007 16:34:31 -0600
Subject:        	[R] Suggestion about "R equivalent of Splus peaks() function"

> In 2004 there was this R-Help posting from Jan 2004:
> 
>     http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html
>     R equivalent of Splus peaks() function?
> 
> The peaks function there has worked well for me on a couple of
> projects, but some code using "peaks" failed today, which had worked
> fine in the past.
> 
> I was looking for a peak in a test case that was a sine curve over one
> cycle, so there should have been only one peak.  My unexpected
> surprise was to sometimes get one peak, or two adjoining peaks (a
> tie), but the no peaks case cause subsequent code to fail.  I wanted
> to eliminate this "no peak" case when there was an obvious peak.
> 
> I thought it was odd that the peak failure could be controlled by the
> random number seed.
> 
> # R equivalent of Splus peaks() function
> # http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html
> 
> peaks <- function(series,span=3)
> {
>   z <- embed(series, span)
>   s <- span%/%2
>   v <- max.col(z) == 1 + s
>   result <- c(rep(FALSE,s),v)
>   result <- result[1:(length(result)-s)]
>   result
> }
> 
> > set.seed(19)
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1] FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE
> > peaks(c(1,4,4,1,6,1,5,1,1),3)
> [1] FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE
> 
> 
> Above, the "4" peak at positions 2 and 3 is shown by the TRUE and
> FALSE in positions 2 and 3 above.  Case 4 of FALSE, FALSE was most
> unexpected -- no peak.
> 
> 
> I studied the peaks code and found the problem seems to be in max.col:
> > z
>      [,1] [,2] [,3]
> [1,]    4    4    1
> [2,]    1    4    4
> [3,]    6    1    4
> [4,]    1    6    1
> [5,]    5    1    6
> [6,]    1    5    1
> [7,]    1    1    5
> 
> > max.col(z)
> [1] 2 3 1 2 3 2 3
> > max.col(z)
> [1] 2 2 1 2 3 2 3
> > max.col(z)
> [1] 1 2 1 2 3 2 3
> > max.col(z)
> [1] 2 2 1 2 3 2 3
> > max.col(z)
> [1] 1 3 1 2 3 2 3
> > max.col(z)
> [1] 2 2 1 2 3 2 3
> 
> The ?max.col help shows that it has a ties.method that defaults to
> "random". I want a peak, any peak if there is a tie, but I don't want
> the case that a tie is treated as "no peak".  For now, I added a
> "first" parameter to max.col in peaks:
> 
> # Break ties by using "first"
> 
> peaks <- function(series,span=3)
> {
>   z <- embed(series, span)
>   s <- span%/%2
>   v <- max.col(z, "first") == 1 + s
>   result <- c(rep(FALSE,s),v)
>   result <- result[1:(length(result)-s)]
>   result
> }

Here is a little bit different version of peaks which I currently 
use. It uses ties method for peaks function, controls odd span and 
works differently with adding FALSE values to keep resulting vector 
the same length as the input one.

peaks<-function(series,span=3, ties.method = "first")
{
if((span <- as.integer(span)) %% 2 != 1) stop("'span' must be odd")
z <- embed(series, span)
s <- span%/%2
v <- max.col(z, ties.method=ties.method) == 1 + s
pad <- rep(FALSE, s)
result <- c(pad, v, pad)
result
}

Petr

> 
> A better solution might be a ties.method parameter to peaks, which can
> be passed to max.col.
> 
> I did all of this in R 2.4.1, but the problem seems to be in earlier
> versions too.
> 
> Just in case anyone else is using this "peaks" function.
> 
> efg
> 
> Earl F. Glynn
> Stowers Institute for Medical Research
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From lauri.nikkinen at iki.fi  Fri Feb  9 09:17:48 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Fri, 9 Feb 2007 10:17:48 +0200
Subject: [R] Data.frame columns in R console
In-Reply-To: <Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>
References: <ba8c09910702081001o3a759e7bk73bc0313f752fd9b@mail.gmail.com>
	<Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>
Message-ID: <ba8c09910702090017x2d0cd2cfkf745bcedc9c07c22@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/abbd7a70/attachment.pl 

From petr.pikal at precheza.cz  Fri Feb  9 09:29:00 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Feb 2007 09:29:00 +0100
Subject: [R] subset function
In-Reply-To: <E1HFP8C-0003DS-00@smtp08.web.de>
Message-ID: <45CC3EDC.6169.546885@localhost>

Hi

works for me

> zeta
  tepl tio2 al2o3  iep
1   60    1   3.5 5.65
2   60    1   2.0 5.00
3   60    0   3.5 5.30
4   60    0   2.0 4.65
5   40    1   3.5 5.20
6   40    1   2.0 4.85
7   40    0   3.5 5.70
8   40    0   2.0 5.25
> fit<-lm(iep~al2o3, data=zeta)
> fit<-lm(iep~al2o3, data=zeta, subset=tepl==60)

so you shall check what results from just subsetting your data e.g.

subset(in.mi01, C_X01=="Berlin")

HTH
Petr



On 9 Feb 2007 at 7:21, Simon P. Kempf wrote:

From:           	"Simon P. Kempf" <simon.kempf at web.de>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Fri, 9 Feb 2007 07:21:00 +0100
Subject:        	[R] subset function

> Hello R-Users,
> 
> 
> 
> I have the following problem with the subset function:
> 
> 
> 
> See the following simple linear model. Here everything works fine:
> 
> 
> 
> >germany<-lm(RENT~AGE1, in.mi01)
> 
> 
> 
> However, if a use the same regression equation and only specify a
> subset, I get an error message:
> 
> 
> 
> > berlin<-lm(RENT~AGE1, in.mi01, subset=C_X01=="Berlin")
> 
> 
> 
> Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)
> : 
> 
>         0 (non-NA) cases
> 
> 
> 
>  The datasets contains no missing values and for the city Berlin there
>  are
> 2200 observations.
> 
> 
> 
> > summary(in.mi01$C_X01)
> 
> Berlin               D?sseldorf           Frankfurt am Main    Hamburg
> K?ln                 
> 
>                 2200                 1638                 2943
> 2068                  759 
> 
> Leipzig              Munich               Others              
> Stuttgart
> 
> 
>                  344                 1514                 7955
> 383
> 
> 
> 
> What am I doing wrong?
> 
> 
> 
> Thanks in advance for any help and suggestions,
> 
> 
> 
> Simon
> 
> 
> 
> 
> 
> 
>  [[alternative HTML version deleted]]
> 
> 

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Fri Feb  9 09:32:34 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Feb 2007 08:32:34 +0000 (GMT)
Subject: [R] "class" package
In-Reply-To: <371003692.09420@capitalbio.com>
References: <371003692.09420@capitalbio.com>
Message-ID: <Pine.LNX.4.64.0702090830570.26921@gannet.stats.ox.ac.uk>

It is part of the VR bundle and should be in all R installations, as a 
recommended package.

Did you try

> library(class)

?  If that really does not work, please re-install R.

On Fri, 9 Feb 2007, XinMeng wrote:

> Hello sir:
> Where can I download the package "class"?
> What I can find is only:
> http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/class/html/00Index.html
>
> But I can't find where I can download it.
>
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Feb  9 09:38:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Feb 2007 08:38:24 +0000 (GMT)
Subject: [R] Data.frame columns in R console
In-Reply-To: <ba8c09910702090017x2d0cd2cfkf745bcedc9c07c22@mail.gmail.com>
References: <ba8c09910702081001o3a759e7bk73bc0313f752fd9b@mail.gmail.com>
	<Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>
	<ba8c09910702090017x2d0cd2cfkf745bcedc9c07c22@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702090834300.26921@gannet.stats.ox.ac.uk>

On Fri, 9 Feb 2007, Lauri Nikkinen wrote:

> Thank you for your answer. When I set options(width=250) I still get the
> same result when I print the data.frame on my Rgui console (R 2.4.1, Windows
> XP). Colums become underneath each other. I also get an error (?) message [
> reached getOption("max.print") -- omitted 3462 rows ]]. For example if I
> have a data.frame with 4000 rows and 200 columns I would like to be able to
> use scroll bars in Rconsole to investigate the whole data.frame.

200 columns will take far more than 250 characters.  The help says

      'width': controls the number of characters on a line. You may want
           to change this if you re-size the window that R is running
           in.  Valid values are 10...10000 with default normally 80.

I would use the spreadsheet view of edit(mydf) in preference.


> btw, R is very useful system, my sincere thanks goes to R developers!
>
>
>
> -Lauri
>
>
> 2007/2/8, Prof Brian Ripley <ripley at stats.ox.ac.uk>:
>>
>> ?options, look for 'width'.
>>
>> I don't know what OS this in: the Windows Rgui has an option to set the
>> width to the width of the console, but you can override it.
>>
>> On Thu, 8 Feb 2007, Lauri Nikkinen wrote:
>>
>>> Hi R-users,
>>>
>>>
>>>
>>> A newbie question: assume that I have for example 30 columns in my
>>> data.frame named DF. When I print DF in R console I get columns that
>> don't
>>> fit on the same row underneath each other. So how do I change the R
>> console
>>> preferences so that the console does not wrap my data.frame columns? I
>> want
>>> the columns to be printed next to each other, as in a normal table.
>>>
>>>
>>>
>>> Cheers,
>>>
>>> Lauri
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Fri Feb  9 09:42:13 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Feb 2007 09:42:13 +0100
Subject: [R] Data.frame columns in R console
In-Reply-To: <ba8c09910702090017x2d0cd2cfkf745bcedc9c07c22@mail.gmail.com>
References: <Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>
Message-ID: <45CC41F5.16345.608171@localhost>

Hi


On 9 Feb 2007 at 10:17, Lauri Nikkinen wrote:

Date sent:      	Fri, 9 Feb 2007 10:17:48 +0200
From:           	"Lauri Nikkinen" <lauri.nikkinen at iki.fi>
To:             	"Prof Brian Ripley" <ripley at stats.ox.ac.uk>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Data.frame columns in R console

> Thank you for your answer. When I set options(width=250) I still get
> the same result when I print the data.frame on my Rgui console (R
> 2.4.1, Windows XP). Colums become underneath each other. I also get an
> error (?) message [ reached getOption("max.print") -- omitted 3462
> rows ]]. For example if I have a data.frame with 4000 rows and 200
> columns I would like to be able to use scroll bars in Rconsole to
> investigate the whole data.frame.

I am not sure if it is the best idea. You shall probably use other 
means for checking your data frame.

Try ?summary, ?str or if you really want to check all values in data 
frame you can use

invisible(edit(test))

to open a spreadsheet like editor.

HTH
Petr





> 
> 
> 
> btw, R is very useful system, my sincere thanks goes to R developers!
> 
> 
> 
> -Lauri
> 
> 
> 2007/2/8, Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> >
> > ?options, look for 'width'.
> >
> > I don't know what OS this in: the Windows Rgui has an option to set
> > the width to the width of the console, but you can override it.
> >
> > On Thu, 8 Feb 2007, Lauri Nikkinen wrote:
> >
> > > Hi R-users,
> > >
> > >
> > >
> > > A newbie question: assume that I have for example 30 columns in my
> > > data.frame named DF. When I print DF in R console I get columns
> > > that
> > don't
> > > fit on the same row underneath each other. So how do I change the
> > > R
> > console
> > > preferences so that the console does not wrap my data.frame
> > > columns? I
> > want
> > > the columns to be printed next to each other, as in a normal
> > > table.
> > >
> > >
> > >
> > > Cheers,
> > >
> > > Lauri
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self) 1
> > South Parks Road,                     +44 1865 272866 (PA) Oxford
> > OX1 3TG, UK                Fax:  +44 1865 272595
> >
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From maechler at stat.math.ethz.ch  Fri Feb  9 09:46:39 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Feb 2007 09:46:39 +0100
Subject: [R] R vs Matlab {R in Industry}"
In-Reply-To: <000601c74bb8$e07750d0$7c94100a@win.ad.jhu.edu>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<45CB5945.10403@gmx.net>
	<000601c74bb8$e07750d0$7c94100a@win.ad.jhu.edu>
Message-ID: <17868.13551.821330.752550@stat.math.ethz.ch>


>>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
>>>>>     on Thu, 8 Feb 2007 14:39:41 -0500 writes:

    Ravi> Here is a function to create a Toeplitz matrix of any size, and an example
    Ravi> of a 220 x 220 toeplitz matrix, which was created in almost no time:

Thanks Ravi,
but note two things

- ?toeplitz  tells you that R already has a fast (R-code-only)
  toeplitz() function

- The point of that benchmark is not to measure how fast you can
  build a Toeplitz matrix but simply to exercise a double
  (actually triple) for loop.
  {and the benchmark R script says so as comment in the code}

BTW {not to Ravi, but on the subject}:
 
1) When comparing this (the for-loop benchmark) --- with
   Matlab I would want to be sure that Matlab is not simply
   using an internal short cut since the benchmark is maybe
   too simplistic:
               for(i in 1:N) for(j in 1:N)  b[i,j] <- abs(i - j)

   {and it maybe interesting to see if R's experimental byte
    compiler would speed that up}

2) The above is very fast (IMO) and I cannot say why this could
   be too slow in a realistic situation.

3) The tables I've seen said that Matlab was about a factor of 2
   faster for the above loop benchmark.  That's scarcely a
   reason for downgrading (from R to Matlab).

Martin


From pbulian at cro.it  Fri Feb  9 10:16:39 2007
From: pbulian at cro.it (Pietro Bulian)
Date: Fri, 9 Feb 2007 10:16:39 +0100
Subject: [R] step in a model with strata
References: <000001c74b67$0a578110$2bb411ac@cro.sanita.fvg.it>
	<31495.212.209.13.15.1170946842.squirrel@www.sorch.se>
Message-ID: <00af01c74c2b$03fa2ba0$2bb411ac@cro.sanita.fvg.it>

Henric,
thank you very much, you guessed exactly what I want, excuse me for my poor 
explanations.
I  tried step with scope argument and all went nice. Before my first posting 
I read ?step, but I did not understand  the meaning and usage of "scope".
I said "manual step", but  indeed I did not use AIC(model1,model2) but 
anova(model1,model2).
I  mail ordered F. Harrell book two weeks ago and today I just received my 
copy.

Thank again

Pietro


----- Original Message ----- 
From: "Henric Nilsson (Public)" <nilsson.henric at gmail.com>
To: "Pietro Bulian" <pbulian at cro.it>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 08, 2007 4:00 PM
Subject: Re: [R] step in a model with strata


> Den To, 2007-02-08, 10:47 skrev Pietro Bulian:
>> Dear experts,
>> when I call the step function for a coxph model with n covariates and a
>> dicotomous variable included as strata, the first term removed by step is
>> always the strata variable. This is not what I want
>
> So, what do you want exactly? (You didn't tell.)
>
> I'm just guessing here, but it sounds like you'd always want the strata
> to stay in the model. In that case, use the `scope' argument i.e.
> something like `step(fit, scope = list(lower = ~ strata(x)))' if your
> fitted model object is called `fit' and your stratification variable is
> called `x' -- see ?step.
>
>> and then I do a manual step updating the model minus the least
>> significant covariate and testing with anova, until I have minimized
>> the model.
>
> So, let me see if I understand this correctly, you have a two-stage
> procedure where you first minimize the AIC criterion and then remove
> non-significant predictors in a stepwise fashion?
>
>> Is there a package were this can be done?
>
> If you're referring to the procedure above, I'm not aware of any such
> package.
>
>> or am I doing something wrong ? (I'm not a statistician).
>
> Well, it depends... If you want some guidance on model selection, see e.g.
>
> @BOOK{R:Harrell:2001,
>  AUTHOR = {Frank E. Harrell},
>  TITLE = {Regression Modeling Strategies, with Applications to
>                  Linear Models, Survival Analysis and Logistic
>                  Regression},
>  PUBLISHER = {Springer},
>  YEAR = 2001,
>  NOTE = {ISBN 0-387-95232-2},
>  URL = {http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS}
> }
>
>
> HTH,
> Henric
>
>
>
>>
>> Thanks for hints
>>
>> Pietro Bulian
>>
>> Clinical and Experimental Hematology Research Unit
>> Centro di Riferimento Oncologico, I.R.C.C.S.
>> Via Pedemontana, 12
>> I-33081 Aviano (PN) - Italy
>>
>> phone: +39 0434 659 412
>> fax: +39 0434 659 409
>> e-mail: pbulian at cro.it
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>


From maechler at stat.math.ethz.ch  Fri Feb  9 10:24:55 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Feb 2007 10:24:55 +0100
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <004801c74bda$b1e30f30$7c94100a@win.ad.jhu.edu>
References: <40e66e0b0702081424u5c25420cp5d915d99552ba568@mail.gmail.com>
	<40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>
	<004801c74bda$b1e30f30$7c94100a@win.ad.jhu.edu>
Message-ID: <17868.15847.789394.466496@stat.math.ethz.ch>

>>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
>>>>>     on Thu, 8 Feb 2007 18:41:38 -0500 writes:

    Ravi> Hi,
    Ravi> "greaterOf" is indeed an interesting function.  It is much faster than the
    Ravi> equivalent R function, "pmax", because pmax does a lot of checking for
    Ravi> missing data and for recycling.  Tom Lumley suggested a simple function to
    Ravi> replace pmax, without these checks, that is analogous to greaterOf, which I
    Ravi> call fast.pmax.  

    Ravi> fast.pmax <- function(x,y) {i<- x<y; x[i]<-y[i]; x}

    Ravi> Interestingly, greaterOf is even faster than fast.pmax, although you have to
    Ravi> be dealing with very large vectors (O(10^6)) to see any real difference.

Yes. Indeed, I have a file, first version dated from 1992
where I explore the "slowness" of pmin() and pmax() (in S-plus
3.2 then). I had since added quite a few experiments and versions to that
file in the past.

As consequence, in the robustbase CRAN package (which is only a bit
more than a year old though), there's a file, available as
  https://svn.r-project.org/R-packages/robustbase/R/Auxiliaries.R
with the very simple content {note line 3 !}:

-------------------------------------------------------------------------
### Fast versions of pmin() and pmax() for 2 arguments only:

### FIXME: should rather add these to R
pmin2 <- function(k,x) (x+k - abs(x-k))/2
pmax2 <- function(k,x) (x+k + abs(x-k))/2
-------------------------------------------------------------------------

{the "funny" argument name 'k' comes from the use of these to
 compute Huber's psi() fast :

  psiHuber <- function(x,k)  pmin2(k, pmax2(- k, x))
  curve(psiHuber(x, 1.35), -3,3, asp = 1)
}

One point *is* that I think proper function names would be pmin2() and
pmax2() since they work with exactly 2 arguments,
whereas IIRC the feature to work with '...' is exactly the
reason that pmax() and pmin() are so much slower.

I've haven't checked if Gabor's 
     pmax2.G <- function(x,y) {z <- x > y; z * (x-y) + y}
is even faster than the abs() using one.
It may have the advantage of giving *identical* results (to the
last bit!)  to pmax()  which my version does not --- IIRC the
only reason I did not follow my own 'FIXME' above.

I  had then planned to implement pmin2() and pmax2() in C code, trivially,
and and hence get identical (to the last bit!) behavior as
pmin()/pmax(); but I now tend to think that the proper approach is to
code pmin() and pmax() via .Internal() and hence C code ...

[Not before DSC and my vacations though!!]

Martin Maechler, ETH Zurich


From shubhak at ambaresearch.com  Fri Feb  9 10:27:13 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Fri, 9 Feb 2007 14:57:13 +0530
Subject: [R] Stop Execution of R for sometime
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3F8A2C4@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/f609d862/attachment.pl 

From lauri.nikkinen at iki.fi  Fri Feb  9 10:38:26 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Fri, 9 Feb 2007 11:38:26 +0200
Subject: [R] Data.frame columns in R console
In-Reply-To: <45CC41F5.16345.608171@localhost>
References: <Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>
	<ba8c09910702090017x2d0cd2cfkf745bcedc9c07c22@mail.gmail.com>
	<45CC41F5.16345.608171@localhost>
Message-ID: <ba8c09910702090138s4ae6aa61i20e4072fe896624c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/c3af9df9/attachment.pl 

From ccleland at optonline.net  Fri Feb  9 10:40:43 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 09 Feb 2007 04:40:43 -0500
Subject: [R] How to count the number of NAs in each column of a df?
In-Reply-To: <20070208221659.BTZ03190@po-d.temple.edu>
References: <20070208221659.BTZ03190@po-d.temple.edu>
Message-ID: <45CC419B.3060407@optonline.net>

Richard M. Heiberger wrote:
> drop.col.kna <- function(mydf, k)
>  mydf[sapply(mydf, function(x) sum(is.na(x))) < k]
> 
> tmp <- data.frame(matrix(1:24, 6,4, dimnames=list(letters[1:6], LETTERS[1:4])))
> tmp[1:3,1] <- NA
> tmp[2:5,2] <- NA
> tmp[6,3] <- NA
> 
> drop.col.kna(tmp, 0)
> drop.col.kna(tmp, 1)
> drop.col.kna(tmp, 2)
> drop.col.kna(tmp, 3)
> drop.col.kna(tmp, 4)
> drop.col.kna(tmp, 5)
> drop.col.kna(tmp, 6)

  Possibly simpler (does not require a new function definition and seems
highly intuitive) might be something like this:

tmp.dropna <- tmp[,colSums(is.na(tmp)) < 2]

tmp.dropna
   C  D
a 13 19
b 14 20
c 15 21
d 16 22
e 17 23
f NA 24

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From mikeaddr at hotmail.com  Thu Feb  8 09:53:28 2007
From: mikeaddr at hotmail.com (mike Ad.)
Date: Thu, 08 Feb 2007 03:53:28 -0500
Subject: [R] persp plot help
Message-ID: <BAY101-F2712C817314B9D7D8E177ED69D0@phx.gbl>

Dear list,

I am trying to make a perspective plot with persp(x, y, z). The problem is 
that z data set is incomplete, with lots of NA. And I can not get a smooth 
surface plot.
Could any one tell me how to generate a smooth surface with incomplete 
dataset?
(I tried to fill in some values for the miss values, but I still can not get 
a smooth surface in the persp plot)

/Mike


From lorenzo.isella at gmail.com  Fri Feb  9 11:00:39 2007
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 9 Feb 2007 11:00:39 +0100
Subject: [R] Numerical Recipes in R
Message-ID: <a2b3004b0702090200u6174affbw350809802751bdba@mail.gmail.com>

Dear All,
So far I have mainly used R for data analysis and simple numerics
(integration of functions, splines etc...).
However, I have recently been astonished at finding out that many
things I thought were only achievable with Fortran or C can be done
e.g. entirely using MatLab.
When I try asking around if the same could be achieved by R,
inevitably the answer is that either people do not know R or there is
so much numerical MatLab code (for instance for solving partial
differential equations), that there is no point in switching to R.
Does anyone know if this is really the situation? I am wondering if
there is anywhere a kind of freely available collection of reliable
numerical software written in R which is not only geared towards
statistics and data analysis.
Kind Regards

Lorenzo


From maechler at stat.math.ethz.ch  Fri Feb  9 11:01:24 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Feb 2007 11:01:24 +0100
Subject: [R] Data.frame columns in R console
In-Reply-To: <45CC41F5.16345.608171@localhost>
References: <Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>
	<45CC41F5.16345.608171@localhost>
Message-ID: <17868.18036.905558.215761@stat.math.ethz.ch>

>>>>> "Petr" == Petr Pikal <petr.pikal at precheza.cz>
>>>>>     on Fri, 09 Feb 2007 09:42:13 +0100 writes:

    Petr> Hi
    Petr> On 9 Feb 2007 at 10:17, Lauri Nikkinen wrote:

    >> Thank you for your answer. When I set options(width=250) I still get
    >> the same result when I print the data.frame on my Rgui console (R
    >> 2.4.1, Windows XP). Colums become underneath each other. I also get an
    >> error (?) message  
    >> [ reached getOption("max.print") -- omitted 3462 rows ]]. 

As Petr explains below (and Brian Ripley), you
*really* should use different means here ---
but I think this is the first time that  the relatively new
option 'max.print' has "hit R-help", hence one other hint, maybe
useful to the public:

Note that the 'max.print' option was introduced exactly for the
purpose of **protecting** the inadvertent user from a flood of output
spilling into his console/gui/..
(and apparently locking up R completely, we have even seen
 crashes when people wanted to print dataframes/matrices/arrays
 with millions of entries).

So, given the above message (yes, not an error),
why did you not try to read
     help(getOption)
and look for the word 'max.print' there ?

--> if you really really don't want to follow the advice of
Brian and Petr, then say something like
      options(max.print = 1e6)

Martin Maechler, ETH Zurich


    >> For example if I have a data.frame with 4000 rows and 200
    >> columns I would like to be able to use scroll bars in
    >> Rconsole to investigate the whole data.frame.

    Petr> I am not sure if it is the best idea. You shall probably use other 
    Petr> means for checking your data frame.

    Petr> Try ?summary, ?str or if you really want to check all values in data 
    Petr> frame you can use

    Petr> invisible(edit(test))

    Petr> to open a spreadsheet like editor.

    Petr> HTH
    Petr> Petr


From jim at bitwrit.com.au  Fri Feb  9 11:08:17 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 09 Feb 2007 21:08:17 +1100
Subject: [R] R in Industry
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>	<45CB4E87.4040605@pburns.seanet.com>
	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
Message-ID: <45CC4811.40102@bitwrit.com.au>

Ben Fairbank wrote:
> To those following this thRead:
> 
> There was a thread on this topic a year or so ago on this list, in which
> contributors mentioned reasons that corporate powers-that-be were
> reluctant to commit to R as a corporate statistical platform.  (My
> favorite was "There is no one to sue if something goes wrong.")
> 
> One reason that I do not think was discussed then, nor have I seen
> discussed since, is the issue of the continuity of support.  If one
> person has contributed disproportionately heavily to the development and
> maintenance of a package, and then retires or follows other interests,
> and the package needs maintenance (perhaps as a consequence of new
> operating systems or a new version of R), is there any assurance that it
> will be available?  With a commercial package such as, say, SPSS, the
> corporate memory and continuance makes such continued maintenance
> likely, but is there such a commitment with R packages?  If my company
> came to depend heavily on a fairly obscure R package (as we are
> contemplating doing), what guarantee is there that it will be available
> next month/year/decade?  I know of none, nor would I expect one.
> 
> As R says when it starts up, "R is free software and comes with
> ABSOLUTELY NO WARRANTY."
> 
Hi Ben,
This is a good point, and one that has been made many times. One aspect 
of the "price" of R is that it is user-determined. If you want to take 
the time to check that your particular analysis is doing what you think 
it's doing, that costs you something. For the user who just wants to 
push the button and pass the buck to the software company if anything 
goes wrong, R is not ideal. If you want to go beyond just using R, you 
can report problems, share your knowledge with others, and maybe 
contribute a bit here and there to the project. Perhaps even decide to 
take over maintaining an obscure package if it is of sufficient value to 
you. All this costs you something. It may pay you back, for the 
knowledge you gain from being involved in a cooperative project is worth
something, too. While I have tried to contribute what I can, I think 
that what I have gained from R is much more than I have given. However, 
if the great majority of R users feel that way, it is a pretty good 
example of overall benefit from cooperation. You don't pay any money, 
but you do take your chances.

Jim


From petr.pikal at precheza.cz  Fri Feb  9 11:48:07 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Feb 2007 11:48:07 +0100
Subject: [R] persp plot help
In-Reply-To: <BAY101-F2712C817314B9D7D8E177ED69D0@phx.gbl>
Message-ID: <45CC5F77.9688.D3D157@localhost>

Hi

try to look at function interp from akima package.

HTH
Petr


On 8 Feb 2007 at 3:53, mike Ad. wrote:

From:           	"mike Ad." <mikeaddr at hotmail.com>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Thu, 08 Feb 2007 03:53:28 -0500
Subject:        	[R] persp plot help

> Dear list,
> 
> I am trying to make a perspective plot with persp(x, y, z). The
> problem is that z data set is incomplete, with lots of NA. And I can
> not get a smooth surface plot. Could any one tell me how to generate a
> smooth surface with incomplete dataset? (I tried to fill in some
> values for the miss values, but I still can not get a smooth surface
> in the persp plot)
> 
> /Mike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From tagett at ipsogen.com  Fri Feb  9 12:14:07 2007
From: tagett at ipsogen.com (Rebecca Tagett)
Date: Fri, 9 Feb 2007 11:14:07 +0000 (UTC)
Subject: [R] problems installing R on Linux
References: <200702070818.l178IwIF032402@hypatia.math.ethz.ch>
	<Pine.LNX.4.64.0702070901230.2418@auk.stats>
Message-ID: <loom.20070209T121109-757@post.gmane.org>


Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

> 
> We don't know what 'Linux' is here.  What Linux distribution, what are 
> your C and Fortran compilers (in detail, e.g. from gcc --version and g77 
> --version)?
> 
> We need to see the tail of tests/Examples/base-Ex.Rout.fail to know what 
> went wrong.
> 
> If you can supply those pieces of information we can begin to help.
> (But zlib is a red herring: at most it affects the png() device.)
> 
> On Wed, 7 Feb 2007, Rebecca wrote:
> 
> > Hi everyone,
> >
> > I am having installation problems, but this is how it all started:
> > I had some errors running the bioconductor package affyPLM that uses
> > LAPACK/Blas
> >
> > 	> Pset <- fitPLM(Data)
> > 	Background correcting PM
> > 	Normalizing PM
> > 	Fitting models
> > 	/usr/local/lib/R/bin/exec/R: relocation error:
> > /usr/local/lib/R/lib/libRlapack.so: undefined symbol: s_copy
> > 	# thrown out of R ....
> >
> > I was using R version 2.4.0, so I decided to upgrade to 2.4.1 (on
> > i686-pc-linux-gnu) and to try various configuration options : default,
> > '--with -lapack' and '--with-blas="lacml"', as described in "appendix A of
> > the Installation and Admin" manual.
> >
> > Everytime I configure and make, the message streams seem clean. But the
> > 'make check' is always a disaster (see below), and if I make install
> > regardless of the make check errors, I get the same "relocation error" from
> > fitPLM as above (of course the appropriate BioC packages were installed
> > too).
> >
> > I believe my problems are dealt with in Appendix A of the Installation and
> > Administration Guide, but I can't seem to resolve them. Indeed, I have 
never
> > been able to use png() for graphics because of some unresolved issues
> > concerning access to the X11 graphics device, which I suspected had to do
> > with the libpng and zlib programs (also mentioned in Appendix A).
> >
> > Some of the postings that I have read on this forum seem to imply that
> > installation problems are sometimes due to old versions of zlib. I don't
> > understand this since the recent zlib (1.2.3) is in
> > "/usr/local/lib/R-2.4.1/src/extra/zlib". Appendix A says, referring to 
zlib,
> > that "the versions in the R sources will be compiled in".
> > But just to be sure, I verified that my system's version is old :
> > rpm -q zlib
> > zlib-1.1.3-25.7
> > I downloaded and tried to install zlib-1.2.3 using tar, configure, make,
> > make test, and make install. But when I type "rpm -q zlib", I am still
> > informed that my version is "zlib-1.1.3-25.7". So, still hopeful, I thought
> > that I instead would use rpm to install zlib; I found an intermediate rpm
> > zlib version 1.2.1.2-1.2 and tried "rpm -Uhv zlib-1.2.1.2-1.2.src.rpm". No
> > diagnostics appear, but when I type "rpm -q zlib", my version is still
> > "zlib-1.1.3-25.7". I tried to uninstall the old zlib using "rpm -e zlib",
> > but rpm refuses, saying that there are too many dependencies...
> >
> > Does anyone have any suggestions?
> > Thanks!
> >
> >
> >
> > The 'make check' disaster :
> > 	make[1]: Entering directory `/usr/local/lib/R-2.4.1/tests'
> > 	make[2]: Entering directory `/usr/local/lib/R-2.4.1/tests'
> > 	make[3]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
> > 	make[4]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
> > 	make[4]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
> > 	make[4]: Entering directory `/usr/local/lib/R-2.4.1/tests/Examples'
> > 	collecting examples for package 'base' ...
> > 	make[5]: Entering directory `/usr/local/lib/R-2.4.1/src/library'
> > 	 >>> Building/Updating help pages for package 'base'
> > 	     Formats: text html latex example
> > 	make[5]: Leaving directory `/usr/local/lib/R-2.4.1/src/library'
> > 	running code in 'base-Ex.R' ...make[4]: *** [base-Ex.Rout] Error 1
> > 	make[4]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
> > 	make[3]: *** [test-Examples-Base] Error 2
> > 	make[3]: Leaving directory `/usr/local/lib/R-2.4.1/tests/Examples'
> > 	make[2]: *** [test-Examples] Error 2
> > 	make[2]: Leaving directory `/usr/local/lib/R-2.4.1/tests'
> > 	make[1]: *** [test-all-basics] Error 1
> > 	make[1]: Leaving directory `/usr/local/lib/R-2.4.1/tests'
> > 	make: *** [check] Error 2
> >
> >
> > I don't really *know* if the configure results are "clean". Here is a 
subset
> > of the configure results (that may be suspect) :
> >
> > 	checking build system type... i686-pc-linux-gnu
> > 	checking host system type... i686-pc-linux-gnu
> > 	. . .
> > 	checking for cblas_cdotu_sub in vecLib framework... no
> > 	checking iconv.h usability... yes
> > 	checking iconv.h presence... yes
> > 	checking for iconv.h... yes
> > 	checking for iconv... yes
> > 	checking whether iconv() accepts "UTF-8", "latin1" and "UCS-*"...
> > yes
> > 	checking for iconvlist... no
> > 	. . .
> > 	checking for g77... g77
> > 	checking whether we are using the GNU Fortran 77 compiler... yes
> > 	checking whether g77 accepts -g... yes
> > 	checking for g++... g++
> > 	checking whether we are using the GNU C++ compiler... yes
> > 	checking whether g++ accepts -g... yes
> > 	checking how to run the C++ preprocessor... g++ -E
> > 	checking whether __attribute__((visibility())) is supported... no
> > 	checking whether gcc accepts -fvisibility... no
> > 	checking whether g77 accepts -fvisibility... no
> > 	. . .
> > 	checking if libtool supports shared libraries... yes
> > 	checking whether to build shared libraries... yes
> > 	checking whether to build static libraries... no
> > 	. . .
> > 	checking floatingpoint.h usability... no
> > 	checking floatingpoint.h presence... no
> > 	checking for floatingpoint.h... no
> > 	. . .
> > 	checking ieee754.h usability... yes
> > 	checking ieee754.h presence... yes
> > 	checking for ieee754.h... yes
> > 	checking ieeefp.h usability... no
> > 	checking ieeefp.h presence... no
> > 	checking for ieeefp.h... no
> > 	. . .
> > 	checking for dummy main to link with Fortran libraries... none
> > 	. . .
> > 	checking whether isfinite is declared... no
> > 	. . .
> > 	checking for mkdtemp... no
> > 	. . .
> > 	checking whether mkdtemp is declared... no
> > 	. . .
> > 	checking for special C compiler options needed for large files... no
> > 	checking for _FILE_OFFSET_BITS value needed for large files... 64
> > 	checking for _LARGE_FILES value needed for large files... no
> > 	checking for _LARGEFILE_SOURCE value needed for large files... no
> > 	checking for fseeko... (cached) yes
> > 	checking whether KERN_USRSTACK sysctl is supported... no
> > 	checking for visible __lib_stack_end... yes
> > 	. . .
> >
> > 	checking for g77... g77
> > 	checking whether we are using the GNU Fortran compiler... yes
> > 	checking whether g77 accepts -g... yes
> > 	. . .
> > 	checking for bison... no
> > 	checking for CFPreferencesCopyAppValue... (cached) no
> > 	checking for CFLocaleCopyCurrent... (cached) no
> > 	checking whether NLS is requested... yes
> > 	checking whether included gettext is requested... no
> > 	checking for GNU gettext in libc... no
> > 	checking for GNU gettext in libintl... no
> > 	. . .
> > 	R is now configured for i686-pc-linux-gnu
> >  	Source directory:          .
> >  	Installation directory:    /usr/local
> >  	C compiler:                gcc  -g -O2
> >  	Fortran 77 compiler:       g77  -g -O2
> >  	C++ compiler:              g++  -g -O2
> >  	Fortran 90/95 compiler:    g77 -g -O2
> >  	Interfaces supported:      X11, tcltk
> >  	External libraries:        readline
> >  	Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
> >  	Options enabled:           shared BLAS, R profiling
> >  	Recommended packages:      yes
> >
> > ______________________________________________
> > R-help <at> stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 


Thank you both Pr. Ripley and Dr. Skyler for taking the time to help. I got 
your further responses in my email here at work. I really think that Dr. 
Ripley is correct : "The symptoms look like a very old Linux distro and a 
missing/damaged/incompatible libg2c". 


# gcc --version
2.96
# g77 --version
GNU Fortran 0.5.26 20000731 (Red Hat Linux 7.3 2.96-110)

(embarrassingly old?)

My collegues and I are going to try to update it to the most recent free 
version (9.something). 

Since you asked, the tail of tests/Examples/base-Ex.Rout.fail are below. I'll 
come back to the list if, after upgrading Linux, R still does not configure 
correctly. But do let me know if the errors below are nothing to do with old 
Linux-ness.

In the meantime, in your opinion, will I need to use any configure options for 
BLAS and LAPACK described in Appendix A of the Installation and Administration 
Manual? I hope that "simple compliation" will be sufficient.



base-Ex.Rout.fail 


    --- ### Name: bindenv
    --- ### Title: Binding and Environment Adjustments
    --- ### Aliases: bindenv lockEnvironment environmentIsLocked lockBinding
    --- ###   unlockBinding makeActiveBinding bindingIsLocked bindingIsActive
    --- ### Keywords: internal
. . . 
    --- assign("x", 2, env = e)
    --- try(assign("y", 2, env = e)) # error
Error in assign("y", 2, env = e) : cannot add bindings to a locked environment
. . .
    --- try(assign("x", 2, env = e)) # error
Error in assign("x", 2, env = e) : cannot change value of locked binding 
for 'x'
+++++++++++++++++++++++++
    --- ### Name: chartr
    --- ### Title: Character Translation and Casefolding
    --- ### Aliases: chartr tolower toupper casefold
    --- ### Keywords: character
. . .
    --- ## now for a non-positive-definite matrix
    --- ( m <- matrix(c(5,-5,-5,3),2,2) )
     [,1] [,2]
[1,]    5   -5
[2,]   -5    3
    --- try(chol(m))  # fails
Error in chol(m) : the leading minor of order 2 is not positive definite
    --- try(chol(m, LINPACK=TRUE))   # fails
Error in chol(m, LINPACK = TRUE) : non-positive definite matrix in 'chol'
    --- (Q <- chol(m, pivot = TRUE)) # warning
Warning in chol(m, pivot = TRUE) : matrix not positive definite
         [,1]      [,2]
[1,] 2.236068 -2.236068
[2,] 0.000000 -2.000000
attr(,"pivot")
[1] 1 2
attr(,"rank")
[1] 1
    --- crossprod(Q)  # not equal to m
     [,1] [,2]
[1,]    5   -5
[2,]   -5    9
+++++++++++++++++++++++++
    --- ### Name: match
    --- ### Title: Value Matching
    --- ### Aliases: match \%in\%
    --- ### Keywords: manip logic
. . .
    --- try(center(x, "m"))  # Error
Error in match.arg(type) : 'arg' should be one of mean, median, trimmed
+++++++++++++++++++++++++
    --- ### Name: NotYet
    --- ### Title: Not Yet Implemented Functions and Unused Arguments
    --- ### Aliases: NotYetImplemented .NotYetImplemented 
NotYetUsed .NotYetUsed
    --- ### Keywords: documentation utilities
    ---
. . .
    --- try(plot.mlm())
Error: 'plot.mlm' is not implemented yet
    ---
    --- barplot(1:5, inside = TRUE) # 'inside' is not yet used
Warning: argument 'inside' is not used (yet)

+++++++++++++++++++++++++
    --- ### Name: sample
    --- ### Title: Random Samples and Permutations
    --- ### Aliases: sample
    --- ### Keywords: distribution
. . .
    --- try(sample(x[x     --- 10]))# error!
Error in sample(length(x), size, replace, prob) :
    invalid 'x' argument

+++++++++++++++++++++++++
    --- ### Name: seq.Date
    --- ### Title: Generate Regular Sequences of Dates
    --- ### Aliases: seq.Date
    --- ### Keywords: manip chron
    ---
    --- ### ** Examples
    ---
    --- ## first days of years
    --- seq(as.Date("1910/1/1"), as.Date("1999/1/1"), "years")
Error in fromchar(x) : character string is not in a standard unambiguous format
Execution halted


From np at alambic.org  Fri Feb  9 12:21:10 2007
From: np at alambic.org (Nicolas Prune)
Date: Fri, 09 Feb 2007 12:21:10 +0100
Subject: [R] as.Date() behaviour when incomplete input string
Message-ID: <1171020070.45cc592630338@imp2.online.net>

Dear all,

I would like to know the month on a string formatted as "2004-01", using as.Date
(not just stripping the string !)

?as.Date says that in case of an incomplete input string, the answer is
system-specific.
The following has been tested on  R 2.4.1, on Ubuntu Linux and WinXP.

> mydate <- "2004-01"
> as.Date(mydate)
Error in fromchar(x) : character string is not in a standard unambiguous format.
(I understand that).
> as.Date(mydate,format="%Y-%m")
NA (is there still ambiguity in my format ??)

Investigating a bit, I found a strange behaviour in :

> as.Date("2004-01-12",format="%Y-%m-%d")
"2005-01-12"

> as.Date("2004-01",format="%Y-%m")
NA (why can't R see anything ?)

> as.Date("2004",format="%Y")
"2005-02-09" (month and day are replaced by today's values, normal behaviour).

Why can't R see the month, but see the year ?

For the moment, I do this ugly thing :

> mydate <- paste(mydate,"-01",sep="") in order send an unambiguous string to
as.Date.

Any idea how I can do better ?

Thanks and regards,

Nicolas Prune


From singularitaet at gmx.net  Fri Feb  9 12:27:56 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Fri, 09 Feb 2007 12:27:56 +0100
Subject: [R] problems installing R on Linux
In-Reply-To: <loom.20070209T121109-757@post.gmane.org>
References: <200702070818.l178IwIF032402@hypatia.math.ethz.ch>
	<Pine.LNX.4.64.0702070901230.2418@auk.stats>
	<loom.20070209T121109-757@post.gmane.org>
Message-ID: <45CC5ABC.10409@gmx.net>


>
> Thank you both Pr. Ripley and Dr. Skyler for taking the time to help. I got 
> your further responses in my email here at work. I really think that Dr. 
> Ripley is correct : "The symptoms look like a very old Linux distro and a 
> missing/damaged/incompatible libg2c". 
>
>
> # gcc --version
> 2.96
> # g77 --version
> GNU Fortran 0.5.26 20000731 (Red Hat Linux 7.3 2.96-110)
>
> (embarrassingly old?)
>
> My collegues and I are going to try to update it to the most recent free 
> version (9.something). 
>
>   

Yes it looks terribly old. You should try Fedora Core 6, a re-spin (all
updates till January) is available here: 
http://fedoraunity.org/news-archives/core-6-re-spin-20070111

It is Red Hat based (follow up?), up to date, and a R rpm is provided by
the r-project so you do not need to compile by yourself. Get it from
here: http://stat.ethz.ch/CRAN/bin/linux/redhat/fc6/

Stefan


From avilella at gmail.com  Fri Feb  9 12:28:07 2007
From: avilella at gmail.com (Albert Vilella)
Date: Fri, 9 Feb 2007 11:28:07 +0000
Subject: [R] densityplot png in a for loop
Message-ID: <358f4d650702090328x2b1df8c2r8d0a83ed73a9faac@mail.gmail.com>

Hi all,

I am trying to plot a list of densityplots as png files, but when I do
it in a for loop, I get empty png files as a result.

If I manually run the instructions inside the loop, it works... any hints?

library(lattice)
names_list = c("alfa","beta","gamma")
for (i in 1:length(names_list)) {
  pngfile = paste("example_",names_list[i],".png", sep="")
  png(file=pngfile, width=1280,height=1024)
  input = rnorm(1000,i)
  densityplot(~input,
              plot.points=FALSE,
              auto.key=TRUE
              )
  dev.off()
}

Thanks,

    Albert.


From petr.pikal at precheza.cz  Fri Feb  9 12:46:04 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Feb 2007 12:46:04 +0100
Subject: [R] densityplot png in a for loop
In-Reply-To: <358f4d650702090328x2b1df8c2r8d0a83ed73a9faac@mail.gmail.com>
Message-ID: <45CC6D0C.15224.108E049@localhost>

Hi 

Most probably see the FAQ 7.22 Why do lattice/trellis graphics not 
work?

HTH
Petr




On 9 Feb 2007 at 11:28, Albert Vilella wrote:

Date sent:      	Fri, 9 Feb 2007 11:28:07 +0000
From:           	"Albert Vilella" <avilella at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] densityplot png in a for loop

> Hi all,
> 
> I am trying to plot a list of densityplots as png files, but when I do
> it in a for loop, I get empty png files as a result.
> 
> If I manually run the instructions inside the loop, it works... any
> hints?
> 
> library(lattice)
> names_list = c("alfa","beta","gamma")
> for (i in 1:length(names_list)) {
>   pngfile = paste("example_",names_list[i],".png", sep="")
>   png(file=pngfile, width=1280,height=1024)
>   input = rnorm(1000,i)
>   densityplot(~input,
>               plot.points=FALSE,
>               auto.key=TRUE
>               )
>   dev.off()
> }
> 
> Thanks,
> 
>     Albert.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Fri Feb  9 12:54:03 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Feb 2007 11:54:03 +0000 (GMT)
Subject: [R] densityplot png in a for loop
In-Reply-To: <358f4d650702090328x2b1df8c2r8d0a83ed73a9faac@mail.gmail.com>
References: <358f4d650702090328x2b1df8c2r8d0a83ed73a9faac@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702091152310.3559@auk.stats>

This is FAQ Q7.22

On Fri, 9 Feb 2007, Albert Vilella wrote:

> Hi all,
>
> I am trying to plot a list of densityplots as png files, but when I do
> it in a for loop, I get empty png files as a result.
>
> If I manually run the instructions inside the loop, it works... any hints?
>
> library(lattice)
> names_list = c("alfa","beta","gamma")
> for (i in 1:length(names_list)) {
>  pngfile = paste("example_",names_list[i],".png", sep="")
>  png(file=pngfile, width=1280,height=1024)
>  input = rnorm(1000,i)
>  densityplot(~input,
>              plot.points=FALSE,
>              auto.key=TRUE
>              )
>  dev.off()
> }
>
> Thanks,
>
>    Albert.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Fri Feb  9 12:54:26 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Feb 2007 06:54:26 -0500
Subject: [R] densityplot png in a for loop
In-Reply-To: <358f4d650702090328x2b1df8c2r8d0a83ed73a9faac@mail.gmail.com>
References: <358f4d650702090328x2b1df8c2r8d0a83ed73a9faac@mail.gmail.com>
Message-ID: <971536df0702090354re321b1fr1b52227435c2b32c@mail.gmail.com>

Check out:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-lattice_002ftrellis-graphics-not-work_003f

On 2/9/07, Albert Vilella <avilella at gmail.com> wrote:
> Hi all,
>
> I am trying to plot a list of densityplots as png files, but when I do
> it in a for loop, I get empty png files as a result.
>
> If I manually run the instructions inside the loop, it works... any hints?
>
> library(lattice)
> names_list = c("alfa","beta","gamma")
> for (i in 1:length(names_list)) {
>  pngfile = paste("example_",names_list[i],".png", sep="")
>  png(file=pngfile, width=1280,height=1024)
>  input = rnorm(1000,i)
>  densityplot(~input,
>              plot.points=FALSE,
>              auto.key=TRUE
>              )
>  dev.off()
> }
>
> Thanks,
>
>    Albert.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pburns at pburns.seanet.com  Fri Feb  9 12:58:02 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 09 Feb 2007 11:58:02 +0000
Subject: [R] How to protect two jobs running on the same directory at
 the same time not to corrupt each other results:
In-Reply-To: <45CB8907.50302@wustl.edu>
References: <45CB8907.50302@wustl.edu>
Message-ID: <45CC61CA.3040303@pburns.seanet.com>

You can 'save' the objects produced by each BATCH
job in a file whose name relates to the job.  With this
technique, you can run as many BATCH jobs on the same
data as you like.

Once the jobs are done, then you can 'load' the files that
were saved.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Aldi Kraja wrote:

>Hi,
>
>I have a large group of jobs, some of them are running on the same 
>directory.  All of them in batch mode.
>What are the best ways to protect from corrupting the results two or 
>more jobs running on the same directory.
>One, I would think can be to run each job in a separate directory, 
>collect the results and after remove the directories. But I have 
>thousands of jobs that will run in parallel and I have placed about 100 
>of them in each directory. They all do the same process, but on 
>different variables, replications etc.
>
>Is there any other solution better than creating separate directories in 
>R? I am thinking if there is any option in R to create a unique id which 
>has its own unique .Rdata, although in the same directory?
>
>SAS for example to each batch job it assigns a different ID and a 
>separate temp space, and does not mix it with another job running in 
>parallel.
>
>Thanks,
>
>Aldi
>
>--
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From gregor.gorjanc at bfro.uni-lj.si  Fri Feb  9 13:03:46 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Fri, 9 Feb 2007 12:03:46 +0000 (UTC)
Subject: [R] generate Binomial (not Binary) data
References: <426059.35923.qm@web23404.mail.ird.yahoo.com>
Message-ID: <loom.20070209T130245-287@post.gmane.org>

Marc Bernard <bernarduse1 <at> yahoo.fr> writes:
>   I am looking for an R function or any other reference to generate a series
of correlated Binomial (not a
> Bernoulli) data. The "bindata" library can do this for the binary not the
binomial case.

But binomial is just a sum of Bernoulli vars so this should be doable. 

Gregor


From Thomas.Adams at noaa.gov  Fri Feb  9 13:06:09 2007
From: Thomas.Adams at noaa.gov (Thomas Adams)
Date: Fri, 09 Feb 2007 07:06:09 -0500
Subject: [R] Numerical Recipes in R
In-Reply-To: <a2b3004b0702090200u6174affbw350809802751bdba@mail.gmail.com>
References: <a2b3004b0702090200u6174affbw350809802751bdba@mail.gmail.com>
Message-ID: <45CC63B1.7040008@noaa.gov>

Lorenzo,

You may want to look at Octave, which is a MatLab gnu clone:

http://www.gnu.org/software/octave/

Regards,
Tom


Lorenzo Isella wrote:
> Dear All,
> So far I have mainly used R for data analysis and simple numerics
> (integration of functions, splines etc...).
> However, I have recently been astonished at finding out that many
> things I thought were only achievable with Fortran or C can be done
> e.g. entirely using MatLab.
> When I try asking around if the same could be achieved by R,
> inevitably the answer is that either people do not know R or there is
> so much numerical MatLab code (for instance for solving partial
> differential equations), that there is no point in switching to R.
> Does anyone know if this is really the situation? I am wondering if
> there is anywhere a kind of freely available collection of reliable
> numerical software written in R which is not only geared towards
> statistics and data analysis.
> Kind Regards
>
> Lorenzo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Thomas E Adams
National Weather Service
Ohio River Forecast Center
1901 South State Route 134
Wilmington, OH 45177

EMAIL:	thomas.adams at noaa.gov

VOICE:	937-383-0528
FAX:	937-383-0033


From lauri.nikkinen at iki.fi  Fri Feb  9 13:21:26 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Fri, 9 Feb 2007 14:21:26 +0200
Subject: [R] Data.frame columns in R console
In-Reply-To: <17868.18036.905558.215761@stat.math.ethz.ch>
References: <Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>
	<45CC41F5.16345.608171@localhost>
	<17868.18036.905558.215761@stat.math.ethz.ch>
Message-ID: <ba8c09910702090421x26c68454ld7a140205d055861@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/fb8cefa0/attachment.pl 

From gavin.simpson at ucl.ac.uk  Fri Feb  9 13:49:59 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 09 Feb 2007 12:49:59 +0000
Subject: [R] Point estimate from loess contour plot
In-Reply-To: <Pine.LNX.4.44.0702081904400.1670-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0702081904400.1670-100000@gw.env.leeds.ac.uk>
Message-ID: <1171025399.17729.11.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2007-02-08 at 19:09 +0000, Laura Quinn wrote:
> Hi,
> 
> I was wondering if anyone knows of a way by which one can estimate values
> from a contour plot created by using the loess function? I am hoping to
> use the loess contour plot as a means of interpolation to identify
> the loess created values at points at pre-defined (x,y) locations.
> 
> Could anyone point me in the right direction please?
> 
> Thanks.
> 
> Laura Quinn

Hi Laura,

Using an example from MASS (the book by Venables and Ripley, page 423 in
4th Ed (2002)) and the topo data set:

require(MASS)
## loess surface
topo.lo <- loess(z ~ x * y, topo, degree = 1, span = 0.25,
                 normalize = FALSE)
topo.mar <- list(x = seq(0, 6.5, 0.1), y = seq(0, 6.5, 0.1))
new.dat <- expand.grid(topo.mar)
topo.pred <- predict(topo.lo, new.dat)
## draw the contour map based on loess predictions
eqscplot(topo.mar, type = "n")
contour(topo.mar$x, topo.mar$y, topo.pred,
        levels = seq(700, 1000, 25), add = TRUE)
## original points
points(topo, col = "red")

So now we have a loess surface defined by the model (topo.lo) and we can
use the predict.loess method to get point predictions based on the
model. This is what was used to produce the points draw the contour
surface, but on a regular grid. For some new point, not on this regular
grid we can use the same approach:

> predict(topo.lo, data.frame(x = 4.8, y = 3.1))
[1] 824.0046

which yields a height of 824 and a bit feet for those coordinates. You
can get the standard errors of the predicted value as well:

> predict(topo.lo, data.frame(x = 4.8, y = 3.1), se = TRUE)
$fit
[1] 824.0046

$se.fit
[1] 7.677035

$residual.scale
[1] 18.95324

$df
[1] 34.00484

And of course, you aren't restricted to doing this one point at a time:

> predict(topo.lo, data.frame(x = c(4.8, 4.9, 3.1, 2.6),
+                             y = c(3.1, 2.3, 4.5, 5.6)),
+         se = TRUE)
$fit
[1] 824.0046 849.2514 760.2926 744.2987

$se.fit
[1] 7.677035 7.127979 6.364493 7.093619

$residual.scale
[1] 18.95324

$df
[1] 34.00484

Is this what you were looking for?

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From Flom at ndri.org  Fri Feb  9 13:55:18 2007
From: Flom at ndri.org (Peter Flom)
Date: Fri, 09 Feb 2007 07:55:18 -0500
Subject: [R] LM Model
In-Reply-To: <E1HFPMy-00018e-00@smtp07.web.de>
References: <E1HFPMy-00018e-00@smtp07.web.de>
Message-ID: <45CC28E5.B875.00C9.0@ndri.org>

>>> "Simon P. Kempf" <simon.kempf at web.de> 2/9/2007 1:36 am >>> asked
<<<< 
But I want to skip the lm function and specify my own regression
equation RENT= 15 -0.15*AGE1 and then use the predict.lm function.
However, in order to use the predict.lm function I need an object of
class lm. Is there any way to do so? Or maybe somebody has another
solution? 
>>>

Maybe I am missing something, but why not just write the formula
without the predict.lm?

e.g
<<<
AGE1 <- c(18, 19, 20, 21)
RENT= 15 -0.15*AGE1
RENT
>>>

Peter


 

Thanks in advance,

 

Simon

 

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at yahoo.ca  Fri Feb  9 14:13:48 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 9 Feb 2007 08:13:48 -0500 (EST)
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <45CC2F21.19905.16F64B@localhost>
Message-ID: <993935.16184.qm@web32809.mail.mud.yahoo.com>


--- Petr Pikal <petr.pikal at precheza.cz> wrote:

> Hi
> 
> Strange. It works for me without any problem.

The problem is that my dataframe has 1,s in about 50%
of the columns and I only want it to apply to a few
specified columns. My explanation may not have been
clear enough.

Using your example,I want all values for tio2 set to 1
but not any values in al2o3 whereas zeta[zeta==1]<-NA
is also changing al2o3[3] to NA.

Thanks

> 
> > zeta
>   tepl tio2 al2o3  iep
> 1   60    1   3.5 5.65
> 2   60    1   2.0 5.00
> 3   60    1   1.0 5.30
> 4   60    0   2.0 4.65
> 5   40    1   3.5 5.20
> 6   40    1   2.0 4.85
> 7   40    0   3.5 5.70
> 8   40    0   2.0 5.25
> > zeta[zeta==1]<-NA
> > zeta
>   tepl tio2 al2o3  iep
> 1   60   NA   3.5 5.65
> 2   60   NA   2.0 5.00
> 3   60   NA    NA 5.30
> 4   60    0   2.0 4.65
> 5   40   NA   3.5 5.20
> 6   40   NA   2.0 4.85
> 7   40    0   3.5 5.70
> 8   40    0   2.0 5.25
> 
> > str(zeta)
> 'data.frame':   8 obs. of  4 variables:
>  $ tepl : int  60 60 60 60 40 40 40 40
>  $ tio2 : num  NA NA NA 0 NA NA 0 0
>  $ al2o3: num  3.5 2 NA 2 3.5 2 3.5 2
>  $ iep  : num  5.65 5 5.3 4.65 5.2 4.85 5.7 5.25
> >
> 
> HTH
> Petr
> 
> 
> 
> On 7 Feb 2007 at 16:57, Erik Iverson wrote:
> 
> Date sent:      	Wed, 07 Feb 2007 16:57:40 -0600
> From:           	Erik Iverson
> <iverson at biostat.wisc.edu>
> To:             	John Kane <jrkrideau at yahoo.ca>
> Copies to:      	R R-help <r-help at stat.math.ethz.ch>
> Subject:        	Re: [R] setting a number of values
> to NA over  a data.frame.
> 
> > John -
> > 
> > Your initial problem uses 0, but the example uses
> 1 for the value that
> > gets an NA.  My solution uses 1 to fit with your
> example.  There may
> > be a better way, but try something like
> > 
> > data1[3:5] <- data.frame(lapply(data1[3:5],
> function(x) ifelse(x==1,
> > NA, x)))
> > 
> > The data1[3:5] is just a test subset  of columns I
> chose from your
> > data1 example.  Notice it appears twice, once on
> each side of the
> > assignment operator.
> > 
> > In English, apply to each column of the data frame
> (which is a list) a
> > function that will return NA if the element is 1,
> and the value
> > otherwise, and then turn the modified lists into a
> data.frame, and
> > save it as data1.
> > 
> > 
> > 
> > See the help files for lapply and ifelse if you
> haven't seen those
> > before.
> > 
> > Maybe someone has a better way?
> > 
> > Erik
> > 
> > John Kane wrote:
> > > This is probably a simple problem but I don't
> see a
> > > solution.
> > > 
> > > I have a data.frame with a number of columns
> where I
> > > would like 0 <- NA
> > > 
> > > thus I have df1[,144:157] <- NA if df1[, 144:
> 157] ==0
> > > and df1[, 190:198] <- NA if df1[, 190:198] ==0
> > > 
> > > but I cannot figure out a way do this.  
> > > 
> > > cata <- c( 1,1,6,1,1,NA)
> > > catb <- c( 1,2,3,4,5,6)
> > > doga <- c(3,5,3,6,4, 0)
> > > dogb <- c(2,4,6,8,10, 12)
> > > rata <- c (NA, 9, 9, 8, 9, 8)
> > > ratb <- c( 1,2,3,4,5,6)
> > > bata <- c( 12, 42,NA, 45, 32, 54)
> > > batb <- c( 13, 15, 17,19,21,23)
> > > id <- c('a', 'b', 'b', 'c', 'a', 'b')
> > > site <- c(1,1,4,4,1,4)
> > > mat1 <-  cbind(cata, catb, doga, dogb, rata,
> ratb,
> > > bata, batb)
> > > 
> > > data1 <- data.frame(site, id, mat1)
> > > data1
> > > 
> > >  # Obviously this works fine for one column
> > > 
> > > data1$site[data1$site ==1] <- NA  ; data1
> > > 
> > > but I cannot see how to do this with indices
> that
> > > would allow me to do more than one column in the
> > > data.frame.
> > > 
> > > At one point I even tried something like this
> > > a <- c("site")
> > > data1$a[data1$a ==1] <- NA
> > > 
> > > which seems to produce a corrupt data.frame.
> > > 
> > > I am sure it is simple but I don't see it.  
> > > 
> > > Any help would be much appreciated.
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and
> provide commented,
> > > minimal, self-contained, reproducible code.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and
> provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
>


From amberti at inwind.it  Fri Feb  9 14:21:36 2007
From: amberti at inwind.it (Daniele Amberti)
Date: Fri,  9 Feb 2007 14:21:36 +0100
Subject: [R] get.hist.quote problem yahoo
Message-ID: <JD76G0$4AB165473482240FB88260F0B2A8F9C8@libero.it>

I have functions using get.hist.quote() from library tseries.

It seems that something changed (yahoo) and function get broken.

try with a simple

get.hist.quote('IBM')

and let me kow if for someone it is still working.

I get this error:
Error in if (!quiet && dat[n] != start) cat(format(dat[n], "time series starts %Y-%m-%d\n")) : 
        missing value where TRUE/FALSE needed

Looking at the code it seems that before the format of dates in yahoo's cv file was not iso.
Now it is iso standard year-month-day

Anyone get the same problem?


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada9feb07


From amberti at inwind.it  Fri Feb  9 14:22:17 2007
From: amberti at inwind.it (Daniele Amberti)
Date: Fri,  9 Feb 2007 14:22:17 +0100
Subject: [R] get.hist.quote problem yahoo
Message-ID: <JD76H5$264232457CE289BCC2F55186A93BE37E@libero.it>

I have functions using get.hist.quote() from library tseries.

It seems that something changed (yahoo) and function get broken.

try with a simple

get.hist.quote('IBM')

and let me kow if for someone it is still working.

I get this error:
Error in if (!quiet && dat[n] != start) cat(format(dat[n], "time series starts %Y-%m-%d\n")) : 
        missing value where TRUE/FALSE needed

Looking at the code it seems that before the format of dates in yahoo's cv file was not iso.
Now it is iso standard year-month-day

Anyone get the same problem?


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada9feb07


From amberti at inwind.it  Fri Feb  9 14:22:54 2007
From: amberti at inwind.it (Daniele Amberti)
Date: Fri,  9 Feb 2007 14:22:54 +0100
Subject: [R] get.hist.quote problem yahoo
Message-ID: <JD76I6$F4748E9DC214443641255B1FF8B8DF98@libero.it>

I have functions using get.hist.quote() from library tseries.

It seems that something changed (yahoo) and function get broken.

try with a simple

get.hist.quote('IBM')

and let me kow if for someone it is still working.

I get this error:
Error in if (!quiet && dat[n] != start) cat(format(dat[n], "time series starts %Y-%m-%d\n")) : 
        missing value where TRUE/FALSE needed

Looking at the code it seems that before the format of dates in yahoo's cv file was not iso.
Now it is iso standard year-month-day

Anyone get the same problem?


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada9feb07


From P.Dalgaard at biostat.ku.dk  Fri Feb  9 14:24:35 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 09 Feb 2007 14:24:35 +0100
Subject: [R] Problem with factor state when subset()ing a data.frame
In-Reply-To: <87d54kuxlg.fsf@hardknott.home>
References: <87d54kuxlg.fsf@hardknott.home>
Message-ID: <45CC7613.3080905@biostat.ku.dk>

Roger Leigh wrote:
> Hi folks,
>
> I am running into a problem when calling subset() on a large
> data.frame.  One of the columns contains strings which are used as
> factors.  R seems to automatically factor the column when the
> data.frame is contstructed, and this appears to not get updated when I
> create a subset of the table.
>
> A minimal testcase to demonstrate the problem follows:
> [snip]
> Am I doing something wrong here, or is this an R bug?  
Not really, and no.

This has been discussed a number of times in the past, and the consensus
(grudgingly by some) seems to be that R's current behaviour is the
rational one. The basic issue is whether the fact that a factor level is
absent in a subgroup should change the level set . I.e., if you split a
population by occupation, should the fact that there are no women in the
subgroup of firefighters turn gender in to a one-level factor for that
group?  Sometimes it is sensible, but often it is not: If you do a
series of barplots of the gender distribution, should they not have an
empty bar for females when there are none? Similarly, if you have a
semiquantitative scale like terrible-poor-mediocre-good-excellent would
you not prefer to have tables and plots represent all five possible
values always?


> How can I get
> the new data.frame to update the factors, so I don't get redundant
> "empty" factors on the plot by eliminating the "phantom" factors?  (I
> also need to remove the unused factors for other analyses, and
> factoring them "by hand" seems a little redundant.)
>
>   
You already know how (it's not redundant as you might want not to do
it). I don't think there's an easier way, but you can automate, as in

sb <- subset(.....)
isf <-  sapply(sb, is.factor)
sb[isf] <- lapply(sb[isf], factor)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From r.hankin at noc.soton.ac.uk  Fri Feb  9 14:27:57 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 9 Feb 2007 13:27:57 +0000
Subject: [R] blockwise matrix multiplication
Message-ID: <36AE39E9-5D67-44C0-8754-FA1EC7B59370@soc.soton.ac.uk>

Hi

Given an n-by-n  matrix A, say n=10 and

A <- matrix(1:100,10,10)

and a vector x of length n where 1 <=x[i] <= n for i=1..n
say

x <- c(1,1,1,2,4,3,3,3,4,4)

and a matrix M of size max(x)-by-max(x), say

M <- matrix(c(1, 0.1, 0, 0.2, 0.1, 1, 0, 0, 0, 0, 1, 0.2, 0.2,
0, 0.2, 1),4,4)

how do I partition A according to the equivalence classes
of the elements  of x and  "block multiply"  by M?

I want

for(i in 1:4){for(j in 1:4){
   A[which(x==i),which(x==j)] <-  A[which(x==i),which(x==j)]*M[i,j]
}}


Is there a better way than this ghastly for() loop?


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From skiadas at hanover.edu  Fri Feb  9 14:40:44 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Fri, 9 Feb 2007 08:40:44 -0500
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <5E5263AE-B414-42F5-806C-0C624F8776B3@hanover.edu>
References: <993935.16184.qm@web32809.mail.mud.yahoo.com>
	<5E5263AE-B414-42F5-806C-0C624F8776B3@hanover.edu>
Message-ID: <4E0FC87D-8A69-4CA4-B746-71804565363B@hanover.edu>

Once again I forgot to reply to the whole list....
On Feb 9, 2007, at 8:39 AM, Charilaos Skiadas wrote:

> On Feb 9, 2007, at 8:13 AM, John Kane wrote:
>
>> The problem is that my dataframe has 1,s in about 50%
>> of the columns and I only want it to apply to a few
>> specified columns. My explanation may not have been
>> clear enough.
>>
>> Using your example,I want all values for tio2 set to 1
>> but not any values in al2o3 whereas zeta[zeta==1]<-NA
>> is also changing al2o3[3] to NA.
>>
>
> You need to index the zeta in zeta==1 in the same way as you do  
> with the zeta outside.
> I think the point is that if you do zeta[,cols][zeta==1] <- NA,  
> then the recycling of NA to obtain the correct number of elements  
> is done based on the elements in zeta[,cols]. But since zeta==1 is  
> a much longer vector than zeta[,cols], then zeta[,cols][zeta==1]  
> has a number of NA objects attached to its end, and hence has now   
> a longer length than the recycled NA that is supposed to replace it.
> But perhaps someone more expert in the internals can explain it in  
> greater detail, if the above is not right. In the mean time, the  
> following seems to work:
>
> > y <- rbinom(20, 1, 1/2)
> > dim(y) <- c(5,4)
> > colnames(y) <- c("one", "two", "three", "four")
> > x <- as.data.frame(y)
> > cl <- c("two", "three")
> > x[,cl][x[,cl]==1] <- NA
> > x
>   one two three four
> 1   0   0    NA    0
> 2   0   0     0    0
> 3   1   0     0    0
> 4   0  NA     0    1
> 5   1   0     0    1
>
>> Thanks
>
> Haris

Haris


From ezhil02 at yahoo.com  Fri Feb  9 14:49:30 2007
From: ezhil02 at yahoo.com (A Ezhil)
Date: Fri, 9 Feb 2007 05:49:30 -0800 (PST)
Subject: [R] Help in using multcomp.
Message-ID: <777449.83236.qm@web32412.mail.mud.yahoo.com>

Hi All,

I am trying use 'multcomp' for multiple comparisons
after my ANOVA analysis. I have used the following
code to do ANOVA:

dat <- matrix(rnorm(45), nrow=5, ncol=9)
f <- gl(3,3,9, label=c("C", "Tl", "T2"))

aof <- function(x) {
        m <- data.frame(f, x);
        aov(x ~ f, m)
}
amod <- apply(dat,1,aof)

Now, how can I use 'glht' for the above amod. I know
that I cannot use simply 

glht(amod, linfct = mcp(f = "Dunnett")). 

Also, if I want to use Dunnett for comparing C vs (T1
and T2), how can I specify this in the glht function.

Thanks in advance. 
Regards,
Ezhil


From maechler at stat.math.ethz.ch  Fri Feb  9 14:49:16 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Feb 2007 14:49:16 +0100
Subject: [R] Data.frame columns in R console
In-Reply-To: <ba8c09910702090421x26c68454ld7a140205d055861@mail.gmail.com>
References: <Pine.LNX.4.64.0702081948160.10800@gannet.stats.ox.ac.uk>
	<45CC41F5.16345.608171@localhost>
	<17868.18036.905558.215761@stat.math.ethz.ch>
	<ba8c09910702090421x26c68454ld7a140205d055861@mail.gmail.com>
Message-ID: <17868.31708.680596.679217@stat.math.ethz.ch>

>>>>> "Lauri" == Lauri Nikkinen <lauri.nikkinen at iki.fi>
>>>>>     on Fri, 9 Feb 2007 14:21:26 +0200 writes:


    Lauri> This still does not solve the issue that when I print in R console I get
    Lauri> columns that don't fit in the window underneath each other. Thanks anyway!

But Brian did give you all you needed (even more I'd say) to
solve that !?!?
Please apologize if I use a bit frank language, but using R, you
*really* are expected to read the documentation which is written
pretty carefully {probably that's what some people don't like
about it and call "confusing" ??}.

Specifically, Brian said

 BDR> 200 columns will take far more than 250 characters.  The help says

and then pointed you to the docu for options(width = .).
I think you need to reread that paragraph, particularly the word
'character' and then you will understand that your original
approach of using options(width = 250) can *not* be what you
want if your dataframe has 200 columns.

Martin


    Lauri> 2007/2/9, Martin Maechler <maechler at stat.math.ethz.ch>:
    >> 
    >> >>>>> "Petr" == Petr Pikal <petr.pikal at precheza.cz>
    >> >>>>>     on Fri, 09 Feb 2007 09:42:13 +0100 writes:
    >> 
    Petr> Hi
    Petr> On 9 Feb 2007 at 10:17, Lauri Nikkinen wrote:
    >> 
    >> >> Thank you for your answer. When I set options(width=250) I still get
    >> >> the same result when I print the data.frame on my Rgui console (R
    >> >> 2.4.1, Windows XP). Colums become underneath each other. I also get
    >> an
    >> >> error (?) message
    >> >> [ reached getOption("max.print") -- omitted 3462 rows ]].
    >> 
    >> As Petr explains below (and Brian Ripley), you
    >> *really* should use different means here ---
    >> but I think this is the first time that  the relatively new
    >> option 'max.print' has "hit R-help", hence one other hint, maybe
    >> useful to the public:
    >> 
    >> Note that the 'max.print' option was introduced exactly for the
    >> purpose of **protecting** the inadvertent user from a flood of output
    >> spilling into his console/gui/..
    >> (and apparently locking up R completely, we have even seen
    >> crashes when people wanted to print dataframes/matrices/arrays
    >> with millions of entries).
    >> 
    >> So, given the above message (yes, not an error),
    >> why did you not try to read
    >> help(getOption)
    >> and look for the word 'max.print' there ?
    >> 
    --> if you really really don't want to follow the advice of
    >> Brian and Petr, then say something like
    >> options(max.print = 1e6)
    >> 
    >> Martin Maechler, ETH Zurich
    >> 
    >> 
    >> >> For example if I have a data.frame with 4000 rows and 200
    >> >> columns I would like to be able to use scroll bars in
    >> >> Rconsole to investigate the whole data.frame.
    >> 
    Petr> I am not sure if it is the best idea. You shall probably use
    >> other
    Petr> means for checking your data frame.
    >> 
    Petr> Try ?summary, ?str or if you really want to check all values in
    >> data
    Petr> frame you can use
    >> 
    Petr> invisible(edit(test))
    >> 
    Petr> to open a spreadsheet like editor.
    >> 
    Petr> HTH
    Petr> Petr
    >> 
    >> 

    Lauri> [[alternative HTML version deleted]]

    Lauri> ______________________________________________
    Lauri> R-help at stat.math.ethz.ch mailing list
    Lauri> https://stat.ethz.ch/mailman/listinfo/r-help
    Lauri> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    Lauri> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Fri Feb  9 14:50:09 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Feb 2007 14:50:09 +0100
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <993935.16184.qm@web32809.mail.mud.yahoo.com>
References: <45CC2F21.19905.16F64B@localhost>
Message-ID: <45CC8A21.19350.17A7A71@localhost>

Hi

but you can easily extend it to only several columns

data[, col.selection] [data[, col.selection]==1]<-NA

so in my case

zeta[,2] [zeta[,2] == 1] <- NA

shall change only 1 in column 2 to NA (not tested)

HTH
Petr

On 9 Feb 2007 at 8:13, John Kane wrote:

Date sent:      	Fri, 9 Feb 2007 08:13:48 -0500 (EST)
From:           	John Kane <jrkrideau at yahoo.ca>
To:             	Petr Pikal <petr.pikal at precheza.cz>,
	Erik Iverson <iverson at biostat.wisc.edu>,
	R R-help <r-help at stat.math.ethz.ch>
Subject:        	Re: [R] setting a number of values to NA over  a data.frame.

> 
> --- Petr Pikal <petr.pikal at precheza.cz> wrote:
> 
> > Hi
> > 
> > Strange. It works for me without any problem.
> 
> The problem is that my dataframe has 1,s in about 50%
> of the columns and I only want it to apply to a few
> specified columns. My explanation may not have been
> clear enough.
> 
> Using your example,I want all values for tio2 set to 1
> but not any values in al2o3 whereas zeta[zeta==1]<-NA
> is also changing al2o3[3] to NA.
> 
> Thanks
> 
> > 
> > > zeta
> >   tepl tio2 al2o3  iep
> > 1   60    1   3.5 5.65
> > 2   60    1   2.0 5.00
> > 3   60    1   1.0 5.30
> > 4   60    0   2.0 4.65
> > 5   40    1   3.5 5.20
> > 6   40    1   2.0 4.85
> > 7   40    0   3.5 5.70
> > 8   40    0   2.0 5.25
> > > zeta[zeta==1]<-NA
> > > zeta
> >   tepl tio2 al2o3  iep
> > 1   60   NA   3.5 5.65
> > 2   60   NA   2.0 5.00
> > 3   60   NA    NA 5.30
> > 4   60    0   2.0 4.65
> > 5   40   NA   3.5 5.20
> > 6   40   NA   2.0 4.85
> > 7   40    0   3.5 5.70
> > 8   40    0   2.0 5.25
> > 
> > > str(zeta)
> > 'data.frame':   8 obs. of  4 variables:
> >  $ tepl : int  60 60 60 60 40 40 40 40
> >  $ tio2 : num  NA NA NA 0 NA NA 0 0
> >  $ al2o3: num  3.5 2 NA 2 3.5 2 3.5 2
> >  $ iep  : num  5.65 5 5.3 4.65 5.2 4.85 5.7 5.25
> > >
> > 
> > HTH
> > Petr
> > 
> > 
> > 
> > On 7 Feb 2007 at 16:57, Erik Iverson wrote:
> > 
> > Date sent:      	Wed, 07 Feb 2007 16:57:40 -0600
> > From:           	Erik Iverson
> > <iverson at biostat.wisc.edu>
> > To:             	John Kane <jrkrideau at yahoo.ca>
> > Copies to:      	R R-help <r-help at stat.math.ethz.ch>
> > Subject:        	Re: [R] setting a number of values
> > to NA over  a data.frame.
> > 
> > > John -
> > > 
> > > Your initial problem uses 0, but the example uses
> > 1 for the value that
> > > gets an NA.  My solution uses 1 to fit with your
> > example.  There may
> > > be a better way, but try something like
> > > 
> > > data1[3:5] <- data.frame(lapply(data1[3:5],
> > function(x) ifelse(x==1,
> > > NA, x)))
> > > 
> > > The data1[3:5] is just a test subset  of columns I
> > chose from your
> > > data1 example.  Notice it appears twice, once on
> > each side of the
> > > assignment operator.
> > > 
> > > In English, apply to each column of the data frame
> > (which is a list) a
> > > function that will return NA if the element is 1,
> > and the value
> > > otherwise, and then turn the modified lists into a
> > data.frame, and
> > > save it as data1.
> > > 
> > > 
> > > 
> > > See the help files for lapply and ifelse if you
> > haven't seen those
> > > before.
> > > 
> > > Maybe someone has a better way?
> > > 
> > > Erik
> > > 
> > > John Kane wrote:
> > > > This is probably a simple problem but I don't
> > see a
> > > > solution.
> > > > 
> > > > I have a data.frame with a number of columns
> > where I
> > > > would like 0 <- NA
> > > > 
> > > > thus I have df1[,144:157] <- NA if df1[, 144:
> > 157] ==0
> > > > and df1[, 190:198] <- NA if df1[, 190:198] ==0
> > > > 
> > > > but I cannot figure out a way do this.  
> > > > 
> > > > cata <- c( 1,1,6,1,1,NA)
> > > > catb <- c( 1,2,3,4,5,6)
> > > > doga <- c(3,5,3,6,4, 0)
> > > > dogb <- c(2,4,6,8,10, 12)
> > > > rata <- c (NA, 9, 9, 8, 9, 8)
> > > > ratb <- c( 1,2,3,4,5,6)
> > > > bata <- c( 12, 42,NA, 45, 32, 54)
> > > > batb <- c( 13, 15, 17,19,21,23)
> > > > id <- c('a', 'b', 'b', 'c', 'a', 'b')
> > > > site <- c(1,1,4,4,1,4)
> > > > mat1 <-  cbind(cata, catb, doga, dogb, rata,
> > ratb,
> > > > bata, batb)
> > > > 
> > > > data1 <- data.frame(site, id, mat1)
> > > > data1
> > > > 
> > > >  # Obviously this works fine for one column
> > > > 
> > > > data1$site[data1$site ==1] <- NA  ; data1
> > > > 
> > > > but I cannot see how to do this with indices
> > that
> > > > would allow me to do more than one column in the
> > > > data.frame.
> > > > 
> > > > At one point I even tried something like this
> > > > a <- c("site")
> > > > data1$a[data1$a ==1] <- NA
> > > > 
> > > > which seems to produce a corrupt data.frame.
> > > > 
> > > > I am sure it is simple but I don't see it.  
> > > > 
> > > > Any help would be much appreciated.
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html and
> > provide commented,
> > > > minimal, self-contained, reproducible code.
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and
> > provide commented,
> > > minimal, self-contained, reproducible code.
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From jrkrideau at yahoo.ca  Fri Feb  9 14:59:34 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 9 Feb 2007 08:59:34 -0500 (EST)
Subject: [R] Re : Re: setting a number of values to NA over a data.frame.
In-Reply-To: <45CB1CAA.6030400@ema.fr>
Message-ID: <298129.64679.qm@web32805.mail.mud.yahoo.com>


--- Olivier ETERRADOSSI <olivier.eterradossi at ema.fr>
wrote:

> Hi again,
> 
> Awfully sorry John, I should have been sleeping and
> did not see your 
> full post....
> 
> here is a way, unless I miss the point again :
> 
>
fake<-as.data.frame(cbind(seq(1,10,by=1),c(rep(1,4),rep(0,4),rep(2,2))))
> 
> # from my previous post
> 
> # one moree column this time !
> fake3<-cbind(fake,fake$V2)
> index<-c(2,3)
> fake3[,index][fake3[,index]==0]<-NA
> 
> not nice, but seems to do the job.
> Hope this helps... this time :-)
> Olivier

That looks like it and is similar to one Haris
suggested. I think I was messing up the fake3[,index]
reference somehow.

 I need to make my explanations a bit clearer. 

Thanks



> 
> > Date: Thu, 08 Feb 2007 13:21:47 +0100
> From: Olivier ETERRADOSSI
> <olivier.eterradossi at ema.fr>
> To:  r-help at stat.math.ethz.ch
> Subject: Re : Re: [R] setting a number of values to
> NA over  a data.frame.
> 
> Hi John,
> 
> Unless I miss a point, why dont you try something
> like :
> 
> # some fake data
>  >
>
fake<-as.data.frame(cbind(seq(1,10,by=1),c(rep(1,4),rep(0,4),rep(2,2))))
>       V1 V2
>  1    1    1
>  2    2    1
>  3    3    1
>  4    4    1
>  5    5    0
>  6    6    0
>  7    7    0
>  8    8    0
>  9    9    2
> 10   10    2
> 
> # change 0 by NA
>  > fake[fake==0]<-NA  # or fake$V2[fake$V2==0]<-NA
> if you don't want all 
> 0 in the dataframe to be changed to NA
> # test
>  > is.na(fake$V2)
> [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE
> FALSE FALSE
> 
> Sorry if I did not understand the issue. Hope this
> helps. Olivier
> 
> 
> Jim Lemon wrote :
> 
> > John Kane wrote:
> >   
> >> > This is probably a simple problem but I don't
> see a
> >> > solution.
> >> > 
> >> > I have a data.frame with a number of columns
> where I
> >> > would like 0 <- NA
> >> > 
> >>     
> > Hi John,
> > You might have a look at "toNA" in the prettyR
> package. Wait for version 
> > 1.0-4, just uploaded, as I have fixed a bug in
> that function.
> >
> > Jim
> 
> -- 
> Olivier ETERRADOSSI
> Ma?tre-Assistant
> CMGD / Equipe "Propri?t?s Psycho-Sensorielles des
> Mat?riaux"
> Ecole des Mines d'Al?s
> H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
> tel std: +33 (0)5.59.30.54.25
> tel direct: +33 (0)5.59.30.90.35 
> fax: +33 (0)5.59.30.63.68
> http://www.ema.fr
> 
> 
>


From jrkrideau at yahoo.ca  Fri Feb  9 15:01:54 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 9 Feb 2007 09:01:54 -0500 (EST)
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <45CAED75.6060401@bitwrit.com.au>
Message-ID: <2884.22388.qm@web32808.mail.mud.yahoo.com>


--- Jim Lemon <jim at bitwrit.com.au> wrote:

> John Kane wrote:
> > This is probably a simple problem but I don't see
> a
> > solution.
> > 
> > I have a data.frame with a number of columns where
> I
> > would like 0 <- NA
> > 
> Hi John,
> You might have a look at "toNA" in the prettyR
> package. Wait for version 
> 1.0-4, just uploaded, as I have fixed a bug in that
> function.
> 
> Jim

Thanks I will try it Monday or so


From Galina_Glazko at URMC.Rochester.edu  Fri Feb  9 15:18:03 2007
From: Galina_Glazko at URMC.Rochester.edu (Glazko, Galina)
Date: Fri, 9 Feb 2007 09:18:03 -0500
Subject: [R] extract submatrix with unique names
Message-ID: <4E6D6ECA03F7CF48AC313DBD3BA5B2579E632D@e2k3ms4.urmc-sh.rochester.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/0fa6a4ea/attachment.pl 

From ggrothendieck at gmail.com  Fri Feb  9 15:19:45 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Feb 2007 09:19:45 -0500
Subject: [R] blockwise matrix multiplication
In-Reply-To: <36AE39E9-5D67-44C0-8754-FA1EC7B59370@soc.soton.ac.uk>
References: <36AE39E9-5D67-44C0-8754-FA1EC7B59370@soc.soton.ac.uk>
Message-ID: <971536df0702090619p47d8752di465104873661e945@mail.gmail.com>

Try this:

   A * M[as.matrix(expand.grid(x,x))[,2:1]]


On 2/9/07, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi
>
> Given an n-by-n  matrix A, say n=10 and
>
> A <- matrix(1:100,10,10)
>
> and a vector x of length n where 1 <=x[i] <= n for i=1..n
> say
>
> x <- c(1,1,1,2,4,3,3,3,4,4)
>
> and a matrix M of size max(x)-by-max(x), say
>
> M <- matrix(c(1, 0.1, 0, 0.2, 0.1, 1, 0, 0, 0, 0, 1, 0.2, 0.2,
> 0, 0.2, 1),4,4)
>
> how do I partition A according to the equivalence classes
> of the elements  of x and  "block multiply"  by M?
>
> I want
>
> for(i in 1:4){for(j in 1:4){
>   A[which(x==i),which(x==j)] <-  A[which(x==i),which(x==j)]*M[i,j]
> }}
>
>
> Is there a better way than this ghastly for() loop?
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.hankin at noc.soton.ac.uk  Fri Feb  9 15:25:06 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 9 Feb 2007 14:25:06 +0000
Subject: [R] blockwise matrix multiplication
In-Reply-To: <971536df0702090619p47d8752di465104873661e945@mail.gmail.com>
References: <36AE39E9-5D67-44C0-8754-FA1EC7B59370@soc.soton.ac.uk>
	<971536df0702090619p47d8752di465104873661e945@mail.gmail.com>
Message-ID: <6B1F23E2-1BBC-4654-8926-25EDF895AF17@soc.soton.ac.uk>

Wow.


It generalizes nicely to arbitrary dimensional arrays too.

thanks a lot!


rksh


On 9 Feb 2007, at 14:19, Gabor Grothendieck wrote:

> Try this:
>
>   A * M[as.matrix(expand.grid(x,x))[,2:1]]
>
>
> On 2/9/07, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
>> Hi
>>
>> Given an n-by-n  matrix A, say n=10 and
>>
>> A <- matrix(1:100,10,10)
>>
>> and a vector x of length n where 1 <=x[i] <= n for i=1..n
>> say
>>
>> x <- c(1,1,1,2,4,3,3,3,4,4)
>>
>> and a matrix M of size max(x)-by-max(x), say
>>
>> M <- matrix(c(1, 0.1, 0, 0.2, 0.1, 1, 0, 0, 0, 0, 1, 0.2, 0.2,
>> 0, 0.2, 1),4,4)
>>
>> how do I partition A according to the equivalence classes
>> of the elements  of x and  "block multiply"  by M?
>>
>> I want
>>
>> for(i in 1:4){for(j in 1:4){
>>   A[which(x==i),which(x==j)] <-  A[which(x==i),which(x==j)]*M[i,j]
>> }}
>>
>>
>> Is there a better way than this ghastly for() loop?
>>
>>
>> --
>> Robin Hankin
>> Uncertainty Analyst
>> National Oceanography Centre, Southampton
>> European Way, Southampton SO14 3ZH, UK
>>  tel  023-8059-7743
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From murdoch at stats.uwo.ca  Fri Feb  9 15:29:33 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Feb 2007 09:29:33 -0500
Subject: [R] generate Binomial (not Binary) data
In-Reply-To: <426059.35923.qm@web23404.mail.ird.yahoo.com>
References: <426059.35923.qm@web23404.mail.ird.yahoo.com>
Message-ID: <45CC854D.5040805@stats.uwo.ca>

On 2/7/2007 8:20 AM, Marc Bernard wrote:
> Dear All,
>    
>   I am looking for an R function or any other reference to generate a series of correlated Binomial (not a Bernoulli) data. The "bindata" library can do this for the binary not the binomial case.

Ted asked how you want your series correlated.  Another question is how 
you want it "binomial":  do you want each value to be binomial with 
parameters conditional on previous values, or do you want each to be 
marginally binomial with some fixed parameters?

I can only think of two trivial solutions that meet both conditions 
simultaneously:  the i.i.d. sequence, and a sequence that repeats 0 
indefinitely.  I'd be interested in hearing if anyone knows of any 
non-trivial examples of sequences where the distributions are both 
conditionally and marginally binomial, or a proof that there are none.

Duncan Murdoch


From murdoch at stats.uwo.ca  Fri Feb  9 15:44:09 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Feb 2007 09:44:09 -0500
Subject: [R] Numerical Recipes in R
In-Reply-To: <a2b3004b0702090200u6174affbw350809802751bdba@mail.gmail.com>
References: <a2b3004b0702090200u6174affbw350809802751bdba@mail.gmail.com>
Message-ID: <45CC88B9.30204@stats.uwo.ca>

On 2/9/2007 5:00 AM, Lorenzo Isella wrote:
> Dear All,
> So far I have mainly used R for data analysis and simple numerics
> (integration of functions, splines etc...).
> However, I have recently been astonished at finding out that many
> things I thought were only achievable with Fortran or C can be done
> e.g. entirely using MatLab.
> When I try asking around if the same could be achieved by R,
> inevitably the answer is that either people do not know R or there is
> so much numerical MatLab code (for instance for solving partial
> differential equations), that there is no point in switching to R.
> Does anyone know if this is really the situation? I am wondering if
> there is anywhere a kind of freely available collection of reliable
> numerical software written in R which is not only geared towards
> statistics and data analysis.

CRAN is the place to look, but I wouldn't expect to find PDE solvers 
there.  (I haven't looked, they may well be there.)  Matlab does a good 
job at those, and most users who want to do that sort of calculation are 
familiar with Matlab, so there isn't much motivation to rewrite those 
things for R.

On the other hand, if you already have Fortran or C code to do these and 
you like it, it is not very hard to write an R interface to it, so you 
can put together your own package and access that code from R.  I think 
this would be a big improvement over standalone Fortran or C (because 
the data handling and graphics functions in R are much easier to use 
than most Fortran or C libraries); whether it would be an improvement 
over Matlab really depends on what you want to do, how good your 
existing code is, and how well you already know Matlab.

Duncan Murdoch


From ripley at stats.ox.ac.uk  Fri Feb  9 15:50:53 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Feb 2007 14:50:53 +0000 (GMT)
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <17868.15847.789394.466496@stat.math.ethz.ch>
References: <40e66e0b0702081424u5c25420cp5d915d99552ba568@mail.gmail.com>
	<40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>
	<004801c74bda$b1e30f30$7c94100a@win.ad.jhu.edu>
	<17868.15847.789394.466496@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0702091428520.30945@gannet.stats.ox.ac.uk>

The other reason why pmin/pmax are preferable to your functions is that 
they are fully generic.  It is not easy to write C code which takes into 
account that <, [, [<- and is.na are all generic.  That is not to say that 
it is not worth having faster restricted alternatives, as indeed we do 
with rep.int and seq.int.

Anything that uses arithmetic is making strong assumptions about the 
inputs.  It ought to be possible to write a fast C version that worked for 
atomic vectors (logical, integer, real and character), but is there 
any evidence of profiled real problems where speed is an issue?

On Fri, 9 Feb 2007, Martin Maechler wrote:

>>>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
>>>>>>     on Thu, 8 Feb 2007 18:41:38 -0500 writes:
>
>    Ravi> Hi,
>    Ravi> "greaterOf" is indeed an interesting function.  It is much faster than the
>    Ravi> equivalent R function, "pmax", because pmax does a lot of checking for
>    Ravi> missing data and for recycling.  Tom Lumley suggested a simple function to
>    Ravi> replace pmax, without these checks, that is analogous to greaterOf, which I
>    Ravi> call fast.pmax.
>
>    Ravi> fast.pmax <- function(x,y) {i<- x<y; x[i]<-y[i]; x}
>
>    Ravi> Interestingly, greaterOf is even faster than fast.pmax, although you have to
>    Ravi> be dealing with very large vectors (O(10^6)) to see any real difference.
>
> Yes. Indeed, I have a file, first version dated from 1992
> where I explore the "slowness" of pmin() and pmax() (in S-plus
> 3.2 then). I had since added quite a few experiments and versions to that
> file in the past.
>
> As consequence, in the robustbase CRAN package (which is only a bit
> more than a year old though), there's a file, available as
>  https://svn.r-project.org/R-packages/robustbase/R/Auxiliaries.R
> with the very simple content {note line 3 !}:
>
> -------------------------------------------------------------------------
> ### Fast versions of pmin() and pmax() for 2 arguments only:
>
> ### FIXME: should rather add these to R
> pmin2 <- function(k,x) (x+k - abs(x-k))/2
> pmax2 <- function(k,x) (x+k + abs(x-k))/2
> -------------------------------------------------------------------------
>
> {the "funny" argument name 'k' comes from the use of these to
> compute Huber's psi() fast :
>
>  psiHuber <- function(x,k)  pmin2(k, pmax2(- k, x))
>  curve(psiHuber(x, 1.35), -3,3, asp = 1)
> }
>
> One point *is* that I think proper function names would be pmin2() and
> pmax2() since they work with exactly 2 arguments,
> whereas IIRC the feature to work with '...' is exactly the
> reason that pmax() and pmin() are so much slower.
>
> I've haven't checked if Gabor's
>     pmax2.G <- function(x,y) {z <- x > y; z * (x-y) + y}
> is even faster than the abs() using one.
> It may have the advantage of giving *identical* results (to the
> last bit!)  to pmax()  which my version does not --- IIRC the
> only reason I did not follow my own 'FIXME' above.
>
> I  had then planned to implement pmin2() and pmax2() in C code, trivially,
> and and hence get identical (to the last bit!) behavior as
> pmin()/pmax(); but I now tend to think that the proper approach is to
> code pmin() and pmax() via .Internal() and hence C code ...
>
> [Not before DSC and my vacations though!!]
>
> Martin Maechler, ETH Zurich
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bates at stat.wisc.edu  Fri Feb  9 16:05:07 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 9 Feb 2007 09:05:07 -0600
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <Pine.LNX.4.64.0702091428520.30945@gannet.stats.ox.ac.uk>
References: <40e66e0b0702081424u5c25420cp5d915d99552ba568@mail.gmail.com>
	<40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>
	<004801c74bda$b1e30f30$7c94100a@win.ad.jhu.edu>
	<17868.15847.789394.466496@stat.math.ethz.ch>
	<Pine.LNX.4.64.0702091428520.30945@gannet.stats.ox.ac.uk>
Message-ID: <40e66e0b0702090705u669db638yfa79c4c71e4ea343@mail.gmail.com>

On 2/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> The other reason why pmin/pmax are preferable to your functions is that
> they are fully generic.  It is not easy to write C code which takes into
> account that <, [, [<- and is.na are all generic.  That is not to say that
> it is not worth having faster restricted alternatives, as indeed we do
> with rep.int and seq.int.
>
> Anything that uses arithmetic is making strong assumptions about the
> inputs.  It ought to be possible to write a fast C version that worked for
> atomic vectors (logical, integer, real and character), but is there
> any evidence of profiled real problems where speed is an issue?

Yes.  I don't have the profiled timings available now and one would
need to go back to earlier versions of R to reproduce them but I did
encounter a situation where the bottleneck in a practical computation
was pmin/pmax.  The binomial and poisson families for generalized
linear models used pmin and pmax to avoid boundary conditions when
evaluating the inverse link and other functions.  When I profiled the
execution of some generalized linear model and, more importantly for
me, generalized linear mixed model fits, these calls to pmin and pmax
were the bottleneck.  That is why I moved some of the calculations for
the binomial and poisson families in the stats package to compiled
code.

In that case I didn't rewrite the general form of pmin and pmax, I
replaced specific calls in the compiled code.

>
> On Fri, 9 Feb 2007, Martin Maechler wrote:
>
> >>>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
> >>>>>>     on Thu, 8 Feb 2007 18:41:38 -0500 writes:
> >
> >    Ravi> Hi,
> >    Ravi> "greaterOf" is indeed an interesting function.  It is much faster than the
> >    Ravi> equivalent R function, "pmax", because pmax does a lot of checking for
> >    Ravi> missing data and for recycling.  Tom Lumley suggested a simple function to
> >    Ravi> replace pmax, without these checks, that is analogous to greaterOf, which I
> >    Ravi> call fast.pmax.
> >
> >    Ravi> fast.pmax <- function(x,y) {i<- x<y; x[i]<-y[i]; x}
> >
> >    Ravi> Interestingly, greaterOf is even faster than fast.pmax, although you have to
> >    Ravi> be dealing with very large vectors (O(10^6)) to see any real difference.
> >
> > Yes. Indeed, I have a file, first version dated from 1992
> > where I explore the "slowness" of pmin() and pmax() (in S-plus
> > 3.2 then). I had since added quite a few experiments and versions to that
> > file in the past.
> >
> > As consequence, in the robustbase CRAN package (which is only a bit
> > more than a year old though), there's a file, available as
> >  https://svn.r-project.org/R-packages/robustbase/R/Auxiliaries.R
> > with the very simple content {note line 3 !}:
> >
> > -------------------------------------------------------------------------
> > ### Fast versions of pmin() and pmax() for 2 arguments only:
> >
> > ### FIXME: should rather add these to R
> > pmin2 <- function(k,x) (x+k - abs(x-k))/2
> > pmax2 <- function(k,x) (x+k + abs(x-k))/2
> > -------------------------------------------------------------------------
> >
> > {the "funny" argument name 'k' comes from the use of these to
> > compute Huber's psi() fast :
> >
> >  psiHuber <- function(x,k)  pmin2(k, pmax2(- k, x))
> >  curve(psiHuber(x, 1.35), -3,3, asp = 1)
> > }
> >
> > One point *is* that I think proper function names would be pmin2() and
> > pmax2() since they work with exactly 2 arguments,
> > whereas IIRC the feature to work with '...' is exactly the
> > reason that pmax() and pmin() are so much slower.
> >
> > I've haven't checked if Gabor's
> >     pmax2.G <- function(x,y) {z <- x > y; z * (x-y) + y}
> > is even faster than the abs() using one.
> > It may have the advantage of giving *identical* results (to the
> > last bit!)  to pmax()  which my version does not --- IIRC the
> > only reason I did not follow my own 'FIXME' above.
> >
> > I  had then planned to implement pmin2() and pmax2() in C code, trivially,
> > and and hence get identical (to the last bit!) behavior as
> > pmin()/pmax(); but I now tend to think that the proper approach is to
> > code pmin() and pmax() via .Internal() and hence C code ...
> >
> > [Not before DSC and my vacations though!!]
> >
> > Martin Maechler, ETH Zurich
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From info at aghmed.fsnet.co.uk  Fri Feb  9 16:26:18 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Fri, 09 Feb 2007 15:26:18 +0000
Subject: [R] enhanced question / standardized coefficients
In-Reply-To: <OF70F7CAB8.E870EA54-ON8725727B.00546962-8725727B.00576C7A@
	usgs.gov>
References: <20070207144918.LNXW1750.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<OF70F7CAB8.E870EA54-ON8725727B.00546962-8725727B.00576C7A@usgs.gov>
Message-ID: <7.0.0.16.0.20070209152435.0195b130@aghmed.fsnet.co.uk>

At 15:51 07/02/2007, Brian S Cade wrote:
>There was a nice paper in The American Statistician by Johan Bring (1994.
>How to standardize regression coefficients.  The American Statistician
>48(3):209-213) pointing out that comparing ratios of t-test statistic
>values (for null hypothesis that parameter = 0) is equivalent to comparing
>ratios of standardized coefficients where standardization is based on the
>partial (conditional) standard deviations of the parameter estimates.  And
>this is equivalent to thinking about the incremental improvement in
>R-squared that is obtained by including a variable in the regression model
>after all others are already in the model.   It would seem possible to
>extend this idea to categorical factor variables with more than 2 levels
>(>1 indicator variable), given the relation between an F and t-test
>statistic.

You may also be interested in
http://www.tfh-berlin.de/~groemp/rpack.html
As well as her package relaimpo this also links to her article in JSS 
which I found thought provoking.


>Any way something to think about, though there are no doubt still
>limitations in trying to equate effects of variables measured on disparate
>scales.
>
>Brian
>
>Brian S. Cade
>
>U. S. Geological Survey
>Fort Collins Science Center
>2150 Centre Ave., Bldg. C
>Fort Collins, CO  80526-8818
>
>email:  brian_cade at usgs.gov
>tel:  970 226-9326
>
>
>
>"John Fox" <jfox at mcmaster.ca>
>Sent by: r-help-bounces at stat.math.ethz.ch
>02/07/2007 07:49 AM
>
>To
>"'Simon P. Kempf'" <simon.kempf at web.de>
>cc
>r-help at stat.math.ethz.ch
>Subject
>Re: [R] enhanced question / standardized coefficients
>
>
>
>
>
>
>Dear Simon,
>
>In my opinion, standardized coefficients only offer the illusion of
>comparison for quantitative explanatory variables, since there's no deep
>reason that the standard deviation of one variable has the same meaning as
>the standard deviation of another. Indeed, if the variables are in the
>same
>units of measurement in the first place, permitting direct comparison of
>unstandardized coefficients, then separate standardization of each X is
>like
>using a rubber ruler.
>
>That said, as you point out, it makes no sense to standardize the dummy
>regressors for a factor, so you can just standardize the quantitative
>variables (Y and X's) in the regression equation.
>
>I hope that this helps,
>  John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox
>--------------------------------
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Simon P. Kempf
> > Sent: Wednesday, February 07, 2007 9:27 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] enhanced question / standardized coefficients
> >
> > Hello,
> >
> >
> >
> > I would like to repost the question of Joerg:
> >
> >
> >
> > Hello everybody,
> >
> > a question that connect to the question of Frederik Karlsons
> > about 'how to stand. betas'
> > With the stand. betas i can compare the influence of the
> > different explaning variables. What do i with the betas of
> > factors? I can't use the solution of JohnFox, because there
> > is no sd of an factor. How can i compare the influence of the
> > factor with the influence of the numeric variables?
> >
> > I got the same problem. In my regression equation there are
> > several categorical variables and  I would like to compute
> > the standard coefficients. How can I do this?
> >
> >
> >
> > Simon
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >                [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]

Michael Dewey
http://www.aghmed.fsnet.co.uk


From dingjia at gmail.com  Fri Feb  9 16:35:21 2007
From: dingjia at gmail.com (jia ding)
Date: Fri, 9 Feb 2007 16:35:21 +0100
Subject: [R] cluster "non-diet", "diet" example.
Message-ID: <91ae6e350702090735q25c703f0y46f8337cbbe36cb2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/bc9f381b/attachment.pl 

From anthony.staines at gmail.com  Fri Feb  9 16:41:34 2007
From: anthony.staines at gmail.com (Anthony Staines)
Date: Fri, 9 Feb 2007 15:41:34 +0000
Subject: [R] Using variable names in for loops - Generating plots
	semi-automatically from a series of variables Partly solved
Message-ID: <6d3975af0702090741j5400dd8bj59758b8515a478c9@mail.gmail.com>

Hi,

This code is trying to produce a series of graphics files, with plots
of male and female disease rates by age, one plot per disease. The
dataframe contains a variable 'Age' and a set of variables called
'Male_CVD, Female_CVD,Male_RTA,Female_RTA, and so on. For each
disease, I want to pull out the column of data containing the word
'Male' and plot this against age, and then add a line to the plot for
the corresponding column containing 'Female'.
--
attach(data)

Diseases <- c("Cardiovascular disease","Road Traffic Injury",  ...
,"All causes")
Male <- names(data)[grep("Male",names(data))]
Female <- names(data)[grep("Female",names(data))]
#Disease contains disease labels in the correct order, and Male and
Female now hold the (correct) variables.

for (i in seq(1,length(Diseases)))
  {
jpeg(paste(Diseases[i],".jpg")) #This works fine!
plot(Male[i]~Age)                  #This does not
lines(Female[i]~Age)
dev.off()
}
detach(data)
-- 

This doesn't work, of course, because Male[i] is a character variable
which is the same as the name of a column in the dataframe, but it is
not the column itself, nor is it the name of the column.

> eval(expression(Male[1]))
[1] "Male_HEART"

So, how can I refer here to something in the dataframe?

This next code works, but by kind of bypassing my real question :-
--
Diseases <- c("Cardiovascular disease" ... ,"All causes")
Male <- names(data)[grep("Male",names(data))]
Female <- names(data)[grep("Female",names(data))]

for (i in seq(1,length(Diseases)))
  {

jpeg(paste(Diseases[i],".jpg"))

plot(data[Male][[1]]~data$Age,type='b')

lines(data[Female][[i]]~data$Age,type='b')
dev.off()
}
--

Is this the R way to do it, or am I missing something more basic?

Anthony Staines

P.S. Using R 2.4.1 on Linux
-- 
Dr. Anthony Staines, Senior Lecturer in Epidemiology.
***Note New Address *** Note New Address ****
School  of Public Health and Population Sciences, Woodview House,
UCD, Belfield, Dublin 4, Ireland.
Tel:- +353 1 716 7345. Fax:- +353 1 716 7407 Mobile:- +353 86 606 9713
Web:- http://phm.ucd.ie


From kubovy at virginia.edu  Fri Feb  9 16:43:36 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 9 Feb 2007 10:43:36 -0500
Subject: [R] How to count the number of NAs in each column of a df?
In-Reply-To: <45CC419B.3060407@optonline.net>
References: <20070208221659.BTZ03190@po-d.temple.edu>
	<45CC419B.3060407@optonline.net>
Message-ID: <172C3279-2CED-4005-892F-899B3D337909@virginia.edu>

Dear Jim (25 minutes!), Richard (27 minutes!), and Chuck,

Thanks to your hints, I have come up with what I hope is a pithy  
idiom that drops columns of a dataframe (df) in which the number of  
NAs is > (e.g.) 30.

tmp <- df
tmp <- tmp[, which(as.numeric(colSums(is.na(tmp))) > 30)]
df <- tmp

I wonder if we have a place to keep R programming idioms (which  
probably get unnecessarily reinvented). Is the R-Wiki suitable?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From luojingqin at yahoo.com  Fri Feb  9 16:57:32 2007
From: luojingqin at yahoo.com (Jingqin luo)
Date: Fri, 9 Feb 2007 07:57:32 -0800 (PST)
Subject: [R] problem with eigen() function
Message-ID: <127186.10085.qm@web56814.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/cf5c6e0b/attachment.pl 

From roland.rproject at gmail.com  Fri Feb  9 17:00:30 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Fri, 9 Feb 2007 11:00:30 -0500
Subject: [R] two perspective plots in in plot
Message-ID: <47c7c59e0702090800r47f8426fj383d2745f99be367@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/5ee2023e/attachment.pl 

From jhorn at bu.edu  Fri Feb  9 17:10:15 2007
From: jhorn at bu.edu (Jason Horn)
Date: Fri, 9 Feb 2007 11:10:15 -0500
Subject: [R] Replace individual values in a data frame with NA
Message-ID: <4B6A3858-0F3F-49BB-8EEA-8B81D5389B0F@bu.edu>

I'd like to replace a value in a data frame with an NA, but can't  
figure out how.

For example, say you have

a<-c(1,2,3,4)
b<-c(5,6,7,8)
data<-data.frame(a,b)

Now, how would you set the third row of the second column ( data 
[[3,2]] ) to NA?

I have tried all types of permutations with is.na, including is.na<- 
data[[3,2]], which does not work.

Thanks


From rvaradhan at jhmi.edu  Fri Feb  9 17:10:50 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 9 Feb 2007 11:10:50 -0500
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <40e66e0b0702090705u669db638yfa79c4c71e4ea343@mail.gmail.com>
References: <40e66e0b0702081424u5c25420cp5d915d99552ba568@mail.gmail.com>
	<40e66e0b0702081500g24ccf64cx87d792e720a14612@mail.gmail.com>
	<004801c74bda$b1e30f30$7c94100a@win.ad.jhu.edu>
	<17868.15847.789394.466496@stat.math.ethz.ch>
	<Pine.LNX.4.64.0702091428520.30945@gannet.stats.ox.ac.uk>
	<40e66e0b0702090705u669db638yfa79c4c71e4ea343@mail.gmail.com>
Message-ID: <001001c74c64$de1076d0$7c94100a@win.ad.jhu.edu>

Hi Profs. Ripley and Bates,

I also recollect from a Tom Lumley email that when he profiled an MCMC
computation, he found that pmin/pmax was the bottleneck.  That is why he
suggested the function that I called fast.pmax.  I think that it would be
nice to have restricted alternative functions dealing exclusively with
numeric mode.

Best,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Douglas Bates
Sent: Friday, February 09, 2007 10:05 AM
To: Prof Brian Ripley
Cc: R-Help
Subject: Re: [R] Timings of function execution in R [was Re: R in Industry]

On 2/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> The other reason why pmin/pmax are preferable to your functions is that
> they are fully generic.  It is not easy to write C code which takes into
> account that <, [, [<- and is.na are all generic.  That is not to say that
> it is not worth having faster restricted alternatives, as indeed we do
> with rep.int and seq.int.
>
> Anything that uses arithmetic is making strong assumptions about the
> inputs.  It ought to be possible to write a fast C version that worked for
> atomic vectors (logical, integer, real and character), but is there
> any evidence of profiled real problems where speed is an issue?

Yes.  I don't have the profiled timings available now and one would
need to go back to earlier versions of R to reproduce them but I did
encounter a situation where the bottleneck in a practical computation
was pmin/pmax.  The binomial and poisson families for generalized
linear models used pmin and pmax to avoid boundary conditions when
evaluating the inverse link and other functions.  When I profiled the
execution of some generalized linear model and, more importantly for
me, generalized linear mixed model fits, these calls to pmin and pmax
were the bottleneck.  That is why I moved some of the calculations for
the binomial and poisson families in the stats package to compiled
code.

In that case I didn't rewrite the general form of pmin and pmax, I
replaced specific calls in the compiled code.

>
> On Fri, 9 Feb 2007, Martin Maechler wrote:
>
> >>>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
> >>>>>>     on Thu, 8 Feb 2007 18:41:38 -0500 writes:
> >
> >    Ravi> Hi,
> >    Ravi> "greaterOf" is indeed an interesting function.  It is much
faster than the
> >    Ravi> equivalent R function, "pmax", because pmax does a lot of
checking for
> >    Ravi> missing data and for recycling.  Tom Lumley suggested a simple
function to
> >    Ravi> replace pmax, without these checks, that is analogous to
greaterOf, which I
> >    Ravi> call fast.pmax.
> >
> >    Ravi> fast.pmax <- function(x,y) {i<- x<y; x[i]<-y[i]; x}
> >
> >    Ravi> Interestingly, greaterOf is even faster than fast.pmax,
although you have to
> >    Ravi> be dealing with very large vectors (O(10^6)) to see any real
difference.
> >
> > Yes. Indeed, I have a file, first version dated from 1992
> > where I explore the "slowness" of pmin() and pmax() (in S-plus
> > 3.2 then). I had since added quite a few experiments and versions to
that
> > file in the past.
> >
> > As consequence, in the robustbase CRAN package (which is only a bit
> > more than a year old though), there's a file, available as
> >  https://svn.r-project.org/R-packages/robustbase/R/Auxiliaries.R
> > with the very simple content {note line 3 !}:
> >
> >
-------------------------------------------------------------------------
> > ### Fast versions of pmin() and pmax() for 2 arguments only:
> >
> > ### FIXME: should rather add these to R
> > pmin2 <- function(k,x) (x+k - abs(x-k))/2
> > pmax2 <- function(k,x) (x+k + abs(x-k))/2
> >
-------------------------------------------------------------------------
> >
> > {the "funny" argument name 'k' comes from the use of these to
> > compute Huber's psi() fast :
> >
> >  psiHuber <- function(x,k)  pmin2(k, pmax2(- k, x))
> >  curve(psiHuber(x, 1.35), -3,3, asp = 1)
> > }
> >
> > One point *is* that I think proper function names would be pmin2() and
> > pmax2() since they work with exactly 2 arguments,
> > whereas IIRC the feature to work with '...' is exactly the
> > reason that pmax() and pmin() are so much slower.
> >
> > I've haven't checked if Gabor's
> >     pmax2.G <- function(x,y) {z <- x > y; z * (x-y) + y}
> > is even faster than the abs() using one.
> > It may have the advantage of giving *identical* results (to the
> > last bit!)  to pmax()  which my version does not --- IIRC the
> > only reason I did not follow my own 'FIXME' above.
> >
> > I  had then planned to implement pmin2() and pmax2() in C code,
trivially,
> > and and hence get identical (to the last bit!) behavior as
> > pmin()/pmax(); but I now tend to think that the proper approach is to
> > code pmin() and pmax() via .Internal() and hence C code ...
> >
> > [Not before DSC and my vacations though!!]
> >
> > Martin Maechler, ETH Zurich
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From tlumley at u.washington.edu  Fri Feb  9 17:13:54 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 9 Feb 2007 08:13:54 -0800 (PST)
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <40e66e0b0702090705u669db638yfa79c4c71e4ea343@mail.gmail.com>
Message-ID: <Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>


On 2/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>> The other reason why pmin/pmax are preferable to your functions is that
>> they are fully generic.  It is not easy to write C code which takes into
>> account that <, [, [<- and is.na are all generic.  That is not to say that
>> it is not worth having faster restricted alternatives, as indeed we do
>> with rep.int and seq.int.
>>
>> Anything that uses arithmetic is making strong assumptions about the
>> inputs.  It ought to be possible to write a fast C version that worked for
>> atomic vectors (logical, integer, real and character), but is there
>> any evidence of profiled real problems where speed is an issue?


I had an example just last month of an MCMC calculation where profiling showed that pmax(x,0) was taking about 30% of the total time.  I used

     function(x) {z <- x<0; x[z] <- 0; x}

which was significantly faster. I didn't try the arithmetic solution. Also, I didn't check if a solution like this would still be faster when both arguments are vectors (but there was a recent mailing list thread where someone else did).


      -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From Ted.Harding at manchester.ac.uk  Fri Feb  9 17:21:58 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Fri, 09 Feb 2007 16:21:58 -0000 (GMT)
Subject: [R] generate Binomial (not Binary) data
In-Reply-To: <45CC854D.5040805@stats.uwo.ca>
Message-ID: <XFMail.070209162158.Ted.Harding@manchester.ac.uk>

On 09-Feb-07 Duncan Murdoch wrote:
> On 2/7/2007 8:20 AM, Marc Bernard wrote:
>> Dear All,
>>    
>>   I am looking for an R function or any other reference to generate a
>>   series of correlated Binomial (not a Bernoulli) data. The "bindata"
>>   library can do this for the binary not the binomial case.
> 
> Ted asked how you want your series correlated.  Another question is how
> you want it "binomial":  do you want each value to be binomial with 
> parameters conditional on previous values, or do you want each to be 
> marginally binomial with some fixed parameters?
> 
> I can only think of two trivial solutions that meet both conditions 
> simultaneously:  the i.i.d. sequence, and a sequence that repeats 0 
> indefinitely.  I'd be interested in hearing if anyone knows of any 
> non-trivial examples of sequences where the distributions are both 
> conditionally and marginally binomial, or a proof that there are none.
> 
> Duncan Murdoch

I've been thinking a bot more about this issue, and (somewhat on
the same olines as Duncan above) have explicitly identified some
difficulties.

A.
Gregor Gorjanc, replying to Bernard, wrote:
>> The "bindata" library can do this for the binary not the
>> binomial case.
> 
> But binomial is just a sum of Bernoulli vars so this should
> be doable. 

Suppose, following Grigor's suggestion, we make up correlated binomial
variables as sums of correlated Bernoulli variables.

I.e. we have 2 binomial variabes:

  X = U1 + U2 + ... + Un
  Y = V1 + V2 + ... + Vn

where {U1,...,Un} are independent bernoullis (0/1) with P["1"] = p,
and similar for {V1,...,Vn}. Then X and Y are Binomial (n,p).

Suppose that cor(U1,V1,) = r, which depends uniquely on

  P[U1=1 & V1=1] = p11

say, and similarly for (U2,V2), (U3,V3), ...

Then X and Y will be correlated, and their correlation can be
calculated in terms of p11. So this is one model that can generate
correlated binomial variables.

However, while a bivariate normal (given means and variances)
is uniquely defined by the correlation coefficient, this is not
the case for a bivariate binomial:

B.
Let U be binomial(m,p), V binomial((n-m),p), W binomial((n-m),p)
and all three independent.

Then

  X = U + V is binomial(n,p)
  Y = U + W is binomial(n,p)

and X,Y are correlated, with correlation coefficient (if I've
done it right) m/(m+n). So, while in general you cannot choose
m (outof n) to get an exact correlation coefficient c, this is
a different kind of model to generate correlated binomial variables.

There will be many others (some easier to think up then others).

C. The above concern only correlation between two binomial
variables (X,Y). Bernard clarified his original query in a private
email to me:

  "Let i indexes  a subject and Y_i = (Y_i1, Y_i2,...,Y_iT)
   be a vector of binomial variables for subject i  such that
   Y_it ~ Bin(n,p_t) with t = 1,2, ....T.
   A simple correlation I would like to have is :
   corr(Y_ij, Y_ik) = c for all (j,k)"

(I think the index i is irrelevant to the present query).

This could be done on the lines of case (B) above with

  Y_i1 = U + V1
  Y_i2 = U + V2
  ....
  Y_iT = U + VT

provided the desired value c is adequately approximated by m/(m+n).

Alternatively, you can start off from a situation inspired by (A):

Let W = U + V | U+V <=1, i.e. X is the sum of two independent
bernoullis conditional on excluding the case where both are 1.

Then W is bernoulli with

  P[W=0] = q^12/(q^2 + 2*p*q), P[W=1] = 2*p*q/(q^2 + 2*p*q)

where p = P[U=1] = P[V=1] and q = (1-p). Let P[W=1] = p11.

Then create binomial(n,p11) variables Y_i1, Y_i2, ... , Y_iT by

  Y_i1 = (U1 + V11) + (U2 + V12) + ... + (Un + V1n)
  Y_i2 = (U1 + V21) + (U2 + V22) + ... + (Un + V2n)
  ....
  Y_iT = (U1 + VT1) + (U2 + VT2) + ... + (Un + VTn)

where each (Ui + Vij) is conditional on U+V <= 1 as in (B)..

Then we have pairwise equi-correlated variables Y_i1,...,Y_iT.

Although the two methods described above (i.e. in (C)) give
pairwise equicorrelated binomial variables, I suspect that
their higher-order moments, and indeed the joint distribution
of (Y_ir,Y_is), will not be the same in the two methods.

So, as I see it, there is not a unique answet to the question.
[Primitive versions of the above led me to ask the question

 How do you want your series of binomial datato be "correlated"?]

Anyone wiser than I am?

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-Feb-07                                       Time: 16:13:14
------------------------------ XFMail ------------------------------


From marcos at icesi.edu.co  Fri Feb  9 17:40:27 2007
From: marcos at icesi.edu.co (Mauricio Arcos)
Date: Fri, 09 Feb 2007 11:40:27 -0500
Subject: [R] fCalendar Q/.
Message-ID: <45CCA3FB.906@icesi.edu.co>

Dear List,

i got stuck on this...

I've a very big data base with tic by tic $COP/US$ exchange rate and trade volume in millions (obs =3 millions). I?ve been trying around for some time, but I could not find a  efficient way  to calculate a weighted mean exchange rate using blocks by 5 mins.

Currently, I'm using R 2.4.0 and the fCalendar package, There is another package to analyze big time series?

Thanks a lot
Mauricio


From kubovy at virginia.edu  Fri Feb  9 17:39:24 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 9 Feb 2007 11:39:24 -0500
Subject: [R] Replace individual values in a data frame with NA
In-Reply-To: <4B6A3858-0F3F-49BB-8EEA-8B81D5389B0F@bu.edu>
References: <4B6A3858-0F3F-49BB-8EEA-8B81D5389B0F@bu.edu>
Message-ID: <D232A9AF-B9CE-4031-9AEF-A498744258C4@virginia.edu>

On Feb 9, 2007, at 11:10 AM, Jason Horn wrote:

> I'd like to replace a value in a data frame with an NA, but can't
> figure out how.
>
> For example, say you have
>
> a<-c(1,2,3,4)
> b<-c(5,6,7,8)
> data<-data.frame(a,b)
>
> Now, how would you set the third row of the second column ( data
> [[3,2]] ) to NA?

a<-c(1,2,3,4)
b<-c(5,6,7,8)
data<-data.frame(a,b)
data[3, 2] <- NA

?Extract
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From wwwhsd at gmail.com  Fri Feb  9 17:35:35 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 9 Feb 2007 14:35:35 -0200
Subject: [R] Replace individual values in a data frame with NA
In-Reply-To: <4B6A3858-0F3F-49BB-8EEA-8B81D5389B0F@bu.edu>
References: <4B6A3858-0F3F-49BB-8EEA-8B81D5389B0F@bu.edu>
Message-ID: <da79af330702090835r169cf50bnf59b580b8a5342f6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/fd56ae76/attachment.pl 

From pburns at pburns.seanet.com  Fri Feb  9 17:46:22 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 09 Feb 2007 16:46:22 +0000
Subject: [R] problem with eigen() function
In-Reply-To: <127186.10085.qm@web56814.mail.re3.yahoo.com>
References: <127186.10085.qm@web56814.mail.re3.yahoo.com>
Message-ID: <45CCA55E.9090103@pburns.seanet.com>

This problem reminds me of the problem described in:

tolstoy.newcastle.edu.au/*R*/devel/03b/1304.html

In that case a specific matrix causes 'eigen' to return
NaNs in the eigen vectors.  But dumping the matrix
and reading it back in does not reproduce the problem.
That you are getting a hang rather than merely garbage
returned is considerably more annoying I would imagine.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jingqin luo wrote:

>Dear R-users,
>   Recently, I have come across a weird problem. I run a large number of iterations and at one of the step within each iteration, I calculate the eigen values of a updated covariance matrix. From all my intermediate output, the code freezes after printing out the covariance matrix but before printing out the eigen values. So, obviously it stops at the only step, the eigen() function calculation. However, it didn't give any error message from this eigen function. Also, when I copy the printed out covariance matrix over and calculate its eigen function, there is no problem of giving the answer. So, nothing is wrong with this covariance matrix either. I suspect that probably during these iteration, I have returned too many objects or R had sth wrong with the memory allocation or ...... I have no idea how to debug it so far since I can't go into the eigen function step by step each time is run. If someone has any similar experience or good advice on debugging this problem, I
> would really appreciate it. Thanks!
>  Jingqin
>
> 
>---------------------------------
>It's here! Your new message!
>Get new email alerts with the free Yahoo! Toolbar.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From mothsailor at googlemail.com  Fri Feb  9 17:47:08 2007
From: mothsailor at googlemail.com (David Barron)
Date: Fri, 9 Feb 2007 16:47:08 +0000
Subject: [R] extract submatrix with unique names
In-Reply-To: <4E6D6ECA03F7CF48AC313DBD3BA5B2579E632D@e2k3ms4.urmc-sh.rochester.edu>
References: <4E6D6ECA03F7CF48AC313DBD3BA5B2579E632D@e2k3ms4.urmc-sh.rochester.edu>
Message-ID: <815b70590702090847r124c304ej9d560426509ae356@mail.gmail.com>

This should do it, assuming you data is in a data frame called dat:

sel <- unique(dat$V1)
ix <- match(sel,dat$V1)
dat[ix,]


On 09/02/07, Glazko, Galina <Galina_Glazko at urmc.rochester.edu> wrote:
> Dear list,
>
>
>
> I have a table where first 3 columns are identical if the name in the
> first column is the same, and
>
> the number in N4 is slightly different for all identical names, like
> this:
>
> -------------------------------------------------------------------
>
> 29        Mm.1_at   3          +  93649936
>
> 30        Mm.1_at   3          +  93649990
>
> 31        Mm.1_at   3          +  93649993
>
> 32        Mm.1_at   3          +  93650001
>
> 33        Mm.1_at   3          +  93650010
>
> 34       Mm.10_at   4          + 147438101
>
> 35       Mm.10_at   4          + 147438192
>
> 36       Mm.10_at   4          + 147438214
>
> 37   Mm.100043_at   5          +  31602952
>
> 38   Mm.100043_at   5          +  31602982
>
> 39   Mm.100043_at   5          +  31603000
>
>  -------------------------------------------------------------------
>
>
>
> I need to select unique identifiers in column N 1, without paying
> attention to what is in column N4, taking the first number in N4
> corresponding to first unique ID in N1, like this:
>
> 29        Mm.1_at   3          +  93649936
>
> 34       Mm.10_at   4          + 147438101
>
> 37   Mm.100043_at   5          +  31602952
>
>
>
> Could someone tell me how to do it without FOR cycle over all IDs in N1?
>
>
>
> Thank you!
>
>
>
> Best regards
>
> Galina
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mark.lyman at gmail.com  Fri Feb  9 17:49:11 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Fri, 9 Feb 2007 16:49:11 +0000 (UTC)
Subject: [R] LM Model
References: <E1HFPMy-00018e-00@smtp07.web.de>
Message-ID: <loom.20070209T174251-727@post.gmane.org>

Simon P. Kempf <simon.kempf <at> web.de> writes:

[SNIP]
> 
> But I want to skip the lm function and specify my own regression equation
> RENT= 15 -0.15*AGE1 and then use the predict.lm function. However, in order
> to use the predict.lm function I need an object of class lm. Is there any
> way to do so? Or maybe somebody has another solution? 
> 
> Thanks in advance,
> 
> Simon

Here is one way. Take a look at help(offset), help(lm), and help(lm.predict).
> xx <- runif(30)
> yy <- rnorm(30)
> mydata<-data.frame(xx,yy)
> lm(yy~offset(15*rep(1,30))+offset(-0.15*xx)-1,mydata)


Mark Lyman


From pburns at pburns.seanet.com  Fri Feb  9 17:57:18 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 09 Feb 2007 16:57:18 +0000
Subject: [R] problem with eigen() function
In-Reply-To: <45CCA55E.9090103@pburns.seanet.com>
References: <127186.10085.qm@web56814.mail.re3.yahoo.com>
	<45CCA55E.9090103@pburns.seanet.com>
Message-ID: <45CCA7EE.7050202@pburns.seanet.com>

The original address has extraneous asterisks (*) around
the R, here is one that works (for me at least):

tolstoy.newcastle.edu.au/R/devel/03b/1304.html

Patrick Burns wrote:

>This problem reminds me of the problem described in:
>
>tolstoy.newcastle.edu.au/*R*/devel/03b/1304.html
>
>In that case a specific matrix causes 'eigen' to return
>NaNs in the eigen vectors.  But dumping the matrix
>and reading it back in does not reproduce the problem.
>That you are getting a hang rather than merely garbage
>returned is considerably more annoying I would imagine.
>
>
>Patrick Burns
>patrick at burns-stat.com
>+44 (0)20 8525 0696
>http://www.burns-stat.com
>(home of S Poetry and "A Guide for the Unwilling S User")
>
>Jingqin luo wrote:
>
>  
>
>>Dear R-users,
>>  Recently, I have come across a weird problem. I run a large number of iterations and at one of the step within each iteration, I calculate the eigen values of a updated covariance matrix. From all my intermediate output, the code freezes after printing out the covariance matrix but before printing out the eigen values. So, obviously it stops at the only step, the eigen() function calculation. However, it didn't give any error message from this eigen function. Also, when I copy the printed out covariance matrix over and calculate its eigen function, there is no problem of giving the answer. So, nothing is wrong with this covariance matrix either. I suspect that probably during these iteration, I have returned too many objects or R had sth wrong with the memory allocation or ...... I have no idea how to debug it so far since I can't go into the eigen function step by step each time is run. If someone has any similar experience or good advice on debugging this problem, I
>>would really appreciate it. Thanks!
>> Jingqin
>>
>>
>>---------------------------------
>>It's here! Your new message!
>>Get new email alerts with the free Yahoo! Toolbar.
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> 
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From bouscaut at scsv.ups-tlse.fr  Fri Feb  9 14:18:13 2007
From: bouscaut at scsv.ups-tlse.fr (=?iso-8859-1?Q?J=E9r=F4me_Bouscaut?=)
Date: Fri, 9 Feb 2007 14:18:13 +0100
Subject: [R] inquiry
Message-ID: <014001c74c4c$c06fb400$9537c7c2@umrbouscaut>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/e0c3da35/attachment.pl 

From grahamleask at btopenworld.com  Fri Feb  9 14:31:38 2007
From: grahamleask at btopenworld.com (GRAHAM LEASK)
Date: Fri, 9 Feb 2007 13:31:38 +0000 (GMT)
Subject: [R] RE gap statistic in cluster analysis
Message-ID: <888328.52667.qm@web86210.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/e5a8a12a/attachment.pl 

From jeremy.miles at gmail.com  Fri Feb  9 18:15:07 2007
From: jeremy.miles at gmail.com (Jeremy Miles)
Date: Fri, 9 Feb 2007 09:15:07 -0800
Subject: [R] R in Industry
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
References: <B3E803F92F909741B050C9FA4DDEDE7543A0F4@naimucog.allianzde.rootdom.net>
	<45CB4E87.4040605@pburns.seanet.com>
	<CA612484A337C6479EA341DF9EEE14AC05E2D88D@hercules.ssainfo>
Message-ID: <7b53245b0702090915q52f63366ma35b239fdd0bdbf9@mail.gmail.com>

I was under the impression that most software has a licence agreement
that does not allow you to sue them.  If Windows crashes at a crucial
moment, and loses me millions of dollars [I can't imagine that
happening to me, but it might happen to someone], I don't think I can
sue microsoft.

A few years ago, there was a recalculation bug in Excel.  This caused
a lot of people's financial planning to get messed up (I heard, and
ironically, including SPSS Inc).

There's also the issue of being abandoned - as users of BMDP were,
when SPSS bought them.

Jeremy



On 08/02/07, Ben Fairbank <BEN at ssanet.com> wrote:
> To those following this thRead:
>
> There was a thread on this topic a year or so ago on this list, in which
> contributors mentioned reasons that corporate powers-that-be were
> reluctant to commit to R as a corporate statistical platform.  (My
> favorite was "There is no one to sue if something goes wrong.")
>

[snip]





-- 

Jeremy Miles
www.jeremymiles.co.uk


From Serguei.Kaniovski at wifo.ac.at  Fri Feb  9 18:19:45 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Fri, 9 Feb 2007 18:19:45 +0100
Subject: [R] Error handling with try function
Message-ID: <OF15C1D881.A2554E23-ONC125727D.005F316A-C125727D.005F3171@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/3799398d/attachment.pl 

From ccleland at optonline.net  Fri Feb  9 18:23:58 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 09 Feb 2007 12:23:58 -0500
Subject: [R] Help in using multcomp.
In-Reply-To: <777449.83236.qm@web32412.mail.mud.yahoo.com>
References: <777449.83236.qm@web32412.mail.mud.yahoo.com>
Message-ID: <45CCAE2E.5000905@optonline.net>

A Ezhil wrote:
> Hi All,
> 
> I am trying use 'multcomp' for multiple comparisons
> after my ANOVA analysis. I have used the following
> code to do ANOVA:
> 
> dat <- matrix(rnorm(45), nrow=5, ncol=9)
> f <- gl(3,3,9, label=c("C", "Tl", "T2"))
> 
> aof <- function(x) {
>         m <- data.frame(f, x);
>         aov(x ~ f, m)
> }
> amod <- apply(dat,1,aof)
> 
> Now, how can I use 'glht' for the above amod. I know
> that I cannot use simply 
> 
> glht(amod, linfct = mcp(f = "Dunnett")). 

  Since amod is a list of models rather than one model, do you want
something like this?

lapply(amod, function(x){summary(glht(x, linfct = mcp(f = "Dunnett")))})

> Also, if I want to use Dunnett for comparing C vs (T1
> and T2), how can I specify this in the glht function.

  How about doing it with user-defined contrasts?

contr <- rbind("C - T1   " = c(-1, 1, 0),
               "C - T2   " = c(-1, 0, 1),
               "C - All T" = c(-1,.5,.5))

lapply(amod, function(x){summary(glht(x, linfct = mcp(f = contr)))})

> Thanks in advance. 
> Regards,
> Ezhil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Matthias.Kohl at stamats.de  Fri Feb  9 18:31:54 2007
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Fri, 09 Feb 2007 18:31:54 +0100
Subject: [R] RE gap statistic in cluster analysis
In-Reply-To: <888328.52667.qm@web86210.mail.ird.yahoo.com>
References: <888328.52667.qm@web86210.mail.ird.yahoo.com>
Message-ID: <45CCB00A.1040805@stamats.de>

there is an implementation in package SLmisc and also in the 
bioconductor package SAGx.

hth
Matthias


GRAHAM LEASK schrieb:
> Has anyone implemented Tibrishani's gap statistic in R or S plus? If so I would greatly appreciate the relevant script file.
>    
>   Any help will be much appreciated
>
>
> Kind regards
>
>
> Dr Graham Leask
> Economics and Strategy Group
> Aston Business School
> Aston University
> Aston Triangle
> Birmingham
> B4 7ET
>
> Tel: Direct line 0121 204 3150
> email g.leask at aston.ac.uk
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Dr. rer. nat. Matthias Kohl
E-Mail: matthias.kohl at stamats.de
Home: www.stamats.de


From ccleland at optonline.net  Fri Feb  9 18:34:09 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 09 Feb 2007 12:34:09 -0500
Subject: [R] RE gap statistic in cluster analysis
In-Reply-To: <888328.52667.qm@web86210.mail.ird.yahoo.com>
References: <888328.52667.qm@web86210.mail.ird.yahoo.com>
Message-ID: <45CCB091.4020007@optonline.net>

GRAHAM LEASK wrote:
> Has anyone implemented Tibrishani's gap statistic in R or S plus? If so I would greatly appreciate the relevant script file.
>    
>   Any help will be much appreciated

RSiteSearch("gap cluster") finds

http://finzi.psych.upenn.edu/R/library/SAGx/html/gap.html

> Kind regards
> 
> 
> Dr Graham Leask
> Economics and Strategy Group
> Aston Business School
> Aston University
> Aston Triangle
> Birmingham
> B4 7ET
> 
> Tel: Direct line 0121 204 3150
> email g.leask at aston.ac.uk
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From f.harrell at vanderbilt.edu  Fri Feb  9 18:35:42 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 09 Feb 2007 11:35:42 -0600
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>
References: <Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>
Message-ID: <45CCB0EE.90907@vanderbilt.edu>

Thomas Lumley wrote:
> On 2/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>> The other reason why pmin/pmax are preferable to your functions is that
>>> they are fully generic.  It is not easy to write C code which takes into
>>> account that <, [, [<- and is.na are all generic.  That is not to say that
>>> it is not worth having faster restricted alternatives, as indeed we do
>>> with rep.int and seq.int.
>>>
>>> Anything that uses arithmetic is making strong assumptions about the
>>> inputs.  It ought to be possible to write a fast C version that worked for
>>> atomic vectors (logical, integer, real and character), but is there
>>> any evidence of profiled real problems where speed is an issue?
> 
> 
> I had an example just last month of an MCMC calculation where profiling showed that pmax(x,0) was taking about 30% of the total time.  I used
> 
>      function(x) {z <- x<0; x[z] <- 0; x}
> 
> which was significantly faster. I didn't try the arithmetic solution. Also, I didn't check if a solution like this would still be faster when both arguments are vectors (but there was a recent mailing list thread where someone else did).
> 
> 
>       -thomas

I looked in all the code for the Hmisc and Design packages and didn't 
find a single example where pmin or pmax did not have 2 arguments.  So I 
think it is important to have pmin2 and pmax2.

Frank
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From mark.lyman at gmail.com  Fri Feb  9 18:46:39 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Fri, 9 Feb 2007 17:46:39 +0000 (UTC)
Subject: [R]
	=?utf-8?q?Using_variable_names_in_for_loops_-_Generating_plot?=
	=?utf-8?q?s=09semi-automatically_from_a_series_of_variables_Partly?=
	=?utf-8?q?_solved?=
References: <6d3975af0702090741j5400dd8bj59758b8515a478c9@mail.gmail.com>
Message-ID: <loom.20070209T184209-571@post.gmane.org>

Anthony Staines <anthony.staines <at> gmail.com> writes:

For each
> disease, I want to pull out the column of data containing the word
> 'Male' and plot this against age, and then add a line to the plot for
> the corresponding column containing 'Female'.
> --
> attach(data)
> 
> Diseases <- c("Cardiovascular disease","Road Traffic Injury",  ...
> ,"All causes")
> Male <- names(data)[grep("Male",names(data))]
> Female <- names(data)[grep("Female",names(data))]
> #Disease contains disease labels in the correct order, and Male and
> Female now hold the (correct) variables.
> 
> for (i in seq(1,length(Diseases)))
>   {
> jpeg(paste(Diseases[i],".jpg")) #This works fine!
> plot(Male[i]~Age)                  #This does not
> lines(Female[i]~Age)
> dev.off()
> }
> detach(data)

[SNIP]

There are a few ways you can do this. Using plot.formula like you do here you 
can use this:

> mydata <- as.data.frame(matrix(runif(300),30,10))
> attach(mydata)
> plot(formula(paste(mynames[1],mynames[2],sep='~')))

A way to do this without a formula would be:
> plot(eval(parse(text=mynames[1])),eval(parse(text=mynames[2])))
> detach(mydata)

Take a look at the help files for eval and parse. I still do not have a firm 
grasp on how to use them and other related functions, like substitute, but 
what I have been able figure out has been very useful.

Mark Lyman


From maechler at stat.math.ethz.ch  Fri Feb  9 19:10:43 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Feb 2007 19:10:43 +0100
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>
References: <40e66e0b0702090705u669db638yfa79c4c71e4ea343@mail.gmail.com>
	<Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>
Message-ID: <17868.47395.618746.286144@stat.math.ethz.ch>

>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>     on Fri, 9 Feb 2007 08:13:54 -0800 (PST) writes:

    TL> On 2/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
    >>> The other reason why pmin/pmax are preferable to your functions is that
    >>> they are fully generic.  It is not easy to write C code which takes into
    >>> account that <, [, [<- and is.na are all generic.  That is not to say that
    >>> it is not worth having faster restricted alternatives, as indeed we do
    >>> with rep.int and seq.int.
    >>> 
    >>> Anything that uses arithmetic is making strong assumptions about the
    >>> inputs.  It ought to be possible to write a fast C version that worked for
    >>> atomic vectors (logical, integer, real and character), but is there
    >>> any evidence of profiled real problems where speed is an issue?


    TL> I had an example just last month of an MCMC calculation where profiling showed that pmax(x,0) was taking about 30% of the total time.  I used

    TL> function(x) {z <- x<0; x[z] <- 0; x}

    TL> which was significantly faster. I didn't try the
    TL> arithmetic solution. 

I did - eons ago as mentioned in my message earlier in this
thread. I can assure you that those (also mentioned)

  pmin2 <- function(k,x) (x+k - abs(x-k))/2
  pmax2 <- function(k,x) (x+k + abs(x-k))/2

are faster still, particularly if you hardcode the special case of k=0!
{that's how I came about these:  pmax(x,0) is also denoted  x_+, and
	x_+ := (x + |x|)/2
	x_- := (x - |x|)/2
}

    TL> Also, I didn't check if a solution like this would still
    TL> be faster when both arguments are vectors (but there was
    TL> a recent mailing list thread where someone else did).

indeed, and they are faster.
Martin


From Greg.Snow at intermountainmail.org  Fri Feb  9 19:11:24 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 9 Feb 2007 11:11:24 -0700
Subject: [R] two perspective plots in in plot
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7FCB73@LP-EXCHVS07.CO.IHC.COM>

Probably the easiest way is to use the "wireframe" function in the
lattice package.  The second example in the help shows 2 surfaces (you
do need to combine the data into a single data frame).

If you really want to use the "persp" function, then you could create
the first plot, then call "par(new=TRUE)" and then do the 2nd plot, but
that would take a lot of thinking to get the axes and scales to line up
properly and make it look good.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roland Rau
> Sent: Friday, February 09, 2007 9:01 AM
> To: r-help
> Subject: [R] two perspective plots in in plot
> 
> Dear all,
> 
> I would like to put two perspective plots into one plot. The 
> help page for ?persp shows how one can add points and lines 
> but not another perspective plot.
> 
> data(volcano)
> z <- 2 * volcano        # Exaggerate the relief
> x <- 10 * (1:nrow(z))   # 10 meter spacing (S to N)
> y <- 10 * (1:ncol(z))   # 10 meter spacing (E to W)
> ## Don't draw the grid lines :  border = NA persp(x, y, z, 
> theta = 135, phi = 30, col = "green3", scale = FALSE,
>      ltheta = -120, shade = 0.75, border = NA, box = FALSE)
> 
> and now I would like to include another surface. I was hoping 
> for a possibility like add=TRUE such as in contour:
> 
> persp(x, y, z+10, theta = 135, phi = 30, col = "red", add=TRUE)
> 
> Can someone point out to me how it can be accomplished (maybe 
> a function in another package)?
> 
> Thank you very much,
> Roland
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Mauro.Rossi at irpi.cnr.it  Fri Feb  9 17:12:09 2007
From: Mauro.Rossi at irpi.cnr.it (Mauro Rossi)
Date: Fri, 09 Feb 2007 17:12:09 +0100
Subject: [R] Zeta and Zipf distribution
In-Reply-To: <815b70590702080903vf629811kd8c8331e42e26579@mail.gmail.com>
References: <45CB017F.8070909@irpi.cnr.it>
	<815b70590702080314t1c5580d4wc44305bc4c0b11f2@mail.gmail.com>
	<45CB480D.8050109@irpi.cnr.it>
	<815b70590702080903vf629811kd8c8331e42e26579@mail.gmail.com>
Message-ID: <45CC9D59.4060806@irpi.cnr.it>

Thank you David,
	I've followed you example and I found the parameters I need. I also 
write a script using zetaff distribution and also this works well.

Thank you again,

Mauro

David Barron ha scritto:
> I don't claim to be a huge expert on this, but I think you are mistaken
> about what you are getting when you use the zipf family with the vglm
> function.  From what I can tell from the documentation, this does indeed
> give you an estimate of the parameter of the zipf distribution.  I've tried
> to "test" this using some random numbers (probably not strictly correct
> procedure, but I think it's a reasonable approximation):
> 
> set.seed(1234)
>  N <- 5
>   y <- (1:N)
>   alpha <- 2.5  # this is the parameter of the zipf distribution
>   p <- 1/(y^alpha) ; p <- p/sum(p)
>   n <- 100000
>   x <- sample (y, n, replace=TRUE, prob=p)
> w <- as.vector(table(x))
> fit = vglm  (y ~ 1, zipf(link=identity, init=2), tra=TRUE, weight=w)
>> Coef(fit)
> 
>        s
> 2.501086
> 
> Is this not what you need?
> 
> On 08/02/07, Mauro Rossi <Mauro.Rossi a irpi.cnr.it> wrote:
>> Dear David,
>>         thank you for your reply.
>> I tried to use the package VGAM, the function "zipf" and also the
>> function "zetaff", but these functions don't allow me to estimate
>> parameters directly, I have to use a Gerneralized Linear Model or a
>> Generalized Additive Model (vgam or vglm functions) and I don't want to
>> use those. Don't you know a way to apply these tools to my data?
>> At the end my PMF has to be Y=f(X) where f(X) is a zeta or a zipf
>> distribution, while using VGAM the PMF is Y = b0 + b1*f(X1)+ ...
>> +bn*f(Xn). Do you know how I can write the script using the VGAM
>> function for the PMF I need?
>>
>> Thank you in advance,
>>
>> Mauro Rossi
>>
>>
>> David Barron ha scritto:
>>> Does the zipf function in the VGAM package do what you want?
>>>
>>> On 08/02/07, *Mauro Rossi* <Mauro.Rossi a irpi.cnr.it
>>> <mailto:Mauro.Rossi a irpi.cnr.it>> wrote:
>>>
>>>     Dear R user,
>>>     I want to estimate the parameter of ZETA or/and ZIPF distributions
>>>     using R, given a series of integer values. Do you know a package
>>>     (similar to MASS) or a function (similar to fitdistr) I can use to
>>>     estimate the parameter of these distributions using MLE method?
>>>     Otherwise do you know a function (which use MLE method to estimate
>>>     distribution parameters) that allow me to specify a PDF or PMF?
>>>     Thanks,
>>>     Regards
>>>     Mauro Rossi
>>>
>>>     --
>>>     Mauro Rossi
>>>     Istituto di Ricerca per la Protezione Idrogeologica
>>>     Consiglio Nazionale delle Ricerche
>>>     Via della Madonna Alta, 126
>>>     06128 Perugia
>>>     Italia
>>>     Tel. +39 075 5014421
>>>     Fax +39 075 5014420
>>>
>>>     ______________________________________________
>>>     R-help a stat.math.ethz.ch <mailto:R-help a stat.math.ethz.ch> mailing
>> list
>>>     https://stat.ethz.ch/mailman/listinfo/r-help
>>>     PLEASE do read the posting guide
>>>     http://www.R-project.org/posting-guide.html
>>>     <http://www.R-project.org/posting-guide.html>
>>>     and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>>
>>> --
>>> =================================
>>> David Barron
>>> Said Business School
>>> University of Oxford
>>> Park End Street
>>> Oxford OX1 1HP
>> --
>> Mauro Rossi
>>
>> Istituto di Ricerca per la Protezione Idrogeologica
>>
>> Consiglio Nazionale delle Ricerche
>>
>> Via della Madonna Alta, 126
>>
>> 06128 Perugia
>>
>> Italia
>>
>> Tel. +39 075 5014421
>>
>> Fax +39 075 5014420
>>
>>
>>
> 
> 

-- 
Mauro Rossi

Istituto di Ricerca per la Protezione Idrogeologica

Consiglio Nazionale delle Ricerche

Via della Madonna Alta, 126

06128 Perugia

Italia

Tel. +39 075 5014421

Fax +39 075 5014420


From ripley at stats.ox.ac.uk  Fri Feb  9 19:16:14 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Feb 2007 18:16:14 +0000 (GMT)
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <45CCB0EE.90907@vanderbilt.edu>
References: <Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>
	<45CCB0EE.90907@vanderbilt.edu>
Message-ID: <Pine.LNX.4.64.0702091812520.6831@gannet.stats.ox.ac.uk>

On Fri, 9 Feb 2007, Frank E Harrell Jr wrote:

[...]

> I looked in all the code for the Hmisc and Design packages and didn't find a 
> single example where pmin or pmax did not have 2 arguments.  So I think it is 
> important to have pmin2 and pmax2.

Why?  Do you have any reason to suppose that these will be significantly 
faster than the general case of 1 or more arguments?  (The current code 
fails obscurely for 0 args.)

What I am playing with are fast internal pmin.int and pmax.int for the 
case of all atomic vectors and no classes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jacinthe at gmx.de  Fri Feb  9 19:26:12 2007
From: jacinthe at gmx.de (jacinthe at gmx.de)
Date: Fri, 09 Feb 2007 19:26:12 +0100
Subject: [R] TA-Lib and R
Message-ID: <20070209182612.139370@gmx.net>

Dear all,

does someone know how to use TA-Lib in R (http://ta-lib.org/)?

Regards
Jaci
-- 
"Feel free" - 5 GB Mailbox, 50 FreeSMS/Monat ...
Jetzt GMX ProMail testen: www.gmx.net/de/go/mailfooter/promail-out


From RMan54 at cox.net  Fri Feb  9 19:31:23 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Fri, 9 Feb 2007 10:31:23 -0800
Subject: [R] get.hist.quote problem yahoo
In-Reply-To: <JD76H5$264232457CE289BCC2F55186A93BE37E@libero.it>
References: <JD76H5$264232457CE289BCC2F55186A93BE37E@libero.it>
Message-ID: <01a301c74c78$806b7160$0900a8c0@rman>

I had the same problem some time ago. Below is a function that I picked up
on the web somewhere (can't remember where; may have been a newsletter).
It's based on the tseries function but the difference is that this function
produces a data frame with a column containing the dates of the quotes,
instead of a time series object. I had to replace "%d-%b-%y" by "%Y-%m-%d"
to make it work, probably as you stated because the format was changed by
Yahoo.

Hope this helps.

Rene

# ----------------------------------------------------------------------
# "df.get.hist.quote()" function
#
# Based on code by A. Trapletti (package tseries)
#
# The main difference is that this function produces a data frame with
# a column containing the dates of the quotes, instead of a time series
# object.
df.get.hist.quote <- function (instrument = "ibm",
                               start, end,
                               quote = c("Open","High", "Low",
"Close","Volume"),
                               provider = "yahoo", method = "auto") 
{
    if (missing(start)) 
        start <- "1970-01-02"
    if (missing(end)) 
        end <- format(Sys.time() - 86400, "%Y-%m-%d")
    provider <- match.arg(provider)
    start <- as.POSIXct(start, tz = "GMT")
    end <- as.POSIXct(end, tz = "GMT")
    if (provider == "yahoo") {
        url <- paste("http://chart.yahoo.com/table.csv?s=", instrument, 
            format(start, "&a=%m&b=%d&c=%Y"), format(end,
"&d=%m&e=%d&f=%Y"), 
            "&g=d&q=q&y=0&z=", instrument, "&x=.csv", sep = "")
        destfile <- tempfile()
        status <- download.file(url, destfile, method = method)
        if (status != 0) {
            unlink(destfile)
            stop(paste("download error, status", status))
        }
        status <- scan(destfile, "", n = 1, sep = "\n", quiet = TRUE)
        if (substring(status, 1, 2) == "No") {
            unlink(destfile)
            stop(paste("No data available for", instrument))
        }
        x <- read.table(destfile, header = TRUE, sep = ",")
        unlink(destfile)
        nser <- pmatch(quote, names(x))
        if (any(is.na(nser))) 
            stop("This quote is not available")
        n <- nrow(x)
        lct <- Sys.getlocale("LC_TIME")
        Sys.setlocale("LC_TIME", "C")
        on.exit(Sys.setlocale("LC_TIME", lct))
        dat <- gsub(" ", "0", as.character(x[, 1]))
        dat <- as.POSIXct(strptime(dat, "%Y-%m-%d"), tz = "GMT")        
        if (dat[n] != start) 
            cat(format(dat[n], "time series starts %Y-%m-%d\n"))
        if (dat[1] != end) 
            cat(format(dat[1], "time series ends   %Y-%m-%d\n"))
 
return(data.frame(cbind(Date=I(format(dat[n:1],"%Y-%m-%d")),x[n:1,nser]),row
.names=1:n))
      }
    else stop("Provider not implemented")
} 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Daniele Amberti
Sent: Friday, February 09, 2007 5:22 AM
To: r-help
Subject: [R] get.hist.quote problem yahoo

I have functions using get.hist.quote() from library tseries.

It seems that something changed (yahoo) and function get broken.

try with a simple

get.hist.quote('IBM')

and let me kow if for someone it is still working.

I get this error:
Error in if (!quiet && dat[n] != start) cat(format(dat[n], "time series
starts %Y-%m-%d\n")) : 
        missing value where TRUE/FALSE needed

Looking at the code it seems that before the format of dates in yahoo's cv
file was not iso.
Now it is iso standard year-month-day

Anyone get the same problem?


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada9feb07

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From azzalini at stat.unipd.it  Fri Feb  9 19:33:03 2007
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Fri, 9 Feb 2007 19:33:03 +0100
Subject: [R] generate Binomial (not Binary) data
In-Reply-To: <XFMail.070209162158.Ted.Harding@manchester.ac.uk>
References: <45CC854D.5040805@stats.uwo.ca>
	<XFMail.070209162158.Ted.Harding@manchester.ac.uk>
Message-ID: <20070209193303.7e5ad224.azzalini@stat.unipd.it>

On Fri, 09 Feb 2007 16:21:58 -0000 (GMT), (Ted Harding) wrote:

> 
>   "Let i indexes  a subject and Y_i = (Y_i1, Y_i2,...,Y_iT)
>    be a vector of binomial variables for subject i  such that
>    Y_it ~ Bin(n,p_t) with t = 1,2, ....T.
>    A simple correlation I would like to have is :
>    corr(Y_ij, Y_ik) = c for all (j,k)"

I have only seen only a portion of this discussion, so I hope that
the following is not unrelated to the question.

Would it be Ok to generate a sequence p_1,...,p_t, ...p_T
from a Beta stationary process (which exists, actual more
than one sort), and then sample from Bin(n,p_t)?

Alternatively, some 20 years ago, there was a stream of literature 
for discrete sttionary processes, based on the idea of "thinning"
a discrete variable. There are for sure stationary processes
with Negative binomial distribution of this sort. I have
not followed that literature, but it is conceivable that
the similar process for the binomial case has been considered.

Best wishes,

Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini a stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/


From ripley at stats.ox.ac.uk  Fri Feb  9 19:33:15 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Feb 2007 18:33:15 +0000 (GMT)
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <17868.47395.618746.286144@stat.math.ethz.ch>
References: <40e66e0b0702090705u669db638yfa79c4c71e4ea343@mail.gmail.com>
	<Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>
	<17868.47395.618746.286144@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0702091827150.6954@gannet.stats.ox.ac.uk>

> x <- rnorm(10000)
> system.time(for(i in 1:1000) pmax(x, 0))
    user  system elapsed
    4.43    0.05    4.54
> pmax2 <- function(k,x) (x+k + abs(x-k))/2
> system.time(for(i in 1:1000) pmax2(x, 0))
    user  system elapsed
    0.64    0.03    0.67
> pm <- function(x) {z <- x<0; x[z] <- 0; x}
> system.time(for(i in 1:1000) pm(x))
    user  system elapsed
    0.59    0.00    0.59
> system.time(for(i in 1:1000) pmax.int(x, 0))
    user  system elapsed
    0.36    0.00    0.36

So at least on one system Thomas' solution is a little faster, but a 
C-level n-args solution handling NAs is quite a lot faster.

On Fri, 9 Feb 2007, Martin Maechler wrote:

>>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>>     on Fri, 9 Feb 2007 08:13:54 -0800 (PST) writes:
>
>    TL> On 2/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>    >>> The other reason why pmin/pmax are preferable to your functions is that
>    >>> they are fully generic.  It is not easy to write C code which takes into
>    >>> account that <, [, [<- and is.na are all generic.  That is not to say that
>    >>> it is not worth having faster restricted alternatives, as indeed we do
>    >>> with rep.int and seq.int.
>    >>>
>    >>> Anything that uses arithmetic is making strong assumptions about the
>    >>> inputs.  It ought to be possible to write a fast C version that worked for
>    >>> atomic vectors (logical, integer, real and character), but is there
>    >>> any evidence of profiled real problems where speed is an issue?
>
>
>    TL> I had an example just last month of an MCMC calculation where profiling showed that pmax(x,0) was taking about 30% of the total time.  I used
>
>    TL> function(x) {z <- x<0; x[z] <- 0; x}
>
>    TL> which was significantly faster. I didn't try the
>    TL> arithmetic solution.
>
> I did - eons ago as mentioned in my message earlier in this
> thread. I can assure you that those (also mentioned)
>
>  pmin2 <- function(k,x) (x+k - abs(x-k))/2
>  pmax2 <- function(k,x) (x+k + abs(x-k))/2
>
> are faster still, particularly if you hardcode the special case of k=0!
> {that's how I came about these:  pmax(x,0) is also denoted  x_+, and
> 	x_+ := (x + |x|)/2
> 	x_- := (x - |x|)/2
> }
>
>    TL> Also, I didn't check if a solution like this would still
>    TL> be faster when both arguments are vectors (but there was
>    TL> a recent mailing list thread where someone else did).
>
> indeed, and they are faster.
> Martin
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Fri Feb  9 19:41:05 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Feb 2007 13:41:05 -0500
Subject: [R] two perspective plots in in plot
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB7FCB73@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB7FCB73@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <45CCC041.3050902@stats.uwo.ca>

On 2/9/2007 1:11 PM, Greg Snow wrote:
> Probably the easiest way is to use the "wireframe" function in the
> lattice package.  The second example in the help shows 2 surfaces (you
> do need to combine the data into a single data frame).
> 
> If you really want to use the "persp" function, then you could create
> the first plot, then call "par(new=TRUE)" and then do the 2nd plot, but
> that would take a lot of thinking to get the axes and scales to line up
> properly and make it look good.

Another alternative is to use the persp3d function and surface3d 
functions in the rgl package.  It would be quite tricky to get persp to 
handle hidden surfaces properly, whereas rgl will just do it (as long as 
neither is transparent.  Transparency is hard.)

For example, after running example(persp) so that x, y, and z contain 
values that were just used in

persp(x, y, z, theta = 135, phi = 30, col = "green3", scale = FALSE,
       ltheta = -120, shade = 0.75, border = NA, box = FALSE)

you can run

  library(rgl)

  persp3d(x,y,z, col="green3", aspect="iso", axes=FALSE, box=FALSE, 
xlab="", ylab="", zlab="")

  persp3d(x,y,(z + mean(z))/2, col="red", add=TRUE)

and then rotate the surfaces to the desired viewing angle.

Duncan Murdoch


From topkatz at msn.com  Fri Feb  9 19:47:10 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 09 Feb 2007 13:47:10 -0500
Subject: [R] Local R Users Groups?
Message-ID: <BAY132-F39AD2AC5D8495B1F1D4E3EAA9C0@phx.gbl>

Hi.

I have only recently become a regular R user, and have already benefited 
from the R-help list (Thank you!).  I have spent many years in the SAS world 
(insert your own sassy remark here), which has its own very nice users list 
site (http://listserv.uga.edu/archives/sas-l.html), plus users groups at 
local, regional, and higher levels 
(http://support.sas.com/usergroups/intro.html).  I've learned a lot and met 
many interesting folks at SAS users group meetings.  So now I'm wondering 
whether there are local R users groups, specifically in the New York City 
metropolitan area.  I would like to join such an NYC-based organization, if 
it exists.  Otherwise, if there are any R users in the NYC area who'd like 
to form a local R users groups, I'd happily help to spearhead such an 
effort.

--  TMK  --
212-460-5430	home
917-656-5351	cell


From murdoch at stats.uwo.ca  Fri Feb  9 19:52:25 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Feb 2007 13:52:25 -0500
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <Pine.LNX.4.64.0702091827150.6954@gannet.stats.ox.ac.uk>
References: <40e66e0b0702090705u669db638yfa79c4c71e4ea343@mail.gmail.com>	<Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>	<17868.47395.618746.286144@stat.math.ethz.ch>
	<Pine.LNX.4.64.0702091827150.6954@gannet.stats.ox.ac.uk>
Message-ID: <45CCC2E9.1050102@stats.uwo.ca>

On 2/9/2007 1:33 PM, Prof Brian Ripley wrote:
>> x <- rnorm(10000)
>> system.time(for(i in 1:1000) pmax(x, 0))
>     user  system elapsed
>     4.43    0.05    4.54
>> pmax2 <- function(k,x) (x+k + abs(x-k))/2
>> system.time(for(i in 1:1000) pmax2(x, 0))
>     user  system elapsed
>     0.64    0.03    0.67
>> pm <- function(x) {z <- x<0; x[z] <- 0; x}
>> system.time(for(i in 1:1000) pm(x))
>     user  system elapsed
>     0.59    0.00    0.59
>> system.time(for(i in 1:1000) pmax.int(x, 0))
>     user  system elapsed
>     0.36    0.00    0.36
> 
> So at least on one system Thomas' solution is a little faster, but a 
> C-level n-args solution handling NAs is quite a lot faster.

For this special case we can do a lot better using

pospart <- function(x) (x + abs(x))/2

The less specialized function

pmax2 <- function(x,y) {
   diff <- x - y
   y + (diff + abs(diff))/2
}

is faster on my system than pm, but not as fast as pospart:

 > system.time(for(i in 1:1000) pm(x))
[1] 0.77 0.01 0.78   NA   NA
 > system.time(for(i in 1:1000) pospart(x))
[1] 0.27 0.02 0.28   NA   NA
 > system.time(for(i in 1:1000) pmax2(x,0))
[1] 0.47 0.00 0.47   NA   NA



Duncan Murdoch

> 
> On Fri, 9 Feb 2007, Martin Maechler wrote:
> 
>>>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>>>     on Fri, 9 Feb 2007 08:13:54 -0800 (PST) writes:
>>
>>    TL> On 2/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>    >>> The other reason why pmin/pmax are preferable to your functions is that
>>    >>> they are fully generic.  It is not easy to write C code which takes into
>>    >>> account that <, [, [<- and is.na are all generic.  That is not to say that
>>    >>> it is not worth having faster restricted alternatives, as indeed we do
>>    >>> with rep.int and seq.int.
>>    >>>
>>    >>> Anything that uses arithmetic is making strong assumptions about the
>>    >>> inputs.  It ought to be possible to write a fast C version that worked for
>>    >>> atomic vectors (logical, integer, real and character), but is there
>>    >>> any evidence of profiled real problems where speed is an issue?
>>
>>
>>    TL> I had an example just last month of an MCMC calculation where profiling showed that pmax(x,0) was taking about 30% of the total time.  I used
>>
>>    TL> function(x) {z <- x<0; x[z] <- 0; x}
>>
>>    TL> which was significantly faster. I didn't try the
>>    TL> arithmetic solution.
>>
>> I did - eons ago as mentioned in my message earlier in this
>> thread. I can assure you that those (also mentioned)
>>
>>  pmin2 <- function(k,x) (x+k - abs(x-k))/2
>>  pmax2 <- function(k,x) (x+k + abs(x-k))/2
>>
>> are faster still, particularly if you hardcode the special case of k=0!
>> {that's how I came about these:  pmax(x,0) is also denoted  x_+, and
>> 	x_+ := (x + |x|)/2
>> 	x_- := (x - |x|)/2
>> }
>>
>>    TL> Also, I didn't check if a solution like this would still
>>    TL> be faster when both arguments are vectors (but there was
>>    TL> a recent mailing list thread where someone else did).
>>
>> indeed, and they are faster.
>> Martin
>>
>


From chenxh007 at gmail.com  Fri Feb  9 20:25:59 2007
From: chenxh007 at gmail.com (Xiaohui)
Date: Fri, 09 Feb 2007 11:25:59 -0800
Subject: [R]  heatmap color specification
Message-ID: <45CCCAC7.8000902@gmail.com>

hi,

I have a positive integer matrix like:

test<-matrix(c(1,2,2,2,2,1,1,2,3),3)

and based on the distant function I made like this:

generateDistMat<-function (target)
{
    n <- nrow(target)
    rn <- rownames(target)
    distM <- matrix(NA, n, n)
    diag(distM) <- 0
    for (i in 1:(n - 1)) for (j in (i + 1):n) {
        distM[i, j] <- length(which(target[i, ] != target[j,
            ]))
        distM[j, i] <- distM[i, j]
    }
    colnames(distM) <- rownames(distM) <- rn
    distM
}

dist.fun <- function(M) return(as.dist(generateDistMat(M)))

Then, I did a heatmap for 'test' matrix. But for now, I want to specify 
each of the cell in the heatmap according to the values of the 
corresponding matrix elements of test.

Let's say: col<-c("red","yellow","green")

for test[1,1], the color on the map should be "red".

I have tried par('usr') and par('mar') with rect function. But this does 
not work because the rect shift from the original map. Could any one 
tell me how to fill the cells on the map with corresponding values? Or 
can we get the actual coordinates of the image excluding the dendregram. 
Any help would be appreciated. Thanks in advance!

Xiaohui


From roland.rproject at gmail.com  Fri Feb  9 20:46:14 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Fri, 9 Feb 2007 14:46:14 -0500
Subject: [R] two perspective plots in in plot
In-Reply-To: <45CCC041.3050902@stats.uwo.ca>
References: <07E228A5BE53C24CAD490193A7381BBB7FCB73@LP-EXCHVS07.CO.IHC.COM>
	<45CCC041.3050902@stats.uwo.ca>
Message-ID: <47c7c59e0702091146t1b19d9c6q44a00e5e42087f06@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/c4aa25c4/attachment.pl 

From rding1 at jhmi.edu  Fri Feb  9 20:54:57 2007
From: rding1 at jhmi.edu (Ru Ding)
Date: Fri, 09 Feb 2007 14:54:57 -0500
Subject: [R] How to get predicted time from coxph model
Message-ID: <45CC8B41020000E70000D39E@cis27.hosts.jhmi.edu>

** Reply Requested When Convenient **


From sebastiendurand at videotron.ca  Fri Feb  9 20:57:57 2007
From: sebastiendurand at videotron.ca (Sebastien Durand)
Date: Fri, 09 Feb 2007 14:57:57 -0500
Subject: [R] Graphical device questions?
Message-ID: <p06230902c1f2826f7354@[192.168.2.3]>

Dear all,

Here is my questions:

1- Under a WINDOWS installation of R-2.4.1,  can 
we change the naming of a new ploting device open 
by the command "windows()"?. Instead of the 
default name e.g.: "Device 2" I would like to use 
something like "Density plot" or whatever!


2- Under a MAC OS X installation of R-2.4.1, 
using quartz devices, is there a way to perform 
bringToTop operation like the one available under 
WINDOWS using bringToTop function.

Thank you very much for your time.

Cheers

S?bastien
-- 
Dans le Ssu Ma Fa on lit: "Celui qui place la vie 
au dessus de toute chose sera paralys? par 
l'irr?solution"


From vistocco at unicas.it  Fri Feb  9 21:31:16 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Fri, 09 Feb 2007 21:31:16 +0100
Subject: [R] dyn.load problem under linux
Message-ID: <45CCDA14.5030502@unicas.it>

Dear HelpeRs,
I am trying to use an thirdy-part library under Linux  (the library is 
developed
both for Windows and for Linux).

I have tried different solutions (with the library developer) but we are 
not able to
solve the problem. So I try to ask for your help in order to escape from 
the full stop
where we are at the moment.

The problem looks to depend on the dyn.load function (technical details 
are below).

Thanks a lot for your work (it is an invaluable resource).
Best,
domenico

The library is called POP.R. I installed it but I have the following 
error when
I try to load it:
---------------------------------------------------------------------------------------------------------------------------------------------
 > library(POP.R)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library 
'/usr/lib/R/library/POP.R/libs/ezlic20.so':
  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file 
or directory
Error in library(POP.R) : .First.lib failed for 'POP.R'
---------------------------------------------------------------------------------------------------------------------------------------------

Nevertheless the file exists:
---------------------------------------------------------------------------------------------------------------------------------------------
 > file.exists("/usr/lib/R/library/POP.R/libs/ezlic20.so")
[1] TRUE
---------------------------------------------------------------------------------------------------------------------------------------------

I tried also directly using the dyn.load function on the command line
(without and with the local flag):
---------------------------------------------------------------------------------------------------------------------------------------------
 > dyn.load("/usr/lib/R/library/POP.R/libs/ezlic20.so",local=FALSE)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library 
'/usr/lib/R/library/POP.R/libs/ezlic20.so':
  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file 
or directory
 > dyn.load("/usr/lib/R/library/POP.R/libs/ezlic20.so",local=TRUE)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library 
'/usr/lib/R/library/POP.R/libs/ezlic20.so':
  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file 
or directory
---------------------------------------------------------------------------------------------------------------------------------------------

In the lib directory there is also another library, for which I have a 
different error
message using the dyn.load function (I have the same message using 
local=FALSE):
---------------------------------------------------------------------------------------------------------------------------------------------
 > dyn.load("/usr/lib/R/library/POP.R/libs/pop_BurSt.so")
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library 
'/usr/lib/R/library/POP.R/libs/pop_BurSt.so':
  /usr/lib/R/library/POP.R/libs/pop_BurSt.so: undefined symbol: 
getChainedKeyId
---------------------------------------------------------------------------------------------------------------------------------------------

My operating system is Ubuntu 6.10 -  Edgy.

Follow the answers to version and sessionInfo:
--------------------------------------------------------------------------------------------------
 > version
               _                          
platform       i486-pc-linux-gnu          
arch           i486                       
os             linux-gnu                  
system         i486, linux-gnu            
status                                    
major          2                          
minor          4.1                        
year           2006                       
month          12                         
day            18                         
svn rev        40228                      
language       R                          
version.string R version 2.4.1 (2006-12-18)

 > sessionInfo()
R version 2.4.1 (2006-12-18)
i486-pc-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods" 
[7] "base"    
--------------------------------------------------------------------------------------------------

Chiacchiera con i tuoi amici in tempo reale! 
 http://it.yahoo.com/mail_it/foot/*http://it.messenger.yahoo.com


From jcroot at gmail.com  Fri Feb  9 21:38:03 2007
From: jcroot at gmail.com (James Root)
Date: Fri, 9 Feb 2007 15:38:03 -0500
Subject: [R] plotting derived values not raw
Message-ID: <acb1f1cc0702091238od457b97w952ae4da0ddb570a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/52af3a0f/attachment.pl 

From mark.lyman at gmail.com  Fri Feb  9 22:01:06 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Fri, 9 Feb 2007 21:01:06 +0000 (UTC)
Subject: [R] plotting derived values not raw
References: <acb1f1cc0702091238od457b97w952ae4da0ddb570a@mail.gmail.com>
Message-ID: <loom.20070209T215842-405@post.gmane.org>

James Root <jcroot <at> gmail.com> writes:

> 
> I am trying to plot the mean and standard error of three separate
> conditions.  For various reasons, I do not have access to the raw data from
> which the mean and error were derived and would like to make error bar plots
> utilizing only the actual mean and standard error values.  Is there a way to
> do this in R?  Thanks for any help in advance.
> james

I believe that xYplot in the Hmisc package will do what you want

Mark Lyman


From bates at stat.wisc.edu  Fri Feb  9 22:10:02 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 9 Feb 2007 15:10:02 -0600
Subject: [R] TA-Lib and R
In-Reply-To: <20070209182612.139370@gmx.net>
References: <20070209182612.139370@gmx.net>
Message-ID: <40e66e0b0702091310q2d088dc9j8e562e8e2adf1976@mail.gmail.com>

On 2/9/07, jacinthe at gmx.de <jacinthe at gmx.de> wrote:

> Does someone know how to use TA-Lib in R (http://ta-lib.org/)?

Simple.  Read the documentation for TA-Lib, then read the manual
"Writing R Extensions", then write and test  the necessary interface
routines.


From Greg.Snow at intermountainmail.org  Fri Feb  9 22:16:42 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 9 Feb 2007 14:16:42 -0700
Subject: [R] two perspective plots in in plot
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7FCBEF@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/ed6e1cc8/attachment.pl 

From mikewhite.diu at tiscali.co.uk  Fri Feb  9 17:51:29 2007
From: mikewhite.diu at tiscali.co.uk (mikewhite.diu at tiscali.co.uk)
Date: Fri, 9 Feb 2007 16:51:29 +0000
Subject: [R] Confidence intervals of quantiles
Message-ID: <45A41F4B0004F581@mail-2-uk.mail.tiscali.sys>

Many thanks for all the contributions to this problem.
As inferred by Ted Harding, I was after a distribution-free CIs as a lot
of
the data I use is not normally distributed.

The method provided by Ted for calculating exact CIs gave good results with
the limits almost symmetric about the quantile.  Even for my smaller data
set with only 115 samples and a very skewed distribution the results clearly
showed the increase in range and asymmetry of the confidence limits.

The bootstrap method provided by Dimitris also works reasonably well but
is
slower and the ranges for the CIs are sometimes very asymmetric and in one
case did not actually encompass the quantile. The method would not work at
all with the skewed distribution of 115 samples until I reduced the quantile
range from 0.1 and 0.9 to 0.2 and 0.8  However, as Dimitris warned, you have
to be careful with this method for extreme quantiles and small samples.

I was also sent a method by S?ren Merser, but this was incomplete.  This
method was written by Scott Chasalow and the full code can be found at
http://www.dpw.wau.nl/pv/pub/chasalow/S/win/ci.quantile/
The code was written for S-Plus but it worked ok for me in R.  This method
actually gives several ranges about the quantile, each with about the same
level of confidence and the level of confidence is also in the output. As
with Ted Harding's method, these may not exactly match the desired
confidence level.  There is an option to select the shortest range but it
would be easy enough to add code to give the most symmetric range.

As a chemist I am not able to comment on the statistical pros and cons of
the methods but they are certainly very helpful for my purposes.

Many thanks
Mike White

----- Original Message ----- 
From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
To: "Mike White" <mikewhite.diu at btconnect.com>
Cc: <R-help at stat.math.ethz.ch>
Sent: Monday, February 05, 2007 2:43 PM
Subject: Re: [R] Confidence intervals of quantiles


> you could use the Bootstrap method, using package 'boot', e.g.,
>
> library(boot)
>
> f.quantile <- function(x, ind, ...){
>     quantile(x[ind], ...)
> }
>
> ###########
>
> x <- rgamma(750, 2)
> quant.boot <- boot(x, f.quantile, R = 1000, probs = c(0.025, 0.25,
> 0.5, 0.75, 0.975))
> lapply(1:5, function(i) boot.ci(quant.boot, c(0.90, 0.95), type =
> c("perc", "bca"), index = i))
>
> y <- rgamma(150, 2)
> quant.boot <- boot(y, f.quantile, R = 1000, probs = c(0.025, 0.25,
> 0.5, 0.75, 0.975))
> lapply(1:5, function(i) boot.ci(quant.boot, c(0.90, 0.95), type =
> c("perc", "bca"), index = i))
>
>
> However, you should be a little bit careful with Bootstrap if you wish
> to obtain CIs for extreme quantiles in small samples.
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message ----- 
> From: "Mike White" <mikewhite.diu at btconnect.com>
> To: <R-help at stat.math.ethz.ch>
> Sent: Monday, February 05, 2007 2:47 PM
> Subject: [R] Confidence intervals of quantiles
>
>
> > Can anyone please tell me if there is a function to calculate
> > confidence
> > intervals for the results of the quantile function.
> > Some of my data is normally distributed but some is also a squewed
> > distribution or a capped normal distribution. Some of the data sets
> > contain
> > about 700 values whereas others are smaller with about 100-150
> > values, so I
> > would like to see how the confidence intervals change for the
> > different
> > distributions and different data sizes.
> >
> > Thanks
> > Mike White
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>


___________________________________________________________

Tiscali Broadband only 9.99 a month for your first 3 months!
http://www.tiscali.co.uk/products/broadband/


From pburns at pburns.seanet.com  Fri Feb  9 22:29:21 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 09 Feb 2007 21:29:21 +0000
Subject: [R] dyn.load problem under linux
In-Reply-To: <45CCDA14.5030502@unicas.it>
References: <45CCDA14.5030502@unicas.it>
Message-ID: <45CCE7B1.1030507@pburns.seanet.com>

Some additional notes inserted below.

Domenico Vistocco wrote:

>Dear HelpeRs,
>I am trying to use an thirdy-part library under Linux  (the library is 
>developed
>both for Windows and for Linux).
>
>I have tried different solutions (with the library developer) but we are 
>not able to
>solve the problem. So I try to ask for your help in order to escape from 
>the full stop
>where we are at the moment.
>
>The problem looks to depend on the dyn.load function (technical details 
>are below).
>
>Thanks a lot for your work (it is an invaluable resource).
>Best,
>domenico
>
>The library is called POP.R. I installed it but I have the following 
>error when
>I try to load it:
>---------------------------------------------------------------------------------------------------------------------------------------------
> > library(POP.R)
>Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library 
>'/usr/lib/R/library/POP.R/libs/ezlic20.so':
>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file 
>or directory
>Error in library(POP.R) : .First.lib failed for 'POP.R'
>---------------------------------------------------------------------------------------------------------------------------------------------
>
>Nevertheless the file exists:
>---------------------------------------------------------------------------------------------------------------------------------------------
> > file.exists("/usr/lib/R/library/POP.R/libs/ezlic20.so")
>[1] TRUE
>---------------------------------------------------------------------------------------------------------------------------------------------
>
>I tried also directly using the dyn.load function on the command line
>(without and with the local flag):
>---------------------------------------------------------------------------------------------------------------------------------------------
> > dyn.load("/usr/lib/R/library/POP.R/libs/ezlic20.so",local=FALSE)
>Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library 
>'/usr/lib/R/library/POP.R/libs/ezlic20.so':
>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file 
>or directory
> > dyn.load("/usr/lib/R/library/POP.R/libs/ezlic20.so",local=TRUE)
>Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library 
>'/usr/lib/R/library/POP.R/libs/ezlic20.so':
>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file 
>or directory
>  
>

We have seen that problems like this can arise if
there is a mixture of 32-bit and 64-bit items.  Everything
here is definitely 32-bit.

>---------------------------------------------------------------------------------------------------------------------------------------------
>
>In the lib directory there is also another library, for which I have a 
>different error
>message using the dyn.load function (I have the same message using 
>local=FALSE):
>---------------------------------------------------------------------------------------------------------------------------------------------
> > dyn.load("/usr/lib/R/library/POP.R/libs/pop_BurSt.so")
>Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library 
>'/usr/lib/R/library/POP.R/libs/pop_BurSt.so':
>  /usr/lib/R/library/POP.R/libs/pop_BurSt.so: undefined symbol: 
>getChainedKeyId
>  
>

This error message is actually good news -- it finds the
file (sitting right next to the one it can't "find") and gets so
far as to find a symbol that is in the first file.  That is, this
command appears like it would work if the first load worked.

>---------------------------------------------------------------------------------------------------------------------------------------------
>
>My operating system is Ubuntu 6.10 -  Edgy.
>  
>

I'm not sure what versions of Linux other clients have
(it's never been an issue before), but I know it works
in Suse.

Pat

>Follow the answers to version and sessionInfo:
>--------------------------------------------------------------------------------------------------
> > version
>               _                          
>platform       i486-pc-linux-gnu          
>arch           i486                       
>os             linux-gnu                  
>system         i486, linux-gnu            
>status                                    
>major          2                          
>minor          4.1                        
>year           2006                       
>month          12                         
>day            18                         
>svn rev        40228                      
>language       R                          
>version.string R version 2.4.1 (2006-12-18)
>
> > sessionInfo()
>R version 2.4.1 (2006-12-18)
>i486-pc-linux-gnu
>
>locale:
>LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
>attached base packages:
>[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods" 
>[7] "base"    
>--------------------------------------------------------------------------------------------------
>
>Chiacchiera con i tuoi amici in tempo reale! 
> http://it.yahoo.com/mail_it/foot/*http://it.messenger.yahoo.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From ccleland at optonline.net  Fri Feb  9 22:47:29 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 09 Feb 2007 16:47:29 -0500
Subject: [R] plotting derived values not raw
In-Reply-To: <acb1f1cc0702091238od457b97w952ae4da0ddb570a@mail.gmail.com>
References: <acb1f1cc0702091238od457b97w952ae4da0ddb570a@mail.gmail.com>
Message-ID: <45CCEBF1.7010702@optonline.net>

James Root wrote:
> I am trying to plot the mean and standard error of three separate
> conditions.  For various reasons, I do not have access to the raw data from
> which the mean and error were derived and would like to make error bar plots
> utilizing only the actual mean and standard error values.  Is there a way to
> do this in R?  Thanks for any help in advance.

RSiteSearch("error bars", restrict="function") points to several
packages that will plot errors bars or confidence intervals only from
the summary information.  For example:

library(sfsmisc)

mymeans <- rnorm(10)

myerrors <- 1 + .1*rnorm(10)

errbar(1:10, mymeans, mymeans + myerrors, mymeans - myerrors,
main="Error Bars example")

> james
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From maechler at stat.math.ethz.ch  Fri Feb  9 23:13:49 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Feb 2007 23:13:49 +0100
Subject: [R] Timings of function execution in R [was Re: R in Industry]
In-Reply-To: <45CCC2E9.1050102@stats.uwo.ca>
References: <40e66e0b0702090705u669db638yfa79c4c71e4ea343@mail.gmail.com>
	<Pine.LNX.4.43.0702090813540.5604@hymn01.u.washington.edu>
	<17868.47395.618746.286144@stat.math.ethz.ch>
	<Pine.LNX.4.64.0702091827150.6954@gannet.stats.ox.ac.uk>
	<45CCC2E9.1050102@stats.uwo.ca>
Message-ID: <17868.61981.286125.456925@stat.math.ethz.ch>

>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
>>>>>     on Fri, 09 Feb 2007 13:52:25 -0500 writes:

    Duncan> On 2/9/2007 1:33 PM, Prof Brian Ripley wrote:
    >>> x <- rnorm(10000)
    >>> system.time(for(i in 1:1000) pmax(x, 0))
    >> user  system elapsed
    >> 4.43    0.05    4.54
    >>> pmax2 <- function(k,x) (x+k + abs(x-k))/2
    >>> system.time(for(i in 1:1000) pmax2(x, 0))
    >> user  system elapsed
    >> 0.64    0.03    0.67
    >>> pm <- function(x) {z <- x<0; x[z] <- 0; x}
    >>> system.time(for(i in 1:1000) pm(x))
    >> user  system elapsed
    >> 0.59    0.00    0.59
    >>> system.time(for(i in 1:1000) pmax.int(x, 0))
    >> user  system elapsed
    >> 0.36    0.00    0.36
    >> 
    >> So at least on one system Thomas' solution is a little faster, but a 
    >> C-level n-args solution handling NAs is quite a lot faster.

    Duncan> For this special case we can do a lot better using

    Duncan> pospart <- function(x) (x + abs(x))/2

Indeed, that's what I meant when I talked about doing the
special case 'k = 0' explicitly -- and also what my timings
where based on.

Thank you Duncan -- and Brian for looking into providing an even
faster and more general C-internal version!
Martin

    Duncan> The less specialized function

    Duncan> pmax2 <- function(x,y) {
    Duncan> diff <- x - y
    Duncan> y + (diff + abs(diff))/2
    Duncan> }

    Duncan> is faster on my system than pm, but not as fast as pospart:

    >> system.time(for(i in 1:1000) pm(x))
    Duncan> [1] 0.77 0.01 0.78   NA   NA
    >> system.time(for(i in 1:1000) pospart(x))
    Duncan> [1] 0.27 0.02 0.28   NA   NA
    >> system.time(for(i in 1:1000) pmax2(x,0))
    Duncan> [1] 0.47 0.00 0.47   NA   NA



    Duncan> Duncan Murdoch

    >> 
    >> On Fri, 9 Feb 2007, Martin Maechler wrote:
    >> 
    >>>>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
    >>>>>>>> on Fri, 9 Feb 2007 08:13:54 -0800 (PST) writes:
    >>> 
    TL> On 2/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
    >>> >>> The other reason why pmin/pmax are preferable to your functions is that
    >>> >>> they are fully generic.  It is not easy to write C code which takes into
    >>> >>> account that <, [, [<- and is.na are all generic.  That is not to say that
    >>> >>> it is not worth having faster restricted alternatives, as indeed we do
    >>> >>> with rep.int and seq.int.
    >>> >>>
    >>> >>> Anything that uses arithmetic is making strong assumptions about the
    >>> >>> inputs.  It ought to be possible to write a fast C version that worked for
    >>> >>> atomic vectors (logical, integer, real and character), but is there
    >>> >>> any evidence of profiled real problems where speed is an issue?
    >>> 
    >>> 
    TL> I had an example just last month of an MCMC calculation where profiling showed that pmax(x,0) was taking about 30% of the total time.  I used
    >>> 
    TL> function(x) {z <- x<0; x[z] <- 0; x}
    >>> 
    TL> which was significantly faster. I didn't try the
    TL> arithmetic solution.
    >>> 
    >>> I did - eons ago as mentioned in my message earlier in this
    >>> thread. I can assure you that those (also mentioned)
    >>> 
    >>> pmin2 <- function(k,x) (x+k - abs(x-k))/2
    >>> pmax2 <- function(k,x) (x+k + abs(x-k))/2
    >>> 
    >>> are faster still, particularly if you hardcode the special case of k=0!
    >>> {that's how I came about these:  pmax(x,0) is also denoted  x_+, and
    >>> x_+ := (x + |x|)/2
    >>> x_- := (x - |x|)/2
    >>> }
    >>> 
    TL> Also, I didn't check if a solution like this would still
    TL> be faster when both arguments are vectors (but there was
    TL> a recent mailing list thread where someone else did).
    >>> 
    >>> indeed, and they are faster.
    >>> Martin
    >>> 
    >>


From kubovy at virginia.edu  Fri Feb  9 23:26:16 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 9 Feb 2007 17:26:16 -0500
Subject: [R] How to add the variable name to a qqplot or densityplot in the
	diagonal of an splom?
Message-ID: <DCAC7467-5BC1-4D17-8E86-3113BDA19D67@virginia.edu>

splom() doesn't complain here, but writes no names in the diagonal  
boxes. What am I missing?
I believe that I need to add something like grid.text(x, ...) to the  
diagonal panel, but I don't know how to get it cycle through the  
column labels. And should
     varname.col = 'blue', varname.cex = 1
be inside the diag.panel() function?

splom(szw[, n], pscales = 0,
     diag.panel = function(x, ...){
         panel.qqmathline(x, ...)
         panel.qqmath(x, ...)
     },
     lower.panel = function(x, y, ...){
         panel.xyplot(x, y, ..., col = 'lightblue')
         panel.loess(x, y, ..., col = 'red')
     },
     upper.panel = function(x, y, ...){
         panel.abline(lm(y~x),...)
         grid.text(round(cor(x, y, use = 'pairwise.complete.obs'), 2),
             x = unit(1, 'mm'),
             y = unit(1, 'npc') - unit(1, 'mm'),
             just = c('left', 'top'),
             gp = gpar(fontsize = 10))
     },
     varname.col = 'blue', varname.cex = 1
)


_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From murdoch at stats.uwo.ca  Sat Feb 10 03:13:37 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Feb 2007 21:13:37 -0500
Subject: [R] two perspective plots in in plot
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB7FCBEF@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB7FCBEF@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <45CD2A51.9030609@stats.uwo.ca>

On 2/9/2007 4:16 PM, Greg Snow wrote:
> The rgl package has an rgl.postscript function that should do that for
> you (I think there was a bug discovered and fixed recently, so make sure
> to get the latest version).

Yes, that's right.  If you see any other bugs, please let me know.  (One 
known bug is not in rgl:  Mac OSX Preview won't show .pdf files created 
by rgl properly.  That's an Apple bug, not an rgl bug.  Please complain 
to them:  it's been known for more than a year.)

Duncan Murdoch
> 
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
> 
> 
> 
>  
> 
> 
> ________________________________
> 
> 	From: Roland Rau [mailto:roland.rproject at gmail.com] 
> 	Sent: Friday, February 09, 2007 12:46 PM
> 	To: Duncan Murdoch
> 	Cc: Greg Snow; r-help
> 	Subject: Re: [R] two perspective plots in in plot
> 	
> 	
> 	Thanks Duncan and Greg.
> 	My current solution is to use the rgl-package.
> 	Is there an easy way to obtain a screenshot in eps- or
> pdf-Format from such an rgl-window?
> 	I saw the rgl.snapshot function but it does not provide this
> format. 
> 	
> 	So far, I take a snapshot, save it as jpeg and convert it to eps
> via jpeg2ps.exe
> 	Maybe not the most elegant way but the results are better than I
> anticipated.
> 	
> 	Thanks,
> 	Roland
> 	
> 	
> 	
> 	
> 	On 2/9/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote: 
> 
> 		On 2/9/2007 1:11 PM, Greg Snow wrote:
> 		> Probably the easiest way is to use the "wireframe"
> function in the
> 		> lattice package.  The second example in the help shows
> 2 surfaces (you
> 		> do need to combine the data into a single data frame).
> 
> 		>
> 		> If you really want to use the "persp" function, then
> you could create
> 		> the first plot, then call "par(new=TRUE)" and then do
> the 2nd plot, but
> 		> that would take a lot of thinking to get the axes and
> scales to line up 
> 		> properly and make it look good.
> 		
> 		Another alternative is to use the persp3d function and
> surface3d
> 		functions in the rgl package.  It would be quite tricky
> to get persp to
> 		handle hidden surfaces properly, whereas rgl will just
> do it (as long as 
> 		neither is transparent.  Transparency is hard.)
> 		
> 		For example, after running example(persp) so that x, y,
> and z contain
> 		values that were just used in
> 		
> 		persp(x, y, z, theta = 135, phi = 30, col = "green3",
> scale = FALSE, 
> 		       ltheta = -120, shade = 0.75, border = NA, box =
> FALSE)
> 		
> 		you can run
> 		
> 		  library(rgl)
> 		
> 		  persp3d(x,y,z, col="green3", aspect="iso", axes=FALSE,
> box=FALSE,
> 		xlab="", ylab="", zlab="") 
> 		
> 		  persp3d(x,y,(z + mean(z))/2, col="red", add=TRUE)
> 		
> 		and then rotate the surfaces to the desired viewing
> angle.
> 		
> 		Duncan Murdoch
> 		
> 
> 
>


From chenxh007 at gmail.com  Sat Feb 10 03:25:41 2007
From: chenxh007 at gmail.com (Xiaohui)
Date: Fri, 09 Feb 2007 18:25:41 -0800
Subject: [R]  Heatmap color specification
Message-ID: <45CD2D25.9000203@gmail.com>

hi,

I have a positive integer matrix like:

test<-matrix(c(1,2,2,2,2,1,1,2,3),3)

and based on the distant function I made like this:

generateDistMat<-function (target)
{
   n <- nrow(target)
   rn <- rownames(target)
   distM <- matrix(NA, n, n)
   diag(distM) <- 0
   for (i in 1:(n - 1)) for (j in (i + 1):n) {
       distM[i, j] <- length(which(target[i, ] != target[j,
           ]))
       distM[j, i] <- distM[i, j]
   }
   colnames(distM) <- rownames(distM) <- rn
   distM
}

dist.fun <- function(M) return(as.dist(generateDistMat(M)))

Then, I did a heatmap for 'test' matrix. But for now, I want to specify 
each of the cell in the heatmap according to the values of the 
corresponding matrix elements of test.

Let's say: col<-c("red","yellow","green")

for test[1,1], the color on the map should be "red".

I have tried par('usr') and par('mar') with rect function. But this does 
not work because the rect shift from the original map. Could any one 
tell me how to fill the cells on the map with corresponding values? Or 
can we get the actual coordinates of the image excluding the dendregram. 
Any help would be appreciated. Thanks in advance!

Xiaohui


From megh700004 at yahoo.com  Sat Feb 10 07:09:05 2007
From: megh700004 at yahoo.com (Megh Dal)
Date: Fri, 9 Feb 2007 22:09:05 -0800 (PST)
Subject: [R] X11 method
Message-ID: <538254.18778.qm@web58104.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070209/437af6b9/attachment.pl 

From ott.toomet at ut.ee  Sat Feb 10 07:11:21 2007
From: ott.toomet at ut.ee (Ott-Siim Toomet)
Date: Sat, 10 Feb 2007 08:11:21 +0200 (EET)
Subject: [R] how to eval subset?
Message-ID: <47123.80.235.68.118.1171087881.squirrel@mailhost.ut.ee>

Hi,

I have problems with subset when calling a function from inside a function
from inside a function.  Here is a small example to be called as 'f1()'.
'eval()' in f3 fails with error message

Error in eval(expr, envir, enclos) : object "subs" not found

Is it possible to supplement subset to data, to be calculated in a
different environment than the data itself?

Thanks in advance
Ott

The example follows:
----------------------------------------------------------
N <- 10
f1 <- function() {
   x <- runif(N)
   y <- runif(N)
   f2(y ~ x)
}

f2 <- function(formula, data=sys.frame(sys.parent())) {
   subs <- rep(c(TRUE, FALSE), length.out=N)
   f3(formula, data=data, subset=subs)
}

f3 <- function(formula, data=sys.frame(sys.parent()), subset) {
   mf <- match.call()
   m <- match(c("formula", "subset", "data"), names(mf), 0)
   mf <- mf[c(1, m)]
   mf[[1]] <- as.name("model.frame")
   eval(data)
# anyone able to explain me why this eval is necessary?
# can it be done in a smarter way?
   mf <- eval(mf, envir=parent.frame())
# finds the data environment, but not environment for subset
}


From ripley at stats.ox.ac.uk  Sat Feb 10 08:32:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 10 Feb 2007 07:32:13 +0000 (GMT)
Subject: [R] how to eval subset?
In-Reply-To: <47123.80.235.68.118.1171087881.squirrel@mailhost.ut.ee>
References: <47123.80.235.68.118.1171087881.squirrel@mailhost.ut.ee>
Message-ID: <Pine.LNX.4.64.0702100724250.32592@gannet.stats.ox.ac.uk>

You mean the 'subset' argument to model.frame and not subset() the 
function, I think.

The answer is on the help page for model.frame[.default].

      All the variables in 'formula', 'subset' and in '...' are looked
      for first in 'data' and then in the environment of 'formula' (see
      the help for 'formula()' for further details) and collected into a
      data frame.

The evaluation frame of f2 (where you defined 'subs') is not a place that 
is searched.  There are various workarounds, and in your case calling 
model.frame() in f2 to assemble 'data' would work.  (Had you 
transformations had in your formula it would not, but R-devel has 
get_all_vars() for this purpose.)

On Sat, 10 Feb 2007, Ott-Siim Toomet wrote:

> Hi,
>
> I have problems with subset when calling a function from inside a function
> from inside a function.  Here is a small example to be called as 'f1()'.
> 'eval()' in f3 fails with error message
>
> Error in eval(expr, envir, enclos) : object "subs" not found
>
> Is it possible to supplement subset to data, to be calculated in a
> different environment than the data itself?
>
> Thanks in advance
> Ott
>
> The example follows:
> ----------------------------------------------------------
> N <- 10
> f1 <- function() {
>   x <- runif(N)
>   y <- runif(N)
>   f2(y ~ x)
> }
>
> f2 <- function(formula, data=sys.frame(sys.parent())) {
>   subs <- rep(c(TRUE, FALSE), length.out=N)
>   f3(formula, data=data, subset=subs)
> }
>
> f3 <- function(formula, data=sys.frame(sys.parent()), subset) {
>   mf <- match.call()
>   m <- match(c("formula", "subset", "data"), names(mf), 0)
>   mf <- mf[c(1, m)]
>   mf[[1]] <- as.name("model.frame")
>   eval(data)
> # anyone able to explain me why this eval is necessary?
> # can it be done in a smarter way?
>   mf <- eval(mf, envir=parent.frame())
> # finds the data environment, but not environment for subset
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bartjoosen at hotmail.com  Sat Feb 10 08:43:36 2007
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Sat, 10 Feb 2007 08:43:36 +0100
Subject: [R] Near function?
Message-ID: <BAY134-DAV163B546ACF849218F01779D8930@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070210/9b7aacb0/attachment.pl 

From tcallawa at redhat.com  Sat Feb 10 10:10:58 2007
From: tcallawa at redhat.com (Tom 'spot' Callaway)
Date: Sat, 10 Feb 2007 03:10:58 -0600
Subject: [R] dyn.load problem under linux
In-Reply-To: <45CCDA14.5030502@unicas.it>
References: <45CCDA14.5030502@unicas.it>
Message-ID: <1171098658.1479.40.camel@dhcp-32-109.ord.redhat.com>

On Fri, 2007-02-09 at 21:31 +0100, Domenico Vistocco wrote:

>   libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file 
> or directory
> Error in library(POP.R) : .First.lib failed for 'POP.R'
> ---------------------------------------------------------------------------------------------------------------------------------------------
> 
> Nevertheless the file exists:
> ---------------------------------------------------------------------------------------------------------------------------------------------
>  > file.exists("/usr/lib/R/library/POP.R/libs/ezlic20.so")

You're misreading the error message:

libstdc++-libc6.2-2.so.3 is the file that doesn't exist. Its a rather
ancient copy of libstdc++ from Red Hat 6.2, which almost certainly
doesn't exist in a normal Ubuntu installation. It used to be in Fedora
as part of the compat-libstdc++-296-2.96 package, but Fedora dropped it
around FC-4.

So, either, you should convince whomever compiled that library to
recompile it against a modern version of Linux (without any compat-*
libs installed), made in the last 5 years, or you could find an old copy
of the Fedora Core 4 RPM, unpack the libraries from the rpm, put them
in /usr/lib, and hope that is the only ancient library you need.

Hooray for closed source, eh?

~spot


From ripley at stats.ox.ac.uk  Sat Feb 10 10:36:18 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 10 Feb 2007 09:36:18 +0000 (GMT)
Subject: [R] dyn.load problem under linux
In-Reply-To: <45CCE7B1.1030507@pburns.seanet.com>
References: <45CCDA14.5030502@unicas.it> <45CCE7B1.1030507@pburns.seanet.com>
Message-ID: <Pine.LNX.4.64.0702092211130.3084@gannet.stats.ox.ac.uk>

This is a mismatch of compiler between the machine that the code was 
compiled on and that running the code.

>> '/usr/lib/R/library/POP.R/libs/ezlic20.so':
>>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file

libstdc++-libc6.2-2.so.3 is a g++ runtime.  On my FC5 system it is 
provided by compat-libstdc++-296-2.96-135, so I think the problem is that 
the package was compiled on a very old version of g++.

If this is a 'package' ask for the sources.  (People who distribute 
compiled C++ code of general use really should compile in the runtime 
needed, taking care to meet the licence conditions.)


On Fri, 9 Feb 2007, Patrick Burns wrote:

> Some additional notes inserted below.
>
> Domenico Vistocco wrote:
>
>> Dear HelpeRs,
>> I am trying to use an thirdy-part library under Linux  (the library is
>> developed
>> both for Windows and for Linux).
>>
>> I have tried different solutions (with the library developer) but we are
>> not able to
>> solve the problem. So I try to ask for your help in order to escape from
>> the full stop
>> where we are at the moment.
>>
>> The problem looks to depend on the dyn.load function (technical details
>> are below).
>>
>> Thanks a lot for your work (it is an invaluable resource).
>> Best,
>> domenico
>>
>> The library is called POP.R. I installed it but I have the following
>> error when
>> I try to load it:
>> ---------------------------------------------------------------------------------------------------------------------------------------------
>>> library(POP.R)
>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>        unable to load shared library
>> '/usr/lib/R/library/POP.R/libs/ezlic20.so':
>>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file
>> or directory
>> Error in library(POP.R) : .First.lib failed for 'POP.R'
>> ---------------------------------------------------------------------------------------------------------------------------------------------
>>
>> Nevertheless the file exists:
>> ---------------------------------------------------------------------------------------------------------------------------------------------
>>> file.exists("/usr/lib/R/library/POP.R/libs/ezlic20.so")
>> [1] TRUE
>> ---------------------------------------------------------------------------------------------------------------------------------------------
>>
>> I tried also directly using the dyn.load function on the command line
>> (without and with the local flag):
>> ---------------------------------------------------------------------------------------------------------------------------------------------
>>> dyn.load("/usr/lib/R/library/POP.R/libs/ezlic20.so",local=FALSE)
>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>        unable to load shared library
>> '/usr/lib/R/library/POP.R/libs/ezlic20.so':
>>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file
>> or directory
>>> dyn.load("/usr/lib/R/library/POP.R/libs/ezlic20.so",local=TRUE)
>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>        unable to load shared library
>> '/usr/lib/R/library/POP.R/libs/ezlic20.so':
>>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file
>> or directory
>>
>>
>
> We have seen that problems like this can arise if
> there is a mixture of 32-bit and 64-bit items.  Everything
> here is definitely 32-bit.
>
>> ---------------------------------------------------------------------------------------------------------------------------------------------
>>
>> In the lib directory there is also another library, for which I have a
>> different error
>> message using the dyn.load function (I have the same message using
>> local=FALSE):
>> ---------------------------------------------------------------------------------------------------------------------------------------------
>>> dyn.load("/usr/lib/R/library/POP.R/libs/pop_BurSt.so")
>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>        unable to load shared library
>> '/usr/lib/R/library/POP.R/libs/pop_BurSt.so':
>>  /usr/lib/R/library/POP.R/libs/pop_BurSt.so: undefined symbol:
>> getChainedKeyId
>>
>>
>
> This error message is actually good news -- it finds the
> file (sitting right next to the one it can't "find") and gets so
> far as to find a symbol that is in the first file.  That is, this
> command appears like it would work if the first load worked.
>
>> ---------------------------------------------------------------------------------------------------------------------------------------------
>>
>> My operating system is Ubuntu 6.10 -  Edgy.
>>
>>
>
> I'm not sure what versions of Linux other clients have
> (it's never been an issue before), but I know it works
> in Suse.
>
> Pat
>
>> Follow the answers to version and sessionInfo:
>> --------------------------------------------------------------------------------------------------
>>> version
>>               _
>> platform       i486-pc-linux-gnu
>> arch           i486
>> os             linux-gnu
>> system         i486, linux-gnu
>> status
>> major          2
>> minor          4.1
>> year           2006
>> month          12
>> day            18
>> svn rev        40228
>> language       R
>> version.string R version 2.4.1 (2006-12-18)
>>
>>> sessionInfo()
>> R version 2.4.1 (2006-12-18)
>> i486-pc-linux-gnu
>>
>> locale:
>> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
>> [7] "base"
>> --------------------------------------------------------------------------------------------------
>>
>> Chiacchiera con i tuoi amici in tempo reale!
>> http://it.yahoo.com/mail_it/foot/*http://it.messenger.yahoo.com
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dieter.menne at menne-biomed.de  Sat Feb 10 11:38:06 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 10 Feb 2007 10:38:06 +0000 (UTC)
Subject: [R] Near function?
References: <BAY134-DAV163B546ACF849218F01779D8930@phx.gbl>
Message-ID: <loom.20070210T113538-258@post.gmane.org>

Bart Joosen <bartjoosen <at> hotmail.com> writes:

> 
> Hi,
> 
> I have an integer which is extracted from a dataframe, which is sorted by
another column of the dataframe.
> Now I would like to remove some elements of the integer, which are near to
others by their value. For example:
> integer: c(1,20,2,21) should be c(1,20).

....
> Sorting the integer is not an option, the order is important.

Why not? It's extremely efficient for large series and the only method that
would work with large array. The idea: Keep the indexes of the sort order, mark
the "near others" for example making their index NA, and restore original order.
No for-loop needed.

Dieter


From jim at bitwrit.com.au  Sat Feb 10 12:29:08 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 10 Feb 2007 22:29:08 +1100
Subject: [R] heatmap color specification
In-Reply-To: <45CCCAC7.8000902@gmail.com>
References: <45CCCAC7.8000902@gmail.com>
Message-ID: <45CDAC84.9030104@bitwrit.com.au>

Xiaohui wrote:
>... 
> Then, I did a heatmap for 'test' matrix. But for now, I want to specify 
> each of the cell in the heatmap according to the values of the 
> corresponding matrix elements of test.
> 
> Let's say: col<-c("red","yellow","green")
> 
> for test[1,1], the color on the map should be "red".
> 
> I have tried par('usr') and par('mar') with rect function. But this does 
> not work because the rect shift from the original map. Could any one 
> tell me how to fill the cells on the map with corresponding values? Or 
> can we get the actual coordinates of the image excluding the dendregram. 
>
Hi Xiaohui,
You may be looking for something like color2D.matplot in the plotrix 
package or "image" in the graphics package.

Jim


From jholtman at gmail.com  Sat Feb 10 14:05:03 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 10 Feb 2007 08:05:03 -0500
Subject: [R] Near function?
In-Reply-To: <BAY134-DAV163B546ACF849218F01779D8930@phx.gbl>
References: <BAY134-DAV163B546ACF849218F01779D8930@phx.gbl>
Message-ID: <644e1f320702100505x47d447a4nedbfb97f0a86bcdf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070210/e7b2f42f/attachment.pl 

From ezhil02 at yahoo.com  Sat Feb 10 14:11:06 2007
From: ezhil02 at yahoo.com (A Ezhil)
Date: Sat, 10 Feb 2007 05:11:06 -0800 (PST)
Subject: [R] Help in using multcomp.
In-Reply-To: <45CCAE2E.5000905@optonline.net>
Message-ID: <409691.92823.qm@web32409.mail.mud.yahoo.com>

Hi Chuck,

Thank you very for this help. I am able to store the
results. Now, I am facing the following problems:

1. I am trying to extract only the p-values from 

dunres <- lapply(amod, function(x){summary(glht(x,
linfct=mcp(f = contr)))})

but I am stuck. If I use dunres[[1]], it displays the
results. But I don't know how to extract the p value
from this. 

2. If I want to get raw pvalues instead of adjusted
ones, what should I do in summary(glht)? 

Thanks again for your help. I look forward to your
reply.

Kind regards,
Ezhil


--- Chuck Cleland <ccleland at optonline.net> wrote:

> A Ezhil wrote:
> > Hi All,
> > 
> > I am trying use 'multcomp' for multiple
> comparisons
> > after my ANOVA analysis. I have used the following
> > code to do ANOVA:
> > 
> > dat <- matrix(rnorm(45), nrow=5, ncol=9)
> > f <- gl(3,3,9, label=c("C", "Tl", "T2"))
> > 
> > aof <- function(x) {
> >         m <- data.frame(f, x);
> >         aov(x ~ f, m)
> > }
> > amod <- apply(dat,1,aof)
> > 
> > Now, how can I use 'glht' for the above amod. I
> know
> > that I cannot use simply 
> > 
> > glht(amod, linfct = mcp(f = "Dunnett")). 
> 
>   Since amod is a list of models rather than one
> model, do you want
> something like this?
> 
> lapply(amod, function(x){summary(glht(x, linfct =
> mcp(f = "Dunnett")))})
> 
> > Also, if I want to use Dunnett for comparing C vs
> (T1
> > and T2), how can I specify this in the glht
> function.
> 
>   How about doing it with user-defined contrasts?
> 
> contr <- rbind("C - T1   " = c(-1, 1, 0),
>                "C - T2   " = c(-1, 0, 1),
>                "C - All T" = c(-1,.5,.5))
> 
> lapply(amod, function(x){summary(glht(x, linfct =
> mcp(f = contr)))})
> 
> > Thanks in advance. 
> > Regards,
> > Ezhil
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
> 



 
____________________________________________________________________________________
Need a quick answer? Get one in minutes from people who know.
Ask your question on www.Answers.yahoo.com


From ccleland at optonline.net  Sat Feb 10 15:33:54 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 10 Feb 2007 09:33:54 -0500
Subject: [R] Help in using multcomp.
In-Reply-To: <409691.92823.qm@web32409.mail.mud.yahoo.com>
References: <409691.92823.qm@web32409.mail.mud.yahoo.com>
Message-ID: <45CDD7D2.5020500@optonline.net>

A Ezhil wrote:
> Hi Chuck,
> 
> Thank you very for this help. I am able to store the
> results. Now, I am facing the following problems:
> 
> 1. I am trying to extract only the p-values from 
> 
> dunres <- lapply(amod, function(x){summary(glht(x,
> linfct=mcp(f = contr)))})
> 
> but I am stuck. If I use dunres[[1]], it displays the
> results. But I don't know how to extract the p value
> from this. 

Ezhil:

str(summary(glht(amod[[1]], linfct=mcp(f = contr)))) suggests a way to
extract just the p values.  Try this:

library(multcomp)

dat <- matrix(rnorm(45), nrow=5, ncol=9)
f <- gl(3,3,9, label=c("C", "Tl", "T2"))

aof <- function(x) {
        m <- data.frame(f, x);
        aov(x ~ f, m)
}

amod <- apply(dat,1,aof)

my.pvals <- sapply(amod, function(x){summary(glht(x, linfct=mcp(f =
contr)))$test$pvalues})

rownames(my.pvals) <- rownames(contr)
colnames(my.pvals) <- paste("amod", 1:5, sep="")

my.pvals
               amod1     amod2     amod3     amod4     amod5
C - T1    0.03348242 0.3581771 0.9873633 0.9764219 0.9225445
C - T2    0.67794496 0.7138491 0.2183949 0.9962458 0.8439224
C - All T 0.10621039 0.4344881 0.4913578 0.9970689 0.8519888

> 2. If I want to get raw pvalues instead of adjusted
> ones, what should I do in summary(glht)? 

  See the test argument of summary.glht().  You could do something like
this:

my.pvals <- sapply(amod, function(x){summary(glht(x, linfct=mcp(f =
contr)), test = adjusted("none"))$test$pvalues})

rownames(my.pvals) <- rownames(contr)
colnames(my.pvals) <- paste("amod", 1:5, sep="")

my.pvals
               amod1     amod2     amod3     amod4     amod5
C - T1    0.01770489 0.2197886 0.8985143 0.8611198 0.7466660
C - T2    0.46651736 0.4994115 0.1278132 0.9448342 0.6364631
C - All T 0.05953715 0.2733017 0.3149223 0.9512189 0.6464735

hope this helps,

Chuck Cleland

> Thanks again for your help. I look forward to your
> reply.
> 
> Kind regards,
> Ezhil
> 
> 
> --- Chuck Cleland <ccleland at optonline.net> wrote:
> 
>> A Ezhil wrote:
>>> Hi All,
>>>
>>> I am trying use 'multcomp' for multiple
>> comparisons
>>> after my ANOVA analysis. I have used the following
>>> code to do ANOVA:
>>>
>>> dat <- matrix(rnorm(45), nrow=5, ncol=9)
>>> f <- gl(3,3,9, label=c("C", "Tl", "T2"))
>>>
>>> aof <- function(x) {
>>>         m <- data.frame(f, x);
>>>         aov(x ~ f, m)
>>> }
>>> amod <- apply(dat,1,aof)
>>>
>>> Now, how can I use 'glht' for the above amod. I
>> know
>>> that I cannot use simply 
>>>
>>> glht(amod, linfct = mcp(f = "Dunnett")). 
>>   Since amod is a list of models rather than one
>> model, do you want
>> something like this?
>>
>> lapply(amod, function(x){summary(glht(x, linfct =
>> mcp(f = "Dunnett")))})
>>
>>> Also, if I want to use Dunnett for comparing C vs
>> (T1
>>> and T2), how can I specify this in the glht
>> function.
>>
>>   How about doing it with user-defined contrasts?
>>
>> contr <- rbind("C - T1   " = c(-1, 1, 0),
>>                "C - T2   " = c(-1, 0, 1),
>>                "C - All T" = c(-1,.5,.5))
>>
>> lapply(amod, function(x){summary(glht(x, linfct =
>> mcp(f = contr)))})
>>
>>> Thanks in advance. 
>>> Regards,
>>> Ezhil
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained,
>> reproducible code.
>>
>> -- 
>> Chuck Cleland, Ph.D.
>> NDRI, Inc.
>> 71 West 23rd Street, 8th floor
>> New York, NY 10010
>> tel: (212) 845-4495 (Tu, Th)
>> tel: (732) 512-0171 (M, W, F)
>> fax: (917) 438-0894
>>
> 
> 
> 
>  
> ____________________________________________________________________________________
> Need a quick answer? Get one in minutes from people who know.
> Ask your question on www.Answers.yahoo.com

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ezhil02 at yahoo.com  Sat Feb 10 16:15:40 2007
From: ezhil02 at yahoo.com (A Ezhil)
Date: Sat, 10 Feb 2007 07:15:40 -0800 (PST)
Subject: [R] Help in using multcomp.
In-Reply-To: <45CDD7D2.5020500@optonline.net>
Message-ID: <68549.76397.qm@web32412.mail.mud.yahoo.com>

Hi Chuck,

Thank you very much for this is help. It is fantastic
and the code works perfectly.

Thanks again, Chuck.

Kind regards,
Ezhil
  
--- Chuck Cleland <ccleland at optonline.net> wrote:

> A Ezhil wrote:
> > Hi Chuck,
> > 
> > Thank you very for this help. I am able to store
> the
> > results. Now, I am facing the following problems:
> > 
> > 1. I am trying to extract only the p-values from 
> > 
> > dunres <- lapply(amod, function(x){summary(glht(x,
> > linfct=mcp(f = contr)))})
> > 
> > but I am stuck. If I use dunres[[1]], it displays
> the
> > results. But I don't know how to extract the p
> value
> > from this. 
> 
> Ezhil:
> 
> str(summary(glht(amod[[1]], linfct=mcp(f = contr))))
> suggests a way to
> extract just the p values.  Try this:
> 
> library(multcomp)
> 
> dat <- matrix(rnorm(45), nrow=5, ncol=9)
> f <- gl(3,3,9, label=c("C", "Tl", "T2"))
> 
> aof <- function(x) {
>         m <- data.frame(f, x);
>         aov(x ~ f, m)
> }
> 
> amod <- apply(dat,1,aof)
> 
> my.pvals <- sapply(amod, function(x){summary(glht(x,
> linfct=mcp(f =
> contr)))$test$pvalues})
> 
> rownames(my.pvals) <- rownames(contr)
> colnames(my.pvals) <- paste("amod", 1:5, sep="")
> 
> my.pvals
>                amod1     amod2     amod3     amod4  
>   amod5
> C - T1    0.03348242 0.3581771 0.9873633 0.9764219
> 0.9225445
> C - T2    0.67794496 0.7138491 0.2183949 0.9962458
> 0.8439224
> C - All T 0.10621039 0.4344881 0.4913578 0.9970689
> 0.8519888
> 
> > 2. If I want to get raw pvalues instead of
> adjusted
> > ones, what should I do in summary(glht)? 
> 
>   See the test argument of summary.glht().  You
> could do something like
> this:
> 
> my.pvals <- sapply(amod, function(x){summary(glht(x,
> linfct=mcp(f =
> contr)), test = adjusted("none"))$test$pvalues})
> 
> rownames(my.pvals) <- rownames(contr)
> colnames(my.pvals) <- paste("amod", 1:5, sep="")
> 
> my.pvals
>                amod1     amod2     amod3     amod4  
>   amod5
> C - T1    0.01770489 0.2197886 0.8985143 0.8611198
> 0.7466660
> C - T2    0.46651736 0.4994115 0.1278132 0.9448342
> 0.6364631
> C - All T 0.05953715 0.2733017 0.3149223 0.9512189
> 0.6464735
> 
> hope this helps,
> 
> Chuck Cleland
> 
> > Thanks again for your help. I look forward to your
> > reply.
> > 
> > Kind regards,
> > Ezhil
> > 
> > 
> > --- Chuck Cleland <ccleland at optonline.net> wrote:
> > 
> >> A Ezhil wrote:
> >>> Hi All,
> >>>
> >>> I am trying use 'multcomp' for multiple
> >> comparisons
> >>> after my ANOVA analysis. I have used the
> following
> >>> code to do ANOVA:
> >>>
> >>> dat <- matrix(rnorm(45), nrow=5, ncol=9)
> >>> f <- gl(3,3,9, label=c("C", "Tl", "T2"))
> >>>
> >>> aof <- function(x) {
> >>>         m <- data.frame(f, x);
> >>>         aov(x ~ f, m)
> >>> }
> >>> amod <- apply(dat,1,aof)
> >>>
> >>> Now, how can I use 'glht' for the above amod. I
> >> know
> >>> that I cannot use simply 
> >>>
> >>> glht(amod, linfct = mcp(f = "Dunnett")). 
> >>   Since amod is a list of models rather than one
> >> model, do you want
> >> something like this?
> >>
> >> lapply(amod, function(x){summary(glht(x, linfct =
> >> mcp(f = "Dunnett")))})
> >>
> >>> Also, if I want to use Dunnett for comparing C
> vs
> >> (T1
> >>> and T2), how can I specify this in the glht
> >> function.
> >>
> >>   How about doing it with user-defined contrasts?
> >>
> >> contr <- rbind("C - T1   " = c(-1, 1, 0),
> >>                "C - T2   " = c(-1, 0, 1),
> >>                "C - All T" = c(-1,.5,.5))
> >>
> >> lapply(amod, function(x){summary(glht(x, linfct =
> >> mcp(f = contr)))})
> >>
> >>> Thanks in advance. 
> >>> Regards,
> >>> Ezhil
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained,
> >> reproducible code.
> >>
> >> -- 
> >> Chuck Cleland, Ph.D.
> >> NDRI, Inc.
> >> 71 West 23rd Street, 8th floor
> >> New York, NY 10010
> >> tel: (212) 845-4495 (Tu, Th)
> >> tel: (732) 512-0171 (M, W, F)
> >> fax: (917) 438-0894
> >>
> > 
> > 
> > 
> >  
> >
>
____________________________________________________________________________________
> > Need a quick answer? Get one in minutes from
> people who know.
> > Ask your question on www.Answers.yahoo.com
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
> 



 
____________________________________________________________________________________
Get your own web address.


From duncan at wald.ucdavis.edu  Sat Feb 10 16:18:30 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Sat, 10 Feb 2007 07:18:30 -0800
Subject: [R] XML and str
In-Reply-To: <17866.12346.900084.986303@stat.math.ethz.ch>
References: <1170868736.27202.159.camel@fordpc.signal.qinetiq.com>
	<17866.12346.900084.986303@stat.math.ethz.ch>
Message-ID: <45CDE246.8000903@wald.ucdavis.edu>



Martin Maechler wrote:
>>>>>> "Ashley" == Ashley Ford <ford at signal.QinetiQ.com>
>>>>>>     on Wed, 07 Feb 2007 17:18:56 +0000 writes:
> 
>     Ashley> If I read in an .xml file eg with 
> 
>     >> xeg <- xmlTreeParse(system.file("exampleData", "test.xml",
>                                        package="XML"))
> 
>     Ashley> It appears to be OK however examining it with str() gives an apparent
>     Ashley> error
> 
>     >> str(xeg, 2)
>     Ashley> List of 2
>     Ashley> $ doc:List of 3
>     Ashley> ..$ file    : list()
>     Ashley> .. ..- attr(*, "class")= chr [1:2] "XMLComment" "XMLNode"
>     Ashley> ..$ version :List of 4
>     Ashley> .. ..- attr(*, "class")= chr "XMLNode"
>     Ashley> ..$ children:Error in obj$children[[...]] : subscript out of bounds
> 
>     Ashley> I am unsure if this is a feature or a bug and if the latter whether it
>     Ashley> is in XML or str, it is not causing a problem but I would like to
>     Ashley> understand what is happening, any ideas ?
> 
> Yes -  thank you for providing a well-reproducible example.
> After setting  
>       options(error = recover)
> 
> I do
> 
>    > obj <- xeg$doc
>    > mode(obj)     # "list"
>    [1] "list"
>    > is.list(obj)  # TRUE
>    [1] TRUE
>    > length(obj)   # 3
>    [1] 3
>    > obj[[3]]      # ---> the error you see above.
>    Error in obj$children[[...]] : subscript out of bounds
> 
>    Enter a frame number, or 0 to exit   
> 
>    1: obj[[3]]
>    2: `[[.XMLDocumentContent`(obj, 3)
> 
>    Selection: 0
> 
>    > obj$children  # works, should be identical to obj[[3]]
>    $comment
>    <!--A comment-->
> 
>    $foo
>    <foo x="1">
>     <element attrib1="my value"/>
>    ......
> 
> This shows that the XML package implements the "[[" method
> wrongly IMHO and also inconsistently with the "$" method.
> 
>>From a strict OOP view, the XML author could argue that
> this is not a bug in XML but rather str() which assumes that
> x[[length(x)]] works for objects of mode "list" even when they
> are not of *class* "list", but I hope he would still rather
> consider changing [[.XMLDocumentContent ...
> 


More likely, the appropriate fix is to have
length() return the relevant value.
I even recall considering this at the time of writing
the package initially.  But that was back in 1999/2000
and S4 and R/S-Plus compatibility were not what they
are now.  It could be changed.  Not certain when I will
get a chance.

 D.


> Martin
> 
>     Ashley> examining components eg 
>     >> str(xeg$doc$children,2)
> 
>     Ashley> List of 2
>     Ashley> $ comment: list()
>     Ashley> ..- attr(*, "class")= chr [1:2] "XMLComment" "XMLNode"
>     Ashley> etc 
> 
>     Ashley> is OK.
> 
>     Ashley> XML Version 1.4-1, 
>     Ashley> same behaviour on Windows and Linux, R version 2.4.1 (2006-12-18)
> 
> 
> 
> 
>     Ashley> The information contained in this E-Mail and any subsequent
>     Ashley> correspondence is private and is intended solely for the intended
>     Ashley> recipient(s).  The information in this communication may be confidential
>     Ashley> and/or legally privileged.  Nothing in this e-mail is intended to
>     Ashley> conclude a contract on behalf of QinetiQ or make QinetiQ subject to any
>     Ashley> other legally binding commitments, unless the e-mail contains an express
>     Ashley> statement to the contrary or incorporates a formal Purchase Order.
> 
>     Ashley> For those other than the recipient any disclosure, copying,
>     Ashley> distribution, or any action taken or omitted to be taken in reliance on
>     Ashley> such information is prohibited and may be unlawful.
> 
>     Ashley> Emails and other electronic communication with QinetiQ may be monitored
>     Ashley> and recorded for business purposes including security, audit and
>     Ashley> archival purposes.  Any response to this email indicates consent to
>     Ashley> this.
> 
>     Ashley> Telephone calls to QinetiQ may be monitored or recorded for quality
>     Ashley> control, security and other business purposes.
> 
>     Ashley> QinetiQ Group plc,
> 
>     Ashley> Company Registration No: 4586941,  
> 
>     Ashley> Registered office: 85 Buckingham Gate, London SW1E 6PD
> 
>     Ashley> ______________________________________________
>     Ashley> R-help at stat.math.ethz.ch mailing list
>     Ashley> https://stat.ethz.ch/mailman/listinfo/r-help
>     Ashley> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>     Ashley> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Sat Feb 10 16:54:23 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 10 Feb 2007 16:54:23 +0100
Subject: [R] XML and str
In-Reply-To: <45CDE246.8000903@wald.ucdavis.edu>
References: <1170868736.27202.159.camel@fordpc.signal.qinetiq.com>
	<17866.12346.900084.986303@stat.math.ethz.ch>
	<45CDE246.8000903@wald.ucdavis.edu>
Message-ID: <17869.60079.427109.670641@stat.math.ethz.ch>

>>>>> "DTL" == Duncan Temple Lang <duncan at wald.ucdavis.edu>
>>>>>     on Sat, 10 Feb 2007 07:18:30 -0800 writes:

    DTL> Martin Maechler wrote:
    >>>>>>> "Ashley" == Ashley Ford <ford at signal.QinetiQ.com>
    >>>>>>> on Wed, 07 Feb 2007 17:18:56 +0000 writes:
    >> 
    Ashley> If I read in an .xml file eg with 
    >> 
    >> >> xeg <- xmlTreeParse(system.file("exampleData", "test.xml",
    >> package="XML"))
    >> 
    Ashley> It appears to be OK however examining it with str() gives an apparent
    Ashley> error
    >> 
    >> >> str(xeg, 2)
    Ashley> List of 2
    Ashley> $ doc:List of 3
    Ashley> ..$ file    : list()
    Ashley> .. ..- attr(*, "class")= chr [1:2] "XMLComment" "XMLNode"
    Ashley> ..$ version :List of 4
    Ashley> .. ..- attr(*, "class")= chr "XMLNode"
    Ashley> ..$ children:Error in obj$children[[...]] : subscript out of bounds
    >> 
    Ashley> I am unsure if this is a feature or a bug and if the latter whether it
    Ashley> is in XML or str, it is not causing a problem but I would like to
    Ashley> understand what is happening, any ideas ?
    >> 
    >> Yes -  thank you for providing a well-reproducible example.
    >> After setting  
    >> options(error = recover)
    >> 
    >> I do
    >> 
    >> > obj <- xeg$doc
    >> > mode(obj)     # "list"
    >> [1] "list"
    >> > is.list(obj)  # TRUE
    >> [1] TRUE
    >> > length(obj)   # 3
    >> [1] 3
    >> > obj[[3]]      # ---> the error you see above.
    >> Error in obj$children[[...]] : subscript out of bounds
    >> 
    >> Enter a frame number, or 0 to exit   
    >> 
    >> 1: obj[[3]]
    >> 2: `[[.XMLDocumentContent`(obj, 3)
    >> 
    >> Selection: 0
    >> 
    >> > obj$children  # works, should be identical to obj[[3]]
    >> $comment
    >> <!--A comment-->
    >> 
    >> $foo
    >> <foo x="1">
    >> <element attrib1="my value"/>
    >> ......
    >> 
    >> This shows that the XML package implements the "[[" method
    >> wrongly IMHO and also inconsistently with the "$" method.
    >> 
    >>> From a strict OOP view, the XML author could argue that
    >> this is not a bug in XML but rather str() which assumes that
    >> x[[length(x)]] works for objects of mode "list" even when they
    >> are not of *class* "list", but I hope he would still rather
    >> consider changing [[.XMLDocumentContent ...
    >> 


    DTL> More likely, the appropriate fix is to have
    DTL> length() return the relevant value.

Hmm. 

  > library(XML)
  > xeg <- xmlTreeParse(system.file("exampleData", "test.xml", package= "XML"))
  > obj <- xeg$doc
  > mode(obj)     # "list"
  [1] "list"
  > is.list(obj)  # TRUE
  [1] TRUE
  > length(obj)   # 3
  [1] 3
  > obj[[3]]      # ---> the error you see above.
  Error in obj$children[[...]] : subscript out of bounds
  > names(obj)
  [1] "file"     "version"  "children"
  > class(obj)
  [1] "XMLDocumentContent"
  > methods(class=class(obj))
  [1] xmlApply.XMLDocumentContent*  [[.XMLDocumentContent*       
  [3] xmlRoot.XMLDocumentContent*   xmlSApply.XMLDocumentContent*

  > XML:::`[[.XMLDocumentContent`
  function (obj, ...) 
  {
      obj$children[[...]]
  }
  <environment: namespace:XML>

so  length(obj) is 3 and obj is a simple S3 object
which is just a list with 3 named components,
Do you really want to define  length(.) to also return the
length of obj$children instead of the length() of the list
itself?   
With that you'd have your XMLDocumentContent objects ``look''
like lists with three named components on one hand
(and help(xmlTreeParse) does mention these components)
but behave in other contexts as if it was just its own component
'obj$children'.   Of course you then should also define 
  print.XMLDocumentContent() and
  str.XMLDocumentContent()   accordingly, 
so users would barely know about the "file" and "version"
component of 'obj'.
But is this really desirable ?
With the above "[[.XMLDoc..."  you break the basic S-language
premise of  "[[" and "$" to behave accordingly.

You could solve "everything" elegantly if you used S4 instead of S3
classes, since there's no defined correspondence between slot
access and "[[" (and yes, then (with S4), I'd agree that 

setMethod("length", "XMLDocumentContent", 
          function(x) length(x at children))

would be needed too -- and fine.

Martin

    DTL> I even recall considering this at the time of writing
    DTL> the package initially.  But that was back in 1999/2000
    DTL> and S4 and R/S-Plus compatibility were not what they
    DTL> are now.  It could be changed.  Not certain when I will
    DTL> get a chance.


    Ashley> examining components eg 
    >> >> str(xeg$doc$children,2)
    >> 
    Ashley> List of 2
    Ashley> $ comment: list()
    Ashley> ..- attr(*, "class")= chr [1:2] "XMLComment" "XMLNode"
    Ashley> etc 
    >> 
    Ashley> is OK.
    >> 
    Ashley> XML Version 1.4-1, 
    Ashley> same behaviour on Windows and Linux, R version 2.4.1 (2006-12-18)
    >>


From muenchen at utk.edu  Sat Feb 10 17:12:52 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Sat, 10 Feb 2007 11:12:52 -0500
Subject: [R] JGR data editor question
Message-ID: <7270AEC73132194E8BC0EE06B35D93D879C154@UTKFSVS3.utk.tennessee.edu>

Hi All,

I'm learning JGR 1.4-15 with R 2.4.1 in Windows XP (all patches
applied). JGR looks great but I'm having trouble getting the data editor
to save my results. I don't see anything in R-help about it. Here are
the steps I followed:

1. I chose "Tools>Object Browser" & double-clicked on a data frame,
"mydata". 
2. A spreadsheet editor popped up and allowed me to make changes. 
3. I clicked "Update" at the bottom right of the data editor screen. 
4. It asked, "Export to R?" and has "Export as: mydata" filled in. 
5. I clicked "Yes" and then closed the window by clicking the usual [X]
in the top right corner. 
6. Double-clicking the data file again opened it back up but the changes
were gone. 

Am I missing a step?

Thanks,
Bob

=========================================================
Bob Muenchen (pronounced Min'-chen), Manager 
Statistical Consulting Center
U of TN Office of Information Technology
200 Stokely Management Center, Knoxville, TN 37996-0520
Voice: (865) 974-5230 
FAX: (865) 974-4810
Email: muenchen at utk.edu
Web: http://oit.utk.edu/scc, 
News: http://listserv.utk.edu/archives/statnews.html


From deepayan.sarkar at gmail.com  Sat Feb 10 17:34:02 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 10 Feb 2007 10:34:02 -0600
Subject: [R] How to add the variable name to a qqplot or densityplot in
	the diagonal of an splom?
In-Reply-To: <DCAC7467-5BC1-4D17-8E86-3113BDA19D67@virginia.edu>
References: <DCAC7467-5BC1-4D17-8E86-3113BDA19D67@virginia.edu>
Message-ID: <eb555e660702100834h3ed454f3ub88dd4af7edd3b95@mail.gmail.com>

On 2/9/07, Michael Kubovy <kubovy at virginia.edu> wrote:
> splom() doesn't complain here, but writes no names in the diagonal
> boxes. What am I missing?
> I believe that I need to add something like grid.text(x, ...) to the
> diagonal panel, but I don't know how to get it cycle through the
> column labels. And should
>      varname.col = 'blue', varname.cex = 1
> be inside the diag.panel() function?

These are passed on to panel.pairs, which in turn passes these on to
diag.panel. Whether your diag.panel uses it or not is up to you (see
below for an example that does).

>
> splom(szw[, n], pscales = 0,

<obligatory rant>
Simply using a built in data set, like 'iris', instead of undefined
variables 'szw' and 'n' would have made this a reproducible example
</obligatory rant>

>      diag.panel = function(x, ...){
>          panel.qqmathline(x, ...)
>          panel.qqmath(x, ...)
>      },

change this to

     diag.panel = function(x, ...){
         panel.qqmathline(x, ...)
         panel.qqmath(x, ...)
         diag.panel.splom(x = x, ...)
     },

You could also use grid.text of course, but you need to capture the
'varname' argument that panel.pairs passes on to diag.panel.splom.


>      lower.panel = function(x, y, ...){
>          panel.xyplot(x, y, ..., col = 'lightblue')
>          panel.loess(x, y, ..., col = 'red')
>      },
>      upper.panel = function(x, y, ...){
>          panel.abline(lm(y~x),...)
>          grid.text(round(cor(x, y, use = 'pairwise.complete.obs'), 2),
>              x = unit(1, 'mm'),
>              y = unit(1, 'npc') - unit(1, 'mm'),
>              just = c('left', 'top'),
>              gp = gpar(fontsize = 10))
>      },
>      varname.col = 'blue', varname.cex = 1
> )
>
>
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jporzak at gmail.com  Sat Feb 10 17:45:59 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Sat, 10 Feb 2007 08:45:59 -0800
Subject: [R] JGR data editor question
In-Reply-To: <7270AEC73132194E8BC0EE06B35D93D879C154@UTKFSVS3.utk.tennessee.edu>
References: <7270AEC73132194E8BC0EE06B35D93D879C154@UTKFSVS3.utk.tennessee.edu>
Message-ID: <2a9c000c0702100845p7f86ee49lb43839bba3e4c4c2@mail.gmail.com>

Hi Bob,

I can not reproduce your problem, with possible exception in your step 2:
In data editor, you need to click off of the last cell you edited for
the changes to "take"

On 2/10/07, Muenchen, Robert A (Bob) <muenchen at utk.edu> wrote:
> Hi All,
>
> I'm learning JGR 1.4-15 with R 2.4.1 in Windows XP (all patches
> applied). JGR looks great but I'm having trouble getting the data editor
> to save my results. I don't see anything in R-help about it. Here are
> the steps I followed:
>
> 1. I chose "Tools>Object Browser" & double-clicked on a data frame,
> "mydata".
> 2. A spreadsheet editor popped up and allowed me to make changes.
> 3. I clicked "Update" at the bottom right of the data editor screen.
> 4. It asked, "Export to R?" and has "Export as: mydata" filled in.
> 5. I clicked "Yes" and then closed the window by clicking the usual [X]
> in the top right corner.
> 6. Double-clicking the data file again opened it back up but the changes
> were gone.
>
> Am I missing a step?
>
> Thanks,
> Bob
>
> =========================================================
> Bob Muenchen (pronounced Min'-chen), Manager
> Statistical Consulting Center
> U of TN Office of Information Technology
> 200 Stokely Management Center, Knoxville, TN 37996-0520
> Voice: (865) 974-5230
> FAX: (865) 974-4810
> Email: muenchen at utk.edu
> Web: http://oit.utk.edu/scc,
> News: http://listserv.utk.edu/archives/statnews.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
HTH,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA
http://www.linkedin.com/in/jimporzak


From muenchen at utk.edu  Sat Feb 10 18:39:55 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Sat, 10 Feb 2007 12:39:55 -0500
Subject: [R] JGR data editor question
In-Reply-To: <2a9c000c0702100845p7f86ee49lb43839bba3e4c4c2@mail.gmail.com>
References: <7270AEC73132194E8BC0EE06B35D93D879C154@UTKFSVS3.utk.tennessee.edu>
	<2a9c000c0702100845p7f86ee49lb43839bba3e4c4c2@mail.gmail.com>
Message-ID: <7270AEC73132194E8BC0EE06B35D93D879C155@UTKFSVS3.utk.tennessee.edu>

That's it! I tried an absurd number of variations, but never that! I had
only changed one value and never left the cell. I assumed hitting Enter
would do it. Thanks! -Bob

=========================================================
Bob Muenchen (pronounced Min'-chen), Manager 
Statistical Consulting Center
U of TN Office of Information Technology
200 Stokely Management Center, Knoxville, TN 37996-0520
Voice: (865) 974-5230 
FAX: (865) 974-4810
Email: muenchen at utk.edu
Web: http://oit.utk.edu/scc, 
News: http://listserv.utk.edu/archives/statnews.html
=========================================================


-----Original Message-----
From: Jim Porzak [mailto:jporzak at gmail.com] 
Sent: Saturday, February 10, 2007 11:46 AM
To: Muenchen, Robert A (Bob)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] JGR data editor question

Hi Bob,

I can not reproduce your problem, with possible exception in your step
2:
In data editor, you need to click off of the last cell you edited for
the changes to "take"

On 2/10/07, Muenchen, Robert A (Bob) <muenchen at utk.edu> wrote:
> Hi All,
>
> I'm learning JGR 1.4-15 with R 2.4.1 in Windows XP (all patches
> applied). JGR looks great but I'm having trouble getting the data
editor
> to save my results. I don't see anything in R-help about it. Here are
> the steps I followed:
>
> 1. I chose "Tools>Object Browser" & double-clicked on a data frame,
> "mydata".
> 2. A spreadsheet editor popped up and allowed me to make changes.
> 3. I clicked "Update" at the bottom right of the data editor screen.
> 4. It asked, "Export to R?" and has "Export as: mydata" filled in.
> 5. I clicked "Yes" and then closed the window by clicking the usual
[X]
> in the top right corner.
> 6. Double-clicking the data file again opened it back up but the
changes
> were gone.
>
> Am I missing a step?
>
> Thanks,
> Bob
>
> =========================================================
> Bob Muenchen (pronounced Min'-chen), Manager
> Statistical Consulting Center
> U of TN Office of Information Technology
> 200 Stokely Management Center, Knoxville, TN 37996-0520
> Voice: (865) 974-5230
> FAX: (865) 974-4810
> Email: muenchen at utk.edu
> Web: http://oit.utk.edu/scc,
> News: http://listserv.utk.edu/archives/statnews.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
HTH,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA
http://www.linkedin.com/in/jimporzak


From semgogo at hotmail.com  Sat Feb 10 18:57:41 2007
From: semgogo at hotmail.com (semgogo sem)
Date: Sat, 10 Feb 2007 17:57:41 +0000
Subject: [R] This web site make writing R easier
Message-ID: <BAY23-F17F6B36331E917CE3A543CB4930@phx.gbl>

Hello, I sugguest you try a web site, www.mycodegarden.org. It offer many 
functionalities such as autocomplete, intellisense and syntax highlighting 
that will save you lots of time on writing R procedure.

Open  http://www.mycodegarden.org/index.php?title=PlayPage&action=edit,
press intellisense button and select R language, enjoy it. 

best regard!

_________________________________________________________________
?????????????????????????????? MSN Hotmail??  http://www.hotmail.com


From ivowel at gmail.com  Sat Feb 10 19:02:46 2007
From: ivowel at gmail.com (ivo welch)
Date: Sat, 10 Feb 2007 13:02:46 -0500
Subject: [R] practical memory limits
Message-ID: <50d1c22d0702101002la59f502gf7228703369fbda1@mail.gmail.com>

Dear R experts:  I want to learn what the practically useful memory
limits are for good work with R.

(My specific problem is that I want work with daily stock returns.
In ASCII, the data set is about 72 million returns, that would have to
go into a sparse matrix (not all stocks exist for the whole series).
As a guess, this will consume about 700MB.  My main use will be linear
operations---regressions, means, etc.)

I am on linux, so I can create swap space, but I am concerned that the
thrashing will be so bad that the computer will become worthless.  In
fact, the last time I used it was over 3 years ago.  Since then, I
have just turned it off.

I have 2GB of RAM right now, and could upgrade this to 4GB.

Are there some general guidelines as to what the relationship between
data sets and memory should be under R?  I know this will vary with
the task involved, but some guidance would be better than none.

regards,

/iaw


From mwkimpel at gmail.com  Sat Feb 10 20:10:32 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Sat, 10 Feb 2007 14:10:32 -0500
Subject: [R] This web site make writing R easier
In-Reply-To: <BAY23-F17F6B36331E917CE3A543CB4930@phx.gbl>
References: <BAY23-F17F6B36331E917CE3A543CB4930@phx.gbl>
Message-ID: <45CE18A8.1020903@gmail.com>

When I visited the site it seems to be a work in the pre-alpha testing
phase. Nothing is functional.

Mark

semgogo sem wrote:
> Hello, I sugguest you try a web site, www.mycodegarden.org. It offer 
> many functionalities such as autocomplete, intellisense and syntax 
> highlighting that will save you lots of time on writing R procedure.
> 
> Open  http://www.mycodegarden.org/index.php?title=PlayPage&action=edit,
> press intellisense button and select R language, enjoy it.
> best regard!
> 
> _________________________________________________________________
> ?????????????????????????????? MSN Hotmail??  http://www.hotmail.com
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Mark W. Kimpel MD
Neuroinformatics
Department of Psychiatry
Indiana University School of Medicine


From jessilbrown at gmail.com  Sat Feb 10 20:45:39 2007
From: jessilbrown at gmail.com (Jessi Brown)
Date: Sat, 10 Feb 2007 11:45:39 -0800
Subject: [R] error using user-defined link function with mixed models (LMER)
Message-ID: <371904a90702101145h2f3d58afga3ec3ddf5ec78516@mail.gmail.com>

Greetings, everyone. I've been trying to analyze bird nest survival
data using generalized linear mixed models (because we documented
several consecutive nesting attempts by the same individuals; i.e.
repeated measures data) and have been unable to persuade the various
GLMM models to work with my user-defined link function. Actually,
glmmPQL seems to work, but as I want to evaluate a suite of competing
models, I'd like to use ML or REML estimation methods in order to end
up with meaningful log-likelihoods.

Here's the link function I use:
logexp <- function(days = 1)
{
    linkfun <- function(mu) qlogis(mu^(1/days))
    linkinv <- function(eta) plogis(eta)^days
    mu.eta <- function(eta)
      days*.Call("logit_mu_eta", eta,
       PACKAGE = "stats")*plogis(eta)^(days-1)
    valideta <- function(eta) TRUE
    link <- paste("logexp(", days, ")", sep="")
    structure(list(linkfun = linkfun, linkinv = linkinv,
       mu.eta = mu.eta, valideta = valideta, name = link),
        class = "link-glm")
}

# Modified binomial family function (that allows logexp link function)
logexposure<-function (link="logexp",ExposureDays) {
    variance <- function(mu) mu * (1 - mu)
    validmu <- function(mu) all(mu > 0) && all(mu < 1)
    dev.resids <- function(y, mu, wt) .Call("binomial_dev_resids",
        y, mu, wt, PACKAGE = "stats")
    aic <- function(y, n, mu, wt, dev) {
        m <- if (any(n > 1))
            n
        else wt
        -2 * sum(ifelse(m > 0, (wt/m), 0) * dbinom(round(m *
            y), round(m), mu, log = TRUE))
    }
    initialize <- expression({
        if (NCOL(y) == 1) {
            if (is.factor(y)) y <- y != levels(y)[1]
            n <- rep.int(1, nobs)
            if (any(y < 0 | y > 1)) stop("y values must be 0 <= y <= 1")
            mustart <- (weights * y + 0.5)/(weights + 1)
            m <- weights * y
            if (any(abs(m - round(m)) > 0.001))
                 warning("non-integer successes in a binomial glm!")
        } else if (NCOL(y) == 2) {
            if (any(abs(y - round(y)) > 0.001))
                warning("non-integer counts in a binomial glm!")
            n <- y[, 1] + y[, 2]
            y <- ifelse(n == 0, 0, y[, 1]/n)
            weights <- weights * n
            mustart <- (n * y + 0.5)/(n + 1)
        } else stop("for the binomial family,",
                " y must be a vector of 0 and 1's\n",
            "or a 2 column matrix where col 1 is",
            " no. successes and col 2 is no. failures")
    })
    structure(list(family="binomial", link=logexp(ExposureDays),
       linkfun=logexp(ExposureDays)$linkfun,
       linkinv=logexp(ExposureDays)$linkinv, variance=variance,
       dev.resids=dev.resids, aic=aic,
       mu.eta=logexp(ExposureDays)$mu.eta, initialize=initialize,
       validmu=validmu, valideta=logexp$valideta), class = "family")
}



Now, here's how it works in a GLM:

> apfa.glm.1<-glm(Success~MeanAge+I(MeanAge^2), family=logexposure(link="logexp", ExposureDays=apfa4$Days), data=apfa4)
> summary(apfa.glm.1)

Call:
glm(formula = Success ~ MeanAge + I(MeanAge^2), family =
logexposure(link = "logexp",
    ExposureDays = apfa4$Days), data = apfa4)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-3.1525   0.2802   0.3637   0.4291   0.7599

Coefficients:
               Estimate Std. Error z value Pr(>|z|)
(Intercept)   5.5594830  0.6085542   9.136   <2e-16 ***
MeanAge      -0.0908251  0.0407218  -2.230   0.0257 *
I(MeanAge^2)  0.0014926  0.0006104   2.445   0.0145 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 323.58  on 661  degrees of freedom
Residual deviance: 285.65  on 659  degrees of freedom
AIC: 291.65

Number of Fisher Scoring iterations: 6



Next, here's the results of a glmmPQL run:

> apfa.glmm.1<-glmmPQL(Success~MeanAge+I(MeanAge^2), random=~1|Territory, family=logexposure(link="logexp", ExposureDays=apfa4$Days), data=apfa4)
iteration 1
> summary(apfa.glmm.1)
Linear mixed-effects model fit by maximum likelihood
 Data: apfa4
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Territory
         (Intercept) Residual
StdDev: 0.0003431913 1.051947

Variance function:
 Structure: fixed weights
 Formula: ~invwt
Fixed effects: Success ~ MeanAge + I(MeanAge^2)
                 Value Std.Error  DF   t-value p-value
(Intercept)   5.559466 0.6416221 624  8.664705  0.0000
MeanAge      -0.090824 0.0429346 624 -2.115397  0.0348
I(MeanAge^2)  0.001493 0.0006436 624  2.319090  0.0207
 Correlation:
             (Intr) MeanAg
MeanAge      -0.927
I(MeanAge^2)  0.826 -0.968

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-11.3646020   0.1901969   0.2485473   0.2951632   0.5499915

Number of Observations: 662
Number of Groups: 36



Finally, here's what happens when I try to run an LMER model (same
error messages no matter which estimation method I choose):

> apfa.lmer.1<-lmer(Success~MeanAge+I(MeanAge^2)+(1|Territory), data=apfa4, family=logexposure(link="logexp", ExposureDays=apfa4$Days), method="Laplace")
> summary(apfa.lmer.1)
Error in if (any(sd < 0)) return("'sd' slot has negative entries") :
	missing value where TRUE/FALSE needed
> names(apfa.lmer.1)
NULL


So, does anyone have any idea as to whether the problem is in the
user-defined link function as written, or have any thoughts about how
to get around this problem? If LMER and LME can't do it, could there
be some way to trick the glmmML function into accepting the
user-defined link function?

Thank you in advance for any help or advice.

cheers, Jessi Brown


From kubovy at virginia.edu  Sat Feb 10 20:58:58 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 10 Feb 2007 14:58:58 -0500
Subject: [R] OT: question about transformation
Message-ID: <D8740F22-A741-422A-B664-BE6329B03374@virginia.edu>

If x is a r.v.
	1/log(x) ~ N (approximately)
what does that tell me about the distribution of x (i.e., is it a  
standard distribution?)

Can you point me to a list where I would have appropriately posted this?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From bates at stat.wisc.edu  Sat Feb 10 21:40:16 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 10 Feb 2007 14:40:16 -0600
Subject: [R] error using user-defined link function with mixed models
	(LMER)
In-Reply-To: <371904a90702101145h2f3d58afga3ec3ddf5ec78516@mail.gmail.com>
References: <371904a90702101145h2f3d58afga3ec3ddf5ec78516@mail.gmail.com>
Message-ID: <40e66e0b0702101240sa6091bajed546e0863373147@mail.gmail.com>

On 2/10/07, Jessi Brown <jessilbrown at gmail.com> wrote:
> Greetings, everyone. I've been trying to analyze bird nest survival
> data using generalized linear mixed models (because we documented
> several consecutive nesting attempts by the same individuals; i.e.
> repeated measures data) and have been unable to persuade the various
> GLMM models to work with my user-defined link function. Actually,
> glmmPQL seems to work, but as I want to evaluate a suite of competing
> models, I'd like to use ML or REML estimation methods in order to end
> up with meaningful log-likelihoods.
>
> Here's the link function I use:
> logexp <- function(days = 1)
> {
>     linkfun <- function(mu) qlogis(mu^(1/days))
>     linkinv <- function(eta) plogis(eta)^days
>     mu.eta <- function(eta)
>       days*.Call("logit_mu_eta", eta,
>        PACKAGE = "stats")*plogis(eta)^(days-1)
>     valideta <- function(eta) TRUE
>     link <- paste("logexp(", days, ")", sep="")
>     structure(list(linkfun = linkfun, linkinv = linkinv,
>        mu.eta = mu.eta, valideta = valideta, name = link),
>         class = "link-glm")
> }
>
> # Modified binomial family function (that allows logexp link function)
> logexposure<-function (link="logexp",ExposureDays) {
>     variance <- function(mu) mu * (1 - mu)
>     validmu <- function(mu) all(mu > 0) && all(mu < 1)
>     dev.resids <- function(y, mu, wt) .Call("binomial_dev_resids",
>         y, mu, wt, PACKAGE = "stats")
>     aic <- function(y, n, mu, wt, dev) {
>         m <- if (any(n > 1))
>             n
>         else wt
>         -2 * sum(ifelse(m > 0, (wt/m), 0) * dbinom(round(m *
>             y), round(m), mu, log = TRUE))
>     }
>     initialize <- expression({
>         if (NCOL(y) == 1) {
>             if (is.factor(y)) y <- y != levels(y)[1]
>             n <- rep.int(1, nobs)
>             if (any(y < 0 | y > 1)) stop("y values must be 0 <= y <= 1")
>             mustart <- (weights * y + 0.5)/(weights + 1)
>             m <- weights * y
>             if (any(abs(m - round(m)) > 0.001))
>                  warning("non-integer successes in a binomial glm!")
>         } else if (NCOL(y) == 2) {
>             if (any(abs(y - round(y)) > 0.001))
>                 warning("non-integer counts in a binomial glm!")
>             n <- y[, 1] + y[, 2]
>             y <- ifelse(n == 0, 0, y[, 1]/n)
>             weights <- weights * n
>             mustart <- (n * y + 0.5)/(n + 1)
>         } else stop("for the binomial family,",
>                 " y must be a vector of 0 and 1's\n",
>             "or a 2 column matrix where col 1 is",
>             " no. successes and col 2 is no. failures")
>     })
>     structure(list(family="binomial", link=logexp(ExposureDays),
>        linkfun=logexp(ExposureDays)$linkfun,
>        linkinv=logexp(ExposureDays)$linkinv, variance=variance,
>        dev.resids=dev.resids, aic=aic,
>        mu.eta=logexp(ExposureDays)$mu.eta, initialize=initialize,
>        validmu=validmu, valideta=logexp$valideta), class = "family")
> }
>
>
>
> Now, here's how it works in a GLM:
>
> > apfa.glm.1<-glm(Success~MeanAge+I(MeanAge^2), family=logexposure(link="logexp", ExposureDays=apfa4$Days), data=apfa4)
> > summary(apfa.glm.1)
>
> Call:
> glm(formula = Success ~ MeanAge + I(MeanAge^2), family =
> logexposure(link = "logexp",
>     ExposureDays = apfa4$Days), data = apfa4)
>
> Deviance Residuals:
>     Min       1Q   Median       3Q      Max
> -3.1525   0.2802   0.3637   0.4291   0.7599
>
> Coefficients:
>                Estimate Std. Error z value Pr(>|z|)
> (Intercept)   5.5594830  0.6085542   9.136   <2e-16 ***
> MeanAge      -0.0908251  0.0407218  -2.230   0.0257 *
> I(MeanAge^2)  0.0014926  0.0006104   2.445   0.0145 *
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> (Dispersion parameter for binomial family taken to be 1)
>
>     Null deviance: 323.58  on 661  degrees of freedom
> Residual deviance: 285.65  on 659  degrees of freedom
> AIC: 291.65
>
> Number of Fisher Scoring iterations: 6
>
>
>
> Next, here's the results of a glmmPQL run:
>
> > apfa.glmm.1<-glmmPQL(Success~MeanAge+I(MeanAge^2), random=~1|Territory, family=logexposure(link="logexp", ExposureDays=apfa4$Days), data=apfa4)
> iteration 1
> > summary(apfa.glmm.1)
> Linear mixed-effects model fit by maximum likelihood
>  Data: apfa4
>   AIC BIC logLik
>    NA  NA     NA
>
> Random effects:
>  Formula: ~1 | Territory
>          (Intercept) Residual
> StdDev: 0.0003431913 1.051947
>
> Variance function:
>  Structure: fixed weights
>  Formula: ~invwt
> Fixed effects: Success ~ MeanAge + I(MeanAge^2)
>                  Value Std.Error  DF   t-value p-value
> (Intercept)   5.559466 0.6416221 624  8.664705  0.0000
> MeanAge      -0.090824 0.0429346 624 -2.115397  0.0348
> I(MeanAge^2)  0.001493 0.0006436 624  2.319090  0.0207
>  Correlation:
>              (Intr) MeanAg
> MeanAge      -0.927
> I(MeanAge^2)  0.826 -0.968
>
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3         Max
> -11.3646020   0.1901969   0.2485473   0.2951632   0.5499915
>
> Number of Observations: 662
> Number of Groups: 36
>
>
>
> Finally, here's what happens when I try to run an LMER model (same
> error messages no matter which estimation method I choose):
>
> > apfa.lmer.1<-lmer(Success~MeanAge+I(MeanAge^2)+(1|Territory), data=apfa4, family=logexposure(link="logexp", ExposureDays=apfa4$Days), method="Laplace")
> > summary(apfa.lmer.1)
> Error in if (any(sd < 0)) return("'sd' slot has negative entries") :
>         missing value where TRUE/FALSE needed
> > names(apfa.lmer.1)
> NULL

> So, does anyone have any idea as to whether the problem is in the
> user-defined link function as written, or have any thoughts about how
> to get around this problem? If LMER and LME can't do it, could there
> be some way to trick the glmmML function into accepting the
> user-defined link function?

lmer is designed to work with arbitrary families but I haven't done a
lot of testing outside the binomial and poisson families.

Try looking at the structure of an instance of your family and
comparing it to, say,

str(binomial())

Make sure that all the components are there and have the correct form.

The next step is to use verbose output so you can see the progress of
the iterations.  Add the optional argument control = list(msVerbose =
1) to your call to lmer.

Instead of immediately requesting a summary, use str(apfa.lmer.1) to
check the structure.  Again you may want to compare this description
to that from str applied to a similar fit using the binomial family.


From muenchen at utk.edu  Sat Feb 10 22:26:23 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Sat, 10 Feb 2007 16:26:23 -0500
Subject: [R] SAS, SPSS Product Comparison Table
Message-ID: <7270AEC73132194E8BC0EE06B35D93D879C15F@UTKFSVS3.utk.tennessee.edu>

Hi All,

My paper "R for SAS and SPSS Users" received a bit more of a reaction
than I expected. I posted the link
(http://oit.utk.edu/scc/RforSAS&SPSSusers.pdf) about 12 days ago on
R-help and the equivalent SAS and SPSS lists. Since then people have
downloaded it 5,503 times and I've gotten lots of questions along the
lines of, "Surely R can't do for free what [fill in a SAS or SPSS
product here] does?" To try to address those, I've compiled a table that
is organized by the product categories SAS and SPSS offer. Keep in mind
that I still know far more about SAS and SPSS than I do about R, so I
could really use some help with this. The table is below in tabbed form.
I would appreciate it if the many R gurus out there would look it over
and send suggestions. I'll add it as an appendix when it's done (well,
as done as a moving target like this ever is!) 

Thanks,
Bob

Topic	SAS Product	SPSS Product	R Package
Advanced Models	SAS/STAT	SPSS Advanced Models(tm)	R
Automated Data Preparation	None	SPSS Data Preparation(tm)
None?
Automated Forecasting	SAS Forecast Studio	DecisionTime/WhatIf(tm)
None?
Basics	SAS	SPSS Base(tm)	R
Conjoint Analysis	SAS/STAT: Transreg	SPSS Conjoint(tm)
Acepack?
Correspondence Analysis	SAS/STAT: Corresp	SPSS Categories(tm)
Homals, MASS, FactoMineR, ade4, PTAk, ccoresp, vegan, made4,PsychoR
Custom Tables	Base: Proc Tabulate	SPSS Custom Tables(tm)	reshape
Data Mining	Enterprise Miner	Clementine	Rattle
Exact Tests	SAS/STAT: various	SPSS Exact Tests(tm)
exactLoglinTest
Genetics	SAS/Genetics, SAS/Microarray Solution, JMP Genomics
None	Bioconductor
GIS/Mapping	SAS/GIS	SPSS Maps(tm)	maps
Graphical User Interface	Enterprise Guide	SPSS	JGR, R
Commander, pmg, Sciviews
Graphics	SAS/GRAPH(r)	SPSS Base(tm)	R, ggplot
Guided Analysis	SAS/LAB	None	None
Matrix/Linear Algebra	SAS/IML(tm), SAS/IML Workshop	SPSS Matrix(tm)
R
Missing Values Imputation	SAS/STAT: Proc MI	SPSS Missing
Values Analysis(tm)	aregImpute (Hmisc), fit.mult.impute (Design)
Mixed Models	Proc Mixed	SPSS Advanced Models	lmer
Operations Research	SAS/OR	None	TSP
Power Analysis	SAS/STAT:  Power,GLM Power	SamplePower(tm)	asypow,
powerpkg, pwr
 Regression Models	SAS/BASE	SPSS Regression Models(tm)
R
Sampling, Nonrandom	SAS/STAT: surveymeans, etc.	SPSS Complex
Samples(tm)	survey
Structural Equations	SAS/STAT: Calis	Amos(tm)	sem
Text Analysis	Text Miner	SPSS Text Analysis for Surveys(tm)
tm
Time Series	SAS/ETS(tm)	SPSS Trends(tm)	ArDec, brainwaver, dyn,
fame, Systemfit, tsDyn, tseries, tseriesChaos, tsfa, urca, uroot
Trees, Decision or Regression	Enterprise Miner	SPSS
Classification Trees(tm), AnswerTree(tm)	tree, rpart
Visualization	SAS/INSIGHT	None	rggobi, GGobi

=========================================================
Bob Muenchen (pronounced Min'-chen), Manager 
Statistical Consulting Center
U of TN Office of Information Technology
200 Stokely Management Center, Knoxville, TN 37996-0520
Voice: (865) 974-5230 
FAX: (865) 974-4810
Email: muenchen at utk.edu
Web: http://oit.utk.edu/scc, 
News: http://listserv.utk.edu/archives/statnews.html


From jessilbrown at gmail.com  Sat Feb 10 22:27:16 2007
From: jessilbrown at gmail.com (Jessi Brown)
Date: Sat, 10 Feb 2007 13:27:16 -0800
Subject: [R] error using user-defined link function with mixed models
	(LMER)
In-Reply-To: <40e66e0b0702101240sa6091bajed546e0863373147@mail.gmail.com>
References: <371904a90702101145h2f3d58afga3ec3ddf5ec78516@mail.gmail.com>
	<40e66e0b0702101240sa6091bajed546e0863373147@mail.gmail.com>
Message-ID: <371904a90702101327w59d74b57ld182f4cec56e2386@mail.gmail.com>

Ok, I've tried checking out the structure of the binomial and
logexposure families, the big difference  appears to be the valideta
parameter (it's "NULL" in the logexposure family).

First, binomial:
> str(binomial())
List of 11
 $ family    : chr "binomial"
 $ link      : chr "logit"
 $ linkfun   :function (mu)
 $ linkinv   :function (eta)
 $ variance  :function (mu)
 $ dev.resids:function (y, mu, wt)
 $ aic       :function (y, n, mu, wt, dev)
 $ mu.eta    :function (eta)
 $ initialize:  expression({     if (NCOL(y) == 1) {         if
(is.factor(y))              y <- y != levels(y)[1]         n <-
rep.int(1, nobs)         if (any(y < 0 | y > 1))              stop("y
values must be 0 <= y <= 1")         mustart <- (weights * y +
0.5)/(weights + 1)         m <- weights * y         if (any(abs(m -
round(m)) > 0.001))              warning("non-integer #successes in a
binomial glm!")     }     else if (NCOL(y) == 2) {         if
(any(abs(y - round(y)) > 0.001))              warning("non-integer
counts in a binomial glm!")         n <- y[, 1] + y[, 2]         y <-
ifelse(n == 0, 0, y[, 1]/n)         weights <- weights * n
mustart <- (n * y + 0.5)/(n + 1)     }     else stop("for the binomial
family, y must be a vector of 0 and 1's\n",          "or a 2 column
matrix where col 1 is no. successes and col 2 is no. failures") })
 $ validmu   :function (mu)
 $ valideta  :function (eta)
 - attr(*, "class")= chr "family"


Now, logexposure:
> str(logexposure(link="logexp", ExposureDays=apfa4$days))
List of 11
 $ family    : chr "binomial"
 $ link      :List of 5
  ..$ linkfun :function (mu)
  .. ..- attr(*, "source")= chr "function(mu) qlogis(mu^(1/days))"
  ..$ linkinv :function (eta)
  .. ..- attr(*, "source")= chr "function(eta) plogis(eta)^days"
  ..$ mu.eta  :function (eta)
  .. ..- attr(*, "source")= chr [1:3] "function(eta)" ...
  ..$ valideta:function (eta)
  .. ..- attr(*, "source")= chr "function(eta) TRUE"
  ..$ name    : chr "logexp()"
  ..- attr(*, "class")= chr "link-glm"
 $ linkfun   :function (mu)
  ..- attr(*, "source")= chr "function(mu) qlogis(mu^(1/days))"
 $ linkinv   :function (eta)
  ..- attr(*, "source")= chr "function(eta) plogis(eta)^days"
 $ variance  :function (mu)
  ..- attr(*, "source")= chr "function(mu) mu * (1 - mu)"
 $ dev.resids:function (y, mu, wt)
  ..- attr(*, "source")= chr [1:2] "function(y, mu, wt)
.Call(\"binomial_dev_resids\"," ...
 $ aic       :function (y, n, mu, wt, dev)
  ..- attr(*, "source")= chr [1:7] "function(y, n, mu, wt, dev) {" ...
 $ mu.eta    :function (eta)
  ..- attr(*, "source")= chr [1:3] "function(eta)" ...
 $ initialize:  expression({     if (NCOL(y) == 1) {         if
(is.factor(y))              y <- y != levels(y)[1]         n <-
rep.int(1, nobs)         if (any(y < 0 | y > 1))              stop("y
values must be 0 <= y <= 1")         mustart <- (weights * y +
0.5)/(weights + 1)         m <- weights * y         if (any(abs(m -
round(m)) > 0.001))              warning("non-integer successes in a
binomial glm!")     }     else if (NCOL(y) == 2) {         if
(any(abs(y - round(y)) > 0.001))              warning("non-integer
counts in a binomial glm!")         n <- y[, 1] + y[, 2]         y <-
ifelse(n == 0, 0, y[, 1]/n)         weights <- weights * n
mustart <- (n * y + 0.5)/(n + 1)     }     else stop("for the binomial
family,", " y must be a vector of 0 and 1's\n",          "or a 2
column matrix where col 1 is", " no. successes and col 2 is no.
failures") })
 $ validmu   :function (mu)
  ..- attr(*, "source")= chr "function(mu) all(mu > 0) && all(mu < 1)"
 $ valideta  : NULL
 - attr(*, "class")= chr "family"


So, could this be the root of the problem?

Here again is the logexp function:
logexp <- function(days = 1)
{
    linkfun <- function(mu) qlogis(mu^(1/days))
    linkinv <- function(eta) plogis(eta)^days
    mu.eta <- function(eta)
      days*.Call("logit_mu_eta", eta,
       PACKAGE = "stats")*plogis(eta)^(days-1)
    valideta <- function(eta) TRUE
    link <- paste("logexp(", days, ")", sep="")
    structure(list(linkfun = linkfun, linkinv = linkinv,
       mu.eta = mu.eta, valideta = valideta, name = link),
        class = "link-glm")
}

So, does something seem obviously wrong about the "valideta <-
function(eta) TRUE" bit? I readily admit that I did not write these
user-defined link and family functions (I believe they resulted from
the combined efforts of Mark Herzog and Brian Ripley), so my clumsy
efforts to toy with the valideta portion of the logexp link function
aren't working.

Thanks again in advance for any further advice.

cheers, Jessi Brown

On 2/10/07, Douglas Bates <bates at stat.wisc.edu> wrote:
> On 2/10/07, Jessi Brown <jessilbrown at gmail.com> wrote:
> > Greetings, everyone. I've been trying to analyze bird nest survival
> > data using generalized linear mixed models (because we documented
> > several consecutive nesting attempts by the same individuals; i.e.
> > repeated measures data) and have been unable to persuade the various
> > GLMM models to work with my user-defined link function. Actually,
> > glmmPQL seems to work, but as I want to evaluate a suite of competing
> > models, I'd like to use ML or REML estimation methods in order to end
> > up with meaningful log-likelihoods.
> >
> > Here's the link function I use:
> > logexp <- function(days = 1)
> > {
> >     linkfun <- function(mu) qlogis(mu^(1/days))
> >     linkinv <- function(eta) plogis(eta)^days
> >     mu.eta <- function(eta)
> >       days*.Call("logit_mu_eta", eta,
> >        PACKAGE = "stats")*plogis(eta)^(days-1)
> >     valideta <- function(eta) TRUE
> >     link <- paste("logexp(", days, ")", sep="")
> >     structure(list(linkfun = linkfun, linkinv = linkinv,
> >        mu.eta = mu.eta, valideta = valideta, name = link),
> >         class = "link-glm")
> > }
> >
> > # Modified binomial family function (that allows logexp link function)
> > logexposure<-function (link="logexp",ExposureDays) {
> >     variance <- function(mu) mu * (1 - mu)
> >     validmu <- function(mu) all(mu > 0) && all(mu < 1)
> >     dev.resids <- function(y, mu, wt) .Call("binomial_dev_resids",
> >         y, mu, wt, PACKAGE = "stats")
> >     aic <- function(y, n, mu, wt, dev) {
> >         m <- if (any(n > 1))
> >             n
> >         else wt
> >         -2 * sum(ifelse(m > 0, (wt/m), 0) * dbinom(round(m *
> >             y), round(m), mu, log = TRUE))
> >     }
> >     initialize <- expression({
> >         if (NCOL(y) == 1) {
> >             if (is.factor(y)) y <- y != levels(y)[1]
> >             n <- rep.int(1, nobs)
> >             if (any(y < 0 | y > 1)) stop("y values must be 0 <= y <= 1")
> >             mustart <- (weights * y + 0.5)/(weights + 1)
> >             m <- weights * y
> >             if (any(abs(m - round(m)) > 0.001))
> >                  warning("non-integer successes in a binomial glm!")
> >         } else if (NCOL(y) == 2) {
> >             if (any(abs(y - round(y)) > 0.001))
> >                 warning("non-integer counts in a binomial glm!")
> >             n <- y[, 1] + y[, 2]
> >             y <- ifelse(n == 0, 0, y[, 1]/n)
> >             weights <- weights * n
> >             mustart <- (n * y + 0.5)/(n + 1)
> >         } else stop("for the binomial family,",
> >                 " y must be a vector of 0 and 1's\n",
> >             "or a 2 column matrix where col 1 is",
> >             " no. successes and col 2 is no. failures")
> >     })
> >     structure(list(family="binomial", link=logexp(ExposureDays),
> >        linkfun=logexp(ExposureDays)$linkfun,
> >        linkinv=logexp(ExposureDays)$linkinv, variance=variance,
> >        dev.resids=dev.resids, aic=aic,
> >        mu.eta=logexp(ExposureDays)$mu.eta, initialize=initialize,
> >        validmu=validmu, valideta=logexp$valideta), class = "family")
> > }
> >
> >
> >
> > Now, here's how it works in a GLM:
> >
> > > apfa.glm.1<-glm(Success~MeanAge+I(MeanAge^2), family=logexposure(link="logexp", ExposureDays=apfa4$Days), data=apfa4)
> > > summary(apfa.glm.1)
> >
> > Call:
> > glm(formula = Success ~ MeanAge + I(MeanAge^2), family =
> > logexposure(link = "logexp",
> >     ExposureDays = apfa4$Days), data = apfa4)
> >
> > Deviance Residuals:
> >     Min       1Q   Median       3Q      Max
> > -3.1525   0.2802   0.3637   0.4291   0.7599
> >
> > Coefficients:
> >                Estimate Std. Error z value Pr(>|z|)
> > (Intercept)   5.5594830  0.6085542   9.136   <2e-16 ***
> > MeanAge      -0.0908251  0.0407218  -2.230   0.0257 *
> > I(MeanAge^2)  0.0014926  0.0006104   2.445   0.0145 *
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > (Dispersion parameter for binomial family taken to be 1)
> >
> >     Null deviance: 323.58  on 661  degrees of freedom
> > Residual deviance: 285.65  on 659  degrees of freedom
> > AIC: 291.65
> >
> > Number of Fisher Scoring iterations: 6
> >
> >
> >
> > Next, here's the results of a glmmPQL run:
> >
> > > apfa.glmm.1<-glmmPQL(Success~MeanAge+I(MeanAge^2), random=~1|Territory, family=logexposure(link="logexp", ExposureDays=apfa4$Days), data=apfa4)
> > iteration 1
> > > summary(apfa.glmm.1)
> > Linear mixed-effects model fit by maximum likelihood
> >  Data: apfa4
> >   AIC BIC logLik
> >    NA  NA     NA
> >
> > Random effects:
> >  Formula: ~1 | Territory
> >          (Intercept) Residual
> > StdDev: 0.0003431913 1.051947
> >
> > Variance function:
> >  Structure: fixed weights
> >  Formula: ~invwt
> > Fixed effects: Success ~ MeanAge + I(MeanAge^2)
> >                  Value Std.Error  DF   t-value p-value
> > (Intercept)   5.559466 0.6416221 624  8.664705  0.0000
> > MeanAge      -0.090824 0.0429346 624 -2.115397  0.0348
> > I(MeanAge^2)  0.001493 0.0006436 624  2.319090  0.0207
> >  Correlation:
> >              (Intr) MeanAg
> > MeanAge      -0.927
> > I(MeanAge^2)  0.826 -0.968
> >
> > Standardized Within-Group Residuals:
> >         Min          Q1         Med          Q3         Max
> > -11.3646020   0.1901969   0.2485473   0.2951632   0.5499915
> >
> > Number of Observations: 662
> > Number of Groups: 36
> >
> >
> >
> > Finally, here's what happens when I try to run an LMER model (same
> > error messages no matter which estimation method I choose):
> >
> > > apfa.lmer.1<-lmer(Success~MeanAge+I(MeanAge^2)+(1|Territory), data=apfa4, family=logexposure(link="logexp", ExposureDays=apfa4$Days), method="Laplace")
> > > summary(apfa.lmer.1)
> > Error in if (any(sd < 0)) return("'sd' slot has negative entries") :
> >         missing value where TRUE/FALSE needed
> > > names(apfa.lmer.1)
> > NULL
>
> > So, does anyone have any idea as to whether the problem is in the
> > user-defined link function as written, or have any thoughts about how
> > to get around this problem? If LMER and LME can't do it, could there
> > be some way to trick the glmmML function into accepting the
> > user-defined link function?
>
> lmer is designed to work with arbitrary families but I haven't done a
> lot of testing outside the binomial and poisson families.
>
> Try looking at the structure of an instance of your family and
> comparing it to, say,
>
> str(binomial())
>
> Make sure that all the components are there and have the correct form.
>
> The next step is to use verbose output so you can see the progress of
> the iterations.  Add the optional argument control = list(msVerbose =
> 1) to your call to lmer.
>
> Instead of immediately requesting a summary, use str(apfa.lmer.1) to
> check the structure.  Again you may want to compare this description
> to that from str applied to a similar fit using the binomial family.
>


From A.Robinson at ms.unimelb.edu.au  Sat Feb 10 23:31:34 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 11 Feb 2007 09:31:34 +1100
Subject: [R] Can we change environment within the browser?
Message-ID: <20070210223134.GQ24437@ms.unimelb.edu.au>

Dear R-helpers,

when in the browser, is it possible to change the environment, so as
to be able to easily access (print, manipulate) objects in the parent,
or elsehwere?

I know that it is possible to evaluate expressions in different
environments, using eval(), but I would prefer to avoid that if
possible.

Thanks,

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From skiadas at hanover.edu  Sun Feb 11 00:15:13 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sat, 10 Feb 2007 18:15:13 -0500
Subject: [R] SAS, SPSS Product Comparison Table
In-Reply-To: <7270AEC73132194E8BC0EE06B35D93D879C15F@UTKFSVS3.utk.tennessee.edu>
References: <7270AEC73132194E8BC0EE06B35D93D879C15F@UTKFSVS3.utk.tennessee.edu>
Message-ID: <3698FAED-7371-4076-94B9-9D69C2E9F001@hanover.edu>

On Feb 10, 2007, at 4:26 PM, Muenchen, Robert A (Bob) wrote:
>  "Surely R can't do for free what [fill in a SAS or SPSS
> product here] does?" To try to address those, I've compiled a table  
> that
> is organized by the product categories SAS and SPSS offer. Keep in  
> mind
> that I still know far more about SAS and SPSS than I do about R, so I
> could really use some help with this. The table is below in tabbed  
> form.
> I would appreciate it if the many R gurus out there would look it over
> and send suggestions. I'll add it as an appendix when it's done (well,
> as done as a moving target like this ever is!)
>
Great idea, this should come in handy! Here is a more readable  
version of Bob's table (don't know if I can post attachments like  
that to the list, so I figured I'll put it up like this):

http://skiadas.dcostanet.net/uploads/StatsComparisonTable.pdf

> Thanks,
> Bob

Haris


From kubovy at virginia.edu  Sun Feb 11 02:03:34 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 10 Feb 2007 20:03:34 -0500
Subject: [R] Suppresing default text in pairs.lmList() in package = nlme
Message-ID: <FB8D2AD9-ED46-4ECE-A0D8-45C3E44E16EF@virginia.edu>

I would like to suppress the text 'Scatter Plot Matrix' that appears  
under the plot. Could someone please suggest how?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From adrian_d at eskimo.com  Sun Feb 11 04:08:32 2007
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Sat, 10 Feb 2007 19:08:32 -0800 (PST)
Subject: [R] R in a production environment
Message-ID: <Pine.SUN.4.58.0702101800140.9782@eskimo.com>


Recently there have been a lot of discussions on the list on the use of R
in industry and comparisons with SAS & SPSS.  These are very important
questions and it might be worth having them all grouped together for
reference.  I would like to add to these topics a related one.

Can we share our experience of using R in a production environment?  I
work in a financial company that is cautiously interested in using R in a
production environment.  I'm trying to convince them that R is serious
software, modern and powerful.  Their first question is who else out
there is using R.  I can guess from some contributed packages, or from
the list postings, but it would be great if there would be a centralized
place.  We could have a standard set of questions and a contact
person (maybe on the wiki site?)

What do you think?  I believe it could create a snowball effect to
increase the visibility and use of R beyond academia and personal use.

Thanks,
Adrian


From Bartjoosen at hotmail.com  Sun Feb 11 08:18:17 2007
From: Bartjoosen at hotmail.com (Bart Joosen)
Date: Sun, 11 Feb 2007 08:18:17 +0100
Subject: [R] Near function?
Message-ID: <BAY134-DAV9C6971B5990127203D046D8920@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070211/f55760d7/attachment.pl 

From Dimitris.Rizopoulos at med.kuleuven.be  Sun Feb 11 10:17:34 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sun, 11 Feb 2007 10:17:34 +0100
Subject: [R] Near function?
In-Reply-To: <BAY134-DAV9C6971B5990127203D046D8920@phx.gbl>
References: <BAY134-DAV9C6971B5990127203D046D8920@phx.gbl>
Message-ID: <20070211101734.3onvwqo7nm1wggs8@webmail3.kuleuven.be>

maybe you could try something along these lines:

x <- c(1, 3, 2, 5, 11)
thr <- 3
###
ind <- t(combn(x, 2))
unique(c(ind[abs(ind[, 1] - ind[, 2]) <= thr, ]))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Bart Joosen <Bartjoosen at hotmail.com>:

> All,
>
> thanks for your help.
>
> Dieter,
>
> thanks, it's a different way of tackling the problem.
> But I still need a for loop to scroll throug the list?
>
> For example:
> c(1,2,3,5,)
> and a threshold of 3, then c(1,5) should remain. If I make an   
> integer with the difference between each element and the previous   
> element,
> then 5 should be eliminated, while it shouldn't.
>
> Or am I wrong with this assumption?
>
> Thanks anyway
>
> Bart
>
>
>>
>> Hi,
>>
>> I have an integer which is extracted from a dataframe, which is sorted by
> another column of the dataframe.
>> Now I would like to remove some elements of the integer, which are near to
> others by their value. For example:
>> integer: c(1,20,2,21) should be c(1,20).
>
> ...
>> Sorting the integer is not an option, the order is important.
>
> Why not? It's extremely efficient for large series and the only method that
> would work with large array. The idea: Keep the indexes of the sort   
> order, mark
> the "near others" for example making their index NA, and restore   
> original order.
> No for-loop needed.
>
> Dieter
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From dieter.menne at menne-biomed.de  Sun Feb 11 10:25:28 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 11 Feb 2007 09:25:28 +0000 (UTC)
Subject: [R] Suppresing default text in pairs.lmList() in package = nlme
References: <FB8D2AD9-ED46-4ECE-A0D8-45C3E44E16EF@virginia.edu>
Message-ID: <loom.20070211T102351-109@post.gmane.org>

Michael Kubovy <kubovy <at> virginia.edu> writes:

> 
> I would like to suppress the text 'Scatter Plot Matrix' that appears  
> under the plot. Could someone please suggest how?

Must be as special Virginia Brand of lmList. That Test does not turn up in my
output, and the only place I found it in the sources was as a comment

## scatter plot matrix plots, generally based on coef or ranef



Dieter


From knoblauch at lyon.inserm.fr  Sun Feb 11 11:20:04 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sun, 11 Feb 2007 09:20:04 -0100 (CET)
Subject: [R]  Suppresing default text in pairs.lmList() in package = nlme
Message-ID: <49201.82.231.93.240.1171182004.squirrel@webmail.lyon.inserm.fr>

It looks like you get this default xlab with splom so the title is
misleading,
though pairs.lmList calls splom in some cases, see near the end of the
function
where plotfun is assigned a value of either "xyplot" or "splom". I was able
to get the xlab to change by providing an explicit xlab = "machin" argument
to override the xlab argument in the call to splom and which is passed
along by
the ... argument.

ken



Dieter Menne a ?crit:
Michael Kubovy <kubovy <at> virginia.edu> writes:

>
> I would like to suppress the text 'Scatter Plot Matrix' that appears
> under the plot. Could someone please suggest how?

Must be as special Virginia Brand of lmList. That Test does not turn up in my
output, and the only place I found it in the sources was as a comment

## scatter plot matrix plots, generally based on coef or ranef



Dieter
-- 
Ken Knoblauch
Inserm U846
Institut Cellule Souche et Cerveau
D?partement Neurosciences Int?gratives
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.lyon.inserm.fr/371/


From kubovy at virginia.edu  Sun Feb 11 13:06:05 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 11 Feb 2007 07:06:05 -0500
Subject: [R] Suppresing default text in pairs.lmList() in package = nlme
In-Reply-To: <49201.82.231.93.240.1171182004.squirrel@webmail.lyon.inserm.fr>
References: <49201.82.231.93.240.1171182004.squirrel@webmail.lyon.inserm.fr>
Message-ID: <F86884D4-E96F-437D-9FF3-2BDE4D388D27@virginia.edu>

Thanks Ken and Dieter,

I added xlab = '' and the text 'Scatter Plot Matrix' produced by pairs 
() applied to an lmList object (in nlme) went away. Skip the rest ---  
which is mainly autobiographical --- unless you're curious.

On Feb 11, 2007, at 4:25 AM, Dieter Menne wrote:

> Michael Kubovy <kubovy <at> virginia.edu> writes:
>
>> I would like to suppress the text 'Scatter Plot Matrix' that appears
>> under the plot. Could someone please suggest how?
>
> Must be as special Virginia Brand of lmList. That Test does not  
> turn up in my
> output, and the only place I found it in the sources was as a comment
>
> ## scatter plot matrix plots, generally based on coef or ranef

Overnight I flew to New York (a slightly more left-leaning state),  
bought a new computer, installed R and all the packages, and then  
flew back with the new machine; the symptom remained  ;)

On Feb 11, 2007, at 5:20 AM, Ken Knoblauch wrote:

> It looks like you get this default xlab with splom so the title is
> misleading,
> though pairs.lmList calls splom in some cases, see near the end of the
> function
> where plotfun is assigned a value of either "xyplot" or "splom". I  
> was able
> to get the xlab to change by providing an explicit xlab = "machin"  
> argument
> to override the xlab argument in the call to splom and which is passed
> along by
> the ... argument.

Is it documented somewhere how to figure this out? It just didn't  
occur to me that the problem was as simple as changing the xlab and I  
don't yet have the habit of looking at code. Nevertheless I had done  
the following:

 > pairs
function (x, ...)
UseMethod("pairs")
<environment: namespace:graphics>

... which did not make me happy. So then I tried:

 > pairs.lmList
Error: object "pairs.lmList" not found

... which did not make me happy. So then I tried:

 > apropos('pairs')
[1] "pairs"         "pairs.default" "panel.pairs"
 > pairs.default
function (x, labels, panel = points, ..., lower.panel = panel,
...

... and then my eyes glazed over because nothing I saw seemed relevant.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From milton_ruser at yahoo.com.br  Sun Feb 11 13:23:55 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sun, 11 Feb 2007 04:23:55 -0800 (PST)
Subject: [R] deleting row when any col is.na
Message-ID: <571572.42813.qm@web56614.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070211/e93b6988/attachment.pl 

From jrkrideau at yahoo.ca  Sun Feb 11 14:13:57 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 11 Feb 2007 08:13:57 -0500 (EST)
Subject: [R] deleting row when any col is.na
In-Reply-To: <571572.42813.qm@web56614.mail.re3.yahoo.com>
Message-ID: <409153.99978.qm@web32809.mail.mud.yahoo.com>


--- Milton Cezar Ribeiro <milton_ruser at yahoo.com.br>
wrote:

> How can I delete rows from a data.frame where almost
> one collumns is.na()?
> 
> Kind regards,
> 
> miltinho

Can you explain a bit more and provide as simple
example of the problem? It is not clear what
importance the column with NA's has. 

If you simply wish to delete a row in a data.frame 

To delete row 6 in a data.frame: 

data1 <- data1[-6,]

Have a look at Part 6 of the Introduction to R for
more information


From dicook at iastate.edu  Sun Feb 11 14:00:48 2007
From: dicook at iastate.edu (Dianne Cook)
Date: Sun, 11 Feb 2007 07:00:48 -0600
Subject: [R] useR! 2007
Message-ID: <BBAC5956-FC69-4795-94F5-0477A02BB3E1@iastate.edu>

R Users and Developers,

Plans are being made to hold the first North American useR! will be
held at Iowa State University, Ames, Iowa, August 8?10, 2007, which
will be a week after JSM'07.

This follows successful meetings in Vienna, Austria, in 2006 and 2004,
and also Directions in Statistical Computing (DSC) meetings in
Auckland, NZ (Feb 2007), Seattle (2005), and Vienna (1999, 2001,
2003).

Information about the meeting can be found at
http://www.user2007.org/

If you have suggestions for the meeting please email the program
committee at user2007 at iastate.edu.

Regards,
useR! 2007 Program Committee
Douglas Bates, Dianne Cook, Dave Henderson, Heike Hofmann, Luke Tierney

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From bates at stat.wisc.edu  Sun Feb 11 14:31:06 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 11 Feb 2007 07:31:06 -0600
Subject: [R] deleting row when any col is.na
In-Reply-To: <571572.42813.qm@web56614.mail.re3.yahoo.com>
References: <571572.42813.qm@web56614.mail.re3.yahoo.com>
Message-ID: <40e66e0b0702110531j4caa27d9ibc52f26188897f34@mail.gmail.com>

On 2/11/07, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:

> How can I delete rows from a data.frame where almost one column is.na()?

The na.omit function does this.

> set.seed(123454321)
> x <- matrix(rnorm(50, mean = 1), ncol = 5)
> x[x < 0] <- NA
> df <- data.frame(x)
> df
           X1         X2        X3        X4        X5
1          NA         NA        NA 2.1072043 2.5623080
2  2.42544607 1.41903027 0.9652384 0.8869558 0.6824612
3  0.09657245         NA 1.6891345        NA 0.9867981
4  0.36620256 0.20111707        NA 0.6959298 0.3167434
5  2.73609165 0.07639585        NA 0.6348793 2.3797860
6          NA 0.43590076        NA 1.6082227 0.6580598
7  2.11889699 0.23726675 0.5066854 0.8967605 1.7082769
8  2.77162997 1.24540928        NA 0.4278059 1.1767292
9  0.55998073 1.80824613 0.6645171 1.7956458 2.1733173
10         NA 0.41624884 0.6055874 0.4814155 0.6728243
> na.omit(df)
         X1        X2        X3        X4        X5
2 2.4254461 1.4190303 0.9652384 0.8869558 0.6824612
7 2.1188970 0.2372667 0.5066854 0.8967605 1.7082769
9 0.5599807 1.8082461 0.6645171 1.7956458 2.1733173


From bates at stat.wisc.edu  Sun Feb 11 14:38:34 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 11 Feb 2007 07:38:34 -0600
Subject: [R] error using user-defined link function with mixed models
	(LMER)
In-Reply-To: <371904a90702101327w59d74b57ld182f4cec56e2386@mail.gmail.com>
References: <371904a90702101145h2f3d58afga3ec3ddf5ec78516@mail.gmail.com>
	<40e66e0b0702101240sa6091bajed546e0863373147@mail.gmail.com>
	<371904a90702101327w59d74b57ld182f4cec56e2386@mail.gmail.com>
Message-ID: <40e66e0b0702110538k2e0ab5bbgd80e1540cb7d1529@mail.gmail.com>

Look at the 'link' component of the two lists.  In the binomial family
object the link component is a character vector of link 1.  In your
logexposure family object it is a list of length 5.

On 2/10/07, Jessi Brown <jessilbrown at gmail.com> wrote:
> Ok, I've tried checking out the structure of the binomial and
> logexposure families, the big difference  appears to be the valideta
> parameter (it's "NULL" in the logexposure family).
>
> First, binomial:
> > str(binomial())
> List of 11
>  $ family    : chr "binomial"
>  $ link      : chr "logit"
>  $ linkfun   :function (mu)
>  $ linkinv   :function (eta)
>  $ variance  :function (mu)
>  $ dev.resids:function (y, mu, wt)
>  $ aic       :function (y, n, mu, wt, dev)
>  $ mu.eta    :function (eta)
>  $ initialize:  expression({     if (NCOL(y) == 1) {         if
> (is.factor(y))              y <- y != levels(y)[1]         n <-
> rep.int(1, nobs)         if (any(y < 0 | y > 1))              stop("y
> values must be 0 <= y <= 1")         mustart <- (weights * y +
> 0.5)/(weights + 1)         m <- weights * y         if (any(abs(m -
> round(m)) > 0.001))              warning("non-integer #successes in a
> binomial glm!")     }     else if (NCOL(y) == 2) {         if
> (any(abs(y - round(y)) > 0.001))              warning("non-integer
> counts in a binomial glm!")         n <- y[, 1] + y[, 2]         y <-
> ifelse(n == 0, 0, y[, 1]/n)         weights <- weights * n
> mustart <- (n * y + 0.5)/(n + 1)     }     else stop("for the binomial
> family, y must be a vector of 0 and 1's\n",          "or a 2 column
> matrix where col 1 is no. successes and col 2 is no. failures") })
>  $ validmu   :function (mu)
>  $ valideta  :function (eta)
>  - attr(*, "class")= chr "family"
>
>
> Now, logexposure:
> > str(logexposure(link="logexp", ExposureDays=apfa4$days))
> List of 11
>  $ family    : chr "binomial"
>  $ link      :List of 5
>   ..$ linkfun :function (mu)
>   .. ..- attr(*, "source")= chr "function(mu) qlogis(mu^(1/days))"
>   ..$ linkinv :function (eta)
>   .. ..- attr(*, "source")= chr "function(eta) plogis(eta)^days"
>   ..$ mu.eta  :function (eta)
>   .. ..- attr(*, "source")= chr [1:3] "function(eta)" ...
>   ..$ valideta:function (eta)
>   .. ..- attr(*, "source")= chr "function(eta) TRUE"
>   ..$ name    : chr "logexp()"
>   ..- attr(*, "class")= chr "link-glm"
>  $ linkfun   :function (mu)
>   ..- attr(*, "source")= chr "function(mu) qlogis(mu^(1/days))"
>  $ linkinv   :function (eta)
>   ..- attr(*, "source")= chr "function(eta) plogis(eta)^days"
>  $ variance  :function (mu)
>   ..- attr(*, "source")= chr "function(mu) mu * (1 - mu)"
>  $ dev.resids:function (y, mu, wt)
>   ..- attr(*, "source")= chr [1:2] "function(y, mu, wt)
> .Call(\"binomial_dev_resids\"," ...
>  $ aic       :function (y, n, mu, wt, dev)
>   ..- attr(*, "source")= chr [1:7] "function(y, n, mu, wt, dev) {" ...
>  $ mu.eta    :function (eta)
>   ..- attr(*, "source")= chr [1:3] "function(eta)" ...
>  $ initialize:  expression({     if (NCOL(y) == 1) {         if
> (is.factor(y))              y <- y != levels(y)[1]         n <-
> rep.int(1, nobs)         if (any(y < 0 | y > 1))              stop("y
> values must be 0 <= y <= 1")         mustart <- (weights * y +
> 0.5)/(weights + 1)         m <- weights * y         if (any(abs(m -
> round(m)) > 0.001))              warning("non-integer successes in a
> binomial glm!")     }     else if (NCOL(y) == 2) {         if
> (any(abs(y - round(y)) > 0.001))              warning("non-integer
> counts in a binomial glm!")         n <- y[, 1] + y[, 2]         y <-
> ifelse(n == 0, 0, y[, 1]/n)         weights <- weights * n
> mustart <- (n * y + 0.5)/(n + 1)     }     else stop("for the binomial
> family,", " y must be a vector of 0 and 1's\n",          "or a 2
> column matrix where col 1 is", " no. successes and col 2 is no.
> failures") })
>  $ validmu   :function (mu)
>   ..- attr(*, "source")= chr "function(mu) all(mu > 0) && all(mu < 1)"
>  $ valideta  : NULL
>  - attr(*, "class")= chr "family"
>
>
> So, could this be the root of the problem?
>
> Here again is the logexp function:
> logexp <- function(days = 1)
> {
>     linkfun <- function(mu) qlogis(mu^(1/days))
>     linkinv <- function(eta) plogis(eta)^days
>     mu.eta <- function(eta)
>       days*.Call("logit_mu_eta", eta,
>        PACKAGE = "stats")*plogis(eta)^(days-1)
>     valideta <- function(eta) TRUE
>     link <- paste("logexp(", days, ")", sep="")
>     structure(list(linkfun = linkfun, linkinv = linkinv,
>        mu.eta = mu.eta, valideta = valideta, name = link),
>         class = "link-glm")
> }
>
> So, does something seem obviously wrong about the "valideta <-
> function(eta) TRUE" bit? I readily admit that I did not write these
> user-defined link and family functions (I believe they resulted from
> the combined efforts of Mark Herzog and Brian Ripley), so my clumsy
> efforts to toy with the valideta portion of the logexp link function
> aren't working.
>
> Thanks again in advance for any further advice.
>
> cheers, Jessi Brown
>
> On 2/10/07, Douglas Bates <bates at stat.wisc.edu> wrote:
> > On 2/10/07, Jessi Brown <jessilbrown at gmail.com> wrote:
> > > Greetings, everyone. I've been trying to analyze bird nest survival
> > > data using generalized linear mixed models (because we documented
> > > several consecutive nesting attempts by the same individuals; i.e.
> > > repeated measures data) and have been unable to persuade the various
> > > GLMM models to work with my user-defined link function. Actually,
> > > glmmPQL seems to work, but as I want to evaluate a suite of competing
> > > models, I'd like to use ML or REML estimation methods in order to end
> > > up with meaningful log-likelihoods.
> > >
> > > Here's the link function I use:
> > > logexp <- function(days = 1)
> > > {
> > >     linkfun <- function(mu) qlogis(mu^(1/days))
> > >     linkinv <- function(eta) plogis(eta)^days
> > >     mu.eta <- function(eta)
> > >       days*.Call("logit_mu_eta", eta,
> > >        PACKAGE = "stats")*plogis(eta)^(days-1)
> > >     valideta <- function(eta) TRUE
> > >     link <- paste("logexp(", days, ")", sep="")
> > >     structure(list(linkfun = linkfun, linkinv = linkinv,
> > >        mu.eta = mu.eta, valideta = valideta, name = link),
> > >         class = "link-glm")
> > > }
> > >
> > > # Modified binomial family function (that allows logexp link function)
> > > logexposure<-function (link="logexp",ExposureDays) {
> > >     variance <- function(mu) mu * (1 - mu)
> > >     validmu <- function(mu) all(mu > 0) && all(mu < 1)
> > >     dev.resids <- function(y, mu, wt) .Call("binomial_dev_resids",
> > >         y, mu, wt, PACKAGE = "stats")
> > >     aic <- function(y, n, mu, wt, dev) {
> > >         m <- if (any(n > 1))
> > >             n
> > >         else wt
> > >         -2 * sum(ifelse(m > 0, (wt/m), 0) * dbinom(round(m *
> > >             y), round(m), mu, log = TRUE))
> > >     }
> > >     initialize <- expression({
> > >         if (NCOL(y) == 1) {
> > >             if (is.factor(y)) y <- y != levels(y)[1]
> > >             n <- rep.int(1, nobs)
> > >             if (any(y < 0 | y > 1)) stop("y values must be 0 <= y <= 1")
> > >             mustart <- (weights * y + 0.5)/(weights + 1)
> > >             m <- weights * y
> > >             if (any(abs(m - round(m)) > 0.001))
> > >                  warning("non-integer successes in a binomial glm!")
> > >         } else if (NCOL(y) == 2) {
> > >             if (any(abs(y - round(y)) > 0.001))
> > >                 warning("non-integer counts in a binomial glm!")
> > >             n <- y[, 1] + y[, 2]
> > >             y <- ifelse(n == 0, 0, y[, 1]/n)
> > >             weights <- weights * n
> > >             mustart <- (n * y + 0.5)/(n + 1)
> > >         } else stop("for the binomial family,",
> > >                 " y must be a vector of 0 and 1's\n",
> > >             "or a 2 column matrix where col 1 is",
> > >             " no. successes and col 2 is no. failures")
> > >     })
> > >     structure(list(family="binomial", link=logexp(ExposureDays),
> > >        linkfun=logexp(ExposureDays)$linkfun,
> > >        linkinv=logexp(ExposureDays)$linkinv, variance=variance,
> > >        dev.resids=dev.resids, aic=aic,
> > >        mu.eta=logexp(ExposureDays)$mu.eta, initialize=initialize,
> > >        validmu=validmu, valideta=logexp$valideta), class = "family")
> > > }
> > >
> > >
> > >
> > > Now, here's how it works in a GLM:
> > >
> > > > apfa.glm.1<-glm(Success~MeanAge+I(MeanAge^2), family=logexposure(link="logexp", ExposureDays=apfa4$Days), data=apfa4)
> > > > summary(apfa.glm.1)
> > >
> > > Call:
> > > glm(formula = Success ~ MeanAge + I(MeanAge^2), family =
> > > logexposure(link = "logexp",
> > >     ExposureDays = apfa4$Days), data = apfa4)
> > >
> > > Deviance Residuals:
> > >     Min       1Q   Median       3Q      Max
> > > -3.1525   0.2802   0.3637   0.4291   0.7599
> > >
> > > Coefficients:
> > >                Estimate Std. Error z value Pr(>|z|)
> > > (Intercept)   5.5594830  0.6085542   9.136   <2e-16 ***
> > > MeanAge      -0.0908251  0.0407218  -2.230   0.0257 *
> > > I(MeanAge^2)  0.0014926  0.0006104   2.445   0.0145 *
> > > ---
> > > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> > >
> > > (Dispersion parameter for binomial family taken to be 1)
> > >
> > >     Null deviance: 323.58  on 661  degrees of freedom
> > > Residual deviance: 285.65  on 659  degrees of freedom
> > > AIC: 291.65
> > >
> > > Number of Fisher Scoring iterations: 6
> > >
> > >
> > >
> > > Next, here's the results of a glmmPQL run:
> > >
> > > > apfa.glmm.1<-glmmPQL(Success~MeanAge+I(MeanAge^2), random=~1|Territory, family=logexposure(link="logexp", ExposureDays=apfa4$Days), data=apfa4)
> > > iteration 1
> > > > summary(apfa.glmm.1)
> > > Linear mixed-effects model fit by maximum likelihood
> > >  Data: apfa4
> > >   AIC BIC logLik
> > >    NA  NA     NA
> > >
> > > Random effects:
> > >  Formula: ~1 | Territory
> > >          (Intercept) Residual
> > > StdDev: 0.0003431913 1.051947
> > >
> > > Variance function:
> > >  Structure: fixed weights
> > >  Formula: ~invwt
> > > Fixed effects: Success ~ MeanAge + I(MeanAge^2)
> > >                  Value Std.Error  DF   t-value p-value
> > > (Intercept)   5.559466 0.6416221 624  8.664705  0.0000
> > > MeanAge      -0.090824 0.0429346 624 -2.115397  0.0348
> > > I(MeanAge^2)  0.001493 0.0006436 624  2.319090  0.0207
> > >  Correlation:
> > >              (Intr) MeanAg
> > > MeanAge      -0.927
> > > I(MeanAge^2)  0.826 -0.968
> > >
> > > Standardized Within-Group Residuals:
> > >         Min          Q1         Med          Q3         Max
> > > -11.3646020   0.1901969   0.2485473   0.2951632   0.5499915
> > >
> > > Number of Observations: 662
> > > Number of Groups: 36
> > >
> > >
> > >
> > > Finally, here's what happens when I try to run an LMER model (same
> > > error messages no matter which estimation method I choose):
> > >
> > > > apfa.lmer.1<-lmer(Success~MeanAge+I(MeanAge^2)+(1|Territory), data=apfa4, family=logexposure(link="logexp", ExposureDays=apfa4$Days), method="Laplace")
> > > > summary(apfa.lmer.1)
> > > Error in if (any(sd < 0)) return("'sd' slot has negative entries") :
> > >         missing value where TRUE/FALSE needed
> > > > names(apfa.lmer.1)
> > > NULL
> >
> > > So, does anyone have any idea as to whether the problem is in the
> > > user-defined link function as written, or have any thoughts about how
> > > to get around this problem? If LMER and LME can't do it, could there
> > > be some way to trick the glmmML function into accepting the
> > > user-defined link function?
> >
> > lmer is designed to work with arbitrary families but I haven't done a
> > lot of testing outside the binomial and poisson families.
> >
> > Try looking at the structure of an instance of your family and
> > comparing it to, say,
> >
> > str(binomial())
> >
> > Make sure that all the components are there and have the correct form.
> >
> > The next step is to use verbose output so you can see the progress of
> > the iterations.  Add the optional argument control = list(msVerbose =
> > 1) to your call to lmer.
> >
> > Instead of immediately requesting a summary, use str(apfa.lmer.1) to
> > check the structure.  Again you may want to compare this description
> > to that from str applied to a similar fit using the binomial family.
> >
>


From knoblauch at lyon.inserm.fr  Sun Feb 11 16:15:54 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sun, 11 Feb 2007 14:15:54 -0100 (CET)
Subject: [R] error using user-defined link function with mixed models	(LMER)
Message-ID: <49660.82.231.93.240.1171199754.squirrel@webmail.lyon.inserm.fr>


Isn't it the case, that since R 2.40 that all one ought to need do is
define one's own link and pass it to the family, rather re-defining the whole
family?

CHANGES IN R VERSION 2.4.0

...

 o	make.link() now returns an object of class "link-glm".
	The GLM families accept an object of this class for their
	'link' argument, which allows user-specified link functions.
	Also, quasi() allows user-specified variance functions.

I thought that was the point of the example on the family help page.

ken


Douglas Bates a ?crit :
Look at the 'link' component of the two lists.  In the binomial family
object the link component is a character vector of link 1.  In your
logexposure family object it is a list of length 5.

On 2/10/07, Jessi Brown <jessilbrown at gmail.com> wrote:
> Ok, I've tried checking out the structure of the binomial and
> logexposure families, the big difference  appears to be the valideta
> parameter (it's "NULL" in the logexposure family).
>
-- 
Ken Knoblauch
Inserm U846
Institut Cellule Souche et Cerveau
D?partement Neurosciences Int?gratives
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.lyon.inserm.fr/371/


From huber at ebi.ac.uk  Sun Feb 11 16:09:13 2007
From: huber at ebi.ac.uk (Wolfgang Huber)
Date: Sun, 11 Feb 2007 15:09:13 +0000
Subject: [R] Near function?
In-Reply-To: <BAY134-DAV163B546ACF849218F01779D8930@phx.gbl>
References: <BAY134-DAV163B546ACF849218F01779D8930@phx.gbl>
Message-ID: <45CF3199.8040708@ebi.ac.uk>

Dear Bart,

"hclust" might be useful for this as well:

   dat = c(1,20,2,21)

   hc = hclust(dist(dat))

   thresh = 2
   ct = cutree(hc, h=thresh)

   clusteredNumbers = split(dat, ct)
   firstOne = dat[!duplicated(ct)]

 >  clusteredNumbers
$`1`
[1] 1 2
$`2`
[1] 20 21


 > firstOne
[1]  1 20


  Best wishes
   Wolfgang


> 
> I have an integer which is extracted from a dataframe, which is sorted by another column of the dataframe.
> Now I would like to remove some elements of the integer, which are near to others by their value. For example: integer: c(1,20,2,21) should be c(1,20).
> 
> I tried to write a function, but for some reason, somethings won't work
> 
> x <- 1:20
> near <- function(x,th) {
>     nr <- NROW(x)
>         for (i in 1:(nr-1)){
>         for (j in (i+1):nr){
>             if (j > nr) break
>             t=0
>             if (abs(x[i] - x[j]) < th) t = 1
>             if (t== 1) x <- x[-j]
>             if (t== 1) nr <- nr-1
>             if (t== 1) j <- (j-1)
>             cat (" i",i," j",j,"\n")
>             }} 
> x
> }
> near(x,10)
> 
> 
> This gives you 1  3  7 13 17 while I was suspecting 1, 20 as the outcome.
> If you look at the intermediate results of the cat instruction, you see that, after he substracted a number, he skipped the next one.
> 
> Sorting the integer is not an option, the order is important.
> I used an integer from 1:20 as an example, while x <- sample((1:20),20) is maybe a bit more representable for our data, but isn't reproducible for the output of the function.
> 
> Maybe there is already an R-function, which does such thing, or what is wrong with my coding?
> 
> 
> thanks a lot for your time
> 
> 
> Bart
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help a stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
------------------------------------------------------------------
Wolfgang Huber  EBI/EMBL  Cambridge UK  http://www.ebi.ac.uk/huber


From bates at stat.wisc.edu  Sun Feb 11 16:19:47 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 11 Feb 2007 09:19:47 -0600
Subject: [R] error using user-defined link function with mixed models
	(LMER)
In-Reply-To: <49660.82.231.93.240.1171199754.squirrel@webmail.lyon.inserm.fr>
References: <49660.82.231.93.240.1171199754.squirrel@webmail.lyon.inserm.fr>
Message-ID: <40e66e0b0702110719o4f34d7baq4536d39144e5f813@mail.gmail.com>

Was make.link() used in the example code?

On 2/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
>
> Isn't it the case, that since R 2.40 that all one ought to need do is
> define one's own link and pass it to the family, rather re-defining the whole
> family?
>
> CHANGES IN R VERSION 2.4.0
>
> ...
>
>  o      make.link() now returns an object of class "link-glm".
>         The GLM families accept an object of this class for their
>         'link' argument, which allows user-specified link functions.
>         Also, quasi() allows user-specified variance functions.
>
> I thought that was the point of the example on the family help page.
>
> ken
>
>
> Douglas Bates a ?crit :
> Look at the 'link' component of the two lists.  In the binomial family
> object the link component is a character vector of link 1.  In your
> logexposure family object it is a list of length 5.
>
> On 2/10/07, Jessi Brown <jessilbrown at gmail.com> wrote:
> > Ok, I've tried checking out the structure of the binomial and
> > logexposure families, the big difference  appears to be the valideta
> > parameter (it's "NULL" in the logexposure family).
> >
> --
> Ken Knoblauch
> Inserm U846
> Institut Cellule Souche et Cerveau
> D?partement Neurosciences Int?gratives
> 18 avenue du Doyen L?pine
> 69500 Bron
> France
> tel: +33 (0)4 72 91 34 77
> fax: +33 (0)4 72 91 34 61
> portable: +33 (0)6 84 10 64 10
> http://www.lyon.inserm.fr/371/
>
>


From bartjoosen at hotmail.com  Sun Feb 11 16:38:34 2007
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Sun, 11 Feb 2007 16:38:34 +0100
Subject: [R] Near function?
References: <BAY134-DAV163B546ACF849218F01779D8930@phx.gbl>
	<45CF3199.8040708@ebi.ac.uk>
Message-ID: <BAY134-DAV15DD5F31C8F271D7B41406D8920@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070211/2c60b1f6/attachment.pl 

From knoblauch at lyon.inserm.fr  Sun Feb 11 17:27:42 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sun, 11 Feb 2007 15:27:42 -0100 (CET)
Subject: [R] error using user-defined link function with mixed models
 (LMER)
In-Reply-To: <40e66e0b0702110719o4f34d7baq4536d39144e5f813@mail.gmail.com>
References: <49660.82.231.93.240.1171199754.squirrel@webmail.lyon.inserm.fr>
	<40e66e0b0702110719o4f34d7baq4536d39144e5f813@mail.gmail.com>
Message-ID: <49996.82.231.93.240.1171204062.squirrel@webmail.lyon.inserm.fr>

Well, you can just check directly,

?family,

but it doesn't look like it.  It looks to me (and I am admittedly no
expert) as
if make.link is only returning the standard glm links. In ?make.link, it
says,

link	 character or numeric; one of "logit", "probit", "cloglog",
"identity", "log", "sqrt", "1/mu^2", "inverse", or (deprecated) a
non-negative number, say lambda resulting in power link mu ^ lambda. Also
(deprecated) a string like "power(0.5)" to indicate a call to power.

 But the example
from ?family, is a user-specified link of class "link-glm" that in the
example
is given directly to binomial().

For my own work, I have written two links for the binomial family, in this
manner
(i.e., thanks to this extension of R from the developers) for dealing with
cases when the response probability is expected to have a lower asymptote
other than 0 (for example, if an observer is making a decision among 4
choices).
It seems to work and I didn't have to use make.link or re-define the family.

I was motivated to try this from this thread:

https://www.stat.math.ethz.ch/pipermail/r-help/2006-December/122353.html

ken




Douglas Bates a ?crit :
> Was make.link() used in the example code?
>
> On 2/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
>>
>> Isn't it the case, that since R 2.40 that all one ought to need do is
>> define one's own link and pass it to the family, rather re-defining the
>> whole
>> family?
>>
>> CHANGES IN R VERSION 2.4.0
>>
>> ...
>>
>>  o      make.link() now returns an object of class "link-glm".
>>         The GLM families accept an object of this class for their
>>         'link' argument, which allows user-specified link functions.
>>         Also, quasi() allows user-specified variance functions.
>>
>> I thought that was the point of the example on the family help page.
>>
>> ken
>>
>>
>> Douglas Bates a ?crit :
>> Look at the 'link' component of the two lists.  In the binomial family
>> object the link component is a character vector of link 1.  In your
>> logexposure family object it is a list of length 5.
>>
>> On 2/10/07, Jessi Brown <jessilbrown at gmail.com> wrote:
>> > Ok, I've tried checking out the structure of the binomial and
>> > logexposure families, the big difference  appears to be the valideta
>> > parameter (it's "NULL" in the logexposure family).
>> >
>> --
>> Ken Knoblauch
>> Inserm U846
>> Institut Cellule Souche et Cerveau
>> D?partement Neurosciences Int?gratives
>> 18 avenue du Doyen L?pine
>> 69500 Bron
>> France
>> tel: +33 (0)4 72 91 34 77
>> fax: +33 (0)4 72 91 34 61
>> portable: +33 (0)6 84 10 64 10
>> http://www.lyon.inserm.fr/371/
>>
>>
>


-- 
Ken Knoblauch
Inserm U846
Institut Cellule Souche et Cerveau
D?partement Neurosciences Int?gratives
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.lyon.inserm.fr/371/


From pierrelap at gmail.com  Sun Feb 11 17:42:46 2007
From: pierrelap at gmail.com (Pierre Lapointe)
Date: Sun, 11 Feb 2007 11:42:46 -0500
Subject: [R] Extract NULL column in a matrix e.g. matrix[,-NULL]
Message-ID: <001801c74dfb$a94b8fa0$6401a8c0@claudiapc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070211/e5e0bd21/attachment.pl 

From vincent.goulet at act.ulaval.ca  Sun Feb 11 18:30:02 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Sun, 11 Feb 2007 12:30:02 -0500
Subject: [R] Extract NULL column in a matrix e.g. matrix[,-NULL]
In-Reply-To: <001801c74dfb$a94b8fa0$6401a8c0@claudiapc>
References: <001801c74dfb$a94b8fa0$6401a8c0@claudiapc>
Message-ID: <2970EFCA-B9AC-4799-B8D5-BDBC6028B3E5@act.ulaval.ca>


Le 07-02-11 ? 11:42, Pierre Lapointe a ?crit :

> Hello,
>
> I need to remove columns in a matrix.  The number of columns varies  
> from 0
> to n.  I can't figure out how to specify the zero case.
>
> aa <-matrix(runif(5^2),5,5)
>
> #remove column 3
>
> aa[,-3]
>
> #remove no column
>
> aa[,-NULL]
>
> Error in -NULL : invalid argument to unary operator
>
> I know I could use ifelse, but it would complicate my model a lot.  
> Is there
> a direct way to specify that the number of columns to remove is 0?
>
> Pierre Lapointe

Would

aa <- if (n > 0) aa[, -n] else aa

still be too complicated?

---
   Vincent Goulet, Associate Professor
   ?cole d'actuariat
   Universit? Laval, Qu?bec
   Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From ggrothendieck at gmail.com  Sun Feb 11 19:04:19 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Feb 2007 13:04:19 -0500
Subject: [R] Extract NULL column in a matrix e.g. matrix[,-NULL]
In-Reply-To: <001801c74dfb$a94b8fa0$6401a8c0@claudiapc>
References: <001801c74dfb$a94b8fa0$6401a8c0@claudiapc>
Message-ID: <971536df0702111004x75fb4537uf4d89939d35b951e@mail.gmail.com>

Try:

aa[, !seq(ncol(aa)) %in% NULL]

On 2/11/07, Pierre Lapointe <pierrelap at gmail.com> wrote:
> Hello,
>
>
>
> I need to remove columns in a matrix.  The number of columns varies from 0
> to n.  I can't figure out how to specify the zero case.
>
>
>
> aa <-matrix(runif(5^2),5,5)
>
>
>
> #remove column 3
>
> aa[,-3]
>
>
>
> #remove no column
>
> aa[,-NULL]
>
> Error in -NULL : invalid argument to unary operator
>
>
>
> I know I could use ifelse, but it would complicate my model a lot. Is there
> a direct way to specify that the number of columns to remove is 0?
>
>
>
> Pierre Lapointe
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gelman at stat.columbia.edu  Sun Feb 11 19:23:44 2007
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Sun, 11 Feb 2007 13:23:44 -0500
Subject: [R] problem with Matrix package
Message-ID: <45CF5F30.1080209@stat.columbia.edu>

I decided to update my packages and then had a problem with loading the 
Matrix package
http://cran.at.r-project.org/bin/windows/contrib/2.4/Matrix_0.9975-9.zip
This is what happened when I tried to load it in:

 > library("Matrix")
Error in importIntoEnv(impenv, impnames, ns, impvars) :
        object 'Logic' is not exported by 'namespace:methods'
Error: package/namespace load failed for 'Matrix'
 >

I hadn't had this probldem before; any thoughts?

Andrew

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
Columbia University, New York
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Office hours spring 2006:
  to be announced

Statistics department office:
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Political Science department office:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 731
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164


From adelagm at ugr.es  Sun Feb 11 19:30:38 2007
From: adelagm at ugr.es (Adela =?iso-8859-1?Q?Gonz=E1lez_Meg=EDas?=)
Date: Sun, 11 Feb 2007 19:30:38 +0100 (CET)
Subject: [R] help with linear mixed models
Message-ID: <1623.83.38.243.106.1171218638.squirrel@goliat4.ugr.es>


Dear all,
I want to evaluate a full-factorial linear mixed model with two fixed
factors (S, T) and a random factor (TM. I have tried to do the model in
lmer but at the moment this function do not provide for the F and p-values
of the fixed factors.

I tried: lmer (Arc~S*T*(1|TM), data=b,

 I also tried to do it in lme (nlme package). I do not have grouping
factors in my models, so I do not really know if that it is correct. In
any case, I cannot find the way to include the interactions between fixed
and random factors using this type of models. Someone suggested me to use
glmmML but seems to be implementing for poisson and binomial model (mine
is a Gaussian model).

I tried: lme (ArcP~S*T, data=b, ~1|TIME)

So, please can anybody tell me which one if the correct way/package to do it?
Moreover, in case anybody can help me, is it possible to get the p-value
calculated thru bootstrapping?

Thank you very much for your help in advance,

Adela


-- 
Adela Gonz?lez Meg?as
Depto. Biolog?a Animal
Fac. Ciencias
Universidad de Granada
18071 Granada
Spain


From bates at stat.wisc.edu  Sun Feb 11 19:38:37 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 11 Feb 2007 12:38:37 -0600
Subject: [R] problem with Matrix package
In-Reply-To: <45CF5F30.1080209@stat.columbia.edu>
References: <45CF5F30.1080209@stat.columbia.edu>
Message-ID: <40e66e0b0702111038k3db7e0c9j9c89a0d694d35325@mail.gmail.com>

On 2/11/07, Andrew Gelman <gelman at stat.columbia.edu> wrote:
> I decided to update my packages and then had a problem with loading the
> Matrix package
> http://cran.at.r-project.org/bin/windows/contrib/2.4/Matrix_0.9975-9.zip
> This is what happened when I tried to load it in:
>
>  > library("Matrix")
> Error in importIntoEnv(impenv, impnames, ns, impvars) :
>         object 'Logic' is not exported by 'namespace:methods'
> Error: package/namespace load failed for 'Matrix'
>  >
>
> I hadn't had this probldem before; any thoughts?
>
> Andrew

And the version of R that you are using is?

Version 0.9975-9 of the Matrix package depends on R >= 2.4.1 for
exactly this reason.


From damerdji at gmail.com  Sun Feb 11 19:49:45 2007
From: damerdji at gmail.com (Halim Damerdji)
Date: Sun, 11 Feb 2007 10:49:45 -0800
Subject: [R] problem with Matrix package
In-Reply-To: <45CF5F30.1080209@stat.columbia.edu>
References: <45CF5F30.1080209@stat.columbia.edu>
Message-ID: <928aa1b70702111049i5681bd89oc018041fe468e73a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070211/9b6cdbc0/attachment.pl 

From ubk at kogalur-shear.com  Sun Feb 11 17:40:02 2007
From: ubk at kogalur-shear.com (K. B. Udaya)
Date: Sun, 11 Feb 2007 11:40:02 -0500
Subject: [R] [R-pkgs] randomSurvivalForest 2.0.0 now available
Message-ID: <38c08c270702110840v72971cd4n9b9cc9117b65c38d@mail.gmail.com>

Dear useRs:

Release 2.0.0 of the randomSurvivalForest package is now available.

---------------------------------------------------------------------------------
CHANGES TO RELEASE 2.0.0

Release 2.0.0 represents a major upgrade in the functionality and stability
of the original 1.0.0 release.  Key changes are as follows:

o Two new splitting rules, 'logrankscore' and 'logrankapprox', added.

o Expanded output from 'rsf()'.  Now out-of-bag objects 'oob.ensemble' and
  'oob.mortality' are included in addition to the full ensemble objects
  'ensemble' and 'mortality'.

o Importance values for predictors can now be calculated (set
'importace = TRUE'
  in the initial 'rsf()' call).  Extended 'plot.error()' to print, as
well as plot,
  such values.

o Prediction on test data can now be implemented using 'rsf.predict()' (set
  'forest = TRUE' in the initial 'rsf()' call).

o Included option 'predictorWt' used for weighted sampling of predictors when
  growing a tree.

o Formula no longer restricted to main effects.  Formula for 'rsf' interpreted
  as in typical R applications.  However, users should be aware that including
  interactions or higher order terms in a formula may not be an optimal way to
  grow a forest.

o Three types of objects are generated in an RSF analysis: '(rsf, grow)',
  '(rsf, predict)' and '(rsf, forest)'.  Wrappers handle each type of object
  in different ways.

o Improved error checking in all wrappers.

o Extended 'plot.variable()' wrapper to generate partial plots for predictors.

o Improved control over trace output.  See the 'do.trace' option in 'rsf()'.

o Implements the Predictive Model Markup Language specification for an
  '(rsf, forest)' forest object.  PMML is an XML based language which
  provides a way for applications to define statistical and data mining
  models and to share models between PMML compliant applications.  More
  information about PMML and the Data Mining Group can be found at
  http://www.dmg.org.  Our implementation gives the user the ability to
  save the geometry of a forest as a PMML XML document for export or
  later retrieval.

---------------------------------------------------------------------------------

ubk2101 at columbia.edu

Udaya B. Kogalur, Ph.D.
Kogalur Shear Corporation
5425 Nestleway Drive, Suite L1
Clemmons, NC 27012

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From gelman at stat.columbia.edu  Sun Feb 11 20:08:00 2007
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Sun, 11 Feb 2007 14:08:00 -0500
Subject: [R] problem with Matrix package
In-Reply-To: <40e66e0b0702111038k3db7e0c9j9c89a0d694d35325@mail.gmail.com>
References: <45CF5F30.1080209@stat.columbia.edu>
	<40e66e0b0702111038k3db7e0c9j9c89a0d694d35325@mail.gmail.com>
Message-ID: <45CF6990.1000201@stat.columbia.edu>

Doug
Yes, it's R 2.4.1.  I'm trying to keep up with Matrix for various 
reasons, most immediately that our "arm" package requires Matrix.
Andrew

Douglas Bates wrote:
> On 2/11/07, Andrew Gelman <gelman at stat.columbia.edu> wrote:
>> I decided to update my packages and then had a problem with loading the
>> Matrix package
>> http://cran.at.r-project.org/bin/windows/contrib/2.4/Matrix_0.9975-9.zip
>> This is what happened when I tried to load it in:
>>
>>  > library("Matrix")
>> Error in importIntoEnv(impenv, impnames, ns, impvars) :
>>         object 'Logic' is not exported by 'namespace:methods'
>> Error: package/namespace load failed for 'Matrix'
>>  >
>>
>> I hadn't had this probldem before; any thoughts?
>>
>> Andrew
>
> And the version of R that you are using is?
>
> Version 0.9975-9 of the Matrix package depends on R >= 2.4.1 for
> exactly this reason.

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
Columbia University, New York
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Office hours spring 2006:
  to be announced

Statistics department office:
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Political Science department office:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 731
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164


From bates at stat.wisc.edu  Sun Feb 11 20:29:13 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 11 Feb 2007 13:29:13 -0600
Subject: [R] problem with Matrix package
In-Reply-To: <45CF6990.1000201@stat.columbia.edu>
References: <45CF5F30.1080209@stat.columbia.edu>
	<40e66e0b0702111038k3db7e0c9j9c89a0d694d35325@mail.gmail.com>
	<45CF6990.1000201@stat.columbia.edu>
Message-ID: <40e66e0b0702111129g7c196eefnf18128601e203a78@mail.gmail.com>

On 2/11/07, Andrew Gelman <gelman at stat.columbia.edu> wrote:
> Doug
> Yes, it's R 2.4.1.  I'm trying to keep up with Matrix for various
> reasons, most immediately that our "arm" package requires Matrix.
> Andrew

The source package for version 0.9975-10 Matrix has just been moved
onto CRAN.  Binary versions should hit the mirrors in a day or two.  I
suggest you try with that version.

In the meantime try

str(get("Logic", "package:methods"))

I get

> str(get("Logic", "package:methods"))
Formal class 'groupGenericFunction' [package "methods"] with 9 slots
  ..@ .Data       :function (e1, e2)
  ..@ groupMembers:List of 2
  .. ..$ : chr "&"
  .. ..$ : chr "|"
  ..@ generic     : atomic [1:1] Logic
  .. ..- attr(*, "package")= chr "base"
  ..@ package     : chr "base"
  ..@ group       :List of 1
  .. ..$ : chr "Ops"
  ..@ valueClass  : chr(0)
  ..@ signature   : chr [1:2] "e1" "e2"
  ..@ default     :Formal class 'MethodsList' [package "methods"] with 3 slots
  .. .. ..@ methods   : list()
  .. .. ..@ argument  : symbol e1
  .. .. ..@ allMethods: list()
  ..@ skeleton    : language function (e1, e2)  stop("invalid call in
method dispatch to \"Logic\" (no default method)",  ...

The Windows binary version of the Matrix package that you installed is
several weeks old.  If it didn't install on a stock version of R-2.4.1
I imagine we would have heard about it before now.  I think you may
need to check if somehow you have an old version of the methods
package on your search path or just install a fresh copy of R-2.4.1


>
> Douglas Bates wrote:
> > On 2/11/07, Andrew Gelman <gelman at stat.columbia.edu> wrote:
> >> I decided to update my packages and then had a problem with loading the
> >> Matrix package
> >> http://cran.at.r-project.org/bin/windows/contrib/2.4/Matrix_0.9975-9.zip
> >> This is what happened when I tried to load it in:
> >>
> >>  > library("Matrix")
> >> Error in importIntoEnv(impenv, impnames, ns, impvars) :
> >>         object 'Logic' is not exported by 'namespace:methods'
> >> Error: package/namespace load failed for 'Matrix'
> >>  >
> >>
> >> I hadn't had this probldem before; any thoughts?
> >>
> >> Andrew
> >
> > And the version of R that you are using is?
> >
> > Version 0.9975-9 of the Matrix package depends on R >= 2.4.1 for
> > exactly this reason.
>
> --
> Andrew Gelman
> Professor, Department of Statistics
> Professor, Department of Political Science
> Columbia University, New York
> gelman at stat.columbia.edu
> www.stat.columbia.edu/~gelman
>
> Office hours spring 2006:
>   to be announced
>
> Statistics department office:
>   Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
>   212-851-2142
> Political Science department office:
>   International Affairs Bldg (Amsterdam Ave at 118 St), Room 731
>   212-854-7075
>
> Mailing address:
>   1255 Amsterdam Ave, Room 1016
>   Columbia University
>   New York, NY 10027-5904
>   212-851-2142
>   (fax) 212-851-2164
>
>


From robert-mcfadden at o2.pl  Sun Feb 11 21:39:12 2007
From: robert-mcfadden at o2.pl (Robert McFadden)
Date: Sun, 11 Feb 2007 21:39:12 +0100
Subject: [R] merge words=data name
Message-ID: <001901c74e1c$b03bfa40$1191680a@statman>

I would like to merge two parts of words to get a name of the data. First
M3$N (invariable) and second is a number from 0001 to 3003 -
M3$N0001,M3$N0002,...,M3$N3003. For example if I do it like this:
my.data <- paste("M3$N",2456,sep="")
I get
> my.data
[1] "M3$N2456"
But I want to get something equivalent to
my.data<- M3$N2456

Is there any way to do it? 
It's important, because the second part are randomly chosen.
(The data that I would like to extract are in package Mcomp.)  

Thanks in advance,
Rob


From murdoch at stats.uwo.ca  Sun Feb 11 22:08:52 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 11 Feb 2007 16:08:52 -0500
Subject: [R] merge words=data name
In-Reply-To: <001901c74e1c$b03bfa40$1191680a@statman>
References: <001901c74e1c$b03bfa40$1191680a@statman>
Message-ID: <45CF85E4.5000509@stats.uwo.ca>

On 2/11/2007 3:39 PM, Robert McFadden wrote:
> I would like to merge two parts of words to get a name of the data. First
> M3$N (invariable) and second is a number from 0001 to 3003 -
> M3$N0001,M3$N0002,...,M3$N3003. For example if I do it like this:
> my.data <- paste("M3$N",2456,sep="")
> I get
>> my.data
> [1] "M3$N2456"
> But I want to get something equivalent to
> my.data<- M3$N2456
> 
> Is there any way to do it? 
> It's important, because the second part are randomly chosen.
> (The data that I would like to extract are in package Mcomp.)  

You can use

eval(parse(text=my.data))

if my.data holds an expression (like "M3$N2456") in text form.  Note 
that M3$N2456 is *not* a name in R; it's an expression saying to extract 
the component named N2456 from the object named M3.

Duncan Murdoch


From robert-mcfadden at o2.pl  Sun Feb 11 22:17:46 2007
From: robert-mcfadden at o2.pl (Robert McFadden)
Date: Sun, 11 Feb 2007 22:17:46 +0100
Subject: [R] merge words=data name
In-Reply-To: <45CF85E4.5000509@stats.uwo.ca>
References: <001901c74e1c$b03bfa40$1191680a@statman>
	<45CF85E4.5000509@stats.uwo.ca>
Message-ID: <001d01c74e22$134322d0$1191680a@statman>

 

> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch w stats.uwo.ca] 
> 
> eval(parse(text=my.data))
> 

I would like to thank everybody very much for help, but especially for
Duncan - it works wonderful.

Rob


From ebbaalm at uiuc.edu  Sun Feb 11 22:54:26 2007
From: ebbaalm at uiuc.edu (Cecilia Alm)
Date: Sun, 11 Feb 2007 15:54:26 -0600
Subject: [R] echo vs. print (for loop)
Message-ID: <7a4620dc0702111354q44c32518xdeb531416d0006ef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070211/6d6d40a5/attachment.pl 

From ggrothendieck at gmail.com  Sun Feb 11 22:56:17 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Feb 2007 16:56:17 -0500
Subject: [R] merge words=data name
In-Reply-To: <001901c74e1c$b03bfa40$1191680a@statman>
References: <001901c74e1c$b03bfa40$1191680a@statman>
Message-ID: <971536df0702111356i1e436aebx14a35f9807c3e690@mail.gmail.com>

Try this:

my.data <- M3[[sprintf("N%04d", 1)]]

On 2/11/07, Robert McFadden <robert-mcfadden at o2.pl> wrote:
> I would like to merge two parts of words to get a name of the data. First
> M3$N (invariable) and second is a number from 0001 to 3003 -
> M3$N0001,M3$N0002,...,M3$N3003. For example if I do it like this:
> my.data <- paste("M3$N",2456,sep="")
> I get
> > my.data
> [1] "M3$N2456"
> But I want to get something equivalent to
> my.data<- M3$N2456
>
> Is there any way to do it?
> It's important, because the second part are randomly chosen.
> (The data that I would like to extract are in package Mcomp.)
>
> Thanks in advance,
> Rob
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ebbaalm at uiuc.edu  Sun Feb 11 23:09:38 2007
From: ebbaalm at uiuc.edu (Cecilia Alm)
Date: Sun, 11 Feb 2007 16:09:38 -0600
Subject: [R] echo vs. print (for loop)
Message-ID: <7a4620dc0702111409t4338c6c4p715952492d2024d0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070211/27f64b5a/attachment.pl 

From vistocco at unicas.it  Sun Feb 11 23:12:28 2007
From: vistocco at unicas.it (Domenico Vistocco)
Date: Sun, 11 Feb 2007 23:12:28 +0100
Subject: [R] dyn.load problem under linux
In-Reply-To: <Pine.LNX.4.64.0702092211130.3084@gannet.stats.ox.ac.uk>
References: <45CCDA14.5030502@unicas.it> <45CCE7B1.1030507@pburns.seanet.com>
	<Pine.LNX.4.64.0702092211130.3084@gannet.stats.ox.ac.uk>
Message-ID: <45CF94CC.1010001@unicas.it>

Dear All,
thanks for your help. I have misreaded the error message (I am sorry).
The message of Phil Spector solved the problem.
I think it was posted out of the list: I copy it below (it could be 
useful to someone else).

Thanks a lot,
domenico

----------------------------------------------------------------------------------------------------------------------------------------------------------------
Domenico -
   Please read the error message more carefully.  The file R claims is 
missing is libstdc++-libc6.2-2.so.3, which, in Ubuntu Edgy, is provided
by the package libstdc++2.10-glibc2.2 .  So

      sudo apt-get install libstdc++2.10-glibc2.2

should resolve the problem.
                                       - Phil Spector
                     Statistical Computing Facility
                     Department of Statistics
                     UC Berkeley
                     spector at stat.berkeley.edu
----------------------------------------------------------------------------------------------------------------------------------------------------------------


Prof Brian Ripley wrote:
> This is a mismatch of compiler between the machine that the code was 
> compiled on and that running the code.
>
>>> '/usr/lib/R/library/POP.R/libs/ezlic20.so':
>>>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file
>
> libstdc++-libc6.2-2.so.3 is a g++ runtime.  On my FC5 system it is 
> provided by compat-libstdc++-296-2.96-135, so I think the problem is 
> that the package was compiled on a very old version of g++.
>
> If this is a 'package' ask for the sources.  (People who distribute 
> compiled C++ code of general use really should compile in the runtime 
> needed, taking care to meet the licence conditions.)
>
>
> On Fri, 9 Feb 2007, Patrick Burns wrote:
>
>> Some additional notes inserted below.
>>
>> Domenico Vistocco wrote:
>>
>>> Dear HelpeRs,
>>> I am trying to use an thirdy-part library under Linux  (the library is
>>> developed
>>> both for Windows and for Linux).
>>>
>>> I have tried different solutions (with the library developer) but we 
>>> are
>>> not able to
>>> solve the problem. So I try to ask for your help in order to escape 
>>> from
>>> the full stop
>>> where we are at the moment.
>>>
>>> The problem looks to depend on the dyn.load function (technical details
>>> are below).
>>>
>>> Thanks a lot for your work (it is an invaluable resource).
>>> Best,
>>> domenico
>>>
>>> The library is called POP.R. I installed it but I have the following
>>> error when
>>> I try to load it:
>>> --------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>> library(POP.R)
>>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>>        unable to load shared library
>>> '/usr/lib/R/library/POP.R/libs/ezlic20.so':
>>>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file
>>> or directory
>>> Error in library(POP.R) : .First.lib failed for 'POP.R'
>>> --------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>
>>> Nevertheless the file exists:
>>> --------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>> file.exists("/usr/lib/R/library/POP.R/libs/ezlic20.so")
>>> [1] TRUE
>>> --------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>
>>> I tried also directly using the dyn.load function on the command line
>>> (without and with the local flag):
>>> --------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>> dyn.load("/usr/lib/R/library/POP.R/libs/ezlic20.so",local=FALSE)
>>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>>        unable to load shared library
>>> '/usr/lib/R/library/POP.R/libs/ezlic20.so':
>>>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file
>>> or directory
>>>> dyn.load("/usr/lib/R/library/POP.R/libs/ezlic20.so",local=TRUE)
>>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>>        unable to load shared library
>>> '/usr/lib/R/library/POP.R/libs/ezlic20.so':
>>>  libstdc++-libc6.2-2.so.3: cannot open shared object file: No such file
>>> or directory
>>>
>>>
>>
>> We have seen that problems like this can arise if
>> there is a mixture of 32-bit and 64-bit items.  Everything
>> here is definitely 32-bit.
>>
>>> --------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>
>>> In the lib directory there is also another library, for which I have a
>>> different error
>>> message using the dyn.load function (I have the same message using
>>> local=FALSE):
>>> --------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>> dyn.load("/usr/lib/R/library/POP.R/libs/pop_BurSt.so")
>>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>>        unable to load shared library
>>> '/usr/lib/R/library/POP.R/libs/pop_BurSt.so':
>>>  /usr/lib/R/library/POP.R/libs/pop_BurSt.so: undefined symbol:
>>> getChainedKeyId
>>>
>>>
>>
>> This error message is actually good news -- it finds the
>> file (sitting right next to the one it can't "find") and gets so
>> far as to find a symbol that is in the first file.  That is, this
>> command appears like it would work if the first load worked.
>>
>>> --------------------------------------------------------------------------------------------------------------------------------------------- 
>>>
>>>
>>> My operating system is Ubuntu 6.10 -  Edgy.
>>>
>>>
>>
>> I'm not sure what versions of Linux other clients have
>> (it's never been an issue before), but I know it works
>> in Suse.
>>
>> Pat
>>
>>> Follow the answers to version and sessionInfo:
>>> -------------------------------------------------------------------------------------------------- 
>>>
>>>> version
>>>               _
>>> platform       i486-pc-linux-gnu
>>> arch           i486
>>> os             linux-gnu
>>> system         i486, linux-gnu
>>> status
>>> major          2
>>> minor          4.1
>>> year           2006
>>> month          12
>>> day            18
>>> svn rev        40228
>>> language       R
>>> version.string R version 2.4.1 (2006-12-18)
>>>
>>>> sessionInfo()
>>> R version 2.4.1 (2006-12-18)
>>> i486-pc-linux-gnu
>>>
>>> locale:
>>> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C 
>>>
>>>
>>> attached base packages:
>>> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  
>>> "methods"
>>> [7] "base"
>>> -------------------------------------------------------------------------------------------------- 
>>>
>>>
>>> Chiacchiera con i tuoi amici in tempo reale!
>>> http://it.yahoo.com/mail_it/foot/*http://it.messenger.yahoo.com
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

Chiacchiera con i tuoi amici in tempo reale! 
 http://it.yahoo.com/mail_it/foot/*http://it.messenger.yahoo.com


From jessilbrown at gmail.com  Sun Feb 11 23:38:52 2007
From: jessilbrown at gmail.com (Jessi Brown)
Date: Sun, 11 Feb 2007 14:38:52 -0800
Subject: [R] error using user-defined link function with mixed models
	(LMER)
In-Reply-To: <49996.82.231.93.240.1171204062.squirrel@webmail.lyon.inserm.fr>
References: <49660.82.231.93.240.1171199754.squirrel@webmail.lyon.inserm.fr>
	<40e66e0b0702110719o4f34d7baq4536d39144e5f813@mail.gmail.com>
	<49996.82.231.93.240.1171204062.squirrel@webmail.lyon.inserm.fr>
Message-ID: <371904a90702111438x3252fcbdhdf1083d3be22610a@mail.gmail.com>

Hmmm.. I had actually at one point tried the newer (>R 2.4.0) method
of specifying only a logexp link function and using it with the
binomial family, and once again, it seems to work as expected for
glm's, and not so much with LMER. The error message when using
summary() was the same. Here's output from a non-nest exposure
logistic regression:

> apfa.lmer.b.b.1<-lmer(Success~MeanAge+I(MeanAge^2)+(1|Territory), data=apfa4, family=binomial(link="logit"), method="Laplace", control=list(msVerbose=1))
relative tolerance set to 3.18713398786316e-05
  0      313.762:  3.69488 -0.100835 0.00165992 0.145015
  1      313.758:  3.69488 -0.100834 0.00166623 0.145015
  2      313.758:  3.69488 -0.100834 0.00166623 0.145015
  3      313.758:  3.69488 -0.100834 0.00166623 0.145015
> str(apfa.lmer.b.b.1)
Formal class 'glmer' [package "lme4"] with 33 slots
  ..@ family  :List of 11
  .. ..$ family    : chr "binomial"
  .. ..$ link      : chr "logit"
  .. ..$ linkfun   :function (mu)
  .. ..$ linkinv   :function (eta)
  .. ..$ variance  :function (mu)
  .. ..$ dev.resids:function (y, mu, wt)
  .. ..$ aic       :function (y, n, mu, wt, dev)
  .. ..$ mu.eta    :function (eta)
  .. ..$ initialize:  expression({     if (NCOL(y) == 1) {         if
(is.factor(y))              y <- y != levels(y)[1]         n <-
rep.int(1, nobs)         if (any(y < 0 | y > 1))              stop("y
values must be 0 <= y <= 1")         mustart <- (weights * y +
0.5)/(weights + 1)         m <- weights * y         if (any(abs(m -
round(m)) > 0.001))              warning("non-integer #successes in a
binomial glm!")     }     else if (NCOL(y) == 2) {         if
(any(abs(y - round(y)) > 0.001))              warning("non-integer
counts in a binomial glm!")         n <- y[, 1] + y[, 2]         y <-
ifelse(n == 0, 0, y[, 1]/n)         weights <- weights * n
mustart <- (n * y + 0.5)/(n + 1)     }     else stop("for the binomial
family, y must be a vector of 0 and 1's\n",          "or a 2 column
matrix where col 1 is no. successes and col 2 is no. failures") })
  .. ..$ validmu   :function (mu)
  .. ..$ valideta  :function (eta)
  .. ..- attr(*, "class")= chr "family"
  ..@ weights : Named num [1:662] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..- attr(*, "names")= chr [1:662] "1" "2" "3" "4" ...
  ..@ frame   :'data.frame':	662 obs. of  4 variables:
  .. ..$ Success     : int [1:662] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ MeanAge     : num [1:662] 5.5 12.5 19.5 26.5 33.5 40.5 47.5
54.5 61.5 66.5 ...
  .. ..$ I(MeanAge^2):Class 'AsIs'  num [1:662]   30.2  156.2  380.2
702.2 1122.2 ...
  .. ..$ Territory   : Factor w/ 36 levels "9 MILE","B-1",..: 1 1 1 1
1 1 1 1 1 1 ...
  .. ..- attr(*, "terms")=Classes 'terms', 'formula' length 3 Success
~ MeanAge + I(MeanAge^2) + (1 + Territory)
  .. .. .. ..- attr(*, "variables")= language list(Success, MeanAge,
I(MeanAge^2), Territory)
  .. .. .. ..- attr(*, "factors")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...
  .. .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. .. ..$ : chr [1:4] "Success" "MeanAge" "I(MeanAge^2)" "Territory"
  .. .. .. .. .. ..$ : chr [1:3] "MeanAge" "I(MeanAge^2)" "Territory"
  .. .. .. ..- attr(*, "term.labels")= chr [1:3] "MeanAge"
"I(MeanAge^2)" "Territory"
  .. .. .. ..- attr(*, "order")= int [1:3] 1 1 1
  .. .. .. ..- attr(*, "intercept")= int 1
  .. .. .. ..- attr(*, "response")= int 1
  .. .. .. ..- attr(*, ".Environment")=length 26 <environment>
  .. .. .. ..- attr(*, "predvars")= language list(Success, MeanAge,
I(MeanAge^2), Territory)
  .. .. .. ..- attr(*, "dataClasses")= Named chr [1:4] "numeric"
"numeric" "other" "factor"
  .. .. .. .. ..- attr(*, "names")= chr [1:4] "Success" "MeanAge"
"I(MeanAge^2)" "Territory"
  ..@ call    : language lmer(formula = Success ~ MeanAge +
I(MeanAge^2) + (1 | Territory),      data = apfa4, family =
binomial(link = "logit"), method = "Laplace",  ...
  ..@ terms   :Classes 'terms', 'formula' length 3 Success ~ MeanAge +
I(MeanAge^2)
  .. .. ..- attr(*, "variables")= language list(Success, MeanAge, I(MeanAge^2))
  .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:3] "Success" "MeanAge" "I(MeanAge^2)"
  .. .. .. .. ..$ : chr [1:2] "MeanAge" "I(MeanAge^2)"
  .. .. ..- attr(*, "term.labels")= chr [1:2] "MeanAge" "I(MeanAge^2)"
  .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=length 26 <environment>
  .. .. ..- attr(*, "predvars")= language list(Success, MeanAge, I(MeanAge^2))
  .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "numeric" "numeric" "other"
  .. .. .. ..- attr(*, "names")= chr [1:3] "Success" "MeanAge" "I(MeanAge^2)"
  ..@ flist   :List of 1
  .. ..$ Territory: Factor w/ 36 levels "9 MILE","B-1",..: 1 1 1 1 1 1
1 1 1 1 ...
  ..@ Zt      :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
  .. .. ..@ i       : int [1:662] 0 0 0 0 0 0 0 0 0 0 ...
  .. .. ..@ p       : int [1:663] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ Dim     : int [1:2] 36 662
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : NULL
  .. .. ..@ x       : num [1:662] 1 1 1 1 1 1 1 1 1 1 ...
  .. .. ..@ factors : list()
  ..@ X       : num [1:662, 1:3] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:662] "1" "2" "3" "4" ...
  .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. ..- attr(*, "assign")= int [1:3] 0 1 2
  ..@ y       : num [1:662] 1 1 1 1 1 1 1 1 1 1 ...
  ..@ wts     : num [1:662] 0.176 0.221 0.256 0.275 0.275 ...
  ..@ wrkres  : num [1:662] 4.44 3.97 3.66 3.50 3.50 ...
  ..@ cnames  :List of 2
  .. ..$ Territory: chr "(Intercept)"
  .. ..$ .fixed   : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  ..@ nc      : Named int 1
  .. ..- attr(*, "names")= chr "Territory"
  ..@ Gp      : int [1:2] 0 36
  ..@ XtX     :Formal class 'dpoMatrix' [package "Matrix"] with 5 slots
  .. .. ..@ x       : num [1:9]    39.7     0.0     0.0  1179.4 43661.6 ...
  .. .. ..@ Dim     : int [1:2] 3 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ uplo    : chr "U"
  .. .. ..@ factors : list()
  ..@ ZtZ     :Formal class 'dsCMatrix' [package "Matrix"] with 7 slots
  .. .. ..@ i       : int [1:36] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ p       : int [1:37] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ Dim     : int [1:2] 36 36
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : NULL
  .. .. ..@ x       : num [1:36] 1.416 0.534 1.069 0.529 1.376 ...
  .. .. ..@ uplo    : chr "U"
  .. .. ..@ factors : list()
  ..@ ZtX     :Formal class 'dgeMatrix' [package "Matrix"] with 4 slots
  .. .. ..@ x       : num [1:108] 1.416 0.534 1.069 0.529 1.376 ...
  .. .. ..@ Dim     : int [1:2] 36 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ factors : list()
  ..@ Zty     : num [1:36] 5.47 2.00 1.77 1.98 4.08 ...
  ..@ Xty     : num [1:3]    100   3026 117899
  ..@ Omega   :List of 1
  .. ..$ Territory:Formal class 'dpoMatrix' [package "Matrix"] with 5 slots
  .. .. .. ..@ x       : num 6.9
  .. .. .. ..@ Dim     : int [1:2] 1 1
  .. .. .. ..@ Dimnames:List of 2
  .. .. .. .. ..$ : chr "(Intercept)"
  .. .. .. .. ..$ : chr "(Intercept)"
  .. .. .. ..@ uplo    : chr "U"
  .. .. .. ..@ factors :List of 1
  .. .. .. .. ..$ Cholesky:Formal class 'Cholesky' [package "Matrix"]
with 5 slots
  .. .. .. .. .. .. ..@ x       : num 2.63
  .. .. .. .. .. .. ..@ Dim     : int [1:2] 1 1
  .. .. .. .. .. .. ..@ Dimnames:List of 2
  .. .. .. .. .. .. .. ..$ : NULL
  .. .. .. .. .. .. .. ..$ : NULL
  .. .. .. .. .. .. ..@ uplo    : chr "U"
  .. .. .. .. .. .. ..@ diag    : chr "N"
  ..@ L       :Formal class 'dCHMsuper' [package "Matrix"] with 9 slots
  .. .. ..@ x       : num [1:36] 2.88 2.73 2.82 2.72 2.88 ...
  .. .. ..@ super   : int [1:37] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ pi      : int [1:37] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ px      : int [1:37] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ s       : int [1:36] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ colcount: int [1:36] 1 1 1 1 1 1 1 1 1 1 ...
  .. .. ..@ perm    : int [1:36] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ type    : int [1:6] 0 1 1 1 1 1
  .. .. ..@ Dim     : int [1:2] 36 36
  ..@ RZX     :Formal class 'dgeMatrix' [package "Matrix"] with 4 slots
  .. .. ..@ x       : num [1:108] 0.491 0.196 0.379 0.194 0.479 ...
  .. .. ..@ Dim     : int [1:2] 36 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ factors : list()
  ..@ RXX     :Formal class 'dtrMatrix' [package "Matrix"] with 5 slots
  .. .. ..@ x       : num [1:9]   5.8   0.0   0.0 172.2  92.7 ...
  .. .. ..@ Dim     : int [1:2] 3 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ uplo    : chr "U"
  .. .. ..@ diag    : chr "N"
  ..@ rZy     : num [1:36] 1.896 0.733 0.628 0.726 1.419 ...
  ..@ rXy     : num [1:3] 14.54  0.49  2.66
  ..@ devComp : Named num [1:8] 662.00   3.00 872.61   6.41  74.81 ...
  .. ..- attr(*, "names")= chr [1:8] "n" "p" "yty" "logryy2" ...
  ..@ deviance: Named num [1:2] 314  NA
  .. ..- attr(*, "names")= chr [1:2] "ML" "REML"
  ..@ fixef   : num [1:3]  3.69488 -0.10083  0.00167
  ..@ ranef   : num [1:36]  0.2191  0.0834 -0.1204  0.0827  0.0703 ...
  ..@ RZXinv  :Formal class 'dgeMatrix' [package "Matrix"] with 4 slots
  .. .. ..@ x       : num [1:108]  2.12e-313 -1.57e-154   0.00e+00
6.88e-310  9.60e-206 ...
  .. .. ..@ Dim     : int [1:2] 36 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ factors : list()
  ..@ bVar    :List of 1
  .. ..$ Territory: num [1, 1, 1:36] 1.84e-299 3.83e-301 1.60e-213
1.34e-300 2.47e-300 ...
  ..@ gradComp:List of 1
  .. ..$ Territory: num [1, 1, 1:4] 2.74e-208 2.74e-208 8.39e-204 2.74e-208
  ..@ status  : Named int [1:3] 2 0 2
  .. ..- attr(*, "names")= chr [1:3] "stage" "REML" "glmm"



Now, here's the equivalent from using the logexp link function as
copied from the "family" help file example:

> apfa.lmer.b.1<-lmer(Success~MeanAge+I(MeanAge^2)+(1|Territory), data=apfa4, family=binomial(link="logexp"(apfa4$Days)), method="Laplace", control=list(msVerbose=1))
relative tolerance set to 3.49161751721116e-05
  0      286.400:  5.55948 -0.0908251 0.00149257 0.145015
  1      286.400:  5.55948 -0.0908251 0.00149257 0.145015
> str(apfa.lmer.b.1)
Formal class 'glmer' [package "lme4"] with 33 slots
  ..@ family  :List of 11
  .. ..$ family    : chr "binomial"
  .. ..$ link      : chr [1:662] "logexp(7)" "logexp(7)" "logexp(7)"
"logexp(7)" ...
  .. ..$ linkfun   :function (mu)
  .. .. ..- attr(*, "source")= chr "function(mu) qlogis(mu^(1/days))"
  .. ..$ linkinv   :function (eta)
  .. .. ..- attr(*, "source")= chr "function(eta) plogis(eta)^days"
  .. ..$ variance  :function (mu)
  .. ..$ dev.resids:function (y, mu, wt)
  .. ..$ aic       :function (y, n, mu, wt, dev)
  .. ..$ mu.eta    :function (eta)
  .. .. ..- attr(*, "source")= chr [1:2] "function(eta) days *
plogis(eta)^(days-1) *" ...
  .. ..$ initialize:  expression({     if (NCOL(y) == 1) {         if
(is.factor(y))              y <- y != levels(y)[1]         n <-
rep.int(1, nobs)         if (any(y < 0 | y > 1))              stop("y
values must be 0 <= y <= 1")         mustart <- (weights * y +
0.5)/(weights + 1)         m <- weights * y         if (any(abs(m -
round(m)) > 0.001))              warning("non-integer #successes in a
binomial glm!")     }     else if (NCOL(y) == 2) {         if
(any(abs(y - round(y)) > 0.001))              warning("non-integer
counts in a binomial glm!")         n <- y[, 1] + y[, 2]         y <-
ifelse(n == 0, 0, y[, 1]/n)         weights <- weights * n
mustart <- (n * y + 0.5)/(n + 1)     }     else stop("for the binomial
family, y must be a vector of 0 and 1's\n",          "or a 2 column
matrix where col 1 is no. successes and col 2 is no. failures") })
  .. ..$ validmu   :function (mu)
  .. ..$ valideta  :function (eta)
  .. .. ..- attr(*, "source")= chr "function(eta) TRUE"
  .. ..- attr(*, "class")= chr "family"
  ..@ weights : Named num [1:662] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..- attr(*, "names")= chr [1:662] "1" "2" "3" "4" ...
  ..@ frame   :'data.frame':	662 obs. of  4 variables:
  .. ..$ Success     : int [1:662] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ MeanAge     : num [1:662] 5.5 12.5 19.5 26.5 33.5 40.5 47.5
54.5 61.5 66.5 ...
  .. ..$ I(MeanAge^2):Class 'AsIs'  num [1:662]   30.2  156.2  380.2
702.2 1122.2 ...
  .. ..$ Territory   : Factor w/ 36 levels "9 MILE","B-1",..: 1 1 1 1
1 1 1 1 1 1 ...
  .. ..- attr(*, "terms")=Classes 'terms', 'formula' length 3 Success
~ MeanAge + I(MeanAge^2) + (1 + Territory)
  .. .. .. ..- attr(*, "variables")= language list(Success, MeanAge,
I(MeanAge^2), Territory)
  .. .. .. ..- attr(*, "factors")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...
  .. .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. .. ..$ : chr [1:4] "Success" "MeanAge" "I(MeanAge^2)" "Territory"
  .. .. .. .. .. ..$ : chr [1:3] "MeanAge" "I(MeanAge^2)" "Territory"
  .. .. .. ..- attr(*, "term.labels")= chr [1:3] "MeanAge"
"I(MeanAge^2)" "Territory"
  .. .. .. ..- attr(*, "order")= int [1:3] 1 1 1
  .. .. .. ..- attr(*, "intercept")= int 1
  .. .. .. ..- attr(*, "response")= int 1
  .. .. .. ..- attr(*, ".Environment")=length 25 <environment>
  .. .. .. ..- attr(*, "predvars")= language list(Success, MeanAge,
I(MeanAge^2), Territory)
  .. .. .. ..- attr(*, "dataClasses")= Named chr [1:4] "numeric"
"numeric" "other" "factor"
  .. .. .. .. ..- attr(*, "names")= chr [1:4] "Success" "MeanAge"
"I(MeanAge^2)" "Territory"
  ..@ call    : language lmer(formula = Success ~ MeanAge +
I(MeanAge^2) + (1 | Territory),      data = apfa4, family =
binomial(link = logexp(apfa4$Days)),  ...
  ..@ terms   :Classes 'terms', 'formula' length 3 Success ~ MeanAge +
I(MeanAge^2)
  .. .. ..- attr(*, "variables")= language list(Success, MeanAge, I(MeanAge^2))
  .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:3] "Success" "MeanAge" "I(MeanAge^2)"
  .. .. .. .. ..$ : chr [1:2] "MeanAge" "I(MeanAge^2)"
  .. .. ..- attr(*, "term.labels")= chr [1:2] "MeanAge" "I(MeanAge^2)"
  .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=length 25 <environment>
  .. .. ..- attr(*, "predvars")= language list(Success, MeanAge, I(MeanAge^2))
  .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "numeric" "numeric" "other"
  .. .. .. ..- attr(*, "names")= chr [1:3] "Success" "MeanAge" "I(MeanAge^2)"
  ..@ flist   :List of 1
  .. ..$ Territory: Factor w/ 36 levels "9 MILE","B-1",..: 1 1 1 1 1 1
1 1 1 1 ...
  ..@ Zt      :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
  .. .. ..@ i       : int [1:662] 0 0 0 0 0 0 0 0 0 0 ...
  .. .. ..@ p       : int [1:663] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ Dim     : int [1:2] 36 662
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : NULL
  .. .. ..@ x       : num [1:662] 1 1 1 1 1 1 1 1 1 1 ...
  .. .. ..@ factors : list()
  ..@ X       : num [1:662, 1:3] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:662] "1" "2" "3" "4" ...
  .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. ..- attr(*, "assign")= int [1:3] 0 1 2
  ..@ y       : num [1:662] 1 1 1 1 1 1 1 1 1 1 ...
  ..@ wts     : num [1:662] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  ..@ wrkres  : num [1:662] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  ..@ cnames  :List of 2
  .. ..$ Territory: chr "(Intercept)"
  .. ..$ .fixed   : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  ..@ nc      : Named int 1
  .. ..- attr(*, "names")= chr "Territory"
  ..@ Gp      : int [1:2] 0 36
  ..@ XtX     :Formal class 'dpoMatrix' [package "Matrix"] with 5 slots
  .. .. ..@ x       : num [1:9] NaN 0 0 NaN NaN 0 NaN NaN NaN
  .. .. ..@ Dim     : int [1:2] 3 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ uplo    : chr "U"
  .. .. ..@ factors : list()
  ..@ ZtZ     :Formal class 'dsCMatrix' [package "Matrix"] with 7 slots
  .. .. ..@ i       : int [1:36] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ p       : int [1:37] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ Dim     : int [1:2] 36 36
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : NULL
  .. .. ..@ x       : num [1:36] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  .. .. ..@ uplo    : chr "U"
  .. .. ..@ factors : list()
  ..@ ZtX     :Formal class 'dgeMatrix' [package "Matrix"] with 4 slots
  .. .. ..@ x       : num [1:108] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  .. .. ..@ Dim     : int [1:2] 36 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ factors : list()
  ..@ Zty     : num [1:36] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  ..@ Xty     : num [1:3] NaN NaN NaN
  ..@ Omega   :List of 1
  .. ..$ Territory:Formal class 'dpoMatrix' [package "Matrix"] with 5 slots
  .. .. .. ..@ x       : num 6.9
  .. .. .. ..@ Dim     : int [1:2] 1 1
  .. .. .. ..@ Dimnames:List of 2
  .. .. .. .. ..$ : chr "(Intercept)"
  .. .. .. .. ..$ : chr "(Intercept)"
  .. .. .. ..@ uplo    : chr "U"
  .. .. .. ..@ factors :List of 1
  .. .. .. .. ..$ Cholesky:Formal class 'Cholesky' [package "Matrix"]
with 5 slots
  .. .. .. .. .. .. ..@ x       : num 2.63
  .. .. .. .. .. .. ..@ Dim     : int [1:2] 1 1
  .. .. .. .. .. .. ..@ Dimnames:List of 2
  .. .. .. .. .. .. .. ..$ : NULL
  .. .. .. .. .. .. .. ..$ : NULL
  .. .. .. .. .. .. ..@ uplo    : chr "U"
  .. .. .. .. .. .. ..@ diag    : chr "N"
  ..@ L       :Formal class 'dCHMsuper' [package "Matrix"] with 9 slots
  .. .. ..@ x       : num [1:36] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  .. .. ..@ super   : int [1:37] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ pi      : int [1:37] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ px      : int [1:37] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ s       : int [1:36] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ colcount: int [1:36] 1 1 1 1 1 1 1 1 1 1 ...
  .. .. ..@ perm    : int [1:36] 0 1 2 3 4 5 6 7 8 9 ...
  .. .. ..@ type    : int [1:6] 0 1 1 1 1 1
  .. .. ..@ Dim     : int [1:2] 36 36
  ..@ RZX     :Formal class 'dgeMatrix' [package "Matrix"] with 4 slots
  .. .. ..@ x       : num [1:108] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  .. .. ..@ Dim     : int [1:2] 36 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ factors : list()
  ..@ RXX     :Formal class 'dtrMatrix' [package "Matrix"] with 5 slots
  .. .. ..@ x       : num [1:9] NaN 0 0 NaN NaN 0 NaN NaN NaN
  .. .. ..@ Dim     : int [1:2] 3 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ uplo    : chr "U"
  .. .. ..@ diag    : chr "N"
  ..@ rZy     : num [1:36] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  ..@ rXy     : num [1:3] NaN NaN NaN
  ..@ devComp : Named num [1:8] 662   3 NaN NaN NaN ...
  .. ..- attr(*, "names")= chr [1:8] "n" "p" "yty" "logryy2" ...
  ..@ deviance: Named num [1:2] NaN NaN
  .. ..- attr(*, "names")= chr [1:2] "ML" "REML"
  ..@ fixef   : num [1:3]  5.55948 -0.09083  0.00149
  ..@ ranef   : num [1:36] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
  ..@ RZXinv  :Formal class 'dgeMatrix' [package "Matrix"] with 4 slots
  .. .. ..@ x       : num [1:108]  2.12e-313 -1.57e-154   0.00e+00
7.53e-310  1.74e-206 ...
  .. .. ..@ Dim     : int [1:2] 36 3
  .. .. ..@ Dimnames:List of 2
  .. .. .. ..$ : NULL
  .. .. .. ..$ : chr [1:3] "(Intercept)" "MeanAge" "I(MeanAge^2)"
  .. .. ..@ factors : list()
  ..@ bVar    :List of 1
  .. ..$ Territory: num [1, 1, 1:36]  0.00e+00  0.00e+00  0.00e+00
1.76e-315 1.76e-315 ...
  ..@ gradComp:List of 1
  .. ..$ Territory: num [1, 1, 1:4] 2.12e-314 4.24e-314       NaN       NaN
  ..@ status  : Named int [1:3] 2 0 2
  .. ..- attr(*, "names")= chr [1:3] "stage" "REML" "glmm"


It's obvious that there are a lot of NaN's throughout the second
example, which I imagine are indicative of something not going as
planned, but I'm afraid I'm not familiar enough with the mathematical
reality behind these models to know what exactly is going on.

To review, here's the logexp link function suggested in the ?family file:
logexp <- function(days = 1)
{
    linkfun <- function(mu) qlogis(mu^(1/days))
    linkinv <- function(eta) plogis(eta)^days
    mu.eta <- function(eta) days * plogis(eta)^(days-1) *
      .Call("logit_mu_eta", eta, PACKAGE = "stats")
    valideta <- function(eta) TRUE
    link <- paste("logexp(", days, ")", sep="")
    structure(list(linkfun = linkfun, linkinv = linkinv,
                   mu.eta = mu.eta, valideta = valideta, name = link),
              class = "link-glm")
}

Any ideas? By the way, I am extremely grateful for all of the help so
far - I know this sort of thing must be tedious for you. Thank you for
putting up with a graduate student whose strength is in finding and
following birds, not programming or even thoroughly understanding the
complex underpinnings of the sorts of models I blithely attempt to
run!

cheers, Jessi Brown


From wangtong at usc.edu  Mon Feb 12 00:21:55 2007
From: wangtong at usc.edu (Tong Wang)
Date: Sun, 11 Feb 2007 15:21:55 -0800
Subject: [R] problem with rinvgamma ?
Message-ID: <dd179780bb76.45cf3493@usc.edu>

Hi,
    By  rinvgamma(10000,1.33,2.33) ,   am I supposed to get a var=3 ?   Tried it many times , not even close.  Why is this?
thanks a lot .

best


From murdoch at stats.uwo.ca  Mon Feb 12 00:32:14 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 11 Feb 2007 18:32:14 -0500
Subject: [R] merge words=data name
In-Reply-To: <001d01c74e22$134322d0$1191680a@statman>
References: <001901c74e1c$b03bfa40$1191680a@statman>	<45CF85E4.5000509@stats.uwo.ca>
	<001d01c74e22$134322d0$1191680a@statman>
Message-ID: <45CFA77E.8050308@stats.uwo.ca>

On 2/11/2007 4:17 PM, Robert McFadden wrote:
>  
> 
>> -----Original Message-----
>> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
>>
>> eval(parse(text=my.data))
>>
> 
> I would like to thank everybody very much for help, but especially for
> Duncan - it works wonderful.

You're welcome, but I have to say I like Gabor's solution better than 
mine, assuming that M3 is fixed.

Duncan Murdoch


From gelman at stat.columbia.edu  Mon Feb 12 00:34:09 2007
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Sun, 11 Feb 2007 18:34:09 -0500
Subject: [R] problem with Matrix package
In-Reply-To: <40e66e0b0702111129g7c196eefnf18128601e203a78@mail.gmail.com>
References: <45CF5F30.1080209@stat.columbia.edu>	
	<40e66e0b0702111038k3db7e0c9j9c89a0d694d35325@mail.gmail.com>	
	<45CF6990.1000201@stat.columbia.edu>
	<40e66e0b0702111129g7c196eefnf18128601e203a78@mail.gmail.com>
Message-ID: <45CFA7F1.8060902@stat.columbia.edu>

Doug
Reinstalling R did the trick.  Thanks.
Andrew


Douglas Bates wrote:
> On 2/11/07, Andrew Gelman <gelman at stat.columbia.edu> wrote:
>> Doug
>> Yes, it's R 2.4.1.  I'm trying to keep up with Matrix for various
>> reasons, most immediately that our "arm" package requires Matrix.
>> Andrew
>
> The source package for version 0.9975-10 Matrix has just been moved
> onto CRAN.  Binary versions should hit the mirrors in a day or two.  I
> suggest you try with that version.
>
> In the meantime try
>
> str(get("Logic", "package:methods"))
>
> I get
>
>> str(get("Logic", "package:methods"))
> Formal class 'groupGenericFunction' [package "methods"] with 9 slots
>  ..@ .Data       :function (e1, e2)
>  ..@ groupMembers:List of 2
>  .. ..$ : chr "&"
>  .. ..$ : chr "|"
>  ..@ generic     : atomic [1:1] Logic
>  .. ..- attr(*, "package")= chr "base"
>  ..@ package     : chr "base"
>  ..@ group       :List of 1
>  .. ..$ : chr "Ops"
>  ..@ valueClass  : chr(0)
>  ..@ signature   : chr [1:2] "e1" "e2"
>  ..@ default     :Formal class 'MethodsList' [package "methods"] with 
> 3 slots
>  .. .. ..@ methods   : list()
>  .. .. ..@ argument  : symbol e1
>  .. .. ..@ allMethods: list()
>  ..@ skeleton    : language function (e1, e2)  stop("invalid call in
> method dispatch to \"Logic\" (no default method)",  ...
>
> The Windows binary version of the Matrix package that you installed is
> several weeks old.  If it didn't install on a stock version of R-2.4.1
> I imagine we would have heard about it before now.  I think you may
> need to check if somehow you have an old version of the methods
> package on your search path or just install a fresh copy of R-2.4.1
>
>
>>
>> Douglas Bates wrote:
>> > On 2/11/07, Andrew Gelman <gelman at stat.columbia.edu> wrote:
>> >> I decided to update my packages and then had a problem with 
>> loading the
>> >> Matrix package
>> >> 
>> http://cran.at.r-project.org/bin/windows/contrib/2.4/Matrix_0.9975-9.zip
>> >> This is what happened when I tried to load it in:
>> >>
>> >>  > library("Matrix")
>> >> Error in importIntoEnv(impenv, impnames, ns, impvars) :
>> >>         object 'Logic' is not exported by 'namespace:methods'
>> >> Error: package/namespace load failed for 'Matrix'
>> >>  >
>> >>
>> >> I hadn't had this probldem before; any thoughts?
>> >>
>> >> Andrew
>> >
>> > And the version of R that you are using is?
>> >
>> > Version 0.9975-9 of the Matrix package depends on R >= 2.4.1 for
>> > exactly this reason.
>>
>> -- 
>> Andrew Gelman
>> Professor, Department of Statistics
>> Professor, Department of Political Science
>> Columbia University, New York
>> gelman at stat.columbia.edu
>> www.stat.columbia.edu/~gelman
>>
>> Office hours spring 2006:
>>   to be announced
>>
>> Statistics department office:
>>   Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
>>   212-851-2142
>> Political Science department office:
>>   International Affairs Bldg (Amsterdam Ave at 118 St), Room 731
>>   212-854-7075
>>
>> Mailing address:
>>   1255 Amsterdam Ave, Room 1016
>>   Columbia University
>>   New York, NY 10027-5904
>>   212-851-2142
>>   (fax) 212-851-2164
>>
>>

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
Columbia University, New York
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Office hours spring 2006:
  to be announced

Statistics department office:
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Political Science department office:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 731
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164


From mwkimpel at gmail.com  Mon Feb 12 02:57:08 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Sun, 11 Feb 2007 20:57:08 -0500
Subject: [R] merge words=data name
In-Reply-To: <45CFA77E.8050308@stats.uwo.ca>
References: <001901c74e1c$b03bfa40$1191680a@statman>	<45CF85E4.5000509@stats.uwo.ca>	<001d01c74e22$134322d0$1191680a@statman>
	<45CFA77E.8050308@stats.uwo.ca>
Message-ID: <45CFC974.7030307@gmail.com>

Duncan,

Both yours and Gabor's methods were far superior to mine. I am curious 
why you like Gabor's better than yours. From the perspective of someone 
who uses R regularly but has only read about C, yours seems more 
"R-like". Would Gabor's be more computationally efficient if the loop 
was big enough?

I ask this because it made me ask myself, "are the C-like functions of R 
'better' (computationally) than the more R-like ones?"

Am I making sense?

Mark

Duncan Murdoch wrote:
> On 2/11/2007 4:17 PM, Robert McFadden wrote:
>>  
>>
>>> -----Original Message-----
>>> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
>>>
>>> eval(parse(text=my.data))
>>>
>> I would like to thank everybody very much for help, but especially for
>> Duncan - it works wonderful.
> 
> You're welcome, but I have to say I like Gabor's solution better than 
> mine, assuming that M3 is fixed.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Mark W. Kimpel MD
Neuroinformatics
Department of Psychiatry
Indiana University School of Medicine


From skiadas at hanover.edu  Mon Feb 12 03:22:29 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sun, 11 Feb 2007 21:22:29 -0500
Subject: [R] merge words=data name
In-Reply-To: <45CFC974.7030307@gmail.com>
References: <001901c74e1c$b03bfa40$1191680a@statman>	<45CF85E4.5000509@stats.uwo.ca>	<001d01c74e22$134322d0$1191680a@statman>
	<45CFA77E.8050308@stats.uwo.ca> <45CFC974.7030307@gmail.com>
Message-ID: <723D6629-2B44-4774-86A7-C4E11D409E33@hanover.edu>

On Feb 11, 2007, at 8:57 PM, Mark W Kimpel wrote:
> Duncan,
>
> Both yours and Gabor's methods were far superior to mine. I am curious
> why you like Gabor's better than yours.

Don't know if the following is why Duncan prefers Gabor's method, but  
here is why I would avoid the eval version: In general "eval" is very  
dangerous to call, unless you have full control over the expression  
you are asking it to evaluate. For instance imagine the following:

txt<- "system('ls')"
eval(parse(text=txt))

(replace 'ls' with 'dir' on a windows system)

With these two commands you will get a listing of everything in your  
home directory, or wherever the current path for R is. But suppose  
instead that the 'ls' was replaced by 'rm -rf *'. Then EVERYTHING in  
that directory will be DELETED, for ever, NO questions asked. (at  
least on a unix based system, perhaps even Windows with cygwin, I  
don't know. There is probably a similar call for windows).

In other words, make sure you know EXACTLY what the thing you are  
evaluating is.

I am not saying this is necessarily a danger here, but it brings up  
an important point that is good to be aware of.

Haris


From murdoch at stats.uwo.ca  Mon Feb 12 03:39:25 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 11 Feb 2007 21:39:25 -0500
Subject: [R] merge words=data name
In-Reply-To: <45CFC974.7030307@gmail.com>
References: <001901c74e1c$b03bfa40$1191680a@statman>	<45CF85E4.5000509@stats.uwo.ca>	<001d01c74e22$134322d0$1191680a@statman>	<45CFA77E.8050308@stats.uwo.ca>
	<45CFC974.7030307@gmail.com>
Message-ID: <45CFD35D.4030909@stats.uwo.ca>

On 2/11/2007 8:57 PM, Mark W Kimpel wrote:
> Duncan,
> 
> Both yours and Gabor's methods were far superior to mine. I am curious 
> why you like Gabor's better than yours. From the perspective of someone 
> who uses R regularly but has only read about C, yours seems more 
> "R-like". Would Gabor's be more computationally efficient if the loop 
> was big enough?
> 
> I ask this because it made me ask myself, "are the C-like functions of R 
> 'better' (computationally) than the more R-like ones?"

Basically the reason I liked Gabor's solution is related to what Haris 
said.  I wouldn't worry so much about potential damage, but the 
possibility of getting something quite different from what you expect is 
larger with my solution.  Gabor's selected a particular component from a 
list, which is really what you wanted to do.

I suspect the difference in computational efficiency between the two 
wouldn't really be measurable, but Gabor's would be a little better. 
The main benefit of it is clarity.

Duncan Murdoch

> 
> Am I making sense?
> 
> Mark
> 
> Duncan Murdoch wrote:
>> On 2/11/2007 4:17 PM, Robert McFadden wrote:
>>>  
>>>
>>>> -----Original Message-----
>>>> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
>>>>
>>>> eval(parse(text=my.data))
>>>>
>>> I would like to thank everybody very much for help, but especially for
>>> Duncan - it works wonderful.
>> You're welcome, but I have to say I like Gabor's solution better than 
>> mine, assuming that M3 is fixed.
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From estrain at postoffice.utas.edu.au  Mon Feb 12 04:47:00 2007
From: estrain at postoffice.utas.edu.au (estrain at postoffice.utas.edu.au)
Date: Mon, 12 Feb 2007 14:47:00 +1100
Subject: [R] lmer
Message-ID: <200702120347.l1C3l1j4001441@corinna.its.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070212/cbabdd19/attachment.pl 

From ebbaalm at uiuc.edu  Mon Feb 12 05:04:42 2007
From: ebbaalm at uiuc.edu (Cecilia Alm)
Date: Sun, 11 Feb 2007 22:04:42 -0600
Subject: [R] Boxplot: quartiles/outliers
Message-ID: <7a4620dc0702112004s69037383r986a11ace1a196c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070211/448240a0/attachment.pl 

From mrennie at utm.utoronto.ca  Mon Feb 12 07:51:54 2007
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 12 Feb 2007 01:51:54 -0500
Subject: [R] Trying to replicate error message in subset()
Message-ID: <6.1.0.6.0.20070212014013.01baf970@mail.utm.utoronto.ca>


Hi, there

I am trying to replicate an error message in subset() to see what it is 
that I'm doing wrong with the datasets I am trying to work with.

Essentially, I am trying to pass a string vector to subset() in order to 
select a specific collection of cases (i.e., I have data for these cases in 
one table, and want to select data from another table that match up with 
the cases in the first table).

The error I get is as follows:

Warning messages:
1: longer object length
         is not a multiple of shorter object length in: is.na(e1) | is.na(e2)
2: longer object length
         is not a multiple of shorter object length in: `==.default`(LAKE, g)

Here is an example case I've been working with (which works) that I've been 
trying to "break"such that I can get this error message to figure out what 
I am doing wrong in my case.

y1<-rnorm(100, 2)
x1<-rep(1:5, each=20)
x2<-rep(1:2, each=10, times=10)

ex.dat<-data.frame(cbind(y1,x1,x2))


ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B", "C", "D", "E"))
ex.dat$x2<-factor(ex.dat$x2, labels=c("B", "D"))

a<-c("D", "F")
a

new.dat<-subset(ex.dat, x1 == a)
new.dat

I thought maybe I was getting errors because I had cases in my selection 
vector ('a' in this case) that weren't in my ex.dat list, but subset 
handles this fine and just gives me what it can find in the larger list.

Any thoughts on how I can replicate the error? As far as I can tell, the 
only difference between the case where I am getting errors and the example 
above is that the levels of x1 in my case are words (i.e., "Smelly", 
"Howdy"), but strings are strings, aren't they?

Mike


Michael Rennie
Ph.D. Candidate, University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792
www.utm.utoronto.ca/~w3rennie


From ndoye_p at hotmail.com  Mon Feb 12 08:22:43 2007
From: ndoye_p at hotmail.com (Ndoye Souleymane)
Date: Mon, 12 Feb 2007 07:22:43 +0000
Subject: [R] spss file import
In-Reply-To: <90AAFBD4-FA98-4F76-B6E7-882C6B7E0686@imperial.ac.uk>
Message-ID: <BAY102-F69ADE8E139B33BBBA677599910@phx.gbl>

Hi,

Let me suggest you to save your spss file in txt and use the read.table 
function to load your file in R.
That is what I use to do.

Souleymane


>From: Federico Calboli <f.calboli at imperial.ac.uk>
>To: r-help <r-help at stat.math.ethz.ch>
>Subject: [R] spss file import
>Date: Wed, 7 Feb 2007 16:16:14 +0000
>
>Hi All,
>
>does anyone ever import old SPSS files in a sl3 format?
>
>read.spss('file.sl3') does not seem to work... it's not recognised as
>a supported SPSS format at all.
>
>Best,
>
>Fede
>
>--
>Federico C. F. Calboli
>Department of Epidemiology and Public Health
>Imperial College, St. Mary's Campus
>Norfolk Place, London W2 1PG
>
>Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
>
>f.calboli [.a.t] imperial.ac.uk
>f.calboli [.a.t] gmail.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

_________________________________________________________________
Ten :  Messenger en illimit? sur votre mobile !


From petr.pikal at precheza.cz  Mon Feb 12 08:46:52 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 12 Feb 2007 08:46:52 +0100
Subject: [R] Trying to replicate error message in subset()
In-Reply-To: <6.1.0.6.0.20070212014013.01baf970@mail.utm.utoronto.ca>
Message-ID: <45D0297C.5943.4DFF87@localhost>

Hi

it is not error it is just warning (Beeping a tea kettle with boiling 
water is also not an error :-)
and it tells you pretty explicitly what is wrong
see length of your objects

> a<-c("D", "F", "A")
> new.dat<-subset(ex.dat, x1 == a)
Warning messages:
1: longer object length
        is not a multiple of shorter object length in: is.na(e1) | 
is.na(e2) 
2: longer object length
        is not a multiple of shorter object length in: 
`==.default`(x1, a) 
> new.dat
            y1 x1 x2
3    0.5977786  A  B
6    2.5470739  A  B
9    0.9128595  A  B
12   1.0953531  A  D
15   2.4984470  A  D
18   1.7289529  A  D
61  -0.4848938  D  B
6

you can do better with %in% operator.

HTH
Petr



On 12 Feb 2007 at 1:51, Michael Rennie wrote:

Date sent:      	Mon, 12 Feb 2007 01:51:54 -0500
To:             	r-help at stat.math.ethz.ch
From:           	Michael Rennie <mrennie at utm.utoronto.ca>
Subject:        	[R] Trying to replicate error message in subset()

> 
> Hi, there
> 
> I am trying to replicate an error message in subset() to see what it
> is that I'm doing wrong with the datasets I am trying to work with.
> 
> Essentially, I am trying to pass a string vector to subset() in order
> to select a specific collection of cases (i.e., I have data for these
> cases in one table, and want to select data from another table that
> match up with the cases in the first table).
> 
> The error I get is as follows:
> 
> Warning messages:
> 1: longer object length
>          is not a multiple of shorter object length in: is.na(e1) |
>          is.na(e2)
> 2: longer object length
>          is not a multiple of shorter object length in:
>          `==.default`(LAKE, g)
> 
> Here is an example case I've been working with (which works) that I've
> been trying to "break"such that I can get this error message to figure
> out what I am doing wrong in my case.
> 
> y1<-rnorm(100, 2)
> x1<-rep(1:5, each=20)
> x2<-rep(1:2, each=10, times=10)
> 
> ex.dat<-data.frame(cbind(y1,x1,x2))
> 
> 
> ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B", "C", "D", "E"))
> ex.dat$x2<-factor(ex.dat$x2, labels=c("B", "D"))
> 
> a<-c("D", "F")
> a
> 
> new.dat<-subset(ex.dat, x1 == a)
> new.dat
> 
> I thought maybe I was getting errors because I had cases in my
> selection vector ('a' in this case) that weren't in my ex.dat list,
> but subset handles this fine and just gives me what it can find in the
> larger list.
> 
> Any thoughts on how I can replicate the error? As far as I can tell,
> the only difference between the case where I am getting errors and the
> example above is that the levels of x1 in my case are words (i.e.,
> "Smelly", "Howdy"), but strings are strings, aren't they?
> 
> Mike
> 
> 
> Michael Rennie
> Ph.D. Candidate, University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> www.utm.utoronto.ca/~w3rennie
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ry.tamura at gmail.com  Mon Feb 12 08:47:04 2007
From: ry.tamura at gmail.com (ry.tamura at gmail.com)
Date: Mon, 12 Feb 2007 02:47:04 -0500
Subject: [R] Rdonlp2 - an extension library for constrained optimization
Message-ID: <27455534.1171266424279.JavaMail.root@wombat.diezmil.com>

Hello R-lists,

I have created an extension library called "Rdonlp2".
This is a wrapper for Peter Spellucci's DONLP2:
http://plato.la.asu.edu/donlp2.html

DONLP2 is a standalone C program that can solve
following minimization problem:

min f(x) subject to
* parameter bounds
* linear (equality) constraints 
* nonlinear (equality) constraints 

Rdonlp2 is a bridge between R and DONLP2, that is,
you can execute the constrained optimization with 
R function and objects and get detailed outputs from DONLP2 as
R objects.

Some tutorial, source code, Windows binary, OSX binary are available from:
http://arumat.net/Rdonlp2/
The package contains the tutorial as PDF, and 4 demo scripts.

Any comments or suggestions are welcome.

Regards,

Ryuichi Tamura


--
This message was sent on behalf of ry.tamura at gmail.com at openSubscriber.com
http://www.opensubscriber.com/messages/r-help at stat.math.ethz.ch/topic.html


From ripley at stats.ox.ac.uk  Mon Feb 12 08:50:01 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Feb 2007 07:50:01 +0000 (GMT)
Subject: [R] Boxplot: quartiles/outliers
In-Reply-To: <7a4620dc0702112004s69037383r986a11ace1a196c@mail.gmail.com>
References: <7a4620dc0702112004s69037383r986a11ace1a196c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702120743560.24914@gannet.stats.ox.ac.uk>

If you look at boxplot.default you will see that it calls boxplot.stats to 
compute the statistics used.  So you can make renamed copies of both 
functions and alter them to behave as you like.  (Because of namespace 
issues, you cannot just use your own boxplot.stats.)

I am not sure why you would want to do this, and a 'box and whiskers plot' 
was pretty closely defined by the original authors so R does not allow 
options for forms they did not consider.  But as it is a programming 
language it is pretty easy to produce your own version.

On Sun, 11 Feb 2007, Cecilia Alm wrote:

> For boxplot(),  is it possible to pass in a parameter to change the default
> way that the 1st and 3rd quartiles are computed? (specifically, I'd like to
> use type 6 described in the quantile function).
>
> Also, what are the options for how outliers are computed, and how can one
> change them?
>
> Thank you
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p_connolly at ihug.co.nz  Mon Feb 12 08:50:16 2007
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Mon, 12 Feb 2007 20:50:16 +1300
Subject: [R] R in Industry
In-Reply-To: <45C988DC.20006@bitwrit.com.au>
References: <45C988DC.20006@bitwrit.com.au>
Message-ID: <20070212075016.GM15607@ihug.co.nz>

On Wed, 07-Feb-2007 at 07:07PM +1100, Jim Lemon wrote:

|> Matthew Keller wrote:
|>  > Far from flaming you, I think you made a good point - one that I
|>  > imagine most people who use R have come across. The name "R" is a big
|>  > impediment to effective online searches. As a check, I entered "R
|>  > software", "SAS software", SPSS software", and "S+ software" into
|>  > google. The R 'hit rate' was only ten out of the first 20 results (I
|>  > didn't look any further). For the other three software packages, the
|>  > hit rates were all 100% (20/20).
|>  >
|>  > I do wonder if anything can/should be done about this. I generally
|>  > search using the term "CRAN" but of course, that omits lots of stuff
|>  > relevant to R. Any ideas about how to do effective online searches for
|>  > "R" related materials?
|>  >
|> Try "r stats". I get 18/20 on Google with that.

Not bad, but the original question was about R related employment.
Trying "R jobs" or "R employment" comes up with Hungarian girls
looking for a job in Cork (I think the letter 'r' in 'Cork' that had
that one show up) and somewhat further down the list comes a question
about jobs that take a long time running MCMC using R in ESS.  And
somewhat further still before there's an R-help archive where someone
asked a similar question to what started this thread.  Not a lot of
use.

Trouble is now I've clicked on some of those, they'll rate higher on
Google's ranking so I'm perpetuating the problem.

Did anyone think of a search string that wasn't useless?


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From petr.pikal at precheza.cz  Mon Feb 12 08:56:01 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 12 Feb 2007 08:56:01 +0100
Subject: [R] Boxplot: quartiles/outliers
In-Reply-To: <7a4620dc0702112004s69037383r986a11ace1a196c@mail.gmail.com>
Message-ID: <45D02BA1.9831.56600F@localhost>

Hi

as documentation does not say anything about changing quantile 
computation it seems to me that you have 2 options:

1.	change source code
2. 	change values computed by boxplot.stats and do plotting with 
newly computed values by bxp

see respective help pages and look at structure for boxplot

e.g.

mybox <- boxplot(something)
mybox$stats <- changing.function(something)
bxp(mybox)

HTH
Petr


On 11 Feb 2007 at 22:04, Cecilia Alm wrote:

Date sent:      	Sun, 11 Feb 2007 22:04:42 -0600
From:           	"Cecilia Alm" <ebbaalm at uiuc.edu>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Boxplot: quartiles/outliers

> For boxplot(),  is it possible to pass in a parameter to change the
> default way that the 1st and 3rd quartiles are computed?
> (specifically, I'd like to use type 6 described in the quantile
> function).
> 
> Also, what are the options for how outliers are computed, and how can
> one change them?
> 
> Thank you
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From gregor.gorjanc at bfro.uni-lj.si  Mon Feb 12 09:22:43 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 12 Feb 2007 08:22:43 +0000 (UTC)
Subject: [R] setting a number of values to NA over  a data.frame.
References: <27739.27117.qm@web32804.mail.mud.yahoo.com>
	<45CAED75.6060401@bitwrit.com.au>
Message-ID: <loom.20070212T091928-363@post.gmane.org>

Jim Lemon <jim <at> bitwrit.com.au> writes:
> Hi John,
> You might have a look at "toNA" in the prettyR package. Wait for version 
> 1.0-4, just uploaded, as I have fixed a bug in that function.

There is also a set of generic functions exactly for such cases: unknownToNA(),
NAToUnknown() and isUnknown() in gdata package.

Gregor


From christian.ritter at shell.com  Mon Feb 12 09:30:59 2007
From: christian.ritter at shell.com (christian.ritter at shell.com)
Date: Mon, 12 Feb 2007 09:30:59 +0100
Subject: [R] SQL statements (directly)  in R
In-Reply-To: <8828460.post@talk.nabble.com>
Message-ID: <156CDC8CCFD1894295D2907F16337A4801420A83@bru-s-006.europe.shell.com>

Hi R-users,

This note will interest people who would like to use sql statements on R data frames (a bit like proc sql in SAS). Please reply to my only, unless you really want to keep the entire R-help list posted on this. 

I've been thinking about a packgage implementing sql queries in R. I'm almost about starting to write it in a very rudimentary version. What I have in mind is the following:

Work in two ways:
via a generic sql("..") wrapper which allows a generic query statement
and via convenience functions, such as SELECT("..."), ...
what would be needed is an "sqlTable" class extending the data frame. This class will have to have extra slots for indices and some other stuff. I would try to stay very basic in the beginning and also use relatively inefficient handling of the tables. Later-on, direct calls using the binary representations could  replace the high level handling. 

Now come my questions:
- have others started working on this?
- are others interested in this?
- ideas on how to go about it?

Chris

P.S.: 
Here are a few ideas I was thinking about
One way would be to incorporate a gpl or lgpl rdbms into the package, to push the data-frame to it, to execute the statement there and to get the result back. The advantage: fast to implement. The disadvantage: pushing the data is a bad idea (but then again, at the top level, R will make a copy of it anyway, most probably). The convenience wrappers would then construct sql statements and the db engine would evaluate them. 

The other idea is to stay in R and to link the wrappers to adequately composed calls to subset, cbind, rbind, etc. Here it would be more challenging to create the sql("..") interface since its string would have to be parsed. 

The political incorrect thing about these SQL functions is that they (UPDATE, INSERT) will have to modify objects within the function call. They would not work via the return object. 

As I said, comments welcome.


From k00aher at student.chalmers.se  Mon Feb 12 10:47:23 2007
From: k00aher at student.chalmers.se (Ernst O Ahlberg Helgee)
Date: Mon, 12 Feb 2007 10:47:23 +0100 (CET)
Subject: [R] How to get the polynomials out of poly()
Message-ID: <Pine.LNX.4.64.0702121041190.20641@gamma02.me.chalmers.se>

Hi Folks!
Im using the function poly to generate orthogonal polynomials, but Id like 
to see the actual polynomials so that I could convert it to a polynomial 
in my original variable. Is that possible and if so how do I do it?
/E


From ripley at stats.ox.ac.uk  Mon Feb 12 11:22:42 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Feb 2007 10:22:42 +0000 (GMT)
Subject: [R] How to get the polynomials out of poly()
In-Reply-To: <Pine.LNX.4.64.0702121041190.20641@gamma02.me.chalmers.se>
References: <Pine.LNX.4.64.0702121041190.20641@gamma02.me.chalmers.se>
Message-ID: <Pine.LNX.4.64.0702121018210.7482@gannet.stats.ox.ac.uk>

plot(raw=TRUE) does this for you: there is little point in fitting by 
orthogonal polynomials and then converting back.

As the help page says quite explicitly, poly() does not store the 
orthogonal polynomials: please also consult the reference it gives.  It 
also points out that predict() creates them.


On Mon, 12 Feb 2007, Ernst O Ahlberg Helgee wrote:

> Hi Folks!
> Im using the function poly to generate orthogonal polynomials, but Id like
> to see the actual polynomials so that I could convert it to a polynomial
> in my original variable. Is that possible and if so how do I do it?
> /E

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From f.calboli at imperial.ac.uk  Mon Feb 12 12:03:42 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 12 Feb 2007 11:03:42 +0000
Subject: [R] spss file import
In-Reply-To: <BAY102-F69ADE8E139B33BBBA677599910@phx.gbl>
References: <BAY102-F69ADE8E139B33BBBA677599910@phx.gbl>
Message-ID: <45D0498E.5050206@imperial.ac.uk>

Ndoye Souleymane wrote:
> Hi,
> 
> Let me suggest you to save your spss file in txt and use the read.table 
> function to load your file in R.
> That is what I use to do.

The problem here is that the files are old data that were made with an ancient 
version of spss and cannot be changed to txt (or anything else) now.

It looks like read.spss() does not import them in whatever .sl3 is.

Best

Federico

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From jrkrideau at yahoo.ca  Mon Feb 12 12:09:39 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 12 Feb 2007 06:09:39 -0500 (EST)
Subject: [R] setting a number of values to NA over  a data.frame.
In-Reply-To: <loom.20070212T091928-363@post.gmane.org>
Message-ID: <20070212110939.87835.qmail@web32801.mail.mud.yahoo.com>


--- Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si>
wrote:

> Jim Lemon <jim <at> bitwrit.com.au> writes:
> > Hi John,
> > You might have a look at "toNA" in the prettyR
> package. Wait for version 
> > 1.0-4, just uploaded, as I have fixed a bug in
> that function.
> 
> There is also a set of generic functions exactly for
> such cases: unknownToNA(),
> NAToUnknown() and isUnknown() in gdata package.
> 
> Gregor
> 

Thanks very much.  I have a wealth of approaches now.


From J.Hadfield at ed.ac.uk  Mon Feb 12 12:28:10 2007
From: J.Hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 12 Feb 2007 11:28:10 +0000
Subject: [R] Problem with kronecker and Matrix
Message-ID: <83AB4352-A288-4795-A768-FDFDCF2A3AA4@ed.ac.uk>

Hi,

I'm trying to speed up some code by using the Matrix package.  For  
most matrix manipulations I get a good increase in speed, however  
applying the kronecker function is orders of magnitude slower than  
with simple martix classes.  In addition if I loop through kronecker  
products for profiling (taking the example from Diagonal):

M1<-Matrix(0+0:5, 2, 3)
system.time(for(i in 1:1000){
M<-kronecker(Diagonal(3), M1)})

  I (occasionaly) get the warning:

Warning message:
Ambiguous method selection for "kronecker", target  
"ddiMatrix#dgeMatrix" (the first of the signatures shown will be used)
     Matrix#ANY
     ANY#Matrix
in: .findInheritedMethods(classes, fdef, mtable)

I'm using R.2.4.1 Matrix 0.9975-8 and -9 (Mac) and 2.4.0 Matrix  
0.9975-8 (Linux)

Thanks in advance,

Jarrod.


From murdoch at stats.uwo.ca  Mon Feb 12 13:03:50 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 12 Feb 2007 07:03:50 -0500
Subject: [R] R in Industry
In-Reply-To: <20070212075016.GM15607@ihug.co.nz>
References: <45C988DC.20006@bitwrit.com.au> <20070212075016.GM15607@ihug.co.nz>
Message-ID: <45D057A6.1050305@stats.uwo.ca>

On 2/12/2007 2:50 AM, Patrick Connolly wrote:
> On Wed, 07-Feb-2007 at 07:07PM +1100, Jim Lemon wrote:
> 
> |> Matthew Keller wrote:
> |>  > Far from flaming you, I think you made a good point - one that I
> |>  > imagine most people who use R have come across. The name "R" is a big
> |>  > impediment to effective online searches. As a check, I entered "R
> |>  > software", "SAS software", SPSS software", and "S+ software" into
> |>  > google. The R 'hit rate' was only ten out of the first 20 results (I
> |>  > didn't look any further). For the other three software packages, the
> |>  > hit rates were all 100% (20/20).
> |>  >
> |>  > I do wonder if anything can/should be done about this. I generally
> |>  > search using the term "CRAN" but of course, that omits lots of stuff
> |>  > relevant to R. Any ideas about how to do effective online searches for
> |>  > "R" related materials?
> |>  >
> |> Try "r stats". I get 18/20 on Google with that.
> 
> Not bad, but the original question was about R related employment.
> Trying "R jobs" or "R employment" comes up with Hungarian girls
> looking for a job in Cork (I think the letter 'r' in 'Cork' that had
> that one show up) and somewhat further down the list comes a question
> about jobs that take a long time running MCMC using R in ESS.  And
> somewhat further still before there's an R-help archive where someone
> asked a similar question to what started this thread.  Not a lot of
> use.
> 
> Trouble is now I've clicked on some of those, they'll rate higher on
> Google's ranking so I'm perpetuating the problem.
> 
> Did anyone think of a search string that wasn't useless?

"R statistician job" doesn't do too badly.  The top 4 hits are job ads 
for statisticians requiring R.  Further down irrelevant uses of the 
letter R start to show up.  "R programming job" finds a few different 
jobs, but has a lower hit rate.

Your results will vary; Google tailors its results to what it knows 
about you.

Duncan Murdoch


From Serguei.Kaniovski at wifo.ac.at  Mon Feb 12 13:17:53 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Mon, 12 Feb 2007 13:17:53 +0100
Subject: [R] Colouring the polygons, correlation matrix
Message-ID: <OFE24312F2.B2AAA11A-ONC1257280.00438E5A-C1257280.00438E5C@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070212/9280ee17/attachment.pl 

From Christoph.Scherber at agr.uni-goettingen.de  Mon Feb 12 13:58:23 2007
From: Christoph.Scherber at agr.uni-goettingen.de (Christoph Scherber)
Date: Mon, 12 Feb 2007 13:58:23 +0100
Subject: [R] lmer and estimation of p-values: error with mcmcpvalue()
Message-ID: <45D0646F.20606@agr.uni-goettingen.de>

Dear all,

I am currently analyzing count data from a hierarchical design, and I?ve 
tried to follow the suggestions for a correct estimation of p-values as 
discusssed at R-Wiki 
(http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests&s=lme%20and%20aov).

However, I have the problem that my model only consists of parameters 
with just 1 d.f. (intercepts, slopes), so that the "mcmcpvalue" function 
defined below obviously produces error messages.

How can I proceed in estimating the p-values, then?

I very much acknowledge any suggestions.

Best regards
Christoph.

##
mcmcpvalue <- function(samp)
{  std <- backsolve(chol(var(samp)),

                     cbind(0, t(samp)) - colMeans(samp),
                     transpose = TRUE)


    sqdist <- colSums(std * std)
    sum(sqdist[-1] > sqdist[1])/nrow(samp) }

m1<-lmer(number_pollinators~logpatch+loghab+landscape_diversity+(1|site),quasipoisson)

Generalized linear mixed model fit using Laplace
Formula: number_pollinators ~ logpatch + loghab + landscape_diversity + 
     (1 | site)
    Data: primula
  Family: quasipoisson(log link)
    AIC   BIC logLik deviance
  84.83 93.75 -37.42    74.83
Random effects:
  Groups   Name        Variance Std.Dev.
  site     (Intercept) 0.036592 0.19129
  Residual             1.426886 1.19452
number of obs: 44, groups: site, 15

Fixed effects:
                     Estimate Std. Error t value
(Intercept)          -0.4030     0.6857 -0.5877
logpatch              0.1091     0.0491  2.2228
loghab                0.0875     0.0732  1.1954
landscape_diversity   1.0234     0.4850  2.1099

Correlation of Fixed Effects:
             (Intr) lgptch loghab
logpatch     0.091
loghab      -0.637 -0.121
lndscp_dvrs -0.483 -0.098 -0.348


markov1=mcmcsamp(m1,5000)
HPDinterval(markov1)

mcmcpvalue(as.matrix(markov1)[,1])

Error in colMeans(samp) : 'x' must be an array of at least two dimensions


From nilsson.henric at gmail.com  Mon Feb 12 14:26:09 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Mon, 12 Feb 2007 14:26:09 +0100 (CET)
Subject: [R] lmer and estimation of p-values: error with mcmcpvalue()
In-Reply-To: <45D0646F.20606@agr.uni-goettingen.de>
References: <45D0646F.20606@agr.uni-goettingen.de>
Message-ID: <27291.212.209.13.15.1171286769.squirrel@www.sorch.se>

Den M?, 2007-02-12, 13:58 skrev Christoph Scherber:
> Dear all,
>
> I am currently analyzing count data from a hierarchical design, and I?ve
> tried to follow the suggestions for a correct estimation of p-values as
> discusssed at R-Wiki
> (http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests&s=lme%20and%20aov).
>
> However, I have the problem that my model only consists of parameters
> with just 1 d.f. (intercepts, slopes), so that the "mcmcpvalue" function
> defined below obviously produces error messages.
>
> How can I proceed in estimating the p-values, then?
>
> I very much acknowledge any suggestions.
>
> Best regards
> Christoph.
>
> ##
> mcmcpvalue <- function(samp)
> {  std <- backsolve(chol(var(samp)),
>
>                      cbind(0, t(samp)) - colMeans(samp),
>                      transpose = TRUE)
>
>
>     sqdist <- colSums(std * std)
>     sum(sqdist[-1] > sqdist[1])/nrow(samp) }
>
> m1<-lmer(number_pollinators~logpatch+loghab+landscape_diversity+(1|site),quasipoisson)
>
> Generalized linear mixed model fit using Laplace
> Formula: number_pollinators ~ logpatch + loghab + landscape_diversity +
>      (1 | site)
>     Data: primula
>   Family: quasipoisson(log link)
>     AIC   BIC logLik deviance
>   84.83 93.75 -37.42    74.83
> Random effects:
>   Groups   Name        Variance Std.Dev.
>   site     (Intercept) 0.036592 0.19129
>   Residual             1.426886 1.19452
> number of obs: 44, groups: site, 15
>
> Fixed effects:
>                      Estimate Std. Error t value
> (Intercept)          -0.4030     0.6857 -0.5877
> logpatch              0.1091     0.0491  2.2228
> loghab                0.0875     0.0732  1.1954
> landscape_diversity   1.0234     0.4850  2.1099
>
> Correlation of Fixed Effects:
>              (Intr) lgptch loghab
> logpatch     0.091
> loghab      -0.637 -0.121
> lndscp_dvrs -0.483 -0.098 -0.348
>
>
> markov1=mcmcsamp(m1,5000)
> HPDinterval(markov1)
>
> mcmcpvalue(as.matrix(markov1)[,1])

Try `mcmcpvalue(as.matrix(markov1[,1]))'.


HTH,
Henric



>
> Error in colMeans(samp) : 'x' must be an array of at least two dimensions
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From grahamleask at btopenworld.com  Mon Feb 12 12:03:20 2007
From: grahamleask at btopenworld.com (GRAHAM LEASK)
Date: Mon, 12 Feb 2007 11:03:20 +0000 (GMT)
Subject: [R] Duda Hart Index
Message-ID: <347270.24257.qm@web86204.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070212/e1941310/attachment.pl 

From grahamleask at btopenworld.com  Mon Feb 12 12:11:32 2007
From: grahamleask at btopenworld.com (GRAHAM LEASK)
Date: Mon, 12 Feb 2007 11:11:32 +0000 (GMT)
Subject: [R] V fold cross validation
Message-ID: <405554.45361.qm@web86201.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070212/56fe9b46/attachment.pl 

From Christoph.Scherber at agr.uni-goettingen.de  Mon Feb 12 14:33:20 2007
From: Christoph.Scherber at agr.uni-goettingen.de (Christoph Scherber)
Date: Mon, 12 Feb 2007 14:33:20 +0100
Subject: [R] lmer and estimation of p-values: error with mcmcpvalue()
In-Reply-To: <27291.212.209.13.15.1171286769.squirrel@www.sorch.se>
References: <45D0646F.20606@agr.uni-goettingen.de>
	<27291.212.209.13.15.1171286769.squirrel@www.sorch.se>
Message-ID: <45D06CA0.7090603@agr.uni-goettingen.de>

Dear Henric,

Thanks, now it works; but how reliable are these estimates? Especially 
with p-values close to 0.05 it is of course important that the range of 
the estimates is not too large. I?ve just run several simulations, each 
of which yielding sometimes quite different p-values.

Best wishes
Christoph


##
Dr. rer. nat. Christoph Scherber
DNPW, Agroecology
University of Goettingen
Waldweg 26
D-37073 Goettingen
http://wwwuser.gwdg.de/~uaoe/Agroecology.html
+49-(0)551-39-8807



Henric Nilsson (Public) schrieb:
> Den M?, 2007-02-12, 13:58 skrev Christoph Scherber:
>> Dear all,
>>
>> I am currently analyzing count data from a hierarchical design, and I?ve
>> tried to follow the suggestions for a correct estimation of p-values as
>> discusssed at R-Wiki
>> (http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests&s=lme%20and%20aov).
>>
>> However, I have the problem that my model only consists of parameters
>> with just 1 d.f. (intercepts, slopes), so that the "mcmcpvalue" function
>> defined below obviously produces error messages.
>>
>> How can I proceed in estimating the p-values, then?
>>
>> I very much acknowledge any suggestions.
>>
>> Best regards
>> Christoph.
>>
>> ##
>> mcmcpvalue <- function(samp)
>> {  std <- backsolve(chol(var(samp)),
>>
>>                      cbind(0, t(samp)) - colMeans(samp),
>>                      transpose = TRUE)
>>
>>
>>     sqdist <- colSums(std * std)
>>     sum(sqdist[-1] > sqdist[1])/nrow(samp) }
>>
>> m1<-lmer(number_pollinators~logpatch+loghab+landscape_diversity+(1|site),quasipoisson)
>>
>> Generalized linear mixed model fit using Laplace
>> Formula: number_pollinators ~ logpatch + loghab + landscape_diversity +
>>      (1 | site)
>>     Data: primula
>>   Family: quasipoisson(log link)
>>     AIC   BIC logLik deviance
>>   84.83 93.75 -37.42    74.83
>> Random effects:
>>   Groups   Name        Variance Std.Dev.
>>   site     (Intercept) 0.036592 0.19129
>>   Residual             1.426886 1.19452
>> number of obs: 44, groups: site, 15
>>
>> Fixed effects:
>>                      Estimate Std. Error t value
>> (Intercept)          -0.4030     0.6857 -0.5877
>> logpatch              0.1091     0.0491  2.2228
>> loghab                0.0875     0.0732  1.1954
>> landscape_diversity   1.0234     0.4850  2.1099
>>
>> Correlation of Fixed Effects:
>>              (Intr) lgptch loghab
>> logpatch     0.091
>> loghab      -0.637 -0.121
>> lndscp_dvrs -0.483 -0.098 -0.348
>>
>>
>> markov1=mcmcsamp(m1,5000)
>> HPDinterval(markov1)
>>
>> mcmcpvalue(as.matrix(markov1)[,1])
> 
> Try `mcmcpvalue(as.matrix(markov1[,1]))'.
> 
> 
> HTH,
> Henric
> 
> 
> 
>> Error in colMeans(samp) : 'x' must be an array of at least two dimensions
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> 
> 
> .
>


From friedman.steve at gmail.com  Mon Feb 12 15:14:54 2007
From: friedman.steve at gmail.com (Steve Friedman)
Date: Mon, 12 Feb 2007 09:14:54 -0500
Subject: [R] Linking R with Microsoft SQL Server / Client
Message-ID: <2439f5740702120614h6b3ca7a1r24ec3118bcfab9c7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070212/11c3e792/attachment.pl 

From john.gavin at ubs.com  Mon Feb 12 15:13:52 2007
From: john.gavin at ubs.com (john.gavin at ubs.com)
Date: Mon, 12 Feb 2007 14:13:52 -0000
Subject: [R] job advertisement - finance - London based
Message-ID: <182544D7A3144B42994EEA5662C54E010521B78D@NLDNC105PEX1.ubsw.net>

Hi,

We are looking for a skilled statistician/programmer 
seeking a career at the intersection of 
finance, applied statistics and computer science. 
This position requires a person with a strong background in 
- data analysis, 
- design and implementation of algorithms, 
- software development
- statistical methodology.

The primary responsibilities are to develop software, 
using the R language, for:
- Handling/analysing high-frequency, real-time, streaming, financial
data. 
- Interfacing R/S-Plus to other languages, especially Java.
- Applying high-dimensional regression/machine learning applications.
- Assisting in the prototyping and testing of trading algorithms. 

For more details please go to
http://tinyurl.com/36bwe6
click on 'Search our global job board',
click on 'Search openings',
scroll down to 'keyword' and type the job reference '19257BR' 
and click the search button 
(sorry but there isn't a more direct link on this site).

Alternatively, the job is also advertised at
http://tinyurl.com/2clsy8

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Commodities, FIRC,
UBS Investment Bank, 2nd floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
This communication is issued by UBS AG and/or affiliates to\...{{dropped}}


From liuwensui at gmail.com  Mon Feb 12 15:35:49 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 12 Feb 2007 09:35:49 -0500
Subject: [R] Linking R with Microsoft SQL Server / Client
In-Reply-To: <2439f5740702120614h6b3ca7a1r24ec3118bcfab9c7@mail.gmail.com>
References: <2439f5740702120614h6b3ca7a1r24ec3118bcfab9c7@mail.gmail.com>
Message-ID: <1115a2b00702120635k4072c5e2h8d424f06bd6cf0a0@mail.gmail.com>

steve,
i think you can use R to link to sql server directly with RODBC. but
it is not wise to dump the whole table from db into R and then do
manipulation, which will slow the speed of R.


On 2/12/07, Steve Friedman <friedman.steve at gmail.com> wrote:
> Hello
>
> My colleagues and I have recently established a large database (40 tables
> each with greater than 15 variables) in Microsoft's SQL Server 2000.
> Currently we are accessing this database via SQL client running an Windows
> XP.   Our objectives are many fold including running SQL applications,
> outputting results to ARC/INFO IMS, production of summarizing tables -
> graphs and web interfaces for user accessibility.
>
> The project is still very much in a design phase.  I'm interested in knowing
> if we can link R directly to the database  as it is either stored in SQL
> Server, or SQL Client, or if we are better off keeping it simple and
> extracting ascii (csv)  files from SQL server prior to processing
> summarizing and model development.
>
> Any insight provided will be greatly appreciated.
>
> Steve
>
> --
> Steve Friedman
> Computational Ecology and Visualization Laboratory
> Michigan State University
>
> Envisioning Ecosystem Decisions
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From therneau at mayo.edu  Mon Feb 12 15:36:10 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Mon, 12 Feb 2007 08:36:10 -0600 (CST)
Subject: [R]  Problem with factor state when subset()ing a data frame
Message-ID: <200702121436.l1CEaAL09721@natasha.mayo.edu>

  The solution to most "factors" questions on the R mailing list is
to set the global option stringsAsFactors to F.  Make it your part of your
default R startup.  Even better, do what we have done at Mayo for the
last 10+ years and make it the default for your whole unit.  (150+ users,
20+ years of S experience).  We were one of the groups that whined to
Insightful until they added this feature, which unfortunately did not
become a part of R until fairly recently. 

  For some character variables the factor logic makes sense, for other it
does not.  If you set the option above, then you can use an explicit
    mydata$variable  <- factor(mydata$variable)
for the variables that should be factors.   In my experience, with a wide
variety of data analysis, that is about 1/10 of my character variables.
Others may disagree about the fraction, but one of the really bad aspects 
of the default design is that it forces 100% conversion of characters
to another class, which is certainly not best state.  (Street address,
for instance, never makes sense as a factor).

  When factor are the right thing, they are very useful.  I would agree with
Peter Dalgaard's assessment of past discussion about automatically dropping
unused levels: there is no approach that always works best, and the current
default has been extensively talked over and appears to be the best current
default.  They most certainly should not disappear from the language, or have
major changes without a lot of discussion.  

   Terry Therneau
   Biostatistics, Mayo Clinic


From Thierry.ONKELINX at inbo.be  Mon Feb 12 15:36:27 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 12 Feb 2007 15:36:27 +0100
Subject: [R] Linking R with Microsoft SQL Server / Client
In-Reply-To: <2439f5740702120614h6b3ca7a1r24ec3118bcfab9c7@mail.gmail.com>
Message-ID: <2E9C414912813E4EB981326983E0A1040293D9F2@inboexch.inbo.be>

Dear Steve,

Reading the data shouldn't be a problem with RODBC. I've been doing that
plenty of times. I'm not sure about writing, but I suppose you probably
will.

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Steve Friedman
Verzonden: maandag 12 februari 2007 15:15
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Linking R with Microsoft SQL Server / Client

Hello

My colleagues and I have recently established a large database (40
tables
each with greater than 15 variables) in Microsoft's SQL Server 2000.
Currently we are accessing this database via SQL client running an
Windows
XP.   Our objectives are many fold including running SQL applications,
outputting results to ARC/INFO IMS, production of summarizing tables -
graphs and web interfaces for user accessibility.

The project is still very much in a design phase.  I'm interested in
knowing
if we can link R directly to the database  as it is either stored in SQL
Server, or SQL Client, or if we are better off keeping it simple and
extracting ascii (csv)  files from SQL server prior to processing
summarizing and model development.

Any insight provided will be greatly appreciated.

Steve

-- 
Steve Friedman
Computational Ecology and Visualization Laboratory
Michigan State University

Envisioning Ecosystem Decisions

	[[alternative HTML version deleted]]

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From liuwensui at gmail.com  Mon Feb 12 15:37:54 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 12 Feb 2007 09:37:54 -0500
Subject: [R] SQL statements (directly) in R
In-Reply-To: <156CDC8CCFD1894295D2907F16337A4801420A83@bru-s-006.europe.shell.com>
References: <8828460.post@talk.nabble.com>
	<156CDC8CCFD1894295D2907F16337A4801420A83@bru-s-006.europe.shell.com>
Message-ID: <1115a2b00702120637pba3cb7etfd911bf536994dd2@mail.gmail.com>

chris,
if you could create a package similar to proc sql in SAs, that will be so sweet.


On 2/12/07, christian.ritter at shell.com <christian.ritter at shell.com> wrote:
> Hi R-users,
>
> This note will interest people who would like to use sql statements on R data frames (a bit like proc sql in SAS). Please reply to my only, unless you really want to keep the entire R-help list posted on this.
>
> I've been thinking about a packgage implementing sql queries in R. I'm almost about starting to write it in a very rudimentary version. What I have in mind is the following:
>
> Work in two ways:
> via a generic sql("..") wrapper which allows a generic query statement
> and via convenience functions, such as SELECT("..."), ...
> what would be needed is an "sqlTable" class extending the data frame. This class will have to have extra slots for indices and some other stuff. I would try to stay very basic in the beginning and also use relatively inefficient handling of the tables. Later-on, direct calls using the binary representations could  replace the high level handling.
>
> Now come my questions:
> - have others started working on this?
> - are others interested in this?
> - ideas on how to go about it?
>
> Chris
>
> P.S.:
> Here are a few ideas I was thinking about
> One way would be to incorporate a gpl or lgpl rdbms into the package, to push the data-frame to it, to execute the statement there and to get the result back. The advantage: fast to implement. The disadvantage: pushing the data is a bad idea (but then again, at the top level, R will make a copy of it anyway, most probably). The convenience wrappers would then construct sql statements and the db engine would evaluate them.
>
> The other idea is to stay in R and to link the wrappers to adequately composed calls to subset, cbind, rbind, etc. Here it would be more challenging to create the sql("..") interface since its string would have to be parsed.
>
> The political incorrect thing about these SQL functions is that they (UPDATE, INSERT) will have to modify objects within the function call. They would not work via the return object.
>
> As I said, comments welcome.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From BEN at SSANET.COM  Mon Feb 12 16:20:27 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Mon, 12 Feb 2007 09:20:27 -0600
Subject: [R] 'Save Workspace' gives "recursive default argument reference"
	-- workaround?
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05E447C8@hercules.ssainfo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070212/f3d36767/attachment.pl 

From mrennie at utm.utoronto.ca  Mon Feb 12 16:42:21 2007
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 12 Feb 2007 10:42:21 -0500
Subject: [R] Trying to replicate error message in subset()
In-Reply-To: <45D0297C.5943.4DFF87@localhost>
References: <6.1.0.6.0.20070212014013.01baf970@mail.utm.utoronto.ca>
	<45D0297C.5943.4DFF87@localhost>
Message-ID: <6.1.0.6.0.20070212103217.01bf5ae0@mail.utm.utoronto.ca>


Okay

First- I apologise for my poor use of terminology (error vs. warning).

Second- thanks for pointing out what I hadn't noticed before- when I pass 
one case for selection to subset, I get all observations. When I pass two 
cases (as a vector), I get every second case for both cases (if both are 
present, if not, I just get every second case for the one that is present). 
Same happens for three cases, as pointed out by Petr below.

So, trying the %in% operator, I get slightly different behabviour, but the 
selection still seems dependent on the length of the vector given as a 
selector:

 > b<-c("D", "F", "A")
 > new2.dat<-subset(ex.dat, a%in%x1)
 > new2.dat
              y1 x1 x2
1    2.34870479  A  B
3    1.66090055  A  B
5   -0.07904798  A  B
7    2.07053656  A  B
9    2.97980444  A  B
.....

Now, I just get every second observation, over all cases of x1. Probably 
doesn't get as far as A because F is not present?

According to the documentation on subset(), the function works on rows of 
dataframes. I'm guessing this explains the behaviour I'm seeing- somehow, 
the length of the vector being passed as the subset argument is dictating 
which rows to evaluate.  So, can anyone offer advice on how to select EVERY 
instance for multiple cases in a dataframe (i.e., all cases of both A and D 
from ex.dat), or will subset always be tied to the length of the 'subset' 
argument when a vector is passed to it?

Cheers,

Mike


At 02:46 AM 12/02/2007, Petr Pikal wrote:
>Hi
>
>it is not error it is just warning (Beeping a tea kettle with boiling
>water is also not an error :-)
>and it tells you pretty explicitly what is wrong
>see length of your objects
>
> > a<-c("D", "F", "A")
> > new.dat<-subset(ex.dat, x1 == a)
>Warning messages:
>1: longer object length
>         is not a multiple of shorter object length in: is.na(e1) |
>is.na(e2)
>2: longer object length
>         is not a multiple of shorter object length in:
>`==.default`(x1, a)
> > new.dat
>             y1 x1 x2
>3    0.5977786  A  B
>6    2.5470739  A  B
>9    0.9128595  A  B
>12   1.0953531  A  D
>15   2.4984470  A  D
>18   1.7289529  A  D
>61  -0.4848938  D  B
>6
>
>you can do better with %in% operator.
>
>HTH
>Petr
>
>
>
>On 12 Feb 2007 at 1:51, Michael Rennie wrote:
>
>Date sent:              Mon, 12 Feb 2007 01:51:54 -0500
>To:                     r-help at stat.math.ethz.ch
>From:                   Michael Rennie <mrennie at utm.utoronto.ca>
>Subject:                [R] Trying to replicate error message in subset()
>
> >
> > Hi, there
> >
> > I am trying to replicate an error message in subset() to see what it
> > is that I'm doing wrong with the datasets I am trying to work with.
> >
> > Essentially, I am trying to pass a string vector to subset() in order
> > to select a specific collection of cases (i.e., I have data for these
> > cases in one table, and want to select data from another table that
> > match up with the cases in the first table).
> >
> > The error I get is as follows:
> >
> > Warning messages:
> > 1: longer object length
> >          is not a multiple of shorter object length in: is.na(e1) |
> >          is.na(e2)
> > 2: longer object length
> >          is not a multiple of shorter object length in:
> >          `==.default`(LAKE, g)
> >
> > Here is an example case I've been working with (which works) that I've
> > been trying to "break"such that I can get this error message to figure
> > out what I am doing wrong in my case.
> >
> > y1<-rnorm(100, 2)
> > x1<-rep(1:5, each=20)
> > x2<-rep(1:2, each=10, times=10)
> >
> > ex.dat<-data.frame(cbind(y1,x1,x2))
> >
> >
> > ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B", "C", "D", "E"))
> > ex.dat$x2<-factor(ex.dat$x2, labels=c("B", "D"))
> >
> > a<-c("D", "F")
> > a
> >
> > new.dat<-subset(ex.dat, x1 == a)
> > new.dat
> >
> > I thought maybe I was getting errors because I had cases in my
> > selection vector ('a' in this case) that weren't in my ex.dat list,
> > but subset handles this fine and just gives me what it can find in the
> > larger list.
> >
> > Any thoughts on how I can replicate the error? As far as I can tell,
> > the only difference between the case where I am getting errors and the
> > example above is that the levels of x1 in my case are words (i.e.,
> > "Smelly", "Howdy"), but strings are strings, aren't they?
> >
> > Mike
> >
> >
> > Michael Rennie
> > Ph.D. Candidate, University of Toronto at Mississauga
> > 3359 Mississauga Rd. N.
> > Mississauga, ON  L5L 1C6
> > Ph: 905-828-5452  Fax: 905-828-3792
> > www.utm.utoronto.ca/~w3rennie
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
>Petr Pikal
>petr.pikal at precheza.cz

Michael Rennie
Ph.D. Candidate, University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792
www.utm.utoronto.ca/~w3rennie


From roger.bos at us.rothschild.com  Mon Feb 12 16:58:14 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Mon, 12 Feb 2007 10:58:14 -0500
Subject: [R] Linking R with Microsoft SQL Server / Client
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD341DC6C1@RINNYCSE000.rth.ad.rothschild.com>

Yes, you can use RODBC to connect to MS SQL.  You can query, issue
commands, and save to the db from R and it works very well (at least for
me).

For example, with a database named xf, create an ODBC data source and
then link open a connection in R:
library(RODBC)
xf <- odbcConnect("xf", username, password)
Then you can issue commands such as those below:

go <- sqlQuery(xf, "alter table roger_test alter column name varchar(3)
NOT NULL")
go <- sqlQuery(xf, "select * from roger_test")
sqlSave(xf, val, tablename="roger_test", append=TRUE, rownames=FALSE)

HTH,

Roger
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ONKELINX, Thierry
Sent: Monday, February 12, 2007 9:36 AM
To: Steve Friedman; r-help at stat.math.ethz.ch
Subject: Re: [R] Linking R with Microsoft SQL Server / Client

Dear Steve,

Reading the data shouldn't be a problem with RODBC. I've been doing that
plenty of times. I'm not sure about writing, but I suppose you probably
will.

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx at inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] Namens Steve Friedman
Verzonden: maandag 12 februari 2007 15:15
Aan: r-help at stat.math.ethz.ch
Onderwerp: [R] Linking R with Microsoft SQL Server / Client

Hello

My colleagues and I have recently established a large database (40
tables each with greater than 15 variables) in Microsoft's SQL Server
2000.
Currently we are accessing this database via SQL client running an
Windows
XP.   Our objectives are many fold including running SQL applications,
outputting results to ARC/INFO IMS, production of summarizing tables -
graphs and web interfaces for user accessibility.

The project is still very much in a design phase.  I'm interested in
knowing if we can link R directly to the database  as it is either
stored in SQL Server, or SQL Client, or if we are better off keeping it
simple and extracting ascii (csv)  files from SQL server prior to
processing summarizing and model development.

Any insight provided will be greatly appreciated.

Steve

--
Steve Friedman
Computational Ecology and Visualization Laboratory Michigan State
University

Envisioning Ecosystem Decisions

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From rjohnson at ncifcrf.gov  Mon Feb 12 17:02:36 2007
From: rjohnson at ncifcrf.gov (Randall C Johnson [Contr.])
Date: Mon, 12 Feb 2007 11:02:36 -0500
Subject: [R] Width of a plotting point (in inches) in grid package
Message-ID: <C1F5F9CC.105C3%rjohnson@ncifcrf.gov>

Hello,
I'm trying to determine the width of a plotting point (in inches) in the
grid package. I naively thought I could create a pointsGrob with only one
point and get the width (as tried below), but this results in an object with
a size of 0inches (changing cex has no effect). Does anyone have a better
approach? Of course, it would be dependent upon the graphics parameters and
viewport...

Thanks,
Randy

> library(grid)

> pushViewport(viewport())

> convertX(grobWidth(pointsGrob(1, 1)), 'inches')
[1] 0inches

# I think we're measuring the size of the point here...
# changing cex has no effect.
> convertX(grobWidth(pointsGrob(1, 1, gp = gpar(cex = 3))), 'inches')
[1] 0inches

# If I add a second point, the size should increase...
# how big is the plotting point though???
> convertX(grobWidth(pointsGrob(1:2, 1:2)), 'inches')
[1] 11.1929133858268inches

> sessionInfo()
R version 2.4.1 (2006-12-18)
i386-apple-darwin8.8.2

locale:
C

attached base packages:
[1] "grid"      "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "methods"   "base"


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Randall C Johnson
Bioinformatics Analyst
SAIC-Frederick, Inc (Contractor)
Laboratory of Genomic Diversity
NCI-Frederick, P.O. Box B
Bldg 560, Rm 11-85
Frederick, MD 21702
Phone: (301) 846-1304
Fax: (301) 846-1686
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From friedman.steve at gmail.com  Mon Feb 12 17:13:06 2007
From: friedman.steve at gmail.com (Steve Friedman)
Date: Mon, 12 Feb 2007 11:13:06 -0500
Subject: [R] R and MS SQL Server
Message-ID: <2439f5740702120813x1227bf9cq29596a2c08c8cb5a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070212/9252064a/attachment.pl 

From patrick.leoni at nuim.ie  Mon Feb 12 17:18:25 2007
From: patrick.leoni at nuim.ie (Patrick Leoni)
Date: Mon, 12 Feb 2007 16:18:25 +0000
Subject: [R] Handling large calculations and memory
Message-ID: <5.1.0.14.0.20070212160909.00d716e8@mail.nuim.ie>

Dear All,

I am planning to run a Monte-Carlo experiment which involves to do roughly 
100.000 times the following

1- Generating a sample of, say, 50.000 numbers from an ARMA or GARCH
2- Doing some regressions on the series
3- On each regression storing one special value from the results into a 
sequence that will be analyzed later.

The experiment is calculation-intensive, and I suspect some problems of 
memory management. I would appreciate some advices for the experiment to 
run smoothly in terms of memory.

Many Thanks,

Patrick Leoni

Department of Economics and Finance
NUI at Maynooth

Maynooth, Co. Kildare
Republic of Ireland


From bcarvalh at jhsph.edu  Mon Feb 12 17:20:49 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 12 Feb 2007 11:20:49 -0500
Subject: [R] predict on biglm class
Message-ID: <F4D91215-28A1-40E8-A43B-BECFE5BBA2BF@jhsph.edu>

Hi Everyone,

I often use the 'safe prediction' feature available through glm().  
Now, I'm at a situation where I must use biglm:::bigglm.

## begin example

library(splines)
library(biglm)
ff <- log(Volume)~ns(log(Girth), df=5)
fit.glm <- glm(ff, data=trees)
fit.biglm <- bigglm(ff, data=trees)
predict(fit.glm, newdata=data.frame(Girth=2:5))

## -1.3161465 -0.2975659  0.4251285  0.9856938

predict(fit.biglm, newdata=data.frame(Girth=2:5))

## Error in predict(fit.biglm, newdata = data.frame(Girth = 2:5)) :
##        no applicable method for "predict"

## end example

So, it is my understanding that there is no 'predict' method for  
'bigglm' class. That suggests me that I need to create my own  
prediction method, right? What would be an efficient way of making  
these predictions that use ns() on a very large dataset?

My initial thought is that saving the Boundary.knots and knots, I  
could create the linear predictor by chunks (and therefore get the  
predictions). Is there a better way of doing this?

Thank you very much.

Benilton Carvalho
Department of Biostatistics
Bloomberg School of Public Health
Johns Hopkins University


From Greg.Snow at intermountainmail.org  Mon Feb 12 17:23:42 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 12 Feb 2007 09:23:42 -0700
Subject: [R] practical memory limits
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7FCCBA@LP-EXCHVS07.CO.IHC.COM>

An old rule of thumb was that you should have 6 times as much memory as
your dataset will take.  But I think pretty much everything has been
improved since then, so you should be able to get by with less (others
may be able to give a better rule of thumb these days).

You might want to look at the biglm package, it allows you to do
regression models with only a portion of your data loaded at a time,
allowing for pretty much any size of data set in a limited memory
situation.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ivo welch
> Sent: Saturday, February 10, 2007 11:03 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] practical memory limits
> 
> Dear R experts:  I want to learn what the practically useful 
> memory limits are for good work with R.
> 
> (My specific problem is that I want work with daily stock returns.
> In ASCII, the data set is about 72 million returns, that 
> would have to go into a sparse matrix (not all stocks exist 
> for the whole series).
> As a guess, this will consume about 700MB.  My main use will 
> be linear operations---regressions, means, etc.)
> 
> I am on linux, so I can create swap space, but I am concerned 
> that the thrashing will be so bad that the computer will 
> become worthless.  In fact, the last time I used it was over 
> 3 years ago.  Since then, I have just turned it off.
> 
> I have 2GB of RAM right now, and could upgrade this to 4GB.
> 
> Are there some general guidelines as to what the relationship 
> between data sets and memory should be under R?  I know this 
> will vary with the task involved, but some guidance would be 
> better than none.
> 
> regards,
> 
> /iaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Mon Feb 12 17:50:21 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 12 Feb 2007 09:50:21 -0700
Subject: [R] Boxplot: quartiles/outliers
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7FCCC7@LP-EXCHVS07.CO.IHC.COM>

For your outlier question, look at the 'range' argument to the boxplot
function.  It defaults to 1.5 meaning that any points more than 1.5*IQR
from the 1st and 3rd quartiles are considered outliers.  If you make
this smaller, you will potentially see more outliers, if you make it
larger then it will show fewer outliers.  The special case of 0 means
that you don't want any points to be shown as outliers and the wiskers
will go all the way to the min and max points.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Cecilia Alm
> Sent: Sunday, February 11, 2007 9:05 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Boxplot: quartiles/outliers
> 
> For boxplot(),  is it possible to pass in a parameter to 
> change the default way that the 1st and 3rd quartiles are 
> computed? (specifically, I'd like to use type 6 described in 
> the quantile function).
> 
> Also, what are the options for how outliers are computed, and 
> how can one change them?
> 
> Thank you
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Mon Feb 12 18:07:51 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 12 Feb 2007 12:07:51 -0500
Subject: [R] 'Save Workspace' gives "recursive default argument
 reference" -- workaround?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05E447C8@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05E447C8@hercules.ssainfo>
Message-ID: <45D09EE7.7020003@stats.uwo.ca>

On 2/12/2007 10:20 AM, Ben Fairbank wrote:
> When signing off R or trying to save a workspace in Windows XP pro SP2,
> I receive the following error message -
> 
>  
> 
> save.image("C:\\Program Files\\R\\R-2.4.1\\Responses3.RData")
> 
>  
> 
> Error in save.image("C:\\Program Files\\R\\R-2.4.1\\Responses3.RData") :
> 
> 
>         recursive default argument reference
> 
>  
> 
> Everything else seems to work fine, and the only function I have written
> using "outer" (which I gather to be associated with this error) runs
> without problems.
> 
>  
> 
> An excursion through the list archives did not yield information
> discussing this message in connection with workspace saves.
> 
>  
> 
> Can a user suggest a workaround or alternative method to save the
> workspace before I exit R and see if restarting will cure it?

Instead of save.image(), use save() with an explicit list.  The man page 
says save.image() is equivalent to save(list = ls(all=TRUE), file = 
".RData"), so try leaving out some of your objects until you identify 
the problematic one(s).

Duncan Murdoch
> 
>  
> 
> With thanks for any suggestions,
> 
>  
> 
> Ben Fairbank
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From vinodkgul at yahoo.com  Mon Feb 12 18:55:27 2007
From: vinodkgul at yahoo.com (vinod gullu)
Date: Mon, 12 Feb 2007 09:55:27 -0800 (PST)
Subject: [R] Help neural network in R
In-Reply-To: <mailman.7.1171278002.31388.r-help@stat.math.ethz.ch>
Message-ID: <407292.39134.qm@web53808.mail.yahoo.com>

I am interested in Neural network models in R. Is
there any reference material/tutorial which i can use.
Regards, 


 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From liuwensui at gmail.com  Mon Feb 12 19:35:29 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 12 Feb 2007 13:35:29 -0500
Subject: [R] Help neural network in R
In-Reply-To: <407292.39134.qm@web53808.mail.yahoo.com>
References: <mailman.7.1171278002.31388.r-help@stat.math.ethz.ch>
	<407292.39134.qm@web53808.mail.yahoo.com>
Message-ID: <1115a2b00702121035v11879aafsf1ee6f2736af8500@mail.gmail.com>

the one I will recommend is MASS by Dr B. Ripley.

On 2/12/07, vinod gullu <vinodkgul at yahoo.com> wrote:
> I am interested in Neural network models in R. Is
> there any reference material/tutorial which i can use.
> Regards,
>
>
>
> ____________________________________________________________________________________
> TV dinner still cooling?
> Check out "Tonight's Picks" on Yahoo! TV.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From p.murrell at auckland.ac.nz  Mon Feb 12 20:27:06 2007
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 13 Feb 2007 08:27:06 +1300
Subject: [R] Width of a plotting point (in inches) in grid package
In-Reply-To: <C1F5F9CC.105C3%rjohnson@ncifcrf.gov>
References: <C1F5F9CC.105C3%rjohnson@ncifcrf.gov>
Message-ID: <45D0BF8A.50605@stat.auckland.ac.nz>

Hi


Randall C Johnson [Contr.] wrote:
> Hello,
> I'm trying to determine the width of a plotting point (in inches) in the
> grid package. I naively thought I could create a pointsGrob with only one
> point and get the width (as tried below), but this results in an object with
> a size of 0inches (changing cex has no effect). Does anyone have a better
> approach? Of course, it would be dependent upon the graphics parameters and
> viewport...


The width of a pointsGrob is based on a bounding box surrounding all of
the (x, y) locations at which the points are located.  It takes no
notice of the size of the symbol drawn at the locations.  With one
point, the bounding box has zero size.  The wisdom of this design could
be debated ...

What do you need the symbol width for?  Could you use a circle,
rectangle, or polygon instead (all of which calculate their width based
on the bounding box of the shape that is drawn)?

Paul


> Thanks,
> Randy
> 
>> library(grid)
> 
>> pushViewport(viewport())
> 
>> convertX(grobWidth(pointsGrob(1, 1)), 'inches')
> [1] 0inches
> 
> # I think we're measuring the size of the point here...
> # changing cex has no effect.
>> convertX(grobWidth(pointsGrob(1, 1, gp = gpar(cex = 3))), 'inches')
> [1] 0inches
> 
> # If I add a second point, the size should increase...
> # how big is the plotting point though???
>> convertX(grobWidth(pointsGrob(1:2, 1:2)), 'inches')
> [1] 11.1929133858268inches
> 
>> sessionInfo()
> R version 2.4.1 (2006-12-18)
> i386-apple-darwin8.8.2
> 
> locale:
> C
> 
> attached base packages:
> [1] "grid"      "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "methods"   "base"
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Randall C Johnson
> Bioinformatics Analyst
> SAIC-Frederick, Inc (Contractor)
> Laboratory of Genomic Diversity
> NCI-Frederick, P.O. Box B
> Bldg 560, Rm 11-85
> Frederick, MD 21702
> Phone: (301) 846-1304
> Fax: (301) 846-1686
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From chenxh007 at gmail.com  Mon Feb 12 21:05:44 2007
From: chenxh007 at gmail.com (Xiaohui)
Date: Mon, 12 Feb 2007 12:05:44 -0800
Subject: [R] heatmap color specification
In-Reply-To: <45CDAC84.9030104@bitwrit.com.au>
References: <45CCCAC7.8000902@gmail.com> <45CDAC84.9030104@bitwrit.com.au>
Message-ID: <45D0C898.4080901@gmail.com>

Jim Lemon wrote:
> Xiaohui wrote:
>> ... Then, I did a heatmap for 'test' matrix. But for now, I want to 
>> specify each of the cell in the heatmap according to the values of 
>> the corresponding matrix elements of test.
>>
>> Let's say: col<-c("red","yellow","green")
>>
>> for test[1,1], the color on the map should be "red".
>>
>> I have tried par('usr') and par('mar') with rect function. But this 
>> does not work because the rect shift from the original map. Could any 
>> one tell me how to fill the cells on the map with corresponding 
>> values? Or can we get the actual coordinates of the image excluding 
>> the dendregram.
> Hi Xiaohui,
> You may be looking for something like color2D.matplot in the plotrix 
> package or "image" in the graphics package.
>
> Jim
>
Thanks, Jim. I found this problem can be solve with the 'scale' 
parameter specified to FALSE. If it is TRUE, then the positive integer 
is scale to a real number which will cause the problem earlier.

Xiaohui


From ozric at web.de  Mon Feb 12 21:18:56 2007
From: ozric at web.de (Christian Schulz)
Date: Mon, 12 Feb 2007 21:18:56 +0100
Subject: [R] Help neural network in R
In-Reply-To: <407292.39134.qm@web53808.mail.yahoo.com>
References: <407292.39134.qm@web53808.mail.yahoo.com>
Message-ID: <45D0CBB0.2020700@web.de>

...maybe library(neural) with examples should a good staring point.

regards, christian

> I am interested in Neural network models in R. Is
> there any reference material/tutorial which i can use.
> Regards, 
>
>
>  
> ____________________________________________________________________________________
> TV dinner still cooling? 
> Check out "Tonight's Picks" on Yahoo! TV.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From andy1983 at excite.com  Mon Feb 12 21:55:09 2007
From: andy1983 at excite.com (andy1983)
Date: Mon, 12 Feb 2007 12:55:09 -0800 (PST)
Subject: [R] processing a large matrix
Message-ID: <8932591.post@talk.nabble.com>


I would like to compare every column in my matrix with every other column and
get the r-squared.

I tried using the following formula and looping through every column:
> summary(lm(matrix[,x]~matrix[,y]))$r.squared
If I have 10,000 columns, the loops (10,000 * 10,000) take forever even if
there is no formula inside.

Then, I attempted to vectorize my code:
> cor(matrix)^2
With 10,000 columns, this works great. With 30,000, R tells me it cannot
allocate vector of that length even if the memory limit is set to 4 GBs.

Is there anything else I can do to resolve this issue?

Thanks.
-- 
View this message in context: http://www.nabble.com/processing-a-large-matrix-tf3216447.html#a8932591
Sent from the R help mailing list archive at Nabble.com.


From roger.bos at us.rothschild.com  Mon Feb 12 22:13:34 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Mon, 12 Feb 2007 16:13:34 -0500
Subject: [R] Handling large calculations and memory
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD341DC6D9@RINNYCSE000.rth.ad.rothschild.com>

We don't know how much memory your computer has and how exactly you plan
to generate these samples, so no one can really help you all that much.
Its best for you to just code up what you want to do and find out if you
get any memory errors or not.  If you run into trouble and you can show
us exactly what you are doing, I am sure that at that point many people
will be very helpful.

Thanks,

Roger

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Leoni
Sent: Monday, February 12, 2007 11:18 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Handling large calculations and memory

Dear All,

I am planning to run a Monte-Carlo experiment which involves to do
roughly 100.000 times the following

1- Generating a sample of, say, 50.000 numbers from an ARMA or GARCH
2- Doing some regressions on the series
3- On each regression storing one special value from the results into a
sequence that will be analyzed later.

The experiment is calculation-intensive, and I suspect some problems of
memory management. I would appreciate some advices for the experiment to
run smoothly in terms of memory.

Many Thanks,

Patrick Leoni

Department of Economics and Finance
NUI at Maynooth

Maynooth, Co. Kildare
Republic of Ireland

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From cberry at tajo.ucsd.edu  Mon Feb 12 22:17:57 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Mon, 12 Feb 2007 13:17:57 -0800
Subject: [R] processing a large matrix
In-Reply-To: <8932591.post@talk.nabble.com>
References: <8932591.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0702121307070.25628@tajo.ucsd.edu>

On Mon, 12 Feb 2007, andy1983 wrote:

>
> I would like to compare every column in my matrix with every other column and
> get the r-squared.
>
> I tried using the following formula and looping through every column:
>> summary(lm(matrix[,x]~matrix[,y]))$r.squared
> If I have 10,000 columns, the loops (10,000 * 10,000) take forever even if
> there is no formula inside.
>
> Then, I attempted to vectorize my code:
>> cor(matrix)^2
> With 10,000 columns, this works great. With 30,000, R tells me it cannot
> allocate vector of that length even if the memory limit is set to 4 GBs.


30000^2 doubles * 8 Bytes/double > 6.5 GBs.

And that's just to store the result; you will need some space to work 
in, too.


>
> Is there anything else I can do to resolve this issue?
>
> Thanks.
> -- 
> View this message in context: http://www.nabble.com/processing-a-large-matrix-tf3216447.html#a8932591
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From Greg.Snow at intermountainmail.org  Mon Feb 12 22:34:16 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 12 Feb 2007 14:34:16 -0700
Subject: [R] processing a large matrix
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7FCD7D@LP-EXCHVS07.CO.IHC.COM>

One approach is to split up the work of doing the correlations, if you
give the 'cor' function 2 matricies then it gives you the correlations
between all pairs of columns.  Since you said it works fine with 10,000
columns but not 30,000 you could split into 3 pieces and do something
like (untested):

 out <- rbind(  
	cbind( cor(mymatrix[,1:10000])^2, 
            cor(mymatrix[,1:10000], mymatrix[10001:20000])^2, 
            cor(mymatrix[,1:10000], mymatrix[20001:30000])^2 ),
     cbind( matrix(NA,10000,10000),
            cor(mymatrix[,10001:20000])^2,
            cor(mymatrix[,20001:30000],mymatrix[,1:10000])^2),
     cbind( matrix(NA,10000,10000),
            matrix(NA,10000,10000),
            cor(mymatrix[,20001:30000])^2 )
     )

out[ lower.tri(out) ] <- t(out)[ lower.tri(out) ]

For breaking into 3 pieces, this is probably easier/quicker than trying
to find and alternative.  If you need to break it into even more pieces
(doing blocks of 1,000 when there are 30,000 columns) then there are
probably better alternatives (you could do a loop over blocks, that
would be faster than the loop over individual columns).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of andy1983
> Sent: Monday, February 12, 2007 1:55 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] processing a large matrix
> 
> 
> I would like to compare every column in my matrix with every 
> other column and get the r-squared.
> 
> I tried using the following formula and looping through every column:
> > summary(lm(matrix[,x]~matrix[,y]))$r.squared
> If I have 10,000 columns, the loops (10,000 * 10,000) take 
> forever even if there is no formula inside.
> 
> Then, I attempted to vectorize my code:
> > cor(matrix)^2
> With 10,000 columns, this works great. With 30,000, R tells 
> me it cannot allocate vector of that length even if the 
> memory limit is set to 4 GBs.
> 
> Is there anything else I can do to resolve this issue?
> 
> Thanks.
> --
> View this message in context: 
> http://www.nabble.com/processing-a-large-matrix-tf3216447.html
#a8932591
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Mon Feb 12 22:42:47 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 12 Feb 2007 14:42:47 -0700
Subject: [R] processing a large matrix
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7FCD84@LP-EXCHVS07.CO.IHC.COM>

Given the response by Carles Berry, you should probably really think
about what you want to do with the results (I'm hoping that you do not
plan to look at every R^2 value personally).  For instance if you want
to find which variable gives the highest R^2 value for each variable,
then this approach may work better:

myR2fun <- function(i){
  cat("\r",i)     # optional
  flush.console() # optional
 tmp <- cor( mymat[,i], mymat[,-i] )^2
 which.max(tmp)
}

out <- sapply( 1:30000, myR2fun )



-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greg Snow
> Sent: Monday, February 12, 2007 2:34 PM
> To: andy1983; r-help at stat.math.ethz.ch
> Subject: Re: [R] processing a large matrix
> 
> One approach is to split up the work of doing the 
> correlations, if you give the 'cor' function 2 matricies then 
> it gives you the correlations between all pairs of columns.  
> Since you said it works fine with 10,000 columns but not 
> 30,000 you could split into 3 pieces and do something like (untested):
> 
>  out <- rbind(  
> 	cbind( cor(mymatrix[,1:10000])^2, 
>             cor(mymatrix[,1:10000], mymatrix[10001:20000])^2, 
>             cor(mymatrix[,1:10000], mymatrix[20001:30000])^2 ),
>      cbind( matrix(NA,10000,10000),
>             cor(mymatrix[,10001:20000])^2,
>             cor(mymatrix[,20001:30000],mymatrix[,1:10000])^2),
>      cbind( matrix(NA,10000,10000),
>             matrix(NA,10000,10000),
>             cor(mymatrix[,20001:30000])^2 )
>      )
> 
> out[ lower.tri(out) ] <- t(out)[ lower.tri(out) ]
> 
> For breaking into 3 pieces, this is probably easier/quicker 
> than trying to find and alternative.  If you need to break it 
> into even more pieces (doing blocks of 1,000 when there are 
> 30,000 columns) then there are probably better alternatives 
> (you could do a loop over blocks, that would be faster than 
> the loop over individual columns).
> 
> Hope this helps,
> 
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of andy1983
> > Sent: Monday, February 12, 2007 1:55 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] processing a large matrix
> > 
> > 
> > I would like to compare every column in my matrix with every other 
> > column and get the r-squared.
> > 
> > I tried using the following formula and looping through 
> every column:
> > > summary(lm(matrix[,x]~matrix[,y]))$r.squared
> > If I have 10,000 columns, the loops (10,000 * 10,000) take forever 
> > even if there is no formula inside.
> > 
> > Then, I attempted to vectorize my code:
> > > cor(matrix)^2
> > With 10,000 columns, this works great. With 30,000, R tells me it 
> > cannot allocate vector of that length even if the memory 
> limit is set 
> > to 4 GBs.
> > 
> > Is there anything else I can do to resolve this issue?
> > 
> > Thanks.
> > --
> > View this message in context: 
> > http://www.nabble.com/processing-a-large-matrix-tf3216447.html
> #a8932591
> > Sent from the R help mailing list archive at Nabble.com.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sje at mast.queensu.ca  Mon Feb 12 22:43:12 2007
From: sje at mast.queensu.ca (Stephen Bond)
Date: Mon, 12 Feb 2007 15:43:12 -0600
Subject: [R] help with tryCatch
Message-ID: <45D0DF70.1090801@mast.queensu.ca>

Could smb please help with try-catch encapsulating a function for 
downloading. Let's say I have a character vector of symbols and want to 
download each one and surround by try and catch to be safe

# get.hist.quote() is in library(tseries), but the question does not 
depend on it, I could be sourcing local files instead

ans=null;error=null;
for ( sym in sym.vec){
try(ans=cbind(ans,get.hist.quote(sym,start=start))) #accumulate in a zoo 
matrix
catch(theurlerror){error=c(error,sym)} #accumulate failed symbols
}

I know the code above does not work, but it conveys the idea. tryCatch 
help page says it is similar to Java try-catch, but I know how to do a 
try-catch in Java and still can't do it in R.

Thank you very much.
stephen


From plynchnlm at gmail.com  Tue Feb 13 00:28:17 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Mon, 12 Feb 2007 18:28:17 -0500
Subject: [R] make check failure, internet.Rout.fail, Error in strsplit
Message-ID: <50d6c72a0702121528s6b9a3d56q682a65d1b4f2d11e@mail.gmail.com>

I'm trying to build R on RedHat EL4.  The compile went fine, but a
make check ran into a problem and produced a file
"internet.Rout.fail".  Judging by the last part of that file, it was
trying to run an R routine called "httpget" to retrieve the URL
http://www.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat.  The precise
error it encountered was:

Error in strsplit(grep("Content-Length", b, value = TRUE), ":")[[1]] :
        subscript out of bounds

So, it looks like the data it read from that URL was not what was
expected.  I tried mimicking the script's request of the header
information for that URL, and got back the following header lines:

HTTP/1.1 200 OK
Date: Mon, 12 Feb 2007 23:22:06 GMT
Server: Apache/2.0.40 (Red Hat Linux)
Last-Modified: Fri, 19 May 1995 10:27:04 GMT
ETag: "7bc27-836-39a78e00"
Accept-Ranges: bytes
Content-Type: text/plain; charset=ISO-8859-1
Content-length: 2102
Connection: Keep-Alive

The script appears to be looking for a "Content-Length" field, but as
you can see the returned header is "Content-length" with a lower-case
l.  I don't know R yet, so I'm not sure if the grep in the test code
is case-sensitive or not, but if it is, that would seem to be the
problem.  But then, surely everyone would be hitting this error?

Can anyone offer some suggestions as how to proceed from here?  Thanks,
      --Paul


From skiadas at hanover.edu  Tue Feb 13 02:43:31 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 12 Feb 2007 20:43:31 -0500
Subject: [R] make check failure, internet.Rout.fail, Error in strsplit
In-Reply-To: <50d6c72a0702121528s6b9a3d56q682a65d1b4f2d11e@mail.gmail.com>
References: <50d6c72a0702121528s6b9a3d56q682a65d1b4f2d11e@mail.gmail.com>
Message-ID: <7B594544-8B7F-4BE5-80AA-D86313845CF1@hanover.edu>

On Feb 12, 2007, at 6:28 PM, Paul Lynch wrote:

> I'm trying to build R on RedHat EL4.  The compile went fine, but a
> make check ran into a problem and produced a file
> "internet.Rout.fail".  Judging by the last part of that file, it was
> trying to run an R routine called "httpget" to retrieve the URL
> http://www.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat.  The precise
> error it encountered was:
>
> Error in strsplit(grep("Content-Length", b, value = TRUE), ":")[[1]] :
>         subscript out of bounds
>
> So, it looks like the data it read from that URL was not what was
> expected.  I tried mimicking the script's request of the header
> information for that URL, and got back the following header lines:
>
> HTTP/1.1 200 OK
> Date: Mon, 12 Feb 2007 23:22:06 GMT
> Server: Apache/2.0.40 (Red Hat Linux)
> Last-Modified: Fri, 19 May 1995 10:27:04 GMT
> ETag: "7bc27-836-39a78e00"
> Accept-Ranges: bytes
> Content-Type: text/plain; charset=ISO-8859-1
> Content-length: 2102
> Connection: Keep-Alive
>
> The script appears to be looking for a "Content-Length" field, but as
> you can see the returned header is "Content-length" with a lower-case
> l.  I don't know R yet, so I'm not sure if the grep in the test code
> is case-sensitive or not, but if it is, that would seem to be the
> problem.  But then, surely everyone would be hitting this error?

The grep is indeed case sensitive, as a quick test can show. However,  
the header I got back when I tried the above address had Length in it:
HTTP/1.1 200 OK
Date: Tue, 13 Feb 2007 01:40:48 GMT
Server: Apache/2.0.40 (Red Hat Linux)
Last-Modified: Fri, 19 May 1995 10:27:04 GMT
ETag: "7bc27-836-39a78e00"
Accept-Ranges: bytes
Content-Length: 2102
Content-Type: text/plain; charset=ISO-8859-1
X-Pad: avoid browser bug

( I used curl for this, if it makes a difference)

Hope this helps in some way.

>       --Paul

Haris


From hb at stat.berkeley.edu  Tue Feb 13 04:04:07 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Mon, 12 Feb 2007 19:04:07 -0800
Subject: [R] help with tryCatch
In-Reply-To: <45D0DF70.1090801@mast.queensu.ca>
References: <45D0DF70.1090801@mast.queensu.ca>
Message-ID: <59d7961d0702121904n2104d327udd7c964db0048956@mail.gmail.com>

See ?tryCatch. /Henrik

On 2/12/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> Could smb please help with try-catch encapsulating a function for
> downloading. Let's say I have a character vector of symbols and want to
> download each one and surround by try and catch to be safe
>
> # get.hist.quote() is in library(tseries), but the question does not
> depend on it, I could be sourcing local files instead
>
> ans=null;error=null;
> for ( sym in sym.vec){
> try(ans=cbind(ans,get.hist.quote(sym,start=start))) #accumulate in a zoo
> matrix
> catch(theurlerror){error=c(error,sym)} #accumulate failed symbols
> }
>
> I know the code above does not work, but it conveys the idea. tryCatch
> help page says it is similar to Java try-catch, but I know how to do a
> try-catch in Java and still can't do it in R.
>
> Thank you very much.
> stephen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From christian.convey at gmail.com  Tue Feb 13 05:29:14 2007
From: christian.convey at gmail.com (Christian Convey)
Date: Mon, 12 Feb 2007 23:29:14 -0500
Subject: [R] Can a data.frame column contain lists/arrays?
Message-ID: <6addebae0702122029h68e0b4d0ub64f3444a8704409@mail.gmail.com>

I'd like to have a data.frame structured something like the following:

d <- data.frame (
   x=list( c(1,2), c(5,2), c(9,1) ),
   y=c( 1, -1, -1)
)

The reason is this: 'd' is the training data for a machine learning
algorithm.  d$x is the independent data, and d$y is the dependent
data.

In general my machine learning code will work where each element of
d$x is a vector of one or more real numbers.  So for instance, the
same code should work when d$x[1] = 42, or when d$x[1] = (42, 3, 5).
All that matters is that all element within d$x are lists/vectors of
the same length.

Does anyone know if/how I can get a data.frame set up like that?

Thanks,
Christian


From wangtong at usc.edu  Tue Feb 13 05:30:01 2007
From: wangtong at usc.edu (Tong Wang)
Date: Mon, 12 Feb 2007 20:30:01 -0800
Subject: [R] Really need help here
Message-ID: <dd67df649e3d.45d0ce49@usc.edu>

Hi there,
     I had a serious problem here . Consider the following Bayesian model(discretized variance gamma):    
#Likelihood
      J[i]<-lambda*G[i]+sigma*sqrt(G[i])*rnorm(0,1)
      G[i]<-rgamma(1/nu,1/nu)

#Prior:   
     nu<-rinvgamma(m,M)

# Parameters 
     lambda=-.04 ; sigam=.38; nu=6.48; m=10,M=10 ; T=5000 (length of data)
An author claimed that he got posterior distribution of nu with standard deviation .0965 

But I could only get posterior sd=.84 with the same setting. I have been working on my code for quite a while
, but still don't see what is wrong with it.  I would really appreciate it if any body could fit this model and let me
know if you can get it and how ? I attached my R code with data generating and posterior simulating functions, 
I used the slice sampler to do it ,  you could use your own version of slice sampler or other generic samplers. 

Again , I would really appreciate any help that could save me from the pain. 


vg <- function(T=5000,nu=6.48,n.iter=1000,beta=-.04,sig=.38,m=10,M=10){

#------------------- Data Generating -------------------------#

  G <- rgamma(T,1/nu,1/nu)
  J <- rnorm(T,beta*G,sig*sqrt(G))

#------------------- Gibbs Sampler ---------------------------#

  nu.stor <- rep(NA,n.iter)
  for(i in 1:n.iter){
  G <- MCslice1D(dgamma(x,1/nu,1/nu,log=TRUE)+dnorm(J, beta*x, sig*sqrt(x),log=TRUE),w=20,m=10,x0=G)
  nu <- MCslice1D(log(dinvgamma(x,m,M))-T*log(x)*1/x-T*log(gamma(1/x))+(1/x-1)*sum(log(G))-sum(G)/x, w=3,m=12, x0=nu)

  nu.stor[i] <- nu
}

  return(list(nu=nu.stor))

}


From christos at nuverabio.com  Tue Feb 13 05:58:03 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Mon, 12 Feb 2007 23:58:03 -0500
Subject: [R] Can a data.frame column contain lists/arrays?
In-Reply-To: <6addebae0702122029h68e0b4d0ub64f3444a8704409@mail.gmail.com>
References: <6addebae0702122029h68e0b4d0ub64f3444a8704409@mail.gmail.com>
Message-ID: <000f01c74f2b$8bba7830$0202a8c0@headquarters.silicoinsights>

Why do you need to use a data frame?  A list will give you the flexibility
you want:

d <- list( x=list( c(1,2), c(5,2), c(9,1) ), y=c( 1, -1, -1) )

Then you can access the individual elements 

> d$x
[[1]]
[1] 1 2

[[2]]
[1] 5 2

[[3]]
[1] 9 1

> d$y
[1]  1 -1 -1

> d$x[[1]]
[1] 1 2
  
-Christos

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Christian Convey
> Sent: Monday, February 12, 2007 11:29 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Can a data.frame column contain lists/arrays?
> 
> I'd like to have a data.frame structured something like the following:
> 
> d <- data.frame (
>    x=list( c(1,2), c(5,2), c(9,1) ),
>    y=c( 1, -1, -1)
> )
> 
> The reason is this: 'd' is the training data for a machine 
> learning algorithm.  d$x is the independent data, and d$y is 
> the dependent data.
> 
> In general my machine learning code will work where each 
> element of d$x is a vector of one or more real numbers.  So 
> for instance, the same code should work when d$x[1] = 42, or 
> when d$x[1] = (42, 3, 5).
> All that matters is that all element within d$x are 
> lists/vectors of the same length.
> 
> Does anyone know if/how I can get a data.frame set up like that?
> 
> Thanks,
> Christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From gregor.gorjanc at bfro.uni-lj.si  Tue Feb 13 08:49:27 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 13 Feb 2007 07:49:27 +0000 (UTC)
Subject: [R] SQL statements (directly) in R
References: <8828460.post@talk.nabble.com>
	<156CDC8CCFD1894295D2907F16337A4801420A83@bru-s-006.europe.shell.com>
	<1115a2b00702120637pba3cb7etfd911bf536994dd2@mail.gmail.com>
Message-ID: <loom.20070213T084639-170@post.gmane.org>

Wensui Liu <liuwensui <at> gmail.com> writes:

> 
> chris,
> if you could create a package similar to proc sql in SAs, that will be 
> so sweet.

Nice task, but I would not suggest to go this way. I found it quite easy to 
work with R subsetting system after you get some practice and that is more
than enough - at least for me. Maybe you should look into sqllite stuff that
BioC guys are working on.

Gregor


From gregor.gorjanc at bfro.uni-lj.si  Tue Feb 13 08:52:38 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 13 Feb 2007 07:52:38 +0000 (UTC)
Subject: [R] lmer and estimation of p-values: error with mcmcpvalue()
References: <45D0646F.20606@agr.uni-goettingen.de>
	<27291.212.209.13.15.1171286769.squirrel@www.sorch.se>
	<45D06CA0.7090603@agr.uni-goettingen.de>
Message-ID: <loom.20070213T085055-882@post.gmane.org>

Christoph Scherber <Christoph.Scherber <at> agr.uni-goettingen.de> writes:
> 
> Dear Henric,
> 
> Thanks, now it works; but how reliable are these estimates? Especially 
> with p-values close to 0.05 it is of course important that the range of 
> the estimates is not too large. I?ve just run several simulations, each 
> of which yielding sometimes quite different p-values.

I have not been using mcmcsamp(), but try increasing number of simulations.
Additionally, since you have only "single" parameters, you can directly look
at their posterior distributions and compute the areas under the curve.

Gregor


From osklyar at ebi.ac.uk  Tue Feb 13 09:55:11 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Tue, 13 Feb 2007 08:55:11 +0000
Subject: [R] anyone has C++ STL classes stability issue if used with R
Message-ID: <45D17CEF.9010304@ebi.ac.uk>

Hello,

is there any one who uses C++ STL classes when programming shared libs 
for R and has had any problems with STL?

In the very simple example below I am constantly getting segfaults when 
trying to populate the queue. The segfault occurs at what looks like a 
random index in the loop when pushing another element to the queue. 
Reproduced on 4 machines. Object x is an Image as in EBImage, i.e. a 3D 
R-array of numerics for the purpose of this code.

LENGTH(x) can be up to 1e6 and the number of elements potentially to be 
in the queue is about 20% of those. But I get segfaults often on a third 
of fours element being added.

Tried on R2.5.0-devel, R2.4.1-release and all machines were 64bit Linux 
with kernels 2.6.9 (stable CentOS), 2.6.17 (stable Ubuntu) and 2.6.20 
(Ubuntu devel).

Here are the compilation options of this particular module (built as 
part of EBImage, which generally compiles and works just fine):

--------------------------------------------------------------------------
g++ -I/home/osklyar/R/R-2.5.0-40659/include 
-I/home/osklyar/R/R-2.5.0-40659/include  -I/usr/local/include 
-DUSE_GTK -DGLIB_GETTEXT -I/usr/include/gtk-2.0 
-I/usr/lib/gtk-2.0/include -I/usr/include/atk-1.0 -I/usr/include/cairo 
-I/usr/include/pango-1.0 -I/usr/include/glib-2.0 
-I/usr/lib/glib-2.0/include   -Wall -g -O2 -Wall -pthread -I/usr/include 
-O2 -g -O2 -g  -fpic  -O2 -g  -c filters_watershed.cpp -o 
filters_watershed.o
--------------------------------------------------------------------------
And the linker:
--------------------------------------------------------------------------
g++ -shared -L/usr/local/lib64 -o EBImage.so colors.o conversions.o 
display.o filters_distmap.o filters_magick.o filters_morph.o 
filters_propagate.o filters_thresh.o filters_watershed.o init.o io.o 
object_counting.o tools.o -lgtk-x11-2.0 -lgdk-x11-2.0 -latk-1.0 
-lgdk_pixbuf-2.0 -lm -lpangocairo-1.0 -lfontconfig -lXext -lXrender 
-lXinerama -lXi -lXrandr -lXcursor -lXfixes -lpango-1.0 -lcairo -lX11 
-lgobject-2.0 -lgmodule-2.0 -ldl -lglib-2.0   -L/usr/lib 
-L/usr/X11R6/lib -lfreetype -lz -L/usr/lib -lMagick -llcms -ltiff 
-lfreetype -ljasper -ljpeg -lpng -lXext -lSM -lICE -lX11 -lbz2 -lxml2 
-lz -lpthread -lm -lpthread
--------------------------------------------------------------------------

It could be I am missing something totally simple and therefore get 
these errors, but I cannot identify what.

Thanks for help,
Oleg

-----------------------------------------
// common.h also includes R includes:
// #include <R.h>
// #include <Rdefines.h>
// #include <R_ext/Error.h>

#include "common.h"

#include <queue>

using namespace std;

struct Pixel {
     int x, y;
     double intens;
     /* the code will also fail with the same segfault if I remove all
      * the constructors here and use the commented block below instead
      * of pq.push( Pixel(i, j, val) */
     Pixel() {x = 0; y = 0; intens = 0; };
     Pixel(const Pixel& px) { x = px.x; y = px.y; intens = px.intens; };
     Pixel(int i, int j, double val): x(i), y(j), intens(val) {};
};

bool operator < (const Pixel & a, const Pixel & b) {
     return a.intens < b.intens;
};

typedef priority_queue<Pixel> PixelPrQueue;

SEXP
lib_filterInvWS (SEXP x) {
     SEXP res;
     int i, j, index;
     double val;
     PixelPrQueue pq;

     int nx = INTEGER ( GET_DIM(x) )[0];
     int ny = INTEGER ( GET_DIM(x) )[1];
     int nz = INTEGER ( GET_DIM(x) )[2];
     int nprotect = 0;

     PROTECT ( res = Rf_duplicate(x) );
     nprotect++;

     // Pixel px;
     for (int im = 0; im < nz; im++ ) {
         double * src = &( REAL(x)[ im * nx * ny ] );
         double * tgt = &( REAL(res)[ im * nx * ny ] );

         for ( j = 0; j < ny; j++ )
             for ( i = 0; i < nx; i++ ) {
                 index = i + nx * j;
                 val = src[ index ];
                 if ( val > BG ) {
                     tgt[ index ] = -1;
                     // px.x = i; px.y = j; px.intens = val; pq.push(px);
                     pq.push( Pixel(i, j, val) );
                     continue;
                 }
                 tgt[ index ] = BG;
             }
     }

     /* my main code was here, but deleted for debug */

     UNPROTECT (nprotect);
     return res;
}




-- 
Dr Oleg Sklyar * EBI/EMBL, Cambridge CB10 1SD, England * +44-1223-494466


From osklyar at ebi.ac.uk  Tue Feb 13 10:24:01 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Tue, 13 Feb 2007 09:24:01 +0000
Subject: [R]  anyone has C++ STL classes stability issue if used with R
In-Reply-To: <45D17CEF.9010304@ebi.ac.uk>
References: <45D17CEF.9010304@ebi.ac.uk>
Message-ID: <45D183B1.5020308@ebi.ac.uk>

Continued: With the following modifications (using pointers) it works 
(needs memory cleaning afterwards and new less operator though) and I do 
not understand why:

typedef priority_queue<Pixel *> PixelPrQueue;
...
pq.push( new Pixel(i, j, val) );
...

Oleg Sklyar wrote:
> Hello,
> 
> is there any one who uses C++ STL classes when programming shared libs 
> for R and has had any problems with STL?
> 
> In the very simple example below I am constantly getting segfaults when 
> trying to populate the queue. The segfault occurs at what looks like a 
> random index in the loop when pushing another element to the queue. 
> Reproduced on 4 machines. Object x is an Image as in EBImage, i.e. a 3D 
> R-array of numerics for the purpose of this code.
> 
> LENGTH(x) can be up to 1e6 and the number of elements potentially to be 
> in the queue is about 20% of those. But I get segfaults often on a third 
> of fours element being added.
> 
> Tried on R2.5.0-devel, R2.4.1-release and all machines were 64bit Linux 
> with kernels 2.6.9 (stable CentOS), 2.6.17 (stable Ubuntu) and 2.6.20 
> (Ubuntu devel).
> 
> Here are the compilation options of this particular module (built as 
> part of EBImage, which generally compiles and works just fine):
> 
> --------------------------------------------------------------------------
> g++ -I/home/osklyar/R/R-2.5.0-40659/include 
> -I/home/osklyar/R/R-2.5.0-40659/include  -I/usr/local/include 
> -DUSE_GTK -DGLIB_GETTEXT -I/usr/include/gtk-2.0 
> -I/usr/lib/gtk-2.0/include -I/usr/include/atk-1.0 -I/usr/include/cairo 
> -I/usr/include/pango-1.0 -I/usr/include/glib-2.0 
> -I/usr/lib/glib-2.0/include   -Wall -g -O2 -Wall -pthread -I/usr/include 
> -O2 -g -O2 -g  -fpic  -O2 -g  -c filters_watershed.cpp -o 
> filters_watershed.o
> --------------------------------------------------------------------------
> And the linker:
> --------------------------------------------------------------------------
> g++ -shared -L/usr/local/lib64 -o EBImage.so colors.o conversions.o 
> display.o filters_distmap.o filters_magick.o filters_morph.o 
> filters_propagate.o filters_thresh.o filters_watershed.o init.o io.o 
> object_counting.o tools.o -lgtk-x11-2.0 -lgdk-x11-2.0 -latk-1.0 
> -lgdk_pixbuf-2.0 -lm -lpangocairo-1.0 -lfontconfig -lXext -lXrender 
> -lXinerama -lXi -lXrandr -lXcursor -lXfixes -lpango-1.0 -lcairo -lX11 
> -lgobject-2.0 -lgmodule-2.0 -ldl -lglib-2.0   -L/usr/lib 
> -L/usr/X11R6/lib -lfreetype -lz -L/usr/lib -lMagick -llcms -ltiff 
> -lfreetype -ljasper -ljpeg -lpng -lXext -lSM -lICE -lX11 -lbz2 -lxml2 
> -lz -lpthread -lm -lpthread
> --------------------------------------------------------------------------
> 
> It could be I am missing something totally simple and therefore get 
> these errors, but I cannot identify what.
> 
> Thanks for help,
> Oleg
> 
> -----------------------------------------
> // common.h also includes R includes:
> // #include <R.h>
> // #include <Rdefines.h>
> // #include <R_ext/Error.h>
> 
> #include "common.h"
> 
> #include <queue>
> 
> using namespace std;
> 
> struct Pixel {
>      int x, y;
>      double intens;
>      /* the code will also fail with the same segfault if I remove all
>       * the constructors here and use the commented block below instead
>       * of pq.push( Pixel(i, j, val) */
>      Pixel() {x = 0; y = 0; intens = 0; };
>      Pixel(const Pixel& px) { x = px.x; y = px.y; intens = px.intens; };
>      Pixel(int i, int j, double val): x(i), y(j), intens(val) {};
> };
> 
> bool operator < (const Pixel & a, const Pixel & b) {
>      return a.intens < b.intens;
> };
> 
> typedef priority_queue<Pixel> PixelPrQueue;
> 
> SEXP
> lib_filterInvWS (SEXP x) {
>      SEXP res;
>      int i, j, index;
>      double val;
>      PixelPrQueue pq;
> 
>      int nx = INTEGER ( GET_DIM(x) )[0];
>      int ny = INTEGER ( GET_DIM(x) )[1];
>      int nz = INTEGER ( GET_DIM(x) )[2];
>      int nprotect = 0;
> 
>      PROTECT ( res = Rf_duplicate(x) );
>      nprotect++;
> 
>      // Pixel px;
>      for (int im = 0; im < nz; im++ ) {
>          double * src = &( REAL(x)[ im * nx * ny ] );
>          double * tgt = &( REAL(res)[ im * nx * ny ] );
> 
>          for ( j = 0; j < ny; j++ )
>              for ( i = 0; i < nx; i++ ) {
>                  index = i + nx * j;
>                  val = src[ index ];
>                  if ( val > BG ) {
>                      tgt[ index ] = -1;
>                      // px.x = i; px.y = j; px.intens = val; pq.push(px);
>                      pq.push( Pixel(i, j, val) );
>                      continue;
>                  }
>                  tgt[ index ] = BG;
>              }
>      }
> 
>      /* my main code was here, but deleted for debug */
> 
>      UNPROTECT (nprotect);
>      return res;
> }
> 
> 
> 
> 

-- 
Dr Oleg Sklyar * EBI/EMBL, Cambridge CB10 1SD, England * +44-1223-494466


From wl2776 at gmail.com  Tue Feb 13 11:22:55 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 13 Feb 2007 02:22:55 -0800 (PST)
Subject: [R] Help neural network in R
In-Reply-To: <407292.39134.qm@web53808.mail.yahoo.com>
References: <407292.39134.qm@web53808.mail.yahoo.com>
Message-ID: <8941445.post@talk.nabble.com>


Here is the list of NN related packages 
http://cran.r-project.org/src/contrib/Views/MachineLearning.html

I also have written some bindings from R to the SNNS (Stuttgart neural
network simulator), however, they are still not on the release stage.


vinod gullu wrote:
> 
> I am interested in Neural network models in R. Is
> there any reference material/tutorial which i can use.
> Regards, 
> 

-- 
View this message in context: http://www.nabble.com/Help-neural-network-in-R-tf3215374.html#a8941445
Sent from the R help mailing list archive at Nabble.com.


From petr.pikal at precheza.cz  Tue Feb 13 11:28:32 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 13 Feb 2007 11:28:32 +0100
Subject: [R] Trying to replicate error message in subset()
In-Reply-To: <6.1.0.6.0.20070212103217.01bf5ae0@mail.utm.utoronto.ca>
References: <45D0297C.5943.4DFF87@localhost>
Message-ID: <45D1A0E0.128.4BFDBC@localhost>

Hi

On 12 Feb 2007 at 10:42, Michael Rennie wrote:

Date sent:      	Mon, 12 Feb 2007 10:42:21 -0500
To:             	"Petr Pikal" <petr.pikal at precheza.cz>, r-help at stat.math.ethz.ch
From:           	Michael Rennie <mrennie at utm.utoronto.ca>
Subject:        	Re: [R] Trying to replicate error message in subset()

> 
> Okay
> 
> First- I apologise for my poor use of terminology (error vs. warning).
> 
> Second- thanks for pointing out what I hadn't noticed before- when I
> pass one case for selection to subset, I get all observations. When I
> pass two cases (as a vector), I get every second case for both cases
> (if both are present, if not, I just get every second case for the one
> that is present). Same happens for three cases, as pointed out by Petr
> below.
> 
> So, trying the %in% operator, I get slightly different behabviour, but
> the selection still seems dependent on the length of the vector given
> as a selector:
> 
>  > b<-c("D", "F", "A")
>  > new2.dat<-subset(ex.dat, a%in%x1)
>  > new2.dat
>               y1 x1 x2
> 1    2.34870479  A  B
> 3    1.66090055  A  B
> 5   -0.07904798  A  B
> 7    2.07053656  A  B
> 9    2.97980444  A  B
> .....
> 
> Now, I just get every second observation, over all cases of x1.
> Probably doesn't get as far as A because F is not present?

Are you completely sure?

I get

> table(ex.dat$x1)

 A  B  C  D  E 
40 40 40 40 40 
> table(new.dat$x1)

 A  B  C  D  E 
40  0  0 40  0 

so all ceses for A and D with subset like this

a<-c("D", "F", "A")
new.dat<-subset(ex.dat, x1 %in% a)

and ex.dat constructed according to your example.

If I get something what I do not expect:

1.	I check if my data are what they should be
2.	I check if search path and working directory does not contain some 
objects with conflicting names
3.	If my functions are complicated I try to look how their parts 
really work

If everything seems OK and unexpected behaviour still occures, I go 
through docummentation, help archives and finally I try to seek an 
advice from help list.

I must say that this is a bit time consuming but I usually learn a 
lot from my mistakes which I am able to resolve myself.

HTH
Petr


> 
> According to the documentation on subset(), the function works on rows
> of dataframes. I'm guessing this explains the behaviour I'm seeing-
> somehow, the length of the vector being passed as the subset argument
> is dictating which rows to evaluate.  So, can anyone offer advice on
> how to select EVERY instance for multiple cases in a dataframe (i.e.,
> all cases of both A and D from ex.dat), or will subset always be tied
> to the length of the 'subset' argument when a vector is passed to it?
> 
> Cheers,
> 
> Mike
> 
> 
> At 02:46 AM 12/02/2007, Petr Pikal wrote:
> >Hi
> >
> >it is not error it is just warning (Beeping a tea kettle with boiling
> >water is also not an error :-) and it tells you pretty explicitly
> >what is wrong see length of your objects
> >
> > > a<-c("D", "F", "A")
> > > new.dat<-subset(ex.dat, x1 == a)
> >Warning messages:
> >1: longer object length
> >         is not a multiple of shorter object length in: is.na(e1) |
> >is.na(e2)
> >2: longer object length
> >         is not a multiple of shorter object length in:
> >`==.default`(x1, a)
> > > new.dat
> >             y1 x1 x2
> >3    0.5977786  A  B
> >6    2.5470739  A  B
> >9    0.9128595  A  B
> >12   1.0953531  A  D
> >15   2.4984470  A  D
> >18   1.7289529  A  D
> >61  -0.4848938  D  B
> >6
> >
> >you can do better with %in% operator.
> >
> >HTH
> >Petr
> >
> >
> >
> >On 12 Feb 2007 at 1:51, Michael Rennie wrote:
> >
> >Date sent:              Mon, 12 Feb 2007 01:51:54 -0500
> >To:                     r-help at stat.math.ethz.ch
> >From:                   Michael Rennie <mrennie at utm.utoronto.ca>
> >Subject:                [R] Trying to replicate error message in
> >subset()
> >
> > >
> > > Hi, there
> > >
> > > I am trying to replicate an error message in subset() to see what
> > > it is that I'm doing wrong with the datasets I am trying to work
> > > with.
> > >
> > > Essentially, I am trying to pass a string vector to subset() in
> > > order to select a specific collection of cases (i.e., I have data
> > > for these cases in one table, and want to select data from another
> > > table that match up with the cases in the first table).
> > >
> > > The error I get is as follows:
> > >
> > > Warning messages:
> > > 1: longer object length
> > >          is not a multiple of shorter object length in: is.na(e1)
> > >          | is.na(e2)
> > > 2: longer object length
> > >          is not a multiple of shorter object length in:
> > >          `==.default`(LAKE, g)
> > >
> > > Here is an example case I've been working with (which works) that
> > > I've been trying to "break"such that I can get this error message
> > > to figure out what I am doing wrong in my case.
> > >
> > > y1<-rnorm(100, 2)
> > > x1<-rep(1:5, each=20)
> > > x2<-rep(1:2, each=10, times=10)
> > >
> > > ex.dat<-data.frame(cbind(y1,x1,x2))
> > >
> > >
> > > ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B", "C", "D", "E"))
> > > ex.dat$x2<-factor(ex.dat$x2, labels=c("B", "D"))
> > >
> > > a<-c("D", "F")
> > > a
> > >
> > > new.dat<-subset(ex.dat, x1 == a)
> > > new.dat
> > >
> > > I thought maybe I was getting errors because I had cases in my
> > > selection vector ('a' in this case) that weren't in my ex.dat
> > > list, but subset handles this fine and just gives me what it can
> > > find in the larger list.
> > >
> > > Any thoughts on how I can replicate the error? As far as I can
> > > tell, the only difference between the case where I am getting
> > > errors and the example above is that the levels of x1 in my case
> > > are words (i.e., "Smelly", "Howdy"), but strings are strings,
> > > aren't they?
> > >
> > > Mike
> > >
> > >
> > > Michael Rennie
> > > Ph.D. Candidate, University of Toronto at Mississauga
> > > 3359 Mississauga Rd. N.
> > > Mississauga, ON  L5L 1C6
> > > Ph: 905-828-5452  Fax: 905-828-3792
> > > www.utm.utoronto.ca/~w3rennie
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> >Petr Pikal
> >petr.pikal at precheza.cz
> 
> Michael Rennie
> Ph.D. Candidate, University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> www.utm.utoronto.ca/~w3rennie 
> 

Petr Pikal
petr.pikal at precheza.cz


From ravis at ambaresearch.com  Tue Feb 13 11:43:58 2007
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Tue, 13 Feb 2007 16:13:58 +0530
Subject: [R] Unable to load RMySQL
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E8C5DD@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070213/2e524c39/attachment.pl 

From philip.leifeld at uni-konstanz.de  Tue Feb 13 11:49:15 2007
From: philip.leifeld at uni-konstanz.de (Philip Leifeld)
Date: Tue, 13 Feb 2007 11:49:15 +0100
Subject: [R] isoMDS vs. other non-metric non-R routines
Message-ID: <45D1A5BB.882.62CEEC@philip.leifeld.uni-konstanz.de>

Dear useRs,

last week I asked you about a problem related to isoMDS. It turned 
out that in my case isoMDS was trapped. Nonetheless, I still have 
some problems with other data sets. Therefore I would like to know if 
anyone here has experience with how well isoMDS performs in 
comparison to other non-metric MDS routines, like Minissa.

I have the feeling that for large data sets with a high stress value 
(e.g. around 0.20) in cases where the intrinsic dimensionality of the 
data cannot be significantly reduced without considerably increasing 
stress, isoMDS performs worse (and yields a stress value of 0.31 in 
my example), while solutions tend to be similar for better fits and 
lower intrinsic dimensionality. I tried this on another data set 
where isoMDS yields a stress value of 0.19 and Minissa a stress value 
of 0.14.

Now the latter would still be considered a fair solution by some 
people while the former indicates a poor fit regardless of how strict 
your judgment is. I generally prefer using R over mixing with 
different programs, so it would be nice if results were of comparable 
quality...

Cheers

Phil


From jrkrideau at yahoo.ca  Tue Feb 13 12:14:44 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 13 Feb 2007 06:14:44 -0500 (EST)
Subject: [R] Can a data.frame column contain lists/arrays?
In-Reply-To: <6addebae0702122029h68e0b4d0ub64f3444a8704409@mail.gmail.com>
Message-ID: <592015.38234.qm@web32807.mail.mud.yahoo.com>


--- Christian Convey <christian.convey at gmail.com>
wrote:

> I'd like to have a data.frame structured something
> like the following:
> 
> d <- data.frame (
>    x=list( c(1,2), c(5,2), c(9,1) ),
>    y=c( 1, -1, -1)
> )
> 
> The reason is this: 'd' is the training data for a
> machine learning
> algorithm.  d$x is the independent data, and d$y is
> the dependent
> data.
> 
> In general my machine learning code will work where
> each element of
> d$x is a vector of one or more real numbers.  So for
> instance, the
> same code should work when d$x[1] = 42, or when
> d$x[1] = (42, 3, 5).
> All that matters is that all element within d$x are
> lists/vectors of
> the same length.
> 
> Does anyone know if/how I can get a data.frame set
> up like that?
> 
> Thanks,
> Christian


I doubt it.  A data.frame is a specific subset of a
list.  You should be able to do anything you want with
a list.  Have a look at the Lists and Dataframes
chapter of Intro to R.


From murdoch at stats.uwo.ca  Tue Feb 13 13:14:27 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Feb 2007 07:14:27 -0500
Subject: [R] anyone has C++ STL classes stability issue if used with R
In-Reply-To: <45D17CEF.9010304@ebi.ac.uk>
References: <45D17CEF.9010304@ebi.ac.uk>
Message-ID: <45D1ABA3.5020500@stats.uwo.ca>

On 2/13/2007 3:55 AM, Oleg Sklyar wrote:
> Hello,
> 
> is there any one who uses C++ STL classes when programming shared libs 
> for R and has had any problems with STL?

I don't, but I'd suggest asking a technical question like this on 
R-devel instead of R-help if you don't get help here.

I can see a few probably innocuous changes I'd suggest in your code 
below, but nothing obvious:  use Rinternals.h instead of Rdefines.h, 
don't use the Rf_ prefix, check the length of inputs before working with 
the values.

Duncan Murdoch

> 
> In the very simple example below I am constantly getting segfaults when 
> trying to populate the queue. The segfault occurs at what looks like a 
> random index in the loop when pushing another element to the queue. 
> Reproduced on 4 machines. Object x is an Image as in EBImage, i.e. a 3D 
> R-array of numerics for the purpose of this code.
> 
> LENGTH(x) can be up to 1e6 and the number of elements potentially to be 
> in the queue is about 20% of those. But I get segfaults often on a third 
> of fours element being added.
> 
> Tried on R2.5.0-devel, R2.4.1-release and all machines were 64bit Linux 
> with kernels 2.6.9 (stable CentOS), 2.6.17 (stable Ubuntu) and 2.6.20 
> (Ubuntu devel).
> 
> Here are the compilation options of this particular module (built as 
> part of EBImage, which generally compiles and works just fine):
> 
> --------------------------------------------------------------------------
> g++ -I/home/osklyar/R/R-2.5.0-40659/include 
> -I/home/osklyar/R/R-2.5.0-40659/include  -I/usr/local/include 
> -DUSE_GTK -DGLIB_GETTEXT -I/usr/include/gtk-2.0 
> -I/usr/lib/gtk-2.0/include -I/usr/include/atk-1.0 -I/usr/include/cairo 
> -I/usr/include/pango-1.0 -I/usr/include/glib-2.0 
> -I/usr/lib/glib-2.0/include   -Wall -g -O2 -Wall -pthread -I/usr/include 
> -O2 -g -O2 -g  -fpic  -O2 -g  -c filters_watershed.cpp -o 
> filters_watershed.o
> --------------------------------------------------------------------------
> And the linker:
> --------------------------------------------------------------------------
> g++ -shared -L/usr/local/lib64 -o EBImage.so colors.o conversions.o 
> display.o filters_distmap.o filters_magick.o filters_morph.o 
> filters_propagate.o filters_thresh.o filters_watershed.o init.o io.o 
> object_counting.o tools.o -lgtk-x11-2.0 -lgdk-x11-2.0 -latk-1.0 
> -lgdk_pixbuf-2.0 -lm -lpangocairo-1.0 -lfontconfig -lXext -lXrender 
> -lXinerama -lXi -lXrandr -lXcursor -lXfixes -lpango-1.0 -lcairo -lX11 
> -lgobject-2.0 -lgmodule-2.0 -ldl -lglib-2.0   -L/usr/lib 
> -L/usr/X11R6/lib -lfreetype -lz -L/usr/lib -lMagick -llcms -ltiff 
> -lfreetype -ljasper -ljpeg -lpng -lXext -lSM -lICE -lX11 -lbz2 -lxml2 
> -lz -lpthread -lm -lpthread
> --------------------------------------------------------------------------
> 
> It could be I am missing something totally simple and therefore get 
> these errors, but I cannot identify what.
> 
> Thanks for help,
> Oleg
> 
> -----------------------------------------
> // common.h also includes R includes:
> // #include <R.h>
> // #include <Rdefines.h>
> // #include <R_ext/Error.h>
> 
> #include "common.h"
> 
> #include <queue>
> 
> using namespace std;
> 
> struct Pixel {
>      int x, y;
>      double intens;
>      /* the code will also fail with the same segfault if I remove all
>       * the constructors here and use the commented block below instead
>       * of pq.push( Pixel(i, j, val) */
>      Pixel() {x = 0; y = 0; intens = 0; };
>      Pixel(const Pixel& px) { x = px.x; y = px.y; intens = px.intens; };
>      Pixel(int i, int j, double val): x(i), y(j), intens(val) {};
> };
> 
> bool operator < (const Pixel & a, const Pixel & b) {
>      return a.intens < b.intens;
> };
> 
> typedef priority_queue<Pixel> PixelPrQueue;
> 
> SEXP
> lib_filterInvWS (SEXP x) {
>      SEXP res;
>      int i, j, index;
>      double val;
>      PixelPrQueue pq;
> 
>      int nx = INTEGER ( GET_DIM(x) )[0];
>      int ny = INTEGER ( GET_DIM(x) )[1];
>      int nz = INTEGER ( GET_DIM(x) )[2];
>      int nprotect = 0;
> 
>      PROTECT ( res = Rf_duplicate(x) );
>      nprotect++;
> 
>      // Pixel px;
>      for (int im = 0; im < nz; im++ ) {
>          double * src = &( REAL(x)[ im * nx * ny ] );
>          double * tgt = &( REAL(res)[ im * nx * ny ] );
> 
>          for ( j = 0; j < ny; j++ )
>              for ( i = 0; i < nx; i++ ) {
>                  index = i + nx * j;
>                  val = src[ index ];
>                  if ( val > BG ) {
>                      tgt[ index ] = -1;
>                      // px.x = i; px.y = j; px.intens = val; pq.push(px);
>                      pq.push( Pixel(i, j, val) );
>                      continue;
>                  }
>                  tgt[ index ] = BG;
>              }
>      }
> 
>      /* my main code was here, but deleted for debug */
> 
>      UNPROTECT (nprotect);
>      return res;
> }
> 
> 
> 
>


From chrish at stats.ucl.ac.uk  Tue Feb 13 13:53:11 2007
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Tue, 13 Feb 2007 12:53:11 +0000 (GMT)
Subject: [R] isoMDS vs. other non-metric non-R routines
In-Reply-To: <45D1A5BB.882.62CEEC@philip.leifeld.uni-konstanz.de>
References: <45D1A5BB.882.62CEEC@philip.leifeld.uni-konstanz.de>
Message-ID: <Pine.LNX.4.64.0702131247390.25544@egon.stats.ucl.ac.uk>

Dear Phil,

I don't have experiences with Minissa but I know that isoMDS is bad in 
some situations. I have even seen situations with non-metric 
dissimilarities in which the classical MDS was preferable.

Some alternatives that you have:
1) Try to start isoMDS from other initial configurations (by default, it 
starts from the classical solution).
2) Try sammon mapping (command should be "sammon").
3) Have a look at XGvis/GGvis (which may be part of XGobi/GGobi). These 
are not directly part of R but have R interfaces. They allow you to toy
around quite a lot with different algorithms, stress functions (the 
isoMDS stress is not necessarily what you want) and initial 
configurations so that you can find a better solution and understand your 
data better. Unfortunately I don't have the time to give you more detail, 
but google for it (or somebody else will tell you more).

Best,
Christian


On Tue, 13 Feb 2007, Philip Leifeld wrote:

> Dear useRs,
>
> last week I asked you about a problem related to isoMDS. It turned
> out that in my case isoMDS was trapped. Nonetheless, I still have
> some problems with other data sets. Therefore I would like to know if
> anyone here has experience with how well isoMDS performs in
> comparison to other non-metric MDS routines, like Minissa.
>
> I have the feeling that for large data sets with a high stress value
> (e.g. around 0.20) in cases where the intrinsic dimensionality of the
> data cannot be significantly reduced without considerably increasing
> stress, isoMDS performs worse (and yields a stress value of 0.31 in
> my example), while solutions tend to be similar for better fits and
> lower intrinsic dimensionality. I tried this on another data set
> where isoMDS yields a stress value of 0.19 and Minissa a stress value
> of 0.14.
>
> Now the latter would still be considered a fair solution by some
> people while the former indicates a poor fit regardless of how strict
> your judgment is. I generally prefer using R over mixing with
> different programs, so it would be nice if results were of comparable
> quality...
>
> Cheers
>
> Phil
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From osklyar at ebi.ac.uk  Tue Feb 13 14:06:16 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Tue, 13 Feb 2007 13:06:16 +0000
Subject: [R] anyone has C++ STL classes stability issue if used with R
In-Reply-To: <45D1ABA3.5020500@stats.uwo.ca>
References: <45D17CEF.9010304@ebi.ac.uk> <45D1ABA3.5020500@stats.uwo.ca>
Message-ID: <45D1B7C8.6090104@ebi.ac.uk>

Duncan,

you are right about Rf_..., otherwise the lengths are checked in the R 
side, this is just one of the functions I have in the package and all 
arguments are thoroughly checked.

But apparently, the same code if redefined for using pointers instead of 
references works just perfectly fine (given below). And I do not see why 
the one I posted before fails. I will try to run it outside of R to see 
if the issue is anyhow connected to R.

// comparison operator redefined for pointers:
struct Pixel_compare: public binary_function<Pixel*, Pixel*, bool> {
     bool operator() (Pixel* a, Pixel* b) {
         return a->intens < b->intens;
     }
};
// was:
// struct Pixel_compare: public binary_function<Pixel&, Pixel&, bool> {
//    bool operator() (Pixel& a, Pixel& b) {
//        return a.intens < b.intens;
//    }
// };


// the queue redefined for pointers
typedef priority_queue<Pixel*, vector<Pixel*>, Pixel_compare> PixelPrQueue;
// was: typedef priority_queue<Pixel, vector<Pixel>, Pixel_compare> 
PixelPrQueue;

SEXP
lib_filterInvWS (SEXP x) {
     SEXP res;
     int i, j, index;
     double val;

     int nx = INTEGER ( GET_DIM(x) )[0];
     int ny = INTEGER ( GET_DIM(x) )[1];
     int nz = INTEGER ( GET_DIM(x) )[2];
     int nprotect = 0;

     PROTECT ( res = Rf_duplicate(x) );
     nprotect++;

     for (int im = 0; im < nz; im++ ) {
         double * src = &( REAL(x)[ im * nx * ny ] );
         double * tgt = &( REAL(res)[ im * nx * ny ] );

         PixelPrQueue pq;
         for ( j = 0; j < ny; j++ )
             for ( i = 0; i < nx; i++ ) {
                 index = i + nx * j;
                 val = src[ index ];
                 if ( val > BG ) {
                     tgt[ index ] = -1;
// new pixels are created as pointer to objects
// was: pq.push( Pixel(i, j, val) );
                     pq.push( new Pixel(i, j, val) );
                     continue;
                 }
                 tgt[ index ] = BG;
             }
// printed and all pointers deleted
         Pixel * px;
         while ( !pq.empty() ) {
             px = pq.top();
             pq.pop();
             Rprintf("%f\n", px->intens);
             delete px;
         }

     }

     UNPROTECT (nprotect);
     return res;
}

The above works fine. The "Compare" operator is defined differently from 
my previous post, but both fail if used with references.
Oleg

Duncan Murdoch wrote:
> On 2/13/2007 3:55 AM, Oleg Sklyar wrote:
>> Hello,
>>
>> is there any one who uses C++ STL classes when programming shared libs 
>> for R and has had any problems with STL?
> 
> I don't, but I'd suggest asking a technical question like this on 
> R-devel instead of R-help if you don't get help here.
> 
> I can see a few probably innocuous changes I'd suggest in your code 
> below, but nothing obvious:  use Rinternals.h instead of Rdefines.h, 
> don't use the Rf_ prefix, check the length of inputs before working with 
> the values.
> 
> Duncan Murdoch
> 
>>
>> In the very simple example below I am constantly getting segfaults 
>> when trying to populate the queue. The segfault occurs at what looks 
>> like a random index in the loop when pushing another element to the 
>> queue. Reproduced on 4 machines. Object x is an Image as in EBImage, 
>> i.e. a 3D R-array of numerics for the purpose of this code.
>>
>> LENGTH(x) can be up to 1e6 and the number of elements potentially to 
>> be in the queue is about 20% of those. But I get segfaults often on a 
>> third of fours element being added.
>>
>> Tried on R2.5.0-devel, R2.4.1-release and all machines were 64bit 
>> Linux with kernels 2.6.9 (stable CentOS), 2.6.17 (stable Ubuntu) and 
>> 2.6.20 (Ubuntu devel).
>>
>> Here are the compilation options of this particular module (built as 
>> part of EBImage, which generally compiles and works just fine):
>>
>> -------------------------------------------------------------------------- 
>>
>> g++ -I/home/osklyar/R/R-2.5.0-40659/include 
>> -I/home/osklyar/R/R-2.5.0-40659/include  -I/usr/local/include 
>> -DUSE_GTK -DGLIB_GETTEXT -I/usr/include/gtk-2.0 
>> -I/usr/lib/gtk-2.0/include -I/usr/include/atk-1.0 -I/usr/include/cairo 
>> -I/usr/include/pango-1.0 -I/usr/include/glib-2.0 
>> -I/usr/lib/glib-2.0/include   -Wall -g -O2 -Wall -pthread 
>> -I/usr/include -O2 -g -O2 -g  -fpic  -O2 -g  -c filters_watershed.cpp 
>> -o filters_watershed.o
>> -------------------------------------------------------------------------- 
>>
>> And the linker:
>> -------------------------------------------------------------------------- 
>>
>> g++ -shared -L/usr/local/lib64 -o EBImage.so colors.o conversions.o 
>> display.o filters_distmap.o filters_magick.o filters_morph.o 
>> filters_propagate.o filters_thresh.o filters_watershed.o init.o io.o 
>> object_counting.o tools.o -lgtk-x11-2.0 -lgdk-x11-2.0 -latk-1.0 
>> -lgdk_pixbuf-2.0 -lm -lpangocairo-1.0 -lfontconfig -lXext -lXrender 
>> -lXinerama -lXi -lXrandr -lXcursor -lXfixes -lpango-1.0 -lcairo -lX11 
>> -lgobject-2.0 -lgmodule-2.0 -ldl -lglib-2.0   -L/usr/lib 
>> -L/usr/X11R6/lib -lfreetype -lz -L/usr/lib -lMagick -llcms -ltiff 
>> -lfreetype -ljasper -ljpeg -lpng -lXext -lSM -lICE -lX11 -lbz2 -lxml2 
>> -lz -lpthread -lm -lpthread
>> -------------------------------------------------------------------------- 
>>
>>
>> It could be I am missing something totally simple and therefore get 
>> these errors, but I cannot identify what.
>>
>> Thanks for help,
>> Oleg
>>
>> -----------------------------------------
>> // common.h also includes R includes:
>> // #include <R.h>
>> // #include <Rdefines.h>
>> // #include <R_ext/Error.h>
>>
>> #include "common.h"
>>
>> #include <queue>
>>
>> using namespace std;
>>
>> struct Pixel {
>>      int x, y;
>>      double intens;
>>      /* the code will also fail with the same segfault if I remove all
>>       * the constructors here and use the commented block below instead
>>       * of pq.push( Pixel(i, j, val) */
>>      Pixel() {x = 0; y = 0; intens = 0; };
>>      Pixel(const Pixel& px) { x = px.x; y = px.y; intens = px.intens; };
>>      Pixel(int i, int j, double val): x(i), y(j), intens(val) {};
>> };
>>
>> bool operator < (const Pixel & a, const Pixel & b) {
>>      return a.intens < b.intens;
>> };
>>
>> typedef priority_queue<Pixel> PixelPrQueue;
>>
>> SEXP
>> lib_filterInvWS (SEXP x) {
>>      SEXP res;
>>      int i, j, index;
>>      double val;
>>      PixelPrQueue pq;
>>
>>      int nx = INTEGER ( GET_DIM(x) )[0];
>>      int ny = INTEGER ( GET_DIM(x) )[1];
>>      int nz = INTEGER ( GET_DIM(x) )[2];
>>      int nprotect = 0;
>>
>>      PROTECT ( res = Rf_duplicate(x) );
>>      nprotect++;
>>
>>      // Pixel px;
>>      for (int im = 0; im < nz; im++ ) {
>>          double * src = &( REAL(x)[ im * nx * ny ] );
>>          double * tgt = &( REAL(res)[ im * nx * ny ] );
>>
>>          for ( j = 0; j < ny; j++ )
>>              for ( i = 0; i < nx; i++ ) {
>>                  index = i + nx * j;
>>                  val = src[ index ];
>>                  if ( val > BG ) {
>>                      tgt[ index ] = -1;
>>                      // px.x = i; px.y = j; px.intens = val; pq.push(px);
>>                      pq.push( Pixel(i, j, val) );
>>                      continue;
>>                  }
>>                  tgt[ index ] = BG;
>>              }
>>      }
>>
>>      /* my main code was here, but deleted for debug */
>>
>>      UNPROTECT (nprotect);
>>      return res;
>> }
>>
>>
>>
>>

-- 
Dr Oleg Sklyar * EBI/EMBL, Cambridge CB10 1SD, England * +44-1223-494466


From shubhak at ambaresearch.com  Tue Feb 13 14:17:45 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Tue, 13 Feb 2007 18:47:45 +0530
Subject: [R] Fatigued R
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3FEF349@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070213/9547e569/attachment.pl 

From sarah.goslee at gmail.com  Tue Feb 13 14:38:59 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 13 Feb 2007 08:38:59 -0500
Subject: [R] Fatigued R
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3FEF349@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3FEF349@BAN-MAILSRV03.Amba.com>
Message-ID: <efb536d50702130538m1c7dffebi4fef0eaee4c2d476@mail.gmail.com>

Hi Shubha,

Perhaps you haven't gotten any help because you haven't provided a
reproducible example, or even told us what you are trying to do (specifically)
or what errors you are receiving. Frankly, your problem statement doesn't
make any sense to me, and I can't provide advice without more information.

As the footer of every email says:
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Sarah

On 2/13/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> Hi R,
>
>
>
> Please solve my problem...........
>
>
>
> I am extracting Bloomberg data from R, in a loop. R is getting fatigued
> by doing this process and gives some errors. I introduced sleep
> function. Doing this sometimes I get the results and sometimes not. I
> even noticed that if I give complete rest for R (don't open R window)
> for 1 day and then run my code with the sleep function, then the program
> works. But if I keep on doing this with on R, repeatedly I get errors.
>
>
>
> Please can anyone do something for this? Is there any function of
> refreshing R completely.....Or any other techniques?...I am really
> getting bugged with this.......
>
>
>
> Thanks,
>
> Shubha


-- 
Sarah Goslee
http://www.functionaldiversity.org


From tramni at abv.bg  Tue Feb 13 10:55:12 2007
From: tramni at abv.bg (Martin Ivanov)
Date: Tue, 13 Feb 2007 11:55:12 +0200 (GMT+02:00)
Subject: [R] lag orders with ADF.test
Message-ID: <1696621856.231241171360512915.JavaMail.nobody@mail12.abv.bg>

Hello! 
 I do not understand what is meant by: 
 
 "aic" and "bic" follow a top-down strategy based on the Akaike's and Schwarz's information criteria 
 
in the datails to the ADF.test function. What does a "top-down strategy" mean? Probably the respective criterion is minimized and the mode vector contains the lag orders at which the criterion attains it local minima? When the calculation is over, the ADF.test function gives info about "Lag orders". What are these lag orders? Are they the local minima of the criterion? 
 
 I will be very thankful if you clarify this to me. I browsed a lot, but I could not find a clear answer. 
 
 Thank you for your attention. 
 Regards, 
 Martin

-----------------------------------------------------------------
http://auto-motor-und-sport.bg/ 
? ?????? ? ??????!


From waniujjwal at rediffmail.com  Tue Feb 13 08:41:30 2007
From: waniujjwal at rediffmail.com (ujjwal wani)
Date: 13 Feb 2007 07:41:30 -0000
Subject: [R] glpk package
Message-ID: <20070213074130.8964.qmail@webmail105.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070213/64536655/attachment.pl 

From shubhak at ambaresearch.com  Tue Feb 13 14:51:08 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Tue, 13 Feb 2007 19:21:08 +0530
Subject: [R] Fatigued R
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3FEF360@BAN-MAILSRV03.Amba.com>

Ohkkk....I will try to do that now....

This is my download function...


download<-function(fil)
{
con<-blpConnect(show.days=show_day, na.action=na_action,
periodicity=periodicity)
for(i in 1:lent)
{
Sys.sleep(3)
cdaily<-blpGetData(con,unique(t[,i]),fil,start=as.chron(as.Date("1/1/199
6", "%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))
#Sys.sleep(3)
if(!exists("d_mer")) d_mer=cdaily else d_mer=merge(d_mer,cdaily)
rm(cdaily)
}
dat<-data.frame(Date=index(d_mer),d_mer)
dat$ABCDEFG=NULL
path1<-paste("D:\\SAS\\Project2\\Daily\\",fil,"_root.sas7bdat",sep="")
path2<-paste("D:\\SAS\\Project2\\Daily\\CODEFILES\\",fil,".sas",sep="")
sasname<-paste(x1,fil,"'",sep="")
write.foreign(dat,path1,path2,package="SAS",dataname=sasname)
blpDisconnect(con)
}

for(j in 1:lenf)
{
fname<-paste("D:\\SAS\\Project2\\Daily\\",filename[j],"_root.sas7bdat",s
ep="")
Sys.sleep(600)
if(!file.exists(fname)) download(fil=filename[j])
}

And lent=58 and lenf=8... 8 text files will be generated in this process
if the program would have run properly, and each would be of size 4,000
KB.

The error message I get if the program is not run is:

Error in dimnames(x) <- dn : length of 'dimnames' [2] not equal to array
extent
In addition: Warning message:
Index vectors are of different classes: chron chron dates in:
merge(d_mer,cdaily)



Please could any one help on this?

-----Original Message-----
From: Sarah Goslee [mailto:sarah.goslee at gmail.com] 
Sent: Tuesday, February 13, 2007 7:09 PM
To: Shubha Vishwanath Karanth
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Fatigued R

Hi Shubha,

Perhaps you haven't gotten any help because you haven't provided a
reproducible example, or even told us what you are trying to do
(specifically)
or what errors you are receiving. Frankly, your problem statement
doesn't
make any sense to me, and I can't provide advice without more
information.

As the footer of every email says:
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Sarah

On 2/13/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> Hi R,
>
>
>
> Please solve my problem...........
>
>
>
> I am extracting Bloomberg data from R, in a loop. R is getting
fatigued
> by doing this process and gives some errors. I introduced sleep
> function. Doing this sometimes I get the results and sometimes not. I
> even noticed that if I give complete rest for R (don't open R window)
> for 1 day and then run my code with the sleep function, then the
program
> works. But if I keep on doing this with on R, repeatedly I get errors.
>
>
>
> Please can anyone do something for this? Is there any function of
> refreshing R completely.....Or any other techniques?...I am really
> getting bugged with this.......
>
>
>
> Thanks,
>
> Shubha


-- 
Sarah Goslee
http://www.functionaldiversity.org


From Christoph.Scherber at agr.uni-goettingen.de  Tue Feb 13 14:45:44 2007
From: Christoph.Scherber at agr.uni-goettingen.de (Christoph Scherber)
Date: Tue, 13 Feb 2007 14:45:44 +0100
Subject: [R] lme4/lmer: P-Values from mcmc samples or chi2-tests?
Message-ID: <45D1C108.2030908@agr.uni-goettingen.de>

Dear R users,

I have now tried out several options of obtaining p-values for 
(quasi)poisson lmer models, including Markov-chain Monte Carlo sampling 
and single-term deletions with subsequent chi-square tests (although I 
am aware that the latter may be problematic).

However, I encountered several problems that can be classified as
(1) the quasipoisson lmer model does not give p-values when summary() is 
called (see below)
(2) dependence on the size of the mcmc sample
(3) lack of correspondence between different p-value estimates.

How can I proceed, left with these uncertainties in the estimations of 
the p-values?

Below is the corresponding R code with some output so that you can see 
it all for your own:

##
m1<-lmer(number_pollinators~logpatch+loghab+landscape_diversity+(1|site),primula,poisson,method="ML")
m2<-lmer(number_pollinators~logpatch+loghab+landscape_diversity+(1|site),primula,quasipoisson,method="ML")
summary(m1);summary(m2)

#m1: [...]
Fixed effects:
                     Estimate Std. Error z value Pr(>|z|)
(Intercept)         -0.40302    0.57403 -0.7021  0.48262
logpatch             0.10915    0.04111  2.6552  0.00793 **
loghab               0.08750    0.06128  1.4279  0.15331
landscape_diversity  1.02338    0.40604  2.5204  0.01172 *

#m2: [...] #for the quasipoisson model, no p values are shown?!
Fixed effects:
                     Estimate Std. Error t value
(Intercept)          -0.4030     0.6857 -0.5877
logpatch              0.1091     0.0491  2.2228
loghab                0.0875     0.0732  1.1954
landscape_diversity   1.0234     0.4850  2.1099

##

anova(m1)
#here, no p-values appear (only in the current version of lme4)

Analysis of Variance Table
                     Df  Sum Sq Mean Sq
logpatch             1 11.6246 11.6246
loghab               1  6.0585  6.0585
landscape_diversity  1  6.3024  6.3024

anova(m2)
Analysis of Variance Table
                     Df  Sum Sq Mean Sq
logpatch             1 11.6244 11.6244
loghab               1  6.0581  6.0581
landscape_diversity  1  6.3023  6.3023

So I am left with the p-values estimated from the poisson model; 
single-term deletion tests for each of the individual terms lead to 
different p-values; I restrict this here to m2:

##
m2a=update(m2,~.-loghab)
anova(m2,m2a,test="Chi")

Data: primula
Models:
m2a: number_pollinators ~ logpatch + landscape_diversity + (1 | site)
m2: number_pollinators ~ logpatch + loghab + landscape_diversity + (1 | 
site)
     Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
m2a  4  84.713  91.850 -38.357
m2   5  84.834  93.755 -37.417 1.8793      1     0.1704

##
m2b=update(m2,~.-logpatch)
anova(m2,m2b,test="Chi")

Data: primula
Models:
m2b: number_pollinators ~ loghab + landscape_diversity + (1 | site)
m2: number_pollinators ~ logpatch + loghab + landscape_diversity +
m2b:     (1 | site)
     Df     AIC     BIC  logLik Chisq Chi Df Pr(>Chisq)
m2b  4  90.080  97.217 -41.040
m2   5  84.834  93.755 -37.417 7.246      1   0.007106 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

##
m2c=update(m2,~.-landscape_diversity)
anova(m2,m2c,test="Chi")

Data: primula
Models:
m2c: number_pollinators ~ logpatch + loghab + (1 | site)
m2: number_pollinators ~ logpatch + loghab + landscape_diversity +
m2c:     (1 | site)
     Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
m2c  4  89.036  96.173 -40.518
m2   5  84.834  93.755 -37.417 6.2023      1    0.01276 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


The p-values from mcmc are:

##
markov1=mcmcsamp(m2,5000)

HPDinterval(markov1)
                             lower      upper
(Intercept)          -1.394287660  0.6023229
logpatch              0.031154910  0.1906861
loghab                0.002961281  0.2165285
landscape_diversity   0.245623183  1.6442544
log(site.(In))      -41.156007604 -1.6993996
attr(,"Probability")
[1] 0.95

##

mcmcpvalue(as.matrix(markov1[,1])) #i.e. the p value for the intercept
[1] 0.3668
 > mcmcpvalue(as.matrix(markov1[,2])) #i.e. the p-value for logpatch
[1] 0.004
 > mcmcpvalue(as.matrix(markov1[,3])) #i.e. the p-value for loghab
[1] 0.0598
 > mcmcpvalue(as.matrix(markov1[,4])) #i.e. the p-value for landscape.div
[1] 0.0074

If one runs the mcmcsamp function for, say, 50,000 runs, the p-values 
are slightly different (not shown here).

##here are the p-values summarized in tabular form:

(Intercept)        	0.3668
logpatch         	0.004
loghab            	0.0598
landscape_diversity    	0.0074


##and from the single-term deletions:

(Intercept)        N.A.
logpatch        	0.007106
loghab            	0.1704
landscape_diversity    	0.01276


## Compare this with the p-values from the poisson model:
Fixed effects:
                     Estimate Std. Error z value Pr(>|z|)
(Intercept)         -0.40302    0.57403 -0.7021  0.48262
logpatch             0.10915    0.04111  2.6552  0.00793 **
loghab               0.08750    0.06128  1.4279  0.15331
landscape_diversity  1.02338    0.40604  2.5204  0.01172 *

##

To summarize, at least for quasipoisson models, the p-values obtained 
from mcmcpvalue() are quite different from those obtained using 
single-term deletions followed by a chisquare test.

Especially in the case of "loghab", the difference is so huge that one 
could tend to interpret one of the p-values as "marginally significant".

However, the mcmc chains look allright.

What would your suggestions be on how to proceed?

Thanks a lot for your help!

Best wishes,
Christoph.



##
I am using R 2.4.1 and the lme4 version I use is  0.9975-11 (Date: 
2007-01-25)


From roderick.castillo at metanomics.de  Tue Feb 13 15:15:02 2007
From: roderick.castillo at metanomics.de (roderick.castillo at metanomics.de)
Date: Tue, 13 Feb 2007 15:15:02 +0100
Subject: [R] Suddenly "Subscript out of bounds"
Message-ID: <OFF4024685.69E614DC-ONC1257281.004BF8F0-C1257281.004E5423@basf-c-s.be>


Hello

Using R Version 2.3.1 I have setup a cronjob to update packages,
which worked successfully almost a year (it was called daily).
Basically, it runs like this:

Sys.getenv("http_proxy")
update.packages(ask=F,repos="http://cran.r-project.org")

(the http_proxy environment variable is set prior to the call).

All of a sudden, I started to get this error:

Error in inherits(x,  "factor") :  subscript out of bounds

I have no clue about what this means. Is "factor" a buggy package?

Clearly, that kind of things can happen when you just update things
automatically...

Any help with be appreciated!

Bye
Rick


From sarah.goslee at gmail.com  Tue Feb 13 15:11:24 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 13 Feb 2007 09:11:24 -0500
Subject: [R] Fatigued R
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3FEF360@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3FEF360@BAN-MAILSRV03.Amba.com>
Message-ID: <efb536d50702130611v62c7e7b3wadc7f69ec7a46401@mail.gmail.com>

That's a help. It isn't actually reproducible since I have no idea where
for example blpConnect() comes from, but it's still an improvement.

Moving the important bit to the top:

> The error message I get if the program is not run is:
>
> Error in dimnames(x) <- dn : length of 'dimnames' [2] not equal to array
> extent
> In addition: Warning message:
> Index vectors are of different classes: chron chron dates in:
> merge(d_mer,cdaily)

These seem to mean that your data don't (always?) look like you are
expecting, and have nothing to do with fatigue or sleepiness. Except
maybe yours?

I would suggest going through your function one command at a time
(without the loop, but set the loop index manually if needed), and check
carefully the format of the data. If it works sometimes and not others,
identify a loop index for which it doesn't work and repeat the process.

Someone familiar with Bloomberg data might be able to provide more
assistance.

Sarah

On 2/13/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> Ohkkk....I will try to do that now....
>
> This is my download function...
>
>
> download<-function(fil)
> {
> con<-blpConnect(show.days=show_day, na.action=na_action,
> periodicity=periodicity)
> for(i in 1:lent)
> {
> Sys.sleep(3)
> cdaily<-blpGetData(con,unique(t[,i]),fil,start=as.chron(as.Date("1/1/199
> 6", "%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))
> #Sys.sleep(3)
> if(!exists("d_mer")) d_mer=cdaily else d_mer=merge(d_mer,cdaily)
> rm(cdaily)
> }
> dat<-data.frame(Date=index(d_mer),d_mer)
> dat$ABCDEFG=NULL
> path1<-paste("D:\\SAS\\Project2\\Daily\\",fil,"_root.sas7bdat",sep="")
> path2<-paste("D:\\SAS\\Project2\\Daily\\CODEFILES\\",fil,".sas",sep="")
> sasname<-paste(x1,fil,"'",sep="")
> write.foreign(dat,path1,path2,package="SAS",dataname=sasname)
> blpDisconnect(con)
> }
>
> for(j in 1:lenf)
> {
> fname<-paste("D:\\SAS\\Project2\\Daily\\",filename[j],"_root.sas7bdat",s
> ep="")
> Sys.sleep(600)
> if(!file.exists(fname)) download(fil=filename[j])
> }
>
> And lent=58 and lenf=8... 8 text files will be generated in this process
> if the program would have run properly, and each would be of size 4,000
> KB.
>
> The error message I get if the program is not run is:
>
> Error in dimnames(x) <- dn : length of 'dimnames' [2] not equal to array
> extent
> In addition: Warning message:
> Index vectors are of different classes: chron chron dates in:
> merge(d_mer,cdaily)
>
>
>
> Please could any one help on this?
>
> -----Original Message-----
> From: Sarah Goslee [mailto:sarah.goslee at gmail.com]
> Sent: Tuesday, February 13, 2007 7:09 PM
> To: Shubha Vishwanath Karanth
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Fatigued R
>
> Hi Shubha,
>
> Perhaps you haven't gotten any help because you haven't provided a
> reproducible example, or even told us what you are trying to do
> (specifically)
> or what errors you are receiving. Frankly, your problem statement
> doesn't
> make any sense to me, and I can't provide advice without more
> information.
>
> As the footer of every email says:
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> Sarah
>
> On 2/13/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> > Hi R,
> >
> >
> >
> > Please solve my problem...........
> >
> >
> >
> > I am extracting Bloomberg data from R, in a loop. R is getting
> fatigued
> > by doing this process and gives some errors. I introduced sleep
> > function. Doing this sometimes I get the results and sometimes not. I
> > even noticed that if I give complete rest for R (don't open R window)
> > for 1 day and then run my code with the sleep function, then the
> program
> > works. But if I keep on doing this with on R, repeatedly I get errors.
> >
> >
> >
> > Please can anyone do something for this? Is there any function of
> > refreshing R completely.....Or any other techniques?...I am really
> > getting bugged with this.......
> >
> >
> >
> > Thanks,
> >
> > Shubha
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From rahmad at gwdg.de  Tue Feb 13 15:14:18 2007
From: rahmad at gwdg.de (Rauf Ahmad)
Date: Tue, 13 Feb 2007 15:14:18 +0100
Subject: [R] Generating MVN Data
Message-ID: <45D1C7BA.1020402@gwdg.de>

Dear All

I want to generate multivariate normal data in R for a given covariance 
matrix, i.e. my generated data must have the given covariance matrix. I 
know the rmvnorm command is to be used but may be I am failing to 
properly assign the covariance matrix.

Any help will be greatly appreciated

thanks.

M. R. Ahmad


From Thierry.ONKELINX at inbo.be  Tue Feb 13 15:25:55 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 13 Feb 2007 15:25:55 +0100
Subject: [R] Fatigued R
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3FEF360@BAN-MAILSRV03.Amba.com>
Message-ID: <2E9C414912813E4EB981326983E0A1040293DE83@inboexch.inbo.be>

Dear Shubha,

The error message tells you that the error occurs in the line:
	merge(d_mer,cdaily)
And the problem is that d_mer and cdaily have a different class. It
looks as you need to convert cdaily to the correct class (same class as
d_mer).
Don't forget to use traceback() when debugging your program.

You code could use some tweaking. Don't be afraid to add spaces and
indentation. That will make you code a lot more readable.
I noticed that you have defined some variables within your function
(lent, t). This might lead to strange behaviour of your function, as it
will use the global variables. Giving a variable the same name as a
function (t) is confusing. t[2] and t(2) look very similar but do
something very different.
Sometimes it pays off to covert for-loop in apply-type functions.

Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Shubha Vishwanath
Karanth
Verzonden: dinsdag 13 februari 2007 14:51
Aan: Sarah Goslee; r-help op stat.math.ethz.ch
Onderwerp: Re: [R] Fatigued R

Ohkkk....I will try to do that now....

This is my download function...


download<-function(fil)
{
con<-blpConnect(show.days=show_day, na.action=na_action,
periodicity=periodicity)
for(i in 1:lent)
{
Sys.sleep(3)
cdaily<-blpGetData(con,unique(t[,i]),fil,start=as.chron(as.Date("1/1/199
6", "%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))
#Sys.sleep(3)
if(!exists("d_mer")) d_mer=cdaily else d_mer=merge(d_mer,cdaily)
rm(cdaily)
}
dat<-data.frame(Date=index(d_mer),d_mer)
dat$ABCDEFG=NULL
path1<-paste("D:\\SAS\\Project2\\Daily\\",fil,"_root.sas7bdat",sep="")
path2<-paste("D:\\SAS\\Project2\\Daily\\CODEFILES\\",fil,".sas",sep="")
sasname<-paste(x1,fil,"'",sep="")
write.foreign(dat,path1,path2,package="SAS",dataname=sasname)
blpDisconnect(con)
}

for(j in 1:lenf)
{
fname<-paste("D:\\SAS\\Project2\\Daily\\",filename[j],"_root.sas7bdat",s
ep="")
Sys.sleep(600)
if(!file.exists(fname)) download(fil=filename[j])
}

And lent=58 and lenf=8... 8 text files will be generated in this process
if the program would have run properly, and each would be of size 4,000
KB.

The error message I get if the program is not run is:

Error in dimnames(x) <- dn : length of 'dimnames' [2] not equal to array
extent
In addition: Warning message:
Index vectors are of different classes: chron chron dates in:
merge(d_mer,cdaily)



Please could any one help on this?

-----Original Message-----
From: Sarah Goslee [mailto:sarah.goslee op gmail.com] 
Sent: Tuesday, February 13, 2007 7:09 PM
To: Shubha Vishwanath Karanth
Cc: r-help op stat.math.ethz.ch
Subject: Re: [R] Fatigued R

Hi Shubha,

Perhaps you haven't gotten any help because you haven't provided a
reproducible example, or even told us what you are trying to do
(specifically)
or what errors you are receiving. Frankly, your problem statement
doesn't
make any sense to me, and I can't provide advice without more
information.

As the footer of every email says:
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Sarah

On 2/13/07, Shubha Vishwanath Karanth <shubhak op ambaresearch.com> wrote:
> Hi R,
>
>
>
> Please solve my problem...........
>
>
>
> I am extracting Bloomberg data from R, in a loop. R is getting
fatigued
> by doing this process and gives some errors. I introduced sleep
> function. Doing this sometimes I get the results and sometimes not. I
> even noticed that if I give complete rest for R (don't open R window)
> for 1 day and then run my code with the sleep function, then the
program
> works. But if I keep on doing this with on R, repeatedly I get errors.
>
>
>
> Please can anyone do something for this? Is there any function of
> refreshing R completely.....Or any other techniques?...I am really
> getting bugged with this.......
>
>
>
> Thanks,
>
> Shubha


-- 
Sarah Goslee
http://www.functionaldiversity.org

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Tue Feb 13 15:27:07 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Feb 2007 09:27:07 -0500
Subject: [R] Polygon triangulation?
Message-ID: <45D1CABB.7000907@stats.uwo.ca>

Can anyone point me to a package that contains code to triangulate a 
polygon?  This is easy if the polygon is convex, but tricky if not.  One 
algorithm to do it is due to Meister, and is described here:

www.math.gatech.edu/~randall/AlgsF06/planartri.pdf

Duncan Murdoch


From ccleland at optonline.net  Tue Feb 13 15:35:40 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 13 Feb 2007 09:35:40 -0500
Subject: [R] Generating MVN Data
In-Reply-To: <45D1C7BA.1020402@gwdg.de>
References: <45D1C7BA.1020402@gwdg.de>
Message-ID: <45D1CCBC.2010007@optonline.net>

Rauf Ahmad wrote:
> Dear All
> 
> I want to generate multivariate normal data in R for a given covariance 
> matrix, i.e. my generated data must have the given covariance matrix. I 
> know the rmvnorm command is to be used but may be I am failing to 
> properly assign the covariance matrix.
> 
> Any help will be greatly appreciated

library(MASS)

mat <- mvrnorm(100, mu=rep(0,4), Sigma = diag(4), empirical=TRUE)

cov(mat)
              [,1]          [,2]          [,3]          [,4]
[1,]  1.000000e+00 -1.634448e-16 -1.004223e-16 -2.015521e-16
[2,] -1.634448e-16  1.000000e+00  4.244391e-17  1.544399e-17
[3,] -1.004223e-16  4.244391e-17  1.000000e+00 -1.921951e-16
[4,] -2.015521e-16  1.544399e-17 -1.921951e-16  1.000000e+00

?mvrnorm

> thanks.
> 
> M. R. Ahmad
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From r.hankin at noc.soton.ac.uk  Tue Feb 13 15:39:31 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 13 Feb 2007 14:39:31 +0000
Subject: [R] Generating MVN Data
In-Reply-To: <45D1C7BA.1020402@gwdg.de>
References: <45D1C7BA.1020402@gwdg.de>
Message-ID: <7EDBBB79-08F5-41E3-AAAD-17770BBF999E@soc.soton.ac.uk>

Hi

give rmvnorm() any symmetric positive definite matrix and it should  
work:



 > rmvnorm(n=10,mean=1:2,sigma=matrix(c(1,0.5,0.5,1),2,2))
          [,1]   [,2]
[1,] -0.1118  2.514
[2,]  1.8667  1.628
[3,]  3.2477  2.263
[4,]  1.0166  2.381
[5,] -0.0888 -0.132
[6,] -0.9249  0.610
[7,]  1.5046  3.578
[8,]  0.8530  0.802
[9,]  2.2940  2.240
[10,]  1.1660  2.528
 >



HTH

rksh


On 13 Feb 2007, at 14:14, Rauf Ahmad wrote:

> Dear All
>
> I want to generate multivariate normal data in R for a given  
> covariance
> matrix, i.e. my generated data must have the given covariance  
> matrix. I
> know the rmvnorm command is to be used but may be I am failing to
> properly assign the covariance matrix.
>
> Any help will be greatly appreciated
>
> thanks.
>
> M. R. Ahmad
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From dimitris.rizopoulos at med.kuleuven.be  Tue Feb 13 15:40:25 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 13 Feb 2007 15:40:25 +0100
Subject: [R] Generating MVN Data
References: <45D1C7BA.1020402@gwdg.de>
Message-ID: <008e01c74f7c$e5f9f090$0540210a@www.domain>

you probably want to use mvrnorm() from package MASS, e.g.,

library(MASS)
mu <- c(-3, 0, 3)
Sigma <- rbind(c(5,3,2), c(3,4,1), c(2,1,3))
x <- mvrnorm(1000, mu, Sigma, empirical = TRUE)

colMeans(x)
var(x)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rauf Ahmad" <rahmad at gwdg.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, February 13, 2007 3:14 PM
Subject: [R] Generating MVN Data


> Dear All
>
> I want to generate multivariate normal data in R for a given 
> covariance
> matrix, i.e. my generated data must have the given covariance 
> matrix. I
> know the rmvnorm command is to be used but may be I am failing to
> properly assign the covariance matrix.
>
> Any help will be greatly appreciated
>
> thanks.
>
> M. R. Ahmad
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Thierry.ONKELINX at inbo.be  Tue Feb 13 15:43:20 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 13 Feb 2007 15:43:20 +0100
Subject: [R] Polygon triangulation?
In-Reply-To: <45D1CABB.7000907@stats.uwo.ca>
Message-ID: <2E9C414912813E4EB981326983E0A1040293DEA8@inboexch.inbo.be>

Have you tried the tri-package?

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 
 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Duncan Murdoch
Verzonden: dinsdag 13 februari 2007 15:27
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Polygon triangulation?

Can anyone point me to a package that contains code to triangulate a 
polygon?  This is easy if the polygon is convex, but tricky if not.  One

algorithm to do it is due to Meister, and is described here:

www.math.gatech.edu/~randall/AlgsF06/planartri.pdf

Duncan Murdoch

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Bernhard_Pfaff at fra.invesco.com  Tue Feb 13 15:50:32 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 13 Feb 2007 14:50:32 -0000
Subject: [R] lag orders with ADF.test
In-Reply-To: <1696621856.231241171360512915.JavaMail.nobody@mail12.abv.bg>
References: <1696621856.231241171360512915.JavaMail.nobody@mail12.abv.bg>
Message-ID: <E4A9111DA23BA048B9A46686BF727CF461BF3F@DEFRAXMB01.corp.amvescap.net>

>Hello!
> I do not understand what is meant by:
>
> "aic" and "bic" follow a top-down strategy based on the
>Akaike's and Schwarz's information criteria
>
>in the datails to the ADF.test function. What does a "top-down
>strategy" mean? Probably the respective criterion is minimized

Hello Martin,

are you referring to ADF.test() in package "uroot"? If so, it would be good to adress this question to the package maintainer first, which I have cc'ed.
Different approaches for determining an appropriate lag length for ADF tests are propagated in the literature. One of them is the usage of information criteria; a second one starting from a high lag number and cutting the lag length down by signifcance (top to bottom) or vice versa (bottom to top); or finally inspect the ACF/PACF's of the residuals in the ADF-test regression and choose the lowest possible lag length such that the residuals are uncorrelated.

Best,
Bernhard


>and the mode vector contains the lag orders at which the
>criterion attains it local minima? When the calculation is
>over, the ADF.test function gives info about "Lag orders".
>What are these lag orders? Are they the local minima of the criterion?
>
> I will be very thankful if you clarify this to me. I browsed
>a lot, but I could not find a clear answer.
>
> Thank you for your attention.
> Regards,
> Martin
>
>-----------------------------------------------------------------
>http://auto-motor-und-sport.bg/
>? ?????? ? ??????!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
> 
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From ripley at stats.ox.ac.uk  Tue Feb 13 15:54:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Feb 2007 14:54:07 +0000 (GMT)
Subject: [R] Generating MVN Data
In-Reply-To: <7EDBBB79-08F5-41E3-AAAD-17770BBF999E@soc.soton.ac.uk>
References: <45D1C7BA.1020402@gwdg.de>
	<7EDBBB79-08F5-41E3-AAAD-17770BBF999E@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.64.0702131449220.29950@gannet.stats.ox.ac.uk>

'should work', yes.
Do what he asked for (in any reasonable reading), no.

> set.seed(1)
> library(mvtnorm)   ## you both omitted to mention that
> X <-  rmvnorm(n=10,mean=1:2,sigma=matrix(c(1,0.5,0.5,1),2,2))
> var(X)
           [,1]      [,2]
[1,] 0.4878773 0.1238040
[2,] 0.1238040 0.9508090

Fortunately there is a way to do it, and without even adding unstated 
packages to your R installation:

> library(MASS)
> set.seed(1)
> X <-  mvrnorm(n=10, mu=1:2, Sigma=matrix(c(1,0.5,0.5,1),2,2), emp=TRUE)
> var(X)
      [,1] [,2]
[1,]  1.0  0.5
[2,]  0.5  1.0


On Tue, 13 Feb 2007, Robin Hankin wrote:

> Hi
>
> give rmvnorm() any symmetric positive definite matrix and it should
> work:
>
>
>
> > rmvnorm(n=10,mean=1:2,sigma=matrix(c(1,0.5,0.5,1),2,2))
>          [,1]   [,2]
> [1,] -0.1118  2.514
> [2,]  1.8667  1.628
> [3,]  3.2477  2.263
> [4,]  1.0166  2.381
> [5,] -0.0888 -0.132
> [6,] -0.9249  0.610
> [7,]  1.5046  3.578
> [8,]  0.8530  0.802
> [9,]  2.2940  2.240
> [10,]  1.1660  2.528
> >
>
>
>
> HTH
>
> rksh
>
>
> On 13 Feb 2007, at 14:14, Rauf Ahmad wrote:
>
>> Dear All
>>
>> I want to generate multivariate normal data in R for a given
>> covariance
>> matrix, i.e. my generated data must have the given covariance
>> matrix. I
>> know the rmvnorm command is to be used but may be I am failing to
>> properly assign the covariance matrix.
>>
>> Any help will be greatly appreciated
>>
>> thanks.
>>
>> M. R. Ahmad
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Tue Feb 13 15:54:15 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Feb 2007 09:54:15 -0500
Subject: [R] Polygon triangulation?
In-Reply-To: <2E9C414912813E4EB981326983E0A1040293DEA8@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A1040293DEA8@inboexch.inbo.be>
Message-ID: <45D1D117.8080507@stats.uwo.ca>

On 2/13/2007 9:43 AM, ONKELINX, Thierry wrote:
> Have you tried the tri-package?

You mean tripack?

I looked and saw Delaunay triangulation (triangulating points), but 
triangulating polygons is a different (unrelated?) problem.

Duncan Murdoch

> 
> Cheers,
> 
> Thierry
> 
> ------------------------------------------------------------------------
> ----
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be 
>  
> 
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
> 
> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens Duncan Murdoch
> Verzonden: dinsdag 13 februari 2007 15:27
> Aan: r-help at stat.math.ethz.ch
> Onderwerp: [R] Polygon triangulation?
> 
> Can anyone point me to a package that contains code to triangulate a 
> polygon?  This is easy if the polygon is convex, but tricky if not.  One
> 
> algorithm to do it is due to Meister, and is described here:
> 
> www.math.gatech.edu/~randall/AlgsF06/planartri.pdf
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Tue Feb 13 15:57:18 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Feb 2007 09:57:18 -0500
Subject: [R] Suddenly "Subscript out of bounds"
In-Reply-To: <OFF4024685.69E614DC-ONC1257281.004BF8F0-C1257281.004E5423@basf-c-s.be>
References: <OFF4024685.69E614DC-ONC1257281.004BF8F0-C1257281.004E5423@basf-c-s.be>
Message-ID: <45D1D1CE.5080002@stats.uwo.ca>

On 2/13/2007 9:15 AM, roderick.castillo at metanomics.de wrote:
> Hello
> 
> Using R Version 2.3.1 I have setup a cronjob to update packages,
> which worked successfully almost a year (it was called daily).
> Basically, it runs like this:
> 
> Sys.getenv("http_proxy")
> update.packages(ask=F,repos="http://cran.r-project.org")
> 
> (the http_proxy environment variable is set prior to the call).
> 
> All of a sudden, I started to get this error:
> 
> Error in inherits(x,  "factor") :  subscript out of bounds
> 
> I have no clue about what this means. Is "factor" a buggy package?
> 
> Clearly, that kind of things can happen when you just update things
> automatically...

The error message says that there's an error in the call to inherits() 
with the given args, but it doesn't say why that was being called.
Use traceback() to see where this error happens.

And please update to R 2.4.1; R 2.3.1 is out of date.

Duncan Murdoch


From rjohnson at ncifcrf.gov  Tue Feb 13 15:58:59 2007
From: rjohnson at ncifcrf.gov (Randall C Johnson [Contr.])
Date: Tue, 13 Feb 2007 09:58:59 -0500
Subject: [R] Width of a plotting point (in inches) in grid package
In-Reply-To: <45D0BF8A.50605@stat.auckland.ac.nz>
Message-ID: <C1F73C63.10635%rjohnson@ncifcrf.gov>

Hello,
I was thinking that was probably the case. I'm creating a series of graphics
that contain smaller graphics, and was trying to reduce the bounding box as
much as possible with out the plotting points bleeding over to the
surrounding features. I could hard code the size of the bounding boxes (or
use a creative calculation), but that would be tedious, not to mention what
would happen if someone decides they would like to change the plotting point
or it's size. ;) The number of predefined types of points available in a
pointsGrob is handy, but I also like the idea of using circles or
polygons...

I might also add that I've just started using the grid graphics system in
the last few months, and I'm impressed at how powerful and flexible it is.

Thanks!
Randy


On 2/12/07 2:27 PM, "Paul Murrell" <p.murrell at auckland.ac.nz> wrote:

> Hi
> 
> 
> Randall C Johnson [Contr.] wrote:
>> Hello,
>> I'm trying to determine the width of a plotting point (in inches) in the
>> grid package. I naively thought I could create a pointsGrob with only one
>> point and get the width (as tried below), but this results in an object with
>> a size of 0inches (changing cex has no effect). Does anyone have a better
>> approach? Of course, it would be dependent upon the graphics parameters and
>> viewport...
> 
> 
> The width of a pointsGrob is based on a bounding box surrounding all of
> the (x, y) locations at which the points are located.  It takes no
> notice of the size of the symbol drawn at the locations.  With one
> point, the bounding box has zero size.  The wisdom of this design could
> be debated ...
> 
> What do you need the symbol width for?  Could you use a circle,
> rectangle, or polygon instead (all of which calculate their width based
> on the bounding box of the shape that is drawn)?
> 
> Paul
> 
> 
>> Thanks,
>> Randy
>> 
>>> library(grid)
>> 
>>> pushViewport(viewport())
>> 
>>> convertX(grobWidth(pointsGrob(1, 1)), 'inches')
>> [1] 0inches
>> 
>> # I think we're measuring the size of the point here...
>> # changing cex has no effect.
>>> convertX(grobWidth(pointsGrob(1, 1, gp = gpar(cex = 3))), 'inches')
>> [1] 0inches
>> 
>> # If I add a second point, the size should increase...
>> # how big is the plotting point though???
>>> convertX(grobWidth(pointsGrob(1:2, 1:2)), 'inches')
>> [1] 11.1929133858268inches
>> 
>>> sessionInfo()
>> R version 2.4.1 (2006-12-18)
>> i386-apple-darwin8.8.2
>> 
>> locale:
>> C
>> 
>> attached base packages:
>> [1] "grid"      "stats"     "graphics"  "grDevices" "utils"     "datasets"
>> [7] "methods"   "base"
>> 
>> 
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> Randall C Johnson
>> Bioinformatics Analyst
>> SAIC-Frederick, Inc (Contractor)
>> Laboratory of Genomic Diversity
>> NCI-Frederick, P.O. Box B
>> Bldg 560, Rm 11-85
>> Frederick, MD 21702
>> Phone: (301) 846-1304
>> Fax: (301) 846-1686
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Randall C Johnson
Bioinformatics Analyst
SAIC-Frederick, Inc (Contractor)
Laboratory of Genomic Diversity
NCI-Frederick, P.O. Box B
Bldg 560, Rm 11-85
Frederick, MD 21702
Phone: (301) 846-1304
Fax: (301) 846-1686
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From shubhak at ambaresearch.com  Tue Feb 13 16:05:33 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Tue, 13 Feb 2007 20:35:33 +0530
Subject: [R] Fatigued R
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3FEF42C@BAN-MAILSRV03.Amba.com>

Hi all, thanks for your reply....

But I have to make one thing clear that there are no errors in
programming...I assure that to you, because I have extracted the data
many times from the same program...

The problem is with the connection of R with Bloomberg, sometimes the
data is not fetched at all and so I get the below errors...It is mainly
due to some network jam problems and all...

Could anyone suggest me how to refresh R, or how to always make sure
that the data is downloaded without any errors for each loop? I tried
with Sys.sleep() to give some free time to R...but it is not successful
always...

Could anybody help me out?

Thank you,
Shubha

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be] 
Sent: Tuesday, February 13, 2007 7:56 PM
To: Shubha Vishwanath Karanth; r-help at stat.math.ethz.ch
Subject: RE: [R] Fatigued R

Dear Shubha,

The error message tells you that the error occurs in the line:
	merge(d_mer,cdaily)
And the problem is that d_mer and cdaily have a different class. It
looks as you need to convert cdaily to the correct class (same class as
d_mer).
Don't forget to use traceback() when debugging your program.

You code could use some tweaking. Don't be afraid to add spaces and
indentation. That will make you code a lot more readable.
I noticed that you have defined some variables within your function
(lent, t). This might lead to strange behaviour of your function, as it
will use the global variables. Giving a variable the same name as a
function (t) is confusing. t[2] and t(2) look very similar but do
something very different.
Sometimes it pays off to covert for-loop in apply-type functions.

Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx at inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] Namens Shubha Vishwanath
Karanth
Verzonden: dinsdag 13 februari 2007 14:51
Aan: Sarah Goslee; r-help at stat.math.ethz.ch
Onderwerp: Re: [R] Fatigued R

Ohkkk....I will try to do that now....

This is my download function...


download<-function(fil)
{
con<-blpConnect(show.days=show_day, na.action=na_action,
periodicity=periodicity)
for(i in 1:lent)
{
Sys.sleep(3)
cdaily<-blpGetData(con,unique(t[,i]),fil,start=as.chron(as.Date("1/1/199
6", "%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))
#Sys.sleep(3)
if(!exists("d_mer")) d_mer=cdaily else d_mer=merge(d_mer,cdaily)
rm(cdaily)
}
dat<-data.frame(Date=index(d_mer),d_mer)
dat$ABCDEFG=NULL
path1<-paste("D:\\SAS\\Project2\\Daily\\",fil,"_root.sas7bdat",sep="")
path2<-paste("D:\\SAS\\Project2\\Daily\\CODEFILES\\",fil,".sas",sep="")
sasname<-paste(x1,fil,"'",sep="")
write.foreign(dat,path1,path2,package="SAS",dataname=sasname)
blpDisconnect(con)
}

for(j in 1:lenf)
{
fname<-paste("D:\\SAS\\Project2\\Daily\\",filename[j],"_root.sas7bdat",s
ep="")
Sys.sleep(600)
if(!file.exists(fname)) download(fil=filename[j])
}

And lent=58 and lenf=8... 8 text files will be generated in this process
if the program would have run properly, and each would be of size 4,000
KB.

The error message I get if the program is not run is:

Error in dimnames(x) <- dn : length of 'dimnames' [2] not equal to array
extent
In addition: Warning message:
Index vectors are of different classes: chron chron dates in:
merge(d_mer,cdaily)



Please could any one help on this?

-----Original Message-----
From: Sarah Goslee [mailto:sarah.goslee at gmail.com] 
Sent: Tuesday, February 13, 2007 7:09 PM
To: Shubha Vishwanath Karanth
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Fatigued R

Hi Shubha,

Perhaps you haven't gotten any help because you haven't provided a
reproducible example, or even told us what you are trying to do
(specifically)
or what errors you are receiving. Frankly, your problem statement
doesn't
make any sense to me, and I can't provide advice without more
information.

As the footer of every email says:
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Sarah

On 2/13/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> Hi R,
>
>
>
> Please solve my problem...........
>
>
>
> I am extracting Bloomberg data from R, in a loop. R is getting
fatigued
> by doing this process and gives some errors. I introduced sleep
> function. Doing this sometimes I get the results and sometimes not. I
> even noticed that if I give complete rest for R (don't open R window)
> for 1 day and then run my code with the sleep function, then the
program
> works. But if I keep on doing this with on R, repeatedly I get errors.
>
>
>
> Please can anyone do something for this? Is there any function of
> refreshing R completely.....Or any other techniques?...I am really
> getting bugged with this.......
>
>
>
> Thanks,
>
> Shubha


-- 
Sarah Goslee
http://www.functionaldiversity.org

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Roger.Bivand at nhh.no  Tue Feb 13 16:10:48 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 13 Feb 2007 16:10:48 +0100 (CET)
Subject: [R] Polygon triangulation?
In-Reply-To: <2E9C414912813E4EB981326983E0A1040293DEA8@inboexch.inbo.be>
Message-ID: <Pine.LNX.4.44.0702131608310.6240-100000@reclus.nhh.no>

On Tue, 13 Feb 2007, ONKELINX, Thierry wrote:

> Have you tried the tri-package?

Perhaps the GPC C library used in the gpclib package, and in PBSmapping 
will get closer - it partitions polygons into tristrip sets.

> 
> Cheers,
> 
> Thierry
> 
> ------------------------------------------------------------------------
> ----
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be 
>  
> 
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
> 
> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens Duncan Murdoch
> Verzonden: dinsdag 13 februari 2007 15:27
> Aan: r-help at stat.math.ethz.ch
> Onderwerp: [R] Polygon triangulation?
> 
> Can anyone point me to a package that contains code to triangulate a 
> polygon?  This is easy if the polygon is convex, but tricky if not.  One
> 
> algorithm to do it is due to Meister, and is described here:
> 
> www.math.gatech.edu/~randall/AlgsF06/planartri.pdf
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From Thierry.ONKELINX at inbo.be  Tue Feb 13 16:27:40 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 13 Feb 2007 16:27:40 +0100
Subject: [R] Fatigued R
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3FEF42C@BAN-MAILSRV03.Amba.com>
Message-ID: <2E9C414912813E4EB981326983E0A1040293DEF6@inboexch.inbo.be>

Shubha,

You suggested the solution yourself: first make sure that the downloaded
data has no errors and reload it when it has errors. But that's
something you'll have to do yourself. Have a look at the data downloaded
with and without errors and try to see the difference. Again: it's
impossible to solve your problem without an example of the correct and
faulty data.

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: Shubha Vishwanath Karanth [mailto:shubhak op ambaresearch.com] 
Verzonden: dinsdag 13 februari 2007 16:06
Aan: ONKELINX, Thierry; r-help op stat.math.ethz.ch; sarah.goslee op gmail.com
Onderwerp: RE: [R] Fatigued R

Hi all, thanks for your reply....

But I have to make one thing clear that there are no errors in
programming...I assure that to you, because I have extracted the data
many times from the same program...

The problem is with the connection of R with Bloomberg, sometimes the
data is not fetched at all and so I get the below errors...It is mainly
due to some network jam problems and all...

Could anyone suggest me how to refresh R, or how to always make sure
that the data is downloaded without any errors for each loop? I tried
with Sys.sleep() to give some free time to R...but it is not successful
always...

Could anybody help me out?

Thank you,
Shubha

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX op inbo.be] 
Sent: Tuesday, February 13, 2007 7:56 PM
To: Shubha Vishwanath Karanth; r-help op stat.math.ethz.ch
Subject: RE: [R] Fatigued R

Dear Shubha,

The error message tells you that the error occurs in the line:
	merge(d_mer,cdaily)
And the problem is that d_mer and cdaily have a different class. It
looks as you need to convert cdaily to the correct class (same class as
d_mer).
Don't forget to use traceback() when debugging your program.

You code could use some tweaking. Don't be afraid to add spaces and
indentation. That will make you code a lot more readable.
I noticed that you have defined some variables within your function
(lent, t). This might lead to strange behaviour of your function, as it
will use the global variables. Giving a variable the same name as a
function (t) is confusing. t[2] and t(2) look very similar but do
something very different.
Sometimes it pays off to covert for-loop in apply-type functions.

Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Shubha Vishwanath
Karanth
Verzonden: dinsdag 13 februari 2007 14:51
Aan: Sarah Goslee; r-help op stat.math.ethz.ch
Onderwerp: Re: [R] Fatigued R

Ohkkk....I will try to do that now....

This is my download function...


download<-function(fil)
{
con<-blpConnect(show.days=show_day, na.action=na_action,
periodicity=periodicity)
for(i in 1:lent)
{
Sys.sleep(3)
cdaily<-blpGetData(con,unique(t[,i]),fil,start=as.chron(as.Date("1/1/199
6", "%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))
#Sys.sleep(3)
if(!exists("d_mer")) d_mer=cdaily else d_mer=merge(d_mer,cdaily)
rm(cdaily)
}
dat<-data.frame(Date=index(d_mer),d_mer)
dat$ABCDEFG=NULL
path1<-paste("D:\\SAS\\Project2\\Daily\\",fil,"_root.sas7bdat",sep="")
path2<-paste("D:\\SAS\\Project2\\Daily\\CODEFILES\\",fil,".sas",sep="")
sasname<-paste(x1,fil,"'",sep="")
write.foreign(dat,path1,path2,package="SAS",dataname=sasname)
blpDisconnect(con)
}

for(j in 1:lenf)
{
fname<-paste("D:\\SAS\\Project2\\Daily\\",filename[j],"_root.sas7bdat",s
ep="")
Sys.sleep(600)
if(!file.exists(fname)) download(fil=filename[j])
}

And lent=58 and lenf=8... 8 text files will be generated in this process
if the program would have run properly, and each would be of size 4,000
KB.

The error message I get if the program is not run is:

Error in dimnames(x) <- dn : length of 'dimnames' [2] not equal to array
extent
In addition: Warning message:
Index vectors are of different classes: chron chron dates in:
merge(d_mer,cdaily)



Please could any one help on this?

-----Original Message-----
From: Sarah Goslee [mailto:sarah.goslee op gmail.com] 
Sent: Tuesday, February 13, 2007 7:09 PM
To: Shubha Vishwanath Karanth
Cc: r-help op stat.math.ethz.ch
Subject: Re: [R] Fatigued R

Hi Shubha,

Perhaps you haven't gotten any help because you haven't provided a
reproducible example, or even told us what you are trying to do
(specifically)
or what errors you are receiving. Frankly, your problem statement
doesn't
make any sense to me, and I can't provide advice without more
information.

As the footer of every email says:
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Sarah

On 2/13/07, Shubha Vishwanath Karanth <shubhak op ambaresearch.com> wrote:
> Hi R,
>
>
>
> Please solve my problem...........
>
>
>
> I am extracting Bloomberg data from R, in a loop. R is getting
fatigued
> by doing this process and gives some errors. I introduced sleep
> function. Doing this sometimes I get the results and sometimes not. I
> even noticed that if I give complete rest for R (don't open R window)
> for 1 day and then run my code with the sleep function, then the
program
> works. But if I keep on doing this with on R, repeatedly I get errors.
>
>
>
> Please can anyone do something for this? Is there any function of
> refreshing R completely.....Or any other techniques?...I am really
> getting bugged with this.......
>
>
>
> Thanks,
>
> Shubha


-- 
Sarah Goslee
http://www.functionaldiversity.org

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From hb at stat.berkeley.edu  Tue Feb 13 17:02:46 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 13 Feb 2007 08:02:46 -0800
Subject: [R] help with tryCatch
In-Reply-To: <45D1CEB1.9080603@mast.queensu.ca>
References: <45D0DF70.1090801@mast.queensu.ca>
	<59d7961d0702121904n2104d327udd7c964db0048956@mail.gmail.com>
	<45D1CEB1.9080603@mast.queensu.ca>
Message-ID: <59d7961d0702130802t5cd28a91ia269f2bdbc085a0f@mail.gmail.com>

Hi,

google "R tryCatch example" and you'll find:

  http://www.maths.lth.se/help/R/ExceptionHandlingInR/

Hope this helps

Henrik

On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> Henrik,
>
> I had looked at tryCatch before posting the question and asked the
> question because the help file was not adequate for me. Could you pls
> provide a sample code of
> try{ try code}
> catch(error){catch code}
>
> let's say you have a vector of local file names and want to source them
> encapsulating in a tryCatch to avoid the skipping of all good file names
> after a bad file name.
>
> thank you
> stephen
>
>
> Henrik Bengtsson wrote:
>
> > See ?tryCatch. /Henrik
> >
> > On 2/12/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> >
> >> Could smb please help with try-catch encapsulating a function for
> >> downloading. Let's say I have a character vector of symbols and want to
> >> download each one and surround by try and catch to be safe
> >>
> >> # get.hist.quote() is in library(tseries), but the question does not
> >> depend on it, I could be sourcing local files instead
> >>
> >> ans=null;error=null;
> >> for ( sym in sym.vec){
> >> try(ans=cbind(ans,get.hist.quote(sym,start=start))) #accumulate in a zoo
> >> matrix
> >> catch(theurlerror){error=c(error,sym)} #accumulate failed symbols
> >> }
> >>
> >> I know the code above does not work, but it conveys the idea. tryCatch
> >> help page says it is similar to Java try-catch, but I know how to do a
> >> try-catch in Java and still can't do it in R.
> >>
> >> Thank you very much.
> >> stephen
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
>


From plynchnlm at gmail.com  Tue Feb 13 17:16:32 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Tue, 13 Feb 2007 11:16:32 -0500
Subject: [R] make check failure, internet.Rout.fail, Error in strsplit
In-Reply-To: <7B594544-8B7F-4BE5-80AA-D86313845CF1@hanover.edu>
References: <50d6c72a0702121528s6b9a3d56q682a65d1b4f2d11e@mail.gmail.com>
	<7B594544-8B7F-4BE5-80AA-D86313845CF1@hanover.edu>
Message-ID: <50d6c72a0702130816r5a5989a3i5456296e9d7dd21b@mail.gmail.com>

Thanks for giving it a try.  It is very odd that you got
"Content-Length" when I am getting "Content-length".  I just tried
curl (I had been using telnet to port 80) and I got the same (error
causing) "length" result:

> curl --head http://www.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat
HTTP/1.1 200 OK
Date: Tue, 13 Feb 2007 16:10:41 GMT
Server: Apache/2.0.40 (Red Hat Linux)
Last-Modified: Fri, 19 May 1995 10:27:04 GMT
ETag: "7bc27-836-39a78e00"
Accept-Ranges: bytes
Content-Type: text/plain; charset=ISO-8859-1
Content-length: 2102
Connection: Keep-Alive

Perhaps we are hitting different web servers?  I ran nslookup on
www.stats.ox.ac.uk, and it appears to be an alias for
web2.stats.ox.ac.uk.  Is that the machine you are getting?  What
happens if you run curl against web2, i.e.:
   curl --head http://web2.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat

?
(I get "Content-length").
Thanks,
     --Paul

On 2/12/07, Charilaos Skiadas <skiadas at hanover.edu> wrote:
> On Feb 12, 2007, at 6:28 PM, Paul Lynch wrote:
>
> > I'm trying to build R on RedHat EL4.  The compile went fine, but a
> > make check ran into a problem and produced a file
> > "internet.Rout.fail".  Judging by the last part of that file, it was
> > trying to run an R routine called "httpget" to retrieve the URL
> > http://www.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat.  The precise
> > error it encountered was:
> >
> > Error in strsplit(grep("Content-Length", b, value = TRUE), ":")[[1]] :
> >         subscript out of bounds
> >
> > So, it looks like the data it read from that URL was not what was
> > expected.  I tried mimicking the script's request of the header
> > information for that URL, and got back the following header lines:
> >
> > HTTP/1.1 200 OK
> > Date: Mon, 12 Feb 2007 23:22:06 GMT
> > Server: Apache/2.0.40 (Red Hat Linux)
> > Last-Modified: Fri, 19 May 1995 10:27:04 GMT
> > ETag: "7bc27-836-39a78e00"
> > Accept-Ranges: bytes
> > Content-Type: text/plain; charset=ISO-8859-1
> > Content-length: 2102
> > Connection: Keep-Alive
> >
> > The script appears to be looking for a "Content-Length" field, but as
> > you can see the returned header is "Content-length" with a lower-case
> > l.  I don't know R yet, so I'm not sure if the grep in the test code
> > is case-sensitive or not, but if it is, that would seem to be the
> > problem.  But then, surely everyone would be hitting this error?
>
> The grep is indeed case sensitive, as a quick test can show. However,
> the header I got back when I tried the above address had Length in it:
> HTTP/1.1 200 OK
> Date: Tue, 13 Feb 2007 01:40:48 GMT
> Server: Apache/2.0.40 (Red Hat Linux)
> Last-Modified: Fri, 19 May 1995 10:27:04 GMT
> ETag: "7bc27-836-39a78e00"
> Accept-Ranges: bytes
> Content-Length: 2102
> Content-Type: text/plain; charset=ISO-8859-1
> X-Pad: avoid browser bug
>
> ( I used curl for this, if it makes a difference)
>
> Hope this helps in some way.
>
> >       --Paul
>
> Haris
>
>
>


From Romain.Mayor at ville-ge.ch  Tue Feb 13 17:16:41 2007
From: Romain.Mayor at ville-ge.ch (Romain.Mayor at ville-ge.ch)
Date: Tue, 13 Feb 2007 17:16:41 +0100
Subject: [R] Hierarchical ANOVA
Message-ID: <OF3681548B.0105C3F6-ONC1257281.0055E4DB-C1257281.00596B29@ville-ge.ch>


Hello,

Does somebody could help me in the computation(formulation) of a
hierarchical ANOVA using linear model in R?
I'm working in a population biology study of an endangered species. My aim
is to see if I have effects of "Density of individuals/m2" on several
measured plants fitness traits.

As independent variables I have:

Humidity (continuous)
Nutritive substance (continuous)
LUX (continuous)
Density of individuals/m2 (continuous)
Population (categorical)
Year (categorical)

I want to test the 4 continous variables with Population as error term. And
the Population, the Year and interaction term, with the error of Population
x Year.

Thank you for any help, best regards.

Romain Mayor


From lassana.koita at aviation-civile.gouv.fr  Tue Feb 13 17:22:20 2007
From: lassana.koita at aviation-civile.gouv.fr (KOITA Lassana - STAC/ACE)
Date: Tue, 13 Feb 2007 17:22:20 +0100
Subject: [R] GEV by weighted moments method
Message-ID: <OFD2FD83D4.1E0D4646-ONC1257281.00582435@aviation-civile.gouv.fr>


Hi R-users,

Could anyone point me to package of GEV (not the POT package) including
function that estimates parameters by weighted moments method?

Best regrads


Lassana KOITA
Charg? d'Etudes de S?curit? et d'Exploitation a?roportuaires / Project
Engineer Airport Safety Studies & Statistical analysis
Service Technique de l'Aviation Civile (STAC) / Civil Aviation Technical
Department
Direction G?n?rale de l'Aviation Civile (DGAC) / French Civil Aviation
Authority
Tel: 01 49 56 80 60
Fax: 01 49 56 82 14
E-mail: Lassana.Koita at aviation-civile.gouv.fr
http://www.stac.aviation-civile.gouv.fr/


From anthony.brooks at csc.mrc.ac.uk  Tue Feb 13 17:31:55 2007
From: anthony.brooks at csc.mrc.ac.uk (Strongbad78)
Date: Tue, 13 Feb 2007 08:31:55 -0800 (PST)
Subject: [R] Advanced graphics for points
Message-ID: <8947660.post@talk.nabble.com>


Hi all,
For aestethic purposes, I would like to plot points on an xy plot that
consist of a filled circle with a black outline.
I can sort of bodge it by plotting pch=21, col="black" and pch=16, col="red"
on the same plot, but it looks a bit ugly and there are problems when points
overlap.
Can anyone think of a way around this?
Thanks in advance
Tony
-- 
View this message in context: http://www.nabble.com/Advanced-graphics-for-points-tf3221672.html#a8947660
Sent from the R help mailing list archive at Nabble.com.


From marc_schwartz at comcast.net  Tue Feb 13 17:40:43 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 13 Feb 2007 10:40:43 -0600
Subject: [R] Advanced graphics for points
In-Reply-To: <8947660.post@talk.nabble.com>
References: <8947660.post@talk.nabble.com>
Message-ID: <1171384843.4794.4.camel@Bellerophon>

On Tue, 2007-02-13 at 08:31 -0800, Strongbad78 wrote:
> Hi all,
> For aestethic purposes, I would like to plot points on an xy plot that
> consist of a filled circle with a black outline.
> I can sort of bodge it by plotting pch=21, col="black" and pch=16, col="red"
> on the same plot, but it looks a bit ugly and there are problems when points
> overlap.
> Can anyone think of a way around this?
> Thanks in advance
> Tony

Is this what you want?

  plot(rnorm(10), col = "black", bg = "red", pch = 21)

See the Details in ?points

HTH,

Marc Schwartz


From roderick.castillo at metanomics.de  Tue Feb 13 18:04:02 2007
From: roderick.castillo at metanomics.de (roderick.castillo at metanomics.de)
Date: Tue, 13 Feb 2007 18:04:02 +0100
Subject: [R] RE2:  Suddenly "Subscript out of bounds"
In-Reply-To: <2E9C414912813E4EB981326983E0A1040293DE8B@inboexch.inbo.be>
Message-ID: <OFE56393B0.71969749-ONC1257281.005BE2D1-C1257281.005DCD0E@basf-c-s.be>


If you tell me how to update R itself automatically, I will go for your
advice.
I am not aware of any method to do it...
Bye
Rick




                                                                           
             "ONKELINX,                                                    
             Thierry"                                                      
             <Thierry.ONKELINX                                          An 
             @inbo.be>                  <roderick.castillo at metanomics.de>  
                                                                     Kopie 
             13.02.2007 15:29                                              
                                                                     Thema 
                                        RE: [R] Suddenly "Subscript out of 
                                        bounds"                            
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




You update all packages but not R itself?

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx at inbo.be

www.inbo.be



Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] Namens
roderick.castillo at metanomics.de
Verzonden: dinsdag 13 februari 2007 15:15
Aan: r-help at stat.math.ethz.ch
Onderwerp: [R] Suddenly "Subscript out of bounds"


Hello

Using R Version 2.3.1 I have setup a cronjob to update packages,
which worked successfully almost a year (it was called daily).
Basically, it runs like this:

Sys.getenv("http_proxy")
update.packages(ask=F,repos="http://cran.r-project.org")

(the http_proxy environment variable is set prior to the call).

All of a sudden, I started to get this error:

Error in inherits(x,  "factor") :  subscript out of bounds

I have no clue about what this means. Is "factor" a buggy package?

Clearly, that kind of things can happen when you just update things
automatically...

Any help with be appreciated!

Bye
Rick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From br44114 at gmail.com  Tue Feb 13 17:48:45 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 13 Feb 2007 11:48:45 -0500
Subject: [R] Fatigued R
Message-ID: <8d5a36350702130848o1a8da7fdnfb45a2c1c2209826@mail.gmail.com>

The problem with your code is that it doesn't check for errors. See
?try, ?tryCatch. For example:

my.download <- function(forloop) {
  notok <- vector()
  for (i in forloop) {
    cdaily <- try(blpGetData(...))
    if (class(cdaily) == "try-error") {
      notok <- c(notok, i)
    } else {
      #proceed as usual
    }
  }
  notok
}

forloop <- 1:x
repeat {
  ddata <- my.download(forloop)
  forloop <- ddata
  if (length(forloop) == 0) break  #no download errors; stop
}


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shubha
> Vishwanath Karanth
> Sent: Tuesday, February 13, 2007 10:06 AM
> To: ONKELINX, Thierry; r-help at stat.math.ethz.ch;
> sarah.goslee at gmail.com
> Subject: Re: [R] Fatigued R
>
> Hi all, thanks for your reply....
>
> But I have to make one thing clear that there are no errors in
> programming...I assure that to you, because I have extracted the data
> many times from the same program...
>
> The problem is with the connection of R with Bloomberg, sometimes the
> data is not fetched at all and so I get the below errors...It
> is mainly
> due to some network jam problems and all...
>
> Could anyone suggest me how to refresh R, or how to always make sure
> that the data is downloaded without any errors for each loop? I tried
> with Sys.sleep() to give some free time to R...but it is not
> successful
> always...
>
> Could anybody help me out?
>
> Thank you,
> Shubha
>
> -----Original Message-----
> From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
> Sent: Tuesday, February 13, 2007 7:56 PM
> To: Shubha Vishwanath Karanth; r-help at stat.math.ethz.ch
> Subject: RE: [R] Fatigued R
>
> Dear Shubha,
>
> The error message tells you that the error occurs in the line:
> 	merge(d_mer,cdaily)
> And the problem is that d_mer and cdaily have a different class. It
> looks as you need to convert cdaily to the correct class
> (same class as
> d_mer).
> Don't forget to use traceback() when debugging your program.
>
> You code could use some tweaking. Don't be afraid to add spaces and
> indentation. That will make you code a lot more readable.
> I noticed that you have defined some variables within your function
> (lent, t). This might lead to strange behaviour of your
> function, as it
> will use the global variables. Giving a variable the same name as a
> function (t) is confusing. t[2] and t(2) look very similar but do
> something very different.
> Sometimes it pays off to covert for-loop in apply-type functions.
>
> Cheers,
>
> Thierry
> --------------------------------------------------------------
> ----------
> ----
>
> ir. Thierry Onkelinx
>
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
>
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
>
> Gaverstraat 4
>
> 9500 Geraardsbergen
>
> Belgium
>
> tel. + 32 54/436 185
>
> Thierry.Onkelinx at inbo.be
>
> www.inbo.be
>
>
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
>
> A statistical analysis, properly conducted, is a delicate
> dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
>
> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens Shubha Vishwanath
> Karanth
> Verzonden: dinsdag 13 februari 2007 14:51
> Aan: Sarah Goslee; r-help at stat.math.ethz.ch
> Onderwerp: Re: [R] Fatigued R
>
> Ohkkk....I will try to do that now....
>
> This is my download function...
>
>
> download<-function(fil)
> {
> con<-blpConnect(show.days=show_day, na.action=na_action,
> periodicity=periodicity)
> for(i in 1:lent)
> {
> Sys.sleep(3)
> cdaily<-blpGetData(con,unique(t[,i]),fil,start=as.chron(as.Dat
e("1/1/199
> 6", "%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))
> #Sys.sleep(3)
> if(!exists("d_mer")) d_mer=cdaily else d_mer=merge(d_mer,cdaily)
> rm(cdaily)
> }
> dat<-data.frame(Date=index(d_mer),d_mer)
> dat$ABCDEFG=NULL
> path1<-paste("D:\\SAS\\Project2\\Daily\\",fil,"_root.sas7bdat",sep="")
> path2<-paste("D:\\SAS\\Project2\\Daily\\CODEFILES\\",fil,".sas
> ",sep="")
> sasname<-paste(x1,fil,"'",sep="")
> write.foreign(dat,path1,path2,package="SAS",dataname=sasname)
> blpDisconnect(con)
> }
>
> for(j in 1:lenf)
> {
> fname<-paste("D:\\SAS\\Project2\\Daily\\",filename[j],"_root.s
as7bdat",s
> ep="")
> Sys.sleep(600)
> if(!file.exists(fname)) download(fil=filename[j])
> }
>
> And lent=58 and lenf=8... 8 text files will be generated in
> this process
> if the program would have run properly, and each would be of
> size 4,000
> KB.
>
> The error message I get if the program is not run is:
>
> Error in dimnames(x) <- dn : length of 'dimnames' [2] not
> equal to array
> extent
> In addition: Warning message:
> Index vectors are of different classes: chron chron dates in:
> merge(d_mer,cdaily)
>
>
>
> Please could any one help on this?
>
> -----Original Message-----
> From: Sarah Goslee [mailto:sarah.goslee at gmail.com]
> Sent: Tuesday, February 13, 2007 7:09 PM
> To: Shubha Vishwanath Karanth
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Fatigued R
>
> Hi Shubha,
>
> Perhaps you haven't gotten any help because you haven't provided a
> reproducible example, or even told us what you are trying to do
> (specifically)
> or what errors you are receiving. Frankly, your problem statement
> doesn't
> make any sense to me, and I can't provide advice without more
> information.
>
> As the footer of every email says:
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> Sarah
>
> On 2/13/07, Shubha Vishwanath Karanth
> <shubhak at ambaresearch.com> wrote:
> > Hi R,
> >
> >
> >
> > Please solve my problem...........
> >
> >
> >
> > I am extracting Bloomberg data from R, in a loop. R is getting
> fatigued
> > by doing this process and gives some errors. I introduced sleep
> > function. Doing this sometimes I get the results and
> sometimes not. I
> > even noticed that if I give complete rest for R (don't open
> R window)
> > for 1 day and then run my code with the sleep function, then the
> program
> > works. But if I keep on doing this with on R, repeatedly I
> get errors.
> >
> >
> >
> > Please can anyone do something for this? Is there any function of
> > refreshing R completely.....Or any other techniques?...I am really
> > getting bugged with this.......
> >
> >
> >
> > Thanks,
> >
> > Shubha
>
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From a.letertre at invs.sante.fr  Tue Feb 13 17:58:49 2007
From: a.letertre at invs.sante.fr (LE TERTRE Alain)
Date: Tue, 13 Feb 2007 17:58:49 +0100
Subject: [R] Missing variable in new dataframe for prediction
Message-ID: <A7EAEBFDDEDEF440A7BDD772C73046A20986C518@xch-vac.ivs.local>

Hi,
I'm using a loop to evaluate several models by taking adjacent variables from my dataframe.
When i try to get predictions for new values, i get an error message about a missing variable in my new dataframe.

Below is an example adapted from ?gam in mgcv package
library(mgcv)
set.seed(0) 
n<-400
sig<-2
x0 <- runif(n, 0, 1)
x1 <- runif(n, 0, 1)
x2 <- runif(n, 0, 1)
x3 <- runif(n, 0, 1)
f0 <- function(x) 2 * sin(pi * x)
f1 <- function(x) exp(2 * x)
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
f3 <- function(x) 0*x
f <- f0(x0) + f1(x1) + f2(x2)
e <- rnorm(n, 0, sig)
y <- f + e
Mydata<-data.frame(y=y,x0=x0,x1=x1,x2=x2,x3=x3)
remove(list=c("y","x0","x1","x2","x3"))

# Note below the syntax of the 3rd variable required for my loop
for (i in 4:5){
 b<-gam(y~s(x0)+ s(x1)+ ns(Mydata[,i], 3), data=Mydata)

newd <- data.frame(x0=(0:399)/30,x1=(0:399)/30,x2=(0:399)/30,x3=(0:399)/30)
pred <- predict.gam(b,newd)
}
Erreur dans model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
        type (list) incorrect pour la variable 'Mydata'
De plus : Warning message:
not all required variables have been supplied in  newdata!
 in: predict.gam(b, newd) 

#Defining the name for the variable as in the gam function doesn't solve the problem 
 newd <- data.frame(x0=(0:399)/30,x1=(0:399)/30,x2=(0:399)/30,"Mydata[,i]"=(0:399)/30)

Erreur dans model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
        type (list) incorrect pour la variable 'Mydata'
De plus : Warning message:
not all required variables have been supplied in  newdata!
 in: predict.gam(b, newd) 

How should i define my new dataset to be able to get my predictions ?

Thanks in advance


O__ ---- Alain Le Tertre
 c/ /'_ --- Institut de Veille Sanitaire (InVS)/ D?partement Sant? Environnement
(*) \(*) -- Responsable de l'unit? Syst?mes d'Information & Statistiques
~~~~~~~~~~ - 12 rue du val d'Osne 
94415 Saint Maurice cedex FRANCE
Voice: 33 1 41 79 68 76 Fax: 33 1 41 79 67 68
email: a.letertre at invs.sante.fr


From Thierry.ONKELINX at inbo.be  Tue Feb 13 18:07:52 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 13 Feb 2007 18:07:52 +0100
Subject: [R] RE2:  Suddenly "Subscript out of bounds"
In-Reply-To: <OFE56393B0.71969749-ONC1257281.005BE2D1-C1257281.005DCD0E@basf-c-s.be>
Message-ID: <2E9C414912813E4EB981326983E0A1040293DF59@inboexch.inbo.be>

Roderick,

I'm not suggesting that you need a script to update R automatically. I
don't think it is possible to do that. I just was wondering why you are
so eager to update all the packages daily, but still are working with an
outdated version of R.
Myself, I tend to check the R lists for new updates on R and it's
packages.

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: roderick.castillo op metanomics.de
[mailto:roderick.castillo op metanomics.de] 
Verzonden: dinsdag 13 februari 2007 18:04
Aan: ONKELINX, Thierry
CC: r-help op stat.math.ethz.ch
Onderwerp: RE2: [R] Suddenly "Subscript out of bounds"


If you tell me how to update R itself automatically, I will go for your
advice.
I am not aware of any method to do it...
Bye
Rick




 

             "ONKELINX,

             Thierry"

             <Thierry.ONKELINX
An 
             @inbo.be>
<roderick.castillo op metanomics.de>  
 
Kopie 
             13.02.2007 15:29

 
Thema 
                                        RE: [R] Suddenly "Subscript out
of 
                                        bounds"

 

 

 

 

 

 





You update all packages but not R itself?

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be



Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens
roderick.castillo op metanomics.de
Verzonden: dinsdag 13 februari 2007 15:15
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Suddenly "Subscript out of bounds"


Hello

Using R Version 2.3.1 I have setup a cronjob to update packages,
which worked successfully almost a year (it was called daily).
Basically, it runs like this:

Sys.getenv("http_proxy")
update.packages(ask=F,repos="http://cran.r-project.org")

(the http_proxy environment variable is set prior to the call).

All of a sudden, I started to get this error:

Error in inherits(x,  "factor") :  subscript out of bounds

I have no clue about what this means. Is "factor" a buggy package?

Clearly, that kind of things can happen when you just update things
automatically...

Any help with be appreciated!

Bye
Rick

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From xrysoflis at gmail.com  Tue Feb 13 18:19:34 2007
From: xrysoflis at gmail.com (Maria Vatapitakapha)
Date: Tue, 13 Feb 2007 17:19:34 +0000
Subject: [R] matlab style plotting in R
Message-ID: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070213/bc021e0a/attachment.pl 

From ggrothendieck at gmail.com  Tue Feb 13 18:31:27 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Feb 2007 12:31:27 -0500
Subject: [R] Missing variable in new dataframe for prediction
In-Reply-To: <A7EAEBFDDEDEF440A7BDD772C73046A20986C518@xch-vac.ivs.local>
References: <A7EAEBFDDEDEF440A7BDD772C73046A20986C518@xch-vac.ivs.local>
Message-ID: <971536df0702130931p21b5cabfo7c940f79e3dc8c9c@mail.gmail.com>

The call to library(splines) is missing and also try replacing the
line b <- ... with

 fo <- as.formula(sprintf("y ~ s(x0) + s(x1) + ns(%s, 3)", names(Mydata)[i]))
 b <- do.call("gam", list(fo, data = Mydata))

to dynamically recreate the formula on each iteration of the loop
with the correct name, x2 or x3, inserted.

On 2/13/07, LE TERTRE Alain <a.letertre at invs.sante.fr> wrote:
> Hi,
> I'm using a loop to evaluate several models by taking adjacent variables from my dataframe.
> When i try to get predictions for new values, i get an error message about a missing variable in my new dataframe.
>
> Below is an example adapted from ?gam in mgcv package
> library(mgcv)
> set.seed(0)
> n<-400
> sig<-2
> x0 <- runif(n, 0, 1)
> x1 <- runif(n, 0, 1)
> x2 <- runif(n, 0, 1)
> x3 <- runif(n, 0, 1)
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> f3 <- function(x) 0*x
> f <- f0(x0) + f1(x1) + f2(x2)
> e <- rnorm(n, 0, sig)
> y <- f + e
> Mydata<-data.frame(y=y,x0=x0,x1=x1,x2=x2,x3=x3)
> remove(list=c("y","x0","x1","x2","x3"))
>
> # Note below the syntax of the 3rd variable required for my loop
> for (i in 4:5){
>  b<-gam(y~s(x0)+ s(x1)+ ns(Mydata[,i], 3), data=Mydata)
>
> newd <- data.frame(x0=(0:399)/30,x1=(0:399)/30,x2=(0:399)/30,x3=(0:399)/30)
> pred <- predict.gam(b,newd)
> }
> Erreur dans model.frame(formula, rownames, variables, varnames, extras, extranames,  :
>        type (list) incorrect pour la variable 'Mydata'
> De plus : Warning message:
> not all required variables have been supplied in  newdata!
>  in: predict.gam(b, newd)
>
> #Defining the name for the variable as in the gam function doesn't solve the problem
>  newd <- data.frame(x0=(0:399)/30,x1=(0:399)/30,x2=(0:399)/30,"Mydata[,i]"=(0:399)/30)
>
> Erreur dans model.frame(formula, rownames, variables, varnames, extras, extranames,  :
>        type (list) incorrect pour la variable 'Mydata'
> De plus : Warning message:
> not all required variables have been supplied in  newdata!
>  in: predict.gam(b, newd)
>
> How should i define my new dataset to be able to get my predictions ?
>
> Thanks in advance
>
>
> O__ ---- Alain Le Tertre
>  c/ /'_ --- Institut de Veille Sanitaire (InVS)/ D?partement Sant? Environnement
> (*) \(*) -- Responsable de l'unit? Syst?mes d'Information & Statistiques
> ~~~~~~~~~~ - 12 rue du val d'Osne
> 94415 Saint Maurice cedex FRANCE
> Voice: 33 1 41 79 68 76 Fax: 33 1 41 79 67 68
> email: a.letertre at invs.sante.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Tue Feb 13 18:38:24 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Feb 2007 12:38:24 -0500
Subject: [R] Missing variable in new dataframe for prediction
In-Reply-To: <971536df0702130931p21b5cabfo7c940f79e3dc8c9c@mail.gmail.com>
References: <A7EAEBFDDEDEF440A7BDD772C73046A20986C518@xch-vac.ivs.local>
	<971536df0702130931p21b5cabfo7c940f79e3dc8c9c@mail.gmail.com>
Message-ID: <971536df0702130938j260a637dja32b1bd3ff8a2def@mail.gmail.com>

Actually this simpler replacement for the b <- ... line would work just as well:

fo <- as.formula(sprintf("y ~ s(x0) + s(x1) + ns(%s, 3)", names(Mydata)[i]))
b <- gam(fo, data = Mydata)


On 2/13/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The call to library(splines) is missing and also try replacing the
> line b <- ... with
>
>  fo <- as.formula(sprintf("y ~ s(x0) + s(x1) + ns(%s, 3)", names(Mydata)[i]))
>  b <- do.call("gam", list(fo, data = Mydata))
>
> to dynamically recreate the formula on each iteration of the loop
> with the correct name, x2 or x3, inserted.
>
> On 2/13/07, LE TERTRE Alain <a.letertre at invs.sante.fr> wrote:
> > Hi,
> > I'm using a loop to evaluate several models by taking adjacent variables from my dataframe.
> > When i try to get predictions for new values, i get an error message about a missing variable in my new dataframe.
> >
> > Below is an example adapted from ?gam in mgcv package
> > library(mgcv)
> > set.seed(0)
> > n<-400
> > sig<-2
> > x0 <- runif(n, 0, 1)
> > x1 <- runif(n, 0, 1)
> > x2 <- runif(n, 0, 1)
> > x3 <- runif(n, 0, 1)
> > f0 <- function(x) 2 * sin(pi * x)
> > f1 <- function(x) exp(2 * x)
> > f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> > f3 <- function(x) 0*x
> > f <- f0(x0) + f1(x1) + f2(x2)
> > e <- rnorm(n, 0, sig)
> > y <- f + e
> > Mydata<-data.frame(y=y,x0=x0,x1=x1,x2=x2,x3=x3)
> > remove(list=c("y","x0","x1","x2","x3"))
> >
> > # Note below the syntax of the 3rd variable required for my loop
> > for (i in 4:5){
> >  b<-gam(y~s(x0)+ s(x1)+ ns(Mydata[,i], 3), data=Mydata)
> >
> > newd <- data.frame(x0=(0:399)/30,x1=(0:399)/30,x2=(0:399)/30,x3=(0:399)/30)
> > pred <- predict.gam(b,newd)
> > }
> > Erreur dans model.frame(formula, rownames, variables, varnames, extras, extranames,  :
> >        type (list) incorrect pour la variable 'Mydata'
> > De plus : Warning message:
> > not all required variables have been supplied in  newdata!
> >  in: predict.gam(b, newd)
> >
> > #Defining the name for the variable as in the gam function doesn't solve the problem
> >  newd <- data.frame(x0=(0:399)/30,x1=(0:399)/30,x2=(0:399)/30,"Mydata[,i]"=(0:399)/30)
> >
> > Erreur dans model.frame(formula, rownames, variables, varnames, extras, extranames,  :
> >        type (list) incorrect pour la variable 'Mydata'
> > De plus : Warning message:
> > not all required variables have been supplied in  newdata!
> >  in: predict.gam(b, newd)
> >
> > How should i define my new dataset to be able to get my predictions ?
> >
> > Thanks in advance
> >
> >
> > O__ ---- Alain Le Tertre
> >  c/ /'_ --- Institut de Veille Sanitaire (InVS)/ D?partement Sant? Environnement
> > (*) \(*) -- Responsable de l'unit? Syst?mes d'Information & Statistiques
> > ~~~~~~~~~~ - 12 rue du val d'Osne
> > 94415 Saint Maurice cedex FRANCE
> > Voice: 33 1 41 79 68 76 Fax: 33 1 41 79 67 68
> > email: a.letertre at invs.sante.fr
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From rlevy at ucsd.edu  Tue Feb 13 18:40:15 2007
From: rlevy at ucsd.edu (Roger Levy)
Date: Tue, 13 Feb 2007 09:40:15 -0800
Subject: [R] multinomial logistic regression with equality constraints?
In-Reply-To: <17867.62534.447496.543187@lapo.berkeley.edu>
References: <45C38E90.9070709@ucsd.edu>	<17860.61176.520222.602660@lapo.berkeley.edu>	<17861.4763.359422.817825@macht.arts.cornell.edu>	<45C52F17.1030601@ucsd.edu>	<17861.13360.697951.302800@macht.arts.cornell.edu>	<17861.25637.324856.683429@lapo.berkeley.edu>	<45C93EEF.9080509@ucsd.edu>	<17865.18890.134246.794371@macht.arts.cornell.edu>	<45CB6D53.5000002@ucsd.edu>
	<17867.62534.447496.543187@lapo.berkeley.edu>
Message-ID: <45D1F7FF.5080702@ucsd.edu>

Many thanks for this, Jas.  I was successfully able to use the revised 
version of multinomRob, and it satisfies exactly the needs I was looking 
for.

Thanks once again.

Best,

Roger

Jasjeet Singh Sekhon wrote:
> As we noted earlier and as is clearly stated in the docs, multinomRob
> is estimating an OVERDISPERSED multinomial model.  And in your models
> here the overdispersion parameter is not identified; you need more
> observations.  Walter pointed out using the print.level trick to get
> the coefs for the standard MNL model, but when the model the function
> is actually trying to estimate is not identified, that trick will not
> work.
> 
> As I also previously noted, it is a simple matter to change the
> multinomMLE function to estimate the standard MNL model.  Since you
> don't want to change that file and since nnet's multinom function
> doesn't have some functionality people need, we'll add a "MLEonly"
> function to multinomRob which will allow you to do what you want.
> We'll post a new version on my webpage later tonight:
> http://sekhon.berkeley.edu/robust.  And after some testing, we'll
> forward the new version to CRAN.
> 
> Jas.
> 
> =======================================
> Jasjeet S. Sekhon                     
>                                       
> Associate Professor             
> Travers Department of Political Science
> Survey Research Center          
> UC Berkeley                     
> 
> http://sekhon.berkeley.edu/
> V: 510-642-9974  F: 617-507-5524
> =======================================
> 
> 
> 
> 
> Roger Levy writes:
>  > Walter Mebane wrote:
>  > > Roger,
>  > > 
>  > >  > Error in if (logliklambda < loglik) bvec <- blambda :
>  > >  > 	missing value where TRUE/FALSE needed
>  > >  > In addition: Warning message:
>  > >  > NaNs produced in: sqrt(sigma2GN)
>  > > 
>  > > That message comes from the Newton algorithm (defined in source file
>  > > multinomMLE.R).  It would be better if we bullet-proofed it a bit
>  > > more.  The first thing is to check the data.  I don't have the
>  > > multinomLogis() function, so I can't run your code.  
>  > 
>  > Whoops, sorry about that -- I'm putting revised code at the end of the 
>  > message.
>  > 
>  > > But do you really
>  > > mean
>  > > 
>  > >  > for(i in 1:length(choice)) {
>  > > and
>  > >  > dim(counts) <- c(length(choice),length(choice))
>  > > 
>  > > Should that be
>  > > 
>  > >   for(i in 1:n) {
>  > > and
>  > >   dim(counts) <- c(n, length(choice))
>  > > 
>  > > or instead of n, some number m > length(choice).  As it is it seems to
>  > > me you have three observations for three categories, which isn't going
>  > > to work (there are five coefficient parameters, plus sigma for the
>  > > dispersion).
>  > 
>  > I really did mean for(i in 1:length(choice)) -- once again, the proper 
>  > code is at the end of this message.
>  > 
>  > Also, I notice that I get the same error with another kind of data, 
>  > which works for multinom from nnet:
>  > 
>  > 
>  > library(nnet)
>  > library(multinomRob)
>  > dtf <- data.frame(y1=c(1,1),y2=c(2,1),y3=c(1,2),x=c(0,1))
>  > summary(multinom(as.matrix(dtf[,1:3]) ~ x, data=dtf))
>  > summary(multinomRob(list(y1 ~ 0, y2 ~ x, y3 ~ x), data=dtf,print.level=128))
>  > 
>  > 
>  > The call to multinom fits the following coefficients:
>  > 
>  > Coefficients:
>  >      (Intercept)          x
>  > y2 0.6933809622 -0.6936052
>  > y3 0.0001928603  0.6928327
>  > 
>  > but the call to multinomRob gives me the following error:
>  > 
>  > multinomRob(): Grouped MNL Estimation
>  > [1] "multinomMLE: -loglik initial: 9.48247391895106"
>  > Error in if (logliklambda < loglik) bvec <- blambda :
>  > 	missing value where TRUE/FALSE needed
>  > In addition: Warning message:
>  > NaNs produced in: sqrt(sigma2GN)
>  > 
>  > 
>  > Does this shed any light on things?
>  > 
>  > 
>  > Thanks again,
>  > 
>  > Roger
>  > 
>  > 
>  > 
>  > 
>  > 
>  > ***
>  > 
>  > set.seed(10)
>  > library(multinomRob)
>  > multinomLogis <- function(vector) {
>  >    x <- exp(vector)
>  >    z <- sum(x)
>  >    x/z
>  > }
>  > 
>  > n <- 20
>  > choice <- c("A","B","C")
>  > intercepts <- c(0.5,0.3,0.2)
>  > prime.strength <- rep(0.4,length(intercepts))
>  > counts <- c()
>  > for(i in 1:length(choice)) {
>  >    u <- intercepts[1:length(choice)]
>  >    u[i] <- u[i] + prime.strength[i]
>  >    counts <- c(counts,rmultinomial(n = n, pr = multinomLogis(u)))
>  > }
>  > dim(counts) <- c(length(choice),length(choice))
>  > counts <- t(counts)
>  > row.names(counts) <- choice
>  > colnames(counts) <- choice
>  > data <- data.frame(Prev.Choice=choice,counts)
>  > 
>  > for(i in 1:length(choice)) {
>  >    data[[paste("last",choice[i],sep=".")]] <- 
>  > ifelse(data$Prev.Choice==choice[i],1,0)
>  > }
>  > 
>  > multinomRob(list(A ~ last.A ,
>  >                   B ~ last.B ,
>  >                   C ~ last.C - 1 ,
>  >                   ),
>  >              data=data,
>  >              print.level=128)
>  > 
>  > 
>  > 
>  > I obtained this output:
>  > 
>  > 
>  > Your Model (xvec):
>  >                                 A B C
>  > (Intercept)/(Intercept)/last.C 1 1 1
>  > last.A/last.B/NA               1 1 0
>  > 
>  > [1] "multinomRob:  WARNING.  Limited regressor variation..."
>  > [1] "WARNING.  ... A regressor has a distinct value for only one 
>  > observation."
>  > [1] "WARNING.  ... I'm using a modified estimation algorithm (i.e., 
>  > preventing LQD"
>  > [1] "WARNING.  ... from modifying starting values for the affected 
>  > parameters)."
>  > [1] "WARNING.  ... Affected parameters are TRUE in the following table."
>  > 
>  >                                     A     B     C
>  > (Intercept)/(Intercept)/last.C FALSE FALSE  TRUE
>  > last.A/last.B/NA                TRUE  TRUE FALSE
>  > 
>  > 
>  > 
>  > multinomRob(): Grouped MNL Estimation
>  > [1] "multinomMLE: -loglik initial: 70.2764843511374"
>  > Error in if (logliklambda < loglik) bvec <- blambda :
>  > 	missing value where TRUE/FALSE needed
>  > In addition: Warning message:
>  > NaNs produced in: sqrt(sigma2GN)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From matthew_wiener at merck.com  Tue Feb 13 18:40:26 2007
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 13 Feb 2007 12:40:26 -0500
Subject: [R] matlab style plotting in R  [Broadcast]
In-Reply-To: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>
References: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>
Message-ID: <4E9A692D8755DF478B56A2892388EE1F01828F8A@usctmx1118.merck.com>

In traditional R graphics, you can take a look at "matplot".
You might also want to look at the lattice package.

Hope this helps,

Matt Wiener
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Maria
Vatapitakapha
Sent: Tuesday, February 13, 2007 12:20 PM
To: r-help at stat.math.ethz.ch
Subject: [R] matlab style plotting in R [Broadcast]

Hello

I was wondering how I can achieve matlab style plotting in R,
in the sense that matlab allows you to plot multiple sets of variables
within the same
x-y axes. plot in R does not seem to cater for this. I tried 'overplot'
from
the gplots package but this assumes different y axes for the variables.

any suggestions would be very appreciated

Maria

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From lanre.okusanya at gmail.com  Tue Feb 13 18:48:15 2007
From: lanre.okusanya at gmail.com (Lanre Okusanya)
Date: Tue, 13 Feb 2007 12:48:15 -0500
Subject: [R] matlab style plotting in R
In-Reply-To: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>
References: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>
Message-ID: <6e25bb420702130948s702e5a12qd2e96b52e565e748@mail.gmail.com>

try
?lines
?points




On 2/13/07, Maria Vatapitakapha <xrysoflis at gmail.com> wrote:
> Hello
>
> I was wondering how I can achieve matlab style plotting in R,
> in the sense that matlab allows you to plot multiple sets of variables
> within the same
> x-y axes. plot in R does not seem to cater for this. I tried 'overplot' from
> the gplots package but this assumes different y axes for the variables.
>
> any suggestions would be very appreciated
>
> Maria
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mckellercran at gmail.com  Tue Feb 13 18:50:05 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Tue, 13 Feb 2007 12:50:05 -0500
Subject: [R] matlab style plotting in R
In-Reply-To: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>
References: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>
Message-ID: <3f547caa0702130950m1598dca2sb37a8b19e80eb54d@mail.gmail.com>

Hi Maria,

I'm interested in the responses you get. The way I do this is to use
par(new=TRUE), which tells R not to clean the frame before plotting
the next plot. So, eg

###Overlaid plot
op <- par(mar = c(5, 4, 4, 5) + 0.1, las = 2)
plot(x=1:5,y=rnorm(5)+1:5,type='b',xlab="xlab",ylab="ylab",main="Title")
par(new=TRUE)
plot(x=1:5,y=rnorm(5),axes=FALSE,ylab="",xlab="",type='b',col="red")
axis(4,at=c(-2,-1,0,1,2),labels=c(6,7,8,9,10))
par(las=0)
mtext("Other Y",side=4,line=3)


The trick is this: if your 2nd set of Y variables are on a different
scale than your 1st set of Y-variables, you'll need to transform the
second set so that they'll be on the same scale - otherwise they won't
show up on the old plot. You'll also, of course, need to
back-transform your labels for the 2nd set of Y variables so that they
read appropriately.

A terrific site for R graphics, with lots of worked examples
(including ones similar to the one I just did), is here:
http://zoonek2.free.fr/UNIX/48_R/all.html




On 2/13/07, Maria Vatapitakapha <xrysoflis at gmail.com> wrote:
> Hello
>
> I was wondering how I can achieve matlab style plotting in R,
> in the sense that matlab allows you to plot multiple sets of variables
> within the same
> x-y axes. plot in R does not seem to cater for this. I tried 'overplot' from
> the gplots package but this assumes different y axes for the variables.
>
> any suggestions would be very appreciated
>
> Maria
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics


From maitra at iastate.edu  Tue Feb 13 19:04:25 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Tue, 13 Feb 2007 12:04:25 -0600
Subject: [R] simulating from Langevin distributions
Message-ID: <20070213120425.0bed871d@triveni.stat.iastate.edu>

Dear all,

I have been looking for a while for ways to simulate from Langevin distributions and I thought I would ask here. I am ok with finding an algorithmic reference, though of course, a R package would be stupendous!

Btw, just to clarify, the Langevin distribution with (mu, K), where mu is a vector and K>0 the concentration parameter is defined to be:

f(x) = exp(K*mu'x) / const where both mu and x are p-variate vectors with norm 1.

For p=2, this corresponds to von-Mises (for which algorithms exist, including in R/Splus) while for p=3, I believe it is called the Fisher distribution. I am looking for general p.

Can anyone please help in this?

Many thanks and best wishes,
Ranjan


From jebyrnes at ucdavis.edu  Tue Feb 13 19:11:04 2007
From: jebyrnes at ucdavis.edu (Jarrett Byrnes)
Date: Tue, 13 Feb 2007 10:11:04 -0800
Subject: [R] Advice on visual graph packages
Message-ID: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070213/e900ba62/attachment.pl 

From jarioksa at sun3.oulu.fi  Tue Feb 13 18:27:09 2007
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 13 Feb 2007 19:27:09 +0200
Subject: [R]  isoMDS vs. other non-metric non-R routines
Message-ID: <f76ba882d871063b6f95a97992df50a1@sun3.oulu.fi>

Sorry for not threading: I don't subscribe to this list, and the 
linking of web browser and email seems to be rudimentary.

I don't know what is Minissa. Sounds like a piece of software. What is 
the method it implements? That is, is it supposed to implement the same 
method as isoMDS or something else? IsoMDS implements Kruskal's (and 
Young's and Sheperd's and Torgeson's) NMDS, but there are other methods 
too. You are supposed to get similar results only with the same method. 
For instance, there are various definitions of stress, two of them 
amusingly called stress-1 and stress-2, but there are others.

You didn't give much detail about how you used isoMDS. We already 
discussed the danger of trapping in the starting configuration which 
you can avoid with trying (several) random starting configurations. 
Have you used 'tol' (and 'maxit') arguments in isoMDS? The default 
'tol' is rather slack, and 'maxit' fairly low, since (speculation) the 
function was written a long time ago when computer were slow, but if 
you have something better than 75MHz i486, you can try with other 
values.

I have used isoMDS quite a lot, and I have had good experience.

Cheers, Jari Oksanen


From ggrothendieck at gmail.com  Tue Feb 13 19:23:06 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Feb 2007 13:23:06 -0500
Subject: [R] Advice on visual graph packages
In-Reply-To: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>
References: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>
Message-ID: <971536df0702131023v5e49ee02re98f8fb2b37beea2@mail.gmail.com>

Also try gplot in the sna package to see if it does what you want.
Here are some examples from the r-help archives:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/87003.html

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/84442.html



On 2/13/07, Jarrett Byrnes <jebyrnes at ucdavis.edu> wrote:
> Hey, all.  I'm looking for packages that are good at two things
>
> 1) Drawing directed graphs (i.e nodes and edges), both with single
> and double headed arrows, as well as allowing for differences in line
> width and solid versus dashed.  Note: I've tried Rgraphviz here, but
> have run into some problems (which seem fixable and I may go with it
> in the end), and it doesn't satisfy need # 2 (which would be ideal if
> there is a package that does both).
>
> 2) Allowing a user to create a directed graph, and have some text
> object created that can be reprocessed easily reprocessed into a
> matrix representation, or other representation of my choosing.   I've
> tried dynamicGraph, but it seems buggy, and continually either
> crashes, behaves very erratically (nodes disappearing when I modify
> edges), nor is it clear from the UI how one outputs a new graph, nor
> how one even accesses many graph attributes.  This may be my own
> ignorance on the latter.
>
> Do you have any suggestions?
>
> Thanks!
>
> -Jarrett
>
>
>
>
> --------------------------------------------------------------
> A Quick and (Very) Dirty Guide to Stats in R
> http://didemnid.ucdavis.edu/rtutorial.html
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From csardi at rmki.kfki.hu  Tue Feb 13 19:24:41 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Tue, 13 Feb 2007 19:24:41 +0100
Subject: [R] Advice on visual graph packages
In-Reply-To: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>
References: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>
Message-ID: <20070213182441.GH12258@guzu>

Jarrett, 

check the gplot function in package SNA and the plot.igraph and 
tkplot functions in package 'igraph'. SNA's gplot is more flexible,
it knows different shapes, edges can be curved, etc, tkplot is interactive
if you desire that. It is also easy to convert between the two
graph representations of the two packages, see the graph.adjacency and
get.adjacency function in igraph.

Gabor

On Tue, Feb 13, 2007 at 10:11:04AM -0800, Jarrett Byrnes wrote:
> Hey, all.  I'm looking for packages that are good at two things
> 
> 1) Drawing directed graphs (i.e nodes and edges), both with single  
> and double headed arrows, as well as allowing for differences in line  
> width and solid versus dashed.  Note: I've tried Rgraphviz here, but  
> have run into some problems (which seem fixable and I may go with it  
> in the end), and it doesn't satisfy need # 2 (which would be ideal if  
> there is a package that does both).
> 
> 2) Allowing a user to create a directed graph, and have some text  
> object created that can be reprocessed easily reprocessed into a  
> matrix representation, or other representation of my choosing.   I've  
> tried dynamicGraph, but it seems buggy, and continually either  
> crashes, behaves very erratically (nodes disappearing when I modify  
> edges), nor is it clear from the UI how one outputs a new graph, nor  
> how one even accesses many graph attributes.  This may be my own  
> ignorance on the latter.
> 
> Do you have any suggestions?
> 
> Thanks!
> 
> -Jarrett
> 
> 
> 
> 
> --------------------------------------------------------------
> A Quick and (Very) Dirty Guide to Stats in R
> http://didemnid.ucdavis.edu/rtutorial.html
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From mothsailor at googlemail.com  Tue Feb 13 19:31:14 2007
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 13 Feb 2007 18:31:14 +0000
Subject: [R] Advice on visual graph packages
In-Reply-To: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>
References: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>
Message-ID: <815b70590702131031v59b73c72j5d99d4ac60a1d21f@mail.gmail.com>

You might also want to look at the network package, which includes a
plot method for network objects that is pretty flexible.  It also
enables you to convert a network object into various different matrix
reprentations.

On 13/02/07, Jarrett Byrnes <jebyrnes at ucdavis.edu> wrote:
> Hey, all.  I'm looking for packages that are good at two things
>
> 1) Drawing directed graphs (i.e nodes and edges), both with single
> and double headed arrows, as well as allowing for differences in line
> width and solid versus dashed.  Note: I've tried Rgraphviz here, but
> have run into some problems (which seem fixable and I may go with it
> in the end), and it doesn't satisfy need # 2 (which would be ideal if
> there is a package that does both).
>
> 2) Allowing a user to create a directed graph, and have some text
> object created that can be reprocessed easily reprocessed into a
> matrix representation, or other representation of my choosing.   I've
> tried dynamicGraph, but it seems buggy, and continually either
> crashes, behaves very erratically (nodes disappearing when I modify
> edges), nor is it clear from the UI how one outputs a new graph, nor
> how one even accesses many graph attributes.  This may be my own
> ignorance on the latter.
>
> Do you have any suggestions?
>
> Thanks!
>
> -Jarrett
>
>
>
>
> --------------------------------------------------------------
> A Quick and (Very) Dirty Guide to Stats in R
> http://didemnid.ucdavis.edu/rtutorial.html
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From singularitaet at gmx.net  Tue Feb 13 19:32:28 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Tue, 13 Feb 2007 19:32:28 +0100
Subject: [R] matlab style plotting in R
In-Reply-To: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>
References: <bd3d127c0702130919td2d950fl788e8241301017fb@mail.gmail.com>
Message-ID: <45D2043C.7030201@gmx.net>

There are many ways if I understood what you indend to do:

example data:
toplot<-data.frame(x=c(1:5,1:5),y=rnorm(10),sbs=c(rep("a",5),rep("b",5)))

1st with plot itself:

plot(y~x,data=toplot,type="n")
lines(y~x,data=subset(toplot,sbs=="a"),type="b",pch=4,col="blue")
lines(y~x,data=subset(toplot,sbs=="b"),type="b",pch=2,col="red")


2nd with xyplot out of the lattice package:
library(lattice)
xyplot(y~x,groups=sbs,data=toplot,pch=c(2,4),main="Title")

to see some variants. My advice is to look at a good documentation at
the homepage (contributed documentation) or book ...


Maria Vatapitakapha wrote:
> Hello
>
> I was wondering how I can achieve matlab style plotting in R,
> in the sense that matlab allows you to plot multiple sets of variables
> within the same
> x-y axes. plot in R does not seem to cater for this. I tried 'overplot' from
> the gplots package but this assumes different y axes for the variables.
>
> any suggestions would be very appreciated
>
> Maria
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From Charles.Annis at StatisticalEngineering.com  Tue Feb 13 19:46:09 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Tue, 13 Feb 2007 13:46:09 -0500
Subject: [R] Hyperlinks in home-brew R  packages
Message-ID: <00b201c74f9f$3a79e590$6400a8c0@DD4XFW31>

Greetings R Friends:

I?ve built a package consisting of 154 R objects, including a winMenu to
call them all (or call routines that call others).  I?ve cleaned up all the
*.Rd files, and "R CMD build --binary mh1823" builds my mh1823 package,
which loads and executes without difficulty.

In my compiled html files I have my e-mail address and homepage, however
they do not have hyperlinks.

How can I include the hyperlinks?  (Or how did I mess up and have R CMD
build not do that?)

Thanks.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From feanor0 at hotmail.com  Tue Feb 13 18:27:44 2007
From: feanor0 at hotmail.com (Murali Menon)
Date: Tue, 13 Feb 2007 17:27:44 +0000
Subject: [R] Computing stats on common parts of multiple dataframes
Message-ID: <BAY113-F3506DAB25426D029C4D2B8EE900@phx.gbl>

Folks,

I have three dataframes storing some information about
two currency pairs, as follows:

R> a

EUR-USD	NOK-SEK
1.23	1.33
1.22	1.43
1.26	1.42
1.24	1.50
1.21	1.36
1.26	1.60
1.29	1.44
1.25	1.36
1.27	1.39
1.23	1.48
1.22	1.26
1.24	1.29
1.27	1.57
1.21	1.55
1.23	1.35
1.25	1.41
1.25	1.30
1.23	1.11
1.28	1.37
1.27	1.23



R> b
EUR-USD	NOK-SEK
1.23	1.22
1.21	1.36
1.28	1.61
1.23	1.34
1.21	1.22



R> d

EUR-USD	NOK-SEK
1.27	1.39
1.23	1.48
1.22	1.26
1.24	1.29
1.27	1.57
1.21	1.55
1.23	1.35
1.25	1.41
1.25	1.33
1.23	1.11
1.28	1.37
1.27	1.23

The twist is that these entries correspond to dates where the
*last* rows in each frame are today's entries, and so on
backwards in time.

I would like to create a matrix of medians (a median for each row
and for each currency pair), but only for those rows where all
dataframes have entries.

My answer in this case should look like:

EUR-USD	NOK-SEK

1.25	1.41
1.25	1.33
1.23	1.11
1.28	1.37
1.27	1.23

where the last EUR-USD entry = median(1.27, 1.21, 1.27), etc.

Notice that the output is of the same dimensions as the smallest dataframe
(in this case 'b').

I can do it in a clumsy fashion by first obtaining the number
of rows in the smallest matrix, chopping off the top rows
of the other matrices to reduce them this size, then doing a
for-loop across each currency pair, row-wise, to create a
3-vector which I then apply median() on.

Surely there's a better way to do this?

Please advise.

Thanks,

Murali Menon

_________________________________________________________________
Valentine?s Day -- Shop for gifts that spell L-O-V-E at MSN Shopping


From davidr at rhotrading.com  Tue Feb 13 20:11:42 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 13 Feb 2007 13:11:42 -0600
Subject: [R] Fatigued R
References: <8d5a36350702130848o1a8da7fdnfb45a2c1c2209826@mail.gmail.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE7720C2EB@rhopost.rhotrading.com>

I can't be sure what is happening to Shubhak, but I know that
the Bloomberg server bbcomm.exe throws unhandled exceptions in a 
unreproducable and uncatchable way, somewhere in their tcp code. 

It is one of the big frustrations for me in my work. 

Shubhak, I think you will just have to look carefully at what is
returned,
as Bogdan suggested in general, and take action based on that. It may be

that try[Catch] will catch some problems, but other times, you will just
get 
something that is not right, but you can tell since it has the wrong 
structure.
The bbcomm.exe essentially crashes and gets restarted automatically, so
the 
next time you try to get something, it works; so sleeping after an error
is 
a good idea.

HTH,
David

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Tuesday, February 13, 2007 10:49 AM
To: shubhak at ambaresearch.com
Cc: r-help
Subject: Re: [R] Fatigued R

The problem with your code is that it doesn't check for errors. See
?try, ?tryCatch. For example:

my.download <- function(forloop) {
  notok <- vector()
  for (i in forloop) {
    cdaily <- try(blpGetData(...))
    if (class(cdaily) == "try-error") {
      notok <- c(notok, i)
    } else {
      #proceed as usual
    }
  }
  notok
}

forloop <- 1:x
repeat {
  ddata <- my.download(forloop)
  forloop <- ddata
  if (length(forloop) == 0) break  #no download errors; stop
}


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shubha
> Vishwanath Karanth
> Sent: Tuesday, February 13, 2007 10:06 AM
> To: ONKELINX, Thierry; r-help at stat.math.ethz.ch;
> sarah.goslee at gmail.com
> Subject: Re: [R] Fatigued R
>
> Hi all, thanks for your reply....
>
> But I have to make one thing clear that there are no errors in
> programming...I assure that to you, because I have extracted the data
> many times from the same program...
>
> The problem is with the connection of R with Bloomberg, sometimes the
> data is not fetched at all and so I get the below errors...It
> is mainly
> due to some network jam problems and all...
>
> Could anyone suggest me how to refresh R, or how to always make sure
> that the data is downloaded without any errors for each loop? I tried
> with Sys.sleep() to give some free time to R...but it is not
> successful
> always...
>
> Could anybody help me out?
>
> Thank you,
> Shubha
>
> -----Original Message-----
> From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
> Sent: Tuesday, February 13, 2007 7:56 PM
> To: Shubha Vishwanath Karanth; r-help at stat.math.ethz.ch
> Subject: RE: [R] Fatigued R
>
> Dear Shubha,
>
> The error message tells you that the error occurs in the line:
> 	merge(d_mer,cdaily)
> And the problem is that d_mer and cdaily have a different class. It
> looks as you need to convert cdaily to the correct class
> (same class as
> d_mer).
> Don't forget to use traceback() when debugging your program.
>
> You code could use some tweaking. Don't be afraid to add spaces and
> indentation. That will make you code a lot more readable.
> I noticed that you have defined some variables within your function
> (lent, t). This might lead to strange behaviour of your
> function, as it
> will use the global variables. Giving a variable the same name as a
> function (t) is confusing. t[2] and t(2) look very similar but do
> something very different.
> Sometimes it pays off to covert for-loop in apply-type functions.
>
> Cheers,
>
> Thierry
> --------------------------------------------------------------
> ----------
> ----
>
> ir. Thierry Onkelinx
>
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
>
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
>
> Gaverstraat 4
>
> 9500 Geraardsbergen
>
> Belgium
>
> tel. + 32 54/436 185
>
> Thierry.Onkelinx at inbo.be
>
> www.inbo.be
>
>
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
>
> A statistical analysis, properly conducted, is a delicate
> dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
>
> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens Shubha Vishwanath
> Karanth
> Verzonden: dinsdag 13 februari 2007 14:51
> Aan: Sarah Goslee; r-help at stat.math.ethz.ch
> Onderwerp: Re: [R] Fatigued R
>
> Ohkkk....I will try to do that now....
>
> This is my download function...
>
>
> download<-function(fil)
> {
> con<-blpConnect(show.days=show_day, na.action=na_action,
> periodicity=periodicity)
> for(i in 1:lent)
> {
> Sys.sleep(3)
> cdaily<-blpGetData(con,unique(t[,i]),fil,start=as.chron(as.Dat
e("1/1/199
> 6", "%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))
> #Sys.sleep(3)
> if(!exists("d_mer")) d_mer=cdaily else d_mer=merge(d_mer,cdaily)
> rm(cdaily)
> }
> dat<-data.frame(Date=index(d_mer),d_mer)
> dat$ABCDEFG=NULL
> path1<-paste("D:\\SAS\\Project2\\Daily\\",fil,"_root.sas7bdat",sep="")
> path2<-paste("D:\\SAS\\Project2\\Daily\\CODEFILES\\",fil,".sas
> ",sep="")
> sasname<-paste(x1,fil,"'",sep="")
> write.foreign(dat,path1,path2,package="SAS",dataname=sasname)
> blpDisconnect(con)
> }
>
> for(j in 1:lenf)
> {
> fname<-paste("D:\\SAS\\Project2\\Daily\\",filename[j],"_root.s
as7bdat",s
> ep="")
> Sys.sleep(600)
> if(!file.exists(fname)) download(fil=filename[j])
> }
>
> And lent=58 and lenf=8... 8 text files will be generated in
> this process
> if the program would have run properly, and each would be of
> size 4,000
> KB.
>
> The error message I get if the program is not run is:
>
> Error in dimnames(x) <- dn : length of 'dimnames' [2] not
> equal to array
> extent
> In addition: Warning message:
> Index vectors are of different classes: chron chron dates in:
> merge(d_mer,cdaily)
>
>
>
> Please could any one help on this?
>
> -----Original Message-----
> From: Sarah Goslee [mailto:sarah.goslee at gmail.com]
> Sent: Tuesday, February 13, 2007 7:09 PM
> To: Shubha Vishwanath Karanth
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Fatigued R
>
> Hi Shubha,
>
> Perhaps you haven't gotten any help because you haven't provided a
> reproducible example, or even told us what you are trying to do
> (specifically)
> or what errors you are receiving. Frankly, your problem statement
> doesn't
> make any sense to me, and I can't provide advice without more
> information.
>
> As the footer of every email says:
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> Sarah
>
> On 2/13/07, Shubha Vishwanath Karanth
> <shubhak at ambaresearch.com> wrote:
> > Hi R,
> >
> >
> >
> > Please solve my problem...........
> >
> >
> >
> > I am extracting Bloomberg data from R, in a loop. R is getting
> fatigued
> > by doing this process and gives some errors. I introduced sleep
> > function. Doing this sometimes I get the results and
> sometimes not. I
> > even noticed that if I give complete rest for R (don't open
> R window)
> > for 1 day and then run my code with the sleep function, then the
> program
> > works. But if I keep on doing this with on R, repeatedly I
> get errors.
> >
> >
> >
> > Please can anyone do something for this? Is there any function of
> > refreshing R completely.....Or any other techniques?...I am really
> > getting bugged with this.......
> >
> >
> >
> > Thanks,
> >
> > Shubha
>
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rvaradhan at jhmi.edu  Tue Feb 13 20:44:08 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 13 Feb 2007 14:44:08 -0500
Subject: [R] simulating from Langevin distributions
In-Reply-To: <20070213120425.0bed871d@triveni.stat.iastate.edu>
References: <20070213120425.0bed871d@triveni.stat.iastate.edu>
Message-ID: <001101c74fa7$53a910b0$7c94100a@win.ad.jhu.edu>

Hi Ranjan,

I think that the following would work:

library(MASS)

rlangevin <- function(n, mu, K) {
q <- length(mu)
norm.sim <- mvrnorm(n, mu=mu, Sigma=diag(1/K, q))
cp <- apply(norm.sim, 1, function(x) sqrt(crossprod(x)))
sweep(norm.sim, 1, cp, FUN="/")
}

> mu <- runif(7)
> mu <- mu / sqrt(crossprod(mu))
> K <- 1.2
> ylang <- rlangevin(n=10, mu=mu, K=K)
> apply(ylang,1,crossprod)
 [1] 1 1 1 1 1 1 1 1 1 1
>

I hope that this helps.

Ravi.
----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ranjan Maitra
Sent: Tuesday, February 13, 2007 1:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] simulating from Langevin distributions

Dear all,

I have been looking for a while for ways to simulate from Langevin
distributions and I thought I would ask here. I am ok with finding an
algorithmic reference, though of course, a R package would be stupendous!

Btw, just to clarify, the Langevin distribution with (mu, K), where mu is a
vector and K>0 the concentration parameter is defined to be:

f(x) = exp(K*mu'x) / const where both mu and x are p-variate vectors with
norm 1.

For p=2, this corresponds to von-Mises (for which algorithms exist,
including in R/Splus) while for p=3, I believe it is called the Fisher
distribution. I am looking for general p.

Can anyone please help in this?

Many thanks and best wishes,
Ranjan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Tue Feb 13 21:21:56 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Feb 2007 15:21:56 -0500
Subject: [R] Polygon triangulation?
In-Reply-To: <Pine.LNX.4.44.0702131608310.6240-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0702131608310.6240-100000@reclus.nhh.no>
Message-ID: <45D21DE4.8060409@stats.uwo.ca>

On 2/13/2007 10:10 AM, Roger Bivand wrote:
> On Tue, 13 Feb 2007, ONKELINX, Thierry wrote:
> 
>> Have you tried the tri-package?
> 
> Perhaps the GPC C library used in the gpclib package, and in PBSmapping 
> will get closer - it partitions polygons into tristrip sets.

That would be just what I need.  Thanks!

Duncan Murdoch
> 
>> 
>> Cheers,
>> 
>> Thierry
>> 
>> ------------------------------------------------------------------------
>> ----
>> 
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
>> and Forest
>> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
>> methodology and quality assurance
>> Gaverstraat 4
>> 9500 Geraardsbergen
>> Belgium
>> tel. + 32 54/436 185
>> Thierry.Onkelinx at inbo.be
>> www.inbo.be 
>>  
>> 
>> Do not put your faith in what statistics say until you have carefully
>> considered what they do not say.  ~William W. Watt
>> A statistical analysis, properly conducted, is a delicate dissection of
>> uncertainties, a surgery of suppositions. ~M.J.Moroney
>> 
>> -----Oorspronkelijk bericht-----
>> Van: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] Namens Duncan Murdoch
>> Verzonden: dinsdag 13 februari 2007 15:27
>> Aan: r-help at stat.math.ethz.ch
>> Onderwerp: [R] Polygon triangulation?
>> 
>> Can anyone point me to a package that contains code to triangulate a 
>> polygon?  This is easy if the polygon is convex, but tricky if not.  One
>> 
>> algorithm to do it is due to Meister, and is described here:
>> 
>> www.math.gatech.edu/~randall/AlgsF06/planartri.pdf
>> 
>> Duncan Murdoch
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>


From tramni at abv.bg  Tue Feb 13 21:25:32 2007
From: tramni at abv.bg (Martin Ivanov)
Date: Tue, 13 Feb 2007 22:25:32 +0200 (GMT+02:00)
Subject: [R] adf test: trend, no drift - rep: invalid 'times' argument
Message-ID: <1888317581.125301171398332357.JavaMail.nobody@mail05.abv.bg>

Hello!

I am applying the ADF.test function from package uroot to a time series of data. When I apply the full test, incorporating drift and trend terms, the regressor estimate of the drift term is not significantly different from zero. So I  apply the test to a model without drift term, with deterministic trend only. But then I always get the following error:

summary(ADF.test(wts=ts(seasons$summer, start=1850, frequency=1), itsd=c(0,1,c(0)), regvar=0, selectlags=list(mode=c(1,2,3))))
Error in rep(NA, ncol(table)) : invalid 'times' argument
Error in summary(ADF.test(wts = ts(seasons$summer, start = 1850, frequency = 1),  : 
	error in evaluating the argument 'object' in selecting a method for function 'summary'

I have no idea why this error occurs. Any suggestions will be appreciated.

Regards,
Martin

-----------------------------------------------------------------
http://auto-motor-und-sport.bg/ 
? ?????? ? ??????!


From topkatz at msn.com  Tue Feb 13 21:33:10 2007
From: topkatz at msn.com (Talbot Katz)
Date: Tue, 13 Feb 2007 15:33:10 -0500
Subject: [R] Questions about results from PCAproj for robust principal
	component analysis
Message-ID: <BAY132-F39187E0724DB2CB3164B83AA900@phx.gbl>

Hi.

I have been looking at the PCAproj function in package pcaPP (R 2.4.1) for 
robust principal components, and I'm trying to interpret the results.  I 
started with a data matrix of dimensions RxC (R is the number of rows / 
observations, C the number of columns / variables).  PCAproj returns a list 
of class princomp, similar to the output of the function princomp.  In a 
case where I can run princomp, I would get the following, from executing  
dmpca = princomp(datamatrix) :
-	the vector, sdev, of length C, contains the standard deviations of the 
components in
		order by descending value; the squares are the eigenvalues of the
		covariance matrix
-	the matrix, loadings, has dimension CxC, and the columns are the 
eigenvectors of the
		covariance matrix, in the same order as the sdev vector; the columns are
		orthonormal:
		sum(dmpca$loadings[,i]*dmpca$loadings[,j]) = 1 if i == j, ~ 0 if i != j
-	the vector, center, of length C, contains the means of the variable 
columns in the original
		data matrix, in the same order as the original columns
-	the vector, scale, of length C, contains the scalings applied to each 
variable, in the same
		order as the original columns
-	n.obs contains the number of observations used in the computation; this 
number equals
		R when there is no missing data
-	the matrix, scores, has dimension RxC, and it can be thought of as the 
projection of the
		eigenvector matrix, loadings, back onto the original data; these columns 
of
		scores are the principal components.  princomp typically removes the mean,
		so the formula is:
		dmpca$scores = t(t(datamatrix) - dmpca$center)%*%dmpca$loadings
		and apply(dmpca$scores,2,mean) returns a length C vector of (effectively)
		zeroes; also the principal components (columns of scores) are orthogonal
		(but not orthonormal):
		sum(dmpca$scores[,i]*dmpca$scores[,j]) ~ 0 if i != j, > 0 if i == j
-	call contains the function call, in this case princomp(x = datamatrix)

That is all as it should be.


In my case R < C, which produces singular results for standard PCA, but 
robust methods, like PCAproj, are designed to handle this.  Also, I had 
"de-meaned" the data beforehand, so apply(datamatrix,2,mean) produces a 
length C vector of (effectively) zeroes.  I ran the following:
dmpcaprj=PCAproj(datamatrix,k=4,CalcMethod="sphere",update=TRUE)
to get the first four robust components.  When I look at the princomp object 
returned as dmpcaprj, some of the results are just what I expect.  For 
example,
-	dmpcaprj$loadings has dimensions Cx4, as expected, and the first four 
eigenvectors of
		the (robust) covariance matrix are orthonormal:
		sum(dmpcaprj$loadings[,i]*dmpcaprj$loadings[,j]) = 1 if i == j, ~ 0 if i 
!= j
-	dmpcaprj$sdev contains the square roots of the four corresponding 
eigenvalues.
-	dmpcaprj$n.obs equals R.
-	dmpcaprj$scores has dimensions Rx4, as it should.

HOWEVER, the columns of dmpcaprj$scores are neither de-meaned nor 
orthogonal.  So,
		apply(dmpcaprj$scores,2,mean) is a non-zero vector, and
		sum(dmpcaprj$scores[,i]*dmpcaprj$scores[,j]) != 0 if i != j, > 0 if i == j
ALSO,
-	dmpcaprj$scale is in this case a vector of all 1's, as expected.  But the 
length is C, not R.
-	dmpcaprj$center is a vector of length C, not R, and the entries are not 
equal to either
		apply(datamatrix,1,mean)  or  apply(datamatrix,2,mean); I can't figure out
		where they came from.

One interesting thing is that the columns of the Rx4 matrix,
		dmpcaprj$scores - datamatrix%*%dmpcaprj$loadings
are all identically constant vectors, such that each row equals 
apply(dmpcaprj$scores,2,mean), since
apply(datamatrix%*%dmpcaprj$loadings,2,mean) is a length four vector of 
(effectively) zeroes,
but I can't interpret the values of these means of dmpcaprj$scores.


Can anyone please explain to me what is happening with the scores, scale, 
and center parts of the PCAproj results?


Thanks!


--  TMK  --
212-460-5430	home
917-656-5351	cell


From sapsi at pobox.com  Tue Feb 13 21:33:40 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 13 Feb 2007 15:33:40 -0500
Subject: [R] Multidimensional Integration over arbitrary sets
Message-ID: <11B81983-0D1C-4CC4-BAAA-E9AD3687C4BA@pobox.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070213/b91afa5e/attachment.pl 

From iverson at biostat.wisc.edu  Tue Feb 13 21:42:21 2007
From: iverson at biostat.wisc.edu (Erik Iverson)
Date: Tue, 13 Feb 2007 14:42:21 -0600
Subject: [R] Computing stats on common parts of multiple dataframes
In-Reply-To: <BAY113-F3506DAB25426D029C4D2B8EE900@phx.gbl>
References: <BAY113-F3506DAB25426D029C4D2B8EE900@phx.gbl>
Message-ID: <45D222AD.7080402@biostat.wisc.edu>

Murali -

I've come up with something that might with work, with gratutious use of 
the *apply functions.  See ?apply, ?lappy, and ?mapply for how this 
would work.  Basically, just set my.list equal to a list of data.frames 
  you would like included.  I made this to work with matrices first, so 
it does use as.matrix() in my function.  Also, this could be turned into 
a general function so that you could specify a different function other 
than "median".

#Make my.list equal to a list of dataframes you want
my.list <- list(df1,df2)

#What's the shortest?
minrow <- min(sapply(my.list,nrow))
#Chop all to the shortest
tmp <- lapply(my.list, function(x) x[(nrow(x)-(minrow-1)):nrow(x),])

#Do the computation, could change median to mean, or a user defined
#function
matrix(apply(mapply("[",lapply(tmp,as.matrix), 
MoreArgs=list(1:(minrow*2))), 1, median),
        ncol=2)

HTH, whether or not this is any "better" than your for loop solution is 
left up to you.

Erik


Murali Menon wrote:
> Folks,
> 
> I have three dataframes storing some information about
> two currency pairs, as follows:
> 
> R> a
> 
> EUR-USD    NOK-SEK
> 1.23    1.33
> 1.22    1.43
> 1.26    1.42
> 1.24    1.50
> 1.21    1.36
> 1.26    1.60
> 1.29    1.44
> 1.25    1.36
> 1.27    1.39
> 1.23    1.48
> 1.22    1.26
> 1.24    1.29
> 1.27    1.57
> 1.21    1.55
> 1.23    1.35
> 1.25    1.41
> 1.25    1.30
> 1.23    1.11
> 1.28    1.37
> 1.27    1.23
> 
> 
> 
> R> b
> EUR-USD    NOK-SEK
> 1.23    1.22
> 1.21    1.36
> 1.28    1.61
> 1.23    1.34
> 1.21    1.22
> 
> 
> 
> R> d
> 
> EUR-USD    NOK-SEK
> 1.27    1.39
> 1.23    1.48
> 1.22    1.26
> 1.24    1.29
> 1.27    1.57
> 1.21    1.55
> 1.23    1.35
> 1.25    1.41
> 1.25    1.33
> 1.23    1.11
> 1.28    1.37
> 1.27    1.23
> 
> The twist is that these entries correspond to dates where the
> *last* rows in each frame are today's entries, and so on
> backwards in time.
> 
> I would like to create a matrix of medians (a median for each row
> and for each currency pair), but only for those rows where all
> dataframes have entries.
> 
> My answer in this case should look like:
> 
> EUR-USD    NOK-SEK
> 
> 1.25    1.41
> 1.25    1.33
> 1.23    1.11
> 1.28    1.37
> 1.27    1.23
> 
> where the last EUR-USD entry = median(1.27, 1.21, 1.27), etc.
> 
> Notice that the output is of the same dimensions as the smallest dataframe
> (in this case 'b').
> 
> I can do it in a clumsy fashion by first obtaining the number
> of rows in the smallest matrix, chopping off the top rows
> of the other matrices to reduce them this size, then doing a
> for-loop across each currency pair, row-wise, to create a
> 3-vector which I then apply median() on.
> 
> Surely there's a better way to do this?
> 
> Please advise.
> 
> Thanks,
> 
> Murali Menon
> 
> _________________________________________________________________
> Valentine?s Day -- Shop for gifts that spell L-O-V-E at MSN Shopping
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From plynchnlm at gmail.com  Tue Feb 13 21:44:18 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Tue, 13 Feb 2007 15:44:18 -0500
Subject: [R] make check failure, internet.Rout.fail, Error in strsplit
In-Reply-To: <E36B38C9-12F7-4821-9211-66E322DD9A1F@hanover.edu>
References: <50d6c72a0702121528s6b9a3d56q682a65d1b4f2d11e@mail.gmail.com>
	<7B594544-8B7F-4BE5-80AA-D86313845CF1@hanover.edu>
	<50d6c72a0702130816r5a5989a3i5456296e9d7dd21b@mail.gmail.com>
	<E36B38C9-12F7-4821-9211-66E322DD9A1F@hanover.edu>
Message-ID: <50d6c72a0702131244j1e750480jc7ace066b100070c@mail.gmail.com>

I just happen to also have a MacOS 10.4 machine, but when I tried from
there, I still got "Content-length".  Anyway, I am fairly certain that
headers received from web servers would not be modified by the
receiving machine or anything in-between.  I suspect that the machine
at www.stats.ox.ac.uk, even though we see it as the same IP, must be
doing some sort of load balancing, probably on the basis of our IP
addresses, and sending our requests to different servers.

If this is correct, then it would seem that the test code for this
part of the "make test" should be modified to do the grep in a
case-insensitive way, or at the very least to support
"Content-length."  I guess the next thing to do would be to submit a
bug report.

Thanks a lot for helping me look into this problem,
       --Paul

On 2/13/07, Charilaos Skiadas <skiadas at hanover.edu> wrote:
> On Feb 13, 2007, at 11:16 AM, Paul Lynch wrote:
>
> > Thanks for giving it a try.  It is very odd that you got
> > "Content-Length" when I am getting "Content-length".  I just tried
> > curl (I had been using telnet to port 80) and I got the same (error
> > causing) "length" result:
> >
> > Perhaps we are hitting different web servers?  I ran nslookup on
> > www.stats.ox.ac.uk, and it appears to be an alias for
> > web2.stats.ox.ac.uk.  Is that the machine you are getting?
>
> I get:
> Server:         192.200.129.190
> Address:        192.200.129.190#53
>
> Non-authoritative answer:
> www.stats.ox.ac.uk      canonical name = web2.stats.ox.ac.uk.
> Name:   web2.stats.ox.ac.uk
> Address: 163.1.210.2
>
> > What
> > happens if you run curl against web2, i.e.:
>
> >   curl --head http://web2.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat
> >
> > ?
> > (I get "Content-length").
>
> I get Content-Length.
>
> I'm on MacOSX 10.4.8, don't know if that makes any difference.
>
> > Thanks,
> >     --Paul
>
> Haris
>
>
>


From muenchen at utk.edu  Tue Feb 13 21:51:48 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Tue, 13 Feb 2007 15:51:48 -0500
Subject: [R] SAS, SPSS Product Comparison Table
In-Reply-To: <3698FAED-7371-4076-94B9-9D69C2E9F001@hanover.edu>
References: <7270AEC73132194E8BC0EE06B35D93D879C15F@UTKFSVS3.utk.tennessee.edu>
	<3698FAED-7371-4076-94B9-9D69C2E9F001@hanover.edu>
Message-ID: <7270AEC73132194E8BC0EE06B35D93D88054D6@UTKFSVS3.utk.tennessee.edu>

Hi All,

Thanks to lots of good ideas from R-helpers, I've polished up the table
and posted it here:
http://oit.utk.edu/scc/RforSAS&SPSSproducts.pdf 

To be consistent with its product orientation, I dropped mixed models
(it's not a separate product in either SAS or SPSS). I also added SAS/QC
and links to similar pages such as CRAN's Task Views. People (especially
Patrick Burns) sent the following list of topics that are not SAS or
SPSS products, but which might make good additions to Task Views:

resampling techniques: boot, coin (and many others)
report generation: R (the Sweave function)
neural networks: nnet, AMORE, neural, grnnR
finance: Rmetrics, portfolio (and several more)
designed experiments: BHH2, blockrand, conf.design, spc
Bayesian: BRugs, R2WinBUGS, bayesm (and many more)
circular statistics: CircStats, circular
robustness: R and many packages
medical imaging: DICOM, AnalyzeFMRI, fmri
functional data analysis: fda, MFDA
Robust
spatial statistics: spatial, spatstat, pastecs, fields, geoR (and more)
Markov chain Monte Carlo: MCMCpack, mcmc
meta-analysis: meta
graphical models: mimR, ggm
Mixed Models:	lmer, nlme, lme4
mixture models: mixreg, mixtools
pharmacokinetics: PK, PKfit, PKtools
musicology: tuneR
sudoku: sudoku

Frank Harrell made an excellent suggestion that this be a page at the
R-wiki. It's unlikely that any one person would know all these areas so
it might work out if everyone could edit the sections they know. If
anyone wants to put it up there, let me know & I'll be happy to send it
to you in any form you like. I expect once a table format was
established editing it would be easy.

I acknowledged everyone who wrote at the bottom of the table. If I
forgot anyone, it was an oversight. Drop me a line & I'll put you on
there. Thanks again to everyone for all the help!

Cheers,
Bob







=========================================================
  Bob Muenchen (pronounced Min'-chen), Manager  
  Statistical Consulting Center
  U of TN Office of Information Technology
  200 Stokely Management Center, Knoxville, TN 37996-0520
  Voice: (865) 974-5230  
  FAX:   (865) 974-4810
  Email: muenchen at utk.edu
  Web:   http://oit.utk.edu/scc, 
  News:  http://listserv.utk.edu/archives/statnews.html


From rvaradhan at jhmi.edu  Tue Feb 13 21:56:51 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 13 Feb 2007 15:56:51 -0500
Subject: [R] Multidimensional Integration over arbitrary sets
In-Reply-To: <11B81983-0D1C-4CC4-BAAA-E9AD3687C4BA@pobox.com>
References: <11B81983-0D1C-4CC4-BAAA-E9AD3687C4BA@pobox.com>
Message-ID: <000301c74fb1$7d1b3810$7c94100a@win.ad.jhu.edu>

Hi,

By defining your function appropriately (e.g. using indicator functions),
you can make "adapt" work:

myfunc <- function(x) {
x[1]*x[2] * (x[1] >= x[2])
}
# Exact answer is 1/8

> library(adapt)
> adapt(2, lo=c(0,0), up=c(1,1), functn=myfunc)
      value      relerr      minpts      lenwrk       ifail 
  0.1250612 0.009995054        5907        1123           0 


Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Saptarshi Guha
Sent: Tuesday, February 13, 2007 3:34 PM
To: R-Help
Subject: [R] Multidimensional Integration over arbitrary sets

Hi,
	I need to integrate a 2D function over range where the limits depend

on the other e.g integrate f(x,y)=x*y over {x,0,1} and {y,x,1}.
	i.e \int_0^1 \int_x^1 xy dydx

	I checked adapt but it doesn't seem to help here. Are they any  
packages for this sort of thing?
	I tried RSitesearch but couldn't find the answer to this.
	Many thanks for you help.
	Regards
	Saptarshi

Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Graham.Williams at togaware.com  Tue Feb 13 20:58:02 2007
From: Graham.Williams at togaware.com (Graham Williams)
Date: Wed, 14 Feb 2007 06:58:02 +1100
Subject: [R] [R-pkgs] New version of rattle released
Message-ID: <20070213195802.GA14089@athene.togaware.com>

A new version of Rattle (2.1.123), a Gnome-base GUI for data mining,
written copmletely in R, and available on GNU/Linux, Unix, Mac OSX, and
MS/Windows, has been released to CRAN.

There has been quite a lot of activity since the last update, including:

Transform:
        Now include basic imputation of missing values. More to follow.

Models: 
        Move to using ada for boosting.
        Better missing value handling for random forest
        Use arules package for market basket analysis
        Add more model visualisations

Export:
        RPart PMML export completed.
        PMML export has been separated out to its own package (pmml)

Complete change log available at

	http://rattle.togaware.com/changes.html

Rattle mailing list at

	http://groups.google.com/group/rattle-users

Regards,
Graham

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From dsa at life.ku.dk  Tue Feb 13 22:09:14 2007
From: dsa at life.ku.dk (Daniela Salvini)
Date: Tue, 13 Feb 2007 22:09:14 +0100
Subject: [R] nls: "missing value or an infinity" (Error in numericDeriv)	and
	"singular gradient matrix"Error in nlsModel
Message-ID: <45D23705.1875.0079.0@life.ku.dk>

Hi,

I am a non-expert user of R. I am essaying the fit of two different functions to my data, but I receive two different error messages. I suppose I have two different problems here... But, of which nature? In the first instance I did try with some different starting values for the parameters, but without success. 
If anyone could suggest a sensible way to proceed to solve these I would be really thankful! 

Daniela

*******

Details on data and formulas:

>  polcurve=read.table("polcurve.txt",header=TRUE)
> polcurve
   dist        fath
1     1 0.138908965
2     2 0.113768871
3     3 0.114361753
4     4 0.051065765
5     5 0.095787946
6     6 0.123601131
7     7 0.117340746
8     8 0.054049434
9     9 0.112000962
10   10 0.074843028
11   11 0.064735196
12   12 0.098210497
13   13 0.093786895
14   14 0.033752379
15   15 0.038961039
16   16 0.023048216
17   17 0.010018243
18   18 0.007177034
19   19 0.003787879
20   20 0.000000000
21   21 0.000000000

#Exponential power function
> s=nls(fath~((b^2)/(a^2))*exp((-dist/a)^b),polcurve)


Error in numericDeriv(form[[3]], names(ind), env) : 
        Missing value or an infinity produced when evaluating the model
In addition: Warning message:
No starting values specified for some parameters.
Intializing 'b', 'a' to '1.'.
Consider specifying 'start' or using a selfStart model in: nls(fath ~ ((b^2)/(a^2)) * exp((-dist/a)^b), polcurve) 

#Geometric function
>s=nls(fath~((b-1)*(b-2)/a)*((1+dist/a)^-b),polcurve)


Error in nlsModel(formula, mf, start, wts) : 
        singular gradient matrix at initial parameter estimates
In addition: Warning message:
No starting values specified for some parameters.
Intializing 'b', 'a' to '1.'.
Consider specifying 'start' or using a selfStart model in: nls(fath ~ ((b - 1) * (b - 2)/a) * ((1 + dist/a)^-b),  

Daniela Salvini 
Ph.D. student
University of Copenhagen, 
Faculty of Life Sciences, 
Centre for Forest and Landscape
H?rsholm Kongevej 11
2970 H?rsholm, Denmark
 
e-mail: dsa at life.ku.dk 
Tel.: (+45)35281639 / 35281645
Fax.: (+45)35281517


From ggrothendieck at gmail.com  Tue Feb 13 22:23:44 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Feb 2007 16:23:44 -0500
Subject: [R] Computing stats on common parts of multiple dataframes
In-Reply-To: <BAY113-F3506DAB25426D029C4D2B8EE900@phx.gbl>
References: <BAY113-F3506DAB25426D029C4D2B8EE900@phx.gbl>
Message-ID: <971536df0702131323l47a42e22lfa7ad2ca82743748@mail.gmail.com>

Suppose our data frames are called DF1, DF2 and DF3.  Then
find the least number of rows, n, among them.  Create a
list, DFs, of the last n rows of the data frames and another
list, mats, which is the same but in which each component is a
matrix.  Create a parallel median function, pmedian, analogous
to pmax and mapply it to the matrices.  Finally replace that
back into a data frame.

n <- min(sapply(L, nrow))
DFs <- lapply(list(DF1, DF2, DF3), tail, n)
mats <- lapply(DFs, as.matrix)
pmedian <- function(...) median(c(...))
medians <- do.call(mapply, c(pmedian, mats))
replace(DFs[[1]], TRUE, medians)


On 2/13/07, Murali Menon <feanor0 at hotmail.com> wrote:
> Folks,
>
> I have three dataframes storing some information about
> two currency pairs, as follows:
>
> R> a
>
> EUR-USD NOK-SEK
> 1.23    1.33
> 1.22    1.43
> 1.26    1.42
> 1.24    1.50
> 1.21    1.36
> 1.26    1.60
> 1.29    1.44
> 1.25    1.36
> 1.27    1.39
> 1.23    1.48
> 1.22    1.26
> 1.24    1.29
> 1.27    1.57
> 1.21    1.55
> 1.23    1.35
> 1.25    1.41
> 1.25    1.30
> 1.23    1.11
> 1.28    1.37
> 1.27    1.23
>
>
>
> R> b
> EUR-USD NOK-SEK
> 1.23    1.22
> 1.21    1.36
> 1.28    1.61
> 1.23    1.34
> 1.21    1.22
>
>
>
> R> d
>
> EUR-USD NOK-SEK
> 1.27    1.39
> 1.23    1.48
> 1.22    1.26
> 1.24    1.29
> 1.27    1.57
> 1.21    1.55
> 1.23    1.35
> 1.25    1.41
> 1.25    1.33
> 1.23    1.11
> 1.28    1.37
> 1.27    1.23
>
> The twist is that these entries correspond to dates where the
> *last* rows in each frame are today's entries, and so on
> backwards in time.
>
> I would like to create a matrix of medians (a median for each row
> and for each currency pair), but only for those rows where all
> dataframes have entries.
>
> My answer in this case should look like:
>
> EUR-USD NOK-SEK
>
> 1.25    1.41
> 1.25    1.33
> 1.23    1.11
> 1.28    1.37
> 1.27    1.23
>
> where the last EUR-USD entry = median(1.27, 1.21, 1.27), etc.
>
> Notice that the output is of the same dimensions as the smallest dataframe
> (in this case 'b').
>
> I can do it in a clumsy fashion by first obtaining the number
> of rows in the smallest matrix, chopping off the top rows
> of the other matrices to reduce them this size, then doing a
> for-loop across each currency pair, row-wise, to create a
> 3-vector which I then apply median() on.
>
> Surely there's a better way to do this?
>
> Please advise.
>
> Thanks,
>
> Murali Menon
>
> _________________________________________________________________
> Valentine's Day -- Shop for gifts that spell L-O-V-E at MSN Shopping
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From ggrothendieck at gmail.com  Tue Feb 13 22:25:49 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Feb 2007 16:25:49 -0500
Subject: [R] Computing stats on common parts of multiple dataframes
In-Reply-To: <971536df0702131323l47a42e22lfa7ad2ca82743748@mail.gmail.com>
References: <BAY113-F3506DAB25426D029C4D2B8EE900@phx.gbl>
	<971536df0702131323l47a42e22lfa7ad2ca82743748@mail.gmail.com>
Message-ID: <971536df0702131325n59fb7edfpa899dfa1c0942e10@mail.gmail.com>

Sorry, I switched variable names part way through.  Here it is again:


DFs <- list(DF1, DF2, DF3)
n <- min(sapply(DFs, nrow))
DFs <- lapply(DFs, tail, n)
mats <- lapply(DFs, as.matrix)
pmedian <- function(...) median(c(...))
medians <- do.call(mapply, c(pmedian, mats))
replace(DFs[[1]], TRUE, medians)


On 2/13/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Suppose our data frames are called DF1, DF2 and DF3.  Then
> find the least number of rows, n, among them.  Create a
> list, DFs, of the last n rows of the data frames and another
> list, mats, which is the same but in which each component is a
> matrix.  Create a parallel median function, pmedian, analogous
> to pmax and mapply it to the matrices.  Finally replace that
> back into a data frame.
>
> n <- min(sapply(L, nrow))
> DFs <- lapply(list(DF1, DF2, DF3), tail, n)
> mats <- lapply(DFs, as.matrix)
> pmedian <- function(...) median(c(...))
> medians <- do.call(mapply, c(pmedian, mats))
> replace(DFs[[1]], TRUE, medians)
>
>
> On 2/13/07, Murali Menon <feanor0 at hotmail.com> wrote:
> > Folks,
> >
> > I have three dataframes storing some information about
> > two currency pairs, as follows:
> >
> > R> a
> >
> > EUR-USD NOK-SEK
> > 1.23    1.33
> > 1.22    1.43
> > 1.26    1.42
> > 1.24    1.50
> > 1.21    1.36
> > 1.26    1.60
> > 1.29    1.44
> > 1.25    1.36
> > 1.27    1.39
> > 1.23    1.48
> > 1.22    1.26
> > 1.24    1.29
> > 1.27    1.57
> > 1.21    1.55
> > 1.23    1.35
> > 1.25    1.41
> > 1.25    1.30
> > 1.23    1.11
> > 1.28    1.37
> > 1.27    1.23
> >
> >
> >
> > R> b
> > EUR-USD NOK-SEK
> > 1.23    1.22
> > 1.21    1.36
> > 1.28    1.61
> > 1.23    1.34
> > 1.21    1.22
> >
> >
> >
> > R> d
> >
> > EUR-USD NOK-SEK
> > 1.27    1.39
> > 1.23    1.48
> > 1.22    1.26
> > 1.24    1.29
> > 1.27    1.57
> > 1.21    1.55
> > 1.23    1.35
> > 1.25    1.41
> > 1.25    1.33
> > 1.23    1.11
> > 1.28    1.37
> > 1.27    1.23
> >
> > The twist is that these entries correspond to dates where the
> > *last* rows in each frame are today's entries, and so on
> > backwards in time.
> >
> > I would like to create a matrix of medians (a median for each row
> > and for each currency pair), but only for those rows where all
> > dataframes have entries.
> >
> > My answer in this case should look like:
> >
> > EUR-USD NOK-SEK
> >
> > 1.25    1.41
> > 1.25    1.33
> > 1.23    1.11
> > 1.28    1.37
> > 1.27    1.23
> >
> > where the last EUR-USD entry = median(1.27, 1.21, 1.27), etc.
> >
> > Notice that the output is of the same dimensions as the smallest dataframe
> > (in this case 'b').
> >
> > I can do it in a clumsy fashion by first obtaining the number
> > of rows in the smallest matrix, chopping off the top rows
> > of the other matrices to reduce them this size, then doing a
> > for-loop across each currency pair, row-wise, to create a
> > 3-vector which I then apply median() on.
> >
> > Surely there's a better way to do this?
> >
> > Please advise.
> >
> > Thanks,
> >
> > Murali Menon
> >
> > _________________________________________________________________
> > Valentine's Day -- Shop for gifts that spell L-O-V-E at MSN Shopping
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>


From rvaradhan at jhmi.edu  Tue Feb 13 22:28:11 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 13 Feb 2007 16:28:11 -0500
Subject: [R] nls: "missing value or an infinity" (Error in
	numericDeriv)	and"singular gradient matrix"Error in nlsModel
In-Reply-To: <45D23705.1875.0079.0@life.ku.dk>
References: <45D23705.1875.0079.0@life.ku.dk>
Message-ID: <000f01c74fb5$dcb65440$7c94100a@win.ad.jhu.edu>

Hi Daniela,

Please read the error message from nls.  The problem is with the "start
values" for the parameters a and b.  You haven't specified any, so it uses
default values of a=1 and b=1, which may not be very good.  So, you should
specify good start values, if you have a reasonable idea of what they should
be.  If you don't, then you could try selfStart, as the error message tells
you to do.

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Daniela Salvini
Sent: Tuesday, February 13, 2007 4:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] nls: "missing value or an infinity" (Error in numericDeriv)
and"singular gradient matrix"Error in nlsModel

Hi,

I am a non-expert user of R. I am essaying the fit of two different
functions to my data, but I receive two different error messages. I suppose
I have two different problems here... But, of which nature? In the first
instance I did try with some different starting values for the parameters,
but without success. 
If anyone could suggest a sensible way to proceed to solve these I would be
really thankful! 

Daniela

*******

Details on data and formulas:

>  polcurve=read.table("polcurve.txt",header=TRUE)
> polcurve
   dist        fath
1     1 0.138908965
2     2 0.113768871
3     3 0.114361753
4     4 0.051065765
5     5 0.095787946
6     6 0.123601131
7     7 0.117340746
8     8 0.054049434
9     9 0.112000962
10   10 0.074843028
11   11 0.064735196
12   12 0.098210497
13   13 0.093786895
14   14 0.033752379
15   15 0.038961039
16   16 0.023048216
17   17 0.010018243
18   18 0.007177034
19   19 0.003787879
20   20 0.000000000
21   21 0.000000000

#Exponential power function
> s=nls(fath~((b^2)/(a^2))*exp((-dist/a)^b),polcurve)


Error in numericDeriv(form[[3]], names(ind), env) : 
        Missing value or an infinity produced when evaluating the model
In addition: Warning message:
No starting values specified for some parameters.
Intializing 'b', 'a' to '1.'.
Consider specifying 'start' or using a selfStart model in: nls(fath ~
((b^2)/(a^2)) * exp((-dist/a)^b), polcurve) 

#Geometric function
>s=nls(fath~((b-1)*(b-2)/a)*((1+dist/a)^-b),polcurve)


Error in nlsModel(formula, mf, start, wts) : 
        singular gradient matrix at initial parameter estimates
In addition: Warning message:
No starting values specified for some parameters.
Intializing 'b', 'a' to '1.'.
Consider specifying 'start' or using a selfStart model in: nls(fath ~ ((b -
1) * (b - 2)/a) * ((1 + dist/a)^-b),  

Daniela Salvini 
Ph.D. student
University of Copenhagen, 
Faculty of Life Sciences, 
Centre for Forest and Landscape
H?rsholm Kongevej 11
2970 H?rsholm, Denmark
 
e-mail: dsa at life.ku.dk 
Tel.: (+45)35281639 / 35281645
Fax.: (+45)35281517

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From magno_yu at ml.com  Tue Feb 13 22:35:25 2007
From: magno_yu at ml.com (yoooooo)
Date: Tue, 13 Feb 2007 13:35:25 -0800 (PST)
Subject: [R] Matrix manipulation
Message-ID: <8953806.post@talk.nabble.com>


Hi, let's say I have this

A = matrix(c(1, 2, 4), nrow=1)
colnames(A)=c("YOO1", "YOO2", "YOO3")

# ie
#      YOO1 YOO2 YOO3
#[1,]    1    2    4

HELLO <- NULL
HELLO$YOO1="BOO"
HELLO$YOO2="BOO"
HELLO$YOO3="HOO"

and I want a matrix that will sum my categorization.. how can I do it
efficiently without any loop?

#ie   BOO   HOO
#[1,]  3         4

Thanks a lot!
-- 
View this message in context: http://www.nabble.com/Matrix-manipulation-tf3223616.html#a8953806
Sent from the R help mailing list archive at Nabble.com.


From magno_yu at ml.com  Tue Feb 13 22:36:33 2007
From: magno_yu at ml.com (yoooooo)
Date: Tue, 13 Feb 2007 13:36:33 -0800 (PST)
Subject: [R] Matrix manipulation
Message-ID: <8953808.post@talk.nabble.com>


Hi, let's say I have this

A = matrix(c(1, 2, 4), nrow=1)
colnames(A)=c("YOO1", "YOO2", "YOO3")

# ie
#      YOO1 YOO2 YOO3
#[1,]    1    2    4

HELLO <- NULL
HELLO$YOO1="BOO"
HELLO$YOO2="BOO"
HELLO$YOO3="HOO"

and I want a matrix that will sum my categorization.. how can I do it
efficiently without any loop?

#ie   BOO   HOO
#[1,]  3         4

Thanks a lot!
-- 
View this message in context: http://www.nabble.com/Matrix-manipulation-tf3223618.html#a8953808
Sent from the R help mailing list archive at Nabble.com.


From philip.leifeld at uni-konstanz.de  Tue Feb 13 23:08:36 2007
From: philip.leifeld at uni-konstanz.de (Philip Leifeld)
Date: Tue, 13 Feb 2007 23:08:36 +0100
Subject: [R] isoMDS vs. other non-metric non-R routines
Message-ID: <45D244F4.24756.2D0C371@philip.leifeld.uni-konstanz.de>

Thanks for your message.

> I don't know what is Minissa. Sounds like a piece of software. What
> is the method it implements? That is, is it supposed to implement 
> the same method as isoMDS or something else? IsoMDS implements
> Kruskal's (and Young's and Sheperd's and Torgeson's) NMDS, but
> there are other methods too. You are supposed to get similar
> results only with the same method. For instance, there are various
> definitions of stress, two of them amusingly called stress-1 and
> stress-2, but there are others.

Yes, Minissa uses Kruskal's NMDS and stress1, so results should be 
comparable.

> You didn't give much detail about how you used isoMDS. We already
> discussed the danger of trapping in the starting configuration
> which you can avoid with trying (several) random starting
> configurations. Have you used 'tol' (and 'maxit') arguments in
> isoMDS? The default 'tol' is rather slack, and 'maxit' fairly low,
> since (speculation) the function was written a long time ago when
> computer were slow, but if you have something better than 75MHz
> i486, you can try with other values.
> Cheers, Jari Oksanen

This was my initial call:

mds <- isoMDS(dist, y = cmdscale(dist, k = 2), k=2, tol = 1e-3, maxit 
= 500)

I played around a little bit with tol and maxit (adding some 
zeros...) and increased the number of dimensions, but it did not 
change the results significantly. Using initMDS did not improve the 
result either. Unfortunately, my data set is too large to be 
displayed here. Any other ideas? My stress value is still 1.5 as much 
as in other implementations of NMDS.

Cheers

Phil


From Kurt.Hornik at wu-wien.ac.at  Tue Feb 13 23:19:04 2007
From: Kurt.Hornik at wu-wien.ac.at (Kurt Hornik)
Date: Tue, 13 Feb 2007 23:19:04 +0100
Subject: [R] get.hist.quote problem yahoo
In-Reply-To: <01a301c74c78$806b7160$0900a8c0@rman>
References: <JD76H5$264232457CE289BCC2F55186A93BE37E@libero.it>
	<01a301c74c78$806b7160$0900a8c0@rman>
Message-ID: <17874.14680.559999.920573@mithrandir.hornik.net>

>>>>> Rene Braeckman writes:

> I had the same problem some time ago. Below is a function that I
> picked up on the web somewhere (can't remember where; may have been a
> newsletter).  It's based on the tseries function but the difference is
> that this function produces a data frame with a column containing the
> dates of the quotes, instead of a time series object. I had to replace
> "%d-%b-%y" by "%Y-%m-%d" to make it work, probably as you stated
> because the format was changed by Yahoo.

This issue should be taken care of now by a new release of tseries I put
out two days ago.

-k

> Hope this helps.

> Rene

> # ----------------------------------------------------------------------
> # "df.get.hist.quote()" function
> #
> # Based on code by A. Trapletti (package tseries)
> #
> # The main difference is that this function produces a data frame with
> # a column containing the dates of the quotes, instead of a time series
> # object.
> df.get.hist.quote <- function (instrument = "ibm",
>                                start, end,
>                                quote = c("Open","High", "Low",
> "Close","Volume"),
>                                provider = "yahoo", method = "auto") 
> {
>     if (missing(start)) 
>         start <- "1970-01-02"
>     if (missing(end)) 
>         end <- format(Sys.time() - 86400, "%Y-%m-%d")
>     provider <- match.arg(provider)
>     start <- as.POSIXct(start, tz = "GMT")
>     end <- as.POSIXct(end, tz = "GMT")
>     if (provider == "yahoo") {
>         url <- paste("http://chart.yahoo.com/table.csv?s=", instrument, 
>             format(start, "&a=%m&b=%d&c=%Y"), format(end,
> "&d=%m&e=%d&f=%Y"), 
>             "&g=d&q=q&y=0&z=", instrument, "&x=.csv", sep = "")
>         destfile <- tempfile()
>         status <- download.file(url, destfile, method = method)
>         if (status != 0) {
>             unlink(destfile)
>             stop(paste("download error, status", status))
>         }
>         status <- scan(destfile, "", n = 1, sep = "\n", quiet = TRUE)
>         if (substring(status, 1, 2) == "No") {
>             unlink(destfile)
>             stop(paste("No data available for", instrument))
>         }
>         x <- read.table(destfile, header = TRUE, sep = ",")
>         unlink(destfile)
>         nser <- pmatch(quote, names(x))
>         if (any(is.na(nser))) 
>             stop("This quote is not available")
>         n <- nrow(x)
>         lct <- Sys.getlocale("LC_TIME")
>         Sys.setlocale("LC_TIME", "C")
>         on.exit(Sys.setlocale("LC_TIME", lct))
>         dat <- gsub(" ", "0", as.character(x[, 1]))
>         dat <- as.POSIXct(strptime(dat, "%Y-%m-%d"), tz = "GMT")        
>         if (dat[n] != start) 
>             cat(format(dat[n], "time series starts %Y-%m-%d\n"))
>         if (dat[1] != end) 
>             cat(format(dat[1], "time series ends   %Y-%m-%d\n"))
 
> return(data.frame(cbind(Date=I(format(dat[n:1],"%Y-%m-%d")),x[n:1,nser]),row
> .names=1:n))
>       }
>     else stop("Provider not implemented")
> } 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Daniele Amberti
> Sent: Friday, February 09, 2007 5:22 AM
> To: r-help
> Subject: [R] get.hist.quote problem yahoo

> I have functions using get.hist.quote() from library tseries.

> It seems that something changed (yahoo) and function get broken.

> try with a simple

> get.hist.quote('IBM')

> and let me kow if for someone it is still working.

> I get this error:
> Error in if (!quiet && dat[n] != start) cat(format(dat[n], "time series
> starts %Y-%m-%d\n")) : 
>         missing value where TRUE/FALSE needed

> Looking at the code it seems that before the format of dates in yahoo's cv
> file was not iso.
> Now it is iso standard year-month-day

> Anyone get the same problem?


> ------------------------------------------------------
> Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
> http://click.libero.it/infostrada9feb07

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maitra at iastate.edu  Tue Feb 13 23:31:52 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Tue, 13 Feb 2007 16:31:52 -0600
Subject: [R] simulating from Langevin distributions
In-Reply-To: <001101c74fa7$53a910b0$7c94100a@win.ad.jhu.edu>
References: <20070213120425.0bed871d@triveni.stat.iastate.edu>
	<001101c74fa7$53a910b0$7c94100a@win.ad.jhu.edu>
Message-ID: <20070213163152.3da8c8a4@triveni.stat.iastate.edu>

Thanks to Ravi Varadhan for providing the solution. I guess the answer is that if 

X is MVN with mean mu and dispersion matrix given by I/K, then X/norm(X) is Langevin with the required parameters. A reference for this is Watson's Statistics on Spheres.

Many thanks again and best wishes.
Ranjan


On Tue, 13 Feb 2007 14:44:08 -0500 "Ravi Varadhan" <rvaradhan at jhmi.edu> wrote:

> Hi Ranjan,
> 
> I think that the following would work:
> 
> library(MASS)
> 
> rlangevin <- function(n, mu, K) {
> q <- length(mu)
> norm.sim <- mvrnorm(n, mu=mu, Sigma=diag(1/K, q))
> cp <- apply(norm.sim, 1, function(x) sqrt(crossprod(x)))
> sweep(norm.sim, 1, cp, FUN="/")
> }
> 
> > mu <- runif(7)
> > mu <- mu / sqrt(crossprod(mu))
> > K <- 1.2
> > ylang <- rlangevin(n=10, mu=mu, K=K)
> > apply(ylang,1,crossprod)
>  [1] 1 1 1 1 1 1 1 1 1 1
> >
> 
> I hope that this helps.
> 
> Ravi.
> ----------------------------------------------------------------------------
> -------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor, The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology 
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email: rvaradhan at jhmi.edu
> 
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
> 
>  
> 
> ----------------------------------------------------------------------------
> --------
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ranjan Maitra
> Sent: Tuesday, February 13, 2007 1:04 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] simulating from Langevin distributions
> 
> Dear all,
> 
> I have been looking for a while for ways to simulate from Langevin
> distributions and I thought I would ask here. I am ok with finding an
> algorithmic reference, though of course, a R package would be stupendous!
> 
> Btw, just to clarify, the Langevin distribution with (mu, K), where mu is a
> vector and K>0 the concentration parameter is defined to be:
> 
> f(x) = exp(K*mu'x) / const where both mu and x are p-variate vectors with
> norm 1.
> 
> For p=2, this corresponds to von-Mises (for which algorithms exist,
> including in R/Splus) while for p=3, I believe it is called the Fisher
> distribution. I am looking for general p.
> 
> Can anyone please help in this?
> 
> Many thanks and best wishes,
> Ranjan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pfh at acoustics.aau.dk  Tue Feb 13 23:40:37 2007
From: pfh at acoustics.aau.dk (Pablo Faundez Hoffmann)
Date: Tue, 13 Feb 2007 23:40:37 +0100
Subject: [R] Multiple comparisons
Message-ID: <45D23E65.10102@acoustics.aau.dk>

Dear list,

I have to do an ANOVA analysis with one fixed effect A and one random 
effect SUBJECT. TO do this I used aov in the form
 > aov.m1 <- aov(depvar ~ A + Error(SUBJECT/(A)));

My question is if I obtain significant differences within the strata, 
does it make any sense to make multiple comparisons in order to know 
what levels of the factor are significant?. I asked this because I get 
an error when I try

TukeyHSD(aov.m1), and I am not sure if this is correct.

thanks

Pablo Hoffmann


From zhangyiosu at yahoo.com  Tue Feb 13 23:47:51 2007
From: zhangyiosu at yahoo.com (YI ZHANG)
Date: Tue, 13 Feb 2007 14:47:51 -0800 (PST)
Subject: [R] errors when installing new packages
Message-ID: <257220.83197.qm@web33004.mail.mud.yahoo.com>

Dear all,
I met a problem when installing new packages on R, my system is linux
fedora 6.0, the following is output. please help me. Thanks.

> install.packages('lars')
--- Please select a CRAN mirror for use in this session ---
Loading Tcl/Tk interface ... done
trying URL 'http://www.stathy.com/cran/src/contrib/lars_0.9-5.tar.gz'
Content type 'application/x-tar' length 188248 bytes
opened URL
==================================================
downloaded 183Kb

* Installing *source* package 'lars' ...
** libs
gfortran   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2
-fexceptions -fstack-protector --param=ssp-buffer-size=4 -m32
-march=i386 -mtune=generic -fasynchronous-unwind-tables -c delcol.f
-o delcol.o
make: gfortran: Command not found
make: *** [delcol.o] Error 127
ERROR: compilation failed for package 'lars'
** Removing '/usr/lib/R/library/lars'

The downloaded packages are in
        /tmp/RtmpxYqqFS/downloaded_packages
Warning message:
installation of package 'lars' had non-zero exit status in:
install.packages("lars") 



 
____________________________________________________________________________________
Get your own web address.


From mannelbras at fairfieldweekly.com  Tue Feb 13 23:56:25 2007
From: mannelbras at fairfieldweekly.com (Norm Roemer)
Date: Tue, 13 Feb 2007 19:56:25 -0300
Subject: [R] to loosenes
Message-ID: <01c74fc2$30475760$6c01a8c0@MURILINNNN>

Hi,

Save over 50% on your medication

http://www.ledrx .com

Remove space in the above link



month will affect you, with reference to your personal chart,  she
snapped, sounding much more like Professor McGonagall than her usual
airy-fairy self. I want it ready to hand in next Monday, and no excuses!


From huber at ebi.ac.uk  Wed Feb 14 00:14:05 2007
From: huber at ebi.ac.uk (Wolfgang Huber)
Date: Tue, 13 Feb 2007 23:14:05 +0000
Subject: [R] Advice on visual graph packages
In-Reply-To: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>
References: <BDC5B858-F8E7-4BBC-A3A5-F2E983F67028@ucdavis.edu>
Message-ID: <45D2463D.4040706@ebi.ac.uk>

Hi Jarrett,

would the coercion methods for the "graph" class, provided by the 
package of the same name at Bioconductor be useful for doing what you 
want? This is the same class that also Rgraphviz works on. Try

library("graph")
example("graphNEL-class")
as(gR, "matrix")

class ? graph
class ? graphNEL
? toGXL

There is a rich sets of methods for setting and accessing node and edge 
attributes, and it is straightforward R to convert into any other 
representation you like. See the vignette "Attributes for Graph 
Objects". I am looking at version >= 1.13.6 of the package as I write this,

  Best wishes

-- 
------------------------------------------------------------------
Wolfgang Huber  EBI/EMBL  Cambridge UK  http://www.ebi.ac.uk/huber

> Hey, all.  I'm looking for packages that are good at two things
> 
> 1) Drawing directed graphs (i.e nodes and edges), both with single  
> and double headed arrows, as well as allowing for differences in line  
> width and solid versus dashed.  Note: I've tried Rgraphviz here, but  
> have run into some problems (which seem fixable and I may go with it  
> in the end), and it doesn't satisfy need # 2 (which would be ideal if  
> there is a package that does both).
> 
> 2) Allowing a user to create a directed graph, and have some text  
> object created that can be reprocessed easily reprocessed into a  
> matrix representation, or other representation of my choosing.   I've  
> tried dynamicGraph, but it seems buggy, and continually either  
> crashes, behaves very erratically (nodes disappearing when I modify  
> edges), nor is it clear from the UI how one outputs a new graph, nor  
> how one even accesses many graph attributes.  This may be my own  
> ignorance on the latter.
> 
> Do you have any suggestions?
> 
> Thanks!
> 
> -Jarrett
> 
>


From GPetris at uark.edu  Wed Feb 14 00:16:52 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Tue, 13 Feb 2007 17:16:52 -0600 (CST)
Subject: [R] Matrix manipulation
In-Reply-To: <8953806.post@talk.nabble.com> (message from yoooooo on Tue, 13
	Feb 2007 13:35:25 -0800 (PST))
References: <8953806.post@talk.nabble.com>
Message-ID: <200702132316.l1DNGqOC025657@definetti.ddns.uark.edu>


> Hi, let's say I have this
> 
> A = matrix(c(1, 2, 4), nrow=1)
> colnames(A)=c("YOO1", "YOO2", "YOO3")

Why do you need A to be a matrix and not simply a vector?

> 
> # ie
> #      YOO1 YOO2 YOO3
> #[1,]    1    2    4
> 
> HELLO <- NULL
> HELLO$YOO1="BOO"
> HELLO$YOO2="BOO"
> HELLO$YOO3="HOO"
> 

Why do you need HELLO to be a list and not simply a character vector?

> and I want a matrix that will sum my categorization.. how can I do it
> efficiently without any loop?
> 
> #ie   BOO   HOO
> #[1,]  3         4
> 

Anyway, here is a solution:

x <- tapply(A, match(unlist(HELLO), unique(unlist(HELLO))), sum)
names(x) <- unique(unlist(HELLO))
x

HTH,
Giovanni

-- 

Giovanni Petris  <GPetris at uark.edu>
Associate Professor
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From joris.dewolf at cropdesign.com  Wed Feb 14 00:20:33 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Wed, 14 Feb 2007 00:20:33 +0100
Subject: [R] Hierarchical ANOVA
In-Reply-To: <OF3681548B.0105C3F6-ONC1257281.0055E4DB-C1257281.00596B29@ville-ge.ch>
Message-ID: <OF1861A65E.01EF18E2-ONC1257281.0080206B-C1257281.00803861@basf-c-s.be>


Romain,

Look for info on mixed models.
In R you do this either with the library nlme or lme4.

A good starting point is an article by Doug Bates in Rnews
http://cran.r-project.org/doc/Rnews/Rnews_2005-1.pdf

Joris


r-help-bounces at stat.math.ethz.ch wrote on 13/02/2007 17:16:41:

>
> Hello,
>
> Does somebody could help me in the computation(formulation) of a
> hierarchical ANOVA using linear model in R?
> I'm working in a population biology study of an endangered species. My
aim
> is to see if I have effects of "Density of individuals/m2" on several
> measured plants fitness traits.
>
> As independent variables I have:
>
> Humidity (continuous)
> Nutritive substance (continuous)
> LUX (continuous)
> Density of individuals/m2 (continuous)
> Population (categorical)
> Year (categorical)
>
> I want to test the 4 continous variables with Population as error term.
And
> the Population, the Year and interaction term, with the error of
Population
> x Year.
>
> Thank you for any help, best regards.
>
> Romain Mayor
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sapsi at pobox.com  Wed Feb 14 00:42:32 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 13 Feb 2007 18:42:32 -0500
Subject: [R] Multidimensional Integration over arbitrary sets
In-Reply-To: <000301c74fb1$7d1b3810$7c94100a@win.ad.jhu.edu>
References: <11B81983-0D1C-4CC4-BAAA-E9AD3687C4BA@pobox.com>
	<000301c74fb1$7d1b3810$7c94100a@win.ad.jhu.edu>
Message-ID: <11463848-37BA-442D-B2F0-51865D5494DF@pobox.com>

Hi,
	Thanks! That should do it.
Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha

On Feb 13, 2007, at 3:56 PM, Ravi Varadhan wrote:

> Hi,
>
> By defining your function appropriately (e.g. using indicator  
> functions),
> you can make "adapt" work:
>
> myfunc <- function(x) {
> x[1]*x[2] * (x[1] >= x[2])
> }
> # Exact answer is 1/8
>
>> library(adapt)
>> adapt(2, lo=c(0,0), up=c(1,1), functn=myfunc)
>       value      relerr      minpts      lenwrk       ifail
>   0.1250612 0.009995054        5907        1123           0
>
>
> Ravi.
>
> ---------------------------------------------------------------------- 
> ------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/ 
> Varadhan.html
>
>
>
> ---------------------------------------------------------------------- 
> ------
> --------
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Saptarshi Guha
> Sent: Tuesday, February 13, 2007 3:34 PM
> To: R-Help
> Subject: [R] Multidimensional Integration over arbitrary sets
>
> Hi,
> 	I need to integrate a 2D function over range where the limits depend
>
> on the other e.g integrate f(x,y)=x*y over {x,0,1} and {y,x,1}.
> 	i.e \int_0^1 \int_x^1 xy dydx
>
> 	I checked adapt but it doesn't seem to help here. Are they any
> packages for this sort of thing?
> 	I tried RSitesearch but couldn't find the answer to this.
> 	Many thanks for you help.
> 	Regards
> 	Saptarshi
>
> Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maitra at iastate.edu  Wed Feb 14 01:14:48 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Tue, 13 Feb 2007 18:14:48 -0600
Subject: [R] errors when installing new packages
In-Reply-To: <257220.83197.qm@web33004.mail.mud.yahoo.com>
References: <257220.83197.qm@web33004.mail.mud.yahoo.com>
Message-ID: <20070213181448.669bae89@triveni.stat.iastate.edu>

Hello Ying,

This is really a Fedora Core 6 question. But anyway, it appears that you do not have gfortran which comes in the appropriate development package installed.

Assuming you use yum, you find out the RPM using the following

yum provides gfortran 

which will give you the RPM which contains the binary for gfortran.

 gcc-gfortran   4.1.1-51.fc6

If yum says this is installed, then your path is not set right. Otherwise, go ahead and install using 

yum install gcc-gfortran 

as root or with sudo privileges.

HTH.

Many thanks and best wishes,
Ranjan

On Tue, 13 Feb 2007 14:47:51 -0800 (PST) YI ZHANG <zhangyiosu at yahoo.com> wrote:

> Dear all,
> I met a problem when installing new packages on R, my system is linux
> fedora 6.0, the following is output. please help me. Thanks.
> 
> > install.packages('lars')
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://www.stathy.com/cran/src/contrib/lars_0.9-5.tar.gz'
> Content type 'application/x-tar' length 188248 bytes
> opened URL
> ==================================================
> downloaded 183Kb
> 
> * Installing *source* package 'lars' ...
> ** libs
> gfortran   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2
> -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m32
> -march=i386 -mtune=generic -fasynchronous-unwind-tables -c delcol.f
> -o delcol.o
> make: gfortran: Command not found
> make: *** [delcol.o] Error 127
> ERROR: compilation failed for package 'lars'
> ** Removing '/usr/lib/R/library/lars'
> 
> The downloaded packages are in
>         /tmp/RtmpxYqqFS/downloaded_packages
> Warning message:
> installation of package 'lars' had non-zero exit status in:
> install.packages("lars") 
> 
> 
> 
>  
> ____________________________________________________________________________________
> Get your own web address.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lmaa515 at uow.edu.au  Wed Feb 14 01:27:32 2007
From: lmaa515 at uow.edu.au (Loai Mahmoud Awad Alzoubi)
Date: Wed, 14 Feb 2007 11:27:32 +1100 (EST)
Subject: [R] (no subject)
Message-ID: <20070214112732.AIJ59465@metis.its.uow.edu.au>

Hello All
I am using R to find the impact of ignoring clustering, when indicated by these tests, on bias, variance and inference of estimates of interest, including population totals and regression coefficients. But when U have used the apVar function using the command
test.lme1$apVar
to find the var-cov matrix I got the follwing error
"Non-positive definite approximate variance-covariance"
I think the problem was with my data which contain the following two columns
m <- 250
g <- as.factor(rep(1:m,each=10))
     y <- rnorm(10*m)
     test <- data.frame(cbind(g,y))
the lme model used was
test.lme1 <- lme (y~1, data = test, random = ~1| g)
This error affects the t-statistic value, which was NA for more than 240 value of the 250 values which I expect to have. Some times all t-stat values are NA and sometimes I don't have  the NA result at all.
I'll send you all of the simulation program which I've used.
I hope you can help to solve this problem
Thanks
LOAI
PhD student 
University of Wollongong
Australia

From sje at mast.queensu.ca  Wed Feb 14 01:39:31 2007
From: sje at mast.queensu.ca (Stephen Bond)
Date: Tue, 13 Feb 2007 18:39:31 -0600
Subject: [R] help with tryCatch
In-Reply-To: <59d7961d0702130802t5cd28a91ia269f2bdbc085a0f@mail.gmail.com>
References: <45D0DF70.1090801@mast.queensu.ca>	
	<59d7961d0702121904n2104d327udd7c964db0048956@mail.gmail.com>	
	<45D1CEB1.9080603@mast.queensu.ca>
	<59d7961d0702130802t5cd28a91ia269f2bdbc085a0f@mail.gmail.com>
Message-ID: <45D25A43.4050203@mast.queensu.ca>

Henrik,

thank you for the reference. Can you please tell me why the following 
does not work?

vec=c("hdfhjfd","jdhfhjfg")    # non-existent file names
catch=function(vec){
  tryCatch({
    ans =NULL;err=NULL;
    for (i in vec) {
      source(i)
      ans=c(ans,i)
    }
  },
  interrupt=function(ex){print(ex)},
  error=function(er){
     print(er)
     cat(i,"\n")
     err=c(err,i)          
  },
  finally={
    cat("finish")
  }
 ) #tryCatch
}
 
catch(vec) # throws an error after the first file and stops there while 
I want it to go through the list and accumulate the nonexistent 
filenames in err.

Thank you
Stephen

Henrik Bengtsson wrote:

> Hi,
>
> google "R tryCatch example" and you'll find:
>
>  http://www.maths.lth.se/help/R/ExceptionHandlingInR/
>
> Hope this helps
>
> Henrik
>
> On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
>
>> Henrik,
>>
>> I had looked at tryCatch before posting the question and asked the
>> question because the help file was not adequate for me. Could you pls
>> provide a sample code of
>> try{ try code}
>> catch(error){catch code}
>>
>> let's say you have a vector of local file names and want to source them
>> encapsulating in a tryCatch to avoid the skipping of all good file names
>> after a bad file name.
>>
>> thank you
>> stephen
>>
>>
>> Henrik Bengtsson wrote:
>>
>> > See ?tryCatch. /Henrik
>> >
>> > On 2/12/07, Stephen Bond <sje at mast.queensu.ca> wrote:
>> >
>> >> Could smb please help with try-catch encapsulating a function for
>> >> downloading. Let's say I have a character vector of symbols and 
>> want to
>> >> download each one and surround by try and catch to be safe
>> >>
>> >> # get.hist.quote() is in library(tseries), but the question does not
>> >> depend on it, I could be sourcing local files instead
>> >>
>> >> ans=null;error=null;
>> >> for ( sym in sym.vec){
>> >> try(ans=cbind(ans,get.hist.quote(sym,start=start))) #accumulate in 
>> a zoo
>> >> matrix
>> >> catch(theurlerror){error=c(error,sym)} #accumulate failed symbols
>> >> }
>> >>
>> >> I know the code above does not work, but it conveys the idea. 
>> tryCatch
>> >> help page says it is similar to Java try-catch, but I know how to 
>> do a
>> >> try-catch in Java and still can't do it in R.
>> >>
>> >> Thank you very much.
>> >> stephen
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>>


From dli at scharp.org  Wed Feb 14 01:44:24 2007
From: dli at scharp.org (Dongfeng LI)
Date: Tue, 13 Feb 2007 16:44:24 -0800
Subject: [R] lattice graphics and source()
Message-ID: <45D25B68.6070005@scharp.org>

Hi,

   I am trying the lattice graphics in R. Let's say, such a little
experiment in file myprog.r:

f <- function(){
  x <- 1:10
  y <- x^2
  xyplot(y ~ x)
}
f()

Then I run the program:
> source("myprog.r")

but nothing happens. Manully run f() at the command line:

> f()

then the figure is shown. This seems to be a bug only associated with
lattice graphics, the old style plots do not exibit these behavior.

   Dongfeng Li


From marc_schwartz at comcast.net  Wed Feb 14 01:55:41 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 13 Feb 2007 18:55:41 -0600
Subject: [R] lattice graphics and source()
In-Reply-To: <45D25B68.6070005@scharp.org>
References: <45D25B68.6070005@scharp.org>
Message-ID: <1171414541.4922.0.camel@Bellerophon>

On Tue, 2007-02-13 at 16:44 -0800, Dongfeng LI wrote:
> Hi,
> 
>    I am trying the lattice graphics in R. Let's say, such a little
> experiment in file myprog.r:
> 
> f <- function(){
>   x <- 1:10
>   y <- x^2
>   xyplot(y ~ x)
> }
> f()
> 
> Then I run the program:
> > source("myprog.r")
> 
> but nothing happens. Manully run f() at the command line:
> 
> > f()
> 
> then the figure is shown. This seems to be a bug only associated with
> lattice graphics, the old style plots do not exibit these behavior.


See the following FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-lattice_002ftrellis-graphics-not-work_003f

HTH,

Marc Schwartz


From dylan.beaudette at gmail.com  Wed Feb 14 02:39:14 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Tue, 13 Feb 2007 17:39:14 -0800
Subject: [R] model diagnostics for logistic regression
Message-ID: <200702131739.15002.dylan.beaudette@gmail.com>

Greetings,

I am using both the lrm() {Design} and glm( , family=binomial()) to perform a 
a logisitic regression in R. Apart from the typical summary() methods, what 
other methods of diagnosing logistic regression models does R provide? i.e. 
plotting an 'lm' object, etc. 

Secondly, is there any facility to calculate the R^{2)_{L} as suggested by 
Menard in "Applied Logistic Regression Analysis" (2002) ?

Any thought would be greatly appreciated.

Cheers,


-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341


From harshal at yahoo-inc.com  Wed Feb 14 02:24:49 2007
From: harshal at yahoo-inc.com (Harshal D Dedhia)
Date: Tue, 13 Feb 2007 17:24:49 -0800
Subject: [R] Character size of labels in heatmap
Message-ID: <959DC0987E39DB43B963EA5ADC9E1A2901D756D7@SNV-XCHMAIL2.xch.corp.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070213/b2c6f6f0/attachment.pl 

From hb at stat.berkeley.edu  Wed Feb 14 02:50:00 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 13 Feb 2007 17:50:00 -0800
Subject: [R] help with tryCatch
In-Reply-To: <45D25A43.4050203@mast.queensu.ca>
References: <45D0DF70.1090801@mast.queensu.ca>
	<59d7961d0702121904n2104d327udd7c964db0048956@mail.gmail.com>
	<45D1CEB1.9080603@mast.queensu.ca>
	<59d7961d0702130802t5cd28a91ia269f2bdbc085a0f@mail.gmail.com>
	<45D25A43.4050203@mast.queensu.ca>
Message-ID: <59d7961d0702131750occ6945dx3ce445aef898deaf@mail.gmail.com>

Put the for loop outside the tryCatch(). /H

On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> Henrik,
>
> thank you for the reference. Can you please tell me why the following
> does not work?
>
> vec=c("hdfhjfd","jdhfhjfg")    # non-existent file names
> catch=function(vec){
>   tryCatch({
>     ans =NULL;err=NULL;
>     for (i in vec) {
>       source(i)
>       ans=c(ans,i)
>     }
>   },
>   interrupt=function(ex){print(ex)},
>   error=function(er){
>      print(er)
>      cat(i,"\n")
>      err=c(err,i)
>   },
>   finally={
>     cat("finish")
>   }
>  ) #tryCatch
> }
>
> catch(vec) # throws an error after the first file and stops there while
> I want it to go through the list and accumulate the nonexistent
> filenames in err.
>
> Thank you
> Stephen
>
> Henrik Bengtsson wrote:
>
> > Hi,
> >
> > google "R tryCatch example" and you'll find:
> >
> >  http://www.maths.lth.se/help/R/ExceptionHandlingInR/
> >
> > Hope this helps
> >
> > Henrik
> >
> > On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> >
> >> Henrik,
> >>
> >> I had looked at tryCatch before posting the question and asked the
> >> question because the help file was not adequate for me. Could you pls
> >> provide a sample code of
> >> try{ try code}
> >> catch(error){catch code}
> >>
> >> let's say you have a vector of local file names and want to source them
> >> encapsulating in a tryCatch to avoid the skipping of all good file names
> >> after a bad file name.
> >>
> >> thank you
> >> stephen
> >>
> >>
> >> Henrik Bengtsson wrote:
> >>
> >> > See ?tryCatch. /Henrik
> >> >
> >> > On 2/12/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> >> >
> >> >> Could smb please help with try-catch encapsulating a function for
> >> >> downloading. Let's say I have a character vector of symbols and
> >> want to
> >> >> download each one and surround by try and catch to be safe
> >> >>
> >> >> # get.hist.quote() is in library(tseries), but the question does not
> >> >> depend on it, I could be sourcing local files instead
> >> >>
> >> >> ans=null;error=null;
> >> >> for ( sym in sym.vec){
> >> >> try(ans=cbind(ans,get.hist.quote(sym,start=start))) #accumulate in
> >> a zoo
> >> >> matrix
> >> >> catch(theurlerror){error=c(error,sym)} #accumulate failed symbols
> >> >> }
> >> >>
> >> >> I know the code above does not work, but it conveys the idea.
> >> tryCatch
> >> >> help page says it is similar to Java try-catch, but I know how to
> >> do a
> >> >> try-catch in Java and still can't do it in R.
> >> >>
> >> >> Thank you very much.
> >> >> stephen
> >> >>
> >> >> ______________________________________________
> >> >> R-help at stat.math.ethz.ch mailing list
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >> http://www.R-project.org/posting-guide.html
> >> >> and provide commented, minimal, self-contained, reproducible code.
> >> >>
> >>
>


From hb at stat.berkeley.edu  Wed Feb 14 02:51:44 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 13 Feb 2007 17:51:44 -0800
Subject: [R] help with tryCatch
In-Reply-To: <59d7961d0702131750occ6945dx3ce445aef898deaf@mail.gmail.com>
References: <45D0DF70.1090801@mast.queensu.ca>
	<59d7961d0702121904n2104d327udd7c964db0048956@mail.gmail.com>
	<45D1CEB1.9080603@mast.queensu.ca>
	<59d7961d0702130802t5cd28a91ia269f2bdbc085a0f@mail.gmail.com>
	<45D25A43.4050203@mast.queensu.ca>
	<59d7961d0702131750occ6945dx3ce445aef898deaf@mail.gmail.com>
Message-ID: <59d7961d0702131751j416e6109t17cca70eb47a0e3@mail.gmail.com>

To be more precise, put the tryCatch() only around the code causing
the problem, i.e. around source().  /H

On 2/13/07, Henrik Bengtsson <hb at stat.berkeley.edu> wrote:
> Put the for loop outside the tryCatch(). /H
>
> On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> > Henrik,
> >
> > thank you for the reference. Can you please tell me why the following
> > does not work?
> >
> > vec=c("hdfhjfd","jdhfhjfg")    # non-existent file names
> > catch=function(vec){
> >   tryCatch({
> >     ans =NULL;err=NULL;
> >     for (i in vec) {
> >       source(i)
> >       ans=c(ans,i)
> >     }
> >   },
> >   interrupt=function(ex){print(ex)},
> >   error=function(er){
> >      print(er)
> >      cat(i,"\n")
> >      err=c(err,i)
> >   },
> >   finally={
> >     cat("finish")
> >   }
> >  ) #tryCatch
> > }
> >
> > catch(vec) # throws an error after the first file and stops there while
> > I want it to go through the list and accumulate the nonexistent
> > filenames in err.
> >
> > Thank you
> > Stephen
> >
> > Henrik Bengtsson wrote:
> >
> > > Hi,
> > >
> > > google "R tryCatch example" and you'll find:
> > >
> > >  http://www.maths.lth.se/help/R/ExceptionHandlingInR/
> > >
> > > Hope this helps
> > >
> > > Henrik
> > >
> > > On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> > >
> > >> Henrik,
> > >>
> > >> I had looked at tryCatch before posting the question and asked the
> > >> question because the help file was not adequate for me. Could you pls
> > >> provide a sample code of
> > >> try{ try code}
> > >> catch(error){catch code}
> > >>
> > >> let's say you have a vector of local file names and want to source them
> > >> encapsulating in a tryCatch to avoid the skipping of all good file names
> > >> after a bad file name.
> > >>
> > >> thank you
> > >> stephen
> > >>
> > >>
> > >> Henrik Bengtsson wrote:
> > >>
> > >> > See ?tryCatch. /Henrik
> > >> >
> > >> > On 2/12/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> > >> >
> > >> >> Could smb please help with try-catch encapsulating a function for
> > >> >> downloading. Let's say I have a character vector of symbols and
> > >> want to
> > >> >> download each one and surround by try and catch to be safe
> > >> >>
> > >> >> # get.hist.quote() is in library(tseries), but the question does not
> > >> >> depend on it, I could be sourcing local files instead
> > >> >>
> > >> >> ans=null;error=null;
> > >> >> for ( sym in sym.vec){
> > >> >> try(ans=cbind(ans,get.hist.quote(sym,start=start))) #accumulate in
> > >> a zoo
> > >> >> matrix
> > >> >> catch(theurlerror){error=c(error,sym)} #accumulate failed symbols
> > >> >> }
> > >> >>
> > >> >> I know the code above does not work, but it conveys the idea.
> > >> tryCatch
> > >> >> help page says it is similar to Java try-catch, but I know how to
> > >> do a
> > >> >> try-catch in Java and still can't do it in R.
> > >> >>
> > >> >> Thank you very much.
> > >> >> stephen
> > >> >>
> > >> >> ______________________________________________
> > >> >> R-help at stat.math.ethz.ch mailing list
> > >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> >> PLEASE do read the posting guide
> > >> >> http://www.R-project.org/posting-guide.html
> > >> >> and provide commented, minimal, self-contained, reproducible code.
> > >> >>
> > >>
> >
>


From f.harrell at vanderbilt.edu  Wed Feb 14 04:15:44 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 13 Feb 2007 21:15:44 -0600
Subject: [R] model diagnostics for logistic regression
In-Reply-To: <200702131739.15002.dylan.beaudette@gmail.com>
References: <200702131739.15002.dylan.beaudette@gmail.com>
Message-ID: <45D27EE0.8040109@vanderbilt.edu>

Dylan Beaudette wrote:
> Greetings,
> 
> I am using both the lrm() {Design} and glm( , family=binomial()) to perform a 
> a logisitic regression in R. Apart from the typical summary() methods, what 
> other methods of diagnosing logistic regression models does R provide? i.e. 
> plotting an 'lm' object, etc. 

The best way of checking fit in a lrm is to do directed regression tests 
(e.g. extend linear effects into regression splines; interactions).  An 
omnibus test is in residuals.lrm as is partial residual plots.

> 
> Secondly, is there any facility to calculate the R^{2)_{L} as suggested by 
> Menard in "Applied Logistic Regression Analysis" (2002) ?

Don't know that paper but lrm has various indexes including 
Nagelkerke-Maddala R^2.

Frank

> 
> Any thought would be greatly appreciated.
> 
> Cheers,
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ivowel at gmail.com  Wed Feb 14 04:21:07 2007
From: ivowel at gmail.com (ivo welch)
Date: Tue, 13 Feb 2007 22:21:07 -0500
Subject: [R] linear coefficient combination stderr?
Message-ID: <50d1c22d0702131921w53472a48gb9446149a85d0203@mail.gmail.com>

dear r-experts---I have scrounged around in google (searching r-help)
for the really obvious, but failed.  could someone please point me to
whatever function computes the standard error of a linear combination
of the coefficients?

m=lm( y ~ x1 + x2 + x3 );
t.stat= (coef(m)[2]+coef(m)[3])) / mystery.function( m, x2 + x3 );

obviously, this does not work, but hopefully explains my intent.
quick pointer appreciated.

regards,

/iaw


From shirley0818 at gmail.com  Wed Feb 14 04:36:36 2007
From: shirley0818 at gmail.com (shirley zhang)
Date: Tue, 13 Feb 2007 22:36:36 -0500
Subject: [R] nested model: lme, aov and LSMeans
Message-ID: <6fb73d020702131936o5ba77e35ydceeeb36f973a567@mail.gmail.com>

I'm working with a nested model (mixed).

I have four factors: Patients, Tissue, sex, and tissue_stage.

Totally I have 10 patients, for each patient, there are 2 tissues
(Cancer vs. Normal).

I think Tissue and sex are fixed. Patient is nested in sex,Tissue is
nested in patient,  and tissue_stage is nested in Tissue.


I tried aov and lme as the following,

> aov(gene ~ tissue + gender + patients%in%gender + stage%in%tissue
>lme(gene ~ tissue + gender, random = list(patients = ~1 , stage = ~ 1))


I got results from aov, but I got error message from lme which is
false convergence (8). so I could not compare the results of aov with
lme.

Could anybody tell me whether I use the correct command? How can I get
LSMeans for Cancer vs. Normal in R ( I know it is easy in SAS)?

Thanks,

Shirley


From amnakhan493 at gmail.com  Wed Feb 14 05:37:23 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Wed, 14 Feb 2007 09:37:23 +0500
Subject: [R] Legend function
Message-ID: <3ffd3bb60702132037i1f590ffahd92bb8176d432b46@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/36117f18/attachment.pl 

From h.wickham at gmail.com  Wed Feb 14 06:07:19 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 14 Feb 2007 18:07:19 +1300
Subject: [R] Can a data.frame column contain lists/arrays?
In-Reply-To: <6addebae0702122029h68e0b4d0ub64f3444a8704409@mail.gmail.com>
References: <6addebae0702122029h68e0b4d0ub64f3444a8704409@mail.gmail.com>
Message-ID: <f8e6ff050702132107q5da7a9dcnd25b80fdc826ffb8@mail.gmail.com>

> I'd like to have a data.frame structured something like the following:
>
> d <- data.frame (
>    x=list( c(1,2), c(5,2), c(9,1) ),
>    y=c( 1, -1, -1)
> )
>
> The reason is this: 'd' is the training data for a machine learning
> algorithm.  d$x is the independent data, and d$y is the dependent
> data.
>
> In general my machine learning code will work where each element of
> d$x is a vector of one or more real numbers.  So for instance, the
> same code should work when d$x[1] = 42, or when d$x[1] = (42, 3, 5).
> All that matters is that all element within d$x are lists/vectors of
> the same length.
>
> Does anyone know if/how I can get a data.frame set up like that?

You certainly can, although it requires a little work. A data.frame is
a list of vectors, each of the same length, and a list is a type of
vector.   I use this structure fairly often in my own work, and find
it quite useful.

However, the data.frame and as.data.frame functions try to be helpful
at converting lists to regular columns so you must first create your
data.frame and then add the column which is a list:

> df <- data.frame(a=1:2)
> df$b <- list(1:5, 6:10)
> df
  a              b
1 1  1, 2, 3, 4, 5
2 2 6, 7, 8, 9, 10

> str(df)
'data.frame':   2 obs. of  2 variables:
 $ a: int  1 2
 $ b:List of 2
  ..$ : int  1 2 3 4 5
  ..$ : int  6 7 8 9 10

but

> data.frame(a=1:2, b = list(1:5, 6:10))
Error in data.frame(a = 1:2, b = list(1:5, 6:10)) :
        arguments imply differing number of rows: 2, 5

Note that it is possible to create structures like this which do not
print, but still contain valid objects:

> df$b <- list(lm(mpg~wt, data=mtcars), lm(mpg~vs, data=mtcars))
> df
Error in unlist(x, recursive, use.names) :
        argument not a list

> summary(df[1,2])
     Length Class Mode
[1,] 12     lm    list
> summary(df[1,2][[1]])

Call:
lm(formula = mpg ~ wt, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max
-4.5432 -2.3647 -0.1252  1.4096  6.8727

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  37.2851     1.8776  19.858  < 2e-16 ***
wt           -5.3445     0.5591  -9.559 1.29e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.046 on 30 degrees of freedom
Multiple R-Squared: 0.7528,     Adjusted R-squared: 0.7446
F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10



There are some functions in the reshape package, in particular stamp,
which make this a bit easier for particular types of data.

Regards,

Hadley


From Soren.Hojsgaard at agrsci.dk  Wed Feb 14 06:15:33 2007
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Wed, 14 Feb 2007 06:15:33 +0100
Subject: [R] linear coefficient combination stderr?
References: <50d1c22d0702131921w53472a48gb9446149a85d0203@mail.gmail.com>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03878606@DJFPOST01.djf.agrsci.dk>

Try the esticon function in the doBy package.
Regards
S?ren

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p? vegne af ivo welch
Sendt: on 14-02-2007 04:21
Til: r-help
Emne: [R] linear coefficient combination stderr?



dear r-experts---I have scrounged around in google (searching r-help)
for the really obvious, but failed.  could someone please point me to
whatever function computes the standard error of a linear combination
of the coefficients?

m=lm( y ~ x1 + x2 + x3 );
t.stat= (coef(m)[2]+coef(m)[3])) / mystery.function( m, x2 + x3 );

obviously, this does not work, but hopefully explains my intent.
quick pointer appreciated.

regards,

/iaw

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From shubhak at ambaresearch.com  Wed Feb 14 06:19:35 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 14 Feb 2007 10:49:35 +0530
Subject: [R] Fatigued R
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3FEF638@BAN-MAILSRV03.Amba.com>

Hi all,

So, is there any method to make bbcomm.exe to sleep for sometime...?

-----Original Message-----
From: davidr at rhotrading.com [mailto:davidr at rhotrading.com] 
Sent: Wednesday, February 14, 2007 12:42 AM
To: bogdan romocea; Shubha Vishwanath Karanth
Cc: r-help
Subject: RE: [R] Fatigued R

I can't be sure what is happening to Shubhak, but I know that
the Bloomberg server bbcomm.exe throws unhandled exceptions in a 
unreproducable and uncatchable way, somewhere in their tcp code. 

It is one of the big frustrations for me in my work. 

Shubhak, I think you will just have to look carefully at what is
returned,
as Bogdan suggested in general, and take action based on that. It may be

that try[Catch] will catch some problems, but other times, you will just
get 
something that is not right, but you can tell since it has the wrong 
structure.
The bbcomm.exe essentially crashes and gets restarted automatically, so
the 
next time you try to get something, it works; so sleeping after an error
is 
a good idea.

HTH,
David

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Tuesday, February 13, 2007 10:49 AM
To: shubhak at ambaresearch.com
Cc: r-help
Subject: Re: [R] Fatigued R

The problem with your code is that it doesn't check for errors. See
?try, ?tryCatch. For example:

my.download <- function(forloop) {
  notok <- vector()
  for (i in forloop) {
    cdaily <- try(blpGetData(...))
    if (class(cdaily) == "try-error") {
      notok <- c(notok, i)
    } else {
      #proceed as usual
    }
  }
  notok
}

forloop <- 1:x
repeat {
  ddata <- my.download(forloop)
  forloop <- ddata
  if (length(forloop) == 0) break  #no download errors; stop
}


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shubha
> Vishwanath Karanth
> Sent: Tuesday, February 13, 2007 10:06 AM
> To: ONKELINX, Thierry; r-help at stat.math.ethz.ch;
> sarah.goslee at gmail.com
> Subject: Re: [R] Fatigued R
>
> Hi all, thanks for your reply....
>
> But I have to make one thing clear that there are no errors in
> programming...I assure that to you, because I have extracted the data
> many times from the same program...
>
> The problem is with the connection of R with Bloomberg, sometimes the
> data is not fetched at all and so I get the below errors...It
> is mainly
> due to some network jam problems and all...
>
> Could anyone suggest me how to refresh R, or how to always make sure
> that the data is downloaded without any errors for each loop? I tried
> with Sys.sleep() to give some free time to R...but it is not
> successful
> always...
>
> Could anybody help me out?
>
> Thank you,
> Shubha
>
> -----Original Message-----
> From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
> Sent: Tuesday, February 13, 2007 7:56 PM
> To: Shubha Vishwanath Karanth; r-help at stat.math.ethz.ch
> Subject: RE: [R] Fatigued R
>
> Dear Shubha,
>
> The error message tells you that the error occurs in the line:
> 	merge(d_mer,cdaily)
> And the problem is that d_mer and cdaily have a different class. It
> looks as you need to convert cdaily to the correct class
> (same class as
> d_mer).
> Don't forget to use traceback() when debugging your program.
>
> You code could use some tweaking. Don't be afraid to add spaces and
> indentation. That will make you code a lot more readable.
> I noticed that you have defined some variables within your function
> (lent, t). This might lead to strange behaviour of your
> function, as it
> will use the global variables. Giving a variable the same name as a
> function (t) is confusing. t[2] and t(2) look very similar but do
> something very different.
> Sometimes it pays off to covert for-loop in apply-type functions.
>
> Cheers,
>
> Thierry
> --------------------------------------------------------------
> ----------
> ----
>
> ir. Thierry Onkelinx
>
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
>
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
>
> Gaverstraat 4
>
> 9500 Geraardsbergen
>
> Belgium
>
> tel. + 32 54/436 185
>
> Thierry.Onkelinx at inbo.be
>
> www.inbo.be
>
>
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
>
> A statistical analysis, properly conducted, is a delicate
> dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
>
> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens Shubha Vishwanath
> Karanth
> Verzonden: dinsdag 13 februari 2007 14:51
> Aan: Sarah Goslee; r-help at stat.math.ethz.ch
> Onderwerp: Re: [R] Fatigued R
>
> Ohkkk....I will try to do that now....
>
> This is my download function...
>
>
> download<-function(fil)
> {
> con<-blpConnect(show.days=show_day, na.action=na_action,
> periodicity=periodicity)
> for(i in 1:lent)
> {
> Sys.sleep(3)
> cdaily<-blpGetData(con,unique(t[,i]),fil,start=as.chron(as.Dat
e("1/1/199
> 6", "%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))
> #Sys.sleep(3)
> if(!exists("d_mer")) d_mer=cdaily else d_mer=merge(d_mer,cdaily)
> rm(cdaily)
> }
> dat<-data.frame(Date=index(d_mer),d_mer)
> dat$ABCDEFG=NULL
> path1<-paste("D:\\SAS\\Project2\\Daily\\",fil,"_root.sas7bdat",sep="")
> path2<-paste("D:\\SAS\\Project2\\Daily\\CODEFILES\\",fil,".sas
> ",sep="")
> sasname<-paste(x1,fil,"'",sep="")
> write.foreign(dat,path1,path2,package="SAS",dataname=sasname)
> blpDisconnect(con)
> }
>
> for(j in 1:lenf)
> {
> fname<-paste("D:\\SAS\\Project2\\Daily\\",filename[j],"_root.s
as7bdat",s
> ep="")
> Sys.sleep(600)
> if(!file.exists(fname)) download(fil=filename[j])
> }
>
> And lent=58 and lenf=8... 8 text files will be generated in
> this process
> if the program would have run properly, and each would be of
> size 4,000
> KB.
>
> The error message I get if the program is not run is:
>
> Error in dimnames(x) <- dn : length of 'dimnames' [2] not
> equal to array
> extent
> In addition: Warning message:
> Index vectors are of different classes: chron chron dates in:
> merge(d_mer,cdaily)
>
>
>
> Please could any one help on this?
>
> -----Original Message-----
> From: Sarah Goslee [mailto:sarah.goslee at gmail.com]
> Sent: Tuesday, February 13, 2007 7:09 PM
> To: Shubha Vishwanath Karanth
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Fatigued R
>
> Hi Shubha,
>
> Perhaps you haven't gotten any help because you haven't provided a
> reproducible example, or even told us what you are trying to do
> (specifically)
> or what errors you are receiving. Frankly, your problem statement
> doesn't
> make any sense to me, and I can't provide advice without more
> information.
>
> As the footer of every email says:
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> Sarah
>
> On 2/13/07, Shubha Vishwanath Karanth
> <shubhak at ambaresearch.com> wrote:
> > Hi R,
> >
> >
> >
> > Please solve my problem...........
> >
> >
> >
> > I am extracting Bloomberg data from R, in a loop. R is getting
> fatigued
> > by doing this process and gives some errors. I introduced sleep
> > function. Doing this sometimes I get the results and
> sometimes not. I
> > even noticed that if I give complete rest for R (don't open
> R window)
> > for 1 day and then run my code with the sleep function, then the
> program
> > works. But if I keep on doing this with on R, repeatedly I
> get errors.
> >
> >
> >
> > Please can anyone do something for this? Is there any function of
> > refreshing R completely.....Or any other techniques?...I am really
> > getting bugged with this.......
> >
> >
> >
> > Thanks,
> >
> > Shubha
>
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Abhijit.Dasgupta at mail.jci.tju.edu  Wed Feb 14 06:43:09 2007
From: Abhijit.Dasgupta at mail.jci.tju.edu (Abhijit Dasgupta)
Date: Wed, 14 Feb 2007 00:43:09 -0500
Subject: [R] linear coefficient combination stderr?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC03878606@DJFPOST01.djf.agrsci.dk>
References: <50d1c22d0702131921w53472a48gb9446149a85d0203@mail.gmail.com>
	<C83C5E3DEEE97E498B74729A33F6EAEC03878606@DJFPOST01.djf.agrsci.dk>
Message-ID: <45D2A16D.6060104@mail.jci.tju.edu>

look at the function "estimable" in the package gmodels. This does the 
t.stat that you mention below.

Otherwise, if you wanted to do this by hand, :

beta = m$coef
V = summary(m)$cov.unscaled
sigma = summary(m)$sigma

covBeta = V*sigma^2

Now if your contrast is a%*%beta, then its se is 
sqrt(t(a)%*%covBeta%*%a) and so the t.stat below is

t.stat = (a%*%beta)/sqrt(t(a)%*%covBeta%*%a)
or, in your particular case

t.stat = (beta[2]+beta[3])/sqrt(t(c(0,1,1))%*%covBeta%*%c(0,1,1))

Abhijit

S?ren H?jsgaard wrote:
> Try the esticon function in the doBy package.
> Regards
> S?ren
>
> ________________________________
>
> Fra: r-help-bounces at stat.math.ethz.ch p? vegne af ivo welch
> Sendt: on 14-02-2007 04:21
> Til: r-help
> Emne: [R] linear coefficient combination stderr?
>
>
>
> dear r-experts---I have scrounged around in google (searching r-help)
> for the really obvious, but failed.  could someone please point me to
> whatever function computes the standard error of a linear combination
> of the coefficients?
>
> m=lm( y ~ x1 + x2 + x3 );
> t.stat= (coef(m)[2]+coef(m)[3])) / mystery.function( m, x2 + x3 );
>
> obviously, this does not work, but hopefully explains my intent.
> quick pointer appreciated.
>
> regards,
>
> /iaw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From adik at ilovebacon.org  Wed Feb 14 07:57:04 2007
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Tue, 13 Feb 2007 22:57:04 -0800 (PST)
Subject: [R] se.contrast confusion
Message-ID: <Pine.LNX.4.64.0702132212090.2541@parser.ilovebacon.org>

Hello,

I've got what I'd expect to be a pretty simple issue: I fit an aov object
using multiple error strata, and would like some significance tests for the
contrasts I specified.

In this contrived example, I model some test score as the interaction of a
subject's gender and two emotion variables (angry, happy, neutral), measured
at entry to the experiment (entry) and later manipulated (manip). This is a
fully within-subjects design.

Polynomial contrasts examine the effect of increasing emotional state

levels(entry) and (manip) are "a" "n" "h", so I set
contrasts(entry) <- contrasts(manip) <- contr.poly(3)
(I am led to believe that this is equivalent to specifying
contrasts=list(entry=contr.poly(3), manip=contr.poly(3)) in the aov call)

...and then fit the model:

fit <- aov(score ~ gender*entry*manip + Error(subject/(entry*manip))

...at this point, summary(fit) tests all the main effects and interactions
I'm interested in, and coef(fit) has the coefficients for all the contrasts
and their interactions. The question, however, is how to test the
significance of the contrasts, which is to say, compute the standard errors
and apply them to coef(fit) in a meaningful way.

The se.contrasts() function looks quite appealing, though it appears to
require me to respecify the contrasts...in both a contrast.obj and a coef.
It is not at all clear from the instrutions what contrast.obj is, especially
given that I have already specified the contrasts and they are already
represented in coef(fit). I may be missing something here.

Could someone suggest a way to go from coef(fit) to a table like
summary(fit) which tests the single-df contrasts (and interactions
therebetween) which I specified? Or a way to compute the standard errors for
these contrasts?

Many thanks,
Adam Kramer


From ripley at stats.ox.ac.uk  Wed Feb 14 08:39:46 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 14 Feb 2007 07:39:46 +0000 (GMT)
Subject: [R] se.contrast confusion
In-Reply-To: <Pine.LNX.4.64.0702132212090.2541@parser.ilovebacon.org>
References: <Pine.LNX.4.64.0702132212090.2541@parser.ilovebacon.org>
Message-ID: <Pine.LNX.4.64.0702140709580.2600@auk.stats>

On Tue, 13 Feb 2007, Adam D. I. Kramer wrote:

> Hello,
>
> I've got what I'd expect to be a pretty simple issue: I fit an aov object
> using multiple error strata, and would like some significance tests for the
> contrasts I specified.

It _is_ covered on the relevant help page.  See the 'split' argument in 
?summary.aov, and the examples there.

> In this contrived example, I model some test score as the interaction of a
> subject's gender and two emotion variables (angry, happy, neutral), measured
> at entry to the experiment (entry) and later manipulated (manip). This is a
> fully within-subjects design.
>
> Polynomial contrasts examine the effect of increasing emotional state
>
> levels(entry) and (manip) are "a" "n" "h", so I set
> contrasts(entry) <- contrasts(manip) <- contr.poly(3)
> (I am led to believe that this is equivalent to specifying
> contrasts=list(entry=contr.poly(3), manip=contr.poly(3)) in the aov call)
>
> ...and then fit the model:
>
> fit <- aov(score ~ gender*entry*manip + Error(subject/(entry*manip))
>
> ...at this point, summary(fit) tests all the main effects and interactions
> I'm interested in, and coef(fit) has the coefficients for all the contrasts
> and their interactions. The question, however, is how to test the
> significance of the contrasts, which is to say, compute the standard errors
> and apply them to coef(fit) in a meaningful way.
>
> The se.contrasts() function looks quite appealing, though it appears to

Do you mean se.contrast?  Or where did you find this function?

> require me to respecify the contrasts...in both a contrast.obj and a coef.
> It is not at all clear from the instrutions what contrast.obj is, especially
> given that I have already specified the contrasts and they are already
> represented in coef(fit). I may be missing something here.

You seem to be missing the ideas of 'optional' and 'default argument', 
as well as (I suspect) the correct name of the function.

You do need to specify _which_ contrasts you want, and these can be 
different from those in the fit (and often will not be all of those in the 
fit).  Looking at the examples would have made clear that 'coef' is 
optional.

> Could someone suggest a way to go from coef(fit) to a table like
> summary(fit) which tests the single-df contrasts (and interactions
> therebetween) which I specified? Or a way to compute the standard errors for
> these contrasts?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jarioksa at sun3.oulu.fi  Wed Feb 14 08:53:12 2007
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Wed, 14 Feb 2007 09:53:12 +0200
Subject: [R]  isoMDS vs. other non-metric non-R routines
Message-ID: <1171439592.26953.17.camel@biol102145.oulu.fi>

philip.leifeld at uni-konstanz.de wrote:
> This was my initial call:
> 
> mds <- isoMDS(dist, y = cmdscale(dist, k = 2), k=2, tol = 1e-3, maxit 
> = 500)
> 
> I played around a little bit with tol and maxit (adding some 
> zeros...) and increased the number of dimensions, but it did not 
> change the results significantly. Using initMDS did not improve the 
> result either. Unfortunately, my data set is too large to be 
> displayed here. Any other ideas? My stress value is still 1.5 as much 
> as in other implementations of NMDS.
> 
It is really difficult to believe that isoMDS would work so completely
differently from other implementations. I guess you already tried
tol=1e-7? After this, a radical trick is to give the Minissa result as
the starting configuration, and see if you stay there and  get the same
stress as Minissa reported. You should. In particular, if you iterate
away from the starting configuration, then the starting configuration
was not as good as you assumed.  If this happens, it would be time to
check the data. I assume you have read in dissimilarities from external
files, and surprises do happen (it makes sense to check the data
anyway).

Increasing the number of dimensions should not get you into a similar
solution as with some other implementation using a lower number of
dimensions.

About the problems Christian Hennig mentioned: My interpretation of his
message was that he was not concerned about isoMDS in particular but
about NMDS in general (but he will correct me if my interpretation was
wrong). I can imagine cases where non-metric solution works badly, in
particular with small data sets. However, that should concern all
implementations similarly, and probably it should be visible in Shepard
plots (see isoMDS help). 

Cheers, Jari Oksanen


From phdhwang at gmail.com  Wed Feb 14 08:56:11 2007
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Wed, 14 Feb 2007 16:56:11 +0900
Subject: [R] Any packages for conducting AHP( Analytic Hierarchy Process)
	data
Message-ID: <b040cbb00702132356m55070e13tb5ba29c3b142589f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/77f607ea/attachment.pl 

From commandeur.edwin at gmail.com  Wed Feb 14 09:48:01 2007
From: commandeur.edwin at gmail.com (Edwin Commandeur)
Date: Wed, 14 Feb 2007 09:48:01 +0100
Subject: [R] how to report logistic regression results
Message-ID: <a09537370702140048l68a99f51h7dc4eec9af557ca7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/918a5adc/attachment.pl 

From adik at ilovebacon.org  Wed Feb 14 10:27:46 2007
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Wed, 14 Feb 2007 01:27:46 -0800 (PST)
Subject: [R] se.contrast confusion
In-Reply-To: <Pine.LNX.4.64.0702140709580.2600@auk.stats>
References: <Pine.LNX.4.64.0702132212090.2541@parser.ilovebacon.org>
	<Pine.LNX.4.64.0702140709580.2600@auk.stats>
Message-ID: <Pine.LNX.4.64.0702140027540.2541@parser.ilovebacon.org>


On Wed, 14 Feb 2007, Prof Brian Ripley wrote:

>> I've got what I'd expect to be a pretty simple issue: I fit an aov object
>> using multiple error strata, and would like some significance tests for the
>> contrasts I specified.
>
> It _is_ covered on the relevant help page.  See the 'split' argument in
> ?summary.aov, and the examples there.

While I am a bit embarassed that I did not fully explore this possibility,
it still does not quite do what I hoped, but perhaps is close enough. The
remaining trouble:

summary(fit,split=list(manip=list(L=1,Q=2),entry=list(L=1,Q=2)))

...describes tests of two contrasts of the manip variable, but they are not
the contrasts I specified. For instance, if I set

contrasts(lab6.3$manip) <- contr.poly(3)
or
... <- contr.helmert(3)

...the contrasts tested in the summary are the same...even if I refit the
model using the same syntax (fit <- aov(...)). If, however, I specify
contrasts=list(manip=contr.poly(3), entry=contr.poly(3)) as an argument to
aov, then the output using split is correct.

I was under the impression that "contrasts(dataframe$factor) <- matrix"
format was an acceptable way to define the contrasts which are used for that
factor, by default...is this mistaken? It is attractive to be able to set
contrasts for factors which are used across multiple analyses, with the
"contrasts" argument to aov overriding them. The contrasts() function's help
implies that this is how contrasts function.

>> The se.contrasts() function looks quite appealing, though it appears to
>
> Do you mean se.contrast?  Or where did you find this function?

Yes, that is what I meant. Too much typing of contrasts(manip) confuses the
fingers. I am not complaining about functions which do not exist...apologies
for the lack of specificity.

>> require me to respecify the contrasts...in both a contrast.obj and a
>> coef. It is not at all clear from the instrutions what contrast.obj is,
>> especially given that I have already specified the contrasts and they are
>> already represented in coef(fit). I may be missing something here.
>
> You seem to be missing the ideas of 'optional' and 'default argument', as 
> well as (I suspect) the correct name of the function.

I think I understood these concepts, and was suffering a deeper
misunderstanding regarding the purpose of "split." It appears nearly exactly
what I want, and I thank you for pointing it out to me.

> You do need to specify _which_ contrasts you want, and these can be
> different from those in the fit (and often will not be all of those in the
> fit).  Looking at the examples would have made clear that 'coef' is
> optional.

I saw that coef is optional when specifying a matrix as the contrast.obj,
though I did not understand the use of contrast.obj, especially for cases
with more than one variable. I'm still not quite clear, but the examples
make a lot more sense in this context.

In any case, it appears that se.contrast() is not what I want, and that more
careful examination of the "split" argument to summary.aov is what I was
missing.

Thanks again,
--
Adam D. I. Kramer
Ph.D. Student, Social and Personality Psychology
University of Oregon
adik at uoregon.edu


From feanor0 at hotmail.com  Wed Feb 14 10:32:17 2007
From: feanor0 at hotmail.com (Murali Menon)
Date: Wed, 14 Feb 2007 09:32:17 +0000
Subject: [R] Computing stats on common parts of multiple dataframes
In-Reply-To: <45D222AD.7080402@biostat.wisc.edu>
Message-ID: <BAY113-F350F3A500916EFD889BD52EE970@phx.gbl>

Hi Gabor, Eric,

Your suggestions are certainly more elegant (and involve less typing) than 
my for-loop. Thanks!

Patrick, I see what you mean regarding dates. The problem is that I'm doing 
all sorts of manipulations on the original data, which work best on numeric 
matrices, and the dates just mess it all up. I could, of course, reintroduce 
the date columns for the issue at hand, and do an intersect.

Best wishes,
Murali



>From: Erik Iverson <iverson at biostat.wisc.edu>
>To: Murali Menon <feanor0 at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Computing stats on common parts of multiple dataframes
>Date: Tue, 13 Feb 2007 14:42:21 -0600
>
>Murali -
>
>I've come up with something that might with work, with gratutious use of 
>the *apply functions.  See ?apply, ?lappy, and ?mapply for how this would 
>work.  Basically, just set my.list equal to a list of data.frames  you 
>would like included.  I made this to work with matrices first, so it does 
>use as.matrix() in my function.  Also, this could be turned into a general 
>function so that you could specify a different function other than 
>"median".
>
>#Make my.list equal to a list of dataframes you want
>my.list <- list(df1,df2)
>
>#What's the shortest?
>minrow <- min(sapply(my.list,nrow))
>#Chop all to the shortest
>tmp <- lapply(my.list, function(x) x[(nrow(x)-(minrow-1)):nrow(x),])
>
>#Do the computation, could change median to mean, or a user defined
>#function
>matrix(apply(mapply("[",lapply(tmp,as.matrix), 
>MoreArgs=list(1:(minrow*2))), 1, median),
>        ncol=2)
>
>HTH, whether or not this is any "better" than your for loop solution is 
>left up to you.
>
>Erik
>
>
>Murali Menon wrote:
>>Folks,
>>
>>I have three dataframes storing some information about
>>two currency pairs, as follows:
>>
>>R> a
>>
>>EUR-USD    NOK-SEK
>>1.23    1.33
>>1.22    1.43
>>1.26    1.42
>>1.24    1.50
>>1.21    1.36
>>1.26    1.60
>>1.29    1.44
>>1.25    1.36
>>1.27    1.39
>>1.23    1.48
>>1.22    1.26
>>1.24    1.29
>>1.27    1.57
>>1.21    1.55
>>1.23    1.35
>>1.25    1.41
>>1.25    1.30
>>1.23    1.11
>>1.28    1.37
>>1.27    1.23
>>
>>
>>
>>R> b
>>EUR-USD    NOK-SEK
>>1.23    1.22
>>1.21    1.36
>>1.28    1.61
>>1.23    1.34
>>1.21    1.22
>>
>>
>>
>>R> d
>>
>>EUR-USD    NOK-SEK
>>1.27    1.39
>>1.23    1.48
>>1.22    1.26
>>1.24    1.29
>>1.27    1.57
>>1.21    1.55
>>1.23    1.35
>>1.25    1.41
>>1.25    1.33
>>1.23    1.11
>>1.28    1.37
>>1.27    1.23
>>
>>The twist is that these entries correspond to dates where the
>>*last* rows in each frame are today's entries, and so on
>>backwards in time.
>>
>>I would like to create a matrix of medians (a median for each row
>>and for each currency pair), but only for those rows where all
>>dataframes have entries.
>>
>>My answer in this case should look like:
>>
>>EUR-USD    NOK-SEK
>>
>>1.25    1.41
>>1.25    1.33
>>1.23    1.11
>>1.28    1.37
>>1.27    1.23
>>
>>where the last EUR-USD entry = median(1.27, 1.21, 1.27), etc.
>>
>>Notice that the output is of the same dimensions as the smallest dataframe
>>(in this case 'b').
>>
>>I can do it in a clumsy fashion by first obtaining the number
>>of rows in the smallest matrix, chopping off the top rows
>>of the other matrices to reduce them this size, then doing a
>>for-loop across each currency pair, row-wise, to create a
>>3-vector which I then apply median() on.
>>
>>Surely there's a better way to do this?
>>
>>Please advise.
>>
>>Thanks,
>>
>>Murali Menon
>>
>>_________________________________________________________________
>>Valentine?s Day -- Shop for gifts that spell L-O-V-E at MSN Shopping
>>
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.

_________________________________________________________________
Turn searches into helpful donations. Make your search count.


From phdhwang at gmail.com  Wed Feb 14 10:47:56 2007
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Wed, 14 Feb 2007 18:47:56 +0900
Subject: [R] Are there any packages or methods for running data from
	AHP(Analytic Hierarchy Process) survey?
Message-ID: <b040cbb00702140147g118153eahc592f93e1c95a073@mail.gmail.com>

Hi, R Lovers!

I have some survey data. I'd like to run R or R packages for
processing data inputted
from AHP(Analytic Hierarchy Process) survey.

Are there any R packages or subsititues for running data from AHP survey.

Thanks in advance,

-- 
Kum-Hoe Hwang, Ph.D.Phone : 82-31-250-3516Email : phdhwang at gmail.com


From roderick.castillo at metanomics.de  Wed Feb 14 11:18:58 2007
From: roderick.castillo at metanomics.de (roderick.castillo at metanomics.de)
Date: Wed, 14 Feb 2007 11:18:58 +0100
Subject: [R] RE3:  Suddenly "Subscript out of bounds"
In-Reply-To: <2E9C414912813E4EB981326983E0A1040293DF59@inboexch.inbo.be>
Message-ID: <OF900A422D.7B4511CF-ONC1257282.0034C413-C1257282.0038B776@basf-c-s.be>


Hello Thierry

Sorry for my assumption about automatically updating R itself.

Your question is a reasonable one. I have to update the modules
regularly because the users urge me to do so. Updating R itself
means more work, specially if you have to recompile all modules!

Of course you can just move the existing module files to the library
location of the new version (or relink it), but I don't know if this
procedure
works "officially" or just by chance, and secondly, self-installed module
files are mixed up with R own core modules, so I wouln't know how to
separate them clean and sure.

I am open for any ideas!

Bye
Rick





                                                                           
             "ONKELINX,                                                    
             Thierry"                                                      
             <Thierry.ONKELINX                                          An 
             @inbo.be>                  <roderick.castillo at metanomics.de>  
                                                                     Kopie 
             13.02.2007 18:07           <r-help at stat.math.ethz.ch>         
                                                                     Thema 
                                        RE: RE2: [R] Suddenly "Subscript   
                                        out of bounds"                     
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Roderick,

I'm not suggesting that you need a script to update R automatically. I
don't think it is possible to do that. I just was wondering why you are
so eager to update all the packages daily, but still are working with an
outdated version of R.
Myself, I tend to check the R lists for new updates on R and it's
packages.

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx at inbo.be

www.inbo.be



Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: roderick.castillo at metanomics.de
[mailto:roderick.castillo at metanomics.de]
Verzonden: dinsdag 13 februari 2007 18:04
Aan: ONKELINX, Thierry
CC: r-help at stat.math.ethz.ch
Onderwerp: RE2: [R] Suddenly "Subscript out of bounds"


If you tell me how to update R itself automatically, I will go for your
advice.
I am not aware of any method to do it...
Bye
Rick






             "ONKELINX,

             Thierry"

             <Thierry.ONKELINX
An
             @inbo.be>
<roderick.castillo at metanomics.de>

Kopie
             13.02.2007 15:29


Thema
                                        RE: [R] Suddenly "Subscript out
of
                                        bounds"

















You update all packages but not R itself?

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx at inbo.be

www.inbo.be



Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] Namens
roderick.castillo at metanomics.de
Verzonden: dinsdag 13 februari 2007 15:15
Aan: r-help at stat.math.ethz.ch
Onderwerp: [R] Suddenly "Subscript out of bounds"


Hello

Using R Version 2.3.1 I have setup a cronjob to update packages,
which worked successfully almost a year (it was called daily).
Basically, it runs like this:

Sys.getenv("http_proxy")
update.packages(ask=F,repos="http://cran.r-project.org")

(the http_proxy environment variable is set prior to the call).

All of a sudden, I started to get this error:

Error in inherits(x,  "factor") :  subscript out of bounds

I have no clue about what this means. Is "factor" a buggy package?

Clearly, that kind of things can happen when you just update things
automatically...

Any help with be appreciated!

Bye
Rick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From samay.sar at gmail.com  Wed Feb 14 11:40:38 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Wed, 14 Feb 2007 16:10:38 +0530
Subject: [R] Is this a correct forum to discuss basic R problem?
Message-ID: <d4327f7e0702140240r6470bf56y8235941705570ef5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/1e79a14f/attachment.pl 

From wwwhsd at gmail.com  Wed Feb 14 11:51:32 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 14 Feb 2007 07:51:32 -0300
Subject: [R] Is this a correct forum to discuss basic R problem?
In-Reply-To: <d4327f7e0702140240r6470bf56y8235941705570ef5@mail.gmail.com>
References: <d4327f7e0702140240r6470bf56y8235941705570ef5@mail.gmail.com>
Message-ID: <da79af330702140251s2e123c2qfbb1a854275d571@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/77fa6e73/attachment.pl 

From Thierry.ONKELINX at inbo.be  Wed Feb 14 11:51:36 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 14 Feb 2007 11:51:36 +0100
Subject: [R] Is this a correct forum to discuss basic R problem?
In-Reply-To: <d4327f7e0702140240r6470bf56y8235941705570ef5@mail.gmail.com>
Message-ID: <2E9C414912813E4EB981326983E0A1040293E1C1@inboexch.inbo.be>

?aggregate
?by

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens d. sarthi maheshwari
Verzonden: woensdag 14 februari 2007 11:41
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Is this a correct forum to discuss basic R problem?

Hi

Sorry if this is a wrong post in the forum. Please suggest if this is a
correct forum to discuss R related basic problem.

I wanted to perform the following task by using R:

e.g.
 input data.frame
x                  y
============
a                10
b                20
a                10
a                10
b                15
b                15
b                20

In o/p i need

x            y
=========
a           30
b           70

Currently i am storing the data.frame as a database table by using
sqlSave.
Then I am retrieving the data using sqlQuery command. In the query I am
using Group by function of SQL.

is there any smarter way to this in R?


-- 
thanks
Sar

	[[alternative HTML version deleted]]

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From vikasrawal at gmail.com  Wed Feb 14 12:05:43 2007
From: vikasrawal at gmail.com (Vikas Rawal)
Date: Wed, 14 Feb 2007 16:35:43 +0530
Subject: [R] Boxplot: quartiles/outliers
In-Reply-To: <mailman.7.1171278002.31388.r-help@stat.math.ethz.ch>
References: <mailman.7.1171278002.31388.r-help@stat.math.ethz.ch>
Message-ID: <20070214110543.GA6322@shireen.jnu.ac.in>


> I am not sure why you would want to do this, and a 'box and whiskers plot' 
> was pretty closely defined by the original authors so R does not allow 
> options for forms they did not consider. 

I was wondering who the original authors were?  Would be interested in
looking up their description.

VR


From vikasrawal at gmail.com  Wed Feb 14 12:18:45 2007
From: vikasrawal at gmail.com (Vikas Rawal)
Date: Wed, 14 Feb 2007 16:48:45 +0530
Subject: [R] Boxplot: quartiles/outliers
In-Reply-To: <20070214110543.GA6322@shireen.jnu.ac.in>
References: <mailman.7.1171278002.31388.r-help@stat.math.ethz.ch>
	<20070214110543.GA6322@shireen.jnu.ac.in>
Message-ID: <20070214111845.GB7953@shireen.jnu.ac.in>


> 
> > I am not sure why you would want to do this, and a 'box and whiskers plot' 
> > was pretty closely defined by the original authors so R does not allow 
> > options for forms they did not consider. 
> 
> I was wondering who the original authors were?  Would be interested in
> looking up their description.

It was John Tukey (1977) book on EDA.
Sorry, I should have googled before sending the first mail.

V.


From petr.pikal at precheza.cz  Wed Feb 14 12:46:40 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 14 Feb 2007 12:46:40 +0100
Subject: [R] Boxplot: quartiles/outliers
In-Reply-To: <20070214110543.GA6322@shireen.jnu.ac.in>
References: <mailman.7.1171278002.31388.r-help@stat.math.ethz.ch>
Message-ID: <45D304B0.31403.1602BB3@localhost>

Hi

Google "boxplot"
see Wikipedia

The boxplot was invented in 1977 by American statistician John Tukey.

John W. Tukey. "Exploratory Data Analysis". Addison-Wesley, Reading, 
MA. 1977

So you shall probably look there.

HTH
Petr

On 14 Feb 2007 at 16:35, Vikas Rawal wrote:

Date sent:      	Wed, 14 Feb 2007 16:35:43 +0530
From:           	Vikas Rawal <vikasrawal at gmail.com>
To:             	r-help at stat.math.ethz.ch
Copies to:      	Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject:        	Re: [R] Boxplot: quartiles/outliers

> 
> > I am not sure why you would want to do this, and a 'box and whiskers
> > plot' was pretty closely defined by the original authors so R does
> > not allow options for forms they did not consider. 
> 
> I was wondering who the original authors were?  Would be interested in
> looking up their description.
> 
> VR
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ggrothendieck at gmail.com  Wed Feb 14 12:52:39 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Feb 2007 06:52:39 -0500
Subject: [R] linear coefficient combination stderr?
In-Reply-To: <45D2A16D.6060104@mail.jci.tju.edu>
References: <50d1c22d0702131921w53472a48gb9446149a85d0203@mail.gmail.com>
	<C83C5E3DEEE97E498B74729A33F6EAEC03878606@DJFPOST01.djf.agrsci.dk>
	<45D2A16D.6060104@mail.jci.tju.edu>
Message-ID: <971536df0702140352i4a3cd9f1je8646517f50ff8d1@mail.gmail.com>

vcov can simplify it slightly:

se <- sqrt( t(a) %*% vcov(m) %*% a )
( a %*% beta ) / se


On 2/14/07, Abhijit Dasgupta <Abhijit.Dasgupta at mail.jci.tju.edu> wrote:
> look at the function "estimable" in the package gmodels. This does the
> t.stat that you mention below.
>
> Otherwise, if you wanted to do this by hand, :
>
> beta = m$coef
> V = summary(m)$cov.unscaled
> sigma = summary(m)$sigma
>
> covBeta = V*sigma^2
>
> Now if your contrast is a%*%beta, then its se is
> sqrt(t(a)%*%covBeta%*%a) and so the t.stat below is
>
> t.stat = (a%*%beta)/sqrt(t(a)%*%covBeta%*%a)
> or, in your particular case
>
> t.stat = (beta[2]+beta[3])/sqrt(t(c(0,1,1))%*%covBeta%*%c(0,1,1))
>
> Abhijit
>
> S?ren H?jsgaard wrote:
> > Try the esticon function in the doBy package.
> > Regards
> > S?ren
> >
> > ________________________________
> >
> > Fra: r-help-bounces at stat.math.ethz.ch p? vegne af ivo welch
> > Sendt: on 14-02-2007 04:21
> > Til: r-help
> > Emne: [R] linear coefficient combination stderr?
> >
> >
> >
> > dear r-experts---I have scrounged around in google (searching r-help)
> > for the really obvious, but failed.  could someone please point me to
> > whatever function computes the standard error of a linear combination
> > of the coefficients?
> >
> > m=lm( y ~ x1 + x2 + x3 );
> > t.stat= (coef(m)[2]+coef(m)[3])) / mystery.function( m, x2 + x3 );
> >
> > obviously, this does not work, but hopefully explains my intent.
> > quick pointer appreciated.
> >
> > regards,
> >
> > /iaw
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Graham.Williams at togaware.com  Wed Feb 14 10:41:40 2007
From: Graham.Williams at togaware.com (Graham Williams)
Date: Wed, 14 Feb 2007 20:41:40 +1100
Subject: [R] [R-pkgs] New PMML package
Message-ID: <20070214094140.GA17839@athene.togaware.com>

A new package is now available on CRAN - pmml.

PMML is the Predictive Modelling Markup Language, and is accepted by a
number of large database and data warehouse systems (IBM DB2 and NCR
Teradata) for deployment of models as SQL.

The current package is an "early release" in that it is very basic (and
primarily supports the Rattle package). But it is a start! It currently
supports rpart classification trees and kmeans clusters.

Contributions of support for other models are welcome.

Regards,
Graham

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From Graham.Williams at togaware.com  Wed Feb 14 11:00:10 2007
From: Graham.Williams at togaware.com (Graham Williams)
Date: Wed, 14 Feb 2007 21:00:10 +1100
Subject: [R] [R-pkgs] New version of rattle released
Message-ID: <20070214100010.GA18511@athene.togaware.com>

A new version of Rattle (2.1.123), a Gnome-base GUI for data mining,
written completely in R, and available on GNU/Linux, Unix, Mac OSX, and
MS/Windows, has been released to CRAN.

There has been quite a lot of activity since the last update, including:

Transform:
        Now include basic imputation of missing values. More to follow.

Models: 
        Move to using ada for boosting.
        Better missing value handling for random forest
        Use arules package for market basket analysis
        Add more model visualisations

Export:
        RPart PMML export completed.
        PMML export has been separated out to its own package (pmml)

Complete change log available at

	http://rattle.togaware.com/changes.html

Rattle mailing list at

	http://groups.google.com/group/rattle-users

Regards,
Graham

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From avilella at gmail.com  Wed Feb 14 14:45:33 2007
From: avilella at gmail.com (Albert Vilella)
Date: Wed, 14 Feb 2007 13:45:33 +0000
Subject: [R] legend in lattice densityplot
In-Reply-To: <971536df0611300926g78e5129ct9d28d968e5528abe@mail.gmail.com>
References: <358f4d650611290911t3f1f017dm5c84e04c5d4a1210@mail.gmail.com>
	<456DC0F5.1090605@optonline.net>
	<358f4d650611290939n261771e3sa4ed1d0ea0674978@mail.gmail.com>
	<456DCD87.6080908@optonline.net>
	<358f4d650611300711o3dcc4fbfjea3a8af84d376940@mail.gmail.com>
	<971536df0611300725u47528643p4b0bcec815fa176c@mail.gmail.com>
	<358f4d650611300814i7be5fc54l24877976e3c3e114@mail.gmail.com>
	<971536df0611300926g78e5129ct9d28d968e5528abe@mail.gmail.com>
Message-ID: <358f4d650702140545j661fde9epf4f6db098055d241@mail.gmail.com>

How can I place the legend to the left or right of the densityplot? By
default, it goes at the top, and as it is a rather long list, the
density plot only uses half the space of the whole graphic...

On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Me too on Windows XP.
>
> Its probably just a bug or unimplemented feature in the SVG driver.
> Write to the maintainer of that package
>
> For a workaround generate fig output and then convert it to svg using whatever
> fig editor or converter you have.
>
> (On my windows system I use the free fig2dev converter although it inserted
> a DOCTYPE statement into the generated SVG file that IE7 did not recognize
> but once I manually deleted that it displayed ok in IE7.)
>
> # after producing file01.fig run
> #   fig2dev -L svg file01.fig file01.svg
> # or use some other fig to svg converter or editor
> xfig(file = "/file01.fig", onefile = TRUE)
> library(lattice)
> set.seed(1)
> DF <- data.frame(x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
>        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points = FALSE,
>  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty = 1:2,
>  lwd = c(1,1,1,1,2))))
> dev.off()
>
>
> On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > Should it be a problem to print this dashed line plots as svgs?
> >
> > library(RSvgDevice)
> > devSVG(file = "/home/avilella/file01.svg",
> >       width = 20, height = 16, bg = "white", fg = "black", onefile=TRUE,
> >       xmlHeader=TRUE)
> > densityplot(...)
> > dev.off()
> >
> > I am getting all the lines as continuous, not dashed...
> >
> > On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > Yes  by using the lty suboption of superpose.line.
> > > Here is a modification of the prior example to illustrate:
> > > We also use lwd as well in this example.
> > >
> > > set.seed(1)
> > > DF <- data.frame(x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
> > >        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> > > library(lattice)
> > > densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points = FALSE,
> > >  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty = 1:2,
> > >  lwd = c(1,1,1,1,2))))
> > >
> > >
> > > On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > > > Can I combine colors and line types? For example, would it be possible
> > > > to have 5 colors per 2 types of lines (continuous and dashed)?
> > > >
> > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > Albert Vilella wrote:
> > > > > > Are this legend colors correlated to the plot?
> > > > >
> > > > >   They are if you rely on the colors in
> > > > >
> > > > > trellis.par.get("superpose.line")$col
> > > > >
> > > > >   If you want different colors you might use trellis.par.set() to
> > > > > temporarily change the colors:
> > > > >
> > > > > x <- c(rnorm(100,-2,1),rnorm(100,0,1),rnorm(100,2,1))
> > > > > f <- rep(c("A","B","C"), each=100)
> > > > > df <- data.frame(x,f)
> > > > > library(lattice)
> > > > >
> > > > > oldpar <- trellis.par.get("superpose.line")$col
> > > > >
> > > > > trellis.par.set(superpose.line = list(col = heat.colors(3)))
> > > > >
> > > > > densityplot(~ x, groups = f, data = df,
> > > > >                  plot.points=FALSE,
> > > > >                  auto.key=TRUE)
> > > > >
> > > > > trellis.par.set(superpose.line = list(col = oldpar))
> > > > >
> > > > >   If you don't require points or lines in the key, you also could do
> > > > > something like this:
> > > > >
> > > > > densityplot(~ x, groups = f, data = df,
> > > > >                  plot.points=FALSE,
> > > > >                  key = simpleKey(levels(df$f),
> > > > >                                  lines=FALSE,
> > > > >                                  points=FALSE,
> > > > >                                  col=heat.colors(3)),
> > > > >                  col=heat.colors(3))
> > > > >
> > > > >   To use your own colors without changing the trellis settings and to
> > > > > get lines or points in the key, you probably need at least to use key =
> > > > > simpleKey() rather than the auto.key argument, and you may need to look
> > > > > into draw.key().  Other people on the list might know simpler approaches
> > > > > for using your own colors in this situation.
> > > > >
> > > > > > If I do a:
> > > > > >
> > > > > > densityplot(~x, groups=f, plot.points=FALSE,
> > > > > > auto.key=TRUE,col=heat.colors(5))
> > > > > >
> > > > > > I get different colors in the legend than the plot...
> > > > > >
> > > > > >
> > > > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > >> Albert Vilella wrote:
> > > > > >> > Hi,
> > > > > >> >
> > > > > >> > I have a densityplot like this:
> > > > > >> >
> > > > > >> > x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6))
> > > > > >> > f = sample(c("A","B","C","D","E"),300,replace=TRUE)
> > > > > >> > df=data.frame(x,f)
> > > > > >> > library(lattice)
> > > > > >> > attach(df)
> > > > > >> > densityplot(~x, groups=f)
> > > > > >> >
> > > > > >> > And I want to add a legend with the colours for the factors. How can
> > > > > >> I do that?
> > > > > >> > How can I not have the dots of the distribution at the bottom, or at
> > > > > >> > least, make them occupy less vertical space?
> > > > > >>
> > > > > >>   Change the last line to the following:
> > > > > >>
> > > > > >> densityplot(~x, groups=f, plot.points=FALSE, auto.key=TRUE)
> > > > > >>
> > > > > >> See ?panel.densityplot .
> > > > > >>
> > > > > >> > ______________________________________________
> > > > > >> > R-help at stat.math.ethz.ch mailing list
> > > > > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > >> > PLEASE do read the posting guide
> > > > > >> http://www.R-project.org/posting-guide.html
> > > > > >> > and provide commented, minimal, self-contained, reproducible code.
> > > > > >> >
> > > > > >>
> > > > > >> --
> > > > > >> Chuck Cleland, Ph.D.
> > > > > >> NDRI, Inc.
> > > > > >> 71 West 23rd Street, 8th floor
> > > > > >> New York, NY 10010
> > > > > >> tel: (212) 845-4495 (Tu, Th)
> > > > > >> tel: (732) 512-0171 (M, W, F)
> > > > > >> fax: (917) 438-0894
> > > > > >>
> > > > > >
> > > > >
> > > > > --
> > > > > Chuck Cleland, Ph.D.
> > > > > NDRI, Inc.
> > > > > 71 West 23rd Street, 8th floor
> > > > > New York, NY 10010
> > > > > tel: (212) 845-4495 (Tu, Th)
> > > > > tel: (732) 512-0171 (M, W, F)
> > > > > fax: (917) 438-0894
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> >
>


From rmh at temple.edu  Wed Feb 14 14:50:46 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 14 Feb 2007 08:50:46 -0500 (EST)
Subject: [R] Boxplot: quartiles/outliers
Message-ID: <20070214085046.BUJ65457@po-d.temple.edu>

Please see
Michael Frigge and David C. Hoaglin and Boris Iglewicz. "Some Implementations of the Boxplot". The 
American Statistician. Vol. 43 (1), February 1989. 50?54. 

They investigate and compared several alternate definitions of the hinges (quartiles)
and conclude that Tukey's definition is best on several criteria.


From matthew_wiener at merck.com  Wed Feb 14 15:07:16 2007
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 14 Feb 2007 09:07:16 -0500
Subject: [R] legend in lattice densityplot  [Broadcast]
In-Reply-To: <358f4d650702140545j661fde9epf4f6db098055d241@mail.gmail.com>
References: <358f4d650611290911t3f1f017dm5c84e04c5d4a1210@mail.gmail.com>
	<456DC0F5.1090605@optonline.net>
	<358f4d650611290939n261771e3sa4ed1d0ea0674978@mail.gmail.com>
	<456DCD87.6080908@optonline.net>
	<358f4d650611300711o3dcc4fbfjea3a8af84d376940@mail.gmail.com>
	<971536df0611300725u47528643p4b0bcec815fa176c@mail.gmail.com>
	<358f4d650611300814i7be5fc54l24877976e3c3e114@mail.gmail.com>
	<971536df0611300926g78e5129ct9d28d968e5528abe@mail.gmail.com>
	<358f4d650702140545j661fde9epf4f6db098055d241@mail.gmail.com>
Message-ID: <4E9A692D8755DF478B56A2892388EE1F0182919E@usctmx1118.merck.com>

>From the documentation for xyplot (referred to from densityplot):

The position of the key can be controlled in either of two possible
ways. If a component called space is present, the key is positioned
outside the plot region, in one of the four sides, determined by the
value of space, which can be one of "top", "bottom", "left" and "right".


Hope this helps,

Matt Wiener 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
Sent: Wednesday, February 14, 2007 8:46 AM
To: R-help at stat.math.ethz.ch
Subject: Re: [R] legend in lattice densityplot [Broadcast]

How can I place the legend to the left or right of the densityplot? By
default, it goes at the top, and as it is a rather long list, the
density plot only uses half the space of the whole graphic...

On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Me too on Windows XP.
>
> Its probably just a bug or unimplemented feature in the SVG driver.
> Write to the maintainer of that package
>
> For a workaround generate fig output and then convert it to svg using
whatever
> fig editor or converter you have.
>
> (On my windows system I use the free fig2dev converter although it
inserted
> a DOCTYPE statement into the generated SVG file that IE7 did not
recognize
> but once I manually deleted that it displayed ok in IE7.)
>
> # after producing file01.fig run
> #   fig2dev -L svg file01.fig file01.svg
> # or use some other fig to svg converter or editor
> xfig(file = "/file01.fig", onefile = TRUE)
> library(lattice)
> set.seed(1)
> DF <- data.frame(x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
>        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points = FALSE,
>  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty =
1:2,
>  lwd = c(1,1,1,1,2))))
> dev.off()
>
>
> On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > Should it be a problem to print this dashed line plots as svgs?
> >
> > library(RSvgDevice)
> > devSVG(file = "/home/avilella/file01.svg",
> >       width = 20, height = 16, bg = "white", fg = "black",
onefile=TRUE,
> >       xmlHeader=TRUE)
> > densityplot(...)
> > dev.off()
> >
> > I am getting all the lines as continuous, not dashed...
> >
> > On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > Yes  by using the lty suboption of superpose.line.
> > > Here is a modification of the prior example to illustrate:
> > > We also use lwd as well in this example.
> > >
> > > set.seed(1)
> > > DF <- data.frame(x =
c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
> > >        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> > > library(lattice)
> > > densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points =
FALSE,
> > >  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty =
1:2,
> > >  lwd = c(1,1,1,1,2))))
> > >
> > >
> > > On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > > > Can I combine colors and line types? For example, would it be
possible
> > > > to have 5 colors per 2 types of lines (continuous and dashed)?
> > > >
> > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > Albert Vilella wrote:
> > > > > > Are this legend colors correlated to the plot?
> > > > >
> > > > >   They are if you rely on the colors in
> > > > >
> > > > > trellis.par.get("superpose.line")$col
> > > > >
> > > > >   If you want different colors you might use trellis.par.set()
to
> > > > > temporarily change the colors:
> > > > >
> > > > > x <- c(rnorm(100,-2,1),rnorm(100,0,1),rnorm(100,2,1))
> > > > > f <- rep(c("A","B","C"), each=100)
> > > > > df <- data.frame(x,f)
> > > > > library(lattice)
> > > > >
> > > > > oldpar <- trellis.par.get("superpose.line")$col
> > > > >
> > > > > trellis.par.set(superpose.line = list(col = heat.colors(3)))
> > > > >
> > > > > densityplot(~ x, groups = f, data = df,
> > > > >                  plot.points=FALSE,
> > > > >                  auto.key=TRUE)
> > > > >
> > > > > trellis.par.set(superpose.line = list(col = oldpar))
> > > > >
> > > > >   If you don't require points or lines in the key, you also
could do
> > > > > something like this:
> > > > >
> > > > > densityplot(~ x, groups = f, data = df,
> > > > >                  plot.points=FALSE,
> > > > >                  key = simpleKey(levels(df$f),
> > > > >                                  lines=FALSE,
> > > > >                                  points=FALSE,
> > > > >                                  col=heat.colors(3)),
> > > > >                  col=heat.colors(3))
> > > > >
> > > > >   To use your own colors without changing the trellis settings
and to
> > > > > get lines or points in the key, you probably need at least to
use key =
> > > > > simpleKey() rather than the auto.key argument, and you may
need to look
> > > > > into draw.key().  Other people on the list might know simpler
approaches
> > > > > for using your own colors in this situation.
> > > > >
> > > > > > If I do a:
> > > > > >
> > > > > > densityplot(~x, groups=f, plot.points=FALSE,
> > > > > > auto.key=TRUE,col=heat.colors(5))
> > > > > >
> > > > > > I get different colors in the legend than the plot...
> > > > > >
> > > > > >
> > > > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > >> Albert Vilella wrote:
> > > > > >> > Hi,
> > > > > >> >
> > > > > >> > I have a densityplot like this:
> > > > > >> >
> > > > > >> > x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6))
> > > > > >> > f = sample(c("A","B","C","D","E"),300,replace=TRUE)
> > > > > >> > df=data.frame(x,f)
> > > > > >> > library(lattice)
> > > > > >> > attach(df)
> > > > > >> > densityplot(~x, groups=f)
> > > > > >> >
> > > > > >> > And I want to add a legend with the colours for the
factors. How can
> > > > > >> I do that?
> > > > > >> > How can I not have the dots of the distribution at the
bottom, or at
> > > > > >> > least, make them occupy less vertical space?
> > > > > >>
> > > > > >>   Change the last line to the following:
> > > > > >>
> > > > > >> densityplot(~x, groups=f, plot.points=FALSE, auto.key=TRUE)
> > > > > >>
> > > > > >> See ?panel.densityplot .
> > > > > >>
> > > > > >> > ______________________________________________
> > > > > >> > R-help at stat.math.ethz.ch mailing list
> > > > > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > >> > PLEASE do read the posting guide
> > > > > >> http://www.R-project.org/posting-guide.html
> > > > > >> > and provide commented, minimal, self-contained,
reproducible code.
> > > > > >> >
> > > > > >>
> > > > > >> --
> > > > > >> Chuck Cleland, Ph.D.
> > > > > >> NDRI, Inc.
> > > > > >> 71 West 23rd Street, 8th floor
> > > > > >> New York, NY 10010
> > > > > >> tel: (212) 845-4495 (Tu, Th)
> > > > > >> tel: (732) 512-0171 (M, W, F)
> > > > > >> fax: (917) 438-0894
> > > > > >>
> > > > > >
> > > > >
> > > > > --
> > > > > Chuck Cleland, Ph.D.
> > > > > NDRI, Inc.
> > > > > 71 West 23rd Street, 8th floor
> > > > > New York, NY 10010
> > > > > tel: (212) 845-4495 (Tu, Th)
> > > > > tel: (732) 512-0171 (M, W, F)
> > > > > fax: (917) 438-0894
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible
code.
> > > >
> > >
> >
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From bates at stat.wisc.edu  Wed Feb 14 15:26:45 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 14 Feb 2007 08:26:45 -0600
Subject: [R] nested model: lme, aov and LSMeans
In-Reply-To: <6fb73d020702131936o5ba77e35ydceeeb36f973a567@mail.gmail.com>
References: <6fb73d020702131936o5ba77e35ydceeeb36f973a567@mail.gmail.com>
Message-ID: <40e66e0b0702140626p5a52fec2y12bced7d0768d847@mail.gmail.com>

On 2/13/07, shirley zhang <shirley0818 at gmail.com> wrote:
> I'm working with a nested model (mixed).
>
> I have four factors: Patients, Tissue, sex, and tissue_stage.
>
> Totally I have 10 patients, for each patient, there are 2 tissues
> (Cancer vs. Normal).
>
> I think Tissue and sex are fixed. Patient is nested in sex,Tissue is
> nested in patient,  and tissue_stage is nested in Tissue.
>
> I tried aov and lme as the following,
>
> > aov(gene ~ tissue + gender + patients%in%gender + stage%in%tissue
> >lme(gene ~ tissue + gender, random = list(patients = ~1 , stage = ~ 1))

I think it is easier to specify the model for the lmer function in the
lme4 package

lmer(gene ~ tissue + gender + (1|patient:gender) +
(1|patient:gender:tissue:tissue_stage))

It sounds like you have coded the factors with implicit nesting.  That
is, even though you have 20 different tissue samples (I believe) the
tissue factor takes only two levels.  I don't know if you have 10
distinct levels of patient or if, for example, there is a male patient
1 and a female patient 1.  If the latter case then you must use the
grouping factor patient:gender to be able to distinguish the
observations on the male patient 1 from those on the female patient 1.
 Even if you have 10 distinct levels of the patient factor it does no
harm to specify the grouping as patient:gender - it's just a longer
expression to type.

The basic rule is tha the number of distinct levels in the grouping
factor must correspond to the number of levels of the corresponding
experimental unit.  (I know, that seems obvious but it doesn't always
work out that way.)  This is why the number of levels of the different
grouping factors are reported following the estimates of the variance
components in the summary of the fitted model.  You should always
check that to see that it makes sense.

>
>
> I got results from aov, but I got error message from lme which is
> false convergence (8). so I could not compare the results of aov with
> lme.
>
> Could anybody tell me whether I use the correct command? How can I get
> LSMeans for Cancer vs. Normal in R ( I know it is easy in SAS)?
>
> Thanks,
>
> Shirley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.harrell at vanderbilt.edu  Wed Feb 14 15:30:02 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 14 Feb 2007 08:30:02 -0600
Subject: [R] how to report logistic regression results
In-Reply-To: <a09537370702140048l68a99f51h7dc4eec9af557ca7@mail.gmail.com>
References: <a09537370702140048l68a99f51h7dc4eec9af557ca7@mail.gmail.com>
Message-ID: <45D31CEA.1080305@vanderbilt.edu>

Edwin Commandeur wrote:
> Dear all,
> 
> I am comparing logistic regression models to evaluate if one predictor
> explains additional variance that is not yet explained by another predictor.
> As far as I understand Baron and Li describe how to do this, but my question
> is now: how do I report this in an article? Can anyone recommend a
> particular article that shows a concrete example of how the results from te
> following simple modeling can be reported:
> 
> glm1  = glm(DV ~ A, family = binomial)
> glm2  = glm(DV ~ A + B, family = binomial)
> anova(glm1, glm2, test = "Chisq")
> 
> Any help on how this simple kind of modeling should be reported is
> appreciated.
> 
> Greetings,
> Edwin Commandeur

There are many ways, including odds ratios and partial effect plots and 
Brier scores.  For a pure likelihood measure I talk about an 'adequacy 
index' (adequacy of the smaller model) in my book, which was used in a 
medical paper:

@ARTICLE{cal85,
   author = {Califf, R. M. and Phillips, H. R. and others},
   year = 1985,
   title = {Prognostic value of a coronary artery jeopardy score},
   journal = J Am Coll Cardiology,
   volume = 5,
   pages = {1055-1063}
}

Frank Harrell

> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From avilella at gmail.com  Wed Feb 14 15:49:59 2007
From: avilella at gmail.com (Albert Vilella)
Date: Wed, 14 Feb 2007 14:49:59 +0000
Subject: [R] legend in lattice densityplot [Broadcast]
In-Reply-To: <4E9A692D8755DF478B56A2892388EE1F0182919E@usctmx1118.merck.com>
References: <358f4d650611290911t3f1f017dm5c84e04c5d4a1210@mail.gmail.com>
	<456DC0F5.1090605@optonline.net>
	<358f4d650611290939n261771e3sa4ed1d0ea0674978@mail.gmail.com>
	<456DCD87.6080908@optonline.net>
	<358f4d650611300711o3dcc4fbfjea3a8af84d376940@mail.gmail.com>
	<971536df0611300725u47528643p4b0bcec815fa176c@mail.gmail.com>
	<358f4d650611300814i7be5fc54l24877976e3c3e114@mail.gmail.com>
	<971536df0611300926g78e5129ct9d28d968e5528abe@mail.gmail.com>
	<358f4d650702140545j661fde9epf4f6db098055d241@mail.gmail.com>
	<4E9A692D8755DF478B56A2892388EE1F0182919E@usctmx1118.merck.com>
Message-ID: <358f4d650702140649k26eb483dnd50859b79c5e23b1@mail.gmail.com>

I am defining the legend using trellis.par.set (not sure if
correctly), and space does not seem to do the trick. auto-key (here
commented) places it to the top...

a = rep(c("alfa","beta","gamma","alfa","beta","gamma"),100)
b = rnorm(600)
input=data.frame(a,b)
densityplot(~(input$b),
  groups=input$a,
  plot.points=FALSE,
#  auto.key=TRUE,
  space = "left",
  trellis.par.set(superpose.line = list(
    col = rep( c("yellow","green","red","blue","orange","pink","lightblue","black","brown"),
3) ,
    lwd=3,
    lty = rep( c(1,2,3), each = 9) )
  )
)


On 2/14/07, Wiener, Matthew <matthew_wiener at merck.com> wrote:
> From the documentation for xyplot (referred to from densityplot):
>
> The position of the key can be controlled in either of two possible
> ways. If a component called space is present, the key is positioned
> outside the plot region, in one of the four sides, determined by the
> value of space, which can be one of "top", "bottom", "left" and "right".
>
>
> Hope this helps,
>
> Matt Wiener
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
> Sent: Wednesday, February 14, 2007 8:46 AM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] legend in lattice densityplot [Broadcast]
>
> How can I place the legend to the left or right of the densityplot? By
> default, it goes at the top, and as it is a rather long list, the
> density plot only uses half the space of the whole graphic...
>
> On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Me too on Windows XP.
> >
> > Its probably just a bug or unimplemented feature in the SVG driver.
> > Write to the maintainer of that package
> >
> > For a workaround generate fig output and then convert it to svg using
> whatever
> > fig editor or converter you have.
> >
> > (On my windows system I use the free fig2dev converter although it
> inserted
> > a DOCTYPE statement into the generated SVG file that IE7 did not
> recognize
> > but once I manually deleted that it displayed ok in IE7.)
> >
> > # after producing file01.fig run
> > #   fig2dev -L svg file01.fig file01.svg
> > # or use some other fig to svg converter or editor
> > xfig(file = "/file01.fig", onefile = TRUE)
> > library(lattice)
> > set.seed(1)
> > DF <- data.frame(x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
> >        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> > densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points = FALSE,
> >  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty =
> 1:2,
> >  lwd = c(1,1,1,1,2))))
> > dev.off()
> >
> >
> > On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > > Should it be a problem to print this dashed line plots as svgs?
> > >
> > > library(RSvgDevice)
> > > devSVG(file = "/home/avilella/file01.svg",
> > >       width = 20, height = 16, bg = "white", fg = "black",
> onefile=TRUE,
> > >       xmlHeader=TRUE)
> > > densityplot(...)
> > > dev.off()
> > >
> > > I am getting all the lines as continuous, not dashed...
> > >
> > > On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > Yes  by using the lty suboption of superpose.line.
> > > > Here is a modification of the prior example to illustrate:
> > > > We also use lwd as well in this example.
> > > >
> > > > set.seed(1)
> > > > DF <- data.frame(x =
> c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
> > > >        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> > > > library(lattice)
> > > > densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points =
> FALSE,
> > > >  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty =
> 1:2,
> > > >  lwd = c(1,1,1,1,2))))
> > > >
> > > >
> > > > On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > > > > Can I combine colors and line types? For example, would it be
> possible
> > > > > to have 5 colors per 2 types of lines (continuous and dashed)?
> > > > >
> > > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > > Albert Vilella wrote:
> > > > > > > Are this legend colors correlated to the plot?
> > > > > >
> > > > > >   They are if you rely on the colors in
> > > > > >
> > > > > > trellis.par.get("superpose.line")$col
> > > > > >
> > > > > >   If you want different colors you might use trellis.par.set()
> to
> > > > > > temporarily change the colors:
> > > > > >
> > > > > > x <- c(rnorm(100,-2,1),rnorm(100,0,1),rnorm(100,2,1))
> > > > > > f <- rep(c("A","B","C"), each=100)
> > > > > > df <- data.frame(x,f)
> > > > > > library(lattice)
> > > > > >
> > > > > > oldpar <- trellis.par.get("superpose.line")$col
> > > > > >
> > > > > > trellis.par.set(superpose.line = list(col = heat.colors(3)))
> > > > > >
> > > > > > densityplot(~ x, groups = f, data = df,
> > > > > >                  plot.points=FALSE,
> > > > > >                  auto.key=TRUE)
> > > > > >
> > > > > > trellis.par.set(superpose.line = list(col = oldpar))
> > > > > >
> > > > > >   If you don't require points or lines in the key, you also
> could do
> > > > > > something like this:
> > > > > >
> > > > > > densityplot(~ x, groups = f, data = df,
> > > > > >                  plot.points=FALSE,
> > > > > >                  key = simpleKey(levels(df$f),
> > > > > >                                  lines=FALSE,
> > > > > >                                  points=FALSE,
> > > > > >                                  col=heat.colors(3)),
> > > > > >                  col=heat.colors(3))
> > > > > >
> > > > > >   To use your own colors without changing the trellis settings
> and to
> > > > > > get lines or points in the key, you probably need at least to
> use key =
> > > > > > simpleKey() rather than the auto.key argument, and you may
> need to look
> > > > > > into draw.key().  Other people on the list might know simpler
> approaches
> > > > > > for using your own colors in this situation.
> > > > > >
> > > > > > > If I do a:
> > > > > > >
> > > > > > > densityplot(~x, groups=f, plot.points=FALSE,
> > > > > > > auto.key=TRUE,col=heat.colors(5))
> > > > > > >
> > > > > > > I get different colors in the legend than the plot...
> > > > > > >
> > > > > > >
> > > > > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > > >> Albert Vilella wrote:
> > > > > > >> > Hi,
> > > > > > >> >
> > > > > > >> > I have a densityplot like this:
> > > > > > >> >
> > > > > > >> > x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6))
> > > > > > >> > f = sample(c("A","B","C","D","E"),300,replace=TRUE)
> > > > > > >> > df=data.frame(x,f)
> > > > > > >> > library(lattice)
> > > > > > >> > attach(df)
> > > > > > >> > densityplot(~x, groups=f)
> > > > > > >> >
> > > > > > >> > And I want to add a legend with the colours for the
> factors. How can
> > > > > > >> I do that?
> > > > > > >> > How can I not have the dots of the distribution at the
> bottom, or at
> > > > > > >> > least, make them occupy less vertical space?
> > > > > > >>
> > > > > > >>   Change the last line to the following:
> > > > > > >>
> > > > > > >> densityplot(~x, groups=f, plot.points=FALSE, auto.key=TRUE)
> > > > > > >>
> > > > > > >> See ?panel.densityplot .
> > > > > > >>
> > > > > > >> > ______________________________________________
> > > > > > >> > R-help at stat.math.ethz.ch mailing list
> > > > > > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > >> > PLEASE do read the posting guide
> > > > > > >> http://www.R-project.org/posting-guide.html
> > > > > > >> > and provide commented, minimal, self-contained,
> reproducible code.
> > > > > > >> >
> > > > > > >>
> > > > > > >> --
> > > > > > >> Chuck Cleland, Ph.D.
> > > > > > >> NDRI, Inc.
> > > > > > >> 71 West 23rd Street, 8th floor
> > > > > > >> New York, NY 10010
> > > > > > >> tel: (212) 845-4495 (Tu, Th)
> > > > > > >> tel: (732) 512-0171 (M, W, F)
> > > > > > >> fax: (917) 438-0894
> > > > > > >>
> > > > > > >
> > > > > >
> > > > > > --
> > > > > > Chuck Cleland, Ph.D.
> > > > > > NDRI, Inc.
> > > > > > 71 West 23rd Street, 8th floor
> > > > > > New York, NY 10010
> > > > > > tel: (212) 845-4495 (Tu, Th)
> > > > > > tel: (732) 512-0171 (M, W, F)
> > > > > > fax: (917) 438-0894
> > > > > >
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible
> code.
> > > > >
> > > >
> > >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From Serguei.Kaniovski at wifo.ac.at  Wed Feb 14 16:00:04 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Wed, 14 Feb 2007 16:00:04 +0100
Subject: [R] simple question on intervals
Message-ID: <OF3EE5778F.93DDDFE7-ONC1257282.005267A3-C1257282.005267AF@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/cf51e9f8/attachment.pl 

From vogranovich at jumptrading.com  Wed Feb 14 16:01:42 2007
From: vogranovich at jumptrading.com (Vadim Ogranovich)
Date: Wed, 14 Feb 2007 09:01:42 -0600 (CST)
Subject: [R] Snow vs Rmpi
Message-ID: <25385622.30231171465302711.JavaMail.root@jumpmail1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/a4558ea6/attachment.pl 

From Fabio.Sanchez at ki.se  Wed Feb 14 16:09:06 2007
From: Fabio.Sanchez at ki.se (Fabio Sanchez)
Date: Wed, 14 Feb 2007 16:09:06 +0100
Subject: [R] How to generate observed/predicted value tables from logistic
	analysis
Message-ID: <f945a766f7bf.f7bff945a766@ki.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/afadf3b9/attachment.pl 

From s-dhar at northwestern.edu  Wed Feb 14 06:41:42 2007
From: s-dhar at northwestern.edu (Sumitrajit Dhar)
Date: Tue, 13 Feb 2007 23:41:42 -0600
Subject: [R] symbols hidden in polar.plot
Message-ID: <37DAD1EC-45F8-4914-AE22-EDCA239EB0B4@northwestern.edu>

Hi Folks,

Here is my attempt at a simple polar plot.


 > pos <- seq(0,360,by=5)
 > tspk <- rep(c(1,0,1),c(13,47,13))
 > require(plotrix)
 > polar.plot(tspk,pos,rp.type="s",point.symbols=17,point.col="green4")


I only see half the symbols, the other half of each symbol is hidden  
under the circular grid. In fact if I change rp.type="r", I see the  
lines being plotted and then the grid gets plotted over it and hides  
the lines.

What am I doing wrong?

Regards,
Sumit


From fabascal at cnb.uam.es  Wed Feb 14 16:42:20 2007
From: fabascal at cnb.uam.es (Federico Abascal)
Date: Wed, 14 Feb 2007 16:42:20 +0100
Subject: [R] font size in plots
Message-ID: <45D32DDC.60005@cnb.uam.es>

Dear members of the list,

it is likely a stupid question but I cannot find the information neither
in R manuals nor in google.

I am generating a plot (from hclust results) but I cannot see properly
the labels because the default font size is too large. How can I change it?

Thanks!
Federico


From tamir at imp.univie.ac.at  Wed Feb 14 16:55:31 2007
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Wed, 14 Feb 2007 16:55:31 +0100
Subject: [R] simple question on intervals
Message-ID: <200702141655.31356.tamir@imp.univie.ac.at>

>I have a number (correlation coefficient) "x" in [-1,1], and a color
>palette col<-grey(1:N/N) for a given N. I want to assign a color from "col"
>to "x" which corresponds to "x" in levels of cut(-1:1,N).
>
>So for N<-4 and x<-0.3, the color should be col[3]. For N<-4 and x<--0.8,
>the color should be col[1], etc.


findInterval(-0.3,seq(-1,1,2/4))

HTH
ido


From johannes_graumann at web.de  Wed Feb 14 17:35:13 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 14 Feb 2007 17:35:13 +0100
Subject: [R] ELIF?
Message-ID: <eqvdo1$rj1$1@sea.gmane.org>

Hello,

Is there an elegant way to implement something like the elif function (e.g.
python) and prevent multiple if-else contruct concatenation when coding in
R?

Sorry for my newbieishness,

Joh


From wshaffer at funbiznow.com  Wed Feb 14 17:36:21 2007
From: wshaffer at funbiznow.com (Mollie Campbell)
Date: Thu, 15 Feb 2007 01:36:21 +0900
Subject: [R] Be at metropolis
Message-ID: <001a01c750a2$53b65210$01aa3f1c@LocalHost>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/7470da6e/attachment.pl 

From gunter.berton at gene.com  Wed Feb 14 17:54:11 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 14 Feb 2007 08:54:11 -0800
Subject: [R] font size in plots
In-Reply-To: <45D32DDC.60005@cnb.uam.es>
Message-ID: <000d01c75058$c033c1b0$4d908980@gne.windows.gene.com>

In general, most methods for R's generic "plot" command (try:
getAnywhere("plot.hclust")) in R's base graphics system accept further
arguments in the (...) portion that provide these sorts of capabilities.
?par will tell you about these further graphical parameters. 


Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Federico Abascal
Sent: Wednesday, February 14, 2007 7:42 AM
To: r-help at stat.math.ethz.ch
Subject: [R] font size in plots

Dear members of the list,

it is likely a stupid question but I cannot find the information neither
in R manuals nor in google.

I am generating a plot (from hclust results) but I cannot see properly
the labels because the default font size is too large. How can I change it?

Thanks!
Federico

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Mark.Leeds at morganstanley.com  Wed Feb 14 17:59:12 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Wed, 14 Feb 2007 11:59:12 -0500
Subject: [R] xts.dvs function in fCalendar
Message-ID: <D3AEEDA31E57474B840BEBC25A8A83440148B54A@NYWEXMB23.msad.ms.com>

There is a function in the fCalendar package called xts.dvs but I m
unable to see the code inside the function.
Is this common with functions in the fCalendar package or maybe there is
something else that I have to
do first or use the colon colon command in some manner  I want to see
the function coee because I want to see the detials
Of the algorithm used in the function. I am on linux and my information
is below. thank you very much and my apologies
if this is obvious to most.


 
 version
               _                           
platform       i686-pc-linux-gnu           
arch           i686                        
os             linux-gnu                   
system         i686, linux-gnu             
status                                     
major          2                           
minor          4.0                         
year           2006                        
month          10                          
day            03                          
svn rev        39566                       
language       R                           
version.string R version 2.4.0 (2006-10-03)
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From perezperezmm at yahoo.es  Wed Feb 14 17:59:23 2007
From: perezperezmm at yahoo.es (M Perez)
Date: Wed, 14 Feb 2007 17:59:23 +0100 (CET)
Subject: [R] font size in plots
In-Reply-To: <45D32DDC.60005@cnb.uam.es>
Message-ID: <20070214165923.21912.qmail@web25514.mail.ukl.yahoo.com>

Dear Federico

Check the legend function.

?legend

HTH.
Manuel

--- Federico Abascal <fabascal at cnb.uam.es> escribi?:

> Dear members of the list,
> 
> it is likely a stupid question but I cannot find the
> information neither
> in R manuals nor in google.
> 
> I am generating a plot (from hclust results) but I
> cannot see properly
> the labels because the default font size is too
> large. How can I change it?
> 
> Thanks!
> Federico
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From atoriell at benetton.it  Wed Feb 14 18:30:31 2007
From: atoriell at benetton.it (atoriell at benetton.it)
Date: Wed, 14 Feb 2007 18:30:31 +0100
Subject: [R] sprintf error clustering with rattle
Message-ID: <OF5D0E6694.267913C5-ONC1257282.00602DC8-C1257282.00602DDC@benetton.it>


Hi,
I'm an italian R (and data mining) beginner.
I installed R, Rattle and rggobi with no trouble.

But I'm not able to do clustering (both kmean and hierarchical).

When I press F5 in Rattle Cluster tab nothing happens (and no log rows in
the Log Tab).

On the R console I get the following error (in italian):

"Errore in sprintf(fmt, ...) : argomento di lunghezza zero" (it means zero
length argument)

I set only input variables (no target).

Could anyone help me?

Bye

Amerigo


From murdoch at stats.uwo.ca  Wed Feb 14 18:45:17 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 14 Feb 2007 12:45:17 -0500
Subject: [R] ELIF?
In-Reply-To: <eqvdo1$rj1$1@sea.gmane.org>
References: <eqvdo1$rj1$1@sea.gmane.org>
Message-ID: <45D34AAD.2070509@stats.uwo.ca>

On 2/14/2007 11:35 AM, Johannes Graumann wrote:
> Hello,
> 
> Is there an elegant way to implement something like the elif function (e.g.
> python) and prevent multiple if-else contruct concatenation when coding in
> R?

Why not just concatenate, using "else if"?  For example,

if (condition1) {
    action 1
} else if (condition2) {
    action 2
} else if (condition3) {
    action 3
} else {
    default action
}

There is a switch() function if the condition is something that can be 
computed in advance, e.g. from the example on its man page:

  answer <- switch(type,
              mean = mean(x),
              median = median(x),
              trimmed = mean(x, trim = .1))


From stevenmh at muohio.edu  Wed Feb 14 19:21:52 2007
From: stevenmh at muohio.edu (stevenmh at muohio.edu)
Date: Wed, 14 Feb 2007 13:21:52 -0500 (EST)
Subject: [R] isoMDS vs. other non-metric non-R routines
In-Reply-To: <Pine.LNX.4.64.0702131247390.25544@egon.stats.ucl.ac.uk>
References: <45D1A5BB.882.62CEEC@philip.leifeld.uni-konstanz.de>
	<Pine.LNX.4.64.0702131247390.25544@egon.stats.ucl.ac.uk>
Message-ID: <61140.134.53.7.120.1171477312.squirrel@134.53.7.120>

Hi Phil,
Are you using metaMDS in the vegan package? This allows you to determine
the number of random starts, and selects the best. It might help.
Hank Stevens
> Dear Phil,
>
> I don't have experiences with Minissa but I know that isoMDS is bad in
> some situations. I have even seen situations with non-metric
> dissimilarities in which the classical MDS was preferable.
>
> Some alternatives that you have:
> 1) Try to start isoMDS from other initial configurations (by default, it
> starts from the classical solution).
> 2) Try sammon mapping (command should be "sammon").
> 3) Have a look at XGvis/GGvis (which may be part of XGobi/GGobi). These
> are not directly part of R but have R interfaces. They allow you to toy
> around quite a lot with different algorithms, stress functions (the
> isoMDS stress is not necessarily what you want) and initial
> configurations so that you can find a better solution and understand your
> data better. Unfortunately I don't have the time to give you more detail,
> but google for it (or somebody else will tell you more).
>
> Best,
> Christian
>
>
> On Tue, 13 Feb 2007, Philip Leifeld wrote:
>
>> Dear useRs,
>>
>> last week I asked you about a problem related to isoMDS. It turned
>> out that in my case isoMDS was trapped. Nonetheless, I still have
>> some problems with other data sets. Therefore I would like to know if
>> anyone here has experience with how well isoMDS performs in
>> comparison to other non-metric MDS routines, like Minissa.
>>
>> I have the feeling that for large data sets with a high stress value
>> (e.g. around 0.20) in cases where the intrinsic dimensionality of the
>> data cannot be significantly reduced without considerably increasing
>> stress, isoMDS performs worse (and yields a stress value of 0.31 in
>> my example), while solutions tend to be similar for better fits and
>> lower intrinsic dimensionality. I tried this on another data set
>> where isoMDS yields a stress value of 0.19 and Minissa a stress value
>> of 0.14.
>>
>> Now the latter would still be considered a fair solution by some
>> people while the former indicates a poor fit regardless of how strict
>> your judgment is. I generally prefer using R over mixing with
>> different programs, so it would be nice if results were of comparable
>> quality...
>>
>> Cheers
>>
>> Phil
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> *** --- ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wht_crl at yahoo.com  Wed Feb 14 19:28:45 2007
From: wht_crl at yahoo.com (carol white)
Date: Wed, 14 Feb 2007 10:28:45 -0800 (PST)
Subject: [R] ran out of iteration in coxph
In-Reply-To: <Pine.LNX.4.64.0702051554240.27564@gannet.stats.ox.ac.uk>
Message-ID: <228514.51942.qm@web62003.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/c6ea9337/attachment.pl 

From chrisgignoux at yahoo.com  Wed Feb 14 19:32:05 2007
From: chrisgignoux at yahoo.com (Chris Gignoux)
Date: Wed, 14 Feb 2007 10:32:05 -0800 (PST)
Subject: [R] help re: compiling BLACS for RscaLAPACK on OS X
Message-ID: <334802.85952.qm@web60920.mail.yahoo.com>

Hello, folks!  I hope you can help a newbie out.  I need to install the parallel-R functionality for a software package I am trying to download.  I have been going through the steps in the install readme but I cannot find the necessary "Bmake.inc" file to allow the file to compile properly.  I keep getting this error message:

( cd SRC/MPI ; make  )
( cd INTERNAL ; make -f ../Makefile I_int "dlvl=/Users/Chris/BLACS" )
make[2]: *** No rule to make target `BI_HypBS.o', needed by `I_int'.  Stop.
make[1]: *** [INTERN] Error 2
make: *** [MPI] Error 2


If someone has the proper input file (the recommended one in the readme is at a hyperlink that no longer exists) or something else that could help me I would surely appreciate it.

Thanks!


From jiho.han at yahoo.com  Wed Feb 14 19:50:41 2007
From: jiho.han at yahoo.com (jiho.han)
Date: Wed, 14 Feb 2007 10:50:41 -0800 (PST)
Subject: [R] How to use Rpad
Message-ID: <8971101.post@talk.nabble.com>


I am a beginner and I don't know how to use Rpad package. 
I installed it and opened the following example .Rpad page in Internet
Explorer.
When I clicked "Calculate" button, nothing seems to happen. Can anyone tell
me how to use Rpad?



<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html>
<!-- by Tom Short, EPRI, tshort at epri.com
(c) Copyright 2005 by EPRI

Licence
=======

    This is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program; if not, write to the Free Software
    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA.


The file COPYING in the Rpad top-level directory is a copy of the 'GNU
General Public License'.
 -->

  <head>
    <title>Rpad Example</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<script type="text/javascript" src="gui/dojo.js"></script>
<script type="text/javascript" src="gui/Rpad.js"></script>

  </head>

  <body>
<table><tr valign="top"><td>
<p>Here is a basic R input section followed by the output: </p>


<pre dojoType="Rpad" rpad_type="R">
data(iris)
dataset = iris
options(width=50)
summary(dataset)
</pre>

<p>Now lets do some fancy HTML output: <span contenteditable="false">
<input onclick=
      "javascript:rpad.calculatePage()" value="Calculate" type=
      "button"></span>
</p>

<textarea dojoType="Rpad" rows="4" cols="50">
HTMLon()
Html(head(dataset))
</textarea>

</td><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td> <td>
<p>Here is a simple distribution plotting example with a lognormal
distribution with meanlog=10 and variable logsd:</p>

      <p><span contenteditable="false"><input class="Rpad_input" name=
      "sdlog" rpadType="Rvariable" value="2"></span>
      Standard deviation<br></p>

<pre dojoType="Rpad">
plot(function(x) dlnorm(x, meanlog = 5, sdlog = sdlog), 
     0, 100, main = "lognormal density", ylab="",
     xlab="", col="red")
HTMLon()
showgraph()
</pre>
<p>Here is another graphic: </p>

<pre dojoType="Rpad">
data(volcano)
z <- 2 * volcano; x <- 10 * (1:nrow(z)); y <- 10 * (1:ncol(z))
persp(x, y, z, theta = 135, phi = 30, col = "green3", 
      scale = FALSE, ltheta = -120, shade = 0.75, 
      border = NA, box = FALSE)
HTMLon()
showgraph()
</pre>


</td></tr></table>

<br><br><br><sub>by Tom Short, tshort at epri.com, Copyright
  2005 EPRI, license: GNU GPL v2 or greater</sub>
  </body>
</html>





-- 
View this message in context: http://www.nabble.com/How-to-use-Rpad-tf3228996.html#a8971101
Sent from the R help mailing list archive at Nabble.com.


From RMan54 at cox.net  Wed Feb 14 20:01:25 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Wed, 14 Feb 2007 11:01:25 -0800
Subject: [R] legend in lattice densityplot [Broadcast]
In-Reply-To: <358f4d650702140649k26eb483dnd50859b79c5e23b1@mail.gmail.com>
References: <358f4d650611290911t3f1f017dm5c84e04c5d4a1210@mail.gmail.com><456DC0F5.1090605@optonline.net><358f4d650611290939n261771e3sa4ed1d0ea0674978@mail.gmail.com><456DCD87.6080908@optonline.net><358f4d650611300711o3dcc4fbfjea3a8af84d376940@mail.gmail.com><971536df0611300725u47528643p4b0bcec815fa176c@mail.gmail.com><358f4d650611300814i7be5fc54l24877976e3c3e114@mail.gmail.com><971536df0611300926g78e5129ct9d28d968e5528abe@mail.gmail.com><358f4d650702140545j661fde9epf4f6db098055d241@mail.gmail.com><4E9A692D8755DF478B56A2892388EE1F0182919E@usctmx1118.merck.com>
	<358f4d650702140649k26eb483dnd50859b79c5e23b1@mail.gmail.com>
Message-ID: <00bc01c7506a$86a27e20$0900a8c0@rman>

I use "key=" instead. Much more flexible. I set the parameters in
"trellis.par.set" for the plot and then take these settings in "key" to get
them in the legend. "space=" is part of the "key=" settings. 

As in this (to stick with your example):

library(lattice)
lg <- c("alfa","beta","gamma")
a <- rep(lg, 200)
b <- rnorm(600)
input <- data.frame(a,b)
densityplot(~(input$b),
  groups = input$a,
  plot.points = FALSE,
  trellis.par.set(superpose.line = list(
    col = rep(
c("yellow","green","red","blue","orange","pink","lightblue","black","brown")
,3),
    lwd = rep( 3, 27),
    lty = rep( c(1,2,3), each = 9) )
  ),
  key = list(space="left", 
             lines=list( 
                  col = trellis.par.get()$superpose.line$col[1:3], 
                  lwd = trellis.par.get()$superpose.line$lwd[1:3], 
                  lty = trellis.par.get()$superpose.line$lty[1:3]
             	), 
             text=list(lg))
)

Hope this helps.
Rene
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
Sent: Wednesday, February 14, 2007 6:50 AM
To: Wiener, Matthew
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] legend in lattice densityplot [Broadcast]

I am defining the legend using trellis.par.set (not sure if correctly), and
space does not seem to do the trick. auto-key (here
commented) places it to the top...

a = rep(c("alfa","beta","gamma","alfa","beta","gamma"),100)
b = rnorm(600)
input=data.frame(a,b)
densityplot(~(input$b),
  groups=input$a,
  plot.points=FALSE,
#  auto.key=TRUE,
  space = "left",
  trellis.par.set(superpose.line = list(
    col = rep(
c("yellow","green","red","blue","orange","pink","lightblue","black","brown")
,
3) ,
    lwd=3,
    lty = rep( c(1,2,3), each = 9) )
  )
)


On 2/14/07, Wiener, Matthew <matthew_wiener at merck.com> wrote:
> From the documentation for xyplot (referred to from densityplot):
>
> The position of the key can be controlled in either of two possible 
> ways. If a component called space is present, the key is positioned 
> outside the plot region, in one of the four sides, determined by the 
> value of space, which can be one of "top", "bottom", "left" and "right".
>
>
> Hope this helps,
>
> Matt Wiener
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
> Sent: Wednesday, February 14, 2007 8:46 AM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] legend in lattice densityplot [Broadcast]
>
> How can I place the legend to the left or right of the densityplot? By 
> default, it goes at the top, and as it is a rather long list, the 
> density plot only uses half the space of the whole graphic...
>
> On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Me too on Windows XP.
> >
> > Its probably just a bug or unimplemented feature in the SVG driver.
> > Write to the maintainer of that package
> >
> > For a workaround generate fig output and then convert it to svg 
> > using
> whatever
> > fig editor or converter you have.
> >
> > (On my windows system I use the free fig2dev converter although it
> inserted
> > a DOCTYPE statement into the generated SVG file that IE7 did not
> recognize
> > but once I manually deleted that it displayed ok in IE7.)
> >
> > # after producing file01.fig run
> > #   fig2dev -L svg file01.fig file01.svg
> > # or use some other fig to svg converter or editor xfig(file = 
> > "/file01.fig", onefile = TRUE)
> > library(lattice)
> > set.seed(1)
> > DF <- data.frame(x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
> >        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> > densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points = 
> > FALSE,  par.settings = list(superpose.line = list(col = c(1,1,2,2), 
> > lty =
> 1:2,
> >  lwd = c(1,1,1,1,2))))
> > dev.off()
> >
> >
> > On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > > Should it be a problem to print this dashed line plots as svgs?
> > >
> > > library(RSvgDevice)
> > > devSVG(file = "/home/avilella/file01.svg",
> > >       width = 20, height = 16, bg = "white", fg = "black",
> onefile=TRUE,
> > >       xmlHeader=TRUE)
> > > densityplot(...)
> > > dev.off()
> > >
> > > I am getting all the lines as continuous, not dashed...
> > >
> > > On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > Yes  by using the lty suboption of superpose.line.
> > > > Here is a modification of the prior example to illustrate:
> > > > We also use lwd as well in this example.
> > > >
> > > > set.seed(1)
> > > > DF <- data.frame(x =
> c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
> > > >        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> > > > library(lattice)
> > > > densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points =
> FALSE,
> > > >  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty 
> > > > =
> 1:2,
> > > >  lwd = c(1,1,1,1,2))))
> > > >
> > > >
> > > > On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > > > > Can I combine colors and line types? For example, would it be
> possible
> > > > > to have 5 colors per 2 types of lines (continuous and dashed)?
> > > > >
> > > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > > Albert Vilella wrote:
> > > > > > > Are this legend colors correlated to the plot?
> > > > > >
> > > > > >   They are if you rely on the colors in
> > > > > >
> > > > > > trellis.par.get("superpose.line")$col
> > > > > >
> > > > > >   If you want different colors you might use 
> > > > > > trellis.par.set()
> to
> > > > > > temporarily change the colors:
> > > > > >
> > > > > > x <- c(rnorm(100,-2,1),rnorm(100,0,1),rnorm(100,2,1))
> > > > > > f <- rep(c("A","B","C"), each=100) df <- data.frame(x,f)
> > > > > > library(lattice)
> > > > > >
> > > > > > oldpar <- trellis.par.get("superpose.line")$col
> > > > > >
> > > > > > trellis.par.set(superpose.line = list(col = heat.colors(3)))
> > > > > >
> > > > > > densityplot(~ x, groups = f, data = df,
> > > > > >                  plot.points=FALSE,
> > > > > >                  auto.key=TRUE)
> > > > > >
> > > > > > trellis.par.set(superpose.line = list(col = oldpar))
> > > > > >
> > > > > >   If you don't require points or lines in the key, you also
> could do
> > > > > > something like this:
> > > > > >
> > > > > > densityplot(~ x, groups = f, data = df,
> > > > > >                  plot.points=FALSE,
> > > > > >                  key = simpleKey(levels(df$f),
> > > > > >                                  lines=FALSE,
> > > > > >                                  points=FALSE,
> > > > > >                                  col=heat.colors(3)),
> > > > > >                  col=heat.colors(3))
> > > > > >
> > > > > >   To use your own colors without changing the trellis 
> > > > > > settings
> and to
> > > > > > get lines or points in the key, you probably need at least 
> > > > > > to
> use key =
> > > > > > simpleKey() rather than the auto.key argument, and you may
> need to look
> > > > > > into draw.key().  Other people on the list might know 
> > > > > > simpler
> approaches
> > > > > > for using your own colors in this situation.
> > > > > >
> > > > > > > If I do a:
> > > > > > >
> > > > > > > densityplot(~x, groups=f, plot.points=FALSE,
> > > > > > > auto.key=TRUE,col=heat.colors(5))
> > > > > > >
> > > > > > > I get different colors in the legend than the plot...
> > > > > > >
> > > > > > >
> > > > > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > > >> Albert Vilella wrote:
> > > > > > >> > Hi,
> > > > > > >> >
> > > > > > >> > I have a densityplot like this:
> > > > > > >> >
> > > > > > >> > x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6))
> > > > > > >> > f = sample(c("A","B","C","D","E"),300,replace=TRUE)
> > > > > > >> > df=data.frame(x,f)
> > > > > > >> > library(lattice)
> > > > > > >> > attach(df)
> > > > > > >> > densityplot(~x, groups=f)
> > > > > > >> >
> > > > > > >> > And I want to add a legend with the colours for the
> factors. How can
> > > > > > >> I do that?
> > > > > > >> > How can I not have the dots of the distribution at the
> bottom, or at
> > > > > > >> > least, make them occupy less vertical space?
> > > > > > >>
> > > > > > >>   Change the last line to the following:
> > > > > > >>
> > > > > > >> densityplot(~x, groups=f, plot.points=FALSE, 
> > > > > > >> auto.key=TRUE)
> > > > > > >>
> > > > > > >> See ?panel.densityplot .
> > > > > > >>
> > > > > > >> > ______________________________________________
> > > > > > >> > R-help at stat.math.ethz.ch mailing list 
> > > > > > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > >> > PLEASE do read the posting guide
> > > > > > >> http://www.R-project.org/posting-guide.html
> > > > > > >> > and provide commented, minimal, self-contained,
> reproducible code.
> > > > > > >> >
> > > > > > >>
> > > > > > >> --
> > > > > > >> Chuck Cleland, Ph.D.
> > > > > > >> NDRI, Inc.
> > > > > > >> 71 West 23rd Street, 8th floor New York, NY 10010
> > > > > > >> tel: (212) 845-4495 (Tu, Th)
> > > > > > >> tel: (732) 512-0171 (M, W, F)
> > > > > > >> fax: (917) 438-0894
> > > > > > >>
> > > > > > >
> > > > > >
> > > > > > --
> > > > > > Chuck Cleland, Ph.D.
> > > > > > NDRI, Inc.
> > > > > > 71 West 23rd Street, 8th floor New York, NY 10010
> > > > > > tel: (212) 845-4495 (Tu, Th)
> > > > > > tel: (732) 512-0171 (M, W, F)
> > > > > > fax: (917) 438-0894
> > > > > >
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list 
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible
> code.
> > > > >
> > > >
> > >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> ----------------------------------------------------------------------
> --------
> Notice:  This e-mail message, together with any 
> attachment...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From tyler.smith at mail.mcgill.ca  Wed Feb 14 19:32:50 2007
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Wed, 14 Feb 2007 14:32:50 -0400
Subject: [R] legend font
Message-ID: <20070214183248.GA3674@blackbart.mynetwork>

Hi,

I'd like to make the text in my legends italic, but I can't figure out
how to do so. font=3 doesn't work. Googling brings up the possibility
of expression(italic()), which produces italics, but I can't get this
to work with my label data, which is a vector of strings:

  legend(locator(1), legend = levels(factor(label.vector)),
         col = plotting.colours, pch =plotsym.bw, cex = 0.7 )

How can I do this?

-- 
Regards,

Tyler Smith


From murdoch at stats.uwo.ca  Wed Feb 14 20:40:47 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 14 Feb 2007 14:40:47 -0500
Subject: [R] legend font
In-Reply-To: <20070214183248.GA3674@blackbart.mynetwork>
References: <20070214183248.GA3674@blackbart.mynetwork>
Message-ID: <45D365BF.7000102@stats.uwo.ca>

On 2/14/2007 1:32 PM, Tyler Smith wrote:
> Hi,
> 
> I'd like to make the text in my legends italic, but I can't figure out
> how to do so. font=3 doesn't work. Googling brings up the possibility
> of expression(italic()), which produces italics, but I can't get this
> to work with my label data, which is a vector of strings:
> 
>   legend(locator(1), legend = levels(factor(label.vector)),
>          col = plotting.colours, pch =plotsym.bw, cex = 0.7 )
> 
> How can I do this?

This should work:

plot(1,1)
savefont <- par(font=3)
legend("topright", legend=c('Label 1', 'Label 2'), pch=1:2)
par(savefont)

Duncan Murdoch


From P.Filzmoser at tuwien.ac.at  Wed Feb 14 20:43:13 2007
From: P.Filzmoser at tuwien.ac.at (Peter Filzmoser)
Date: Wed, 14 Feb 2007 20:43:13 +0100
Subject: [R] Questions about results from PCAproj for robust principal
 component analysis
In-Reply-To: <BAY132-F39187E0724DB2CB3164B83AA900@phx.gbl>
References: <BAY132-F39187E0724DB2CB3164B83AA900@phx.gbl>
Message-ID: <45D36651.4020002@tuwien.ac.at>

Hi,

PCAproj is mainly designed for robust PCA and not for classical PCA.
Therefore, when applying classical estimators to the results of a
robust PCA, like the mean to the robust PCA scores, this will usually
not give zeros. The robust PCs have been centred robustly, and
not classically by the mean.
In your case you ran PCAproj with the default method="mad", thus
robust PCA was performed, maximizing the mad instead of the usual
variance (standard deviation). Moreover, you used the default for
centring the PCs, which is center="l1median", so a robust centring
rather than centring by the classical mean (which would have been
the zero vector because your data were already classically centred).
Run the procedure with the options
center=mean and method="sd"
which then gives classical PCA. The mean of the resulting PCA
scores will be 0. However, the scores will not be orthogonal
(or uncorrelated), but close to. The reason is that PCAproj
is an axxproximative algorithm for finding the (robust) PCs.
The eigen-analysis of princomp gives the exact solution (but
it is not robust).

The wrong length of result$scale and result$center is definitely
an error in the procedure that I will have to change quickly.
For data sets with more columns than rows we automatically
apply a singular value decomposition (SVD) to reduce the
dimensionality (without information loss). Then we perform
centring and scaling in the space of reduced dimension, but
we did not back-transform these values - sorry. Will be repaired
soon.

Best regards,
Peter


Talbot Katz wrote:
> Hi.
> 
> I have been looking at the PCAproj function in package pcaPP (R 2.4.1) 
> for robust principal components, and I'm trying to interpret the 
> results.  I started with a data matrix of dimensions RxC (R is the 
> number of rows / observations, C the number of columns / variables).  
> PCAproj returns a list of class princomp, similar to the output of the 
> function princomp.  In a case where I can run princomp, I would get the 
> following, from executing  dmpca = princomp(datamatrix) :
> -    the vector, sdev, of length C, contains the standard deviations of 
> the components in
>         order by descending value; the squares are the eigenvalues of the
>         covariance matrix
> -    the matrix, loadings, has dimension CxC, and the columns are the 
> eigenvectors of the
>         covariance matrix, in the same order as the sdev vector; the 
> columns are
>         orthonormal:
>         sum(dmpca$loadings[,i]*dmpca$loadings[,j]) = 1 if i == j, ~ 0 if 
> i != j
> -    the vector, center, of length C, contains the means of the variable 
> columns in the original
>         data matrix, in the same order as the original columns
> -    the vector, scale, of length C, contains the scalings applied to 
> each variable, in the same
>         order as the original columns
> -    n.obs contains the number of observations used in the computation; 
> this number equals
>         R when there is no missing data
> -    the matrix, scores, has dimension RxC, and it can be thought of as 
> the projection of the
>         eigenvector matrix, loadings, back onto the original data; these 
> columns of
>         scores are the principal components.  princomp typically removes 
> the mean,
>         so the formula is:
>         dmpca$scores = t(t(datamatrix) - dmpca$center)%*%dmpca$loadings
>         and apply(dmpca$scores,2,mean) returns a length C vector of 
> (effectively)
>         zeroes; also the principal components (columns of scores) are 
> orthogonal
>         (but not orthonormal):
>         sum(dmpca$scores[,i]*dmpca$scores[,j]) ~ 0 if i != j, > 0 if i == j
> -    call contains the function call, in this case princomp(x = datamatrix)
> 
> That is all as it should be.
> 
> 
> In my case R < C, which produces singular results for standard PCA, but 
> robust methods, like PCAproj, are designed to handle this.  Also, I had 
> "de-meaned" the data beforehand, so apply(datamatrix,2,mean) produces a 
> length C vector of (effectively) zeroes.  I ran the following:
> dmpcaprj=PCAproj(datamatrix,k=4,CalcMethod="sphere",update=TRUE)
> to get the first four robust components.  When I look at the princomp 
> object returned as dmpcaprj, some of the results are just what I 
> expect.  For example,
> -    dmpcaprj$loadings has dimensions Cx4, as expected, and the first 
> four eigenvectors of
>         the (robust) covariance matrix are orthonormal:
>         sum(dmpcaprj$loadings[,i]*dmpcaprj$loadings[,j]) = 1 if i == j, 
> ~ 0 if i != j
> -    dmpcaprj$sdev contains the square roots of the four corresponding 
> eigenvalues.
> -    dmpcaprj$n.obs equals R.
> -    dmpcaprj$scores has dimensions Rx4, as it should.
> 
> HOWEVER, the columns of dmpcaprj$scores are neither de-meaned nor 
> orthogonal.  So,
>         apply(dmpcaprj$scores,2,mean) is a non-zero vector, and
>         sum(dmpcaprj$scores[,i]*dmpcaprj$scores[,j]) != 0 if i != j, > 0 
> if i == j
> ALSO,
> -    dmpcaprj$scale is in this case a vector of all 1's, as expected.  
> But the length is C, not R.
> -    dmpcaprj$center is a vector of length C, not R, and the entries are 
> not equal to either
>         apply(datamatrix,1,mean)  or  apply(datamatrix,2,mean); I can't 
> figure out
>         where they came from.
> 
> One interesting thing is that the columns of the Rx4 matrix,
>         dmpcaprj$scores - datamatrix%*%dmpcaprj$loadings
> are all identically constant vectors, such that each row equals 
> apply(dmpcaprj$scores,2,mean), since
> apply(datamatrix%*%dmpcaprj$loadings,2,mean) is a length four vector of 
> (effectively) zeroes,
> but I can't interpret the values of these means of dmpcaprj$scores.
> 
> 
> Can anyone please explain to me what is happening with the scores, 
> scale, and center parts of the PCAproj results?
> 
> 
> Thanks!
> 
> 
> --  TMK  --
> 212-460-5430    home
> 917-656-5351    cell
> 
> 
> 
> 


-- 
-------------------------------------------------------
From: Prof. Dr. Peter Filzmoser
       Dept. of Statistics & Probability Theory
       Vienna University of Technology
       Wiedner Hauptstrasse 8-10
       A-1040 Vienna, Austria
       Tel. +43 1 58801/10733
       Fax. +43 1 58801/10799
       E-mail: P.Filzmoser at tuwien.ac.at
       Internet:
       http://www.statistik.tuwien.ac.at/public/filz/


From tyler.smith at mail.mcgill.ca  Wed Feb 14 21:12:26 2007
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Wed, 14 Feb 2007 16:12:26 -0400
Subject: [R] legend font
In-Reply-To: <45D365BF.7000102@stats.uwo.ca>
References: <20070214183248.GA3674@blackbart.mynetwork>
	<45D365BF.7000102@stats.uwo.ca>
Message-ID: <20070214201226.GA25877@blackbart.mynetwork>

On Wed, Feb 14, 2007 at 02:40:47PM -0500, Duncan Murdoch wrote:
> On 2/14/2007 1:32 PM, Tyler Smith wrote:
> >Hi,
> >
> >I'd like to make the text in my legends italic, 

...

> >How can I do this?
> 
> This should work:
> 
> plot(1,1)
> savefont <- par(font=3)
> legend("topright", legend=c('Label 1', 'Label 2'), pch=1:2)
> par(savefont)
> 

Thanks! I don't understand it yet, but it does indeed work.


-- 
Regards,

Tyler Smith


From murdoch at stats.uwo.ca  Wed Feb 14 21:19:31 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 14 Feb 2007 15:19:31 -0500
Subject: [R] legend font
In-Reply-To: <20070214201226.GA25877@blackbart.mynetwork>
References: <20070214183248.GA3674@blackbart.mynetwork>
	<45D365BF.7000102@stats.uwo.ca>
	<20070214201226.GA25877@blackbart.mynetwork>
Message-ID: <45D36ED3.9010005@stats.uwo.ca>

On 2/14/2007 3:12 PM, Tyler Smith wrote:
> On Wed, Feb 14, 2007 at 02:40:47PM -0500, Duncan Murdoch wrote:
>> On 2/14/2007 1:32 PM, Tyler Smith wrote:
>> >Hi,
>> >
>> >I'd like to make the text in my legends italic, 
> 
> ...
> 
>> >How can I do this?
>> 
>> This should work:
>> 
>> plot(1,1)
>> savefont <- par(font=3)
>> legend("topright", legend=c('Label 1', 'Label 2'), pch=1:2)
>> par(savefont)
>> 
> 
> Thanks! I don't understand it yet, but it does indeed work.

The idea is that the first par() command changes the default font for 
everything. (The legend() function doesn't pass any font request down to 
the graphics system, it just uses the default font.)  It also returns 
the old font setting and I saved it in "savefont".

The second par() call restores the old font setting.

Duncan Murdoch


From topkatz at msn.com  Wed Feb 14 21:22:55 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 14 Feb 2007 15:22:55 -0500
Subject: [R] Questions about results from PCAproj for robust principal
	component analysis
In-Reply-To: <45D36651.4020002@tuwien.ac.at>
Message-ID: <BAY132-F29D0B6D0AA40557BC83EE9AA970@phx.gbl>

Professor Filzmoser.

Thank you so much for the detailed response.  It is very helpful.

--  TMK  --
212-460-5430	home
917-656-5351	cell


>From: Peter Filzmoser <P.Filzmoser at tuwien.ac.at>
>To: Talbot Katz <topkatz at msn.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: Questions about results from PCAproj for robust principal 
>component analysis
>Date: Wed, 14 Feb 2007 20:43:13 +0100
>
>Hi,
>
>PCAproj is mainly designed for robust PCA and not for classical PCA.
>Therefore, when applying classical estimators to the results of a
>robust PCA, like the mean to the robust PCA scores, this will usually
>not give zeros. The robust PCs have been centred robustly, and
>not classically by the mean.
>In your case you ran PCAproj with the default method="mad", thus
>robust PCA was performed, maximizing the mad instead of the usual
>variance (standard deviation). Moreover, you used the default for
>centring the PCs, which is center="l1median", so a robust centring
>rather than centring by the classical mean (which would have been
>the zero vector because your data were already classically centred).
>Run the procedure with the options
>center=mean and method="sd"
>which then gives classical PCA. The mean of the resulting PCA
>scores will be 0. However, the scores will not be orthogonal
>(or uncorrelated), but close to. The reason is that PCAproj
>is an axxproximative algorithm for finding the (robust) PCs.
>The eigen-analysis of princomp gives the exact solution (but
>it is not robust).
>
>The wrong length of result$scale and result$center is definitely
>an error in the procedure that I will have to change quickly.
>For data sets with more columns than rows we automatically
>apply a singular value decomposition (SVD) to reduce the
>dimensionality (without information loss). Then we perform
>centring and scaling in the space of reduced dimension, but
>we did not back-transform these values - sorry. Will be repaired
>soon.
>
>Best regards,
>Peter
>
>
>Talbot Katz wrote:
>>Hi.
>>
>>I have been looking at the PCAproj function in package pcaPP (R 2.4.1) for 
>>robust principal components, and I'm trying to interpret the results.  I 
>>started with a data matrix of dimensions RxC (R is the number of rows / 
>>observations, C the number of columns / variables).  PCAproj returns a 
>>list of class princomp, similar to the output of the function princomp.  
>>In a case where I can run princomp, I would get the following, from 
>>executing  dmpca = princomp(datamatrix) :
>>-    the vector, sdev, of length C, contains the standard deviations of 
>>the components in
>>         order by descending value; the squares are the eigenvalues of the
>>         covariance matrix
>>-    the matrix, loadings, has dimension CxC, and the columns are the 
>>eigenvectors of the
>>         covariance matrix, in the same order as the sdev vector; the 
>>columns are
>>         orthonormal:
>>         sum(dmpca$loadings[,i]*dmpca$loadings[,j]) = 1 if i == j, ~ 0 if 
>>i != j
>>-    the vector, center, of length C, contains the means of the variable 
>>columns in the original
>>         data matrix, in the same order as the original columns
>>-    the vector, scale, of length C, contains the scalings applied to each 
>>variable, in the same
>>         order as the original columns
>>-    n.obs contains the number of observations used in the computation; 
>>this number equals
>>         R when there is no missing data
>>-    the matrix, scores, has dimension RxC, and it can be thought of as 
>>the projection of the
>>         eigenvector matrix, loadings, back onto the original data; these 
>>columns of
>>         scores are the principal components.  princomp typically removes 
>>the mean,
>>         so the formula is:
>>         dmpca$scores = t(t(datamatrix) - dmpca$center)%*%dmpca$loadings
>>         and apply(dmpca$scores,2,mean) returns a length C vector of 
>>(effectively)
>>         zeroes; also the principal components (columns of scores) are 
>>orthogonal
>>         (but not orthonormal):
>>         sum(dmpca$scores[,i]*dmpca$scores[,j]) ~ 0 if i != j, > 0 if i == 
>>j
>>-    call contains the function call, in this case princomp(x = 
>>datamatrix)
>>
>>That is all as it should be.
>>
>>
>>In my case R < C, which produces singular results for standard PCA, but 
>>robust methods, like PCAproj, are designed to handle this.  Also, I had 
>>"de-meaned" the data beforehand, so apply(datamatrix,2,mean) produces a 
>>length C vector of (effectively) zeroes.  I ran the following:
>>dmpcaprj=PCAproj(datamatrix,k=4,CalcMethod="sphere",update=TRUE)
>>to get the first four robust components.  When I look at the princomp 
>>object returned as dmpcaprj, some of the results are just what I expect.  
>>For example,
>>-    dmpcaprj$loadings has dimensions Cx4, as expected, and the first four 
>>eigenvectors of
>>         the (robust) covariance matrix are orthonormal:
>>         sum(dmpcaprj$loadings[,i]*dmpcaprj$loadings[,j]) = 1 if i == j, ~ 
>>0 if i != j
>>-    dmpcaprj$sdev contains the square roots of the four corresponding 
>>eigenvalues.
>>-    dmpcaprj$n.obs equals R.
>>-    dmpcaprj$scores has dimensions Rx4, as it should.
>>
>>HOWEVER, the columns of dmpcaprj$scores are neither de-meaned nor 
>>orthogonal.  So,
>>         apply(dmpcaprj$scores,2,mean) is a non-zero vector, and
>>         sum(dmpcaprj$scores[,i]*dmpcaprj$scores[,j]) != 0 if i != j, > 0 
>>if i == j
>>ALSO,
>>-    dmpcaprj$scale is in this case a vector of all 1's, as expected.  But 
>>the length is C, not R.
>>-    dmpcaprj$center is a vector of length C, not R, and the entries are 
>>not equal to either
>>         apply(datamatrix,1,mean)  or  apply(datamatrix,2,mean); I can't 
>>figure out
>>         where they came from.
>>
>>One interesting thing is that the columns of the Rx4 matrix,
>>         dmpcaprj$scores - datamatrix%*%dmpcaprj$loadings
>>are all identically constant vectors, such that each row equals 
>>apply(dmpcaprj$scores,2,mean), since
>>apply(datamatrix%*%dmpcaprj$loadings,2,mean) is a length four vector of 
>>(effectively) zeroes,
>>but I can't interpret the values of these means of dmpcaprj$scores.
>>
>>
>>Can anyone please explain to me what is happening with the scores, scale, 
>>and center parts of the PCAproj results?
>>
>>
>>Thanks!
>>
>>
>>--  TMK  --
>>212-460-5430    home
>>917-656-5351    cell
>>
>>
>>
>>
>
>
>--
>-------------------------------------------------------
>From: Prof. Dr. Peter Filzmoser
>       Dept. of Statistics & Probability Theory
>       Vienna University of Technology
>       Wiedner Hauptstrasse 8-10
>       A-1040 Vienna, Austria
>       Tel. +43 1 58801/10733
>       Fax. +43 1 58801/10799
>       E-mail: P.Filzmoser at tuwien.ac.at
>       Internet:
>       http://www.statistik.tuwien.ac.at/public/filz/
>-------------------------------------------------------
>
>
>


From ssj1364 at gmail.com  Wed Feb 14 21:54:39 2007
From: ssj1364 at gmail.com (sj)
Date: Wed, 14 Feb 2007 13:54:39 -0700
Subject: [R] predict.lm point forecasts with factors
Message-ID: <1c6126db0702141254o4c6b7539j1d9b9da4a0a59696@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/4cf79e4a/attachment.pl 

From Manuel.A.Morales at williams.edu  Wed Feb 14 21:54:41 2007
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Wed, 14 Feb 2007 15:54:41 -0500
Subject: [R] lme4/lmer: P-Values from mcmc samples or chi2-tests?
In-Reply-To: <45D1C108.2030908@agr.uni-goettingen.de>
References: <45D1C108.2030908@agr.uni-goettingen.de>
Message-ID: <1171486482.18200.9.camel@solidago.localdomain>

On Tue, 2007-02-13 at 14:45 +0100, Christoph Scherber wrote:
> Dear R users,
> 
> I have now tried out several options of obtaining p-values for 
> (quasi)poisson lmer models, including Markov-chain Monte Carlo sampling 
> and single-term deletions with subsequent chi-square tests (although I 
> am aware that the latter may be problematic).
> 
> However, I encountered several problems that can be classified as
> (1) the quasipoisson lmer model does not give p-values when summary() is 
> called (see below)
> (2) dependence on the size of the mcmc sample
> (3) lack of correspondence between different p-value estimates.
> 
> How can I proceed, left with these uncertainties in the estimations of 
> the p-values?
> 
> Below is the corresponding R code with some output so that you can see 
> it all for your own:
> 
> ##
> m1<-lmer(number_pollinators~logpatch+loghab+landscape_diversity+(1|site),primula,poisson,method="ML")
> m2<-lmer(number_pollinators~logpatch+loghab+landscape_diversity+(1|site),primula,quasipoisson,method="ML")
> summary(m1);summary(m2)
> 
> #m1: [...]
> Fixed effects:
>                      Estimate Std. Error z value Pr(>|z|)
> (Intercept)         -0.40302    0.57403 -0.7021  0.48262
> logpatch             0.10915    0.04111  2.6552  0.00793 **
> loghab               0.08750    0.06128  1.4279  0.15331
> landscape_diversity  1.02338    0.40604  2.5204  0.01172 *
<snip>
> The p-values from mcmc are:
> 
> ##
> markov1=mcmcsamp(m2,5000)
> 
> HPDinterval(markov1)
>                              lower      upper
> (Intercept)          -1.394287660  0.6023229
> logpatch              0.031154910  0.1906861
> loghab                0.002961281  0.2165285
> landscape_diversity   0.245623183  1.6442544
> log(site.(In))      -41.156007604 -1.6993996
> attr(,"Probability")
> [1] 0.95
> 
> ##
> 
> mcmcpvalue(as.matrix(markov1[,1])) #i.e. the p value for the intercept
> [1] 0.3668
>  > mcmcpvalue(as.matrix(markov1[,2])) #i.e. the p-value for logpatch
> [1] 0.004
>  > mcmcpvalue(as.matrix(markov1[,3])) #i.e. the p-value for loghab
> [1] 0.0598
>  > mcmcpvalue(as.matrix(markov1[,4])) #i.e. the p-value for landscape.div
> [1] 0.0074
> 
> If one runs the mcmcsamp function for, say, 50,000 runs, the p-values 
> are slightly different (not shown here).
> 
> ##here are the p-values summarized in tabular form:
<snip>
[MCMC] 
> logpatch         	0.004
> loghab            	0.0598
> landscape_diversity    	0.0074
<snip>
[single-term deletions] 
> logpatch        	0.007106
> loghab            	0.1704
> landscape_diversity    	0.01276
<snip> 
> To summarize, at least for quasipoisson models, the p-values obtained 
> from mcmcpvalue() are quite different from those obtained using 
> single-term deletions followed by a chisquare test.
> 
> Especially in the case of "loghab", the difference is so huge that one 
> could tend to interpret one of the p-values as "marginally significant".

The P-values from the MCMC chain look suspiciously like 1/2 the others.
Are you effectively doing a 1-tailed test in your MCMC comparison?

-- 
Manuel A. Morales
http://mutualism.williams.edu
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: This is a digitally signed message part
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070214/8ea87faa/attachment.bin 

From roberto.perdisci at gmail.com  Wed Feb 14 22:05:27 2007
From: roberto.perdisci at gmail.com (Roberto Perdisci)
Date: Wed, 14 Feb 2007 16:05:27 -0500
Subject: [R] Putting splom in a function
Message-ID: <cf94d0090702141305k12e7a97chb21395b4f48e9a1@mail.gmail.com>

Hello R list,
  I have a little problem with splom. I'd like to wrap it in a
function, for example:

multi.scatterplot <- function(data,groups,cols,colors) {
    splom(~data[,cols], groups = as.symbol(groups), data = data, panel
= panel.superpose, col=colors)
}

and then call it like in

multi.scatterplot(iris,"Species",1:4,c("green","blue","red"))

but the problem is:
Error in form$groups[form$subscr] : object is not subsettable

if I use
              groups = groups
instead of
              groups = as.symbol(groups)

shomthing is plotted, but not the correct scatterplot.

I think the problem is that I don't cast the 'groups' variable to the
correct type. Besides as.symbol() I tried also as.expression(),
because ?xyplot says "groups: a variable or expression to be evaluated
in the data frame specified by 'data'".
What is the correct type? What as.* should I use?

thank you,
regards,
Roberto


From marc_schwartz at comcast.net  Wed Feb 14 22:24:51 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 14 Feb 2007 15:24:51 -0600
Subject: [R] predict.lm point forecasts with factors
In-Reply-To: <1c6126db0702141254o4c6b7539j1d9b9da4a0a59696@mail.gmail.com>
References: <1c6126db0702141254o4c6b7539j1d9b9da4a0a59696@mail.gmail.com>
Message-ID: <1171488291.4806.119.camel@localhost.localdomain>

On Wed, 2007-02-14 at 13:54 -0700, sj wrote:
> hello,
> 
> I am trying to use predict.lm to make point forecasts based on a model with
> continuous and categorical independent variables
> I have no problems fitting the model using lm, but when I try to use predict
> to make point predictions. it reverts back to the original dataframe and
> gives me the point predictions for the fitted data rather than for the new
> data, I imagine that I am missing something simple but for whatever reason I
> can't figure out why it does not like the new data and is reverting to the
> fitted data. The following code illustrates the problem I am running in to.
> Any help would be appreciated.
> 
> f1 <- rep(c("a","b","c","d"),25)
> f2 <- sample(rep(c("e","f","g","h"),250),100)
> x <- rnorm(100,100)
> y <- rnorm(100,150)
> 
> mdl <- lm(y~x+f1+f2)
> 
> f12 <-rep(c("a","b","c","d"),5)
> f22 <- sample(rep(c("e","f","g","h"),250),20)
> x2 <- rnorm(20,100)
> 
> new <- data.frame(cbind(f12[1],f22[1],x2[1]))
> 
> 
> predict(mdl,new)
> 
> 
> best,
> 
> Spencer

Spencer,

You have two distinct issues going on here:

The initial model that you create 'mdl' is based upon 'f1' and 'f2'
being created as character vectors, not as factors. While the modeling
functions will internally do the coercion, I do not believe that the
predict functions will. 

In fact, you should have noted the following error messages:

> mdl <- lm(y~x+f1+f2)
Warning messages:
1: variable 'f1' converted to a factor in: model.matrix.default(mt, mf,
contrasts) 
2: variable 'f2' converted to a factor in: model.matrix.default(mt, mf,
contrasts) 


So you end up with a 'class' conflict between the model frame object and
the new data object, since the latter will default to coercing 'f12' and
'f22' to factors.

Secondly, 'new' needs to have columns created with the SAME names as
those used in the original model.

Thus, a code sequence along the lines of the following should work:

f1 <- rep(c("a","b","c","d"), 25)
f2 <- sample(rep(c("e","f","g","h"), 250), 100)
x <- rnorm(100, 100)
y <- rnorm(100, 150)

# Create a data frame from the data so
# so that f1 and f2 become factors
DF <- data.frame(y, x, f1, f2)

mdl <- lm(y ~ x + f1 + f2, DF)


f12 <-rep(c("a","b","c","d"), 5)
f22 <- sample(rep(c("e","f","g","h"), 250), 20)
x2 <- rnorm(20, 100)

# Create 'new' in the same way, but naming the
# columns the same as 'DF" above
new <- data.frame(f1 = f12, f2 = f22, x = x2)


# Now run predict on the first row in 'new
> predict(mdl, new[1, ])
[1] 150.3273


The number you come up with should be different, since you are using
random data.

HTH,

Marc Schwartz


From aiminy at iastate.edu  Wed Feb 14 23:14:57 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Wed, 14 Feb 2007 16:14:57 -0600
Subject: [R] rpart tree node label
Message-ID: <6.1.2.0.2.20070214161251.01ca7880@aiminy.mail.iastate.edu>

I generate a tree use rpart.
In the node of tree, split is based on the some factor.
I want to label these node based on the levels of this factor.

Does anyone know how to do this?

Thanks,

Aimin


From liuwensui at gmail.com  Wed Feb 14 23:23:22 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 14 Feb 2007 17:23:22 -0500
Subject: [R] rpart tree node label
In-Reply-To: <6.1.2.0.2.20070214161251.01ca7880@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070214161251.01ca7880@aiminy.mail.iastate.edu>
Message-ID: <1115a2b00702141423l20b0dcfah4686429788808788@mail.gmail.com>

not sure how you want to label it.
could you be more specific?
thanks.

On 2/14/07, Aimin Yan <aiminy at iastate.edu> wrote:
> I generate a tree use rpart.
> In the node of tree, split is based on the some factor.
> I want to label these node based on the levels of this factor.
>
> Does anyone know how to do this?
>
> Thanks,
>
> Aimin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From gunter.berton at gene.com  Wed Feb 14 23:40:43 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 14 Feb 2007 14:40:43 -0800
Subject: [R] Putting splom in a function
In-Reply-To: <cf94d0090702141305k12e7a97chb21395b4f48e9a1@mail.gmail.com>
Message-ID: <002e01c75089$29522760$4d908980@gne.windows.gene.com>

Roberto:

You need to do what ?xyplot says. as.symbol(groups) is not a variable and is
certainly not subsettable. If you have a variable named "groups" in your
data.frame , then ... groups = groups gives the grouping according to the
levels of that variable. If you do not, then it may be picking up a variable
named "groups" from somewhere else, probably your global workspace, which
may be producihg your unexpected results. Or perhaps there is a variable
named "groups" in you data frame which is not what you think it is. Have you
checked?

In any case, please examine or run the examples in ?xyplot, especially those
that use the group = argument.

One note: I do grant you that the phrase "variable or expression" may be
confusing in this context. But do note that ?as.expression explicitly says:

" 'Expression' here is not being used in its colloquial sense, that of
mathematical expressions. Those are calls (see call) in R, and an R
expression vector is a list of calls etc, typically as returned by parse."  

What is meant by the phrase in the xyplot help is "expression" in its
colloquial sense of a math (or more generally, any R) expression, not a
formal expression object, which is what the cast as.expression() gives.

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roberto Perdisci
Sent: Wednesday, February 14, 2007 1:05 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Putting splom in a function

Hello R list,
  I have a little problem with splom. I'd like to wrap it in a
function, for example:

multi.scatterplot <- function(data,groups,cols,colors) {
    splom(~data[,cols], groups = as.symbol(groups), data = data, panel
= panel.superpose, col=colors)
}

and then call it like in

multi.scatterplot(iris,"Species",1:4,c("green","blue","red"))

but the problem is:
Error in form$groups[form$subscr] : object is not subsettable

if I use
              groups = groups
instead of
              groups = as.symbol(groups)

shomthing is plotted, but not the correct scatterplot.

I think the problem is that I don't cast the 'groups' variable to the
correct type. Besides as.symbol() I tried also as.expression(),
because ?xyplot says "groups: a variable or expression to be evaluated
in the data frame specified by 'data'".
What is the correct type? What as.* should I use?

thank you,
regards,
Roberto

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From aiminy at iastate.edu  Wed Feb 14 23:47:05 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Wed, 14 Feb 2007 16:47:05 -0600
Subject: [R] rpart tree node label
In-Reply-To: <1115a2b00702141423l20b0dcfah4686429788808788@mail.gmail.co
 m>
References: <6.1.2.0.2.20070214161251.01ca7880@aiminy.mail.iastate.edu>
	<1115a2b00702141423l20b0dcfah4686429788808788@mail.gmail.com>
Message-ID: <6.1.2.0.2.20070214164028.01cab4f8@aiminy.mail.iastate.edu>

 > levels(training$aa_one)
  [1] "A" "C" "D" "E" "F" "H" "I" "K" "L" "M" "N" "P" "Q" "R" "S" "T" "V" 
"W" "Y"
this is 19 levels of aa_one.

When I see tree,

in one node, it is labeled by

aa_one=bcdfgknop

it is obvious that it is labeled by alphabet letter ,not by levels of aa_one.

I want to get

aa_one=CDE...... such like.

Do you know how to do this

Aimin



At 04:23 PM 2/14/2007, Wensui Liu wrote:
>not sure how you want to label it.
>could you be more specific?
>thanks.
>
>On 2/14/07, Aimin Yan <aiminy at iastate.edu> wrote:
>>I generate a tree use rpart.
>>In the node of tree, split is based on the some factor.
>>I want to label these node based on the levels of this factor.
>>
>>Does anyone know how to do this?
>>
>>Thanks,
>>
>>Aimin
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>--
>WenSui Liu
>A lousy statistician who happens to know a little programming
>(http://spaces.msn.com/statcompute/blog)


From andy_liaw at merck.com  Wed Feb 14 23:49:09 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Feb 2007 17:49:09 -0500
Subject: [R] rpart tree node label  [Broadcast]
In-Reply-To: <1115a2b00702141423l20b0dcfah4686429788808788@mail.gmail.com>
References: <6.1.2.0.2.20070214161251.01ca7880@aiminy.mail.iastate.edu>
	<1115a2b00702141423l20b0dcfah4686429788808788@mail.gmail.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03B7215A@usctmx1106.merck.com>

Try the following to see:

library(rpart)
iris.rp(Sepal.Length ~ Species, iris)
plot(iris.rp)
text(iris.rp)

Two possible solutions:

1. Use text(..., pretty=0).  See ?text.rpart.
2. Use post(..., filename="").

Andy 

From: Wensui Liu
> 
> not sure how you want to label it.
> could you be more specific?
> thanks.
> 
> On 2/14/07, Aimin Yan <aiminy at iastate.edu> wrote:
> > I generate a tree use rpart.
> > In the node of tree, split is based on the some factor.
> > I want to label these node based on the levels of this factor.
> >
> > Does anyone know how to do this?
> >
> > Thanks,
> >
> > Aimin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> --
> WenSui Liu
> A lousy statistician who happens to know a little programming
> (http://spaces.msn.com/statcompute/blog)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From dalfes at itu.edu.tr  Thu Feb 15 00:05:45 2007
From: dalfes at itu.edu.tr (=?UTF-8?B?TsO8emhldCBEYWxmZXM=?=)
Date: Thu, 15 Feb 2007 01:05:45 +0200
Subject: [R] An error message...
Message-ID: <45D395C9.4010302@itu.edu.tr>

Hi,

When I start R on my MacBook Pro (Mac OS X 10.4.8) I get this message:

2007-02-15 00:51:16.672 R[268] *** -[NSBundle load]: Error loading code 
/Users/dalfes/Library/InputManagers/iPLM/iPLM.bundle/Contents/MacOS/iPLM 
for bundle /Users/dalfes/Library/InputManagers/iPLM/iPLM.bundle, error code 
2 (link edit error code 0, error number 0 ())

Can someone help me what it means, and what should I do to fix it?

Thanks,

N?zhet Dalfes
Istanbul Tech. Univ.


From wht_crl at yahoo.com  Wed Feb 14 16:19:23 2007
From: wht_crl at yahoo.com (carol white)
Date: Wed, 14 Feb 2007 07:19:23 -0800 (PST)
Subject: [R] cox PH
Message-ID: <307812.79962.qm@web62006.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/b696f813/attachment.pl 

From youchko at mail.ru  Wed Feb 14 15:30:29 2007
From: youchko at mail.ru (yuri Ilioutchenko)
Date: Wed, 14 Feb 2007 17:30:29 +0300
Subject: [R] glade 3 encoding problem
Message-ID: <E1HHL9R-000FGZ-00.youchko-mail-ru@f77.mail.ru>

Hello dear Developers,

I have a problem in using glade with the statistical package R(through RGtk). While the dialogs in russian encoding work pretty well, the R can't read from these dialogs (getTabLabelText) in russian properly. I run WinXP.

TX.


From tchur at optushome.com.au  Thu Feb 15 02:24:37 2007
From: tchur at optushome.com.au (Tim Churches)
Date: Thu, 15 Feb 2007 12:24:37 +1100
Subject: [R] How to speed up or avoid the for-loops in this example?
Message-ID: <45D3B655.8070601@optushome.com.au>

Any advice, tips, clues or pointers to resources on how best to speed up
or, better still, avoid the loops in the following example code much
appreciated. My actual dataset has several tens of thousands of rows and
lots of columns, and these loops take a rather long time to run.
Everything else which I need to do is done using vectors and those parts
all run very quickly indeed. I spent quite a while doing searches on
r-help and re-reading the various manuals, but couldn't find any
existing relevant advice. I am sure the solution is obvious, but it
escapes me.

Tim C

# create an example data frame, multiple events per subject

year <- c(1980,1982,1996,1985,1987,1990,1991,1992,1999,1972,1983)
event.of.interest <- c(F,T,T,F,F,F,T,F,T,T,F)
subject <- c(1,1,1,2,2,3,3,3,3,4,4)
df <- data.frame(cbind(subject,year,event.of.interest))

# add a per-subject sequence number

df$subject.seq <- 1
for (i in 2:nrow(df)) {
 if (df$subject[i-1] == df$subject[i]) df$subject.seq[i] <-
df$subject.seq[i-1] + 1
}
df

# add an event sequence number which is zero until the first
# event of interest for that subject happens, and then increments
# thereafter

df$event.seq <- 0
for (i in 1:nrow(df)) {
 if (df$subject.seq[i] == 1 ) {
    current.event.seq <- 0
 }
 if (event.of.interest[i] == 1 | current.event.seq > 0)
current.event.seq <- current.event.seq + 1
 df$event.seq[i] <- current.event.seq
}
df


From xuhy at ucla.edu  Thu Feb 15 03:06:55 2007
From: xuhy at ucla.edu (Haiyong Xu)
Date: Wed, 14 Feb 2007 18:06:55 -0800
Subject: [R] integrate over polygon
Message-ID: <E9C86560-5BA2-4DF7-875A-DF462CE31AA3@ucla.edu>

Hi there,

I want to integrate a function over an irregular polygon. Is there  
any function which can implement this easily? Otherwise, I am  
thinking of divide the polygon into very small rectangles and use  
"adapt" to approximate it. Do you have any suggestions to get the  
fine division? Any advice is appreciated.

Haiyong


From jholtman at gmail.com  Thu Feb 15 03:25:24 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 14 Feb 2007 21:25:24 -0500
Subject: [R] How to speed up or avoid the for-loops in this example?
In-Reply-To: <45D3B655.8070601@optushome.com.au>
References: <45D3B655.8070601@optushome.com.au>
Message-ID: <644e1f320702141825r5689056ds72ea5def10cb6f78@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/5182dd82/attachment.pl 

From marc_schwartz at comcast.net  Thu Feb 15 03:48:50 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 14 Feb 2007 20:48:50 -0600
Subject: [R] How to speed up or avoid the for-loops in this example?
In-Reply-To: <45D3B655.8070601@optushome.com.au>
References: <45D3B655.8070601@optushome.com.au>
Message-ID: <1171507730.5002.41.camel@localhost.localdomain>

On Thu, 2007-02-15 at 12:24 +1100, Tim Churches wrote:
> Any advice, tips, clues or pointers to resources on how best to speed up
> or, better still, avoid the loops in the following example code much
> appreciated. My actual dataset has several tens of thousands of rows and
> lots of columns, and these loops take a rather long time to run.
> Everything else which I need to do is done using vectors and those parts
> all run very quickly indeed. I spent quite a while doing searches on
> r-help and re-reading the various manuals, but couldn't find any
> existing relevant advice. I am sure the solution is obvious, but it
> escapes me.
> 
> Tim C
> 
> # create an example data frame, multiple events per subject
> 
> year <- c(1980,1982,1996,1985,1987,1990,1991,1992,1999,1972,1983)
> event.of.interest <- c(F,T,T,F,F,F,T,F,T,T,F)
> subject <- c(1,1,1,2,2,3,3,3,3,4,4)
> df <- data.frame(cbind(subject,year,event.of.interest))
> 
> # add a per-subject sequence number
> 
> df$subject.seq <- 1
> for (i in 2:nrow(df)) {
>  if (df$subject[i-1] == df$subject[i]) df$subject.seq[i] <-
> df$subject.seq[i-1] + 1
> }
> df
> 
> # add an event sequence number which is zero until the first
> # event of interest for that subject happens, and then increments
> # thereafter
> 
> df$event.seq <- 0
> for (i in 1:nrow(df)) {
>  if (df$subject.seq[i] == 1 ) {
>     current.event.seq <- 0
>  }
>  if (event.of.interest[i] == 1 | current.event.seq > 0)
> current.event.seq <- current.event.seq + 1
>  df$event.seq[i] <- current.event.seq
> }
> df

OK, here is one possible solution, though perhaps with a bit more time,
there may be more optimal approaches. 

Using your example data above, but first noting that you do not want to
use:

  df <- data.frame(cbind(subject,year,event.of.interest))

Using cbind() first, creates a matrix and causes all columns to be
coerced to a common data type, obviating the benefit of data frames to
be able to handle multiple data types. For example:

> str(df)
'data.frame':	11 obs. of  3 variables:
 $ subject          : num  1 1 1 2 2 3 3 3 3 4 ...
 $ year             : num  1980 1982 1996 1985 1987 ...
 $ event.of.interest: num  0 1 1 0 0 0 1 0 1 1 ...

Note that your column "event.of.interest" is coerced to a numeric,
rather than staying as a logical.

Thus, use:

df <- data.frame(subject, year, event.of.interest)

> str(df)
'data.frame':	11 obs. of  3 variables:
 $ subject          : num  1 1 1 2 2 3 3 3 3 4 ...
 $ year             : num  1980 1982 1996 1985 1987 ...
 $ event.of.interest: logi  FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE


So, now on to the solution:

# First, order the data frame by increasing order of
# subject number and decreasing order for event.of.interest
# This ensures that these columns are properly sorted
# to facilitate the subsequent code. 

df <- df[order(df$subject, -df$event.of.interest), ]


So, 'df' will look like:

> df
   subject year event.of.interest
2        1 1982              TRUE
3        1 1996              TRUE
1        1 1980             FALSE
4        2 1985             FALSE
5        2 1987             FALSE
7        3 1991              TRUE
9        3 1999              TRUE
6        3 1990             FALSE
8        3 1992             FALSE
10       4 1972              TRUE
11       4 1983             FALSE


# Now use the combinations of sapply(), rle(), seq() and unlist() to
# generate per subject sequences. Note that rle() returns:
#
# > rle(df$subject)
# Run Length Encoding
#   lengths: int [1:4] 3 2 4 2
#   values : num [1:4] 1 2 3 4
#
# See ?rle, ?seq, ?sapply and ?unlist

df$subject.seq <- unlist(sapply(rle(df$subject)$lengths, 
                                function(x) seq(x)))


So, 'df' now looks like:

> df
   subject year event.of.interest subject.seq
2        1 1982              TRUE           1
3        1 1996              TRUE           2
1        1 1980             FALSE           3
4        2 1985             FALSE           1
5        2 1987             FALSE           2
7        3 1991              TRUE           1
9        3 1999              TRUE           2
6        3 1990             FALSE           3
8        3 1992             FALSE           4
10       4 1972              TRUE           1
11       4 1983             FALSE           2


# Now set event.seq to all 0's

df$event.seq <- 0


So, 'df' now looks like:

> df
   subject year event.of.interest subject.seq event.seq
2        1 1982              TRUE           1         0
3        1 1996              TRUE           2         0
1        1 1980             FALSE           3         0
4        2 1985             FALSE           1         0
5        2 1987             FALSE           2         0
7        3 1991              TRUE           1         0
9        3 1999              TRUE           2         0
6        3 1990             FALSE           3         0
8        3 1992             FALSE           4         0
10       4 1972              TRUE           1         0
11       4 1983             FALSE           2         0


# Get the unique subject id's
# See ?unique

subj.id <- unique(df$subject)


# Now get the indices for each subject where event.of.interest
# is TRUE.  See ?which

events <- sapply(subj.id, 
                 function(x) which(df$subject == x & df$event.of.interest))


So, 'events' looks like:

> events
[[1]]
[1] 1 2

[[2]]
integer(0)

[[3]]
[1] 6 7

[[4]]
[1] 10


# Now use sapply() on the above list to create
# individual sequences per list element:

seq <- sapply(events, function(x) seq(along = x))


So 'seq' looks like:

> seq
[[1]]
[1] 1 2

[[2]]
integer(0)

[[3]]
[1] 1 2

[[4]]
[1] 1


# So, for the final step, assign the event sequence values in 'seq' to
# the row indices in 'events':

df$event.seq[unlist(events)] <- unlist(seq)


So, 'df' now looks like this:

> df
   subject year event.of.interest subject.seq event.seq
2        1 1982              TRUE           1         1
3        1 1996              TRUE           2         2
1        1 1980             FALSE           3         0
4        2 1985             FALSE           1         0
5        2 1987             FALSE           2         0
7        3 1991              TRUE           1         1
9        3 1999              TRUE           2         2
6        3 1990             FALSE           3         0
8        3 1992             FALSE           4         0
10       4 1972              TRUE           1         1
11       4 1983             FALSE           2         0


HTH,

Marc SChwartz


From tchur at optushome.com.au  Thu Feb 15 04:25:07 2007
From: tchur at optushome.com.au (Tim Churches)
Date: Thu, 15 Feb 2007 14:25:07 +1100
Subject: [R] How to speed up or avoid the for-loops in this example?
In-Reply-To: <644e1f320702141825r5689056ds72ea5def10cb6f78@mail.gmail.com>
References: <45D3B655.8070601@optushome.com.au>
	<644e1f320702141825r5689056ds72ea5def10cb6f78@mail.gmail.com>
Message-ID: <45D3D293.6030008@optushome.com.au>

jim holtman wrote:
> On 2/14/07, Tim Churches <tchur at optushome.com.au> wrote:
>> Any advice, tips, clues or pointers to resources on how best to speed up
>> or, better still, avoid the loops in the following example code much
>> appreciated. My actual dataset has several tens of thousands of rows and
>> lots of columns, and these loops take a rather long time to run.
>> Everything else which I need to do is done using vectors and those parts
>> all run very quickly indeed. I spent quite a while doing searches on
>> r-help and re-reading the various manuals, but couldn't find any
>> existing relevant advice. I am sure the solution is obvious, but it
>> escapes me.
>>
>> Tim C
>>
>> # create an example data frame, multiple events per subject
>>
>> year <- c(1980,1982,1996,1985,1987,1990,1991,1992,1999,1972,1983)
>> event.of.interest <- c(F,T,T,F,F,F,T,F,T,T,F)
>> subject <- c(1,1,1,2,2,3,3,3,3,4,4)
>> df <- data.frame(cbind(subject,year,event.of.interest))
>>
>> # add a per-subject sequence number
>>
>> df$subject.seq <- 1
>> for (i in 2:nrow(df)) {
>> if (df$subject[i-1] == df$subject[i]) df$subject.seq[i] <-
>> df$subject.seq[i-1] + 1
>> }
>> df
> 
> # add an event sequence number which is zero until the first
>> # event of interest for that subject happens, and then increments
>> # thereafter
>>
>> df$event.seq <- 0
>> for (i in 1:nrow(df)) {
>> if (df$subject.seq[i] == 1 ) {
>>    current.event.seq <- 0
>> }
>> if (event.of.interest[i] == 1 | current.event.seq > 0)
>> current.event.seq <- current.event.seq + 1
>> df$event.seq[i] <- current.event.seq
>> }
>> df
> 
> 
> 
> try:
> 
>> df <- data.frame(cbind(subject,year,event.of.interest))
>> df <- do.call(rbind,by(df, df$subject, function(z){z$subject.seq <-
> seq(nrow(z)); z}))
>> df
>      subject year event.of.interest subject.seq
> 1.1        1 1980                 0           1
> 1.2        1 1982                 1           2
> 1.3        1 1996                 1           3
> 2.4        2 1985                 0           1
> 2.5        2 1987                 0           2
> 3.6        3 1990                 0           1
> 3.7        3 1991                 1           2
> 3.8        3 1992                 0           3
> 3.9        3 1999                 1           4
> 4.10       4 1972                 1           1
> 4.11       4 1983                 0           2
>> # determine first event
>> df <- do.call(rbind, by(df, df$subject, function(x){
> +     # determine first event
> +     .first <- cumsum(x$event.of.interest)
> +     # create sequence after first non-zero
> +     .first <- cumsum(.first > 0)
> +     x$event.seq <- .first
> +     x
> + }))
>> df
>        subject year event.of.interest subject.seq event.seq
> 1.1.1        1 1980                 0           1         0
> 1.1.2        1 1982                 1           2         1
> 1.1.3        1 1996                 1           3         2
> 2.2.4        2 1985                 0           1         0
> 2.2.5        2 1987                 0           2         0
> 3.3.6        3 1990                 0           1         0
> 3.3.7        3 1991                 1           2         1
> 3.3.8        3 1992                 0           3         2
> 3.3.9        3 1999                 1           4         3
> 4.4.10       4 1972                 1           1         1
> 4.4.11       4 1983                 0           2         2

Thanks Jim, that works a treat, over an order of magnitude faster than
the for-loops.

Anders Nielsen also provided this solution:

  df$subject.seq<-unlist(tapply(df$subject,
                                  df$subject,
                                  function(x)1:length(x)
                               )
                        )

Doing it that way is about 5 times faster than using rbind(). But Jim's
use of cumsum on the logical vector is very nifty.

I have now combined Jim's function with Anders' column-oriented approach
and the result is that my code now runs about two orders of magnitude
faster.

Many thanks,

Tim C


From tchur at optushome.com.au  Thu Feb 15 04:29:27 2007
From: tchur at optushome.com.au (Tim Churches)
Date: Thu, 15 Feb 2007 14:29:27 +1100
Subject: [R] How to speed up or avoid the for-loops in this example?
In-Reply-To: <1171507730.5002.41.camel@localhost.localdomain>
References: <45D3B655.8070601@optushome.com.au>
	<1171507730.5002.41.camel@localhost.localdomain>
Message-ID: <45D3D397.1070708@optushome.com.au>

Marc Schwartz wrote:
> OK, here is one possible solution, though perhaps with a bit more time,
> there may be more optimal approaches. 
> 
> Using your example data above, but first noting that you do not want to
> use:
> 
>   df <- data.frame(cbind(subject,year,event.of.interest))
> 
> Using cbind() first, creates a matrix and causes all columns to be
> coerced to a common data type, obviating the benefit of data frames to
> be able to handle multiple data types. 

Yes, quite right, the cbind() was unnecessary. I'm not making my real
data frame that way, however.

> So, now on to the solution:
> 
> # First, order the data frame by increasing order of
> # subject number and decreasing order for event.of.interest
> # This ensures that these columns are properly sorted
> # to facilitate the subsequent code. 
> 
> df <- df[order(df$subject, -df$event.of.interest), ]
> 
> 
> So, 'df' will look like:
> 
>> df
>    subject year event.of.interest
> 2        1 1982              TRUE
> 3        1 1996              TRUE
> 1        1 1980             FALSE
> 4        2 1985             FALSE
> 5        2 1987             FALSE
> 7        3 1991              TRUE
> 9        3 1999              TRUE
> 6        3 1990             FALSE
> 8        3 1992             FALSE
> 10       4 1972              TRUE
> 11       4 1983             FALSE
> 
> 
> # Now use the combinations of sapply(), rle(), seq() and unlist() to
> # generate per subject sequences. Note that rle() returns:
> #
> # > rle(df$subject)
> # Run Length Encoding
> #   lengths: int [1:4] 3 2 4 2
> #   values : num [1:4] 1 2 3 4
> #
> # See ?rle, ?seq, ?sapply and ?unlist
> 
> df$subject.seq <- unlist(sapply(rle(df$subject)$lengths, 
>                                 function(x) seq(x)))
> 
> 
> So, 'df' now looks like:
> 
>> df
>    subject year event.of.interest subject.seq
> 2        1 1982              TRUE           1
> 3        1 1996              TRUE           2
> 1        1 1980             FALSE           3
> 4        2 1985             FALSE           1
> 5        2 1987             FALSE           2
> 7        3 1991              TRUE           1
> 9        3 1999              TRUE           2
> 6        3 1990             FALSE           3
> 8        3 1992             FALSE           4
> 10       4 1972              TRUE           1
> 11       4 1983             FALSE           2
> 
> 
> # Now set event.seq to all 0's
> 
> df$event.seq <- 0
> 
> 
> So, 'df' now looks like:
> 
>> df
>    subject year event.of.interest subject.seq event.seq
> 2        1 1982              TRUE           1         0
> 3        1 1996              TRUE           2         0
> 1        1 1980             FALSE           3         0
> 4        2 1985             FALSE           1         0
> 5        2 1987             FALSE           2         0
> 7        3 1991              TRUE           1         0
> 9        3 1999              TRUE           2         0
> 6        3 1990             FALSE           3         0
> 8        3 1992             FALSE           4         0
> 10       4 1972              TRUE           1         0
> 11       4 1983             FALSE           2         0
> 
> 
> # Get the unique subject id's
> # See ?unique
> 
> subj.id <- unique(df$subject)
> 
> 
> # Now get the indices for each subject where event.of.interest
> # is TRUE.  See ?which
> 
> events <- sapply(subj.id, 
>                  function(x) which(df$subject == x & df$event.of.interest))
> 
> 
> So, 'events' looks like:
> 
>> events
> [[1]]
> [1] 1 2
> 
> [[2]]
> integer(0)
> 
> [[3]]
> [1] 6 7
> 
> [[4]]
> [1] 10
> 
> 
> # Now use sapply() on the above list to create
> # individual sequences per list element:
> 
> seq <- sapply(events, function(x) seq(along = x))
> 
> 
> So 'seq' looks like:
> 
>> seq
> [[1]]
> [1] 1 2
> 
> [[2]]
> integer(0)
> 
> [[3]]
> [1] 1 2
> 
> [[4]]
> [1] 1
> 
> 
> # So, for the final step, assign the event sequence values in 'seq' to
> # the row indices in 'events':
> 
> df$event.seq[unlist(events)] <- unlist(seq)
> 
> 
> So, 'df' now looks like this:
> 
>> df
>    subject year event.of.interest subject.seq event.seq
> 2        1 1982              TRUE           1         1
> 3        1 1996              TRUE           2         2
> 1        1 1980             FALSE           3         0
> 4        2 1985             FALSE           1         0
> 5        2 1987             FALSE           2         0
> 7        3 1991              TRUE           1         1
> 9        3 1999              TRUE           2         2
> 6        3 1990             FALSE           3         0
> 8        3 1992             FALSE           4         0
> 10       4 1972              TRUE           1         1
> 11       4 1983             FALSE           2         0
> 
> 
> HTH,
> 
> Marc SChwartz

Wow, that's very trick, or tricky. It works but it is a bit slower and
more complex than the Holtzman/Nielsen approach. But some interesting
ides there which I shall bear in mind.

Tim C


From jholtman at gmail.com  Thu Feb 15 05:52:18 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 14 Feb 2007 23:52:18 -0500
Subject: [R] How to speed up or avoid the for-loops in this example?
In-Reply-To: <45D3D293.6030008@optushome.com.au>
References: <45D3B655.8070601@optushome.com.au>
	<644e1f320702141825r5689056ds72ea5def10cb6f78@mail.gmail.com>
	<45D3D293.6030008@optushome.com.au>
Message-ID: <644e1f320702142052o5ddae13xe40fa3edc0bc27cd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/df4b8575/attachment.pl 

From e.catchpole at adfa.edu.au  Thu Feb 15 06:12:47 2007
From: e.catchpole at adfa.edu.au (ecatchpole)
Date: Thu, 15 Feb 2007 16:12:47 +1100
Subject: [R] integrate over polygon
In-Reply-To: <E9C86560-5BA2-4DF7-875A-DF462CE31AA3@ucla.edu>
References: <E9C86560-5BA2-4DF7-875A-DF462CE31AA3@ucla.edu>
Message-ID: <45D3EBCF.9020206@adfa.edu.au>

Haiyong,

There may be better ways, but this what I'd do.  (And I'm not an expert 
on this.)

(a) surround the polygon with a rectangle,

(b) define, via an indicator function, a new function that is equal to 
your desired function within the polygon, and zero outside it,

(c) use adapt() to integrate the new function over the whole rectangle.

The tricky part is (b).  How difficult this is depends on how 
complicated the polygon is.  If it's convex then it can be represented 
by a set of inequalities Ax >= 0 and Bx <= 0.

Ted.

Haiyong Xu wrote on 02/15/2007 01:06 PM:
> Hi there,
>
> I want to integrate a function over an irregular polygon. Is there  
> any function which can implement this easily? Otherwise, I am  
> thinking of divide the polygon into very small rectangles and use  
> "adapt" to approximate it. Do you have any suggestions to get the  
> fine division? Any advice is appreciated.
>
> Haiyong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
 Dr E.A. Catchpole  
 Visiting Fellow
 Univ of New South Wales at ADFA, Canberra, Australia
    _	  and University of Kent, Canterbury, England
   'v'	  - www.pems.adfa.edu.au/~ecatchpole          
  /   \	  - fax: +61 2 6268 8786		   
   m m    - ph:  +61 2 6268 8895


From Scott.Williams at petermac.org  Thu Feb 15 08:32:21 2007
From: Scott.Williams at petermac.org (Williams Scott)
Date: Thu, 15 Feb 2007 18:32:21 +1100
Subject: [R] bootcov and cph error
Message-ID: <46B75B4A4A45914ABB0901364EFF4A209B0275@PMC-EMAIL.petermac.org.au>

Hi all,
I am trying to get bootstrap resampled estimates of covariates in a Cox
model using cph (Design library).

Using the following I get the error:

> ddist2.abr <- datadist(data2.abr)
> options(datadist='ddist2.abr') 
> cph1.abr <- cph(Surv(strt3.abr,loc3.abr)~cov.a.abr+cov.b.abr,
data=data2.abr, x=T, y=T) 
> boot.cph1 <- bootcov(cph1.abr, B=100, coef.reps=TRUE, pr=T)
1 Error in oosl(f, matxv(X, cof), Y) : not implemented for cph models
>

Removing coef.reps argument works fine, but I really need the
coefficients if at all possible. I cant find anything in the help files
suggesting that I cant use coef.reps in a cph model. Any help
appreciated.

Cheers

Scott

_____________________________

 

Dr. Scott Williams MD

Peter MacCallum Cancer Centre

Melbourne, Australia

scott.williams at petermac.org


From johannes_graumann at web.de  Thu Feb 15 09:40:18 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 15 Feb 2007 09:40:18 +0100
Subject: [R] ELIF?
References: <eqvdo1$rj1$1@sea.gmane.org> <45D34AAD.2070509@stats.uwo.ca>
Message-ID: <er169i$2r9$1@sea.gmane.org>

Thanks for your help!

Joh

Duncan Murdoch wrote:

> On 2/14/2007 11:35 AM, Johannes Graumann wrote:
>> Hello,
>> 
>> Is there an elegant way to implement something like the elif function
>> (e.g. python) and prevent multiple if-else contruct concatenation when
>> coding in R?
> 
> Why not just concatenate, using "else if"?  For example,
> 
> if (condition1) {
>     action 1
> } else if (condition2) {
>     action 2
> } else if (condition3) {
>     action 3
> } else {
>     default action
> }
> 
> There is a switch() function if the condition is something that can be
> computed in advance, e.g. from the example on its man page:
> 
>   answer <- switch(type,
>               mean = mean(x),
>               median = median(x),
>               trimmed = mean(x, trim = .1))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From johannes_graumann at web.de  Thu Feb 15 09:41:20 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 15 Feb 2007 09:41:20 +0100
Subject: [R] lapply and use.names howto
Message-ID: <er16bh$2r9$2@sea.gmane.org>

Hello,

How do I make R use the use.names option in a context like this:


lapply(data, function(x)(read.table(x,quote="",header=TRUE,sep="\t")),
use.names=TRUE)

Thanks for any insights,

Joh


From ripley at stats.ox.ac.uk  Thu Feb 15 09:59:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Feb 2007 08:59:13 +0000 (GMT)
Subject: [R] lapply and use.names howto
In-Reply-To: <er16bh$2r9$2@sea.gmane.org>
References: <er16bh$2r9$2@sea.gmane.org>
Message-ID: <Pine.LNX.4.64.0702150854240.14688@auk.stats>

On Thu, 15 Feb 2007, Johannes Graumann wrote:

> How do I make R use the use.names option in a context like this:

What 'use.names option'?  sapply has a USE.NAMES argument, but lapply does 
not.

> lapply(data, function(x)(read.table(x,quote="",header=TRUE,sep="\t")),
> use.names=TRUE)

'data' is the name of an R function.  If say 'dd' were a character vector 
of file names you could do

res <- lapply(dd, function(x)(read.table(x,quote="",header=TRUE,sep="\t"))
names(res) <- dd

or even

sapply(dd, function(x)(read.table(x,quote="",header=TRUE,sep="\t"),
        simplify = FALSE)

which does the same thing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From johannes_graumann at web.de  Thu Feb 15 10:06:37 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 15 Feb 2007 10:06:37 +0100
Subject: [R] lapply and use.names howto
References: <er16bh$2r9$2@sea.gmane.org>
	<Pine.LNX.4.64.0702150854240.14688@auk.stats>
Message-ID: <er17qt$7ia$1@sea.gmane.org>

Thanks a lot for opening my eyes.

Joh

Prof Brian Ripley wrote:

> On Thu, 15 Feb 2007, Johannes Graumann wrote:
> 
>> How do I make R use the use.names option in a context like this:
> 
> What 'use.names option'?  sapply has a USE.NAMES argument, but lapply does
> not.
> 
>> lapply(data, function(x)(read.table(x,quote="",header=TRUE,sep="\t")),
>> use.names=TRUE)
> 
> 'data' is the name of an R function.  If say 'dd' were a character vector
> of file names you could do
> 
> res <- lapply(dd, function(x)(read.table(x,quote="",header=TRUE,sep="\t"))
> names(res) <- dd
> 
> or even
> 
> sapply(dd, function(x)(read.table(x,quote="",header=TRUE,sep="\t"),
>         simplify = FALSE)
> 
> which does the same thing.
>


From jim at bitwrit.com.au  Thu Feb 15 10:12:05 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 15 Feb 2007 20:12:05 +1100
Subject: [R] symbols hidden in polar.plot
In-Reply-To: <37DAD1EC-45F8-4914-AE22-EDCA239EB0B4@northwestern.edu>
References: <37DAD1EC-45F8-4914-AE22-EDCA239EB0B4@northwestern.edu>
Message-ID: <45D423E5.1090605@bitwrit.com.au>

Sumitrajit Dhar wrote:
> Hi Folks,
> 
> Here is my attempt at a simple polar plot.
> 
> 
>  > pos <- seq(0,360,by=5)
>  > tspk <- rep(c(1,0,1),c(13,47,13))
>  > require(plotrix)
>  > polar.plot(tspk,pos,rp.type="s",point.symbols=17,point.col="green4")
> 
> 
> I only see half the symbols, the other half of each symbol is hidden  
> under the circular grid. In fact if I change rp.type="r", I see the  
> lines being plotted and then the grid gets plotted over it and hides  
> the lines.
> 
> What am I doing wrong?

Hi Sumitrajit,
You aren't doing anything wrong, just unusual. 47 of your symbols are 
placed exactly in the center of the plot, so that only one of them is 
visible. Most of that is covered by the radial lines that the function 
draws. The other 26 are placed on the periphery of the plotting area and 
  seven of these will have the same radial lines drawn across them. If 
you want to see what I mean, try this:

fix(radial.plot)

now comment out line 78

  # segments(0,0,xpos,ypos,col=grid.col)

save and quit your editor. Now try your last line like this:

polar.plot(tspk,pos,rp.type="s",point.symbols=17,
  point.col="green4",show.grid=FALSE)

This gives me a chance to ask any interested R users the question:

"Do you prefer the radial.plot family to draw the lines, symbols or 
polygons first or last (that is, on top of everything else)?"

Thanks for any answers.

Jim


From wolfram at fischer-zim.ch  Thu Feb 15 10:34:26 2007
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Thu, 15 Feb 2007 10:34:26 +0100
Subject: [R] html image maps for panels of a lattice graphic
Message-ID: <20070215093426.GA6909@s1x.fischer-zim.local>

Is there a function to produce images and corresponding
html image maps for the panels of a lattice graphic?

Thanks - Wolfram


From auditing_dept at abnamrobank.nl  Thu Feb 15 11:06:22 2007
From: auditing_dept at abnamrobank.nl (Abn Amro Bank)
Date: Thu, 15 Feb 2007 05:06:22 -0500
Subject: [R] GoodNews!
Message-ID: <E1HHdVO-000846-BZ@ws19.peruserver.com>

Good day!

 Upon special auditing panel set-up by the Boards of directors of my bank ABN-AMRO BANK LTD (Amsterdam, The Netherlands) to investigate all credit account operated by it costumers, that has been in operation for the last Seven (7) years. I discover a credit account with A/C N0:NL85ABNA6008341843 floating with a huge funds amounting to(7,467,000.00Euros). 

On further trace I came across the file and found out that the true owner of this funds is Late Engineer Christian Eich, a German who Ran carmaker BMW's museum unfortunately lost his life, including his wife and two children in the plane crash ( Flight AF4590) which crashed on July 31st 2000 killing all 109 people on board.

For more information visit the web link below:
http://news.bbc.co.uk/1/hi/world/europe/859479.stm

I have been monitoring this account ever since I discovered this information and found out that no body has ever came on behalf of the Late Engineer Christian Eich as is next of kin for this fund. I am contacting you as a reliable partner to execute this transaction with me. Upon your acceptance, I will provide you with his account file information's and documents of deposit of the account and a text of application you will submit to the bank. I will then instruct you on how to claim as the next of kin to the late Engineer Christian Eich, so that the funds could be transfer to your A/C for business purpose between you and I, without any question arising from the bank.

 For your information, the transfer payment to apply for as the next of kin should be Five Million Five Hundred Thousand And Fifty Five Euros (5,500,055.00 Euros). As soon as you confirm your interest and co-operation with me, I will give you guidelines on how you can transact with the bank for the release of the funds to you.

For more details contact me via email: ryan.vandam at yahoo.de 

Regards, 
Ryan F. VanDam, 
Auditor (Abn-Amro Bank), 
Amsterdam, De Netherlands.


From rahmad at gwdg.de  Thu Feb 15 11:09:26 2007
From: rahmad at gwdg.de (Rauf Ahmad)
Date: Thu, 15 Feb 2007 11:09:26 +0100
Subject: [R] Generating MVN Data
Message-ID: <45D43156.4050908@gwdg.de>


Dear All

many thanks for your kind help and quick response. I actually used the 
same code (but as rmvnorm in library mvtnorm) to generate data. But I 
was not sure since I thought I might have to do some decomposition of 
the matrix and then generate the data. But now it is confirmed.

with many thanks again and regards

M. R. Ahmad


Dear All

I want to generate multivariate normal data in R for a given covariance
matrix, i.e. my generated data must have the given covariance matrix. I
know the rmvnorm command is to be used but may be I am failing to
properly assign the covariance matrix.

Any help will be greatly appreciated

thanks.

M. R. Ahmad


From snunes at gmail.com  Thu Feb 15 11:23:51 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Thu, 15 Feb 2007 10:23:51 +0000
Subject: [R] Working with temporal data
Message-ID: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>

Hi,

I have several files with data in this format:

20070102
20070102
20070106
20070201
...

The data is sorted and each line represents a date (YYYYMMDD). I would
like to analyze this data using R. For instance, I would like to have
a histogram by year, month or day.

I've already made a simple Perl script that aggregates this data but I
believe that R can be much more powerful and easy on this kind of
work.

Any suggestions on where to start?

Thanks in advance,
S?rgio Nunes


From deepayan.sarkar at gmail.com  Thu Feb 15 11:28:55 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 15 Feb 2007 02:28:55 -0800
Subject: [R] legend in lattice densityplot [Broadcast]
In-Reply-To: <358f4d650702140649k26eb483dnd50859b79c5e23b1@mail.gmail.com>
References: <358f4d650611290911t3f1f017dm5c84e04c5d4a1210@mail.gmail.com>
	<358f4d650611290939n261771e3sa4ed1d0ea0674978@mail.gmail.com>
	<456DCD87.6080908@optonline.net>
	<358f4d650611300711o3dcc4fbfjea3a8af84d376940@mail.gmail.com>
	<971536df0611300725u47528643p4b0bcec815fa176c@mail.gmail.com>
	<358f4d650611300814i7be5fc54l24877976e3c3e114@mail.gmail.com>
	<971536df0611300926g78e5129ct9d28d968e5528abe@mail.gmail.com>
	<358f4d650702140545j661fde9epf4f6db098055d241@mail.gmail.com>
	<4E9A692D8755DF478B56A2892388EE1F0182919E@usctmx1118.merck.com>
	<358f4d650702140649k26eb483dnd50859b79c5e23b1@mail.gmail.com>
Message-ID: <eb555e660702150228g2a848c4ax63fdaaed2d8bea5c@mail.gmail.com>

On 2/14/07, Albert Vilella <avilella at gmail.com> wrote:
> I am defining the legend using trellis.par.set (not sure if
> correctly), and space does not seem to do the trick. auto-key (here
> commented) places it to the top...

You want 'auto.key = list(space = "right")'.

Deepayan

> a = rep(c("alfa","beta","gamma","alfa","beta","gamma"),100)
> b = rnorm(600)
> input=data.frame(a,b)
> densityplot(~(input$b),
>   groups=input$a,
>   plot.points=FALSE,
> #  auto.key=TRUE,
>   space = "left",
>   trellis.par.set(superpose.line = list(
>     col = rep(
> c("yellow","green","red","blue","orange","pink","lightblue","black","brown"),
> 3) ,
>     lwd=3,
>     lty = rep( c(1,2,3), each = 9) )
>   )
> )
>
>
> On 2/14/07, Wiener, Matthew <matthew_wiener at merck.com> wrote:
> > From the documentation for xyplot (referred to from densityplot):
> >
> > The position of the key can be controlled in either of two possible
> > ways. If a component called space is present, the key is positioned
> > outside the plot region, in one of the four sides, determined by the
> > value of space, which can be one of "top", "bottom", "left" and "right".
> >
> >
> > Hope this helps,
> >
> > Matt Wiener
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
> > Sent: Wednesday, February 14, 2007 8:46 AM
> > To: R-help at stat.math.ethz.ch
> > Subject: Re: [R] legend in lattice densityplot [Broadcast]
> >
> > How can I place the legend to the left or right of the densityplot? By
> > default, it goes at the top, and as it is a rather long list, the
> > density plot only uses half the space of the whole graphic...
> >
> > On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > Me too on Windows XP.
> > >
> > > Its probably just a bug or unimplemented feature in the SVG driver.
> > > Write to the maintainer of that package
> > >
> > > For a workaround generate fig output and then convert it to svg using
> > whatever
> > > fig editor or converter you have.
> > >
> > > (On my windows system I use the free fig2dev converter although it
> > inserted
> > > a DOCTYPE statement into the generated SVG file that IE7 did not
> > recognize
> > > but once I manually deleted that it displayed ok in IE7.)
> > >
> > > # after producing file01.fig run
> > > #   fig2dev -L svg file01.fig file01.svg
> > > # or use some other fig to svg converter or editor
> > > xfig(file = "/file01.fig", onefile = TRUE)
> > > library(lattice)
> > > set.seed(1)
> > > DF <- data.frame(x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
> > >        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> > > densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points = FALSE,
> > >  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty =
> > 1:2,
> > >  lwd = c(1,1,1,1,2))))
> > > dev.off()
> > >
> > >
> > > On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > > > Should it be a problem to print this dashed line plots as svgs?
> > > >
> > > > library(RSvgDevice)
> > > > devSVG(file = "/home/avilella/file01.svg",
> > > >       width = 20, height = 16, bg = "white", fg = "black",
> > onefile=TRUE,
> > > >       xmlHeader=TRUE)
> > > > densityplot(...)
> > > > dev.off()
> > > >
> > > > I am getting all the lines as continuous, not dashed...
> > > >
> > > > On 11/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > Yes  by using the lty suboption of superpose.line.
> > > > > Here is a modification of the prior example to illustrate:
> > > > > We also use lwd as well in this example.
> > > > >
> > > > > set.seed(1)
> > > > > DF <- data.frame(x =
> > c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6)),
> > > > >        f = sample(c("A","B","C","D","E"),300,replace=TRUE))
> > > > > library(lattice)
> > > > > densityplot(~ x, DF, groups = f, auto.key = TRUE, plot.points =
> > FALSE,
> > > > >  par.settings = list(superpose.line = list(col = c(1,1,2,2), lty =
> > 1:2,
> > > > >  lwd = c(1,1,1,1,2))))
> > > > >
> > > > >
> > > > > On 11/30/06, Albert Vilella <avilella at gmail.com> wrote:
> > > > > > Can I combine colors and line types? For example, would it be
> > possible
> > > > > > to have 5 colors per 2 types of lines (continuous and dashed)?
> > > > > >
> > > > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > > > Albert Vilella wrote:
> > > > > > > > Are this legend colors correlated to the plot?
> > > > > > >
> > > > > > >   They are if you rely on the colors in
> > > > > > >
> > > > > > > trellis.par.get("superpose.line")$col
> > > > > > >
> > > > > > >   If you want different colors you might use trellis.par.set()
> > to
> > > > > > > temporarily change the colors:
> > > > > > >
> > > > > > > x <- c(rnorm(100,-2,1),rnorm(100,0,1),rnorm(100,2,1))
> > > > > > > f <- rep(c("A","B","C"), each=100)
> > > > > > > df <- data.frame(x,f)
> > > > > > > library(lattice)
> > > > > > >
> > > > > > > oldpar <- trellis.par.get("superpose.line")$col
> > > > > > >
> > > > > > > trellis.par.set(superpose.line = list(col = heat.colors(3)))
> > > > > > >
> > > > > > > densityplot(~ x, groups = f, data = df,
> > > > > > >                  plot.points=FALSE,
> > > > > > >                  auto.key=TRUE)
> > > > > > >
> > > > > > > trellis.par.set(superpose.line = list(col = oldpar))
> > > > > > >
> > > > > > >   If you don't require points or lines in the key, you also
> > could do
> > > > > > > something like this:
> > > > > > >
> > > > > > > densityplot(~ x, groups = f, data = df,
> > > > > > >                  plot.points=FALSE,
> > > > > > >                  key = simpleKey(levels(df$f),
> > > > > > >                                  lines=FALSE,
> > > > > > >                                  points=FALSE,
> > > > > > >                                  col=heat.colors(3)),
> > > > > > >                  col=heat.colors(3))
> > > > > > >
> > > > > > >   To use your own colors without changing the trellis settings
> > and to
> > > > > > > get lines or points in the key, you probably need at least to
> > use key =
> > > > > > > simpleKey() rather than the auto.key argument, and you may
> > need to look
> > > > > > > into draw.key().  Other people on the list might know simpler
> > approaches
> > > > > > > for using your own colors in this situation.
> > > > > > >
> > > > > > > > If I do a:
> > > > > > > >
> > > > > > > > densityplot(~x, groups=f, plot.points=FALSE,
> > > > > > > > auto.key=TRUE,col=heat.colors(5))
> > > > > > > >
> > > > > > > > I get different colors in the legend than the plot...
> > > > > > > >
> > > > > > > >
> > > > > > > > On 11/29/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > > > > > > >> Albert Vilella wrote:
> > > > > > > >> > Hi,
> > > > > > > >> >
> > > > > > > >> > I have a densityplot like this:
> > > > > > > >> >
> > > > > > > >> > x = c(rnorm(100,1,2),rnorm(100,2,4),rnorm(100,3,6))
> > > > > > > >> > f = sample(c("A","B","C","D","E"),300,replace=TRUE)
> > > > > > > >> > df=data.frame(x,f)
> > > > > > > >> > library(lattice)
> > > > > > > >> > attach(df)
> > > > > > > >> > densityplot(~x, groups=f)
> > > > > > > >> >
> > > > > > > >> > And I want to add a legend with the colours for the
> > factors. How can
> > > > > > > >> I do that?
> > > > > > > >> > How can I not have the dots of the distribution at the
> > bottom, or at
> > > > > > > >> > least, make them occupy less vertical space?
> > > > > > > >>
> > > > > > > >>   Change the last line to the following:
> > > > > > > >>
> > > > > > > >> densityplot(~x, groups=f, plot.points=FALSE, auto.key=TRUE)
> > > > > > > >>
> > > > > > > >> See ?panel.densityplot .
> > > > > > > >>
> > > > > > > >> > ______________________________________________
> > > > > > > >> > R-help at stat.math.ethz.ch mailing list
> > > > > > > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > > >> > PLEASE do read the posting guide
> > > > > > > >> http://www.R-project.org/posting-guide.html
> > > > > > > >> > and provide commented, minimal, self-contained,
> > reproducible code.
> > > > > > > >> >
> > > > > > > >>
> > > > > > > >> --
> > > > > > > >> Chuck Cleland, Ph.D.
> > > > > > > >> NDRI, Inc.
> > > > > > > >> 71 West 23rd Street, 8th floor
> > > > > > > >> New York, NY 10010
> > > > > > > >> tel: (212) 845-4495 (Tu, Th)
> > > > > > > >> tel: (732) 512-0171 (M, W, F)
> > > > > > > >> fax: (917) 438-0894
> > > > > > > >>
> > > > > > > >
> > > > > > >
> > > > > > > --
> > > > > > > Chuck Cleland, Ph.D.
> > > > > > > NDRI, Inc.
> > > > > > > 71 West 23rd Street, 8th floor
> > > > > > > New York, NY 10010
> > > > > > > tel: (212) 845-4495 (Tu, Th)
> > > > > > > tel: (732) 512-0171 (M, W, F)
> > > > > > > fax: (917) 438-0894
> > > > > > >
> > > > > >
> > > > > > ______________________________________________
> > > > > > R-help at stat.math.ethz.ch mailing list
> > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > > > > and provide commented, minimal, self-contained, reproducible
> > code.
> > > > > >
> > > > >
> > > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> >
> ------------------------------------------------------------------------------
> > Notice:  This e-mail message, together with any attachment...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kemp.samuel at googlemail.com  Thu Feb 15 11:34:23 2007
From: kemp.samuel at googlemail.com (Samuel Kemp)
Date: Thu, 15 Feb 2007 10:34:23 +0000
Subject: [R] count sequence of integers
Message-ID: <3a5596550702150234r40466120u90e5705897bf30ec@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/33f49b3a/attachment.pl 

From Roger.Bivand at nhh.no  Thu Feb 15 11:45:44 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 15 Feb 2007 11:45:44 +0100 (CET)
Subject: [R] count sequence of integers
In-Reply-To: <3a5596550702150234r40466120u90e5705897bf30ec@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0702151145230.7954-100000@reclus.nhh.no>

On Thu, 15 Feb 2007, Samuel Kemp wrote:

> Hi,
> 
> I would like to be able to count a sequence of numbers. For example, given a
> vector of the following integers
> 
> (1,1,1,2,2,2,2,3,3,3,3,3,3,1,1,1,1, 3,3)
> 
> the function would return
> 
> (3, 4, 6, 4,2)

?rle

> x <- c(1,1,1,2,2,2,2,3,3,3,3,3,3,1,1,1,1, 3,3)
> rle(x)$lengths
[1] 3 4 6 4 2

> 
> Does anyone have any cool ideas to solve this?
> 
> Any help appreciated.
> 
> Regards,
> 
> Sam.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ima at difres.dk  Thu Feb 15 11:49:57 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Thu, 15 Feb 2007 11:49:57 +0100
Subject: [R] FW: simpleR or usingR package by Verzani
Message-ID: <68E7981938EAF54F987AD3848A0A6416DDC99B@ka-mail01.dfu.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/886fcc94/attachment.pl 

From martin.becker at mx.uni-saarland.de  Thu Feb 15 11:52:00 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Thu, 15 Feb 2007 11:52:00 +0100
Subject: [R] count sequence of integers
In-Reply-To: <3a5596550702150234r40466120u90e5705897bf30ec@mail.gmail.com>
References: <3a5596550702150234r40466120u90e5705897bf30ec@mail.gmail.com>
Message-ID: <45D43B50.7060001@mx.uni-saarland.de>

Samuel Kemp wrote:
> Hi,
>
> I would like to be able to count a sequence of numbers. For example, given a
> vector of the following integers
>
> (1,1,1,2,2,2,2,3,3,3,3,3,3,1,1,1,1, 3,3)
>
> the function would return
>
> (3, 4, 6, 4,2)
>
> Does anyone have any cool ideas to solve this?
>
>   
Maybe not the most efficient way, but

 >  count <- function (x) diff(c(0,which(diff(x)!=0),length(x)))

should work (for integer sequences!):

 >  x<-c(1,1,1,2,2,2,2,3,3,3,3,3,3,1,1,1,1,3,3)
 >  count(x)
[1] 3 4 6 4 2

Regards,

  Martin

> Any help appreciated.
>
> Regards,
>
> Sam.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ima at difres.dk  Thu Feb 15 11:52:59 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Thu, 15 Feb 2007 11:52:59 +0100
Subject: [R] simpleR or usingR package by Verzani
Message-ID: <68E7981938EAF54F987AD3848A0A6416DDC99C@ka-mail01.dfu.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/b6aabc62/attachment.pl 

From eric.archer at noaa.gov  Thu Feb 15 11:59:58 2007
From: eric.archer at noaa.gov (Eric Archer)
Date: Thu, 15 Feb 2007 02:59:58 -0800
Subject: [R] combining lists with same names with 'c'
Message-ID: <45D43D2E.9030004@noaa.gov>

I recently ran across an unexpected problem caused by combining lists 
that contained elements of the same name.  I naively thought that the 
elements would get overwritten, but rather the "new" element gets added 
with the same name. A simplified version is :

 > my.list <- list(a = 1)
 > new.list <- c(my.list, list(a = "a", b = "b"))
 > new.list
$a
[1] 1

$a
[1] "a"

$b
[1] "b"

 > new.list$a
[1] 1


I think I understand why this happens, but is there a way to do this 
kind of combining and force overwriting if the list already contains an 
equivalent name?  I can imagine a way using 'lapply', but I'm just 
checking to see if there is a way to do it with 'c'.  In the cases I'd 
be using this, I wouldn't necessarily know beforehand if the new list 
contained unique elements or was an "update" list.

Thanks!
Cheers,
e.


-- 

Eric Archer, Ph.D.
NOAA-SWFSC
8604 La Jolla Shores Dr.
La Jolla, CA 92037
858-546-7121,7003(FAX)
eric.archer at noaa.gov


"Lighthouses are more helpful than churches."
    - Benjamin Franklin

"...but I'll take a GPS over either one."
    - Craig George


From deepayan.sarkar at gmail.com  Thu Feb 15 12:12:47 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 15 Feb 2007 03:12:47 -0800
Subject: [R] Putting splom in a function
In-Reply-To: <cf94d0090702141305k12e7a97chb21395b4f48e9a1@mail.gmail.com>
References: <cf94d0090702141305k12e7a97chb21395b4f48e9a1@mail.gmail.com>
Message-ID: <eb555e660702150312q5630f216yf4f15675661b50da@mail.gmail.com>

On 2/14/07, Roberto Perdisci <roberto.perdisci at gmail.com> wrote:
> Hello R list,
>   I have a little problem with splom. I'd like to wrap it in a
> function, for example:
>
> multi.scatterplot <- function(data,groups,cols,colors) {
>     splom(~data[,cols], groups = as.symbol(groups), data = data, panel
> = panel.superpose, col=colors)
> }
>
> and then call it like in
>
> multi.scatterplot(iris,"Species",1:4,c("green","blue","red"))
>
> but the problem is:
> Error in form$groups[form$subscr] : object is not subsettable

Non-standard evaluation is a pain generally. See, e.g.,
lattice:::stripplot.formula for a possible solution that seems to
work.

> if I use
>               groups = groups
> instead of
>               groups = as.symbol(groups)
>
> shomthing is plotted, but not the correct scatterplot.

Try groups = eval(as.name(groups))

Deepayan

> I think the problem is that I don't cast the 'groups' variable to the
> correct type. Besides as.symbol() I tried also as.expression(),
> because ?xyplot says "groups: a variable or expression to be evaluated
> in the data frame specified by 'data'".
> What is the correct type? What as.* should I use?
>
> thank you,
> regards,
> Roberto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Thu Feb 15 12:52:08 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 15 Feb 2007 06:52:08 -0500
Subject: [R] combining lists with same names with 'c'
In-Reply-To: <45D43D2E.9030004@noaa.gov>
References: <45D43D2E.9030004@noaa.gov>
Message-ID: <45D44968.6060506@stats.uwo.ca>

On 2/15/2007 5:59 AM, Eric Archer wrote:
> I recently ran across an unexpected problem caused by combining lists 
> that contained elements of the same name.  I naively thought that the 
> elements would get overwritten, but rather the "new" element gets added 
> with the same name. A simplified version is :
> 
>  > my.list <- list(a = 1)
>  > new.list <- c(my.list, list(a = "a", b = "b"))
>  > new.list
> $a
> [1] 1
> 
> $a
> [1] "a"
> 
> $b
> [1] "b"
> 
>  > new.list$a
> [1] 1
> 
> 
> I think I understand why this happens, but is there a way to do this 
> kind of combining and force overwriting if the list already contains an 
> equivalent name?  I can imagine a way using 'lapply', but I'm just 
> checking to see if there is a way to do it with 'c'.  In the cases I'd 
> be using this, I wouldn't necessarily know beforehand if the new list 
> contained unique elements or was an "update" list.

If you know that your update always has all elements named, you could 
use something like this:

 > my.list <- list(a = 1, c = 2)
 > update <- list(a = "a", b = "b")
 > my.list[names(update)] <- update
 > my.list
$a
[1] "a"

$c
[1] 2

$b
[1] "b"


If some elements of the update are named and some are not, I think 
you'll need several steps:

Check if any are named.  If so, use the method above for the named 
entries.

If any are not named, decide what to do with them, e.g. use c() to put 
them on the end of the result, ignore them, whatever.

Duncan Murdoch


From kajla at bioinfo.pl  Thu Feb 15 13:00:04 2007
From: kajla at bioinfo.pl (laszlo kajan)
Date: Thu, 15 Feb 2007 13:00:04 +0100
Subject: [R] Finally incorporated your rgl patch
In-Reply-To: <45CE193B.1040808@stats.uwo.ca>
References: <45CE193B.1040808@stats.uwo.ca>
Message-ID: <45D44B44.9090603@bioinfo.pl>

Dear Duncan,

I apologize for the delay. I upgraded my system to x64_64 (AMD64) in the 
last few days.

I tested the package

http://www.stats.uwo.ca/faculty/murdoch/temp/rgl_0.70.553.tar.gz

with R CMD INSTALL rgl_0.70.553.tar.gz and rgl.snapshot works.

However, we might want to consider finding a way to make it locate the X 
headers instead of just handling the case when it does not find them...

Best regards,

Laszlo Kajan


Duncan Murdoch wrote:
> Just wanted to let you know that I incorporated the configure.ac patch 
> that you sent a while ago.  Just to check that I got it right, could you 
> take a quick look at
> 
> http://www.stats.uwo.ca/faculty/murdoch/temp/rgl_0.70.553.tar.gz
> 
> and see if it installs properly, and that rgl.snapshot works?  Thanks.
> 
> Duncan Murdoch


From ravis at ambaresearch.com  Thu Feb 15 13:13:17 2007
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Thu, 15 Feb 2007 17:43:17 +0530
Subject: [R] Unable to connect to PostgreSQL
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E8C5F2@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/3854ee35/attachment.pl 

From jtjokine at cc.helsinki.fi  Thu Feb 15 12:30:47 2007
From: jtjokine at cc.helsinki.fi (Jukka Jokinen)
Date: Thu, 15 Feb 2007 13:30:47 +0200 (EET)
Subject: [R] [R-pkgs] New package 'drm' for repeated categorical data
	analysis
Message-ID: <Pine.OSF.4.58.0702121514210.426251@sirppi.helsinki.fi>

Dear useRs,

A new package 'drm', version 0.5-4, is available on CRAN.

The drm package provides functions for marginal regression analysis of
repeated (or otherwise clustered) binary, ordinal and nominal responses.
This package can be considered as a likelihood-based alternative to GEE
approach for marginal regression. In addition to regression modelling,
several temporal and latent variable models for the associations between
repeated responses can be specified. In other words, the package provides
joint regression and association modelling for repeated categorical data.

For longitudinal studies with dropout, the package also provides a
possibility to model the dropout mechanism by adding a selection model on
top of the joint regression and association model. This can be used to
explore the effect of dropout on the regression and association parameters
when dropout is considered nonignorable.

The novelty of the proposed approach is that there exists an explicit
solution for the joint distribution, and the parameterisation has an
inherent unit-sum constraint, which substantially facilitate maximum
likelihood fitting for datasets with large cluster sizes. For an
application to a binary response with 12 repeated measurements, see the
help-file of function drm.

For more information, see:
http://www.helsinki.fi/~jtjokine/drm/

Bug reports, comments or suggestions are welcome.

best,
Jukka Jokinen

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From dfarrar at newrvana.com  Thu Feb 15 00:30:37 2007
From: dfarrar at newrvana.com (David Farrar)
Date: Wed, 14 Feb 2007 15:30:37 -0800 (PST)
Subject: [R] monitor a simulation with a special console box?
Message-ID: <20070214233038.91481.qmail@web805.biz.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070214/134bd7cc/attachment.pl 

From wbm504 at uow.edu.au  Thu Feb 15 12:55:43 2007
From: wbm504 at uow.edu.au (wbm504 at uow.edu.au)
Date: Thu, 15 Feb 2007 22:55:43 +1100 (EST)
Subject: [R] help!
Message-ID: <20070215225543.AIK32314@metis.its.uow.edu.au>

I am learning to use R. What is the structure of the syntax for specifying a subset of data in fitting GLMMs?

wilford


From niederlein-rstat at yahoo.de  Thu Feb 15 14:06:07 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Thu, 15 Feb 2007 14:06:07 +0100
Subject: [R] sapply and its return value
Message-ID: <45D45ABF.9080700@yahoo.de>

Hello,

I have some problems with sapply. I wanted to do the following:

s <- sapply(filelist, function(x) {
		if(file.exists(x))
		{
			...
			return( list(density(file$V1)$x, density(file$V1)$y))
		}
		else
		{
			print(paste("plotDensity ERROR - File does not exist: ",x,sep=""))
			return( NULL)
		}
	})

	if(is.null( - ??? - ))
	{
		print("no plot")
	}

That means, I try to read every file and to get some data from it. It 
may happen, that the file does not exist and then I don't want to plot 
anything. As return, I get a list and I have quite a lot of problems to 
access its content... (I did not really understand how this works, I 
guess...)
Could anybody give me a hint?

Thank you!
Antje


From ripley at stats.ox.ac.uk  Thu Feb 15 14:09:31 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Feb 2007 13:09:31 +0000 (GMT)
Subject: [R] Unable to connect to PostgreSQL
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E8C5F2@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3E8C5F2@BAN-MAILSRV03.Amba.com>
Message-ID: <Pine.LNX.4.64.0702151304230.24486@gannet.stats.ox.ac.uk>

It looks like you are trying to use package DBI (without telling us). That 
is of no use without a specific DBMS interface, e.g. packages RMySQL, 
ROracle and RSQLite.

AFAIK, there is no DBI interface to PostgreSQL.  RODBC works well, RJDBC 
may well work (if Java works for your system) and there is a different 
interface via Rdbi from (nowadays) the Bioconductor project.

[Please do note the footer of this message.]


On Thu, 15 Feb 2007, Ravi S. Shankar wrote:

> Hi R users,
>
>
>
> I am a beginner trying to connect to PostgreSQL using R
>
>
>
> When I give the following command     drv<-dbDriver("PostgreSQL")
>
>
>
> I am getting the following error    Error in
> do.call(as.character(drvName), list(...)) :  could not find function
> "PostgreSQL"
>
>
>
> I have installed PostgreSQL 8.1 in my system. I am currently using R
> 2.3.1.
>
>
>
> Any help would be welcome
>
> Thanks
>
>
>
> Ravi
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jholtman at gmail.com  Thu Feb 15 14:45:14 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 15 Feb 2007 08:45:14 -0500
Subject: [R] sapply and its return value
In-Reply-To: <45D45ABF.9080700@yahoo.de>
References: <45D45ABF.9080700@yahoo.de>
Message-ID: <644e1f320702150545r5d9c7299x5ad42bc67e87958f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/9a7c2e1b/attachment.pl 

From murdoch at stats.uwo.ca  Thu Feb 15 14:49:13 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 15 Feb 2007 08:49:13 -0500
Subject: [R] monitor a simulation with a special console box?
In-Reply-To: <20070214233038.91481.qmail@web805.biz.mail.mud.yahoo.com>
References: <20070214233038.91481.qmail@web805.biz.mail.mud.yahoo.com>
Message-ID: <45D464D9.7090007@stats.uwo.ca>

On 2/14/2007 6:30 PM, David Farrar wrote:
> I like to monitor simulation by reporting some current values to the 
>   console, every 25th iteration say.  I think it might be nice to have 
>   that appear in a separate window.  Anyone know how?

For relatively portable things to do with opening new windows and such, 
I'd use the tcltk package.  For example, a variation on demo(tkfaq):

     tt <- tktoplevel()
     tkwm.title(tt, "R FAQ")
     txt <- tktext(tt, bg="white")
     scr <- tkscrollbar(tt, repeatinterval=5,
                        command=function(...)tkyview(txt,...))
     ## Safest to make sure scr exists before setting yscrollcommand
     tkconfigure(txt, yscrollcommand=function(...)tkset(scr,...))
     tkpack(txt, side="left", fill="both", expand=TRUE)
     tkpack(scr, side="right", fill="y")

     for (i in 1:10)
        tkinsert(txt, "end", paste("Done step", i, "\n"))

Duncan Murdoch


From matthew_wiener at merck.com  Thu Feb 15 15:01:38 2007
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 15 Feb 2007 09:01:38 -0500
Subject: [R] count sequence of integers  [Broadcast]
In-Reply-To: <45D43B50.7060001@mx.uni-saarland.de>
References: <3a5596550702150234r40466120u90e5705897bf30ec@mail.gmail.com>
	<45D43B50.7060001@mx.uni-saarland.de>
Message-ID: <4E9A692D8755DF478B56A2892388EE1F01829447@usctmx1118.merck.com>

The function "rle" will give you what you are looking for.

Hope this helps,

Matt Wiener 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Becker
Sent: Thursday, February 15, 2007 5:52 AM
To: Samuel Kemp
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] count sequence of integers [Broadcast]

Samuel Kemp wrote:
> Hi,
>
> I would like to be able to count a sequence of numbers. For example,
given a
> vector of the following integers
>
> (1,1,1,2,2,2,2,3,3,3,3,3,3,1,1,1,1, 3,3)
>
> the function would return
>
> (3, 4, 6, 4,2)
>
> Does anyone have any cool ideas to solve this?
>
>   
Maybe not the most efficient way, but

 >  count <- function (x) diff(c(0,which(diff(x)!=0),length(x)))

should work (for integer sequences!):

 >  x<-c(1,1,1,2,2,2,2,3,3,3,3,3,3,1,1,1,1,3,3)
 >  count(x)
[1] 3 4 6 4 2

Regards,

  Martin

> Any help appreciated.
>
> Regards,
>
> Sam.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From petr.pikal at precheza.cz  Thu Feb 15 15:13:02 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Feb 2007 15:13:02 +0100
Subject: [R] help!
In-Reply-To: <20070215225543.AIK32314@metis.its.uow.edu.au>
Message-ID: <45D4787E.28684.1B63565@localhost>

Hi

most probably

(...., subset = some.call.which results in logical vector, ...)

You certainly will find more information on specific help page.

HTH
Petr


On 15 Feb 2007 at 22:55, wbm504 at uow.edu.au wrote:

From:           	<wbm504 at uow.edu.au>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Thu, 15 Feb 2007 22:55:43 +1100 (EST)
Subject:        	[R] help!

> I am learning to use R. What is the structure of the syntax for
> specifying a subset of data in fitting GLMMs?
> 
> wilford
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From jholtman at gmail.com  Thu Feb 15 15:14:41 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 15 Feb 2007 09:14:41 -0500
Subject: [R] Working with temporal data
In-Reply-To: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
References: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
Message-ID: <644e1f320702150614g6ebd7e5bv4f5919006bf3875b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/9ab86a88/attachment.pl 

From rbaer at atsu.edu  Thu Feb 15 15:12:49 2007
From: rbaer at atsu.edu (Robert Baer)
Date: Thu, 15 Feb 2007 08:12:49 -0600
Subject: [R] simpleR or usingR package by Verzani
References: 68E7981938EAF54F987AD3848A0A6416DDC99C@ka-mail01.dfu.local
Message-ID: <015901c7510b$5fe3e750$970c010a@ATSU7B94409E1B>

I think the name of what you want is simple.freqpoly()
Try:
library(UsingR)
?simple.freqpoly

HTH,
Rob Baer

I am a new R user and so I thought I could start with "Using R for
Introductory statistics" by Verzani.
In order to use some of the functions and datasets I have to install the
simpleR package which is is now inside the UsingR package. I did so
using
>install.packages("UsingR"). However, the functions such as
"simple.freqpoly.R" do not work.
I also tried to install the Simple_0.4.zip available by the author, but
I get the following message:

Error in gzfile(file, "r") : unable to open connection
In addition: Warning messages:
1: error 1 in extracting from zip file
2: cannot open compressed file 'Simple/DESCRIPTION'

I should also note that I use windows XP and the latest R version.

Any suggestions?

Thank you!!

______________________________________________________
Danmarks Fiskeriunders?gelser er den 1.1.07 fusioneret med Danmarks Tekniske 
Universitet (DTU), Forskningscenter Ris?, Danmarks F?devareforskning, 
Danmarks Rumcenter og Danmarks Transportforskning. DTU er den forts?ttende 
enhed. L?s mere om fusionen p? www.detnyedtu.dk

As of 1 January 2007 The Danish Institute for Fisheries Research has merged 
with the Technical University of Denmark, Ris? National Laboratory, the 
Danish Institute for Food and Veterinary Research, the Danish National Space 
Centre and the Danish Transport Research Institute. The Technical University 
of Denmark (DTU) is the continuing unit. More information on 
www.detnyedtu.dk


[[alternative HTML version deleted]]




--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From klaster at karlin.mff.cuni.cz  Thu Feb 15 15:18:24 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Thu, 15 Feb 2007 15:18:24 +0100
Subject: [R] Working with temporal data
In-Reply-To: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
References: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
Message-ID: <45D46BB0.5010705@karlin.mff.cuni.cz>

S?rgio Nunes napsal(a):
> Hi,
> 
> I have several files with data in this format:
> 
> 20070102
> 20070102
> 20070106
> 20070201
> ...
> 
> The data is sorted and each line represents a date (YYYYMMDD). I would
> like to analyze this data using R. For instance, I would like to have
> a histogram by year, month or day.
> 
> I've already made a simple Perl script that aggregates this data but I
> believe that R can be much more powerful and easy on this kind of
> work.
> 
> Any suggestions on where to start?
> 
> Thanks in advance,
> S?rgio Nunes

Analysing a particular day is easy - just call e.g. 
hist(your.variable[day==20070101])
if 'day' contains the date stored as an integer.

If you want to do this for all days probably you mmight use a loop 
through unique(day)

Finally, to do a monthly/yearly analysis, just create 'month' and 'year'
month <- trunc(day/100)
year <- trunc(day/10000)

HTH
Petr Klasterecky
-- 
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From xjcatalyst at acgproducciones.com  Thu Feb 15 15:26:25 2007
From: xjcatalyst at acgproducciones.com (Gwendolyn Cook)
Date: Thu, 15 Feb 2007 19:56:25 +0530
Subject: [R] Is quiescent he canoe
Message-ID: <001101c7513b$a76ae7b0$07221b64@bhatia>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/ef4dc433/attachment.pl 

From snunes at gmail.com  Thu Feb 15 15:39:53 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Thu, 15 Feb 2007 14:39:53 +0000
Subject: [R] Working with temporal data
In-Reply-To: <644e1f320702150614g6ebd7e5bv4f5919006bf3875b@mail.gmail.com>
References: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
	<644e1f320702150614g6ebd7e5bv4f5919006bf3875b@mail.gmail.com>
Message-ID: <4c817d530702150639r663fd073t94f37d99f60e479@mail.gmail.com>

Please note that I do not have values, only dates that represent
occurrences (one link, one date, one occurrence). This means that the
same date might appear several times in different rows. Nevertheless,
I think I can manage this based on your samples.

Thanks,

S?rgio Nunes


On 2/15/07, jim holtman <jholtman at gmail.com> wrote:
> Here is a start on what you want to do.  This generates some test data and
> then does a couple of summaries:
>
>
>
> > # generate some data
> > N <- 1000
> > x <- data.frame(date=as.character(20070000 + sample(1:4, N, TRUE) * 100 +
> sample(1:31, N, TRUE)),
> +     value=runif(N))
> > head(x)  # display the data
>        date      value
> 1 20070124 0.07904540
> 2 20070117 0.17864565
> 3 20070109 0.86078870
> 4 20070205 0.93952259
> 5 20070112 0.87904425
> 6 20070323 0.01717623
> > # assuming you read it in as character, convert to Date for processing
> > x$date <- as.Date(strptime(x$date, "%Y%m%d"))
> > x <- x[order(x$date), ] # order by date for plotting
> > plot(x$date, x$value, type='l')   # plot the data
> > # show counts by month
> > table(months(x$date))
>
>    April February  January    March
>      238      236      253      237
> > # average by month
> > aggregate(x$value, list(months(x$date)), mean)
>    Group.1         x
> 1    April 0.4791387
> 2 February 0.5010831
> 3  January 0.5114135
> 4    March 0.4695668
> >
>
>
>
> On 2/15/07, S?rgio Nunes <snunes at gmail.com> wrote:
> >
> > Hi,
> >
> > I have several files with data in this format:
> >
> > 20070102
> > 20070102
> > 20070106
> > 20070201
> > ...
> >
> > The data is sorted and each line represents a date (YYYYMMDD). I would
> > like to analyze this data using R. For instance, I would like to have
> > a histogram by year, month or day.
> >
> > I've already made a simple Perl script that aggregates this data but I
> > believe that R can be much more powerful and easy on this kind of
> > work.
> >
> > Any suggestions on where to start?
> >
> > Thanks in advance,
> > S?rgio Nunes
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?


From ima at difres.dk  Thu Feb 15 16:00:36 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Thu, 15 Feb 2007 16:00:36 +0100
Subject: [R] simpleR or usingR package by Verzani
Message-ID: <68E7981938EAF54F987AD3848A0A6416DDC9C2@ka-mail01.dfu.local>

 Yes, you were right, I did not load the library. Now it works. I just installed R yesterday and I am still trying hard: for sure I will come back for more input!
Thank you:)

-----Original Message-----
From: Phil Spector [mailto:spector at stat.Berkeley.EDU] 
Sent: February 15, 2007 2:57 PM
To: Irene Mantzouni
Subject: Re: [R] simpleR or usingR package by Verzani

Irene -
    After installing a library, you need to use the library command in order to make it available in the current session:

library(UsingR)

Once you do that, you can see the names of the functions and data sets that are available by typing

objects('package:UsingR')

There you will see that the function you mentioned is called "simple.freqpoly" -- in other words, the ".R" that you added to the end of the function name is not necessary.

Hope this helps.

                                        - Phil Spector
 					 Statistical Computing Facility
 					 Department of Statistics
 					 UC Berkeley
 					 spector at stat.berkeley.edu


On Thu, 15 Feb 2007, Irene Mantzouni wrote:

> I am a new R user and so I thought I could start with "Using R for 
> Introductory statistics" by Verzani.
> In order to use some of the functions and datasets I have to install 
> the simpleR package which is is now inside the UsingR package. I did 
> so using
>> install.packages("UsingR"). However, the functions such as
> "simple.freqpoly.R" do not work.
> I also tried to install the Simple_0.4.zip available by the author, 
> but I get the following message:
>
> Error in gzfile(file, "r") : unable to open connection In addition: 
> Warning messages:
> 1: error 1 in extracting from zip file
> 2: cannot open compressed file 'Simple/DESCRIPTION'
>
> I should also note that I use windows XP and the latest R version.
>
> Any suggestions?
>
> Thank you!!
>
> ______________________________________________________
> Danmarks Fiskeriunders?gelser er den 1.1.07 fusioneret med Danmarks 
> Tekniske Universitet (DTU), Forskningscenter Ris?, Danmarks 
> F?devareforskning, Danmarks Rumcenter og Danmarks Transportforskning. 
> DTU er den forts?ttende enhed. L?s mere om fusionen p? 
> www.detnyedtu.dk
>
> As of 1 January 2007 The Danish Institute for Fisheries Research has 
> merged with the Technical University of Denmark, Ris? National 
> Laboratory, the Danish Institute for Food and Veterinary Research, the 
> Danish National Space Centre and the Danish Transport Research 
> Institute. The Technical University of Denmark (DTU) is the continuing 
> unit. More information on www.detnyedtu.dk
>
>
> 	[[alternative HTML version deleted]]
>
>

______________________________________________________
Danmarks Fiskeriunders?gelser er den 1.1.07 fusioneret med Danmarks Tekniske Universitet (DTU), Forskningscenter Ris?, Danmarks F?devareforskning, Danmarks Rumcenter og Danmarks Transportforskning. DTU er den forts?ttende enhed. L?s mere om fusionen p? www.detnyedtu.dk

As of 1 January 2007 The Danish Institute for Fisheries Research has merged with the Technical University of Denmark, Ris? National Laboratory, the Danish Institute for Food and Veterinary Research, the Danish National Space Centre and the Danish Transport Research Institute. The Technical University of Denmark (DTU) is the continuing unit. More information on www.detnyedtu.dk


From rdiaz02 at gmail.com  Thu Feb 15 16:05:13 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Thu, 15 Feb 2007 16:05:13 +0100
Subject: [R] Snow vs Rmpi
In-Reply-To: <25385622.30231171465302711.JavaMail.root@jumpmail1>
References: <25385622.30231171465302711.JavaMail.root@jumpmail1>
Message-ID: <624934630702150705y3cf04b16ia012e9091969985d@mail.gmail.com>

Dear Vadim,

On 2/14/07, Vadim Ogranovich <vogranovich at jumptrading.com> wrote:
> Hi,
>
> I have few high-level questions about the Snow and Rmpi packages . I understand that Snow uses Rmpi as one of possible transport layers, yet my questions about user experience, not technical details:
>
> 1. Does Snow install and work well in Windows?
> 2. Interruptibility. I understand that currently it is impossible to interrupt a running top-level command in Snow ( Ctl-c or the likes), the only way to kill slave processes is to kill the master R process. Is this accurate? What about Rmpi ? Is there any difference between Windows and Linux?


I've never used any of those under Windoze. I think your statement is
accurate under Linux. (In fact, I often get rid of any of those Rmpis
gone astray by issuing a lamhalt and/or lamwipe).

> 3. When the master process dies , is it guaranteed that the slaves will die too? How reliable is this (I've seen some applications, not related to R, that were flaky about killing slaves)


If you use an orderly exit procedure (mpi.close.Rslaves(); mpi.quit())
I've never, ever, seen badly behaved Rmpi slaves. But I've seen them
under strange circumstances (I think network problems that messed up
the lam universe ?).

A kind of fail proof approach, if you can afford it, is to use
different lam universes (using the LAM_MPI_SESSION_SUFFIX) for
different simultaneous runs. Then, if one particular run behaves
poorly, you can issue a lamhalt/lamwipe for just that LAM universe.

A final suggestion: you might want to take a look at the papply
package, which does load-balancing and allows you to run sequential
(if there is no lam universe), and thus makes debugging much simpler.

R.


>
> Thank you very much for your help,
> Vadim
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From tariq.khan at gmail.com  Thu Feb 15 16:19:41 2007
From: tariq.khan at gmail.com (=?ISO-8859-1?Q?=A8Tariq_Khan?=)
Date: Thu, 15 Feb 2007 15:19:41 +0000
Subject: [R] How to speed up or avoid the for-loops in this example?
In-Reply-To: <644e1f320702142052o5ddae13xe40fa3edc0bc27cd@mail.gmail.com>
References: <45D3B655.8070601@optushome.com.au>
	<644e1f320702141825r5689056ds72ea5def10cb6f78@mail.gmail.com>
	<45D3D293.6030008@optushome.com.au>
	<644e1f320702142052o5ddae13xe40fa3edc0bc27cd@mail.gmail.com>
Message-ID: <2310043c0702150719x6a26891cid31b0e3f176c0271@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/ed452559/attachment.pl 

From BPikouni at CNTUS.JNJ.COM  Thu Feb 15 16:23:23 2007
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 15 Feb 2007 10:23:23 -0500
Subject: [R] Nonclinical Statistics Opening at Centocor: suburban
	Philadelphia	 PA (USA)
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB0BD4D353@CNTUSMAEXS4.na.jnj.com>

We are searching to add to our Nonclinical Statistics group at Centocor.
Details and resume/CV submission can be found via
http://www.jnj.com/careers/ : enter 0702215 for Requisition number in the
upper right hand corner textbox of that page.

Thanks,
Bill


-------------------------------
Bill Pikounis, PhD
Nonclinical Statistics
Centocor, Inc.


From antje.niederlein at yahoo.de  Thu Feb 15 16:27:47 2007
From: antje.niederlein at yahoo.de (Antje)
Date: Thu, 15 Feb 2007 16:27:47 +0100
Subject: [R] tapply, levelinformation
Message-ID: <45D47BF3.2050006@yahoo.de>

Hello,

I have another question. I would like to plot something within a self 
written function (plotdensity) called by tapply

t <- tapply(mat, classes, plotdensity)

Now I would like to add each plot the class/level as title.
How can I do this?

Antje


From Eric.Archer at noaa.gov  Thu Feb 15 16:33:09 2007
From: Eric.Archer at noaa.gov (Eric Archer)
Date: Thu, 15 Feb 2007 07:33:09 -0800
Subject: [R] combining lists with same names with 'c'
In-Reply-To: <45D44968.6060506@stats.uwo.ca>
References: <45D43D2E.9030004@noaa.gov> <45D44968.6060506@stats.uwo.ca>
Message-ID: <45D47D35.3050909@noaa.gov>

Thanks Duncan!  That's the perfect solution as the update will always 
have all elements named.  I don't think I would've come up with it on my 
own.

Cheers,
e.

Duncan Murdoch wrote:
> On 2/15/2007 5:59 AM, Eric Archer wrote:
>> I recently ran across an unexpected problem caused by combining lists 
>> that contained elements of the same name.  I naively thought that the 
>> elements would get overwritten, but rather the "new" element gets 
>> added with the same name. A simplified version is :
>>
>>  > my.list <- list(a = 1)
>>  > new.list <- c(my.list, list(a = "a", b = "b"))
>>  > new.list
>> $a
>> [1] 1
>>
>> $a
>> [1] "a"
>>
>> $b
>> [1] "b"
>>
>>  > new.list$a
>> [1] 1
>>
>>
>> I think I understand why this happens, but is there a way to do this 
>> kind of combining and force overwriting if the list already contains 
>> an equivalent name?  I can imagine a way using 'lapply', but I'm just 
>> checking to see if there is a way to do it with 'c'.  In the cases 
>> I'd be using this, I wouldn't necessarily know beforehand if the new 
>> list contained unique elements or was an "update" list.
>
> If you know that your update always has all elements named, you could 
> use something like this:
>
> > my.list <- list(a = 1, c = 2)
> > update <- list(a = "a", b = "b")
> > my.list[names(update)] <- update
> > my.list
> $a
> [1] "a"
>
> $c
> [1] 2
>
> $b
> [1] "b"
>
>
> If some elements of the update are named and some are not, I think 
> you'll need several steps:
>
> Check if any are named.  If so, use the method above for the named 
> entries.
>
> If any are not named, decide what to do with them, e.g. use c() to put 
> them on the end of the result, ignore them, whatever.
>
> Duncan Murdoch
>
>


-- 

Eric Archer, Ph.D.
NOAA-SWFSC
8604 La Jolla Shores Dr.
La Jolla, CA 92037
858-546-7121,7003(FAX)
eric.archer at noaa.gov
http://swfsc.noaa.gov/prd-etp.aspx

"Innocence about Science is the worst crime today."
   - Sir Charles Percy Snow


"Lighthouses are more helpful than churches."
   - Benjamin Franklin

   "...but I'll take a GPS over either one."
       - John C. "Craig" George


From roberto.perdisci at gmail.com  Thu Feb 15 16:37:32 2007
From: roberto.perdisci at gmail.com (Roberto Perdisci)
Date: Thu, 15 Feb 2007 10:37:32 -0500
Subject: [R] Putting splom in a function
In-Reply-To: <eb555e660702150312q5630f216yf4f15675661b50da@mail.gmail.com>
References: <cf94d0090702141305k12e7a97chb21395b4f48e9a1@mail.gmail.com>
	<eb555e660702150312q5630f216yf4f15675661b50da@mail.gmail.com>
Message-ID: <cf94d0090702150737r7366ffa7y3d895622634090ac@mail.gmail.com>

On 2/15/07, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 2/14/07, Roberto Perdisci <roberto.perdisci at gmail.com> wrote:
>
> > if I use
> >               groups = groups
> > instead of
> >               groups = as.symbol(groups)
> >
> > shomthing is plotted, but not the correct scatterplot.
>
> Try groups = eval(as.name(groups))

groups = eval(as.name(groups))  does exactly what I was looking for :)
and then I noticed that groups = eval(as.symbol(groups)) works as well

thank you,
Roberto

> Deepayan
>
> > I think the problem is that I don't cast the 'groups' variable to the
> > correct type. Besides as.symbol() I tried also as.expression(),
> > because ?xyplot says "groups: a variable or expression to be evaluated
> > in the data frame specified by 'data'".
> > What is the correct type? What as.* should I use?
> >
> > thank you,
> > regards,
> > Roberto
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From feldkirc at ihs.ac.at  Thu Feb 15 16:38:15 2007
From: feldkirc at ihs.ac.at (Martin Feldkircher)
Date: Thu, 15 Feb 2007 16:38:15 +0100
Subject: [R] convert to binary to decimal
Message-ID: <45D47E67.9020004@ihs.ac.at>

Hello,
we need to convert a logical vector to a (decimal) integer. Example:

a=c(TRUE, FALSE, TRUE) (binary number 101)

the function we are looking for should return

dec2bin(a)=5

Is there a package for such a function or is it even implemented in the 
base package? We found the hexmode and octmode command, but not a 
binmode. We know how to program it ourselves however we are looking for 
a computationally efficient algorithm.

Martin and Stefan


From ggrothendieck at gmail.com  Thu Feb 15 16:56:21 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 15 Feb 2007 10:56:21 -0500
Subject: [R] Putting splom in a function
In-Reply-To: <cf94d0090702141305k12e7a97chb21395b4f48e9a1@mail.gmail.com>
References: <cf94d0090702141305k12e7a97chb21395b4f48e9a1@mail.gmail.com>
Message-ID: <971536df0702150756h2f04de9ayacd4176f19c6c6ea@mail.gmail.com>

If you call splom with do.call then your solution should work:

   do.call("splom", list(~data[cols], groups = as.symbol(groups), data = data,
     panel = panel.superpose, col = colors))

or use as.name where as.symbol is which also works.

On 2/14/07, Roberto Perdisci <roberto.perdisci at gmail.com> wrote:
> Hello R list,
>  I have a little problem with splom. I'd like to wrap it in a
> function, for example:
>
> multi.scatterplot <- function(data,groups,cols,colors) {
>    splom(~data[,cols], groups = as.symbol(groups), data = data, panel
> = panel.superpose, col=colors)
> }
>
> and then call it like in
>
> multi.scatterplot(iris,"Species",1:4,c("green","blue","red"))
>
> but the problem is:
> Error in form$groups[form$subscr] : object is not subsettable
>
> if I use
>              groups = groups
> instead of
>              groups = as.symbol(groups)
>
> shomthing is plotted, but not the correct scatterplot.
>
> I think the problem is that I don't cast the 'groups' variable to the
> correct type. Besides as.symbol() I tried also as.expression(),
> because ?xyplot says "groups: a variable or expression to be evaluated
> in the data frame specified by 'data'".
> What is the correct type? What as.* should I use?
>
> thank you,
> regards,
> Roberto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From skiadas at hanover.edu  Thu Feb 15 17:02:53 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 15 Feb 2007 11:02:53 -0500
Subject: [R] Proper way to typeset the symbol for R in LaTeX?
Message-ID: <05812B5E-BA8D-42A8-9A51-C7779AEA8D71@hanover.edu>

Hoping this is not off topic...

I am in the process of writing some tutorials for my students for  
learning R, and naturally I'm using Sweave for this. So suddenly a  
question occurred to me: LaTeX has a recommended way of typesetting  
the TeX and LaTeX symbols, via the \TeX and \LaTeX commands. Is there  
a similar command for the R symbol, or in general are there any  
guidelines/recommendations on how to typeset the letter R when  
referring to the  R language?

Haris


From marc_schwartz at comcast.net  Thu Feb 15 17:10:21 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 15 Feb 2007 10:10:21 -0600
Subject: [R] convert to binary to decimal
In-Reply-To: <45D47E67.9020004@ihs.ac.at>
References: <45D47E67.9020004@ihs.ac.at>
Message-ID: <1171555821.4785.7.camel@localhost.localdomain>

On Thu, 2007-02-15 at 16:38 +0100, Martin Feldkircher wrote:
> Hello,
> we need to convert a logical vector to a (decimal) integer. Example:
> 
> a=c(TRUE, FALSE, TRUE) (binary number 101)
> 
> the function we are looking for should return
> 
> dec2bin(a)=5
> 
> Is there a package for such a function or is it even implemented in the 
> base package? We found the hexmode and octmode command, but not a 
> binmode. We know how to program it ourselves however we are looking for 
> a computationally efficient algorithm.
> 
> Martin and Stefan

This is a modification of a function that I had posted a while back, so
that it handles 'x' as a logical vector. I added the first line in the
function to convert the logical vector to it's numeric equivalent and
then coerce to character:

bin2dec <- function(x)
{
  x <- as.character(as.numeric(x))
  b <- as.numeric(unlist(strsplit(x, "")))
  pow <- 2 ^ ((length(b) - 1):0)
  sum(pow[b == 1])
}


a <- c(TRUE, FALSE, TRUE)

> bin2dec(a)
[1] 5

HTH,

Marc Schwartz


From roland.rproject at gmail.com  Thu Feb 15 17:21:43 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Thu, 15 Feb 2007 11:21:43 -0500
Subject: [R] convert to binary to decimal
In-Reply-To: <1171555821.4785.7.camel@localhost.localdomain>
References: <45D47E67.9020004@ihs.ac.at>
	<1171555821.4785.7.camel@localhost.localdomain>
Message-ID: <47c7c59e0702150821l57b39bena5e19fd95e11931b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/f6620fd8/attachment.pl 

From marc_schwartz at comcast.net  Thu Feb 15 17:30:34 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 15 Feb 2007 10:30:34 -0600
Subject: [R] Proper way to typeset the symbol for R in LaTeX?
In-Reply-To: <05812B5E-BA8D-42A8-9A51-C7779AEA8D71@hanover.edu>
References: <05812B5E-BA8D-42A8-9A51-C7779AEA8D71@hanover.edu>
Message-ID: <1171557034.4785.20.camel@localhost.localdomain>

On Thu, 2007-02-15 at 11:02 -0500, Charilaos Skiadas wrote:
> Hoping this is not off topic...
> 
> I am in the process of writing some tutorials for my students for  
> learning R, and naturally I'm using Sweave for this. So suddenly a  
> question occurred to me: LaTeX has a recommended way of typesetting  
> the TeX and LaTeX symbols, via the \TeX and \LaTeX commands. Is there  
> a similar command for the R symbol, or in general are there any  
> guidelines/recommendations on how to typeset the letter R when  
> referring to the  R language?
> 
> Haris

There is a Rnews.sty file on CRAN at:

  http://cran.r-project.org/doc/Rnews/Rnews.sty

However, the key code is really:

  \newcommand{\R}{R}

which is then used in LaTeX as:

  The use of \R{} ...

It is essentially a capital R in the default serif font.

BTW, if you need the full citation for R, just use citation() in a R
session.

HTH,

Marc Schwartz


From jiho.han at yahoo.com  Thu Feb 15 17:34:05 2007
From: jiho.han at yahoo.com (jiho.han)
Date: Thu, 15 Feb 2007 08:34:05 -0800 (PST)
Subject: [R] creating data.frame from interaction of factors
Message-ID: <8988666.post@talk.nabble.com>


Hello, R experts-

If I create a new factor, say Z, by combining other factors, say X and Y, is
there ways to create a data.frame from the resulting factor Z? For example,
suppose I enter the following:

X = 1:3
Y = letters[1:3]
Z = interaction( X, Y, sep=":")

Then, I want to get a data.frame that look like 

1 a
2 b
3 c

Help me--- Thanks


-- 
View this message in context: http://www.nabble.com/creating-data.frame-from-interaction-of-factors-tf3234700.html#a8988666
Sent from the R help mailing list archive at Nabble.com.


From t.bloemberg at science.ru.nl  Thu Feb 15 17:40:07 2007
From: t.bloemberg at science.ru.nl (Tom Bloemberg)
Date: Thu, 15 Feb 2007 17:40:07 +0100
Subject: [R] missing lines when using image()
Message-ID: <45D48CE7.2040000@science.ru.nl>

When making an image of a rather big matrix, containing a lot of NA's 
and not that much real numbers, like in the following (rather simple) 
example

M <- matrix(NA,1000,1000)
M[,seq(1,1000,length.out=50)] <- 1
image(M)

information (in this case: horizontal lines) is missing in the resulting 
image. This probably is because the vertical screen resolution is 
smaller than the number of rows in the matrix.

Does anyone know of a general method (apart from the obvious and not 
really general methods of buying a bigger screen, zooming in on part of 
the matrix, and simply not making an image of such a big matrix) that 
causes the missing lines to be visualized?

When I visualized the same matrix in Matlab (my sincere apologies to 
those considering this to be blasphemy), looking at a mesh plot from the 
top, all the lines were nicely visible, so it should be possible in some 
way...

Regards,

Tom Bloemberg


From wolfram at fischer-zim.ch  Thu Feb 15 09:57:34 2007
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Thu, 15 Feb 2007 09:57:34 +0100
Subject: [R] Problems with 'delay'/'delayedAssign' when installing data
	package
Message-ID: <20070215085734.GA6205@s1x.fischer-zim.local>

I downloaded:
    http://www.bioconductor.org/data/metaData/hgu95av2_1.7.0.tar.gz
described as:
    Package: hgu95av2 
    Title: A data package containing annotation data for hgu95av2 
    Version: 1.7.0 
    Created: Wed Jan 12 16:57:23 2005 
    Author: Lin,Chenwei 
    Description: Annotation data file for hgu95av2 assembled using data
       from public data repositories 
       Maintainer: Lin,Chenwei < clin at fhcrc.org > 
       LazyLoad: yes 
       Depends: R(>= 2.0.0) 
       License: LGPL 
       Packaged: Thu Mar  3 15:43:00 2005; biocbuild

It is an example database of the geneplotter library.

Trying to install, I got:
	$ R CMD INSTALL hgu95av2_1.7.0.tar.gz 

	* Installing *source* package 'hgu95av2' ...
	** R
	** data
	** preparing package for lazy loading
	Error: 'delay' is defunct.
	Use 'delayedAssign' instead.
	See help("Defunct")
	Execution halted
	ERROR: lazy loading failed for package 'hgu95av2'
	** Removing '/usr/local/lib64/R-2.4.1/library/hgu95av2'

How to by-pass this problem?

Thanks - Wolfram


From gunter.berton at gene.com  Thu Feb 15 17:53:17 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 15 Feb 2007 08:53:17 -0800
Subject: [R] convert to binary to decimal
In-Reply-To: <47c7c59e0702150821l57b39bena5e19fd95e11931b@mail.gmail.com>
Message-ID: <002301c75121$ca6425c0$4d908980@gne.windows.gene.com>

why not simply:

sum(x * 2^(rev(seq_along(x)) - 1))   ?


Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374 


Bert Gunter
Nonclinical Statistics
7-7374

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roland Rau
Sent: Thursday, February 15, 2007 8:22 AM
To: marc_schwartz at comcast.net
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] convert to binary to decimal

That was a nice quick distraction. Unfortunately, I am not the first to
answer. :-(
Anyway, I offer two solutions (which are different from the one of Marc
Schwartz); I wrote it quickly but I hope they are correct.

Enjoy and thanks,
Roland

a <- c(TRUE, FALSE, TRUE)
b <- c(TRUE, FALSE, TRUE, TRUE)

bin2dec.easy <- function(binaryvector) {
  sum(2^(which(rev(binaryvector)==TRUE)-1))
}

bin2dec.recursive <- function(binaryvector) {
  reversed.input <- rev(binaryvector)
  binaryhelper(reversed.input, 0, 0)
}

binaryhelper <- function(binvector, currentpower, currentresult) {
  if (length(binvector)<1) {
    currentresult
  } else {
    if (binvector[1]) {
      binaryhelper(binvector[-1], currentpower+1,
currentresult+2^currentpower)
    } else {
      binaryhelper(binvector[-1], currentpower+1, currentresult)
    }
  }
}


bin2dec.easy(a)
bin2dec.recursive(a)
bin2dec.easy(b)
bin2dec.recursive(b)





On 2/15/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
>
> On Thu, 2007-02-15 at 16:38 +0100, Martin Feldkircher wrote:
> > Hello,
> > we need to convert a logical vector to a (decimal) integer. Example:
> >
> > a=c(TRUE, FALSE, TRUE) (binary number 101)
> >
> > the function we are looking for should return
> >
> > dec2bin(a)=5
> >
> > Is there a package for such a function or is it even implemented in the
> > base package? We found the hexmode and octmode command, but not a
> > binmode. We know how to program it ourselves however we are looking for
> > a computationally efficient algorithm.
> >
> > Martin and Stefan
>
> This is a modification of a function that I had posted a while back, so
> that it handles 'x' as a logical vector. I added the first line in the
> function to convert the logical vector to it's numeric equivalent and
> then coerce to character:
>
> bin2dec <- function(x)
> {
>   x <- as.character(as.numeric(x))
>   b <- as.numeric(unlist(strsplit(x, "")))
>   pow <- 2 ^ ((length(b) - 1):0)
>   sum(pow[b == 1])
> }
>
>
> a <- c(TRUE, FALSE, TRUE)
>
> > bin2dec(a)
> [1] 5
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wetenku_m5 at yahoo.co.jp  Thu Feb 15 18:01:46 2007
From: wetenku_m5 at yahoo.co.jp (wetenku_m5 at yahoo.co.jp)
Date: Fri, 16 Feb 2007 02:01:46 +0900 (JST)
Subject: [R] From Dr.Michael Wetenku
Message-ID: <20070215170146.41283.qmail@web3711.mail.tnz.yahoo.co.jp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/075ccd54/attachment.pl 

From marc_schwartz at comcast.net  Thu Feb 15 18:07:08 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 15 Feb 2007 11:07:08 -0600
Subject: [R] creating data.frame from interaction of factors
In-Reply-To: <8988666.post@talk.nabble.com>
References: <8988666.post@talk.nabble.com>
Message-ID: <1171559228.4785.37.camel@localhost.localdomain>

On Thu, 2007-02-15 at 08:34 -0800, jiho.han wrote:
> Hello, R experts-
> 
> If I create a new factor, say Z, by combining other factors, say X and Y, is
> there ways to create a data.frame from the resulting factor Z? For example,
> suppose I enter the following:
> 
> X = 1:3
> Y = letters[1:3]
> Z = interaction( X, Y, sep=":")
> 
> Then, I want to get a data.frame that look like 
> 
> 1 a
> 2 b
> 3 c
> 
> Help me--- Thanks

> as.data.frame(do.call("rbind", strsplit(as.character(Z), ":")))
  V1 V2
1  1  a
2  2  b
3  3  c


See ?strsplit, ?do.call and ?rbind

HTH,

Marc Schwartz


From roland.rproject at gmail.com  Thu Feb 15 18:13:32 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Thu, 15 Feb 2007 12:13:32 -0500
Subject: [R] convert to binary to decimal
In-Reply-To: <002301c75121$ca6425c0$4d908980@gne.windows.gene.com>
References: <47c7c59e0702150821l57b39bena5e19fd95e11931b@mail.gmail.com>
	<002301c75121$ca6425c0$4d908980@gne.windows.gene.com>
Message-ID: <47c7c59e0702150913l1be48edbmd480e5f4e83e9452@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/572f7322/attachment.pl 

From marc_schwartz at comcast.net  Thu Feb 15 18:22:19 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 15 Feb 2007 11:22:19 -0600
Subject: [R] convert to binary to decimal
In-Reply-To: <47c7c59e0702150913l1be48edbmd480e5f4e83e9452@mail.gmail.com>
References: <47c7c59e0702150821l57b39bena5e19fd95e11931b@mail.gmail.com>
	<002301c75121$ca6425c0$4d908980@gne.windows.gene.com>
	<47c7c59e0702150913l1be48edbmd480e5f4e83e9452@mail.gmail.com>
Message-ID: <1171560139.4785.45.camel@localhost.localdomain>

Clearly, my game is off at the moment...  ;-)

Thanks guys.

Marc

<Off to find another pot of coffee...>

On Thu, 2007-02-15 at 12:13 -0500, Roland Rau wrote:
> Hi Bert,
> 
> First, I was very happy with my solution, but you win (see below)!
> 
> Best,
> Roland
> 
> 
> > bert.gunter <- function(x) {
> +   sum(x * 2^(rev(seq(along=x)) - 1))
> + }
> > 
> > marc.schwartz <- function(x) {
> +   x <- as.character(as.numeric(x))
> +   b <- as.numeric(unlist(strsplit(x, "")))
> +   pow <- 2 ^ ((length(b) - 1):0)
> +   sum(pow[b == 1])
> + }
> > 
> > length( huge.list)
> [1] 20000
> > head(huge.list, n=1)
> [[1]]
>  [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE
> TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE
> [21] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
> TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE 
> [41] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE
> 
> > 
> > system.time(lapply(X=huge.list, FUN=bin2dec.easy))
> [1] 2.33 0.00 2.32   NA   NA
> > system.time(lapply(X=huge.list, FUN=bin2dec.recursive ))
> [1] 14.91  0.00 14.90    NA    NA
> > system.time(lapply(X=huge.list, FUN=marc.schwartz))
> [1] 5.31 0.00 5.31   NA   NA
> > system.time(lapply(X=huge.list, FUN=bert.gunter))
> [1] 1.33 0.00 1.33   NA   NA 
> > 
> 
> 
> 
> On 2/15/07, Bert Gunter <gunter.berton at gene.com> wrote:
>         why not simply:
>         
>         sum(x * 2^(rev(seq_along(x)) - 1))   ?
>         
>         
>         Bert Gunter
>         Genentech Nonclinical Statistics
>         South San Francisco, CA 94404
>         650-467-7374
>         
>         
>         Bert Gunter
>         Nonclinical Statistics
>         7-7374
>         
>         -----Original Message-----
>         From: r-help-bounces at stat.math.ethz.ch
>         [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roland
>         Rau
>         Sent: Thursday, February 15, 2007 8:22 AM
>         To: marc_schwartz at comcast.net
>         Cc: r-help at stat.math.ethz.ch
>         Subject: Re: [R] convert to binary to decimal
>         
>         That was a nice quick distraction. Unfortunately, I am not the
>         first to
>         answer. :-(
>         Anyway, I offer two solutions (which are different from the
>         one of Marc 
>         Schwartz); I wrote it quickly but I hope they are correct.
>         
>         Enjoy and thanks,
>         Roland
>         
>         a <- c(TRUE, FALSE, TRUE)
>         b <- c(TRUE, FALSE, TRUE, TRUE)
>         
>         bin2dec.easy <- function(binaryvector) { 
>           sum(2^(which(rev(binaryvector)==TRUE)-1))
>         }
>         
>         bin2dec.recursive <- function(binaryvector) {
>           reversed.input <- rev(binaryvector)
>           binaryhelper(reversed.input, 0, 0)
>         }
>         
>         binaryhelper <- function(binvector, currentpower,
>         currentresult) { 
>           if (length(binvector)<1) {
>             currentresult
>           } else {
>             if (binvector[1]) {
>               binaryhelper(binvector[-1], currentpower+1,
>         currentresult+2^currentpower)
>             } else {
>               binaryhelper(binvector[-1], currentpower+1,
>         currentresult) 
>             }
>           }
>         }
>         
>         
>         bin2dec.easy(a)
>         bin2dec.recursive(a)
>         bin2dec.easy(b)
>         bin2dec.recursive(b)
>         
>         
>         
>         
>         
>         On 2/15/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
>         >
>         > On Thu, 2007-02-15 at 16:38 +0100, Martin Feldkircher wrote:
>         > > Hello,
>         > > we need to convert a logical vector to a (decimal)
>         integer. Example:
>         > >
>         > > a=c(TRUE, FALSE, TRUE) (binary number 101) 
>         > >
>         > > the function we are looking for should return
>         > >
>         > > dec2bin(a)=5
>         > >
>         > > Is there a package for such a function or is it even
>         implemented in the
>         > > base package? We found the hexmode and octmode command,
>         but not a 
>         > > binmode. We know how to program it ourselves however we
>         are looking for
>         > > a computationally efficient algorithm.
>         > >
>         > > Martin and Stefan
>         >
>         > This is a modification of a function that I had posted a
>         while back, so 
>         > that it handles 'x' as a logical vector. I added the first
>         line in the
>         > function to convert the logical vector to it's numeric
>         equivalent and
>         > then coerce to character:
>         >
>         > bin2dec <- function(x) 
>         > {
>         >   x <- as.character(as.numeric(x))
>         >   b <- as.numeric(unlist(strsplit(x, "")))
>         >   pow <- 2 ^ ((length(b) - 1):0)
>         >   sum(pow[b == 1])
>         > }
>         >
>         >
>         > a <- c(TRUE, FALSE, TRUE)
>         >
>         > > bin2dec(a)
>         > [1] 5
>         >
>         > HTH,
>         >
>         > Marc Schwartz
>         >
>


From chrisgignoux at yahoo.com  Thu Feb 15 18:50:52 2007
From: chrisgignoux at yahoo.com (Chris Gignoux)
Date: Thu, 15 Feb 2007 09:50:52 -0800 (PST)
Subject: [R] I can't compile Blacs for the RScaLAPACK on OSX
Message-ID: <52322.83793.qm@web60915.mail.yahoo.com>

Hello, folks!  I hope you can help a newbie out.  I need to install the
parallel-R functionality for a software package I am trying to
download.  I have been going through the steps in the install readme
but I cannot find the necessary "Bmake.inc" file to allow the file to
compile properly.  I keep getting this error message:

( cd SRC/MPI ; make  )
( cd INTERNAL ; make -f ../Makefile I_int "dlvl=/Users/Chris/BLACS" )
make[2]: *** No rule to make target `BI_HypBS.o', needed by `I_int'.  Stop.
make[1]: *** [INTERN] Error 2
make: *** [MPI] Error 2


If
someone has the proper input file (the recommended one in the readme is
at a hyperlink that no longer exists) or something else that could help
me I would surely appreciate it.

Thanks!





 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From murdoch at stats.uwo.ca  Thu Feb 15 19:18:03 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 15 Feb 2007 13:18:03 -0500
Subject: [R] Problems with 'delay'/'delayedAssign' when installing data
 package
In-Reply-To: <20070215085734.GA6205@s1x.fischer-zim.local>
References: <20070215085734.GA6205@s1x.fischer-zim.local>
Message-ID: <45D4A3DB.50608@stats.uwo.ca>

On 2/15/2007 3:57 AM, Wolfram Fischer wrote:
> I downloaded:
>     http://www.bioconductor.org/data/metaData/hgu95av2_1.7.0.tar.gz
> described as:
>     Package: hgu95av2 
>     Title: A data package containing annotation data for hgu95av2 
>     Version: 1.7.0 
>     Created: Wed Jan 12 16:57:23 2005 
>     Author: Lin,Chenwei 
>     Description: Annotation data file for hgu95av2 assembled using data
>        from public data repositories 
>        Maintainer: Lin,Chenwei < clin at fhcrc.org > 
>        LazyLoad: yes 
>        Depends: R(>= 2.0.0) 
>        License: LGPL 
>        Packaged: Thu Mar  3 15:43:00 2005; biocbuild
> 
> It is an example database of the geneplotter library.
> 
> Trying to install, I got:
> 	$ R CMD INSTALL hgu95av2_1.7.0.tar.gz 
> 
> 	* Installing *source* package 'hgu95av2' ...
> 	** R
> 	** data
> 	** preparing package for lazy loading
> 	Error: 'delay' is defunct.
> 	Use 'delayedAssign' instead.
> 	See help("Defunct")
> 	Execution halted
> 	ERROR: lazy loading failed for package 'hgu95av2'
> 	** Removing '/usr/local/lib64/R-2.4.1/library/hgu95av2'
> 
> How to by-pass this problem?

There are two possibilities:  update the package to follow current R 
usage (which is something the maintainer should do, but if the 
maintainer is not active you may have to do it yourself), or run in an 
old version of R (pre-2.2.0) from before delay() was made defunct.

You may also use this as a sign that the package is not being actively 
maintained, since this change happened in R 2.2.0 in 2005.  I don't know 
if that's reasonable for a collection of annotation data or not.

Duncan Murdoch


From RMan54 at cox.net  Thu Feb 15 19:34:33 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Thu, 15 Feb 2007 10:34:33 -0800
Subject: [R] Problem in summaryBy
Message-ID: <010501c7512f$f0172570$0900a8c0@rman>

The R script below gives values of 1 for all minimum values when I use a
custom function in summaryBy. I get the correct values when I use FUN=min
directly. Any help is much appreciated.

The continuous information provided in this forum is fabulous as are the
different R packages available.
Rene  

# Simulated simplified data
Subj  <- rep(1:4, each=6)
Analyte <- rep(c(rep("RBV",3),rep("TBV",3)),4)
Dose <- rep(c(200,400,600),8)
AUC <- rnorm(24, c(40,80,120,4,8,12), c(8,16,24,0.8,0.16,0.24))

# The real dataset may have NAs in it
df <- data.frame(Subj, Analyte, Dose, AUC)

myStats <- function(x) {
    count <- function(x) length(na.omit(x))
    pCV <- function(x) sd(x,T) / mean(x,T) * 100
    c(
      n = count(x),
      mean = mean(x,T),
      SD = sd(x,T),
      CV = pCV(x),
      median = median(x,T),
      min = min(x,T),
      max = max(x,T)
      )
}

library(doBy)
# This does not produce correct minimum values
sData <- summaryBy(AUC ~ Analyte + Dose, data=df, FUN=myStats)
# This gives correct minimum values
sMinData <- summaryBy(AUC ~ Analyte + Dose, data=df, FUN=min)

Here are the results:
> sData
  Analyte Dose AUC.n   AUC.mean     AUC.SD    AUC.CV AUC.median AUC.min
AUC.max
1     RBV  200     4  32.353673 11.4244263 35.311064  32.353673       1
46.068889
2     RBV  400     4  78.568606 17.0237534 21.667374  78.568606       1
100.974408
3     RBV  600     4 112.726681  9.1481701  8.115355 112.726681       1
128.550651
4     TBV  200     4   4.283628  1.3773728 32.154348   4.283628       1
4.837139
5     TBV  400     4   8.036309  0.1166662  1.451739   8.036309       1
8.208499
6     TBV  600     4  12.041576  0.1535308  1.275006  12.041576       1
12.179628
> sMinData
  Analyte Dose    AUC.min
1     RBV  200  18.720478
2     RBV  400  61.422756
3     RBV  600 108.461962
4     TBV  200   1.779275
5     TBV  400   7.944623
6     TBV  600  11.853593


From murdoch at stats.uwo.ca  Thu Feb 15 19:43:15 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 15 Feb 2007 13:43:15 -0500
Subject: [R] Problem in summaryBy
In-Reply-To: <010501c7512f$f0172570$0900a8c0@rman>
References: <010501c7512f$f0172570$0900a8c0@rman>
Message-ID: <45D4A9C3.6070106@stats.uwo.ca>

On 2/15/2007 1:34 PM, Rene Braeckman wrote:
> The R script below gives values of 1 for all minimum values when I use a
> custom function in summaryBy. I get the correct values when I use FUN=min
> directly. Any help is much appreciated.

The man page for min lists the header as

min(..., na.rm=FALSE)

so the T you're using is being taken as data, and being coerced to 1.

Some other advice:  name your parameters in a call.  min(x, na.rm=TRUE) 
would work.

Use TRUE, not T.  In R TRUE is a constant, and T is a variable name. 
You might have T <- 0, in which case

if (T) cat("TRUE!!\n") else cat("FALSE!!\n")

would print FALSE!!.

Duncan Murdoch

> The continuous information provided in this forum is fabulous as are the
> different R packages available.
> Rene  
> 
> # Simulated simplified data
> Subj  <- rep(1:4, each=6)
> Analyte <- rep(c(rep("RBV",3),rep("TBV",3)),4)
> Dose <- rep(c(200,400,600),8)
> AUC <- rnorm(24, c(40,80,120,4,8,12), c(8,16,24,0.8,0.16,0.24))
> 
> # The real dataset may have NAs in it
> df <- data.frame(Subj, Analyte, Dose, AUC)
> 
> myStats <- function(x) {
>     count <- function(x) length(na.omit(x))
>     pCV <- function(x) sd(x,T) / mean(x,T) * 100
>     c(
>       n = count(x),
>       mean = mean(x,T),
>       SD = sd(x,T),
>       CV = pCV(x),
>       median = median(x,T),
>       min = min(x,T),
>       max = max(x,T)
>       )
> }
> 
> library(doBy)
> # This does not produce correct minimum values
> sData <- summaryBy(AUC ~ Analyte + Dose, data=df, FUN=myStats)
> # This gives correct minimum values
> sMinData <- summaryBy(AUC ~ Analyte + Dose, data=df, FUN=min)
> 
> Here are the results:
>> sData
>   Analyte Dose AUC.n   AUC.mean     AUC.SD    AUC.CV AUC.median AUC.min
> AUC.max
> 1     RBV  200     4  32.353673 11.4244263 35.311064  32.353673       1
> 46.068889
> 2     RBV  400     4  78.568606 17.0237534 21.667374  78.568606       1
> 100.974408
> 3     RBV  600     4 112.726681  9.1481701  8.115355 112.726681       1
> 128.550651
> 4     TBV  200     4   4.283628  1.3773728 32.154348   4.283628       1
> 4.837139
> 5     TBV  400     4   8.036309  0.1166662  1.451739   8.036309       1
> 8.208499
> 6     TBV  600     4  12.041576  0.1535308  1.275006  12.041576       1
> 12.179628
>> sMinData
>   Analyte Dose    AUC.min
> 1     RBV  200  18.720478
> 2     RBV  400  61.422756
> 3     RBV  600 108.461962
> 4     TBV  200   1.779275
> 5     TBV  400   7.944623
> 6     TBV  600  11.853593
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rgentlem at fhcrc.org  Thu Feb 15 19:45:30 2007
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Thu, 15 Feb 2007 10:45:30 -0800
Subject: [R] Problems with 'delay'/'delayedAssign' when installing data
 package
In-Reply-To: <45D4A3DB.50608@stats.uwo.ca>
References: <20070215085734.GA6205@s1x.fischer-zim.local>
	<45D4A3DB.50608@stats.uwo.ca>
Message-ID: <45D4AA4A.3040704@fhcrc.org>

Or you could check and see that this is several versions of Bioconductor 
in the past and rather than using the well documented and advertised 
method for obtaining data that are consistent with your version of R you 
have chosen to completely circumvent that procedure and loaded a package
for some obsolete version of R.

So, if you have newer R there are newer versions of this data from 
Bioconductor,
   http://bioconductor.org/packages/1.9/data/annotation/html/hgu95av2.html

shows that for a current R, we are at version 1.14.0 for this package.



Duncan Murdoch wrote:
> On 2/15/2007 3:57 AM, Wolfram Fischer wrote:
>> I downloaded:
>>     http://www.bioconductor.org/data/metaData/hgu95av2_1.7.0.tar.gz
>> described as:
>>     Package: hgu95av2 
>>     Title: A data package containing annotation data for hgu95av2 
>>     Version: 1.7.0 
>>     Created: Wed Jan 12 16:57:23 2005 
>>     Author: Lin,Chenwei 
>>     Description: Annotation data file for hgu95av2 assembled using data
>>        from public data repositories 
>>        Maintainer: Lin,Chenwei < clin at fhcrc.org > 
>>        LazyLoad: yes 
>>        Depends: R(>= 2.0.0) 
>>        License: LGPL 
>>        Packaged: Thu Mar  3 15:43:00 2005; biocbuild
>>
>> It is an example database of the geneplotter library.
>>
>> Trying to install, I got:
>> 	$ R CMD INSTALL hgu95av2_1.7.0.tar.gz 
>>
>> 	* Installing *source* package 'hgu95av2' ...
>> 	** R
>> 	** data
>> 	** preparing package for lazy loading
>> 	Error: 'delay' is defunct.
>> 	Use 'delayedAssign' instead.
>> 	See help("Defunct")
>> 	Execution halted
>> 	ERROR: lazy loading failed for package 'hgu95av2'
>> 	** Removing '/usr/local/lib64/R-2.4.1/library/hgu95av2'
>>
>> How to by-pass this problem?
> 
> There are two possibilities:  update the package to follow current R 
> usage (which is something the maintainer should do, but if the 
> maintainer is not active you may have to do it yourself), or run in an 
> old version of R (pre-2.2.0) from before delay() was made defunct.
> 
> You may also use this as a sign that the package is not being actively 
> maintained, since this change happened in R 2.2.0 in 2005.  I don't know 
> if that's reasonable for a collection of annotation data or not.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From jmacdon at med.umich.edu  Thu Feb 15 19:51:01 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Thu, 15 Feb 2007 13:51:01 -0500
Subject: [R] Problems with 'delay'/'delayedAssign' when installing data
 package
In-Reply-To: <45D4A3DB.50608@stats.uwo.ca>
References: <20070215085734.GA6205@s1x.fischer-zim.local>
	<45D4A3DB.50608@stats.uwo.ca>
Message-ID: <45D4AB95.90202@med.umich.edu>

Duncan Murdoch wrote:
> On 2/15/2007 3:57 AM, Wolfram Fischer wrote:
> 
>>I downloaded:
>>    http://www.bioconductor.org/data/metaData/hgu95av2_1.7.0.tar.gz
>>described as:
>>    Package: hgu95av2 
>>    Title: A data package containing annotation data for hgu95av2 
>>    Version: 1.7.0 
>>    Created: Wed Jan 12 16:57:23 2005 
>>    Author: Lin,Chenwei 
>>    Description: Annotation data file for hgu95av2 assembled using data
>>       from public data repositories 
>>       Maintainer: Lin,Chenwei < clin at fhcrc.org > 
>>       LazyLoad: yes 
>>       Depends: R(>= 2.0.0) 
>>       License: LGPL 
>>       Packaged: Thu Mar  3 15:43:00 2005; biocbuild
>>
>>It is an example database of the geneplotter library.
>>
>>Trying to install, I got:
>>	$ R CMD INSTALL hgu95av2_1.7.0.tar.gz 
>>
>>	* Installing *source* package 'hgu95av2' ...
>>	** R
>>	** data
>>	** preparing package for lazy loading
>>	Error: 'delay' is defunct.
>>	Use 'delayedAssign' instead.
>>	See help("Defunct")
>>	Execution halted
>>	ERROR: lazy loading failed for package 'hgu95av2'
>>	** Removing '/usr/local/lib64/R-2.4.1/library/hgu95av2'
>>
>>How to by-pass this problem?
> 
> 
> There are two possibilities:  update the package to follow current R 
> usage (which is something the maintainer should do, but if the 
> maintainer is not active you may have to do it yourself), or run in an 
> old version of R (pre-2.2.0) from before delay() was made defunct.
> 
> You may also use this as a sign that the package is not being actively 
> maintained, since this change happened in R 2.2.0 in 2005.  I don't know 
> if that's reasonable for a collection of annotation data or not.

It wouldn't be if it were true. However, this is an old annotation 
package that was part of the 1.7 release of Bioconductor that was 
designed to work with R-2.2.0. Direct downloading of BioC packages can 
easily result in unintended version mismatches like this.

The correct paradigm for installing BioC packages is to use biocLite(), 
which is a wrapper to install.packages() that will automatically point 
to the correct repository for a given version of R.

source("http://www.bioconductor.org/biocLite.R")
biocLite("hgu95av2")

Additionally, R-help is not the correct list for questions about 
Bioconductor packages. There is a separate list at 
bioconductor at stat.math.ethz.ch.

Best,

Jim


> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From RMan54 at cox.net  Thu Feb 15 19:54:34 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Thu, 15 Feb 2007 10:54:34 -0800
Subject: [R] Problem in summaryBy
In-Reply-To: <45D4A9C3.6070106@stats.uwo.ca>
References: <010501c7512f$f0172570$0900a8c0@rman>
	<45D4A9C3.6070106@stats.uwo.ca>
Message-ID: <010601c75132$bbbcb670$0900a8c0@rman>

Thanks for the quick response. Problem solved. Below is the corrected
script.
Rene

# Simulated simplified data
Subj  <- rep(1:4, each=6)
Analyte <- rep(c(rep("RBV",3),rep("TBV",3)),4)
Dose <- rep(c(200,400,600),8)
AUC <- rnorm(24, c(40,80,120,4,8,12), c(8,16,24,0.8,0.16,0.24))

# The real dataset may have NAs in it
df <- data.frame(Subj, Analyte, Dose, AUC)

myStats <- function(x) {
    count <- function(x) length(na.omit(x))
    pCV <- function(x) sd(x,na.rm=TRUE) / mean(x,na.rm=TRUE) * 100
    c(
      n = count(x),
      mean = mean(x,na.rm=TRUE),
      SD = sd(x,na.rm=TRUE),
      CV = pCV(x),
      median = median(x,na.rm=TRUE),
      min = min(x,na.rm=TRUE),
      max = max(x,na.rm=TRUE)
      )
}

library(doBy)

sData <- summaryBy(AUC ~ Analyte + Dose, data=df, FUN=myStats)


-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: Thursday, February 15, 2007 10:43 AM
To: Rene Braeckman
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Problem in summaryBy

On 2/15/2007 1:34 PM, Rene Braeckman wrote:
> The R script below gives values of 1 for all minimum values when I use 
> a custom function in summaryBy. I get the correct values when I use 
> FUN=min directly. Any help is much appreciated.

The man page for min lists the header as

min(..., na.rm=FALSE)

so the T you're using is being taken as data, and being coerced to 1.

Some other advice:  name your parameters in a call.  min(x, na.rm=TRUE)
would work.

Use TRUE, not T.  In R TRUE is a constant, and T is a variable name. 
You might have T <- 0, in which case

if (T) cat("TRUE!!\n") else cat("FALSE!!\n")

would print FALSE!!.

Duncan Murdoch

...


From sfalcon at fhcrc.org  Thu Feb 15 19:56:04 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 16 Feb 2007 07:56:04 +1300
Subject: [R] Problems with 'delay'/'delayedAssign' when installing data
	package
In-Reply-To: <20070215085734.GA6205@s1x.fischer-zim.local> (Wolfram Fischer's
	message of "Thu, 15 Feb 2007 09:57:34 +0100")
References: <20070215085734.GA6205@s1x.fischer-zim.local>
Message-ID: <m2fy97xnaz.fsf@fhcrc.org>

Hi Wolfram,

Wolfram Fischer <wolfram at fischer-zim.ch> writes:

> I downloaded:
>     http://www.bioconductor.org/data/metaData/hgu95av2_1.7.0.tar.gz
> described as:
>     Package: hgu95av2 
>     Title: A data package containing annotation data for hgu95av2 
>     Version: 1.7.0 
>     Created: Wed Jan 12 16:57:23 2005 
>     Author: Lin,Chenwei 
>     Description: Annotation data file for hgu95av2 assembled using data
>        from public data repositories 
>        Maintainer: Lin,Chenwei < clin at fhcrc.org > 
>        LazyLoad: yes 
>        Depends: R(>= 2.0.0) 
>        License: LGPL 
>        Packaged: Thu Mar  3 15:43:00 2005; biocbuild

That is not the recommended way to install Bioconductor packages (and
of course, this is not the recommended place to ask questions about
them: there is a dedicated bioconductor list).

Based on this:

> 	** Removing '/usr/local/lib64/R-2.4.1/library/hgu95av2'

I take it you are using R-2.4.1 on a 64-bit Linux platform.  It is
good that you are using an up-to-date version of R.  To install
Bioconductor packages, please try:

  R> source("http://bioconductor.org/biocLite.R")
  R> biocLite("hgu95av2")
  ## or to get everything geneplotter depends and suggests
  R> biocLite("geneplotter", dependencies=TRUE)

The error you are seeing is due to the fact that you have downloaded a
very old version of the data annotation package.  These packages are
maintained and updated version appropriate for current R are
available.  The easiest way is to install using biocLite, but you can
also get to the current stuff from the website.

+ seth


From ningwei.liu at countryfinancial.com  Thu Feb 15 20:19:26 2007
From: ningwei.liu at countryfinancial.com (Liu, Ningwei)
Date: Thu, 15 Feb 2007 13:19:26 -0600
Subject: [R] Does rpart package have some requirements on the original data
	set?
Message-ID: <6629E5915323C74B83E293BCA55923F6011C61EA@c2eml101.corp.alliance.lan>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/d303e346/attachment.pl 

From RMan54 at cox.net  Thu Feb 15 20:22:07 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Thu, 15 Feb 2007 11:22:07 -0800
Subject: [R] How to re-arrange data in table?
Message-ID: <010701c75136$9558b110$0900a8c0@rman>

I like to re-arrange a table (sTable) based on the value of one the rows
(Analyte) as shown below. Blocks of data with different values for Analyte
need to be stacked below each other. Any easy way to do this or any advice
where to look? 

Since it may be possible to get this in an earlier stage of the script, I
have added the R script that produces the original table below.
Thanks for any help.
Rene 


Original table (sTable): 
 
           1             2             3             4             5
6            
Analyte    "RBV"         "RBV"         "RBV"         "TBV"         "TBV"
"TBV"        
Dose       "200"         "400"         "600"         "200"         "400"
"600"        
AUC.n      "4"           "4"           "4"           "4"           "4"
"4"          
AUC.mean   " 44.023714"  " 77.853594"  "113.326952"  "  4.657904"  "
8.140416"  " 12.034377" 
...

Re-arranged table:

           1             2             3          
Analyte    "RBV"         "RBV"         "RBV"        
Dose       "200"         "400"         "600"        
AUC.n      "4"           "4"           "4"          
AUC.mean   " 44.023714"  " 77.853594"  "113.326952"  
...
Analyte    "TBV"         "TBV"         "TBV"        
Dose       "200"         "400"         "600"        
AUC.n      "4"           "4"           "4"          
AUC.mean   "  4.657904"  "  8.140416"  " 12.034377" 
...

The R script to produce the original table:

# Simulated simplified data
Subj  <- rep(1:4, each=6)
Analyte <- rep(c(rep("RBV",3),rep("TBV",3)),4)
Dose <- rep(c(200,400,600),8)
AUC <- rnorm(24, c(40,80,120,4,8,12), c(8,16,24,0.8,0.16,0.24))

# The real dataset may have NAs in it
df <- data.frame(Subj, Analyte, Dose, AUC)

myStats <- function(x) {
    count <- function(x) length(na.omit(x))
    pCV <- function(x) sd(x,na.rm=TRUE) / mean(x,na.rm=TRUE) * 100
    c(
      n = count(x),
      mean = mean(x,na.rm=TRUE),
      SD = sd(x,na.rm=TRUE),
      CV = pCV(x),
      median = median(x,na.rm=TRUE),
      min = min(x,na.rm=TRUE),
      max = max(x,na.rm=TRUE)
      )
}

library(doBy)
sData <- summaryBy(AUC ~ Analyte + Dose, data=df, FUN=myStats)
sTable <- t(sData)


From spalvarez at ico.scs.es  Thu Feb 15 18:47:03 2007
From: spalvarez at ico.scs.es (Perez Alvarez, Susana)
Date: Thu, 15 Feb 2007 18:47:03 +0100
Subject: [R] How to add obj to a list?
Message-ID: <4FC0206FF9D57141A14EF2745AD67A4623CD8F@icosrvmail01.ICO.SCS.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/433911d9/attachment.pl 

From sarah.goslee at gmail.com  Thu Feb 15 20:45:57 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 15 Feb 2007 14:45:57 -0500
Subject: [R] How to add obj to a list?
In-Reply-To: <4FC0206FF9D57141A14EF2745AD67A4623CD8F@icosrvmail01.ICO.SCS.local>
References: <4FC0206FF9D57141A14EF2745AD67A4623CD8F@icosrvmail01.ICO.SCS.local>
Message-ID: <efb536d50702151145k4a44e395w9f4d2ce1a8b7eb7a@mail.gmail.com>

The Introduction to R, available thru the documentation link at
www.r-project.org has some nice material on creating and modifying
lists that should answer your question.

Sarah

On 2/15/07, Perez Alvarez, Susana <spalvarez at ico.scs.es> wrote:
> Hello everybody!
>
> I'm quite new using R and i'm trying to develope a function, but i have
> a problem.
> What i want to build is something like an objects vector. I have a list
> with two tables, and after or next to them, I want to add more tables or
> vectors to that list one by one.  But i cannot find how to do it!
> Does someone can help me?
>
> I will be very grateful for any of your help!
>
> Thank you in advance,
> susana.
>
--
Sarah Goslee
http://www.functionaldiversity.org


From h.wickham at gmail.com  Thu Feb 15 20:55:40 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 16 Feb 2007 08:55:40 +1300
Subject: [R] How to re-arrange data in table?
In-Reply-To: <010701c75136$9558b110$0900a8c0@rman>
References: <010701c75136$9558b110$0900a8c0@rman>
Message-ID: <f8e6ff050702151155p2b9a953dv543c09f7f5554c2@mail.gmail.com>

> I like to re-arrange a table (sTable) based on the value of one the rows
> (Analyte) as shown below. Blocks of data with different values for Analyte
> need to be stacked below each other. Any easy way to do this or any advice
> where to look?

How about:

library(reshape)
dfm <- melt(df, m="AUC")

cast(dfm, Analyte + result_variable ~ Dose, myStats)

# A few other variations
cast(dfm, Analyte ~ Dose ~ result_variable, myStats)
cast(dfm, Analyte + Dose ~ result_variable, myStats)

# See http://had.co.nz/reshape for more documentation

Hadley


From JensScheidtmann at web.de  Thu Feb 15 21:06:37 2007
From: JensScheidtmann at web.de (Jens Scheidtmann)
Date: Thu, 15 Feb 2007 21:06:37 +0100
Subject: [R] R and threading
References: <BAY108-F19030BFD4ED37A74125033E4B90@phx.gbl>
Message-ID: <umz3fkwxe.fsf@grobi.scheidtmann.ath.cx>

"Simon Pears" <s.p.pears at hotmail.co.uk> writes:

> Hi,
>
> I am considering using R to integrate with a Java application. However, 
> before deciding upon R I need to understand if R is capable of dealing with 
> multiple requests simulataneously.
>
> Is a single instance of R capable of dealing with multiple simulataneous 
> requests or does a new instance of R have to be started for each request?
>
> I have read Luke Tierney's 2001 notes on threading at 
> http://www.stat.uiowa.edu/~luke/R/thrgui/thrgui.pdf.
> Have these concepts been introduced into the R engine?

Check out rserve at http://rosuda.org/Rserve/

HTH,

Jens 
---
Jens Scheidtmann


From liuwensui at gmail.com  Thu Feb 15 21:23:56 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 15 Feb 2007 15:23:56 -0500
Subject: [R] How to add obj to a list?
In-Reply-To: <4FC0206FF9D57141A14EF2745AD67A4623CD8F@icosrvmail01.ICO.SCS.local>
References: <4FC0206FF9D57141A14EF2745AD67A4623CD8F@icosrvmail01.ICO.SCS.local>
Message-ID: <1115a2b00702151223t314224f0kbf6c4383810e15a9@mail.gmail.com>

a = 1:2
b = 1:3
yours = list()
yours[[1]] = a
yours[[2]] = b

On 2/15/07, Perez Alvarez, Susana <spalvarez at ico.scs.es> wrote:
> Hello everybody!
>
> I'm quite new using R and i'm trying to develope a function, but i have
> a problem.
> What i want to build is something like an objects vector. I have a list
> with two tables, and after or next to them, I want to add more tables or
> vectors to that list one by one.  But i cannot find how to do it!
> Does someone can help me?
>
> I will be very grateful for any of your help!
>
> Thank you in advance,
> susana.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From JensScheidtmann at web.de  Thu Feb 15 21:23:46 2007
From: JensScheidtmann at web.de (Jens Scheidtmann)
Date: Thu, 15 Feb 2007 21:23:46 +0100
Subject: [R] Software for kriging
References: <9B2962F493D17F4F81A9211B1F9C56FB9B1A69@mail.ga.gov.au>
Message-ID: <uire3kw4t.fsf@grobi.scheidtmann.ath.cx>

<Augusto.Sanabria at ga.gov.au> writes:

> Dear R-list members,
>
> I wish everyone a happy and successful 2007!
>
> Does anyone know of R-based software for
> optimal spatial prediction (kriging)?
>
> We are working on a seismic event characterisation
> technique and need to do some kriging.
>
> Any help would be greatly appreciated.
>
> Augusto

MASS also has a chapter on Kriging, see 
http://www.stats.ox.ac.uk/pub/MASS4/

Cressie also does R:
http://www.stat.ohio-state.edu/~sses/research_cmck.html

Jens
---
Jens Scheidtmann


From Bartjoosen at hotmail.com  Thu Feb 15 21:24:19 2007
From: Bartjoosen at hotmail.com (Bart Joosen)
Date: Thu, 15 Feb 2007 21:24:19 +0100
Subject: [R] Time of failure, Arrhenius and Weibull distribution
Message-ID: <BAY134-DAV10CF5D17CA786EB0D7D2A3D8960@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/16a50d06/attachment.pl 

From RMan54 at cox.net  Thu Feb 15 21:34:54 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Thu, 15 Feb 2007 12:34:54 -0800
Subject: [R] How to re-arrange data in table?
In-Reply-To: <f8e6ff050702151155p2b9a953dv543c09f7f5554c2@mail.gmail.com>
References: <010701c75136$9558b110$0900a8c0@rman>
	<f8e6ff050702151155p2b9a953dv543c09f7f5554c2@mail.gmail.com>
Message-ID: <010e01c75140$c0113b20$0900a8c0@rman>

Thanks for the reply. I applied your suggestions on my example and maybe I
am missing something but these are alternate ways to create the original
table but not the re-arranged table. The data are not stacked by Analyte. I
will take a look at the documentation for this library to check whether
other code can do it.
Rene 

-----Original Message-----
From: hadley wickham [mailto:h.wickham at gmail.com] 
Sent: Thursday, February 15, 2007 11:56 AM
To: Rene Braeckman
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to re-arrange data in table?

> I like to re-arrange a table (sTable) based on the value of one the 
> rows
> (Analyte) as shown below. Blocks of data with different values for 
> Analyte need to be stacked below each other. Any easy way to do this 
> or any advice where to look?

How about:

library(reshape)
dfm <- melt(df, m="AUC")

cast(dfm, Analyte + result_variable ~ Dose, myStats)

# A few other variations
cast(dfm, Analyte ~ Dose ~ result_variable, myStats) cast(dfm, Analyte +
Dose ~ result_variable, myStats)

# See http://had.co.nz/reshape for more documentation

Hadley


From h.wickham at gmail.com  Thu Feb 15 21:46:15 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 16 Feb 2007 09:46:15 +1300
Subject: [R] How to re-arrange data in table?
In-Reply-To: <010e01c75140$c0113b20$0900a8c0@rman>
References: <010701c75136$9558b110$0900a8c0@rman>
	<f8e6ff050702151155p2b9a953dv543c09f7f5554c2@mail.gmail.com>
	<010e01c75140$c0113b20$0900a8c0@rman>
Message-ID: <f8e6ff050702151246v6f002e3ayf789e9df6c072f71@mail.gmail.com>

On 2/16/07, Rene Braeckman <RMan54 at cox.net> wrote:
> Thanks for the reply. I applied your suggestions on my example and maybe I
> am missing something but these are alternate ways to create the original
> table but not the re-arranged table. The data are not stacked by Analyte. I
> will take a look at the documentation for this library to check whether
> other code can do it.
> Rene

The first example produces:

> cast(dfm, Analyte + result_variable ~ Dose, myStats)
   Analyte result_variable       X200        X400        X600
1      RBV               n  4.0000000   4.0000000   4.0000000
2      RBV            mean 36.3162818  75.9106859 103.3703973
3      RBV              SD  6.2482597  21.9670933  16.6163538
4      RBV              CV 17.2051197  28.9380777  16.0745767
5      RBV          median 34.9785799  70.2915655  96.9303908
6      RBV             min 30.5608993  56.5248140  92.2041812
7      RBV             max 44.7470678 106.5347986 127.4166264
8      TBV               n  4.0000000   4.0000000   4.0000000
9      TBV            mean  3.4647015   8.0472022  11.9026746
10     TBV              SD  0.8035588   0.1214145   0.1639920
11     TBV              CV 23.1927273   1.5087796   1.3777746
12     TBV          median  3.3883839   8.0290574  11.9685917
13     TBV             min  2.5689998   7.9216424  11.6625081
14     TBV             max  4.5130385   8.2090518  12.0110069

Which looks stacked by analyte to me (although not in exactly the same
format as your table).  Perhaps I misunderstand what you want.

Maybe you want:

> cast(dfm, result_variable ~ Dose | Analyte, myStats)
$RBV
  result_variable     X200      X400      X600
1               n  4.00000   4.00000   4.00000
2            mean 36.31628  75.91069 103.37040
3              SD  6.24826  21.96709  16.61635
4              CV 17.20512  28.93808  16.07458
5          median 34.97858  70.29157  96.93039
6             min 30.56090  56.52481  92.20418
7             max 44.74707 106.53480 127.41663

$TBV
  result_variable       X200      X400       X600
1               n  4.0000000 4.0000000  4.0000000
2            mean  3.4647015 8.0472022 11.9026746
3              SD  0.8035588 0.1214145  0.1639920
4              CV 23.1927273 1.5087796  1.3777746
5          median  3.3883839 8.0290574 11.9685917
6             min  2.5689998 7.9216424 11.6625081
7             max  4.5130385 8.2090518 12.0110069


Hadley


From sje at mast.queensu.ca  Thu Feb 15 21:50:56 2007
From: sje at mast.queensu.ca (Stephen Bond)
Date: Thu, 15 Feb 2007 14:50:56 -0600
Subject: [R] help with tryCatch
In-Reply-To: <59d7961d0702131751j416e6109t17cca70eb47a0e3@mail.gmail.com>
References: <45D0DF70.1090801@mast.queensu.ca>	
	<59d7961d0702121904n2104d327udd7c964db0048956@mail.gmail.com>	
	<45D1CEB1.9080603@mast.queensu.ca>	
	<59d7961d0702130802t5cd28a91ia269f2bdbc085a0f@mail.gmail.com>	
	<45D25A43.4050203@mast.queensu.ca>	
	<59d7961d0702131750occ6945dx3ce445aef898deaf@mail.gmail.com>
	<59d7961d0702131751j416e6109t17cca70eb47a0e3@mail.gmail.com>
Message-ID: <45D4C7B0.5090203@mast.queensu.ca>

Henrik,
Please, stay with me.
there is a problem with the way tryCatch processes statements and that 
is exactly what the help file does not describe at all. I am trying

************************
catch=function(vec){
  ans=NULL;err=NULL;
  for (i in vec) {
  tryCatch({
      source(i);
      ans=c(ans,i);
      cat(ans," from try");
  },
  error=function(er){
    cat(i," from catch\n");
    err=c(err,i);

  }
           )
 
}
  ret=list(ans=ans,err=err)
  ret
}

v=c("gdhfdh","hdhdfjh")  #non-existent files
ret=catch(v)  # err is NULL and none of the statements in that block is 
executed ??

below is a Java example that executes the catch block as it should. How 
can I achieve the same in R?
***************************
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.Vector;

public class ReadFiles {
   
    public static void main(String[] args) {
        String[] files={"asdf","qwert"};
        Vector err=new Vector();
        FileReader inputStream = null;
        for (int j=0;j<2;j++){
            try {
                inputStream = new FileReader(files[j]);
                int c;
                while ((c = inputStream.read()) != -1) {}
            } catch(IOException e){
                err.add(j,files[j]); // this statement executes!
            } finally {
                if (inputStream != null) {
                    inputStream.close(); // IO not encapsulated here, 
but declared in "main throws". pls ignore since "source(arg)" in R takes 
care of closing.
                }
            }
        }
        for (Object j:err){
        System.out.println(j);
    }
    }
}









Henrik Bengtsson wrote:

> To be more precise, put the tryCatch() only around the code causing
> the problem, i.e. around source().  /H
>
> On 2/13/07, Henrik Bengtsson <hb at stat.berkeley.edu> wrote:
>
>> Put the for loop outside the tryCatch(). /H
>>
>> On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
>> > Henrik,
>> >
>> > thank you for the reference. Can you please tell me why the following
>> > does not work?
>> >
>> > vec=c("hdfhjfd","jdhfhjfg")    # non-existent file names
>> > catch=function(vec){
>> >   tryCatch({
>> >     ans =NULL;err=NULL;
>> >     for (i in vec) {
>> >       source(i)
>> >       ans=c(ans,i)
>> >     }
>> >   },
>> >   interrupt=function(ex){print(ex)},
>> >   error=function(er){
>> >      print(er)
>> >      cat(i,"\n")
>> >      err=c(err,i)
>> >   },
>> >   finally={
>> >     cat("finish")
>> >   }
>> >  ) #tryCatch
>> > }
>> >
>> > catch(vec) # throws an error after the first file and stops there 
>> while
>> > I want it to go through the list and accumulate the nonexistent
>> > filenames in err.
>> >
>> > Thank you
>> > Stephen
>> >
>> > Henrik Bengtsson wrote:
>> >
>> > > Hi,
>> > >
>> > > google "R tryCatch example" and you'll find:
>> > >
>> > >  http://www.maths.lth.se/help/R/ExceptionHandlingInR/
>> > >
>> > > Hope this helps
>> > >
>> > > Henrik
>> > >
>> > > On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
>> > >
>> > >> Henrik,
>> > >>
>> > >> I had looked at tryCatch before posting the question and asked the
>> > >> question because the help file was not adequate for me. Could 
>> you pls
>> > >> provide a sample code of
>> > >> try{ try code}
>> > >> catch(error){catch code}
>> > >>
>> > >> let's say you have a vector of local file names and want to 
>> source them
>> > >> encapsulating in a tryCatch to avoid the skipping of all good 
>> file names
>> > >> after a bad file name.
>> > >>
>> > >> thank you
>> > >> stephen
>> > >>
>> > >>
>> > >> Henrik Bengtsson wrote:
>> > >>
>> > >> > See ?tryCatch. /Henrik
>> > >> >
>> > >> > On 2/12/07, Stephen Bond <sje at mast.queensu.ca> wrote:
>> > >> >
>> > >> >> Could smb please help with try-catch encapsulating a function 
>> for
>> > >> >> downloading. Let's say I have a character vector of symbols and
>> > >> want to
>> > >> >> download each one and surround by try and catch to be safe
>> > >> >>
>> > >> >> # get.hist.quote() is in library(tseries), but the question 
>> does not
>> > >> >> depend on it, I could be sourcing local files instead
>> > >> >>
>> > >> >> ans=null;error=null;
>> > >> >> for ( sym in sym.vec){
>> > >> >> try(ans=cbind(ans,get.hist.quote(sym,start=start))) 
>> #accumulate in
>> > >> a zoo
>> > >> >> matrix
>> > >> >> catch(theurlerror){error=c(error,sym)} #accumulate failed 
>> symbols
>> > >> >> }
>> > >> >>
>> > >> >> I know the code above does not work, but it conveys the idea.
>> > >> tryCatch
>> > >> >> help page says it is similar to Java try-catch, but I know 
>> how to
>> > >> do a
>> > >> >> try-catch in Java and still can't do it in R.
>> > >> >>
>> > >> >> Thank you very much.
>> > >> >> stephen
>> > >> >>
>> > >> >> ______________________________________________
>> > >> >> R-help at stat.math.ethz.ch mailing list
>> > >> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >> >> PLEASE do read the posting guide
>> > >> >> http://www.R-project.org/posting-guide.html
>> > >> >> and provide commented, minimal, self-contained, reproducible 
>> code.
>> > >> >>
>> > >>
>> >
>>
>


From RMan54 at cox.net  Thu Feb 15 22:59:14 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Thu, 15 Feb 2007 13:59:14 -0800
Subject: [R] How to re-arrange data in table?
In-Reply-To: <f8e6ff050702151246v6f002e3ayf789e9df6c072f71@mail.gmail.com>
References: <010701c75136$9558b110$0900a8c0@rman>
	<f8e6ff050702151155p2b9a953dv543c09f7f5554c2@mail.gmail.com>
	<010e01c75140$c0113b20$0900a8c0@rman>
	<f8e6ff050702151246v6f002e3ayf789e9df6c072f71@mail.gmail.com>
Message-ID: <011601c7514c$87e75e80$0900a8c0@rman>

The last example in your last message comes really close to the re-arranged
table listed in my original message.

Re-arranged table listed in my original message:

           1             2             3          
Analyte    "RBV"         "RBV"         "RBV"        
Dose       "200"         "400"         "600"        
AUC.n      "4"           "4"           "4"          
AUC.mean   " 44.023714"  " 77.853594"  "113.326952"  
...
Analyte    "TBV"         "TBV"         "TBV"        
Dose       "200"         "400"         "600"        
AUC.n      "4"           "4"           "4"          
AUC.mean   "  4.657904"  "  8.140416"  " 12.034377" 
...

The final step was to write this table to disk as follows:

write.table(myTable, file="stest.xls", sep="\t", row.names=T, col.names=F,
qmethod="double")

Since

cast(dfm, result_variable ~ Dose | Analyte, myStats)

it not a table, I can't use this. Is there another way to write the data to
disk after the cast and get the same results in the file? Or convert the
array into a table?

Thanks again for all your help,
Rene
Irvine, California, USA 

-----Original Message-----
From: hadley wickham [mailto:h.wickham at gmail.com] 
Sent: Thursday, February 15, 2007 12:46 PM
To: Rene Braeckman
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to re-arrange data in table?

On 2/16/07, Rene Braeckman <RMan54 at cox.net> wrote:
> Thanks for the reply. I applied your suggestions on my example and 
> maybe I am missing something but these are alternate ways to create 
> the original table but not the re-arranged table. The data are not 
> stacked by Analyte. I will take a look at the documentation for this 
> library to check whether other code can do it.
> Rene

The first example produces:

> cast(dfm, Analyte + result_variable ~ Dose, myStats)
   Analyte result_variable       X200        X400        X600
1      RBV               n  4.0000000   4.0000000   4.0000000
2      RBV            mean 36.3162818  75.9106859 103.3703973
3      RBV              SD  6.2482597  21.9670933  16.6163538
4      RBV              CV 17.2051197  28.9380777  16.0745767
5      RBV          median 34.9785799  70.2915655  96.9303908
6      RBV             min 30.5608993  56.5248140  92.2041812
7      RBV             max 44.7470678 106.5347986 127.4166264
8      TBV               n  4.0000000   4.0000000   4.0000000
9      TBV            mean  3.4647015   8.0472022  11.9026746
10     TBV              SD  0.8035588   0.1214145   0.1639920
11     TBV              CV 23.1927273   1.5087796   1.3777746
12     TBV          median  3.3883839   8.0290574  11.9685917
13     TBV             min  2.5689998   7.9216424  11.6625081
14     TBV             max  4.5130385   8.2090518  12.0110069

Which looks stacked by analyte to me (although not in exactly the same
format as your table).  Perhaps I misunderstand what you want.

Maybe you want:

> cast(dfm, result_variable ~ Dose | Analyte, myStats)
$RBV
  result_variable     X200      X400      X600
1               n  4.00000   4.00000   4.00000
2            mean 36.31628  75.91069 103.37040
3              SD  6.24826  21.96709  16.61635
4              CV 17.20512  28.93808  16.07458
5          median 34.97858  70.29157  96.93039
6             min 30.56090  56.52481  92.20418
7             max 44.74707 106.53480 127.41663

$TBV
  result_variable       X200      X400       X600
1               n  4.0000000 4.0000000  4.0000000
2            mean  3.4647015 8.0472022 11.9026746
3              SD  0.8035588 0.1214145  0.1639920
4              CV 23.1927273 1.5087796  1.3777746
5          median  3.3883839 8.0290574 11.9685917
6             min  2.5689998 7.9216424 11.6625081
7             max  4.5130385 8.2090518 12.0110069


Hadley


From ted.harding at nessie.mcc.ac.uk  Thu Feb 15 23:15:03 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 15 Feb 2007 22:15:03 -0000 (GMT)
Subject: [R] Time of failure, Arrhenius and Weibull distribution
In-Reply-To: <BAY134-DAV10CF5D17CA786EB0D7D2A3D8960@phx.gbl>
Message-ID: <XFMail.070215221503.ted.harding@nessie.mcc.ac.uk>

On 15-Feb-07 Bart Joosen wrote:
> Hi,
> 
> I'm currently doing some analyses on time of failure of a product.
> I found on the internet some article about the Arrhenius equation,
> and I can calculate the results with R.
> Equation:
> k=A*exp(-Ea/R*T)
> 
> I can fit a model with lm for this purpose, so far no problem.
> 
> But for the confidence interval, how can I use the Weibull
> distribution?
> Or should I use the weibull distribution at all? Or can I safely use
> the predict.lm method to predict my time of failure at a certain
> temperature?
> Is there anyone who has experience with this kind of calculations?

Can you clarify your query?

Since you are using lm, would you write what your model is,
in terms like

  lm( Y ~ X1 + X2 + ... )

explaining how Y relates to failure time, and what sort of
variables X1, X2, ... are, and how Arrhenius (normally used
for evaluating the rate coefficient k of a chemical reaction,
where the only quantity that might be subject to statistical
estimation is the coefficient A) comes into it?

The standard output of summary(lm(...)) will give you the
estimates, and their standard errors, of the coefficients
of X1, X2, ... in the linear model, from which confidence
intervals for these coefficients can readily be computed.

You can certainly use predict.lm to produce

a) A confidence interval for the estimated expected ("predicted")
   value of Y, taking account of the uncertainties in the
   estimates of a1, a2, ... in Y = a1*X1 + a2*X2 + ...

b) A "prediction interval" (also called "tolerance interval")
   for the value of Y that would be observed at given values
   of X1, X2, ... , taking account both the uncertainties
   in the estimated coefficients (i.e. in the expected value
   of Y) and the estimated random scatter of Y about its
   expected value.

How these would relate to the failure time (tF say) depends
on the relationship between Y and tF.

Your query does not describe how any Weibull considerations
come into your linear model!

If you can spell it all out for us in sufficient detail, I'm
sure people will be able to contribute suggestions and explanations.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 15-Feb-07                                       Time: 22:15:00
------------------------------ XFMail ------------------------------


From betty.health at gmail.com  Thu Feb 15 23:21:39 2007
From: betty.health at gmail.com (Betty Health)
Date: Thu, 15 Feb 2007 15:21:39 -0700
Subject: [R] Does rpart package have some requirements on the original
	data set?
In-Reply-To: <6629E5915323C74B83E293BCA55923F6011C61EA@c2eml101.corp.alliance.lan>
References: <6629E5915323C74B83E293BCA55923F6011C61EA@c2eml101.corp.alliance.lan>
Message-ID: <34eed650702151421u1550bbb9l66bf23318c1f475a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070215/f10f017b/attachment.pl 

From hb at stat.berkeley.edu  Thu Feb 15 23:27:07 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Thu, 15 Feb 2007 14:27:07 -0800
Subject: [R] help with tryCatch
In-Reply-To: <45D4C7B0.5090203@mast.queensu.ca>
References: <45D0DF70.1090801@mast.queensu.ca>
	<59d7961d0702121904n2104d327udd7c964db0048956@mail.gmail.com>
	<45D1CEB1.9080603@mast.queensu.ca>
	<59d7961d0702130802t5cd28a91ia269f2bdbc085a0f@mail.gmail.com>
	<45D25A43.4050203@mast.queensu.ca>
	<59d7961d0702131750occ6945dx3ce445aef898deaf@mail.gmail.com>
	<59d7961d0702131751j416e6109t17cca70eb47a0e3@mail.gmail.com>
	<45D4C7B0.5090203@mast.queensu.ca>
Message-ID: <59d7961d0702151427u7b72fdb9ocadd7828cb4912c1@mail.gmail.com>

Note that the error catch is calling your locally defined function.
Like anywhere in R, if you assign something to a variable inside a
function, it only applies there and not "outside".  The quick a dirty
fix is to do:

 err <<- c(err, er)

/H

On 2/15/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> Henrik,
> Please, stay with me.
> there is a problem with the way tryCatch processes statements and that
> is exactly what the help file does not describe at all. I am trying
>
> ************************
> catch=function(vec){
>   ans=NULL;err=NULL;
>   for (i in vec) {
>   tryCatch({
>       source(i);
>       ans=c(ans,i);
>       cat(ans," from try");
>   },
>   error=function(er){
>     cat(i," from catch\n");
>     err=c(err,i);
>
>   }
>            )
>
> }
>   ret=list(ans=ans,err=err)
>   ret
> }
>
> v=c("gdhfdh","hdhdfjh")  #non-existent files
> ret=catch(v)  # err is NULL and none of the statements in that block is
> executed ??
>
> below is a Java example that executes the catch block as it should. How
> can I achieve the same in R?
> ***************************
> import java.io.FileReader;
> import java.io.FileWriter;
> import java.io.IOException;
> import java.util.Vector;
>
> public class ReadFiles {
>
>     public static void main(String[] args) {
>         String[] files={"asdf","qwert"};
>         Vector err=new Vector();
>         FileReader inputStream = null;
>         for (int j=0;j<2;j++){
>             try {
>                 inputStream = new FileReader(files[j]);
>                 int c;
>                 while ((c = inputStream.read()) != -1) {}
>             } catch(IOException e){
>                 err.add(j,files[j]); // this statement executes!
>             } finally {
>                 if (inputStream != null) {
>                     inputStream.close(); // IO not encapsulated here,
> but declared in "main throws". pls ignore since "source(arg)" in R takes
> care of closing.
>                 }
>             }
>         }
>         for (Object j:err){
>         System.out.println(j);
>     }
>     }
> }
>
>
>
>
>
>
>
>
>
> Henrik Bengtsson wrote:
>
> > To be more precise, put the tryCatch() only around the code causing
> > the problem, i.e. around source().  /H
> >
> > On 2/13/07, Henrik Bengtsson <hb at stat.berkeley.edu> wrote:
> >
> >> Put the for loop outside the tryCatch(). /H
> >>
> >> On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> >> > Henrik,
> >> >
> >> > thank you for the reference. Can you please tell me why the following
> >> > does not work?
> >> >
> >> > vec=c("hdfhjfd","jdhfhjfg")    # non-existent file names
> >> > catch=function(vec){
> >> >   tryCatch({
> >> >     ans =NULL;err=NULL;
> >> >     for (i in vec) {
> >> >       source(i)
> >> >       ans=c(ans,i)
> >> >     }
> >> >   },
> >> >   interrupt=function(ex){print(ex)},
> >> >   error=function(er){
> >> >      print(er)
> >> >      cat(i,"\n")
> >> >      err=c(err,i)
> >> >   },
> >> >   finally={
> >> >     cat("finish")
> >> >   }
> >> >  ) #tryCatch
> >> > }
> >> >
> >> > catch(vec) # throws an error after the first file and stops there
> >> while
> >> > I want it to go through the list and accumulate the nonexistent
> >> > filenames in err.
> >> >
> >> > Thank you
> >> > Stephen
> >> >
> >> > Henrik Bengtsson wrote:
> >> >
> >> > > Hi,
> >> > >
> >> > > google "R tryCatch example" and you'll find:
> >> > >
> >> > >  http://www.maths.lth.se/help/R/ExceptionHandlingInR/
> >> > >
> >> > > Hope this helps
> >> > >
> >> > > Henrik
> >> > >
> >> > > On 2/13/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> >> > >
> >> > >> Henrik,
> >> > >>
> >> > >> I had looked at tryCatch before posting the question and asked the
> >> > >> question because the help file was not adequate for me. Could
> >> you pls
> >> > >> provide a sample code of
> >> > >> try{ try code}
> >> > >> catch(error){catch code}
> >> > >>
> >> > >> let's say you have a vector of local file names and want to
> >> source them
> >> > >> encapsulating in a tryCatch to avoid the skipping of all good
> >> file names
> >> > >> after a bad file name.
> >> > >>
> >> > >> thank you
> >> > >> stephen
> >> > >>
> >> > >>
> >> > >> Henrik Bengtsson wrote:
> >> > >>
> >> > >> > See ?tryCatch. /Henrik
> >> > >> >
> >> > >> > On 2/12/07, Stephen Bond <sje at mast.queensu.ca> wrote:
> >> > >> >
> >> > >> >> Could smb please help with try-catch encapsulating a function
> >> for
> >> > >> >> downloading. Let's say I have a character vector of symbols and
> >> > >> want to
> >> > >> >> download each one and surround by try and catch to be safe
> >> > >> >>
> >> > >> >> # get.hist.quote() is in library(tseries), but the question
> >> does not
> >> > >> >> depend on it, I could be sourcing local files instead
> >> > >> >>
> >> > >> >> ans=null;error=null;
> >> > >> >> for ( sym in sym.vec){
> >> > >> >> try(ans=cbind(ans,get.hist.quote(sym,start=start)))
> >> #accumulate in
> >> > >> a zoo
> >> > >> >> matrix
> >> > >> >> catch(theurlerror){error=c(error,sym)} #accumulate failed
> >> symbols
> >> > >> >> }
> >> > >> >>
> >> > >> >> I know the code above does not work, but it conveys the idea.
> >> > >> tryCatch
> >> > >> >> help page says it is similar to Java try-catch, but I know
> >> how to
> >> > >> do a
> >> > >> >> try-catch in Java and still can't do it in R.
> >> > >> >>
> >> > >> >> Thank you very much.
> >> > >> >> stephen
> >> > >> >>
> >> > >> >> ______________________________________________
> >> > >> >> R-help at stat.math.ethz.ch mailing list
> >> > >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> > >> >> PLEASE do read the posting guide
> >> > >> >> http://www.R-project.org/posting-guide.html
> >> > >> >> and provide commented, minimal, self-contained, reproducible
> >> code.
> >> > >> >>
> >> > >>
> >> >
> >>
> >
>
>


From plynchnlm at gmail.com  Thu Feb 15 23:30:14 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Thu, 15 Feb 2007 17:30:14 -0500
Subject: [R] integrate over polygon
In-Reply-To: <E9C86560-5BA2-4DF7-875A-DF462CE31AA3@ucla.edu>
References: <E9C86560-5BA2-4DF7-875A-DF462CE31AA3@ucla.edu>
Message-ID: <50d6c72a0702151430i24b09e80i8050b925764f5359@mail.gmail.com>

I'm still pretty ignorant about R, but I think it might be possible to
work out an algorithm using  cross products.  First you would want to
subdivide the polygon into convex polygons.  I haven't tried to do
that before, but it looks like it might be possible by looking at the
sign of cross products of vectors between vertices.  (In other words,
pick a vertex, and then start working your way around the polygon and
pay attention to the sign of cross products of vectors from the
starting vertex to successive vertices.)  Once you have convex
polygons, you can calculate the area using cross products of vectors
from some point (e.g. the origin) to adjacent vertices of the polygon.
 I think that probably most computer graphics texts would have such an
algorithm.

How to implement that in R is not something I can answer, but it
doesn't sound hard.
       --Paul

On 2/14/07, Haiyong Xu <xuhy at ucla.edu> wrote:
> Hi there,
>
> I want to integrate a function over an irregular polygon. Is there
> any function which can implement this easily? Otherwise, I am
> thinking of divide the polygon into very small rectangles and use
> "adapt" to approximate it. Do you have any suggestions to get the
> fine division? Any advice is appreciated.
>
> Haiyong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From plynchnlm at gmail.com  Thu Feb 15 23:47:37 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Thu, 15 Feb 2007 17:47:37 -0500
Subject: [R] integrate over polygon
In-Reply-To: <50d6c72a0702151430i24b09e80i8050b925764f5359@mail.gmail.com>
References: <E9C86560-5BA2-4DF7-875A-DF462CE31AA3@ucla.edu>
	<50d6c72a0702151430i24b09e80i8050b925764f5359@mail.gmail.com>
Message-ID: <50d6c72a0702151447g70e6ddb8p29eaa514cd6f91ed@mail.gmail.com>

Oops.  I just re-read your message and saw you were trying to
integrate a function over a polygon, not calculate its area.  I'm
sorry I didn't read more carefully.
         --Paul

On 2/15/07, Paul Lynch <plynchnlm at gmail.com> wrote:
> I'm still pretty ignorant about R, but I think it might be possible to
> work out an algorithm using  cross products.  First you would want to
> subdivide the polygon into convex polygons.  I haven't tried to do
> that before, but it looks like it might be possible by looking at the
> sign of cross products of vectors between vertices.  (In other words,
> pick a vertex, and then start working your way around the polygon and
> pay attention to the sign of cross products of vectors from the
> starting vertex to successive vertices.)  Once you have convex
> polygons, you can calculate the area using cross products of vectors
> from some point (e.g. the origin) to adjacent vertices of the polygon.
>  I think that probably most computer graphics texts would have such an
> algorithm.
>
> How to implement that in R is not something I can answer, but it
> doesn't sound hard.
>        --Paul
>
> On 2/14/07, Haiyong Xu <xuhy at ucla.edu> wrote:
> > Hi there,
> >
> > I want to integrate a function over an irregular polygon. Is there
> > any function which can implement this easily? Otherwise, I am
> > thinking of divide the polygon into very small rectangles and use
> > "adapt" to approximate it. Do you have any suggestions to get the
> > fine division? Any advice is appreciated.
> >
> > Haiyong
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From topkatz at msn.com  Thu Feb 15 23:50:16 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 15 Feb 2007 17:50:16 -0500
Subject: [R] Function to assign quantiles?
Message-ID: <BAY132-F3226C9E1F129DFD6343A54AA960@phx.gbl>

Hi.

If I call the quantiles function as follows:

qvec = quantiles(dvals,probs=seq(0,1,0.1))

the results will return a vector something like the following example:

      0%    10%    20%    30%     40%     50%    60%    70%      80%     90% 
     100%
    56.0   137.3   238.4   317.9   495.8   568.5   807.4  1207.7  1713.0  
2951.1  8703.0

Now I want to assign the deciles, 1 - 10, to each observation, so, in the 
above example, if dvals[322] = 256, I want to assign qvals[322] = 3, and if 
dvals[7216] = 1083, I want qvals[7216] = 7, etc.  I would think there would 
be a function, or some very quick code to do that, but I couldn't find it.

Here's what I have now.  It works, but I figure there must be a better way:

asdc <- function(q){max(1,which(qvec<q))}
qvals = apply(matrix(dvals,nrow=1),2,asdc)


Any suggestions?  Thanks!


--  TMK  --
212-460-5430	home
917-656-5351	cell


From marc_schwartz at comcast.net  Fri Feb 16 00:15:07 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 15 Feb 2007 17:15:07 -0600
Subject: [R] Function to assign quantiles?
In-Reply-To: <BAY132-F3226C9E1F129DFD6343A54AA960@phx.gbl>
References: <BAY132-F3226C9E1F129DFD6343A54AA960@phx.gbl>
Message-ID: <1171581307.4785.85.camel@localhost.localdomain>

On Thu, 2007-02-15 at 17:50 -0500, Talbot Katz wrote:
> Hi.
> 
> If I call the quantiles function as follows:
> 
> qvec = quantiles(dvals,probs=seq(0,1,0.1))
> 
> the results will return a vector something like the following example:
> 
>       0%    10%    20%    30%     40%     50%    60%    70%      80%     90% 
>      100%
>     56.0   137.3   238.4   317.9   495.8   568.5   807.4  1207.7  1713.0  
> 2951.1  8703.0
> 
> Now I want to assign the deciles, 1 - 10, to each observation, so, in the 
> above example, if dvals[322] = 256, I want to assign qvals[322] = 3, and if 
> dvals[7216] = 1083, I want qvals[7216] = 7, etc.  I would think there would 
> be a function, or some very quick code to do that, but I couldn't find it.
> 
> Here's what I have now.  It works, but I figure there must be a better way:
> 
> asdc <- function(q){max(1,which(qvec<q))}
> qvals = apply(matrix(dvals,nrow=1),2,asdc)
> 
> 
> Any suggestions?  Thanks!

Take a look at ?cut and use the output of quantile() to define the
'breaks' argument:

x <- rnorm(100)

> cut(x, breaks = quantile(x, probs = seq(0, 1, 0.1)), 
      include.lowest = TRUE, labels = 1:10)
  [1] 2  1  9  10 9  3  1  7  5  2  4  1  10 8  4  10 9  9  5  3  5  8 
 [23] 6  9  7  1  10 1  10 9  3  3  9  3  5  5  6  4  8  10 6  2  2  8 
 [45] 6  3  6  9  1  4  7  10 8  7  5  3  1  10 2  1  7  8  8  2  8  8 
 [67] 3  10 4  6  5  1  4  6  4  4  5  9  9  7  2  8  2  5  1  3  7  6 
 [89] 4  6  4  6  2  7  2  10 7  3  5  7 
Levels: 1 2 3 4 5 6 7 8 9 10


HTH,

Marc Schwartz


From plynchnlm at gmail.com  Fri Feb 16 00:35:16 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Thu, 15 Feb 2007 18:35:16 -0500
Subject: [R] R book advice
Message-ID: <50d6c72a0702151535p5ee6da4bq404176d3290cc0f2@mail.gmail.com>

I'm looking for a book for someone completely ignorant of statistics
who wishes to learn both statistics and R.  I've found three
possibilities, one by Verzani ("Using R for Introductory Statistics"),
one by Crawley ("Statistics: An Introduction using R"), and one by
Dalgaard ("Introductory Statistics with R").  Do these books have
different emphases, perspectives, or strengths?  Should I just pick
one at random and buy it?

Thanks,
        --Paul


From g_smits at verizon.net  Fri Feb 16 00:46:09 2007
From: g_smits at verizon.net (Gerard Smits)
Date: Thu, 15 Feb 2007 15:46:09 -0800
Subject: [R] something missing in summary()
Message-ID: <7.0.1.0.2.20070215154327.0342f808@verizon.net>


I just noticed that two key pieces of information are not given by 
the summary() command:  N and SD.  we are given the N missing, but 
not the converse.  I know these summary value can be obtained easy, 
but can't understand why these two pieces of information are not 
provided with the other info.

Thanks,

Gerard


From roberto.perdisci at gmail.com  Fri Feb 16 00:51:05 2007
From: roberto.perdisci at gmail.com (Roberto Perdisci)
Date: Thu, 15 Feb 2007 18:51:05 -0500
Subject: [R] Does rpart package have some requirements on the original
	data set?
In-Reply-To: <6629E5915323C74B83E293BCA55923F6011C61EA@c2eml101.corp.alliance.lan>
References: <6629E5915323C74B83E293BCA55923F6011C61EA@c2eml101.corp.alliance.lan>
Message-ID: <cf94d0090702151551w4afbd8e0wb0e426244c35d1bf@mail.gmail.com>

Hi,
  try to set minsplit=2 and cp=0. After training you can prune with
different values of cp, and plot how the accuracy changes.

try this code (which I'm sure can be improved)

require(rpart)

rpart.prune.stats <- function(unpruned.tree,testset,class.index.name,cp) {
    acc.rpart.pruned <- list()
    nnodes <- NULL

    rpart.pruned <- unpruned.tree;
    for(i in 1:length(cp)) {
        print(paste("cp =",cp[i]))

        rpart.pruned <- prune(rpart.pruned,cp[i])
        pred.rpart.pruned <- predict(rpart.pruned,testset,type="class")
        acc <- sum(pred.rpart.pruned==testset[,class.index.name])/nrow(testset)
        acc.rpart.pruned <- c(acc.rpart.pruned,list(acc))
        nnodes <- c(nnodes,nrow(rpart.pruned$frame))
    }

    return(list(acc = acc.rpart.pruned, nnodes = nnodes))
}


plot.rpart.prune.results <-
function(formula,traininingset,testset,class.index.name,dataset.name,cp,add=F,ylim=NULL)
{

     rpart.unpruned <-
rpart(formula,data=traininingset,control=rpart.control(minsplit=2,cp=0))
     res <- rpart.prune.stats(rpart.unpruned,testset,class.index.name,cp)

     x <- unlist(res$acc)
     y <- unlist(res$nnodes)

     print(x)
     print(y)


    if(add)
        par(new=T)
    plot(cp,x,type="l",col="blue",ylim=ylim,ann=F)
    text(cp[c(seq(1,length(cp),by=5))],x[c(seq(1,length(cp),by=5))],paste("(",y[seq(1,length(cp),by=5)],")",sep=""),pos=3,cex=0.5)
    title(main=dataset.name,xlab="cp",ylab="Accuracy",font=3,cex=0.5)
}


and call it using something similar
plot.rpart.prune.results(Class~.,DatasetX.train,DatasetX.test,"Class","DatasetX",cp=seq(0,0.005,by=0.0001))


You can also oversample the minority class using sampling with
replacement or undersample the majority class.  This are two very
simple techniques used in machine learning when dealing with
unbalanced datasets (there are more complicated techniques which
produce better results, though)

hope this helps,
cheers,
Roberto

On 2/15/07, Liu, Ningwei <ningwei.liu at countryfinancial.com> wrote:
> Hi,
>
>
>
> I am currently studying Decision Trees by using rpart package in R. I
> artificially created a data set which includes the dependant variable
> (y) and a few independent variables (x1, x2...). The dependant variable
> y only comprises 0 and 1. 90% of y are 1 and 10% of y are 0. When I
> apply rpart to it, there is no splitting at all.
>
>
>
> I am wondering whether this is because of the "special" distribution of
> y. Since the majority of y is 1 (information in the data set is small),
> rpart automatically regards it as already a single class and therefore
> won't proceed any further. If this understanding is correct, what I
> should do if I still want rpart to do something on this data set?
>
>
>
>
>
> Thanks a lot!
>
>
>
>
>
> Ningwei
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From RMan54 at cox.net  Fri Feb 16 01:03:39 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Thu, 15 Feb 2007 16:03:39 -0800
Subject: [R] How to re-arrange data in table?
In-Reply-To: <011601c7514c$87e75e80$0900a8c0@rman>
References: <010701c75136$9558b110$0900a8c0@rman><f8e6ff050702151155p2b9a953dv543c09f7f5554c2@mail.gmail.com><010e01c75140$c0113b20$0900a8c0@rman><f8e6ff050702151246v6f002e3ayf789e9df6c072f71@mail.gmail.com>
	<011601c7514c$87e75e80$0900a8c0@rman>
Message-ID: <011701c7515d$e9a6ce60$0900a8c0@rman>

I found the answer myself. Since the casted object is an array of
data.frames, I am looping though the array to write/append the data.frames
to disk. Matters got a bit more complicated in the melting and casting since
I have more than just AUC as measured values. Got that handled as well. A
full example with two measured variables (AUC and Cmax) is listed below in
case other users like to see.

The only remaining thing is that the headers for each data.frame column are
a combination, formed as "Analyte.Variable_Dose" while I need all 3 elements
(Analyte, Variable and Dose) on different lines. If there is a quick fix,
please let me know. I noticed, for example, that "prettyprint(res$RBV)" adds
separate lines for Variable and Dose.
 
Thanks again for pointing me to your Reshape library. Very powerful.
Rene

# Simulated simplified data
Subj  <- rep(1:4, each=6)
Analyte <- rep(c(rep("RBV",3),rep("TBV",3)),4)
Dose <- rep(c(200,400,600),8)
AUC <- rnorm(24, c(40,80,120,4,8,12), c(8,16,24,0.8,0.16,0.24))
Cmax <- rnorm(24, c(4,8,12,0.4,0.8,1.2), c(0.8,1.6,2.4,0.08,0.016,0.024))

# The real dataset may have NAs in it
df <- data.frame(Subj, Analyte, Dose, AUC, Cmax)

myStats <- function(x) {
    count <- function(x) length(na.omit(x))
    pCV <- function(x) sd(x,na.rm=TRUE) / mean(x,na.rm=TRUE) * 100
    c(
      n = count(x),
      mean = mean(x,na.rm=TRUE),
      SD = sd(x,na.rm=TRUE),
      CV = pCV(x),
      median = median(x,na.rm=TRUE),
      min = min(x,na.rm=TRUE),
      max = max(x,na.rm=TRUE)
      )
}

library(reshape)
# Melting: one measured value per record + several identifiers
dfm <- melt.data.frame(df, measure.var=c("AUC","Cmax"),
                            id.var=c("Analyte","Subj","Dose"),
                            variable_name="Variable")
# Casting: Array of data.frames with statistics
res <- cast(dfm, result_variable ~ Variable + Dose | Analyte, myStats)
# Write to disk
for (i in seq(1,dim(res))) {
    if (i == 1)
        write.table(res[i], file="stest.xls", sep="\t", row.names=FALSE,
col.names=TRUE, qmethod="double")
    else
        write.table(res[i], file="stest.xls", sep="\t", row.names=FALSE,
col.names=TRUE, qmethod="double", append=TRUE)
}

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rene Braeckman
Sent: Thursday, February 15, 2007 1:59 PM
To: 'hadley wickham'
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to re-arrange data in table?

The last example in your last message comes really close to the re-arranged
table listed in my original message.

Re-arranged table listed in my original message:

           1             2             3          
Analyte    "RBV"         "RBV"         "RBV"        
Dose       "200"         "400"         "600"        
AUC.n      "4"           "4"           "4"          
AUC.mean   " 44.023714"  " 77.853594"  "113.326952"  
...
Analyte    "TBV"         "TBV"         "TBV"        
Dose       "200"         "400"         "600"        
AUC.n      "4"           "4"           "4"          
AUC.mean   "  4.657904"  "  8.140416"  " 12.034377" 
...

The final step was to write this table to disk as follows:

write.table(myTable, file="stest.xls", sep="\t", row.names=T, col.names=F,
qmethod="double")

Since

cast(dfm, result_variable ~ Dose | Analyte, myStats)

it not a table, I can't use this. Is there another way to write the data to
disk after the cast and get the same results in the file? Or convert the
array into a table?

Thanks again for all your help,
Rene
Irvine, California, USA


From f.harrell at vanderbilt.edu  Fri Feb 16 01:04:03 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 15 Feb 2007 18:04:03 -0600
Subject: [R] Function to assign quantiles?
In-Reply-To: <1171581307.4785.85.camel@localhost.localdomain>
References: <BAY132-F3226C9E1F129DFD6343A54AA960@phx.gbl>
	<1171581307.4785.85.camel@localhost.localdomain>
Message-ID: <45D4F4F3.2080904@vanderbilt.edu>

Marc Schwartz wrote:
> On Thu, 2007-02-15 at 17:50 -0500, Talbot Katz wrote:
>> Hi.
>>
>> If I call the quantiles function as follows:
>>
>> qvec = quantiles(dvals,probs=seq(0,1,0.1))
>>
>> the results will return a vector something like the following example:
>>
>>       0%    10%    20%    30%     40%     50%    60%    70%      80%     90% 
>>      100%
>>     56.0   137.3   238.4   317.9   495.8   568.5   807.4  1207.7  1713.0  
>> 2951.1  8703.0
>>
>> Now I want to assign the deciles, 1 - 10, to each observation, so, in the 
>> above example, if dvals[322] = 256, I want to assign qvals[322] = 3, and if 
>> dvals[7216] = 1083, I want qvals[7216] = 7, etc.  I would think there would 
>> be a function, or some very quick code to do that, but I couldn't find it.
>>
>> Here's what I have now.  It works, but I figure there must be a better way:
>>
>> asdc <- function(q){max(1,which(qvec<q))}
>> qvals = apply(matrix(dvals,nrow=1),2,asdc)
>>
>>
>> Any suggestions?  Thanks!
> 
> Take a look at ?cut and use the output of quantile() to define the
> 'breaks' argument:
> 
> x <- rnorm(100)
> 
>> cut(x, breaks = quantile(x, probs = seq(0, 1, 0.1)), 
>       include.lowest = TRUE, labels = 1:10)
>   [1] 2  1  9  10 9  3  1  7  5  2  4  1  10 8  4  10 9  9  5  3  5  8 
>  [23] 6  9  7  1  10 1  10 9  3  3  9  3  5  5  6  4  8  10 6  2  2  8 
>  [45] 6  3  6  9  1  4  7  10 8  7  5  3  1  10 2  1  7  8  8  2  8  8 
>  [67] 3  10 4  6  5  1  4  6  4  4  5  9  9  7  2  8  2  5  1  3  7  6 
>  [89] 4  6  4  6  2  7  2  10 7  3  5  7 
> Levels: 1 2 3 4 5 6 7 8 9 10
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Or see the cut2 function in the Hmisc package


From rolf at math.unb.ca  Fri Feb 16 01:11:11 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Thu, 15 Feb 2007 20:11:11 -0400 (AST)
Subject: [R] integrate over polygon
Message-ID: <200702160011.l1G0BBGQ009840@weisner.math.unb.ca>

If you can integrate over a trapezoid (with horizontal base and
vertical sides) then it's fairly easy to integrate over any (non-self
intersecting) polygon.  ``Draw'' a line which is below the polygon.
For each segment in the polygon, integrate over trapezoid whose base
is on the aforesaid line and whose top edge is the aforesaid edge of
the polygon.  If the edge in question goes from left to right,
multiply by -1.  (Leave alone if the edge goes from right to left;
vertical edges contribute 0.) Add up the results.

This amounts to applying Green's Theorem in the plane.

I don't know of any code that does this, but it shouldn't be
hard to write.  (*Given* that you can integrate over the
trapezoids.)

Another approach:

        library(spatstat)
        W <- owin(poly=X) # X is your polygon, represented as a list
                          # with components ``x'' and ``y'' giving
                          # the vertices of the polygon in *anti*
                          # clockwise order.  The first vertex should
                          # NOT be repeated at the end.
        IM <- as.im(foo,W)
        int <- summary(IM)$integral # Based on a 100 x 100 pixellation
                                    # of the bounding box of the polygon.
        oop <- spatstat.options(npixel=500)
        IM <- as.im(foo,W)
        int <- summary(IM)$integral # Based on a 500 x 500 pixellation
                                    # of the bounding box of the polygon.
        spatstat.options(oop)       # Set npixel back to the default.

HTH.

                                cheers,

                                        Rolf Turner
                                        rolf at math.unb.ca


From Charles.Annis at StatisticalEngineering.com  Fri Feb 16 01:16:29 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 15 Feb 2007 19:16:29 -0500
Subject: [R] R book advice
In-Reply-To: <50d6c72a0702151535p5ee6da4bq404176d3290cc0f2@mail.gmail.com>
References: <50d6c72a0702151535p5ee6da4bq404176d3290cc0f2@mail.gmail.com>
Message-ID: <018c01c7515f$b49bf090$6400a8c0@DD4XFW31>

I know you asked for a comparison (which I can't provide) but on an absolute
scale for a beginner it'd be hard to beat Dalgaard ("Introductory Statistics
with R").  It assumes nothing, and teaches you statistics in a lucid
no-nonsense way AND teaches you R along the way as a mechanism for
implementing the statistical thinking you've acquired.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Lynch
Sent: Thursday, February 15, 2007 6:35 PM
To: r-help at stat.math.ethz.ch
Subject: [R] R book advice

I'm looking for a book for someone completely ignorant of statistics
who wishes to learn both statistics and R.  I've found three
possibilities, one by Verzani ("Using R for Introductory Statistics"),
one by Crawley ("Statistics: An Introduction using R"), and one by
Dalgaard ("Introductory Statistics with R").  Do these books have
different emphases, perspectives, or strengths?  Should I just pick
one at random and buy it?

Thanks,
        --Paul

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From anthony.staines at gmail.com  Fri Feb 16 01:39:33 2007
From: anthony.staines at gmail.com (Anthony Staines)
Date: Fri, 16 Feb 2007 00:39:33 +0000
Subject: [R] SPSS and library(foreign)
Message-ID: <6d3975af0702151639w63b7d24fha6394ed2fc5879a3@mail.gmail.com>

Hi,
I have a valid SPSS .sav file (which I can open happily in SPSS v11 on
Windows XP).

Opening it in R2.41 on Linux we get this message :-

> HSE3023 <- read.spss("HSE.sav")
Error in read.spss("HSE.sav") :
 	error reading system-file header
In addition: Warning message: HSE.sav:
Variable X234 indicates variable label of invalid length 256

Now variable X234 has indeed a silly label, which we can easily fix,
but I'm more worried about the  'error reading system-file header'.
Any ideas - are there multiple versions of the SPSS format, are there
OS issues?  I can't find anything helpful about the format either on
the SPSS site, or on the PSPP site. We have earlier versions of SPSS
around, so we re-install one and we could re-write it if that would
help.

Anthony Staines
-- 
-- 
Dr. Anthony Staines, Senior Lecturer in Epidemiology.
***Note New Address *** Note New Address ****
School  of Public Health and Population Sciences, Woodview House,
UCD, Belfield, Dublin 4, Ireland.
Tel:- +353 1 716 7345. Fax:- +353 1 716 7407 Mobile:- +353 86 606 9713
Web:- http://phm.ucd.ie


From Inman.Brant at mayo.edu  Fri Feb 16 01:44:13 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Thu, 15 Feb 2007 18:44:13 -0600
Subject: [R] Sampling distribution of the range statistic
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664B16@msgebe23.mfad.mfroot.org>


R-helpers:

In the construction of control charts for statistical quality control
objectives, one might choose to estimate the control limits for the mean
using the mean range of the samples.  This requires multiplying the mean
range by a correction factor, often called "d2", that is tabulated in
many books. The origin of "d2" seems to be table 2 of the following
paper:

Harter, HL.  Ann Math Stat (1960) 31: 1122.

In Table 2 of this paper, the mean of the sampling distribution of the
range statistic is calculated for various sample sizes.  Does anyone
know whether R has a function that can reproduce the sampling
distribution of the range represented in the Harter paper so that I
don't have to look at tables in old books?

Brant


From marc_schwartz at comcast.net  Fri Feb 16 01:58:31 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 15 Feb 2007 18:58:31 -0600
Subject: [R] Sampling distribution of the range statistic
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664B16@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664B16@msgebe23.mfad.mfroot.org>
Message-ID: <1171587511.4938.4.camel@localhost.localdomain>

On Thu, 2007-02-15 at 18:44 -0600, Inman, Brant A. M.D. wrote:
> R-helpers:
> 
> In the construction of control charts for statistical quality control
> objectives, one might choose to estimate the control limits for the mean
> using the mean range of the samples.  This requires multiplying the mean
> range by a correction factor, often called "d2", that is tabulated in
> many books. The origin of "d2" seems to be table 2 of the following
> paper:
> 
> Harter, HL.  Ann Math Stat (1960) 31: 1122.
> 
> In Table 2 of this paper, the mean of the sampling distribution of the
> range statistic is calculated for various sample sizes.  Does anyone
> know whether R has a function that can reproduce the sampling
> distribution of the range represented in the Harter paper so that I
> don't have to look at tables in old books?
> 
> Brant

Brant,

You might want to look at the 'qcc' package on CRAN.  Brief info here:

  http://cran.us.r-project.org/src/contrib/Descriptions/qcc.html

Also, take a look at the Qtoolbox at:

  http://www.cmis.csiro.au/S-PLUS/qtoolbox/R.html

HTH,

Marc Schwartz


From murdoch at stats.uwo.ca  Fri Feb 16 02:45:06 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 15 Feb 2007 20:45:06 -0500
Subject: [R] integrate over polygon
In-Reply-To: <E9C86560-5BA2-4DF7-875A-DF462CE31AA3@ucla.edu>
References: <E9C86560-5BA2-4DF7-875A-DF462CE31AA3@ucla.edu>
Message-ID: <45D50CA2.3030708@stats.uwo.ca>

On 2/14/2007 9:06 PM, Haiyong Xu wrote:
> Hi there,
> 
> I want to integrate a function over an irregular polygon. Is there  
> any function which can implement this easily? Otherwise, I am  
> thinking of divide the polygon into very small rectangles and use  
> "adapt" to approximate it. Do you have any suggestions to get the  
> fine division? Any advice is appreciated.

If you can integrate over a triangle, you will soon be able to use 
gpclib to triangulate the polygon.  (The code was already there in C; I 
wrote an interface to it and Roger Peng has packaged it up to send to 
CRAN.)  Wait for version 1.4 or greater to appear before downloading.

Duncan Murdoch


From mnair at iusb.edu  Fri Feb 16 04:15:11 2007
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Thu, 15 Feb 2007 22:15:11 -0500
Subject: [R] slice usage
Message-ID: <A32055BDEA88C34BB3DBBCD229380778051056@iu-mssg-mbx109.ads.iu.edu>


I am trying to understand the usage of slice while plotting. Can any one provide me with some explanation to it or point me to a resource where I can read it in greater detail. I have an example from the usage page for SVM.

library(MASS)
library(e1071)
data(iris)
m2 <- svm(Species~., data = iris) 
plot(m2,iris, Petal.Width~Petal.Length)
readline(prompt = "Pause. Press <Enter> to continue...")
plot(m2, iris, Petal.Width ~ Petal.Length, slice = list(Sepal.Width=3, Sepal.Length=4 ))

How do I decide Sepal.Width should be set to 3 and Sepal Length set to 4?

Thanks../Murli


From jporzak at gmail.com  Fri Feb 16 06:46:04 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Thu, 15 Feb 2007 21:46:04 -0800
Subject: [R] R book advice
In-Reply-To: <50d6c72a0702151535p5ee6da4bq404176d3290cc0f2@mail.gmail.com>
References: <50d6c72a0702151535p5ee6da4bq404176d3290cc0f2@mail.gmail.com>
Message-ID: <2a9c000c0702152146v7226028wccda7e341ae9b1ee@mail.gmail.com>

Hi Paul,

All three are excellent choices, so you won't go wrong with random
choice. Here is your first R lesson:

RBooks <- c("Verzani", "Crawley", "Dalgaard")
sample(RBooks, 1)

Seriously, I  expect you will end up with all three. Here are my
mini-reviews (in order of publication)

Peter Dalgaard's book came out just before I first discovered R in the
winter of 2002. It was my intro to R and a good stats refresher.
Charles' assessment correct. At only ~250 pages, it is not at all
intimidating, however Peter does build up to some intermediate topics
like logistic regression and survival analysis. My copy is now
somewhat tattered & I should get a replacement!

John Verzani had, and still has, a preliminary version of his book on
CRAN: http://cran.cnr.berkeley.edu/doc/contrib/Verzani-SimpleR.pdf so
I was very excited when it come out in hard copy - much expanded - as
"Using R." He has more visualization examples - which I like. I do
wish John would have used "<-" instead of "=" for assignment. It's
important to start "thinking in R" - "=" drags me back to my FORTRAN
days.

Being a mid-western American, I love Michael Crawley's British view of
the world! He really forces you to get an intuitive feel for what is
going on. Also good visualization emphasis. My only criticism is he
suggests using Word to save your work. You should really use a more
serious text editor/environment. I generally use JGR today, having
moved from RWinEdt and TextPad. The Linux folks love ESS, but that is
how they were brought up.

On 2/15/07, Paul Lynch <plynchnlm at gmail.com> wrote:
> I'm looking for a book for someone completely ignorant of statistics
> who wishes to learn both statistics and R.  I've found three
> possibilities, one by Verzani ("Using R for Introductory Statistics"),
> one by Crawley ("Statistics: An Introduction using R"), and one by
> Dalgaard ("Introductory Statistics with R").  Do these books have
> different emphases, perspectives, or strengths?  Should I just pick
> one at random and buy it?
>
> Thanks,
>         --Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
HTH,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA
http://www.linkedin.com/in/jimporzak


From h.wickham at gmail.com  Fri Feb 16 07:51:49 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 16 Feb 2007 19:51:49 +1300
Subject: [R] Function to assign quantiles?
In-Reply-To: <45D4F4F3.2080904@vanderbilt.edu>
References: <BAY132-F3226C9E1F129DFD6343A54AA960@phx.gbl>
	<1171581307.4785.85.camel@localhost.localdomain>
	<45D4F4F3.2080904@vanderbilt.edu>
Message-ID: <f8e6ff050702152251r2e13dc61m43eacbb5620e2d8b@mail.gmail.com>

On 2/16/07, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> Marc Schwartz wrote:
> > On Thu, 2007-02-15 at 17:50 -0500, Talbot Katz wrote:
> >> Hi.
> >>
> >> If I call the quantiles function as follows:
> >>
> >> qvec = quantiles(dvals,probs=seq(0,1,0.1))
> >>
> >> the results will return a vector something like the following example:
> >>
> >>       0%    10%    20%    30%     40%     50%    60%    70%      80%     90%
> >>      100%
> >>     56.0   137.3   238.4   317.9   495.8   568.5   807.4  1207.7  1713.0
> >> 2951.1  8703.0
> >>
> >> Now I want to assign the deciles, 1 - 10, to each observation, so, in the
> >> above example, if dvals[322] = 256, I want to assign qvals[322] = 3, and if
> >> dvals[7216] = 1083, I want qvals[7216] = 7, etc.  I would think there would
> >> be a function, or some very quick code to do that, but I couldn't find it.
> >>
> >> Here's what I have now.  It works, but I figure there must be a better way:
> >>
> >> asdc <- function(q){max(1,which(qvec<q))}
> >> qvals = apply(matrix(dvals,nrow=1),2,asdc)
> >>
> >>
> >> Any suggestions?  Thanks!
> >
> > Take a look at ?cut and use the output of quantile() to define the
> > 'breaks' argument:
> >
> > x <- rnorm(100)
> >
> >> cut(x, breaks = quantile(x, probs = seq(0, 1, 0.1)),
> >       include.lowest = TRUE, labels = 1:10)
> >   [1] 2  1  9  10 9  3  1  7  5  2  4  1  10 8  4  10 9  9  5  3  5  8
> >  [23] 6  9  7  1  10 1  10 9  3  3  9  3  5  5  6  4  8  10 6  2  2  8
> >  [45] 6  3  6  9  1  4  7  10 8  7  5  3  1  10 2  1  7  8  8  2  8  8
> >  [67] 3  10 4  6  5  1  4  6  4  4  5  9  9  7  2  8  2  5  1  3  7  6
> >  [89] 4  6  4  6  2  7  2  10 7  3  5  7
> > Levels: 1 2 3 4 5 6 7 8 9 10
> >
> >
> > HTH,
> >
> > Marc Schwartz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> Or see the cut2 function in the Hmisc package

Or the chop function in ggplot package - it's a common problem.

Hadley


From petr.pikal at precheza.cz  Fri Feb 16 08:19:30 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 16 Feb 2007 08:19:30 +0100
Subject: [R] convert to binary to decimal
In-Reply-To: <47c7c59e0702150913l1be48edbmd480e5f4e83e9452@mail.gmail.com>
References: <002301c75121$ca6425c0$4d908980@gne.windows.gene.com>
Message-ID: <45D56912.7771.2FEB94@localhost>

Hi

slight modification of your function can be probably even quicker:

fff<-function(x) sum(2^(which(rev(x))-1))
:-)
Petr

On 15 Feb 2007 at 12:13, Roland Rau wrote:

Date sent:      	Thu, 15 Feb 2007 12:13:32 -0500
From:           	"Roland Rau" <roland.rproject at gmail.com>
To:             	"Bert Gunter" <gunter.berton at gene.com>
Copies to:      	marc_schwartz at comcast.net, r-help at stat.math.ethz.ch
Subject:        	Re: [R] convert to binary to decimal

> Hi Bert,
> 
> First, I was very happy with my solution, but you win (see below)!
> 
> Best,
> Roland
> 
> 
> > bert.gunter <- function(x) {
> +   sum(x * 2^(rev(seq(along=x)) - 1))
> + }
> >
> > marc.schwartz <- function(x) {
> +   x <- as.character(as.numeric(x))
> +   b <- as.numeric(unlist(strsplit(x, "")))
> +   pow <- 2 ^ ((length(b) - 1):0)
> +   sum(pow[b == 1])
> + }
> >
> > length(huge.list)
> [1] 20000
> > head(huge.list, n=1)
> [[1]]
>  [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE
>   TRUE
> FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE
> [21] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
>  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE [41] FALSE FALSE
> FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE
> 
> >
> > system.time(lapply(X=huge.list, FUN=bin2dec.easy))
> [1] 2.33 0.00 2.32   NA   NA
> > system.time(lapply(X=huge.list, FUN=bin2dec.recursive))
> [1] 14.91  0.00 14.90    NA    NA
> > system.time(lapply(X=huge.list, FUN=marc.schwartz))
> [1] 5.31 0.00 5.31   NA   NA
> > system.time(lapply(X=huge.list, FUN=bert.gunter))
> [1] 1.33 0.00 1.33   NA   NA
> >
> 
> 
> 
> On 2/15/07, Bert Gunter <gunter.berton at gene.com> wrote:
> >
> > why not simply:
> >
> > sum(x * 2^(rev(seq_along(x)) - 1))   ?
> >
> >
> > Bert Gunter
> > Genentech Nonclinical Statistics
> > South San Francisco, CA 94404
> > 650-467-7374
> >
> >
> > Bert Gunter
> > Nonclinical Statistics
> > 7-7374
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roland Rau
> > Sent: Thursday, February 15, 2007 8:22 AM To:
> > marc_schwartz at comcast.net Cc: r-help at stat.math.ethz.ch Subject: Re:
> > [R] convert to binary to decimal
> >
> > That was a nice quick distraction. Unfortunately, I am not the first
> > to answer. :-( Anyway, I offer two solutions (which are different
> > from the one of Marc Schwartz); I wrote it quickly but I hope they
> > are correct.
> >
> > Enjoy and thanks,
> > Roland
> >
> > a <- c(TRUE, FALSE, TRUE)
> > b <- c(TRUE, FALSE, TRUE, TRUE)
> >
> > bin2dec.easy <- function(binaryvector) {
> >   sum(2^(which(rev(binaryvector)==TRUE)-1))
> > }
> >
> > bin2dec.recursive <- function(binaryvector) {
> >   reversed.input <- rev(binaryvector)
> >   binaryhelper(reversed.input, 0, 0)
> > }
> >
> > binaryhelper <- function(binvector, currentpower, currentresult) {
> >   if (length(binvector)<1) {
> >     currentresult
> >   } else {
> >     if (binvector[1]) {
> >       binaryhelper(binvector[-1], currentpower+1,
> > currentresult+2^currentpower)
> >     } else {
> >       binaryhelper(binvector[-1], currentpower+1, currentresult)
> >     }
> >   }
> > }
> >
> >
> > bin2dec.easy(a)
> > bin2dec.recursive(a)
> > bin2dec.easy(b)
> > bin2dec.recursive(b)
> >
> >
> >
> >
> >
> > On 2/15/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > >
> > > On Thu, 2007-02-15 at 16:38 +0100, Martin Feldkircher wrote:
> > > > Hello,
> > > > we need to convert a logical vector to a (decimal) integer.
> > > > Example:
> > > >
> > > > a=c(TRUE, FALSE, TRUE) (binary number 101)
> > > >
> > > > the function we are looking for should return
> > > >
> > > > dec2bin(a)=5
> > > >
> > > > Is there a package for such a function or is it even implemented
> > > > in
> > the
> > > > base package? We found the hexmode and octmode command, but not
> > > > a binmode. We know how to program it ourselves however we are
> > > > looking
> > for
> > > > a computationally efficient algorithm.
> > > >
> > > > Martin and Stefan
> > >
> > > This is a modification of a function that I had posted a while
> > > back, so that it handles 'x' as a logical vector. I added the
> > > first line in the function to convert the logical vector to it's
> > > numeric equivalent and then coerce to character:
> > >
> > > bin2dec <- function(x)
> > > {
> > >   x <- as.character(as.numeric(x))
> > >   b <- as.numeric(unlist(strsplit(x, "")))
> > >   pow <- 2 ^ ((length(b) - 1):0)
> > >   sum(pow[b == 1])
> > > }
> > >
> > >
> > > a <- c(TRUE, FALSE, TRUE)
> > >
> > > > bin2dec(a)
> > > [1] 5
> > >
> > > HTH,
> > >
> > > Marc Schwartz
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Fri Feb 16 09:55:45 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Feb 2007 08:55:45 +0000 (GMT)
Subject: [R] SPSS and library(foreign)
In-Reply-To: <6d3975af0702151639w63b7d24fha6394ed2fc5879a3@mail.gmail.com>
References: <6d3975af0702151639w63b7d24fha6394ed2fc5879a3@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702160836300.2662@auk.stats>

On Fri, 16 Feb 2007, Anthony Staines wrote:

> Hi,
> I have a valid SPSS .sav file (which I can open happily in SPSS v11 on
> Windows XP).
>
> Opening it in R2.41 on Linux we get this message :-
>
>> HSE3023 <- read.spss("HSE.sav")
> Error in read.spss("HSE.sav") :
> 	error reading system-file header
> In addition: Warning message: HSE.sav:
> Variable X234 indicates variable label of invalid length 256
>
> Now variable X234 has indeed a silly label, which we can easily fix,
> but I'm more worried about the  'error reading system-file header'.

The warning is the reason for the error.

> Any ideas - are there multiple versions of the SPSS format, are there

There are multiple versions.  It is a proprietary format, and AFAIK has 
been reverse-engineered for PSPP in the late 1980s (in the version used in 
package foreign).

PSPP has been changed since its code was forked for package foreign, but 
this limit has not.  It is likely this is a limitation of PSPP and not the 
format.

> OS issues?  I can't find anything helpful about the format either on
> the SPSS site, or on the PSPP site. We have earlier versions of SPSS
> around, so we re-install one and we could re-write it if that would
> help.
>
> Anthony Staines
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rdiaz02 at gmail.com  Fri Feb 16 10:10:05 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Fri, 16 Feb 2007 10:10:05 +0100
Subject: [R] R book advice
In-Reply-To: <50d6c72a0702151535p5ee6da4bq404176d3290cc0f2@mail.gmail.com>
References: <50d6c72a0702151535p5ee6da4bq404176d3290cc0f2@mail.gmail.com>
Message-ID: <624934630702160110v728efc47m94fcb3228085ef6a@mail.gmail.com>

Dear Paul,

You might want to add Everitt & Hothorn's  "A Handbook of Statistical
Analyses Using R". If I had to recommend just one book it'd be this
one.

My own (i.e., highly subjective) suggestion, if you can afford two
books, would be to first go through Dalgaard's and then through
Everitt & Hothorn's.

I do not have a direct experience with Verzani's, but I've heard great
things about it. I think a pdf of a preliminary version is available
from the R page. Regarding Crawley's ... well, I find some/many of his
comments and suggestions unorthodox (my experience is with his
"Statistical Computing: An Introduction to Data Analysis using
S-Plus", a book I would not recommend to a novice).

HTH,

R.



On 2/16/07, Paul Lynch <plynchnlm at gmail.com> wrote:
> I'm looking for a book for someone completely ignorant of statistics
> who wishes to learn both statistics and R.  I've found three
> possibilities, one by Verzani ("Using R for Introductory Statistics"),
> one by Crawley ("Statistics: An Introduction using R"), and one by
> Dalgaard ("Introductory Statistics with R").  Do these books have
> different emphases, perspectives, or strengths?  Should I just pick
> one at random and buy it?
>
> Thanks,
>         --Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From niederlein-rstat at yahoo.de  Fri Feb 16 10:13:00 2007
From: niederlein-rstat at yahoo.de (niederlein-rstat at yahoo.de)
Date: Fri, 16 Feb 2007 10:13:00 +0100 (CET)
Subject: [R] tapply, levelinformation
Message-ID: <20070216091300.80106.qmail@web27105.mail.ukl.yahoo.com>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/fde2c645/attachment.pl 

From niederlein-rstat at yahoo.de  Fri Feb 16 10:20:55 2007
From: niederlein-rstat at yahoo.de (niederlein-rstat at yahoo.de)
Date: Fri, 16 Feb 2007 10:20:55 +0100 (CET)
Subject: [R] sapply again return value
Message-ID: <20070216092055.42200.qmail@web27101.mail.ukl.yahoo.com>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/a2dbaa8e/attachment.pl 

From gunther.hoening at ukmainz.de  Fri Feb 16 10:29:27 2007
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Fri, 16 Feb 2007 10:29:27 +0100
Subject: [R] 3 dimensional histogram
In-Reply-To: <624934630702160110v728efc47m94fcb3228085ef6a@mail.gmail.com>
References: <50d6c72a0702151535p5ee6da4bq404176d3290cc0f2@mail.gmail.com>
	<624934630702160110v728efc47m94fcb3228085ef6a@mail.gmail.com>
Message-ID: <000001c751ac$f4527d60$0f1e0b0a@3med.klinik.unimainz.de>

Dear list,

I want to plot a "3 dimensional histogram" and  I am looking for a funktion
to do so.
I have a dataframe with the following information:

	<1	<0.5	<0	<-0.5	<-1
"a"	13	11	6	2	1	
"b"	9	8	6	5	3	
"c"	5	4	3	2	1
"d"	8	4	3	1	0

The first column is the name, the other columns are the amounts under a
certain threshold. 
First row are the thresholds.

Can anybody help me ?

Gunther


From niederlein-rstat at yahoo.de  Fri Feb 16 10:53:15 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Fri, 16 Feb 2007 10:53:15 +0100
Subject: [R] tapply, levelinformation
In-Reply-To: <644e1f320702150950n5d5f358fg8d7015d01998c265@mail.gmail.com>
References: <45D47BF3.2050006@yahoo.de>
	<644e1f320702150950n5d5f358fg8d7015d01998c265@mail.gmail.com>
Message-ID: <45D57F0B.2030207@yahoo.de>

Hi Jim,

jim holtman schrieb:
> Here is one way:
>  
> t <- split(mat, classes)
> for (i in names(t)) plotdensity(t[[i]], main=i)
> 

But then I don't use the advantages of the tapply anymore...

> What is the problem you are trying to solve?

I have a set of data (multiple files), which belong to different
conditions (one or more files per condition). I wanted to read the data
set and a "description" of the conditions and then automatically create
plots for data of the same condition.

Maybe it's much to complicate the way I do...

Antje


From niederlein-rstat at yahoo.de  Fri Feb 16 10:54:19 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Fri, 16 Feb 2007 10:54:19 +0100
Subject: [R] sapply again return value
Message-ID: <45D57F4B.7040304@yahoo.de>

Hello,

I used an sapply to get some data back (s <- sapply(...) ). The output 
of s would then deliver something like this:

      B06_lamp.csv C06_lamp.csv D06_lamp.csv
[1,] NULL         NULL         Numeric,512
[2,] NULL         NULL         Numeric,512
[3,] NULL         NULL         2
 > mode(s)
[1] "list"
 > dim(s)
[1] 3 3
 >

Now, I'd like to remove the columns which contain NULL (it's alway the 
whole column).
How can I do this???

Antje


From bartjoosen at hotmail.com  Fri Feb 16 11:09:03 2007
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Fri, 16 Feb 2007 10:09:03 +0000
Subject: [R] Time of failure, Arrhenius and Weibull distribution
Message-ID: <BAY134-F23114696D52406BFCFF879D8950@phx.gbl>

My model is as follows:

mod <- lm(log(Degrad/Time) ~ I(1e+05/(8.617*(Temp + 273.16))), data)

Where Degrad/Time = k, and because of taking logs, the intercept = log(A)
and the coefficient of the term = -Ea.

This is how I used the Arrhenius formule in a linear model.
I know I can use predict to estimate my failure times, with confidence 
intervals, but I'm not sure wether I should use least square regression 
(assuming a normal distribution) or a MLE with a Weibull distrbution.
And if I should use MLE with Weibull, I dont know how to implement it in R.


Thanks for your reply

Bart




Ted Harding
Thu, 15 Feb 2007 14:18:16 -0800
On 15-Feb-07 Bart Joosen wrote:
>Hi,
>
>I'm currently doing some analyses on time of failure of a product.
>I found on the internet some article about the Arrhenius equation,
>and I can calculate the results with R.
>Equation:
>k=A*exp(-Ea/R*T)
>
>I can fit a model with lm for this purpose, so far no problem.
>
>But for the confidence interval, how can I use the Weibull
>distribution?
>Or should I use the weibull distribution at all? Or can I safely use
>the predict.lm method to predict my time of failure at a certain
>temperature?
>Is there anyone who has experience with this kind of calculations?

Can you clarify your query?

Since you are using lm, would you write what your model is,
in terms like

  lm( Y ~ X1 + X2 + ... )

explaining how Y relates to failure time, and what sort of
variables X1, X2, ... are, and how Arrhenius (normally used
for evaluating the rate coefficient k of a chemical reaction,
where the only quantity that might be subject to statistical
estimation is the coefficient A) comes into it?

The standard output of summary(lm(...)) will give you the
estimates, and their standard errors, of the coefficients
of X1, X2, ... in the linear model, from which confidence
intervals for these coefficients can readily be computed.

You can certainly use predict.lm to produce

a) A confidence interval for the estimated expected ("predicted")
   value of Y, taking account of the uncertainties in the
   estimates of a1, a2, ... in Y = a1*X1 + a2*X2 + ...

b) A "prediction interval" (also called "tolerance interval")
   for the value of Y that would be observed at given values
   of X1, X2, ... , taking account both the uncertainties
   in the estimated coefficients (i.e. in the expected value
   of Y) and the estimated random scatter of Y about its
   expected value.

How these would relate to the failure time (tF say) depends
on the relationship between Y and tF.

Your query does not describe how any Weibull considerations
come into your linear model!

If you can spell it all out for us in sufficient detail, I'm
sure people will be able to contribute suggestions and explanations.

Best wishes


From feanor0 at hotmail.com  Fri Feb 16 11:14:30 2007
From: feanor0 at hotmail.com (Murali Menon)
Date: Fri, 16 Feb 2007 10:14:30 +0000
Subject: [R] array searches
Message-ID: <BAY113-F191B663BED1069C6C1EDE9EE950@phx.gbl>

Folks,

I have a dataframe comprising a column of dates and a column of signals (-1, 
0, 1) that looks something like this:

30/01/2007	0
31/01/2007	-1
01/02/2007	-1
02/02/2007	-1
03/02/2007	1
04/02/2007	1
05/02/2007	1
06/02/2007	1
07/02/2007	1
08/02/2007	1
09/02/2007	0
10/02/2007	0
11/02/2007	0
12/02/2007	1
13/02/2007	1
14/02/2007	1
15/02/2007	0
16/02/2007	0

What I need to do is for each signal *in reverse chronological order* to 
find the date that it first appeared. So, for the zero on 16/02/2007 and 
15/02/2007, the 'inception' date would be 15/02/2007, because the day 
before, the signal was 1. Likewise, the 'inception' date for the signal 1 on 
08/02/2007 and the five days prior, would be 03/02/2007. I need to create a 
structure of inception dates that would finally look as follows:

-1	31/01/2007
-1	31/01/2007
-1	31/01/2007
1	03/02/2007
1	03/02/2007
1	03/02/2007
1	03/02/2007
1	03/02/2007
1	03/02/2007
0	09/02/2007
0	09/02/2007
0	09/02/2007
1	12/02/2007
1	12/02/2007
1	12/02/2007
0	15/02/2007
0	15/02/2007

Is there a clever way of doing this? My sadly C-oriented upbringing can only 
think in terms of for-loops.

Thanks!

Murali

_________________________________________________________________
The average US Credit Score is 675. The cost to see yours: $0 by Experian.


From jlampur at eagrof.UdL.es  Fri Feb 16 11:37:40 2007
From: jlampur at eagrof.UdL.es (Jorge Lampurlanes Castel)
Date: Fri, 16 Feb 2007 11:37:40 +0100 (CET)
Subject: [R] Multiple comparisons when interacction
In-Reply-To: <20070208093611.BTX71495@po-d.temple.edu>
References: <20070208093611.BTX71495@po-d.temple.edu>
Message-ID: <1597.172.16.103.148.1171622260.squirrel@correu.udl.es>

Hello,

Data comes from a multiyear field experiment in which 4 levels of a
treatment (2, 3, 4, 6) are compared to see the effect on yield. It is a
randomized complete block design.

The SAS code follows:

options ls=95;
data uno;
        infile 'data.csv' delimiter=';' firstobs=2;
        input year plot block treat yield;
run;

proc mixed data=uno;
     class treat year block;
     model yield=block year treat treat*year;
     lsmeans year treat  /pdiff;
     lsmeans treat*year /slice=year pdiff;
     ods output diffs=dos;
run;

data tres;
      set dos;
      if year=_year;
proc print data=tres;
      var year _year treat _treat estimate stderr df tvalue probt;
run;

Data are attached as a file: data.csv.

In fact, I do not know if this is the best approach to analyze the data:

- Should block be considered as random? We use the same file and
randomization every year. We are interested in the long term effect of the
treatment.
- Data should be considered as repeated measurements over time (years)?

In multcomp package:

- What is the equivalence between the tests proposed  ("Sequen", "AVE",
"Changepoint", "Williams", "Marcus", "McDermott") and the tests agronomist
are used to do: LSD (least significant difference), Duncan multiple range
test, Scheffe, S-N-K (Student-Newman-Keuls)?


Thanks a lot for your interest.

Jorge Lampurlan?s
Agronomist


>> Is it possible to do this analysis in R?
>
> Yes, it is possible.  The syntax isn't in place yet.
>
> If you send me the complete SAS code and data for an example using slice,
> I will duplicate it for you in the multcomp package in R.  I will send
> that
> to the R-help list and to Torsten and it will bring us one step closer
> to the syntax.
>
> The example I showed before was designed to get the same answer as S-Plus
> multicomp using the adjust= argument.
>
> Rich
>


From guillermojsanmartin at googlemail.com  Fri Feb 16 11:39:36 2007
From: guillermojsanmartin at googlemail.com (=?ISO-8859-1?Q?Guillermo_Juli=E1n_San_Mart=EDn?=)
Date: Fri, 16 Feb 2007 11:39:36 +0100
Subject: [R] Sending greetings and a question !!!
Message-ID: <48a5ea80702160239yc282a2bt38ac53a0c6badfe9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/98b5cba9/attachment.pl 

From jlampur at eagrof.UdL.es  Fri Feb 16 11:39:42 2007
From: jlampur at eagrof.UdL.es (Jorge Lampurlanes Castel)
Date: Fri, 16 Feb 2007 11:39:42 +0100 (CET)
Subject: [R] Multiple comparisons when interacction
In-Reply-To: <20070208093611.BTX71495@po-d.temple.edu>
References: <20070208093611.BTX71495@po-d.temple.edu>
Message-ID: <1615.172.16.103.148.1171622382.squirrel@correu.udl.es>

Data is here.

I'm sorry.

>> Is it possible to do this analysis in R?
>
> Yes, it is possible.  The syntax isn't in place yet.
>
> If you send me the complete SAS code and data for an example using slice,
> I will duplicate it for you in the multcomp package in R.  I will send
> that
> to the R-help list and to Torsten and it will bring us one step closer
> to the syntax.
>
> The example I showed before was designed to get the same answer as S-Plus
> multicomp using the adjust= argument.
>
> Rich
>


-- 
**************************************************
Jorge Lampurlan?s Castel
Departament d'Enginyeria Agroforestal
Escola T?cnica Superior d'Enginyeria Agr?ria
Universitat de Lleida
Avinguda Rovira Roure, 191
25198-LLEIDA
SPAIN

Tl.: +34 973 70 25 37
Fax.:+34 073 70 26 73
e-mail: jlampur at eagrof.udl.es
**************************************************

From guillermojsanmartin at googlemail.com  Fri Feb 16 11:42:49 2007
From: guillermojsanmartin at googlemail.com (=?ISO-8859-1?Q?Guillermo_Juli=E1n_San_Mart=EDn?=)
Date: Fri, 16 Feb 2007 11:42:49 +0100
Subject: [R] Sendingd greetings and a question !!!
Message-ID: <48a5ea80702160242l30df1bc5n3a486cd80934fd32@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/6658ff81/attachment.pl 

From wwwhsd at gmail.com  Fri Feb 16 11:55:04 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 16 Feb 2007 08:55:04 -0200
Subject: [R] sapply again return value
In-Reply-To: <45D57F4B.7040304@yahoo.de>
References: <45D57F4B.7040304@yahoo.de>
Message-ID: <da79af330702160255l6c594262jb9d6bae819477f79@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/6db74c61/attachment.pl 

From niederlein-rstat at yahoo.de  Fri Feb 16 12:48:25 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Fri, 16 Feb 2007 12:48:25 +0100
Subject: [R] sapply again return value
In-Reply-To: <da79af330702160255l6c594262jb9d6bae819477f79@mail.gmail.com>
References: <45D57F4B.7040304@yahoo.de>
	<da79af330702160255l6c594262jb9d6bae819477f79@mail.gmail.com>
Message-ID: <45D59A09.50303@yahoo.de>

Henrique Dallazuanna schrieb:
> Hi,
> A simple way is:
> 
> s <- s[3]

but what if I don't know how many and which columns are NULL?

Antje


From lise.bellanger at univ-nantes.fr  Fri Feb 16 13:10:44 2007
From: lise.bellanger at univ-nantes.fr (Bellanger Lise)
Date: Fri, 16 Feb 2007 13:10:44 +0100
Subject: [R] cluster analysis under contiguity constraints with R ?
Message-ID: <45D59F44.8050807@univ-nantes.fr>

Hello,
 
    I would like to know if there is a function in an R library that 
allows to do cluster analysis under contiguity constraints ?
 
 
    Thank you very much for your answer !

    Lise Bellanger

-- 
Lise Bellanger, 
Universit? de Nantes
D?partement de Math?matiques, Laboratoire Jean Leray UMR CNRS 6629 
2, Rue de la Houssini?re BP 92208 - F-44322 Nantes Cedex 03 
T?l. : (33|0) 2 51 12 59 00 (ou 43) - Fax : (33|0) 2 51 12 59 12 
E-Mail : lise.bellanger at univ-nantes.fr
URL : http://www.math.sciences.univ-nantes.fr/


From jarioksa at sun3.oulu.fi  Fri Feb 16 13:53:35 2007
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Fri, 16 Feb 2007 14:53:35 +0200
Subject: [R] something missing in summary()
Message-ID: <1171630415.12333.15.camel@biol102145.oulu.fi>

Gerard Smits g_smits at verizon.net Fri Feb 16 00:46:09 CET 2007:
> just noticed that two key pieces of information are not given by 
> the summary() command:  N and SD.  we are given the N missing, but 
> not the converse.  I know these summary value can be obtained easy, 
> but can't understand why these two pieces of information are not 
> provided with the other info.
> 
I assume you mean summary.data.frame?

There has even been an "appeal" on this:
http://tolstoy.newcastle.edu.au/R/help/06/02/20706.html

However, I didn't find any petition you could sign (but I found many
surprising petitions when googling on this). Perhaps somebody will set
up a petition page some day.

With time, I've learnt that if something obvious is missing in the base
R, there is a reason. Probably the Core thinks that you shouldn't use sd
in a summary, but it is a poor and misleading statistic (they neither
have skewness and kurtosis). You may learn to live without sd if you
survive over the first impact. 

On the other hand, there are things like R-squared and significance
stars in summary.lm, which spoils the image of purity in the Core. 

Number of observations may not be very useful in summary.data.frame,
because it varies so little among variables.

The R-help message cited above and its follow-ups suggest some ways of
locally modifying the code and maintaining the modifications over the
upgrades of R. 

Best wishes, Jari Oksanen


From hdazizi at yahoo.com  Fri Feb 16 12:50:05 2007
From: hdazizi at yahoo.com (Azizi)
Date: Fri, 16 Feb 2007 03:50:05 -0800 (PST)
Subject: [R] plotting
Message-ID: <20070216115005.18949.qmail@web54008.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/d792f0cb/attachment.pl 

From Thomas.Adams at noaa.gov  Fri Feb 16 14:34:54 2007
From: Thomas.Adams at noaa.gov (Thomas Adams)
Date: Fri, 16 Feb 2007 08:34:54 -0500
Subject: [R] SAS, SPSS Product Comparison Table
In-Reply-To: <3698FAED-7371-4076-94B9-9D69C2E9F001@hanover.edu>
References: <7270AEC73132194E8BC0EE06B35D93D879C15F@UTKFSVS3.utk.tennessee.edu>
	<3698FAED-7371-4076-94B9-9D69C2E9F001@hanover.edu>
Message-ID: <45D5B2FE.5000100@noaa.gov>

Charilaos,

I noticed within your table, under GIS/Mapping for 'R' you only listed 
'maps'. You should probably include GRASS GIS, which can be interfaced 
to R using the spgrass6 package.

Regards,
Tom


Charilaos Skiadas wrote:
> On Feb 10, 2007, at 4:26 PM, Muenchen, Robert A (Bob) wrote:
>   
>>  "Surely R can't do for free what [fill in a SAS or SPSS
>> product here] does?" To try to address those, I've compiled a table  
>> that
>> is organized by the product categories SAS and SPSS offer. Keep in  
>> mind
>> that I still know far more about SAS and SPSS than I do about R, so I
>> could really use some help with this. The table is below in tabbed  
>> form.
>> I would appreciate it if the many R gurus out there would look it over
>> and send suggestions. I'll add it as an appendix when it's done (well,
>> as done as a moving target like this ever is!)
>>
>>     
> Great idea, this should come in handy! Here is a more readable  
> version of Bob's table (don't know if I can post attachments like  
> that to the list, so I figured I'll put it up like this):
>
> http://skiadas.dcostanet.net/uploads/StatsComparisonTable.pdf
>
>   
>> Thanks,
>> Bob
>>     
>
> Haris
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Thomas E Adams
National Weather Service
Ohio River Forecast Center
1901 South State Route 134
Wilmington, OH 45177

EMAIL:	thomas.adams at noaa.gov

VOICE:	937-383-0528
FAX:	937-383-0033


From Thierry.ONKELINX at inbo.be  Fri Feb 16 14:46:27 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 16 Feb 2007 14:46:27 +0100
Subject: [R] plotting
In-Reply-To: <20070216115005.18949.qmail@web54008.mail.yahoo.com>
Message-ID: <2E9C414912813E4EB981326983E0A104029778C6@inboexch.inbo.be>

See ?points, ?lines and ?par (especially par(new = T)).

Reading an introduction book about R might be a good idea if you're new.
Dalgaard ("Introductory Statistics with R")
Everitt & Hothorn ("A Handbook of Statistical Analyses Using R")

Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Azizi
Verzonden: vrijdag 16 februari 2007 12:50
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] plotting

  Hello,
  I use newly R! I'd like to plot several data set together in one
output window! How can I do that?
  Best regards
  Hadi

 
---------------------------------
Never miss an email again!

	[[alternative HTML version deleted]]

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mark at wardle.org  Fri Feb 16 14:48:33 2007
From: mark at wardle.org (Mark Wardle)
Date: Fri, 16 Feb 2007 13:48:33 +0000
Subject: [R] plotting
In-Reply-To: <20070216115005.18949.qmail@web54008.mail.yahoo.com>
References: <20070216115005.18949.qmail@web54008.mail.yahoo.com>
Message-ID: <45D5B631.8000500@wardle.org>

Azizi wrote:
>   Hello,
>   I use newly R! I'd like to plot several data set together in one output window! How can I do that?
>   Best regards
>   Hadi
> 
>  
> ---------------------------------
> Never miss an email again!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> 
> 

RSiteSearch("multiple plots")

But see

?layout

and

?par

(especially the mfrow parameter)


From bates at stat.wisc.edu  Fri Feb 16 14:58:01 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 16 Feb 2007 07:58:01 -0600
Subject: [R] something missing in summary()
In-Reply-To: <1171630415.12333.15.camel@biol102145.oulu.fi>
References: <1171630415.12333.15.camel@biol102145.oulu.fi>
Message-ID: <40e66e0b0702160558j40ea290em771de69a59749683@mail.gmail.com>

On 2/16/07, Jari Oksanen <jarioksa at sun3.oulu.fi> wrote:
> Gerard Smits g_smits at verizon.net Fri Feb 16 00:46:09 CET 2007:
> > just noticed that two key pieces of information are not given by
> > the summary() command:  N and SD.  we are given the N missing, but
> > not the converse.  I know these summary value can be obtained easy,
> > but can't understand why these two pieces of information are not
> > provided with the other info.
> >
> I assume you mean summary.data.frame?

Given a data frame, df, I would use

str(df)

before

summary(df)

because I want to see, for example, which columns are factors or
ordered factors or ...  That information is present in the value of
summary(df) but in a more subtle way.  As pointed out below the number
of rows in the data frame is the total number of observations for each
of the variables so putting that information in the summary for each
variable is redundant.

> There has even been an "appeal" on this:
> http://tolstoy.newcastle.edu.au/R/help/06/02/20706.html
>
> However, I didn't find any petition you could sign (but I found many
> surprising petitions when googling on this). Perhaps somebody will set
> up a petition page some day.
>
> With time, I've learnt that if something obvious is missing in the base
> R, there is a reason. Probably the Core thinks that you shouldn't use sd
> in a summary, but it is a poor and misleading statistic (they neither
> have skewness and kurtosis). You may learn to live without sd if you
> survive over the first impact.

I don't think this was an explicit decision by R-core.  It was a case
of S compatibility so the original decision was made at Bell Labs and
that group was highly influenced by John Tukey who worked with them. I
imagine that is why the summary of a numeric is a 'five-number'
summary plus the mean.  I would say the surprising and unconventional
part of that summary is the fact that it includes the mean.

> On the other hand, there are things like R-squared and significance
> stars in summary.lm, which spoils the image of purity in the Core.

However there is the option show.signif.stars which can be set to
FALSE and which I always do.

> Number of observations may not be very useful in summary.data.frame,
> because it varies so little among variables.
>
> The R-help message cited above and its follow-ups suggest some ways of
> locally modifying the code and maintaining the modifications over the
> upgrades of R.
>
> Best wishes, Jari Oksanen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From nilsson.henric at gmail.com  Fri Feb 16 15:05:03 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Fri, 16 Feb 2007 15:05:03 +0100 (CET)
Subject: [R] Unable to load RMySQL
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E8C5DD@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3E8C5DD@BAN-MAILSRV03.Amba.com>
Message-ID: <17191.212.209.13.15.1171634703.squirrel@www.sorch.se>

Den Ti, 2007-02-13, 11:43 skrev Ravi S. Shankar:
> Hi R users,
>
>
>
> I am unable to load RMySQL. The zip file is not available which I guess
> is needed to load this pakage.

Please read http://cran.r-project.org/bin/windows/contrib/2.4/ReadMe to
find out why.

> I also tried extracting the package from RMySQL_0.5-11.tar.gz  and then
> pasted the package in the directory where R is loaded for which I am
> getting the following error message
>
> "Error in library(RMySQL) : 'RMySQL' is not a valid package -- installed
> < 2.0.0?"

Did you really expect that to work?

> Any help would be welcome

Joe Byers (http://bus.cba.utulsa.edu/byersj/Research.asp) kindly provides
a Windows binary of RMySQL.


HTH,
Henric



>
>
>
> Thank you,
>
>
>
> Ravi
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From mihainica at yahoo.com  Fri Feb 16 15:10:20 2007
From: mihainica at yahoo.com (Mihai Nica)
Date: Fri, 16 Feb 2007 06:10:20 -0800 (PST)
Subject: [R] something missing in summary()
Message-ID: <643671.89925.qm@web50806.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/ef74b08e/attachment.pl 

From snunes at gmail.com  Fri Feb 16 15:32:04 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Fri, 16 Feb 2007 14:32:04 +0000
Subject: [R] Working with temporal data
In-Reply-To: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
References: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
Message-ID: <4c817d530702160632i485d62d5q550b1be45cc93b09@mail.gmail.com>

Hi again,

I'm still trying to read my data but I'm having some difficulties
converting it to dates.
My data file has lines and in each line a single date exists in the
format >2007/02/16< (without the >,<). I've tried the following:

> d <- readLines("file.dat")
> d
[1] "2006/08/09" "2004/02/11" "2004/06/09" ...
[2] ...

> d2 <- as.Date(d, format="%Y/%m/$d")
> d2
[1] NA NA NA ...
...

I'm surely doing something wrong.
Any advice would be welcomed.

Thanks!
S?rgio Nunes

On 2/15/07, S?rgio Nunes <snunes at gmail.com> wrote:
> Hi,
>
> I have several files with data in this format:
>
> 20070102
> 20070102
> 20070106
> 20070201
> ...
>
> The data is sorted and each line represents a date (YYYYMMDD). I would
> like to analyze this data using R. For instance, I would like to have
> a histogram by year, month or day.
>
> I've already made a simple Perl script that aggregates this data but I
> believe that R can be much more powerful and easy on this kind of
> work.
>
> Any suggestions on where to start?
>
> Thanks in advance,
> S?rgio Nunes
>


From jholtman at gmail.com  Fri Feb 16 16:02:19 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 16 Feb 2007 10:02:19 -0500
Subject: [R] Working with temporal data
In-Reply-To: <4c817d530702160632i485d62d5q550b1be45cc93b09@mail.gmail.com>
References: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
	<4c817d530702160632i485d62d5q550b1be45cc93b09@mail.gmail.com>
Message-ID: <644e1f320702160702v3016858ayf567e3e2f5ea23e9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/d68d1e8f/attachment.pl 

From olivier.eterradossi at ema.fr  Fri Feb 16 16:02:51 2007
From: olivier.eterradossi at ema.fr (Olivier ETERRADOSSI)
Date: Fri, 16 Feb 2007 16:02:51 +0100
Subject: [R] re :  array searches
Message-ID: <45D5C79B.1040808@ema.fr>

Hi,
I am not sure to get the issue, but assuming your data are arranged as 
in your example with dates in column x$V1 and signals as x$V2 ,
I think that you could use "rle" in the following way :

test<-rle(x$V2)
testmat<-matrix(NA,length(test$values),2)
testmat[,1]<-x[c(1,cumsum(test$length)[1:(length(test$values)-1)]+1),1]
testmat[,2]<-test$values

And you'll get your result without the duplicates, arranged as follow :
     [,1]         [,2]
[1,] "31/01/2007" "-1"
[2,] "03/02/2007" "1"
[3,] "09/02/2007" "0"
[4,] "12/02/2007" "1"
[5,] "15/02/2007" "0"

Is that what you want or did I miss it ?!
Hope this helps. Olivier



 
> essage: 92
> Date: Fri, 16 Feb 2007 10:14:30 +0000
> From: "Murali Menon" <feanor0 at hotmail.com>
> Subject: [R] array searches
> To: r-help at stat.math.ethz.ch
> Message-ID: <BAY113-F191B663BED1069C6C1EDE9EE950 at phx.gbl>
> Content-Type: text/plain; format=flowed
>
> Folks,
>
> I have a dataframe comprising a column of dates and a column of signals (-1, 
> 0, 1) that looks something like this:
>
> 30/01/2007	0
> 31/01/2007	-1
> 01/02/2007	-1
> 02/02/2007	-1
> 03/02/2007	1
> 04/02/2007	1
> 05/02/2007	1
> 06/02/2007	1
> 07/02/2007	1
> 08/02/2007	1
> 09/02/2007	0
> 10/02/2007	0
> 11/02/2007	0
> 12/02/2007	1
> 13/02/2007	1
> 14/02/2007	1
> 15/02/2007	0
> 16/02/2007	0
>
> What I need to do is for each signal *in reverse chronological order* to 
> find the date that it first appeared. So, for the zero on 16/02/2007 and 
> 15/02/2007, the 'inception' date would be 15/02/2007, because the day 
> before, the signal was 1. Likewise, the 'inception' date for the signal 1 on 
> 08/02/2007 and the five days prior, would be 03/02/2007. I need to create a 
> structure of inception dates that would finally look as follows:
>
> -1	31/01/2007
> -1	31/01/2007
> -1	31/01/2007
> 1	03/02/2007
> 1	03/02/2007
> 1	03/02/2007
> 1	03/02/2007
> 1	03/02/2007
> 1	03/02/2007
> 0	09/02/2007
> 0	09/02/2007
> 0	09/02/2007
> 1	12/02/2007
> 1	12/02/2007
> 1	12/02/2007
> 0	15/02/2007
> 0	15/02/2007
>
> Is there a clever way of doing this? My sadly C-oriented upbringing can only 
> think in terms of for-loops.
>
> Thanks!
>
> Murali

-- 
Olivier ETERRADOSSI
Ma?tre-Assistant
CMGD / Equipe "Propri?t?s Psycho-Sensorielles des Mat?riaux"
Ecole des Mines d'Al?s
H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
tel std: +33 (0)5.59.30.54.25
tel direct: +33 (0)5.59.30.90.35 
fax: +33 (0)5.59.30.63.68
http://www.ema.fr


From gsp05jm at sheffield.ac.uk  Fri Feb 16 15:58:33 2007
From: gsp05jm at sheffield.ac.uk (Jon Minton)
Date: Fri, 16 Feb 2007 14:58:33 -0000
Subject: [R] missing -> nonmissing levels
Message-ID: <001c01c751da$f20dcea0$d6296be0$@ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/d1b2b7e2/attachment.pl 

From marc_schwartz at comcast.net  Fri Feb 16 16:11:52 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 16 Feb 2007 09:11:52 -0600
Subject: [R] Working with temporal data
In-Reply-To: <4c817d530702160632i485d62d5q550b1be45cc93b09@mail.gmail.com>
References: <4c817d530702150223y22df21ban8bd31aeca8e01a6d@mail.gmail.com>
	<4c817d530702160632i485d62d5q550b1be45cc93b09@mail.gmail.com>
Message-ID: <1171638712.4804.6.camel@localhost.localdomain>

On Fri, 2007-02-16 at 14:32 +0000, S?rgio Nunes wrote:
> Hi again,
> 
> I'm still trying to read my data but I'm having some difficulties
> converting it to dates.
> My data file has lines and in each line a single date exists in the
> format >2007/02/16< (without the >,<). I've tried the following:
> 
> > d <- readLines("file.dat")
> > d
> [1] "2006/08/09" "2004/02/11" "2004/06/09" ...
> [2] ...
> 
> > d2 <- as.Date(d, format="%Y/%m/$d")

You have a typo above.  '$d' should be "%d":

d2 <- as.Date(d, format = "%Y/%m/%d")

> str(d2)
Class 'Date'  num [1:3] 13369 12459 12578

HTH,

Marc Schwartz

<snip>


From jholtman at gmail.com  Fri Feb 16 16:21:40 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 16 Feb 2007 10:21:40 -0500
Subject: [R] array searches
In-Reply-To: <BAY113-F191B663BED1069C6C1EDE9EE950@phx.gbl>
References: <BAY113-F191B663BED1069C6C1EDE9EE950@phx.gbl>
Message-ID: <644e1f320702160721mfcd4795w886906cb0568d173@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/bcb3c7e8/attachment.pl 

From ccleland at optonline.net  Fri Feb 16 16:28:36 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 16 Feb 2007 10:28:36 -0500
Subject: [R] missing -> nonmissing levels
In-Reply-To: <001c01c751da$f20dcea0$d6296be0$@ac.uk>
References: <001c01c751da$f20dcea0$d6296be0$@ac.uk>
Message-ID: <45D5CDA4.1090904@optonline.net>

Jon Minton wrote:
> Hi, 
> 
> I expect this is simple but haven?t found an answer looking on the
> archives...
> 
> I want to convert ?NA? (missing) to particular levels (nonmissing) in factor
> vectors.
> 
> e.g. I know
> 
>> X <- c(1, 2, 3)
> 
>> summary(X)
> 
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
> 
>     1.0     1.5     2.0     2.0     2.5     3.0 
> 
>> X <- as.factor(X)
> 
>> summary(X)
> 
> 1 2 3 
> 
> 1 1 1 
> 
>> levels(X)
> 
> [1] "1" "2" "3"
> 
>> levels(X) <- c("A", NA, "B")
> 
>> summary(X)
> 
>    A    B NA's 
> 
>    1    1    1
> 
> But what if I want to turn the NA back into a level? 
> 
> How do I do this?

  One way is to use recode() in the car package by John Fox.  For example:

library(car)

X <- factor(c(1, NA, 3))

X
[1] 1    <NA> 3
Levels: 1 3

recode(X, "NA='J'")
[1] 1 J 3
Levels: 1 3 J

> Thanks,  Jon
> 
> ------------------------------------------------------------------------
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From derek.eder at lungall.gu.se  Fri Feb 16 16:32:06 2007
From: derek.eder at lungall.gu.se (Derek Eder)
Date: Fri, 16 Feb 2007 16:32:06 +0100
Subject: [R] Migrating .RData from Windows to Linux?
Message-ID: <45D5CE76.4050408@lungall.gu.se>

I want to begin a migration of R workspaces from Windows to Ubuntu Linux.

(1)  Can someone suggest the most appropriate path?
      dump()  ?
      save()  ?

(2)  Are there any R platform migration related resources "out there"?
      (I have not found any yet).

Thank you,


-- 
Derek N. Eder

Gothenburg University
VINKLA - Vigilance and Neurocognition laboratory


From jholtman at gmail.com  Fri Feb 16 16:36:01 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 16 Feb 2007 10:36:01 -0500
Subject: [R] tapply, levelinformation
In-Reply-To: <20070216091300.80106.qmail@web27105.mail.ukl.yahoo.com>
References: <20070216091300.80106.qmail@web27105.mail.ukl.yahoo.com>
Message-ID: <644e1f320702160736v61bb8e3akabdcd5e6b5594b55@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/b4bde3b4/attachment.pl 

From snunes at gmail.com  Fri Feb 16 16:39:54 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Fri, 16 Feb 2007 15:39:54 +0000
Subject: [R] Working with temporal data [Solved]
Message-ID: <4c817d530702160739x2b0a1984ka12d788835595409@mail.gmail.com>

Just for the record, here are my steps for producing a date based histogram.
Data is stored in a file where each line only has a date - 2007/02/16

>d<-readLines("filename.dat")
>d<-as.Date(d, format="%Y/%m/%d")
>pdf(yearly.pdf)
>hist(d, "years")
>dev.off()

Instead of "years" you can also use "days", "weeks", "months", "secs",
"mins", "hours".

One final question, how could I easily filter my dataset if, for
instance, I only wanted to see results from 2006 ?

Thanks to all who helped,
S?rgio Nunes

On 2/15/07, S?rgio Nunes <snunes at gmail.com> wrote:
> Hi,
>
> I have several files with data in this format:
>
> 20070102
> 20070102
> 20070106
> 20070201
> ...
>
> The data is sorted and each line represents a date (YYYYMMDD). I would
> like to analyze this data using R. For instance, I would like to have
> a histogram by year, month or day.
>
> I've already made a simple Perl script that aggregates this data but I
> believe that R can be much more powerful and easy on this kind of
> work.
>
> Any suggestions on where to start?
>
> Thanks in advance,
> S?rgio Nunes
>


From marc_schwartz at comcast.net  Fri Feb 16 16:46:29 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 16 Feb 2007 09:46:29 -0600
Subject: [R] Migrating .RData from Windows to Linux?
In-Reply-To: <45D5CE76.4050408@lungall.gu.se>
References: <45D5CE76.4050408@lungall.gu.se>
Message-ID: <1171640790.4804.11.camel@localhost.localdomain>

On Fri, 2007-02-16 at 16:32 +0100, Derek Eder wrote:
> I want to begin a migration of R workspaces from Windows to Ubuntu Linux.
> 
> (1)  Can someone suggest the most appropriate path?
>       dump()  ?
>       save()  ?
> 
> (2)  Are there any R platform migration related resources "out there"?
>       (I have not found any yet).
> 
> Thank you,

The easiest way is to use save() (or even save.image()) then load().

>From the Details section in ?save:

All R platforms use the XDR representation of binary objects in binary
save-d files, and these are portable across all R platforms. (ASCII
saves used to be useful for moving data between platforms but are now
only of historical interest.)


Beyond that, you should review the R Installation and Administration
manual, which is available in your installation or on CRAN.  That will
cover platform specific issues.

HTH,

Marc Schwartz


From topkatz at msn.com  Fri Feb 16 16:48:29 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 16 Feb 2007 10:48:29 -0500
Subject: [R] Function to assign quantiles?
In-Reply-To: <f8e6ff050702152251r2e13dc61m43eacbb5620e2d8b@mail.gmail.com>
Message-ID: <BAY132-F16AB967DB6585F5AFAE8BFAA950@phx.gbl>

Thank you so much to all who replied on and off list.  It's good to have so 
many options!  One of the off-list responders also mentioned the quantcut 
function in the gtools package.

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: "hadley wickham" <h.wickham at gmail.com>
>To: f.harrell at vanderbilt.edu
>CC: marc_schwartz at comcast.net, r-help at stat.math.ethz.ch, "Talbot Katz" 
><topkatz at msn.com>
>Subject: Re: [R] Function to assign quantiles?
>Date: Fri, 16 Feb 2007 19:51:49 +1300
>
>On 2/16/07, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
>>Marc Schwartz wrote:
>> > On Thu, 2007-02-15 at 17:50 -0500, Talbot Katz wrote:
>> >> Hi.
>> >>
>> >> If I call the quantiles function as follows:
>> >>
>> >> qvec = quantiles(dvals,probs=seq(0,1,0.1))
>> >>
>> >> the results will return a vector something like the following example:
>> >>
>> >>       0%    10%    20%    30%     40%     50%    60%    70%      80%   
>>   90%
>> >>      100%
>> >>     56.0   137.3   238.4   317.9   495.8   568.5   807.4  1207.7  
>>1713.0
>> >> 2951.1  8703.0
>> >>
>> >> Now I want to assign the deciles, 1 - 10, to each observation, so, in 
>>the
>> >> above example, if dvals[322] = 256, I want to assign qvals[322] = 3, 
>>and if
>> >> dvals[7216] = 1083, I want qvals[7216] = 7, etc.  I would think there 
>>would
>> >> be a function, or some very quick code to do that, but I couldn't find 
>>it.
>> >>
>> >> Here's what I have now.  It works, but I figure there must be a better 
>>way:
>> >>
>> >> asdc <- function(q){max(1,which(qvec<q))}
>> >> qvals = apply(matrix(dvals,nrow=1),2,asdc)
>> >>
>> >>
>> >> Any suggestions?  Thanks!
>> >
>> > Take a look at ?cut and use the output of quantile() to define the
>> > 'breaks' argument:
>> >
>> > x <- rnorm(100)
>> >
>> >> cut(x, breaks = quantile(x, probs = seq(0, 1, 0.1)),
>> >       include.lowest = TRUE, labels = 1:10)
>> >   [1] 2  1  9  10 9  3  1  7  5  2  4  1  10 8  4  10 9  9  5  3  5  8
>> >  [23] 6  9  7  1  10 1  10 9  3  3  9  3  5  5  6  4  8  10 6  2  2  8
>> >  [45] 6  3  6  9  1  4  7  10 8  7  5  3  1  10 2  1  7  8  8  2  8  8
>> >  [67] 3  10 4  6  5  1  4  6  4  4  5  9  9  7  2  8  2  5  1  3  7  6
>> >  [89] 4  6  4  6  2  7  2  10 7  3  5  7
>> > Levels: 1 2 3 4 5 6 7 8 9 10
>> >
>> >
>> > HTH,
>> >
>> > Marc Schwartz
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>Or see the cut2 function in the Hmisc package
>
>Or the chop function in ggplot package - it's a common problem.
>
>Hadley


From marc_schwartz at comcast.net  Fri Feb 16 17:08:51 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 16 Feb 2007 10:08:51 -0600
Subject: [R] Working with temporal data [Solved]
In-Reply-To: <4c817d530702160739x2b0a1984ka12d788835595409@mail.gmail.com>
References: <4c817d530702160739x2b0a1984ka12d788835595409@mail.gmail.com>
Message-ID: <1171642131.4804.19.camel@localhost.localdomain>

On Fri, 2007-02-16 at 15:39 +0000, S?rgio Nunes wrote:
> Just for the record, here are my steps for producing a date based histogram.
> Data is stored in a file where each line only has a date - 2007/02/16
> 
> >d<-readLines("filename.dat")
> >d<-as.Date(d, format="%Y/%m/%d")
> >pdf(yearly.pdf)
> >hist(d, "years")
> >dev.off()
> 
> Instead of "years" you can also use "days", "weeks", "months", "secs",
> "mins", "hours".
> 
> One final question, how could I easily filter my dataset if, for
> instance, I only wanted to see results from 2006 ?
> 
> Thanks to all who helped,
> S?rgio Nunes

The easiest way may be to create your own "Year" extractor function,
since there does not appear to be one by default, unless I missed it
someplace (it is not listed in ?weekdays).

For example, using your data from my prior reply:

> d2
[1] "2006-08-09" "2004-02-11" "2004-06-09"


Years <- function(x) format(x, "%Y")


> Years(d2)
[1] "2006" "2004" "2004"


> Years(d2) == 2006
[1]  TRUE FALSE FALSE


> d2[Years(d2) == 2006]
[1] "2006-08-09"


So, just use something like:

 hist(d2[Years(d2) == 2006], "years")

HTH,

Marc Schwartz


From Nicolas.Degallier at ird.fr  Fri Feb 16 17:10:05 2007
From: Nicolas.Degallier at ird.fr (Nicolas Degallier)
Date: Fri, 16 Feb 2007 17:10:05 +0100
Subject: [R] if() for() { }
Message-ID: <F866E443-09E9-43C8-A20E-031CE2ED392F@ird.fr>

Dear R'helpers,

I guess the solution is trivial but despite some hours of testing and  
looking at the doc, I was unable to fix the following problem.

I want to do either:

for (i in 1:lat) a[i,which(a[i,]==0] <- NA

or:
for (i in 1:lat) a[i,which(a[i,]==-9999|a[i,]==9999|a[i,]==9998|a[i,] 
==-9998)] <- NA

if the word "Time" is or is not present in variable "nomfichier":
test<-grep("Time",nomfichier)

the controlling test would be:

if (test==1)

But in either cases I obtain:

if (grep("Time",nomfichier)>0) for (i in 1:lat) a[i,which(a[i,]==0]  
<- NA
Erreur : syntax error

 > if (grep("Time",nomfichier)<>0) {for (i in 1:lat) a[i,which(a[i,] 
==-9999|a[i,]==9999|a[i,]==9998|a[i,]==-9998)] <- NA}
Erreur : syntax error
 >

I would appreciate if someone can show me the right way to do this.

Sincerely
Nicolas

Nicolas Degallier

UMR 7159 / IRD UR182
Laboratoire d'Oc?anographie et du Climat, Exp?rimentation et  
Approches Num?riques (LOCEAN)
Tour 45-55, 4e ?t., case 100, 4 place Jussieu
75252  Paris Cedex 5  France
t?l: (33) 01 44 27 51 57
fax: (33) 01 44 27 38 05

E-mail: <Nicolas.Degallier at ird.fr>
pdf reprints (login="anonymous"; password="your at email.address"):  
ftp://ftp.locean-ipsl.upmc.fr/LOCEAN/ndelod


From jholtman at gmail.com  Fri Feb 16 17:29:30 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 16 Feb 2007 11:29:30 -0500
Subject: [R] if() for() { }
In-Reply-To: <F866E443-09E9-43C8-A20E-031CE2ED392F@ird.fr>
References: <F866E443-09E9-43C8-A20E-031CE2ED392F@ird.fr>
Message-ID: <644e1f320702160829q7a48f8a2jbd886a81d9997f9e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/b94bd1a8/attachment.pl 

From Jan.Kleinn at partnerre.com  Fri Feb 16 17:31:35 2007
From: Jan.Kleinn at partnerre.com (Jan.Kleinn at partnerre.com)
Date: Fri, 16 Feb 2007 17:31:35 +0100
Subject: [R] R_decompress1 and zlib
Message-ID: <OF5CFA0D7A.0CBD6A85-ONC1257284.00594F92-C1257284.005B217A@partnerre.com>


Dear all,

I have a problem similar to what was already posted about two years ago
(http://tolstoy.newcastle.edu.au/R/help/05/01/10011.html): I have a binary
file, which contains a header and data compressed by zlib and written to
the file as unsigned character. Here is, what I try to do:

> zz <- file(in.file, 'rb')
> ## the header has to be read in three parts due to the different data
types
> header1 <- readBin(zz, integer(), 8, size = 4)
> header2 <- readBin(zz, numeric(), 5, size = 8)
> header3 <- readBin(zz, integer(), 14, size = 4)
> data.raw <- readBin(zz, raw(), 1e6)
> close(zz)
> ## it works fine up to here
> mydata <- .Call('R_decompress1', data.raw)
Error: cannot allocate vector of size 1976123 Kb
In addition: Warning messages:
1: Reached total allocation of 1534Mb: see help(memory.size)
2: Reached total allocation of 1534Mb: see help(memory.size)
> length(data.raw)
[1] 481

The .Call is, where R complains about not having enough memory.
Interestingly, when I generate some test-data, compress it, write it to a
file, read it from the file and decompress it, it works fine:
> t <- round(abs(rnorm(1000)), digits = 2)
> t.asc <- paste(t, collapse = ' ')
> t.raw <- charToRaw(t.asc)
> t.comp <- .Call('R_compress1', t.raw)
> ## write to file
> zz <- file('test.file', 'wb')
> writeBin(t.comp, zz)
> close(zz)
> ## read from file
> zz <- file('test.file', 'rb')
> comp.in <- readBin(zz, raw(), 1e6)
> close(zz)
> t.decomp <- .Call('R_decompress1', comp.in)
> out.asc <- rawToChar(t.decomp)
> out.num <- as.numeric(unlist(strsplit(out.asc, ' ')))
> length(comp.in)
[1] 1627

I don't understand, why the decompression of the 481 element long raw
vector out of my input file does not work, whereas the decompression works
fine for a raw vector of over 1000 elements I created myself. Could it be,
that my data is not properly zlib-compressed? Does anyone know, whether
there could be some other How could I check the integrity of the data? Any
ideas are greatly appreciated.

I'm working on Windows XP with R version 2.4.1 under ESS in Emacs (but I
suspect, this shouldn't matter):
> version
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          4.1
year           2006
month          12
day            18
svn rev        40228
language       R
version.string R version 2.4.1 (2006-12-18)

Many thanks in advance and best regards, Jan



DISCLAIMER: This e-mail contains information solely intended...{{dropped}}


From marc_schwartz at comcast.net  Fri Feb 16 17:38:31 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 16 Feb 2007 10:38:31 -0600
Subject: [R] if() for() { }
In-Reply-To: <F866E443-09E9-43C8-A20E-031CE2ED392F@ird.fr>
References: <F866E443-09E9-43C8-A20E-031CE2ED392F@ird.fr>
Message-ID: <1171643911.4804.37.camel@localhost.localdomain>

On Fri, 2007-02-16 at 17:10 +0100, Nicolas Degallier wrote:
> Dear R'helpers,
> 
> I guess the solution is trivial but despite some hours of testing and  
> looking at the doc, I was unable to fix the following problem.
> 
> I want to do either:
> 
> for (i in 1:lat) a[i,which(a[i,]==0] <- NA
> 
> or:
> for (i in 1:lat) a[i,which(a[i,]==-9999|a[i,]==9999|a[i,]==9998|a[i,] 
> ==-9998)] <- NA

I may be missing something with respect to the structure of 'a'. It
would help to know if 'a' is a matrix (seems so).

In either case, the following may work:

  is.na(a) <- (abs(a) >= 9998) | (a == 0)

a <- sample(c(-9999, 9999, -9998, 9998, 0, 1:10))


> a
 [1]     0 -9998     8     7  9999     5    10     2 -9999     1  9998
[12]     6     9     3     4


is.na(a) <- (abs(a) >= 9998) | (a == 0)


> a
 [1] NA NA  8  7 NA  5 10  2 NA  1 NA  6  9  3  4


See ?is.na for more information


> if the word "Time" is or is not present in variable "nomfichier":
> test<-grep("Time",nomfichier)
> 
> the controlling test would be:
> 
> if (test==1)
> 
> But in either cases I obtain:
> 
> if (grep("Time",nomfichier)>0) for (i in 1:lat) a[i,which(a[i,]==0]  
> <- NA
> Erreur : syntax error
> 
>  > if (grep("Time",nomfichier)<>0) {for (i in 1:lat) a[i,which(a[i,] 
> ==-9999|a[i,]==9999|a[i,]==9998|a[i,]==-9998)] <- NA}
> Erreur : syntax error
>  >
> 
> I would appreciate if someone can show me the right way to do this.
> 
> Sincerely
> Nicolas


The above is also a bit confusing, but let me offer some insight into
grep().

If the value is present, then grep() returns the index of the matching
value (or the value itself, if "value = TRUE').

If the value is not present, then a 0 length integer or character
(depending upon whether 'value' is FALSE or TRUE) is returned.

Using the example from ?grep:

> txt <- c("arm","foot","lefroo", "bafoobar")

> grep("foo", txt)
[1] 2 4

> grep("foo", txt, value = TRUE)
[1] "foot"     "bafoobar"


However, if the expression is not matched:

> grep("MISS", txt)
integer(0)

> grep("MISS", txt, value = TRUE)
character(0)


Depending upon what you need to do, you can use any() for a simple
boolean result:

> any(grep("foo", txt))
[1] TRUE

> any(grep("MISS", txt))
[1] FALSE

This could provide an initial filter test to see if the value you
require is present in the target vector and if so, then get the actual
indices or values in a subsequent call to grep() if needed.

In other words, your initial 'if' statement and subsequent code may need
to be:

  if (any(grep("Time", nomfichier)))
  {
     is.na(a) <- (abs(a) >= 9998) | (a == 0)
  }


HTH,

Marc Schwartz


From friendly at yorku.ca  Fri Feb 16 17:46:23 2007
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 16 Feb 2007 11:46:23 -0500
Subject: [R] R implementations of scatterplot/map labeling algorithims?
Message-ID: <45D5DFDF.80107@yorku.ca>

Dear R community

In a current paper, I'm (briefly) considering the topic of producing
scatterplots or maps with point labels positioned in such a way as to 
minimize label overlap and occlusion.  This is a topic with a large, but 
scattered literature. In CS, it is considered NP-hard, but there are
a variety of approximate solutions.  The most complete bibliography I've 
found is
the Map-Labeling Bibliography,
http://liinwww.ira.uka.de/bibliography/Theory/map.labeling.html

AFAIK, the only concrete and published implementation is a Fortran
program published by Noma (below), and then adapted by Warren Kuhfeld
at SAS in PROC PLOT, and used in his %plotit macro.

     Journal Title  - Psychometrika
     Article Title  - Heuristic method for label placement in scatterplots
     Volume  - Volume 52
     Issue  - 3
     First Page  - 463
     Last Page  - 468
     Issue Cover Date  - 1987-09-27
     Author  - Elliot Noma
     DOI  - 10.1007/BF02294366
     Link  - http://www.springerlink.com/content/c4k6205r83156165

Does anyone know of a related R (or other) public implementation of a 
solution to this problem?

thanks,
-Michael

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA


From ripley at stats.ox.ac.uk  Fri Feb 16 17:47:55 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Feb 2007 16:47:55 +0000 (GMT)
Subject: [R] Working with temporal data [Solved]
In-Reply-To: <1171642131.4804.19.camel@localhost.localdomain>
References: <4c817d530702160739x2b0a1984ka12d788835595409@mail.gmail.com>
	<1171642131.4804.19.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0702161645020.26421@auk.stats>

On Fri, 16 Feb 2007, Marc Schwartz wrote:

> On Fri, 2007-02-16 at 15:39 +0000, S?rgio Nunes wrote:
>> Just for the record, here are my steps for producing a date based histogram.
>> Data is stored in a file where each line only has a date - 2007/02/16
>>
>>> d<-readLines("filename.dat")
>>> d<-as.Date(d, format="%Y/%m/%d")
>>> pdf(yearly.pdf)
>>> hist(d, "years")
>>> dev.off()
>>
>> Instead of "years" you can also use "days", "weeks", "months", "secs",
>> "mins", "hours".
>>
>> One final question, how could I easily filter my dataset if, for
>> instance, I only wanted to see results from 2006 ?
>>
>> Thanks to all who helped,
>> S?rgio Nunes
>
> The easiest way may be to create your own "Year" extractor function,
> since there does not appear to be one by default, unless I missed it
> someplace (it is not listed in ?weekdays).

>From that help page

Note:

      Other components such as the day of the month or the year are very
      easy to compute: just use 'as.POSIXlt' and extract the relevant
      component.

> 1900+as.POSIXlt(d2)$year
[1] 2006 2004 2004
...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ccleland at optonline.net  Fri Feb 16 17:56:59 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 16 Feb 2007 11:56:59 -0500
Subject: [R] R implementations of scatterplot/map labeling algorithims?
In-Reply-To: <45D5DFDF.80107@yorku.ca>
References: <45D5DFDF.80107@yorku.ca>
Message-ID: <45D5E25B.90801@optonline.net>

Michael Friendly wrote:
> Dear R community
> 
> In a current paper, I'm (briefly) considering the topic of producing
> scatterplots or maps with point labels positioned in such a way as to 
> minimize label overlap and occlusion.  This is a topic with a large, but 
> scattered literature. In CS, it is considered NP-hard, but there are
> a variety of approximate solutions.  The most complete bibliography I've 
> found is
> the Map-Labeling Bibliography,
> http://liinwww.ira.uka.de/bibliography/Theory/map.labeling.html
> 
> AFAIK, the only concrete and published implementation is a Fortran
> program published by Noma (below), and then adapted by Warren Kuhfeld
> at SAS in PROC PLOT, and used in his %plotit macro.
> 
>      Journal Title  - Psychometrika
>      Article Title  - Heuristic method for label placement in scatterplots
>      Volume  - Volume 52
>      Issue  - 3
>      First Page  - 463
>      Last Page  - 468
>      Issue Cover Date  - 1987-09-27
>      Author  - Elliot Noma
>      DOI  - 10.1007/BF02294366
>      Link  - http://www.springerlink.com/content/c4k6205r83156165
> 
> Does anyone know of a related R (or other) public implementation of a 
> solution to this problem?

  One thing that comes to mind is labcurve() in Frank Harrell's Hmisc
package.

"Optionally draws a set of curves then labels the curves. A variety of
methods for drawing labels are implemented, ranging from positioning
using the mouse to automatic labeling to automatic placement of key
symbols with manual placement of key legends to automatic placement of
legends. For automatic positioning of labels or keys, a curve is labeled
at a point that is maximally separated from all of the other curves.
Gaps occurring when curves do not start or end at the same x-coordinates
are given preference for positioning labels. If labels are offset from
the curves (the default behaviour), if the closest curve to curve i is
above curve i, curve i is labeled below its line. If the closest curve
is below curve i, curve i is labeled above its line. These directions
are reversed if the resulting labels would appear outside the plot region."

> thanks,
> -Michael

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From marc_schwartz at comcast.net  Fri Feb 16 18:00:54 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 16 Feb 2007 11:00:54 -0600
Subject: [R] Working with temporal data [Solved]
In-Reply-To: <Pine.LNX.4.64.0702161645020.26421@auk.stats>
References: <4c817d530702160739x2b0a1984ka12d788835595409@mail.gmail.com>
	<1171642131.4804.19.camel@localhost.localdomain>
	<Pine.LNX.4.64.0702161645020.26421@auk.stats>
Message-ID: <1171645254.4804.47.camel@localhost.localdomain>

On Fri, 2007-02-16 at 16:47 +0000, Prof Brian Ripley wrote:
> On Fri, 16 Feb 2007, Marc Schwartz wrote:
> 
> > On Fri, 2007-02-16 at 15:39 +0000, S?rgio Nunes wrote:
> >> Just for the record, here are my steps for producing a date based histogram.
> >> Data is stored in a file where each line only has a date - 2007/02/16
> >>
> >>> d<-readLines("filename.dat")
> >>> d<-as.Date(d, format="%Y/%m/%d")
> >>> pdf(yearly.pdf)
> >>> hist(d, "years")
> >>> dev.off()
> >>
> >> Instead of "years" you can also use "days", "weeks", "months", "secs",
> >> "mins", "hours".
> >>
> >> One final question, how could I easily filter my dataset if, for
> >> instance, I only wanted to see results from 2006 ?
> >>
> >> Thanks to all who helped,
> >> S?rgio Nunes
> >
> > The easiest way may be to create your own "Year" extractor function,
> > since there does not appear to be one by default, unless I missed it
> > someplace (it is not listed in ?weekdays).
> 
> >From that help page
> 
> Note:
> 
>       Other components such as the day of the month or the year are very
>       easy to compute: just use 'as.POSIXlt' and extract the relevant
>       component.
> 
> > 1900+as.POSIXlt(d2)$year
> [1] 2006 2004 2004
> ...

Yes, indeed....and just as I was thinking that I had consumed a
sufficient volume of coffee this morning...

Thanks!

Looking at the code for months() and weekdays(), for example:

> months.Date
function (x, abbreviate = FALSE) 
format(x, ifelse(abbreviate, "%b", "%B"))
<environment: namespace:base>

from a 'consistency' perspective, would it not make sense to have a
years.Date() function along the same lines as what I had proposed?

quarters() and julian(), of course, are appropriately different.

Regards,

Marc


From mnair at iusb.edu  Fri Feb 16 18:16:16 2007
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Fri, 16 Feb 2007 12:16:16 -0500
Subject: [R] slice usage
Message-ID: <A32055BDEA88C34BB3DBBCD229380778D49082@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/fdd94cbe/attachment.pl 

From roland.rproject at gmail.com  Fri Feb 16 16:53:13 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Fri, 16 Feb 2007 10:53:13 -0500
Subject: [R] convert to binary to decimal
In-Reply-To: <45D56912.7771.2FEB94@localhost>
References: <002301c75121$ca6425c0$4d908980@gne.windows.gene.com>
	<47c7c59e0702150913l1be48edbmd480e5f4e83e9452@mail.gmail.com>
	<45D56912.7771.2FEB94@localhost>
Message-ID: <47c7c59e0702160753m544dd961q485f7eb6fb31d7ed@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/24cc8c83/attachment.pl 

From bphealey at hotmail.com  Fri Feb 16 18:39:28 2007
From: bphealey at hotmail.com (Brian Healey)
Date: Fri, 16 Feb 2007 14:09:28 -0330
Subject: [R] optim() and resultant hessian
Message-ID: <BAY128-F33EC05450848DD01375FE8C1950@phx.gbl>

R users;

A question about optimization within R.

I've been using both optim() and nlminb() to estimate parameters and all 
seems to be working fine. For context (but without getting into specifics - 
sorry), I'm working with a problem that is known to have correlated 
parameters, and parameter estimation can be difficult. I have a question on 
optim() - I'm using method="L-BFGS-B" to accommodate box constraints.

For my dataset, I obtain parameter estimates using a few iterations of 
optim() - iterations in that I'm simply taking the results from a previous 
optim() call and using these as starting values in the next function call.

The final call to optim() returns the following:
$par
[1] 0.2272361 0.8037642 26.8591998 3.0631280 0.2224566
$value
[1] -46.13906
$counts
function gradient
4 4
$convergence
[1] 0
$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
$hessian
[,1] [,2] [,3] [,4] [,5]
[1,] 1.267070e+17 1.012691e+17 1.348054e+15 625551.58724 9.359559e+07
[2,] 1.012691e+17 8.189877e+16 1.144248e+15 569562.44945 8.699072e+07
[3,] 1.348054e+15 1.144248e+15 2.457323e+05 3426.60293 -2.297009e+03
[4,] 6.255516e+05 5.695624e+05 3.426603e+03 99.06880 -6.750806e+01
[5,] 9.359559e+07 8.699072e+07 -2.297009e+03 -67.50806 1.905247e+03

i.e. convergence and message report that things look "ok".

However; if I take the hessian and compute eigenvalues and eigenvectors, the 
result is:
$values
[1] 2.080357e+17 5.889416e+14 1.907746e+03 9.648828e+01 -1.886641e+13
$vectors
[,1] [,2] [,3] [,4] [,5]
[1,] 7.797175e-01 6.246383e-01 -5.024932e-09 -2.214316e-10 -4.321721e-02
[2,] 6.260739e-01 -7.768497e-01 5.917821e-09 2.637892e-10 6.734993e-02
[3,] 8.496067e-03 -7.957109e-02 9.710924e-08 4.049691e-09 -9.967930e-01
[4,] 4.058779e-12 -8.828316e-11 3.729724e-02 -9.993042e-01 -4.192523e-10
[5,] 6.125907e-10 -1.547717e-08 -9.993042e-01 -3.729724e-02 -9.626469e-08

Optim() indicates convergence when one of the eigenvalues is negative !?!? 
Any reason to not be concerned? (Possibly the Hessian is simply computed at 
the solution and not used to arrive at the estimate?)

Thanks in advance for any helpful feedback.
B Healey


From labone at gforcecable.com  Fri Feb 16 18:57:28 2007
From: labone at gforcecable.com (Tom La Bone)
Date: Fri, 16 Feb 2007 12:57:28 -0500
Subject: [R] How is the line in av.plot calculated?
Message-ID: <004401c751f3$ef2ff9b0$6401a8c0@Boozoo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/3f751d8a/attachment.pl 

From murdoch at stats.uwo.ca  Fri Feb 16 19:08:19 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 16 Feb 2007 13:08:19 -0500
Subject: [R] How is the line in av.plot calculated?
In-Reply-To: <004401c751f3$ef2ff9b0$6401a8c0@Boozoo>
References: <004401c751f3$ef2ff9b0$6401a8c0@Boozoo>
Message-ID: <45D5F313.1060303@stats.uwo.ca>

On 2/16/2007 12:57 PM, Tom La Bone wrote:
> I suspect that the line in the added variable plot (library car) is a SLR of
> the residuals, but I can't seem to find this written anywhere.  Can someone
> confirm this?  Thanks.

You can look at the source:

 > av.plot
function (model, ...)
{
     UseMethod("av.plot")
}
<environment: namespace:car>

So the answer will depend on what kind of model you want.  The method 
for lm objects would be in av.plot.lm, which is not directly visible:

 > av.plot.lm
Error: object "av.plot.lm" not found

however getAnywhere finds it:

 > getAnywhere("av.plot.lm")
A single object matching 'av.plot.lm' was found
It was found in the following places
   registered S3 method for av.plot from namespace car
   namespace:car
with value

function (model, variable, labels = 
names(residuals(model)[!is.na(residuals(model))]),
     identify.points = TRUE, las = par("las"), col = palette()[2],
     pch = 1, lwd = 2, main = "Added-Variable Plot", ...)
{
     variable <- if (is.character(variable) & 1 == length(variable))
         variable
     else deparse(substitute(variable))
     mod.mat <- model.matrix(model)
     var.names <- colnames(mod.mat)
     var <- which(variable == var.names)
     if (0 == length(var))
         stop(paste(variable, "is not a column of the model matrix."))
     response <- response(model)
     responseName <- responseName(model)
     if (is.null(weights(model)))
         wt <- rep(1, length(response))
     else wt <- weights(model)
     res <- lsfit(mod.mat[, -var], cbind(mod.mat[, var], response),
         wt = wt, intercept = FALSE)$residuals
     plot(res[, 1], res[, 2], xlab = paste(var.names[var], "| others"),
         ylab = paste(responseName, " | others"), main = main,
         las = las, col = col, pch = pch)
     abline(lsfit(res[, 1], res[, 2], wt = wt), col = col, lwd = lwd)
     if (identify.points)
         identify(res[, 1], res[, 2], labels)
}
<environment: namespace:car>
 >

from which I would conclude that your description is correct:  the line 
is drawn in the abline() call near the bottom.

Duncan Murdoch


From ripley at stats.ox.ac.uk  Fri Feb 16 19:08:43 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Feb 2007 18:08:43 +0000 (GMT)
Subject: [R] optim() and resultant hessian
In-Reply-To: <BAY128-F33EC05450848DD01375FE8C1950@phx.gbl>
References: <BAY128-F33EC05450848DD01375FE8C1950@phx.gbl>
Message-ID: <Pine.LNX.4.64.0702161801230.14334@gannet.stats.ox.ac.uk>

On Fri, 16 Feb 2007, Brian Healey wrote:

> R users;
>
> A question about optimization within R.
>
> I've been using both optim() and nlminb() to estimate parameters and all
> seems to be working fine. For context (but without getting into specifics -
> sorry), I'm working with a problem that is known to have correlated
> parameters, and parameter estimation can be difficult. I have a question on
> optim() - I'm using method="L-BFGS-B" to accommodate box constraints.
>
> For my dataset, I obtain parameter estimates using a few iterations of
> optim() - iterations in that I'm simply taking the results from a previous
> optim() call and using these as starting values in the next function call.
>
> The final call to optim() returns the following:
> $par
> [1] 0.2272361 0.8037642 26.8591998 3.0631280 0.2224566
> $value
> [1] -46.13906
> $counts
> function gradient
> 4 4
> $convergence
> [1] 0
> $message
> [1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
> $hessian
> [,1] [,2] [,3] [,4] [,5]
> [1,] 1.267070e+17 1.012691e+17 1.348054e+15 625551.58724 9.359559e+07
> [2,] 1.012691e+17 8.189877e+16 1.144248e+15 569562.44945 8.699072e+07
> [3,] 1.348054e+15 1.144248e+15 2.457323e+05 3426.60293 -2.297009e+03
> [4,] 6.255516e+05 5.695624e+05 3.426603e+03 99.06880 -6.750806e+01
> [5,] 9.359559e+07 8.699072e+07 -2.297009e+03 -67.50806 1.905247e+03
>
> i.e. convergence and message report that things look "ok".
>
> However; if I take the hessian and compute eigenvalues and eigenvectors, the
> result is:
> $values
> [1] 2.080357e+17 5.889416e+14 1.907746e+03 9.648828e+01 -1.886641e+13
> $vectors
> [,1] [,2] [,3] [,4] [,5]
> [1,] 7.797175e-01 6.246383e-01 -5.024932e-09 -2.214316e-10 -4.321721e-02
> [2,] 6.260739e-01 -7.768497e-01 5.917821e-09 2.637892e-10 6.734993e-02
> [3,] 8.496067e-03 -7.957109e-02 9.710924e-08 4.049691e-09 -9.967930e-01
> [4,] 4.058779e-12 -8.828316e-11 3.729724e-02 -9.993042e-01 -4.192523e-10
> [5,] 6.125907e-10 -1.547717e-08 -9.993042e-01 -3.729724e-02 -9.626469e-08
>
> Optim() indicates convergence when one of the eigenvalues is negative !?!?
> Any reason to not be concerned?

I don't think you have scaled your problem as recommended on the help 
page, or the Hessian would not have elements differing by 15 orders of 
magnitude.  So, yes, there is reason to be worried about the 
finite-difference approximations used, as well as the numerical accuracy 
of such extreme differences.  (If you did scale, you don't want to do this 
on the rescaled Hessian.)

> Possibly the Hessian is simply computed at
> the solution and not used to arrive at the estimate?)

Please do read what it says about the method on the help page and the 
references.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ccleland at optonline.net  Fri Feb 16 19:20:22 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 16 Feb 2007 13:20:22 -0500
Subject: [R] How is the line in av.plot calculated?
In-Reply-To: <004401c751f3$ef2ff9b0$6401a8c0@Boozoo>
References: <004401c751f3$ef2ff9b0$6401a8c0@Boozoo>
Message-ID: <45D5F5E6.4050902@optonline.net>

Tom La Bone wrote:
> I suspect that the line in the added variable plot (library car) is a SLR of
> the residuals, but I can't seem to find this written anywhere.  Can someone
> confirm this?  Thanks.

  See the Fox(1997) reference on the help page (pages 281-283).  Note
that both the x-axis and the y-axis are residuals.  For example, you
could do the av.plot for income in the Duncan data as follows:

library(car)

res1 <- residuals(lm(prestige ~ education+type, data=Duncan))
res2 <- residuals(lm(income ~ education+type, data=Duncan))

plot(res2, res1)
abline(lm(res1 ~ res2))

> Tom
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ripley at stats.ox.ac.uk  Fri Feb 16 19:39:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Feb 2007 18:39:58 +0000 (GMT)
Subject: [R] missing -> nonmissing levels
In-Reply-To: <001c01c751da$f20dcea0$d6296be0$@ac.uk>
References: <001c01c751da$f20dcea0$d6296be0$@ac.uk>
Message-ID: <Pine.LNX.4.64.0702161833320.15470@gannet.stats.ox.ac.uk>

Well, if it is missing, how do you know what level to turn it back into?
That is what NA means in R: not available.

If you want missings to be a separate level, you could use

> factor(as.character(X), exclude=NULL)
[1] A    <NA> B
Levels: A B <NA>

BTW, using summary() for a length-3 object is not helpful, and please send 
properly formatted text emails as the posting guide does ask of you.

On Fri, 16 Feb 2007, Jon Minton wrote:

> Hi,
>
> I expect this is simple but haven?t found an answer looking on the
> archives...
>
>
>
> I want to convert ?NA? (missing) to particular levels (nonmissing) in factor
> vectors.
>
>
>
> e.g. I know
>
>> X <- c(1, 2, 3)
>
>> summary(X)
>
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>
>    1.0     1.5     2.0     2.0     2.5     3.0
>
>> X <- as.factor(X)
>
>> summary(X)
>
> 1 2 3
>
> 1 1 1
>
>> levels(X)
>
> [1] "1" "2" "3"
>
>> levels(X) <- c("A", NA, "B")
>
>> summary(X)
>
>   A    B NA's
>
>   1    1    1
>
>
>
> But what if I want to turn the NA back into a level?
>
> How do I do this?
>
>
>
> Thanks,  Jon
>
>
>
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ted.harding at nessie.mcc.ac.uk  Fri Feb 16 19:41:45 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 16 Feb 2007 18:41:45 -0000 (GMT)
Subject: [R] Time of failure, Arrhenius and Weibull distribution
In-Reply-To: <BAY134-F23114696D52406BFCFF879D8950@phx.gbl>
Message-ID: <XFMail.070216184145.ted.harding@nessie.mcc.ac.uk>

On 16-Feb-07 Bart Joosen wrote:
> My model is as follows:
> 
> mod <- lm(log(Degrad/Time) ~ I(1e+05/(8.617*(Temp + 273.16))), data)
> 
> Where Degrad/Time = k, and because of taking logs,
> the intercept = log(A)
> and the coefficient of the term = -Ea.
> 
> This is how I used the Arrhenius formule in a linear model.
> I know I can use predict to estimate my failure times, with
> confidence intervals, but I'm not sure wether I should use
> least square regression (assuming a normal distribution) or
> a MLE with a Weibull distrbution.
> And if I should use MLE with Weibull, I dont know how to
> implement it in R.

There is no law of the Universe which requires that failure
times follow a Weibull (or other) distribution.

The use of the Arrhenius formula suggests that the failure
mechanism in your application is the result of a progressive
chemical reaction, which ultimately reaches a certain level.

Random variations in the time taken to reach a fixed level
will depend on random fluctutations in the rate of reaction,
and then there will be the variations from item to item in
the level at which items fail. You will know more about how
these things behave than any of the rest of us!

Nevertheless, I would suggest fitting your model

  mod <- lm(Y ~ X)

where

  Y = log(Degrad/Time)
  X = I(1e+05/(8.617*(Temp + 273.16)))

and then looking at the residuals from the fit:

a) As a histogram of residuals: hist(mod$res)

b) As a plot of residuals against fitted values:

     plot(mod$fit, mod$res)

If (a) looks approximately normal, and (b) does not suggest
that there is a strong systematic trend in the size of
residuals relative to the fitted values, then I think I would
be satisfied to treat the problem as adequately represented
by a least squares regression with normally distributed
residuals.

If not, then it's back to you to consider how the underlying
mechanisms behave, anf then how to represent them in a model;
or to infer what you can from the results of (a) and (b).

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Feb-07                                       Time: 18:41:41
------------------------------ XFMail ------------------------------


From hao.liu at bms.com  Fri Feb 16 21:52:47 2007
From: hao.liu at bms.com (Hao Liu)
Date: Fri, 16 Feb 2007 15:52:47 -0500
Subject: [R] R GUI programming
Message-ID: <45D6199F.9080304@bms.com>

Hi, All:

I am having a problem with handling global variable value in GUI 
programming, I hope R gurus can give me some advice on it.

What I want to do is to read in a dataset, display some information 
based on the input and do some calculation on the dataset:
However,  I am having trouble organizing my code, as the following code 
goes,  the comboBox section will have to be put
in the loaddataset function, otherwise, since comboBox is displayed upon 
GUI initialization, later changes to the .dataset won't be reflected in 
the display.
--
dataset.header <- tclVar("header information")
 .dataset <- 0

loaddataset <- function(.dataset) {           # this function is bind to 
a tkbutton
         file <- tclvalue(tkgetOpenFile())
          if (!length(file)) return()
          tclvalue(dataset.file)<-file
          .dataset <- read.csv(file,header=TRUE)   # read in .CSV file 
to R object
          cat(colnames(.dataset))
          tclvalue(dataset.header) <- colnames(.dataset)
          cat(paste("after assign: ", tclvalue(dataset.header), sep=""))
          tclvalue(output.dir) <- tclfile.dir(file)
          tkmessageBox(title="Setting Output Directory", 
message=paste("Setting output directory to ", tclvalue(output.dir), 
sep=" "), icon="info",
          type="ok")
          #cat(paste("\"The column names are: ", colnames(dataset), 
"\"\n", sep=""))
          #cat(paste("\n the dimension is: ", dim(.dataset), "\n", sep=""))
      #headers <- paste("\"", colnames(.dataset), "\"", sep="")   
      # parameters derived from input data....
    dataset.label <- tklabel(boxplotcanvas, text="Select variable from 
dataset: ", font=font1)
    comboBox <- 
tkwidget(boxplotcanvas,"ComboBox",editable=FALSE,values=tclvalue(dataset.header))
    tkgrid(dataset.label, comboBox)
    tkgrid.configure(dataset.label, sticky="w")
    tkgrid.configure(comboBox, sticky="e")
}
---
The same situation for if I want to plot the readin dataset, since I 
want to bind the plot function to a tkbutton, I still have not find a 
way to pass the readin .dataset over to the plot
function.

Thanks
Hao


From preuth at slf.ch  Fri Feb 16 21:53:39 2007
From: preuth at slf.ch (preuth at slf.ch)
Date: Fri, 16 Feb 2007 21:53:39 +0100
Subject: [R] Ubuntu Linux and X11
Message-ID: <1171659219.45d619d39d5bc@it.slf.ch>

Hello,

i have a problem with creating histograms and plots under ubuntu linux.
After creating a vector "test" i want to make a histogram, but the following
error appears:
> hist (test)
Error in X11() : could not find any X11 fonts
Check that the Font Path is correct.


Does anybody know this problem and a solution for this?

Thanks in advance,
Thomas


From gh at bu.edu  Fri Feb 16 22:18:51 2007
From: gh at bu.edu (Greg Howard)
Date: Fri, 16 Feb 2007 16:18:51 -0500
Subject: [R] contour doesn't obey cex.axis?
Message-ID: <37d837350702161318l23c968dek8a64e8e2f3e7d33b@mail.gmail.com>

I can't seem to control the size of the numeric labels for my contour
plots.  I am using "cex.axis", which works with plot():

this makes the tick mark labels very large
plot( 1:3, 1:3, cex.axis=2.0, )

but this doesn't change them:
contour( 1:3, 1:3, array( data=0:9, dim=c(3,3) ), cex.axis=2.0, )

Is there a replacement for cex.axis in contour()?

thanks,


greg


From mark.lyman at gmail.com  Fri Feb 16 22:36:58 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Fri, 16 Feb 2007 21:36:58 +0000 (UTC)
Subject: [R] sapply again return value
References: <45D57F4B.7040304@yahoo.de>
Message-ID: <loom.20070216T221047-641@post.gmane.org>

Antje <niederlein-rstat <at> yahoo.de> writes:

> Hello,
> 
> I used an sapply to get some data back (s <- sapply(...) ). The output 
> of s would then deliver something like this:
> 
>       B06_lamp.csv C06_lamp.csv D06_lamp.csv
> [1,] NULL         NULL         Numeric,512
> [2,] NULL         NULL         Numeric,512
> [3,] NULL         NULL         2
>  > mode(s)
> [1] "list"
>  > dim(s)
> [1] 3 3
>  >
> 
> Now, I'd like to remove the columns which contain NULL (it's alway the 
> whole column).
> How can I do this???
> 
> Antje
 

As long as it is always the whole column, you can just test the first row, 
like this:
> s<-matrix(list(NULL,NULL,NULL,NULL,NULL,NULL,numeric(512),numeric
(512),2),ncol=3)
> s[,!apply(s,2,sapply,is.null)[1,],drop=FALSE]
     [,1]       
[1,] Numeric,512
[2,] Numeric,512
[3,] 2

If you would like to make sure that the whole column is NULL, you could do 
something like the following:

> s[,!apply(apply(s,2,sapply,is.null),2,sum)==nrow(s),drop=FALSE]
     [,1]       
[1,] Numeric,512
[2,] Numeric,512
[3,] 2

I use the "drop=FALSE" here for display purposes, but you may want to leave it 
off depending on desired format.

Mark


From gsp05jm at sheffield.ac.uk  Fri Feb 16 23:40:59 2007
From: gsp05jm at sheffield.ac.uk (Jon Minton)
Date: Fri, 16 Feb 2007 22:40:59 -0000
Subject: [R] missing -> nonmissing levels
In-Reply-To: <Pine.LNX.4.64.0702161833320.15470@gannet.stats.ox.ac.uk>
References: <001c01c751da$f20dcea0$d6296be0$@ac.uk>
	<Pine.LNX.4.64.0702161833320.15470@gannet.stats.ox.ac.uk>
Message-ID: <000601c7521b$87df6e30$979e4a90$@ac.uk>



-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: 16 February 2007 18:40
To: Jon Minton
Cc: r-help at stat.math.ethz.ch; 'Jon Minton'
Subject: Re: [R] missing -> nonmissing levels

Well, if it is missing, how do you know what level to turn it back into?
That is what NA means in R: not available.

If you want missings to be a separate level, you could use

> factor(as.character(X), exclude=NULL)
[1] A    <NA> B
Levels: A B <NA>

BTW, using summary() for a length-3 object is not helpful, and please send 
properly formatted text emails as the posting guide does ask of you.

On Fri, 16 Feb 2007, Jon Minton wrote:

> Hi,
>
> I expect this is simple but haven?t found an answer looking on the
> archives...
>
>
>
> I want to convert ?NA? (missing) to particular levels (nonmissing) in
factor
> vectors.
>
>
>
> e.g. I know
>
>> X <- c(1, 2, 3)
>
>> summary(X)
>
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>
>    1.0     1.5     2.0     2.0     2.5     3.0
>
>> X <- as.factor(X)
>
>> summary(X)
>
> 1 2 3
>
> 1 1 1
>
>> levels(X)
>
> [1] "1" "2" "3"
>
>> levels(X) <- c("A", NA, "B")
>
>> summary(X)
>
>   A    B NA's
>
>   1    1    1
>
>
>
> But what if I want to turn the NA back into a level?
>
> How do I do this?
>
>
>
> Thanks,  Jon
>
>
>
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-- 
No virus found in this incoming message.


17:40
 

-- 



17:40


From edd at debian.org  Sat Feb 17 00:05:35 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 16 Feb 2007 17:05:35 -0600
Subject: [R] Ubuntu Linux and X11
In-Reply-To: <1171659219.45d619d39d5bc@it.slf.ch>
References: <1171659219.45d619d39d5bc@it.slf.ch>
Message-ID: <20070216230535.GA4748@eddelbuettel.com>

On Fri, Feb 16, 2007 at 09:53:39PM +0100, preuth at slf.ch wrote:
> i have a problem with creating histograms and plots under ubuntu linux.
> After creating a vector "test" i want to make a histogram, but the following
> error appears:
> > hist (test)
> Error in X11() : could not find any X11 fonts
> Check that the Font Path is correct.
> 
> 
> Does anybody know this problem and a solution for this?

Hm, you didn't by chance built this yourself and thereby ignored the
BIG FAT warnings about x11 headers not being found ?   

Take a shortcut, install the (backported) Ubuntu packages from any
of the CRAN mirrors and then sit back and enjoy a nice hist() chart?

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From Gautam.Bhola at pfizer.com  Sat Feb 17 00:20:36 2007
From: Gautam.Bhola at pfizer.com (Bhola, Gautam)
Date: Fri, 16 Feb 2007 18:20:36 -0500
Subject: [R] Error while trying to update packages in 32-bit R-2.3.1 on Linux
Message-ID: <C583FADA8CD7464CAC20E27A8155521801DDCCF4@groamrexm05.amer.pfizer.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070216/2ff55975/attachment.pl 

From cberry at tajo.ucsd.edu  Sat Feb 17 01:08:12 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 16 Feb 2007 16:08:12 -0800
Subject: [R] Migrating .RData from Windows to Linux?
In-Reply-To: <45D5CE76.4050408@lungall.gu.se>
References: <45D5CE76.4050408@lungall.gu.se>
Message-ID: <Pine.LNX.4.64.0702161558120.19330@tajo.ucsd.edu>

On Fri, 16 Feb 2007, Derek Eder wrote:

> I want to begin a migration of R workspaces from Windows to Ubuntu Linux.
>
> (1)  Can someone suggest the most appropriate path?
>      dump()  ?
>      save()  ?
>
> (2)  Are there any R platform migration related resources "out there"?
>      (I have not found any yet).


It helps to READ THE DOCUMENTATION!!!!

>From ?save:

 	All R platforms use the XDR representation of binary objects in
 	binary save-d files, and these are portable across all R
 	platforms. (ASCII saves used to be useful for moving data between
 	platforms but are now only of historical interest.)

>From ?dump

 	A dump file can be sourced into another R (or perhaps S) session,
 	but the function save is designed to be used for transporting R
 	data, and will work with R objects that dump does not handle.


Using R, I work in WindowsXP, Gentoo (64bit) and RedHat (32bit) Linuxes, 
and - every so often - MAC OS X. And have worked under Solaris and Windows 
98.

I routinely port the files produced by save() on one system to another 
system.  I cannot remember ever having had a problem doing this.

Looks like the documentation was correct! ;-)

>
> Thank you,
>
>
> -- 
> Derek N. Eder
>
> Gothenburg University
> VINKLA - Vigilance and Neurocognition laboratory
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From hpbenton at scripps.edu  Sat Feb 17 01:26:06 2007
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Fri, 16 Feb 2007 16:26:06 -0800
Subject: [R] reading text file not table
Message-ID: <45D64B9E.2070708@scripps.edu>

Hello all,

I'm looking for a way to be able to read a text file into R. It's a csv
file but when I do
"txt <-read.table("F00.csv", header=T, sep=",")" It doesn't read the
file properly, and I only get 2 columns. If I open it up in OOc or Excel
it open right with 7 columns.
What I would really like to do is read the file as text and then split
it and read the bottom section where the 7 columns are. Then I would
re-read the table with read.table.

    Thank you for any help,

    Paul

-- 
Research Technician
Mass Spectrometry
   o The
  /
o Scripps
  \
   o Research
  /
o Institute


From ted.harding at nessie.mcc.ac.uk  Sat Feb 17 01:39:41 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 17 Feb 2007 00:39:41 -0000 (GMT)
Subject: [R] reading text file not table
In-Reply-To: <45D64B9E.2070708@scripps.edu>
Message-ID: <XFMail.070217003941.ted.harding@nessie.mcc.ac.uk>

On 17-Feb-07 H. Paul Benton wrote:
> Hello all,
> 
> I'm looking for a way to be able to read a text file into R.
> It's a csv file but when I do
> "txt <-read.table("F00.csv", header=T, sep=",")"
> It doesn't read the file properly, and I only get 2 columns.
> If I open it up in OOc or Excel it open right with 7 columns.
> What I would really like to do is read the file as text and
> then split it and read the bottom section where the 7 columns are.
> Then I would re-read the table with read.table.
> 
>     Thank you for any help,
> 
>     Paul

What's wrong with using read.csv? (Or have I misunderstood
your query?)

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 17-Feb-07                                       Time: 00:39:37
------------------------------ XFMail ------------------------------


From jm540 at york.ac.uk  Sat Feb 17 01:13:54 2007
From: jm540 at york.ac.uk (Jon Minton)
Date: Sat, 17 Feb 2007 00:13:54 -0000
Subject: [R] -> using eval(parse(text= ... ))
In-Reply-To: <Pine.LNX.4.64.0702161833320.15470@gannet.stats.ox.ac.uk>
References: <001c01c751da$f20dcea0$d6296be0$@ac.uk>
	<Pine.LNX.4.64.0702161833320.15470@gannet.stats.ox.ac.uk>
Message-ID: <001801c75228$82a3a280$87eae780$@ac.uk>

Hi, 

I'm unsure why I can't use eval(parse(text=...)) to represent the thing I
want to assign a value into.

e.g.

> A <- c("Good")
> B <- c("Bad")
> C <- c()
> X <- c("A", "B", "C")
> eval(parse(text=X[1]))
[1] "Good"
> eval(parse(text=X[2])) -> Z
> Z
[1] "Bad"
> eval(parse(text=X[3])) <- "Ugly"
Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file 'C', reason 'No such file or directory'  

Why the error message?

I've tried looking through the archive for a solution but, perhaps because
I'm not too sure how to phrase this problem, I've not found one...

-- 



17:40


From murdoch at stats.uwo.ca  Sat Feb 17 01:46:39 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 16 Feb 2007 19:46:39 -0500
Subject: [R] contour doesn't obey cex.axis?
In-Reply-To: <37d837350702161318l23c968dek8a64e8e2f3e7d33b@mail.gmail.com>
References: <37d837350702161318l23c968dek8a64e8e2f3e7d33b@mail.gmail.com>
Message-ID: <45D6506F.1090503@stats.uwo.ca>

On 2/16/2007 4:18 PM, Greg Howard wrote:
> I can't seem to control the size of the numeric labels for my contour
> plots.  I am using "cex.axis", which works with plot():
> 
> this makes the tick mark labels very large
> plot( 1:3, 1:3, cex.axis=2.0, )
> 
> but this doesn't change them:
> contour( 1:3, 1:3, array( data=0:9, dim=c(3,3) ), cex.axis=2.0, )
> 
> Is there a replacement for cex.axis in contour()?

No, but you can always draw the contour plot with axes=FALSE, and then 
add whatever axes you like.

Duncan Murdoch


From regetz at nceas.ucsb.edu  Sat Feb 17 02:44:28 2007
From: regetz at nceas.ucsb.edu (Jim Regetz)
Date: Fri, 16 Feb 2007 17:44:28 -0800
Subject: [R] convert to binary to decimal
In-Reply-To: <47c7c59e0702160753m544dd961q485f7eb6fb31d7ed@mail.gmail.com>
References: <002301c75121$ca6425c0$4d908980@gne.windows.gene.com>	<47c7c59e0702150913l1be48edbmd480e5f4e83e9452@mail.gmail.com>	<45D56912.7771.2FEB94@localhost>
	<47c7c59e0702160753m544dd961q485f7eb6fb31d7ed@mail.gmail.com>
Message-ID: <45D65DFC.7020400@nceas.ucsb.edu>

Roland Rau wrote:
> On 2/16/07, Petr Pikal <petr.pikal at precheza.cz> wrote:
>> Hi
>>
>> slight modification of your function can be probably even quicker:
>>
>> fff<-function(x) sum(2^(which(rev(x))-1))
>> :-)
>> Petr
>>
>>
> Yes, your function is slightly but consistently faster than my suggestion.
> But my "tests" show still Bert Gunter's function to be far ahead of the
> rest.
> 

Mere trifling at this point, but here's a tweak that yields slightly
faster performance on my system (with a caveat):

# Bert's original function
bert.gunter <- function(x) {
  sum(x * 2^(rev(seq_along(x)) - 1))
}

# A slightly modified function
dead.horse <- function(x) {
  sum( 2^(rev(seq_along(x))-1)[x] )
}

set.seed(1)
huge.list <- replicate(20000,
         sample(c(TRUE,FALSE), 20, replace=TRUE), simplify=FALSE)
horse.time <- replicate(15, system.time(lapply(huge.list, dead.horse)))
bert.time <- replicate(15, system.time(lapply(huge.list, bert.gunter)))


# Print mean times (exclude first 2 to improve consistency)
> rowMeans(bert.time[, -(1:2)])
[1] 0.618600 0.000867 0.621000 0.000000 0.000000
> rowMeans(horse.time[, -(1:2)])
[1] 0.580286 0.000571 0.582143 0.000000 0.000000

Hope no one comes along to beat this function ;-)

Incidentally, I generated huge.list by randomly sampling TRUE and FALSE
values with equal probability. I believe this matches what Roland did,
and it seems quite reasonable given that the vectors are meant to
represent binary numbers. But FWIW, as the vectors get more densely
populated with TRUE values, dead.horse() loses its advantage. In the
limit, if all values are TRUE, Bert's multiplication is slightly faster
than logical indexing.

Fun on a Friday...

Cheers,
Jim

------------------------------
James Regetz, Ph.D.
Scientific Programmer/Analyst
National Center for Ecological Analysis & Synthesis
735 State St, Suite 300
Santa Barbara, CA 93101


From murdoch at stats.uwo.ca  Sat Feb 17 03:03:38 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 16 Feb 2007 21:03:38 -0500
Subject: [R] -> using eval(parse(text= ... ))
In-Reply-To: <001801c75228$82a3a280$87eae780$@ac.uk>
References: <001c01c751da$f20dcea0$d6296be0$@ac.uk>	<Pine.LNX.4.64.0702161833320.15470@gannet.stats.ox.ac.uk>
	<001801c75228$82a3a280$87eae780$@ac.uk>
Message-ID: <45D6627A.90506@stats.uwo.ca>

On 2/16/2007 7:13 PM, Jon Minton wrote:
> Hi, 
> 
> I'm unsure why I can't use eval(parse(text=...)) to represent the thing I
> want to assign a value into.
> 
> e.g.
> 
>> A <- c("Good")
>> B <- c("Bad")
>> C <- c()
>> X <- c("A", "B", "C")
>> eval(parse(text=X[1]))
> [1] "Good"
>> eval(parse(text=X[2])) -> Z
>> Z
> [1] "Bad"
>> eval(parse(text=X[3])) <- "Ugly"
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'C', reason 'No such file or directory'  
> 
> Why the error message?
> 
> I've tried looking through the archive for a solution but, perhaps because
> I'm not too sure how to phrase this problem, I've not found one...

eval(parse(text=X[3])) is asking R to evaluate the expression C, which 
doesn't exist yet.  The expression

C <- "Ugly"

doesn't evaluate C first (because C doesn't exist yet).  It evaluates a 
call to "<-" with C as one of the arguments, and "<-" treats that 
argument as a name.

There are several things that would work to do what you want, but the 
most straightforward is

assign(X[3], "Ugly")

Duncan Murdoch


From liy12 at mskcc.org  Sat Feb 17 06:21:04 2007
From: liy12 at mskcc.org (Yuelin Li)
Date: Sat, 17 Feb 2007 00:21:04 -0500
Subject: [R] Ubuntu Linux and X11
In-Reply-To: <20070216230535.GA4748@eddelbuettel.com>
References: <1171659219.45d619d39d5bc@it.slf.ch>
	<20070216230535.GA4748@eddelbuettel.com>
Message-ID: <20070217052104.GC2144@jdmlab.mskcc.org>

If you are using edgy, try changing your FontPath settings in
/etc/X11/xorg.conf (mine listed below).

Reboot, and check to see there are no warnings about missing font
files in /var/log/Xorg.0.log.

Yuelin.



Section "Files"
        FontPath        "/usr/share/fonts/X11/100dpi/:unscaled"
        FontPath        "/usr/share/fonts/X11/75dpi/:unscaled"
        FontPath        "/usr/share/fonts/X11/Type1"
        FontPath        "/usr/share/fonts/X11/100dpi"
        FontPath        "/usr/share/fonts/X11/75dpi"
        FontPath        "/usr/share/fonts/X11/TTF"
        FontPath        "/usr/share/fonts/X11/OTF"
        FontPath        "/usr/share/fonts/X11/CID"
        FontPath        "/usr/share/fonts/X11/misc"
        # path to defoma fonts
        FontPath	"/var/lib/defoma/x-ttcidfont-conf.d/dirs/TrueType"
EndSection


On Fri, Feb 16, 2007 at 05:05:35PM -0600, Dirk Eddelbuettel wrote:
   On Fri, Feb 16, 2007 at 09:53:39PM +0100, preuth at slf.ch wrote:
   > i have a problem with creating histograms and plots under ubuntu linux.
   > After creating a vector "test" i want to make a histogram, but the following
   > error appears:
   > > hist (test)
   > Error in X11() : could not find any X11 fonts
   > Check that the Font Path is correct.
   > 
   > 
   > Does anybody know this problem and a solution for this?
   
   Hm, you didn't by chance built this yourself and thereby ignored the
   BIG FAT warnings about x11 headers not being found ?   
   
   Take a shortcut, install the (backported) Ubuntu packages from any
   of the CRAN mirrors and then sit back and enjoy a nice hist() chart?
   
   Dirk
   
   -- 
   Hell, there are no rules here - we're trying to accomplish something. 
                                                     -- Thomas A. Edison
   
   ______________________________________________
   R-help at stat.math.ethz.ch mailing list
   https://stat.ethz.ch/mailman/listinfo/r-help
   PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
   and provide commented, minimal, self-contained, reproducible code.
   

 
     =====================================================================
     
     Please note that this e-mail and any files transmitted with it may be 
     privileged, confidential, and protected from disclosure under 
     applicable law. If the reader of this message is not the intended 
     recipient, or an employee or agent responsible for delivering this 
     message to the intended recipient, you are hereby notified that any 
     reading, dissemination, distribution, copying, or other use of this 
     communication or any of its attachments is strictly prohibited.  If 
     you have received this communication in error, please notify the 
     sender immediately by replying to this message and deleting this 
     message, any attachments, and all copies and backups from your 
     computer.


From grahamleask at btopenworld.com  Sat Feb 17 08:15:33 2007
From: grahamleask at btopenworld.com (GRAHAM LEASK)
Date: Sat, 17 Feb 2007 07:15:33 +0000 (GMT)
Subject: [R] help with cluster stopping rules
Message-ID: <438930.419.qm@web86209.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070217/bc958724/attachment.pl 

From jim at bitwrit.com.au  Sat Feb 17 11:20:08 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 17 Feb 2007 21:20:08 +1100
Subject: [R] something missing in summary()
Message-ID: <45D6D6D8.8090408@bitwrit.com.au>

Gerard Smits wrote:
 >
 > just noticed that two key pieces of information are not given by
 > the summary() command:  N and SD.  we are given the N missing, but
 > not the converse.  I know these summary value can be obtained easy,
 > but can't understand why these two pieces of information are not
 > provided with the other info.
 >
This was one reason that I wrote the describe function in the prettyR 
package. You can roll your own summary, and describe makes a reasonable 
attempt to sort out the common data types.

Jim


From pburns at pburns.seanet.com  Sat Feb 17 11:18:31 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 17 Feb 2007 10:18:31 +0000
Subject: [R] reading text file not table
In-Reply-To: <45D64B9E.2070708@scripps.edu>
References: <45D64B9E.2070708@scripps.edu>
Message-ID: <45D6D677.8090405@pburns.seanet.com>

'count.fields' is often useful in such situations to see how
R's view of the file differs from your own.  (It isn't such
a rare occurrence for differences to happen when the file
comes from Excel.)

If I understand properly, you can use

sep='\n'

as part of your alternative plan.  But I don't think you would
need to write a file and read it back in again.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


H. Paul Benton wrote:

>Hello all,
>
>I'm looking for a way to be able to read a text file into R. It's a csv
>file but when I do
>"txt <-read.table("F00.csv", header=T, sep=",")" It doesn't read the
>file properly, and I only get 2 columns. If I open it up in OOc or Excel
>it open right with 7 columns.
>What I would really like to do is read the file as text and then split
>it and read the bottom section where the 7 columns are. Then I would
>re-read the table with read.table.
>
>    Thank you for any help,
>
>    Paul
>
>  
>


From osklyar at ebi.ac.uk  Sat Feb 17 12:23:00 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Sat, 17 Feb 2007 11:23:00 +0000
Subject: [R] Ubuntu Linux and X11
In-Reply-To: <20070217052104.GC2144@jdmlab.mskcc.org>
References: <1171659219.45d619d39d5bc@it.slf.ch>	<20070216230535.GA4748@eddelbuettel.com>
	<20070217052104.GC2144@jdmlab.mskcc.org>
Message-ID: <45D6E594.1020706@ebi.ac.uk>

The problem occurs after updating from Dapper to Edgy. Dapper had font 
paths: /usr/share/X11/fonts and Edgy, to make the whole font system 
unified, moved X11 fonts to /usr/share/fonts/X11. Oleg

Yuelin Li wrote:
> If you are using edgy, try changing your FontPath settings in
> /etc/X11/xorg.conf (mine listed below).
> 
> Reboot, and check to see there are no warnings about missing font
> files in /var/log/Xorg.0.log.
> 
> Yuelin.
> 
> 
> 
> Section "Files"
>         FontPath        "/usr/share/fonts/X11/100dpi/:unscaled"
>         FontPath        "/usr/share/fonts/X11/75dpi/:unscaled"
>         FontPath        "/usr/share/fonts/X11/Type1"
>         FontPath        "/usr/share/fonts/X11/100dpi"
>         FontPath        "/usr/share/fonts/X11/75dpi"
>         FontPath        "/usr/share/fonts/X11/TTF"
>         FontPath        "/usr/share/fonts/X11/OTF"
>         FontPath        "/usr/share/fonts/X11/CID"
>         FontPath        "/usr/share/fonts/X11/misc"
>         # path to defoma fonts
>         FontPath	"/var/lib/defoma/x-ttcidfont-conf.d/dirs/TrueType"
> EndSection
> 
> 
> On Fri, Feb 16, 2007 at 05:05:35PM -0600, Dirk Eddelbuettel wrote:
>    On Fri, Feb 16, 2007 at 09:53:39PM +0100, preuth at slf.ch wrote:
>    > i have a problem with creating histograms and plots under ubuntu linux.
>    > After creating a vector "test" i want to make a histogram, but the following
>    > error appears:
>    > > hist (test)
>    > Error in X11() : could not find any X11 fonts
>    > Check that the Font Path is correct.
>    > 
>    > 
>    > Does anybody know this problem and a solution for this?
>    
>    Hm, you didn't by chance built this yourself and thereby ignored the
>    BIG FAT warnings about x11 headers not being found ?   
>    
>    Take a shortcut, install the (backported) Ubuntu packages from any
>    of the CRAN mirrors and then sit back and enjoy a nice hist() chart?
>    
>    Dirk
>    
>    -- 
>    Hell, there are no rules here - we're trying to accomplish something. 
>                                                      -- Thomas A. Edison
>    
>    ______________________________________________
>    R-help at stat.math.ethz.ch mailing list
>    https://stat.ethz.ch/mailman/listinfo/r-help
>    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>    and provide commented, minimal, self-contained, reproducible code.
>    
> 
>  
>      =====================================================================
>      
>      Please note that this e-mail and any files transmitted with it may be 
>      privileged, confidential, and protected from disclosure under 
>      applicable law. If the reader of this message is not the intended 
>      recipient, or an employee or agent responsible for delivering this 
>      message to the intended recipient, you are hereby notified that any 
>      reading, dissemination, distribution, copying, or other use of this 
>      communication or any of its attachments is strictly prohibited.  If 
>      you have received this communication in error, please notify the 
>      sender immediately by replying to this message and deleting this 
>      message, any attachments, and all copies and backups from your 
>      computer.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


From arun.kumar.saha at gmail.com  Sat Feb 17 12:50:11 2007
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Sat, 17 Feb 2007 17:20:11 +0530
Subject: [R] Cumulant
Message-ID: <d4c57560702170350x1c02c675sa2b5703e9a117f57@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070217/246480d9/attachment.pl 

From IngoBuech at web.de  Sat Feb 17 13:10:21 2007
From: IngoBuech at web.de (=?iso-8859-15?Q?Ingo_B=FCch?=)
Date: Sat, 17 Feb 2007 13:10:21 +0100
Subject: [R] seasonal adjustment
Message-ID: <839709903@web.de>


Are any seasonal adjustment programs, like Tramo/Seats, Census X12 ARIMA or Berliner Verfahren implemented in R? I am doing a simulation study and I don't know how to adjust the series in R. 

The possibility to access external the exe.files of the seasonal adjustment programs seems to be quite difficult.   
 
Can anyone help me?

Thanks,

Ingo


From fisk at bowdoin.edu  Sat Feb 17 14:17:20 2007
From: fisk at bowdoin.edu (steve)
Date: Sat, 17 Feb 2007 08:17:20 -0500
Subject: [R] bystats2 question
Message-ID: <er6v8l$kjb$1@sea.gmane.org>

I get a curious error using bystats2 that I can not track down.

I have two variables that are converted from dates
and one numeric variable:

 > str(mon)
  Ord.factor w/ 12 levels "Jan"<"Feb"<"Mar"<..: 1 1 1 1 1 1 1 1 2 2 ...
 > str(yea)
  Ord.factor w/ 4 levels "2004"<"2005"<..: 3 3 3 3 3 3 3 3 3 3 ...
 > str(amount)
  num [1:1103] -44 -10 -17 -80 -20 -50 -58 -14 -28 -46 ...

library(Hmisc)

  date = dates(as.character(Posted.Date))
  month = months(date)
  year = years(date)


yields the  error

 > bystats2(amount,month,year)
Error: object "n" not found

thanks for any suggestions!

Steve


From osklyar at ebi.ac.uk  Sat Feb 17 15:29:11 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Sat, 17 Feb 2007 14:29:11 +0000
Subject: [R] Ubuntu Linux and X11
In-Reply-To: <17879.3952.466908.391801@basebud.nulle.part>
References: <1171659219.45d619d39d5bc@it.slf.ch>	<20070216230535.GA4748@eddelbuettel.com>	<20070217052104.GC2144@jdmlab.mskcc.org>	<45D6E594.1020706@ebi.ac.uk>
	<17879.3952.466908.391801@basebud.nulle.part>
Message-ID: <45D71137.9070200@ebi.ac.uk>

> Good point.  But on my systems , the upgrade only affected Emacs which
> 'looked ugly' but still worked.  In other words, nothing "failed" after the
After I updated to Edgy 'convert' of ImageMagick stopped working as well 
as its 'display' API command and also nedit stopped functioning -- all 
because of X11 fonts. And in fact, I now recall that I had for while no 
test in my R plots, I thought some R options were set wrongly. But 
correcting the paths in xorg.conf solved all these issues.

Oleg


-- 
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


From preuth at slf.ch  Sat Feb 17 15:43:25 2007
From: preuth at slf.ch (preuth at slf.ch)
Date: Sat, 17 Feb 2007 15:43:25 +0100
Subject: [R] Ubuntu Linux and X11
In-Reply-To: <45D71137.9070200@ebi.ac.uk>
References: <1171659219.45d619d39d5bc@it.slf.ch>
	<20070216230535.GA4748@eddelbuettel.com>
	<20070217052104.GC2144@jdmlab.mskcc.org>
	<45D6E594.1020706@ebi.ac.uk>
	<17879.3952.466908.391801@basebud.nulle.part>
	<45D71137.9070200@ebi.ac.uk>
Message-ID: <1171723405.45d7148d3ced7@it.slf.ch>

Well after installing a debian package of the cran mirors and reconfiguning the
xorg.conf as yuelin li suggested the hist() and other graphical commands now
work properly.
Thanks to the fruitful contributions.
Thomas

Quoting Oleg Sklyar <osklyar at ebi.ac.uk>:

> > Good point.  But on my systems , the upgrade only affected Emacs which
> > 'looked ugly' but still worked.  In other words, nothing "failed" after
> the
> After I updated to Edgy 'convert' of ImageMagick stopped working as well 
> as its 'display' API command and also nedit stopped functioning -- all 
> because of X11 fonts. And in fact, I now recall that I had for while no 
> test in my R plots, I thought some R options were set wrongly. But 
> correcting the paths in xorg.conf solved all these issues.
> 
> Oleg
> 
> 
> -- 
> Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466
>


From nilsson.henric at gmail.com  Sat Feb 17 16:34:42 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Sat, 17 Feb 2007 16:34:42 +0100
Subject: [R] R implementations of scatterplot/map labeling algorithims?
In-Reply-To: <45D5DFDF.80107@yorku.ca>
References: <45D5DFDF.80107@yorku.ca>
Message-ID: <45D72092.3000404@gmail.com>

Den 2007-02-16 17:46, Michael Friendly skrev:
> Dear R community
> 
> In a current paper, I'm (briefly) considering the topic of producing
> scatterplots or maps with point labels positioned in such a way as to 
> minimize label overlap and occlusion.  This is a topic with a large, but 
> scattered literature. In CS, it is considered NP-hard, but there are
> a variety of approximate solutions.  The most complete bibliography I've 
> found is
> the Map-Labeling Bibliography,
> http://liinwww.ira.uka.de/bibliography/Theory/map.labeling.html
> 
> AFAIK, the only concrete and published implementation is a Fortran
> program published by Noma (below), and then adapted by Warren Kuhfeld
> at SAS in PROC PLOT, and used in his %plotit macro.
> 
>      Journal Title  - Psychometrika
>      Article Title  - Heuristic method for label placement in scatterplots
>      Volume  - Volume 52
>      Issue  - 3
>      First Page  - 463
>      Last Page  - 468
>      Issue Cover Date  - 1987-09-27
>      Author  - Elliot Noma
>      DOI  - 10.1007/BF02294366
>      Link  - http://www.springerlink.com/content/c4k6205r83156165
> 
> Does anyone know of a related R (or other) public implementation of a 
> solution to this problem?

Take a look at `thigmophobe.labels' in the `plotrix' package.


HTH,
Henric



> 
> thanks,
> -Michael
>


From dieter.menne at menne-biomed.de  Sat Feb 17 16:48:02 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 17 Feb 2007 16:48:02 +0100
Subject: [R] fit.contrast (package gmodels) and lmer
Message-ID: <LPEJLJACLINDNMBMFAFIMEEFCHAA.dieter.menne@menne-biomed.de>

Friends of lme,

following the documentation of fit.contrast in package gmodels, it should be
possible to use it with lmer. At least with sim.lmer=TRUE and lmer(not 2),
because the latter has no mcmcsamp yet.

I could not get this to work. UseR error?

------------------
library(gmodels)
library(lme4)
options(show.signif.stars = FALSE)
data(ergoStool)
fm1 <- lmer(effort ~ Type + (1|Subject), ergoStool)
mcmcsamp(fm1) ## works

fit.contrast(fm1,"Type",c(1,1,0,0),sim.lmer=TRUE)
# Not an applicable method


------------------
Note: I am well aware of the discussion about df/lmer in

http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76742.html

so I don't expect the method to work with sim.lmer=FALSE. And yes, the
example is simple and results can be obtained directly. However, I looking
for an easy-to-use general contrast test, and fit.contrast and estimable are
the only ones I know of that work rather generally.

Dieter


From ripley at stats.ox.ac.uk  Sat Feb 17 18:19:27 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 17 Feb 2007 17:19:27 +0000 (GMT)
Subject: [R] reading text file not table
In-Reply-To: <45D6D677.8090405@pburns.seanet.com>
References: <45D64B9E.2070708@scripps.edu> <45D6D677.8090405@pburns.seanet.com>
Message-ID: <Pine.LNX.4.64.0702171712360.32039@gannet.stats.ox.ac.uk>

I think he is missing fill=TRUE, which is the default for read.csv but not
read.table.  (As Ted Harding implied but as I recall did not spell out.)

If you want to read a file as text, used readLines.  You can then extract 
the lines you want and use read.table on a textConnection from just those 
lines.

On Sat, 17 Feb 2007, Patrick Burns wrote:

> 'count.fields' is often useful in such situations to see how
> R's view of the file differs from your own.  (It isn't such
> a rare occurrence for differences to happen when the file
> comes from Excel.)
>
> If I understand properly, you can use
>
> sep='\n'
>
> as part of your alternative plan.  But I don't think you would
> need to write a file and read it back in again.
>
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
>
> H. Paul Benton wrote:
>
>> Hello all,
>>
>> I'm looking for a way to be able to read a text file into R. It's a csv
>> file but when I do
>> "txt <-read.table("F00.csv", header=T, sep=",")" It doesn't read the
>> file properly, and I only get 2 columns. If I open it up in OOc or Excel
>> it open right with 7 columns.
>> What I would really like to do is read the file as text and then split
>> it and read the bottom section where the 7 columns are. Then I would
>> re-read the table with read.table.
>>
>>    Thank you for any help,
>>
>>    Paul
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From nilsson.henric at gmail.com  Sat Feb 17 19:06:06 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Sat, 17 Feb 2007 19:06:06 +0100
Subject: [R] help with cluster stopping rules
In-Reply-To: <438930.419.qm@web86209.mail.ird.yahoo.com>
References: <438930.419.qm@web86209.mail.ird.yahoo.com>
Message-ID: <45D7440E.6030201@gmail.com>

Den 2007-02-17 08:15, GRAHAM LEASK skrev:
> Is there a function available in R that implements Mojena's Upper
> Tail Rule or that draws a Mojena plot?
> 
> I would also like to find a function that implements Duda and Hart's
> stopping rule.
> 
> Finally with function cophenet how can I achieve a straightforward
> Cophenet correlation coefficient (i.e. one number for example 0.876)

I'm neither aware of the function `cophenet' (and RSiteSearch couldn't 
find it) nor the ``...Cophenet correlation coefficient...''.

> that gives the agreement between the structure of the data as
> interpreted by the clustering method and the original data?

 From this description, you're probably looking for the cophenet*ic* 
correlation. `cophenetic' (in package `stats' that ships with R) 
computes cophenetic distances, and from these and the original distances 
it's straighforward to get what you want using `cor'. See `?cophenetic' 
for examples.


HTH,
Henric



> 
> If anyone could let me know where I can find a script file for these
> functions I would greatly appreciate it.
> 
> 
> Kind regards
> 
> 
> Dr Graham Leask Economics and Strategy Group Aston Business School 
> Aston University Aston Triangle Birmingham B4 7ET
> 
> Tel: Direct line 0121 204 3150 email g.leask at aston.ac.uk 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.
>


From dickgiesser at gmail.com  Sat Feb 17 19:19:20 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Sat, 17 Feb 2007 18:19:20 +0000
Subject: [R] Constraint maximum (likelihood) using nlm
Message-ID: <b75d67340702171019m38e327a0l6f9949770548a6b5@mail.gmail.com>

Hi,

I'm trying to find the maximum (likelihood) of a function. Therefore,
I'm trying to minimize the negative likelihood function:

# params: vector containing values of mu and sigma
# params[1] - mu, params[2]- sigma
# dat: matrix of data pairs y_i and s_i
# dat[,1] - column of y_i , dat[,2] column of s_i
negll <- function(params,dat,constant=0)
{
	for(i in 1:length(dat[,1]))
	{
		llsum <- log( params[2]^2 + dat[i,2]^2) +
		(( dat[i,1] - params[1])^2/ (params[2]^2 + dat[i,2]^2))
	}
	ll <- -0.5 * llsum  + constant
	return(-ll)
}

Using (find data attached):
data.osl <- read.table("osl.dat",header=TRUE)
data.matrix <- as.matrix(data.frame(data.osl$de,data.osl$se))
nlm(negll,c(0.75,0.5),dat=data.matrix,iterlim=200)

I get estimates for mu and sigma of:
 3.629998e+00 -4.975368e-07

However, sigma obviously has to be >= 0.

Therefore I am trying to transform sigma:

negll.trans <- function(params,...)
{
	params[2] <- log(params[2])
	negll(params,...)

}

where
nlm(negll.trans,c(0.75,0.5),dat=data.matrix)
give me the estimates
3.63 1.00
i.e. mu = 3.63 and sigma = exp(1)

I am not confident that this is the correct answer looking at the graphs:
par(mfrow=c(2,1))
plot(osl$de,osl$se,xlab="equivalent dose",ylab="standard errors",
     main="Equivalent Dose vs. Standard Errors",xlim=c(0,4))
hist(osl$de,xlab="equivalent dose",
     main="Histogram of Equivalent Dose",xlim=c(0,4))

I want to stick with using nlm to minimize the function but can't find
an error in what I am doing.

I'd appreciate your help!

Thank you
Ben

From johannes_graumann at web.de  Sat Feb 17 17:34:20 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Sat, 17 Feb 2007 17:34:20 +0100
Subject: [R] Conditional data frame manipulation
Message-ID: <er7agd$gk8$1@sea.gmane.org>

Hi all,

My current project brought forth the snippet below, which modifies in each
row of a data frame a certain field depending on another field in the same
row. Dealing with data of some 30000 entries this works, but is horribly
slow. Can anyone show this newbie how to do this properly (faster ;0)?

for (i in 1:nrow(dataframe)){
  if (any(grep('^yes$',dataframe[i,][['Field1']]))){
    dataframe[i,]['Field1'] <- dataframe[i,]['Field2']
  } else {
    dataframe[i,]['Field1'] <- NA
  }
}

Thanks for your insights, Joh


From edd at debian.org  Sat Feb 17 15:21:36 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 17 Feb 2007 08:21:36 -0600
Subject: [R] Ubuntu Linux and X11
In-Reply-To: <45D6E594.1020706@ebi.ac.uk>
References: <1171659219.45d619d39d5bc@it.slf.ch>
	<20070216230535.GA4748@eddelbuettel.com>
	<20070217052104.GC2144@jdmlab.mskcc.org>
	<45D6E594.1020706@ebi.ac.uk>
Message-ID: <17879.3952.466908.391801@basebud.nulle.part>


On 17 February 2007 at 11:23, Oleg Sklyar wrote:
| The problem occurs after updating from Dapper to Edgy. Dapper had font 
| paths: /usr/share/X11/fonts and Edgy, to make the whole font system 
| unified, moved X11 fonts to /usr/share/fonts/X11. Oleg

Good point.  But on my systems , the upgrade only affected Emacs which
'looked ugly' but still worked.  In other words, nothing "failed" after the
Dapper / Edgy upgrade. In particular, R -- which I use day-in and day-out on
Ubuntu at work -- worked just fine.  So I am still puzzled as to exactly how
one can manage to get R so scrippled on Ubuntu that graphics fail...

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From marc_schwartz at comcast.net  Sat Feb 17 19:52:27 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 17 Feb 2007 12:52:27 -0600
Subject: [R] Conditional data frame manipulation
In-Reply-To: <er7agd$gk8$1@sea.gmane.org>
References: <er7agd$gk8$1@sea.gmane.org>
Message-ID: <1171738347.10256.14.camel@localhost.localdomain>

On Sat, 2007-02-17 at 17:34 +0100, Johannes Graumann wrote:
> Hi all,
> 
> My current project brought forth the snippet below, which modifies in each
> row of a data frame a certain field depending on another field in the same
> row. Dealing with data of some 30000 entries this works, but is horribly
> slow. Can anyone show this newbie how to do this properly (faster ;0)?
> 
> for (i in 1:nrow(dataframe)){
>   if (any(grep('^yes$',dataframe[i,][['Field1']]))){
>     dataframe[i,]['Field1'] <- dataframe[i,]['Field2']
>   } else {
>     dataframe[i,]['Field1'] <- NA
>   }
> }
> 
> Thanks for your insights, Joh

Beyond the for() loop issue. you are doing a lot of unnecessary
subsetting.

For example:

  dataframe[i,][['Field1']]

can be replaced with:

  dataframe[['Field1']]

or if you have to loop:

  dataframe[i, 'Field1']


See ?Extract

One clarification question on your use of grep(), which is do you have
entries that have a 'yes' at the end of the field, or are you just
looking for a field entry != 'yes'?  If the latter, you don't need to
use grep() of course.

One potential approach is the following:

  dataframe[["Field1"]] <- with(dataframe, 
                                ifelse(any(grep("^yes$", Field1)), 
                                       Field2, NA))

If you are just looking for an entry != "yes", then:

  dataframe[["Field1"]] <- with(dataframe, 
                                ifelse(Field1 != "yes", 
                                       Field2, NA))

See ?ifelse and ?with.  Also look at ?replace for an alternative way to
replace() values based upon conditions.

HTH,

Marc Schwartz


From dickgiesser at gmail.com  Sat Feb 17 20:00:44 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Sat, 17 Feb 2007 19:00:44 +0000
Subject: [R] Constraint maximum (likelihood) using nlm
In-Reply-To: <b75d67340702171019m38e327a0l6f9949770548a6b5@mail.gmail.com>
References: <b75d67340702171019m38e327a0l6f9949770548a6b5@mail.gmail.com>
Message-ID: <b75d67340702171100u227ef0f0o1a0893a1bcf5ba3b@mail.gmail.com>

Nevermind, I had a small mistake in the negll function.

Thank you anyways.

On 2/17/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> Hi,
>
> I'm trying to find the maximum (likelihood) of a function. Therefore,
> I'm trying to minimize the negative likelihood function:
>
> # params: vector containing values of mu and sigma
> # params[1] - mu, params[2]- sigma
> # dat: matrix of data pairs y_i and s_i
> # dat[,1] - column of y_i , dat[,2] column of s_i
> negll <- function(params,dat,constant=0)
> {
>         for(i in 1:length(dat[,1]))
>         {
>                 llsum <- log( params[2]^2 + dat[i,2]^2) +
>                 (( dat[i,1] - params[1])^2/ (params[2]^2 + dat[i,2]^2))
>         }
>         ll <- -0.5 * llsum  + constant
>         return(-ll)
> }
>
> Using (find data attached):
> data.osl <- read.table("osl.dat",header=TRUE)
> data.matrix <- as.matrix(data.frame(data.osl$de,data.osl$se))
> nlm(negll,c(0.75,0.5),dat=data.matrix,iterlim=200)
>
> I get estimates for mu and sigma of:
>  3.629998e+00 -4.975368e-07
>
> However, sigma obviously has to be >= 0.
>
> Therefore I am trying to transform sigma:
>
> negll.trans <- function(params,...)
> {
>         params[2] <- log(params[2])
>         negll(params,...)
>
> }
>
> where
> nlm(negll.trans,c(0.75,0.5),dat=data.matrix)
> give me the estimates
> 3.63 1.00
> i.e. mu = 3.63 and sigma = exp(1)
>
> I am not confident that this is the correct answer looking at the graphs:
> par(mfrow=c(2,1))
> plot(osl$de,osl$se,xlab="equivalent dose",ylab="standard errors",
>      main="Equivalent Dose vs. Standard Errors",xlim=c(0,4))
> hist(osl$de,xlab="equivalent dose",
>      main="Histogram of Equivalent Dose",xlim=c(0,4))
>
> I want to stick with using nlm to minimize the function but can't find
> an error in what I am doing.
>
> I'd appreciate your help!
>
> Thank you
> Ben
>
>


From hassanysabbah at hotmail.com  Sat Feb 17 20:18:58 2007
From: hassanysabbah at hotmail.com (Harry Ho)
Date: Sat, 17 Feb 2007 19:18:58 +0000
Subject: [R]  Solve in maximum likelihood estimation
Message-ID: <BAY107-F289FA01A304534A4DFF710AB940@phx.gbl>

Hi,

I got the following problem.

I am doing a maximum likelihood estimation for a Kalman Filter.

For this purpose, I have to invert an error matrix Ffast of dimension
"no. parameters X no.parameters". The usualy optim methods often find only 
local minima, so I decided to make the optimization using the SANN 
algorithm, which is very slow already.

However, this becomes a real problem because the "reciprocal condition 
number" of Ffast becomes extremely small (up to 1e-1000 for the amount of 
data I have) for non-sensible paramter combinations.
Thus, I need to extend the tolerance of the solve algorithm to this level 
(tol=1e-1000), which means that calculations can become incredibly slow for 
many parameters.$

Is there a way to address this issue? I could imagine replacing the matrix 
by a dummy matrix each time I get the singularity error, hoping that the 
program recognizes the implausibility. Is there a function that allows doing 
so?

Another way might be, as the temperature decreases, to decrease the 
tolerance as well, as estimates become more reasonable (would have to verify 
that).
How could that work?

I hope this characterization is enough. I still provided the code used, just 
in case somebody bothers to take a closer look at it performance wise.

It certainly has a lot of bad programming style though.


Nevertheless, thanks a lot for all replies.

Remarks: n is very small as to keep the code working in an acceptable time 
frame.
Up to the +-line, the data are generated. After that, the Kalman Filter 
estimates them.
I have restricted this estimation to 4 parameters, the true values are given 
for the other ones.
The estimation results in this form are pretty bad, but with more 
observations, maturities and above all computation they get quite good.









set.seed(13234)

n           <- 20
matur       <- c(3,5,12)


no.state            <- 2
no.obs              <- length(matur)

Xav                 <- matrix(  nrow=no.state,ncol=n)
FF <- FF.kal        <- matrix(  nrow=no.state,ncol=no.state)

GG <- GG.kal        <- matrix(  nrow=no.obs,ncol=no.state)

V                   <- matrix(0,nrow=no.state)
W                   <- matrix(0,nrow=no.obs)

SigV <- SigV.kal    <- matrix(0,ncol=no.state,nrow=no.state)
SigW <- SigW.kal    <- matrix(0,nrow=no.obs,ncol=no.obs)

x                   <- rep(0,n)
A <- B  <- G.B      <- numeric(matur[no.obs])
Z                   <- matrix(ncol=n,nrow=no.obs)


kappa       <- 0.97
theta       <- 0.07
sigma       <- 0.005
lambda      <- -0.162
beta        <- sigma*lambda
rho         <- 0.5*sigma^2*lambda^2

Q           <- sigma^2
R           <- sigma^2*0.125



rho         <- 0.5*lambda^2*sigma^2


Xav[1,]               <- rep(1,n)

FF[1,] <- FF.kal[1,]<- c(1,rep(0,no.state-1))
FF[2,]              <- c(theta-theta*kappa,kappa)

diag(SigW)          <- R
SigV[2,2]           <- Q

for (i in 1:matur[length(matur)]){
B[i] <- (1-kappa^(i-1))/(1-kappa)
G.B[i] <- rho+B[i]*theta*(1-kappa)-0.5*beta^2-B[i]*beta*sigma-B[i]^2*sigma^2
}
A <- cumsum(G.B)

GG[1:no.obs,]    <- c(A[matur],B[matur])/matur


X.0         <- c(1,theta)


for (i in 1:n)
                                                {
                                                    V[,1] <- 
c(0,rnorm(no.state-1,0,sqrt(diag(SigV)[2:no.state])))
                                                    W[,1] <- 
rnorm(no.obs,0,sqrt(diag(SigW)))

                                                    if (i==1) Xav[,i]   <- 
FF%*%X.0         +       V
                                                    if (i>1)  Xav[,i]   <- 
FF%*%Xav[,i-1]     +       V
                                                              Z[,i]     <- 
GG%*%Xav[,i]       +       W
                                                }

#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Omega       <- matrix(0,nrow=2,ncol=2)

F <- array(dim=c(no.obs,no.obs,n))
Fminus <- matrix(ncol=no.obs,nrow=no.obs)

Ffast <- matrix(ncol=no.obs,nrow=no.obs)

P <- Pminus <- array(dim=c(no.state,no.state,n))
Pfast <- Pfastmin <- matrix(NA,ncol=no.state,nrow=no.state)

K <- array(dim=c(no.state,no.obs,n))
Kfast <- matrix(nrow=no.state,ncol=no.obs)
X <- Xhatminus <- array(dim=c(no.state,1,n))
Xfast <- Xfastmin <- matrix(ncol=1,nrow=no.state)


v <- array(dim=c(no.obs,1,n))
vfast <- matrix(nrow=no.obs,ncol=1)
Y <- matrix(ncol=1,nrow=no.obs)

log.lik <- numeric(n)

Rprof()

kalman <- function(a)
                                                    {

                                                    #(a <- startwerte)

                                                    theta.kal <- a[1]
                                                    kappa.kal <- a[2]
                                                    sigma.kal <- a[3]

                                                    x.kal     <- theta
                                                    Omega.kal <- sqrt(Q)
                                                    beta.kal  <- beta

                                                    Omega[2,2]      <- 
Omega.kal
                                                    Xhat            <- 
c(1,x.kal)
                                                    FF.kal[2,]      <- 
c(theta.kal-theta.kal*kappa.kal,kappa.kal)
                                                    SigV.kal[2,2]   <- 
sigma.kal^2
                                                    diag(SigW.kal)  <- 
a[4]^2

                                                    for (i in 
1:matur[length(matur)]){
                                                    B[i] <- 
(1-kappa.kal^(i-1))/(1-kappa.kal)
                                                    G.B[i] <- 
rho+B[i]*theta.kal*(1-kappa.kal)-0.5*beta.kal^2-B[i]*beta.kal*sigma.kal-B[i]^2*sigma.kal^2
                                                    }
                                                    A <- cumsum(G.B)

                                                    GG.kal[1:no.obs,]    <- 
c(A[matur],B[matur])/matur

                                                    for(i in 1:n){
                                                                  if 
(i==1){Xhat            <- c(1,x.kal); Omega[2,2] <- Q}else{Xhat  <- 
c(1,Xfast[2,1]);Omega[2,2] <- Pfast[2,2]}
                                                                  (Y[,1] <- 
Z[,i])
                                                                  (Xfastmin 
<- FF.kal%*%Xhat)
                                                                  (Pfastmin 
<- FF.kal%*%Omega%*%t(FF.kal)+SigV.kal )
                                                                  (Ffast <- 
GG.kal%*%Pfastmin%*%t(GG.kal)+SigW.kal)

                                                                  Fminus <- 
solve(Ffast,tol=1e-40)



                                                                  (Kfast <- 
Pfastmin%*%t(GG.kal)%*%Fminus)
                                                                  (vfast <- 
Y-GG.kal%*%Xfastmin)
                                                                  (Xfast <- 
Xfastmin+Kfast%*%vfast)
                                                                  (Pfast <- 
abs(Pfastmin-Kfast%*%GG.kal%*%Pfastmin))
                                                                  log.lik[i] 
<- -1/2*log(2*pi)-0.5*log(det(Ffast))-0.5*t(vfast)%*%Fminus%*%vfast
                                                                  }
                                                    sum(-log.lik)

                                                    }

kalman(c(theta,kappa,sqrt(Q),sqrt(R)))

startwerte <- c(theta,kappa,sqrt(Q),sqrt(R))
optim(startwerte*0.9,kalman,method="SANN")
c(theta,kappa,sqrt(Q),sqrt(R))
#nlmestimate$estimate
Rprof(NULL)
gamma2 <- summaryRprof()

_________________________________________________________________
Haben Spinnen Ohren? Finden Sie es heraus ? mit dem MSN Suche Superquiz via


From mail at xesoftware.com.au  Sat Feb 17 21:10:40 2007
From: mail at xesoftware.com.au (stephenc)
Date: Sun, 18 Feb 2007 07:10:40 +1100
Subject: [R] ripper
Message-ID: <007101c752cf$ba6b71a0$6401a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070218/4fe5e60e/attachment.pl 

From frainj at gmail.com  Sun Feb 18 01:29:52 2007
From: frainj at gmail.com (John C Frain)
Date: Sun, 18 Feb 2007 00:29:52 +0000
Subject: [R] seasonal adjustment
In-Reply-To: <839709903@web.de>
References: <839709903@web.de>
Message-ID: <fad888a10702171629q40998e41meb09866a861b3748@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070218/60f2bd24/attachment.pl 

From neuro3000 at hotmail.com  Sun Feb 18 02:06:24 2007
From: neuro3000 at hotmail.com (=?iso-8859-1?Q?Neuro_LeSuperH=E9ros?=)
Date: Sat, 17 Feb 2007 20:06:24 -0500
Subject: [R] Solve in maximum likelihood estimation
Message-ID: <BAY131-W2A9FB0B8139F1CCD0CE17AF8B0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070217/49512b11/attachment.pl 

From hb at stat.berkeley.edu  Sun Feb 18 02:14:45 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Sat, 17 Feb 2007 17:14:45 -0800
Subject: [R] convert to binary to decimal
In-Reply-To: <45D65DFC.7020400@nceas.ucsb.edu>
References: <002301c75121$ca6425c0$4d908980@gne.windows.gene.com>
	<47c7c59e0702150913l1be48edbmd480e5f4e83e9452@mail.gmail.com>
	<45D56912.7771.2FEB94@localhost>
	<47c7c59e0702160753m544dd961q485f7eb6fb31d7ed@mail.gmail.com>
	<45D65DFC.7020400@nceas.ucsb.edu>
Message-ID: <59d7961d0702171714vb4129bcu68c0a781f0ded234@mail.gmail.com>

On 2/16/07, Jim Regetz <regetz at nceas.ucsb.edu> wrote:
> Roland Rau wrote:
> > On 2/16/07, Petr Pikal <petr.pikal at precheza.cz> wrote:
> >> Hi
> >>
> >> slight modification of your function can be probably even quicker:
> >>
> >> fff<-function(x) sum(2^(which(rev(x))-1))
> >> :-)
> >> Petr
> >>
> >>
> > Yes, your function is slightly but consistently faster than my suggestion.
> > But my "tests" show still Bert Gunter's function to be far ahead of the
> > rest.
> >
>
> Mere trifling at this point, but here's a tweak that yields slightly
> faster performance on my system (with a caveat):
>
> # Bert's original function
> bert.gunter <- function(x) {
>   sum(x * 2^(rev(seq_along(x)) - 1))
> }
>
> # A slightly modified function
> dead.horse <- function(x) {
>   sum( 2^(rev(seq_along(x))-1)[x] )
> }
>
> set.seed(1)
> huge.list <- replicate(20000,
>          sample(c(TRUE,FALSE), 20, replace=TRUE), simplify=FALSE)
> horse.time <- replicate(15, system.time(lapply(huge.list, dead.horse)))
> bert.time <- replicate(15, system.time(lapply(huge.list, bert.gunter)))
>
>
> # Print mean times (exclude first 2 to improve consistency)
> > rowMeans(bert.time[, -(1:2)])
> [1] 0.618600 0.000867 0.621000 0.000000 0.000000
> > rowMeans(horse.time[, -(1:2)])
> [1] 0.580286 0.000571 0.582143 0.000000 0.000000
>
> Hope no one comes along to beat this function ;-)

Why not? ;)

jumpy.roo <- function(x) { sum(2^.subset((length(x)-1):0, x)) }

horse.time <- replicate(15, system.time(lapply(huge.list, dead.horse)))
bert.time <- replicate(15, system.time(lapply(huge.list, bert.gunter)))
roo.time <- replicate(15, system.time(lapply(huge.list, jumpy.roo)))

> rowMeans(bert.time[, -(1:2)])
[1] 0.6169231 0.0000000 0.6176923        NA        NA
> rowMeans(horse.time[, -(1:2)])
[1] 0.604615385 0.001538462 0.603846154          NA          NA
> rowMeans(roo.time[, -(1:2)])
[1] 0.2030769 0.0000000 0.2092308        NA        NA

You can push it further if you allow:

jumpy.redroo <- function(x) .Internal(sum(2^.subset((length(x)-1):0, x)))

redroo.time <- replicate(15, system.time(lapply(huge.list, jumpy.redroo)))
> rowMeans(redroo.time[, -(1:2)])
[1] 0.1653846 0.0000000 0.1646154        NA        NA

which is 3-4 faster than "bert.gunter".

Cheers

Henrik


> Incidentally, I generated huge.list by randomly sampling TRUE and FALSE
> values with equal probability. I believe this matches what Roland did,
> and it seems quite reasonable given that the vectors are meant to
> represent binary numbers. But FWIW, as the vectors get more densely
> populated with TRUE values, dead.horse() loses its advantage. In the
> limit, if all values are TRUE, Bert's multiplication is slightly faster
> than logical indexing.
>
> Fun on a Friday...
>
> Cheers,
> Jim
>
> ------------------------------
> James Regetz, Ph.D.
> Scientific Programmer/Analyst
> National Center for Ecological Analysis & Synthesis
> 735 State St, Suite 300
> Santa Barbara, CA 93101
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From nilsson.henric at gmail.com  Sun Feb 18 02:21:06 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Sun, 18 Feb 2007 02:21:06 +0100
Subject: [R] ripper
In-Reply-To: <007101c752cf$ba6b71a0$6401a8c0@tablet>
References: <007101c752cf$ba6b71a0$6401a8c0@tablet>
Message-ID: <45D7AA02.1080202@gmail.com>

Den 2007-02-17 21:10, stephenc skrev:
> Is there some decision tree method available with R, like ripper, that ends
> up producing a list of the rules and can be used for prediction?

Yes, there are several.

For some guidance, I suggest that you check out the `Recursive 
Partitioning' section of the `Machine Learning' CRAN Task View 
(http://cran.r-project.org/src/contrib/Views/MachineLearning.html).


HTH,
Henric



> 
>  
> 
> Stephen Choularton
> 
> 02 9999 2226
> 
> 0413 545 182
> 
>  
> 
>  
> 
>


From maitra at iastate.edu  Sun Feb 18 03:23:55 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Sat, 17 Feb 2007 20:23:55 -0600
Subject: [R] applying lm on an array of observations with common design
	matrix
Message-ID: <20070217202355.0ee7c70c@triveni.stat.iastate.edu>

Dear list,

I have a 4-dimensional array Y of dimension 330 x 67 x 35 x 51. I have a design matrix X of dimension 330 x 4. I want to fit a linear regression of each

lm( Y[, i, j, k] ~ X). for each i, j, k.

Can I do it in one shot without a loop?

Actually, I am also interested in getting the p-values of some of the coefficients -- lets say the coefficient corresponding to the second column of the design matrix. Can the same be done using array-based operations?

I would be happy to clarify if anything is unclear.

Many thanks and best wishes,
Ranjan


From ripley at stats.ox.ac.uk  Sun Feb 18 08:46:56 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Feb 2007 07:46:56 +0000 (GMT)
Subject: [R] applying lm on an array of observations with common design
 matrix
In-Reply-To: <20070217202355.0ee7c70c@triveni.stat.iastate.edu>
References: <20070217202355.0ee7c70c@triveni.stat.iastate.edu>
Message-ID: <Pine.LNX.4.64.0702180736050.10281@gannet.stats.ox.ac.uk>

On Sat, 17 Feb 2007, Ranjan Maitra wrote:

> Dear list,
>
> I have a 4-dimensional array Y of dimension 330 x 67 x 35 x 51. I have a 
> design matrix X of dimension 330 x 4. I want to fit a linear regression 
> of each
>
> lm( Y[, i, j, k] ~ X). for each i, j, k.
>
> Can I do it in one shot without a loop?

Yes.

YY <- YY
dim(YY) <- c(330, 67*35*51)
fit <- lm(YY ~ X)

> Actually, I am also interested in getting the p-values of some of the 
> coefficients -- lets say the coefficient corresponding to the second 
> column of the design matrix. Can the same be done using array-based 
> operations?

Use lapply(summary(fit), function(x) coef(x)[3,4])  (since there is a 
intercept, you want the third coefficient).

Note that this will give a vector, so set its dimension to c(67,35,51) to 
relate to the original array.

I have not BTW looked into the memory requirements here, and you might 
want to do this on slices of the array for that reason.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tim_b_mcdonald at yahoo.com  Sun Feb 18 09:44:41 2007
From: tim_b_mcdonald at yahoo.com (Tim McDonald)
Date: Sun, 18 Feb 2007 00:44:41 -0800 (PST)
Subject: [R] list of data frame objects
Message-ID: <449791.81207.qm@web58210.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070218/7b347c0c/attachment.pl 

From Dimitris.Rizopoulos at med.kuleuven.be  Sun Feb 18 10:36:20 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sun, 18 Feb 2007 10:36:20 +0100
Subject: [R] list of data frame objects
In-Reply-To: <449791.81207.qm@web58210.mail.re3.yahoo.com>
References: <449791.81207.qm@web58210.mail.re3.yahoo.com>
Message-ID: <20070218103620.bu348oxge54wcgs0@webmail5.kuleuven.be>

try something like the following (untested):

objs <- ls()
sapply(objs, function(obj) inherits(get(obj), "data.frame"))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Tim McDonald <tim_b_mcdonald at yahoo.com>:

> Hi Folks,
>
> I need to extract the list of all my data frame objects. With     
> objects() I can list all objects and was hoping to use something   
> like the following:
>
> objects()[is.data.frame(objects())] to extracts all my objects that   
> are data frame...
>
> What am I doing wrong?
>
> Thanks - Tim
>
>
> ---------------------------------
> Any questions?  Get answers on any topic at Yahoo! Answers. Try it now.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From dickgiesser at gmail.com  Sun Feb 18 13:17:48 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Sun, 18 Feb 2007 12:17:48 +0000
Subject: [R] Help with pair plot
Message-ID: <b75d67340702180417n6fd93f4dg25a958d0022f218e@mail.gmail.com>

Hi,

I'm trying to create a plot using pair.

Currently I'm doing the following

pairs(~mpg + hp + wt , data=cars, labels = c("Miles per
Gallon","Horsepower","Weight"),pch = c(24,25)[unclass(cars$tr + 1)],
bg = c("red", "green3")[unclass(cars$tr + 1)],panel=panel.smooth)

for the attached dataset. However,instead of using panel.smooth I
would rather have two fitted regression lines for each car
transmission type (cars$tr).
Is this possible?

Thank you
Ben

From dickgiesser at gmail.com  Sun Feb 18 13:41:56 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Sun, 18 Feb 2007 12:41:56 +0000
Subject: [R] Help with pair plot
In-Reply-To: <b75d67340702180417n6fd93f4dg25a958d0022f218e@mail.gmail.com>
References: <b75d67340702180417n6fd93f4dg25a958d0022f218e@mail.gmail.com>
Message-ID: <b75d67340702180441pf3c73b7wdcae34610289380a@mail.gmail.com>

I forgot to add that I figured how to do a single regression line using:

panel.fitline<-function(x, y, digits=2, prefix="", cex.cor, ...)
{
 points(x, y, ...)
 reg <- coef(lm(y ~ x))
 abline(coef=reg,untf=F,col="blue")
}

and adding: panel=panel.fitline to pairs. But I am not sure how to add
two lines differentiated by cars$tr.

Thank you for your help!
Ben


On 2/18/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> Hi,
>
> I'm trying to create a plot using pair.
>
> Currently I'm doing the following
>
> pairs(~mpg + hp + wt , data=cars, labels = c("Miles per
> Gallon","Horsepower","Weight"),pch = c(24,25)[unclass(cars$tr + 1)],
> bg = c("red", "green3")[unclass(cars$tr + 1)],panel=panel.smooth)
>
> for the attached dataset. However,instead of using panel.smooth I
> would rather have two fitted regression lines for each car
> transmission type (cars$tr).
> Is this possible?
>
> Thank you
> Ben
>
>


From janek0 at poczta.onet.pl  Sun Feb 18 14:34:18 2007
From: janek0 at poczta.onet.pl (janek0)
Date: Sun, 18 Feb 2007 14:34:18 +0100
Subject: [R] dbi, rodbc, rmysql, charset problem
Message-ID: <1171805658.9489.44.camel@376325>

Dear List

In my short life as a beginning R-user i've encountered a following
problem that i'm unable to solve myself:

I have a database in MySQL containing table and field names as well as
some data containing Polish accentuated characters (like ????),
utf8-encoded. It works just fine with just any external query browser i
can find, jdbc, odbc, native, whatever. Also mysql is happy about my
charset. It also seems to be configured correctly: 

mysql> status
--------------
mysql  Ver 14.12 Distrib 5.0.24a, for pc-linux-gnu (i486) using readline
5.1

Connection id:          16
Current database:
Current user:           root w localhost
SSL:                    Not in use
Current pager:          stdout
Using outfile:          ''
Using delimiter:        ;
Server version:         5.0.24a-Debian_9-log
Protocol version:       10
Connection:             Localhost via UNIX socket
Server characterset:    utf8
Db     characterset:    utf8
Client characterset:    utf8
Conn.  characterset:    utf8
UNIX socket:            /var/run/mysqld/mysqld.sock
Uptime:                 1 hour 37 min 17 sec

Threads: 2  Questions: 240  Slow queries: 0  Opens: 175  Flush tables: 1
Open tables: 64  Queries per second avg: 0.041

Yet if i use R's RODBC or RMySQL to connect to my database i can't see
these accentuated characters:

library(RODBC)
con <-odbcConnect("trybunal", uid="root", pwd="mypassword")
sqlTable(con)

the output (abridged) is like that:

TABLE_CAT TABLE_SCHEM                    TABLE_NAME TABLE_TYPE
REMARKS
14  trybunal             Wyk?adnia innych przepis<f3>w      TABLE MySQL 

instead of <F3> i should see "?" and instead of ? a "?".

It is just the same if i use RMySQL instead of RODBC:

library(RMySQL)
con <-dbConnect(dbDriver("MySQL"), dbname="trybunal", username="root",
password="mypassword")
dbListTables(con)

the output (abridged) is like that:
[13] "Ustawa"                        "Wyk?adnia innych przepis<f3>w"

and if i use
dbReadTable(con, "Metryczka")

("Metryczka" being one table in the database) i get:

Error in make.names(as.character(names), allow_) : 
        invalid multibyte string 11

It works without error if i set LC_ALL to "C", but obviously without
Polish charset.

Strange thing is that Sys.getlocale() gives me

[1]"LC_CTYPE=pl_PL.UTF-8;LC_NUMERIC=C;LC_TIME=pl_PL.UTF-8;LC_COLLATE=pl_PL.UTF-8;
LC_MONETARY=pl_PL.UTF-8;LC_MESSAGES=pl_PL.UTF-8;
LC_PAPER=C;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=C;LC_IDENTIFICATION=C"

So it is utf8 all over the place. Also R works just fine with
read.table() if the table contains utf8-encoded chars. Thus the problem
is just with R-mysql connection. It seems therefore that dbi package
does not support non-ascii charsets.

Questions:
1. Is above conclusion correct or am i doing something wrong ?
2. If it is correct, is there any way to use table and field names as
they are now (with non-ascii chars) in my SQL queries (e.g. SELECT
`Wyk?adnia przedmiotu kontroli`.*) ? 

I can live with Polish characters missing in the output if i have to but
i must address the database fields/tables. I can't change their names of
as this would mean rebuilding database frontend. I can't import data to
R by exporting the database and then importing it via read.table because
i want a "live" application and must keep things simple.

Any help will be greatly appreciated.
-- 
janek0 <janek0 w poczta.onet.pl>


From brown_emu at yahoo.com  Sun Feb 18 15:36:50 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sun, 18 Feb 2007 06:36:50 -0800 (PST)
Subject: [R] Lattice graphics: minor tick marks and panel.axis() question(s)
Message-ID: <331126.86417.qm@web39701.mail.mud.yahoo.com>

Hello,

I have been a long-time Traditional graphics user and now moving to
try Lattice graphics. At the moment I cannot figure out how to place
minor tick marks in a figure made with Lattice's xyplot(). In
Traditional graphics, after calling plot() I would make two calls to
axis().

>From the manuals and help documentation I learned that 'scales' takes
multiple 'at' arguments only if they apply to each of the different
panels, so I thought to call 'panel.axis' in my panel
function. However I have a few questions regarding this procedure:

1) the placement of the ticks is not where I expect,

2) I cannot seem to get tick marks to point outwards even when I
set the clipping parameter in trellis parameters to "off"

3) (for multiple panels) I cannot figure out how to specify "bottom"
or "top" in each panel such that they appear on the side that the
default routine places major tickmarks

I have included example code at the bottom of this message.  I would
appreciate any help if these issues are intuitively obvious to an
experienced Lattice user. Thank you very much in advance!

Stephen Tucker [using Windows XP and R 2.4.0]

######### Example code #########

#+++++ Define example data +++++
# x = 0.1 to 10 uniformly spaced in lognormal(base10) scale
# y = log10(x) + random noise
# this is repeated for four groups: a,b,c,d
xpoints = c(rep(1:9,times=2)*10^rep(c(-1,0),each=9),10)
df = data.frame(x=rep(xpoints,times=4),
  y = rep(log10(xpoints),times=4)+rep(rnorm(length(xpoints),0,0.2),times=4),
  g = rep(rep(letters[1:4],each=length(xpoints)),times=4))
#+++++

#+++++ Set clipping parameter +++++
clip = trellis.par.get("clip")
clip$panel = "off"
trellis.par.set("clip",clip)
#+++++

#+++++ Define panel function +++++
# plot x,y points
# draw axis
myPanelFunction = function(x,y,...) {
  xpoints = c(rep(1:9,times=2)*10^rep(c(-1,0),each=9),10)  
  panel.xyplot(x,y,...)
  panel.axis("bottom",at=xpoints,labels=FALSE,
             tck=-0.5) #cannot get positive values of tck to work
}
#+++++

#+++++ Call to xyplot() +++++
xyplot(y~x|g,data=df,
       panel = myPanelFunction,
       scales=list(x=list(log=TRUE,at=c(0.1,1,10),
                     labels=parse(text=paste("10^",-1:1)))),
       index.cond=list(c(3,4,1,2)))
#+++++





 
____________________________________________________________________________________
The fish are biting.


From wuertz at itp.phys.ethz.ch  Sun Feb 18 15:58:12 2007
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 18 Feb 2007 15:58:12 +0100
Subject: [R] 1st International R/Rmetrics Workshop
Message-ID: <45D86984.3070904@itp.phys.ethz.ch>


The 1st International
R/Rmetrics User and Developer Workshop ...

will take place from July 8-12th, 2007
at Meielisalp, Lake Thune, Switzerland.
See www.rmetrics.org


The Workshop Focuses on ...

  > using R/Rmetrics as the premier
    open source solution for financial
    market analysis, valuation of
    financial instruments, and
    insurance tasks,
  > providing a platform for R/Rmetrics
    users to discuss and exchange
    ideas how R and Rmetrics can be
    used to do computations, data
    analysis, and visualization in
    finance and insurance,
  > giving an overview of the new
    features of the rapidly evolving
    R/Rmetrics project and discussing
    future developments.


The Program Consists of ...

  > presentations of new R/Rmetrics
    directions and developments
    through keynote lectures.
  > user-contributed presentations
    reflecting the wide range of
    fields in which R and Rmetrics
    are used in finance and insurance
    to analyze and model data.
  > bringing together developers,
    practitioners, and users from
    finance and insurance providing a
    platform for common discussions
    and exchange of ideas.


Organization ...

Rmetrics Foundation, co-organized by
Swiss Federal Institute of Technology,
    Zurich, and
University of Economics and Business
    Administration, Vienna.

    Web Site:   
        www.rmetrics.org
    Conference Chairs:
        Diethelm W?rtz, Swiss Federal
            Institute of Technology,
            Zurich
        Kurt Hornik, University of
            Economics and Business
            Administration, Vienna


From ripley at stats.ox.ac.uk  Sun Feb 18 16:09:37 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Feb 2007 15:09:37 +0000 (GMT)
Subject: [R] dbi, rodbc, rmysql, charset problem
In-Reply-To: <1171805658.9489.44.camel@376325>
References: <1171805658.9489.44.camel@376325>
Message-ID: <Pine.LNX.4.64.0702181500300.26658@auk.stats>

You seem never to have told R or us what charset these data are in. I 
think it is likely that they are being transferred in latin2 (like your 
email), and you are running R in UTF-8 according to Sys.getlocale.  So 
what you need to do is to either

1) Run R in latin2

or

2) use iconv() to convert the results from latin2 to UTF-8.

UTF-8 is relatively new in the DBMS world.  For ODBC, look at the bug 
reports on the MySQL site.  Using RODBC with UTF-8 locales is on my TODO 
list, but of no urgency at all.


On Sun, 18 Feb 2007, janek0 wrote:

> Dear List
>
> In my short life as a beginning R-user i've encountered a following
> problem that i'm unable to solve myself:
>
> I have a database in MySQL containing table and field names as well as
> some data containing Polish accentuated characters (like ????),
> utf8-encoded. It works just fine with just any external query browser i
> can find, jdbc, odbc, native, whatever. Also mysql is happy about my
> charset. It also seems to be configured correctly:
>
> mysql> status
> --------------
> mysql  Ver 14.12 Distrib 5.0.24a, for pc-linux-gnu (i486) using readline
> 5.1
>
> Connection id:          16
> Current database:
> Current user:           root at localhost
> SSL:                    Not in use
> Current pager:          stdout
> Using outfile:          ''
> Using delimiter:        ;
> Server version:         5.0.24a-Debian_9-log
> Protocol version:       10
> Connection:             Localhost via UNIX socket
> Server characterset:    utf8
> Db     characterset:    utf8
> Client characterset:    utf8
> Conn.  characterset:    utf8
> UNIX socket:            /var/run/mysqld/mysqld.sock
> Uptime:                 1 hour 37 min 17 sec
>
> Threads: 2  Questions: 240  Slow queries: 0  Opens: 175  Flush tables: 1
> Open tables: 64  Queries per second avg: 0.041
>
> Yet if i use R's RODBC or RMySQL to connect to my database i can't see
> these accentuated characters:
>
> library(RODBC)
> con <-odbcConnect("trybunal", uid="root", pwd="mypassword")
> sqlTable(con)
>
> the output (abridged) is like that:
>
> TABLE_CAT TABLE_SCHEM                    TABLE_NAME TABLE_TYPE
> REMARKS
> 14  trybunal             Wyk?adnia innych przepis<f3>w      TABLE MySQL
>
> instead of <F3> i should see "?" and instead of ? a "?".
>
> It is just the same if i use RMySQL instead of RODBC:
>
> library(RMySQL)
> con <-dbConnect(dbDriver("MySQL"), dbname="trybunal", username="root",
> password="mypassword")
> dbListTables(con)
>
> the output (abridged) is like that:
> [13] "Ustawa"                        "Wyk?adnia innych przepis<f3>w"
>
> and if i use
> dbReadTable(con, "Metryczka")
>
> ("Metryczka" being one table in the database) i get:
>
> Error in make.names(as.character(names), allow_) :
>        invalid multibyte string 11
>
> It works without error if i set LC_ALL to "C", but obviously without
> Polish charset.
>
> Strange thing is that Sys.getlocale() gives me
>
> [1]"LC_CTYPE=pl_PL.UTF-8;LC_NUMERIC=C;LC_TIME=pl_PL.UTF-8;LC_COLLATE=pl_PL.UTF-8;
> LC_MONETARY=pl_PL.UTF-8;LC_MESSAGES=pl_PL.UTF-8;
> LC_PAPER=C;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=C;LC_IDENTIFICATION=C"
>
> So it is utf8 all over the place. Also R works just fine with
> read.table() if the table contains utf8-encoded chars. Thus the problem
> is just with R-mysql connection. It seems therefore that dbi package
> does not support non-ascii charsets.
>
> Questions:
> 1. Is above conclusion correct or am i doing something wrong ?
> 2. If it is correct, is there any way to use table and field names as
> they are now (with non-ascii chars) in my SQL queries (e.g. SELECT
> `Wyk?adnia przedmiotu kontroli`.*) ?
>
> I can live with Polish characters missing in the output if i have to but
> i must address the database fields/tables. I can't change their names of
> as this would mean rebuilding database frontend. I can't import data to
> R by exporting the database and then importing it via read.table because
> i want a "live" application and must keep things simple.
>
> Any help will be greatly appreciated.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ivan.baxter at gmail.com  Sun Feb 18 16:38:46 2007
From: ivan.baxter at gmail.com (Ivan Baxter)
Date: Sun, 18 Feb 2007 10:38:46 -0500
Subject: [R] heatmap row cell size
Message-ID: <3b8dc6ad0702180738h16f8bc50k7521d54e873996fb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070218/2a91c394/attachment.pl 

From ivan.baxter at gmail.com  Sun Feb 18 17:02:03 2007
From: ivan.baxter at gmail.com (Ivan Baxter)
Date: Sun, 18 Feb 2007 11:02:03 -0500
Subject: [R] r-help@stat.math.ethz.ch
Message-ID: <3b8dc6ad0702180802n1428f25fi15413b7f76479d50@mail.gmail.com>

doh-trying again- this time remember text only.
Hello all-  I am having trouble with the cell sizes that heatmap
defaults too.  I have a matrix of 160 rows and 5 columns that I am
trying to display with heatmap(). When I do this, the cells default to
really wide and very short. This makes the labels for the rows very
hard to read, in fact the only way I can read them is to make the jpeg
output very large and zoom in, and even then it's hard to read and the
cells are so wide that  you can't see the dendrogram while seeing the
labels. I would like to try to have a tall, skinny heatmap with row
labels big enough to read.

I was able to get closer to what I want with this command...
jpeg(file = "view_heat.jpg",width = 4000, height = 6000)
heatmap(elmat,col= brewer.pal(9,"PuOr"), cexCol = .8, margin = c(.01,130)) #
dev.off()

but it only uses a quarter of the jpeg and it is still really hard to
see both the dendrogram on the left and read the labels on the right.
Is there a way I can set the actual width and height of the cells so I
can use the full size of the jpeg and read my labels?

thanks in advance

Ivan


From vinodkgul at yahoo.com  Sun Feb 18 18:14:56 2007
From: vinodkgul at yahoo.com (vinod gullu)
Date: Sun, 18 Feb 2007 09:14:56 -0800 (PST)
Subject: [R] Predict(); Warning rank deficient matrix
In-Reply-To: <mailman.7.1171796402.10893.r-help@stat.math.ethz.ch>
Message-ID: <20070218171456.66524.qmail@web53810.mail.yahoo.com>

I am trying to use lm() for resression followed by
stepAIC function. Now when i try to use to predict for
some input,  predict() gives  a warning : prediction
from a Rank deficient matrix may be misleading.  

As I am new to R (or to statistics) How alarming this
warning may be?

Regards,


 
____________________________________________________________________________________
The fish are biting.


From kubovy at virginia.edu  Sun Feb 18 19:30:46 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 18 Feb 2007 13:30:46 -0500
Subject: [R] Suppressing \newcolumntype{} declaration in latex.default() in
	Hmisc
Message-ID: <C73EB904-B4C0-43AA-BC17-1F3303921791@virginia.edu>

Dear r-helpers,

When use latex() on a matrix, I set the option dcolumn = T. As a  
result, in front of each tabular I get
\newcolumntype{.}{D{.}{.}{-1}}
The LaTeX compiler complains about these multiple redeclarations.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From sabinaburrascano at gmail.com  Sun Feb 18 20:09:06 2007
From: sabinaburrascano at gmail.com (sabat)
Date: Sun, 18 Feb 2007 11:09:06 -0800 (PST)
Subject: [R] principal components analysis
Message-ID: <9032578.post@talk.nabble.com>


Hi, just downloaded R. I need to run a simple Principal Components Analysis,
with a plot and the variables scores as output. Does anyone have some
scripts ready and easy to use...that anyone can understand...
Thank you
 
-- 
View this message in context: http://www.nabble.com/principal-components-analysis-tf3249342.html#a9032578
Sent from the R help mailing list archive at Nabble.com.


From bartjoosen at hotmail.com  Sun Feb 18 21:09:08 2007
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Sun, 18 Feb 2007 21:09:08 +0100
Subject: [R] printing intermediate lines while in a function
Message-ID: <BAY134-DAV7E1BCBB63655C9FD87673D88B0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070218/1d028b6f/attachment.pl 

From kubovy at virginia.edu  Sun Feb 18 21:17:58 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 18 Feb 2007 15:17:58 -0500
Subject: [R] Suppressing \newcolumntype{} declaration in latex.default()
	in Hmisc
In-Reply-To: <C73EB904-B4C0-43AA-BC17-1F3303921791@virginia.edu>
References: <C73EB904-B4C0-43AA-BC17-1F3303921791@virginia.edu>
Message-ID: <7B8A5C01-2645-4FD0-AB15-1715483A6077@virginia.edu>

Dear r-helpers,

I didn't include a clear question in my previous posting. Here is a  
better version:

When I use latex() on a matrix, I set the option dcolumn = T. As a   
result, in the *.tex file before each tabular I get
\newcolumntype{.}{D{.}{.}{-1}}
The LaTeX compiler complains about these multiple redeclarations. Is  
there a way to suppress this?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From tguennel at vcu.edu  Sun Feb 18 22:06:05 2007
From: tguennel at vcu.edu (Tobias Guennel)
Date: Sun, 18 Feb 2007 22:06:05 +0100
Subject: [R] User defined split function in rpart
Message-ID: <000401c753a0$9d148930$14b2a8c0@petry>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070218/d22ed4ba/attachment.pl 

From rolf at math.unb.ca  Sun Feb 18 22:04:21 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Sun, 18 Feb 2007 17:04:21 -0400 (AST)
Subject: [R] printing intermediate lines while in a function
Message-ID: <200702182104.l1IL4LI3021567@weisner.math.unb.ca>

You're using Windoze, aren't you?  You didn't say.

See the Windoze-specific FAQ, 7.1.

                                cheers,

                                        Rolf Turner
                                        rolf at math.unb.ca

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
Original message:

> <snip>
>
> But unfortunately R will do first the calculations and then
> afterwards return the strings.
> 
> Is there a way around?


From tguennel at vcu.edu  Sun Feb 18 22:10:46 2007
From: tguennel at vcu.edu (Tobias Guennel)
Date: Sun, 18 Feb 2007 22:10:46 +0100
Subject: [R] User defined split function in rpart
Message-ID: <000901c753a1$43af4af0$14b2a8c0@petry>

Dear R community,

I am trying to write my own user defined split function for rpart. I read
the example in the tests directory and I understand the general idea of the
how to implement user defined splitting functions. However, I am having
troubles with addressing the data frame used in calling rpart in my split
functions. 
For example, in the evaluation function that is called once per node, I want
to fit a proportional odds model to the data in the node and use its
deviance as node deviance:

evalf <- function(y,x,parms) {
?????? 
pomnode<-polr(dataframe$y~dataframe$x,dataframe,weights=dataframe$Freq)

more code
}

The dataframe used in the polr call should be the data of the current node.
How can I address the data of the current node and assign it to the
dataframe?


Thank you for your help,
Tobias Guennel


From bessa_ricardo at hotmail.com  Sun Feb 18 20:32:38 2007
From: bessa_ricardo at hotmail.com (Ricardo Bessa)
Date: Sun, 18 Feb 2007 19:32:38 +0000
Subject: [R] Forecasting Uncertainly
Message-ID: <BAY23-F21CFB7C4421D3CD053F684968B0@phx.gbl>

Does anyone know a function to calculate non-parametric prediction intervals 
of an output from a neural network?
The probabilistic distribution of the target variable is not normal.
I need a way to estimate uncertainly in a forecasting obtained with a neural 
network.


Regards,
Ricardo Bessa


From jrkrideau at yahoo.ca  Mon Feb 19 00:35:10 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 18 Feb 2007 18:35:10 -0500 (EST)
Subject: [R] principal components analysis
In-Reply-To: <9032578.post@talk.nabble.com>
Message-ID: <266949.66154.qm@web32802.mail.mud.yahoo.com>


--- sabat <sabinaburrascano at gmail.com> wrote:

> 
> Hi, just downloaded R. I need to run a simple
> Principal Components Analysis,
> with a plot and the variables scores as output. Does
> anyone have some
> scripts ready and easy to use...that anyone can
> understand...
> Thank you

Would  
?princomp help?

If you try the example I'd suggest modifying
biplot(pc.cr) to biplot(pc.cr, cex=.5) to reduce the
size of the labels to a more readable size.


From guillermojsanmartin at googlemail.com  Mon Feb 19 01:01:35 2007
From: guillermojsanmartin at googlemail.com (=?ISO-8859-1?Q?Guillermo_Juli=E1n_San_Mart=EDn?=)
Date: Mon, 19 Feb 2007 01:01:35 +0100
Subject: [R] Urgent: How to obtain the Consistent Standard Errors after
	apply 2SLS through tsls() from sem or systemfit("2SLS")
	without this error message !!!!!!!!!!!!!
Message-ID: <48a5ea80702181601v42d3018fgc3f678321cba9eb3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/338ced6d/attachment.pl 

From remosammassimo at bigpond.com  Mon Feb 19 03:38:25 2007
From: remosammassimo at bigpond.com (Dr Remo Sammassimo)
Date: Mon, 19 Feb 2007 13:38:25 +1100
Subject: [R]  help with loop over data frame
Message-ID: <OAEGLHBJMNDPPGJKKIIKEEPICAAA.remosammassimo@bigpond.com>

Dear List,

This may be the fifth time Ive tried to send this to the list so apologies
if there are multiple emails.

I need some help getting started with this problem. I have a data frame
containing a year of daily stock prices in the following format:

Date       Open   High    Low    Close
1/15/2000   10      11      8     10
1/16/2000   12      12     10     11
etc..


I want to create a new data frame which shows only the rows where the column
value "Open" for 'today' is higher than the column value "High" for the
previous day (previous row). How do I loop over each day accessing values
from different rows and columns, as is needed here?

I have tried 'if' statements but none have worked.

Any help appreciated.

Regards,
Alf Sammassimo
Melbourne,Australia


From marc_schwartz at comcast.net  Mon Feb 19 03:59:07 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 18 Feb 2007 20:59:07 -0600
Subject: [R] help with loop over data frame
In-Reply-To: <OAEGLHBJMNDPPGJKKIIKEEPICAAA.remosammassimo@bigpond.com>
References: <OAEGLHBJMNDPPGJKKIIKEEPICAAA.remosammassimo@bigpond.com>
Message-ID: <1171853947.10256.30.camel@localhost.localdomain>

On Mon, 2007-02-19 at 13:38 +1100, Dr Remo Sammassimo wrote:
> Dear List,
> 
> This may be the fifth time Ive tried to send this to the list so apologies
> if there are multiple emails.
> 
> I need some help getting started with this problem. I have a data frame
> containing a year of daily stock prices in the following format:
> 
> Date       Open   High    Low    Close
> 1/15/2000   10      11      8     10
> 1/16/2000   12      12     10     11
> etc..
> 
> 
> I want to create a new data frame which shows only the rows where the column
> value "Open" for 'today' is higher than the column value "High" for the
> previous day (previous row). How do I loop over each day accessing values
> from different rows and columns, as is needed here?
> 
> I have tried 'if' statements but none have worked.
> 
> Any help appreciated.
> 
> Regards,
> Alf Sammassimo
> Melbourne,Australia

I think that this should do it.

Presuming that your data frame is called 'DF':

Rows <- which(sapply(seq(along = rownames(DF))[-1], 
              function(x) DF[x, "Open"] > DF[x - 1, "High"])) + 1

DF.New <- DF[Rows, ]


The first line sets up a sequence from 2:nrows(DF) and then loops over
those indices. The indices are passed as 'x' to the function, which
compares the current row (x) "Open" value with the prior row (x - 1)
"High" value. This returns TRUE or FALSE for each row compared.

If TRUE, which() then returns the index of the row plus 1, since we do
not want the first row. Those indices are assigned to 'Rows', which is
then used to subset 'DF' and create 'DF.New'.

Just using the data you have above:

> DF.New
       Date Open High Low Close
2 1/16/2000   12   12  10    11


See ?which, ?sapply and ?seq

HTH,

Marc Schwartz


From fjbuch at gmail.com  Mon Feb 19 04:07:58 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sun, 18 Feb 2007 22:07:58 -0500
Subject: [R] RSNPper SNPinfo and making it handle a vector
References: <bd93cdad0702041620o6ef96599i73faef29aefd5f5b@mail.gmail.com>
	<Pine.GSO.4.58.0702042015440.17325@capecod.bwh.harvard.edu><bd93cdad0702041804u7e0d83fcr7dd2d0e0a7681ab5@mail.gmail.com>
	<Pine.GSO.4.58.0702050729030.25170@capecod.bwh.harvard.edu>
Message-ID: <erb4as$3uj$1@sea.gmane.org>

I tried biomaRt

library(biomaRt)
ensnp = useMart("snp", dataset = "hsapiens_snp")
snp = getSNP(chromosome = 17, start = 73649033, end = 73679033, mart = 
ensnp)
show(snp)

Gave me a nice table but it did not seem to permit starting from the point 
of knowing the SNP and entering a list of rs######. I guess I could always 
fudge around. But it does not provide the one-stop I was looking for.


> you might look at the biomaRt package in Bioconductor and see if its snp 
> query resolution
> facilities meet your needs.


From alw76 at student.canterbury.ac.nz  Mon Feb 19 04:10:30 2007
From: alw76 at student.canterbury.ac.nz (Amy Whitehead)
Date: Mon, 19 Feb 2007 16:10:30 +1300
Subject: [R] Randomly extract rows from a data frame
Message-ID: <002501c753d3$83000550$3cddb584@biolamy>

Hi,

I am looking for a way to randomly extract a specified number of rows from a
data frame.  I was planning on binding a column of random numbers to the
data frame and then sorting the data frame using this bound column.  But I
can't figure out how to use this column to sort the entire data frame so
that the content of the rows remains together.  Does anyone know how I can
do this?  Hints for other ways to approach this problem would also be
appreciated.

Cheers
Amy
 

Amy Whitehead 
School of Biological Sciences 
University of Canterbury 
Private Bag 4800 
Christchurch 
Ph 03 364 2987 ext 7033 
Cellphone 021 2020525 
Email alw76 at student.canterbury.ac.nz


From marc_schwartz at comcast.net  Mon Feb 19 04:16:42 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 18 Feb 2007 21:16:42 -0600
Subject: [R] Randomly extract rows from a data frame
In-Reply-To: <002501c753d3$83000550$3cddb584@biolamy>
References: <002501c753d3$83000550$3cddb584@biolamy>
Message-ID: <1171855002.10256.34.camel@localhost.localdomain>

On Mon, 2007-02-19 at 16:10 +1300, Amy Whitehead wrote:
> Hi,
> 
> I am looking for a way to randomly extract a specified number of rows from a
> data frame.  I was planning on binding a column of random numbers to the
> data frame and then sorting the data frame using this bound column.  But I
> can't figure out how to use this column to sort the entire data frame so
> that the content of the rows remains together.  Does anyone know how I can
> do this?  Hints for other ways to approach this problem would also be
> appreciated.
> 
> Cheers
> Amy

See ?sample

Using the 'iris' dataset in R:

# Select 2 random rows

> iris[sample(nrow(iris), 2), ]
   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
96          5.7         3.0          4.2         1.2 versicolor
17          5.4         3.9          1.3         0.4     setosa



# Select 5 random rows

> iris[sample(nrow(iris), 5), ]
   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
83          5.8         2.7          3.9         1.2 versicolor
12          4.8         3.4          1.6         0.2     setosa
63          6.0         2.2          4.0         1.0 versicolor
80          5.7         2.6          3.5         1.0 versicolor
49          5.3         3.7          1.5         0.2     setosa



HTH,

Marc Schwartz


From d.scott at auckland.ac.nz  Mon Feb 19 05:01:22 2007
From: d.scott at auckland.ac.nz (David Scott)
Date: Mon, 19 Feb 2007 17:01:22 +1300 (NZDT)
Subject: [R] Randomly extract rows from a data frame
In-Reply-To: <002501c753d3$83000550$3cddb584@biolamy>
References: <002501c753d3$83000550$3cddb584@biolamy>
Message-ID: <Pine.LNX.4.64.0702191659320.17653@stat12.stat.auckland.ac.nz>

On Mon, 19 Feb 2007, Amy Whitehead wrote:

> Hi,
>
> I am looking for a way to randomly extract a specified number of rows from a
> data frame.  I was planning on binding a column of random numbers to the
> data frame and then sorting the data frame using this bound column.  But I
> can't figure out how to use this column to sort the entire data frame so
> that the content of the rows remains together.  Does anyone know how I can
> do this?  Hints for other ways to approach this problem would also be
> appreciated.
>
> Cheers
> Amy
>

It is a bit easier than that.

Here is one way:

> df <- airquality
> rNames <- row.names(df)
> sampRows <- sample(rNames,10)
> sampRows
  [1] "137" "56"  "1"   "135" "62"  "43"  "12"  "128" "86"  "54"
> subset(df,rNames%in%sampRows)
     Ozone Solar.R Wind Temp Month Day
1      41     190  7.4   67     5   1
12     16     256  9.7   69     5  12
43     NA     250  9.2   92     6  12
54     NA      91  4.6   76     6  23
56     NA     135  8.0   75     6  25
62    135     269  4.1   84     7   1
86    108     223  8.0   85     7  25
128    47      95  7.4   87     9   5
135    21     259 15.5   76     9  12
137     9      24 10.9   71     9  14

David Scott
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland 1142,    NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz

Graduate Officer, Department of Statistics


From liuwensui at gmail.com  Mon Feb 19 05:48:34 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 18 Feb 2007 23:48:34 -0500
Subject: [R] Randomly extract rows from a data frame
In-Reply-To: <002501c753d3$83000550$3cddb584@biolamy>
References: <002501c753d3$83000550$3cddb584@biolamy>
Message-ID: <1115a2b00702182048j6f975533r6a8a68c11f75bfd8@mail.gmail.com>

amy,
here is a piece of code copied from my blog, which might answer part
of your question.

library(MASS);
data(Boston);

# DIVIDE DATA INTO TESTING AND TRAINING SETS
set.seed(2005);
test.rows <- sample(1:nrow(Boston), 100);
test.set <- Boston[test.rows, ];
train.set <- Boston[-test.rows, ];


On 2/18/07, Amy Whitehead <alw76 at student.canterbury.ac.nz> wrote:
> Hi,
>
> I am looking for a way to randomly extract a specified number of rows from a
> data frame.  I was planning on binding a column of random numbers to the
> data frame and then sorting the data frame using this bound column.  But I
> can't figure out how to use this column to sort the entire data frame so
> that the content of the rows remains together.  Does anyone know how I can
> do this?  Hints for other ways to approach this problem would also be
> appreciated.
>
> Cheers
> Amy
>
>
> Amy Whitehead
> School of Biological Sciences
> University of Canterbury
> Private Bag 4800
> Christchurch
> Ph 03 364 2987 ext 7033
> Cellphone 021 2020525
> Email alw76 at student.canterbury.ac.nz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From tchur at optushome.com.au  Mon Feb 19 08:20:13 2007
From: tchur at optushome.com.au (Tim Churches)
Date: Mon, 19 Feb 2007 18:20:13 +1100
Subject: [R] Google, hard disc drives and R
Message-ID: <45D94FAD.7070403@optushome.com.au>

A recent paper from Google Labs, interesting in many respects, not the
least the exclusive use of R for data analysis and graphics (alas not
cited in the approved manner):

http://labs.google.com/papers/disk_failures.pdf

Perhaps some of the eminences grises of the R Foundation could prevail
upon Google to make some the data reported in the paper available for
inclusion in an R library or two, for pedagogical purposes?

Tim C


From dieter.menne at menne-biomed.de  Mon Feb 19 10:56:58 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 19 Feb 2007 09:56:58 +0000 (UTC)
Subject: [R] Google, hard disc drives and R
References: <45D94FAD.7070403@optushome.com.au>
Message-ID: <loom.20070219T105514-562@post.gmane.org>

Tim Churches <tchur <at> optushome.com.au> writes:

> 
> A recent paper from Google Labs, interesting in many respects, not the
> least the exclusive use of R for data analysis and graphics (alas not
> cited in the approved manner):
> 
> http://labs.google.com/papers/disk_failures.pdf
> 
...
For all of you who noted that the first author is E. Pinheiro: This is not the
first half of D Bates, who's first name is Jos?.

Dieter


From guillermojsanmartin at googlemail.com  Mon Feb 19 11:37:25 2007
From: guillermojsanmartin at googlemail.com (=?ISO-8859-1?Q?Guillermo_Juli=E1n_San_Mart=EDn?=)
Date: Mon, 19 Feb 2007 11:37:25 +0100
Subject: [R] Urgent: How to obtain the Consistent Standard Errors after
	apply 2SLS through tsls() from sem or systemfit("2SLS")
	without this error message !!!!!!!!!!!!!
Message-ID: <48a5ea80702190237p77b808bdlb6733cc82a087fc2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/19e5135b/attachment.pl 

From johannes_graumann at web.de  Mon Feb 19 11:43:57 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 19 Feb 2007 11:43:57 +0100
Subject: [R] Data frame: how to create list of row max?
Message-ID: <erbv1d$psj$1@sea.gmane.org>

Dear all,

Can anyone please shed some light onto how to do this?

This will give me all "intensity" columsn in my data frame:
intensityindeces <- grep("^Intensity",names(dataframe),value=TRUE)

This will give me the maximum intensity for the first row:
intensityone <- max(dataframe[1,intensityindeces])

What I'm now looking for is how to dfo this for the whole data frame. Should
yield a list of maximum intensities of all rows.
Can't figure it out ... please nudge me where I need to go.

Thanks, Joh


From ccleland at optonline.net  Mon Feb 19 11:51:28 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 19 Feb 2007 05:51:28 -0500
Subject: [R] Data frame: how to create list of row max?
In-Reply-To: <erbv1d$psj$1@sea.gmane.org>
References: <erbv1d$psj$1@sea.gmane.org>
Message-ID: <45D98130.9060406@optonline.net>

Johannes Graumann wrote:
> Dear all,
> 
> Can anyone please shed some light onto how to do this?
> 
> This will give me all "intensity" columsn in my data frame:
> intensityindeces <- grep("^Intensity",names(dataframe),value=TRUE)
> 
> This will give me the maximum intensity for the first row:
> intensityone <- max(dataframe[1,intensityindeces])
> 
> What I'm now looking for is how to dfo this for the whole data frame. Should
> yield a list of maximum intensities of all rows.
> Can't figure it out ... please nudge me where I need to go.

  If you want the values themselves:

apply(dataframe[,intensityindeces], 1, max)

  If you want the column in which the max appears:

apply(dataframe[,intensityindeces], 1, which.max)

?apply

> Thanks, Joh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ripley at stats.ox.ac.uk  Mon Feb 19 11:53:18 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Feb 2007 10:53:18 +0000 (GMT)
Subject: [R] Data frame: how to create list of row max?
In-Reply-To: <erbv1d$psj$1@sea.gmane.org>
References: <erbv1d$psj$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.64.0702191051320.28147@gannet.stats.ox.ac.uk>

do.call("pmax", dataframe[,intensityindeces])

if I understand you aright.

On Mon, 19 Feb 2007, Johannes Graumann wrote:

> Dear all,
>
> Can anyone please shed some light onto how to do this?
>
> This will give me all "intensity" columsn in my data frame:
> intensityindeces <- grep("^Intensity",names(dataframe),value=TRUE)
>
> This will give me the maximum intensity for the first row:
> intensityone <- max(dataframe[1,intensityindeces])
>
> What I'm now looking for is how to dfo this for the whole data frame. Should
> yield a list of maximum intensities of all rows.
> Can't figure it out ... please nudge me where I need to go.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bartjoosen at hotmail.com  Mon Feb 19 12:25:56 2007
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Mon, 19 Feb 2007 11:25:56 +0000
Subject: [R] printing intermediate lines while in a function
In-Reply-To: <200702182104.l1IL4LI3021567@weisner.math.unb.ca>
Message-ID: <BAY134-F1870B8D0A2912B3004F364D88A0@phx.gbl>

I've read the Windows FAQ, I tried flush.console(), but then it continious 
with the calculations, while I didn't select a range for the graphics.

Is there a possibility to "wait" for user input or something?


Thanks

Bart



>From: rolf at math.unb.ca
>To: bartjoosen at hotmail.com
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] printing intermediate lines while in a function
>Date: Sun, 18 Feb 2007 17:04:21 -0400 (AST)
>
>You're using Windoze, aren't you?  You didn't say.
>
>See the Windoze-specific FAQ, 7.1.
>
>                                 cheers,
>
>                                         Rolf Turner
>                                         rolf at math.unb.ca
>
>===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
>Original message:
>
> > <snip>
> >
> > But unfortunately R will do first the calculations and then
> > afterwards return the strings.
> >
> > Is there a way around?


From jacinthe at gmx.de  Mon Feb 19 12:28:13 2007
From: jacinthe at gmx.de (jacinthe at gmx.de)
Date: Mon, 19 Feb 2007 12:28:13 +0100
Subject: [R] Rcpp
Message-ID: <20070219112813.299500@gmx.net>

Hello all,

using GNU WinGW under Windows, I have experimented with the Rcpp package. Powerful package. Many thanks to Dominick Samperi.

Now I have tried to modify the RccpExample.cpp file in order to calculate two different moving averages, a simple moving average and an exponential moving average.

I call the function in R with:

params <- list(nobs=10,matype=2)
tempC <- .Call("Test_fun",x,params,PACKAGE = "RcppTemplate")

where nobs are the number of observation to calculate the moving average and matype could be 1 for a simple moving average and 2 for an exponential moving average. x is a dataframe containing the relevant data in the column labeled "Close".

For matype=1 it works fine and the output looks like:
$mab
  [1]    0.000    0.000    0.000    0.000    0.000    0.000    0.000    0.000    0.000 1450.710 1453.658 1459.822 1467.564
 [14] 1477.226 1487.114 1495.231 1496.560 1499.667 1498.295 1488.754 1478.958 1469.565 1460.120 1445.006 1429.891 1417.258
 [27] 1405.487 1393.141 1384.933 1382.347 1379.085 1370.778 1363.517 1359.175 1364.777 1365.676 1368.880 1370.997 1369.357
 [40] 1368.533 1368.512 1374.000 1379.464 1387.269 1387.010 1389.155 1393.638 1400.011 1407.004 1413.114 1421.691 1432.297
 [53] 1441.165 1450.033 1456.677 1467.145 1480.899 1495.727 1509.590 1522.596 1534.066 1545.307 1554.237 1561.848 1567.405
 [66] 1565.960 1561.789 1557.102 1559.802 1561.216 1563.193 1559.656 1560.993 1558.660 1562.106 1567.487 1569.686 1570.313
 [79] 1562.673 1556.586 1548.386 1543.358 1536.041 1532.702 1527.327 1524.134 1526.599 1529.143 1536.038 1542.074 1548.256
 [92] 1552.895 1559.588 1567.022 1574.884 1581.358 1585.991 1590.170 1591.420 1590.616

$nobs
[1] 10

$n
[1] 100

But for matype=2 it seems not to work. The output is
$mab
  [1] 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36
 [16] 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36
 [31] 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36
 [46] 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36
 [61] 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36
 [76] 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36
 [91] 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36 1483.36

$nobs
[1] 10

$n
[1] 100


The modified RcppExpamle.cpp file looks like:

#include "Rcpp.hpp"
// simple moving average
vector<double> SMAVG(vector<double> data, int nobs, int n) {
  int i,j;
  double sum;
  vector<double> mab(n);

  for (i=0;i<(n-nobs+1);i++)
  {
    sum=0.0;
    for (j=i;j<(i+nobs);j++)
    {
      sum += data[j];
    }
  mab[i+nobs-1]=sum/nobs;
  }
  return mab;
}

// exponential moving average
vector<double> EMAVG(vector<double> data, int nobs, int n) {
  int i;
  double optInK;
  optInK=2/(nobs+1);
  vector<double> mab(n);
  mab[0] = data[0];

  for (i=1;i<n;i++)
  {
    mab[i] = ((data[i]-mab[i-1])*optInK) + mab[i-1];
  }
  return mab;
}

// choose between simple and exponential moving average
RcppExport SEXP Test_fun(SEXP data, SEXP params) {
  SEXP rl=R_NilValue;

  char* exceptionMesg=NULL;
  try{
    int i=0;
    RcppParams rparam(params);
      int nobs = rparam.getIntValue("nobs");
      int matype = rparam.getIntValue("matype");
    RcppFrame datafm(data);
      vector<vector<ColDatum> > table = datafm.getTableData();
      int nrow = table.size();
    RcppVector<double> close(nrow);
      for(int row=0; row < nrow; row++) {
	     close(row)= table[row][4].getDoubleValue();
      }

    vector<double> stlvec(close.stlVector());
    vector<double> res(nrow);

    if (matype==1)
    {
       res = SMAVG(stlvec,nobs,nrow);
    }
    if (matype==2)
    {
      res = EMAVG(stlvec,nobs,nrow);
    }

    RcppVector<double> mab(nrow);
    for (i=0; i<nrow; i++) {
      mab(i)=res[i];
    }

    RcppResultSet rs;

    rs.add("mab",mab);
    rs.add("nobs",nobs);
    rs.add("n",nrow);
    rl = rs.getReturnList();
  } catch(std::exception& ex){
      exceptionMesg = copyMessageToR(ex.what());
  } catch(...){
      exceptionMesg = copyMessageToR("unknown reason");
  }
  if(exceptionMesg != NULL)
    error(exceptionMesg);
  return rl;
}

It seems that the problem is in the EMAVG function. I am not an C++ expert, perhapes I have made a mistake there. Could someone with stonger C++ skills    check this function? Or have I made a mistake by calling the function in the Test_fun function, which is later called by R?
Besides, is there a more elegant way to access in the Test_fun the Close-Column of the dataframe data which contains the columns (Date, Open, High, Low, Close, Volume)?

Best regards
Jaci
-- 
"Feel free" - 10 GB Mailbox, 100 FreeSMS/Monat ...
Jetzt GMX TopMail testen: www.gmx.net/de/go/mailfooter/topmail-out


From ggrothendieck at gmail.com  Mon Feb 19 12:31:27 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Feb 2007 06:31:27 -0500
Subject: [R] help with loop over data frame
In-Reply-To: <OAEGLHBJMNDPPGJKKIIKEEPICAAA.remosammassimo@bigpond.com>
References: <OAEGLHBJMNDPPGJKKIIKEEPICAAA.remosammassimo@bigpond.com>
Message-ID: <971536df0702190331u506739e3u68cce73b09bfb433@mail.gmail.com>

Try this:

   DF[c(FALSE, tail(DF$Open, -1) > head(DF$High, -1)), ]

or using zoo objects just compare the Open to the reverse lag of the High.

Lines <- "Date       Open   High    Low    Close
1/15/2000   10      11      8     10
1/16/2000   12      12     10     11
1/17/2000   12      12     10     11
"
library(zoo)
z <- read.zoo(textConnection(Lines), header = TRUE, format = "%m/%d/%Y")
z[ z[, "Open"] > lag(z[, "High"],-1), ]



On 2/18/07, Dr Remo Sammassimo <remosammassimo at bigpond.com> wrote:
> Dear List,
>
> This may be the fifth time Ive tried to send this to the list so apologies
> if there are multiple emails.
>
> I need some help getting started with this problem. I have a data frame
> containing a year of daily stock prices in the following format:
>
> Date       Open   High    Low    Close
> 1/15/2000   10      11      8     10
> 1/16/2000   12      12     10     11
> etc..
>
>
> I want to create a new data frame which shows only the rows where the column
> value "Open" for 'today' is higher than the column value "High" for the
> previous day (previous row). How do I loop over each day accessing values
> from different rows and columns, as is needed here?
>
> I have tried 'if' statements but none have worked.
>
> Any help appreciated.
>
> Regards,
> Alf Sammassimo
> Melbourne,Australia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From yogesh.mpi at googlemail.com  Mon Feb 19 12:48:25 2007
From: yogesh.mpi at googlemail.com (Yogesh Tiwari)
Date: Mon, 19 Feb 2007 12:48:25 +0100
Subject: [R] problem in reading TOMS observed ASCII data file
In-Reply-To: <71cc5ca20702190329h407b5508g799fca713ab7ec6b@mail.gmail.com>
References: <71cc5ca20702190329h407b5508g799fca713ab7ec6b@mail.gmail.com>
Message-ID: <71cc5ca20702190348r7806e21bp4ba6a6f6f8fbf395@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/fccca0ec/attachment.pl 

From johannes_graumann at web.de  Mon Feb 19 13:13:08 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 19 Feb 2007 13:13:08 +0100
Subject: [R] Data frame: how to create list of row max?
In-Reply-To: <Pine.LNX.4.64.0702191051320.28147@gannet.stats.ox.ac.uk>
References: <erbv1d$psj$1@sea.gmane.org>
	<Pine.LNX.4.64.0702191051320.28147@gannet.stats.ox.ac.uk>
Message-ID: <200702191313.16940.johannes_graumann@web.de>

On Monday 19 February 2007 11:53, Prof Brian Ripley wrote:
> do.call("pmax", dataframe[,intensityindeces])
Thank you very much for your help!

Any idea why do.call("pmax",list(na.rm=TRUE),dataframe[,intensityindeces]) 
would give me 

Error in if (quote) { : argument is not interpretable as logical
In addition: Warning message:
the condition has length > 1 and only the first element will be used in: if 
(quote) {

?

Thanks, Joh
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 827 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070219/66c4f910/attachment.bin 

From yogesh.mpi at googlemail.com  Mon Feb 19 13:23:03 2007
From: yogesh.mpi at googlemail.com (Yogesh Tiwari)
Date: Mon, 19 Feb 2007 13:23:03 +0100
Subject: [R] need help in reading TOMS observed ASCII data file
Message-ID: <71cc5ca20702190423x17a6b55k864bc263d1fb4fc3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/b5394abe/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Feb 19 13:30:00 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Feb 2007 12:30:00 +0000 (GMT)
Subject: [R] Data frame: how to create list of row max?
In-Reply-To: <200702191313.16940.johannes_graumann@web.de>
References: <erbv1d$psj$1@sea.gmane.org>
	<Pine.LNX.4.64.0702191051320.28147@gannet.stats.ox.ac.uk>
	<200702191313.16940.johannes_graumann@web.de>
Message-ID: <Pine.LNX.4.64.0702191229030.2556@gannet.stats.ox.ac.uk>

On Mon, 19 Feb 2007, Johannes Graumann wrote:

> On Monday 19 February 2007 11:53, Prof Brian Ripley wrote:
>> do.call("pmax", dataframe[,intensityindeces])
> Thank you very much for your help!
>
> Any idea why do.call("pmax",list(na.rm=TRUE),dataframe[,intensityindeces])

You want something like

do.call("pmax", c(dataframe[,intensityindeces], list(na.rm=TRUE)))

See ?do.call

> would give me
>
> Error in if (quote) { : argument is not interpretable as logical
> In addition: Warning message:
> the condition has length > 1 and only the first element will be used in: if
> (quote) {
>
> ?
>
> Thanks, Joh
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mothsailor at googlemail.com  Mon Feb 19 13:42:01 2007
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 19 Feb 2007 12:42:01 +0000
Subject: [R] Data frame: how to create list of row max?
In-Reply-To: <200702191313.16940.johannes_graumann@web.de>
References: <erbv1d$psj$1@sea.gmane.org>
	<Pine.LNX.4.64.0702191051320.28147@gannet.stats.ox.ac.uk>
	<200702191313.16940.johannes_graumann@web.de>
Message-ID: <815b70590702190442p520abce3h37a4ff982c6b1d08@mail.gmail.com>

Try do.call("pmax",c(dataframe[,intensityindices],na.rm=TRUE))

This is like the second example in the help page for do.call

On 19/02/07, Johannes Graumann <johannes_graumann at web.de> wrote:
> On Monday 19 February 2007 11:53, Prof Brian Ripley wrote:
> > do.call("pmax", dataframe[,intensityindeces])
> Thank you very much for your help!
>
> Any idea why do.call("pmax",list(na.rm=TRUE),dataframe[,intensityindeces])
> would give me
>
> Error in if (quote) { : argument is not interpretable as logical
> In addition: Warning message:
> the condition has length > 1 and only the first element will be used in: if
> (quote) {
>
> ?
>
> Thanks, Joh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From bolker at zoo.ufl.edu  Mon Feb 19 13:50:57 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 19 Feb 2007 12:50:57 +0000 (UTC)
Subject: [R] Google, hard disc drives and R
References: <45D94FAD.7070403@optushome.com.au>
Message-ID: <loom.20070219T134656-322@post.gmane.org>

Tim Churches <tchur <at> optushome.com.au> writes:

> 
> A recent paper from Google Labs, interesting in many respects, not the
> least the exclusive use of R for data analysis and graphics (alas not
> cited in the approved manner):
> 
> http://labs.google.com/papers/disk_failures.pdf
> 
> Perhaps some of the eminences grises of the R Foundation could prevail
> upon Google to make some the data reported in the paper available for
> inclusion in an R library or two, for pedagogical purposes?
> 
> Tim C
> 

   After skimming the paper, I can't help wondering why
they used barplots with error bars instead of boxplots,
and why they broke the data into discrete age groups?
Given that they had a relatively large data set
(several percent of >100,000 disk drives), they could
have done some cool visualization stuff ...

  Ben Bolker


From Roger.Bivand at nhh.no  Mon Feb 19 14:13:07 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 19 Feb 2007 14:13:07 +0100 (CET)
Subject: [R] Google, hard disc drives and R
In-Reply-To: <loom.20070219T134656-322@post.gmane.org>
Message-ID: <Pine.LNX.4.44.0702191412270.12238-100000@reclus.nhh.no>

On Mon, 19 Feb 2007, Ben Bolker wrote:

> Tim Churches <tchur <at> optushome.com.au> writes:
> 
> > 
> > A recent paper from Google Labs, interesting in many respects, not the
> > least the exclusive use of R for data analysis and graphics (alas not
> > cited in the approved manner):
> > 
> > http://labs.google.com/papers/disk_failures.pdf
> > 
> > Perhaps some of the eminences grises of the R Foundation could prevail
> > upon Google to make some the data reported in the paper available for
> > inclusion in an R library or two, for pedagogical purposes?
> > 
> > Tim C
> > 
> 
>    After skimming the paper, I can't help wondering why
> they used barplots with error bars instead of boxplots,
> and why they broke the data into discrete age groups?
> Given that they had a relatively large data set
> (several percent of >100,000 disk drives), they could
> have done some cool visualization stuff ...

For example Mondrian via RServe?

> 
>   Ben Bolker
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From b.rowlingson at lancaster.ac.uk  Mon Feb 19 10:45:26 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 19 Feb 2007 09:45:26 +0000
Subject: [R] Ubuntu Linux and X11
In-Reply-To: <45D6E594.1020706@ebi.ac.uk>
References: <1171659219.45d619d39d5bc@it.slf.ch>	<20070216230535.GA4748@eddelbuettel.com>	<20070217052104.GC2144@jdmlab.mskcc.org>
	<45D6E594.1020706@ebi.ac.uk>
Message-ID: <45D971B6.8010205@lancaster.ac.uk>

Oleg Sklyar wrote:
> The problem occurs after updating from Dapper to Edgy. Dapper had font 
> paths: /usr/share/X11/fonts and Edgy, to make the whole font system 
> unified, moved X11 fonts to /usr/share/fonts/X11. Oleg

  I think I changed the font path in the X config file *and* added a 
symlink of /usr/share/X11/fonts to point to /usr/share/fonts/X11 in case 
any other package had this coded into it.

Barry


From johannes_graumann at web.de  Mon Feb 19 14:37:56 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 19 Feb 2007 14:37:56 +0100
Subject: [R] Data frame: how to create list of row max?
In-Reply-To: <815b70590702190442p520abce3h37a4ff982c6b1d08@mail.gmail.com>
References: <erbv1d$psj$1@sea.gmane.org>
	<200702191313.16940.johannes_graumann@web.de>
	<815b70590702190442p520abce3h37a4ff982c6b1d08@mail.gmail.com>
Message-ID: <200702191437.57124.johannes_graumann@web.de>

Thanks to you and Brian Ripley. Quite confusing all this ...

Thanks again.

Joh

On Monday 19 February 2007 13:42, David Barron wrote:
> Try do.call("pmax",c(dataframe[,intensityindices],na.rm=TRUE))
>
> This is like the second example in the help page for do.call
>
> On 19/02/07, Johannes Graumann <johannes_graumann at web.de> wrote:
> > On Monday 19 February 2007 11:53, Prof Brian Ripley wrote:
> > > do.call("pmax", dataframe[,intensityindeces])
> >
> > Thank you very much for your help!
> >
> > Any idea why
> > do.call("pmax",list(na.rm=TRUE),dataframe[,intensityindeces]) would give
> > me
> >
> > Error in if (quote) { : argument is not interpretable as logical
> > In addition: Warning message:
> > the condition has length > 1 and only the first element will be used in:
> > if (quote) {
> >
> > ?
> >
> > Thanks, Joh
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 827 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070219/10f280a6/attachment.bin 

From herrdittmann at yahoo.co.uk  Mon Feb 19 14:39:07 2007
From: herrdittmann at yahoo.co.uk (Bernd Dittmann)
Date: Mon, 19 Feb 2007 13:39:07 +0000
Subject: [R]  Calculating the Sharpe ratio
Message-ID: <45D9A87B.4060002@yahoo.co.uk>

Hi useRs,

I am trying to calculate the Sharpe ratio with "sharpe" of the library 
"tseries".

The documentation requires the univariate time series to be a 
portfolio's cumulated returns. In this case, the example given

data(EuStockMarkets)
dax <- log(EuStockMarkets[,"FTSE"])

is however not the cumulated returns but rather the daily returns of the 
FTSE stock index.

Is this way of calculating the Sharpe ratio correct?

Here are my own data:

year    Index    PercentReturns
1985    117    0.091
1986    129.9    0.11
1987    149.9    0.154
1988    184.8    0.233
1989    223.1    0.208
1990    223.2    0
1991    220.5    -0.012
1992    208.1    -0.056
1993    202.1    -0.029
1994    203.1    0.005
1995    199.6    -0.017
1996    208.6    0.045
1997    221.7    0.063
1998    233.7    0.054
1999    250.5    0.072
2000    275.1    0.098
2001    298.6    0.085
2002    350.6    0.174
2003    429.1    0.224
2004    507.6    0.183
2005    536.6    0.057
2006    581.3    0.083


I calculated the Sharpe ratio in two different ways:
(1) using natural logs as approximation of % returns, using "sharpe" of 
"tseries".
(2) using the % returns using a variation the "sharpe" function.

In both cases I used the risk free rate r=0 and scale=1 since I am using 
annual data already.

My results:

METHOD 1: "sharpe":

 > index <- log(Index)
 > sharpe(index, scale=1)
[1] 0.9614212



METHOD 2: my own %-based formula:

 > mysharp
function(x, r=0, scale=sqrt(250))
{
if (NCOL(x) > 1)
stop("x is not a vector or univariate time series")
if (any(is.na(x)))
stop("NAs in x")
if (NROW(x) ==1)
return(NA)
else{
return(scale * (mean(x) - r)/sd(x))
}
}



 > mysharp(PercentReturns, scale=1)
[1] 0.982531


Both Sharp ratios differ only slightly since logs approximate percentage 
changes (returns).


Are both methods correct, esp. since I am NOT using cumulated returns as 
the manual says?

If cumulated returns were supposed to be used, could I cumulate the 
%-returns with "cumsum(PercentReturns)"?

Many thanks in advance!

Bernd


From shubhak at ambaresearch.com  Mon Feb 19 14:36:26 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Mon, 19 Feb 2007 19:06:26 +0530
Subject: [R] categorical column to numeric column
Message-ID: <A36876D3F8A5734FA84A4338135E7CC30104A80E@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/a577049e/attachment.pl 

From mail at nhoeller.de  Mon Feb 19 14:56:04 2007
From: mail at nhoeller.de (Nils =?ISO-8859-1?Q?H=F6ller?=)
Date: Mon, 19 Feb 2007 14:56:04 +0100
Subject: [R] Start and Restart R over SSH
Message-ID: <1171893364.14607.6.camel@nils-laptop>

Hi,
 
I have some big calculations in R to be done. 
Since I can use R on a server with ssh, i was wondering if I can reopen
a R Shell after exiting ssh.

I don't want to use the batch mode and nohup doesn't work.

I want to use something like
ssh user at server
R
---do something in R and start calculation ---
close ssh but let R remain on the server, doing the calculation

ssh user at server
open the existing R Shell / Process

Has anyone done something similiar?
Can you help me or suggest an other solution ? 

Thank you for your help

Nils


From Thierry.ONKELINX at inbo.be  Mon Feb 19 15:09:31 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 19 Feb 2007 15:09:31 +0100
Subject: [R] categorical column to numeric column
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC30104A80E@BAN-MAILSRV03.Amba.com>
Message-ID: <2E9C414912813E4EB981326983E0A10402977D79@inboexch.inbo.be>

Maybe this isn't the most elegant way, but it should work.
dd$g <- -1
dd$g[dd$aa == "a"] <- 1

Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Shubha Vishwanath
Karanth
Verzonden: maandag 19 februari 2007 14:36
Aan: r-help
Onderwerp: [R] categorical column to numeric column

Hi R,

 

Let 'dd' be a data frame given as:

 

dd=data.frame(aa=c("a","a","b","a","b","b"),bb=c(1,1,1,2,3,4))

 

Now I want to create a column 'g' such that if dd$aa=a then dd$g=1 else
dd$g= -1 .

 

So, I gave the below syntax:

 

if((dd$aa)=="a") dd$g=1 else dd$g= -1

 

But I get the error message as:

Warning message: 

the condition has length > 1 and only the first element will be used in:
if ((dd$aa) == "a") dd$g = 1 else dd$g = -1 

 

and dd=

 

> dd

  aa bb g

1  a  1 1

2  a  1 1

3  b  1 1

4  a  2 1

5  b  3 1

6  b  4 1

> 

 

Please let me know what is the error I am doing?

 

 

 

 


	[[alternative HTML version deleted]]

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Mon Feb 19 15:11:52 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Feb 2007 09:11:52 -0500
Subject: [R] categorical column to numeric column
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC30104A80E@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC30104A80E@BAN-MAILSRV03.Amba.com>
Message-ID: <45D9B028.5040007@stats.uwo.ca>

On 2/19/2007 8:36 AM, Shubha Vishwanath Karanth wrote:
> Hi R,
> 
>  
> 
> Let 'dd' be a data frame given as:
> 
>  
> 
> dd=data.frame(aa=c("a","a","b","a","b","b"),bb=c(1,1,1,2,3,4))
> 
>  
> 
> Now I want to create a column 'g' such that if dd$aa=a then dd$g=1 else
> dd$g= -1 .
> 
>  
> 
> So, I gave the below syntax:
> 
>  
> 
> if((dd$aa)=="a") dd$g=1 else dd$g= -1

if() looks at just the first entry; it's designed for flow of control 
rather than vectorized calculations.  You want ifelse():

ifelse( dd$aa == "a", 1, -1)

Duncan Murdoch
>  
> 
> But I get the error message as:
> 
> Warning message: 
> 
> the condition has length > 1 and only the first element will be used in:
> if ((dd$aa) == "a") dd$g = 1 else dd$g = -1 
> 
>  
> 
> and dd=
> 
>  
> 
>> dd
> 
>   aa bb g
> 
> 1  a  1 1
> 
> 2  a  1 1
> 
> 3  b  1 1
> 
> 4  a  2 1
> 
> 5  b  3 1
> 
> 6  b  4 1
> 
>> 
> 
>  
> 
> Please let me know what is the error I am doing?
> 
>  
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From brown_emu at yahoo.com  Mon Feb 19 15:26:01 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Mon, 19 Feb 2007 06:26:01 -0800 (PST)
Subject: [R] categorical column to numeric column
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC30104A80E@BAN-MAILSRV03.Amba.com>
Message-ID: <250374.53920.qm@web39707.mail.mud.yahoo.com>

try

dd$g <- ifelse(dd$aa=="a",1,-1)

and in general, you can convert categorical data (factors) into integers with
as.integer(), though the values will be positive:

dd$f <- as.integer(factor(dd$aa))


--- Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:

> Hi R,
> 
>  
> 
> Let 'dd' be a data frame given as:
> 
>  
> 
> dd=data.frame(aa=c("a","a","b","a","b","b"),bb=c(1,1,1,2,3,4))
> 
>  
> 
> Now I want to create a column 'g' such that if dd$aa=a then dd$g=1 else
> dd$g= -1 .
> 
>  
> 
> So, I gave the below syntax:
> 
>  
> 
> if((dd$aa)=="a") dd$g=1 else dd$g= -1
> 
>  
> 
> But I get the error message as:
> 
> Warning message: 
> 
> the condition has length > 1 and only the first element will be used in:
> if ((dd$aa) == "a") dd$g = 1 else dd$g = -1 
> 
>  
> 
> and dd=
> 
>  
> 
> > dd
> 
>   aa bb g
> 
> 1  a  1 1
> 
> 2  a  1 1
> 
> 3  b  1 1
> 
> 4  a  2 1
> 
> 5  b  3 1
> 
> 6  b  4 1
> 
> > 
> 
>  
> 
> Please let me know what is the error I am doing?
> 
>  
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 




 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From rolf at math.unb.ca  Mon Feb 19 15:35:55 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Mon, 19 Feb 2007 10:35:55 -0400 (AST)
Subject: [R] Documenting options specific to a package.
Message-ID: <200702191435.l1JEZtTT029753@weisner.math.unb.ca>

If one specifies new options in a package, using the options()
function, where does/should one document these new options?

E.g. suppose that I put the line

	options(melvin=42)

in a file zzz.R in the R directory of the package source, where the
package contains functions foo(), bar(), clyde(), and irving() which
all query options("melvin") and take some action based on the value
of ``melvin''.  The user can of course change the value of ``melvin''
from its default value by doing, e.g.

        options(melvin=99)

Are there any conventions or standards as to how and where the option
``melvin'' should be documented?  It seems somewhat redundant to
include docmentation about melvin in the help on all 4 of foo(),
bar(), clyde(), and irving().

Thanks for any insights.

                                cheers,

                                        Rolf Turner
                                        rolf at math.unb.ca


From tguennel at vcu.edu  Mon Feb 19 15:40:08 2007
From: tguennel at vcu.edu (Tobias Guennel)
Date: Mon, 19 Feb 2007 15:40:08 +0100
Subject: [R]  User defined split function in rpart
Message-ID: <000901c75433$de0010c0$14b2a8c0@petry>

Maybe I should explain my Problem a little bit more detailed.
The rpart package allows for user defined split functions. An example is
given in the source/test directory of the package as usersplits.R.
The comments say that three functions have to be supplied:
1. "The 'evaluation' function.  Called once per node.
  Produce a label (1 or more elements long) for labeling each node,
  and a deviance." 
2. The split function, where most of the work occurs.
   Called once per split variable per node.
3. The init function:
   fix up y to deal with offsets
   return a dummy parms list
   numresp is the number of values produced by the eval routine's "label".

I have altered the evaluation function and the split function for my needs.
Within those functions, I need to fit a proportional odds model to the data
of the current node. I am using the polr() routine from the MASS package to
fit the model. 
Now my problem is, how can I call the polr() function only with the data of
the current node. That's what I tried so far:

evalfunc <- function(y,x,parms,data) {
       
pomnode<-polr(data$y~data$x,data,weights=data$Freq)
parprobs<-predict(pomnode,type="probs")
dev<-0
K<-dim(parprobs)[2]
N<-dim(parprobs)[1]/K
for(i in 1:N){
tempsum<-0
Ni<-0
for(l in 1:K){
Ni<-Ni+data$Freq[K*(i-1)+l]
}
for(j in 1:K){
tempsum<-tempsum+data$Freq[K*(i-1)+j]/Ni*log(parprobs[i,j]*Ni/data$Freq[K*(i
-1)+j])
}
dev=dev+Ni*tempsum
}
dev=-2*dev
wmean<-1
list(label= wmean, deviance=dev)

} 

I get the error: Error in eval(expr, envir, enclos) : argument "data" is
missing, with no default

How can I use the data of the current node?

Thank you
Tobias Guennel


From bates at stat.wisc.edu  Mon Feb 19 15:40:07 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 19 Feb 2007 08:40:07 -0600
Subject: [R] Start and Restart R over SSH
In-Reply-To: <1171893364.14607.6.camel@nils-laptop>
References: <1171893364.14607.6.camel@nils-laptop>
Message-ID: <40e66e0b0702190640r7531cc73n14ea0af2974e8226@mail.gmail.com>

On 2/19/07, Nils H?ller <mail at nhoeller.de> wrote:
> Hi,
>
> I have some big calculations in R to be done.
> Since I can use R on a server with ssh, i was wondering if I can reopen
> a R Shell after exiting ssh.
>
> I don't want to use the batch mode and nohup doesn't work.
>
> I want to use something like
> ssh user at server
> R
> ---do something in R and start calculation ---
> close ssh but let R remain on the server, doing the calculation
>
> ssh user at server
> open the existing R Shell / Process
>
> Has anyone done something similiar?
> Can you help me or suggest an other solution ?

I don't think it is possible to reattach to a process started in one
ssh session from another ssh session.  However, you can put a session
into the background with the -f flag to ssh.  You haven't told us what
operating system you are starting the ssh connection on and what
system will run the R process.  If I had an X server running on the
local system and the remote system provided X clients like xterm I
would do this by

ssh -X -f user at server xterm

This should spring up an autonomous xterm window on the local machine
after which you can run R in it.

I hope this helps.


From philipp.pagel.lists at t-online.de  Mon Feb 19 15:56:45 2007
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Mon, 19 Feb 2007 15:56:45 +0100
Subject: [R] categorical column to numeric column
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC30104A80E@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC30104A80E@BAN-MAILSRV03.Amba.com>
Message-ID: <20070219145645.GA29601@gsf.de>


> Let 'dd' be a data frame given as:
> 
> dd=data.frame(aa=c("a","a","b","a","b","b"),bb=c(1,1,1,2,3,4))
> 
> Now I want to create a column 'g' such that if dd$aa=a then dd$g=1 else
> dd$g= -1 .

You need to use ifelse instead of the if ... else construction:

dd$g = ifelse(dd$a=='a', 1, -1)

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From pierrelap at gmail.com  Mon Feb 19 15:58:31 2007
From: pierrelap at gmail.com (Pierre Lapointe)
Date: Mon, 19 Feb 2007 09:58:31 -0500
Subject: [R] Need to find most likely betas
Message-ID: <676b0d530702190658n64acaf6btbe660b654888467f@mail.gmail.com>

Hello,

I have a particular situation where a single "wrong" observation is
impacting the results of a traditional regression to the point that
betas become unreliable.  I need a way to calculate the most likely
betas.  Here's an example:

set.seed(1)
unknownbeta <- matrix(seq(100,500,100),25,5,byrow=TRUE)
x <-matrix(runif(25*5),25)
y <- rowSums(unknownbeta*x)
summary(lm(y~0+x)) #gets back the unknown betas.

#Now, let's introduce a single wrong data.

unknownbeta[25,5] <-100
y <- rowSums(unknownbeta*x)
summary(lm(y~0+x)) #every beta changes.

I need to find out what are the most likely betas in the second
example.  There is no obvious way to know that row 25 has wrong input.
I would even be happy if the conclusion was that x1:x4 are 100, 200,
300 and 400 and that x5 is zero.

Thanks


From RKrug at sun.ac.za  Mon Feb 19 16:09:34 2007
From: RKrug at sun.ac.za (Rainer M Krug)
Date: Mon, 19 Feb 2007 17:09:34 +0200
Subject: [R] Start and Restart R over SSH
In-Reply-To: <40e66e0b0702190640r7531cc73n14ea0af2974e8226@mail.gmail.com>
References: <1171893364.14607.6.camel@nils-laptop>
	<40e66e0b0702190640r7531cc73n14ea0af2974e8226@mail.gmail.com>
Message-ID: <45D9BDAE.2030208@sun.ac.za>

Douglas Bates wrote:
> On 2/19/07, Nils H?ller <mail at nhoeller.de> wrote:
>> Hi,
>>
>> I have some big calculations in R to be done.
>> Since I can use R on a server with ssh, i was wondering if I can reopen
>> a R Shell after exiting ssh.
>>
>> I don't want to use the batch mode and nohup doesn't work.
>>
>> I want to use something like
>> ssh user at server
>> R
>> ---do something in R and start calculation ---
>> close ssh but let R remain on the server, doing the calculation
>>
>> ssh user at server
>> open the existing R Shell / Process
>>
>> Has anyone done something similiar?
>> Can you help me or suggest an other solution ?
> 
> I don't think it is possible to reattach to a process started in one
> ssh session from another ssh session.  However, you can put a session

No, it is. Check out the screen command - it is backgrounding your 
session, you can have several running at the same time and reattach and 
they persist between logouts (obviously, if you switch the computer of, 
it is gone...)

Rainer

> into the background with the -f flag to ssh.  You haven't told us what
> operating system you are starting the ssh connection on and what
> system will run the R process.  If I had an X server running on the
> local system and the remote system provided X clients like xterm I
> would do this by
> 
> ssh -X -f user at server xterm
> 
> This should spring up an autonomous xterm window on the local machine
> after which you can run R in it.
> 
> I hope this helps.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)86 516 2782
Fax:		+27 - (0)21 808 3304 (w)
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From johannes_graumann at web.de  Mon Feb 19 16:13:27 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 19 Feb 2007 16:13:27 +0100
Subject: [R] Another subsetting enigma
Message-ID: <erceqn$i08$1@sea.gmane.org>

Hello again,

I'm trying to do the following:

subset(dataframe,list %in% strsplit(dataframe[[Field]],","))

But This returns always the complete dataframe, since the
strsplit(dataframe[[Field]],",") is evaluated as one big list for the whole
data frame rather than one list per row. How can I have this evaluated on a
per row basis?

After 1.5 h hitting head against wall - begging for insights ...

Joh


From ripley at stats.ox.ac.uk  Mon Feb 19 16:13:57 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Feb 2007 15:13:57 +0000 (GMT)
Subject: [R] Documenting options specific to a package.
In-Reply-To: <200702191435.l1JEZtTT029753@weisner.math.unb.ca>
References: <200702191435.l1JEZtTT029753@weisner.math.unb.ca>
Message-ID: <Pine.LNX.4.64.0702191508071.18113@gannet.stats.ox.ac.uk>

(An R-devel topic, I believe.)

I would have documentation for options() in the package that documented 
the additional options and linked to \code{\link[base]{options}}.
Users will be given a choice of which page to view on most systems, so 
just make sure the title makes clear that this is options for the package.

On Mon, 19 Feb 2007, rolf at math.unb.ca wrote:

> If one specifies new options in a package, using the options()
> function, where does/should one document these new options?
>
> E.g. suppose that I put the line
>
> 	options(melvin=42)
>
> in a file zzz.R in the R directory of the package source, where the
> package contains functions foo(), bar(), clyde(), and irving() which
> all query options("melvin") and take some action based on the value
> of ``melvin''.  The user can of course change the value of ``melvin''
> from its default value by doing, e.g.
>
>        options(melvin=99)
>
> Are there any conventions or standards as to how and where the option
> ``melvin'' should be documented?  It seems somewhat redundant to
> include docmentation about melvin in the help on all 4 of foo(),
> bar(), clyde(), and irving().
>
> Thanks for any insights.
>
>                                cheers,
>
>                                        Rolf Turner
>                                        rolf at math.unb.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bcarvalh at jhsph.edu  Mon Feb 19 16:14:41 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 19 Feb 2007 10:14:41 -0500
Subject: [R] Start and Restart R over SSH
In-Reply-To: <40e66e0b0702190640r7531cc73n14ea0af2974e8226@mail.gmail.com>
References: <1171893364.14607.6.camel@nils-laptop>
	<40e66e0b0702190640r7531cc73n14ea0af2974e8226@mail.gmail.com>
Message-ID: <1C710272-FB3C-4016-9D8E-2A5DCFF299E2@jhsph.edu>

Hi Nils,

if the server you're using is *NIX, this is what you can do:

%%%% example
ssh user at server
screen
R

<< do what you need in R>>
<< close the terminal without quitting R >>

ssh user at server
screen -r
<< continue working in R >>
%%%%%% end example

the "problem" is if you need X... it works until you quit the  
terminal, but "screen -r" doesn't reconnect the X11.

b


On Feb 19, 2007, at 9:40 AM, Douglas Bates wrote:

> On 2/19/07, Nils H?ller <mail at nhoeller.de> wrote:
>> Hi,
>>
>> I have some big calculations in R to be done.
>> Since I can use R on a server with ssh, i was wondering if I can  
>> reopen
>> a R Shell after exiting ssh.
>>
>> I don't want to use the batch mode and nohup doesn't work.
>>
>> I want to use something like
>> ssh user at server
>> R
>> ---do something in R and start calculation ---
>> close ssh but let R remain on the server, doing the calculation
>>
>> ssh user at server
>> open the existing R Shell / Process
>>
>> Has anyone done something similiar?
>> Can you help me or suggest an other solution ?
>
> I don't think it is possible to reattach to a process started in one
> ssh session from another ssh session.  However, you can put a session
> into the background with the -f flag to ssh.  You haven't told us what
> operating system you are starting the ssh connection on and what
> system will run the R process.  If I had an X server running on the
> local system and the remote system provided X clients like xterm I
> would do this by
>
> ssh -X -f user at server xterm
>
> This should spring up an autonomous xterm window on the local machine
> after which you can run R in it.
>
> I hope this helps.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From andza at osi.lv  Mon Feb 19 16:16:07 2007
From: andza at osi.lv (Andris Jankevics)
Date: Mon, 19 Feb 2007 17:16:07 +0200
Subject: [R] Start and Restart R over SSH
In-Reply-To: <1171893364.14607.6.camel@nils-laptop>
References: <1171893364.14607.6.camel@nils-laptop>
Message-ID: <200702191716.07969.andza@osi.lv>

Hi,
You can take a look at GNU screen programm:
http://www.gnu.org/software/screen/

Andris Jankevics

On Pirmdiena, 19. Febru?ris 2007 15:56, Nils H?ller wrote:
> Hi,
>
> I have some big calculations in R to be done.
> Since I can use R on a server with ssh, i was wondering if I can reopen
> a R Shell after exiting ssh.
>
> I don't want to use the batch mode and nohup doesn't work.
>
> I want to use something like
> ssh user at server
> R
> ---do something in R and start calculation ---
> close ssh but let R remain on the server, doing the calculation
>
> ssh user at server
> open the existing R Shell / Process
>
> Has anyone done something similiar?
> Can you help me or suggest an other solution ?
>
> Thank you for your help
>
> Nils
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From Paolo.Accadia at port.ac.uk  Mon Feb 19 16:21:00 2007
From: Paolo.Accadia at port.ac.uk (Paolo Accadia)
Date: Mon, 19 Feb 2007 15:21:00 +0000
Subject: [R] summary polr
Message-ID: <45D9C05C020000290000A424@stirling.iso.port.ac.uk>

Hi all,

I have a problem to estimate Std. Error and t-value by ?polr? in library Mass.
They result from the summary of a polr object.

I can obtain them working in the R environment with the following statements:

     temp <- polr(formula = formula1,  data = data1)
     coeff <- summary(temp),

but when the above statements are enclosed in a function, summary reports the following error:

Error in eval(expr, envir, enclos) : object "dat" not found 

Someone knows how I can solve the problem?

Thanks for any help.
Paolo


From mark at wardle.org  Mon Feb 19 16:45:39 2007
From: mark at wardle.org (Mark Wardle)
Date: Mon, 19 Feb 2007 15:45:39 +0000
Subject: [R] Start and Restart R over SSH
In-Reply-To: <1171893364.14607.6.camel@nils-laptop>
References: <1171893364.14607.6.camel@nils-laptop>
Message-ID: <45D9C623.6060908@wardle.org>

Nils H?ller wrote:
> Hi,
>  
> I have some big calculations in R to be done. 
> Since I can use R on a server with ssh, i was wondering if I can reopen
> a R Shell after exiting ssh.
> 
> I don't want to use the batch mode and nohup doesn't work.
> 
> I want to use something like
> ssh user at server
> R
> ---do something in R and start calculation ---
> close ssh but let R remain on the server, doing the calculation
> 
> ssh user at server
> open the existing R Shell / Process
> 
> Has anyone done something similiar?
> Can you help me or suggest an other solution ? 
> 

Use "screen" http://www.gnu.org/software/screen/

1. Login to remote server
2. Run screen
3. Run R, and the long calculation
4. Detach screen (Ctrl-A, Ctrl-D)
5. Logout

Then you may login to server again, re-attach to the running screen and
carry on!

Best wishes,

Mark
-- 
Specialist Registrar and Clinical research fellow
Department of Neurology
Cardiff & Vale NHS Trust and Cardiff University


From drf5n at maplepark.com  Mon Feb 19 17:15:32 2007
From: drf5n at maplepark.com (David Forrest)
Date: Mon, 19 Feb 2007 10:15:32 -0600 (CST)
Subject: [R] Start and Restart R over SSH
In-Reply-To: <1C710272-FB3C-4016-9D8E-2A5DCFF299E2@jhsph.edu>
References: <1171893364.14607.6.camel@nils-laptop>
	<40e66e0b0702190640r7531cc73n14ea0af2974e8226@mail.gmail.com>
	<1C710272-FB3C-4016-9D8E-2A5DCFF299E2@jhsph.edu>
Message-ID: <Pine.LNX.4.64.0702190959590.19318@maplepark.com>

On Mon, 19 Feb 2007, Benilton Carvalho wrote:

> Hi Nils,
>
> if the server you're using is *NIX, this is what you can do:
>
> %%%% example
> ssh user at server
> screen
> R
>
> << do what you need in R>>
> << close the terminal without quitting R >>
>
> ssh user at server
> screen -r
> << continue working in R >>
> %%%%%% end example
>
> the "problem" is if you need X... it works until you quit the
> terminal, but "screen -r" doesn't reconnect the X11.

I find vnc sometimes helps with keeping a X active for an R session.

Also, if you take care to close your x11 devices, and if when you re-ssh 
you happen to get the same forwarded X DISPLAY=localhost:11.0 or whatever, 
the R session in screen will use the new forwarded connection.

It also seems like using Sys.putenv(DISPLAY='localhost:11.0')  overrides 
the DISPLAY that screen's child R session inherits from the initial 
invocation of screen.

Dave
-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From jholtman at gmail.com  Mon Feb 19 17:25:31 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 19 Feb 2007 11:25:31 -0500
Subject: [R] need help in reading TOMS observed ASCII data file
In-Reply-To: <71cc5ca20702190423x17a6b55k864bc263d1fb4fc3@mail.gmail.com>
References: <71cc5ca20702190423x17a6b55k864bc263d1fb4fc3@mail.gmail.com>
Message-ID: <644e1f320702190825u7dc096f7r55c27ad5bb6b6df9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/c87eeae7/attachment.pl 

From Greg.Snow at intermountainmail.org  Mon Feb 19 17:45:49 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 19 Feb 2007 09:45:49 -0700
Subject: [R] advanced plotting
References: <7479700.post@talk.nabble.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A107@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/86d93b37/attachment.pl 

From jholtman at gmail.com  Mon Feb 19 17:54:33 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 19 Feb 2007 11:54:33 -0500
Subject: [R] advanced plotting
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB12A107@LP-EXCHVS07.CO.IHC.COM>
References: <7479700.post@talk.nabble.com>
	<07E228A5BE53C24CAD490193A7381BBB12A107@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <644e1f320702190854u4e0a4b4dp2dd829bb01896b05@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/ecc749a9/attachment.pl 

From spyros.mesomeris at citigroup.com  Mon Feb 19 17:59:25 2007
From: spyros.mesomeris at citigroup.com (Mesomeris, Spyros [CIR])
Date: Mon, 19 Feb 2007 16:59:25 -0000
Subject: [R] Help for constrained linear regression
Message-ID: <0D9D1AC09BC016428D7597A1716AFF710125FDBE@Exukmb73.eur.nsroot.net>

Hello,

I am trying to run a constrained linear regression of a dependent
variable y on three explanatory variables x1, x2, and x3. However, I
need to constrain the coefficients of the explanatory variables as
follows:  b1>0, b2<0, and b3>0. 

Does anyone know how to do this within the lm/glm framework so that one
can still ask for statistics like the r-squared and t-ratios (I realize
that in this case t-stats may be harder to compute than usual and may
not be available from the lm/glm object)?

Any help would be greatly appreciated

Spyros


From marc_schwartz at comcast.net  Mon Feb 19 18:02:57 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 19 Feb 2007 11:02:57 -0600
Subject: [R] Another subsetting enigma
In-Reply-To: <erceqn$i08$1@sea.gmane.org>
References: <erceqn$i08$1@sea.gmane.org>
Message-ID: <1171904577.6873.10.camel@localhost.localdomain>

On Mon, 2007-02-19 at 16:13 +0100, Johannes Graumann wrote:
> Hello again,
> 
> I'm trying to do the following:
> 
> subset(dataframe,list %in% strsplit(dataframe[[Field]],","))
> 
> But This returns always the complete dataframe, since the
> strsplit(dataframe[[Field]],",") is evaluated as one big list for the whole
> data frame rather than one list per row. How can I have this evaluated on a
> per row basis?
> 
> After 1.5 h hitting head against wall - begging for insights ...
> 
> Joh


I may be misunderstanding your desired end result, but what about:

  subset(dataframe,list %in% strsplit(Field, ","))


This way 'Field' is evaluated within the data frame (default behavior
for 'subset') on a row-wise basis, rather than passing the entire
column.

This entire process also presumes that 'Field' has embedded commas,
delimiting component entries. If not, then strsplit() is not required.

HTH,

Marc Schwartz


From marc_schwartz at comcast.net  Mon Feb 19 18:15:10 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 19 Feb 2007 11:15:10 -0600
Subject: [R] Need to find most likely betas
In-Reply-To: <676b0d530702190658n64acaf6btbe660b654888467f@mail.gmail.com>
References: <676b0d530702190658n64acaf6btbe660b654888467f@mail.gmail.com>
Message-ID: <1171905310.6873.23.camel@localhost.localdomain>

On Mon, 2007-02-19 at 09:58 -0500, Pierre Lapointe wrote:
> Hello,
> 
> I have a particular situation where a single "wrong" observation is
> impacting the results of a traditional regression to the point that
> betas become unreliable.  I need a way to calculate the most likely
> betas.  Here's an example:
> 
> set.seed(1)
> unknownbeta <- matrix(seq(100,500,100),25,5,byrow=TRUE)
> x <-matrix(runif(25*5),25)
> y <- rowSums(unknownbeta*x)
> summary(lm(y~0+x)) #gets back the unknown betas.
> 
> #Now, let's introduce a single wrong data.
> 
> unknownbeta[25,5] <-100
> y <- rowSums(unknownbeta*x)
> summary(lm(y~0+x)) #every beta changes.
> 
> I need to find out what are the most likely betas in the second
> example.  There is no obvious way to know that row 25 has wrong input.
> I would even be happy if the conclusion was that x1:x4 are 100, 200,
> 300 and 400 and that x5 is zero.
> 
> Thanks

It is not clear what you mean by a "wrong" observation.  Is the data
completely bad because it was improperly collected?  Is this an
observation that has correct data, but is an "outlier" relative to the
other observations? Is the observation missing data, where values can be
reasonably imputed?

Are you in a setting where the observation MUST be included in the
regression rather than be deleted? For example an "Intent to Treat"
analysis in a clinical trial?

Depending upon the context, your options may range from simply removing
the single observation from the regression, considering some form of
weighting of the observations, to perhaps considering a robust
regression methodology and others.

This is not strictly an R question, but one of methodology.
Clarification of which is potentially impacted upon by "community"
standards and prior work within your particular discipline.

HTH,

Marc Schwartz


From johannes_graumann at web.de  Mon Feb 19 18:25:45 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 19 Feb 2007 18:25:45 +0100
Subject: [R] Another subsetting enigma
In-Reply-To: <644e1f320702190849r4ddb8c12xfb11b68b620b49b1@mail.gmail.com>
References: <erceqn$i08$1@sea.gmane.org>
	<644e1f320702190849r4ddb8c12xfb11b68b620b49b1@mail.gmail.com>
Message-ID: <200702191825.59037.johannes_graumann@web.de>

On Monday 19 February 2007 17:49, jim holtman wrote:
> have you tried 'unlist(strsplit(....))'?    The 'unlist' will probably give
> you more values than you have rows, so it would be good if you could
> explain what it is you are trying to do.
I want to isolate a data frame containing all rows of 'dataframe' which 
contain a member of 'list' in the field 'Field', which itself is a comma 
separated list of entries.

Better this (know in brain ...)?

Joh

> On 2/19/07, Johannes Graumann <johannes_graumann at web.de> wrote:
> > Hello again,
> >
> > I'm trying to do the following:
> >
> > subset(dataframe,list %in% strsplit(dataframe[[Field]],","))
> >
> > But This returns always the complete dataframe, since the
> > strsplit(dataframe[[Field]],",") is evaluated as one big list for the
> > whole
> > data frame rather than one list per row. How can I have this evaluated on
> > a
> > per row basis?
> >
> > After 1.5 h hitting head against wall - begging for insights ...
> >
> > Joh
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 827 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070219/3d05a24c/attachment.bin 

From pedif at web.de  Mon Feb 19 18:56:13 2007
From: pedif at web.de (Petra Finkenbein)
Date: Mon, 19 Feb 2007 18:56:13 +0100
Subject: [R] multiple comparisons
Message-ID: <1150449132@web.de>

Dear "R" users,
I need a professional help:

i am a relatively new "R" user and I am just writing my diploma tesis where I have to conduct some multiple comparison.
I am searching a method which include interaction between fixed factors. The following is my model: lmer(Leaf~water*region+(1|pop)+(1|pop:sib)+(1|block)+(1|block:pool),data=datx,method="ML")
I want to compare: leaf dependent from water * region.

I would prefer to make a comparision according to tukey or student newman keuls if this is possible. 
I am looking forward to your reply, sincerely Petra:-)


From jholtman at gmail.com  Mon Feb 19 19:03:14 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 19 Feb 2007 13:03:14 -0500
Subject: [R] Another subsetting enigma
In-Reply-To: <200702191825.59037.johannes_graumann@web.de>
References: <erceqn$i08$1@sea.gmane.org>
	<644e1f320702190849r4ddb8c12xfb11b68b620b49b1@mail.gmail.com>
	<200702191825.59037.johannes_graumann@web.de>
Message-ID: <644e1f320702191003l48aad05dw1603e55b520530b9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/5da6684a/attachment.pl 

From amnakhan493 at gmail.com  Mon Feb 19 19:11:54 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Mon, 19 Feb 2007 10:11:54 -0800
Subject: [R] Lengend function and moving average plot
Message-ID: <3ffd3bb60702191011j24ad0344of3ce341c2493dde5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/a7c3e2c0/attachment.pl 

From f.calboli at imperial.ac.uk  Mon Feb 19 19:08:59 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 19 Feb 2007 18:08:59 +0000
Subject: [R] memory management uestion
Message-ID: <45D9E7BB.6010902@imperial.ac.uk>

Hi All,

I would like to ask the following.

I have an array of data in an objetct, let's say X.

I need to use a for loop on the elements of one or more columns of X and I am 
having a debate with a colleague about the best memory management.

I believe that if I do:

col1 = X[,1]
col2 = X[,2]
...
colx = X[,x]


and then

for(i in whatever){
do something using col1[i], col2[i] ... colx[i]
}

my memory management is better that doing:

for(i in whatever){
do something using X[i,1], X[i,2] ... X[,x]
}

BTW, here I *have to* use a for() loop an no nifty tapply, lapply and family.

Any comment is welcome.

Best,

Fede

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From pierrelap at gmail.com  Mon Feb 19 19:32:21 2007
From: pierrelap at gmail.com (Pierre Lapointe)
Date: Mon, 19 Feb 2007 13:32:21 -0500
Subject: [R] Need to find most likely betas
In-Reply-To: <1171905310.6873.23.camel@localhost.localdomain>
References: <676b0d530702190658n64acaf6btbe660b654888467f@mail.gmail.com>
	<1171905310.6873.23.camel@localhost.localdomain>
Message-ID: <676b0d530702191032g2e97e4d1v4515770f0a267737@mail.gmail.com>

Hi Mark,

In my example, there has been a regime change at time 25 and I'd like
to find a way to discover 1- what has changed and 2- when it did.

The problem is all that is observed are the x and y values.
unknownbetas are... unknown. If you look at x and y, you can't really
tell something has changed.

It is not an outlier per se as it involves a change of one of the unknownbetas.

In other words, I'm trying to single out which unknownbetas vs. x
relationships still hold after time 25.

I know it's complicated, but I you have any pointers, it will be appreciated.

Thanks

On 2/19/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Mon, 2007-02-19 at 09:58 -0500, Pierre Lapointe wrote:
> > Hello,
> >
> > I have a particular situation where a single "wrong" observation is
> > impacting the results of a traditional regression to the point that
> > betas become unreliable.  I need a way to calculate the most likely
> > betas.  Here's an example:
> >
> > set.seed(1)
> > unknownbeta <- matrix(seq(100,500,100),25,5,byrow=TRUE)
> > x <-matrix(runif(25*5),25)
> > y <- rowSums(unknownbeta*x)
> > summary(lm(y~0+x)) #gets back the unknown betas.
> >
> > #Now, let's introduce a single wrong data.
> >
> > unknownbeta[25,5] <-100
> > y <- rowSums(unknownbeta*x)
> > summary(lm(y~0+x)) #every beta changes.
> >
> > I need to find out what are the most likely betas in the second
> > example.  There is no obvious way to know that row 25 has wrong input.
> > I would even be happy if the conclusion was that x1:x4 are 100, 200,
> > 300 and 400 and that x5 is zero.
> >
> > Thanks
>
> It is not clear what you mean by a "wrong" observation.  Is the data
> completely bad because it was improperly collected?  Is this an
> observation that has correct data, but is an "outlier" relative to the
> other observations? Is the observation missing data, where values can be
> reasonably imputed?
>
> Are you in a setting where the observation MUST be included in the
> regression rather than be deleted? For example an "Intent to Treat"
> analysis in a clinical trial?
>
> Depending upon the context, your options may range from simply removing
> the single observation from the regression, considering some form of
> weighting of the observations, to perhaps considering a robust
> regression methodology and others.
>
> This is not strictly an R question, but one of methodology.
> Clarification of which is potentially impacted upon by "community"
> standards and prior work within your particular discipline.
>
> HTH,
>
> Marc Schwartz
>
>
>


From bessa_ricardo at hotmail.com  Mon Feb 19 19:00:17 2007
From: bessa_ricardo at hotmail.com (Ricardo Bessa)
Date: Mon, 19 Feb 2007 18:00:17 +0000
Subject: [R] Help on prediction intervals
Message-ID: <BAY23-F1490ACB3CDB2D2CAA6BE0968A0@phx.gbl>

I am trying to estimate prediction intervals for wind power production, it 
is a times series of wind power production. I?m using neural networks for 
prediction, but I need to estimate the uncertainly in the prediction. How 
can I do this in R if I have a probability distribution that is right 
skewness. One tip is to use bootstrap but I don?t know if is possible with 
this method to estimate those intervals.

Any help would be greatly appreciated

Ricardo Bessa


From marc_schwartz at comcast.net  Mon Feb 19 19:58:42 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 19 Feb 2007 12:58:42 -0600
Subject: [R] Need to find most likely betas
In-Reply-To: <676b0d530702191032g2e97e4d1v4515770f0a267737@mail.gmail.com>
References: <676b0d530702190658n64acaf6btbe660b654888467f@mail.gmail.com>
	<1171905310.6873.23.camel@localhost.localdomain>
	<676b0d530702191032g2e97e4d1v4515770f0a267737@mail.gmail.com>
Message-ID: <1171911522.6873.33.camel@localhost.localdomain>

Pierre,

Unfortunately, I don't have much in the way of "hands on" experience
with these, but conceptually, latent variable analysis/SEM methods seem
like they might be apropos. If so, John Fox' SEM package might be of
value here. More information here:

http://socserv.mcmaster.ca/jfox/Misc/sem/index.html


Since you mention time, if this is a repeated measures based approach,
then you might want to look at lmer(), for which there is a recently
created SIG list. More information is at:

https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


Perhaps others will jump in with additional thoughts.

HTH,

Marc

On Mon, 2007-02-19 at 13:32 -0500, Pierre Lapointe wrote:
> Hi Mark,
> 
> In my example, there has been a regime change at time 25 and I'd like
> to find a way to discover 1- what has changed and 2- when it did.
> 
> The problem is all that is observed are the x and y values.
> unknownbetas are... unknown. If you look at x and y, you can't really
> tell something has changed.
> 
> It is not an outlier per se as it involves a change of one of the unknownbetas.
> 
> In other words, I'm trying to single out which unknownbetas vs. x
> relationships still hold after time 25.
> 
> I know it's complicated, but I you have any pointers, it will be appreciated.
> 
> Thanks
> 
> On 2/19/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > On Mon, 2007-02-19 at 09:58 -0500, Pierre Lapointe wrote:
> > > Hello,
> > >
> > > I have a particular situation where a single "wrong" observation is
> > > impacting the results of a traditional regression to the point that
> > > betas become unreliable.  I need a way to calculate the most likely
> > > betas.  Here's an example:
> > >
> > > set.seed(1)
> > > unknownbeta <- matrix(seq(100,500,100),25,5,byrow=TRUE)
> > > x <-matrix(runif(25*5),25)
> > > y <- rowSums(unknownbeta*x)
> > > summary(lm(y~0+x)) #gets back the unknown betas.
> > >
> > > #Now, let's introduce a single wrong data.
> > >
> > > unknownbeta[25,5] <-100
> > > y <- rowSums(unknownbeta*x)
> > > summary(lm(y~0+x)) #every beta changes.
> > >
> > > I need to find out what are the most likely betas in the second
> > > example.  There is no obvious way to know that row 25 has wrong input.
> > > I would even be happy if the conclusion was that x1:x4 are 100, 200,
> > > 300 and 400 and that x5 is zero.
> > >
> > > Thanks
> >
> > It is not clear what you mean by a "wrong" observation.  Is the data
> > completely bad because it was improperly collected?  Is this an
> > observation that has correct data, but is an "outlier" relative to the
> > other observations? Is the observation missing data, where values can be
> > reasonably imputed?
> >
> > Are you in a setting where the observation MUST be included in the
> > regression rather than be deleted? For example an "Intent to Treat"
> > analysis in a clinical trial?
> >
> > Depending upon the context, your options may range from simply removing
> > the single observation from the regression, considering some form of
> > weighting of the observations, to perhaps considering a robust
> > regression methodology and others.
> >
> > This is not strictly an R question, but one of methodology.
> > Clarification of which is potentially impacted upon by "community"
> > standards and prior work within your particular discipline.
> >
> > HTH,
> >
> > Marc Schwartz
> >
> >
> >


From rab45+ at pitt.edu  Mon Feb 19 20:35:29 2007
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Mon, 19 Feb 2007 14:35:29 -0500
Subject: [R] Installing Package rgl - Compilation Fails
Message-ID: <1171913730.4041.7.camel@localhost.localdomain>

I'm running  R 2.4.1  (with the latest versions of all packages) on an
FC6 32-bit system. When I try to install the rgl package, compilation
fails:

> install.packages("rgl")
--- Please select a CRAN mirror for use in this session ---
Loading Tcl/Tk interface ... done
trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/rgl_0.70.tar.gz'
Content type 'application/x-gzip' length 705556 bytes
opened URL
==================================================
downloaded 689Kb

* Installing *source* package 'rgl' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for X... libraries , headers 
checking for libpng-config... yes
configure: using libpng-config
configure: using libpng dynamic linkage
configure: creating ./config.status
config.status: creating src/Makevars
** libs
g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H
-I/usr/include/libpng12 -Iext -I/usr/local/include    -fpic  -O2 -g
-pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
--param=ssp-buffer-size=4 -m32 -march=i386 -mtune=generic
-fasynchronous-unwind-tables -c api.cpp -o api.o
In file included from glgui.hpp:9,
                 from gui.hpp:11,
                 from rglview.h:10,
                 from Device.hpp:11,
                 from DeviceManager.hpp:9,
                 from api.cpp:14:
opengl.hpp:24:20: error: GL/glu.h: No such file or directory
Disposable.hpp:13: warning: ?struct IDisposeListener? has virtual
functions but non-virtual destructor
types.h:77: warning: ?class DestroyHandler? has virtual functions but
non-virtual destructor
gui.hpp:56: warning: ?class gui::WindowImpl? has virtual functions but
non-virtual destructor
gui.hpp:90: warning: ?class gui::GUIFactory? has virtual functions but
non-virtual destructor
pixmap.h:39: warning: ?class PixmapFormat? has virtual functions but
non-virtual destructor
api.cpp: In function ?void rgl_user2window(int*, int*, double*, double*,
double*, double*, int*)?:
api.cpp:764: error: ?gluProject? was not declared in this scope
api.cpp: In function ?void rgl_window2user(int*, int*, double*, double*,
double*, double*, int*)?:
api.cpp:792: error: ?gluUnProject? was not declared in this scope
make: *** [api.o] Error 1
chmod: cannot access `/usr/lib/R/library/rgl/libs/*': No such file or
directory
ERROR: compilation failed for package 'rgl'
** Removing '/usr/lib/R/library/rgl'

The downloaded packages are in
        /tmp/RtmpJY8uNp/downloaded_packages
Warning message:
installation of package 'rgl' had non-zero exit status in:
install.packages("rgl") 

I was able to install this on an 64-bit system running FC4 and R 2.4.1.

Any ideas on why it fails on FC6?

Rick B.


From osklyar at ebi.ac.uk  Mon Feb 19 20:56:31 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Mon, 19 Feb 2007 19:56:31 +0000
Subject: [R] Installing Package rgl - Compilation Fails
In-Reply-To: <1171913730.4041.7.camel@localhost.localdomain>
References: <1171913730.4041.7.camel@localhost.localdomain>
Message-ID: <45DA00EF.2050305@ebi.ac.uk>

Check again your error message:

opengl.hpp:24:20: error: GL/glu.h: No such file or directory

you need to install

mesa-libGLU-devel FC6 version is 6.5.1-7

which will provide development files for glut3. Needless to say the 
above will probably pool some dependencies and (-devel) means it will 
install *.h files as well. Start your FC package manager and search for 
"GLU", install the above and try again.

Best,
  Oleg

Rick Bilonick wrote:
> I'm running  R 2.4.1  (with the latest versions of all packages) on an
> FC6 32-bit system. When I try to install the rgl package, compilation
> fails:
> 
>> install.packages("rgl")
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/rgl_0.70.tar.gz'
> Content type 'application/x-gzip' length 705556 bytes
> opened URL
> ==================================================
> downloaded 689Kb
> 
> * Installing *source* package 'rgl' ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables... 
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for X... libraries , headers 
> checking for libpng-config... yes
> configure: using libpng-config
> configure: using libpng dynamic linkage
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H
> -I/usr/include/libpng12 -Iext -I/usr/local/include    -fpic  -O2 -g
> -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
> --param=ssp-buffer-size=4 -m32 -march=i386 -mtune=generic
> -fasynchronous-unwind-tables -c api.cpp -o api.o
> In file included from glgui.hpp:9,
>                  from gui.hpp:11,
>                  from rglview.h:10,
>                  from Device.hpp:11,
>                  from DeviceManager.hpp:9,
>                  from api.cpp:14:
> opengl.hpp:24:20: error: GL/glu.h: No such file or directory
> Disposable.hpp:13: warning: ?struct IDisposeListener? has virtual
> functions but non-virtual destructor
> types.h:77: warning: ?class DestroyHandler? has virtual functions but
> non-virtual destructor
> gui.hpp:56: warning: ?class gui::WindowImpl? has virtual functions but
> non-virtual destructor
> gui.hpp:90: warning: ?class gui::GUIFactory? has virtual functions but
> non-virtual destructor
> pixmap.h:39: warning: ?class PixmapFormat? has virtual functions but
> non-virtual destructor
> api.cpp: In function ?void rgl_user2window(int*, int*, double*, double*,
> double*, double*, int*)?:
> api.cpp:764: error: ?gluProject? was not declared in this scope
> api.cpp: In function ?void rgl_window2user(int*, int*, double*, double*,
> double*, double*, int*)?:
> api.cpp:792: error: ?gluUnProject? was not declared in this scope
> make: *** [api.o] Error 1
> chmod: cannot access `/usr/lib/R/library/rgl/libs/*': No such file or
> directory
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/lib/R/library/rgl'
> 
> The downloaded packages are in
>         /tmp/RtmpJY8uNp/downloaded_packages
> Warning message:
> installation of package 'rgl' had non-zero exit status in:
> install.packages("rgl") 
> 
> I was able to install this on an 64-bit system running FC4 and R 2.4.1.
> 
> Any ideas on why it fails on FC6?
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


From maitra at iastate.edu  Mon Feb 19 20:59:25 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Mon, 19 Feb 2007 13:59:25 -0600
Subject: [R] Installing Package rgl - Compilation Fails
In-Reply-To: <1171913730.4041.7.camel@localhost.localdomain>
References: <1171913730.4041.7.camel@localhost.localdomain>
Message-ID: <20070219135925.63b3ffef@subarnarekha.stat.iastate.edu>

As the error message indicates, there is no GL/glu.h file installed in the system. If it is, the path is not properly set.

% yum provides GL/glu.h

on FC6 should give you some clues and tell you what to install, and also whether it should be installed.

Ranjan


On Mon, 19 Feb 2007 14:35:29 -0500 Rick Bilonick <rab45+ at pitt.edu> wrote:

> I'm running  R 2.4.1  (with the latest versions of all packages) on an
> FC6 32-bit system. When I try to install the rgl package, compilation
> fails:
> 
> > install.packages("rgl")
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/rgl_0.70.tar.gz'
> Content type 'application/x-gzip' length 705556 bytes
> opened URL
> ==================================================
> downloaded 689Kb
> 
> * Installing *source* package 'rgl' ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables... 
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for X... libraries , headers 
> checking for libpng-config... yes
> configure: using libpng-config
> configure: using libpng dynamic linkage
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H
> -I/usr/include/libpng12 -Iext -I/usr/local/include    -fpic  -O2 -g
> -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
> --param=ssp-buffer-size=4 -m32 -march=i386 -mtune=generic
> -fasynchronous-unwind-tables -c api.cpp -o api.o
> In file included from glgui.hpp:9,
>                  from gui.hpp:11,
>                  from rglview.h:10,
>                  from Device.hpp:11,
>                  from DeviceManager.hpp:9,
>                  from api.cpp:14:
> opengl.hpp:24:20: error: GL/glu.h: No such file or directory
> Disposable.hpp:13: warning: ___struct IDisposeListener___ has virtual
> functions but non-virtual destructor
> types.h:77: warning: ___class DestroyHandler___ has virtual functions but
> non-virtual destructor
> gui.hpp:56: warning: ___class gui::WindowImpl___ has virtual functions but
> non-virtual destructor
> gui.hpp:90: warning: ___class gui::GUIFactory___ has virtual functions but
> non-virtual destructor
> pixmap.h:39: warning: ___class PixmapFormat___ has virtual functions but
> non-virtual destructor
> api.cpp: In function ___void rgl_user2window(int*, int*, double*, double*,
> double*, double*, int*)___:
> api.cpp:764: error: ___gluProject___ was not declared in this scope
> api.cpp: In function ___void rgl_window2user(int*, int*, double*, double*,
> double*, double*, int*)___:
> api.cpp:792: error: ___gluUnProject___ was not declared in this scope
> make: *** [api.o] Error 1
> chmod: cannot access `/usr/lib/R/library/rgl/libs/*': No such file or
> directory
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/lib/R/library/rgl'
> 
> The downloaded packages are in
>         /tmp/RtmpJY8uNp/downloaded_packages
> Warning message:
> installation of package 'rgl' had non-zero exit status in:
> install.packages("rgl") 
> 
> I was able to install this on an 64-bit system running FC4 and R 2.4.1.
> 
> Any ideas on why it fails on FC6?
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Mon Feb 19 21:04:21 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Feb 2007 20:04:21 +0000 (GMT)
Subject: [R] Installing Package rgl - Compilation Fails
In-Reply-To: <1171913730.4041.7.camel@localhost.localdomain>
References: <1171913730.4041.7.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0702192000390.10782@auk.stats>

You are missing the OpenGLU headers.  On FC5 they are in mesa-libGLU-devel
>From the README:

REQUIREMENTS
------------
Windowing System (osx/carbon, unix/x11 or win32)
OpenGL Library
OpenGL Utility Library (GLU)


On Mon, 19 Feb 2007, Rick Bilonick wrote:

> I'm running  R 2.4.1  (with the latest versions of all packages) on an
> FC6 32-bit system. When I try to install the rgl package, compilation
> fails:
>
>> install.packages("rgl")
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/rgl_0.70.tar.gz'
> Content type 'application/x-gzip' length 705556 bytes
> opened URL
> ==================================================
> downloaded 689Kb
>
> * Installing *source* package 'rgl' ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for X... libraries , headers
> checking for libpng-config... yes
> configure: using libpng-config
> configure: using libpng dynamic linkage
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H
> -I/usr/include/libpng12 -Iext -I/usr/local/include    -fpic  -O2 -g
> -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
> --param=ssp-buffer-size=4 -m32 -march=i386 -mtune=generic
> -fasynchronous-unwind-tables -c api.cpp -o api.o
> In file included from glgui.hpp:9,
>                 from gui.hpp:11,
>                 from rglview.h:10,
>                 from Device.hpp:11,
>                 from DeviceManager.hpp:9,
>                 from api.cpp:14:
> opengl.hpp:24:20: error: GL/glu.h: No such file or directory
> Disposable.hpp:13: warning: ??struct IDisposeListener?? has virtual
> functions but non-virtual destructor
> types.h:77: warning: ??class DestroyHandler?? has virtual functions but
> non-virtual destructor
> gui.hpp:56: warning: ??class gui::WindowImpl?? has virtual functions but
> non-virtual destructor
> gui.hpp:90: warning: ??class gui::GUIFactory?? has virtual functions but
> non-virtual destructor
> pixmap.h:39: warning: ??class PixmapFormat?? has virtual functions but
> non-virtual destructor
> api.cpp: In function ??void rgl_user2window(int*, int*, double*, double*,
> double*, double*, int*)??:
> api.cpp:764: error: ??gluProject?? was not declared in this scope
> api.cpp: In function ??void rgl_window2user(int*, int*, double*, double*,
> double*, double*, int*)??:
> api.cpp:792: error: ??gluUnProject?? was not declared in this scope
> make: *** [api.o] Error 1
> chmod: cannot access `/usr/lib/R/library/rgl/libs/*': No such file or
> directory
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/lib/R/library/rgl'
>
> The downloaded packages are in
>        /tmp/RtmpJY8uNp/downloaded_packages
> Warning message:
> installation of package 'rgl' had non-zero exit status in:
> install.packages("rgl")
>
> I was able to install this on an 64-bit system running FC4 and R 2.4.1.
>
> Any ideas on why it fails on FC6?
>
> Rick B.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From cberry at tajo.ucsd.edu  Mon Feb 19 21:58:20 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Mon, 19 Feb 2007 12:58:20 -0800
Subject: [R] memory management uestion
In-Reply-To: <45D9E7BB.6010902@imperial.ac.uk>
References: <45D9E7BB.6010902@imperial.ac.uk>
Message-ID: <Pine.LNX.4.64.0702191247150.2818@tajo.ucsd.edu>

On Mon, 19 Feb 2007, Federico Calboli wrote:

> Hi All,
>
> I would like to ask the following.
>
> I have an array of data in an objetct, let's say X.
>
> I need to use a for loop on the elements of one or more columns of X and I am
> having a debate with a colleague about the best memory management.


Yez guys should take this fight out into the parking lot. ;-)

Armed with gc(), system.time(), and whatever memory monitoring tools your 
OS'es provide you can pound each other with memory usage and timing stats 
till one of you screams 'uncle' or you both have had enough and decide to 
shake hands and come back inside.


>
> I believe that if I do:
>
> col1 = X[,1]
> col2 = X[,2]
> ...
> colx = X[,x]
>
>
> and then
>
> for(i in whatever){
> do something using col1[i], col2[i] ... colx[i]
> }
>
> my memory management is better that doing:
>
> for(i in whatever){
> do something using X[i,1], X[i,2] ... X[,x]
> }
>

Whoa! You are accessing one ROW at a time.

Either way this will tangle up your cache if you have many rows and 
columns in your orignal data.

You might do better to do

Y <- t( X ) ### use '<-' !

for (i in whatever ){
 	do something using Y[ , i ]
}



> BTW, here I *have to* use a for() loop an no nifty tapply, lapply and family.
>
> Any comment is welcome.
>
> Best,
>
> Fede
>
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From f.calboli at imperial.ac.uk  Mon Feb 19 22:02:48 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 19 Feb 2007 21:02:48 +0000
Subject: [R] memory management uestion
In-Reply-To: <Pine.LNX.4.64.0702191247150.2818@tajo.ucsd.edu>
References: <45D9E7BB.6010902@imperial.ac.uk>
	<Pine.LNX.4.64.0702191247150.2818@tajo.ucsd.edu>
Message-ID: <45DA1078.50907@imperial.ac.uk>

Charles C. Berry wrote:

> Whoa! You are accessing one ROW at a time.
> 
> Either way this will tangle up your cache if you have many rows and 
> columns in your orignal data.
> 
> You might do better to do
> 
> Y <- t( X ) ### use '<-' !
> 
> for (i in whatever ){
>     do something using Y[ , i ]
> }

My question is NOT how to write the fastest code, it is whether dummy variables 
(for lack of better words) make the memory management better, i.e. faster, or not.

Best,

Fede

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From rab45+ at pitt.edu  Mon Feb 19 22:02:09 2007
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Mon, 19 Feb 2007 16:02:09 -0500
Subject: [R] Installing Package rgl - Compilation Fails
In-Reply-To: <45DA00EF.2050305@ebi.ac.uk>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
Message-ID: <1171918929.4041.22.camel@localhost.localdomain>

On Mon, 2007-02-19 at 19:56 +0000, Oleg Sklyar wrote:
> Check again your error message:
> 
> opengl.hpp:24:20: error: GL/glu.h: No such file or directory
> 
> you need to install
> 
> mesa-libGLU-devel FC6 version is 6.5.1-7
> 
> which will provide development files for glut3. Needless to say the 
> above will probably pool some dependencies and (-devel) means it will 
> install *.h files as well. Start your FC package manager and search for 
> "GLU", install the above and try again.
> 
> Best,
>   Oleg
> 

I installed a slightly newer version (the one that yum found):

mesa-libGLU-devel-6.5.1-9.fc6

but it still fails (I'm installing as root):

> install.packages("rgl")
--- Please select a CRAN mirror for use in this session ---
Loading Tcl/Tk interface ... done
trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/rgl_0.70.tar.gz'
Content type 'application/x-gzip' length 705556 bytes
opened URL

************************
Deleted a bunch of lines
************************


Disposable.hpp:13: warning: ?struct IDisposeListener? has virtual
functions but non-virtual destructor
gui.hpp:56: warning: ?class gui::WindowImpl? has virtual functions but
non-virtual destructor
gui.hpp:90: warning: ?class gui::GUIFactory? has virtual functions but
non-virtual destructor
g++ -shared -L/usr/local/lib -o rgl.so api.o Background.o BBoxDeco.o
Color.o device.o devicemanager.o Disposable.o FaceSet.o fps.o geom.o
gl2ps.o glgui.o gui.o Light.o LineSet.o LineStripSet.o Material.o math.o
osxgui.o osxlib.o par3d.o pixmap.o PointSet.o PrimitiveSet.o QuadSet.o
RenderContext.o render.o rglview.o scene.o select.o Shape.o SphereMesh.o
SphereSet.o SpriteSet.o String.o Surface.o TextSet.o Texture.o
TriangleSet.o Viewpoint.o win32gui.o win32lib.o x11gui.o x11lib.o -L
-lX11 -lXext -lGL -lGLU -L/usr/lib -lpng12  -L/usr/lib/R/lib -lR
/usr/bin/ld: cannot find -lXext
collect2: ld returned 1 exit status
make: *** [rgl.so] Error 1
chmod: cannot access `/usr/lib/R/library/rgl/libs/*': No such file or
directory
ERROR: compilation failed for package 'rgl'
** Removing '/usr/lib/R/library/rgl'

The downloaded packages are in
        /tmp/RtmpMc94yC/downloaded_packages
Warning message:
installation of package 'rgl' had non-zero exit status in:
install.packages("rgl") 


Rick B.


From maitra at iastate.edu  Mon Feb 19 22:11:47 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Mon, 19 Feb 2007 15:11:47 -0600
Subject: [R] Installing Package rgl - Compilation Fails
In-Reply-To: <1171918929.4041.22.camel@localhost.localdomain>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
	<1171918929.4041.22.camel@localhost.localdomain>
Message-ID: <20070219151147.0e964aca@subarnarekha.stat.iastate.edu>

The error is different now. It now cannot find Xext library. Do a yum search on this and install that.

yum provides libXext

which will give you the package Xext which needs to be installed.

You may also need to install the libXext-devel.i386 package.

HTH.
Ranjan



On Mon, 19 Feb 2007 16:02:09 -0500 Rick Bilonick <rab45+ at pitt.edu> wrote:

> On Mon, 2007-02-19 at 19:56 +0000, Oleg Sklyar wrote:
> > Check again your error message:
> > 
> > opengl.hpp:24:20: error: GL/glu.h: No such file or directory
> > 
> > you need to install
> > 
> > mesa-libGLU-devel FC6 version is 6.5.1-7
> > 
> > which will provide development files for glut3. Needless to say the 
> > above will probably pool some dependencies and (-devel) means it will 
> > install *.h files as well. Start your FC package manager and search for 
> > "GLU", install the above and try again.
> > 
> > Best,
> >   Oleg
> > 
> 
> I installed a slightly newer version (the one that yum found):
> 
> mesa-libGLU-devel-6.5.1-9.fc6
> 
> but it still fails (I'm installing as root):
> 
> > install.packages("rgl")
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/rgl_0.70.tar.gz'
> Content type 'application/x-gzip' length 705556 bytes
> opened URL
> 
> ************************
> Deleted a bunch of lines
> ************************
> 
> 
> Disposable.hpp:13: warning: ___struct IDisposeListener___ has virtual
> functions but non-virtual destructor
> gui.hpp:56: warning: ___class gui::WindowImpl___ has virtual functions but
> non-virtual destructor
> gui.hpp:90: warning: ___class gui::GUIFactory___ has virtual functions but
> non-virtual destructor
> g++ -shared -L/usr/local/lib -o rgl.so api.o Background.o BBoxDeco.o
> Color.o device.o devicemanager.o Disposable.o FaceSet.o fps.o geom.o
> gl2ps.o glgui.o gui.o Light.o LineSet.o LineStripSet.o Material.o math.o
> osxgui.o osxlib.o par3d.o pixmap.o PointSet.o PrimitiveSet.o QuadSet.o
> RenderContext.o render.o rglview.o scene.o select.o Shape.o SphereMesh.o
> SphereSet.o SpriteSet.o String.o Surface.o TextSet.o Texture.o
> TriangleSet.o Viewpoint.o win32gui.o win32lib.o x11gui.o x11lib.o -L
> -lX11 -lXext -lGL -lGLU -L/usr/lib -lpng12  -L/usr/lib/R/lib -lR
> /usr/bin/ld: cannot find -lXext
> collect2: ld returned 1 exit status
> make: *** [rgl.so] Error 1
> chmod: cannot access `/usr/lib/R/library/rgl/libs/*': No such file or
> directory
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/lib/R/library/rgl'
> 
> The downloaded packages are in
>         /tmp/RtmpMc94yC/downloaded_packages
> Warning message:
> installation of package 'rgl' had non-zero exit status in:
> install.packages("rgl") 
> 
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From thomas at mangold.com  Mon Feb 19 22:53:55 2007
From: thomas at mangold.com (Thomas Mangold)
Date: Mon, 19 Feb 2007 22:53:55 +0100
Subject: [R] Installing Rmpi on FC5 with lam
Message-ID: <45DA1C73.7050007@mangold.com>

After several trial and error attempts, I managed to install the Rmpi 
package without error on my Linux machine, running Fedora 5.
I installed lam-7.1.2, lam-devel-7.1.2 and R-2.4.1, R-devel-2.4.1
The Package Rmpi_5.0-3.tar.gz, I installed once from within R and once 
from the command line to make sure, I link the right libraries:
R CMD INSTALL Rmpi_5.0-3.tar.gz --configure-args="--with-mpi=/usr/lib/lam"

Rmpi is correctly installed in $R_HOME/library/Rmpi.
However on compiling, I get strange warnings:

internal.c: In function ?mystrcpy?:
internal.c:64: warning: operation on ?i? may be undefined
internal.c: In function ?mpitype?:
internal.c:43: warning: ?datatype? may be used uninitialized in this 
function
Rmpi.c: In function ?mpi_testany?:
Rmpi.c:1315: warning: unused variable ?index?
Rmpi.c: In function ?mpi_get_count?:
Rmpi.c:768: warning: ?datatype? may be used uninitialized in this function
Rmpi.c: In function ?mpi_allreduce?:
Rmpi.c:672: warning: ?op? may be used uninitialized in this function
Rmpi.c:673: warning: ?sexp_recv? may be used uninitialized in this function
Rmpi.c: In function ?mpi_reduce?:
Rmpi.c:586: warning: ?op? may be used uninitialized in this function
Rmpi.c:587: warning: ?sexp_recv? may be used uninitialized in this function
Rmpi.c: In function ?mpi_scatterv?:
Rmpi.c:320: warning: ?displs? may be used uninitialized in this function
Rmpi.c: In function ?mpi_gatherv?:
Rmpi.c:225: warning: ?displs? may be used uninitialized in this function

On loading the library occurs the following error:
 > library(Rmpi)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
unable to load shared library '/usr/lib/R/library/Rmpi/libs/Rmpi.so':
/usr/lib/R/library/Rmpi/libs/Rmpi.so: undefined symbol: lam_mpi_double
Error in library(Rmpi) : .First.lib failed for 'Rmpi'
Error in dyn.unload(x) : dynamic/shared library 
'/usr/lib/R/library/Rmpi/libs/Rmpi.so' was not loaded

I'm running kernel 2.6.18-1.2257.fc5smp on a i686 machine

Thank you for any ideas.


From johnson4 at babel.ling.upenn.edu  Tue Feb 20 00:48:56 2007
From: johnson4 at babel.ling.upenn.edu (Daniel Ezra Johnson)
Date: Mon, 19 Feb 2007 15:48:56 -0800
Subject: [R] random effect nested within fixed effects (binomial lmer)
Message-ID: <02B8B9D9-B62E-42CE-8970-755AA2E72C46@ling.upenn.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070219/d5d3cbb3/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Feb 20 01:04:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Feb 2007 00:04:40 +0000 (GMT)
Subject: [R] Installing Rmpi on FC5 with lam
In-Reply-To: <45DA1C73.7050007@mangold.com>
References: <45DA1C73.7050007@mangold.com>
Message-ID: <Pine.LNX.4.64.0702192343070.15857@auk.stats>

Is /usr/lib/lam in your ld search path?  This sort of message usually 
means a shared library cannot be found.  You do need lam-libs as well 
according to yum.

R CMD ldd /usr/lib/R/library/Rmpi/libs/Rmpi.so
/sbin/ldconfig -p | grep /usr/lib/lam

might be illuminating.

There are various problems with the configure scripts in Rmpi for Fedora 
layouts that the maintainer is looking into: it fails completely on x86_64 
systems.  I don't understand how your line worked: that looks for -lmpi in
/usr/lib/lam/lib/libmpi.a and that is not where FC5 puts the libraries.


On Mon, 19 Feb 2007, Thomas Mangold wrote:

> After several trial and error attempts, I managed to install the Rmpi
> package without error on my Linux machine, running Fedora 5.
> I installed lam-7.1.2, lam-devel-7.1.2 and R-2.4.1, R-devel-2.4.1
> The Package Rmpi_5.0-3.tar.gz, I installed once from within R and once
> from the command line to make sure, I link the right libraries:
> R CMD INSTALL Rmpi_5.0-3.tar.gz --configure-args="--with-mpi=/usr/lib/lam"
>
> Rmpi is correctly installed in $R_HOME/library/Rmpi.
> However on compiling, I get strange warnings:
>
> internal.c: In function ?mystrcpy?:
> internal.c:64: warning: operation on ?i? may be undefined
> internal.c: In function ?mpitype?:
> internal.c:43: warning: ?datatype? may be used uninitialized in this
> function
> Rmpi.c: In function ?mpi_testany?:
> Rmpi.c:1315: warning: unused variable ?index?
> Rmpi.c: In function ?mpi_get_count?:
> Rmpi.c:768: warning: ?datatype? may be used uninitialized in this function
> Rmpi.c: In function ?mpi_allreduce?:
> Rmpi.c:672: warning: ?op? may be used uninitialized in this function
> Rmpi.c:673: warning: ?sexp_recv? may be used uninitialized in this function
> Rmpi.c: In function ?mpi_reduce?:
> Rmpi.c:586: warning: ?op? may be used uninitialized in this function
> Rmpi.c:587: warning: ?sexp_recv? may be used uninitialized in this function
> Rmpi.c: In function ?mpi_scatterv?:
> Rmpi.c:320: warning: ?displs? may be used uninitialized in this function
> Rmpi.c: In function ?mpi_gatherv?:
> Rmpi.c:225: warning: ?displs? may be used uninitialized in this function
>
> On loading the library occurs the following error:
> > library(Rmpi)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
> unable to load shared library '/usr/lib/R/library/Rmpi/libs/Rmpi.so':
> /usr/lib/R/library/Rmpi/libs/Rmpi.so: undefined symbol: lam_mpi_double
> Error in library(Rmpi) : .First.lib failed for 'Rmpi'
> Error in dyn.unload(x) : dynamic/shared library
> '/usr/lib/R/library/Rmpi/libs/Rmpi.so' was not loaded
>
> I'm running kernel 2.6.18-1.2257.fc5smp on a i686 machine
>
> Thank you for any ideas.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From David.Duffy at qimr.edu.au  Tue Feb 20 02:21:27 2007
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Tue, 20 Feb 2007 11:21:27 +1000 (EST)
Subject: [R] RSNPper SNPinfo and making it handle a vector
In-Reply-To: <mailman.5.1171882802.31669.r-help@stat.math.ethz.ch>
References: <mailman.5.1171882802.31669.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0702201118180.16970@orpheus.qimr.edu.au>

"Farrel Buchinsky" <fjbuch at gmail.com> wrote:

> I tried biomaRt
> 
> library(biomaRt)
> ensnp = useMart("snp", dataset = "hsapiens_snp")
> snp = getSNP(chromosome = 17, start = 73649033, end = 73679033, mart = 
> ensnp)
> show(snp)
> 
> Gave me a nice table but it did not seem to permit starting from the point 
> of knowing the SNP and entering a list of rs######. I guess I could always 
> fudge around. But it does not provide the one-stop I was looking for.

Try, for example,

getBM(attributes=c("refsnp_id","chr_name","chrom_start",
                    "ensembl_external_gene_id","allele",
                    "validated"),
       filter="refsnp",
       values=(refsnp=c("rs17166282","rs3897937")), mart=ensnp)


-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v


From parsons at uoguelph.ca  Tue Feb 20 04:37:01 2007
From: parsons at uoguelph.ca (parsons at uoguelph.ca)
Date: Mon, 19 Feb 2007 22:37:01 -0500
Subject: [R] bootstrapping Levene's test
Message-ID: <20070219223701.md4t6hwv8kkck44k@webmail.uoguelph.ca>

Hello all,

I am low down on the learning curve of R but so far I have had little  
trouble using most of the packages. However, recently I have run into  
a wall when it comes to bootstrapping a Levene's test (from the car  
package) and thought you might be able to help. I have not been able  
to find R examples for the "boot" package where the test statistic  
specifically uses a grouping variable (or at least a simple example  
with this condition). I would like to do a  non-parametric bootstrap  
to eventually get 95% confidence intervals using the boot.ci command.  
I have included the coding I have tried on a simple data set below. If  
anyone could provide some help, specifically with regards to how the  
"statistic" arguement should be set up in the boot package, it would  
be greatly appreciated.

> library(boot)
> library(car)
> data<-c(2,45,555,1,77,1,2,1,2,1)
> group<-c(1,1,1,1,1,2,2,2,2,2)
> levene.test(data,group)
Levene's Test for Homogeneity of Variance
       Df F value Pr(>F)
group  1  1.6929 0.2294
        8
> stat<-function(a){levene.test(a,group)}
> trial1<-boot(data,statistic,100)
Error in statistic(data, original, ...) : unused argument(s) ( ...)

Best regards,
Kevin


From mail at xesoftware.com.au  Tue Feb 20 04:45:25 2007
From: mail at xesoftware.com.au (stephenc)
Date: Tue, 20 Feb 2007 14:45:25 +1100
Subject: [R] tree()
Message-ID: <00da01c754a1$8f6ece60$6701a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070220/f03813d5/attachment.pl 

From RMan54 at cox.net  Tue Feb 20 05:28:01 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Mon, 19 Feb 2007 20:28:01 -0800
Subject: [R] How to avoid sort of x values in dotplot?
Message-ID: <009d01c754a7$816ca0c0$0900a8c0@rman>

I am trying to avoid that dotplot sorts my x-values. They are in the correct
order in the data.frame and the connections between the x-y values follows
this order, but the placement of the x-values on the x-axis is re-ordered.
In the following example, the order should be "d1", "d8" and "d15". However,
this script places "d8" at the highest x position. Any help is appreciated.
 
Subj <- rep(1:4,each=3)
Time <- rep(c("d1","d8","d15"),4)
Conc <- 1:12
df <- data.frame(Subj,Time,Conc)
dotplot(Conc ~ Time | Subj,
       data = df,
       layout = c(2,2),
       type="b")
 
Thanks,
-Rene
Irvine, CA, USA


From bernd.weiss at uni-koeln.de  Tue Feb 20 06:25:45 2007
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Tue, 20 Feb 2007 06:25:45 +0100
Subject: [R] How to avoid sort of x values in dotplot?
In-Reply-To: <009d01c754a7$816ca0c0$0900a8c0@rman>
References: <009d01c754a7$816ca0c0$0900a8c0@rman>
Message-ID: <45DA9469.4826.85A34D@bernd.weiss.uni-koeln.de>

Am 19 Feb 2007 um 20:28 hat Rene Braeckman geschrieben:

From:           	"Rene Braeckman" <RMan54 at cox.net>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Mon, 19 Feb 2007 20:28:01 -0800
Subject:        	[R] How to avoid sort of x values in dotplot?

> I am trying to avoid that dotplot sorts my x-values. They are in the
> correct order in the data.frame and the connections between the x-y
> values follows this order, but the placement of the x-values on the
> x-axis is re-ordered. In the following example, the order should be
> "d1", "d8" and "d15". However, this script places "d8" at the highest
> x position. Any help is appreciated.
> 
> Subj <- rep(1:4,each=3)
> Time <- rep(c("d1","d8","d15"),4)
> Conc <- 1:12
> df <- data.frame(Subj,Time,Conc)
> dotplot(Conc ~ Time | Subj,
>        data = df,
>        layout = c(2,2),
>        type="b")
> 

library(lattice)
Subj <- rep(1:4,each=3)
Time <- rep(c("d1","d8","d15"),4)
Conc <- 1:12
df <- data.frame(Subj,Time,Conc)
df$Time <- factor(df$Time,levels=df$Time)
dotplot(Conc ~ Time | Subj,
       data = df,
       layout = c(2,2),
       type="b")

HTH,

Bernd


From petr.pikal at precheza.cz  Tue Feb 20 07:00:27 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 20 Feb 2007 07:00:27 +0100
Subject: [R] categorical column to numeric column
In-Reply-To: <45D9B028.5040007@stats.uwo.ca>
References: <A36876D3F8A5734FA84A4338135E7CC30104A80E@BAN-MAILSRV03.Amba.com>
Message-ID: <45DA9C8B.2452.1DF158@localhost>

Hi

On 19 Feb 2007 at 9:11, Duncan Murdoch wrote:

Date sent:      	Mon, 19 Feb 2007 09:11:52 -0500
From:           	Duncan Murdoch <murdoch at stats.uwo.ca>
To:             	Shubha Vishwanath Karanth <shubhak at ambaresearch.com>
Copies to:      	r-help <R-help at stat.math.ethz.ch>
Subject:        	Re: [R] categorical column to numeric column

> On 2/19/2007 8:36 AM, Shubha Vishwanath Karanth wrote:
> > Hi R,
> > 
> >  
> > 
> > Let 'dd' be a data frame given as:
> > 
> >  
> > 
> > dd=data.frame(aa=c("a","a","b","a","b","b"),bb=c(1,1,1,2,3,4))
> > 
> >  
> > 
> > Now I want to create a column 'g' such that if dd$aa=a then dd$g=1
> > else dd$g= -1 .
> > 
> >  
> > 
> > So, I gave the below syntax:
> > 
> >  
> > 
> > if((dd$aa)=="a") dd$g=1 else dd$g= -1
> 
> if() looks at just the first entry; it's designed for flow of control
> rather than vectorized calculations.  You want ifelse():
> 
> ifelse( dd$aa == "a", 1, -1)

Another approach is to use the fact that logical vector can be 
interpreted as 1 and 0 vector

(dd$a=="a")*2-1

HTH
Petr


> 
> Duncan Murdoch
> >  
> > 
> > But I get the error message as:
> > 
> > Warning message: 
> > 
> > the condition has length > 1 and only the first element will be used
> > in: if ((dd$aa) == "a") dd$g = 1 else dd$g = -1 
> > 
> >  
> > 
> > and dd=
> > 
> >  
> > 
> >> dd
> > 
> >   aa bb g
> > 
> > 1  a  1 1
> > 
> > 2  a  1 1
> > 
> > 3  b  1 1
> > 
> > 4  a  2 1
> > 
> > 5  b  3 1
> > 
> > 6  b  4 1
> > 
> >> 
> > 
> >  
> > 
> > Please let me know what is the error I am doing?
> > 
> >  
> > 
> >  
> > 
> >  
> > 
> >  
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From phdhwang at gmail.com  Tue Feb 20 08:34:25 2007
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Tue, 20 Feb 2007 16:34:25 +0900
Subject: [R] Any packages for conducting AHP( Analytic Hierarchy
	Process) data
In-Reply-To: <002b01c753a5$9de33190$fe01a8c0@cable.rcn.com>
References: <002b01c753a5$9de33190$fe01a8c0@cable.rcn.com>
Message-ID: <b040cbb00702192334n771a4bf2pc39974839f0eed28@mail.gmail.com>

I have so far had the only return to my query from Steve Dutky.

I am familir with matrix eigen value and vector manipulation.
If you have anything to share with me, it would be good chance for me
to follow up the AHP-related thing. Furthermore, I am good at
statistics fairely.

Thanks for your comments,

Kum-Hoe Hwang, Ph.D.

On 2/19/07, Steve Dutky <sdutky at terpalum.umd.edu> wrote:
> Hi, Kum-Hoe Hwang,
>
> Did you get any help for your query?
>
> I have used R/Splus for a number of years, primarily for isolating anomalies
> with TCP/IP network traffic.
>
> I have become somewhat adept at importing data from a variety of sources and
> formats into R.
>
> I first became interested in AHP some time back, but have never gotten
> around to working with it under R.
>
> I probably cannot help you much with the analysis of AHP data.   If your
> main problem involves casting your AHP data into a data.frame, I will be
> happy to assist you as far my time and understanding allow.
>
> Have you reviewed the packages for handling eigen values/vectors?
>
> Thanks, Steve Dutky
>
> 400 Domer Ave
> Takoma Park, MD 20912 US
>
> Message: 101
> Date: Wed, 14 Feb 2007 16:56:11 +0900
> From: "Kum-Hoe Hwang" <phdhwang at gmail.com>
> Subject: [R] Any packages for conducting AHP( Analytic Hierarchy
> Process) data
> To: "R Help mail address" <r-help at stat.math.ethz.ch>
> Message-ID:
> <b040cbb00702132356m55070e13tb5ba29c3b142589f at mail.gmail.com>
> Content-Type: text/plain
>
> Hi, R Lovers!
>
> I have some survey data. I'd like to run R or R packages for processing data
> inputted
> from AHP(Analytic Hierarchy Process) survey.
>
> Are there any R packages or subsititues for running data from AHP survey.
>
> Thanks in advance,
>


-- 
Kum-Hoe Hwang, Ph.D.Phone : 82-31-250-3516Email : phdhwang at gmail.com


From lauri.nikkinen at iki.fi  Tue Feb 20 09:02:44 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Tue, 20 Feb 2007 10:02:44 +0200
Subject: [R] Reshape (pivot) question
Message-ID: <ba8c09910702200002k785dff1ajbee4e2b1916c0aba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070220/53096d99/attachment.pl 

From patrick.giraudoux at univ-fcomte.fr  Tue Feb 20 09:34:45 2007
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Tue, 20 Feb 2007 09:34:45 +0100
Subject: [R] interaction term and scatterplot3d
Message-ID: <45DAB2A5.4030206@univ-fcomte.fr>

Dear Listers,

I would be interested in representing a trend surface including an 
interaction term z = f(x,y,x.y) - eg the type of chart obtained with 
persp() or wireframe(), then adding datapoints as a cloud, ideally with 
dots which are under the surface in a color, and those who are above in 
another color. An other option would be to draw segments between the 
dots and the ground of the chart.

scatterplot3d looks like being close to do such things except it does 
not to include (to my knowledge) a coefficient for the interaction term 
(thus just model z = f(x,y).

Does anybody has an idea where to start with this and the main steps? Or 
a place/website where some script examples can be found?

Patrick


From johannes_graumann at web.de  Tue Feb 20 10:34:52 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Tue, 20 Feb 2007 10:34:52 +0100
Subject: [R] Another subsetting enigma
References: <erceqn$i08$1@sea.gmane.org>
	<644e1f320702190849r4ddb8c12xfb11b68b620b49b1@mail.gmail.com>
	<200702191825.59037.johannes_graumann@web.de>
	<644e1f320702191003l48aad05dw1603e55b520530b9@mail.gmail.com>
Message-ID: <erefbs$f4j$1@sea.gmane.org>

jim holtman wrote:

>> matches <- sapply(result, function(x){
> + ? ? .comma <- strsplit(x, ',')[[1]] ?# get the fields
> + ? ? any(my.list %in% .comma)
> + })

Thanks for that!

Joh


From tjohnson at src.riken.jp  Tue Feb 20 10:48:17 2007
From: tjohnson at src.riken.jp (Todd A. Johnson)
Date: Tue, 20 Feb 2007 18:48:17 +0900
Subject: [R] Difficulties with dataframe filter using elements from an array
 created using a for loop or seq()
Message-ID: <C200F2F1.28615%tjohnson@src.riken.jp>

Hi All-
 
This seems like such a pathetic problem to be posting about, but I have no
idea why this testcase does not work.  I have tried this using R 2.4.1,
2.4.0, 2.3.0, and 2.0.0 on several different computers (Mac OS 10.4.8,
Windows XP, Linux).  Below the signature, you will find my test case R code.
 
My point in this folly is to take a dataframe of 300,000 rows, create a
filter based on two of the rows, and count the number of rows in the
filtered and unfiltered dataframe.  One column in the filter only has the
numbers 0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, so I
thought that I could just iterate in a for loop and get the job done. Just
the simple single column filter case is presented here. Obviously, there are
only ten numbers, so the "manual" method is easy, but I would like to have a
more flexible program. (Plus it worries me if the simple things don't do
what I expect... :-) )

>From the output, you can see that the loop using the "handmadevector" that
creates a filter and counts the elements, correctly finds one match for each
element in the vector, but the seq() and for loop produced vectors each give
a mixture of true and false matches.

Can anyone tell me why the "loopvector" and "seqvector" do not provide the
same output as the "handmadevector".
 

Thank you for your assistance!

Todd

-- 
Todd A. Johnson
Research Associate, Laboratory for Medical Informatics
SNP Research Center,RIKEN
1-7-22Suehiro,Tsurumi-ku,Yokohama
Kanagawa 230-0045,Japan

Cellphone: 090-5309-5867

E-mail: tjohnson at src.riken.jp



Here's the testcase, with the sample code between the lines and the output
following:
 
_____________________________________________________________________
## Set up three different vectors, each with the numbers 0.05, 0.15, 0.25,
0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95
## each of which is used to select records from a dataframe based on
equality to a particular column
## The first vector is created by using a for loop
loopvector <- c()
for (i in 0:9){
loopvector <- c(loopvector, (i*0.10)+0.05);
}
## The second vector is made "by hand"
handmadevector <- c(0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85,
0.95)
## The third vector is made using seq()
seqvector <- seq(0.05, 0.95, 0.10)
## Are the vectors the same?
all.equal(loopvector, handmadevector)
all.equal(loopvector, seqvector)
print(handmadevector)
print(loopvector)
print(seqvector)
## As a simple testcase, I create a dataframe with two variables, a varA of
dummy data, and bBins
## which is the column on which I was trying to filter.
a <- c(0,1,2,0,1,3,4,5,3,5)
b <- c(0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95)
testdf <- data.frame(varA = a, bBins = b)
attach(testdf)
## Loop through each of the vectors, create a filter on the dataframe based
on equality with the current iteration,
## and print that number and the count of records in the dataframe that
match that number.
for (i in loopvector){
aqs_filt <- bBins==i;
print(i);
print(length(testdf$varA[aqs_filt]));
}
for (i in handmadevector){
aqs_filt <- bBins==i;
print(i);
print(length(testdf$varA[aqs_filt]));
}
for (i in seqvector){
aqs_filt <- bBins==i;
print(i);
print(length(testdf$varA[aqs_filt]));
}
 
_____________________________________________________________________
 
Here's the output from R 2.4.1 running on an Apple 12" Powerbook.
 
> ## Set up three different vectors, each with the numbers 0.05, 0.15, 0.25,
0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95
> ## each of which is used to select records from a dataframe based on equality
to a particular column
> ## The first vector is created by using a for loop
> loopvector <- c()
> for (i in 0:9){
+ loopvector <- c(loopvector, (i*0.10)+0.05);
+ }
> ## The second vector is made "by hand"
> handmadevector <- c(0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85,
0.95)
> ## The thirs vector is made using seq()
> seqvector <- seq(0.05, 0.95, 0.10)
> ## Are the vectors the same?
> all.equal(loopvector, handmadevector)
[1] TRUE
> all.equal(loopvector, seqvector)
[1] TRUE
> 
> print(handmadevector)
 [1] 0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
> print(loopvector)
 [1] 0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
> print(seqvector)
 [1] 0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
> ## As a simple testcase, I create a dataframe with two variables, a varA of
dummy data, and bBins
> ## which is the column on which I was trying to filter.
> a <- c(0,1,2,0,1,3,4,5,3,5)
> b <- c(0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95)
> testdf <- data.frame(varA = a, bBins = b)
> attach(testdf)
> ## Loop through each of the vectors, create a filter on the dataframe based on
equality with the current iteration,
> ## and print that number and the count of records in the dataframe that match
that number.
> for (i in loopvector){
+ aqs_filt <- bBins==i;
+ print(i);
+ print(length(testdf$varA[aqs_filt]));
+ }
[1] 0.05
[1] 1
[1] 0.15
[1] 0
[1] 0.25
[1] 1
[1] 0.35
[1] 0
[1] 0.45
[1] 1
[1] 0.55
[1] 1
[1] 0.65
[1] 0
[1] 0.75
[1] 0
[1] 0.85
[1] 0
[1] 0.95
[1] 0
> for (i in handmadevector){
+ aqs_filt <- bBins==i;
+ print(i);
+ print(length(testdf$varA[aqs_filt]));
+ }
[1] 0.05
[1] 1
[1] 0.15
[1] 1
[1] 0.25
[1] 1
[1] 0.35
[1] 1
[1] 0.45
[1] 1
[1] 0.55
[1] 1
[1] 0.65
[1] 1
[1] 0.75
[1] 1
[1] 0.85
[1] 1
[1] 0.95
[1] 1
> for (i in seqvector){
+ aqs_filt <- bBins==i;
+ print(i);
+ print(length(testdf$varA[aqs_filt]));
+ }
[1] 0.05
[1] 1
[1] 0.15
[1] 0
[1] 0.25
[1] 1
[1] 0.35
[1] 0
[1] 0.45
[1] 1
[1] 0.55
[1] 1
[1] 0.65
[1] 0
[1] 0.75
[1] 0
[1] 0.85
[1] 0
[1] 0.95
[1] 0
>


From ccleland at optonline.net  Tue Feb 20 11:06:24 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 20 Feb 2007 05:06:24 -0500
Subject: [R] bootstrapping Levene's test
In-Reply-To: <20070219223701.md4t6hwv8kkck44k@webmail.uoguelph.ca>
References: <20070219223701.md4t6hwv8kkck44k@webmail.uoguelph.ca>
Message-ID: <45DAC820.4080904@optonline.net>

parsons at uoguelph.ca wrote:
> Hello all,
> 
> I am low down on the learning curve of R but so far I have had little  
> trouble using most of the packages. However, recently I have run into  
> a wall when it comes to bootstrapping a Levene's test (from the car  
> package) and thought you might be able to help. I have not been able  
> to find R examples for the "boot" package where the test statistic  
> specifically uses a grouping variable (or at least a simple example  
> with this condition). I would like to do a  non-parametric bootstrap  
> to eventually get 95% confidence intervals using the boot.ci command.  
> I have included the coding I have tried on a simple data set below. If  
> anyone could provide some help, specifically with regards to how the  
> "statistic" arguement should be set up in the boot package, it would  
> be greatly appreciated.
> 
>> library(boot)
>> library(car)
>> data<-c(2,45,555,1,77,1,2,1,2,1)
>> group<-c(1,1,1,1,1,2,2,2,2,2)
>> levene.test(data,group)
> Levene's Test for Homogeneity of Variance
>        Df F value Pr(>F)
> group  1  1.6929 0.2294
>         8
>> stat<-function(a){levene.test(a,group)}
>> trial1<-boot(data,statistic,100)
> Error in statistic(data, original, ...) : unused argument(s) ( ...)

  One problem is that levene.test() returns an ANOVA table, which is not
a statistic.  Looking inside levene.test() to see what might be used as
a statistic, for the two-group case it seems like you could do something
like this:

library(boot)
library(car)
set.seed(671969)

df <- data.frame(Y = c(rnorm(100,0,1), rnorm(100,0,1)),
                 G = rep(c(1,2), each = 100))

boot.levene <- function(data,indices){
      levene.diff <- diff(tapply(df$Y[indices],
                            list(df$G[indices]), mad))
      return(levene.diff)
      }

first.try <- boot(df, boot.levene, 1000)

first.try

ORDINARY NONPARAMETRIC BOOTSTRAP

Call:
boot(data = df, statistic = boot.levene, R = 1000)


Bootstrap Statistics :
      original     bias    std. error
t1* -0.1944924 0.02439172   0.1753512

boot.ci(first.try, index=1, type="bca")

BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 1000 bootstrap replicates

CALL :
boot.ci(boot.out = first.try, type = "bca", index = 1)

Intervals :
Level       BCa
95%   (-0.5772,  0.1159 )
Calculations and Intervals on Original Scale

> Best regards,
> Kevin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ripley at stats.ox.ac.uk  Tue Feb 20 11:07:16 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Feb 2007 10:07:16 +0000 (GMT)
Subject: [R] Difficulties with dataframe filter using elements from an
 array created using a for loop or seq()
In-Reply-To: <C200F2F1.28615%tjohnson@src.riken.jp>
References: <C200F2F1.28615%tjohnson@src.riken.jp>
Message-ID: <Pine.LNX.4.64.0702201006400.9031@auk.stats>

FAQ Q7.31

On Tue, 20 Feb 2007, Todd A. Johnson wrote:

> Hi All-
>
> This seems like such a pathetic problem to be posting about, but I have no
> idea why this testcase does not work.  I have tried this using R 2.4.1,
> 2.4.0, 2.3.0, and 2.0.0 on several different computers (Mac OS 10.4.8,
> Windows XP, Linux).  Below the signature, you will find my test case R code.
>
> My point in this folly is to take a dataframe of 300,000 rows, create a
> filter based on two of the rows, and count the number of rows in the
> filtered and unfiltered dataframe.  One column in the filter only has the
> numbers 0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, so I
> thought that I could just iterate in a for loop and get the job done. Just
> the simple single column filter case is presented here. Obviously, there are
> only ten numbers, so the "manual" method is easy, but I would like to have a
> more flexible program. (Plus it worries me if the simple things don't do
> what I expect... :-) )
>
>> From the output, you can see that the loop using the "handmadevector" that
> creates a filter and counts the elements, correctly finds one match for each
> element in the vector, but the seq() and for loop produced vectors each give
> a mixture of true and false matches.
>
> Can anyone tell me why the "loopvector" and "seqvector" do not provide the
> same output as the "handmadevector".
>
>
> Thank you for your assistance!
>
> Todd
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Feb 20 11:09:59 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Feb 2007 10:09:59 +0000 (GMT)
Subject: [R] tree()
In-Reply-To: <00da01c754a1$8f6ece60$6701a8c0@tablet>
References: <00da01c754a1$8f6ece60$6701a8c0@tablet>
Message-ID: <Pine.LNX.4.64.0702201007310.9031@auk.stats>

This is a function of your data and the tuning parameters you chose to 
use.  See ?tree.control.

On Tue, 20 Feb 2007, stephenc wrote:

> Hi
>
> I am trying to use tree() to classify movements in a futures contract.  My
> data is like this:
>
>         diff      dip                  dim             adx
> 1          0    100.00000    8650.0000    100.00000
> 2          0     93.18540    2044.5455     93.18540
> 3          0     90.30995    1549.1169     90.30995
> 4          1     85.22030     927.0419     85.22030
> 5          1     85.36084     785.6480     85.36084
> 6          0     85.72627     663.3814     85.72627
> 7          0     78.06721     500.1113     78.06721
> 8          1     69.59398     376.7558     69.59398
> 9          1     71.15429     307.4533     71.15429
> 10         1     71.81023     280.6238     71.81023
>
> plus another 6000 lines
>
> The cpus example works fine and I am trying this:
>
> tree.model <- tree(as.factor(indi$diff) ~ indi$dim + indi$dip + indi$adx,
> indi[1:4000,])

Oh, please!  use the data= argument properly.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mikewhite.diu at btconnect.com  Tue Feb 20 12:18:21 2007
From: mikewhite.diu at btconnect.com (Mike White)
Date: Tue, 20 Feb 2007 11:18:21 -0000
Subject: [R] Mahalanobis distance and probability of group membership using
	Hotelling's T2 distribution
Message-ID: <001201c754e0$d529ea90$6201a8c0@FSSFQCV7BGDVED>

I want to calculate the probability that a group will include a particular
point using the squared Mahalanobis distance to the centroid. I understand
that the squared Mahalanobis distance is distributed as chi-squared but that
for a small number of random samples from a multivariate normal population
the Hotellings T2 (T squared) distribution should be used.
I cannot find a function for Hotelling's T2 distribution in R (although from
a previous post I have been provided with functions for the Hotelling Test).
My understanding is that the Hotelling's T2 distribution is related to the F
distribution using the equation:
                             T2(u,v) = F(u, v-u+1)*vu/(v-u+1)
where u is the number of variables and v the number of group members.

I have written the R code below to compare the results from the chi-squared
distribution with the Hotelling's T2 distribution for probability of a
member being included within a group.
Please can anyone confirm whether or not this is the correct way to use
Hotelling's T2 distribution for probability of group membership. Also, when
testing a particular group member, is it preferable to leave that member out
when calculating the centre and covariance of the group for the Mahalanobis
distances?

Thanks
Mike White

############################################################################
####
## Hotelling T^2 distribution function
ph<-function(q, u, v, ...){
# q vector of quantiles as in function pf
# u number of independent variables
# v number of observations
if (!v > u+1) stop("n must be greater than p+1")
df1 <- u
df2 <- v-u+1
pf(q*df2/(v*u), df1, df2, ...)
}

# compare Chi-squared and Hotelling T^2 distributions for a group member
u<-3
v<-10
set.seed(1)
mat<-matrix(rnorm(v*u), nrow=v, ncol=u)
MD2<-mahalanobis(mat, center=colMeans(mat), cov=cov(mat))
d<-MD2[order(MD2)]
# select a point midway between nearest and furthest from centroid
dm<-d[length(d)/2]
1-ph(dm,u,v)    # probability using Hotelling T^2 distribution
# [1] 0.6577069
1-pchisq(dm, u) # probability using Chi-squared distribution
# [1] 0.5538466


From jrkrideau at yahoo.ca  Tue Feb 20 12:39:52 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 20 Feb 2007 06:39:52 -0500 (EST)
Subject: [R] Lengend function and moving average plot
In-Reply-To: <3ffd3bb60702191011j24ad0344of3ce341c2493dde5@mail.gmail.com>
Message-ID: <20070220113952.59255.qmail@web32806.mail.mud.yahoo.com>


--- amna khan <amnakhan493 at gmail.com> wrote:

> Sir I am very new user of R. I am not understanding
> the how to write the
> plot description in box outside the plot. 

Have a look at 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/68585.html

You should read up on par 
?par 

I hope this helps


From guillermojsanmartin at googlemail.com  Tue Feb 20 14:00:56 2007
From: guillermojsanmartin at googlemail.com (=?ISO-8859-1?Q?Guillermo_Juli=E1n_San_Mart=EDn?=)
Date: Tue, 20 Feb 2007 14:00:56 +0100
Subject: [R] Problems with obtaining t-tests of regression coefficients
	applying consistent standard errors after run 2SLS
	estimation. Clearer !!!!!
Message-ID: <48a5ea80702200500tc57d76r67d48d918af57244@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070220/bfd45efe/attachment.pl 

From jholtman at gmail.com  Tue Feb 20 14:12:47 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 20 Feb 2007 08:12:47 -0500
Subject: [R] Reshape (pivot) question
In-Reply-To: <ba8c09910702200002k785dff1ajbee4e2b1916c0aba@mail.gmail.com>
References: <ba8c09910702200002k785dff1ajbee4e2b1916c0aba@mail.gmail.com>
Message-ID: <644e1f320702200512k58a77971x1ee57dc3b58c0ec8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070220/3c835882/attachment.pl 

From T.C.Cameron at leeds.ac.uk  Tue Feb 20 14:13:25 2007
From: T.C.Cameron at leeds.ac.uk (T.C. Cameron)
Date: Tue, 20 Feb 2007 13:13:25 -0000
Subject: [R] Simplification of Generalised Linear mixed effects models using
	glmmPQL
Message-ID: <A3D44A75D69AB347B6AA220F0939096566FBFA@HERMES4.ds.leeds.ac.uk>

Dear R users I have built several glmm models using glmmPQL in the
following structure:
 
m1<-glmmPQL(dev~env*har*treat+dens, random = ~1|pop/rep,  family =
Gamma)
 
(full script below, data attached)
 
I have tried all the methods I can find to obtain some sort of model fit
score or to compare between models using following the deletion of terms
(i.e. AIC, logLik, anova.lme(m1,m2)), but I cannot get any of them to
work.
Yet I see on several R help pages that others have with similar models?
 
I have tried the functions in lme4 as well and lmer or lmer2 will not
accept my random terms of "rep" (replicate) nested within "pop"
population.
 
I have read the appropriate sections of the available books and R help
pages but I am at a loss of where to move from here
 
 

data<-read.table("D:\\bgytcc\\MITES\\Data\\Analysis\\test.txt",header=T)
attach(data)
names(data)
 
 
 
m1<-glmmPQL(dev~env*har*treat+dens, random = ~1|pop/rep, family = Gamma)
summary(m1)
anova.lme(m1)
m2<-update(m1,~.-env:har:treat)
anova.lme(m1,m2)###this does not work
AIC(m1)##this does not work
logLik(m1)##this does not work?
 
 
 
##################this does not work
class(m1) <- "lme"
class(m2) <- "lme"
anova.lme(m1,m2)
#################################
 
m3<-lmer(dev~env*har*treat+dens + (1|pop/rep), family = Gamma)
 
## this generates an error
Error in lmerFactorList(formula, mf, fltype) : 
        number of levels in grouping factor(s) 'rep:pop', 'pop' is too
large
In addition: Warning messages:
1: numerical expression has 1851 elements: only the first used in:
rep:pop 
2: numerical expression has 1851 elements: only the first used in:
rep:pop 
 

m4<-lmer(dev~env*har*treat + dens + (1|rep) +(1|pop), family = Gamma,
method = "Laplace")
## this works but it does not give me an anova output with p values
anova(m4)
Analysis of Variance Table
              Df  Sum Sq Mean Sq
env            1   17858   17858
har            2     879     439
treat          2    2613    1306
dens           1 1016476 1016476
env:har        2     870     435
env:treat      2    1188     594
har:treat      4     313      78
env:har:treat  4    1188     297

 

........................................................................
............
Dr Tom C Cameron
Genetics, Ecology and Evolution
IICB, University of Leeds
Leeds, UK
Office: +44 (0)113 343 2837
Lab:    +44 (0)113 343 2854
Fax:    +44 (0)113 343 2835


Email: t.c.cameron at leeds.ac.uk
Webpage: click here
<http://www.fbs.leeds.ac.uk/staff/profile.php?tag=Cameron_TC> 

 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: test.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070220/82646d15/attachment.txt 

From SAULEAUEA at ch-mulhouse.fr  Tue Feb 20 14:30:46 2007
From: SAULEAUEA at ch-mulhouse.fr (=?iso-8859-1?Q?SAULEAU_Erik-Andr=E9?=)
Date: Tue, 20 Feb 2007 14:30:46 +0100
Subject: [R] Sample size
Message-ID: <2DC3A0F67A1E894BB03AEF915D9BC3F1014146AB@srmessagerie.chm.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070220/cb89f312/attachment.pl 

From andy_liaw at merck.com  Tue Feb 20 14:30:43 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Feb 2007 08:30:43 -0500
Subject: [R] memory management uestion  [Broadcast]
In-Reply-To: <45DA1078.50907@imperial.ac.uk>
References: <45D9E7BB.6010902@imperial.ac.uk>
	<Pine.LNX.4.64.0702191247150.2818@tajo.ucsd.edu>
	<45DA1078.50907@imperial.ac.uk>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03BCD20B@usctmx1106.merck.com>

I don't see why making copies of the columns you need inside the loop is
"better" memory management.  If the data are in a matrix, accessing
elements is quite fast.  If you're worrying about speed of that, do what
Charles suggest: work with the transpose so that you are accessing
elements in the same column in each iteration of the loop.

Andy 

From: Federico Calboli
> 
> Charles C. Berry wrote:
> 
> > Whoa! You are accessing one ROW at a time.
> > 
> > Either way this will tangle up your cache if you have many rows and 
> > columns in your orignal data.
> > 
> > You might do better to do
> > 
> > Y <- t( X ) ### use '<-' !
> > 
> > for (i in whatever ){
> >     do something using Y[ , i ]
> > }
> 
> My question is NOT how to write the fastest code, it is 
> whether dummy variables (for lack of better words) make the 
> memory management better, i.e. faster, or not.
> 
> Best,
> 
> Fede
> 
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health Imperial 
> College, St Mary's Campus Norfolk Place, London W2 1PG
> 
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From h.wickham at gmail.com  Tue Feb 20 14:37:40 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 20 Feb 2007 06:37:40 -0700
Subject: [R] Reshape (pivot) question
In-Reply-To: <644e1f320702200512k58a77971x1ee57dc3b58c0ec8@mail.gmail.com>
References: <ba8c09910702200002k785dff1ajbee4e2b1916c0aba@mail.gmail.com>
	<644e1f320702200512k58a77971x1ee57dc3b58c0ec8@mail.gmail.com>
Message-ID: <f8e6ff050702200537x2c13a6dfn89767378ed4fa913@mail.gmail.com>

> Haven't quite learned to 'cast' yet, but I have always used the 'apply'
> functions for this type of processing:
>
> > x <- "id patient_id date code class eala
> +     ID1564262 1562 6.4.200612:00 5555 1 NA
> +     ID1564262 1562 6.4.200612:00 5555 1 NA
> +     ID1564264 1365 14.2.200614:35 5555 1 50
> +     ID1564265 1342 7.4.200614:30 2222 2 50
> +     ID1564266 1648 7.4.200614:30 2222 2 50
> +     ID1564267 1263 10.2.200615:45 2222 2 10
> +     ID1564267 1263 10.2.200615:45 3333 3 10
> +     ID1564269 5646 13.5.200617:02 3333 3 10
> +     ID1564270 7561 13.5.200617:02 6666 1 10
> +     ID1564271 1676 15.5.200620:41 2222 2 20"
> >
> > x.in <- read.table(textConnection(x), header=TRUE)
> > # 'by' seems to drop NAs so convert to a character string for processing
> > x.in$eala <- ifelse(is.na(x.in$eala), "NA", as.character(x.in$eala))
> > # convert date to POSIXlt so we can access the year and month
> > myDate <- strptime(x.in$date, "%d.%m.%Y%H:%M")
> > x.in$year <- myDate$year + 1900
> > x.in$month <- myDate$mon+1

To do this with the reshape package, all you need is:

> x.in$patient_id <- factor(x.in$patient_id)
> dfm <- melt(x.in, id=c("code", "month", "year"), m=c("id","patient_id"))
> cast(dfm, code + month + year ~ variable, stats)
  code month year id_n id_uniikit patient_id_n patient_id_uniikit
1 2222     2 2006    1          1            1                  1
2 2222     4 2006    2          2            2                  2
3 2222     5 2006    1          1            1                  1
4 3333     2 2006    1          1            1                  1
5 3333     5 2006    1          1            1                  1
6 5555     2 2006    1          1            1                  1
7 5555     4 2006    2          1            2                  1
8 6666     5 2006    1          1            1                  1

Which looks like what you want from your R code, but not your SQL.

Hadley


From info at aghmed.fsnet.co.uk  Tue Feb 20 14:43:10 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Tue, 20 Feb 2007 13:43:10 +0000
Subject: [R] summary polr
In-Reply-To: <45D9C05C020000290000A424@stirling.iso.port.ac.uk>
References: <45D9C05C020000290000A424@stirling.iso.port.ac.uk>
Message-ID: <7.0.0.16.0.20070220134130.0195f1f8@aghmed.fsnet.co.uk>

At 15:21 19/02/2007, Paolo Accadia wrote:
>Hi all,
>
>I have a problem to estimate Std. Error and 
>t-value by ???polr??? in library Mass.
>They result from the summary of a polr object.
>
>I can obtain them working in the R environment with the following statements:
>
>      temp <- polr(formula = formula1,  data = data1)
>      coeff <- summary(temp),
>
>but when the above statements are enclosed in a 
>function, summary reports the following error:
>
>Error in eval(expr, envir, enclos) : object "dat" not found
>
>Someone knows how I can solve the problem?

By giving us a short reproducible example?

Specifically we do not know:
1 - what formula1 is
2 - what the structure of data1 is
3 - what the enclosing function looks like
4 - what dat is


>Thanks for any help.
>Paolo

Michael Dewey
http://www.aghmed.fsnet.co.uk


From f.harrell at vanderbilt.edu  Tue Feb 20 14:48:30 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 20 Feb 2007 07:48:30 -0600
Subject: [R] Sample size
In-Reply-To: <2DC3A0F67A1E894BB03AEF915D9BC3F1014146AB@srmessagerie.chm.com>
References: <2DC3A0F67A1E894BB03AEF915D9BC3F1014146AB@srmessagerie.chm.com>
Message-ID: <45DAFC2E.1050404@vanderbilt.edu>

SAULEAU Erik-Andr? wrote:
> Dear R-list,
> 
> I have to design the validation of a score (ordinal values between 0 and 6) reputed to separate 4 groups of patients with known frequencies in the population. I think the more accurate is to calculate sample size under median test. Is there a function for that in R (not in the pwr package)?
> 
> Thanks in advance, with all my best, erik S.
> 
> 
> 
> 
> =================================
> Erik-Andr? SAULEAU
>  
> SEAIM
> Centre Hospitalier
> 87, Avenue d'Altkirch
> BP 1070
> 68051 Mulhouse C?dex
> Tel: 03 89 64 67 53
> Mel: sauleauea at ch-mulhouse.fr
> Web: www.ch-mulhouse.fr

The median test has low power and should be avoided.  For a validation 
study a hypothesis test is seldom of interest and you might base the 
sample size on the needed precision (margin of error; confidence 
interval width) of some estimate.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From Ralf.Finne at syh.fi  Tue Feb 20 15:06:27 2007
From: Ralf.Finne at syh.fi (Ralf Finne)
Date: Tue, 20 Feb 2007 16:06:27 +0200
Subject: [R] Reading Post-Script files
Message-ID: <45DB1C83020000EE00002EB3@valhall.syh.fi>

Hi everybody!
Is there any way to read a postscrit file into R?

All the best to you
Ralf Finne
SYH University of Applied Sciences
Vasa Finland


From f.calboli at imperial.ac.uk  Tue Feb 20 15:10:16 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Tue, 20 Feb 2007 14:10:16 +0000
Subject: [R] memory management uestion  [Broadcast]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03BCD20B@usctmx1106.merck.com>
References: <45D9E7BB.6010902@imperial.ac.uk>
	<Pine.LNX.4.64.0702191247150.2818@tajo.ucsd.edu>
	<45DA1078.50907@imperial.ac.uk>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03BCD20B@usctmx1106.merck.com>
Message-ID: <45DB0148.2050103@imperial.ac.uk>

Liaw, Andy wrote:
> I don't see why making copies of the columns you need inside the loop is
> "better" memory management.  If the data are in a matrix, accessing
> elements is quite fast.  If you're worrying about speed of that, do what
> Charles suggest: work with the transpose so that you are accessing
> elements in the same column in each iteration of the loop.

As I said, this is pretty academic, I am not looking for how to do something 
differetly.

Having said that, let me present this code:

for(i in gp){
    new[i,1] = ifelse(srow[i]>0, new[srow[i],zippo[i]], sav[i])
    new[i,2] = ifelse(drow[i]>0, new[drow[i],zappo[i]], sav[i])
  }

where gp is large vector and srow and drow are the dummy variables for:

srow = data[,2]
drow = data[,4]

If instead of the dummy variable I access the array directly (and its' a 600000 
x 6 array) the loop takes 2/3 days --not sure here, I killed it after 48 hours.

If I use dummy variables the code runs in 10 minutes-ish.

Comments?

Best,

Fede

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From lauri.nikkinen at iki.fi  Tue Feb 20 15:23:33 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Tue, 20 Feb 2007 16:23:33 +0200
Subject: [R] Reshape (pivot) question
In-Reply-To: <f8e6ff050702200537x2c13a6dfn89767378ed4fa913@mail.gmail.com>
References: <ba8c09910702200002k785dff1ajbee4e2b1916c0aba@mail.gmail.com>
	<644e1f320702200512k58a77971x1ee57dc3b58c0ec8@mail.gmail.com>
	<f8e6ff050702200537x2c13a6dfn89767378ed4fa913@mail.gmail.com>
Message-ID: <ba8c09910702200623u52c00c0cl7a2e3a014df02fc4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070220/4c9add1d/attachment.pl 

From sfinch9 at hotmail.com  Tue Feb 20 15:31:39 2007
From: sfinch9 at hotmail.com (Steven Finch)
Date: Tue, 20 Feb 2007 09:31:39 -0500
Subject: [R] contextstack overflow
Message-ID: <BAY101-F75335D701D95BB3DC2B4CEF890@phx.gbl>

Hello!

I written several implementations in R of R?my's algorithm for
generating random ordered strictly binary trees on 2n+1 vertices.

One implementation involves manufacturing character strings like:

"X <- list(0,list(0,0))"

for the case n=2.  If I perform the following two steps:

cmd <- "X <- list(0,list(0,0))"
eval(parse(text=cmd))

then X becomes a true nested list in R.  This works fine for n=2,
but often for n=200, an error message:

Error in parse(text = cmd) : contextstack overflow

appears and execution stops.  Clearly there exists an upper bound
on the allowable depth of nestings in R!  Can this upper bound be
easily increased?

Other implementations avoid this problem, so this issue is not
crucial to me.  I do wish, however, to understand the limits of
this particular approach.  Thank you!

Steve Finch
http://algo.inria.fr/bsolve/

P.S.  If anyone else has written R code for generating random
trees (on a fixed number of vertices), I would enjoy seeing this!

_________________________________________________________________
Don?t miss your chance to WIN 10 hours of private jet travel from Microsoft? 
Office Live http://clk.atdmt.com/MRT/go/mcrssaub0540002499mrt/direct/01/


From d.levy at maternite.chu-nancy.fr  Tue Feb 20 15:41:04 2007
From: d.levy at maternite.chu-nancy.fr (David LEVY)
Date: Tue, 20 Feb 2007 15:41:04 +0100
Subject: [R] "contingency table" for several variables
Message-ID: <MDEDINGLMBCDOPICGDKOEEDFCAAA.d.levy@maternite.chu-nancy.fr>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070220/b0cc293a/attachment.pl 

From rab45+ at pitt.edu  Tue Feb 20 15:44:02 2007
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Tue, 20 Feb 2007 09:44:02 -0500
Subject: [R] Installing Package rgl - Compilation Fails
In-Reply-To: <20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
	<1171918929.4041.22.camel@localhost.localdomain>
	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
Message-ID: <1171982642.3451.5.camel@localhost.localdomain>

On Mon, 2007-02-19 at 15:11 -0600, Ranjan Maitra wrote:
> The error is different now. It now cannot find Xext library. Do a yum search on this and install that.
> 
> yum provides libXext
> 
> which will give you the package Xext which needs to be installed.
> 
> You may also need to install the libXext-devel.i386 package.
> 
> HTH.
> Ranjan
> 
> 

Thanks. Installing libXext-devel allowed rgl to install.

Rick B.


From f.harrell at vanderbilt.edu  Tue Feb 20 15:56:59 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 20 Feb 2007 08:56:59 -0600
Subject: [R] "contingency table" for several variables
In-Reply-To: <MDEDINGLMBCDOPICGDKOEEDFCAAA.d.levy@maternite.chu-nancy.fr>
References: <MDEDINGLMBCDOPICGDKOEEDFCAAA.d.levy@maternite.chu-nancy.fr>
Message-ID: <45DB0C3B.7020301@vanderbilt.edu>

David LEVY wrote:
> DearList,
> 
> 
> 
> I ?m trying to draw ONE table that summarize SEVERAL categorical variables
> according to one classification variable, say ?sex?. The result would look
> like several contingency tables appended one to the other. All the variables
> belong to a data frame.
> 
> The summary.formula in Hmisc package does something pretty close and is
> ready for a Latex export  but I need either to get rid off the percentage
> (or put the count prior to the percentage )in the ?reverse? option or to add
> a chisquare test in the ?response? method.

Not a good idea.  The % is normalized for sample size differences so 
should be emphasized.

Frank

> 
> 
> 
> The result would look like the one of
> 
>> summary(sex~treatment+Symptoms, fun = table, method = ?response?)
> 
> in the help of  summary.formula but with chisquare tests attached.
> 
> Or :
> 
>> summary(sex~treatment+Symptoms, fun = table, method = ?reverse?, test= T)
> 
>  gives all the information, but I can?t use it for its form is not
> appropriate.
> 
> 
> 
> Is there any package where I could find a solution ? Any way to create a
> function that would add a test in the ?response? method ?
> 
> Otherwise, ftable should help but I don?t know how to use it with a
> data.frame.
> 
> 
> 
> Thanks for your help.
> 
> Regards,
> 
> David
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From murdoch at stats.uwo.ca  Tue Feb 20 15:59:22 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 20 Feb 2007 09:59:22 -0500
Subject: [R] Installing Package rgl - Compilation Fails
In-Reply-To: <1171982642.3451.5.camel@localhost.localdomain>
References: <1171913730.4041.7.camel@localhost.localdomain>	<45DA00EF.2050305@ebi.ac.uk>	<1171918929.4041.22.camel@localhost.localdomain>	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
	<1171982642.3451.5.camel@localhost.localdomain>
Message-ID: <45DB0CCA.5090809@stats.uwo.ca>

On 2/20/2007 9:44 AM, Rick Bilonick wrote:
> On Mon, 2007-02-19 at 15:11 -0600, Ranjan Maitra wrote:
>> The error is different now. It now cannot find Xext library. Do a yum search on this and install that.
>> 
>> yum provides libXext
>> 
>> which will give you the package Xext which needs to be installed.
>> 
>> You may also need to install the libXext-devel.i386 package.
>> 
>> HTH.
>> Ranjan
>> 
>> 
> 
> Thanks. Installing libXext-devel allowed rgl to install.

Would you mind summarizing what you started from, and what extras you 
needed to install?  I'd like to add this information to the rgl docs.

Duncan Murdoch


From Roger.Bivand at nhh.no  Tue Feb 20 16:10:25 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 20 Feb 2007 16:10:25 +0100 (CET)
Subject: [R] Reading Post-Script files
In-Reply-To: <45DB1C83020000EE00002EB3@valhall.syh.fi>
Message-ID: <Pine.LNX.4.44.0702201607130.12950-100000@reclus.nhh.no>

On Tue, 20 Feb 2007, Ralf Finne wrote:

> Hi everybody!
> Is there any way to read a postscrit file into R?

See http://www.r-project.org/useR-2006/Slides/Murrell.pdf

page 4, the grImport package, now on CRAN, with further notes on Paul 
Murrell's home page:

http://www.stat.auckland.ac.nz/~paul/

> 
> All the best to you
> Ralf Finne
> SYH University of Applied Sciences
> Vasa Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ted.harding at nessie.mcc.ac.uk  Tue Feb 20 16:11:37 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 20 Feb 2007 15:11:37 -0000 (GMT)
Subject: [R] Reading Post-Script files
In-Reply-To: <45DB1C83020000EE00002EB3@valhall.syh.fi>
Message-ID: <XFMail.070220151137.ted.harding@nessie.mcc.ac.uk>

On 20-Feb-07 Ralf Finne wrote:
> Hi everybody!
> Is there any way to read a postscrit file into R?
> 
> All the best to you
> Ralf Finne
> SYH University of Applied Sciences
> Vasa Finland

Well, yes ... since a PostScript file is ASCII text you could
use readline() ... but what you'd do with it after that I cannot
imagine! [So I'm joking here]

So, to come to the point: Why do you want to do that?

If, for example, the PS file displays tabular data which you
want to read into R as data, then perhaps one way would be to
use suitable utility software to convert the PS file to a PDF
file. Then you can display the PDF file using Acrobat Reader,
and use the mouse to copy the data into a file which you can
then edit up so that it can be read nicely into a dataframe
in R.

There are also utilities called ps2ascii (part of ghostscript;
often fails to do a clean job) and pstotext which you can get
from

  http://www.cs.wisc.edu/~ghost/doc/pstotext.htm

(which claims to do a better job, though I've not tested it)
which can convert a PS file into an ASCII text file which
gives essentially the same text layout as seen when you
display/print the PS file.

You may then be able to edit this into a form which R can read
as you want.

But, for better targeted advice, it would be useful to know
why you want to "read a PDF file into R"!

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Feb-07                                       Time: 15:11:34
------------------------------ XFMail ------------------------------


From info at aghmed.fsnet.co.uk  Tue Feb 20 16:21:00 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Tue, 20 Feb 2007 15:21:00 +0000
Subject: [R] summary polr
In-Reply-To: <45DB0883020000290000A521@stirling.iso.port.ac.uk>
References: <45DB0883020000290000A521@stirling.iso.port.ac.uk>
Message-ID: <7.0.0.16.0.20070220151714.019d6ce8@aghmed.fsnet.co.uk>

At 14:41 20/02/2007, you wrote:
Please do not just reply to me,
1 - I might not know
2 - it breaks the threading

>Hi
>
>here there is an example extracted from polr help in MASS:
>
>The function could be:
>
>        temp <- function(form, dat) {
>                     house.plr <- polr(formula = 
> form, weights = Freq, data = dat)
>                     coeff <- summary(house.plr)
>        return(coeff)}

Why do you try to redefine the coeff extractor function?
Try calling the results of summary summ or some 
other obvious name and I think you will find the problem goes away.

See also
 > library(fortunes)
 > fortune("dog")

Firstly, don't call your matrix 'matrix'. Would you call your dog 'dog'?
Anyway, it might clash with the function 'matrix'.
    -- Barry Rowlingson
       R-help (October 2004)



>the function can be called by:
>
>       temp(Sat ~ Infl + Type + Cont, housing)
>
>where all data is available from MASS, as it is 
>an example in R Help on 'polr'.
>
>Results are:
>
>        Re-fitting to get Hessian
>
>        Error in eval(expr, envir, enclos) : object "dat" not found
>
>Paolo Accadia
>
>
> >>> Michael Dewey <info at aghmed.fsnet.co.uk> 20/02/07 1:43 PM >>>
>At 15:21 19/02/2007, Paolo Accadia wrote:
> >Hi all,
> >
> >I have a problem to estimate Std. Error and
> >t-value by ????polr???  in library Mass.
>s.
> >They result from the summary of a polr object.
> >
> >I can obtain them working in the R environment 
> with the following statements:
> >
> >      temp <- polr(formula = formula1,  data = data1)
> >      coeff <- summary(temp),
> >
> >but when the above statements are enclosed in a
> >function, summary reports the following error:
> >
> >Error in eval(expr, envir, enclos) : object "dat" not found
> >
> >Someone knows how I can solve the problem?
>
>By giving us a short reproducible example?
>
>Specifically we do not know:
>1 - what formula1 is
>2 - what the structure of data1 is
>3 - what the enclosing function looks like
>4 - what dat is
>
>
> >Thanks for any help.
> >Paolo
>
>Michael Dewey
>http://www.aghmed.fsnet.co.uk

Michael Dewey
http://www.aghmed.fsnet.co.uk


From semspam at inode.at  Tue Feb 20 16:46:59 2007
From: semspam at inode.at (Martin)
Date: Tue, 20 Feb 2007 16:46:59 +0100
Subject: [R] Use R source in .net  (R Source in .net einbinden)
Message-ID: <000301c75506$5b637d40$122a77c0$@at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070220/a2d24f42/attachment.pl 

From Paolo.Accadia at port.ac.uk  Tue Feb 20 16:50:46 2007
From: Paolo.Accadia at port.ac.uk (Paolo Accadia)
Date: Tue, 20 Feb 2007 15:50:46 +0000
Subject: [R] R: Re:  summary polr
Message-ID: <45DB18D6020000290000A531@stirling.iso.port.ac.uk>

Hi all,

The problem is that when you try to use the function summary of a polr object in a function, it does not work.
The problem is not related to the formula or the structure of data involved.
It is probably related to the use of the function "vcov" in the code of summary for polr, and the iterative procedure to estimate the Hessian.

Anyway, here there is an example extracted from polr help in MASS:

The function could be:

      temp <- function(form, dat) {
                   house.plr <- polr(formula = form, weights = Freq, data = dat)
                   summ <- summary(house.plr)
      return(summ)}

the function can be called by:

     temp(Sat ~ Infl + Type + Cont, housing)

where all data is available from MASS, as it is an example in R Help on 'polr'.

Results are:

      Re-fitting to get Hessian

      Error in eval(expr, envir, enclos) : object "dat" not found

Paolo Accadia

>>> Michael Dewey <info at aghmed.fsnet.co.uk> 20/02/07 13.43 >>>
At 15:21 19/02/2007, Paolo Accadia wrote:
>Hi all,
>
>I have a problem to estimate Std. Error and 
>t-value by ???polr??  in library Mass.
>They result from the summary of a polr object.
>
>I can obtain them working in the R environment with the following statements:
>
>      temp <- polr(formula = formula1,  data = data1)
>      coeff <- summary(temp),
>
>but when the above statements are enclosed in a 
>function, summary reports the following error:
>
>Error in eval(expr, envir, enclos) : object "dat" not found
>
>Someone knows how I can solve the problem?

By giving us a short reproducible example?

Specifically we do not know:
1 - what formula1 is
2 - what the structure of data1 is
3 - what the enclosing function looks like
4 - what dat is


>Thanks for any help.
>Paolo

Michael Dewey
http://www.aghmed.fsnet.co.uk


From ripley at stats.ox.ac.uk  Tue Feb 20 17:00:39 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Feb 2007 16:00:39 +0000 (GMT)
Subject: [R] summary polr
In-Reply-To: <7.0.0.16.0.20070220151714.019d6ce8@aghmed.fsnet.co.uk>
References: <45DB0883020000290000A521@stirling.iso.port.ac.uk>
	<7.0.0.16.0.20070220151714.019d6ce8@aghmed.fsnet.co.uk>
Message-ID: <Pine.LNX.4.64.0702201557540.28773@gannet.stats.ox.ac.uk>

That's not it (the function is 'coef' not 'coeff', and R can tell 
functions and lists apart).

If you read the help page for polr you will see you could have used 
Hess=TRUE.  It works then.  THAT is why we needed an example, to see how 
you used the function.

On Tue, 20 Feb 2007, Michael Dewey wrote:

> At 14:41 20/02/2007, you wrote:
> Please do not just reply to me,
> 1 - I might not know
> 2 - it breaks the threading
>
>> Hi
>>
>> here there is an example extracted from polr help in MASS:
>>
>> The function could be:
>>
>>        temp <- function(form, dat) {
>>                     house.plr <- polr(formula =
>> form, weights = Freq, data = dat)
>>                     coeff <- summary(house.plr)
>>        return(coeff)}
>
> Why do you try to redefine the coeff extractor function?
> Try calling the results of summary summ or some
> other obvious name and I think you will find the problem goes away.
>
> See also
> > library(fortunes)
> > fortune("dog")
>
> Firstly, don't call your matrix 'matrix'. Would you call your dog 'dog'?
> Anyway, it might clash with the function 'matrix'.
>    -- Barry Rowlingson
>       R-help (October 2004)
>
>
>
>> the function can be called by:
>>
>>       temp(Sat ~ Infl + Type + Cont, housing)
>>
>> where all data is available from MASS, as it is
>> an example in R Help on 'polr'.
>>
>> Results are:
>>
>>        Re-fitting to get Hessian
>>
>>        Error in eval(expr, envir, enclos) : object "dat" not found
>>
>> Paolo Accadia
>>
>>
>>>>> Michael Dewey <info at aghmed.fsnet.co.uk> 20/02/07 1:43 PM >>>
>> At 15:21 19/02/2007, Paolo Accadia wrote:
>>> Hi all,
>>>
>>> I have a problem to estimate Std. Error and
>>> t-value by ????polr???  in library Mass.
>> s.
>>> They result from the summary of a polr object.
>>>
>>> I can obtain them working in the R environment
>> with the following statements:
>>>
>>>      temp <- polr(formula = formula1,  data = data1)
>>>      coeff <- summary(temp),
>>>
>>> but when the above statements are enclosed in a
>>> function, summary reports the following error:
>>>
>>> Error in eval(expr, envir, enclos) : object "dat" not found
>>>
>>> Someone knows how I can solve the problem?
>>
>> By giving us a short reproducible example?
>>
>> Specifically we do not know:
>> 1 - what formula1 is
>> 2 - what the structure of data1 is
>> 3 - what the enclosing function looks like
>> 4 - what dat is
>>
>>
>>> Thanks for any help.
>>> Paolo
>>
>> Michael Dewey
>> http://www.aghmed.fsnet.co.uk
>
> Michael Dewey
> http://www.aghmed.fsnet.co.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From mikewhite.diu at btconnect.com  Tue Feb 20 17:14:29 2007
From: mikewhite.diu at btconnect.com (Mike White)
Date: Tue, 20 Feb 2007 16:14:29 -0000
Subject: [R] Mahalanobis distance and probability of group membership using
	Hotelling's T2 distribution
Message-ID: <000701c7550a$337ecb50$6201a8c0@FSSFQCV7BGDVED>

I want to calculate the probability that a group will include a particular
point using the squared Mahalanobis distance to the centroid. I understand
that the squared Mahalanobis distance is distributed as chi-squared but that
for a small number of random samples from a multivariate normal population
the Hotellings T2 (T squared) distribution should be used.
I cannot find a function for Hotelling's T2 distribution in R (although from
a previous post I have been provided with functions for the Hotelling Test).
My understanding is that the Hotelling's T2 distribution is related to the F
distribution using the equation:
                             T2(u,v) = F(u, v-u+1)*vu/(v-u+1)
where u is the number of variables and v the number of group members.

I have written the R code below to compare the results from the chi-squared
distribution with the Hotelling's T2 distribution for probability of a
member being included within a group.
Please can anyone confirm whether or not this is the correct way to use
Hotelling's T2 distribution for probability of group membership. Also, when
testing a particular group member, is it preferable to leave that member out
when calculating the centre and covariance of the group for the Mahalanobis
distances?

Thanks
Mike White

############################################################################
####
## Hotelling T^2 distribution function
ph<-function(q, u, v, ...){
# q vector of quantiles as in function pf
# u number of independent variables
# v number of observations
if (!v > u+1) stop("n must be greater than p+1")
df1 <- u
df2 <- v-u+1
pf(q*df2/(v*u), df1, df2, ...)
}

# compare Chi-squared and Hotelling T^2 distributions for a group member
u<-3
v<-10
set.seed(1)
mat<-matrix(rnorm(v*u), nrow=v, ncol=u)
MD2<-mahalanobis(mat, center=colMeans(mat), cov=cov(mat))
d<-MD2[order(MD2)]
# select a point midway between nearest and furthest from centroid
dm<-d[length(d)/2]
1-ph(dm,u,v)    # probability using Hotelling T^2 distribution
# [1] 0.6577069
1-pchisq(dm, u) # probability using Chi-squared distribution
# [1] 0.5538466


From kubovy at virginia.edu  Tue Feb 20 17:31:16 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 20 Feb 2007 11:31:16 -0500
Subject: [R] Help with xlab and ylab distance from axes
Message-ID: <67477BCA-43E5-45DF-BEEE-ECA21B0059AE@virginia.edu>

Dear r-helpers,

In basic graphics, I have a figure with x and y axes suppresed. I  
would like to move the xlab and the ylab closer to the axes. Do I  
have to use mtext()?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From Lukas.Indermaur at eawag.ch  Tue Feb 20 17:33:47 2007
From: Lukas.Indermaur at eawag.ch (Indermaur Lukas)
Date: Tue, 20 Feb 2007 17:33:47 +0100
Subject: [R] testing slopes
Message-ID: <FE8C160D1505B24497FA7C78D4DADACA0478EF@EA-MAIL.eawag.wroot.emp-eaw.ch>

Hello
Instead of testing against 0 i would like to test regression slopes against -1. Any idea if there's an R script (package?) available.
 
Thanks for any hint.
Cheers
Lukas
 
 
 
 
??? 
Lukas Indermaur, PhD student 
eawag / Swiss Federal Institute of Aquatic Science and Technology 
ECO - Department of Aquatic Ecology
?berlandstrasse 133
CH-8600 D?bendorf
Switzerland
 
Phone: +41 (0) 71 220 38 25
Fax    : +41 (0) 44 823 53 15 
Email: lukas.indermaur at eawag.ch
www.lukasindermaur.ch


From Paolo.Accadia at port.ac.uk  Tue Feb 20 17:38:45 2007
From: Paolo.Accadia at port.ac.uk (Paolo Accadia)
Date: Tue, 20 Feb 2007 16:38:45 +0000
Subject: [R] summary polr
Message-ID: <45DB2415020000290000A54B@stirling.iso.port.ac.uk>

Thank you very much
Paolo

>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 20/02/07 4:00 PM >>>
That's not it (the function is 'coef' not 'coeff', and R can tell 
functions and lists apart).

If you read the help page for polr you will see you could have used 
Hess=TRUE.  It works then.  THAT is why we needed an example, to see how 
you used the function.

On Tue, 20 Feb 2007, Michael Dewey wrote:

> At 14:41 20/02/2007, you wrote:
> Please do not just reply to me,
> 1 - I might not know
> 2 - it breaks the threading
>
>> Hi
>>
>> here there is an example extracted from polr help in MASS:
>>
>> The function could be:
>>
>>        temp <- function(form, dat) {
>>                     house.plr <- polr(formula =
>> form, weights = Freq, data = dat)
>>                     coeff <- summary(house.plr)
>>        return(coeff)}
>
> Why do you try to redefine the coeff extractor function?
> Try calling the results of summary summ or some
> other obvious name and I think you will find the problem goes away.
>
> See also
> > library(fortunes)
> > fortune("dog")
>
> Firstly, don't call your matrix 'matrix'. Would you call your dog 'dog'?
> Anyway, it might clash with the function 'matrix'.
>    -- Barry Rowlingson
>       R-help (October 2004)
>
>
>
>> the function can be called by:
>>
>>       temp(Sat ~ Infl + Type + Cont, housing)
>>
>> where all data is available from MASS, as it is
>> an example in R Help on 'polr'.
>>
>> Results are:
>>
>>        Re-fitting to get Hessian
>>
>>        Error in eval(expr, envir, enclos) : object "dat" not found
>>
>> Paolo Accadia
>>
>>
>>>>> Michael Dewey <info at aghmed.fsnet.co.uk> 20/02/07 1:43 PM >>>
>> At 15:21 19/02/2007, Paolo Accadia wrote:
>>> Hi all,
>>>
>>> I have a problem to estimate Std. Error and
>>> t-value by ????polr???  in library Mass.
>> s.
>>> They result from the summary of a polr object.
>>>
>>> I can obtain them working in the R environment
>> with the following statements:
>>>
>>>      temp <- polr(formula = formula1,  data = data1)
>>>      coeff <- summary(temp),
>>>
>>> but when the above statements are enclosed in a
>>> function, summary reports the following error:
>>>
>>> Error in eval(expr, envir, enclos) : object "dat" not found
>>>
>>> Someone knows how I can solve the problem?
>>
>> By giving us a short reproducible example?
>>
>> Specifically we do not know:
>> 1 - what formula1 is
>> 2 - what the structure of data1 is
>> 3 - what the enclosing function looks like
>> 4 - what dat is
>>
>>
>>> Thanks for any help.
>>> Paolo
>>
>> Michael Dewey
>> http://www.aghmed.fsnet.co.uk
>
> Michael Dewey
> http://www.aghmed.fsnet.co.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Mathieu.DAcremont at pse.unige.ch  Tue Feb 20 17:48:25 2007
From: Mathieu.DAcremont at pse.unige.ch (Mathieu d'Acremont)
Date: Tue, 20 Feb 2007 17:48:25 +0100
Subject: [R] Standardized residual variances in SEM
Message-ID: <45DB2659.5010305@pse.unige.ch>


Hello,

I'm using the "sem" package to do a confirmatory factor analysis on data 
collected with a questionnaire. In the model, there is a unique factor G 
and 23 items. I would like to calculate the standardized residual 
variance of the observed variables. "Sem" only gives the residual 
variance with the "summary" function, or the standardized loadings with 
the "standardized.coefficients" function (see below). Does anybody know 
how to standardized the residual variance ?

Sincerely yours,

 > summary(semModif.tmp, digits=2)

  Model Chisquare =  601   Df =  229 Pr(>Chisq) = 0
  Chisquare (null model) =  2936   Df =  253
  Goodness-of-fit index =  0.81
  Adjusted goodness-of-fit index =  0.78
  RMSEA index =  0.08   90% CI: (0.072, 0.088)
  Bentler-Bonnett NFI =  0.8
  Tucker-Lewis NNFI =  0.85
  Bentler CFI =  0.86
  BIC =  -667

  Normalized Residuals
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
-2.6300 -0.5640 -0.0728 -0.0067  0.5530  3.5500

  Parameter Estimates
         Estimate Std Error z value Pr(>|z|)
param1  0.78     0.073     10.7    0.0e+00  Q1 <--- G
param2  0.79     0.065     12.1    0.0e+00  Q2 <--- G
param3  0.63     0.073      8.5    0.0e+00  Q3 <--- G
param4  0.74     0.066     11.2    0.0e+00  Q4 <--- G
param6  0.81     0.068     11.9    0.0e+00  Q6 <--- G
param7  0.65     0.060     11.0    0.0e+00  Q7 <--- G
param9  0.60     0.059     10.1    0.0e+00  Q9 <--- G
param10 0.64     0.065      9.9    0.0e+00  Q10 <--- G
param11 0.72     0.054     13.3    0.0e+00  Q11 <--- G
param12 0.59     0.063      9.3    0.0e+00  Q12 <--- G
param13 0.61     0.069      8.7    0.0e+00  Q13 <--- G
param14 0.70     0.074      9.6    0.0e+00  Q14 <--- G
param15 0.68     0.066     10.4    0.0e+00  Q15 <--- G
param16 0.75     0.056     13.3    0.0e+00  Q16 <--- G
param17 0.86     0.060     14.3    0.0e+00  Q17 <--- G
param18 0.63     0.059     10.7    0.0e+00  Q18 <--- G
param19 0.75     0.062     12.2    0.0e+00  Q19 <--- G
param20 0.68     0.060     11.4    0.0e+00  Q20 <--- G
param21 0.64     0.068      9.3    0.0e+00  Q21 <--- G
param22 0.63     0.065      9.7    0.0e+00  Q22 <--- G
param23 0.71     0.065     10.9    0.0e+00  Q23 <--- G
param24 0.70     0.052     13.7    0.0e+00  Q24 <--- G
param25 0.41     0.066      6.3    3.4e-10  Q25 <--- G
param26 0.98     0.091     10.8    0.0e+00  Q1 <--> Q1
param27 0.72     0.068     10.6    0.0e+00  Q2 <--> Q2
param28 1.09     0.099     11.0    0.0e+00  Q3 <--> Q3
param29 0.77     0.072     10.7    0.0e+00  Q4 <--> Q4
param31 0.79     0.075     10.6    0.0e+00  Q6 <--> Q6
param32 0.64     0.059     10.7    0.0e+00  Q7 <--> Q7
param34 0.66     0.061     10.8    0.0e+00  Q9 <--> Q9
param35 0.79     0.073     10.8    0.0e+00  Q10 <--> Q10
param36 0.45     0.043     10.4    0.0e+00  Q11 <--> Q11
param37 0.79     0.072     10.9    0.0e+00  Q12 <--> Q12
param38 0.96     0.088     11.0    0.0e+00  Q13 <--> Q13
param39 1.05     0.096     10.9    0.0e+00  Q14 <--> Q14
param40 0.80     0.074     10.8    0.0e+00  Q15 <--> Q15
param41 0.49     0.047     10.4    0.0e+00  Q16 <--> Q16
param42 0.51     0.050     10.1    0.0e+00  Q17 <--> Q17
param43 0.63     0.059     10.8    0.0e+00  Q18 <--> Q18
param44 0.64     0.060     10.6    0.0e+00  Q19 <--> Q19
param45 0.63     0.059     10.7    0.0e+00  Q20 <--> Q20
param46 0.91     0.084     10.9    0.0e+00  Q21 <--> Q21
param47 0.82     0.076     10.9    0.0e+00  Q22 <--> Q22
param48 0.77     0.071     10.8    0.0e+00  Q23 <--> Q23
param49 0.39     0.038     10.3    0.0e+00  Q24 <--> Q24
param50 0.95     0.086     11.1    0.0e+00  Q25 <--> Q25
param51 0.29     0.047      6.3    4.0e-10  Q9 <--> Q7

  Iterations =  16
 > standardized.coefficients(semModif.tmp, digits=2)
                 Std. Estimate
param1  param1  0.62          Q1 <--- G
param2  param2  0.68          Q2 <--- G
param3  param3  0.51          Q3 <--- G
param4  param4  0.64          Q4 <--- G
param6  param6  0.67          Q6 <--- G
param7  param7  0.63          Q7 <--- G
param9  param9  0.59          Q9 <--- G
param10 param10 0.58          Q10 <--- G
param11 param11 0.73          Q11 <--- G
param12 param12 0.55          Q12 <--- G
param13 param13 0.53          Q13 <--- G
param14 param14 0.57          Q14 <--- G
param15 param15 0.61          Q15 <--- G
param16 param16 0.73          Q16 <--- G
param17 param17 0.77          Q17 <--- G
param18 param18 0.62          Q18 <--- G
param19 param19 0.69          Q19 <--- G
param20 param20 0.65          Q20 <--- G
param21 param21 0.55          Q21 <--- G
param22 param22 0.57          Q22 <--- G
param23 param23 0.63          Q23 <--- G
param24 param24 0.75          Q24 <--- G
param25 param25 0.39          Q25 <--- G
 >

-- 
Mathieu d'Acremont, PhD		Mathieu.Dacremont at pse.unige.ch
Ma?tre-Assistant               	tel/fax +4122 379 98 20/44

P?le de Recherche National en Sciences Affectives
CISA - Universit? de Gen?ve
Rue des Battoirs 7
CH-1205 Gen?ve
http://affect.unige.ch/


From marc_schwartz at comcast.net  Tue Feb 20 17:51:05 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 20 Feb 2007 10:51:05 -0600
Subject: [R] Help with xlab and ylab distance from axes
In-Reply-To: <67477BCA-43E5-45DF-BEEE-ECA21B0059AE@virginia.edu>
References: <67477BCA-43E5-45DF-BEEE-ECA21B0059AE@virginia.edu>
Message-ID: <1171990265.4804.3.camel@localhost.localdomain>

On Tue, 2007-02-20 at 11:31 -0500, Michael Kubovy wrote:
> Dear r-helpers,
> 
> In basic graphics, I have a figure with x and y axes suppresed. I  
> would like to move the xlab and the ylab closer to the axes. Do I  
> have to use mtext()?

Michael,

You could set the first value in par("mgp") in the plot() call.  

Compare:

# Default values
plot(1, mgp = c(3, 1, 0), axes = FALSE)

# Move the labels closer
plot(1, mgp = c(1, 1, 0), axes = FALSE)


Changing that value is comparable to changing the 'line' argument in
mtext().

See ?par

HTH,

Marc Schwartz


From rab45+ at pitt.edu  Tue Feb 20 17:59:24 2007
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Tue, 20 Feb 2007 11:59:24 -0500
Subject: [R] Installing Package rgl - Compilation Fails - Summary
In-Reply-To: <45DB0CCA.5090809@stats.uwo.ca>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
	<1171918929.4041.22.camel@localhost.localdomain>
	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
	<1171982642.3451.5.camel@localhost.localdomain>
	<45DB0CCA.5090809@stats.uwo.ca>
Message-ID: <1171990765.3451.12.camel@localhost.localdomain>

Summarizing:

I'm running R 2.4.1 on a current FC6 32-bit system. In order to have the
rgl R package install, I needed to install both mesa-libGLU-devel (FC6
version is 6.5.1-9) and libXext-devel (FC6) rpm packages. Thanks to
everyone who commented.

Rick B.


From Dimitris.Rizopoulos at med.kuleuven.be  Tue Feb 20 18:07:18 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 20 Feb 2007 18:07:18 +0100
Subject: [R] testing slopes
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478EF@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA0478EF@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <20070220180718.06pej63dl934w480@webmail5.kuleuven.be>

two options are to use an offset term or the linear.hypothesis()  
function from package car, e.g.,

y <- rnorm(100, 2 - 1 * (x <- runif(100, -3, 3)), 3)
############
fit0 <- lm(y ~ 1 + offset(-x))
fit1 <- lm(y ~ x)
anova(fit0, fit1)

library(car)
linear.hypothesis(fit1, c("x = -1"))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Indermaur Lukas <Lukas.Indermaur at eawag.ch>:

> Hello
> Instead of testing against 0 i would like to test regression slopes   
> against -1. Any idea if there's an R script (package?) available.
>
> Thanks for any hint.
> Cheers
> Lukas
>
>
>
>
> ???
> Lukas Indermaur, PhD student
> eawag / Swiss Federal Institute of Aquatic Science and Technology
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
>
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From cberry at tajo.ucsd.edu  Tue Feb 20 18:24:10 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 20 Feb 2007 09:24:10 -0800
Subject: [R] memory management uestion  [Broadcast]
In-Reply-To: <45DB0148.2050103@imperial.ac.uk>
References: <45D9E7BB.6010902@imperial.ac.uk>
	<Pine.LNX.4.64.0702191247150.2818@tajo.ucsd.edu>
	<45DA1078.50907@imperial.ac.uk>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03BCD20B@usctmx1106.merck.com>
	<45DB0148.2050103@imperial.ac.uk>
Message-ID: <Pine.LNX.4.64.0702200840030.8053@tajo.ucsd.edu>

On Tue, 20 Feb 2007, Federico Calboli wrote:

> Liaw, Andy wrote:
>>  I don't see why making copies of the columns you need inside the loop is
>>  "better" memory management.  If the data are in a matrix, accessing
>>  elements is quite fast.  If you're worrying about speed of that, do what
>>  Charles suggest: work with the transpose so that you are accessing
>>  elements in the same column in each iteration of the loop.
>
> As I said, this is pretty academic, I am not looking for how to do something 
> differetly.
>
> Having said that, let me present this code:
>
> for(i in gp){
>    new[i,1] = ifelse(srow[i]>0, new[srow[i],zippo[i]], sav[i])
>    new[i,2] = ifelse(drow[i]>0, new[drow[i],zappo[i]], sav[i])
>  }
>
> where gp is large vector and srow and drow are the dummy variables for:
>
> srow = data[,2]
> drow = data[,4]
>
> If instead of the dummy variable I access the array directly (and its' a 
> 600000 x 6 array) the loop takes 2/3 days --not sure here, I killed it after 
> 48 hours.
>
> If I use dummy variables the code runs in 10 minutes-ish.
>
> Comments?


This is a bit different than your original post (where it appeared that 
you were manipulating one row of a matrix at a time), but the issue is the 
same.

As suggested in my earlier email this looks like a caching issue, and this 
is not peculiar to R.

Viz.

"Most modern CPUs are so fast that for most program workloads the locality 
of reference of memory accesses, and the efficiency of the caching and 
memory transfer between different levels of the hierarchy, is the 
practical limitation on processing speed. As a result, the CPU spends much 
of its time idling, waiting for memory I/O to complete."

(from http://en.wikipedia.org/wiki/Memory_hierarchy)


The computation you have is challenging to your cache, and the effect of 
dropping unused columns of your 'data' object by assiging the 
columns used  to 'srow' and 'drow' has lightened the load.

If you do not know why SAXPY and friends are written as they are, a little 
bit of study will be rewarded by a much better understanding of these 
issues. I think Golub and Van Loan's 'Matrix Computations' touches on this 
(but I do not have my copy close to hand to check).


>
> Best,
>
> Fede
>
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From f.calboli at imperial.ac.uk  Tue Feb 20 18:36:59 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Tue, 20 Feb 2007 17:36:59 +0000
Subject: [R] memory management uestion  [Broadcast]
In-Reply-To: <Pine.LNX.4.64.0702200840030.8053@tajo.ucsd.edu>
References: <45D9E7BB.6010902@imperial.ac.uk>
	<Pine.LNX.4.64.0702191247150.2818@tajo.ucsd.edu>
	<45DA1078.50907@imperial.ac.uk>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03BCD20B@usctmx1106.merck.com>
	<45DB0148.2050103@imperial.ac.uk>
	<Pine.LNX.4.64.0702200840030.8053@tajo.ucsd.edu>
Message-ID: <45DB31BB.2020505@imperial.ac.uk>

Charles C. Berry wrote:

> 
> 
> This is a bit different than your original post (where it appeared that 
> you were manipulating one row of a matrix at a time), but the issue is 
> the same.
> 
> As suggested in my earlier email this looks like a caching issue, and 
> this is not peculiar to R.
> 
> Viz.
> 
> "Most modern CPUs are so fast that for most program workloads the 
> locality of reference of memory accesses, and the efficiency of the 
> caching and memory transfer between different levels of the hierarchy, 
> is the practical limitation on processing speed. As a result, the CPU 
> spends much of its time idling, waiting for memory I/O to complete."
> 
> (from http://en.wikipedia.org/wiki/Memory_hierarchy)
> 
> 
> The computation you have is challenging to your cache, and the effect of 
> dropping unused columns of your 'data' object by assiging the columns 
> used  to 'srow' and 'drow' has lightened the load.
> 
> If you do not know why SAXPY and friends are written as they are, a 
> little bit of study will be rewarded by a much better understanding of 
> these issues. I think Golub and Van Loan's 'Matrix Computations' touches 
> on this (but I do not have my copy close to hand to check).

Thanks for the clarifications. My bottom line is, I prefer dummy variables 
because they allow me to write cleaner code, with a shorter line for the same 
instruction i.e. less chances of creeping errors (+ turning into -, etc).

I've been challenged that that's memory inefficent, and I wanted to have the 
opinion of people with more experience than mine on the matter.

Best,

Fede

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From aa2007r at gmail.com  Tue Feb 20 18:46:28 2007
From: aa2007r at gmail.com (AA)
Date: Tue, 20 Feb 2007 12:46:28 -0500
Subject: [R] Calculating the Sharpe ratio
In-Reply-To: <45D9A87B.4060002@yahoo.co.uk>
References: <45D9A87B.4060002@yahoo.co.uk>
Message-ID: <55dcc5de0702200946w7de70d02ta888df63b0b5376@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070220/b56ae02d/attachment.pl 

From patrick.giraudoux at univ-fcomte.fr  Tue Feb 20 18:49:06 2007
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Tue, 20 Feb 2007 18:49:06 +0100
Subject: [R] interaction term and scatterplot3d
In-Reply-To: <45DAB2A5.4030206@univ-fcomte.fr>
References: <45DAB2A5.4030206@univ-fcomte.fr>
Message-ID: <45DB3492.2040600@univ-fcomte.fr>

Sorry to answer to myself. A solution was trivial with lattice... (as 
often !)

library(lattice)

modbusetmp<-lm(IKA_buse ~ Ct *Cc,data=dtbuse)

G1<-cloud(IKA_buse~Ct*Cc,type="h",data=dtbuse)
G2<-cloud(IKA_buse~Ct*Cc,data=dtbuse)

seqCc<-seq(min(dtbuse$Cc),max(dtbuse$Cc),l=20)
seqCt<-seq(min(dtbuse$Ct),max(dtbuse$Ct),l=20)
grille<-expand.grid(Cc=seqCc,Ct=seqCt)
zbuse<-predict(modbusetmp,newdata=grille,type=response)
G3<-wireframe(zbuse~grille$Ct*grille$Cc)

print(G3,more=T)
print(G1,more=T)
print(G2)

Some obvious improvements can be done with labels and differencial 
colors (above or below the surface) can be attributed comparing 
observations to predicted values at the same x y values...


Patrick Giraudoux a ?crit :
> Dear Listers,
>
> I would be interested in representing a trend surface including an 
> interaction term z = f(x,y,x.y) - eg the type of chart obtained with 
> persp() or wireframe(), then adding datapoints as a cloud, ideally 
> with dots which are under the surface in a color, and those who are 
> above in another color. An other option would be to draw segments 
> between the dots and the ground of the chart.
>
> scatterplot3d looks like being close to do such things except it does 
> not to include (to my knowledge) a coefficient for the interaction 
> term (thus just model z = f(x,y).
>
> Does anybody has an idea where to start with this and the main steps? 
> Or a place/website where some script examples can be found?
>
> Patrick
>
>


From maitra at iastate.edu  Tue Feb 20 19:11:51 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Tue, 20 Feb 2007 12:11:51 -0600
Subject: [R] Installing Package rgl - Compilation Fails - Summary
In-Reply-To: <1171990765.3451.12.camel@localhost.localdomain>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
	<1171918929.4041.22.camel@localhost.localdomain>
	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
	<1171982642.3451.5.camel@localhost.localdomain>
	<45DB0CCA.5090809@stats.uwo.ca>
	<1171990765.3451.12.camel@localhost.localdomain>
Message-ID: <20070220121151.0e54f526@subarnarekha.stat.iastate.edu>

Hi Duncan,

I don't know if this will list all the dependencies for your documentation, since Rick's error messages did not involve libraries and header files already installed by him for something else, perhaps.

Just a thought.

Ranjan



On Tue, 20 Feb 2007 11:59:24 -0500 Rick Bilonick <rab45+ at pitt.edu> wrote:

> Summarizing:
> 
> I'm running R 2.4.1 on a current FC6 32-bit system. In order to have the
> rgl R package install, I needed to install both mesa-libGLU-devel (FC6
> version is 6.5.1-9) and libXext-devel (FC6) rpm packages. Thanks to
> everyone who commented.
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From zelickr at pdx.edu  Tue Feb 20 19:17:25 2007
From: zelickr at pdx.edu (Randy Zelick)
Date: Tue, 20 Feb 2007 10:17:25 -0800 (PST)
Subject: [R] linux gplots install unhappy
Message-ID: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>

Hello all,

I use R on both windows and a "mainframe" linux installation (RedHat 
enterprise 3.0, which they tell me is soon to be upgraded to 4.0). On 
windows I installed the package gplots without trouble, and it works fine. 
When I attempted to do the same on the unix computer, the following error 
message was forthcoming:




downloaded 216Kb

* Installing *source* package 'gplots' ...
** R
** data
** inst
** preparing package for lazy loading
Loading required package: gtools
Warning in library(pkg, character.only = TRUE, logical = TRUE, lib.loc = 
lib.loc) :
          there is no package called 'gtools'
Error: package 'gtools' could not be loaded
Execution halted
ERROR: lazy loading failed for package 'gplots'
** Removing '/n/fs/disk/resuser02/u/zelickr/R/library/gplots'

The downloaded packages are in
         /tmp/RtmpikM2JW/downloaded_packages
Warning messages:
1: installation of package 'gplots' had non-zero exit status in: 
install.packages("gplots", lib = "~/R/library")
2: cannot create HTML package index in: 
tools:::unix.packages.html(.Library)



Can someone provide the bit of information I need to progress with this?

Thanks very much,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201


From murdoch at stats.uwo.ca  Tue Feb 20 19:18:33 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 20 Feb 2007 13:18:33 -0500
Subject: [R] Installing Package rgl - Compilation Fails - Summary
In-Reply-To: <20070220121151.0e54f526@subarnarekha.stat.iastate.edu>
References: <1171913730.4041.7.camel@localhost.localdomain>	<45DA00EF.2050305@ebi.ac.uk>	<1171918929.4041.22.camel@localhost.localdomain>	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>	<1171982642.3451.5.camel@localhost.localdomain>	<45DB0CCA.5090809@stats.uwo.ca>	<1171990765.3451.12.camel@localhost.localdomain>
	<20070220121151.0e54f526@subarnarekha.stat.iastate.edu>
Message-ID: <45DB3B79.30004@stats.uwo.ca>

On 2/20/2007 1:11 PM, Ranjan Maitra wrote:
> Hi Duncan,
> 
> I don't know if this will list all the dependencies for your documentation, since Rick's error messages did not involve libraries and header files already installed by him for something else, perhaps.

No, but it's a start:  I'm thinking of something more like a FAQ than 
comprehensive documentation.  Comprehensive docs are not feasible to 
maintain (there are so many systems that Daniel and I don't use), but 
hints that two non-standard packages solved one person's problems might 
be enough of a hint to get someone else going.

Duncan Murdoch


From aiminy at iastate.edu  Tue Feb 20 19:21:30 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Tue, 20 Feb 2007 12:21:30 -0600
Subject: [R] linux gplots install unhappy
In-Reply-To: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
References: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
Message-ID: <6.2.3.4.2.20070220122100.02904e10@aiminy.mail.iastate.edu>


install gtools package firstly

Aimin

At 12:17 PM 2/20/2007, Randy Zelick wrote:
>gtools'


From ThadenJohnJ at uams.edu  Tue Feb 20 19:23:02 2007
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Tue, 20 Feb 2007 12:23:02 -0600
Subject: [R] baseline fitters
References: <mailman.11.1171969206.30825.r-help@stat.math.ethz.ch>
Message-ID: <B1614B0C915A654A9C29BB71DA80E0DD1E9650@MAIL2.ad.uams.edu>

I am pretty pleased with baselines I fit to chromatograms using the
runquantile() function in caTools(v1.6) when its probs parameter is 
set to 0.2 and its k parameter to ~1/20th of n (e.g., k ~ 225 for n ~ 
4500, where n is time series length).  This ignores occasional low-
side outliers, and, after baseline subtraction, I can re-adjust any
negative values to zero.
  
But runquantile's computation time proves exceedingly long for my large
datasets, particularly if I set the endrule parameter to 'func'.  Here is
what caTools author Jarek Tuszynski says about relative speeds of various
running-window functions:

   - runmin, runmax, runmean run at O(n) 
   - runmean(..., alg="exact") can have worst case speed of O(n^2) for 
     some small data vectors, but average case is still close to O(n). 
   - runquantile and runmad run at O(n*k) 
   - runmed - related R function runs at O(n*log(k))

The obvious alternative runmin() performs poorly due to dropout (zero-
and low-value 'reverse-spikes') in the data. And runmed fits a baseline that,
upon subtraction, obviously will send half the values into the negative, not
suitable for my application. I jimmied something together
with runmin and runmedian that is considerably faster; unfortunately,
the fit seems less good, at least by eye, due still to the bad runmin
behavior.

I'd be interested in other baseline fitting suggestions implemented
or implementable in R (I'm using v. 2.4.1pat under WinXP).  Why, for
instance, can I not find a running trimmean function? Because it 
offers no computational savings over runquantile?

Also, the 'func' setting for the caTools endrule parameter--which adjusts the
value of k as ends are approached--is said not to be optimized (using
this option does add further time to computations).  Is there an alter-
native that would be speedier, e.g., setting endrule = "keep" and then
subsequently treating ends with R's smoothEnds() function?

-John Thaden
Little Rock, Arkansas USA

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From bcarvalh at jhsph.edu  Tue Feb 20 19:23:27 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 20 Feb 2007 13:23:27 -0500
Subject: [R] linux gplots install unhappy
In-Reply-To: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
References: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
Message-ID: <C24CC6B2-5280-4EC3-99B5-1D5AA239EE51@jhsph.edu>

well, it's complaining because you don't have gtools installed.

how about:

install.packages("gplots", dep=T)

?

b

On Feb 20, 2007, at 1:17 PM, Randy Zelick wrote:

> Hello all,
>
> I use R on both windows and a "mainframe" linux installation (RedHat
> enterprise 3.0, which they tell me is soon to be upgraded to 4.0). On
> windows I installed the package gplots without trouble, and it  
> works fine.
> When I attempted to do the same on the unix computer, the following  
> error
> message was forthcoming:
>
>
>
>
> downloaded 216Kb
>
> * Installing *source* package 'gplots' ...
> ** R
> ** data
> ** inst
> ** preparing package for lazy loading
> Loading required package: gtools
> Warning in library(pkg, character.only = TRUE, logical = TRUE,  
> lib.loc =
> lib.loc) :
>           there is no package called 'gtools'
> Error: package 'gtools' could not be loaded
> Execution halted
> ERROR: lazy loading failed for package 'gplots'
> ** Removing '/n/fs/disk/resuser02/u/zelickr/R/library/gplots'
>
> The downloaded packages are in
>          /tmp/RtmpikM2JW/downloaded_packages
> Warning messages:
> 1: installation of package 'gplots' had non-zero exit status in:
> install.packages("gplots", lib = "~/R/library")
> 2: cannot create HTML package index in:
> tools:::unix.packages.html(.Library)
>
>
>
> Can someone provide the bit of information I need to progress with  
> this?
>
> Thanks very much,
>
> =Randy=
>
> R. Zelick				email: zelickr at pdx.edu
> Department of Biology			voice: 503-725-3086
> Portland State University		fax:   503-725-3888
>
> mailing:
> P.O. Box 751
> Portland, OR 97207
>
> shipping:
> 1719 SW 10th Ave, Room 246
> Portland, OR 97201
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Tue Feb 20 19:27:54 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 20 Feb 2007 12:27:54 -0600
Subject: [R] linux gplots install unhappy
In-Reply-To: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
References: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
Message-ID: <1171996074.4804.13.camel@localhost.localdomain>

On Tue, 2007-02-20 at 10:17 -0800, Randy Zelick wrote:
> Hello all,
> 
> I use R on both windows and a "mainframe" linux installation (RedHat 
> enterprise 3.0, which they tell me is soon to be upgraded to 4.0). On 
> windows I installed the package gplots without trouble, and it works fine. 
> When I attempted to do the same on the unix computer, the following error 
> message was forthcoming:
> 
> 
> 
> 
> downloaded 216Kb
> 
> * Installing *source* package 'gplots' ...
> ** R
> ** data
> ** inst
> ** preparing package for lazy loading
> Loading required package: gtools
> Warning in library(pkg, character.only = TRUE, logical = TRUE, lib.loc = 
> lib.loc) :
>           there is no package called 'gtools'
> Error: package 'gtools' could not be loaded
> Execution halted
> ERROR: lazy loading failed for package 'gplots'
> ** Removing '/n/fs/disk/resuser02/u/zelickr/R/library/gplots'
> 
> The downloaded packages are in
>          /tmp/RtmpikM2JW/downloaded_packages
> Warning messages:
> 1: installation of package 'gplots' had non-zero exit status in: 
> install.packages("gplots", lib = "~/R/library")
> 2: cannot create HTML package index in: 
> tools:::unix.packages.html(.Library)
> 
> 
> 
> Can someone provide the bit of information I need to progress with this?
> 
> Thanks very much,
> 
> =Randy=

gplots has a dependency on other packages (gtools and gdata).

Thus, when you install it use:

  install.packages("gplots", dependencies = TRUE, ...)

That will download and install the other packages as well.

There should have been a similar requirement under Windows, so not sure
what may have been different there, unless there is some confounding due
to your not installing the packages in standard locations, given some of
the output above. I presume that this is because you don't have root
access on the Linux server and you are installing to your local user
path.

HTH,

Marc Schwartz


From tguennel at vcu.edu  Tue Feb 20 19:47:03 2007
From: tguennel at vcu.edu (Tobias Guennel)
Date: Tue, 20 Feb 2007 19:47:03 +0100
Subject: [R] User defined split function in rpart
Message-ID: <002c01c7551f$869be7e0$14b2a8c0@petry>

I have made some progress with the user defined splitting function and I got
a lot of the things I needed to work. However, I am still stuck on accessing
the node data. It would probably be enough if somebody could tell me, how I
can access the original data frame of the call to rpart. 
So if the call is: fit0 <- rpart(Sat ~Infl +Cont+ Type,
	     housing, control=rpart.control(minsplit=10, xval=0),
	     method=alist)
how can I access the housing data frame within the user defined splitting
function?

Any input would be highly appreciated!

Thank you
Tobias Guennel

-----Original Message-----
From: Tobias Guennel [mailto:tguennel at vcu.edu] 
Sent: Monday, February 19, 2007 3:40 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] User defined split function in rpart

Maybe I should explain my Problem a little bit more detailed.
The rpart package allows for user defined split functions. An example is
given in the source/test directory of the package as usersplits.R.
The comments say that three functions have to be supplied:
1. "The 'evaluation' function.  Called once per node.
  Produce a label (1 or more elements long) for labeling each node,
  and a deviance." 
2. The split function, where most of the work occurs.
   Called once per split variable per node.
3. The init function:
   fix up y to deal with offsets
   return a dummy parms list
   numresp is the number of values produced by the eval routine's "label".

I have altered the evaluation function and the split function for my needs.
Within those functions, I need to fit a proportional odds model to the data
of the current node. I am using the polr() routine from the MASS package to
fit the model. 
Now my problem is, how can I call the polr() function only with the data of
the current node. That's what I tried so far:

evalfunc <- function(y,x,parms,data) {
       
pomnode<-polr(data$y~data$x,data,weights=data$Freq)
parprobs<-predict(pomnode,type="probs")
dev<-0
K<-dim(parprobs)[2]
N<-dim(parprobs)[1]/K
for(i in 1:N){
tempsum<-0
Ni<-0
for(l in 1:K){
Ni<-Ni+data$Freq[K*(i-1)+l]
}
for(j in 1:K){
tempsum<-tempsum+data$Freq[K*(i-1)+j]/Ni*log(parprobs[i,j]*Ni/data$Freq[K*(i
-1)+j])
}
dev=dev+Ni*tempsum
}
dev=-2*dev
wmean<-1
list(label= wmean, deviance=dev)

} 

I get the error: Error in eval(expr, envir, enclos) : argument "data" is
missing, with no default

How can I use the data of the current node?

Thank you
Tobias Guennel


From ripley at stats.ox.ac.uk  Tue Feb 20 19:59:03 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Feb 2007 18:59:03 +0000 (GMT)
Subject: [R] linux gplots install unhappy
In-Reply-To: <1171996074.4804.13.camel@localhost.localdomain>
References: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
	<1171996074.4804.13.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0702201857060.19530@gannet.stats.ox.ac.uk>

On Tue, 20 Feb 2007, Marc Schwartz wrote:

> On Tue, 2007-02-20 at 10:17 -0800, Randy Zelick wrote:
>> Hello all,
>>
>> I use R on both windows and a "mainframe" linux installation (RedHat
>> enterprise 3.0, which they tell me is soon to be upgraded to 4.0). On
>> windows I installed the package gplots without trouble, and it works fine.
>> When I attempted to do the same on the unix computer, the following error
>> message was forthcoming:
>>
>>
>>
>>
>> downloaded 216Kb
>>
>> * Installing *source* package 'gplots' ...
>> ** R
>> ** data
>> ** inst
>> ** preparing package for lazy loading
>> Loading required package: gtools
>> Warning in library(pkg, character.only = TRUE, logical = TRUE, lib.loc =
>> lib.loc) :
>>           there is no package called 'gtools'
>> Error: package 'gtools' could not be loaded
>> Execution halted
>> ERROR: lazy loading failed for package 'gplots'
>> ** Removing '/n/fs/disk/resuser02/u/zelickr/R/library/gplots'
>>
>> The downloaded packages are in
>>          /tmp/RtmpikM2JW/downloaded_packages
>> Warning messages:
>> 1: installation of package 'gplots' had non-zero exit status in:
>> install.packages("gplots", lib = "~/R/library")
>> 2: cannot create HTML package index in:
>> tools:::unix.packages.html(.Library)
>>
>>
>>
>> Can someone provide the bit of information I need to progress with this?
>>
>> Thanks very much,
>>
>> =Randy=
>
> gplots has a dependency on other packages (gtools and gdata).
>
> Thus, when you install it use:
>
>  install.packages("gplots", dependencies = TRUE, ...)
>
> That will download and install the other packages as well.
>
> There should have been a similar requirement under Windows, so not sure
> what may have been different there, unless there is some confounding due

You don't need gtools to install a binary package, just to use it!
I suspect the menu was used on Windows, where dependencies = TRUE is the 
default.

Come 2.5.0 this sort of query will go away, as the essential dependencies 
become the default on all platforms.

> to your not installing the packages in standard locations, given some of
> the output above. I presume that this is because you don't have root
> access on the Linux server and you are installing to your local user
> path.
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Feb 20 19:59:29 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Feb 2007 18:59:29 +0000 (GMT)
Subject: [R] Installing Package rgl - Compilation Fails - Summary
In-Reply-To: <20070220121151.0e54f526@subarnarekha.stat.iastate.edu>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
	<1171918929.4041.22.camel@localhost.localdomain>
	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
	<1171982642.3451.5.camel@localhost.localdomain>
	<45DB0CCA.5090809@stats.uwo.ca>
	<1171990765.3451.12.camel@localhost.localdomain>
	<20070220121151.0e54f526@subarnarekha.stat.iastate.edu>
Message-ID: <Pine.LNX.4.64.0702201828560.19530@gannet.stats.ox.ac.uk>

On Tue, 20 Feb 2007, Ranjan Maitra wrote:

> Hi Duncan,
>
> I don't know if this will list all the dependencies for your 
> documentation, since Rick's error messages did not involve libraries and 
> header files already installed by him for something else, perhaps.

It will not, and most of this is already in the README.  The problem is 
the penchant for linux distros to break standard pieces of software into 
ever smaller pieces, so naming the pieces is often no great help 6 months 
later.

On  FC5/6 I think

yum install mesa-libGL-devel mesa-libGLU-devel libXext-devel libpng-devel

should do it.  (Note that mesa-libGL-devel appears to have been already 
there, and libXext-devel pulls in the rest of the X11 stuff.)

A quick check shows -lXext is actually not needed, so it could be removed 
from configure.ac and the requirements: that would make a lot more sense.

>
> Just a thought.
>
> Ranjan
>
>
>
> On Tue, 20 Feb 2007 11:59:24 -0500 Rick Bilonick <rab45+ at pitt.edu> wrote:
>
>> Summarizing:
>>
>> I'm running R 2.4.1 on a current FC6 32-bit system. In order to have the
>> rgl R package install, I needed to install both mesa-libGLU-devel (FC6
>> version is 6.5.1-9) and libXext-devel (FC6) rpm packages. Thanks to
>> everyone who commented.
>>
>> Rick B.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From JAROSLAW.W.TUSZYNSKI at saic.com  Tue Feb 20 20:15:16 2007
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 20 Feb 2007 14:15:16 -0500
Subject: [R] baseline fitters
In-Reply-To: <B1614B0C915A654A9C29BB71DA80E0DD1E9650@MAIL2.ad.uams.edu>
Message-ID: <707E057DF6451B41A7CECB80892D3C690508FD8C@0015-its-exmb03.us.saic.com>

I am not surprised at slowness of runquantile, since it is trying to
perform n=4500 partial sorts of k=225 elements. Here are some thoughts
at speeding it up:
1) playing with different endrule settings can save some time, but
usually results with undesirable effects at first and last 112 values.
All rum* functions in caTools use low level C code for inner elements
between k/2 and n-k/2. However the elements at the edge are calculated
using R functions. In case of runquantile with endrule="func" that means
k calls of R quantile function. One option for endrule not available at
present would be to pad both sides of the array with k/2 numbers and
than use endrule="trim". The trick would be to pick good value for the
padding number.

2) you mentioned that you "jimmied something together
with runmin and runmedian". I would try something like runmean with
window of size 5, 15, 25 and than runmin with window size k. The first
one should get rid of your 'reverse-spikes' and second would take
running min of your smoothed function.

Best,
Jarek Tuszynski

-----Original Message-----
From: Thaden, John J [mailto:ThadenJohnJ at uams.edu] 
Sent: Tuesday, February 20, 2007 1:23 PM
To: r-help at stat.math.ethz.ch
Cc: Tuszynski, Jaroslaw W.
Subject: baseline fitters

I am pretty pleased with baselines I fit to chromatograms using the
runquantile() function in caTools(v1.6) when its probs parameter is 
set to 0.2 and its k parameter to ~1/20th of n (e.g., k ~ 225 for n ~ 
4500, where n is time series length).  This ignores occasional low-
side outliers, and, after baseline subtraction, I can re-adjust any
negative values to zero.
  
But runquantile's computation time proves exceedingly long for my large
datasets, particularly if I set the endrule parameter to 'func'.  Here
is
what caTools author Jarek Tuszynski says about relative speeds of
various
running-window functions:

   - runmin, runmax, runmean run at O(n) 
   - runmean(..., alg="exact") can have worst case speed of O(n^2) for 
     some small data vectors, but average case is still close to O(n). 
   - runquantile and runmad run at O(n*k) 
   - runmed - related R function runs at O(n*log(k))

The obvious alternative runmin() performs poorly due to dropout (zero-
and low-value 'reverse-spikes') in the data. And runmed fits a baseline
that,
upon subtraction, obviously will send half the values into the negative,
not
suitable for my application. I jimmied something together
with runmin and runmedian that is considerably faster; unfortunately,
the fit seems less good, at least by eye, due still to the bad runmin
behavior.

I'd be interested in other baseline fitting suggestions implemented
or implementable in R (I'm using v. 2.4.1pat under WinXP).  Why, for
instance, can I not find a running trimmean function? Because it 
offers no computational savings over runquantile?

Also, the 'func' setting for the caTools endrule parameter--which
adjusts the
value of k as ends are approached--is said not to be optimized (using
this option does add further time to computations).  Is there an alter-
native that would be speedier, e.g., setting endrule = "keep" and then
subsequently treating ends with R's smoothEnds() function?

-John Thaden
Little Rock, Arkansas USA

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From Bartjoosen at hotmail.com  Tue Feb 20 21:06:42 2007
From: Bartjoosen at hotmail.com (Bart Joosen)
Date: Tue, 20 Feb 2007 21:06:42 +0100
Subject: [R] printing intermediate lines while in a function
References: <200702182104.l1IL4LI3021567@weisner.math.unb.ca>
Message-ID: <BAY134-DAV9B375DCE5FF3743EC8C12D8890@phx.gbl>

Thanks for al the tips, it was the readline() function who did the trick for 
me, so thanks for all of your input

Kind Regards

Bart


 Original message:
>
>> <snip>
>>
>> But unfortunately R will do first the calculations and then
>> afterwards return the strings.
>>
>> Is there a way around?
>


From ripley at stats.ox.ac.uk  Tue Feb 20 21:07:35 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Feb 2007 20:07:35 +0000 (GMT)
Subject: [R] contextstack overflow
In-Reply-To: <BAY101-F75335D701D95BB3DC2B4CEF890@phx.gbl>
References: <BAY101-F75335D701D95BB3DC2B4CEF890@phx.gbl>
Message-ID: <Pine.LNX.4.64.0702201944470.30455@gannet.stats.ox.ac.uk>

On Tue, 20 Feb 2007, Steven Finch wrote:

> Hello!
>
> I written several implementations in R of R?my's algorithm for
> generating random ordered strictly binary trees on 2n+1 vertices.
>
> One implementation involves manufacturing character strings like:
>
> "X <- list(0,list(0,0))"
>
> for the case n=2.  If I perform the following two steps:
>
> cmd <- "X <- list(0,list(0,0))"
> eval(parse(text=cmd))
>
> then X becomes a true nested list in R.  This works fine for n=2,
> but often for n=200, an error message:
>
> Error in parse(text = cmd) : contextstack overflow
>
> appears and execution stops.  Clearly there exists an upper bound
> on the allowable depth of nestings in R!  Can this upper bound be
> easily increased?

It is hardcoded (as 50) in 6 places in gram.c, so fairly easily changed.

> Other implementations avoid this problem, so this issue is not
> crucial to me.  I do wish, however, to understand the limits of
> this particular approach.  Thank you!
>
> Steve Finch
> http://algo.inria.fr/bsolve/
>
> P.S.  If anyone else has written R code for generating random
> trees (on a fixed number of vertices), I would enjoy seeing this!
>
> _________________________________________________________________
> Don?t miss your chance to WIN 10 hours of private jet travel from Microsoft? 
> Office Live http://clk.atdmt.com/MRT/go/mcrssaub0540002499mrt/direct/01/
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From marc_schwartz at comcast.net  Tue Feb 20 21:37:22 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 20 Feb 2007 14:37:22 -0600
Subject: [R] linux gplots install unhappy
In-Reply-To: <Pine.LNX.4.64.0702201857060.19530@gannet.stats.ox.ac.uk>
References: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
	<1171996074.4804.13.camel@localhost.localdomain>
	<Pine.LNX.4.64.0702201857060.19530@gannet.stats.ox.ac.uk>
Message-ID: <1172003842.4804.15.camel@localhost.localdomain>

On Tue, 2007-02-20 at 18:59 +0000, Prof Brian Ripley wrote:
> On Tue, 20 Feb 2007, Marc Schwartz wrote:
> 
> > On Tue, 2007-02-20 at 10:17 -0800, Randy Zelick wrote:
> >> Hello all,
> >>
> >> I use R on both windows and a "mainframe" linux installation (RedHat
> >> enterprise 3.0, which they tell me is soon to be upgraded to 4.0). On
> >> windows I installed the package gplots without trouble, and it works fine.
> >> When I attempted to do the same on the unix computer, the following error
> >> message was forthcoming:
> >>
> >>
> >>
> >>
> >> downloaded 216Kb
> >>
> >> * Installing *source* package 'gplots' ...
> >> ** R
> >> ** data
> >> ** inst
> >> ** preparing package for lazy loading
> >> Loading required package: gtools
> >> Warning in library(pkg, character.only = TRUE, logical = TRUE, lib.loc =
> >> lib.loc) :
> >>           there is no package called 'gtools'
> >> Error: package 'gtools' could not be loaded
> >> Execution halted
> >> ERROR: lazy loading failed for package 'gplots'
> >> ** Removing '/n/fs/disk/resuser02/u/zelickr/R/library/gplots'
> >>
> >> The downloaded packages are in
> >>          /tmp/RtmpikM2JW/downloaded_packages
> >> Warning messages:
> >> 1: installation of package 'gplots' had non-zero exit status in:
> >> install.packages("gplots", lib = "~/R/library")
> >> 2: cannot create HTML package index in:
> >> tools:::unix.packages.html(.Library)
> >>
> >>
> >>
> >> Can someone provide the bit of information I need to progress with this?
> >>
> >> Thanks very much,
> >>
> >> =Randy=
> >
> > gplots has a dependency on other packages (gtools and gdata).
> >
> > Thus, when you install it use:
> >
> >  install.packages("gplots", dependencies = TRUE, ...)
> >
> > That will download and install the other packages as well.
> >
> > There should have been a similar requirement under Windows, so not sure
> > what may have been different there, unless there is some confounding due
> 
> You don't need gtools to install a binary package, just to use it!
> I suspect the menu was used on Windows, where dependencies = TRUE is the 
> default.
> 
> Come 2.5.0 this sort of query will go away, as the essential dependencies 
> become the default on all platforms.

Thanks for the clarifications.

Regards,

Marc


From herrdittmann at yahoo.co.uk  Tue Feb 20 21:44:38 2007
From: herrdittmann at yahoo.co.uk (Bernd Dittmann)
Date: Tue, 20 Feb 2007 20:44:38 +0000
Subject: [R] Calculating the Sharpe ratio
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A8344015214D5@NYWEXMB23.msad.ms.com>
References: <D3AEEDA31E57474B840BEBC25A8A8344015214D5@NYWEXMB23.msad.ms.com>
Message-ID: <45DB5DB6.6020508@yahoo.co.uk>

Hi Mark,

thanks for your email.
I used your formula for cumul. returns and plugged them into sharpe:

 > mysharpe <- function(x){
+ return(sharpe(cret(x), r=0, scale=1))
+ }

whereby "cret" is my cumul. returns function as defined by:

 > cret
function(x){
cumprod(diff(log(x))+1)-1
}

For the index series "Index" I obtain a sharpe ratio (r=0 and scale=1) of:

 > mysharpe(Index)
[1] 0.8836429

Do you reckon this result and the method above are correct?

Many thanks in advance!

Bernd




Leeds, Mark (IED) schrieb:
> If the doc says to use cumulated and you didn't, then I supsect the call
> to shaprp in
> Tseries is not correct.  Also, to get PercetnREturns, I hope you did
> diff(log(series))
> Where series is an object containing prices. It's not so clear
> Form your email.
>
> If you want to send in cumulative returns ( which
> You should do if the doc says to ) you just take the returns ( by doing
> above )
> and then , add 1 to each element, do  a cumprod and then subtract 1 so
> something like :
>
> rtns<-diff(log(priceseries)
> oneplusrtns<-1+rtns
> cumprodrtns<-cumprod(oneplusreturns)
>
> cumrtns<-cumprodrtns-1.
>
> Then, the elements in cumrtns represent the cumulative reeturn upto that
> point.
>
> But, test it out with an easy example to make sure because I didn't.
>
>
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bernd Dittmann
> Sent: Monday, February 19, 2007 8:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Calculating the Sharpe ratio
>
> Hi useRs,
>
> I am trying to calculate the Sharpe ratio with "sharpe" of the library
> "tseries".
>
> The documentation requires the univariate time series to be a
> portfolio's cumulated returns. In this case, the example given
>
> data(EuStockMarkets)
> dax <- log(EuStockMarkets[,"FTSE"])
>
> is however not the cumulated returns but rather the daily returns of the
> FTSE stock index.
>
> Is this way of calculating the Sharpe ratio correct?
>
> Here are my own data:
>
> year    Index    PercentReturns
> 1985    117    0.091
> 1986    129.9    0.11
> 1987    149.9    0.154
> 1988    184.8    0.233
> 1989    223.1    0.208
> 1990    223.2    0
> 1991    220.5    -0.012
> 1992    208.1    -0.056
> 1993    202.1    -0.029
> 1994    203.1    0.005
> 1995    199.6    -0.017
> 1996    208.6    0.045
> 1997    221.7    0.063
> 1998    233.7    0.054
> 1999    250.5    0.072
> 2000    275.1    0.098
> 2001    298.6    0.085
> 2002    350.6    0.174
> 2003    429.1    0.224
> 2004    507.6    0.183
> 2005    536.6    0.057
> 2006    581.3    0.083
>
>
> I calculated the Sharpe ratio in two different ways:
> (1) using natural logs as approximation of % returns, using "sharpe" of
> "tseries".
> (2) using the % returns using a variation the "sharpe" function.
>
> In both cases I used the risk free rate r=0 and scale=1 since I am using
> annual data already.
>
> My results:
>
> METHOD 1: "sharpe":
>
>  > index <- log(Index)
>  > sharpe(index, scale=1)
> [1] 0.9614212
>
>
>
> METHOD 2: my own %-based formula:
>
>  > mysharp
> function(x, r=0, scale=sqrt(250))
> {
> if (NCOL(x) > 1)
> stop("x is not a vector or univariate time series") if (any(is.na(x)))
> stop("NAs in x") if (NROW(x) ==1)
> return(NA)
> else{
> return(scale * (mean(x) - r)/sd(x))
> }
> }
>
>
>
>  > mysharp(PercentReturns, scale=1)
> [1] 0.982531
>
>
> Both Sharp ratios differ only slightly since logs approximate percentage
> changes (returns).
>
>
> Are both methods correct, esp. since I am NOT using cumulated returns as
>
> the manual says?
>
> If cumulated returns were supposed to be used, could I cumulate the 
> %-returns with "cumsum(PercentReturns)"?
>
> Many thanks in advance!
>
> Bernd
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are market counterparties or intermediate customers (as defined in the UK Financial Services Authority's rules).
>
>


From rhurlin at gwdg.de  Tue Feb 20 22:08:37 2007
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Tue, 20 Feb 2007 22:08:37 +0100
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <45DB3B79.30004@stats.uwo.ca>
References: <1171913730.4041.7.camel@localhost.localdomain>	<45DA00EF.2050305@ebi.ac.uk>	<1171918929.4041.22.camel@localhost.localdomain>	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>	<1171982642.3451.5.camel@localhost.localdomain>	<45DB0CCA.5090809@stats.uwo.ca>	<1171990765.3451.12.camel@localhost.localdomain>	<20070220121151.0e54f526@subarnarekha.stat.iastate.edu>
	<45DB3B79.30004@stats.uwo.ca>
Message-ID: <45DB6355.6050402@gwdg.de>

Duncan Murdoch schrieb:
> On 2/20/2007 1:11 PM, Ranjan Maitra wrote:
>> Hi Duncan,
>> I don't know if this will list all the dependencies for your documentation, since Rick's error messages did not involve libraries and header files already installed by him for something else, perhaps.
> 
> No, but it's a start:  I'm thinking of something more like a FAQ than 
> comprehensive documentation.  Comprehensive docs are not feasible to 
> maintain (there are so many systems that Daniel and I don't use), but 
> hints that two non-standard packages solved one person's problems might 
> be enough of a hint to get someone else going.

Duncan,

thank you for this purpose. I am such a person who could need some help 
with installing rgl and hope it is ok to place it in this thread.

My trial to install rgl_0.70.tar.gz on R-2.4.1 on FreeBSD 7.0-CURRENT 
ends up with errors. Obiously something is wrong with Makevars. I have 
no idea what to do next. 'rgl' is one of the rare cases that do not 
install under R on FreeBSD.

The install messages are short:

-----
#R CMD INSTALL rgl_0.70.tar.gz
* Installing to library '/usr/local/lib/R/library'
* Installing *source* package 'rgl' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for X... libraries /usr/X11R6/lib, headers /usr/X11R6/include
checking for libpng-config... yes
configure: using libpng-config
configure: using libpng dynamic linkage
configure: creating ./config.status
config.status: creating src/Makevars
** libs
"Makevars", line 9: Need an operator
"Makevars", line 12: Need an operator
"Makevars", line 15: Need an operator
"Makevars", line 21: Need an operator
"Makevars", line 23: Need an operator
"Makevars", line 36: Need an operator
"Makevars", line 38: Need an operator
make: fatal errors encountered -- cannot continue
chmod: /usr/local/lib/R/library/rgl/libs/*: No such file or directory
ERROR: compilation failed for package 'rgl'
** Removing '/usr/local/lib/R/library/rgl'
-----

Are there any experiences with installing rgl on FreeBSD? Do you know if 
it is at all possible to get rgl to work on FreeBSD? If I can do 
anything like testing or giving more information let me know.

I appreciate any help. Thank you in advance.

Rainer Hurling


From ripley at stats.ox.ac.uk  Tue Feb 20 22:45:55 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Feb 2007 21:45:55 +0000 (GMT)
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <45DB6355.6050402@gwdg.de>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
	<1171918929.4041.22.camel@localhost.localdomain>
	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
	<1171982642.3451.5.camel@localhost.localdomain>
	<45DB0CCA.5090809@stats.uwo.ca>
	<1171990765.3451.12.camel@localhost.localdomain>
	<20070220121151.0e54f526@subarnarekha.stat.iastate.edu>
	<45DB3B79.30004@stats.uwo.ca> <45DB6355.6050402@gwdg.de>
Message-ID: <Pine.LNX.4.64.0702202141580.13327@auk.stats>

The problem is that rgl is apparently written for GNU make, and has (as 
shipped)

ifdef MAKINGAGL
PKG_CPPFLAGS=@AGLCPPFLAGS@ -Iext
PKG_LIBS=@AGLLIBS@
else
PKG_CPPFLAGS= -If:/R/R-2.4.1/src/extra/zlib -DHAVE_PNG_H 
-If:/R/R-2.4.1/src/gnuwin32/bitmap/libpng  -Iext
PKG_LIBS=-lgdi32 -lopengl32 -lglu32 
-Lf:/R/R-2.4.1/src/gnuwin32/bitmap/libpng -
lpng -Lf:/R/R-2.4.1/src/extra/zlib -lz
endif

and similar for BUILDAGL.

That seems to have been written to make it workable on MacOS X. Given that 
configure knows (or could know) the OS, it seems better to write (via 
configure) a separate Makevars for MacOS X and remove all the 
ifdef...endif stuff for everyone else.  (If you do that in Makevars.in it 
should work.)


On Tue, 20 Feb 2007, Rainer Hurling wrote:

> Duncan Murdoch schrieb:
>> On 2/20/2007 1:11 PM, Ranjan Maitra wrote:
>>> Hi Duncan,
>>> I don't know if this will list all the dependencies for your documentation, since Rick's error messages did not involve libraries and header files already installed by him for something else, perhaps.
>>
>> No, but it's a start:  I'm thinking of something more like a FAQ than
>> comprehensive documentation.  Comprehensive docs are not feasible to
>> maintain (there are so many systems that Daniel and I don't use), but
>> hints that two non-standard packages solved one person's problems might
>> be enough of a hint to get someone else going.
>
> Duncan,
>
> thank you for this purpose. I am such a person who could need some help
> with installing rgl and hope it is ok to place it in this thread.
>
> My trial to install rgl_0.70.tar.gz on R-2.4.1 on FreeBSD 7.0-CURRENT
> ends up with errors. Obiously something is wrong with Makevars. I have
> no idea what to do next. 'rgl' is one of the rare cases that do not
> install under R on FreeBSD.
>
> The install messages are short:
>
> -----
> #R CMD INSTALL rgl_0.70.tar.gz
> * Installing to library '/usr/local/lib/R/library'
> * Installing *source* package 'rgl' ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for X... libraries /usr/X11R6/lib, headers /usr/X11R6/include
> checking for libpng-config... yes
> configure: using libpng-config
> configure: using libpng dynamic linkage
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> "Makevars", line 9: Need an operator
> "Makevars", line 12: Need an operator
> "Makevars", line 15: Need an operator
> "Makevars", line 21: Need an operator
> "Makevars", line 23: Need an operator
> "Makevars", line 36: Need an operator
> "Makevars", line 38: Need an operator
> make: fatal errors encountered -- cannot continue
> chmod: /usr/local/lib/R/library/rgl/libs/*: No such file or directory
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/local/lib/R/library/rgl'
> -----
>
> Are there any experiences with installing rgl on FreeBSD? Do you know if
> it is at all possible to get rgl to work on FreeBSD? If I can do
> anything like testing or giving more information let me know.
>
> I appreciate any help. Thank you in advance.
>
> Rainer Hurling
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ablukacz at utm.utoronto.ca  Tue Feb 20 23:15:02 2007
From: ablukacz at utm.utoronto.ca (ablukacz)
Date: Tue, 20 Feb 2007 17:15:02 -0500 (EST)
Subject: [R]  analysis of correlation matrices
Message-ID: <Pine.LNX.4.64.0702201712110.5804@river.utm.utoronto.ca>

Hello,

I'm looking for a package in R that performs, analysis of correlation 
matrices: cross-classified by 2 factors.

The orginal reference to this method is by CJ Brien Biometrica (1998) 
75(3):469-76.

THank you,

Agnes


-------------------------------------
E. Agnes Richards, Ph.D. Candidate
Department of Zoology
University of Toronto at Mississauga
3359 Mississauga Rd. North
Mississauga, Ont.
Canada
L5L 1C6

lab 905-828-5452
fax 905-828-3792


From A.Robinson at ms.unimelb.edu.au  Wed Feb 21 00:43:28 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 21 Feb 2007 10:43:28 +1100
Subject: [R] Simplification of Generalised Linear mixed effects models
	using glmmPQL
Message-ID: <20070220234328.GP24437@ms.unimelb.edu.au>

Hello Tom,

the problem is because R has assumed that pop and rep are integers,
not factor levels.  Try:

test <- read.table("test.txt",header=T)

sapply(test, class)

test$pop <- factor(test$pop)
test$rep <- factor(test$rep)

then try fitting the models.  Also, there has been substantial
discussion about the production of p-values for mixed-effects models
in R; it's now a FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-are-p_002dvalues-not-displayed-when-using-lmer_0028_0029_003f

The package writer (Professor Bates) recommends the use of mcmcsamp to
obtain inferential information about your model - see

?mcmcsamp

in the lme4 package for examples of its use.

Cheers,

Andrew

ps it's a bad idea to call things "data" :)

On Tue, Feb 20, 2007 at 01:13:25PM +0000, T.C. Cameron wrote:
> Dear R users I have built several glmm models using glmmPQL in the
> following structure:
>  
> m1<-glmmPQL(dev~env*har*treat+dens, random = ~1|pop/rep,  family =
> Gamma)
>  
> (full script below, data attached)
>  
> I have tried all the methods I can find to obtain some sort of model fit
> score or to compare between models using following the deletion of terms
> (i.e. AIC, logLik, anova.lme(m1,m2)), but I cannot get any of them to
> work.
> Yet I see on several R help pages that others have with similar models?
>  
> I have tried the functions in lme4 as well and lmer or lmer2 will not
> accept my random terms of "rep" (replicate) nested within "pop"
> population.
>  
> I have read the appropriate sections of the available books and R help
> pages but I am at a loss of where to move from here
>  
>  
> 
> data<-read.table("D:\\bgytcc\\MITES\\Data\\Analysis\\test.txt",header=T)
> attach(data)
> names(data)
>  
>  
>  
> m1<-glmmPQL(dev~env*har*treat+dens, random = ~1|pop/rep, family = Gamma)
> summary(m1)
> anova.lme(m1)
> m2<-update(m1,~.-env:har:treat)
> anova.lme(m1,m2)###this does not work
> AIC(m1)##this does not work
> logLik(m1)##this does not work?
>  
>  
>  
> ##################this does not work
> class(m1) <- "lme"
> class(m2) <- "lme"
> anova.lme(m1,m2)
> #################################
>  
> m3<-lmer(dev~env*har*treat+dens + (1|pop/rep), family = Gamma)
>  
> ## this generates an error
> Error in lmerFactorList(formula, mf, fltype) : 
>         number of levels in grouping factor(s) 'rep:pop', 'pop' is too
> large
> In addition: Warning messages:
> 1: numerical expression has 1851 elements: only the first used in:
> rep:pop 
> 2: numerical expression has 1851 elements: only the first used in:
> rep:pop 
>  
> 
> m4<-lmer(dev~env*har*treat + dens + (1|rep) +(1|pop), family = Gamma,
> method = "Laplace")
> ## this works but it does not give me an anova output with p values
> anova(m4)
> Analysis of Variance Table
>               Df  Sum Sq Mean Sq
> env            1   17858   17858
> har            2     879     439
> treat          2    2613    1306
> dens           1 1016476 1016476
> env:har        2     870     435
> env:treat      2    1188     594
> har:treat      4     313      78
> env:har:treat  4    1188     297
> 
>  
> 
> ........................................................................
> ............
> Dr Tom C Cameron
> Genetics, Ecology and Evolution
> IICB, University of Leeds
> Leeds, UK
> Office: +44 (0)113 343 2837
> Lab:    +44 (0)113 343 2854
> Fax:    +44 (0)113 343 2835
> 
> 
> Email: t.c.cameron at leeds.ac.uk
> Webpage: click here
> <http://www.fbs.leeds.ac.uk/staff/profile.php?tag=Cameron_TC> 
> 
>  


-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From murdoch at stats.uwo.ca  Wed Feb 21 01:47:40 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 20 Feb 2007 19:47:40 -0500
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <Pine.LNX.4.64.0702202141580.13327@auk.stats>
References: <1171913730.4041.7.camel@localhost.localdomain>	<45DA00EF.2050305@ebi.ac.uk>	<1171918929.4041.22.camel@localhost.localdomain>	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>	<1171982642.3451.5.camel@localhost.localdomain>	<45DB0CCA.5090809@stats.uwo.ca>	<1171990765.3451.12.camel@localhost.localdomain>	<20070220121151.0e54f526@subarnarekha.stat.iastate.edu>	<45DB3B79.30004@stats.uwo.ca>
	<45DB6355.6050402@gwdg.de>
	<Pine.LNX.4.64.0702202141580.13327@auk.stats>
Message-ID: <45DB96AC.9030000@stats.uwo.ca>

On 2/20/2007 4:45 PM, Prof Brian Ripley wrote:
> The problem is that rgl is apparently written for GNU make, and has (as 
> shipped)
> 
> ifdef MAKINGAGL
> PKG_CPPFLAGS=@AGLCPPFLAGS@ -Iext
> PKG_LIBS=@AGLLIBS@
> else
> PKG_CPPFLAGS= -If:/R/R-2.4.1/src/extra/zlib -DHAVE_PNG_H 
> -If:/R/R-2.4.1/src/gnuwin32/bitmap/libpng  -Iext
> PKG_LIBS=-lgdi32 -lopengl32 -lglu32 
> -Lf:/R/R-2.4.1/src/gnuwin32/bitmap/libpng -
> lpng -Lf:/R/R-2.4.1/src/extra/zlib -lz
> endif
> 
> and similar for BUILDAGL.
> 
> That seems to have been written to make it workable on MacOS X. Given that 
> configure knows (or could know) the OS, it seems better to write (via 
> configure) a separate Makevars for MacOS X and remove all the 
> ifdef...endif stuff for everyone else.  (If you do that in Makevars.in it 
> should work.)

On MacOS X we currently build two libraries, one for X11 and one for 
AGL, and these tests choose between those two targets (which need 
different compilation of a few files, and separate linking of all files 
against different libraries).

Fixing this to work more portably isn't something that I know how to do. 
   If someone wants to fix it and send me a patch I'll incorporate it, 
but otherwise it likely won't get fixed.

By the way, the file you quote is Makevars, which shouldn't have been 
shipped:  it gets produced from Makevars.in.  I'll have to be careful to 
do the packaging from a clean checkout next time.

Duncan Murdoch


From RMan54 at cox.net  Wed Feb 21 05:52:29 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Tue, 20 Feb 2007 20:52:29 -0800
Subject: [R] Different gridlines per panel in xyplot
Message-ID: <00e601c75574$1728ddf0$0900a8c0@rman>

In the example R script below, horizontal gray gridlines are drawn at y
coordinates where the points are drawn with the code:
 
panel.abline(h=y, v=xScale, col.line="gray")

How do I change this so that the horizontal gray gridlines are drawn at y
coordinates where the y labels are drawn? The challenge is that each panel
has different y-ranges (in my real example the y-ranges and y-intervals are
even more different). For example, I wish I could use the yScale list as the
h parameter in abline, but it does not work with a list.
 
Thanks for any help.
Rene
 
library(lattice)
Subj <- rep(1:4,each=3)
Time <- rep(1:3,4) + 0.1
Conc <- (1:12) + 0.1
df <- data.frame(Subj,Time,Conc)
xScale <- 1:3
yScale <- list(1:3,4:6,7:9,10:12)
xyplot(Conc ~ Time | Subj,
       data = df,
       layout = c(2,2),
       type="b",
       scales=list(
          x=list(at=xScale),
          y=list(at=yScale,relation="free")
       ),
       panel = function(x,y,...) {
           panel.abline(h=y, v=xScale, col.line="gray")
           panel.xyplot(x,y,...)
      }
)


From bernd.weiss at uni-koeln.de  Wed Feb 21 06:44:27 2007
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Wed, 21 Feb 2007 06:44:27 +0100
Subject: [R] Different gridlines per panel in xyplot
In-Reply-To: <00e601c75574$1728ddf0$0900a8c0@rman>
References: <00e601c75574$1728ddf0$0900a8c0@rman>
Message-ID: <45DBEA4B.23191.388CDE@bernd.weiss.uni-koeln.de>

Am 20 Feb 2007 um 20:52 hat Rene Braeckman geschrieben:

From:           	"Rene Braeckman" <RMan54 at cox.net>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Tue, 20 Feb 2007 20:52:29 -0800
Subject:        	[R] Different gridlines per panel in xyplot

> In the example R script below, horizontal gray gridlines are drawn at
> y coordinates where the points are drawn with the code:
> 
> panel.abline(h=y, v=xScale, col.line="gray")
> 
> How do I change this so that the horizontal gray gridlines are drawn
> at y coordinates where the y labels are drawn? The challenge is that
> each panel has different y-ranges (in my real example the y-ranges and
> y-intervals are even more different). For example, I wish I could use
> the yScale list as the h parameter in abline, but it does not work
> with a list.
> 
> Thanks for any help.
> Rene
> 
> library(lattice)
> Subj <- rep(1:4,each=3)
> Time <- rep(1:3,4) + 0.1
> Conc <- (1:12) + 0.1
> df <- data.frame(Subj,Time,Conc)
> xScale <- 1:3
> yScale <- list(1:3,4:6,7:9,10:12)
> xyplot(Conc ~ Time | Subj,
>        data = df,
>        layout = c(2,2),
>        type="b",
>        scales=list(
>           x=list(at=xScale),
>           y=list(at=yScale,relation="free")
>        ),
>        panel = function(x,y,...) {
>            panel.abline(h=y, v=xScale, col.line="gray")
>            panel.xyplot(x,y,...)
>       }
> )
> 

Dear Rene,

I am not quite sure whether this is the most elegant/general 
solution, but one option might be to use trunc(), see the full code 
below.

panel.abline(h=trunc(y), v=xScale, col.line="gray")


library(lattice)
Subj <- rep(1:4,each=3)
Time <- rep(1:3,4) + 0.1
Conc <- (1:12) + 0.1
df <- data.frame(Subj,Time,Conc)
xScale <- 1:3
yScale <- list(1:3,4:6,7:9,10:12)
xyplot(Conc ~ Time | Subj,
       data = df,
       layout = c(2,2),
       type="b",
       scales=list(
          x=list(at=xScale),
          y=list(at=yScale,relation="free")
       ),
       panel = function(x,y,...) {
		## use trunc(y)
           panel.abline(h=trunc(y), v=xScale, col.line="gray")
           panel.xyplot(x,y,...)
      }
)


HTH,

Bernd


From RMan54 at cox.net  Wed Feb 21 06:51:10 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Tue, 20 Feb 2007 21:51:10 -0800
Subject: [R] Different gridlines per panel in xyplot
In-Reply-To: <45DBEA4B.23191.388CDE@bernd.weiss.uni-koeln.de>
References: <00e601c75574$1728ddf0$0900a8c0@rman>
	<45DBEA4B.23191.388CDE@bernd.weiss.uni-koeln.de>
Message-ID: <00e701c7557c$49a256f0$0900a8c0@rman>

Thanks for the reply. Trunc would only work when the truncated y values
result in the desired y coordinates for the grid lines as in this simple
data set. Since my real dataset contains many more points, it does not give
a general solution.

Rene  

-----Original Message-----
From: Bernd Weiss [mailto:bernd.weiss at uni-koeln.de] 
Sent: Tuesday, February 20, 2007 9:44 PM
To: Rene Braeckman; r-help at stat.math.ethz.ch
Subject: Re: [R] Different gridlines per panel in xyplot

Am 20 Feb 2007 um 20:52 hat Rene Braeckman geschrieben:

From:           	"Rene Braeckman" <RMan54 at cox.net>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Tue, 20 Feb 2007 20:52:29 -0800
Subject:        	[R] Different gridlines per panel in xyplot

> In the example R script below, horizontal gray gridlines are drawn at 
> y coordinates where the points are drawn with the code:
> 
> panel.abline(h=y, v=xScale, col.line="gray")
> 
> How do I change this so that the horizontal gray gridlines are drawn 
> at y coordinates where the y labels are drawn? The challenge is that 
> each panel has different y-ranges (in my real example the y-ranges and 
> y-intervals are even more different). For example, I wish I could use 
> the yScale list as the h parameter in abline, but it does not work 
> with a list.
> 
> Thanks for any help.
> Rene
> 
> library(lattice)
> Subj <- rep(1:4,each=3)
> Time <- rep(1:3,4) + 0.1
> Conc <- (1:12) + 0.1
> df <- data.frame(Subj,Time,Conc)
> xScale <- 1:3
> yScale <- list(1:3,4:6,7:9,10:12)
> xyplot(Conc ~ Time | Subj,
>        data = df,
>        layout = c(2,2),
>        type="b",
>        scales=list(
>           x=list(at=xScale),
>           y=list(at=yScale,relation="free")
>        ),
>        panel = function(x,y,...) {
>            panel.abline(h=y, v=xScale, col.line="gray")
>            panel.xyplot(x,y,...)
>       }
> )
> 

Dear Rene,

I am not quite sure whether this is the most elegant/general solution, but
one option might be to use trunc(), see the full code below.

panel.abline(h=trunc(y), v=xScale, col.line="gray")


library(lattice)
Subj <- rep(1:4,each=3)
Time <- rep(1:3,4) + 0.1
Conc <- (1:12) + 0.1
df <- data.frame(Subj,Time,Conc)
xScale <- 1:3
yScale <- list(1:3,4:6,7:9,10:12)
xyplot(Conc ~ Time | Subj,
       data = df,
       layout = c(2,2),
       type="b",
       scales=list(
          x=list(at=xScale),
          y=list(at=yScale,relation="free")
       ),
       panel = function(x,y,...) {
		## use trunc(y)
           panel.abline(h=trunc(y), v=xScale, col.line="gray")
           panel.xyplot(x,y,...)
      }
)


HTH,

Bernd


From RMan54 at cox.net  Wed Feb 21 07:06:01 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Tue, 20 Feb 2007 22:06:01 -0800
Subject: [R] Different gridlines per panel in xyplot
In-Reply-To: <00e601c75574$1728ddf0$0900a8c0@rman>
References: <00e601c75574$1728ddf0$0900a8c0@rman>
Message-ID: <000101c7557e$5d85ab70$0900a8c0@rman>

I solved my own problem. Suddenly remembered the list of very useful
functions under "Accessing Auxiliary Information During Plotting" in the
help pages.

Here is the line that did the trick:

panel.abline(h=yScale[[panel.number()]], v=xScale, col.line="gray")

Rene 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rene Braeckman
Sent: Tuesday, February 20, 2007 8:52 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Different gridlines per panel in xyplot

In the example R script below, horizontal gray gridlines are drawn at y
coordinates where the points are drawn with the code:
 
panel.abline(h=y, v=xScale, col.line="gray")

How do I change this so that the horizontal gray gridlines are drawn at y
coordinates where the y labels are drawn? The challenge is that each panel
has different y-ranges (in my real example the y-ranges and y-intervals are
even more different). For example, I wish I could use the yScale list as the
h parameter in abline, but it does not work with a list.
 
Thanks for any help.
Rene
 
library(lattice)
Subj <- rep(1:4,each=3)
Time <- rep(1:3,4) + 0.1
Conc <- (1:12) + 0.1
df <- data.frame(Subj,Time,Conc)
xScale <- 1:3
yScale <- list(1:3,4:6,7:9,10:12)
xyplot(Conc ~ Time | Subj,
       data = df,
       layout = c(2,2),
       type="b",
       scales=list(
          x=list(at=xScale),
          y=list(at=yScale,relation="free")
       ),
       panel = function(x,y,...) {
           panel.abline(h=y, v=xScale, col.line="gray")
           panel.xyplot(x,y,...)
      }
)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From samay.sar at gmail.com  Wed Feb 21 07:15:19 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Wed, 21 Feb 2007 11:45:19 +0530
Subject: [R] Splom plot:how to plot with different symbols?
Message-ID: <d4327f7e0702202215v2117c69eqc53ec9e9a8701012@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/f2df0974/attachment.pl 

From seniorr at aracnet.com  Wed Feb 21 09:51:15 2007
From: seniorr at aracnet.com (Russell Senior)
Date: 21 Feb 2007 00:51:15 -0800
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WGS84)
Message-ID: <86wt2bsxks.fsf@coulee.tdb.com>


I am interested in making a random sample from a uniform distribution
of points over the surface of the earth, using the WGS84 ellipsoid as
a model for the earth.  I know how to do this for a sphere, but would
like to do better.  I can supply random numbers, want latitude
longitude pairs out.

Can anyone point me at a solution?  Thanks very much.


-- 
Russell Senior         ``I have nine fingers; you have ten.''
seniorr at aracnet.com


From florent.baty at unibas.ch  Wed Feb 21 11:18:23 2007
From: florent.baty at unibas.ch (Florent Baty)
Date: Wed, 21 Feb 2007 11:18:23 +0100
Subject: [R] Coxph and ordered factors
Message-ID: <45DC1C6F.5000704@unibas.ch>

Dear useRs,

I am trying to fit a Cox PH model on survival data from a lung cancer 
dataset. I would like to include the patient staging (I-IV) as a 
covariate. For this I use the following function:
coxph(Surv(time,status) ~ stage)
The staging information is a categorical variable, and it is important 
to take the ordering into account (I<II<III<IV). Does the coxph function 
handle correctly ordered factors, or is it preferable to recode the 
covariate into numerical values (1-4)?

Thank you very much for your help,

Florent

-- 
--------------------------------------------------
		Dr Florent BATY
Pulmonary Gene Research, Universit?tsspital Basel
   Petersgraben 4, CH-4031 Basel, Switzerland
 tel: +41 61 265 57 27 - fax: +41 61 265 45 87


From pratap_stat at yahoo.co.in  Wed Feb 21 11:28:27 2007
From: pratap_stat at yahoo.co.in (nalluri pratap)
Date: Wed, 21 Feb 2007 10:28:27 +0000 (GMT)
Subject: [R] reading text file not table
In-Reply-To: <XFMail.070217003941.ted.harding@nessie.mcc.ac.uk>
Message-ID: <870314.84447.qm@web8608.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/a4b289aa/attachment.pl 

From Michael.Dondrup at CeBiTec.Uni-Bielefeld.DE  Wed Feb 21 11:56:11 2007
From: Michael.Dondrup at CeBiTec.Uni-Bielefeld.DE (Michael Dondrup)
Date: Wed, 21 Feb 2007 11:56:11 +0100
Subject: [R] Confindence interval for Levenberg-Marquardt fit
Message-ID: <45DC254B.6080601@cebitec.uni-bielefeld.de>

Dear all,
I would like to use the Levenberg-Marquardt algorithm for non-linear 
least-squares regression using function nls.lm. Can anybody help  me to 
   find a a way to compute confidence intervals on the  fitted 
parameters as it is possible for nls (using confint.nls, which does not 
work for nls.lm)?

Thank you for your help
Michael


From x.sole at iconcologia.net  Wed Feb 21 12:08:16 2007
From: x.sole at iconcologia.net (Sole Acha, Xavi)
Date: Wed, 21 Feb 2007 12:08:16 +0100
Subject: [R] R unstable and crashes after executing .C
Message-ID: <5FF3F11444E3A9439191AA1EDCB69A17B861B6@icosrvmail01.ICO.SCS.local>

Dear R listers,

I have developed a C function to be executed from R through the ".C" interface. After doing dyn.load, the function executes properly and I get the results. However, after executing my function, R seems to get unstable and crashes (giving a segmentation fault and exiting) whenever I try to do ANYTHING with a relatively large object (creating a new one or even just writing the name of an existing one). 

I use R 2.4.0 under a Linux machine with 1 GB RAM. Below there is an example of execution, so you can get an idea of what is happening:

--------------------
dyn.load("my_C_module.so");
res <- .C("my_C_function",.....); #The function executes fine and res is ok
dyn.unload("my_C_module.so") #I know this isn't strictly necessary

#Here R is still running, but when I execute:

m <- matrix(0,1000,100); #I try to create a new object and R crashes

*** caught segfault ***
address 0x10, cause 'memory not mapped'

Traceback:
 1: matrix(0, 1000, 100)

Possible actions:
1: abort (with core dump)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace
--------------------

Although I tell R to abort and give me the core dump, it doesn't succeed in doing so.

I would be grateful if anyone could tell me what could be the problem with my C function that makes R behave this way?

Thank you very much in advance, and apologies for this long email.

Xavier Sol?.


From ripley at stats.ox.ac.uk  Wed Feb 21 12:09:52 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Feb 2007 11:09:52 +0000 (GMT)
Subject: [R] Confindence interval for Levenberg-Marquardt fit
In-Reply-To: <45DC254B.6080601@cebitec.uni-bielefeld.de>
References: <45DC254B.6080601@cebitec.uni-bielefeld.de>
Message-ID: <Pine.LNX.4.64.0702211104570.23374@gannet.stats.ox.ac.uk>

Well, the algorithm used does not affect the confidence interval (provided 
it works correctly), but what is nls.ml (presumably in some package you 
have not mentioned) and why would I want to use an old-fashioned 
algorithm?

You could start nls at the solution you got from nls.ml and use confint() 
on that.

On Wed, 21 Feb 2007, Michael Dondrup wrote:

> Dear all,
> I would like to use the Levenberg-Marquardt algorithm for non-linear
> least-squares regression using function nls.lm. Can anybody help  me to
>   find a a way to compute confidence intervals on the  fitted
> parameters as it is possible for nls (using confint.nls, which does not
> work for nls.lm)?
>
> Thank you for your help
> Michael


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From deepayan.sarkar at gmail.com  Wed Feb 21 12:15:49 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 21 Feb 2007 03:15:49 -0800
Subject: [R] Splom plot:how to plot with different symbols?
In-Reply-To: <d4327f7e0702202215v2117c69eqc53ec9e9a8701012@mail.gmail.com>
References: <d4327f7e0702202215v2117c69eqc53ec9e9a8701012@mail.gmail.com>
Message-ID: <eb555e660702210315p74a8732v5a1c7891dcce4d55@mail.gmail.com>

On 2/20/07, d. sarthi maheshwari <samay.sar at gmail.com> wrote:
> Hi,
>
> Kindly let me know if I posted a wrong question in the forum.
>
> I want to draw a splom plot with different symbols in plot. My command is as
> follows:

>
> splom(~ log10(splomData[2:3]), groups = programs, data = splomData,
> panel = panel.superpose,
> key = list(title = paste(splomLoop,"Programs of Hog Analysis (Sorted by
> LR(GB))"),
>            panel = panel.superpose,
>          columns = splomLoop,
>            points = list(pch = c(1:splomLoop), #this will display different
> symbols in legend but not in actual graph
>          col = rainbow(splomLoop, s=1, v=1),
>            text = trim(as.character(ProgramNameList[1:splomLoop,1])))))
>
> Kindly help.


I can't check since you haven't provided a reproducible example, but I
would suggest something along the lines of


splom(~ log10(splomData[2:3]), groups = programs, data = splomData,
      panel = panel.superpose,

      par.settings =
      list(superpose.symbol =
           list(pch = 1:splomLoop,
                col = rainbow(splomLoop, s=1, v=1))),

      auto.key =
      list(title = paste(splomLoop,"Programs of Hog Analysis (Sorted
by LR(GB))"),
           columns = splomLoop,
           text = trim(as.character(ProgramNameList[1:splomLoop,1]))))

-Deepayan



> Thanks & Regards
> Sarthi M.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From christian.kamenik at giub.unibe.ch  Wed Feb 21 12:16:32 2007
From: christian.kamenik at giub.unibe.ch (Christian Kamenik)
Date: Wed, 21 Feb 2007 12:16:32 +0100
Subject: [R] GLS models - bootstrapping
Message-ID: <45DC2A10.8080203@giub.unibe.ch>

Dear Lillian,

I tried to estimate parameters for time series regression using time 
series bootstrapping as described on page 434 in Davison & Hinkley 
(1997) - bootstrap methods and their application. This approach is based 
on an AR process (ARIMA model) with a regression term (compare also with 
page 414 in Venable & Ripley (2002) - modern applied statistics with S) 
I rewrote the code for R (this comes without any warranty):

fit <- function( data )
{ X <- cbind(rep(1,100),data$activ)
   para <- list( X=X,data=data)
   assign("para",para)
   d <- arima(x=para$data$temp,order=c(1,0,0),xreg=para$X)
   res <- d$residuals
   res <- res[!is.na(res)]
   list(paras=c(d$model$ar,d$reg.coef,sqrt(d$sigma2)),
        res=res-mean(res),fit=X %*% d$reg.coef)
}
beaver.args <- fit( beaver )
white.noise <- function( n.sim, ts) sample(ts,size=n.sim,replace=T)
beaver.gen <- function( ts, n.sim, ran.args )
{ tsb <- ran.args$res
   fit <- ran.args$fit
   coeff <- ran.args$paras
   ts$temp <- fit + coeff[4]*arima.sim( model=list(ar=coeff[1]),
                             n=n.sim,rand.gen=white.noise,ts=tsb )
   ts }
new.beaver <- beaver.gen( beaver, 100, beaver.args )

beaver.fun <- function(ts) fit(ts)$paras
beaver.boot <- tsboot( beaver, beaver.fun, R=99,sim="model",
                n.sim=100,ran.gen=beaver.gen,ran.args=beaver.args)
names(beaver.boot)
beaver.boot$t0
beaver.boot$t[1:10,]

Maybe there is a more elegant way for doing this. Anyway, boot.ci should 
give you confidence intervals.

Let me know how you are doing.

Best, Christian




> From: Lillian Sandeman <l.sandeman>
> Date: Mon, 2 Oct 2006 13:59:09 +0100 (BST)
> 
> Hello,
> 
> I am have fitted GLS models to time series data. Now I wish to bootstrap
> this data to produce confidence intervals for the model.
> 
> However, because this is time series data, normal bootstrapping is not
> applicable. Secondly, 'tsboot' appears to only be useful for ar models -
> and does not seem to be applicable to GLS models.
> 
> I have written code in R to randomly sample blocks of the data (as in
> Davison & Hinkley's book - bootstrap methods and their application) and
> use this resampling to re-run the model, but this does not seem to be the
> correct approach since Confidence Intervals produced do not show the
> underlying pattern (cycles) in the data [even when block length is
> increased, it only picks up a little of this variation].
> 
> Any help as to how to proceed with this would be greatly appreciated, as I
> cannot find anything applicable on the R pages. Alternatively, if there
> is another method to proceed with this (other than bootstrapping), I would
> also be happy to try it.
> 
> Thankyou,
> 
> Lillian.


From Michael.Dondrup at CeBiTec.Uni-Bielefeld.DE  Wed Feb 21 12:24:16 2007
From: Michael.Dondrup at CeBiTec.Uni-Bielefeld.DE (Michael Dondrup)
Date: Wed, 21 Feb 2007 12:24:16 +0100
Subject: [R] Confindence interval for Levenberg-Marquardt fit
In-Reply-To: <Pine.LNX.4.64.0702211104570.23374@gannet.stats.ox.ac.uk>
References: <45DC254B.6080601@cebitec.uni-bielefeld.de>
	<Pine.LNX.4.64.0702211104570.23374@gannet.stats.ox.ac.uk>
Message-ID: <45DC2BE0.4090308@cebitec.uni-bielefeld.de>

Thank you,
sorry, I forgot to mention that nls.lm is in package minpack.lm. It's 
use is motivated by the wish of a colleague to reproduce a result from 
some publication. But if I understand you correctly, use of the methods 
implemented in nls or optim is preferred?


Prof Brian Ripley wrote:
> Well, the algorithm used does not affect the confidence interval 
> (provided it works correctly), but what is nls.ml (presumably in some 
> package you have not mentioned) and why would I want to use an 
> old-fashioned algorithm?
> 
> You could start nls at the solution you got from nls.ml and use 
> confint() on that.
> 
> On Wed, 21 Feb 2007, Michael Dondrup wrote:
> 
>> Dear all,
>> I would like to use the Levenberg-Marquardt algorithm for non-linear
>> least-squares regression using function nls.lm. Can anybody help  me to
>>   find a a way to compute confidence intervals on the  fitted
>> parameters as it is possible for nls (using confint.nls, which does not
>> work for nls.lm)?
>>
>> Thank you for your help
>> Michael
> 
>


From d.levy at maternite.chu-nancy.fr  Wed Feb 21 12:26:40 2007
From: d.levy at maternite.chu-nancy.fr (David LEVY)
Date: Wed, 21 Feb 2007 12:26:40 +0100
Subject: [R] [ANTISPAM CHU_NANCY] Re: "contingency table" for several
	variables
In-Reply-To: <45DB0C3B.7020301@vanderbilt.edu>
Message-ID: <MDEDINGLMBCDOPICGDKOAEDNCAAA.d.levy@maternite.chu-nancy.fr>

Thank you for your replies,

But xtab(PrettyR) does something different, it computes several tables. Each
table corresponds to a different combination of the levels of the right hand
formula's variables. I only have 3 levels according to my ?classification
variable? . I want to gather in a single table the information of chi2 tests
for all the variables I  need to test with this classification variable.

> sapply(mydf, function(x) chisq.test(x,classif)$observed)

gives a part of my result : several tables in a single list.
First trick is to gather these tables in a single one and to keep the labels
of the variables and the formats of the levels at the same time. Second
trick is to add the p.value in a nice way.
Must say I didn?t find the tricks so far. I wrote a program but it fails
when meeting empty levels.

About the priority of the count on the percentage, unfortunately, the
decision is not mine.

Thanks
David


From lorenz.gygax at art.admin.ch  Wed Feb 21 12:33:17 2007
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Wed, 21 Feb 2007 12:33:17 +0100
Subject: [R] random effect nested within fixed effects (binomial lmer)
In-Reply-To: <02B8B9D9-B62E-42CE-8970-755AA2E72C46@ling.upenn.edu>
Message-ID: <145C63777EF3ED41A5A99035845F7DD9D311D8@EVD-C8002.bk.evdad.admin.ch>

> But I recently realized something. Most of the variables that I've  
> tested as fixed effects are properties of the subject (e.g. Race,  
> Gender, etc.). Is it correct to be using a random effect 
> Subject that is nested within (partially-crossed) fixed effects
> like Gender and Race?

Yes. I would even claim that it is necessary. Only if you use subject as a random effect, gender and race are correctly attributed as constant within individuals and are thus treated as 'between-subject' variables. (And thus, basically, the sample on which you can base your gender and race comparisons is the number of individuals).
 
> So today, I accidentally ran a model without the Subject random  
> effect, and the fixed effect of Race was significant for the first  
> time. With the Subject effect included, Race is not significant.

In my view, this is not surprising and can be called pseudo-replication. Every line of your data set is now treated as an independent measure even though the repetition of race and gender for the same individual is, of course, no new, indpendent information. (Here, you base your statistics on the number of observations instead of individuals.)

> This also happens if Race is treated as random, though the effect
> is smaller then.

I do not really see why you would want to do that.

> ... But if there is a real effect of Race, ...

Well, is there? If you conduct your analyses at the proper level, there obviously is no such effect (at least none that is supported statistically). It is of course possible, that there is a weak effect which you might pick up in a larger sample (more individuals tested).

Cheers, Lorenz
- 
Lorenz Gygax
Federal Veterinary Office
Centre for proper housing of ruminants and pigs
T?nikon, CH-8356 Ettenhausen / Switzerland


From Giovanni_Millo at Generali.com  Wed Feb 21 12:53:07 2007
From: Giovanni_Millo at Generali.com (Millo Giovanni)
Date: Wed, 21 Feb 2007 12:53:07 +0100
Subject: [R]  Problems with obtaining t-tests of regression
In-Reply-To: <mailman.7.1172055604.4397.r-help@stat.math.ethz.ch>
Message-ID: <7C95FD2FC68FBB45B9E9FDC1ECD49AF5F6E3A8@BEMAILEXTV03.corp.generali.net>

Guillermo,

I am dropping most of your mail because my answer is very generic.

First, why doesn't it work as you tried it: technically speaking,
coeftest() and the like expect to be feed an lm or a glm object and for
this reason won't accept the result of systemfit(), which is a much
different object. I suppose the same goes for the rest.

Second, what can you do: I'd do at least one step by hand.

a) as you have only one structural equation, maybe the easiest is to get
an lm object equivalent to the 2sls model you need, then apply
coeftest() and the like to this object. The two-step procedure outlined
in any textbook (e.g. Wooldridge, Econometrics of cross section and
panel data, MIT 2002, page 91) *should* produce a suitable object.
Please note: I cannot guarantee, though, that SEs are still appropriate:
see Wooldridge, bottom of page 91.

b) it could be safer to explicitly compute HC SEs by formula 5.34 in
Wooldridge, based on the 2sls residuals you got from systemfit().

c??) Maybe there's a shorter way: I suspect that the following could
work:

- regress Sc on the rest, get Sc_hat
- estimate step2model<-lm(lnP~Ag+Ag2+Var+R+D+Sc_hat)

and now I think coeftest(step2model,vcov=vcovHC) should compute exactly
formula (5.34) in Wooldridge, but *please check this out*, as it is only
an intuition!!

Best,
Giovanni

------------------------------

Message: 3
Date: Tue, 20 Feb 2007 14:00:56 +0100
From: " Guillermo Juli?n San Mart?n "
	<guillermojsanmartin at googlemail.com>
Subject: [R] Problems with obtaining t-tests of regression
	coefficients	applying consistent standard errors after run
2SLS
	estimation. Clearer !!!!!
To: r-help at stat.math.ethz.ch
Message-ID: <48a5ea80702200500tc57d76r67d48d918af57244 at mail.gmail.com>
Content-Type: text/plain

First I have to say I am sorry because I have not been so clear in my
previous e-mails. I will try to explain clearer what it is my problem.

I have the following model:



lnP=Sc+Ag+Ag2+Var+R+D



In this model the variable Sc is endogenous and the rest are all
objective exogenous variables. I verified that Sc is endogenous through
a standard Hausman test. To determine this I defined before a new
instrumental variable, I2. Also I detected through a Breusch Pagan Test
a problem of heteroskedasticity.

With the intention to avoid the problem of the endogenous variable and
the heteroskedasticity I want to apply first the technique 2SLS and then
based in these results I want to obtain the t-tests of the coefficients
applying Heteroskedasticity Consistent Standard Errors (HCSE) or
Huber-White errors.

Like I showed above I have just one structural equation in the model. In
this situation, to apply 2SLS in R until I know there two possible ways:
First to use the function tsls() from package sem, or second, to use the
function systemfit() from package systemfit. I thought that systemfit
was for situations when there are more than one structural equation in
the model. Anyway I probed with the two ways and I obtained similar
results. Below, I show the program lines:
(dropped)
 
Ai sensi del D.Lgs. 196/2003 si precisa che le informazioni ...{{dropped}}


From ripley at stats.ox.ac.uk  Wed Feb 21 12:57:32 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Feb 2007 11:57:32 +0000 (GMT)
Subject: [R] Confindence interval for Levenberg-Marquardt fit
In-Reply-To: <45DC2BE0.4090308@cebitec.uni-bielefeld.de>
References: <45DC254B.6080601@cebitec.uni-bielefeld.de>
	<Pine.LNX.4.64.0702211104570.23374@gannet.stats.ox.ac.uk>
	<45DC2BE0.4090308@cebitec.uni-bielefeld.de>
Message-ID: <Pine.LNX.4.64.0702211155560.25214@gannet.stats.ox.ac.uk>

On Wed, 21 Feb 2007, Michael Dondrup wrote:

> Thank you,
> sorry, I forgot to mention that nls.lm is in package minpack.lm. It's
> use is motivated by the wish of a colleague to reproduce a result from
> some publication. But if I understand you correctly, use of the methods
> implemented in nls or optim is preferred?

Yes, unless perhaps you are trying to achieve a perfect fit, when all bets 
are off for nls.

>
>
> Prof Brian Ripley wrote:
>> Well, the algorithm used does not affect the confidence interval
>> (provided it works correctly), but what is nls.ml (presumably in some
>> package you have not mentioned) and why would I want to use an
>> old-fashioned algorithm?
>>
>> You could start nls at the solution you got from nls.ml and use
>> confint() on that.
>>
>> On Wed, 21 Feb 2007, Michael Dondrup wrote:
>>
>>> Dear all,
>>> I would like to use the Levenberg-Marquardt algorithm for non-linear
>>> least-squares regression using function nls.lm. Can anybody help  me to
>>>   find a a way to compute confidence intervals on the  fitted
>>> parameters as it is possible for nls (using confint.nls, which does not
>>> work for nls.lm)?
>>>
>>> Thank you for your help
>>> Michael
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Feb 21 13:02:20 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Feb 2007 12:02:20 +0000 (GMT)
Subject: [R] R unstable and crashes after executing .C
In-Reply-To: <5FF3F11444E3A9439191AA1EDCB69A17B861B6@icosrvmail01.ICO.SCS.local>
References: <5FF3F11444E3A9439191AA1EDCB69A17B861B6@icosrvmail01.ICO.SCS.local>
Message-ID: <Pine.LNX.4.64.0702211158190.25214@gannet.stats.ox.ac.uk>

This really is the wrong list for C programming questions related to R.
See the posting guide and consider R-devel.
See also the debugging advice in 'Writing R Extensions'.

Almost certainly your C code has destroyed R structures, most likely by 
writing outside array bounds.  Running under valgrind should tell you 
about that.

You may or may not get a core dump, but you can run R under gdb.  I 
suspect though that the problem is that an internal gc is crashing.

On Wed, 21 Feb 2007, Sole Acha, Xavi wrote:

> Dear R listers,
>
> I have developed a C function to be executed from R through the ".C" interface. After doing dyn.load, the function executes properly and I get the results. However, after executing my function, R seems to get unstable and crashes (giving a segmentation fault and exiting) whenever I try to do ANYTHING with a relatively large object (creating a new one or even just writing the name of an existing one).
>
> I use R 2.4.0 under a Linux machine with 1 GB RAM. Below there is an example of execution, so you can get an idea of what is happening:
>
> --------------------
> dyn.load("my_C_module.so");
> res <- .C("my_C_function",.....); #The function executes fine and res is ok
> dyn.unload("my_C_module.so") #I know this isn't strictly necessary
>
> #Here R is still running, but when I execute:
>
> m <- matrix(0,1000,100); #I try to create a new object and R crashes
>
> *** caught segfault ***
> address 0x10, cause 'memory not mapped'
>
> Traceback:
> 1: matrix(0, 1000, 100)
>
> Possible actions:
> 1: abort (with core dump)
> 2: normal R exit
> 3: exit R without saving workspace
> 4: exit R saving workspace
> --------------------
>
> Although I tell R to abort and give me the core dump, it doesn't succeed in doing so.
>
> I would be grateful if anyone could tell me what could be the problem with my C function that makes R behave this way?
>
> Thank you very much in advance, and apologies for this long email.
>
> Xavier Sol?.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From j.van_den_hoff at fzd.de  Wed Feb 21 13:10:56 2007
From: j.van_den_hoff at fzd.de (joerg van den hoff)
Date: Wed, 21 Feb 2007 13:10:56 +0100
Subject: [R] Confindence interval for Levenberg-Marquardt fit
In-Reply-To: <Pine.LNX.4.64.0702211104570.23374@gannet.stats.ox.ac.uk>
References: <45DC254B.6080601@cebitec.uni-bielefeld.de>
	<Pine.LNX.4.64.0702211104570.23374@gannet.stats.ox.ac.uk>
Message-ID: <20070221121056.GA25281@marco.fz-rossendorf.de>

On Wed, Feb 21, 2007 at 11:09:52AM +0000, Prof Brian Ripley wrote:
> Well, the algorithm used does not affect the confidence interval (provided 
> it works correctly), but what is nls.ml (presumably in some package you 
> have not mentioned) and why would I want to use an old-fashioned 
> algorithm?
is'nt this a bit strong? in what respect do you consider levenberg-marquardt
(going back to the 19-forties, I think) as old-fashioned (especially
in comparsion to the `nls' standard gauss-newton approach (both gentlemen
seem to have done their major work a bit longer ago :-))?
AFAICS levenberg-marquardt is generally appreciated for it's rapid
convergence achieved by a smooth transition from an
inverse-hessian approach to steepest descent. my own experience
with non-linear least squares minimization using this algorithm
are positive as well, but
I have not tried out the levenberg-marquardt
implementation in package `minpack.lm' (which originates from netlib.org)
and don't know if it's good. but in any case there sure are implementations
around (e.g. in the CERN MINUIT library) which have proven to be
of high quality.

`nls' sure is a _very_ valuable function, but not necessarily the
"last word" with regards to the chosen algorithm(s).


> 
> You could start nls at the solution you got from nls.ml and use confint() 
> on that.
maybe one should look at profile.nls and confint.nls and see what information
of the usual `nls' object is actually used for the confidence intervall
computation and mimick this for the `nls.lm' output? at a (admittedly)
quick glance it seems that only parameters, std.errs. and the fitted/residual
values are needed which should all be provided by nls.lm as well.
maybe one could even try to map the nls.lm results into a structure of class 
`nls' (although this would not be a clean solution, probably) in order
to use `confint.nls'?

> 
> On Wed, 21 Feb 2007, Michael Dondrup wrote:
> 
> > Dear all,
> > I would like to use the Levenberg-Marquardt algorithm for non-linear
> > least-squares regression using function nls.lm. Can anybody help  me to
> >   find a a way to compute confidence intervals on the  fitted
> > parameters as it is possible for nls (using confint.nls, which does not
> > work for nls.lm)?
> >
> > Thank you for your help
> > Michael
> 
>


From stat700004 at yahoo.co.in  Wed Feb 21 13:13:59 2007
From: stat700004 at yahoo.co.in (stat stat)
Date: Wed, 21 Feb 2007 12:13:59 +0000 (GMT)
Subject: [R] Omiting repeated values
Message-ID: <20070221121359.68089.qmail@web7609.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/26a9ddd6/attachment.pl 

From brown_emu at yahoo.com  Wed Feb 21 13:21:41 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 21 Feb 2007 04:21:41 -0800 (PST)
Subject: [R] Splom plot:how to plot with different symbols?
In-Reply-To: <d4327f7e0702202215v2117c69eqc53ec9e9a8701012@mail.gmail.com>
Message-ID: <89716.43367.qm@web39706.mail.mud.yahoo.com>

I believe you can add these lines before your splom() command:

super.sym <- trellis.par.get("superpose.symbol")
super.sym$pch <- 1:length(super.sym$pch) # change this
                                         # to specify symbols
trellis.par.set("superpose.symbol",super.sym)

Best regards,

--- "d. sarthi maheshwari" <samay.sar at gmail.com> wrote:

> Hi,
> 
> Kindly let me know if I posted a wrong question in the forum.
> 
> I want to draw a splom plot with different symbols in plot. My command is
> as
> follows:
> 
> splom(~ log10(splomData[2:3]), groups = programs, data = splomData,
> panel = panel.superpose,
> key = list(title = paste(splomLoop,"Programs of Hog Analysis (Sorted by
> LR(GB))"),
>            panel = panel.superpose,
>          columns = splomLoop,
>            points = list(pch = c(1:splomLoop), #this will display different
> symbols in legend but not in actual graph
>          col = rainbow(splomLoop, s=1, v=1),
>            text = trim(as.character(ProgramNameList[1:splomLoop,1])))))
> 
> Kindly help.
> 
> Thanks & Regards
> Sarthi M.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Get your own web address.


From jholtman at gmail.com  Wed Feb 21 13:34:57 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 21 Feb 2007 07:34:57 -0500
Subject: [R] Omiting repeated values
In-Reply-To: <20070221121359.68089.qmail@web7609.mail.in.yahoo.com>
References: <20070221121359.68089.qmail@web7609.mail.in.yahoo.com>
Message-ID: <644e1f320702210434m6ac9fb51g98112762188b39f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/7a6857b7/attachment.pl 

From Dimitris.Rizopoulos at med.kuleuven.be  Wed Feb 21 13:45:58 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 21 Feb 2007 13:45:58 +0100
Subject: [R] Omiting repeated values
In-Reply-To: <20070221121359.68089.qmail@web7609.mail.in.yahoo.com>
References: <20070221121359.68089.qmail@web7609.mail.in.yahoo.com>
Message-ID: <20070221134558.0dknq55tjiroo44c@webmail5.kuleuven.be>

x <- sample(1:3, 20, TRUE)
x

# do you mean

unique(x)

# or

rle(x)$values


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting stat stat <stat700004 at yahoo.co.in>:

> Dear all R users,
>
>   Is there any function to omit repeated values in a vector? Your   
> help will be highly appreciated.
>
>   Thanks
>   stat
>
>
> ---------------------------------
>  Here?s a new way to find what you're looking for - Yahoo! Answers
> 	[[alternative HTML version deleted]]
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From osklyar at ebi.ac.uk  Wed Feb 21 14:24:52 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Wed, 21 Feb 2007 13:24:52 +0000
Subject: [R] Omiting repeated values
In-Reply-To: <20070221121359.68089.qmail@web7609.mail.in.yahoo.com>
References: <20070221121359.68089.qmail@web7609.mail.in.yahoo.com>
Message-ID: <45DC4824.2080005@ebi.ac.uk>

Hi,

'unique' or its combination with 'match' if you need to keep the vector 
the same length will do it:

 > a<-c(1,2,4,2,5,5,6,7,8)
 > unique(a)
[1] 1 2 4 5 6 7 8

 > a[ which( is.na( match(1:length(a), match(unique(a),a)) ) ) ]=NA
 > a
[1]  1  2  4 NA  5 NA  6  7  8
This is probably not the best implementation, but it does the job.

'table' will also give you the number of occurances of unique values:

 > a<-c(1,2,4,2,5,5,6,7,8)
 > table(a)
a
1 2 4 5 6 7 8
1 2 1 2 1 1 1

Oleg

stat stat wrote:
> Dear all R users,
>    
>   Is there any function to omit repeated values in a vector? Your help will be highly appreciated.
>    
>   Thanks 
>   stat
> 
>  				
> ---------------------------------
>  Here?s a new way to find what you're looking for - Yahoo! Answers 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Oleg Sklyar * EBI/EMBL, Cambridge CB10 1SD, England * +44-1223-494466


From Rainer.Hurling at nw-fva.de  Wed Feb 21 15:18:48 2007
From: Rainer.Hurling at nw-fva.de (Rainer Hurling)
Date: Wed, 21 Feb 2007 15:18:48 +0100
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <Pine.LNX.4.64.0702211012400.7584@gannet.stats.ox.ac.uk>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
	<1171918929.4041.22.camel@localhost.localdomain>
	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
	<1171982642.3451.5.camel@localhost.localdomain>
	<45DB0CCA.5090809@stats.uwo.ca>
	<1171990765.3451.12.camel@localhost.localdomain>
	<20070220121151.0e54f526@subarnarekha.stat.iastate.edu>
	<45DB3B79.30004@stats.uwo.ca> <45DB6355.6050402@gwdg.de>
	<Pine.LNX.4.64.0702202141580.13327@auk.stats>
	<Pine.LNX.4.64.0702211012400.7584@gannet.stats.ox.ac.uk>
Message-ID: <45DC54C8.1080108@nw-fva.de>

Brian,

thank you for the adaptions. I tried the new version under R-2.5.0 
(2007-01-23 r40560) on FreeBSD 7.0-CURRENT and got the following output.


-----
#R CMD INSTALL rgl_0.70-1.tar.gz
* Installing to library '/usr/local/lib/R/library'
* Installing *source* package 'rgl' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for X... libraries /usr/X11R6/lib, headers /usr/X11R6/include
checking for libpng-config... yes
configure: using libpng-config
configure: using libpng dynamic linkage
configure: creating ./config.status
config.status: creating src/Makevars
** libs
g++ -I/usr/local/lib/R/include -I/usr/local/lib/R/include 
-I/usr/X11R6/include -DHAVE_PNG_H
-I/usr/local/include/libpng -Iext -I/usr/local/include    -fpic  -g -O2 
-c BBoxDeco.cpp -o BBoxDeco.o
In file included from BBoxDeco.hpp:11,
                  from BBoxDeco.cpp:1:
math.h: In function `T math::log2(T)':
math.h:43: error: `::log2' has not been declared
*** Error code 1

Stop in /tmp/R.INSTALL.hVDVXM/rgl/src.
chmod: /usr/local/lib/R/library/rgl/libs/*: No such file or directory
ERROR: compilation failed for package 'rgl'
** Removing '/usr/local/lib/R/library/rgl'
-----


It seems there is a problem with math.h. The FreeBSD manual for math(3) 
says:

   The log2() and nan() functions are missing, and many functions
   are not available in their long double variants.


I do not know how to locate the library that contains function log2, 
sorry. What can I do next to help?

Rainer


Prof Brian Ripley wrote:
> Please try the attached.
> 
> Duncan: I fixed some other errors in configure.ac and src/Makevars.in, 
> and added a FIXME. If this works on MacOS X (and I am fairly confident 
> it will on FreeBSD since it now works on Solaris make), I can fix up the 
> FIXME too.
> 
> I was presuming the MacOS code came from Simon Urbanek, but I would have 
> expected him to know better than to use GNU make extensions.
> 
> Brian
> 
> On Tue, 20 Feb 2007, Prof Brian Ripley wrote:
> 
>> The problem is that rgl is apparently written for GNU make, and has 
>> (as shipped)
>>
>> ifdef MAKINGAGL
>> PKG_CPPFLAGS=@AGLCPPFLAGS@ -Iext
>> PKG_LIBS=@AGLLIBS@
>> else
>> PKG_CPPFLAGS= -If:/R/R-2.4.1/src/extra/zlib -DHAVE_PNG_H 
>> -If:/R/R-2.4.1/src/gnuwin32/bitmap/libpng  -Iext
>> PKG_LIBS=-lgdi32 -lopengl32 -lglu32 
>> -Lf:/R/R-2.4.1/src/gnuwin32/bitmap/libpng -
>> lpng -Lf:/R/R-2.4.1/src/extra/zlib -lz
>> endif
>>
>> and similar for BUILDAGL.
>>
>> That seems to have been written to make it workable on MacOS X. Given 
>> that configure knows (or could know) the OS, it seems better to write 
>> (via configure) a separate Makevars for MacOS X and remove all the 
>> ifdef...endif stuff for everyone else.  (If you do that in Makevars.in 
>> it should work.)
>>
>>
>> On Tue, 20 Feb 2007, Rainer Hurling wrote:
>>
>>> Duncan Murdoch schrieb:
>>>> On 2/20/2007 1:11 PM, Ranjan Maitra wrote:
>>>>> Hi Duncan,
>>>>> I don't know if this will list all the dependencies for your 
>>>>> documentation, since Rick's error messages did not involve 
>>>>> libraries and header files already installed by him for something 
>>>>> else, perhaps.
>>>>
>>>> No, but it's a start:  I'm thinking of something more like a FAQ than
>>>> comprehensive documentation.  Comprehensive docs are not feasible to
>>>> maintain (there are so many systems that Daniel and I don't use), but
>>>> hints that two non-standard packages solved one person's problems might
>>>> be enough of a hint to get someone else going.
>>>
>>> Duncan,
>>>
>>> thank you for this purpose. I am such a person who could need some help
>>> with installing rgl and hope it is ok to place it in this thread.
>>>
>>> My trial to install rgl_0.70.tar.gz on R-2.4.1 on FreeBSD 7.0-CURRENT
>>> ends up with errors. Obiously something is wrong with Makevars. I have
>>> no idea what to do next. 'rgl' is one of the rare cases that do not
>>> install under R on FreeBSD.
>>>
>>> The install messages are short:
>>>
>>> -----
>>> #R CMD INSTALL rgl_0.70.tar.gz
>>> * Installing to library '/usr/local/lib/R/library'
>>> * Installing *source* package 'rgl' ...
>>> checking for gcc... gcc
>>> checking for C compiler default output file name... a.out
>>> checking whether the C compiler works... yes
>>> checking whether we are cross compiling... no
>>> checking for suffix of executables...
>>> checking for suffix of object files... o
>>> checking whether we are using the GNU C compiler... yes
>>> checking whether gcc accepts -g... yes
>>> checking for gcc option to accept ANSI C... none needed
>>> checking how to run the C preprocessor... gcc -E
>>> checking for X... libraries /usr/X11R6/lib, headers /usr/X11R6/include
>>> checking for libpng-config... yes
>>> configure: using libpng-config
>>> configure: using libpng dynamic linkage
>>> configure: creating ./config.status
>>> config.status: creating src/Makevars
>>> ** libs
>>> "Makevars", line 9: Need an operator
>>> "Makevars", line 12: Need an operator
>>> "Makevars", line 15: Need an operator
>>> "Makevars", line 21: Need an operator
>>> "Makevars", line 23: Need an operator
>>> "Makevars", line 36: Need an operator
>>> "Makevars", line 38: Need an operator
>>> make: fatal errors encountered -- cannot continue
>>> chmod: /usr/local/lib/R/library/rgl/libs/*: No such file or directory
>>> ERROR: compilation failed for package 'rgl'
>>> ** Removing '/usr/local/lib/R/library/rgl'
>>> -----
>>>
>>> Are there any experiences with installing rgl on FreeBSD? Do you know if
>>> it is at all possible to get rgl to work on FreeBSD? If I can do
>>> anything like testing or giving more information let me know.
>>>
>>> I appreciate any help. Thank you in advance.
>>>
>>> Rainer Hurling


From ripley at stats.ox.ac.uk  Wed Feb 21 15:56:50 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Feb 2007 14:56:50 +0000 (GMT)
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <45DC54C8.1080108@nw-fva.de>
References: <1171913730.4041.7.camel@localhost.localdomain>
	<45DA00EF.2050305@ebi.ac.uk>
	<1171918929.4041.22.camel@localhost.localdomain>
	<20070219151147.0e964aca@subarnarekha.stat.iastate.edu>
	<1171982642.3451.5.camel@localhost.localdomain>
	<45DB0CCA.5090809@stats.uwo.ca>
	<1171990765.3451.12.camel@localhost.localdomain>
	<20070220121151.0e54f526@subarnarekha.stat.iastate.edu>
	<45DB3B79.30004@stats.uwo.ca> <45DB6355.6050402@gwdg.de>
	<Pine.LNX.4.64.0702202141580.13327@auk.stats>
	<Pine.LNX.4.64.0702211012400.7584@gannet.stats.ox.ac.uk>
	<45DC54C8.1080108@nw-fva.de>
Message-ID: <Pine.LNX.4.64.0702211454470.8414@auk.stats>

It seems log2 is not used so you can just comment out the line.

I am working on a better configure script, and will have something to show 
you later today.

BDR

On Wed, 21 Feb 2007, Rainer Hurling wrote:

> Brian,
>
> thank you for the adaptions. I tried the new version under R-2.5.0 
> (2007-01-23 r40560) on FreeBSD 7.0-CURRENT and got the following output.
>
>
> -----
> #R CMD INSTALL rgl_0.70-1.tar.gz
> * Installing to library '/usr/local/lib/R/library'
> * Installing *source* package 'rgl' ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for X... libraries /usr/X11R6/lib, headers /usr/X11R6/include
> checking for libpng-config... yes
> configure: using libpng-config
> configure: using libpng dynamic linkage
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> g++ -I/usr/local/lib/R/include -I/usr/local/lib/R/include 
> -I/usr/X11R6/include -DHAVE_PNG_H
> -I/usr/local/include/libpng -Iext -I/usr/local/include    -fpic  -g -O2 -c 
> BBoxDeco.cpp -o BBoxDeco.o
> In file included from BBoxDeco.hpp:11,
>                 from BBoxDeco.cpp:1:
> math.h: In function `T math::log2(T)':
> math.h:43: error: `::log2' has not been declared
> *** Error code 1
>
> Stop in /tmp/R.INSTALL.hVDVXM/rgl/src.
> chmod: /usr/local/lib/R/library/rgl/libs/*: No such file or directory
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/local/lib/R/library/rgl'
> -----
>
>
> It seems there is a problem with math.h. The FreeBSD manual for math(3) 
> says:
>
>  The log2() and nan() functions are missing, and many functions
>  are not available in their long double variants.
>
>
> I do not know how to locate the library that contains function log2, sorry. 
> What can I do next to help?
>
> Rainer
>
>
> Prof Brian Ripley wrote:
>> Please try the attached.
>> 
>> Duncan: I fixed some other errors in configure.ac and src/Makevars.in, and 
>> added a FIXME. If this works on MacOS X (and I am fairly confident it will 
>> on FreeBSD since it now works on Solaris make), I can fix up the FIXME 
>> too.
>> 
>> I was presuming the MacOS code came from Simon Urbanek, but I would have 
>> expected him to know better than to use GNU make extensions.
>> 
>> Brian
>> 
>> On Tue, 20 Feb 2007, Prof Brian Ripley wrote:
>> 
>>> The problem is that rgl is apparently written for GNU make, and has (as 
>>> shipped)
>>> 
>>> ifdef MAKINGAGL
>>> PKG_CPPFLAGS=@AGLCPPFLAGS@ -Iext
>>> PKG_LIBS=@AGLLIBS@
>>> else
>>> PKG_CPPFLAGS= -If:/R/R-2.4.1/src/extra/zlib -DHAVE_PNG_H 
>>> -If:/R/R-2.4.1/src/gnuwin32/bitmap/libpng  -Iext
>>> PKG_LIBS=-lgdi32 -lopengl32 -lglu32 
>>> -Lf:/R/R-2.4.1/src/gnuwin32/bitmap/libpng -
>>> lpng -Lf:/R/R-2.4.1/src/extra/zlib -lz
>>> endif
>>> 
>>> and similar for BUILDAGL.
>>> 
>>> That seems to have been written to make it workable on MacOS X. Given 
>>> that configure knows (or could know) the OS, it seems better to write 
>>> (via configure) a separate Makevars for MacOS X and remove all the 
>>> ifdef...endif stuff for everyone else.  (If you do that in Makevars.in it 
>>> should work.)
>>> 
>>> 
>>> On Tue, 20 Feb 2007, Rainer Hurling wrote:
>>> 
>>>> Duncan Murdoch schrieb:
>>>>> On 2/20/2007 1:11 PM, Ranjan Maitra wrote:
>>>>>> Hi Duncan,
>>>>>> I don't know if this will list all the dependencies for your 
>>>>>> documentation, since Rick's error messages did not involve libraries 
>>>>>> and header files already installed by him for something else, perhaps.
>>>>> 
>>>>> No, but it's a start:  I'm thinking of something more like a FAQ than
>>>>> comprehensive documentation.  Comprehensive docs are not feasible to
>>>>> maintain (there are so many systems that Daniel and I don't use), but
>>>>> hints that two non-standard packages solved one person's problems might
>>>>> be enough of a hint to get someone else going.
>>>> 
>>>> Duncan,
>>>> 
>>>> thank you for this purpose. I am such a person who could need some help
>>>> with installing rgl and hope it is ok to place it in this thread.
>>>> 
>>>> My trial to install rgl_0.70.tar.gz on R-2.4.1 on FreeBSD 7.0-CURRENT
>>>> ends up with errors. Obiously something is wrong with Makevars. I have
>>>> no idea what to do next. 'rgl' is one of the rare cases that do not
>>>> install under R on FreeBSD.
>>>> 
>>>> The install messages are short:
>>>> 
>>>> -----
>>>> #R CMD INSTALL rgl_0.70.tar.gz
>>>> * Installing to library '/usr/local/lib/R/library'
>>>> * Installing *source* package 'rgl' ...
>>>> checking for gcc... gcc
>>>> checking for C compiler default output file name... a.out
>>>> checking whether the C compiler works... yes
>>>> checking whether we are cross compiling... no
>>>> checking for suffix of executables...
>>>> checking for suffix of object files... o
>>>> checking whether we are using the GNU C compiler... yes
>>>> checking whether gcc accepts -g... yes
>>>> checking for gcc option to accept ANSI C... none needed
>>>> checking how to run the C preprocessor... gcc -E
>>>> checking for X... libraries /usr/X11R6/lib, headers /usr/X11R6/include
>>>> checking for libpng-config... yes
>>>> configure: using libpng-config
>>>> configure: using libpng dynamic linkage
>>>> configure: creating ./config.status
>>>> config.status: creating src/Makevars
>>>> ** libs
>>>> "Makevars", line 9: Need an operator
>>>> "Makevars", line 12: Need an operator
>>>> "Makevars", line 15: Need an operator
>>>> "Makevars", line 21: Need an operator
>>>> "Makevars", line 23: Need an operator
>>>> "Makevars", line 36: Need an operator
>>>> "Makevars", line 38: Need an operator
>>>> make: fatal errors encountered -- cannot continue
>>>> chmod: /usr/local/lib/R/library/rgl/libs/*: No such file or directory
>>>> ERROR: compilation failed for package 'rgl'
>>>> ** Removing '/usr/local/lib/R/library/rgl'
>>>> -----
>>>> 
>>>> Are there any experiences with installing rgl on FreeBSD? Do you know if
>>>> it is at all possible to get rgl to work on FreeBSD? If I can do
>>>> anything like testing or giving more information let me know.
>>>> 
>>>> I appreciate any help. Thank you in advance.
>>>> 
>>>> Rainer Hurling
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From venkatesh.mantha at pluralsoft.com  Wed Feb 21 16:03:34 2007
From: venkatesh.mantha at pluralsoft.com (Venkatesh Mantha)
Date: Wed, 21 Feb 2007 10:03:34 -0500
Subject: [R] Examples on how to READ/WRITE to database using R-Project
Message-ID: <001001c755c9$7617b0c0$62471240$@mantha@pluralsoft.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/6d032d9f/attachment.pl 

From david.meyer at wu-wien.ac.at  Wed Feb 21 16:21:46 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Wed, 21 Feb 2007 16:21:46 +0100
Subject: [R]  "contingency table" for several variables
Message-ID: <45DC638A.7030108@wu-wien.ac.at>

David:

 >I ?m trying to draw ONE table that summarize SEVERAL categorical 
 >variables
 >according to one classification variable, say ?sex?. The result would 
 >look
 >like several contingency tables appended one to the other. All the 
 >variables
 >belong to a data frame.

 >The summary.formula in Hmisc package does something pretty close and is
 >ready for a Latex export  but I need either to get rid off the >percentage
 >(or put the count prior to the percentage )in the ?reverse? option or 
 >to add
 >a chisquare test in the ?response? method.


You could have a look at structable() in package vcd.


Best
David


From rbaer at atsu.edu  Wed Feb 21 16:33:49 2007
From: rbaer at atsu.edu (Robert Baer)
Date: Wed, 21 Feb 2007 09:33:49 -0600
Subject: [R] Examples on how to READ/WRITE to database using R-Project
References: 001001c755c9$7617b0c0$62471240$@mantha@pluralsoft.com
Message-ID: <00cb01c755cd$af6e0c30$970c010a@ATSU7B94409E1B>


> We are working on a project on forecast modeling and would like to know if
> there are any examples on how to READ/WRITE to a database (e.g. 
> PostgreSQL)
> using R-Project. I do have a sample R Script which takes input as files 
> from
> a directory and writes back output files to a directory.  I would like to
> convert this script to read from a database and write back the output to 
> a
> database.
> - Venkatesh Mantha
You probably want to install the RODBC package.  Then,
library(RODBC)
?RODBC

Also, the the relational database section of the R Data Import/Export Manual 
is worth reading:
http://cran.r-project.org/doc/manuals/R-data.html


From roland.rproject at gmail.com  Wed Feb 21 16:39:49 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Wed, 21 Feb 2007 10:39:49 -0500
Subject: [R] Examples on how to READ/WRITE to database using R-Project
In-Reply-To: <352318013272108833@unknownmsgid>
References: <352318013272108833@unknownmsgid>
Message-ID: <47c7c59e0702210739l16e7be64g94d6cd2e62099cb8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/8b5dd797/attachment.pl 

From venkatesh.mantha at pluralsoft.com  Wed Feb 21 16:43:34 2007
From: venkatesh.mantha at pluralsoft.com (Venkatesh Mantha)
Date: Wed, 21 Feb 2007 10:43:34 -0500
Subject: [R] Examples on how to READ/WRITE to database using R-Project
In-Reply-To: <00cb01c755cd$af6e0c30$970c010a@ATSU7B94409E1B>
References: 001001c755c9$7617b0c0$62471240$@mantha@pluralsoft.com
	<00cb01c755cd$af6e0c30$970c010a@ATSU7B94409E1B>
Message-ID: <002101c755cf$0ccf1e40$266d5ac0$@mantha@pluralsoft.com>

Thanks very much for your reply. We am new to "R" and actually my main
interest is making sure that we can develop a R script that can efficiently
read/write to the database. Today we generate text files from the database,
run R in batch mode on these files and load the text files back into the
database. Even though this process works, given the amount of data we expect
to have and the forecasting results we expect to see this could be very time
consuming.

Our goal is to eliminate this additional I/O of reading/writing to files and
would like to read/write from/to the database in order to save time and have
a better process architecture.

I did read the manual and couldn't make much out of it. I was looking to see
if somebody has already done this so that I could use that to Jump start our
project.

Regards,

- Venkatesh Mantha

-----Original Message-----
From: Robert Baer [mailto:rbaer at atsu.edu] 
Sent: Wednesday, February 21, 2007 10:34 AM
To: Venkatesh Mantha; r-help at stat.math.ethz.ch
Subject: Re: [R] Examples on how to READ/WRITE to database using R-Project


> We are working on a project on forecast modeling and would like to know if
> there are any examples on how to READ/WRITE to a database (e.g. 
> PostgreSQL)
> using R-Project. I do have a sample R Script which takes input as files 
> from
> a directory and writes back output files to a directory.  I would like to
> convert this script to read from a database and write back the output to 
> a
> database.
> - Venkatesh Mantha
You probably want to install the RODBC package.  Then,
library(RODBC)
?RODBC

Also, the the relational database section of the R Data Import/Export Manual

is worth reading:
http://cran.r-project.org/doc/manuals/R-data.html


From jrkrideau at yahoo.ca  Wed Feb 21 16:47:07 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 21 Feb 2007 10:47:07 -0500 (EST)
Subject: [R] Trying to get an apply to work with a list in applying names to
	tables
Message-ID: <255584.99686.qm@web32810.mail.mud.yahoo.com>

I am trying to use apply and a  list to supply names
to a set of tables I want to generate. Below is an
example that I hope mimics the larger original
problem.

EXAMPLE

aa <- c( 2,2,1,1,2)
bb <- c(5,6,6,7,4)
aan <- c("yes", "no")
bbn <- c("a", "b", "c", "d")
mynames <- c("abby", "billy")
mylist <- list(aan, bbn);   names(mylist) <- mynames

cc <- data.frame(aa,bb)
fn1 <- function(x,y) {tt <- table(x); names(tt)<-
mylist[[y]]}
jj <-apply(cc, 2, fn1(cc,mylist))

RESULT:  
Error in fn1(cc, mylist) : invalid subscript type

To be honest I didn't expect it to work since that
fin1(cc  looks recursive but oh well...

Can anyone offer a solution or some advice here.  It
would be greatly appreciated


From Thierry.ONKELINX at inbo.be  Wed Feb 21 17:15:16 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 21 Feb 2007 17:15:16 +0100
Subject: [R] Trying to get an apply to work with a list in applying
	names totables
In-Reply-To: <255584.99686.qm@web32810.mail.mud.yahoo.com>
Message-ID: <2E9C414912813E4EB981326983E0A104029C322D@inboexch.inbo.be>

John,

Two things. You don't need to pout the cc variable in the apply. Use
instead something like this.

apply(cc, 2, fn1, y = mylist)

But this still doesn't solve your problem. You'll need to rewrite your
function like this.

> fn2 <- function(x, y, i){
+   tt <- table(x[, i])
+   names(tt) <- y[[i]]
+   return(tt)
+ }
> sapply(1:ncol(cc), fn2, x = cc, y = mylist)
[[1]]
yes  no 
  2   3 

[[2]]
a b c d 
1 1 2 1 

Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens John Kane
Verzonden: woensdag 21 februari 2007 16:47
Aan: R R-help
Onderwerp: [R] Trying to get an apply to work with a list in applying
names totables

I am trying to use apply and a  list to supply names
to a set of tables I want to generate. Below is an
example that I hope mimics the larger original
problem.

EXAMPLE

aa <- c( 2,2,1,1,2)
bb <- c(5,6,6,7,4)
aan <- c("yes", "no")
bbn <- c("a", "b", "c", "d")
mynames <- c("abby", "billy")
mylist <- list(aan, bbn);   names(mylist) <- mynames

cc <- data.frame(aa,bb)
fn1 <- function(x,y) {tt <- table(x); names(tt)<-
mylist[[y]]}
jj <-apply(cc, 2, fn1(cc,mylist))

RESULT:  
Error in fn1(cc, mylist) : invalid subscript type

To be honest I didn't expect it to work since that
fin1(cc  looks recursive but oh well...

Can anyone offer a solution or some advice here.  It
would be greatly appreciated

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Joseph.F.Lucke at uth.tmc.edu  Wed Feb 21 17:22:34 2007
From: Joseph.F.Lucke at uth.tmc.edu (Lucke, Joseph F)
Date: Wed, 21 Feb 2007 10:22:34 -0600
Subject: [R] 3F2 hypergeometric function
In-Reply-To: <Pine.LNX.4.64.0702201712110.5804@river.utm.utoronto.ca>
References: <Pine.LNX.4.64.0702201712110.5804@river.utm.utoronto.ca>
Message-ID: <4677FCB5A35A0441A0E0C99D56B23D910777FDD2@UTHEVS2.mail.uthouston.edu>

Does anyone have code for the 3F2 hypergeometric function? I am looking
for code similar to the 2F1 hypergeometric function implemented as
hyperg_2F1 in the GSL package. TIA.  ---Joe


From bates at stat.wisc.edu  Wed Feb 21 16:41:29 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 21 Feb 2007 09:41:29 -0600
Subject: [R] Confindence interval for Levenberg-Marquardt fit
In-Reply-To: <20070221121056.GA25281@marco.fz-rossendorf.de>
References: <45DC254B.6080601@cebitec.uni-bielefeld.de>
	<Pine.LNX.4.64.0702211104570.23374@gannet.stats.ox.ac.uk>
	<20070221121056.GA25281@marco.fz-rossendorf.de>
Message-ID: <40e66e0b0702210741i738e9723pd3fb2a43897252f5@mail.gmail.com>

On 2/21/07, joerg van den hoff <j.van_den_hoff at fzd.de> wrote:
> On Wed, Feb 21, 2007 at 11:09:52AM +0000, Prof Brian Ripley wrote:
> > Well, the algorithm used does not affect the confidence interval (provided
> > it works correctly), but what is nls.ml (presumably in some package you
> > have not mentioned) and why would I want to use an old-fashioned
> > algorithm?

> is'nt this a bit strong? in what respect do you consider levenberg-marquardt
> (going back to the 19-forties, I think) as old-fashioned (especially
> in comparsion to the `nls' standard gauss-newton approach (both gentlemen
> seem to have done their major work a bit longer ago :-))?
> AFAICS levenberg-marquardt is generally appreciated for it's rapid
> convergence achieved by a smooth transition from an
> inverse-hessian approach to steepest descent. my own experience
> with non-linear least squares minimization using this algorithm
> are positive as well, but
> I have not tried out the levenberg-marquardt
> implementation in package `minpack.lm' (which originates from netlib.org)
> and don't know if it's good. but in any case there sure are implementations
> around (e.g. in the CERN MINUIT library) which have proven to be
> of high quality.

The nls function provides implementations of the Gauss-Newton
algorithm and the Golub-Pereyra algorithm and the NL2SOL algorithm.
Check the possible values of the optional argument "algorithm" to
nls().

If you will allow me to wax philosophical for a moment, I submit that
there are three stages to any iterative optimization:
  - formulating initial values of the parameters
  - determining what step to take from iteration i to iteration i+1
  - determining when to declare convergence

For the most part optimization research focuses on the second step.
The first and third steps are also important.  One of the advantages
of the implementation of the Gauss-Newton and Golub-Pereyra algorithms
in nls is that they use a convergence criterion (based only on the
current parameter values) not a termination criterion (based on the
changes in the parameter values or the changes in the objective
function or both).  See chapter 2 of Bates and Watts, "Nonlinear
Regression Analysis and Its Applications" (Wiley, 1988) for a
discussion of the particular convergence criterion used.

Another point that is often lost in a discussion of various
optimization algorithms is that one does not compare algorithms - one
compares implementations of algorithms which, at the very least, must
involve all of the above steps.

For many people in the nonlinear least squares field the NL2SOL
algorithm by Dennis, Gay and Welch was considered the "gold standard"
and David Gay's implementation is now available in nls.  This allows
those who are interested to compare implementations of these
algorithms.  Also, with R being an Open Source system anyone can add
their own implementation or modify an existing implementation to try
things out.




> `nls' sure is a _very_ valuable function, but not necessarily the
> "last word" with regards to the chosen algorithm(s).
>
>
> >
> > You could start nls at the solution you got from nls.ml and use confint()
> > on that.
> maybe one should look at profile.nls and confint.nls and see what information
> of the usual `nls' object is actually used for the confidence intervall
> computation and mimick this for the `nls.lm' output? at a (admittedly)
> quick glance it seems that only parameters, std.errs. and the fitted/residual
> values are needed which should all be provided by nls.lm as well.
> maybe one could even try to map the nls.lm results into a structure of class
> `nls' (although this would not be a clean solution, probably) in order
> to use `confint.nls'?

Not really.  The profile.nls function requires, not surprisingly, the
ability to profile the objective function.  (See the discussion of the
profile sum of squares and the profile-t transformations in chapter 6
of Bates and Watts).  Such a capability would need to be added to a
nonlinear least squares implementation before methods for those
generics could be developed.

> >
> > On Wed, 21 Feb 2007, Michael Dondrup wrote:
> >
> > > Dear all,
> > > I would like to use the Levenberg-Marquardt algorithm for non-linear
> > > least-squares regression using function nls.lm. Can anybody help  me to
> > >   find a a way to compute confidence intervals on the  fitted
> > > parameters as it is possible for nls (using confint.nls, which does not
> > > work for nls.lm)?
> > >
> > > Thank you for your help
> > > Michael
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Wed Feb 21 17:38:58 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 21 Feb 2007 10:38:58 -0600
Subject: [R] Trying to get an apply to work with a list in
	applying	names totables
In-Reply-To: <2E9C414912813E4EB981326983E0A104029C322D@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104029C322D@inboexch.inbo.be>
Message-ID: <1172075938.4769.7.camel@localhost.localdomain>

I might suggest an alternative, since you seem to be creating the
underlying data set from scratch.

Create the data frame with the requisite data structures to start with
and then perform the table operations:


# First create your vectors as factors. See ?factor
aa <- factor(c(2,2,1,1,2), levels = 1:2, labels = c("yes", "no"))
bb <- factor(c(5,6,6,7,4), levels = 4:7, labels = letters[1:4])


# Now create your data frame using the names you want for each column
cc <- data.frame(abby = aa, billy = bb)


Now run the table on each column:

> lapply(cc, table)
$abby

yes  no 
  2   3 

$billy

a b c d 
1 1 2 1 


See ?lapply as well. Note that a data frame is a list:

> is.list(cc)
[1] TRUE

> is.data.frame(cc)
[1] TRUE


> as.list(cc)
$abby
[1] no  no  yes yes no 
Levels: yes no

$billy
[1] b c c d a
Levels: a b c d


HTH,

Marc Schwartz


On Wed, 2007-02-21 at 17:15 +0100, ONKELINX, Thierry wrote:
> John,
> 
> Two things. You don't need to pout the cc variable in the apply. Use
> instead something like this.
> 
> apply(cc, 2, fn1, y = mylist)
> 
> But this still doesn't solve your problem. You'll need to rewrite your
> function like this.
> 
> > fn2 <- function(x, y, i){
> +   tt <- table(x[, i])
> +   names(tt) <- y[[i]]
> +   return(tt)
> + }
> > sapply(1:ncol(cc), fn2, x = cc, y = mylist)
> [[1]]
> yes  no 
>   2   3 
> 
> [[2]]
> a b c d 
> 1 1 2 1 
> 
> Cheers,
> 
> Thierry

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens John Kane
> Verzonden: woensdag 21 februari 2007 16:47
> Aan: R R-help
> Onderwerp: [R] Trying to get an apply to work with a list in applying
> names totables
> 
> I am trying to use apply and a  list to supply names
> to a set of tables I want to generate. Below is an
> example that I hope mimics the larger original
> problem.
> 
> EXAMPLE
> 
> aa <- c( 2,2,1,1,2)
> bb <- c(5,6,6,7,4)
> aan <- c("yes", "no")
> bbn <- c("a", "b", "c", "d")
> mynames <- c("abby", "billy")
> mylist <- list(aan, bbn);   names(mylist) <- mynames
> 
> cc <- data.frame(aa,bb)
> fn1 <- function(x,y) {tt <- table(x); names(tt)<-
> mylist[[y]]}
> jj <-apply(cc, 2, fn1(cc,mylist))
> 
> RESULT:  
> Error in fn1(cc, mylist) : invalid subscript type
> 
> To be honest I didn't expect it to work since that
> fin1(cc  looks recursive but oh well...
> 
> Can anyone offer a solution or some advice here.  It
> would be greatly appreciated


From rhurlin at gwdg.de  Wed Feb 21 17:50:13 2007
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Wed, 21 Feb 2007 17:50:13 +0100
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <Pine.LNX.4.64.0702211539350.11585@gannet.stats.ox.ac.uk>
References: <307b90470702210617l66ba714ao8d8ee8cd2ece0b8f@mail.gmail.com>
	<Pine.LNX.4.64.0702211539350.11585@gannet.stats.ox.ac.uk>
Message-ID: <45DC7845.2040507@gwdg.de>

Brian,

I just tried your version rgl_0.70-2 with R-2.5.0 on FreeBSD 7.0-CURRENT 
(i386) and it works!

Thank you very much for this competently and very fast help.

Next I will try if it compiles on amd64, too ...

Rainer


Prof Brian Ripley schrieb:
> This looks to me like a problem in your OpenGL, I am afraid.
> 
> I've made available a (completely unofficial) revised tarball at
> 
> http://www.stats.ox.ac.uk/pub/R/rgl_0.70-2.tar.gz
> 
> that attempts to work around various configure issues.  Not only does it 
> assume a vanilla make, it also tests if the various headers and 
> libraries are actually present (without assuming they are in .../lib, 
> which they are not on my main system).  So the enquiry with originally 
> started this thread will get a clear error from configure about what is 
> wrong.
> 
> Duncan: I have left in -Iext in PKG_CPPFLAGS, but AFAICS it is only 
> needed on Windows.  We've lost the ability to use a static libpng unless 
> we have libpng-config:  that is I am afraid inevitable as there is no 
> way to find where it might be, and you can't just grab a object of the 
> right name on a multi-architecture system.  Actually, I think it is 
> fundamentally broken as only on a few systems would a static library be 
> PIC (and as i386 Linux with recent gcc is one of those, people tend to 
> forget that).
> 
> Brian Ripley
> 
> On Wed, 21 Feb 2007, Hiroyuki Kawakatsu wrote:
> 
>> Hi Rainer,
>>
>> Have you had any luck after Prof Ripley's suggested fix? I am on
>> FreeBSD 6.2 (amd64) and cannot get rgl to work either. With the
>> suggested fix and after commenting out the log2() declaration in
>> math.h, the package builds but segfaults when running the examples in
>> R CMD check. (I have contacted one of the maintainers about this a
>> while back on an earlier version of rgl but no response.) If you had
>> success, are you on a 32bit or 64bit system?
>>
>> h.
>>
>> * checking examples ... ERROR
>> Running examples in 'rgl-Ex.R' failed.
>> The error most likely occurred in:
>>
>>> ### * 3dobjects
>>>
>>> flush(stderr()); flush(stdout())
>>>
>>> ### Name: points3d
>>> ### Title: add primitive set shape
>>> ### Aliases: points3d lines3d segments3d triangles3d quads3d
>>> ### Keywords: dynamic
>>>
>>> ### ** Examples
>>>
>>> # Show 12 random vertices in various ways.
>>>
>>> M <- matrix(rnorm(36), 3, 12, dimnames=list(c('x','y','z'),
>> +                                        rep(LETTERS[1:4], 3)))
>>>
>>> # Force 4-tuples to be convex in planes so that quads3d works.
>>>
>>> for (i in c(1,5,9)) {
>> +     quad <- as.data.frame(M[,i+0:3])
>> +     coeffs <- runif(2,0,3)
>> +     if (mean(coeffs) < 1) coeffs <- coeffs + 1 - mean(coeffs)
>> +     quad$C <- with(quad, coeffs[1]*(B-A) + coeffs[2]*(D-A) + A)
>> +     M[,i+0:3] <- as.matrix(quad)
>> + }
>>>
>>> open3d()
>> [1] 1
>>>
>>> # Rows of M are x, y, z coords; transpose to plot
>>>
>>> M <- t(M)
>>> shift <- matrix(c(-3,3,0), 12, 3, byrow=TRUE)
>>>
>>> points3d(M, size=2)
>>
>> *** caught segfault ***
>> address 0x0, cause 'unknown'
>>
>> Traceback:
>> 1: .C(rgl_primitive, success = as.integer(FALSE), idata,
>> as.numeric(vertex),     NAOK = TRUE)
>> 2: rgl.primitive("points", x, y, z, ...)
>> 3: rgl.points(x = c(-0.626453810742332, 1.59528080213779,
>> 2.00575308060846, -0.305388387156356, -0.621240580541804,
>> -0.0449336090152309, 3.95860460385714, 0.782136300731067,
>> 0.61982574789471, -1.47075238389927, -5.58920873034883,
>> -0.0538050405829051, 0.183643324222082, 0.329507771815361,
>> 1.66524706227070, 1.51178116845085, -2.2146998871775,
>> -0.0161902630989461, 7.05452342645083, 0.0745649833651906,
>> -0.0561287395290008, -0.47815005510862, -2.69588540023329,
>> -1.37705955682861, -0.835628612410047, -0.820468384118015,
>> 0.407234134716211, 0.389843236411431, 1.12493091814311,
>> 0.94383621068530, -7.46953644668121, -1.98935169586337,
>> -0.155795506705329, 0.417941560199702, 1.02866050611420,
>> -0.41499456329968), y = NULL, z = NULL, color = "#000000",     alpha =
>> numeric(0), lit = TRUE, ambient = "#000000", specular = "#FFFFFF",
>> emission = "#000000", shininess = 50, smooth = TRUE, front = "filled",
>>   back = "filled", size = 2, fog = FALSE)
>> 4: do.call("rgl.points", c(list(x = x, y = y, z = z),
>> .fixMaterialArgs(...,     Params = save)))
>> 5: points3d(M, size = 2)
>> aborting ...
>>
>>
>>
>> Message: 53
>> Date: Tue, 20 Feb 2007 21:45:55 +0000 (GMT)
>> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>> Subject: Re: [R] Installing Package rgl - Compilation Fails - FreeBSD
>>
>> The problem is that rgl is apparently written for GNU make, and has (as
>> shipped)
>>
>> ifdef MAKINGAGL
>> PKG_CPPFLAGS=@AGLCPPFLAGS@ -Iext
>> PKG_LIBS=@AGLLIBS@
>> else
>> PKG_CPPFLAGS= -If:/R/R-2.4.1/src/extra/zlib -DHAVE_PNG_H
>> -If:/R/R-2.4.1/src/gnuwin32/bitmap/libpng  -Iext
>> PKG_LIBS=-lgdi32 -lopengl32 -lglu32
>> -Lf:/R/R-2.4.1/src/gnuwin32/bitmap/libpng -
>> lpng -Lf:/R/R-2.4.1/src/extra/zlib -lz
>> endif
>>
>> and similar for BUILDAGL.
>>
>> That seems to have been written to make it workable on MacOS X. Given 
>> that
>> configure knows (or could know) the OS, it seems better to write (via
>> configure) a separate Makevars for MacOS X and remove all the
>> ifdef...endif stuff for everyone else.  (If you do that in Makevars.in it
>> should work.)


From rita.sousa at ine.pt  Wed Feb 21 17:58:53 2007
From: rita.sousa at ine.pt (Rita Sousa)
Date: Wed, 21 Feb 2007 16:58:53 -0000
Subject: [R] file path
Message-ID: <2F01C2D1A2A44B41BD01DDE5F72E1CAB01A1F6CC@rngpew02.drn.ine.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/dd4778bb/attachment.pl 

From rfrancois at mango-solutions.com  Wed Feb 21 18:05:56 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Wed, 21 Feb 2007 17:05:56 +0000
Subject: [R] R Introduction Course - LONDON
Message-ID: <45DC7BF4.6030606@mango-solutions.com>


Mango Solutions are pleased to announce the above course in London as
part of our schedule for Q1 2007.

-----------------------------------------------------------------------
     Introduction to R and R Programming - 26th-28th March 2007
-----------------------------------------------------------------------

* Who Should Attend ?

This is a course suitable for beginners and improvers in the R language
and is ideal for people wanting an all round introduction to R

* Course Goals

- To allow attendees to understand the technology behind the R package
- Improve attendees programming style and confidence
- To enable users to access a wide range of available functionality
- To enable attendees to program in R within their own environment
- To understand how to embed R routines within other applications

* Course Outline

1. Introduction to the R language and the R community
2. The R Environment
3. R data objects
4. Using R functions
5. The "apply" family of functions
6. Writing R functions
7. Standard Graphics
9. Statistical Analysis

Should your organization have more than 3 possible attendees why not
talk to us about hosting a customized and focused course delivered at
your premises? Details of further courses in alternative locations are
available at http://www.mango-solutions.com/services/training.html

More details about this course :
http://www.mango-solutions.com/services/rtraining/r_intro.html

Should you want to book a place on this course or have any questions
please contact training at mango-solutions.com


Cheers,

Romain Francois

--
Mango Solutions
Tel +44 (0)1249 467 467
Mob +44 (0)7813 526 123
Fax +44 (0)1249 467 468

data analysis that delivers


From wwwhsd at gmail.com  Wed Feb 21 18:14:21 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 21 Feb 2007 15:14:21 -0200
Subject: [R] file path
In-Reply-To: <2F01C2D1A2A44B41BD01DDE5F72E1CAB01A1F6CC@rngpew02.drn.ine.pt>
References: <2F01C2D1A2A44B41BD01DDE5F72E1CAB01A1F6CC@rngpew02.drn.ine.pt>
Message-ID: <da79af330702210914u15449ec7v56b64d2be785b701@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/e6799e38/attachment.pl 

From ggrothendieck at gmail.com  Wed Feb 21 18:51:42 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Feb 2007 12:51:42 -0500
Subject: [R] file path
In-Reply-To: <2F01C2D1A2A44B41BD01DDE5F72E1CAB01A1F6CC@rngpew02.drn.ine.pt>
References: <2F01C2D1A2A44B41BD01DDE5F72E1CAB01A1F6CC@rngpew02.drn.ine.pt>
Message-ID: <971536df0702210951w466ab240k43f0a3b2c31a3055@mail.gmail.com>

Its not clear from your post what the framework is that you are working with
but assuming that you have sourced a file and want its name place this

   fn <- parent.frame(2)$ofile
   # other code

in a file a.R, say, and from within R source it:

  source("a.R")

This is not very safe and could easily have to be changed from one version
of R to the next if source() is changed since it makes use of internals
within source().

On 2/21/07, Rita Sousa <rita.sousa at ine.pt> wrote:
> Hello,
>
>
>
> It is possible to return the path of the current working R-file (in
> execution)?
>
>
>
> Thanks,
>
> Rita Sousa.
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From neuro3000 at hotmail.com  Wed Feb 21 18:58:59 2007
From: neuro3000 at hotmail.com (=?iso-8859-1?Q?Neuro_LeSuperH=E9ros?=)
Date: Wed, 21 Feb 2007 12:58:59 -0500
Subject: [R] Examples on how to READ/WRITE to database using R-Project
Message-ID: <BAY131-W168E86D8D34A29DF007F35AF880@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/c2a431d1/attachment.pl 

From jg_liao at yahoo.com  Wed Feb 21 19:38:02 2007
From: jg_liao at yahoo.com (Jason Liao)
Date: Wed, 21 Feb 2007 10:38:02 -0800 (PST)
Subject: [R] how much performance penalty does this incur,
	scalar as a vector of one element?
Message-ID: <748347.87140.qm@web53702.mail.yahoo.com>


 I have been comparing R with other languages and systems. One peculiar feature of R is there is no scalar. Instead, it is just a vector of length one. I wondered how much performance penalty this deign cause, particular in situations with many scalars in a program. Thanks.

 

Jason Liao, http://www.geocities.com/jg_liao    
Associate Professor of Biostatistics
Drexel University School of Public Health
245 N. 15th Street, Mail Stop 660
Philadelphia, PA 19102-1192
phone 215-762-3934




 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From rhurlin at gwdg.de  Wed Feb 21 19:47:35 2007
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Wed, 21 Feb 2007 19:47:35 +0100
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <Pine.LNX.4.64.0702211539350.11585@gannet.stats.ox.ac.uk>
References: <307b90470702210617l66ba714ao8d8ee8cd2ece0b8f@mail.gmail.com>
	<Pine.LNX.4.64.0702211539350.11585@gannet.stats.ox.ac.uk>
Message-ID: <45DC93C7.1000101@gwdg.de>

Just another note:

rgl_0.70-2 works also for R-2.5.0 on FreeBSD 7.0-CURRENT (amd64) and for 
R-2.4.1 on (i386).

Rainer


Prof Brian Ripley wrote:
> This looks to me like a problem in your OpenGL, I am afraid.
> 
> I've made available a (completely unofficial) revised tarball at
> 
> http://www.stats.ox.ac.uk/pub/R/rgl_0.70-2.tar.gz
> 
> that attempts to work around various configure issues.  Not only does it 
> assume a vanilla make, it also tests if the various headers and 
> libraries are actually present (without assuming they are in .../lib, 
> which they are not on my main system).  So the enquiry with originally 
> started this thread will get a clear error from configure about what is 
> wrong.
> 
> Duncan: I have left in -Iext in PKG_CPPFLAGS, but AFAICS it is only 
> needed on Windows.  We've lost the ability to use a static libpng unless 
> we have libpng-config:  that is I am afraid inevitable as there is no 
> way to find where it might be, and you can't just grab a object of the 
> right name on a multi-architecture system.  Actually, I think it is 
> fundamentally broken as only on a few systems would a static library be 
> PIC (and as i386 Linux with recent gcc is one of those, people tend to 
> forget that).
> 
> Brian Ripley


From alobilby at hotmail.com  Wed Feb 21 19:20:43 2007
From: alobilby at hotmail.com (=?iso-8859-1?B?YWluaG9huiBsZXphbWE=?=)
Date: Wed, 21 Feb 2007 18:20:43 +0000
Subject: [R] How to get the equation of a graph after i have plotted the
	datas?
Message-ID: <BAY103-F2223F52F101FF8E1FC340AD1880@phx.gbl>



Hello!

I have a doubt and i need a quick answer please!!
I need to know if its possible to get in R the mathematical equation of a 
graph that you have plotted. I mean i know the y and x values, but i want 
the equation that relate them and that allow me to get the graph.
This option is possible in Excel, but i have  too many datas and i cant 
plotted them on it.
If its possible in R, how?


Thanks!!

_________________________________________________________________
Descubre la descarga digital con MSN Music. M?s de un mill?n de canciones.


From j.van_den_hoff at fzd.de  Wed Feb 21 19:20:12 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Wed, 21 Feb 2007 19:20:12 +0100
Subject: [R] Confindence interval for Levenberg-Marquardt fit
In-Reply-To: <40e66e0b0702210741i738e9723pd3fb2a43897252f5@mail.gmail.com>
References: <45DC254B.6080601@cebitec.uni-bielefeld.de>
	<Pine.LNX.4.64.0702211104570.23374@gannet.stats.ox.ac.uk>
	<20070221121056.GA25281@marco.fz-rossendorf.de>
	<40e66e0b0702210741i738e9723pd3fb2a43897252f5@mail.gmail.com>
Message-ID: <45DC8D5C.3000802@fzd.de>

On Wed, Feb 21, 2007 at 09:41:29AM -0600, Douglas Bates wrote:
> On 2/21/07, joerg van den hoff <j.van_den_hoff at fzd.de> wrote:
> >On Wed, Feb 21, 2007 at 11:09:52AM +0000, Prof Brian Ripley wrote:
> >> Well, the algorithm used does not affect the confidence interval 
> >(provided
> >> it works correctly), but what is nls.ml (presumably in some package you
> >> have not mentioned) and why would I want to use an old-fashioned
> >> algorithm?
> 
> >is'nt this a bit strong? in what respect do you consider 
> >levenberg-marquardt
> >(going back to the 19-forties, I think) as old-fashioned (especially
> >in comparsion to the `nls' standard gauss-newton approach (both gentlemen
> >seem to have done their major work a bit longer ago :-))?
> >AFAICS levenberg-marquardt is generally appreciated for it's rapid
> >convergence achieved by a smooth transition from an
> >inverse-hessian approach to steepest descent. my own experience
> >with non-linear least squares minimization using this algorithm
> >are positive as well, but
> >I have not tried out the levenberg-marquardt
> >implementation in package `minpack.lm' (which originates from netlib.org)
> >and don't know if it's good. but in any case there sure are implementations
> >around (e.g. in the CERN MINUIT library) which have proven to be
> >of high quality.
> 
> The nls function provides implementations of the Gauss-Newton
> algorithm and the Golub-Pereyra algorithm and the NL2SOL algorithm.
> Check the possible values of the optional argument "algorithm" to
> nls().
I'm using `nls' rather frequently and I'm aware of this. maybe the formulation
"standard gauss-newton approach" was misleading (please attribute this to the
fact that I'm not a native speaker: I actually meant the default algorithm used
and was not implying that `nls' can only use some odd "standard"
procedure)
> 
> If you will allow me to wax philosophical for a moment, I submit that
> there are three stages to any iterative optimization:
>  - formulating initial values of the parameters
>  - determining what step to take from iteration i to iteration i+1
>  - determining when to declare convergence
> 
> For the most part optimization research focuses on the second step.
> The first and third steps are also important.  One of the advantages
> of the implementation of the Gauss-Newton and Golub-Pereyra algorithms
> in nls is that they use a convergence criterion (based only on the
> current parameter values) not a termination criterion (based on the
> changes in the parameter values or the changes in the objective
> function or both).  See chapter 2 of Bates and Watts, "Nonlinear
> Regression Analysis and Its Applications" (Wiley, 1988) for a
> discussion of the particular convergence criterion used.
I did this when starting to use `nls' seriously. being a physicist,
not a mathematician, my attitude is nevertheless a bit more pragmatic
(or simple-minded, if you like): I fully appreciate that these
things need a thorough and rigorous mathematical foundation but any
convergence criterion suits me just fine as long as (1) convergence
is actually achieved (i.e. the algorithm reports some results...)
and (2) the final least squares sum is sufficiently near the
achievable minimum value for that model (sufficiently near meaning
that the parameter estimation errors are much larger than any further
changes in the parameters when wandering around any longer near the
current point in parameter space -- possibly finding a slightly
"better mininmum") and (3) the results are obtained in sufficiently
short time.  It's not obivous to me (I did'nt encounter an
example case up to now), for instance, that a good termination
criterion is either more unreliable (yielding wrong answers way off
the true minimum) or less efficient (e.g. driving
the iteration count up) than the convergence criterion used in `nls'
(against which I do not have any objections -- apart from the minor
handicap that it can't converge on 'ideal' data which by and then
comes in the way as the help list shows)

an impression which I cannot prove by hard data (so it may be wrong) is,
that the `nls' gauss-newton approach tends to be more sensitive to
suboptimal start values (sending it towards outer space in the worst
case...) than levenberg-marquardt driven approaches and definitely
has on average a higher iteration count.

> 
> Another point that is often lost in a discussion of various
> optimization algorithms is that one does not compare algorithms - one
> compares implementations of algorithms which, at the very least, must
> involve all of the above steps.
sure
> 
> For many people in the nonlinear least squares field the NL2SOL
> algorithm by Dennis, Gay and Welch was considered the "gold standard"
> and David Gay's implementation is now available in nls.  This allows
> those who are interested to compare implementations of these
> algorithms.  Also, with R being an Open Source system anyone can add
> their own implementation or modify an existing implementation to try
> things out.
for one, I already stressed that I appreciate `nls'. it
is, for me, one of the most useful things in R. your second point
is true in principle, of course, but question is how many people
will dare and try to fiddle with the nls code (which seems rather
hermetic to the non-initiate). I'd love to see levenberg-marquardt among
the available alorithms but I'm far from sure whether I could manage
to integrate it adequately into `nls' myself (the author of the
minpack package seems to have had the same problem choosing to
implement it separately).

and while I'm at it: another thing for the wish-list (I think you
put it somewhere at position 10^(some_small_integer_larger_than_1)
on your TODO list a couple years ago :-)) would be to get access
to the parameter search path in parameter space (essentially the
trace output) as a part of the returned `nls'-object (maybe filling
that component only if trace = TRUE?). As a quick work around I
only managed to copy getPars() to a global variable by modifying
`nlsModel' accordingly but found no easy clean solution.

> 
> 
> 
> 
> >`nls' sure is a _very_ valuable function, but not necessarily the
> >"last word" with regards to the chosen algorithm(s).
> >
> >
> >>
> >> You could start nls at the solution you got from nls.ml and use confint()
> >> on that.
> >maybe one should look at profile.nls and confint.nls and see what 
> >information
> >of the usual `nls' object is actually used for the confidence intervall
> >computation and mimick this for the `nls.lm' output? at a (admittedly)
> >quick glance it seems that only parameters, std.errs. and the 
> >fitted/residual
> >values are needed which should all be provided by nls.lm as well.
> >maybe one could even try to map the nls.lm results into a structure of 
> >class
> >`nls' (although this would not be a clean solution, probably) in order
> >to use `confint.nls'?
> 
> Not really.  The profile.nls function requires, not surprisingly, the
> ability to profile the objective function.  (See the discussion of the
> profile sum of squares and the profile-t transformations in chapter 6
> of Bates and Watts).  Such a capability would need to be added to a
> nonlinear least squares implementation before methods for those
> generics could be developed.
I see.
> 
> >>
> >> On Wed, 21 Feb 2007, Michael Dondrup wrote:
> >>
> >> > Dear all,
> >> > I would like to use the Levenberg-Marquardt algorithm for non-linear
> >> > least-squares regression using function nls.lm. Can anybody help  me to
> >> >   find a a way to compute confidence intervals on the  fitted
> >> > parameters as it is possible for nls (using confint.nls, which does not
> >> > work for nls.lm)?
> >> >
> >> > Thank you for your help
> >> > Michael
> >>
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide 
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >


From marc_schwartz at comcast.net  Wed Feb 21 20:31:56 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 21 Feb 2007 13:31:56 -0600
Subject: [R] How to get the equation of a graph after i have plotted	the
	datas?
In-Reply-To: <BAY103-F2223F52F101FF8E1FC340AD1880@phx.gbl>
References: <BAY103-F2223F52F101FF8E1FC340AD1880@phx.gbl>
Message-ID: <1172086316.4769.28.camel@localhost.localdomain>

On Wed, 2007-02-21 at 18:20 +0000, ainhoa? lezama wrote:
> 
> Hello!
> 
> I have a doubt and i need a quick answer please!!
> I need to know if its possible to get in R the mathematical equation of a 
> graph that you have plotted. I mean i know the y and x values, but i want 
> the equation that relate them and that allow me to get the graph.
> This option is possible in Excel, but i have  too many datas and i cant 
> plotted them on it.
> If its possible in R, how?
> 
> 
> Thanks!!

Presuming that you are referring to a simple linear regression, using
some example data:

  # Create some random data
  # set the random seed first
  set.seed(1)
  x <- rnorm(15)
  y <- x + rnorm(15)

  # Create a linear model
  LM <- lm(y ~ x)

  # Plot the data
  plot(x, y)

  # Draw the fitted line, passing the model
  # object as an argument
  abline(LM)


  # Now get the coefficients for the model
  > coef(LM)
  (Intercept)           x 
   0.05510098  1.08897520 


See ?abline, ?lm, ?coef and ?summary.lm for more information.

You would find value in reading "An Introduction to R", which is
available with your R installation or from the R web site under
"Manuals" as the above is covered there. Also the Posting Guide, for
which there is a link at the bottom of all e-mails coming from the R
lists.


If you need something a bit more, such as general curve fitting, using 

  RSiteSearch("curve fitting")

will get you many hits in the list archives and pointers to a variety of
functions.


HTH,

Marc Schwartz


From jebyrnes at ucdavis.edu  Wed Feb 21 20:37:51 2007
From: jebyrnes at ucdavis.edu (Jarrett Byrnes)
Date: Wed, 21 Feb 2007 11:37:51 -0800
Subject: [R] glms with poisson and negative binomial errors
Message-ID: <E01E15B9-A5B7-4128-B54A-8724DB6C8F1A@ucdavis.edu>

A reviewer recently remarked to me that, due to my data being  
constrained to not fall below zero, a generalized linear model with a  
negative binomial error (or poisson) with a log link would be more  
appropriate for fitting my model.  I ran it in R with glm.nb() and  
got results that matched just using lm on log transformed data pretty  
well.  However, R indicated some warnings.  I checked warnings(), and  
saw a list of warnings as follows:

Warning messages:
1: non-integer x = 0.254825

I got the same error when trying to use the poisson family.

My data is indeed continuous, not discrete (lots of non-integers).

Does this mean that the model was not fit properly?  Was data dropped  
when fitting the model?  Is there an option to deal with this that I  
have overlooked?  It would seem all is in order, but i just wanted to  
make sure.  Thanks!

Thanks.

-Jarrett


From m.bridgman at sbcglobal.net  Wed Feb 21 20:49:15 2007
From: m.bridgman at sbcglobal.net (Matthew Bridgman)
Date: Wed, 21 Feb 2007 11:49:15 -0800
Subject: [R] choose.files() on a mac
Message-ID: <576DF584-3F96-414E-B19F-F3F436005CDF@sbcglobal.net>

the function 'choose.files()' won't work on my mac version of R
Does anyone know of a comparable function?

Thanks,
   Matt


From sue at xlsolutions-corp.com  Wed Feb 21 20:39:57 2007
From: sue at xlsolutions-corp.com (Sue Turner)
Date: Wed, 21 Feb 2007 12:39:57 -0700
Subject: [R] Course*** R/S+: Fundamentals and Programming Techniques -
	Princeton, March 1-2
Message-ID: <20070221123957.9f08cc34deb45d78e54b3b5664e21546.fef621ed3e.wbe@email.secureserver.net>


XLSolutions Corporation is proud to announce our March 2007 R/S:
Fundamentals and Programming Techniques - in  Princeton March 1-2, 2007
:  http://www.xlsolutions-corp.com/Rfund.htm


This two-day beginner to intermediate R/S-plus course focuses on a broad
spectrum of topics, from reading raw data to a comparison of R and S. We
will learn the essentials of data manipulation, graphical visualization
and R/S-plus programming. We will explore statistical data analysis
tools,including graphics with data sets. How to enhance your plots,
build your own packages (librairies) and connect via ODBC,etc. The
course will give beginners a strong foundation for becoming a versatile
programmer, and will expose experienced users to skills that make a
better programmer.
http://www.xlsolutions-corp.com/Rfund.htm

Other courses:
(1) R/S System: Advanced Programming - San Francisco, March 15-16, 2007
(2) Data Mining: Practical Tools and Techniques in R/Splus - Salt Lake
City, March 26-27, 2007

(3) Statistics II: Regression Modeling Strategies in R/Splus, New York
City - March 8-9, 2007
(4) Microarrays Data Analysis with R/S+ and GGobi, Chicago - March 5-6,
2007, Atlanta - March 8-9, 2007, -  San Diego, March 15-16, 2007

Ask for group discount and reserve your seat Now - Earlybird Rates.
Please email us for for April-May courses.
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com

(1) R/S System: Advanced Programming - San Francisco, March 15-16, 2007

This advanced course is designed for people who use R or S-Plus in their
day-to-day work and want to maximize the efficiency of their programs.
Participants will learn in depth advanced programming techniques that
are available in R and S-Plus. This course will improve your general
strategies and extend your programming skills. This two-day course will
introduce participants to many programming techniques and tools. In
addition a special session dedicated to making S-Plus functions more
efficient will focus on "fast objects" and "fast functions". The
advanced programming techniques include object orientation, classes,
inheritance and methods. http://www.xlsolutions-corp.com/Radv.htm

(2) Data Mining: Practical Tools and Techniques in R/Splus - Salt Lake
City, March 26-27, 2007

This course gives students an understanding of R/Splus tools used to
investigate the main tasks that predictive analytics and exploratory
data mining is usually called upon to accomplish and data preparation
which is universally held as the key to successful data mining. We
focus on the most common data mining tasks which are: Description,
Estimation, Prediction, Classification, Clustering, Association and the
need for Dimension Reduction with Principal Components and Factor
Analysis. Analytical methods used in the class include decision trees,
logistic regression, neural networks, link analysis (social networks)
and Kernel-based Methods (SVMs).
http://www.xlsolutions-corp.com/RSMining.htm


(3) S-PLUS / R: Fundamentals and Programming Techniques - San Francisco,
March 12-13, 2007 - Washington DC, March 5-6, 2007 - Princeton March
1-2, 2007 -  San Diego, March 15-16, 2007

This two-day beginner to intermediate R/S-plus course focuses on a broad
spectrum of topics, from reading raw data to a comparison of R and S. We
will learn the essentials of data manipulation, graphical visualization
and R/S-plus programming. We will explore statistical data analysis
tools,including graphics with data sets. How to enhance your plots,
build your own packages (librairies) and connect via ODBC,etc. The
course will give beginners a strong foundation for becoming a versatile
programmer, and will expose experienced users to skills that make a
better programmer.
http://www.xlsolutions-corp.com/Rfund.htm


(4) Statistics II: Regression Modeling Strategies in R/Splus, New York
City - March 8-9, 2007

This two-day course is designed for persons interested in multivariable
regression analysis of univariate responses, in developing, validating,
and graphically describing multivariable predictive models. The first
part of the course presents the following elements of multivariable
predictive modeling for a single response variable: using regression
splines to relax linearity assumptions, perils of variable selection
and overfitting, where to spend degrees of freedom, shrinkage,
imputation of missing data, data reduction, and interaction surfaces.
http://www.xlsolutions-corp.com/Rstats2.htm


(5) Microarrays Data Analysis with R/S+ and GGobi, Chicago - March 5-6,
2007, Atlanta - March 8-9, 2007, -  San Diego, March 15-16, 2007

This two-day beginner to intermediate course is designed for people
involved in microarrays data analysis. Newly developed analysis methods
for microarrays data analysis are often available from open-source
(R,Bioconductor, etc) and can also be used in S+. Thanks to the great
flexibility of R and S languages these methods and tools can be easily
adapted to own data. In this course, we'll review R/S packages
(librairies) for microarray analysis. We'll also explore GGobi
interactive and dynamic graphics for microarrays analysis, statistical
learning methods and strategies for large data.
http://www.xlsolutions-corp.com/Rarrays.htm

Email us for group discounts: sue at xlsolutions-corp.com
Phone:  206 686 1578
 
Visit us: www.xlsolutions-corp.com/training.htm
 
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat!
 
Cheers,
 
Elvis Miller, PhD
Manager Training
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com/training.htm
elvis at xlsolutions-corp.com


From therneau at mayo.edu  Wed Feb 21 20:43:13 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Wed, 21 Feb 2007 13:43:13 -0600 (CST)
Subject: [R] Coxph and ordered factors
Message-ID: <200702211943.l1LJhDi29310@hsrnfs-101.mayo.edu>

 No, coxph does not have any special code to deal with ordered factors.
 You can do it "by hand" by using the pool-adjacent-violators algorithm
    1. Fit the data with stage as a categorical
    2. Verify that the coefficients are ordered
       if so, you are done
       if not, assume that they were
       	     0 = stage 1  (implicit coef)
       	     .1= stage 2
       	     .5 = stage 3
       	     .4 = stage 4
       	  Create a new class variable that combines stages 3 and 4, and
       	  return to step 1.
       	  
       	  
       	  Terry Therneau


From maitra at iastate.edu  Wed Feb 21 20:44:27 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 21 Feb 2007 13:44:27 -0600
Subject: [R] simple question on one-sided p-values for coef() on output of
	lm()
Message-ID: <20070221134427.0e4e52e6@subarnarekha.stat.iastate.edu>

Dear list,

I was wondering if it is possible to get the p-values for one-sided tests on the parameters of a linear regression. 

For instance, I use lm() and store the result in an object. lm() gives me a matrix, using summary() and coef() on which gives me a matrix containing the coefficients, the standard errors, the t-statistics and the two-sided p-values by default. Can I get it to provide me with one-sided p-values (something like alternative less than or greater than)?

Many thanks and best wishes,
Ranjan


From marc_schwartz at comcast.net  Wed Feb 21 20:56:43 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 21 Feb 2007 13:56:43 -0600
Subject: [R] choose.files() on a mac
In-Reply-To: <576DF584-3F96-414E-B19F-F3F436005CDF@sbcglobal.net>
References: <576DF584-3F96-414E-B19F-F3F436005CDF@sbcglobal.net>
Message-ID: <1172087803.4769.32.camel@localhost.localdomain>

On Wed, 2007-02-21 at 11:49 -0800, Matthew Bridgman wrote:
> the function 'choose.files()' won't work on my mac version of R
> Does anyone know of a comparable function?
> 
> Thanks,
>    Matt

choose.files() is Windows specific.

Look at ?file.choose which should be available otherwise.

HTH,

Marc Schwartz


From edd at debian.org  Wed Feb 21 20:57:47 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 21 Feb 2007 13:57:47 -0600
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <45DC93C7.1000101@gwdg.de>
References: <307b90470702210617l66ba714ao8d8ee8cd2ece0b8f@mail.gmail.com>
	<Pine.LNX.4.64.0702211539350.11585@gannet.stats.ox.ac.uk>
	<45DC93C7.1000101@gwdg.de>
Message-ID: <20070221195747.GA30898@eddelbuettel.com>

FWIW, Debian had working rgl packages since March 2004 -- version 0.64.

So I respectfully disagree with the general state of despair regarding
the source package.  I had my build issues at times, but Daniel and
Duncan worked hard and diligently to overcome these. 

Hence, "apt-get install r-cran-rgl" may be all it takes to play with
rgl, at least on Debian and its derivatives such as Ubuntu et al

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ripley at stats.ox.ac.uk  Wed Feb 21 21:23:55 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Feb 2007 20:23:55 +0000 (GMT)
Subject: [R] simple question on one-sided p-values for coef() on output
 of lm()
In-Reply-To: <20070221134427.0e4e52e6@subarnarekha.stat.iastate.edu>
References: <20070221134427.0e4e52e6@subarnarekha.stat.iastate.edu>
Message-ID: <Pine.LNX.4.64.0702212017330.9093@gannet.stats.ox.ac.uk>

On Wed, 21 Feb 2007, Ranjan Maitra wrote:

> I was wondering if it is possible to get the p-values for one-sided 
> tests on the parameters of a linear regression.
>
> For instance, I use lm() and store the result in an object. lm() gives 
> me a matrix, using summary() and coef() on which gives me a matrix 
> containing the coefficients, the standard errors, the t-statistics and 
> the two-sided p-values by default. Can I get it to provide me with 
> one-sided p-values (something like alternative less than or greater 
> than)?

Not 'it', but you can easily do the calculation yourself from the output.
E.g.

example(lm)
s <- summary(lm.D90)
pt(coef(s)[, 2], s$df[2], lower=FALSE) # or TRUE

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maitra at iastate.edu  Wed Feb 21 21:26:18 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 21 Feb 2007 14:26:18 -0600
Subject: [R] simple question on one-sided p-values for coef() on output
 of lm()
In-Reply-To: <Pine.LNX.4.64.0702212017330.9093@gannet.stats.ox.ac.uk>
References: <20070221134427.0e4e52e6@subarnarekha.stat.iastate.edu>
	<Pine.LNX.4.64.0702212017330.9093@gannet.stats.ox.ac.uk>
Message-ID: <20070221142618.294fd921@subarnarekha.stat.iastate.edu>

Yes, of course! Thank you. So, I guess the answer is that R itself can not be made to do so directly.

Many thanks for confirming this.

Sincerely,
Ranjan

On Wed, 21 Feb 2007 20:23:55 +0000 (GMT) Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Wed, 21 Feb 2007, Ranjan Maitra wrote:
> 
> > I was wondering if it is possible to get the p-values for one-sided 
> > tests on the parameters of a linear regression.
> >
> > For instance, I use lm() and store the result in an object. lm() gives 
> > me a matrix, using summary() and coef() on which gives me a matrix 
> > containing the coefficients, the standard errors, the t-statistics and 
> > the two-sided p-values by default. Can I get it to provide me with 
> > one-sided p-values (something like alternative less than or greater 
> > than)?
> 
> Not 'it', but you can easily do the calculation yourself from the output.
> E.g.
> 
> example(lm)
> s <- summary(lm.D90)
> pt(coef(s)[, 2], s$df[2], lower=FALSE) # or TRUE
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From tim_b_mcdonald at yahoo.com  Wed Feb 21 20:53:44 2007
From: tim_b_mcdonald at yahoo.com (Tim McDonald)
Date: Wed, 21 Feb 2007 11:53:44 -0800 (PST)
Subject: [R] Looking for info on  R Advanced programming in the West Coast
Message-ID: <20070221195344.38605.qmail@web58215.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/1559d84d/attachment.pl 

From tolga.uzuner at credit-suisse.com  Wed Feb 21 20:55:59 2007
From: tolga.uzuner at credit-suisse.com (Uzuner, Tolga)
Date: Wed, 21 Feb 2007 14:55:59 -0500
Subject: [R] R under Pocket PC
Message-ID: <59662ECD003A2F42B006C9168044A4FA109792BF@elon12p32002.csfp.co.uk>

Hi Folks,

Picking up on an old thread from 2004... has anyone managed to get a version of R runnning under Windows Mobile 5.0/Pocket PC ?

best,
Tolga



==============================================================================
Please access the attached hyperlink for an important electr...{{dropped}}


From Dimitris.Rizopoulos at med.kuleuven.be  Wed Feb 21 21:44:04 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 21 Feb 2007 21:44:04 +0100
Subject: [R] simple question on one-sided p-values for coef() on
	output	of lm()
In-Reply-To: <20070221142618.294fd921@subarnarekha.stat.iastate.edu>
References: <20070221134427.0e4e52e6@subarnarekha.stat.iastate.edu>
	<Pine.LNX.4.64.0702212017330.9093@gannet.stats.ox.ac.uk>
	<20070221142618.294fd921@subarnarekha.stat.iastate.edu>
Message-ID: <20070221214404.wfwnujkp2nm04k4o@webmail5.kuleuven.be>

Quoting Ranjan Maitra <maitra at iastate.edu>:

> Yes, of course! Thank you. So, I guess the answer is that R itself   
> can not be made to do so directly.
>
> Many thanks for confirming this.
>
> Sincerely,
> Ranjan
>
> On Wed, 21 Feb 2007 20:23:55 +0000 (GMT) Prof Brian Ripley   
> <ripley at stats.ox.ac.uk> wrote:
>
>> On Wed, 21 Feb 2007, Ranjan Maitra wrote:
>>
>> > I was wondering if it is possible to get the p-values for one-sided
>> > tests on the parameters of a linear regression.
>> >
>> > For instance, I use lm() and store the result in an object. lm() gives
>> > me a matrix, using summary() and coef() on which gives me a matrix
>> > containing the coefficients, the standard errors, the t-statistics and
>> > the two-sided p-values by default. Can I get it to provide me with
>> > one-sided p-values (something like alternative less than or greater
>> > than)?
>>
>> Not 'it', but you can easily do the calculation yourself from the output.
>> E.g.
>>
>> example(lm)
>> s <- summary(lm.D90)
>> pt(coef(s)[, 2], s$df[2], lower=FALSE) # or TRUE

I think it should be

pt(coef(s)[, 3], s$df[2], lower=FALSE) # or TRUE
              ^

Best,
Dimitris


>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From zelickr at pdx.edu  Wed Feb 21 21:52:27 2007
From: zelickr at pdx.edu (Randy Zelick)
Date: Wed, 21 Feb 2007 12:52:27 -0800 (PST)
Subject: [R] linux gplots install unhappy
In-Reply-To: <1171996074.4804.13.camel@localhost.localdomain>
References: <Pine.GSO.4.62.0702200955400.17291@freke.odin.pdx.edu>
	<1171996074.4804.13.camel@localhost.localdomain>
Message-ID: <Pine.GSO.4.62.0702211248040.17368@freke.odin.pdx.edu>


Thanks to the many folks who responded, and apologies for being so dense!

The different behavior between windows and linux relative to install 
defaults tripped me up. I had not thought about the dependencies switch.

All working fine now.

Cheers,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201


From rhurlin at gwdg.de  Wed Feb 21 22:02:10 2007
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Wed, 21 Feb 2007 22:02:10 +0100
Subject: [R] Installing Package rgl - Compilation Fails - FreeBSD
In-Reply-To: <20070221195747.GA30898@eddelbuettel.com>
References: <307b90470702210617l66ba714ao8d8ee8cd2ece0b8f@mail.gmail.com>	<Pine.LNX.4.64.0702211539350.11585@gannet.stats.ox.ac.uk>	<45DC93C7.1000101@gwdg.de>
	<20070221195747.GA30898@eddelbuettel.com>
Message-ID: <45DCB352.6080001@gwdg.de>

Dirk,

I agree with you. My statements were only related to rgl on FreeBSD. 
With the changes from Brian since yesterday I am finally able to compile 
and work with this package on all our FreeBSD scientific desktops.

At the same time I am very conscious about the great job of Daniel and 
Duncan. Without there work we would not have rgl :-)

Rainer


Dirk Eddelbuettel schrieb:
> FWIW, Debian had working rgl packages since March 2004 -- version 0.64.
> 
> So I respectfully disagree with the general state of despair regarding
> the source package.  I had my build issues at times, but Daniel and
> Duncan worked hard and diligently to overcome these. 
> 
> Hence, "apt-get install r-cran-rgl" may be all it takes to play with
> rgl, at least on Debian and its derivatives such as Ubuntu et al
> 
> Dirk
>


From jafarikia at gmail.com  Wed Feb 21 23:04:10 2007
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Wed, 21 Feb 2007 17:04:10 -0500
Subject: [R] Chi-Square test
Message-ID: <e3f2a5ab0702211404l2ddaa037ye11873e1143c641@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/ccfb0a0a/attachment.pl 

From alpatici at gmail.com  Wed Feb 21 23:08:24 2007
From: alpatici at gmail.com (Alp ATICI)
Date: Wed, 21 Feb 2007 16:08:24 -0600
Subject: [R] Data frame operations getting slower when accessed by index
Message-ID: <2fd8a4280702211408w5ac744b7y3d1595813244d64f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/6799f9af/attachment.pl 

From drf5n at maplepark.com  Wed Feb 21 23:24:09 2007
From: drf5n at maplepark.com (David Forrest)
Date: Wed, 21 Feb 2007 16:24:09 -0600 (CST)
Subject: [R] color quantization / binning a variable into levels
In-Reply-To: <20060222100946.2d55a8ab.azzalini@stat.unipd.it>
References: <Pine.LNX.4.58.0602211020160.16984@maplepark.com>
	<20060222100946.2d55a8ab.azzalini@stat.unipd.it>
Message-ID: <Pine.LNX.4.64.0702211520390.11643@maplepark.com>

My apologies for the tremendously tardy reply, but I ran into a 
similar problem and discovered findInterval and colorRampPallet 
as an alternative and though it might be worthy of a RSiteSearch() result 
for someone.

?findInterval
?colorRampPalette

jet.colors <-
        colorRampPalette(c("#00007F", "blue", "#007FFF", "cyan",
                           "#7FFF7F", "yellow", "#FF7F00", "red", 
"#7F0000"))

c.min<-range(volcano)[1]
c.max<-range(volcano)[2]
c.n<-200
c.brks<-seq(c.min,c.max,length=c.n)
c.ramp<-jet.colors(c.n)
c.ramp[findInterval(c.min,c.brks)]<-NA  #  make the first one transparent

# hack levelplot() or image()
plot(expand.grid(1:87,1:61),
   col=c.ramp[findInterval(volcano,c.brks,all.inside=TRUE)],
   pch='.',cex=10)

Dave

On Wed, 22 Feb 2006, Adelchi Azzalini wrote:

> On Tue, 21 Feb 2006 11:08:38 -0600 (CST), David Forrest wrote:
>
> perhaps "binning" of package "sm" is what you want
>
> best wishes,
> Adelchi Azzalini
>
> DF> Hi all,
> DF>
> DF> I'd like to quantize a variable to map it into a limited set of
> DF> integers for use with a colormap.  "image" and filled.contour"  do
> DF> this mapping inside somewhere, but I'd like to choose the colors
> DF> for plotting a set of polygons.  Is there a pre-existing function
> DF> that does something like this well?  i.e., is capable of using
> DF> 'breaks'?
> DF>
> DF> quantize<-function(x,n=10, breaks=NULL){
> DF> # bin the variable x into n levels
> DF>   xmin<-min(x)
> DF>   xmax<-max(x)
> DF>   1+floor(n*(x-xmin)/(xmax-xmin)*.999)
> DF> }
> DF>
> DF> x<- -10:10
> DF> cbind(x,quantize(x,2),quantize(x),quantize(x,21))
> DF>
> DF> quantize(x,breaks=c(5,7))   #
> DF>
> DF> Thanks for your time,
> DF>
> DF> Dave
>
>

Dave
-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From Mark.Leeds at morganstanley.com  Wed Feb 21 23:38:16 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Wed, 21 Feb 2007 17:38:16 -0500
Subject: [R] Estimating a bivariate VAR(X) and using F-tests
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401521C34@NYWEXMB23.msad.ms.com>

I would like to estimate bivariate VAR(X) models where I don't know the
optimal lag length X and would also like to use
F-tests to determine the granger causality of each of the variables. I'm
aware of Achim's econometric packages description but I was wondering if
someone could recommend a specific R econometrics package that does
this. 

If it is recommended to use the sort of ideas that Bernard Pfaff gives
in his yellow cointegration book, then
that's fine also. Thanks a lot
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From chrysopa at gmail.com  Thu Feb 22 00:02:38 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Wed, 21 Feb 2007 21:02:38 -0200
Subject: [R] Problem with rjava in linux
Message-ID: <200702212102.38768.chrysopa@gmail.com>

Hi,

I install rJava in linux. The installation, from source, is ok. But I have 
this error on load package.

> library(rJava)
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared 
library '/usr/local/lib/R/site-library/rJava/libs/rJava.so':
  libjvm.so: cannot open shared object file: No such file or directory
Error: .onLoad failed in 'loadNamespace' for 'rJava'
Error: package/namespace load failed for 'rJava'

Look my env:

[root em mobilix ronaldo]# echo $JAVA_HOME
/usr/lib/jvm/java-1.5.0-sun-1.5.0.10

[root em mobilix ronaldo]# R CMD javareconf
Java interpreter : /usr/lib/jvm/java-1.5.0-sun-1.5.0.10/jre/bin/java
Java version     : 1.5.0_10
Java home path   : /usr/lib/jvm/java-1.5.0-sun-1.5.0.10
Java library path: $(JAVA_HOME)/jre/lib/i386/client:$(JAVA_HOME)/jre/lib/i386:
$(JAVA_HOME)/jre/../lib/i386
JNI linker 
flags : -L$(JAVA_HOME)/jre/lib/i386/client -L$(JAVA_HOME)/jre/lib/i386 -L$(JAVA_HOME)/jre/../lib/i386 -ljvm

Updating Java configuration in /etc/R
Done.

and the file exist

[root em mobilix ronaldo]# ls /usr/local/lib/R/site-library/rJava/libs/
rJava.so

The library libjvm.so also exist

[root em mobilix ronaldo]# locate libjvm.so 
/usr/lib/gcj-4.1/libjvm.so
/usr/lib/jvm/java-1.5.0-sun-1.5.0.10/jre/lib/i386/client/libjvm.so
/usr/lib/jvm/java-1.5.0-sun-1.5.0.10/jre/lib/i386/server/libjvm.so

Now I try to remove the /usr/lib/gcj-4.1/libjvm.so because is not a SUN 
package.

[root em mobilix ronaldo]# dpkg -S /usr/lib/gcj-4.1/libjvm.so
libgcj7-0: /usr/lib/gcj-4.1/libjvm.so
[root em mobilix ronaldo]# apt-get remove libgcj7-0 --purge

Now, I try to reinstall rjava, but now, the instalation dont work.

> install.packages('rJava',,'http://rforge.net',type='source')

h src/JRI.jar -d examples examples/rtest.java
Note: examples/rtest.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
make[1]: Leaving directory `/tmp/R.INSTALL.To6153/rJava/jri'
** R
** inst
** preparing package for lazy loading
Error in parse(n = -1, file = file) : input buffer overflow
Error: unable to load R code in package 'rJava'
Execution halted
ERROR: lazy loading failed for package 'rJava'
** Removing '/home/local/lib/R/site-library/rJava'

The downloaded packages are in
        /tmp/RtmpLjxk9t/downloaded_packages
Warning message:
installation of package 'rJava' had non-zero exit status in: 
install.packages("rJava", , "http://rforge.net", type = "source") 

My R version is R version 2.4.1 (2006-12-18)

How to make success on rJava instalation?

Thanks
Ronaldo
-- 
The last thing one knows in constructing a work is what to put first.
		-- Blaise Pascal
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | ronaldo.reis em unimontes.br | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From chrysopa at gmail.com  Thu Feb 22 00:09:36 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Wed, 21 Feb 2007 21:09:36 -0200
Subject: [R] Problem with rjava in linux Ignore
In-Reply-To: <200702212102.38768.chrysopa@gmail.com>
References: <200702212102.38768.chrysopa@gmail.com>
Message-ID: <200702212109.36780.chrysopa@gmail.com>

Ignore my message,

Now it work.

Thanks and sorry.

Ronaldo

-- 
Nunca tente ensinar um porco cantar. Voce perde seu tempo e ainda chateia o 
porco! 
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | ronaldo.reis em unimontes.br | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From ripley at stats.ox.ac.uk  Thu Feb 22 00:16:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Feb 2007 23:16:13 +0000 (GMT)
Subject: [R] Data frame operations getting slower when accessed by index
In-Reply-To: <2fd8a4280702211408w5ac744b7y3d1595813244d64f@mail.gmail.com>
References: <2fd8a4280702211408w5ac744b7y3d1595813244d64f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702212302050.12540@gannet.stats.ox.ac.uk>

What are D and M?   'Index' here could be a number or a name.
In either case, df[[D]] would be the equivalent of df$D.

However, your computation does not need a loop at all, let alone two.
Try something like

tmp <- with(df, paste(D, m))
dates <- unique(tmp)



On Wed, 21 Feb 2007, Alp ATICI wrote:

> I have a data frame called df which has about 100 columns but thousands of
> rows. I set D the index of df$D and M to be the index of df$M.
> When I run the following loop as it is vs. df[,D] and df[,M] replaced with
> df$D and df$M there is a real big time difference in completion (with the
> latter being significantly faster). Is there an easy reason why and how I
> could speed up access?
>
> for (m in 1:12) {
> for (d in 1:31) {
> filter1<-((df[,D]==d) & (df[,M]==m))
> if(sum(filter1)>0) {
> ctr<-ctr+1
> dates[ctr]<-paste(as.character(d),as.character(m))
> }
> }
> }
>
> Thanks...
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rbentosa at yahoo.com  Thu Feb 22 00:41:48 2007
From: rbentosa at yahoo.com (rose bentosa)
Date: Wed, 21 Feb 2007 15:41:48 -0800 (PST)
Subject: [R] loops in R help me please
Message-ID: <380729.50957.qm@web37205.mail.mud.yahoo.com>

I am trying to make the following Kalman filter equations work and therefore produce their graphs.
v_t=y_t - a_t
a_t+1=a_t+K_t*v_t
F_t=P_t+sigma.squared.epsilon
P_t+1=P_t*(1-K_t)+sigma.squared.eta
K_t=P_t/F_t

Given:
a_1=0,P_1=10^7,sigma.squared.epsilon=15099,
sigma.squared.eta=1469.1

I have attached my code,which of course doesnt work.It produces NAs for the Fs,Ks and the a.
Can somebody tell me please what am I doing wrong in this loop?
Why doesnt this loop work as it should be;to produce  plots of a declining filtered state variance(P_t),prediction errors(v_t),and a declining prediction variance(F_t)?
Also,how can I construct and plot the 90% confidence intervals around a_t?
Thanks for your help and look forward.
Leonard


 
---------------------------------
It's here! Your new message!
Get new email alerts with the free Yahoo! Toolbar.
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: kalmancodenotworking.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/21004e1e/attachment.txt 

From johnson4 at babel.ling.upenn.edu  Thu Feb 22 01:16:21 2007
From: johnson4 at babel.ling.upenn.edu (Daniel Ezra Johnson)
Date: Wed, 21 Feb 2007 16:16:21 -0800
Subject: [R] interacting factors in lmer [was: error using user-defined link
	function]
Message-ID: <D88A7B88-44D3-40E7-9318-F08877AB5AAD@ling.upenn.edu>

For what it's worth, I've been getting the exact same error message  
as in the previous thread (see http://tolstoy.newcastle.edu.au/R/e2/ 
help/07/02/10269.html etc.):

Error in if (any(sd < 0)) return("'sd' slot has negative entries") :
	missing value where TRUE/FALSE needed

as well as NaN's all through the model, by using lmer on a formula  
that includes a term of the type (A | B) where A and B are factors  
that aren't robustly crossed, i.e. there are some empty cells in there.

I've been wondering in general about the relative merits and  
appropriateness of the following types of formula:

1) Y ~ A + B + A:B
2) Y ~ (1 | A) + (1 | B) + (1 | A:B)
3) Y ~ A + (A | B)
4) Y ~ (1 | A) + (A | B)

Type 1 is for fixed effects with interactions.

Type 2, logically, should be for random effects with interactions  
(except are random effects supposed to be independent, by some  
assumption of the model?)

Type 3 should be for a fixed effect interacting with a random effect.  
However, this is where I get the error message above, often,  
sometimes after 15 minutes of computer calculation...

Type 4, I'm thinking, is a variant of 2, but maybe it isn't  
legitimate since in (A | B) A is deemed to be a fixed effect?

I know type 1 is normally used and it even has a shorthand, A*B.
But when one or both effects is random, what is the best way to model  
interactions in an lmer formula?

And what is the best way to look for interactions between random  
factors when crossing them results in unavoidable empty cells?

Thanks,
Daniel


From dunn at usq.edu.au  Thu Feb 22 01:36:35 2007
From: dunn at usq.edu.au (Peter Dunn)
Date: Thu, 22 Feb 2007 10:36:35 +1000
Subject: [R] glms with poisson and negative binomial errors
In-Reply-To: <E01E15B9-A5B7-4128-B54A-8724DB6C8F1A@ucdavis.edu>
References: <E01E15B9-A5B7-4128-B54A-8724DB6C8F1A@ucdavis.edu>
Message-ID: <200702221036.36034.dunn@usq.edu.au>

If your data are continuous but must be positive,
consider a gamma or inverse gaussian glm.

If your data are non-negative (is positive but could
include zeros), consider glms based on Tweedie
distributions.  See, for example:
http://cran.ms.unimelb.edu.au/src/contrib/Descriptions/tweedie.html

Tweedie distributions have variances of the form 
	var[Y] = phi * mu^p
for real p not in the interval (0,1).  p=2 is the gamma case,
p=0 the normal case, p=1 and phi=1 the Poisson case.
But when 1 < p < 2, the distribution are defined on the 
non-negative reals (and in fact correspond to a Poisson
sum of gamma distributions).

P.

On Thursday 22 February 2007 05:37, Jarrett Byrnes wrote:
> A reviewer recently remarked to me that, due to my data being
> constrained to not fall below zero, a generalized linear model with a
> negative binomial error (or poisson) with a log link would be more
> appropriate for fitting my model.  I ran it in R with glm.nb() and
> got results that matched just using lm on log transformed data pretty
> well.  However, R indicated some warnings.  I checked warnings(), and
> saw a list of warnings as follows:
>
> Warning messages:
> 1: non-integer x = 0.254825
>
> I got the same error when trying to use the poisson family.
>
> My data is indeed continuous, not discrete (lots of non-integers).
>
> Does this mean that the model was not fit properly?  Was data dropped
> when fitting the model?  Is there an option to deal with this that I
> have overlooked?  It would seem all is in order, but i just wanted to
> make sure.  Thanks!
>
> Thanks.
>
> -Jarrett
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Dr Peter Dunn  |  dunn <at> usq.edu.au
Faculty of Sciences, USQ; http://www.sci.usq.edu.au/staff/dunn
Aust. Centre for Sustainable Catchments: www.usq.edu.au/acsc

This email (including any attached files) is confidential an...{{dropped}}


From jrkrideau at yahoo.ca  Thu Feb 22 01:37:01 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 21 Feb 2007 19:37:01 -0500 (EST)
Subject: [R] Trying to get an apply to work with a list in applying
	names totables
In-Reply-To: <2E9C414912813E4EB981326983E0A104029C322D@inboexch.inbo.be>
Message-ID: <20070222003701.94494.qmail@web32806.mail.mud.yahoo.com>

Thanks Thierry, it works perfectly. And your
explanation of what sapply was doing was really
helpful. 

Actually,  your solution  is showing me that whoever
input the data didn't stick to the coding manual by
that' another story.  


--- "ONKELINX, Thierry" <Thierry.ONKELINX at inbo.be>
wrote:

> John,
> 
> Two things. You don't need to pout the cc variable
> in the apply. Use
> instead something like this.
> 
> apply(cc, 2, fn1, y = mylist)
> 
> But this still doesn't solve your problem. You'll
> need to rewrite your
> function like this.
> 
> > fn2 <- function(x, y, i){
> +   tt <- table(x[, i])
> +   names(tt) <- y[[i]]
> +   return(tt)
> + }
> > sapply(1:ncol(cc), fn2, x = cc, y = mylist)
> [[1]]
> yes  no 
>   2   3 
> 
> [[2]]
> a b c d 
> 1 1 2 1 
> 
> Cheers,
> 
> Thierry
>
------------------------------------------------------------------------
> ----
> 
> ir. Thierry Onkelinx
> 
> Instituut voor natuur- en bosonderzoek / Reseach
> Institute for Nature
> and Forest
> 
> Cel biometrie, methodologie en kwaliteitszorg /
> Section biometrics,
> methodology and quality assurance
> 
> Gaverstraat 4
> 
> 9500 Geraardsbergen
> 
> Belgium
> 
> tel. + 32 54/436 185
> 
> Thierry.Onkelinx at inbo.be
> 
> www.inbo.be 
> 
>  
> 
> Do not put your faith in what statistics say until
> you have carefully
> considered what they do not say.  ~William W. Watt
> 
> A statistical analysis, properly conducted, is a
> delicate dissection of
> uncertainties, a surgery of suppositions.
> ~M.J.Moroney
> 
> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens
> John Kane
> Verzonden: woensdag 21 februari 2007 16:47
> Aan: R R-help
> Onderwerp: [R] Trying to get an apply to work with a
> list in applying
> names totables
> 
> I am trying to use apply and a  list to supply names
> to a set of tables I want to generate. Below is an
> example that I hope mimics the larger original
> problem.
> 
> EXAMPLE
> 
> aa <- c( 2,2,1,1,2)
> bb <- c(5,6,6,7,4)
> aan <- c("yes", "no")
> bbn <- c("a", "b", "c", "d")
> mynames <- c("abby", "billy")
> mylist <- list(aan, bbn);   names(mylist) <- mynames
> 
> cc <- data.frame(aa,bb)
> fn1 <- function(x,y) {tt <- table(x); names(tt)<-
> mylist[[y]]}
> jj <-apply(cc, 2, fn1(cc,mylist))
> 
> RESULT:  
> Error in fn1(cc, mylist) : invalid subscript type
> 
> To be honest I didn't expect it to work since that
> fin1(cc  looks recursive but oh well...
> 
> Can anyone offer a solution or some advice here.  It
> would be greatly appreciated
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From cincinattikid at bigpond.com  Thu Feb 22 02:33:08 2007
From: cincinattikid at bigpond.com (Alfonso Sammassimo)
Date: Thu, 22 Feb 2007 12:33:08 +1100
Subject: [R] Is there better alternative to this loop?
Message-ID: <004c01c75621$68c9beb0$0300a8c0@Vaio>

Dear List,

Thanks to those who helped with my enquiry a few days ago.

I have a another question on loops, in this case I am trying to print out 
the row of a data frame if the previous 3 values (daily values) in col5 are 
in descending order. I have this loop which works, but ask whether this can 
be done differently (without conventional loop) in R:

flag="T"
d= 3 # d represents previous down days
for(i in (d+1): 100)
{
for( j in (i-d):(i-1))
{
if(x[j,5]<x[i,5]){flag="F"}
}
if( flag == "T"){ print(x[i,1])}
flag="T";
}

Any help appreciated,

Regards,
Alf Sammassimo
Melbourne, Australia.


From jiho.han at yahoo.com  Thu Feb 22 04:18:50 2007
From: jiho.han at yahoo.com (jiho.han)
Date: Wed, 21 Feb 2007 19:18:50 -0800 (PST)
Subject: [R] Relative ranking by a factor
Message-ID: <9093712.post@talk.nabble.com>


Hi, R experts-

I am trying to create a function that returns a quintile (or decile) ranking
of a variable controlled by a factor. For example, suppose there are 10
people each in class A (1st graders) and class B (2nd graders). If you want
to calculate quintile rank for each class, how would you do that?

Right now, I create the following function and use it with "by" function.
This way works ok except that it's getting slow when the data.frame is
really really large. 

rank.group = function (x, ng) {
        y = quantile(x, seq(0,1,length.out=ng+1), na.rm=T)
        z = cut( x, y, include.lowest = T); z = as.numeric(z)
        return(z) }

Hope there's a faster way to rank the variable as in SAS.
Thanks for your help.



-- 
View this message in context: http://www.nabble.com/Relative-ranking-by-a-factor-tf3270737.html#a9093712
Sent from the R help mailing list archive at Nabble.com.


From neuro3000 at hotmail.com  Thu Feb 22 04:44:53 2007
From: neuro3000 at hotmail.com (=?iso-8859-1?Q?Neuro_LeSuperH=E9ros?=)
Date: Wed, 21 Feb 2007 22:44:53 -0500
Subject: [R] loops in R help me please
Message-ID: <BAY131-W116E3D67CFADC7A0B4BDFDAF8F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/ae1945b1/attachment.pl 

From yihsu.chen at ucmerced.edu  Thu Feb 22 04:52:17 2007
From: yihsu.chen at ucmerced.edu (YIHSU CHEN)
Date: Wed, 21 Feb 2007 19:52:17 -0800
Subject: [R] write fixed format
Message-ID: <45DD1371.4050900@ucmerced.edu>

Dear R users;

Is there a function in R that I can put "text" with proper alignments in 
a fixed format.  For instance, if I have three fields: A, B and C, where 
both A and C are text with 3 characters and left alignment; B is a 
numeric one with 2 decimals and 3 integer space digits.   How can I put 
the following row in a file? (note there is a space between a and 2, and 
after b.) (A=aaa, B=23.11 and C=bb)
"aaa 23.11bb "

Yours,

Yihsu Chen


From jeff.horner at vanderbilt.edu  Thu Feb 22 04:59:33 2007
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Wed, 21 Feb 2007 21:59:33 -0600
Subject: [R] write fixed format
In-Reply-To: <45DD1371.4050900@ucmerced.edu>
References: <45DD1371.4050900@ucmerced.edu>
Message-ID: <45DD1525.6050803@vanderbilt.edu>

YIHSU CHEN wrote:
> Dear R users;
> 
> Is there a function in R that I can put "text" with proper alignments in 
> a fixed format.  For instance, if I have three fields: A, B and C, where 
> both A and C are text with 3 characters and left alignment; B is a 
> numeric one with 2 decimals and 3 integer space digits.   How can I put 
> the following row in a file? (note there is a space between a and 2, and 
> after b.) (A=aaa, B=23.11 and C=bb)
> "aaa 23.11bb "

Use sprintf:

 > A="aaa"
 > B=23.11
 > C="bb"
 > sprintf("%s %.2f%s",A,B,C)
[1] "aaa 23.11bb"

Be sure to read the documentation for sprintf, as you'll want to fully 
understand left and right adjustment, and field width and precision.

Best,

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From jholtman at gmail.com  Thu Feb 22 05:53:21 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 21 Feb 2007 23:53:21 -0500
Subject: [R] Is there better alternative to this loop?
In-Reply-To: <004c01c75621$68c9beb0$0300a8c0@Vaio>
References: <004c01c75621$68c9beb0$0300a8c0@Vaio>
Message-ID: <644e1f320702212053n26acb7f0g4ec1ad279722e8b6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/7b0911ba/attachment.pl 

From skiadas at hanover.edu  Thu Feb 22 06:07:52 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 22 Feb 2007 00:07:52 -0500
Subject: [R] [MacOSX] Screencast for R and Sweave on TextMate
Message-ID: <C2388398-8F56-49AB-A2D6-B1185A6A7540@hanover.edu>

This will likely be of interest only (if at all) to MacOSX users. I  
use a particular editor called TextMate which I find particularly  
suitable for pretty much any task I have to do, from Ruby to LaTeX to  
R. Its R support is probably not quite up to par with ESS yet, but it  
is at a decent state I would say.

Anyway the reason of the message is that I have prepared a small  
screencast showing how R and Sweave look and feel like in TextMate,  
prompted by a thread in the MacOSX TeX mailing list. I thought it  
might be of interest to some people here also, so here is the link:

http://skiadas.dcostanet.net/afterthought/2007/02/21/r-and-sweave-in- 
textmate/

It's not much, and the sound is not all that great, but it mostly  
shows how things feel like.

Hoping this is not too off topic...

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College

PS: Yes, it is not a free product, though it is the best money I have  
ever spent. No, I am not the developer, and do not profit from it.  
But I am actively involved in its support for LaTeX, R and Sweave.


From RMan54 at cox.net  Thu Feb 22 06:08:39 2007
From: RMan54 at cox.net (Rene Braeckman)
Date: Wed, 21 Feb 2007 21:08:39 -0800
Subject: [R] Minor Tick Marks in Lattice
Message-ID: <002101c7563f$83d71db0$0900a8c0@rman>

I have a couple if issues with the code below.

1. as.Table=TRUE has no effect
2. the minor tick marks on top of the top panels are drawn in the strips and
not on the axes.

Any ideas what's wrong? There are probably better ways to add minor tick
marks...
Thanks for any help.
Rene

--------------------------------  
 
library(lattice)
Subj <- paste("Subj", rep(1:4,each=3))
Time <- rep(1:3,4) + 0.1
Conc <- (1:12) + 0.1
df <- data.frame(Subj,Time,Conc)
xScale <- 1:3
yScale <- list(1:3,4:6,7:9,10:12)
xMinorTicks <- seq(0.5,2.5,0.5)
 
xyplot(Conc ~ Time | Subj,
       data = df,
       layout = c(2,2),
       type="b",
       as.Table=TRUE,
       scales=list(
          x=list(at=xScale),
          y=list(at=yScale,relation="free"),
          alternating=3
       ),
       panel = function(x,y,...) {
          panel.abline(h=yScale[[panel.number()]], v=xScale,
col.line="gray")
          panel.xyplot(x,y,...)             
      }
)
 
# Add minor tick marks on bottom or top
for (coli in 1:2) {
  for (rowi in 1:2) {
    trellis.focus(name="panel", column=coli, row=rowi, clip.off = TRUE) 
    panel.axis(ifelse(rowi==1,"bottom","top"), 
      check.overlap = TRUE, outside = TRUE, labels = FALSE, 
      tck = .5, at = xMinorTicks)  
    trellis.unfocus()
  }
}


From maitra at iastate.edu  Thu Feb 22 06:15:03 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 21 Feb 2007 23:15:03 -0600
Subject: [R] applying lm on an array of observations with common design
 matrix
In-Reply-To: <Pine.LNX.4.64.0702180736050.10281@gannet.stats.ox.ac.uk>
References: <20070217202355.0ee7c70c@triveni.stat.iastate.edu>
	<Pine.LNX.4.64.0702180736050.10281@gannet.stats.ox.ac.uk>
Message-ID: <20070221231503.3f26b06b@triveni.stat.iastate.edu>

On Sun, 18 Feb 2007 07:46:56 +0000 (GMT) Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Sat, 17 Feb 2007, Ranjan Maitra wrote:
> 
> > Dear list,
> >
> > I have a 4-dimensional array Y of dimension 330 x 67 x 35 x 51. I have a 
> > design matrix X of dimension 330 x 4. I want to fit a linear regression 
> > of each
> >
> > lm( Y[, i, j, k] ~ X). for each i, j, k.
> >
> > Can I do it in one shot without a loop?
> 
> Yes.
> 
> YY <- YY
> dim(YY) <- c(330, 67*35*51)
> fit <- lm(YY ~ X)
> 
> > Actually, I am also interested in getting the p-values of some of the 
> > coefficients -- lets say the coefficient corresponding to the second 
> > column of the design matrix. Can the same be done using array-based 
> > operations?
> 
> Use lapply(summary(fit), function(x) coef(x)[3,4])  (since there is a 
> intercept, you want the third coefficient).

In this context, can one also get the variance-covariance matrix of the coefficients?

Thank you, and best wishes!
Ranjan 



> Note that this will give a vector, so set its dimension to c(67,35,51) to 
> relate to the original array.
> 
> I have not BTW looked into the memory requirements here, and you might 
> want to do this on slices of the array for that reason.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From marc_schwartz at comcast.net  Thu Feb 22 06:37:21 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 21 Feb 2007 23:37:21 -0600
Subject: [R] Is there better alternative to this loop?
In-Reply-To: <004c01c75621$68c9beb0$0300a8c0@Vaio>
References: <004c01c75621$68c9beb0$0300a8c0@Vaio>
Message-ID: <1172122641.4882.59.camel@localhost.localdomain>

On Thu, 2007-02-22 at 12:33 +1100, Alfonso Sammassimo wrote:
> Dear List,
> 
> Thanks to those who helped with my enquiry a few days ago.
> 
> I have a another question on loops, in this case I am trying to print out 
> the row of a data frame if the previous 3 values (daily values) in col5 are 
> in descending order. I have this loop which works, but ask whether this can 
> be done differently (without conventional loop) in R:
> 
> flag="T"
> d= 3 # d represents previous down days
> for(i in (d+1): 100)
> {
> for( j in (i-d):(i-1))
> {
> if(x[j,5]<x[i,5]){flag="F"}
> }
> if( flag == "T"){ print(x[i,1])}
> flag="T";
> }
> 
> Any help appreciated,
> 
> Regards,
> Alf Sammassimo
> Melbourne, Australia.

How about this. We'll take it one step at a time.

Get the indices of the rows in the data frame, where a column value in
the prior three rows is in decreasing order.  Let's use the iris data
set as an example:

> iris$Sepal.Length
  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7
 [17] 5.4 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4
 [33] 5.2 5.5 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6
 [49] 5.3 5.0 7.0 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1
 [65] 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7
 [81] 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7
 [97] 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4
[113] 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1
[129] 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8
[145] 6.7 6.7 6.3 6.5 6.2 5.9


Use diff() to get the differences between successive values in the
vector:

> diff(iris$Sepal.Length, 1)
  [1] -0.2 -0.2 -0.1  0.4  0.4 -0.8  0.4 -0.6  0.5  0.5 -0.6  0.0 -0.5
 [14]  1.5 -0.1 -0.3 -0.3  0.6 -0.6  0.3 -0.3 -0.5  0.5 -0.3  0.2  0.0
 [27]  0.2  0.0 -0.5  0.1  0.6 -0.2  0.3 -0.6  0.1  0.5 -0.6 -0.5  0.7
 [40] -0.1 -0.5 -0.1  0.6  0.1 -0.3  0.3 -0.5  0.7 -0.3  2.0 -0.6  0.5
 [53] -1.4  1.0 -0.8  0.6 -1.4  1.7 -1.4 -0.2  0.9  0.1  0.1 -0.5  1.1
 [66] -1.1  0.2  0.4 -0.6  0.3  0.2  0.2 -0.2  0.3  0.2  0.2 -0.1 -0.7
 [79] -0.3 -0.2  0.0  0.3  0.2 -0.6  0.6  0.7 -0.4 -0.7 -0.1  0.0  0.6
 [92] -0.3 -0.8  0.6  0.1  0.0  0.5 -1.1  0.6  0.6 -0.5  1.3 -0.8  0.2
[105]  1.1 -2.7  2.4 -0.6  0.5 -0.7 -0.1  0.4 -1.1  0.1  0.6  0.1  1.2
[118]  0.0 -1.7  0.9 -1.3  2.1 -1.4  0.4  0.5 -1.0 -0.1  0.3  0.8  0.2
[131]  0.5 -1.5 -0.1 -0.2  1.6 -1.4  0.1 -0.4  0.9 -0.2  0.2 -1.1  1.0
[144] -0.1  0.0 -0.4  0.2 -0.3 -0.3


Then use sign() to get the sign of the differences, positive, negative
or zero:

> sign(diff(iris$Sepal.Length, 1))
  [1] -1 -1 -1  1  1 -1  1 -1  1  1 -1  0 -1  1 -1 -1 -1  1 -1  1 -1 -1
 [23]  1 -1  1  0  1  0 -1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1 -1  1  1
 [45] -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1 -1  1  1  1 -1  1 -1
 [67]  1  1 -1  1  1  1 -1  1  1  1 -1 -1 -1 -1  0  1  1 -1  1  1 -1 -1
 [89] -1  0  1 -1 -1  1  1  0  1 -1  1  1 -1  1 -1  1  1 -1  1 -1  1 -1
[111] -1  1 -1  1  1  1  1  0 -1  1 -1  1 -1  1  1 -1 -1  1  1  1  1 -1
[133] -1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  0 -1  1 -1 -1


So, now we essentially want to get the index for a value where it and
the previous value are -1, indicating a decreasing sequence of 3 in the
original data vector. The easiest way to do this may be to actually look
for a moving average of -1 over two successive values in moving windows.
In this case, we use filter():

> which(filter(sign(diff(iris$Sepal.Length)), rep(1/2, 2), 
               sides = 1) == -1)
 [1]   2   3  16  17  22  38  41  42  60  78  79  80  88  89  93 111 127
[18] 133 134 149


That gives us the index of the second value in the sign() sequence.
Since we want the index of the 4th value in the original data vector, we
add 2:

> which(filter(sign(diff(iris$Sepal.Length)), rep(1/2, 2), 
               sides = 1) == -1) + 2
 [1]   4   5  18  19  24  40  43  44  62  80  81  82  90  91  95 113 129
[18] 135 136 151


So these values now should correspond to the indices of the values in
iris$Sepal.Length, where the three preceding values are in a decreasing
sequence.

So to apply that to your dataset, something like the following should
work:

Ind <- which(filter(sign(diff(DF$Col5)), rep(1/2, 2), 
                    sides = 1) == -1) + 2

Then print the rows with:

  DF[Ind, ]

HTH,

Marc Schwartz


From klaster at karlin.mff.cuni.cz  Thu Feb 22 08:04:30 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Thu, 22 Feb 2007 08:04:30 +0100
Subject: [R] applying lm on an array of observations with common design
 matrix
In-Reply-To: <20070221231503.3f26b06b@triveni.stat.iastate.edu>
References: <20070217202355.0ee7c70c@triveni.stat.iastate.edu>	<Pine.LNX.4.64.0702180736050.10281@gannet.stats.ox.ac.uk>
	<20070221231503.3f26b06b@triveni.stat.iastate.edu>
Message-ID: <45DD407E.2070104@karlin.mff.cuni.cz>

Ranjan Maitra napsal(a):
> On Sun, 18 Feb 2007 07:46:56 +0000 (GMT) Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> 
>> On Sat, 17 Feb 2007, Ranjan Maitra wrote:
>>
>>> Dear list,
>>>
>>> I have a 4-dimensional array Y of dimension 330 x 67 x 35 x 51. I have a 
>>> design matrix X of dimension 330 x 4. I want to fit a linear regression 
>>> of each
>>>
>>> lm( Y[, i, j, k] ~ X). for each i, j, k.
>>>
>>> Can I do it in one shot without a loop?
>> Yes.
>>
>> YY <- YY
>> dim(YY) <- c(330, 67*35*51)
>> fit <- lm(YY ~ X)
>>
>>> Actually, I am also interested in getting the p-values of some of the 
>>> coefficients -- lets say the coefficient corresponding to the second 
>>> column of the design matrix. Can the same be done using array-based 
>>> operations?
>> Use lapply(summary(fit), function(x) coef(x)[3,4])  (since there is a 
>> intercept, you want the third coefficient).
> 
> In this context, can one also get the variance-covariance matrix of the coefficients?

Sure:

lapply(summary(fit), function(x) {"$"(x,cov.unscaled)})

Add indexing if you do not want the whole matrix. You can extract 
whatever you want, just take a look at ?summary.lm, section Value.
Petr
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From aalimudin at gmail.com  Thu Feb 22 09:12:23 2007
From: aalimudin at gmail.com (Aalim Weljie)
Date: Thu, 22 Feb 2007 01:12:23 -0700
Subject: [R] MANOVA usage
Message-ID: <78a39f550702220012i16fd302ev8e96afd7dbca7d08@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/4bccc3ef/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Feb 22 09:17:38 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Feb 2007 08:17:38 +0000 (GMT)
Subject: [R] applying lm on an array of observations with common design
 matrix
In-Reply-To: <45DD407E.2070104@karlin.mff.cuni.cz>
References: <20070217202355.0ee7c70c@triveni.stat.iastate.edu>
	<Pine.LNX.4.64.0702180736050.10281@gannet.stats.ox.ac.uk>
	<20070221231503.3f26b06b@triveni.stat.iastate.edu>
	<45DD407E.2070104@karlin.mff.cuni.cz>
Message-ID: <Pine.LNX.4.64.0702220810470.2616@gannet.stats.ox.ac.uk>

On Thu, 22 Feb 2007, Petr Klasterecky wrote:

> Ranjan Maitra napsal(a):
>> On Sun, 18 Feb 2007 07:46:56 +0000 (GMT) Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>
>>> On Sat, 17 Feb 2007, Ranjan Maitra wrote:
>>>
>>>> Dear list,
>>>>
>>>> I have a 4-dimensional array Y of dimension 330 x 67 x 35 x 51. I have a
>>>> design matrix X of dimension 330 x 4. I want to fit a linear regression
>>>> of each
>>>>
>>>> lm( Y[, i, j, k] ~ X). for each i, j, k.
>>>>
>>>> Can I do it in one shot without a loop?
>>> Yes.
>>>
>>> YY <- YY
>>> dim(YY) <- c(330, 67*35*51)
>>> fit <- lm(YY ~ X)
>>>
>>>> Actually, I am also interested in getting the p-values of some of the
>>>> coefficients -- lets say the coefficient corresponding to the second
>>>> column of the design matrix. Can the same be done using array-based
>>>> operations?
>>> Use lapply(summary(fit), function(x) coef(x)[3,4])  (since there is a
>>> intercept, you want the third coefficient).
>>
>> In this context, can one also get the variance-covariance matrix of the 
>> coefficients?
>
> Sure:
>
> lapply(summary(fit), function(x) {"$"(x,cov.unscaled)})

But that is not the variance-covariance matrix (and it is an unusual way 
to write x$cov.unscaled)!

> Add indexing if you do not want the whole matrix. You can extract
> whatever you want, just take a look at ?summary.lm, section Value.

It is unclear to me what the questioner expects: the estimated 
coefficients for different responses are independent.  For a list of 
matrices applying to each response one could mimic vcov.lm and do

lapply(summary(fit, corr=FALSE),
        function(so) so$sigma^2 * so$cov.unscaled)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gregor.gorjanc at bfro.uni-lj.si  Thu Feb 22 09:32:37 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 22 Feb 2007 08:32:37 +0000 (UTC)
Subject: [R] write fixed format
References: <45DD1371.4050900@ucmerced.edu>
Message-ID: <loom.20070222T093115-191@post.gmane.org>

YIHSU CHEN <yihsu.chen <at> ucmerced.edu> writes:
> Dear R users;
> 
> Is there a function in R that I can put "text" with proper alignments in 
> a fixed format.  For instance, if I have three fields: A, B and C, where 
> both A and C are text with 3 characters and left alignment; B is a 
> numeric one with 2 decimals and 3 integer space digits.   How can I put 
> the following row in a file? (note there is a space between a and 2, and 
> after b.) (A=aaa, B=23.11 and C=bb)
> "aaa 23.11bb "

Check write.fwf() in gdata package. It might help you, but you will have to 
create new column to have B and C together i.e. without any space between.

Gregor


From gallon.li at gmail.com  Thu Feb 22 09:44:25 2007
From: gallon.li at gmail.com (gallon li)
Date: Thu, 22 Feb 2007 16:44:25 +0800
Subject: [R] how to install a package in R on a linux machine?
Message-ID: <54f7e7c30702220044s7eecde1bnff77d886fb2543eb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/5ea35f65/attachment.pl 

From cguous at gmail.com  Thu Feb 22 09:48:38 2007
From: cguous at gmail.com (Dong GUO)
Date: Thu, 22 Feb 2007 09:48:38 +0100
Subject: [R] Spatial error model estimation
Message-ID: <2801140d0702220048s2e210987yf5a4e36588b69f09@mail.gmail.com>

Greetings to the list,

I was trying to estimate spatial error model in R, somehow I got the
message below. Would you please help me with it? Many thanks in
advance.

Error in solve.default(asyvar, tol = tol.solve) :
	system is computationally singular: reciprocal condition number = 5.6964e-18


Regards,
Dong


From r.hankin at noc.soton.ac.uk  Thu Feb 22 09:56:25 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 22 Feb 2007 08:56:25 +0000
Subject: [R] 3F2 hypergeometric function
In-Reply-To: <4677FCB5A35A0441A0E0C99D56B23D910777FDD2@UTHEVS2.mail.uthouston.edu>
References: <Pine.LNX.4.64.0702201712110.5804@river.utm.utoronto.ca>
	<4677FCB5A35A0441A0E0C99D56B23D910777FDD2@UTHEVS2.mail.uthouston.edu>
Message-ID: <D714AE77-4654-45CF-9EE5-DCCB69D5197B@soc.soton.ac.uk>

Hello Joe


On 21 Feb 2007, at 16:22, Lucke, Joseph F wrote:

> Does anyone have code for the 3F2 hypergeometric function? I am  
> looking
> for code similar to the 2F1 hypergeometric function implemented as
> hyperg_2F1 in the GSL package. TIA.  ---Joe
>


The GSL library does not have a hyperg_3F2, so neither does the gsl  
package.

BUT the Davies package does have a hyperg() function that is written in
such a way that it would be a cinch to convert it from 2F1 to 3F2.

Let me know how you get on


Robin





> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ola.caster at gmail.com  Thu Feb 22 11:23:19 2007
From: ola.caster at gmail.com (Ola Caster)
Date: Thu, 22 Feb 2007 11:23:19 +0100
Subject: [R] Move y label in xyplot
Message-ID: <e8adda70702220223q2964dc26w56865c46ec82b7c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/19520594/attachment.pl 

From csardi at rmki.kfki.hu  Thu Feb 22 11:40:54 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 22 Feb 2007 11:40:54 +0100
Subject: [R] how to install a package in R on a linux machine?
In-Reply-To: <54f7e7c30702220044s7eecde1bnff77d886fb2543eb@mail.gmail.com>
References: <54f7e7c30702220044s7eecde1bnff77d886fb2543eb@mail.gmail.com>
Message-ID: <20070222104054.GA5393@guzu>

The easiest is perhaps to do 

install.packages("packagename")

this downloads the package and installs it into the default R package
library on your machine. If you want to install it to a different 
directory use the 'lib' argument of 'install.packages'.

If you don't want to download the package again but want to use the 
downloaded one, use the following command:

install.packages(repos=NULL, pkgs="the.file.you've.downloaded")

You can also install R packages from the command line, like this:

R CMD INSTALL -l <lib.directectory> <downloaded.package.file>

Gabor

On Thu, Feb 22, 2007 at 04:44:25PM +0800, gallon li wrote:
> I downloaded the tar.gz file from r-project website (and saved it in a local
> directory) and wish to use the package in R.
> 
> But I am not sure how to use the install.packages command. I tried a few
> times and still couldn't figure out the correct way to install this package.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From shubhak at ambaresearch.com  Thu Feb 22 11:37:46 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Thu, 22 Feb 2007 16:07:46 +0530
Subject: [R] Accessing the class of an object with two elements.
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3010B157D@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/c9efc7a7/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Feb 22 11:47:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Feb 2007 10:47:44 +0000 (GMT)
Subject: [R] MANOVA usage
In-Reply-To: <78a39f550702220012i16fd302ev8e96afd7dbca7d08@mail.gmail.com>
References: <78a39f550702220012i16fd302ev8e96afd7dbca7d08@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702221002040.3919@gannet.stats.ox.ac.uk>

On Thu, 22 Feb 2007, Aalim Weljie wrote:

> Hello,
>
> I had a couple questions about  manova modeling in R.
>
> I have calculated a manova model, and generated a summary.manova output
> using both the Wilks test and Pillai test.
>
> The output is essentially the same, except that the Wilks lambda = 1 -
> Pillai. Is this normal? (The output from both is appended below.)

For a 1-df test, yes: all the statistics give the same test (as 
?summary.manova does say).

> My other question is about the use of MANOVA. If I have one variable which
> has a higher F-stat value (~60) than the MANOVA fstat (~20), but the other
> 15 variables all have lower ANOVA F-stat values (~5-10), am I still okay in
> using the MANOVA? (If I remove the one highly significant variable, the
> MANOVA f-stat is still larger than the other univariate F-stats).

OK for what purpose?  Using (M)ANOVA for variable selection has many 
issues.

> Thanks very much for your help,
>
> Aalim
>
> Output from Manova tests:
>
>> summary.manova(fit)
>          Df  Pillai approx F num Df den Df    Pr(>F)
> response  1  0.9725  19.8967     16      9 4.193e-05 ***
> Residuals 24
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>> summary.manova(fit, test="Wilks")
>          Df   Wilks approx F num Df den Df    Pr(>F)
> response 1  0.0275  19.8967     16      9 4.193e-05 ***
> Residuals 24
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Serguei.Kaniovski at wifo.ac.at  Thu Feb 22 11:50:34 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Thu, 22 Feb 2007 11:50:34 +0100
Subject: [R] Sorting rows of a binary matrix
Message-ID: <OF5B99A59B.A8B8D43E-ONC125728A.003B8FD2-C125728A.003B8FE0@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/17a60819/attachment.pl 

From mothsailor at googlemail.com  Thu Feb 22 12:17:42 2007
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 22 Feb 2007 11:17:42 +0000
Subject: [R] Sorting rows of a binary matrix
In-Reply-To: <OF5B99A59B.A8B8D43E-ONC125728A.003B8FD2-C125728A.003B8FE0@wsr.ac.at>
References: <OF5B99A59B.A8B8D43E-ONC125728A.003B8FD2-C125728A.003B8FE0@wsr.ac.at>
Message-ID: <815b70590702220317w2a15ced7l1bb283ce71c7774e@mail.gmail.com>

I'm sure there are more elegant ways, but this should work:

> ix<-order(mat[,1],mat[,2],mat[,3])
> ix
[1] 1 5 3 7 2 6 4 8
> mat[ix,]
  Var1 Var2 Var3
1    0    0    0
5    0    0    1
3    0    1    0
7    0    1    1
2    1    0    0
6    1    0    1
4    1    1    0
8    1    1    1


On 22/02/07, Serguei Kaniovski <Serguei.Kaniovski at wifo.ac.at> wrote:
>
> Hallo,
>
> The command:
>
> x <- 3
> mat <- as.matrix(expand.grid(rep(list(0:1), x)))
>
> generates a matrix with 2^x columns containing the binary representations
> of the decimals from 0 to (2^x-1), here from 0 to 7. But the rows are not
> sorted in this order.
>
> How can sort the rows the ascending order of the decimals they represent,
> preferably without a function which converts binaries to decimals (which I
> have)? Alternatively, generate a matrix that has the rows sorted that way?
>
> Thanks,
> Serguei
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From jrkrideau at yahoo.ca  Thu Feb 22 12:30:07 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 22 Feb 2007 06:30:07 -0500 (EST)
Subject: [R] Trying to get an apply to work with a list in applying
	names totables
In-Reply-To: <1172075938.4769.7.camel@localhost.localdomain>
Message-ID: <246681.31916.qm@web32812.mail.mud.yahoo.com>


--- Marc Schwartz <marc_schwartz at comcast.net> wrote:

> I might suggest an alternative, since you seem to be
> creating the
> underlying data set from scratch.

Thanks Marc.  I see what you're suggesting but I am
not creating the data from scratch.  The data base
represented by "cc"  is an SPSS file that I have
inherited.  

Still I am having to do enough mucking about with the
SPSS file anyway that your approach might be the best
way to go if I do this again. I wish I had asked this
question a week ago!

> Create the data frame with the requisite data
> structures to start with
> and then perform the table operations:
> 
> 
> # First create your vectors as factors. See ?factor
> aa <- factor(c(2,2,1,1,2), levels = 1:2, labels =
> c("yes", "no"))
> bb <- factor(c(5,6,6,7,4), levels = 4:7, labels =
> letters[1:4])
> 
> 
> # Now create your data frame using the names you
> want for each column
> cc <- data.frame(abby = aa, billy = bb)
> 
> 
> Now run the table on each column:
> 
> > lapply(cc, table)
> $abby
> 
> yes  no 
>   2   3 
> 
> $billy
> 
> a b c d 
> 1 1 2 1 
> 
> 
> See ?lapply as well. Note that a data frame is a
> list:
> 
> > is.list(cc)
> [1] TRUE
> 
> > is.data.frame(cc)
> [1] TRUE
> 
> 
> > as.list(cc)
> $abby
> [1] no  no  yes yes no 
> Levels: yes no
> 
> $billy
> [1] b c c d a
> Levels: a b c d
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> On Wed, 2007-02-21 at 17:15 +0100, ONKELINX, Thierry
> wrote:
> > John,
> > 
> > Two things. You don't need to pout the cc variable
> in the apply. Use
> > instead something like this.
> > 
> > apply(cc, 2, fn1, y = mylist)
> > 
> > But this still doesn't solve your problem. You'll
> need to rewrite your
> > function like this.
> > 
> > > fn2 <- function(x, y, i){
> > +   tt <- table(x[, i])
> > +   names(tt) <- y[[i]]
> > +   return(tt)
> > + }
> > > sapply(1:ncol(cc), fn2, x = cc, y = mylist)
> > [[1]]
> > yes  no 
> >   2   3 
> > 
> > [[2]]
> > a b c d 
> > 1 1 2 1 
> > 
> > Cheers,
> > 
> > Thierry
> 
> > -----Oorspronkelijk bericht-----
> > Van: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] Namens
> John Kane
> > Verzonden: woensdag 21 februari 2007 16:47
> > Aan: R R-help
> > Onderwerp: [R] Trying to get an apply to work with
> a list in applying
> > names totables
> > 
> > I am trying to use apply and a  list to supply
> names
> > to a set of tables I want to generate. Below is an
> > example that I hope mimics the larger original
> > problem.
> > 
> > EXAMPLE
> > 
> > aa <- c( 2,2,1,1,2)
> > bb <- c(5,6,6,7,4)
> > aan <- c("yes", "no")
> > bbn <- c("a", "b", "c", "d")
> > mynames <- c("abby", "billy")
> > mylist <- list(aan, bbn);   names(mylist) <-
> mynames
> > 
> > cc <- data.frame(aa,bb)
> > fn1 <- function(x,y) {tt <- table(x); names(tt)<-
> > mylist[[y]]}
> > jj <-apply(cc, 2, fn1(cc,mylist))
> > 
> > RESULT:  
> > Error in fn1(cc, mylist) : invalid subscript type
> > 
> > To be honest I didn't expect it to work since that
> > fin1(cc  looks recursive but oh well...
> > 
> > Can anyone offer a solution or some advice here. 
> It
> > would be greatly appreciated
> 
> 
>


From Roger.Bivand at nhh.no  Thu Feb 22 12:53:05 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 22 Feb 2007 12:53:05 +0100 (CET)
Subject: [R] Spatial error model estimation
In-Reply-To: <2801140d0702220048s2e210987yf5a4e36588b69f09@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0702221248550.14521-100000@reclus.nhh.no>

On Thu, 22 Feb 2007, Dong GUO wrote:

> Greetings to the list,
> 
> I was trying to estimate spatial error model in R, somehow I got the
> message below. Would you please help me with it? Many thanks in
> advance.
> 
> Error in solve.default(asyvar, tol = tol.solve) :
> 	system is computationally singular: reciprocal condition number = 5.6964e-18

(This refers to function errorsarlm() in package spdep)

Please see ?errorsarlm, the problem is explained there as follows:

tol.solve: the tolerance for detecting linear dependencies in the
          columns of matrices to be inverted - passed to 'solve()'
          (default=1.0e-10). This may be used if necessary to extract
          coefficient standard errors (for instance lowering to 1e-12),
          but errors in 'solve()' may constitute indications of poorly
          scaled variables: if the variables have scales differing much
          from the autoregressive coefficient, the values in this
          matrix may be very different in scale, and inverting such a
          matrix is analytically possible by definition, but
          numerically unstable; rescaling the RHS variables alleviates
          this better than setting tol.solve to a very small value.



> 
> 
> Regards,
> Dong
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From Roger.Humphry at scri.ac.uk  Thu Feb 22 12:53:47 2007
From: Roger.Humphry at scri.ac.uk (Roger Humphry)
Date: Thu, 22 Feb 2007 11:53:47 -0000
Subject: [R] daisy function in cluster- coerced NAs
Message-ID: <858233C5A7842E46B63A77ACE3B0587B73C113@exchange1.scri.sari.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/1f03c164/attachment.pl 

From Roger.Bivand at nhh.no  Thu Feb 22 13:15:15 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 22 Feb 2007 13:15:15 +0100 (CET)
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WGS84)
In-Reply-To: <86wt2bsxks.fsf@coulee.tdb.com>
Message-ID: <Pine.LNX.4.44.0702221311250.14521-100000@reclus.nhh.no>

On 21 Feb 2007, Russell Senior wrote:

> 
> I am interested in making a random sample from a uniform distribution
> of points over the surface of the earth, using the WGS84 ellipsoid as
> a model for the earth.  I know how to do this for a sphere, but would
> like to do better.  I can supply random numbers, want latitude
> longitude pairs out.
> 
> Can anyone point me at a solution?  Thanks very much.
> 

http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.html

looks promising, untried.



> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From rb373 at cam.ac.uk  Thu Feb 22 13:32:44 2007
From: rb373 at cam.ac.uk (R. Baker)
Date: 22 Feb 2007 12:32:44 +0000
Subject: [R] investigating interactions with mixed models
Message-ID: <Prayer.1.0.18.0702221232440.22007@hermes-1.csi.cam.ac.uk>

I'm investigating a number of dependent variables using mixed models, e.g.

data.lmer45 = lmer(ampStopB ~ (type + stress + MorD)^3 + (1|speaker) + 
(1|word), data=data)

The p-values for some of the 2-way and 3-way interactions are significant 
at a 0.05 level and I have been trying to find out how to understand the 
exact nature of the interactions. Does anyone know if it is possible to run 
post-hoc tests on mixed model (lmer) objects? I have read about TukeyHSD 
but it seems that this can only be run on anova (aov) objects.

Any suggestions would be gratefully appreciated!

Rachel Baker

-- 
--------------------------------------------------------------------------
PhD student                
Dept of Linguistics        
Sidgwick Avenue
University of Cambridge              
Cambridge


From olivier.martin at avignon.inra.fr  Thu Feb 22 13:46:06 2007
From: olivier.martin at avignon.inra.fr (Martin Olivier)
Date: Thu, 22 Feb 2007 13:46:06 +0100
Subject: [R] residuals and glm
Message-ID: <45DD908E.6050708@avignon.inra.fr>

Hi all,

I have some problems to compute the residuals from a glm model with
binomial distribution.

Suppose I have the following result :
resfit<-glm(y~x1+x2,weights=we,family=binomial(link="logit"))

Now I would like to obtain the residuals .
the command residuals(resfit) and the vector resfit$residuals give 
different
results, and they do not correspond to residuals E(yi)-yi (that is 
resfit$fitted-resfit$y)

so, I would like to know what formula  is applied to compute these 
residuals.
And moreover,  if I want to compute the standardized residuals, what is the
right command from the glmfit result.
Is it 
(resfit$fitted-resfit$y)/sqrt(1/resfit$prior*resfit$fitt*(1-resfit$fitt)) ??


Thanks for your help,
Olivier.


From dusa.adrian at gmail.com  Thu Feb 22 14:00:09 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 22 Feb 2007 15:00:09 +0200
Subject: [R] Sorting rows of a binary matrix
In-Reply-To: <OF5B99A59B.A8B8D43E-ONC125728A.003B8FD2-C125728A.003B8FE0@wsr.ac.at>
References: <OF5B99A59B.A8B8D43E-ONC125728A.003B8FD2-C125728A.003B8FE0@wsr.ac.at>
Message-ID: <200702221500.10040.dusa.adrian@gmail.com>

Hello Serguei,

Is this what you need?

myfunc <- function(x) {
    create <- function(idx) {
        rep.int(c(rep.int(0,2^(idx-1)), rep.int(1,2^(idx-1))),
                2^x/2^idx)
        }
    sapply(rev(seq(x)), create)
    }

> myfunc(3)
     [,1] [,2] [,3]
[1,]    0    0    0
[2,]    0    0    1
[3,]    0    1    0
[4,]    0    1    1
[5,]    1    0    0
[6,]    1    0    1
[7,]    1    1    0
[8,]    1    1    1

For numerical values only, this is faster than expand.grid().
Alternatively (for multiple values in separate varaibles), you could use the 
function createMatrix() in package QCA.

HTH,
Adrian

On Thursday 22 February 2007 12:50, Serguei Kaniovski wrote:
> Hallo,
>
> The command:
>
> x <- 3
> mat <- as.matrix(expand.grid(rep(list(0:1), x)))
>
> generates a matrix with 2^x columns containing the binary representations
> of the decimals from 0 to (2^x-1), here from 0 to 7. But the rows are not
> sorted in this order.
>
> How can sort the rows the ascending order of the decimals they represent,
> preferably without a function which converts binaries to decimals (which I
> have)? Alternatively, generate a matrix that has the rows sorted that way?
>
> Thanks,
> Serguei
> 	[[alternative HTML version deleted]]

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From mothsailor at googlemail.com  Thu Feb 22 14:25:42 2007
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 22 Feb 2007 13:25:42 +0000
Subject: [R] residuals and glm
In-Reply-To: <45DD908E.6050708@avignon.inra.fr>
References: <45DD908E.6050708@avignon.inra.fr>
Message-ID: <815b70590702220525h9a0066p7480245fcae927f9@mail.gmail.com>

You really need to look at ?glm and ?residuals.glm.

resfit$residuals are the *working* residuals, which are not typically
very useful in themselves.  Far better to use the extractor function.
This enables you to obtain a number of different types of residuals,
but the default (and therefore the type you have obtained) are
deviance residuals.  You can also specify other types, such as pearson
residuals.

Hope this helps.
David

On 22/02/07, Martin Olivier <olivier.martin at avignon.inra.fr> wrote:
> Hi all,
>
> I have some problems to compute the residuals from a glm model with
> binomial distribution.
>
> Suppose I have the following result :
> resfit<-glm(y~x1+x2,weights=we,family=binomial(link="logit"))
>
> Now I would like to obtain the residuals .
> the command residuals(resfit) and the vector resfit$residuals give
> different
> results, and they do not correspond to residuals E(yi)-yi (that is
> resfit$fitted-resfit$y)
>
> so, I would like to know what formula  is applied to compute these
> residuals.
> And moreover,  if I want to compute the standardized residuals, what is the
> right command from the glmfit result.
> Is it
> (resfit$fitted-resfit$y)/sqrt(1/resfit$prior*resfit$fitt*(1-resfit$fitt)) ??
>
>
> Thanks for your help,
> Olivier.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From sergeyg at gmail.com  Thu Feb 22 14:35:06 2007
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Thu, 22 Feb 2007 14:35:06 +0100
Subject: [R] Combining tapply() and cor.test()?
Message-ID: <7cb007bd0702220535vf41c836j41453a7e9565763d@mail.gmail.com>

 Hello, fellow R-users.

Let me describe the setup first. I have a data.frame, a sample of
which is reported below:

           Company.Name      Periods      Returns   MFR.Factor
350         Wartsila Oyj A      1996-07-31      6.82         0.02
351    Custodia Holding AG  1996-07-31      4.15        -0.02
352           Wartsila Oyj       1996-07-31      7.73         0.09
353           GEA Group AG   1996-07-31    10.12         0.04
354            LEGRAND ORD 1996-07-31     -7.46        -0.20
355 Mayr-Melnhof Karton AG 1996-07-31     4.71        -0.05
356            GEVAERT NPV  1996-08-30      NA          NA
357        NOKIA K FMA2.50  1996-08-30     7.65         0.03
358           Altadis S.A.         1996-08-30     7.65         0.55
359       Metrovacesa S.A.     1996-08-30     4.55        -0.17
360               Oce N.V.          1996-08-30    9.43         0.23

The variable "Periods" is a date object, shows the month.
Variables "Returns" and "MFR.Factor" are numeric.
For each month the number of Returns and MFR.Factors varies, sometimes
it is 350, sometimes 320 etc.

What I need is to use cor.test(Returns, MFR.Factor,...) for each
month, and produce a dataframe with columns: "Period", "cor.estimate",
"p.value".

The simplest way would be with tapply() using variable "Period" as a
factor, but tapply() only applies FUN to just one cell.

What is the most painless way to achieve my objective?

Thank you in advance for your help!

Best,
Sergey


From ripley at stats.ox.ac.uk  Thu Feb 22 14:36:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Feb 2007 13:36:13 +0000 (GMT)
Subject: [R] residuals and glm
In-Reply-To: <45DD908E.6050708@avignon.inra.fr>
References: <45DD908E.6050708@avignon.inra.fr>
Message-ID: <Pine.LNX.4.64.0702221323440.6055@gannet.stats.ox.ac.uk>

On Thu, 22 Feb 2007, Martin Olivier wrote:

> I have some problems to compute the residuals from a glm model with 
> binomial distribution.
>
> Suppose I have the following result :
> resfit<-glm(y~x1+x2,weights=we,family=binomial(link="logit"))
>
> Now I would like to obtain the residuals . the command residuals(resfit) 
> and the vector resfit$residuals give different results, and they do not 
> correspond to residuals E(yi)-yi (that is resfit$fitted-resfit$y)

?residuals.glm should help you, but note that residuals are always 
conventionally (observed - fitted), not as you give.

?glm tells you what resfit$residuals is, and it seems to be not what you 
think it is.

> so, I would like to know what formula is applied to compute these 
> residuals. And moreover, if I want to compute the standardized 
> residuals, what is the right command from the glmfit result. Is it 
> (resfit$fitted-resfit$y)/sqrt(1/resfit$prior*resfit$fitt*(1-resfit$fitt)) 
> ??

See ?rstandard and print(rstandard.glm).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From f.harrell at vanderbilt.edu  Thu Feb 22 14:38:15 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 22 Feb 2007 07:38:15 -0600
Subject: [R] residuals and glm
In-Reply-To: <815b70590702220525h9a0066p7480245fcae927f9@mail.gmail.com>
References: <45DD908E.6050708@avignon.inra.fr>
	<815b70590702220525h9a0066p7480245fcae927f9@mail.gmail.com>
Message-ID: <45DD9CC7.2040703@vanderbilt.edu>

David Barron wrote:
> You really need to look at ?glm and ?residuals.glm.
> 
> resfit$residuals are the *working* residuals, which are not typically
> very useful in themselves.  Far better to use the extractor function.
> This enables you to obtain a number of different types of residuals,
> but the default (and therefore the type you have obtained) are
> deviance residuals.  You can also specify other types, such as pearson
> residuals.
> 
> Hope this helps.
> David

And in the Design package's lrm and residuals.lrm function (called by 
resid(fit)) you can get partial residuals that when smoothed estimate 
covariate effects.  I do prefer direct modeling instead of looking at 
any of the residuals though, e.g., adding nonlinear or interaction terms 
to logistic models.  Usually I find that residuals in simple binary 
logistic models are most useful in checking for overly influential 
observations.

Frank Harrell

> 
> On 22/02/07, Martin Olivier <olivier.martin at avignon.inra.fr> wrote:
>> Hi all,
>>
>> I have some problems to compute the residuals from a glm model with
>> binomial distribution.
>>
>> Suppose I have the following result :
>> resfit<-glm(y~x1+x2,weights=we,family=binomial(link="logit"))
>>
>> Now I would like to obtain the residuals .
>> the command residuals(resfit) and the vector resfit$residuals give
>> different
>> results, and they do not correspond to residuals E(yi)-yi (that is
>> resfit$fitted-resfit$y)
>>
>> so, I would like to know what formula  is applied to compute these
>> residuals.
>> And moreover,  if I want to compute the standardized residuals, what is the
>> right command from the glmfit result.
>> Is it
>> (resfit$fitted-resfit$y)/sqrt(1/resfit$prior*resfit$fitt*(1-resfit$fitt)) ??
>>
>>
>> Thanks for your help,
>> Olivier.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From dusa.adrian at gmail.com  Thu Feb 22 14:52:00 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 22 Feb 2007 15:52:00 +0200
Subject: [R] Sorting rows of a binary matrix
In-Reply-To: <200702221500.10040.dusa.adrian@gmail.com>
References: <OF5B99A59B.A8B8D43E-ONC125728A.003B8FD2-C125728A.003B8FE0@wsr.ac.at>
	<200702221500.10040.dusa.adrian@gmail.com>
Message-ID: <200702221552.00841.dusa.adrian@gmail.com>

And, for multiple bases:

myfunc <- function(cols, bases) {
   create <- function(idx) {
        rep.int(c(sapply(seq_len(bases)-1, function(x)
            rep.int(x, bases^(idx-1)))), bases^cols/bases^idx)
        }
    sapply(rev(seq_len(cols)), create)
    }

# For 3 columns in base 2
myfunc(3, 2)

# For 3 columns in base 3
myfunc(3, 3)

hth,
Adrian


On Thursday 22 February 2007 15:00, Adrian Dusa wrote:
> Hello Serguei,
>
> Is this what you need?
>
> myfunc <- function(x) {
>     create <- function(idx) {
>         rep.int(c(rep.int(0,2^(idx-1)), rep.int(1,2^(idx-1))),
>                 2^x/2^idx)
>         }
>     sapply(rev(seq(x)), create)
>     }
>
> > myfunc(3)
>
>      [,1] [,2] [,3]
> [1,]    0    0    0
> [2,]    0    0    1
> [3,]    0    1    0
> [4,]    0    1    1
> [5,]    1    0    0
> [6,]    1    0    1
> [7,]    1    1    0
> [8,]    1    1    1
>
> For numerical values only, this is faster than expand.grid().
> Alternatively (for multiple values in separate varaibles), you could use
> the function createMatrix() in package QCA.
>
> HTH,
> Adrian
>
> On Thursday 22 February 2007 12:50, Serguei Kaniovski wrote:
> > Hallo,
> >
> > The command:
> >
> > x <- 3
> > mat <- as.matrix(expand.grid(rep(list(0:1), x)))
> >
> > generates a matrix with 2^x columns containing the binary representations
> > of the decimals from 0 to (2^x-1), here from 0 to 7. But the rows are not
> > sorted in this order.
> >
> > How can sort the rows the ascending order of the decimals they represent,
> > preferably without a function which converts binaries to decimals (which
> > I have)? Alternatively, generate a matrix that has the rows sorted that
> > way?
> >
> > Thanks,
> > Serguei
> > 	[[alternative HTML version deleted]]

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From jholtman at gmail.com  Thu Feb 22 14:53:38 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 22 Feb 2007 08:53:38 -0500
Subject: [R] Combining tapply() and cor.test()?
In-Reply-To: <7cb007bd0702220535vf41c836j41453a7e9565763d@mail.gmail.com>
References: <7cb007bd0702220535vf41c836j41453a7e9565763d@mail.gmail.com>
Message-ID: <644e1f320702220553g144512b6uc54238b200603534@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/ae976e54/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Thu Feb 22 14:59:10 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 22 Feb 2007 14:59:10 +0100
Subject: [R] Combining tapply() and cor.test()?
References: <7cb007bd0702220535vf41c836j41453a7e9565763d@mail.gmail.com>
Message-ID: <00d101c75689$a03effa0$0540210a@www.domain>

one approach is the following:

dat <- data.frame(
    Period = as.Date(rep(c("1996-07-31", "1996-08-31", "1996-09-30"), 
each = 15)),
    Returns = rnorm(45),
    MFR.Factor = runif(45)
)

###########

do.call(rbind, lapply(split(dat[c("Returns", "MFR.Factor")], 
dat$Period),
    function (x) {
        cr <- cor.test(x$Returns, x$MFR.Factor, method = "spearman")
        c("estimate" = cr$estimate, "p.value" = cr$p.value)
}))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sergey Goriatchev" <sergeyg at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 22, 2007 2:35 PM
Subject: [R] Combining tapply() and cor.test()?


> Hello, fellow R-users.
>
> Let me describe the setup first. I have a data.frame, a sample of
> which is reported below:
>
>           Company.Name      Periods      Returns   MFR.Factor
> 350         Wartsila Oyj A      1996-07-31      6.82         0.02
> 351    Custodia Holding AG  1996-07-31      4.15        -0.02
> 352           Wartsila Oyj       1996-07-31      7.73         0.09
> 353           GEA Group AG   1996-07-31    10.12         0.04
> 354            LEGRAND ORD 1996-07-31     -7.46        -0.20
> 355 Mayr-Melnhof Karton AG 1996-07-31     4.71        -0.05
> 356            GEVAERT NPV  1996-08-30      NA          NA
> 357        NOKIA K FMA2.50  1996-08-30     7.65         0.03
> 358           Altadis S.A.         1996-08-30     7.65         0.55
> 359       Metrovacesa S.A.     1996-08-30     4.55        -0.17
> 360               Oce N.V.          1996-08-30    9.43         0.23
>
> The variable "Periods" is a date object, shows the month.
> Variables "Returns" and "MFR.Factor" are numeric.
> For each month the number of Returns and MFR.Factors varies, 
> sometimes
> it is 350, sometimes 320 etc.
>
> What I need is to use cor.test(Returns, MFR.Factor,...) for each
> month, and produce a dataframe with columns: "Period", 
> "cor.estimate",
> "p.value".
>
> The simplest way would be with tapply() using variable "Period" as a
> factor, but tapply() only applies FUN to just one cell.
>
> What is the most painless way to achieve my objective?
>
> Thank you in advance for your help!
>
> Best,
> Sergey
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From markg at uic.edu  Thu Feb 22 15:05:32 2007
From: markg at uic.edu (Grant, Mark D.)
Date: Thu, 22 Feb 2007 08:05:32 -0600 (CST)
Subject: [R] Error in solve.default
Message-ID: <55655.12.47.15.38.1172153132.squirrel@webmail.uic.edu>

I am trying to run the following function (a hierarchical bayes linear
model) and receive the error in solve.default.  The function was
originally written for an older version of SPlus.  Can anyone give me some
insights into where the problem is?

Thanks

R 2.4.1 on MAC OSX 2mb ram

Mark Grant
markg at uic.edu

> attach(Aspirin.frame)
> hblm(Diff ~ 1, s = SE)
Error in solve.default(R, rinv) : 'a' is 0-diml

> traceback()
6: .Call("La_dgesv", a, b, tol, PACKAGE = "base")
5: solve.default(R, rinv)
4: solve(R, rinv)
3: summary.blm(fit)
2: eb.calc(rho[i], X, Y, s.e., df.se, corrs, prior, ...)
1: hblm(Diff ~ 1, s = SE)
>

> hblm
function(formula, s.e., df.se = Inf, corrs = F, prior = NULL, fast.calc = F,
        ...)
{
# hblm()
# main program to create hblm object
        call <- match.call()
        call.ab <- abbrev.hblm.call(call)
        taumin <- (sum(1/s.e.^2))^-0.5
        if(is.null(prior$error)) {
                prior$error$df <- 1
                prior$error$sd <- taumin
        }
        s.e..old <- s.e.
        if(max(s.e.) == Inf)
                s.e.[s.e. == Inf] <- taumin * 10000
        wts <<- s.e.^(-2)                  # changed 1.10.2007 DMR df.se
<- 1/mean(1/df.se)
        if(df.se == Inf) {
                if(is.null(prior$tau))
                        prior$tau <- taumin * sqrt(length(wts))
        }
        fit <- lm(formula, weights = wts, qr = T, x = T, y = T, singular =
T,
                ...)
        X <- fit$x
        Y <- fit$y
        Terms <- fit$terms
        rss <- sum(wts * fit$residuals^2)
        levs <- hat(fit$qr)
        tau.rss <- (max(0, rss - fit$df.residual)/sum((1 - levs) *
wts))^0.5
        log.r <- if(tau.rss > 0) log(tau.rss) else log(taumin)
        maxval <- max1d(l.p.d.log.r, start = log.r, step =
taumin/exp(log.r), x
                 = X, y = Y, s.e. = s.e., df.se = df.se, prior = prior,
...)
        log.r <- maxval$x
        se.lr <- (2 * maxval$h)^(-0.5)
        g.h.v <- gauss.hermite.vals(log.r, se.lr, fast.calc)
        rho <- exp(g.h.v$x)
        r <- length(rho)
        k <- nrow(X)
        p <- ncol(X)
        tau <- sig <- lpd <- array(0, dim = r)
        coefs <- array(0, dim = c(r, p, 3), dimnames = list(NULL,
dimnames(X)[[
                2]], c("Mean", "S.D.", "Prob > 0")))
        cov.c <- array(0, dim = c(r, p, p), dimnames =
append(dimnames(X)[c(2,
                2)], list(NULL), 0))
        means <- array(0, dim = c(r, k, 3), dimnames = list(NULL,
names(Y), c(
                "Post.Mn", "Post.SD", "Prob > 0")))
        if(corrs)
                cov.m <- array(0, dim = c(r, k, k), dimnames = list(NULL,
names(
                        Y), names(Y)))
        for(i in 1:r) {
                fit <- eb.calc(rho[i], X, Y, s.e., df.se, corrs, prior,
...)
                tau[i] <- fit$tau
                sig[i] <- fit$sigma
                lpd[i] <- fit$lpd
                coefs[i,  ,  ] <- fit$coefs
                cov.c[i,  ,  ] <- fit$cov.c
                means[i,  ,  ] <- fit$means
                if(corrs)
                        cov.m[i,  ,  ] <- fit$cov.means
        }
        prob <- (exp(lpd) * g.h.v$w)/sum(exp(lpd) * g.h.v$w)
        if(fit$df.post == Inf) {
                K1 <- K2 <- 1
        }
        else {
                K1 <- sqrt(fit$df.post/2) * exp(lgamma((fit$df.post -
1)/2) -
                        lgamma(fit$df.post/2))
                K2 <- fit$df.post/(fit$df.post - 2)
        }
        tausq.m <- K2 * prob %*% tau^2
        tau.m <- K1 * prob %*% tau
        tau.sd <- sqrt(tausq.m - tau.m^2)
        sigsq.m <- K2 * prob %*% sig^2
        sig.m <- K1 * prob %*% sig
        sig.sd <- sqrt(sigsq.m - sig.m^2)
        coefs.m <- prob %ip% coefs
        cov.c.m <- K2 * (prob %ip% cov.c)
        coef.d <- coefs[,  , 1] - matrix(coefs.m[, 1], nrow = r, ncol = p,

                byrow = T)
        coef.v <- array(0, dim = dim(cov.c.m), dimnames =
dimnames(cov.c.m))
        for(i in 1:r)
                coef.v <- coef.v + prob[i] * outer(coef.d[i,  ], coef.d[i,
 ])
        coefs.m[, 2] <- sqrt(diag(cov.c.m) + diag(coef.v))
        shrink <- matrix(0, nrow = k, ncol = 6, dimnames = list(names(Y),
c("Y",
                "Prior Mn", "(Y-Prior)/SE", "Post.Mn", "Post.SD", "Prob >
0")))
        fitted <- X %*% coefs.m[, 1]
        residuals <- (Y - fitted)/s.e.
        shrink[, 1:3] <- matrix(c(Y, fitted, residuals), ncol = 3)
shrink[, 4:6] <- prob %ip% means
        means.d <- means[,  , 1] - matrix(shrink[, 4], nrow = r, ncol = k,

                byrow = T)
        means.v <- prob %*% means.d^2
        shrink[, 5] <- sqrt(K2 * (prob %*% means[,  , 2]^2) + means.v)
corr.m.m <- if(!corrs) NULL else {
                covm <- matrix(0, nrow = k, ncol = k, dimnames =
list(names(Y),
                        names(Y)))
                for(i in 1:r) {
                        covd <- means.d[i,  ] %o% means.d[i,  ]
                        covm <- covm + prob[i] * (covd + K2 * cov.m[i,  ,
])
                }
                covm/(shrink[, 5] %o% shrink[, 5])
        }
        trace <- list(rho = rho, tau = tau, sigma = sig, lpd = lpd, prob =
prob,
                coef.s.p = coefs, mean.s = means)
        class(trace) <- "trace"
        class(shrink) <- "shrinkage"
        df <- list(residuals = fit$df.resid, sigma = fit$df.post, prior =
fit$
                df.prior, se = df.se)
        fit <- list(trace = trace, tau = tau.m, tau.sd = tau.sd, sigma =
sig.m,
                sigma.sd = sig.sd, coef.s.p = coefs.m, covar.coef =
cov.c.m +
                coef.v, shrinkage = shrink, post.corr = corr.m.m, tau.rss
=
                tau.rss, rho.maxpd = exp(log.r), se.lr = se.lr, call =
call,
                call.abbrev = call.ab, prior = prior, terms = Terms, rss =
rss,
                df = df, residuals = residuals, fitted = fitted, s.e. = 
s.e..old)
        class(fit) <- "hblm"
        fit
}
>


From ted.harding at nessie.mcc.ac.uk  Thu Feb 22 15:10:36 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 22 Feb 2007 14:10:36 -0000 (GMT)
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <Pine.LNX.4.44.0702221311250.14521-100000@reclus.nhh.no>
Message-ID: <XFMail.070222141036.ted.harding@nessie.mcc.ac.uk>

On 22-Feb-07 Roger Bivand wrote:
> On 21 Feb 2007, Russell Senior wrote:
> 
>> 
>> I am interested in making a random sample from a uniform distribution
>> of points over the surface of the earth, using the WGS84 ellipsoid as
>> a model for the earth.  I know how to do this for a sphere, but would
>> like to do better.  I can supply random numbers, want latitude
>> longitude pairs out.
>> 
>> Can anyone point me at a solution?  Thanks very much.
>> 
> 
> http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.html
> 
> looks promising, untried.

Hmmm ... That page didn't seem to be directly useful, since
on my understanding of the code (and comments) listed under
"subroutine uniform_on_ellipsoid_map(dim_num, n, a, r, seed, x)"
"UNIFORM_ON_ELLIPSOID_MAP maps uniform points onto an ellipsoid."
in

http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.f90

it takes points uniformly distributed on a sphere and then
linearly transforms these onto an ellipsoid. This will not
give unform density over the surface of the ellipsoid: indeed
the example graph they show of points on an ellipse generated
in this way clearly appear to be more dense at the "ends" of
the ellipse, and less dense on its "sides". See:

http://www.csit.fsu.edu/~burkardt/f_src/random_data/
uniform_on_ellipsoid_map.png
[all one line]

Indeed, if I understand their method correctly, in the case
of a horizontal ellipse it is equivalent (modulo rotating
the result) to distributing the points uniformly over a circle,
and then stretching the circle sideways. This will preserve
the vertical distribution (so at the two ends of the major axis
it has the same density as on the circle) but diluting the
horizontal distribution (so that at the two ends of the minor
axis the density isless than on the circle).

I did have a notion about this, but sat on it expecting that
someone would come up with a slick solution -- which hasn't
happened yet.

For the application you have in hand, uniform distribution
over a sphere is a fairly close approximation to uniform
distriobution over the ellipspoid -- but not quite.

But a rejection method, applied to points uniform on the sphere,
can give you points uniform on the ellipsoid and, because of
the close approximation of the sphere to the ellipsoid, you
would not be rejecting many points.

The outline strategy I had in mind (I haven't worked out details)
is based on the following.

Consider a point X0 on the sphere, at radial distance r0 from
the centre of the sphere (same as the centre of the ellipsoid).
Let the radius through that point meet the ellipsoid at a point
X1, at radial distance R1.

Let dS0 be an element of area at X0 on the sphere, which projects
radially onto an element of area dS1 on the ellipsoid. You want
all elements dS1 of equal size to be equally likely to receive
a random point.

Let the angle between the tangent plane to the ellipsoid at X1,
and the tangent plane to the sphere at X0, be phi.

The the ratio of areas dS1/dS0 is R(X0), say, where

  R(X0) = dS1/dS0 = r1^2/(r0^2 * cos(phi))

and the smaller this ratio, the less likely you want a point
u.d. on the sphere to give rise to a point on the ellipsoid.

Now define a rejection probability P(X0) by

  P(X0) = R(X0)/sup[R(X)]

taking the supremum over X on the sphere. Then sample points X0
unformly on the sphere, rejecting each on with probability
P(X0), and continue sampling until you have the number of
points that you need.

Maybe someone has a better idea ... (or code for the above!)

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Feb-07                                       Time: 14:10:13
------------------------------ XFMail ------------------------------


From albert.chris at gmail.com  Thu Feb 22 04:22:42 2007
From: albert.chris at gmail.com (Christopher Albert)
Date: Wed, 21 Feb 2007 19:22:42 -0800
Subject: [R] Updating or installing R packages on Windows Vista
Message-ID: <33a1c64a0702211922l16c8f63anb326b48efc0fae07@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070221/0508fdc0/attachment.pl 

From Alexis.berg at locean-ipsl.upmc.fr  Thu Feb 22 12:14:57 2007
From: Alexis.berg at locean-ipsl.upmc.fr (Alexis.berg at locean-ipsl.upmc.fr)
Date: Thu, 22 Feb 2007 12:14:57 +0100 (CET)
Subject: [R] several Filled.contour plots on the same device...
Message-ID: <39828.134.157.178.21.1172142897.squirrel@www.lodyc.jussieu.fr>



hello -

a question about filled.contour plots, for which i haven't found a
response in previous posts - sorry if already treated.

i'd like to draw several filled.contour plots (that is, maps) on the same
device (a postscript file, actually). I know about layout(matrix) ,
split.screen or par(mfrow) : it works well for simple plots, but with
filled.contour plots, i get several pages instead of one page divided into
several cells.
I'd really like to get these maps directly on one graphs, without having
to process them afterwards.
Does anyone know something about that ?

Thank you for your help,

Alexis Berg

Ing?nieur de recherche
LOCEAN (IPSL) - Paris
alexis.berg at locean-ipsl.upmc.fr


From johannes_graumann at web.de  Thu Feb 22 15:33:14 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 22 Feb 2007 15:33:14 +0100
Subject: [R] List filtration
Message-ID: <erk9ja$3m1$1@sea.gmane.org>

Hello R-ologists,

Imagine you have a list "list" like so:

>list
[[1]]
[1] "IPI00776145.1" "IPI00776187.1"

[[2]]
[1] "Something" "IPI00807764.1" "IPI00807887.1"

[[3]]
[1] "IPI00807764.1"

[[4]]
[1] "Somethingelse"

What I need to achieve is a filtered list "list2" like so:

>list2
[[1]]
[1] "IPI00776145.1"

[[2]]
[1] "IPI00807764.1"

[[3]]
[1] "IPI00807764.1"

So: 
- if sublist-entry 1 start with "^IPI" make it the list-entry.
- otherwise chose the first "^IPI" sublist-entry present.
- delete the list-entry if not "^IPI" sublist-entry present.

Can anybody nudge me towards an elegant solution without looping - I have
LOTS of entries to process ...

Thanks for your Teachings,

Joh


From Charles.Annis at StatisticalEngineering.com  Thu Feb 22 15:33:58 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 22 Feb 2007 09:33:58 -0500
Subject: [R] Updating or installing R packages on Windows Vista
In-Reply-To: <33a1c64a0702211922l16c8f63anb326b48efc0fae07@mail.gmail.com>
References: <33a1c64a0702211922l16c8f63anb326b48efc0fae07@mail.gmail.com>
Message-ID: <037a01c7568e$7d224090$6400a8c0@DD4XFW31>

Or you can right-click on the R icon and choose Run as administrator.  That
way you won't alter the security settings and forget to re-set them. After
the packages are installed R will load in the usual way by clicking on the
icon.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christopher Albert
Sent: Wednesday, February 21, 2007 10:23 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Updating or installing R packages on Windows Vista

Hi,

Windows Vista includes additional security mechanisms (User Access Control)
whose defaults make it difficult to install or update R packages.
To avoid these problems you need to go to Computer-> Program Files
Right click on the R directory and select "properties". Now select the
security tab.
Give your user ( which is the use R whose priviledges R runs under) Full
Control to the R directory.
This should solve the install/update issues.

Keep up the good work.

Chris Albert

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From luke at stat.uiowa.edu  Thu Feb 22 15:36:27 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 22 Feb 2007 08:36:27 -0600 (CST)
Subject: [R] how much performance penalty does this incur,
 scalar as a vector of one element?
In-Reply-To: <748347.87140.qm@web53702.mail.yahoo.com>
References: <748347.87140.qm@web53702.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0702220821400.2959@nokomis.stat.uiowa.edu>

I think the short answer is not much.

Longer answer: In an interpreted framework with double precision
floating point scalars there is little chance of avoiding fresh
allocations for each scalar; given that, the overhead associated with
length checks can be made negligible.  (That isn't to say it currently
is--it may or may not be, but you asked about design.)  Systems that
support integer scalars often represent them as immediate values within
pointers by sacrificing one or two bits of precision in the integers,
but that doesn't work for double precision floats except possibly on
64-bit systems.  Though even there it would be possible to use an
efficient internal representation of vectors of length one without
changing the concept that everything is a vector.

As we think about compilation there are opportunities to produce more
efficient code if values can be assumed to be scalars, but that can be
accomplished by adding a declaration mechanism.  So again the answer
in terms of efficiency cost is not much.

The APL view of everything as an array, with zero-dimensional arrays
being scalars and higher-dimensional arrays being real entities rather
than decorated vectors, is in many ways conceptually cleaner and might
in hindsight have been a better choice for that reason, but efficiency
isn't really a consideration.

Best,

luke

On Wed, 21 Feb 2007, Jason Liao wrote:

>
> I have been comparing R with other languages and systems. One peculiar feature of R is there is no scalar. Instead, it is just a vector of length one. I wondered how much performance penalty this deign cause, particular in situations with many scalars in a program. Thanks.
>
>
>
> Jason Liao, http://www.geocities.com/jg_liao
> Associate Professor of Biostatistics
> Drexel University School of Public Health
> 245 N. 15th Street, Mail Stop 660
> Philadelphia, PA 19102-1192
> phone 215-762-3934
>
>
>
>
>
> ____________________________________________________________________________________
> TV dinner still cooling?
> Check out "Tonight's Picks" on Yahoo! TV.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From simon.kempf at web.de  Thu Feb 22 15:48:04 2007
From: simon.kempf at web.de (Simon P. Kempf)
Date: Thu, 22 Feb 2007 15:48:04 +0100
Subject: [R] Diagnostic Tests: Jarque-Bera Test / RAMSEY
Message-ID: <E1HKFF8-0008SH-00@smtp06.web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/37eba4f6/attachment.pl 

From ebolanger at yahoo.com  Thu Feb 22 15:32:11 2007
From: ebolanger at yahoo.com (Edward Bolanger)
Date: Thu, 22 Feb 2007 06:32:11 -0800 (PST)
Subject: [R] Package for Screen Scrapers?
Message-ID: <135509.75966.qm@web62009.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/652317cf/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Thu Feb 22 15:52:43 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 22 Feb 2007 14:52:43 -0000 (GMT)
Subject: [R] [Correction] random uniform sample of points on an
	ellipsoid (e.g. WG
In-Reply-To: <Pine.LNX.4.44.0702221311250.14521-100000@reclus.nhh.no>
Message-ID: <XFMail.070222141036.ted.harding@nessie.mcc.ac.uk>

[OOPS! A necessary correction to the method I suggested previously]

[preceding content deleted]
The outline strategy I had in mind (I haven't worked out details)
is based on the following.

Consider a point X0 on the sphere, at radial distance r0 from
the centre of the sphere (same as the centre of the ellipsoid).
Let the radius through that point meet the ellipsoid at a point
X1, at radial distance R1.

Let dS0 be an element of area at X0 on the sphere, which projects
radially onto an element of area dS1 on the ellipsoid. You want
all elements dS1 of equal size to be equally likely to receive
a random point.

Let the angle between the tangent plane to the ellipsoid at X1,
and the tangent plane to the sphere at X0, be phi.

The the ratio of areas dS1/dS0 is R(X0), say, where

  R(X0) = dS1/dS0 = r1^2/(r0^2 * cos(phi))

and the smaller this ratio, the less likely you want a point
u.d. on the sphere to give rise to a point on the ellipsoid.

[XXX: Now define a rejection probability P(X0) by :XXX]
Now define an acceptance probability P(X0) by

  P(X0) = R(X0)/sup[R(X)]

taking the supremum over X on the sphere. Then sample points X0
unformly on the sphere, [XXX: rejecting :XXX] accepting each one
with probability P(X0), and continue sampling until you have the
number of points that you need.

Maybe someone has a better idea ... (or code for the above!)

Ted.


From skiadas at hanover.edu  Thu Feb 22 16:01:33 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 22 Feb 2007 10:01:33 -0500
Subject: [R] Cross-tabulations next to each other
Message-ID: <2B4FFDEC-D891-46BC-8A79-FACA63CA20E5@hanover.edu>

I have the following relatively simple problem. Say we have three  
factors, and we want to create a cross-tabulation against each of the  
other two:

x <- factor(rbinom(5, 1, 1/2))
y <- factor(rbinom(5, 1, 1/2))
z <- factor(rbinom(5, 1, 1/2))
table(x,y)
table(x,z)

This looks like:

    y
x   0 1
   0 2 0
   1 1 2

    z
x   0 1
   0 1 1
   1 2 1

I would like to get (surely this will look a mess in non-monospaced  
fonts):

    y    z
x   0 1  0 1
   0 2 0  1 1
   1 1 2  2 1

Or something along those lines. Then I would like to convert this to  
a LaTeX table, in the obvious sort of way.

I couldn't find an answer with a quick look through the  
documentation. Are these two things already done, before I try to  
roll my own?

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From dimitris.rizopoulos at med.kuleuven.be  Thu Feb 22 16:08:08 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 22 Feb 2007 16:08:08 +0100
Subject: [R] List filtration
References: <erk9ja$3m1$1@sea.gmane.org>
Message-ID: <010d01c75693$43117fb0$0540210a@www.domain>

try this:

lis. <- lapply(lis, function(x) if (length(ind <- grep("^IPI", x))) 
x[ind[1]] else NULL)
lis.[!sapply(lis., is.null)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Johannes Graumann" <johannes_graumann at web.de>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 22, 2007 3:33 PM
Subject: [R] List filtration


> Hello R-ologists,
>
> Imagine you have a list "list" like so:
>
>>list
> [[1]]
> [1] "IPI00776145.1" "IPI00776187.1"
>
> [[2]]
> [1] "Something" "IPI00807764.1" "IPI00807887.1"
>
> [[3]]
> [1] "IPI00807764.1"
>
> [[4]]
> [1] "Somethingelse"
>
> What I need to achieve is a filtered list "list2" like so:
>
>>list2
> [[1]]
> [1] "IPI00776145.1"
>
> [[2]]
> [1] "IPI00807764.1"
>
> [[3]]
> [1] "IPI00807764.1"
>
> So:
> - if sublist-entry 1 start with "^IPI" make it the list-entry.
> - otherwise chose the first "^IPI" sublist-entry present.
> - delete the list-entry if not "^IPI" sublist-entry present.
>
> Can anybody nudge me towards an elegant solution without looping - I 
> have
> LOTS of entries to process ...
>
> Thanks for your Teachings,
>
> Joh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rguha at indiana.edu  Thu Feb 22 16:10:46 2007
From: rguha at indiana.edu (Rajarshi Guha)
Date: Thu, 22 Feb 2007 10:10:46 -0500
Subject: [R] List filtration
In-Reply-To: <erk9ja$3m1$1@sea.gmane.org>
References: <erk9ja$3m1$1@sea.gmane.org>
Message-ID: <1172157046.7812.12.camel@localhost>

On Thu, 2007-02-22 at 15:33 +0100, Johannes Graumann wrote:
> Hello R-ologists,
> 
[snip]
> 
> So: 
> - if sublist-entry 1 start with "^IPI" make it the list-entry.
> - otherwise chose the first "^IPI" sublist-entry present.
> - delete the list-entry if not "^IPI" sublist-entry present.

One way to do it would be:

l <- list(c("IPI00776145.1", "IPI00776187.1"),
          c("Something", "IPI00807764.1", "IPI00807887.1"),
          c("IPI00807764.1"),
          c("Somethingelse"))

f <- function(x) {
  r <- grep("^IPI", x, value=TRUE)
  if (length(r) > 0) return(r[1])
  else return(NA)
}

l2 <- unlist(lapply(l, f))
l2 <- l2[!is.na(l2)]

But I'm sure that more elegant solutions will be posted

-------------------------------------------------------------------
Rajarshi Guha <rguha at indiana.edu>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Writing software is more fun than working.


From cincinattikid at bigpond.com  Thu Feb 22 16:14:23 2007
From: cincinattikid at bigpond.com (Alfonso Sammassimo)
Date: Fri, 23 Feb 2007 02:14:23 +1100
Subject: [R] how to show date with this subset
Message-ID: <002201c75694$2250aed0$0300a8c0@Vaio>

Dear List,

Thankyou to Jim and Marc for their help on my previous question.

I have a data frame of five columns, the first being a list of dates and the 
other four columns are numeric values. I wanted to list the days where all 4 
columns of values are less than in the previous row. I used the following 
which works fine, except it doesnt show the dates for each row (the values 
from column 1).

differences <- apply(x, 2, diff)
all.lower.diffs <- subset(differences, apply(differences, 1, 
function(x){all(x<0)}

I tried using the following loop instead, but it would only apply to the 
first column of every row("warning condition has length>1 and only first 
element will be used"):

for ( i in 2:length(x[,1]))  if (x[i,]<x[i-1,]) {print(x[i,])}

How can I resolve with either method? Any help much appreciated.

Regards,
Alf Sammassimo.


From j.van_den_hoff at fzd.de  Thu Feb 22 16:16:21 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Thu, 22 Feb 2007 16:16:21 +0100
Subject: [R] R CMD CHECK question
Message-ID: <20070222151621.GA17637@marco.fz-rossendorf.de>

hi,

I have two private packages, the first (`pkc') depending on the second one
(`roiutils'). The source code and DESCRIPTION files describes the dependency
as it should be ('Imports', `require'), at least I think so.

now, running

R CMD CHECK pkc 

yields the following output in which I have inserted my
questions (lines starting with -->):

* checking for working latex ... OK
* using log directory '/Users/vdh/rfiles/Rlibrary/.check/pkc.Rcheck'
* using R version 2.4.0 (2006-10-03)
* checking for file 'pkc/DESCRIPTION' ... OK
* this is package 'pkc' version '1.1'
* checking package dependencies ... OK
* checking if this is a source package ... OK
* checking whether package 'pkc' can be installed ... WARNING
Found the following significant warnings:
       missing link(s):  readroi readroi readroi figure readroi conv3exmodel readroi
       missing link(s):  figure readroi

--> there _are_ links to the mentioned functions (from `roiutils') in the
--> manpages of `pkc'. after installing the libs, the help system works just
--> fine. why is it, that CHECK is complaining? can one selectively switch of
--> this warning? or how have I to specify the links to tell CHECK that
--> everything is OK?

* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking top-level files ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for non-ASCII characters ... OK
* checking whether the package can be loaded ... OK
* checking whether the package can be loaded with stated dependencies ... OK
* checking whether the name space can be loaded with stated dependencies ... OK
* checking S3 generic/method consistency ... OK
* checking replacement functions ... OK
* checking foreign function calls ... OK
* checking R code for possible problems ... OK
* checking Rd files ... WARNING
Rd files with unknown sections:
  /Users/vdh/rfiles/Rlibrary/pkc/man/fitdemo.Rd: example

See the chapter 'Writing R documentation files' in manual 'Writing R
Extensions'.
* checking Rd cross-references ... WARNING
Missing link(s) in documentation object 'compfit.Rd':
  readroi readroi readroi figure readroi conv3exmodel readroi

Missing link(s) in documentation object 'exp3fit.Rd':
  figure readroi

--> this seems the same problem as above, right?





any hints appreciated,

joerg


From cjkogan111 at yahoo.com  Thu Feb 22 16:23:04 2007
From: cjkogan111 at yahoo.com (cjkogan111)
Date: Thu, 22 Feb 2007 07:23:04 -0800 (PST)
Subject: [R] Debugging S Plus
In-Reply-To: <6344607.post@talk.nabble.com>
References: <6344607.post@talk.nabble.com>
Message-ID: <9101832.post@talk.nabble.com>


Does anyone know the basic commands to debug in s plus. I know how to debug
in r, but I'm having trouble finding similar tools. Mostly what I want to
know is what is the equivalent of the debug() command, and what do I use to
move to the next line, and get the values of different variables. 
Thanks!
-cjkogan111
-- 
View this message in context: http://www.nabble.com/Error-Message-Documentation-tf2283899.html#a9101832
Sent from the R help mailing list archive at Nabble.com.


From dimitris.rizopoulos at med.kuleuven.be  Thu Feb 22 16:27:54 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 22 Feb 2007 16:27:54 +0100
Subject: [R] Cross-tabulations next to each other
References: <2B4FFDEC-D891-46BC-8A79-FACA63CA20E5@hanover.edu>
Message-ID: <012401c75696$058369d0$0540210a@www.domain>

maybe cbind() is close to what you're looking for, e.g.,

tb1 <- table(x, y)
tb2 <- table(x, z)

cbind(tb1, tb2)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Charilaos Skiadas" <skiadas at hanover.edu>
To: "R-Mailingliste" <r-help at stat.math.ethz.ch>
Sent: Thursday, February 22, 2007 4:01 PM
Subject: [R] Cross-tabulations next to each other


>I have the following relatively simple problem. Say we have three
> factors, and we want to create a cross-tabulation against each of 
> the
> other two:
>
> x <- factor(rbinom(5, 1, 1/2))
> y <- factor(rbinom(5, 1, 1/2))
> z <- factor(rbinom(5, 1, 1/2))
> table(x,y)
> table(x,z)
>
> This looks like:
>
>    y
> x   0 1
>   0 2 0
>   1 1 2
>
>    z
> x   0 1
>   0 1 1
>   1 2 1
>
> I would like to get (surely this will look a mess in non-monospaced
> fonts):
>
>    y    z
> x   0 1  0 1
>   0 2 0  1 1
>   1 1 2  2 1
>
> Or something along those lines. Then I would like to convert this to
> a LaTeX table, in the obvious sort of way.
>
> I couldn't find an answer with a quick look through the
> documentation. Are these two things already done, before I try to
> roll my own?
>
> Haris Skiadas
> Department of Mathematics and Computer Science
> Hanover College
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From wexler at yahoo.com  Thu Feb 22 16:29:34 2007
From: wexler at yahoo.com (Michael Wexler)
Date: Thu, 22 Feb 2007 07:29:34 -0800 (PST)
Subject: [R] Crosstabbing multiple response data
Message-ID: <760291.92877.qm@web30801.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/d6990768/attachment.pl 

From johannes_graumann at web.de  Thu Feb 22 17:10:33 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Thu, 22 Feb 2007 17:10:33 +0100
Subject: [R] List filtration
References: <erk9ja$3m1$1@sea.gmane.org>
	<010d01c75693$43117fb0$0540210a@www.domain>
Message-ID: <erkf9p$vir$1@sea.gmane.org>

Thanks for your help!

Joh

Dimitris Rizopoulos wrote:

> try this:
> 
> lis. <- lapply(lis, function(x) if (length(ind <- grep("^IPI", x)))
> x[ind[1]] else NULL)
> lis.[!sapply(lis., is.null)]
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message -----
> From: "Johannes Graumann" <johannes_graumann at web.de>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, February 22, 2007 3:33 PM
> Subject: [R] List filtration
> 
> 
>> Hello R-ologists,
>>
>> Imagine you have a list "list" like so:
>>
>>>list
>> [[1]]
>> [1] "IPI00776145.1" "IPI00776187.1"
>>
>> [[2]]
>> [1] "Something" "IPI00807764.1" "IPI00807887.1"
>>
>> [[3]]
>> [1] "IPI00807764.1"
>>
>> [[4]]
>> [1] "Somethingelse"
>>
>> What I need to achieve is a filtered list "list2" like so:
>>
>>>list2
>> [[1]]
>> [1] "IPI00776145.1"
>>
>> [[2]]
>> [1] "IPI00807764.1"
>>
>> [[3]]
>> [1] "IPI00807764.1"
>>
>> So:
>> - if sublist-entry 1 start with "^IPI" make it the list-entry.
>> - otherwise chose the first "^IPI" sublist-entry present.
>> - delete the list-entry if not "^IPI" sublist-entry present.
>>
>> Can anybody nudge me towards an elegant solution without looping - I
>> have
>> LOTS of entries to process ...
>>
>> Thanks for your Teachings,
>>
>> Joh
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From skiadas at hanover.edu  Thu Feb 22 17:12:27 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 22 Feb 2007 11:12:27 -0500
Subject: [R] Cross-tabulations next to each other
In-Reply-To: <012401c75696$058369d0$0540210a@www.domain>
References: <2B4FFDEC-D891-46BC-8A79-FACA63CA20E5@hanover.edu>
	<012401c75696$058369d0$0540210a@www.domain>
Message-ID: <AADB4850-0E85-4EE2-857A-7EC17B7E6192@hanover.edu>

Hi Dimitris,

On Feb 22, 2007, at 10:27 AM, Dimitris Rizopoulos wrote:

> maybe cbind() is close to what you're looking for, e.g.,
>
> tb1 <- table(x, y)
> tb2 <- table(x, z)
>
> cbind(tb1, tb2)
>
Yes, that was my first thought too, and it does place the values  
where I want them, but it completely destroys the names, which I'd  
like to keep, i.e. it doesn't treat it as a table any more. The  
resulting LaTeX table I would like to have a very top row, with  
multicolumn titles, one for each factor, then a second row with the  
levels for each factor, and then below those the data. I could I  
guess add that stuff separately, but I was hoping someone had already  
done that part.
>
> Best,
> Dimitris

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From ggrothendieck at gmail.com  Thu Feb 22 17:16:05 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 22 Feb 2007 11:16:05 -0500
Subject: [R] Crosstabbing multiple response data
In-Reply-To: <760291.92877.qm@web30801.mail.mud.yahoo.com>
References: <760291.92877.qm@web30801.mail.mud.yahoo.com>
Message-ID: <971536df0702220816p2cf619bam9819f8383c45a108@mail.gmail.com>

Try this:

tab <- crossprod(as.matrix(ratings[,-1]))
tab <- tab - diag(diag(tab))
tab

tab / nrow(ratings)


On 2/22/07, Michael Wexler <wexler at yahoo.com> wrote:
> Using R version 2.4.1 (2006-12-18) on Windows, I have a dataset which resembles this:
>
> id    att1    att2    att3
> 1    1        1        0
> 2    1        0        0
> 3    0        1        1
> 4    1        1        1
>
> ratings <- data.frame(id = c(1,2,3,4), att1 = c(1,1,0,1), att2 = c(1,0,0,1), att3 = c(0,1,1,1))
>
> I would like to get a cross tab of counts of co-ocurrence, which might resemble this:
>
>    att1    att2    att3
> att1         2       1
> att2    2            2
> att3    1    2
>
> with the hope of understanding, at least pairwise, what things "hang together".   (Yes, there are much, much better ways to do this statistically including clustering and binary corrected correlation, but the audience I am working with asked for this version for a specific reason.)
>
> (Later on, I would also like to convert to percentages of the total unique pop, so the final version of the table would be
>
>
>    att1    att2    att3
>
> att1         50%       25%
>
> att2    50%            50%
>
> att3    25%    50%
>
>
> But I can do this in excel if I can get the first table out.)
>
> I have tried the reshape library, but could not get anything resembling this (both on its own, as well as feeding in to table()).  (I have also played with transposing and using some comments from this list from 2002 and 2004, but the questioners appear to assume more knowledge than I have in use of R; the example in the posting guide was also more complex than I was ready for, I'm afraid.)
>
> Sample of some of my efforts:
> library(reshape)
> melt(ratings,id=c("id"))
>
> ds1 <- melt(ratings,id=c("id"))
> table(ds1$variable, ds1$variable) # returns only rowcounts, 3 along diagonal
> xtabs(formula = value ~ ds1$variable + ds1$variable , data=ds1) # returns only a single row of collapsed counts, appears to not allow 1 variable in multiple uses
>
> I suspect I am close, so any nudges in the right direction would be helpful.
>
> Thanks much, Michael
>
> PS: www.rseek.org is very impressive, I heartily encourage its use.
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jeff.hamann at forestinformatics.com  Thu Feb 22 17:17:22 2007
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Thu, 22 Feb 2007 08:17:22 -0800
Subject: [R] adjacency matrix?
Message-ID: <001701c7569c$f85bc840$9ab4fea9@mothra>

I'm curious to know if it's possible to easily generate a grid (lattice)
and obtain the adjacency matrix. For example, I would like to display a
3x3 (or 10x10) lattice then then generate the 10 x 10 adjacency matrix

> matrix( 1:9, 3,3, byrow=TRUE )
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9
>

and then use the mathgraph(?) package to generate the adjacency matrix? I've
been able to generate a simple graph as well as some basic graphs like the
one above but have two concatenate lots of simple graphs together,


gr <- c( mathgraph( ~ 1 / c(2,4) ),
mathgraph( ~ 2 / c(1,3,5) ),
mathgraph( ~ 3 / c(2,6) ),
mathgraph( ~ 4 / c(1,5,7) ),
mathgraph( ~ 5 / c(2,4,6,8) ),
mathgraph( ~ 6 / c(3,5,9) ),
mathgraph( ~ 7 / c(4,8) ),
mathgraph( ~ 8 / c(5,7,9) ),
mathgraph( ~ 9 / c(6,8) ) )

and then

plot( gr )

and

adjamat( gr )

which yields a correct adjacency matrix. Since the names of the nodes are
the values in the elements, is there are easier way to accomplish this?

Thanks,
Jeff.


-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
jeff.hamann at forestinformatics.com
www.forestinformatics.com


From ripley at stats.ox.ac.uk  Thu Feb 22 17:28:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Feb 2007 16:28:05 +0000 (GMT)
Subject: [R] how to show date with this subset
In-Reply-To: <002201c75694$2250aed0$0300a8c0@Vaio>
References: <002201c75694$2250aed0$0300a8c0@Vaio>
Message-ID: <Pine.LNX.4.64.0702221621100.12767@gannet.stats.ox.ac.uk>

On Fri, 23 Feb 2007, Alfonso Sammassimo wrote:

> Dear List,
>
> Thankyou to Jim and Marc for their help on my previous question.
>
> I have a data frame of five columns, the first being a list of dates and the
> other four columns are numeric values. I wanted to list the days where all 4
> columns of values are less than in the previous row. I used the following
> which works fine, except it doesnt show the dates for each row (the values
> from column 1).
>
> differences <- apply(x, 2, diff)

Please don't use apply() columnwise on data frames: it turns them into 
matrices. Here you could use

tmp <- lapply(x[2:5], diff)
ind <- do.call("pmax", tmp) < 0 
x[c(FALSE, ind), ]

> all.lower.diffs <- subset(differences, apply(differences, 1,
> function(x){all(x<0)}
>
> I tried using the following loop instead, but it would only apply to the
> first column of every row("warning condition has length>1 and only first
> element will be used"):
>
> for ( i in 2:length(x[,1]))  if (x[i,]<x[i-1,]) {print(x[i,])}
                                    ^all(        ^)

> How can I resolve with either method? Any help much appreciated.
>
> Regards,
> Alf Sammassimo.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From vincent.goulet at act.ulaval.ca  Thu Feb 22 17:41:10 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Thu, 22 Feb 2007 11:41:10 -0500
Subject: [R] Accessing the class of an object with two elements.
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3010B157D@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3010B157D@BAN-MAILSRV03.Amba.com>
Message-ID: <200702221141.10696.vincent.goulet@act.ulaval.ca>

Le Jeudi 22 F?vrier 2007 05:37, Shubha Vishwanath Karanth a ?crit?:
> Hi R,
>
> Here's my question about accessing the class of an object.
>
> I have an object "dat" which can take any two of the classes, ("dates"
> "times") or ("chron" "dates" "times). Note that the classes have two
> elements within it. I want to read these classes in such a way that
>
> v=class(dat) # let class(dat)= "dates" "times"
>
> If(class(dat)==v) k=1 else k=0
>
> The problem is I can't read the above class. The error which I get for
> the above if statement is as follows:
>
> Warning message:
>
> the condition has length > 1 and only the first element will be used in:
> if (class(index(intra)) == v) k = 1
>
> How should I proceed with this? Any ideas? I tried with readline to read
> the class and access it in the 'if' statement...But doesn't work :-(

Well, given your code the condition in the 'if' statement will return c(TRUE, 
TRUE), hence the warning. Executing your code piece by piece would tell you 
that.

That said, you probably rather want to use inherit() for such purposes.

HTH

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From frainj at gmail.com  Thu Feb 22 17:57:50 2007
From: frainj at gmail.com (John C Frain)
Date: Thu, 22 Feb 2007 16:57:50 +0000
Subject: [R] Diagnostic Tests: Jarque-Bera Test / RAMSEY
In-Reply-To: <E1HKFF8-0008SH-00@smtp06.web.de>
References: <E1HKFF8-0008SH-00@smtp06.web.de>
Message-ID: <fad888a10702220857s413e421vf4eeacfd56a23874@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/bb9e1796/attachment.pl 

From Greg.Snow at intermountainmail.org  Thu Feb 22 18:03:25 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 22 Feb 2007 10:03:25 -0700
Subject: [R] several Filled.contour plots on the same device...
Message-ID: <07E228A5BE53C24CAD490193A7381BBB83C554@LP-EXCHVS07.CO.IHC.COM>

The problem is that filled.contour uses the layout function internally which messes up any other use of layout, split.screen, or mfrow.  One alternative is to use the levelplot function from the lattice package, or you could use filled.contour to make several full page plots to a pdf file, then use an external utility like pdfpages to combine them onto a single page.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Alexis.berg at locean-ipsl.upmc.fr
> Sent: Thursday, February 22, 2007 4:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] several Filled.contour plots on the same device...
> 
> 
> 
> hello -
> 
> a question about filled.contour plots, for which i haven't 
> found a response in previous posts - sorry if already treated.
> 
> i'd like to draw several filled.contour plots (that is, maps) 
> on the same device (a postscript file, actually). I know 
> about layout(matrix) , split.screen or par(mfrow) : it works 
> well for simple plots, but with filled.contour plots, i get 
> several pages instead of one page divided into several cells.
> I'd really like to get these maps directly on one graphs, 
> without having to process them afterwards.
> Does anyone know something about that ?
> 
> Thank you for your help,
> 
> Alexis Berg
> 
> Ing?nieur de recherche
> LOCEAN (IPSL) - Paris
> alexis.berg at locean-ipsl.upmc.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cberry at tajo.ucsd.edu  Thu Feb 22 18:56:27 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 22 Feb 2007 09:56:27 -0800
Subject: [R] Sorting rows of a binary matrix
In-Reply-To: <OF5B99A59B.A8B8D43E-ONC125728A.003B8FD2-C125728A.003B8FE0@wsr.ac.at>
References: <OF5B99A59B.A8B8D43E-ONC125728A.003B8FD2-C125728A.003B8FE0@wsr.ac.at>
Message-ID: <Pine.LNX.4.64.0702220952100.1606@tajo.ucsd.edu>

On Thu, 22 Feb 2007, Serguei Kaniovski wrote:

>
> Hallo,
>
> The command:
>
> x <- 3
> mat <- as.matrix(expand.grid(rep(list(0:1), x)))
>
> generates a matrix with 2^x columns containing the binary representations
> of the decimals from 0 to (2^x-1), here from 0 to 7. But the rows are not
> sorted in this order.
>
> How can sort the rows the ascending order of the decimals they represent,
> preferably without a function which converts binaries to decimals (which I
> have)? Alternatively, generate a matrix that has the rows sorted that way?

The alternative:

    mat <- as.matrix(expand.grid(rep(list(0:1), x))[ , x:1 ]  )


>
> Thanks,
> Serguei
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From cjkogan111 at yahoo.com  Thu Feb 22 19:11:12 2007
From: cjkogan111 at yahoo.com (cjkogan111)
Date: Thu, 22 Feb 2007 10:11:12 -0800 (PST)
Subject: [R] S Plus Debugging
Message-ID: <9105226.post@talk.nabble.com>


Hello,
I am trying to debug in S+. I know how to debug in r, but the commands in S+
seem to be different. I am just looking for a command that allows me to
debug the function, and then commands that allow me to step through and find
the values of different variables.
Thanks!
-cjkogan111
-- 
View this message in context: http://www.nabble.com/S-Plus-Debugging-tf3274285.html#a9105226
Sent from the R help mailing list archive at Nabble.com.


From matthieu.cornec at gmail.com  Thu Feb 22 19:13:47 2007
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Thu, 22 Feb 2007 19:13:47 +0100
Subject: [R] Help using Sweave with wireframe or cloud
Message-ID: <8a83e5000702221013v5d651c13u985776151e7e7b3b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/b5c35f71/attachment.pl 

From cberry at tajo.ucsd.edu  Thu Feb 22 19:17:44 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 22 Feb 2007 10:17:44 -0800
Subject: [R] Crosstabbing multiple response data
In-Reply-To: <760291.92877.qm@web30801.mail.mud.yahoo.com>
References: <760291.92877.qm@web30801.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0702221006430.1606@tajo.ucsd.edu>


> res <- crossprod( as.matrix( ratings[ , -1] ) )
> diag(res) <- ""
> print(res, quote=F)
      att1 att2 att3
att1      2    1
att2 2         2
att3 1    2
> 
> res2 <- crossprod(as.matrix( ratings[ , -1])) * 100 / nrow( ratings )
> res2[] <- paste( res2, "%", sep="" )
> diag(res2) <- ""
> print(res2, quote=F)
      att1 att2 att3
att1      50%  25%
att2 50%       50%
att3 25%  50%
>

Be sure to bone up on format and sprintf before taking this into 
production.

On Thu, 22 Feb 2007, Michael Wexler wrote:

> Using R version 2.4.1 (2006-12-18) on Windows, I have a dataset which resembles this:
>
> id    att1    att2    att3
> 1    1        1        0
> 2    1        0        0
> 3    0        1        1
> 4    1        1        1
>
> ratings <- data.frame(id = c(1,2,3,4), att1 = c(1,1,0,1), att2 = c(1,0,0,1), att3 = c(0,1,1,1))
>
> I would like to get a cross tab of counts of co-ocurrence, which might resemble this:
>
>    att1    att2    att3
> att1         2       1
> att2    2            2
> att3    1    2
>
> with the hope of understanding, at least pairwise, what things "hang together".   (Yes, there are much, much better ways to do this statistically including clustering and binary corrected correlation, but the audience I am working with asked for this version for a specific reason.)
>
> (Later on, I would also like to convert to percentages of the total unique pop, so the final version of the table would be
>
>
>    att1    att2    att3
>
> att1         50%       25%
>
> att2    50%            50%
>
> att3    25%    50%
>
>
> But I can do this in excel if I can get the first table out.)
>
> I have tried the reshape library, but could not get anything resembling this (both on its own, as well as feeding in to table()).  (I have also played with transposing and using some comments from this list from 2002 and 2004, but the questioners appear to assume more knowledge than I have in use of R; the example in the posting guide was also more complex than I was ready for, I'm afraid.)
>
> Sample of some of my efforts:
> library(reshape)
> melt(ratings,id=c("id"))
>
> ds1 <- melt(ratings,id=c("id"))
> table(ds1$variable, ds1$variable) # returns only rowcounts, 3 along diagonal
> xtabs(formula = value ~ ds1$variable + ds1$variable , data=ds1) # returns only a single row of collapsed counts, appears to not allow 1 variable in multiple uses
>
> I suspect I am close, so any nudges in the right direction would be helpful.
>
> Thanks much, Michael
>
> PS: www.rseek.org is very impressive, I heartily encourage its use.
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From kevin.thorpe at utoronto.ca  Thu Feb 22 19:22:42 2007
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Thu, 22 Feb 2007 13:22:42 -0500
Subject: [R] Help using Sweave with wireframe or cloud
In-Reply-To: <8a83e5000702221013v5d651c13u985776151e7e7b3b@mail.gmail.com>
References: <8a83e5000702221013v5d651c13u985776151e7e7b3b@mail.gmail.com>
Message-ID: <45DDDF72.8040208@utoronto.ca>

Matthieu Cornec wrote:
> Hi,
> 
> 
> I used the sweave package to get a 3D plot
> 
> <<echo=F,fig=F>>=
> wireframe(volcano, shade = TRUE,
>           aspect = c(61/87, 0.4),
>           light.source = c(10,0,10))
> @
> 
> but it gives an error message for the pdf file of this picture.
> 
> Any one could help ?
> 

Try this.

<<echo=FALSE,fig=TRUE>>=
library(lattice)
print(wireframe(volcano, shade = TRUE, aspect = c(61/87, 0.4),
light.source = c(10,0,10)))
@


> Thanks in advance,
> 
> Matthieu
> 


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.6057


From roger.bos at us.rothschild.com  Thu Feb 22 20:09:26 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Thu, 22 Feb 2007 14:09:26 -0500
Subject: [R] How to print a double quote
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A1CF9@RINNYCSE000.rth.ad.rothschild.com>

Can anyone tell me how to get R to include a double quote in the middle
of a character string?  

For example, the following code is close:

> fnd<-"Open fnd 'test'"
>            cat(fnd)
Open fnd 'test'>            
>

But instead of Open fnd 'test' I need: Open fnd "test".  Difference
seems minor, but I am writing batch files for another program to read in
and it has to have the double quotes to work.  

Thanks in advance for any help or ideas,

Roger

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From tplate at acm.org  Thu Feb 22 20:17:59 2007
From: tplate at acm.org (Tony Plate)
Date: Thu, 22 Feb 2007 12:17:59 -0700
Subject: [R] How to print a double quote
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A1CF9@RINNYCSE000.rth.ad.rothschild.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1CF9@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <45DDEC67.3080906@acm.org>

 > cat('Open fnd "test"\n')
Open fnd "test"
 > cat("Open fnd \"test\"\n")
Open fnd "test"
 >

Bos, Roger wrote:
> Can anyone tell me how to get R to include a double quote in the middle
> of a character string?  
> 
> For example, the following code is close:
> 
>> fnd<-"Open fnd 'test'"
>>            cat(fnd)
> Open fnd 'test'>            
> 
> But instead of Open fnd 'test' I need: Open fnd "test".  Difference
> seems minor, but I am writing batch files for another program to read in
> and it has to have the double quotes to work.  
> 
> Thanks in advance for any help or ideas,
> 
> Roger
> 
> ********************************************************************** * 
> This message is for the named person's use only. It may 
> contain confidential, proprietary or legally privileged 
> information. No right to confidential or privileged treatment 
> of this message is waived or lost by any error in 
> transmission. If you have received this message in error, 
> please immediately notify the sender by e-mail, 
> delete the message and all copies from your system and destroy 
> any hard copies. You must not, directly or indirectly, use, 
> disclose, distribute, print or copy any part of this message 
> if you are not the intended recipient.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maitra at iastate.edu  Thu Feb 22 20:21:32 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 22 Feb 2007 13:21:32 -0600
Subject: [R] applying lm on an array of observations with common design
 matrix
In-Reply-To: <Pine.LNX.4.64.0702220810470.2616@gannet.stats.ox.ac.uk>
References: <20070217202355.0ee7c70c@triveni.stat.iastate.edu>
	<Pine.LNX.4.64.0702180736050.10281@gannet.stats.ox.ac.uk>
	<20070221231503.3f26b06b@triveni.stat.iastate.edu>
	<45DD407E.2070104@karlin.mff.cuni.cz>
	<Pine.LNX.4.64.0702220810470.2616@gannet.stats.ox.ac.uk>
Message-ID: <20070222132132.7c8107b5@subarnarekha.stat.iastate.edu>

On Thu, 22 Feb 2007 08:17:38 +0000 (GMT) Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Thu, 22 Feb 2007, Petr Klasterecky wrote:
> 
> > Ranjan Maitra napsal(a):
> >> On Sun, 18 Feb 2007 07:46:56 +0000 (GMT) Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> >>
> >>> On Sat, 17 Feb 2007, Ranjan Maitra wrote:
> >>>
> >>>> Dear list,
> >>>>
> >>>> I have a 4-dimensional array Y of dimension 330 x 67 x 35 x 51. I have a
> >>>> design matrix X of dimension 330 x 4. I want to fit a linear regression
> >>>> of each
> >>>>
> >>>> lm( Y[, i, j, k] ~ X). for each i, j, k.
> >>>>
> >>>> Can I do it in one shot without a loop?
> >>> Yes.
> >>>
> >>> YY <- YY
> >>> dim(YY) <- c(330, 67*35*51)
> >>> fit <- lm(YY ~ X)
> >>>
> >>>> Actually, I am also interested in getting the p-values of some of the
> >>>> coefficients -- lets say the coefficient corresponding to the second
> >>>> column of the design matrix. Can the same be done using array-based
> >>>> operations?
> >>> Use lapply(summary(fit), function(x) coef(x)[3,4])  (since there is a
> >>> intercept, you want the third coefficient).
> >>
> >> In this context, can one also get the variance-covariance matrix of the 
> >> coefficients?
> >
> > Sure:
> >
> > lapply(summary(fit), function(x) {"$"(x,cov.unscaled)})
> 
> But that is not the variance-covariance matrix (and it is an unusual way 
> to write x$cov.unscaled)!
> 
> > Add indexing if you do not want the whole matrix. You can extract
> > whatever you want, just take a look at ?summary.lm, section Value.
> 
> It is unclear to me what the questioner expects: the estimated 
> coefficients for different responses are independent.  For a list of 
> matrices applying to each response one could mimic vcov.lm and do
> 
> lapply(summary(fit, corr=FALSE),
>         function(so) so$sigma^2 * so$cov.unscaled)

Thanks! Actually, I am really looking to compare the coefficients (let us say second and the third) beta2 - beta4 = 0 for each regression. Basically, get the two-sided p-value for the test statistic for each regression. 

One way of doing that is to get the dispersion matrix of each regression and then to compute the t-statistic and the p-value. That is the genesis of the question above. Is there a better way?

Many thanks and best wishes,
Ranjan


From Roger.Bivand at nhh.no  Thu Feb 22 20:23:12 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 22 Feb 2007 20:23:12 +0100 (CET)
Subject: [R] How to print a double quote
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A1CF9@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <Pine.LNX.4.44.0702222022360.14521-100000@reclus.nhh.no>

On Thu, 22 Feb 2007, Bos, Roger wrote:

> Can anyone tell me how to get R to include a double quote in the middle
> of a character string?  

FAQ 7.37 Why does backslash behave strangely inside strings?

> 
> For example, the following code is close:
> 
> > fnd<-"Open fnd 'test'"
> >            cat(fnd)
> Open fnd 'test'>            
> >
> 
> But instead of Open fnd 'test' I need: Open fnd "test".  Difference
> seems minor, but I am writing batch files for another program to read in
> and it has to have the double quotes to work.  
> 
> Thanks in advance for any help or ideas,
> 
> Roger
> 
> ********************************************************************** * 
> This message is for the named person's use only. It may 
> contain confidential, proprietary or legally privileged 
> information. No right to confidential or privileged treatment 
> of this message is waived or lost by any error in 
> transmission. If you have received this message in error, 
> please immediately notify the sender by e-mail, 
> delete the message and all copies from your system and destroy 
> any hard copies. You must not, directly or indirectly, use, 
> disclose, distribute, print or copy any part of this message 
> if you are not the intended recipient.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From maitra at iastate.edu  Thu Feb 22 20:23:49 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 22 Feb 2007 13:23:49 -0600
Subject: [R] How to print a double quote
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A1CF9@RINNYCSE000.rth.ad.rothschild.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1CF9@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <20070222132349.6bdde707@subarnarekha.stat.iastate.edu>

try 

> cat("Open fnd \"test\"")

which is the same as for C.

HTH.

Ranjan

On Thu, 22 Feb 2007 14:09:26 -0500 "Bos, Roger" <roger.bos at us.rothschild.com> wrote:

> Can anyone tell me how to get R to include a double quote in the middle
> of a character string?  
> 
> For example, the following code is close:
> 
> > fnd<-"Open fnd 'test'"
> >            cat(fnd)
> Open fnd 'test'>            
> >
> 
> But instead of Open fnd 'test' I need: Open fnd "test".  Difference
> seems minor, but I am writing batch files for another program to read in
> and it has to have the double quotes to work.  
> 
> Thanks in advance for any help or ideas,
> 
> Roger
> 
> ********************************************************************** * 
> This message is for the named person's use only. It may 
> contain confidential, proprietary or legally privileged 
> information. No right to confidential or privileged treatment 
> of this message is waived or lost by any error in 
> transmission. If you have received this message in error, 
> please immediately notify the sender by e-mail, 
> delete the message and all copies from your system and destroy 
> any hard copies. You must not, directly or indirectly, use, 
> disclose, distribute, print or copy any part of this message 
> if you are not the intended recipient.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From milton_ruser at yahoo.com.br  Thu Feb 22 20:36:02 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Thu, 22 Feb 2007 11:36:02 -0800 (PST)
Subject: [R] Principal Component Analysis & explained variance
Message-ID: <783981.21074.qm@web56602.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/25dc9f0c/attachment.pl 

From scionforbai at gmail.com  Thu Feb 22 20:37:58 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Thu, 22 Feb 2007 20:37:58 +0100
Subject: [R] how to install a package in R on a linux machine?
In-Reply-To: <20070222104054.GA5393@guzu>
References: <54f7e7c30702220044s7eecde1bnff77d886fb2543eb@mail.gmail.com>
	<20070222104054.GA5393@guzu>
Message-ID: <e9ee1f0a0702221137g68e10df1mf546259558516c1d@mail.gmail.com>

>> I tried a few times and still couldn't figure out the correct way
>> to install this package.

Help us to help you, Gallon. Which error comes out?

> install.packages("packagename")
> this downloads the package and installs it into the default R package
> library on your machine.

Of course, on a normal installation R need to be run by root to do
this, in order to write in a "system" directory like /usr/lib/.

> If you want to install it to a different
> directory use the 'lib' argument of 'install.packages'.

Again, the user running R must have writing access to the directory
specified by the 'lib' option. This option allows to install R
packages in a directory in your /home, typically.

Maybe you want to try (dependencies=TRUE) to automagically download
and install dependencies.

And of course you downloaded the appropriate package for you R version.


From ccleland at optonline.net  Thu Feb 22 20:43:48 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 22 Feb 2007 14:43:48 -0500
Subject: [R] Principal Component Analysis & explained variance
In-Reply-To: <783981.21074.qm@web56602.mail.re3.yahoo.com>
References: <783981.21074.qm@web56602.mail.re3.yahoo.com>
Message-ID: <45DDF274.1000900@optonline.net>

Milton Cezar Ribeiro wrote:
> Hi there,
> 
> How can I know the explaned variance of a PC axis generated by prcomp()?

  From the standard deviations of each component, you could do something
like this maybe:

prcomp(USArrests, scale = TRUE)$sdev^2 / ncol(USArrests)

[1] 0.62006039 0.24744129 0.08914080 0.04335752

> Kind regards,
> 
> miltinho
> Brazil
> 
> __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From pzs6 at cdc.gov  Thu Feb 22 20:56:48 2007
From: pzs6 at cdc.gov (Smith, Phil (CDC/CCID/NCIRD))
Date: Thu, 22 Feb 2007 14:56:48 -0500
Subject: [R] question about boxplot
Message-ID: <CE5FFC36D6AADB4C93A4D5D2125D8C6ACA7221@exp-clft3.cdc.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/cd207861/attachment.pl 

From chrysopa at gmail.com  Thu Feb 22 21:22:03 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Thu, 22 Feb 2007 18:22:03 -0200
Subject: [R] problem with weights on lmer function
Message-ID: <200702221822.04001.chrysopa@gmail.com>

Hi,

I try to make a model using lmer, but the weigths is not accept.

m1<-lmer(ocup/total~tempo+(tempo|estacao),family=binomial,weights=total)

Erro em lmer(ocup/total ~ tempo + (tempo | estacao), family = binomial,  : 
	object `weights' of incorrect type

I dont understand why this error, with glm this work. the total object is a 
vector.

Any idea?

Thanks
Ronaldo
-- 
God is subtle, but he is not malicious.
		-- Albert Einstein
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | ronaldo.reis em unimontes.br | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From chrysopa at gmail.com  Thu Feb 22 21:29:15 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Thu, 22 Feb 2007 18:29:15 -0200
Subject: [R] JGR launcher for linux
Message-ID: <200702221829.15602.chrysopa@gmail.com>

Hi,

anybody have a JGR launcher for linux? Maybe a script that launch JGR directly 
without open R then library(JGR) and JGR().

Thanks
Ronaldo
-- 
Deflector shields just came on, Captain.
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | ronaldo.reis em unimontes.br | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From Bartjoosen at hotmail.com  Thu Feb 22 21:35:29 2007
From: Bartjoosen at hotmail.com (Bart Joosen)
Date: Thu, 22 Feb 2007 21:35:29 +0100
Subject: [R] confidence intervals
Message-ID: <BAY134-DAV4EEC0C5A9AB063A5D9E09D88F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/fc105a20/attachment.pl 

From Mark.Leeds at morganstanley.com  Thu Feb 22 22:07:05 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 22 Feb 2007 16:07:05 -0500
Subject: [R] R equivalent of the VAR function  in S+Finmetrics
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401521FA6@NYWEXMB23.msad.ms.com>

I was looking at the systemfit package and it seems like I could use it
to solve OLS systems (
which is essentially what VARs are ) but the lag length would have to be
known beforehand, I think. Does anyone know
if there is an equivalent of the VAR function in Eric Zivot's
S+Finmetrics package where the lag length can be selected based on
some kind of criterion such as BIC for example. Thanks for any
idea/suggestions.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From andy_liaw at merck.com  Thu Feb 22 22:13:28 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 22 Feb 2007 16:13:28 -0500
Subject: [R] JGR launcher for linux
In-Reply-To: <200702221829.15602.chrysopa@gmail.com>
References: <200702221829.15602.chrysopa@gmail.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03C35361@usctmx1106.merck.com>

Isn't it right in front of you?  I get:

> JGR()
Starting JGR ...
(You can use /usr/local/lib64/R/library/JGR/cont/run to start JGR directly)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Andy

From: Ronaldo Reis Junior
> 
> Hi,
> 
> anybody have a JGR launcher for linux? Maybe a script that 
> launch JGR directly without open R then library(JGR) and JGR().
> 
> Thanks
> Ronaldo
> --
> Deflector shields just came on, Captain.
> --
> > Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia `. 
> | `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
> |   `- Fone: (38) 3229-8190 | ronaldo.reis at unimontes.br | 
> | chrysopa at gmail.com
> | ICQ#: 5692561 | LinuxUser#: 205366
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From kubovy at virginia.edu  Thu Feb 22 22:51:47 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 22 Feb 2007 16:51:47 -0500
Subject: [R] question about boxplot
In-Reply-To: <CE5FFC36D6AADB4C93A4D5D2125D8C6ACA7221@exp-clft3.cdc.gov>
References: <CE5FFC36D6AADB4C93A4D5D2125D8C6ACA7221@exp-clft3.cdc.gov>
Message-ID: <BE7034E7-1CF0-4E1E-88B8-81188B1895C1@virginia.edu>

On Feb 22, 2007, at 2:56 PM, Smith, Phil ((CDC/CCID/NCIRD)) wrote:

> boxplot( p.prop ~ R  + bins )
>
> This command makes nice boxplot for the factor "R" crossed with the  
> factor "bins".
>
> I am having alot of trouble getting control of the labels on the X  
> axis. I want to control it more by specifying what the labels are,  
> controling the 'size' of those labels (by using cex), and then  
> control the rotation of the character strings of those labels (by  
> using srt or crt).
>
> There is a "names" argument to boxplot, but I haven't had much luck  
> controlling what it prints, the size of the font, and the character  
> rotation.

Would you consider an easy way out---an alternative with reasonable  
defaults?

data(ToothGrowth)
bwplot(dose ~ len | supp, ToothGrowth)
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From ssj1364 at gmail.com  Thu Feb 22 22:57:45 2007
From: ssj1364 at gmail.com (sj)
Date: Thu, 22 Feb 2007 14:57:45 -0700
Subject: [R] is a time series regression model a causal forecasting model?
Message-ID: <1c6126db0702221357x734224cy7e13378de233e6e7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070222/7f0660ca/attachment.pl 

From rkoenker at uiuc.edu  Thu Feb 22 23:08:39 2007
From: rkoenker at uiuc.edu (roger koenker)
Date: Thu, 22 Feb 2007 16:08:39 -0600
Subject: [R] tournaments to dendrograms
Message-ID: <C74B189E-818F-48F3-8E2A-045469B738AA@uiuc.edu>

Does anyone have (good) experience converting tables of tournament
results into dendrogram-like graphics?  Tables, for example, like this:

read.table(url("http://www.econ.uiuc.edu/~roger/research/ncaa/NCAA.d"))

Any pointers appreciated.   RK

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


From roberto.perdisci at gmail.com  Thu Feb 22 23:11:29 2007
From: roberto.perdisci at gmail.com (Roberto Perdisci)
Date: Thu, 22 Feb 2007 17:11:29 -0500
Subject: [R] how to install a package in R on a linux machine?
In-Reply-To: <20070222104054.GA5393@guzu>
References: <54f7e7c30702220044s7eecde1bnff77d886fb2543eb@mail.gmail.com>
	<20070222104054.GA5393@guzu>
Message-ID: <cf94d0090702221411yfd9863bve57a37e8f28c2b72@mail.gmail.com>

Hi,
  try this:

$sudo R CMD INSTALL <downloaded.package.tar.gz>

If you don't use 'sudo' (or do not have privileges to do so), you need
to either become root (with su) or ask the administrator of the
machine you are using to install the package for you

regards,
Roberto


On 2/22/07, Gabor Csardi <csardi at rmki.kfki.hu> wrote:
> The easiest is perhaps to do
>
> install.packages("packagename")
>
> this downloads the package and installs it into the default R package
> library on your machine. If you want to install it to a different
> directory use the 'lib' argument of 'install.packages'.
>
> If you don't want to download the package again but want to use the
> downloaded one, use the following command:
>
> install.packages(repos=NULL, pkgs="the.file.you've.downloaded")
>
> You can also install R packages from the command line, like this:
>
> R CMD INSTALL -l <lib.directectory> <downloaded.package.file>
>
> Gabor
>
> On Thu, Feb 22, 2007 at 04:44:25PM +0800, gallon li wrote:
> > I downloaded the tar.gz file from r-project website (and saved it in a local
> > directory) and wish to use the package in R.
> >
> > But I am not sure how to use the install.packages command. I tried a few
> > times and still couldn't figure out the correct way to install this package.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dingjun_cn at yahoo.com  Thu Feb 22 22:57:42 2007
From: dingjun_cn at yahoo.com (Jun Ding)
Date: Thu, 22 Feb 2007 13:57:42 -0800 (PST)
Subject: [R] A question regarding "cutree"
Message-ID: <20070222215742.97300.qmail@web81009.mail.mud.yahoo.com>

Hi Everyone, 

I am doing hierarchical clustering analysis and have a
question regarding "cutree". 

I am doing things like this:

hc <- hclust(dist(X))
a <- cutree(hc, k=2)

Basically "a" is a vector containing the assignments
of 1 or 2 for each sample. May I know how "cutree"
decides to assign 1 and 2's to each sample (in other
words, how clusters 1 and 2 are decided)? I am having
the feeling that Sample 1 will be always assigned to
Cluster 1, but I am not sure about this. 

Thank you!

Best,
Jun


From A.Robinson at ms.unimelb.edu.au  Thu Feb 22 23:32:01 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 23 Feb 2007 09:32:01 +1100
Subject: [R] investigating interactions with mixed models
In-Reply-To: <Prayer.1.0.18.0702221232440.22007@hermes-1.csi.cam.ac.uk>
References: <Prayer.1.0.18.0702221232440.22007@hermes-1.csi.cam.ac.uk>
Message-ID: <20070222223201.GU24437@ms.unimelb.edu.au>

Hello Rachel,

I don't think that there is any infrastructure for these procedures on
lmer objects, yet.  If you are willing to use lme instead, then the
multcomp package seems to provide post-hoc tests.  It is worth noting
that there is some doubt as to the validity of the reference
distributions for tests of fixed effects in the presence of random
effects. 

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-are-p_002dvalues-not-displayed-when-using-lmer_0028_0029_003f

Cheers

Andrew

On Thu, Feb 22, 2007 at 12:32:44PM +0000, R. Baker wrote:
> I'm investigating a number of dependent variables using mixed models, e.g.
> 
> data.lmer45 = lmer(ampStopB ~ (type + stress + MorD)^3 + (1|speaker) + 
> (1|word), data=data)
> 
> The p-values for some of the 2-way and 3-way interactions are significant 
> at a 0.05 level and I have been trying to find out how to understand the 
> exact nature of the interactions. Does anyone know if it is possible to run 
> post-hoc tests on mixed model (lmer) objects? I have read about TukeyHSD 
> but it seems that this can only be run on anova (aov) objects.
> 
> Any suggestions would be gratefully appreciated!
> 
> Rachel Baker
> 
> -- 
> --------------------------------------------------------------------------
> PhD student                
> Dept of Linguistics        
> Sidgwick Avenue
> University of Cambridge              
> Cambridge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From A.Robinson at ms.unimelb.edu.au  Thu Feb 22 23:36:21 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 23 Feb 2007 09:36:21 +1100
Subject: [R] problem with weights on lmer function
In-Reply-To: <200702221822.04001.chrysopa@gmail.com>
References: <200702221822.04001.chrysopa@gmail.com>
Message-ID: <20070222223621.GV24437@ms.unimelb.edu.au>

Hi Ronaldo,

I suggest that you send us a small, well-documented, code example that
we can reproduce.  It certainly looks as though there is a problem,
but given this information it's hard to know what it is!

Cheers

Andrew

On Thu, Feb 22, 2007 at 06:22:03PM -0200, Ronaldo Reis Junior wrote:
> Hi,
> 
> I try to make a model using lmer, but the weigths is not accept.
> 
> m1<-lmer(ocup/total~tempo+(tempo|estacao),family=binomial,weights=total)
> 
> Erro em lmer(ocup/total ~ tempo + (tempo | estacao), family = binomial,  : 
> 	object `weights' of incorrect type
> 
> I dont understand why this error, with glm this work. the total object is a 
> vector.
> 
> Any idea?
> 
> Thanks
> Ronaldo
> -- 
> God is subtle, but he is not malicious.
> 		-- Albert Einstein
> --
> > Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
> | `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
> |   `- Fone: (38) 3229-8190 | ronaldo.reis at unimontes.br | chrysopa at gmail.com
> | ICQ#: 5692561 | LinuxUser#: 205366
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From gunter.berton at gene.com  Fri Feb 23 00:33:38 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 22 Feb 2007 15:33:38 -0800
Subject: [R] investigating interactions with mixed models
In-Reply-To: <20070222223201.GU24437@ms.unimelb.edu.au>
Message-ID: <00a801c756d9$e1176fd0$4d908980@gne.windows.gene.com>

?interaction.plot

Should help you. This works on the data, not the model. A 3-way interaction
just means that the 2-way interaction differs among the various levels of
the 3rd factor. Clever use of trellis plots (?xyplot -- especially
?panel.linejoin -- gives greater flexibility, but it requires that a steeper
learning curve be climbed).

In general, the presence of interactions is just another manifestation of
the response varying nonlinearly in the factors (**not** in the parameters,
of course -- it's a linear model after all). This is essentially always the
case, it's just a question of whether the signal/noise ratio (which depends
on sample size) is large enough to see it via P-values. So by all means look
at the plots and try to understand and interpret what's going on; but by no
means assume that p-values above and below a threshhold of .05 are a clear
guide to determining this. As usual, statistical significance and scientific
relevance are not equivalent, and the degree of overlap between the two is
often difficult to judge.

Cheers,
Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andrew Robinson
Sent: Thursday, February 22, 2007 2:32 PM
To: R. Baker
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] investigating interactions with mixed models

Hello Rachel,

I don't think that there is any infrastructure for these procedures on
lmer objects, yet.  If you are willing to use lme instead, then the
multcomp package seems to provide post-hoc tests.  It is worth noting
that there is some doubt as to the validity of the reference
distributions for tests of fixed effects in the presence of random
effects. 

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-are-p_002dvalues-not-displa
yed-when-using-lmer_0028_0029_003f

Cheers

Andrew

On Thu, Feb 22, 2007 at 12:32:44PM +0000, R. Baker wrote:
> I'm investigating a number of dependent variables using mixed models, e.g.
> 
> data.lmer45 = lmer(ampStopB ~ (type + stress + MorD)^3 + (1|speaker) + 
> (1|word), data=data)
> 
> The p-values for some of the 2-way and 3-way interactions are significant 
> at a 0.05 level and I have been trying to find out how to understand the 
> exact nature of the interactions. Does anyone know if it is possible to
run 
> post-hoc tests on mixed model (lmer) objects? I have read about TukeyHSD 
> but it seems that this can only be run on anova (aov) objects.
> 
> Any suggestions would be gratefully appreciated!
> 
> Rachel Baker
> 
> -- 
> --------------------------------------------------------------------------
> PhD student                
> Dept of Linguistics        
> Sidgwick Avenue
> University of Cambridge              
> Cambridge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From liuwensui at gmail.com  Fri Feb 23 01:05:05 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 22 Feb 2007 19:05:05 -0500
Subject: [R] Chi-Square test
In-Reply-To: <e3f2a5ab0702211404l2ddaa037ye11873e1143c641@mail.gmail.com>
References: <e3f2a5ab0702211404l2ddaa037ye11873e1143c641@mail.gmail.com>
Message-ID: <1115a2b00702221605g33f54483ve58d0733d2343c7e@mail.gmail.com>

?pchisq

On 2/21/07, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
> Hello all,
>
> I am doing a Likelihood Ratio (LR) test in my simulation and I have a vector
> LR values (each with 1 degree of freedom) at the end of my simulation.
>
> Can anybody tell me how I can write a 'R' code which gives me the p-value
> for each of those LR values.
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From adik at ilovebacon.org  Fri Feb 23 01:19:21 2007
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Thu, 22 Feb 2007 16:19:21 -0800 (PST)
Subject: [R] manova discriminant functions: Addendum
In-Reply-To: <Pine.LNX.4.64.0702051423270.11755@parser.ilovebacon.org>
References: <mailman.7.1170673203.28439.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.64.0702051102180.11755@parser.ilovebacon.org>
	<Pine.LNX.4.64.0702051423270.11755@parser.ilovebacon.org>
Message-ID: <Pine.LNX.4.64.0702221556490.2541@parser.ilovebacon.org>

Three weeks later, I have almost completely solved my problem (quoted below;
about within-subjects manova, and discriminant function analysis to
compliment a manova analysis). So for anyone who was secretly hoping someone
would respond to me:

* manova does not handle within-subjects variables in a manner which is very
   intuitive for those not in the field of statistics or mathematics.  The
   anova.mlm functions allow this sort of analysis, but for applied
   statisticians (read: social scientists), dealing with inner vs outer
   projections is a bit opaque.

* However, a bit of knowledge on what repeated measures MANOVA really does
   is very helpful. (No, really, it helps to know what you're trying to do!)
   The general idea is that you create contrast columns for the
   within-subjects factors, such that the contrast columns are orthogonal.
   Then, you submit these to a general MANOVA with between-subject predictors
   to test the within-between interactions.

* For within-subjects main effects, it's easier (for me) to just compute the
   hypothesis matrix myself. With orthogonal contrast factors, it's pretty
   simple: each row of the hypothesis matrix is the product of the column of
   means for the contrast columns times the mean for that row's contrast,
   times the number of observations. The error matrix can be taken directly
   from the fit for the interaction (see point 2); then, H %*% ginv(E) gives
   the effect matrix for the within subjects main effect, whose eigenvalues
   can be evaluated with the appropriate degrees of freedom. I wrote a
   function to do this, if anyone is interested.

* The current implementation of the Pillai() function uses Pillai's (1954)
   approximation of an F value for a given V, despite the much more robust
   and (seemingly) uncontroversial method of estimating F put forth by Muller
   in 1998. This is, of course, what SPSS and SAS do (unless you require SAS
   to do "exact" tests, which is still a misnomer), but it doesn't seem to me
   like R ought to do things poorly just to give the same results as SPSS.

* Estimating several within-subjects effects basically amounts to providing
   the relevant contrast matrices to the interaction analyses, and then
   testing subsets relative to the overall residual matrix and the right
   number of df.

* To get discriminant functions and variable loadings, use the lda()
   function. The fact that these are provided by manova() analogues in other
   software packages was just a red herring!

* However, this also does not work for discovering within-subjects
   discriminant functions. And that, I'm afraid, is where I'm now stuck...any
   suggestions on an lda() analogue for within-subject factors? An example:

data <- data.frame(treat=factor(rep(c("Control","Treatment"),3)),
                    time1=c(1,5,1,5,1,5),
                    time2=c(2,8,1,7,2,6),
                    time3=c(3,9,3,9,2,10))

data.poly <- data[,2:4] %*% contr.poly(3)

Now, discriminant functions on predicting the difference between Control and
Treatment, using the three timepoints is easy: Convert them to orthogonal
contrast variables, and then see how to discriminate treatment conditions
based on the linear and quadratic effects of time:

lda(data$treat ~ data.poly)

...but my question is how to do an lda predicting whether an observation
came from the time1, time2, or time3 column.

Thanks,
Adam

On Mon, 5 Feb 2007, Adam D. I. Kramer wrote:

> As an addendum to my earlier post, I am having another difficulty with
> getting manova() to behave as I would like: when I specify contrasts for my
> independent variable(s), I am unsure of how to test them. This is a
> contrived example of both of my questions:
>
> Assume three alertness measurements, "alert1" "alert2" "alert3", a
> within-subjects variable measuring their alertness at three timepoints,
> minutes after taking the drug (alert1), one hour (alert2), and two hours
> (alert3). The between-subjects variable is dosage, with dose==0 when
> subjects have had no drug, dose==1 when they have had a single dose, dose==2
> when they have had a double dose.
>
> My intuition says to do the following:
>
> alert <- cbind(alert1,alert2,alert3) %*% contr.poly(3)
> contrasts(dose) <- matrix(c(2,-1,-1,0,1,-1),3,2)
> m <- manova(alert ~ dose)
>
> ...what I want is two main effects (dose and alert) and one interaction
> (dose by alert), but also "main effect" and "interaction" for the two
> individual contrasts for dose. For the main effect for alert, and all of the
> dose*alert interactions, I need the discriminant function loadings of my two
> alertness contrasts in order to interpret the manner in which alertness is
> varying (e.g., is it varying in a linear or quadratic way).
>
> m2 <- manova (alert ~ dose)
> summary(m2)
> ...gives me a test for the dose * alertness interaction. Good! But I can't
> seem to find the contrasts I asked for for dose. In univariate ANOVA, I
> usually just call summary.lm() which gives me t-test coefficients for each
> level of the dose contrast...but calling summary.lm on a manova object
> returns t-tests on three unnamed coefficients, with 27 error degrees of
> freedom (when it should be 26, as I am intending to compute both dosage
> contrasts simultaneously). Also, I cannot tell whether it is the linear or
> quadratic contrast that is contributing to the differentiation of dosage
> levels--this is why I need the discriminant function loadings.
>
> m2 <- lm( apply(cbind(alert1,alert2,alert3),1,mean) ~ dose)
> summary(m2)
> ...gives me a test for the two contrasts, which can be pooled to get a main
> effect. Excellent!
>
> ...finally, for the main effect of alertness, I'm more or less at a loss.
> The question is whether the three alertness conditions differ from each
> other, or whether some linear combination of the linear and quadratic
> contrast columns is significantly different from zero...and then the
> relative weightings of the linear and quadratic contrasts.
>
> Any suggestions?
>
> Thanks,
> Adam
>
> On Mon, 5 Feb 2007, Adam D. I. Kramer wrote:
>
>> Hello,
>>
>> 	I've been playing with the manova() function to do some pretty
>> straightforward multivariate analyses, and I can't for the life of me 
>> figure
>> out how to get at the discriminant functions used. When predicting several
>> variables simultaneously, it's important to be able to gauge how much each
>> variable is contributing to the analysis...a simple p-value isn't really
>> enough. I find examination of the discriminant function loadings to be a
>> good indicator of this.
>> 
>> Thanks,
>> Adam Kramer
>> 
>


From jafarikia at gmail.com  Fri Feb 23 02:36:03 2007
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Thu, 22 Feb 2007 20:36:03 -0500
Subject: [R] Wrinting integers in a matrix faile
Message-ID: <e3f2a5ab0702221736q73b9aa17l7e65da8a5c2651db@mail.gmail.com>

Hello everyone,
I am using the following program to get the p-value of some numbers
(column 'LR' of the data.dat file). I want to write the 1st and 2nd
column of the output file (data.out) as an integer while the program
change them to double. Could anybody please tell me how I can write
the code which writes the values of the first two columns as integer?
Thanks


library ('MASS')
MP<-read.table(file='data.dat')
names(MP)<-c('B','R','S','L','LR','Q')
a<-as.matrix((1-pchisq(MP$LR, df=1)))
b<-cbind(MP$B,MP$R,a,MP$S,MP$L,MP$LR,MP$Q)
write.matrix(b,'data.out')


From kubovy at virginia.edu  Fri Feb 23 03:47:31 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 22 Feb 2007 21:47:31 -0500
Subject: [R] Wrinting integers in a matrix faile
In-Reply-To: <e3f2a5ab0702221736q73b9aa17l7e65da8a5c2651db@mail.gmail.com>
References: <e3f2a5ab0702221736q73b9aa17l7e65da8a5c2651db@mail.gmail.com>
Message-ID: <CDD63171-5709-4444-ACCB-036F1798188E@virginia.edu>

On Feb 22, 2007, at 8:36 PM, Mohsen Jafarikia wrote:

> I am using the following program to get the p-value of some numbers
> (column 'LR' of the data.dat file). I want to write the 1st and 2nd
> column of the output file (data.out) as an integer while the program
> change them to double. Could anybody please tell me how I can write
> the code which writes the values of the first two columns as integer?
>
> library ('MASS')
> MP<-read.table(file='data.dat')
> names(MP)<-c('B','R','S','L','LR','Q')
> a<-as.matrix((1-pchisq(MP$LR, df=1)))
> b<-cbind(MP$B,MP$R,a,MP$S,MP$L,MP$LR,MP$Q)
> write.matrix(b,'data.out')

?Round
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From apjaworski at mmm.com  Fri Feb 23 06:02:17 2007
From: apjaworski at mmm.com (apjaworski at mmm.com)
Date: Thu, 22 Feb 2007 23:02:17 -0600
Subject: [R] mixture of 2 normals - starting values
Message-ID: <OF89608350.3A0617D7-ON8625728B.001ADEC6-8625728B.001BAD0C@mmm.com>

Hi,

I have a problem of estimating a mixture of two normal distributions.  I
need to find the starting points automatically, since this is a part of a
larger piece of image processing code.

I found the mix2normal1 function in VGAM package that mentions a method of
finding starting values for mu1 and mu2 but refers the reader to a book by
Everitt and Hand.  Unfortunately, I do not have an easy access to this
book.  Could anybody point me to a description of the method that I could
use?

Any help will be appreciated.  Thanks in advance,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


From chenxh007 at gmail.com  Fri Feb 23 06:14:06 2007
From: chenxh007 at gmail.com (Xiaohui)
Date: Thu, 22 Feb 2007 21:14:06 -0800
Subject: [R] mixture of 2 normals - starting values
In-Reply-To: <OF89608350.3A0617D7-ON8625728B.001ADEC6-8625728B.001BAD0C@mmm.com>
References: <OF89608350.3A0617D7-ON8625728B.001ADEC6-8625728B.001BAD0C@mmm.com>
Message-ID: <45DE781E.1070907@gmail.com>

Hi,

Try MCLUST package. You can use the hierarchical clustering to find the 
starting values of your EM.

Xiaohui

apjaworski at mmm.com wrote:
> Hi,
>
> I have a problem of estimating a mixture of two normal distributions.  I
> need to find the starting points automatically, since this is a part of a
> larger piece of image processing code.
>
> I found the mix2normal1 function in VGAM package that mentions a method of
> finding starting values for mu1 and mu2 but refers the reader to a book by
> Everitt and Hand.  Unfortunately, I do not have an easy access to this
> book.  Could anybody point me to a description of the method that I could
> use?
>
> Any help will be appreciated.  Thanks in advance,
>
> Andy
>
> __________________________________
> Andy Jaworski
> 518-1-01
> Process Laboratory
> 3M Corporate Research Laboratory
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From Augusto.Sanabria at ga.gov.au  Fri Feb 23 07:26:00 2007
From: Augusto.Sanabria at ga.gov.au (Augusto.Sanabria at ga.gov.au)
Date: Fri, 23 Feb 2007 17:26:00 +1100
Subject: [R] Extracting a subset from a dataframe
Message-ID: <9B2962F493D17F4F81A9211B1F9C56FB9B1CB6@mail.agso.gov.au>

Good day everyone,

Can anyone suggest an effective method to solve
the following problem:

I have 2 dataframes D1 and D2 as follows:

D1:
        dates       ws   wc pwc
 2005-10-19:12:00  10.8  80  81
 2005-10-20:12:00  12.3   5  15
 2005-10-21:15:00  12.3   3  15
 2005-10-22:15:00  11.3  13  95
 2005-10-23:12:00  12.3  13   2
 2005-10-24:15:00  10.3   2  95
 2005-10-25:15:00  10.3   2   2

D2:
        dates       ws   wc  pwc
 2005-02-02:15:00  17.5   5  96
 2005-02-19:15:00  20.1  15  97
 2005-02-20:18:00  16.5  95  95
 2005-03-03:18:00  10.3  95  95
 2005-03-04:00:00  13.4  13  95
 2005-10-22:15:00  11.3  13  95
 2005-10-25:15:00  10.3   2   2

I want to create another dataframe made up
of the values of dataframe1 which are not common
with dataframe2, ie. newD = D1 - (D1 intersection D2)

that is,
newD: 
       dates        ws   wc  pwc
 2005-10-19:12:00  10.8  80  81
 2005-10-20:12:00  12.3   5  15
 2005-10-21:15:00  12.3   3  15
 2005-10-23:12:00  12.3  13   2
 2005-10-24:15:00  10.3   2  95
 
Thanks for any help you can provide,

Augusto

--------------------------------------------
Augusto Sanabria. MSc, PhD.
Mathematical Modeller
Risk Research Group
Geospatial & Earth Monitoring Division
Geoscience Australia (www.ga.gov.au)
Cnr. Jerrabomberra Av. & Hindmarsh Dr.
Symonston ACT 2601
Ph. (02) 6249-9155


From chenxh007 at gmail.com  Fri Feb 23 08:02:19 2007
From: chenxh007 at gmail.com (Xiaohui)
Date: Thu, 22 Feb 2007 23:02:19 -0800
Subject: [R] Extracting a subset from a dataframe
In-Reply-To: <9B2962F493D17F4F81A9211B1F9C56FB9B1CB6@mail.agso.gov.au>
References: <9B2962F493D17F4F81A9211B1F9C56FB9B1CB6@mail.agso.gov.au>
Message-ID: <45DE917B.9020907@gmail.com>

Try: D1[setdiff(D1$dates,D2$dates) , ]

Xiaohui

Augusto.Sanabria at ga.gov.au wrote:
> Good day everyone,
>
> Can anyone suggest an effective method to solve
> the following problem:
>
> I have 2 dataframes D1 and D2 as follows:
>
> D1:
>         dates       ws   wc pwc
>  2005-10-19:12:00  10.8  80  81
>  2005-10-20:12:00  12.3   5  15
>  2005-10-21:15:00  12.3   3  15
>  2005-10-22:15:00  11.3  13  95
>  2005-10-23:12:00  12.3  13   2
>  2005-10-24:15:00  10.3   2  95
>  2005-10-25:15:00  10.3   2   2
>
> D2:
>         dates       ws   wc  pwc
>  2005-02-02:15:00  17.5   5  96
>  2005-02-19:15:00  20.1  15  97
>  2005-02-20:18:00  16.5  95  95
>  2005-03-03:18:00  10.3  95  95
>  2005-03-04:00:00  13.4  13  95
>  2005-10-22:15:00  11.3  13  95
>  2005-10-25:15:00  10.3   2   2
>
> I want to create another dataframe made up
> of the values of dataframe1 which are not common
> with dataframe2, ie. newD = D1 - (D1 intersection D2)
>
> that is,
> newD: 
>        dates        ws   wc  pwc
>  2005-10-19:12:00  10.8  80  81
>  2005-10-20:12:00  12.3   5  15
>  2005-10-21:15:00  12.3   3  15
>  2005-10-23:12:00  12.3  13   2
>  2005-10-24:15:00  10.3   2  95
>  
> Thanks for any help you can provide,
>
> Augusto
>
> --------------------------------------------
> Augusto Sanabria. MSc, PhD.
> Mathematical Modeller
> Risk Research Group
> Geospatial & Earth Monitoring Division
> Geoscience Australia (www.ga.gov.au)
> Cnr. Jerrabomberra Av. & Hindmarsh Dr.
> Symonston ACT 2601
> Ph. (02) 6249-9155
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From FredeA.Togersen at agrsci.dk  Fri Feb 23 08:39:07 2007
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Fri, 23 Feb 2007 08:39:07 +0100
Subject: [R] Extracting a subset from a dataframe
References: <9B2962F493D17F4F81A9211B1F9C56FB9B1CB6@mail.agso.gov.au>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC04E87DEA@DJFPOST01.djf.agrsci.dk>

Augusto

cnd <- D1$dates %in% D2$dates
D1[!cnd,]

should do it.
 
Med venlig hilsen / Regards

Frede Aakmann T?gersen
Forsker / Scientist


 	
 	 AARHUS UNIVERSITET / UNIVERSITY OF AARHUS	
Det Jordbrugsvidenskabelige Fakultet / Faculty of Agricultural Sciences	
Forskningscenter Foulum / Research Centre Foulum	
Genetik og Bioteknologi / Dept. of Genetics and Biotechnology	
Blichers All? 20, P.O. BOX 50	
DK-8830 Tjele	
 	
Tel:	 +45 8999 1900	
Direct:	 +45 8999 1878	
Mobile:	 +45 	
E-mail:	 FredeA.Togersen at agrsci.dk <mailto:FredeA.Togersen at agrsci.dk> 	
Web:	 www.agrsci.dk <https://djfpost.agrsci.dk/exchweb/bin/redir.asp?URL=http://www.agrsci.dk/> 	
________________________________

Tilmeld dig DJF's nyhedsbrev / Subscribe Faculty of Agricultural Sciences Newsletter <https://djfpost.agrsci.dk/exchweb/bin/redir.asp?URL=http://www.agrsci.dk/user/register?lan=dan-DK> . 

Denne email kan indeholde fortrolig information. Enhver brug eller offentligg?relse af denne email uden skriftlig tilladelse fra DJF er ikke tilladt. Hvis De ikke er den tilt?nkte adressat, bedes De venligst straks underrette DJF samt slette emailen.

This email may contain information that is confidential. Any use or publication of this email without written permission from Faculty of Agricultural Sciences is not allowed. If you are not the intended recipient, please notify Faculty of Agricultural Sciences immediately and delete this email.

 

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p? vegne af Augusto.Sanabria at ga.gov.au
Sendt: fr 23-02-2007 07:26
Til: R-help at stat.math.ethz.ch
Emne: [R] Extracting a subset from a dataframe



Good day everyone,

Can anyone suggest an effective method to solve
the following problem:

I have 2 dataframes D1 and D2 as follows:

D1:
        dates       ws   wc pwc
 2005-10-19:12:00  10.8  80  81
 2005-10-20:12:00  12.3   5  15
 2005-10-21:15:00  12.3   3  15
 2005-10-22:15:00  11.3  13  95
 2005-10-23:12:00  12.3  13   2
 2005-10-24:15:00  10.3   2  95
 2005-10-25:15:00  10.3   2   2

D2:
        dates       ws   wc  pwc
 2005-02-02:15:00  17.5   5  96
 2005-02-19:15:00  20.1  15  97
 2005-02-20:18:00  16.5  95  95
 2005-03-03:18:00  10.3  95  95
 2005-03-04:00:00  13.4  13  95
 2005-10-22:15:00  11.3  13  95
 2005-10-25:15:00  10.3   2   2

I want to create another dataframe made up
of the values of dataframe1 which are not common
with dataframe2, ie. newD = D1 - (D1 intersection D2)

that is,
newD:
       dates        ws   wc  pwc
 2005-10-19:12:00  10.8  80  81
 2005-10-20:12:00  12.3   5  15
 2005-10-21:15:00  12.3   3  15
 2005-10-23:12:00  12.3  13   2
 2005-10-24:15:00  10.3   2  95

Thanks for any help you can provide,

Augusto

--------------------------------------------
Augusto Sanabria. MSc, PhD.
Mathematical Modeller
Risk Research Group
Geospatial & Earth Monitoring Division
Geoscience Australia (www.ga.gov.au)
Cnr. Jerrabomberra Av. & Hindmarsh Dr.
Symonston ACT 2601
Ph. (02) 6249-9155

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From feanor0 at hotmail.com  Fri Feb 23 09:11:19 2007
From: feanor0 at hotmail.com (Murali Menon)
Date: Fri, 23 Feb 2007 08:11:19 +0000
Subject: [R] array searches
In-Reply-To: <644e1f320702160721mfcd4795w886906cb0568d173@mail.gmail.com>
Message-ID: <BAY113-F334FC212910D5511832F30EE8E0@phx.gbl>

Hi,

This is truly amazing stuff. Inspired by Jim's and Olivier's suggestions, 
I'm trying to expand it to work with a m x n matrix, where the first column 
is dates and the next columns are all signals. I dare say a suitable 
application of 'apply' should work.

Thanks a ton.
Murali


>From: "jim holtman" <jholtman at gmail.com>
>To: "Murali Menon" <feanor0 at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] array searches
>Date: Fri, 16 Feb 2007 10:21:40 -0500
>
>try this:
>
>>x <- scan(textConnection("30/01/2007      0
>+ 31/01/2007      -1
>+ 01/02/2007      -1
>+ 02/02/2007      -1
>+ 03/02/2007      1
>+ 04/02/2007      1
>+ 05/02/2007      1
>+ 06/02/2007      1
>+ 07/02/2007      1
>+ 08/02/2007      1
>+ 09/02/2007      0
>+ 10/02/2007      0
>+ 11/02/2007      0
>+ 12/02/2007      1
>+ 13/02/2007      1
>+ 14/02/2007      1
>+ 15/02/2007      0
>+ 16/02/2007      0
>+ "), what=list(date="", value=0))
>Read 18 records
>>x$date <- as.Date(x$date, "%d/%m/%Y")
>># determine the breaks
>>x.breaks <- c(TRUE, diff(x$value) != 0)
>># determine the value at the break; assume that it is the minimum
>>x.bdate <- x$date[x.breaks]
>>data.frame(date=x.bdate[cumsum(x.breaks)], value=x$value)
>         date value
>1  2007-01-30     0
>2  2007-01-31    -1
>3  2007-01-31    -1
>4  2007-01-31    -1
>5  2007-02-03     1
>6  2007-02-03     1
>7  2007-02-03     1
>8  2007-02-03     1
>9  2007-02-03     1
>10 2007-02-03     1
>11 2007-02-09     0
>12 2007-02-09     0
>13 2007-02-09     0
>14 2007-02-12     1
>15 2007-02-12     1
>16 2007-02-12     1
>17 2007-02-15     0
>18 2007-02-15     0
>>
>>
>>
>
>
>
>On 2/16/07, Murali Menon <feanor0 at hotmail.com> wrote:
>>
>>Folks,
>>
>>I have a dataframe comprising a column of dates and a column of signals
>>(-1,
>>0, 1) that looks something like this:
>>
>>30/01/2007      0
>>31/01/2007      -1
>>01/02/2007      -1
>>02/02/2007      -1
>>03/02/2007      1
>>04/02/2007      1
>>05/02/2007      1
>>06/02/2007      1
>>07/02/2007      1
>>08/02/2007      1
>>09/02/2007      0
>>10/02/2007      0
>>11/02/2007      0
>>12/02/2007      1
>>13/02/2007      1
>>14/02/2007      1
>>15/02/2007      0
>>16/02/2007      0
>>
>>What I need to do is for each signal *in reverse chronological order* to
>>find the date that it first appeared. So, for the zero on 16/02/2007 and
>>15/02/2007, the 'inception' date would be 15/02/2007, because the day
>>before, the signal was 1. Likewise, the 'inception' date for the signal 1
>>on
>>08/02/2007 and the five days prior, would be 03/02/2007. I need to create
>>a
>>structure of inception dates that would finally look as follows:
>>
>>-1      31/01/2007
>>-1      31/01/2007
>>-1      31/01/2007
>>1       03/02/2007
>>1       03/02/2007
>>1       03/02/2007
>>1       03/02/2007
>>1       03/02/2007
>>1       03/02/2007
>>0       09/02/2007
>>0       09/02/2007
>>0       09/02/2007
>>1       12/02/2007
>>1       12/02/2007
>>1       12/02/2007
>>0       15/02/2007
>>0       15/02/2007
>>
>>Is there a clever way of doing this? My sadly C-oriented upbringing can
>>only
>>think in terms of for-loops.
>>
>>Thanks!
>>
>>Murali
>>
>>_________________________________________________________________
>>The average US Credit Score is 675. The cost to see yours: $0 by Experian.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>
>--
>Jim Holtman
>Cincinnati, OH
>+1 513 646 9390
>
>What is the problem you are trying to solve?

_________________________________________________________________

fast as 1 year


From ravis at ambaresearch.com  Fri Feb 23 09:36:20 2007
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Fri, 23 Feb 2007 14:06:20 +0530
Subject: [R] help with RMySQL
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E8C618@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/4a3a6643/attachment.pl 

From osklyar at ebi.ac.uk  Fri Feb 23 11:35:02 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Fri, 23 Feb 2007 10:35:02 +0000
Subject: [R] Wrinting integers in a matrix faile
In-Reply-To: <e3f2a5ab0702221736q73b9aa17l7e65da8a5c2651db@mail.gmail.com>
References: <e3f2a5ab0702221736q73b9aa17l7e65da8a5c2651db@mail.gmail.com>
Message-ID: <45DEC356.1040909@ebi.ac.uk>

write.matrix(as.integer(b),'data.out')

Mohsen Jafarikia wrote:
> Hello everyone,
> I am using the following program to get the p-value of some numbers
> (column 'LR' of the data.dat file). I want to write the 1st and 2nd
> column of the output file (data.out) as an integer while the program
> change them to double. Could anybody please tell me how I can write
> the code which writes the values of the first two columns as integer?
> Thanks
> 
> 
> library ('MASS')
> MP<-read.table(file='data.dat')
> names(MP)<-c('B','R','S','L','LR','Q')
> a<-as.matrix((1-pchisq(MP$LR, df=1)))
> b<-cbind(MP$B,MP$R,a,MP$S,MP$L,MP$LR,MP$Q)
> write.matrix(b,'data.out')
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


From Romain.Mayor at ville-ge.ch  Fri Feb 23 12:05:01 2007
From: Romain.Mayor at ville-ge.ch (Romain.Mayor at ville-ge.ch)
Date: Fri, 23 Feb 2007 12:05:01 +0100
Subject: [R] kernel regression
Message-ID: <OFC4568334.3881077F-ONC125728B.00380B25-C125728B.003CE226@ville-ge.ch>


Dear R community,

I have tried to use the kernel regression in the "NP" package. I work in a
population biology study and the regression function ("npr") dealing with
continuous and categorical variables seems to be appropriate for such study
with non parametric data.
I have 3 questions:
First: Is there a way to have for each predictor a value indicating the
explanation of each predictor in the model. (not only the R2 for the whole
model but for each predictor).
Second: What should be the correct tolerance value for the bandwidth. I use
0.01.
Third: What is the suitable bootstrap repetition number ("but.num") in the
npsigtest function? I use the default value ("499"), with 7 variables(5
continous,2 categorical) and 1600 data point, it takes 23 hours to compute
the test and normally for publication purpose 999 bootstraps is the minimal
number of repetitions. Is it possible to do the test on a set of suitable
observation?

Could somebody also give me some references of publications using such
analysis in ecological domain?

Than you for answers and best regards.

Romain Mayor.


From klaster at karlin.mff.cuni.cz  Fri Feb 23 12:41:53 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Fri, 23 Feb 2007 12:41:53 +0100
Subject: [R] optim(method="L-BFGS-B") abnormal termination
Message-ID: <45DED301.40303@karlin.mff.cuni.cz>

Hi,
my call of optim() with the L-BFGS-B method ended with the following 
error message: ERROR: ABNORMAL_TERMINATION_IN_LNSRCH

Further tracing shows:
Line search cannot locate an adequate point after 20 function and 
gradient evaluations
final  value 0.086627
stopped after 7 iterations

Could someone pls tell me whether it is possible to increase the limit 
of 20 evaluations? Is it even worth doing so?

My function(s) to be minimized are polynomial functions of tens of 
variables - let say 10 - 60 variables, all of them constrained to the 
(0,1) interval. Is it even possible and meaningfull to attempt such 
minimization? (Suppose I have good starting values.)

Thaks, Petr
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From tab321 at stat.psu.edu  Fri Feb 23 13:54:56 2007
From: tab321 at stat.psu.edu (Tatiana Benaglia)
Date: Fri, 23 Feb 2007 07:54:56 -0500
Subject: [R] mixture of 2 normals - starting values
In-Reply-To: <OF89608350.3A0617D7-ON8625728B.001ADEC6-8625728B.001BAD0C@mmm.com>
References: <OF89608350.3A0617D7-ON8625728B.001ADEC6-8625728B.001BAD0C@mmm.com>
Message-ID: <CA75D906-E438-4668-A1CA-BE28FE4354D0@stat.psu.edu>

Hi,
you can also look at the function normalmix.init in the mixtools  
package.

Tatiana

Tatiana Benaglia
Ph.D. Candidate
Department of Statistics
Penn State University


On Feb 23, 2007, at 12:02 AM, apjaworski at mmm.com wrote:

> Hi,
>
> I have a problem of estimating a mixture of two normal  
> distributions.  I
> need to find the starting points automatically, since this is a  
> part of a
> larger piece of image processing code.
>
> I found the mix2normal1 function in VGAM package that mentions a  
> method of
> finding starting values for mu1 and mu2 but refers the reader to a  
> book by
> Everitt and Hand.  Unfortunately, I do not have an easy access to this
> book.  Could anybody point me to a description of the method that I  
> could
> use?
>
> Any help will be appreciated.  Thanks in advance,
>
> Andy
>
> __________________________________
> Andy Jaworski
> 518-1-01
> Process Laboratory
> 3M Corporate Research Laboratory
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jhallman at frb.gov  Fri Feb 23 13:58:48 2007
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 23 Feb 2007 07:58:48 -0500
Subject: [R] how much performance penalty does this incur,
	scalar as a vector of one element?
References: <748347.87140.qm@web53702.mail.yahoo.com>
	<Pine.LNX.4.64.0702220821400.2959@nokomis.stat.uiowa.edu>
Message-ID: <xmrlkipjaif.fsf@mralx1.rsma.frb.gov>

The 64 bit version of VisualWorks Smalltalk has an immediate ShortDouble,
which sacrifices two bits of exponent for a tag.  It thus has the same
precision as an IEEE double, but one fourth as much range.  Overflows
automatically get promoted to ordinary Double's, which are pointers to objects
holding real IEEE doubles.

Luke Tierney <luke at stat.uiowa.edu> writes:
> Systems that
> support integer scalars often represent them as immediate values within
> pointers by sacrificing one or two bits of precision in the integers,
> but that doesn't work for double precision floats except possibly on
> 64-bit systems. 

-- 
Jeff


From brown_emu at yahoo.com  Fri Feb 23 13:59:29 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Fri, 23 Feb 2007 04:59:29 -0800 (PST)
Subject: [R] question about boxplot
In-Reply-To: <BE7034E7-1CF0-4E1E-88B8-81188B1895C1@virginia.edu>
Message-ID: <5407.19095.qm@web39712.mail.mud.yahoo.com>

You can also set axes=FALSE as in

boxplot( p.prop ~ R  + bins, axes=FALSE )

and then add your own labels with axis(). the argument 'at' for axis() in the
case of boxplots are usually integers going from 1 to # of groups, though I
don't know specifically for your crossed factors.

For rotated names, you probably want to set axis(#arguments#,labels=FALSE)
and then use text() with 'srt' (don't forget to use xpd=TRUE in text()).
par("cxy") may help with positioning text relative to axis.

all the best,

st


--- Michael Kubovy <kubovy at virginia.edu> wrote:

> On Feb 22, 2007, at 2:56 PM, Smith, Phil ((CDC/CCID/NCIRD)) wrote:
> 
> > boxplot( p.prop ~ R  + bins )
> >
> > This command makes nice boxplot for the factor "R" crossed with the  
> > factor "bins".
> >
> > I am having alot of trouble getting control of the labels on the X  
> > axis. I want to control it more by specifying what the labels are,  
> > controling the 'size' of those labels (by using cex), and then  
> > control the rotation of the character strings of those labels (by  
> > using srt or crt).
> >
> > There is a "names" argument to boxplot, but I haven't had much luck  
> > controlling what it prints, the size of the font, and the character  
> > rotation.
> 
> Would you consider an easy way out---an alternative with reasonable  
> defaults?
> 
> data(ToothGrowth)
> bwplot(dose ~ len | supp, ToothGrowth)
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Never miss an email again!


From preuth at slf.ch  Fri Feb 23 14:38:56 2007
From: preuth at slf.ch (Thomas Preuth)
Date: Fri, 23 Feb 2007 14:38:56 +0100
Subject: [R] TRUE/FALSE as numeric values
Message-ID: <45DEEE70.50307@slf.ch>

Hello,

I want to select in a column of a dataframe all numbers smaller than a 
value x
but when I type in test<-(RSF_EU$AREA<=x) I receiv as answer:
 > test
 [1]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE 
FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
[18]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  
TRUE  TRUE FALSE  TRUE  TRUE  TRUE
[35] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  
TRUE  TRUE FALSE FALSE  TRUE FALSE
[52]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE

How can i get the values smaller than x and not the TRUE/FALSE reply?

Thanks in advance,
Thomas


From maitra at iastate.edu  Fri Feb 23 14:45:33 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Fri, 23 Feb 2007 07:45:33 -0600
Subject: [R] TRUE/FALSE as numeric values
In-Reply-To: <45DEEE70.50307@slf.ch>
References: <45DEEE70.50307@slf.ch>
Message-ID: <20070223074533.7d6c13a1@triveni.stat.iastate.edu>

On Fri, 23 Feb 2007 14:38:56 +0100 Thomas Preuth <preuth at slf.ch> wrote:

> Hello,
> 
> I want to select in a column of a dataframe all numbers smaller than a 
> value x
> but when I type in test<-(RSF_EU$AREA<=x) I receiv as answer:
>  > test
>  [1]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE 
> FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
> [18]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  
> TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> [35] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  
> TRUE  TRUE FALSE FALSE  TRUE FALSE
> [52]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE
> 
> How can i get the values smaller than x and not the TRUE/FALSE reply?

if the dataframe is called RSF_EU, and you want the entire dataframe for those rows, then 

RSF_EU [ (RSF_EU$AREA <= x ), ]

if you want to get only that column vector and nothing else

RSF_EU$AREA [ ( RSF_EU$AREA <= x ) ]

Such concepts are very well-explained in "An Introduction to R" which you would benefit by reading at the earliest.

Ranjan


> Thanks in advance,
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Thierry.ONKELINX at inbo.be  Fri Feb 23 14:46:52 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 23 Feb 2007 14:46:52 +0100
Subject: [R] TRUE/FALSE as numeric values
In-Reply-To: <45DEEE70.50307@slf.ch>
Message-ID: <2E9C414912813E4EB981326983E0A104029C399C@inboexch.inbo.be>

RSF_EU$AREA[RSF_EU$AREA<=x]

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch [mailto:r-help-
> bounces op stat.math.ethz.ch] Namens Thomas Preuth
> Verzonden: vrijdag 23 februari 2007 14:39
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] TRUE/FALSE as numeric values
> 
> Hello,
> 
> I want to select in a column of a dataframe all numbers smaller than a
> value x
> but when I type in test<-(RSF_EU$AREA<=x) I receiv as answer:
>  > test
>  [1]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE
> FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
> [18]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
> TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> [35] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE
> TRUE  TRUE FALSE FALSE  TRUE FALSE
> [52]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE
> 
> How can i get the values smaller than x and not the TRUE/FALSE reply?
> 
> Thanks in advance,
> Thomas
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wwwhsd at gmail.com  Fri Feb 23 14:49:01 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 23 Feb 2007 11:49:01 -0200
Subject: [R] TRUE/FALSE as numeric values
In-Reply-To: <45DEEE70.50307@slf.ch>
References: <45DEEE70.50307@slf.ch>
Message-ID: <da79af330702230549l69038c47mae91672aa30e87e0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/6aaebfc9/attachment.pl 

From ccleland at optonline.net  Fri Feb 23 14:50:44 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 23 Feb 2007 08:50:44 -0500
Subject: [R] TRUE/FALSE as numeric values
In-Reply-To: <45DEEE70.50307@slf.ch>
References: <45DEEE70.50307@slf.ch>
Message-ID: <45DEF134.9060600@optonline.net>

Thomas Preuth wrote:
> Hello,
> 
> I want to select in a column of a dataframe all numbers smaller than a 
> value x
> but when I type in test<-(RSF_EU$AREA<=x) I receiv as answer:
>  > test
>  [1]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE 
> FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
> [18]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  
> TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> [35] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  
> TRUE  TRUE FALSE FALSE  TRUE FALSE
> [52]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE
> 
> How can i get the values smaller than x and not the TRUE/FALSE reply?

  This is covered in

http://cran.r-project.org/doc/manuals/R-intro.html

(Section 2.7; Index vectors; selecting and modifying subsets of a data set)

RSF_EU$AREA[RSF_EU$AREA <= x]

or to retain other columns in the data frame

subset(RSF_EU, AREA <= x)

> Thanks in advance,
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From gavin.simpson at ucl.ac.uk  Fri Feb 23 14:55:24 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 23 Feb 2007 13:55:24 +0000
Subject: [R] TRUE/FALSE as numeric values
In-Reply-To: <45DEEE70.50307@slf.ch>
References: <45DEEE70.50307@slf.ch>
Message-ID: <1172238924.14430.19.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-02-23 at 14:38 +0100, Thomas Preuth wrote:
> Hello,
> 
> I want to select in a column of a dataframe all numbers smaller than a 
> value x
> but when I type in test<-(RSF_EU$AREA<=x) I receiv as answer:
>  > test
>  [1]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE 
> FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
> [18]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  
> TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> [35] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  
> TRUE  TRUE FALSE FALSE  TRUE FALSE
> [52]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE
> 
> How can i get the values smaller than x and not the TRUE/FALSE reply?
> 
> Thanks in advance,
> Thomas

You need to subset your object based on the results you achieved above.
What you did was only half the job. See this example, with a number of
ways to get what you want:

## some dummy data to work with
dat <- 10 * runif(100)
dat <- data.frame(AREA = dat, FOO = dat + rnorm(100))

## select values of AREA less than mean AREA
mn <- mean(dat$AREA)
want1 <- with(dat, AREA[AREA <= mn])
## or
want2 <- dat$AREA[dat$AREA <= mn]
## or
want3 <- subset(dat$AREA, dat$AREA <= mn)
## or
want4 <- subset(dat, AREA <= mn)$AREA
## check they all do same thing
all.equal(want1, want2, want3, want4) ## TRUE

want2 is closest to how you tried to do it:

dat$AREA[dat$AREA <= mn]
         ^^^^^^^^^^^^^^
Notice that you only did the inner bit marked, which as you found
returns TRUE/FALSE depending on whether that element of AREA met the
criterion of being less than or equal to your x. This information is
used to select elements from AREA using the subsetting functions for
objects.

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From pzs6 at cdc.gov  Fri Feb 23 15:36:42 2007
From: pzs6 at cdc.gov (Smith, Phil (CDC/CCID/NCIRD))
Date: Fri, 23 Feb 2007 09:36:42 -0500
Subject: [R] controling axes on boxplot() & thanks to ST, JvdH, & MK
References: <CE5FFC36D6AADB4C93A4D5D2125D8C6ACA7221@exp-clft3.cdc.gov>
Message-ID: <CE5FFC36D6AADB4C93A4D5D2125D8C6ACA7236@exp-clft3.cdc.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/e859c468/attachment.pl 

From Rainer.Krug at uct.ac.za  Fri Feb 23 15:34:38 2007
From: Rainer.Krug at uct.ac.za (Rainer M. Krug)
Date: Fri, 23 Feb 2007 16:34:38 +0200
Subject: [R] Question concerning survival analysis
Message-ID: <45DEFB7E.2090104@uct.ac.za>

Hi

I have a data set consisting of pictures on which individual plants were
identified and categorized into "alive", "dead" and "unknown" if it was
not clear. The time of these pictures ranges from 1937 to today and the
intervals range between 13 and 1 year.

I am trying to get an understanding of the mortality rate which I can
then use in an individual based model.

Therefore I thought of using survival analysis. I have ordered my data
in the required format, and classified the events as "interval censored"
(event=3) for the cases where the plant died and "right censored"
(event=0) but I have still a problem

I managed to obtain a Surv() object by calling

> surv <- with(survival, Surv(time, time2, event, type="interval"))

where survival is my dataset

> surv
  [1] [13, 16] 68+      16+      68+      68+      68+      [26, 34] 68+

  [9] 68+      16+      [54, 58] [64, 67] 68+      [34, 54] [34, 54]
[58, 63]
[17] [48, 58] [48, 63] [34, 54]

And it looks correct (I set the year 1937 to 0)

But where to from here? I understand that most of the analysis can not
be done with interval censored data? As far as I understand it, I have
to use interval censored as the observation intervals are not equal and
in addition quite large.

The data is from one site, no interference.

As I said, I would like to have an estimate of the hazard function (I
guess) to get information about the mortality rate of the individuals.

Any help welcome,

Rainer




-- 
NEW EMAIL ADDRESS AND ADDRESS:

Rainer.Krug at uct.ac.za

RKrug at sun.ac.za WILL BE DISCONTINUED END OF MARCH

Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Leslie Hill Institute for Plant Conservation
University of Cape Town
Rondebosch 7701
South Africa

Fax:		+27 - (0)86 516 2782
Fax:		+27 - (0)21 650 2440 (w)
Cell:		+27 - (0)83 9479 042

Skype:		RMkrug

email:	Rainer.Krug at uct.ac.za
       	Rainer at krugs.de


From yunzhang at Princeton.EDU  Fri Feb 23 15:43:18 2007
From: yunzhang at Princeton.EDU (Yun Zhang)
Date: Fri, 23 Feb 2007 09:43:18 -0500
Subject: [R] How to plot two graphs on one single plot?
Message-ID: <45DEFD86.5070708@princeton.edu>

Hi,

I am trying to plot two distribution graph on one plot. But I dont know 
how. I set them to the same x, y limit, even same x, y labels.

Code:
 x1=rnorm(25, mean=0, sd=1)
 y1=dnorm(x1, mean=0, sd=1)

 x2=rnorm(25, mean=0, sd=1)
 y2=dnorm(x2, mean=0, sd=1)
 plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2), xlab='x', 
ylab='y')
 plot(x2, y2, type='p', col="red", xlab='x', ylab='y')

They just dont show up in one plot.

Any hint will be very helpful.

Thanks,
Yun


From Rainer.Krug at uct.ac.za  Fri Feb 23 15:48:08 2007
From: Rainer.Krug at uct.ac.za (Rainer M. Krug)
Date: Fri, 23 Feb 2007 16:48:08 +0200
Subject: [R] R Mailing list in Cape Town - South Africa
Message-ID: <45DEFEA8.80309@uct.ac.za>

Hi

I hope that I don't break any rules -

I just want to announce that we started a mailing list for R in Cape 
Town. The address is:

https://cbio.uct.ac.za/mailman/listinfo/sabior

The basic idea of the mailing list is to bring R users in the region 
around Cape Town and in South Africa closer together and probably 
arrange informal gettogethers.

We also try to provide a forum to discuss issues concerning R in Biology 
and distribute announcements of R courses and similar events in the Cape 
Town area.

We definitely do not want to establish an alternative forum to the 
official R lists and do not want to draw subscribers away from them - it 
is a huge advantage that one large list provides the main forum to get help.

I hope that the new mailing list will be a useful medium to spread the 
word and information about R in South Africa,

Rainer

-- 
Rainer.Krug at uct.ac.za
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Leslie Hill Institute for Plant Conservation
University of Cape Town
Rondebosch 7701
South Africa


From snunes at gmail.com  Fri Feb 23 15:53:51 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Fri, 23 Feb 2007 14:53:51 +0000
Subject: [R] Google Custom Search Engine for R
Message-ID: <4c817d530702230653u330de9a2r144b7a4a667598d0@mail.gmail.com>

Hi,

Since "R" is a (very) generic name, I've been having some trouble
searching the web for this topic. Due to this, I've just created a
Google Custom Search Engine that includes several of the most relevant
sites that have information on R.

See it in action at:

http://google.com/coop/cse?cx=018133866098353049407%3Aozv9awtetwy

This is really a preliminary test. Feel free to add yourself to the
project and contribute with suggestions.

S?rgio Nunes


From wwwhsd at gmail.com  Fri Feb 23 16:23:57 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 23 Feb 2007 13:23:57 -0200
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <45DEFD86.5070708@princeton.edu>
References: <45DEFD86.5070708@princeton.edu>
Message-ID: <da79af330702230723u23b78dd4uacc03bb0beed0232@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/a3180ce9/attachment.pl 

From wwwhsd at gmail.com  Fri Feb 23 16:23:57 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 23 Feb 2007 13:23:57 -0200
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <45DEFD86.5070708@princeton.edu>
References: <45DEFD86.5070708@princeton.edu>
Message-ID: <da79af330702230723u23b78dd4uacc03bb0beed0232@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/a3180ce9/attachment-0001.pl 

From wwwhsd at gmail.com  Fri Feb 23 16:23:57 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 23 Feb 2007 13:23:57 -0200
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <45DEFD86.5070708@princeton.edu>
References: <45DEFD86.5070708@princeton.edu>
Message-ID: <da79af330702230723u23b78dd4uacc03bb0beed0232@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/a3180ce9/attachment-0002.pl 

From luke at stat.uiowa.edu  Fri Feb 23 16:26:05 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 23 Feb 2007 09:26:05 -0600 (CST)
Subject: [R] how much performance penalty does this incur,
 scalar as a vector of one element?
In-Reply-To: <xmrlkipjaif.fsf@mralx1.rsma.frb.gov>
References: <748347.87140.qm@web53702.mail.yahoo.com>
	<Pine.LNX.4.64.0702220821400.2959@nokomis.stat.uiowa.edu>
	<xmrlkipjaif.fsf@mralx1.rsma.frb.gov>
Message-ID: <Pine.LNX.4.64.0702230925360.28762@nokomis.stat.uiowa.edu>

Thanks -- that's good to know.

Best,

luke

On Fri, 23 Feb 2007, Jeffrey J. Hallman wrote:

> The 64 bit version of VisualWorks Smalltalk has an immediate ShortDouble,
> which sacrifices two bits of exponent for a tag.  It thus has the same
> precision as an IEEE double, but one fourth as much range.  Overflows
> automatically get promoted to ordinary Double's, which are pointers to objects
> holding real IEEE doubles.
>
> Luke Tierney <luke at stat.uiowa.edu> writes:
>> Systems that
>> support integer scalars often represent them as immediate values within
>> pointers by sacrificing one or two bits of precision in the integers,
>> but that doesn't work for double precision floats except possibly on
>> 64-bit systems.
>
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From yunzhang at Princeton.EDU  Fri Feb 23 16:34:41 2007
From: yunzhang at Princeton.EDU (Yun Zhang)
Date: Fri, 23 Feb 2007 10:34:41 -0500
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <da79af330702230723u23b78dd4uacc03bb0beed0232@mail.gmail.com>
References: <45DEFD86.5070708@princeton.edu>
	<da79af330702230723u23b78dd4uacc03bb0beed0232@mail.gmail.com>
Message-ID: <45DF0991.70607@princeton.edu>

Thanks. Now R plots two graphs on one plot.
Yet they are still on two graphs, vertically parallelized with each other.

But what I want to do is actually plotting two distribution on one 
single graph, using the same x and y axis. Like:
|
|
|               (dist2)
|   (dist 1)
|
--------------------------->

Is it possible to do that?

Thanks,
Yun

Henrique Dallazuanna wrote:
> par(mfrow=c(2,1))
> #your plot
> #after plot
> par(mfrow=c(1,1))
>
> On 23/02/07, *Yun Zhang* <yunzhang at princeton.edu 
> <mailto:yunzhang at princeton.edu>> wrote:
>
>     Hi,
>
>     I am trying to plot two distribution graph on one plot. But I dont
>     know
>     how. I set them to the same x, y limit, even same x, y labels.
>
>     Code:
>     x1=rnorm(25, mean=0, sd=1)
>     y1=dnorm(x1, mean=0, sd=1)
>
>     x2=rnorm(25, mean=0, sd=1)
>     y2=dnorm(x2, mean=0, sd=1)
>     plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2),
>     xlab='x',
>     ylab='y')
>     plot(x2, y2, type='p', col="red", xlab='x', ylab='y')
>
>     They just dont show up in one plot.
>
>     Any hint will be very helpful.
>
>     Thanks,
>     Yun
>
>     ______________________________________________
>     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
>     list
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> -- 
> Henrique Dallazuanna
> Curitiba-Paran?
> Brasil


From mckellercran at gmail.com  Fri Feb 23 16:42:05 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Fri, 23 Feb 2007 10:42:05 -0500
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <45DEFD86.5070708@princeton.edu>
References: <45DEFD86.5070708@princeton.edu>
Message-ID: <3f547caa0702230742i586acbedh3049c54e4c1cc696@mail.gmail.com>

Hi Yun,

If you're asking how to place new graphic material on the same plot
(e.g., several lines/points/etc in a single x-y region), this is
covered in the Intro to R manual. E.g., you can do:

plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2),
xlab='x', ylab='y')
points(x2, y2, col="red")

see ?lines ?points ?text ?abline Also, see the "new" option in par

Best,

Matt

On 2/23/07, Yun Zhang <yunzhang at princeton.edu> wrote:
> Hi,
>
> I am trying to plot two distribution graph on one plot. But I dont know
> how. I set them to the same x, y limit, even same x, y labels.
>
> Code:
>  x1=rnorm(25, mean=0, sd=1)
>  y1=dnorm(x1, mean=0, sd=1)
>
>  x2=rnorm(25, mean=0, sd=1)
>  y2=dnorm(x2, mean=0, sd=1)
>  plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2), xlab='x',
> ylab='y')
>  plot(x2, y2, type='p', col="red", xlab='x', ylab='y')
>
> They just dont show up in one plot.
>
> Any hint will be very helpful.
>
> Thanks,
> Yun
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics


From wwwhsd at gmail.com  Fri Feb 23 16:45:12 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 23 Feb 2007 13:45:12 -0200
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <45DF0991.70607@princeton.edu>
References: <45DEFD86.5070708@princeton.edu>
	<da79af330702230723u23b78dd4uacc03bb0beed0232@mail.gmail.com>
	<45DF0991.70607@princeton.edu>
Message-ID: <da79af330702230745h1feb1beev49ca28c9d0210da@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/870bb037/attachment.pl 

From mckellercran at gmail.com  Fri Feb 23 16:51:06 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Fri, 23 Feb 2007 10:51:06 -0500
Subject: [R] Google Custom Search Engine for R
In-Reply-To: <4c817d530702230653u330de9a2r144b7a4a667598d0@mail.gmail.com>
References: <4c817d530702230653u330de9a2r144b7a4a667598d0@mail.gmail.com>
Message-ID: <3f547caa0702230751k4a1da915ob3a62ef1db5db920@mail.gmail.com>

Hi Sergio,

There was a discussion on this board recently about the difficulty of
searching for "R" related material on the web. I think the custom
google search engine is a good idea. It would be helpful if we could
have access to the full list of websites it is indexing so that we
could make suggestions about other sites that are missing. As it is,
it only tells us that there are 35 websites, and shows us the first
several.

Also, you might check out Sasha Goodman's Rseek: http://www.rseek.org/

Have you tried to compare the success of yours with Rseek?

All the Best,

Matt

On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> Hi,
>
> Since "R" is a (very) generic name, I've been having some trouble
> searching the web for this topic. Due to this, I've just created a
> Google Custom Search Engine that includes several of the most relevant
> sites that have information on R.
>
> See it in action at:
>
> http://google.com/coop/cse?cx=018133866098353049407%3Aozv9awtetwy
>
> This is really a preliminary test. Feel free to add yourself to the
> project and contribute with suggestions.
>
> S?rgio Nunes
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics


From a.park at uwinnipeg.ca  Fri Feb 23 16:51:06 2007
From: a.park at uwinnipeg.ca (Andrew Park)
Date: Fri, 23 Feb 2007 09:51:06 -0600
Subject: [R] Repeated measures in Classification and Regresssion Trees
Message-ID: <s5deb916.044@ds1.uwinnipeg.ca>

Dear R members,

I have been trying to find out whether one can use multivariate
regression trees (for example mvpart) to analyze repeated measures data.
 As a non-parametric technique, CART is insensitive to most of the
assumptions of parametric regression, but repeated measures data raises
the issue of the independence of several data points measured on the
same subject, or from the same plot over time.

Any perspectives will be welcome,



Andy Park (Assistant Professor)

Centre for Forest Interdisciplinary Research (CFIR),
Department of Biology,
University of Winnipeg,
515 Portage Avenue,
Winnipeg, Manitoba, R3B 2E9,
Canada

Phone: (204) 786-9407


From yunzhang at Princeton.EDU  Fri Feb 23 16:54:07 2007
From: yunzhang at Princeton.EDU (Yun Zhang)
Date: Fri, 23 Feb 2007 10:54:07 -0500
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <3f547caa0702230742i586acbedh3049c54e4c1cc696@mail.gmail.com>
References: <45DEFD86.5070708@princeton.edu>
	<3f547caa0702230742i586acbedh3049c54e4c1cc696@mail.gmail.com>
Message-ID: <45DF0E1F.8050204@princeton.edu>

Thanks, it works. Thank you very much.

Yun

Matthew Keller wrote:
> Hi Yun,
>
> If you're asking how to place new graphic material on the same plot
> (e.g., several lines/points/etc in a single x-y region), this is
> covered in the Intro to R manual. E.g., you can do:
>
> plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2),
> xlab='x', ylab='y')
> points(x2, y2, col="red")
>
> see ?lines ?points ?text ?abline Also, see the "new" option in par
>
> Best,
>
> Matt
>
> On 2/23/07, Yun Zhang <yunzhang at princeton.edu> wrote:
>> Hi,
>>
>> I am trying to plot two distribution graph on one plot. But I dont know
>> how. I set them to the same x, y limit, even same x, y labels.
>>
>> Code:
>>  x1=rnorm(25, mean=0, sd=1)
>>  y1=dnorm(x1, mean=0, sd=1)
>>
>>  x2=rnorm(25, mean=0, sd=1)
>>  y2=dnorm(x2, mean=0, sd=1)
>>  plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2), xlab='x',
>> ylab='y')
>>  plot(x2, y2, type='p', col="red", xlab='x', ylab='y')
>>
>> They just dont show up in one plot.
>>
>> Any hint will be very helpful.
>>
>> Thanks,
>> Yun
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


From clint at ecy.wa.gov  Fri Feb 23 16:55:12 2007
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 23 Feb 2007 07:55:12 -0800 (PST)
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <45DF0991.70607@princeton.edu>
References: <45DEFD86.5070708@princeton.edu>
	<da79af330702230723u23b78dd4uacc03bb0beed0232@mail.gmail.com>
	<45DF0991.70607@princeton.edu>
Message-ID: <Pine.LNX.4.62.0702230754450.6451@aeolus.ecy.wa.gov>

?par

try par(new=TRUE) between plots

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Dispersion Modeler		INTERNET:	clint at math.utah.edu
Air Quality Program		VOICE:		(360) 407-6815
Department of Ecology		FAX:		(360) 407-7534

	USPS:  		PO Box 47600, Olympia, WA 98504-7600
	Parcels:	300 Desmond Drive, Lacey, WA 98503-1274

On Fri, 23 Feb 2007, Yun Zhang wrote:

> Thanks. Now R plots two graphs on one plot.
> Yet they are still on two graphs, vertically parallelized with each other.
>
> But what I want to do is actually plotting two distribution on one
> single graph, using the same x and y axis. Like:
> |
> |
> |               (dist2)
> |   (dist 1)
> |
> --------------------------->
>
> Is it possible to do that?
>
> Thanks,
> Yun
>
> Henrique Dallazuanna wrote:
> > par(mfrow=c(2,1))
> > #your plot
> > #after plot
> > par(mfrow=c(1,1))
> >
> > On 23/02/07, *Yun Zhang* <yunzhang at princeton.edu
> > <mailto:yunzhang at princeton.edu>> wrote:
> >
> >     Hi,
> >
> >     I am trying to plot two distribution graph on one plot. But I dont
> >     know
> >     how. I set them to the same x, y limit, even same x, y labels.
> >
> >     Code:
> >     x1=rnorm(25, mean=0, sd=1)
> >     y1=dnorm(x1, mean=0, sd=1)
> >
> >     x2=rnorm(25, mean=0, sd=1)
> >     y2=dnorm(x2, mean=0, sd=1)
> >     plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2),
> >     xlab='x',
> >     ylab='y')
> >     plot(x2, y2, type='p', col="red", xlab='x', ylab='y')
> >
> >     They just dont show up in one plot.
> >
> >     Any hint will be very helpful.
> >
> >     Thanks,
> >     Yun
> >
> >     ______________________________________________
> >     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
> >     list
> >     https://stat.ethz.ch/mailman/listinfo/r-help
> >     <https://stat.ethz.ch/mailman/listinfo/r-help>
> >     PLEASE do read the posting guide
> >     http://www.R-project.org/posting-guide.html
> >     and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> > --
> > Henrique Dallazuanna
> > Curitiba-Paran?
> > Brasil
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

From ssj1364 at gmail.com  Fri Feb 23 16:56:13 2007
From: ssj1364 at gmail.com (sj)
Date: Fri, 23 Feb 2007 08:56:13 -0700
Subject: [R] Neural Net forecasting
Message-ID: <1c6126db0702230756k3a115fc2kcebd59d72a235b5a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/eafba16e/attachment.pl 

From snunes at gmail.com  Fri Feb 23 16:58:03 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Fri, 23 Feb 2007 15:58:03 +0000
Subject: [R] Google Custom Search Engine for R
In-Reply-To: <3f547caa0702230751k4a1da915ob3a62ef1db5db920@mail.gmail.com>
References: <4c817d530702230653u330de9a2r144b7a4a667598d0@mail.gmail.com>
	<3f547caa0702230751k4a1da915ob3a62ef1db5db920@mail.gmail.com>
Message-ID: <4c817d530702230758qbb7b1cfi7ad253f408dc0779@mail.gmail.com>

Hi,

I've just created this search engine for testing today.
I think that if you volunteer as a contributor (see link on the left
side) you can then see and add to the list of sites.

I've also noticed that I can add sites by importing a XML file.
Is there any list of sites that could be easily converted to XML?

S?rgio Nunes

On 2/23/07, Matthew Keller <mckellercran at gmail.com> wrote:
> Hi Sergio,
>
> There was a discussion on this board recently about the difficulty of
> searching for "R" related material on the web. I think the custom
> google search engine is a good idea. It would be helpful if we could
> have access to the full list of websites it is indexing so that we
> could make suggestions about other sites that are missing. As it is,
> it only tells us that there are 35 websites, and shows us the first
> several.
>
> Also, you might check out Sasha Goodman's Rseek: http://www.rseek.org/
>
> Have you tried to compare the success of yours with Rseek?
>
> All the Best,
>
> Matt
>
> On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> > Hi,
> >
> > Since "R" is a (very) generic name, I've been having some trouble
> > searching the web for this topic. Due to this, I've just created a
> > Google Custom Search Engine that includes several of the most relevant
> > sites that have information on R.
> >
> > See it in action at:
> >
> > http://google.com/coop/cse?cx=018133866098353049407%3Aozv9awtetwy
> >
> > This is really a preliminary test. Feel free to add yourself to the
> > project and contribute with suggestions.
> >
> > S?rgio Nunes
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Matthew C Keller
> Postdoctoral Fellow
> Virginia Institute for Psychiatric and Behavioral Genetics
>


From snunes at gmail.com  Fri Feb 23 17:01:34 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Fri, 23 Feb 2007 16:01:34 +0000
Subject: [R] Google Custom Search Engine for R
In-Reply-To: <4c817d530702230758qbb7b1cfi7ad253f408dc0779@mail.gmail.com>
References: <4c817d530702230653u330de9a2r144b7a4a667598d0@mail.gmail.com>
	<3f547caa0702230751k4a1da915ob3a62ef1db5db920@mail.gmail.com>
	<4c817d530702230758qbb7b1cfi7ad253f408dc0779@mail.gmail.com>
Message-ID: <4c817d530702230801o6b723d4dt724af622f7265b61@mail.gmail.com>

I just seen the link you suggested - RSeek - and it uses exactly the
same service from Google. The only difference is that it is not open
to new contributions. Maybe Sasha could open it to volunteers.

Also it seems to be much more complete than my initial attempt (as expected :).
I think that RSeek "deserves" a link in R's homepage.

S?rgio Nunes

On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> Hi,
>
> I've just created this search engine for testing today.
> I think that if you volunteer as a contributor (see link on the left
> side) you can then see and add to the list of sites.
>
> I've also noticed that I can add sites by importing a XML file.
> Is there any list of sites that could be easily converted to XML?
>
> S?rgio Nunes
>
> On 2/23/07, Matthew Keller <mckellercran at gmail.com> wrote:
> > Hi Sergio,
> >
> > There was a discussion on this board recently about the difficulty of
> > searching for "R" related material on the web. I think the custom
> > google search engine is a good idea. It would be helpful if we could
> > have access to the full list of websites it is indexing so that we
> > could make suggestions about other sites that are missing. As it is,
> > it only tells us that there are 35 websites, and shows us the first
> > several.
> >
> > Also, you might check out Sasha Goodman's Rseek: http://www.rseek.org/
> >
> > Have you tried to compare the success of yours with Rseek?
> >
> > All the Best,
> >
> > Matt
> >
> > On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> > > Hi,
> > >
> > > Since "R" is a (very) generic name, I've been having some trouble
> > > searching the web for this topic. Due to this, I've just created a
> > > Google Custom Search Engine that includes several of the most relevant
> > > sites that have information on R.
> > >
> > > See it in action at:
> > >
> > > http://google.com/coop/cse?cx=018133866098353049407%3Aozv9awtetwy
> > >
> > > This is really a preliminary test. Feel free to add yourself to the
> > > project and contribute with suggestions.
> > >
> > > S?rgio Nunes
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Matthew C Keller
> > Postdoctoral Fellow
> > Virginia Institute for Psychiatric and Behavioral Genetics
> >
>


From michael.watson at bbsrc.ac.uk  Fri Feb 23 17:27:58 2007
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 23 Feb 2007 16:27:58 -0000
Subject: [R] Google Custom Search Engine for R
References: <4c817d530702230653u330de9a2r144b7a4a667598d0@mail.gmail.com>
	<3f547caa0702230751k4a1da915ob3a62ef1db5db920@mail.gmail.com>
Message-ID: <8975119BCD0AC5419D61A9CF1A923E9502068113@iahce2ksrv1.iah.bbsrc.ac.uk>

I always just google for the terms I want and then add R-help to the search, which limits it to the R-help mailing list.  It's quite effective.

________________________________

From: r-help-bounces at stat.math.ethz.ch on behalf of Matthew Keller
Sent: Fri 23/02/2007 3:51 PM
To: S?rgio Nunes
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Google Custom Search Engine for R



Hi Sergio,

There was a discussion on this board recently about the difficulty of
searching for "R" related material on the web. I think the custom
google search engine is a good idea. It would be helpful if we could
have access to the full list of websites it is indexing so that we
could make suggestions about other sites that are missing. As it is,
it only tells us that there are 35 websites, and shows us the first
several.

Also, you might check out Sasha Goodman's Rseek: http://www.rseek.org/

Have you tried to compare the success of yours with Rseek?

All the Best,

Matt

On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> Hi,
>
> Since "R" is a (very) generic name, I've been having some trouble
> searching the web for this topic. Due to this, I've just created a
> Google Custom Search Engine that includes several of the most relevant
> sites that have information on R.
>
> See it in action at:
>
> http://google.com/coop/cse?cx=018133866098353049407%3Aozv9awtetwy
>
> This is really a preliminary test. Feel free to add yourself to the
> project and contribute with suggestions.
>
> S?rgio Nunes
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From coforfe at gmail.com  Fri Feb 23 17:41:39 2007
From: coforfe at gmail.com (Carlos Ortega)
Date: Fri, 23 Feb 2007 17:41:39 +0100
Subject: [R] Neural Net forecasting
In-Reply-To: <1c6126db0702230756k3a115fc2kcebd59d72a235b5a@mail.gmail.com>
References: <1c6126db0702230756k3a115fc2kcebd59d72a235b5a@mail.gmail.com>
Message-ID: <7b18cd4d0702230841y56a01459yfed096d2180afc30@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/19582834/attachment.pl 

From gunter.berton at gene.com  Fri Feb 23 17:46:00 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Fri, 23 Feb 2007 08:46:00 -0800
Subject: [R] Repeated measures in Classification and Regresssion Trees
In-Reply-To: <s5deb916.044@ds1.uwinnipeg.ca>
Message-ID: <001b01c7576a$1a1ce680$4d908980@gne.windows.gene.com>

Andrew:

Good question! AFAIK most of the so-called "machine learning" machinery --
regression and classification trees, SVM's, neural nets, random forests,
and other more chic methods (I make no attempt to keep up with all of them)
-- ignore error structure; that is, they assume the data are at least
independent (not necessarily identically distributed). I don't think merely
exchangeable is good enough either, though I may be wrong about this.

But I believe you have put your finger on a key issue: although all this
"cool" methodology is usually not terribly concerned with inference
(x-validation and bootstrapping being the usual methodology rather than,
say, asymptotics), one wonders how biased the estimators are when there are
various correlations in the data. I suspect a lot, depending on the nature
of the correlations and the methods. I think the moral is: thermodynamics
still rules -- there's no free lunch. You are just as likely to produce
nonsense using all this "nonparametric" methodology as you are using
parametric methods if you ignore the error structure of the data.
Incidentally, I should point out that George Box fulminated on this very
issue about 50 years ago. In his statistics classes he always used to say
that all the fuss (then) about using non-parametric rank-based methods (e.g.
Mann-Whitney-Wilcoxon) rather than parametric t-statistics was silly since
the t-statistics were relatively insensitive to deopartures from normality
anyway and it was lack of independence, not exact normality, that was the
key practical issue, and both approaches were sensitive to that. He
published several papers to this effect, of course.

Needless to say, I would welcome other -- especially better informed and
contrary -- views on these issues, either on or off list.

Cheers,

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andrew Park
Sent: Friday, February 23, 2007 7:51 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Repeated measures in Classification and Regresssion Trees

Dear R members,

I have been trying to find out whether one can use multivariate
regression trees (for example mvpart) to analyze repeated measures data.
 As a non-parametric technique, CART is insensitive to most of the
assumptions of parametric regression, but repeated measures data raises
the issue of the independence of several data points measured on the
same subject, or from the same plot over time.

Any perspectives will be welcome,



Andy Park (Assistant Professor)

Centre for Forest Interdisciplinary Research (CFIR),
Department of Biology,
University of Winnipeg,
515 Portage Avenue,
Winnipeg, Manitoba, R3B 2E9,
Canada

Phone: (204) 786-9407

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From antonio.fabio at gmail.com  Fri Feb 23 17:55:18 2007
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Fri, 23 Feb 2007 17:55:18 +0100
Subject: [R] Neural Net forecasting
In-Reply-To: <1c6126db0702230756k3a115fc2kcebd59d72a235b5a@mail.gmail.com>
References: <1c6126db0702230756k3a115fc2kcebd59d72a235b5a@mail.gmail.com>
Message-ID: <b0808fdc0702230855j332b091v617853d96ca0dfe9@mail.gmail.com>

The tsDyn package has the 'nnetTs' model, for fitting univariate NNET
time series model. The model is presented as a NonLinearAutoRegressive
model.
Boundled with tsDyn there is a vignette with some working examples.
Hope this helps.

Bests,
Antonio.

2007/2/23, sj <ssj1364 a gmail.com>:
> Are there any packages in R that are suitable for doing time series
> forecasting using neural networks? I have looked in the nnet package and
> neural package and they both seem geared towards classification.
>
> thanks,
>
> Spencer
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help a stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Antonio, Fabio Di Narzo
Ph.D. student at
Department of Statistical Sciences
University of Bologna, Italy


From theo_rstt at borm.org  Fri Feb 23 19:42:50 2007
From: theo_rstt at borm.org (theo borm)
Date: Fri, 23 Feb 2007 19:42:50 +0100
Subject: [R] using "integrate" in a function definition
Message-ID: <45DF35AA.5040704@borm.org>

Dear list members,

I'm quite new to R, and though I tried to find the answer to my probably 
very basic question through the available resources (website, mailing 
list archives, docs, google), I've not found it.


If I try to use the "integrate" function from within my own functions, 
my functions seem to misbehave in some contexts. The following example 
is a bit silly, but illustrates what's happening:


The following behaves as expected:

 > jjj<-function(www) {2*integrate(dnorm,0,www)$value}
 > kkk<-function(www) {2*(pnorm(www)-0.5)}
 > jjj(2)
[1] 0.9544997
 > kkk(2)
[1] 0.9544997



However, if I do:
 > plot(jjj,0,5)
Error in xy.coords(x, y, xlabel, ylabel, log) :
         'x' and 'y' lengths differ

whereas
 > plot(kkk,0,5)
produces a nice plot.




 > xxx<-seq(0:5)
 > yyy<-jjj(xxx)
 > zzz<-kkk(xxx)

produces no errors, but:
 > yyy
[1] 0.6826895
 > zzz
[1] 0.6826895 0.9544997 0.9973002 0.9999367 0.9999994 1.0000000




Why is this? Is this some R language feature that I've completely missed?




Ultimately my problem is that I have a mathematical function describing 
a distribution, and I want to use this in a Kolmogorov-Smirnov test 
where I need a cumulative distribution function. If I use the following 
(synthetic) dataset with ks.test with either the "jjj" or "kkk" 
function, I get:

 > ppp<-c(1.74865955,1.12220426,0.24760427,0.24351439,0.10870853,
	0.72023272,0.40245392,0.16002948,0.24203950,0.44029851,
	0.34830446,1.66967152,1.71609574,1.17540830,0.22306379,
	0.64382628,0.37382795,0.84361547,1.92138362,0.02844235,
	0.11238473,0.21237557,1.01058073,2.33108243,1.36034473,
	1.88951679,0.18230647,0.96571916,0.91239760,2.05574766,
	0.33681130,2.76006257,0.83952491,1.24038831,1.46199671,
	0.24694163,0.01565159,0.94816108,1.04642673,0.36556444,
	2.20760578,1.59518590,0.83951030,0.07113652,0.97422886,
	0.59835793,0.08383890,1.09180853,0.43508943,0.35368637,
	0.75805978,0.12790161,1.56798563,1.68669770,0.56168021)
 > ks.test(ppp,kkk)

         One-sample Kolmogorov-Smirnov test

data:  ppp
D = 0.1013, p-value = 0.5895
alternative hypothesis: two.sided

[ which seems correct as the samples come from abs(rnorm()) ]

 > ks.test(ppp,jjj)

         One-sample Kolmogorov-Smirnov test

data:  ppp
D = 0.9875, p-value < 2.2e-16
alternative hypothesis: two.sided

[ which is *incorrect* ]


Note: This example is artificial as I have used a function for which I 
know the integral; my real problem involves a distribution that I can 
only integrate numerically.

How would I go about to solve this problem?



With kind regards,

Theo.


From rvaradhan at jhmi.edu  Fri Feb 23 18:35:49 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 23 Feb 2007 12:35:49 -0500
Subject: [R] using "integrate" in a function definition
In-Reply-To: <45DF35AA.5040704@borm.org>
References: <45DF35AA.5040704@borm.org>
Message-ID: <001e01c75771$0f29b120$7c94100a@win.ad.jhu.edu>

Your function "jjj" is not vectorized.

Try this:

jjj <- function(www) sapply(www, function(x)2*integrate(dnorm,0,x)$value)
plot(jjj, 0, 5)

It should work.

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of theo borm
Sent: Friday, February 23, 2007 1:43 PM
To: r-help at stat.math.ethz.ch
Subject: [R] using "integrate" in a function definition

Dear list members,

I'm quite new to R, and though I tried to find the answer to my probably 
very basic question through the available resources (website, mailing 
list archives, docs, google), I've not found it.


If I try to use the "integrate" function from within my own functions, 
my functions seem to misbehave in some contexts. The following example 
is a bit silly, but illustrates what's happening:


The following behaves as expected:

 > jjj<-function(www) {2*integrate(dnorm,0,www)$value}
 > kkk<-function(www) {2*(pnorm(www)-0.5)}
 > jjj(2)
[1] 0.9544997
 > kkk(2)
[1] 0.9544997



However, if I do:
 > plot(jjj,0,5)
Error in xy.coords(x, y, xlabel, ylabel, log) :
         'x' and 'y' lengths differ

whereas
 > plot(kkk,0,5)
produces a nice plot.




 > xxx<-seq(0:5)
 > yyy<-jjj(xxx)
 > zzz<-kkk(xxx)

produces no errors, but:
 > yyy
[1] 0.6826895
 > zzz
[1] 0.6826895 0.9544997 0.9973002 0.9999367 0.9999994 1.0000000




Why is this? Is this some R language feature that I've completely missed?




Ultimately my problem is that I have a mathematical function describing 
a distribution, and I want to use this in a Kolmogorov-Smirnov test 
where I need a cumulative distribution function. If I use the following 
(synthetic) dataset with ks.test with either the "jjj" or "kkk" 
function, I get:

 > ppp<-c(1.74865955,1.12220426,0.24760427,0.24351439,0.10870853,
	0.72023272,0.40245392,0.16002948,0.24203950,0.44029851,
	0.34830446,1.66967152,1.71609574,1.17540830,0.22306379,
	0.64382628,0.37382795,0.84361547,1.92138362,0.02844235,
	0.11238473,0.21237557,1.01058073,2.33108243,1.36034473,
	1.88951679,0.18230647,0.96571916,0.91239760,2.05574766,
	0.33681130,2.76006257,0.83952491,1.24038831,1.46199671,
	0.24694163,0.01565159,0.94816108,1.04642673,0.36556444,
	2.20760578,1.59518590,0.83951030,0.07113652,0.97422886,
	0.59835793,0.08383890,1.09180853,0.43508943,0.35368637,
	0.75805978,0.12790161,1.56798563,1.68669770,0.56168021)
 > ks.test(ppp,kkk)

         One-sample Kolmogorov-Smirnov test

data:  ppp
D = 0.1013, p-value = 0.5895
alternative hypothesis: two.sided

[ which seems correct as the samples come from abs(rnorm()) ]

 > ks.test(ppp,jjj)

         One-sample Kolmogorov-Smirnov test

data:  ppp
D = 0.9875, p-value < 2.2e-16
alternative hypothesis: two.sided

[ which is *incorrect* ]


Note: This example is artificial as I have used a function for which I 
know the integral; my real problem involves a distribution that I can 
only integrate numerically.

How would I go about to solve this problem?



With kind regards,

Theo.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From elvis at xlsolutions-corp.com  Fri Feb 23 18:51:34 2007
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Fri, 23 Feb 2007 10:51:34 -0700
Subject: [R] Looking for info on R Advanced programming in the West Coast
Message-ID: <20070223105134.9f08cc34deb45d78e54b3b5664e21546.ad67bee7a2.wbe@email.secureserver.net>

Hi Tim,

We'll be offering our R/S Advanced Programming course in San Franciscon
on March 15-16. 

All the Best  - Elvis


Tim McDonald <tim_b_mcdonald <at> yahoo.com> writes:

> 
> 
>  Hi folks, 
> 
>   If you know of any upcoming R Advanced Programming in the West Coast - USA, please let me know.
> 
>   Thanks - Tim
> 
> ---------------------------------
> Need Mail bonding?
> 
>  
> ---------------------------------
> Never Miss an Email
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From sashag at stanford.edu  Fri Feb 23 18:53:02 2007
From: sashag at stanford.edu (Sasha Goodman)
Date: Fri, 23 Feb 2007 09:53:02 -0800
Subject: [R] Google Custom Search Engine for R
In-Reply-To: <3f547caa0702230809m64c19e69n54ea86c9f7b0b83f@mail.gmail.com>
References: <4c817d530702230653u330de9a2r144b7a4a667598d0@mail.gmail.com>
	<3f547caa0702230751k4a1da915ob3a62ef1db5db920@mail.gmail.com>
	<4c817d530702230758qbb7b1cfi7ad253f408dc0779@mail.gmail.com>
	<4c817d530702230801o6b723d4dt724af622f7265b61@mail.gmail.com>
	<3f547caa0702230809m64c19e69n54ea86c9f7b0b83f@mail.gmail.com>
Message-ID: <e6820af20702230953ve51d331t62e00bf595375c96@mail.gmail.com>

Thanks for emailing. I've added volunteers on RSeek on a trial basis.
There are several concerns, though:

One big issue is quality. Allowing volunteers means that someone might
need to police the volunteers, as they can put in advertisements for
books, links to sites that try to hack your computer, and links to
commercial software. Importantly, there seems to be no built in
policing mechanism yet in the Google coop architecture, nor any means
of sharing the maintenance.

Another issue is that I have to periodically visit the Google coop
webpage and manually check if people have volunteered. Google has not
added automatic acceptance.

As for open sourcing the php and javascript that makes up R seek, I'm
open to doing that during spring break if there is enough interest.

--Sasha



On 2/23/07, Matthew Keller <mckellercran at gmail.com> wrote:
> Hi Sasha,
>
> You might check out a recent discussion on the R forums (below). One
> possible suggestion about Rseek: can you allow others to contribute to
> it as well (ie, make Rseek open source just like R is?). Also, is
> Rseek going to be linked from the main R page anytime soon? Thanks for
> your efforts,
>
> Matt
>
> ---------- Forwarded message ----------
> From: S?rgio Nunes <snunes at gmail.com>
> Date: Feb 23, 2007 11:01 AM
> Subject: Re: [R] Google Custom Search Engine for R
> To: Matthew Keller <mckellercran at gmail.com>
> Cc: r-help at stat.math.ethz.ch
>
>
> I just seen the link you suggested - RSeek - and it uses exactly the
> same service from Google. The only difference is that it is not open
> to new contributions. Maybe Sasha could open it to volunteers.
>
> Also it seems to be much more complete than my initial attempt (as expected :).
> I think that RSeek "deserves" a link in R's homepage.
>
> S?rgio Nunes
>
> On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> > Hi,
> >
> > I've just created this search engine for testing today.
> > I think that if you volunteer as a contributor (see link on the left
> > side) you can then see and add to the list of sites.
> >
> > I've also noticed that I can add sites by importing a XML file.
> > Is there any list of sites that could be easily converted to XML?
> >
> > S?rgio Nunes
> >
> > On 2/23/07, Matthew Keller <mckellercran at gmail.com> wrote:
> > > Hi Sergio,
> > >
> > > There was a discussion on this board recently about the difficulty of
> > > searching for "R" related material on the web. I think the custom
> > > google search engine is a good idea. It would be helpful if we could
> > > have access to the full list of websites it is indexing so that we
> > > could make suggestions about other sites that are missing. As it is,
> > > it only tells us that there are 35 websites, and shows us the first
> > > several.
> > >
> > > Also, you might check out Sasha Goodman's Rseek: http://www.rseek.org/
> > >
> > > Have you tried to compare the success of yours with Rseek?
> > >
> > > All the Best,
> > >
> > > Matt
> > >
> > > On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> > > > Hi,
> > > >
> > > > Since "R" is a (very) generic name, I've been having some trouble
> > > > searching the web for this topic. Due to this, I've just created a
> > > > Google Custom Search Engine that includes several of the most relevant
> > > > sites that have information on R.
> > > >
> > > > See it in action at:
> > > >
> > > > http://google.com/coop/cse?cx=018133866098353049407%3Aozv9awtetwy
> > > >
> > > > This is really a preliminary test. Feel free to add yourself to the
> > > > project and contribute with suggestions.
> > > >
> > > > S?rgio Nunes
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > >
> > > --
> > > Matthew C Keller
> > > Postdoctoral Fellow
> > > Virginia Institute for Psychiatric and Behavioral Genetics
> > >
> >
>
>
> --
> Matthew C Keller
> Postdoctoral Fellow
> Virginia Institute for Psychiatric and Behavioral Genetics
>


From ripley at stats.ox.ac.uk  Fri Feb 23 18:57:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Feb 2007 17:57:58 +0000 (GMT)
Subject: [R] TRUE/FALSE as numeric values
In-Reply-To: <da79af330702230549l69038c47mae91672aa30e87e0@mail.gmail.com>
References: <45DEEE70.50307@slf.ch>
	<da79af330702230549l69038c47mae91672aa30e87e0@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0702231532090.12613@gannet.stats.ox.ac.uk>

It is inefficient to use which() rather than a logical index, since you 
allocate two numeric index vectors (one the length of the original 
vector) and use an interpreted function rather than optimized C code.
Also, in this usage which() handles NAs incorrectly.

I think the clearest answer is probably

    with(RSF_EU, AREA[AREA <= x])

On Fri, 23 Feb 2007, Henrique Dallazuanna wrote:

> You can also:
>
> test <- RSF_EU[which(RSF_EU$AREA<=x),]
>
> On 23/02/07, Thomas Preuth <preuth at slf.ch> wrote:
>>
>> Hello,
>>
>> I want to select in a column of a dataframe all numbers smaller than a
>> value x
>> but when I type in test<-(RSF_EU$AREA<=x) I receiv as answer:
>>> test
>> [1]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE
>> FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
>> [18]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
>> TRUE  TRUE FALSE  TRUE  TRUE  TRUE
>> [35] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE
>> TRUE  TRUE FALSE FALSE  TRUE FALSE
>> [52]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE
>>
>> How can i get the values smaller than x and not the TRUE/FALSE reply?
>>
>> Thanks in advance,
>> Thomas
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tplate at acm.org  Fri Feb 23 19:06:29 2007
From: tplate at acm.org (Tony Plate)
Date: Fri, 23 Feb 2007 11:06:29 -0700
Subject: [R] optim(method="L-BFGS-B") abnormal termination
In-Reply-To: <45DED301.40303@karlin.mff.cuni.cz>
References: <45DED301.40303@karlin.mff.cuni.cz>
Message-ID: <45DF2D25.5090309@acm.org>

I usually see this message only when my gradient and objective functions 
  do not match each other.  I debug by comparing a finite difference 
approximation to the gradient with the result of the gradient function.

I think you can also run optim() without supplying a gr() function - 
optim() will then use a finite difference approximation.  If optim() 
works fine like this with your function, that's a strong sign that your 
gradient function doesn't match your objective function.

It is of course possible that your gradient function is properly 
specified, and the function along the line being searched is so badly 
behaved that the line search can't find a minimum in 20 steps.  If 
that's the case you might want to look in scaling issues, or 
reformulating the problem.

It's also possible that even if you have a theoretically well-behaved 
objective and gradient, your computation of may be subject to rounding 
error and giving apparently discontinuous results to optim().

I'd look into all of the above possibilities before I tried increasing 
the limit of 20 evaluations in the line search - in my experience 20 
steps is plenty to find an adequate point for a reasonably well-behaved 
function.  It may be possible to increase the number of steps, but I 
don't see how from the docs for ?optim.  Of course, the source is available.

hope this helps,

Tony Plate

Petr Klasterecky wrote:
> Hi,
> my call of optim() with the L-BFGS-B method ended with the following 
> error message: ERROR: ABNORMAL_TERMINATION_IN_LNSRCH
> 
> Further tracing shows:
> Line search cannot locate an adequate point after 20 function and 
> gradient evaluations
> final  value 0.086627
> stopped after 7 iterations
> 
> Could someone pls tell me whether it is possible to increase the limit 
> of 20 evaluations? Is it even worth doing so?
> 
> My function(s) to be minimized are polynomial functions of tens of 
> variables - let say 10 - 60 variables, all of them constrained to the 
> (0,1) interval. Is it even possible and meaningfull to attempt such 
> minimization? (Suppose I have good starting values.)
> 
> Thaks, Petr


From albmont at centroin.com.br  Fri Feb 23 19:18:01 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 23 Feb 2007 16:18:01 -0200
Subject: [R] using "integrate" in a function definition
In-Reply-To: <45DF35AA.5040704@borm.org>
References: <45DF35AA.5040704@borm.org>
Message-ID: <20070223181746.M39373@centroin.com.br>

theo borm wrote: 
> 
>> jjj<-function(www) {2*integrate(dnorm,0,www)$value} 
>> kkk<-function(www) {2*(pnorm(www)-0.5)} 

>> xxx<-seq(0:5) 
>> yyy<-jjj(xxx) 
>> zzz<-kkk(xxx) 
> 
> produces no errors, but: 
>> yyy 
> [1] 0.6826895 
>> zzz 
> [1] 0.6826895 0.9544997 0.9973002 0.9999367 0.9999994 1.0000000 
> 
> Why is this? Is this some R language feature that I've completely missed? 
> 
Yes. Some functions work on vectors (and matrices), so 
when you give a vector, it returns a vector. This is true for 
most common functions (sin, cos), arithmetic operations (with 
the caveat that different dimensions for the arguments may cause 
unexpected outcomes) and some internal functions (dnorm, pnorm). 
So, if you write sin(0:10) or dnorm((-3):3), you get a vector. 

Some other functions don't, and this is the case with integrate. 
For example: 

fff <- function(x) x 
integrate(fff, 0, 1)  # ok 
integrate(fff, 0, 1:5) # will integrate from 0 to 1 and ignore 2:5 

'plot' will probably fall into some code that uses this 
vector-in-vector-out hypothesis, and then fail when the size 
of x differs from the size of y. 

Alberto Monteiro 

PS: fff <- function(x) 1 
integrate(fff, 0, 1)  # error. why?


From chrysopa at gmail.com  Fri Feb 23 19:21:11 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Fri, 23 Feb 2007 16:21:11 -0200
Subject: [R] some caracter dont work with JGR
Message-ID: <200702231621.11884.chrysopa@gmail.com>

Hi,

I testing JGR and I like, but my ~ caracter dont work. My keyboard is 
Brazilian ABNT2.

The key is OK, only in JGR it dont work.

Anybody have any idea about this?

Thanks
Ronaldo
-- 
Mais variado que baldea??o em Cacequi.
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | ronaldo.reis em unimontes.br | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From albmont at centroin.com.br  Fri Feb 23 19:24:54 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 23 Feb 2007 16:24:54 -0200
Subject: [R] pdf with an exact size
Message-ID: <20070223181841.M52887@centroin.com.br>

Is it possible to create a pdf output file with an (as nearly as
possible) exact size?

For example, if I want to draw in an A4 paper (210 x 297 mm) a
square of 100 x 100 mm, how can I do it?

FWIW, about 6 months ago I learned here how to create an exact
png image. For example, if I want a 500 x 500 black square in 
a 1000 x 1000 white png, occupying the center of the png, the 
procedure is this:

  png("image.png", width=1000, height=1000, bg="white")
  par(mar=c(0,0,0,0)) # reset margins
  plot(0, xlim=c(0, 999), ylim=c(0, 999), col="white")
  par(usr=c(0, 999, 0, 999))
  points(c(250, 250, 749, 749, 250), c(250, 749, 749, 250, 250), 
    type="l", col="black")
  dev.off()

However, I don't know how do this with a pdf monstr... oops... file.

Alberto Monteiro


From albmont at centroin.com.br  Fri Feb 23 19:52:45 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 23 Feb 2007 16:52:45 -0200
Subject: [R] pdf with an exact size
In-Reply-To: <20070223181841.M52887@centroin.com.br>
References: <20070223181841.M52887@centroin.com.br>
Message-ID: <20070223184952.M54192@centroin.com.br>

I must be stupid and/or crazy... I figured out the solution a few
minutes after I asked :-/

> Is it possible to create a pdf output file with an (as nearly as
> possible) exact size?

Yes:

  pdf("a4.pdf", width=210, height=297, bg="white", paper="a4")
  par(mar=c(0,0,0,0)) # reset margins
  plot(0, xlim=c(0, 210), ylim=c(0, 297), col="white")
  par(usr=c(0, 210, 0, 297))
  points(c(100, 100, 200, 200, 100), c(100, 200, 200, 100, 100, 100),
    type="l", col="black")
  dev.off()
 
Alberto Monteiro


From chrysopa at gmail.com  Fri Feb 23 20:18:56 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Fri, 23 Feb 2007 17:18:56 -0200
Subject: [R] problem with weights on lmer function
In-Reply-To: <20070222223621.GV24437@ms.unimelb.edu.au>
References: <200702221822.04001.chrysopa@gmail.com>
	<20070222223621.GV24437@ms.unimelb.edu.au>
Message-ID: <200702231718.56551.chrysopa@gmail.com>

Em Quinta 22 Fevereiro 2007 20:36, Andrew Robinson escreveu:
> Hi Ronaldo,
>
> I suggest that you send us a small, well-documented, code example that
> we can reproduce.  It certainly looks as though there is a problem,
> but given this information it's hard to know what it is!
>
> Cheers
>
> Andrew

Andrew and all R users.

Look this example:

test<-structure(list(subject = structure(c(1, 1, 1, 1, 1, 1, 1, 1, 
2, 2, 2, 2, 2, 2, 2, 2), .Label = c("S1", "S2"), class = "factor"), 
    time = c(0, 7, 15, 22, 32, 39, 46, 53, 0, 7, 14, 24, 28, 
    34, 41, 48), noccup = c(0, 1, 2, 1, 6, 4, 3, 3, 0, 18, 
    21, 14, 7, 14, 12, 8), ntotal = c(100, 100, 100, 100, 100, 
    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100)), .Names = 
c("subject", 
"time", "noccup", "ntotal"), class = "data.frame", row.names = c("1", 
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
"14", "15", "16"))

I have 2 subject over 8 times. Each subject is compound by 100 pieces. I 
measure the number of piece occuped in any time.

I try this:

m1<-lmer(noccup/ntotal~time+(time|subject),family=binomial,weights=ntotal)
Error in lmer(noccup/ntotal ~ time + (time | subject), family = binomial,  : 
        object `weights' of incorrect type

I dont understand why this error.

m1<-lmer(noccup/ntotal~1+(time|subject),family=binomial,weights=ntotal)
Error in lmer(noccup/ntotal ~ time + (time | subject), family = binomial,  : 
        object `weights' of incorrect type

Why weights is of incorrect type? In glm this is correct type.

Thanks
Ronaldo

-- 
The right half of the brain controls the left half of the body.  This
means that only left handed people are in their right mind.
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | ronaldo.reis em unimontes.br | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From Serguei.Kaniovski at wifo.ac.at  Fri Feb 23 20:27:45 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Fri, 23 Feb 2007 20:27:45 +0100
Subject: [R] how to use apply with two variables
Message-ID: <OF8694010B.83EF2E85-ONC125728B.006AE96A-C125728B.006AE973@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070223/faf940a5/attachment.pl 

From andy_liaw at merck.com  Fri Feb 23 20:38:59 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 23 Feb 2007 14:38:59 -0500
Subject: [R] how to use apply with two variables
In-Reply-To: <OF8694010B.83EF2E85-ONC125728B.006AE96A-C125728B.006AE973@wsr.ac.at>
References: <OF8694010B.83EF2E85-ONC125728B.006AE96A-C125728B.006AE973@wsr.ac.at>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03C357E2@usctmx1106.merck.com>

Yes.  Just try it and see.

BTW, your usage of return() is not recommended anymore.  This is
probably easier:

myfun<-function(x) c(mean=mean(x), sd=sd(x))
out <- apply(mat, 1, myfun)
## or...
out2 <- cbind(mean=rowMeans(mat), sd=sd(t(mat))) 

Andy


From: Serguei Kaniovski
> 
> Hi,
> 
> this is a made-up example. Function "myfun" returns two 
> arguments. Can "apply" be used so that "myfun" is called only once?
> 
> Thanks
> Serguei
> 
> mat<-matrix(runif(50),nrow=10,ncol=5)
> 
> myfun<-function(x) {
>  mymean<-mean(x)
>  mysd<-sd(x)
>  return(mymean,mysd)
> }
> 
> out1<-t(apply(mat,1,function(x) myfun(x)$mymean))
> out2<-t(apply(mat,1,function(x) myfun(x)$mysd))
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From efg at stowers-institute.org  Fri Feb 23 20:50:34 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Fri, 23 Feb 2007 13:50:34 -0600
Subject: [R] mixture of 2 normals - starting values
References: <OF89608350.3A0617D7-ON8625728B.001ADEC6-8625728B.001BAD0C@mmm.com>
Message-ID: <erngid$sj0$1@sea.gmane.org>

If you don't have too much noise, the peaks in the derivative curves can be 
used:

See:

http://research.stowers-institute.org/efg/R/Statistics/MixturesOfDistributions/index.htm

efg
Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research

<apjaworski at mmm.com> wrote in message 
news:OF89608350.3A0617D7-ON8625728B.001ADEC6-8625728B.001BAD0C at mmm.com...
> Hi,
>
> I have a problem of estimating a mixture of two normal distributions.  I
> need to find the starting points automatically, since this is a part of a
> larger piece of image processing code.


From tkobayas at indiana.edu  Fri Feb 23 21:54:34 2007
From: tkobayas at indiana.edu (Takatsugu Kobayashi)
Date: Fri, 23 Feb 2007 15:54:34 -0500
Subject: [R] Overlaying graphics
Message-ID: <45DF548A.9050402@indiana.edu>

Rusers:

This is a very fundamental and silly question. How can I overlay 
different maps on the same x and y scales? I do neither want to change X 
and Y axes nor show different x and y ticks on top of each other.

Thanks

Taka
Indiana University


From theo.borm at wur.nl  Fri Feb 23 22:36:55 2007
From: theo.borm at wur.nl (theo borm)
Date: Fri, 23 Feb 2007 22:36:55 +0100
Subject: [R] using "integrate" in a function definition
In-Reply-To: <001e01c75771$0f29b120$7c94100a@win.ad.jhu.edu>
References: <45DF35AA.5040704@borm.org>
	<001e01c75771$0f29b120$7c94100a@win.ad.jhu.edu>
Message-ID: <45DF5E77.90209@wur.nl>

Ravi Varadhan wrote:

>Your function "jjj" is not vectorized.
>
>Try this:
>
>jjj <- function(www) sapply(www, function(x)2*integrate(dnorm,0,x)$value)
>plot(jjj, 0, 5)
>
>It should work.
>  
>
Yes it does. Thanks!

Thinking of it, it now starts to make some sort of sense that integrate 
should return a "scalar"; the result is really a list of which I only 
used the "value" component. And now I also understand the "x,q: vector 
of quantiles." phrases in the documentation of dnorm.

Now if I do something silly:
 > jjj<-function(www) {2*integrate(dnorm,0,www)$value+sin(www)-sin(www)}
then
 > jjj(1:5)
[1] 0.6826895 0.6826895 0.6826895 0.6826895 0.6826895

Evidently the "inherritance" of "being vectorized" (which was why my kkk 
function worked) could lead to some unexpected (and probably hard to 
debug) results :-/

Thanks.


with kind regards,

Theo.


From smckinney at bccrc.ca  Fri Feb 23 23:28:06 2007
From: smckinney at bccrc.ca (Steven McKinney)
Date: Fri, 23 Feb 2007 14:28:06 -0800
Subject: [R] Google Custom Search Engine for R
References: <4c817d530702230653u330de9a2r144b7a4a667598d0@mail.gmail.com>
	<3f547caa0702230751k4a1da915ob3a62ef1db5db920@mail.gmail.com>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0235FB1B@crcmail1.BCCRC.CA>


Whereas "R" is very generic,
"CRAN" is much less so.

I've had very good luck adding CRAN
to my search terms, e.g. try to Google

cran 3d scatterplot

This produces all R-related hits on
the first Google page.


Hope this helps


Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney at bccrc.ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch on behalf of Matthew Keller
Sent: Fri 2/23/2007 7:51 AM
To: S?rgio Nunes
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Google Custom Search Engine for R
 
Hi Sergio,

There was a discussion on this board recently about the difficulty of
searching for "R" related material on the web. I think the custom
google search engine is a good idea. It would be helpful if we could
have access to the full list of websites it is indexing so that we
could make suggestions about other sites that are missing. As it is,
it only tells us that there are 35 websites, and shows us the first
several.

Also, you might check out Sasha Goodman's Rseek: http://www.rseek.org/

Have you tried to compare the success of yours with Rseek?

All the Best,

Matt

On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> Hi,
>
> Since "R" is a (very) generic name, I've been having some trouble
> searching the web for this topic. Due to this, I've just created a
> Google Custom Search Engine that includes several of the most relevant
> sites that have information on R.
>
> See it in action at:
>
> http://google.com/coop/cse?cx=018133866098353049407%3Aozv9awtetwy
>
> This is really a preliminary test. Feel free to add yourself to the
> project and contribute with suggestions.
>
> S?rgio Nunes
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From theo_rstt at borm.org  Fri Feb 23 22:58:02 2007
From: theo_rstt at borm.org (theo borm)
Date: Fri, 23 Feb 2007 22:58:02 +0100
Subject: [R] using "integrate" in a function definition
In-Reply-To: <20070223181746.M39373@centroin.com.br>
References: <45DF35AA.5040704@borm.org> <20070223181746.M39373@centroin.com.br>
Message-ID: <45DF636A.8090601@borm.org>

Hi,

Many thanks for the explanation.

Alberto Monteiro wrote:

>
>PS: fff <- function(x) 1 
>integrate(fff, 0, 1)  # error. why?
>

Guess: because integrate itself expects a "vectorized function" ?

 > fff(1:5)
[1] 1
 > ggg<-function(x) { sapply(x, function(x)1) }
 > ggg(1:5)
[1] 1 1 1 1 1
 > integrate(ggg,0,1)
1 with absolute error < 1.1e-14
 > hhh<-function(x) 1+0*x
 > integrate(hhh, 0, 1)
1 with absolute error < 1.1e-14

I sense a certain lack of intuitiveness here :-/


regards, Theo


From j.van_den_hoff at fzd.de  Fri Feb 23 23:41:38 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Fri, 23 Feb 2007 23:41:38 +0100
Subject: [R] pdf with an exact size
In-Reply-To: <20070223181841.M52887@centroin.com.br>
References: <20070223181841.M52887@centroin.com.br>
Message-ID: <20070223224138.GA2569@kati.fz-rossendorf.de>

On Fri, Feb 23, 2007 at 04:24:54PM -0200, Alberto Monteiro wrote:
> Is it possible to create a pdf output file with an (as nearly as
> possible) exact size?
> 
> For example, if I want to draw in an A4 paper (210 x 297 mm) a
> square of 100 x 100 mm, how can I do it?
> 
> FWIW, about 6 months ago I learned here how to create an exact
> png image. For example, if I want a 500 x 500 black square in 
> a 1000 x 1000 white png, occupying the center of the png, the 
> procedure is this:
> 
>   png("image.png", width=1000, height=1000, bg="white")
>   par(mar=c(0,0,0,0)) # reset margins
>   plot(0, xlim=c(0, 999), ylim=c(0, 999), col="white")
>   par(usr=c(0, 999, 0, 999))
>   points(c(250, 250, 749, 749, 250), c(250, 749, 749, 250, 250), 
>     type="l", col="black")
>   dev.off()
> 
> However, I don't know how do this with a pdf monstr... oops... file.
> 
> Alberto Monteiro
> 

going via `bitmap' and using the `pdfwrite' type is probably the better
idea since in this case `ghostscript` is used for pdf generation which seems
over all to generate cleaner/better pdf-output in comparsion
to the R internal pdf-device (I believe there was recently some
discussion of this on the list?).

joerg


From smckinney at bccrc.ca  Sat Feb 24 01:01:53 2007
From: smckinney at bccrc.ca (Steven McKinney)
Date: Fri, 23 Feb 2007 16:01:53 -0800
Subject: [R] How to plot two graphs on one single plot?
References: <45DEFD86.5070708@princeton.edu><da79af330702230723u23b78dd4uacc03bb0beed0232@mail.gmail.com>
	<45DF0991.70607@princeton.edu>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0235FB1C@crcmail1.BCCRC.CA>


Broadly speaking, there are a few classes 
of plotting functions.

1)  "New graph" functions.
The plot() function starts a new graph.

2)  "Add to a graph" functions
The points(), lines(), text() etc. functions
add something to the current graph.

3)  "Control graph" functions
par() controls various aspects of the graph.

R graphics experts might have some better
classification and terminology.

So you want your second plotting function to be
points() rather than plot(), to add to the
existing graph.  

Try

>  x1=rnorm(25, mean=0, sd=1)
>  y1=dnorm(x1, mean=0, sd=1)
> 
>  x2=rnorm(25, mean=0, sd=1)
>  y2=dnorm(x2, mean=0, sd=1)
>  plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2), xlab='x', ylab='y')
>  points(x2, y2, type='p', col="red", xlab='x', ylab='y')
> 

Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney at bccrc.ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch on behalf of Yun Zhang
Sent: Fri 2/23/2007 7:34 AM
To: Henrique Dallazuanna
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to plot two graphs on one single plot?
 
Thanks. Now R plots two graphs on one plot.
Yet they are still on two graphs, vertically parallelized with each other.

But what I want to do is actually plotting two distribution on one 
single graph, using the same x and y axis. Like:
|
|
|               (dist2)
|   (dist 1)
|
--------------------------->

Is it possible to do that?

Thanks,
Yun

Henrique Dallazuanna wrote:
> par(mfrow=c(2,1))
> #your plot
> #after plot
> par(mfrow=c(1,1))
>
> On 23/02/07, *Yun Zhang* <yunzhang at princeton.edu 
> <mailto:yunzhang at princeton.edu>> wrote:
>
>     Hi,
>
>     I am trying to plot two distribution graph on one plot. But I dont
>     know
>     how. I set them to the same x, y limit, even same x, y labels.
>
>     Code:
>     x1=rnorm(25, mean=0, sd=1)
>     y1=dnorm(x1, mean=0, sd=1)
>
>     x2=rnorm(25, mean=0, sd=1)
>     y2=dnorm(x2, mean=0, sd=1)
>     plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2),
>     xlab='x',
>     ylab='y')
>     plot(x2, y2, type='p', col="red", xlab='x', ylab='y')
>
>     They just dont show up in one plot.
>
>     Any hint will be very helpful.
>
>     Thanks,
>     Yun
>
>     ______________________________________________
>     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
>     list
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> -- 
> Henrique Dallazuanna
> Curitiba-Paran?
> Brasil

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From lymanmh at xmission.com  Sat Feb 24 09:22:27 2007
From: lymanmh at xmission.com (Mark and Heather Lyman)
Date: Sat, 24 Feb 2007 01:22:27 -0700
Subject: [R] barchart (lattice) with text labels
Message-ID: <45DFF5C3.60806@xmission.com>

I would like to place the value for each bar in barchart (lattice) at
the top of each bar. Something like the following code produces.

library(lattice)

mypanelfunc <- function(x, y, ...)
{
  panel.barchart(x, y, ...)
  panel.text(x, y, labels=as.character(round(x,2)), ...)
  }

myprepanelfunc <- function(x, y, ...) list(xlim=c(0, max(x)+.1))

mydata <- expand.grid(a=factor(1:5), b=factor(1:3), c=factor(1:2))
mydata$x <- runif(nrow(mydata))

barchart(a~x|b, mydata, groups=c, panel=mypanelfunc,
prepanel=myprepanelfunc, adj=c(-0.1,0.5))

However, I cannot figure out how to shift the values to correspond with
their respective grouped bar. Has anyone done this type of thing? Thanks.

Mark Lyman


From brown_emu at yahoo.com  Sat Feb 24 11:12:56 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 24 Feb 2007 02:12:56 -0800 (PST)
Subject: [R] Google Custom Search Engine for R
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0235FB1B@crcmail1.BCCRC.CA>
Message-ID: <148.15585.qm@web39701.mail.mud.yahoo.com>

There appears to be another site (linked from Wikipedia) that lists some of
the sites that it searches:

http://www.dangoldstein.com/search_r.html


--- Steven McKinney <smckinney at bccrc.ca> wrote:

> 
> Whereas "R" is very generic,
> "CRAN" is much less so.
> 
> I've had very good luck adding CRAN
> to my search terms, e.g. try to Google
> 
> cran 3d scatterplot
> 
> This produces all R-related hits on
> the first Google page.
> 
> 
> Hope this helps
> 
> 
> Steven McKinney
> 
> Statistician
> Molecular Oncology and Breast Cancer Program
> British Columbia Cancer Research Centre
> 
> email: smckinney at bccrc.ca
> 
> tel: 604-675-8000 x7561
> 
> BCCRC
> Molecular Oncology
> 675 West 10th Ave, Floor 4
> Vancouver B.C. 
> V5Z 1L3
> Canada
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch on behalf of Matthew Keller
> Sent: Fri 2/23/2007 7:51 AM
> To: S?rgio Nunes
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Google Custom Search Engine for R
>  
> Hi Sergio,
> 
> There was a discussion on this board recently about the difficulty of
> searching for "R" related material on the web. I think the custom
> google search engine is a good idea. It would be helpful if we could
> have access to the full list of websites it is indexing so that we
> could make suggestions about other sites that are missing. As it is,
> it only tells us that there are 35 websites, and shows us the first
> several.
> 
> Also, you might check out Sasha Goodman's Rseek: http://www.rseek.org/
> 
> Have you tried to compare the success of yours with Rseek?
> 
> All the Best,
> 
> Matt
> 
> On 2/23/07, S?rgio Nunes <snunes at gmail.com> wrote:
> > Hi,
> >
> > Since "R" is a (very) generic name, I've been having some trouble
> > searching the web for this topic. Due to this, I've just created a
> > Google Custom Search Engine that includes several of the most relevant
> > sites that have information on R.
> >
> > See it in action at:
> >
> > http://google.com/coop/cse?cx=018133866098353049407%3Aozv9awtetwy
> >
> > This is really a preliminary test. Feel free to add yourself to the
> > project and contribute with suggestions.
> >
> > S?rgio Nunes
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> -- 
> Matthew C Keller
> Postdoctoral Fellow
> Virginia Institute for Psychiatric and Behavioral Genetics
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From brown_emu at yahoo.com  Sat Feb 24 11:34:39 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 24 Feb 2007 02:34:39 -0800 (PST)
Subject: [R] How to plot two graphs on one single plot?
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0235FB1C@crcmail1.BCCRC.CA>
Message-ID: <934516.87338.qm@web39715.mail.mud.yahoo.com>

you can also call

plot(x1,y1)
par(new=TRUE)
plot(x2,y2,axes=FALSE)

but this is more helpful for when you want to plot with multiple y-axes.

in general, it is also possible to build up from low-level graphical
elements:

plot.new()
plot.window(xlim=range(x1),ylim=range(y1))
lines(x1,y1)
axis(1); axis(2); box()
plot.window(xlim=range(x2),ylim=range(y2))
lines(x2,y2)
axis(1); axis(2); box()

if you want both plots to be on the same scale, you can specify the axes
limits a priori, or retrieve the limits from the first figure to call in the
second by par("usr"). to use these in the second xlim, ylim call, you should
set par(xaxs="i",yaxs="i") so they aren't padded above and below the
specified limits. (see ?par for xaxs and yaxs)

best regards,

st

--- Steven McKinney <smckinney at bccrc.ca> wrote:

> 
> Broadly speaking, there are a few classes 
> of plotting functions.
> 
> 1)  "New graph" functions.
> The plot() function starts a new graph.
> 
> 2)  "Add to a graph" functions
> The points(), lines(), text() etc. functions
> add something to the current graph.
> 
> 3)  "Control graph" functions
> par() controls various aspects of the graph.
> 
> R graphics experts might have some better
> classification and terminology.
> 
> So you want your second plotting function to be
> points() rather than plot(), to add to the
> existing graph.  
> 
> Try
> 
> >  x1=rnorm(25, mean=0, sd=1)
> >  y1=dnorm(x1, mean=0, sd=1)
> > 
> >  x2=rnorm(25, mean=0, sd=1)
> >  y2=dnorm(x2, mean=0, sd=1)
> >  plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2), xlab='x',
> ylab='y')
> >  points(x2, y2, type='p', col="red", xlab='x', ylab='y')
> > 
> 
> Steven McKinney
> 
> Statistician
> Molecular Oncology and Breast Cancer Program
> British Columbia Cancer Research Centre
> 
> email: smckinney at bccrc.ca
> 
> tel: 604-675-8000 x7561
> 
> BCCRC
> Molecular Oncology
> 675 West 10th Ave, Floor 4
> Vancouver B.C. 
> V5Z 1L3
> Canada
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch on behalf of Yun Zhang
> Sent: Fri 2/23/2007 7:34 AM
> To: Henrique Dallazuanna
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] How to plot two graphs on one single plot?
>  
> Thanks. Now R plots two graphs on one plot.
> Yet they are still on two graphs, vertically parallelized with each other.
> 
> But what I want to do is actually plotting two distribution on one 
> single graph, using the same x and y axis. Like:
> |
> |
> |               (dist2)
> |   (dist 1)
> |
> --------------------------->
> 
> Is it possible to do that?
> 
> Thanks,
> Yun
> 
> Henrique Dallazuanna wrote:
> > par(mfrow=c(2,1))
> > #your plot
> > #after plot
> > par(mfrow=c(1,1))
> >
> > On 23/02/07, *Yun Zhang* <yunzhang at princeton.edu 
> > <mailto:yunzhang at princeton.edu>> wrote:
> >
> >     Hi,
> >
> >     I am trying to plot two distribution graph on one plot. But I dont
> >     know
> >     how. I set them to the same x, y limit, even same x, y labels.
> >
> >     Code:
> >     x1=rnorm(25, mean=0, sd=1)
> >     y1=dnorm(x1, mean=0, sd=1)
> >
> >     x2=rnorm(25, mean=0, sd=1)
> >     y2=dnorm(x2, mean=0, sd=1)
> >     plot(x1, y1, type='p', xlim=range(x1,x2), ylim=range(y1, y2),
> >     xlab='x',
> >     ylab='y')
> >     plot(x2, y2, type='p', col="red", xlab='x', ylab='y')
> >
> >     They just dont show up in one plot.
> >
> >     Any hint will be very helpful.
> >
> >     Thanks,
> >     Yun
> >
> >     ______________________________________________
> >     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
> >     list
> >     https://stat.ethz.ch/mailman/listinfo/r-help
> >     <https://stat.ethz.ch/mailman/listinfo/r-help>
> >     PLEASE do read the posting guide
> >     http://www.R-project.org/posting-guide.html
> >     and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> > -- 
> > Henrique Dallazuanna
> > Curitiba-Paran?
> > Brasil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Need a quick answer? Get one in minutes from people who know.
Ask your question on www.Answers.yahoo.com


From brown_emu at yahoo.com  Sat Feb 24 11:54:05 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 24 Feb 2007 02:54:05 -0800 (PST)
Subject: [R] Overlaying graphics
In-Reply-To: <45DF548A.9050402@indiana.edu>
Message-ID: <894735.56868.qm@web39701.mail.mud.yahoo.com>

Not pretty, but you could possibly try:

# first map
map(#arguments#)
xylim = par("usr")

# second map
out = map(#arguments#, plot=FALSE)
par(xaxs="i",yaxs="i")
plot.window(xlim=xylim[1:2],ylim=xylim[3:4])
polygon(out)




--- Takatsugu Kobayashi <tkobayas at indiana.edu> wrote:

> Rusers:
> 
> This is a very fundamental and silly question. How can I overlay 
> different maps on the same x and y scales? I do neither want to change X 
> and Y axes nor show different x and y ticks on top of each other.
> 
> Thanks
> 
> Taka
> Indiana University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pmilin at gmail.com  Sat Feb 24 13:00:15 2007
From: pmilin at gmail.com (Petar Milin)
Date: Sat, 24 Feb 2007 13:00:15 +0100
Subject: [R] Making two lines graph ...
Message-ID: <1172318415.15046.8.camel@localhost>

Hello!
Can anyone help me to build a graph with the alphanumeric values on
x-axis, with two lines (preferably doted and solid, or similar) that
present values on y-axes. In a toy example, data frame could be like
this:
x.orig x.num y1 y2
a 1 0.2 0.4
b 2 0.1 0.1
c 3 0.3 0.3
d 4 0.3 0.15
e 5 0.1 0.05

I can make graph only if I use values converted to numeric in "x.num",
but not original "X.orig":
matplot(dat$x.num, dat[, c("y1","y2")], type="b", lty=1, ylab="(1) y1,
(2) y2")

Also, how to make doted and solid instead of coloured lines?

Thanks in advance. Sincerely,
Petar M


From renaud.lancelot at gmail.com  Sat Feb 24 13:09:21 2007
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Sat, 24 Feb 2007 13:09:21 +0100
Subject: [R] pdf with an exact size
In-Reply-To: <20070223224138.GA2569@kati.fz-rossendorf.de>
References: <20070223181841.M52887@centroin.com.br>
	<20070223224138.GA2569@kati.fz-rossendorf.de>
Message-ID: <c2ee56800702240409u6cd0413bx2c8c494aa899b1c9@mail.gmail.com>

The package grid provides very convenient tools for such things:

library(grid)
pdf(file = "square.pdf", paper = "a4")
pushViewport(viewport())
grid.rect(width = 100, height = 100, default.units = "mm")
dev.off()

works just fine for me (R 2.4.1, MS WIndows XP): the printed output on
my inkjet printer is exactly 100 mm x 100 mm.

Best,

Renaud

2007/2/23, Joerg van den Hoff <j.van_den_hoff at fzd.de>:
> On Fri, Feb 23, 2007 at 04:24:54PM -0200, Alberto Monteiro wrote:
> > Is it possible to create a pdf output file with an (as nearly as
> > possible) exact size?
> >
> > For example, if I want to draw in an A4 paper (210 x 297 mm) a
> > square of 100 x 100 mm, how can I do it?
> >
> > FWIW, about 6 months ago I learned here how to create an exact
> > png image. For example, if I want a 500 x 500 black square in
> > a 1000 x 1000 white png, occupying the center of the png, the
> > procedure is this:
> >
> >   png("image.png", width=1000, height=1000, bg="white")
> >   par(mar=c(0,0,0,0)) # reset margins
> >   plot(0, xlim=c(0, 999), ylim=c(0, 999), col="white")
> >   par(usr=c(0, 999, 0, 999))
> >   points(c(250, 250, 749, 749, 250), c(250, 749, 749, 250, 250),
> >     type="l", col="black")
> >   dev.off()
> >
> > However, I don't know how do this with a pdf monstr... oops... file.
> >
> > Alberto Monteiro
> >
>
> going via `bitmap' and using the `pdfwrite' type is probably the better
> idea since in this case `ghostscript` is used for pdf generation which seems
> over all to generate cleaner/better pdf-output in comparsion
> to the R internal pdf-device (I believe there was recently some
> discussion of this on the list?).
>
> joerg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Renaud LANCELOT
D?partement Syst?mes Biologiques du CIRAD
CIRAD, Biological Systems Department

Campus International de Baillarguet
TA 30 / B
F34398 Montpellier
Tel   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 37 37
Fax   +33 (0)4 67 59 37 95


From ulrich.keller at emacs.lu  Sat Feb 24 12:47:52 2007
From: ulrich.keller at emacs.lu (Ulrich Keller)
Date: Sat, 24 Feb 2007 11:47:52 +0000 (UTC)
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
Message-ID: <loom.20070224T123203-607@post.gmane.org>

I am trying to read a number of XML files using xmlTreeParse(). Unfortunately,
some of them are malformed in a way that makes R crash. The problem is that
closing tags are sometimes repeated like this:

<tag>value1</tag><tag>value2</tag>some garbage</tag></tag><tag>value3</tag>

I want to preprocess the contents of the XML file using gsub() before feeding
them to xmlTreeParse() to clean them up, but I can't figure out how to do it.
What I need is something that transforms the example above into:

<tag>value1</tag><tag>value2</tag><tag>value3</tag>

Some kind of "</tag>.*</tag>" that only matches if there is no "<tag>" in ".*".

Thanks in advance for you ideas,

Uli


From milton_ruser at yahoo.com.br  Sat Feb 24 13:33:18 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sat, 24 Feb 2007 04:33:18 -0800 (PST)
Subject: [R] recovering collums of DF using a text var.list
Message-ID: <537792.40150.qm@web56602.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/7164edfb/attachment.pl 

From n.nguyen at garvan.org.au  Sat Feb 24 13:59:56 2007
From: n.nguyen at garvan.org.au (Nguyen Dinh Nguyen)
Date: Sat, 24 Feb 2007 23:59:56 +1100 (EST)
Subject: [R] How to plot two graphs on one single plot?
Message-ID: <20070224235956.AEN19721@gimr.garvan.unsw.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/7812b34d/attachment.pl 

From jholtman at gmail.com  Sat Feb 24 14:08:03 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 24 Feb 2007 08:08:03 -0500
Subject: [R] Making two lines graph ...
In-Reply-To: <1172318415.15046.8.camel@localhost>
References: <1172318415.15046.8.camel@localhost>
Message-ID: <644e1f320702240508m4a9aa3b3y22b9e060eea71736@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/7ae4cb03/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Sat Feb 24 14:45:56 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 24 Feb 2007 13:45:56 -0000 (GMT)
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <Pine.LNX.4.44.0702221311250.14521-100000@reclus.nhh.no>
Message-ID: <XFMail.070224134556.ted.harding@nessie.mcc.ac.uk>

[Apologies if this is a repeated posting for you. Something seems
 to have gone amiss with my previous attempts to post this reply,
 as seen from my end]

On 22-Feb-07 Roger Bivand wrote:
> On 21 Feb 2007, Russell Senior wrote:
> 
>> 
>> I am interested in making a random sample from a uniform distribution
>> of points over the surface of the earth, using the WGS84 ellipsoid as
>> a model for the earth.  I know how to do this for a sphere, but would
>> like to do better.  I can supply random numbers, want latitude
>> longitude pairs out.
>> 
>> Can anyone point me at a solution?  Thanks very much.
>> 
> 
> http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.html
> 
> looks promising, untried.

Hmmm ... That page didn't seem to be directly useful, since
on my understanding of the code (and comments) listed under
"subroutine uniform_on_ellipsoid_map(dim_num, n, a, r, seed, x)"
"UNIFORM_ON_ELLIPSOID_MAP maps uniform points onto an ellipsoid."
in

http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.f90

it takes points uniformly distributed on a sphere and then
linearly transforms these onto an ellipsoid. This will not
give unform density over the surface of the ellipsoid: indeed
the example graph they show of points on an ellipse generated
in this way clearly appear to be more dense at the "ends" of
the ellipse, and less dense on its "sides". See:

http://www.csit.fsu.edu/~burkardt/f_src/random_data/
uniform_on_ellipsoid_map.png
[all one line]

Indeed, if I understand their method correctly, in the case
of a horizontal ellipse it is equivalent (modulo rotating
the result) to distributing the points uniformly over a circle,
and then stretching the circle sideways. This will preserve
the vertical distribution (so at the two ends of the major axis
it has the same density as on the circle) but diluting the
horizontal distribution (so that at the two ends of the minor
axis the density isless than on the circle).

I did have a notion about this, but sat on it expecting that
someone would come up with a slick solution -- which hasn't
happened yet.

For the application you have in hand, uniform distribution
over a sphere is a fairly close approximation to uniform
distriobution over the ellipspoid -- but not quite.

But a rejection method, applied to points uniform on the sphere,
can give you points uniform on the ellipsoid and, because of
the close approximation of the sphere to the ellipsoid, you
would not be rejecting many points.

The outline strategy I had in mind (I haven't worked out details)
is based on the following.

Consider a point X0 on the sphere, at radial distance r0 from
the centre of the sphere (same as the centre of the ellipsoid).
Let the radius through that point meet the ellipsoid at a point
X1, at radial distance R1.

Let dS0 be an element of area at X0 on the sphere, which projects
radially onto an element of area dS1 on the ellipsoid. You want
all elements dS1 of equal size to be equally likely to receive
a random point.

Let the angle between the tangent plane to the ellipsoid at X1,
and the tangent plane to the sphere at X0, be phi.

The the ratio of areas dS1/dS0 is R(X0), say, where

  R(X0) = dS1/dS0 = r1^2/(r0^2 * cos(phi))

and the smaller this ratio, the less likely you want a point
u.d. on the sphere to give rise to a point on the ellipsoid.

Now define an acceptance probability P(X0) by

  P(X0) = R(X0)/sup[R(X)]

taking the supremum over X on the sphere. Then sample points X0
unformly on the sphere, accepting each one with probability
P(X0), and continue sampling until you have the number of
points that you need.

Maybe someone has a better idea ... (or code for the above!)

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 24-Feb-07                                       Time: 13:45:50
------------------------------ XFMail ------------------------------


From bates at stat.wisc.edu  Sat Feb 24 14:46:09 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 24 Feb 2007 07:46:09 -0600
Subject: [R] problem with weights on lmer function
In-Reply-To: <200702231718.56551.chrysopa@gmail.com>
References: <200702221822.04001.chrysopa@gmail.com>
	<20070222223621.GV24437@ms.unimelb.edu.au>
	<200702231718.56551.chrysopa@gmail.com>
Message-ID: <40e66e0b0702240546v7fa25418v61fe14bca29cb93e@mail.gmail.com>

On 2/23/07, Ronaldo Reis Junior <chrysopa at gmail.com> wrote:
> Em Quinta 22 Fevereiro 2007 20:36, Andrew Robinson escreveu:
> > Hi Ronaldo,
> >
> > I suggest that you send us a small, well-documented, code example that
> > we can reproduce.  It certainly looks as though there is a problem,
> > but given this information it's hard to know what it is!
> >
> > Cheers
> >
> > Andrew
>
> Andrew and all R users.
>
> Look this example:
>
> test<-structure(list(subject = structure(c(1, 1, 1, 1, 1, 1, 1, 1,
> 2, 2, 2, 2, 2, 2, 2, 2), .Label = c("S1", "S2"), class = "factor"),
>     time = c(0, 7, 15, 22, 32, 39, 46, 53, 0, 7, 14, 24, 28,
>     34, 41, 48), noccup = c(0, 1, 2, 1, 6, 4, 3, 3, 0, 18,
>     21, 14, 7, 14, 12, 8), ntotal = c(100, 100, 100, 100, 100,
>     100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100)), .Names =
> c("subject",
> "time", "noccup", "ntotal"), class = "data.frame", row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> "14", "15", "16"))
>
> I have 2 subject over 8 times. Each subject is compound by 100 pieces. I
> measure the number of piece occuped in any time.
>
> I try this:
>
> m1<-lmer(noccup/ntotal~time+(time|subject),family=binomial,weights=ntotal)
> Error in lmer(noccup/ntotal ~ time + (time | subject), family = binomial,  :
>         object `weights' of incorrect type
>
> I dont understand why this error.
>
> m1<-lmer(noccup/ntotal~1+(time|subject),family=binomial,weights=ntotal)
> Error in lmer(noccup/ntotal ~ time + (time | subject), family = binomial,  :
>         object `weights' of incorrect type
>
> Why weights is of incorrect type? In glm this is correct type.

I don't get such an error using lme4 0.9975-10 under R-2.4.1 on
i386-apple-darwin8.8.1

Of course the model fit is singular because you are attempting to
estimate 3 variance-covariance components from 2 groups.


From p.dalgaard at biostat.ku.dk  Sat Feb 24 15:03:01 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 24 Feb 2007 15:03:01 +0100
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
In-Reply-To: <loom.20070224T123203-607@post.gmane.org> (Ulrich Keller's
	message of "Sat, 24 Feb 2007 11:47:52 +0000 (UTC)")
References: <loom.20070224T123203-607@post.gmane.org>
Message-ID: <x2k5y7tzze.fsf@viggo.kubism.ku.dk>

Ulrich Keller <ulrich.keller at emacs.lu> writes:

> I am trying to read a number of XML files using xmlTreeParse(). Unfortunately,
> some of them are malformed in a way that makes R crash. The problem is that
> closing tags are sometimes repeated like this:
>
> <tag>value1</tag><tag>value2</tag>some garbage</tag></tag><tag>value3</tag>
>
> I want to preprocess the contents of the XML file using gsub() before feeding
> them to xmlTreeParse() to clean them up, but I can't figure out how to do it.
> What I need is something that transforms the example above into:
>
> <tag>value1</tag><tag>value2</tag><tag>value3</tag>
>
> Some kind of "</tag>.*</tag>" that only matches if there is no "<tag>" in ".*".
>
> Thanks in advance for you ideas,

Hmm, there are things you just cannot do with RE's, and I suspect that
this is one of them. Something involving explicit splitting of the
strings might work, though. How's this for size?

> trim <-
    function(x)paste(sub("</tag>.*","</tag>",x),collapse="<tag>")
> sapply(strsplit(x,"<tag>"),trim)
[1] "<tag>value1</tag><tag>value2</tag><tag>value3</tag>"


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From marc_schwartz at comcast.net  Sat Feb 24 16:27:31 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 24 Feb 2007 09:27:31 -0600
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
In-Reply-To: <x2k5y7tzze.fsf@viggo.kubism.ku.dk>
References: <loom.20070224T123203-607@post.gmane.org>
	<x2k5y7tzze.fsf@viggo.kubism.ku.dk>
Message-ID: <1172330851.4911.17.camel@localhost.localdomain>

On Sat, 2007-02-24 at 15:03 +0100, Peter Dalgaard wrote:
> Ulrich Keller <ulrich.keller at emacs.lu> writes:
> 
> > I am trying to read a number of XML files using xmlTreeParse(). Unfortunately,
> > some of them are malformed in a way that makes R crash. The problem is that
> > closing tags are sometimes repeated like this:
> >
> > <tag>value1</tag><tag>value2</tag>some garbage</tag></tag><tag>value3</tag>
> >
> > I want to preprocess the contents of the XML file using gsub() before feeding
> > them to xmlTreeParse() to clean them up, but I can't figure out how to do it.
> > What I need is something that transforms the example above into:
> >
> > <tag>value1</tag><tag>value2</tag><tag>value3</tag>
> >
> > Some kind of "</tag>.*</tag>" that only matches if there is no "<tag>" in ".*".
> >
> > Thanks in advance for you ideas,
> 
> Hmm, there are things you just cannot do with RE's, and I suspect that
> this is one of them. Something involving explicit splitting of the
> strings might work, though. How's this for size?
> 
> > trim <-
>     function(x)paste(sub("</tag>.*","</tag>",x),collapse="<tag>")
> > sapply(strsplit(x,"<tag>"),trim)
> [1] "<tag>value1</tag><tag>value2</tag><tag>value3</tag>"

Does this work?

> XML
[1] "<tag>value1</tag><tag>value2</tag>some garbage</tag></tag><tag>value3</tag>"


> gsub("[^>]*(</tag>){2}", "", XML)
[1] "<tag>value1</tag><tag>value2</tag><tag>value3</tag>"


This looks for any characters != '>' that precedes a "</tag></tag>"
sequence. It replaces that with "".

?

Marc Schwartz


From marc_schwartz at comcast.net  Sat Feb 24 16:44:54 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 24 Feb 2007 09:44:54 -0600
Subject: [R] Making two lines graph ...
In-Reply-To: <644e1f320702240508m4a9aa3b3y22b9e060eea71736@mail.gmail.com>
References: <1172318415.15046.8.camel@localhost>
	<644e1f320702240508m4a9aa3b3y22b9e060eea71736@mail.gmail.com>
Message-ID: <1172331894.4911.28.camel@localhost.localdomain>

Or a possible alternative still using matplot:

matplot(DF[, 3:4], type = "b", xaxt = "n", pch = c(21, 22),
        col = "black", lty = c("dotted", "solid"), 
        ylab = "Y Vals", xlab = "Groups")

axis(1, at = 1:5, labels = as.character(DF$x.orig))

legend("topright", legend = c("Group 1", "Group 2"), 
       lty = c("dotted", "solid"), pch = c(21, 22))


See ?points for different point symbols (pch) if you prefer.

HTH,

Marc Schwartz

On Sat, 2007-02-24 at 08:08 -0500, jim holtman wrote:
> Does this do what you want?
> 
> 
> x <- "x.orig x.num y1 y2
> a 1 0.2 0.4
> b 2 0.1 0.1
> c 3 0.3 0.3
> d 4 0.3 0.15
> e 5 0.1 0.05"
> x.in <- read.table(textConnection(x), header=TRUE)
> plot(x.in$y2, type='l', ylim=range(x.in$y1, x.in$y2), xaxt='n', col='red',
>     xlab='', ylab='y')
> lines(x.in$y1, lty=2, col='green')
> axis(1, labels=as.character(x.in$x.orig), at=seq(nrow(x.in)))
> 
> 
> 
> 
> 
> On 2/24/07, Petar Milin <pmilin at gmail.com> wrote:
> >
> > Hello!
> > Can anyone help me to build a graph with the alphanumeric values on
> > x-axis, with two lines (preferably doted and solid, or similar) that
> > present values on y-axes. In a toy example, data frame could be like
> > this:
> > x.orig x.num y1 y2
> > a 1 0.2 0.4
> > b 2 0.1 0.1
> > c 3 0.3 0.3
> > d 4 0.3 0.15
> > e 5 0.1 0.05
> >
> > I can make graph only if I use values converted to numeric in "x.num",
> > but not original "X.orig":
> > matplot(dat$x.num, dat[, c("y1","y2")], type="b", lty=1, ylab="(1) y1,
> > (2) y2")
> >
> > Also, how to make doted and solid instead of coloured lines?
> >
> > Thanks in advance. Sincerely,
> > Petar M
> >


From ggrothendieck at gmail.com  Sat Feb 24 17:07:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 24 Feb 2007 11:07:37 -0500
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
In-Reply-To: <loom.20070224T123203-607@post.gmane.org>
References: <loom.20070224T123203-607@post.gmane.org>
Message-ID: <971536df0702240807s7c8c4bedm1eb9c52e700d1529@mail.gmail.com>

I assume <tag> is known.

This removes any occurrence </tag>.*</tag> where .* does not
contain <tag> or </tag>.

The regular expression, re, matches </tag>, then does a greedy
match (?U) for anything followed by </tag> but uses a zero
width lookahead subexpression (?=...) for the second </tag>
so that it it can be rematched again.  gsubfn in package
gsubfn is like the usual gsub except that instead of
replacing the match with a string it passes the match
to function f and then replaces the match with the output
of f.  See the gsubfn home page:
  http://code.google.com/p/gsubfn/
and vignette.


library(gsubfn)

text <- paste("<tag>value1</tag><tag>value2</tag>some",
"garbage</tag></tag><tag>value3</tag>")

re <- "</tag>((?U).*(?=</tag>))"
f <- function(x) if (regexpr("<tag>", x) > 0) x else ""

gsubfn(re, f, text, backref = 0, perl = TRUE)


On 2/24/07, Ulrich Keller <ulrich.keller at emacs.lu> wrote:
> I am trying to read a number of XML files using xmlTreeParse(). Unfortunately,
> some of them are malformed in a way that makes R crash. The problem is that
> closing tags are sometimes repeated like this:
>
> <tag>value1</tag><tag>value2</tag>some garbage</tag></tag><tag>value3</tag>
>
> I want to preprocess the contents of the XML file using gsub() before feeding
> them to xmlTreeParse() to clean them up, but I can't figure out how to do it.
> What I need is something that transforms the example above into:
>
> <tag>value1</tag><tag>value2</tag><tag>value3</tag>
>
> Some kind of "</tag>.*</tag>" that only matches if there is no "<tag>" in ".*".
>
> Thanks in advance for you ideas,
>
> Uli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From skiadas at hanover.edu  Sat Feb 24 17:24:34 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sat, 24 Feb 2007 11:24:34 -0500
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
In-Reply-To: <971536df0702240807s7c8c4bedm1eb9c52e700d1529@mail.gmail.com>
References: <loom.20070224T123203-607@post.gmane.org>
	<971536df0702240807s7c8c4bedm1eb9c52e700d1529@mail.gmail.com>
Message-ID: <3DC4C57C-17D8-45C1-AB46-74EC4FD8D8A9@hanover.edu>

All these methods do assume that you don't have nested <tag>'s, like so:

<tag><tag>foo</tag>useful stuff</tag>some garbage</tag>

For that you would really need a true parser. So I would double-check  
to make sure this doesn't happen.

Do you have any control on where those XML files are generated  
though? It sounds to me it might be easier to fix the utility  
generating those XML files, since it clearly is doing something wrong.

On Feb 24, 2007, at 11:07 AM, Gabor Grothendieck wrote:

> I assume <tag> is known.
>
> This removes any occurrence </tag>.*</tag> where .* does not
> contain <tag> or </tag>.
>
> The regular expression, re, matches </tag>, then does a greedy
> match (?U) for anything followed by </tag> but uses a zero
> width lookahead subexpression (?=...) for the second </tag>
> so that it it can be rematched again.  gsubfn in package
> gsubfn is like the usual gsub except that instead of
> replacing the match with a string it passes the match
> to function f and then replaces the match with the output
> of f.  See the gsubfn home page:
>   http://code.google.com/p/gsubfn/
> and vignette.

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From ggrothendieck at gmail.com  Sat Feb 24 17:37:32 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 24 Feb 2007 11:37:32 -0500
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
In-Reply-To: <3DC4C57C-17D8-45C1-AB46-74EC4FD8D8A9@hanover.edu>
References: <loom.20070224T123203-607@post.gmane.org>
	<971536df0702240807s7c8c4bedm1eb9c52e700d1529@mail.gmail.com>
	<3DC4C57C-17D8-45C1-AB46-74EC4FD8D8A9@hanover.edu>
Message-ID: <971536df0702240837j7bcec8du9c7bec7dc9d3b221@mail.gmail.com>

The _question_ assumed that, which is why the answers did too.

On 2/24/07, Charilaos Skiadas <skiadas at hanover.edu> wrote:
> All these methods do assume that you don't have nested <tag>'s, like so:
>
> <tag><tag>foo</tag>useful stuff</tag>some garbage</tag>
>
> For that you would really need a true parser. So I would double-check
> to make sure this doesn't happen.
>
> Do you have any control on where those XML files are generated
> though? It sounds to me it might be easier to fix the utility
> generating those XML files, since it clearly is doing something wrong.
>
> On Feb 24, 2007, at 11:07 AM, Gabor Grothendieck wrote:
>
> > I assume <tag> is known.
> >
> > This removes any occurrence </tag>.*</tag> where .* does not
> > contain <tag> or </tag>.
> >
> > The regular expression, re, matches </tag>, then does a greedy
> > match (?U) for anything followed by </tag> but uses a zero
> > width lookahead subexpression (?=...) for the second </tag>
> > so that it it can be rematched again.  gsubfn in package
> > gsubfn is like the usual gsub except that instead of
> > replacing the match with a string it passes the match
> > to function f and then replaces the match with the output
> > of f.  See the gsubfn home page:
> >   http://code.google.com/p/gsubfn/
> > and vignette.
>
> Haris Skiadas
> Department of Mathematics and Computer Science
> Hanover College
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jlampur at eagrof.udl.cat  Sat Feb 24 17:38:19 2007
From: jlampur at eagrof.udl.cat (Jorge Lampurlanes Castel)
Date: Sat, 24 Feb 2007 17:38:19 +0100 (CET)
Subject: [R] Multiple comparisons when interacction]
Message-ID: <3871.217.124.30.144.1172335099.squirrel@correu.udl.es>

Hello,

I send the message again with the data file as txt because it seems not to
be accepted as csv in the R-help list.

Data comes from a multiyear field experiment in which 4 levels of a
treatment (2, 3, 4, 6) are compared to see the effect on yield. It is a
randomized complete block design.

The SAS code follows:

options ls=95;
data uno;
        infile 'data.txt' delimiter=';' firstobs=2;
        input year plot block treat yield;
run;

proc mixed data=uno;
     class treat year block;
     model yield=block year treat treat*year;
     lsmeans year treat  /pdiff;
     lsmeans treat*year /slice=year pdiff;
     ods output diffs=dos;
run;

data tres;
      set dos;
      if year=_year;
proc print data=tres;
      var year _year treat _treat estimate stderr df tvalue probt;
run;

Data are attached as a file: data.csv.

In fact, I do not know if this is the best approach to analyze the data:

- Should block be considered as random? We use the same file and
randomization every year. We are interested in the long term effect of the
treatment.
- Data should be considered as repeated measurements over time (years)?

In multcomp package:

- What is the equivalence between the tests proposed  ("Sequen", "AVE",
"Changepoint", "Williams", "Marcus", "McDermott") and the tests agronomist
are used to do: LSD (least significant difference), Duncan multiple range
test, Scheffe, S-N-K (Student-Newman-Keuls)?


Thanks a lot for your interest.

Jorge Lampurlan?s
Agronomist


>> Is it possible to do this analysis in R?
>
> Yes, it is possible.  The syntax isn't in place yet.
>
> If you send me the complete SAS code and data for an example using slice,
> I will duplicate it for you in the multcomp package in R.  I will send
> that
> to the R-help list and to Torsten and it will bring us one step closer
> to the syntax.
>
> The example I showed before was designed to get the same answer as S-Plus
> multicomp using the adjust= argument.
>
> Rich
>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: DATA.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/069d953b/attachment.txt 

From skiadas at hanover.edu  Sat Feb 24 18:22:28 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sat, 24 Feb 2007 12:22:28 -0500
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
In-Reply-To: <971536df0702240837j7bcec8du9c7bec7dc9d3b221@mail.gmail.com>
References: <loom.20070224T123203-607@post.gmane.org>
	<971536df0702240807s7c8c4bedm1eb9c52e700d1529@mail.gmail.com>
	<3DC4C57C-17D8-45C1-AB46-74EC4FD8D8A9@hanover.edu>
	<971536df0702240837j7bcec8du9c7bec7dc9d3b221@mail.gmail.com>
Message-ID: <C9EA3942-DE66-4E56-9DC5-9E6BFCDB4BBB@hanover.edu>

On Feb 24, 2007, at 11:37 AM, Gabor Grothendieck wrote:

> The _question_ assumed that, which is why the answers did too.

Oh yes, I totally agree, the file snippet the OP provided did indeed  
assume that, though nothing in the text of his question did, so I  
wasn't entirely clear whether the actual file that is going to be  
processed has this form or not. So I just wanted to make sure the OP  
is aware of this limitation, in case the actual file is more  
problematic.

But most importantly, I wanted to suggest a reevaluation, if  
possible, of the process that generates these XML's, and perhaps  
fixing that, instead of patching the problem after it has been created.

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From jeff.horner at vanderbilt.edu  Sat Feb 24 18:39:36 2007
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Sat, 24 Feb 2007 11:39:36 -0600
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
In-Reply-To: <C9EA3942-DE66-4E56-9DC5-9E6BFCDB4BBB@hanover.edu>
References: <loom.20070224T123203-607@post.gmane.org>	<971536df0702240807s7c8c4bedm1eb9c52e700d1529@mail.gmail.com>	<3DC4C57C-17D8-45C1-AB46-74EC4FD8D8A9@hanover.edu>	<971536df0702240837j7bcec8du9c7bec7dc9d3b221@mail.gmail.com>
	<C9EA3942-DE66-4E56-9DC5-9E6BFCDB4BBB@hanover.edu>
Message-ID: <45E07858.5030409@vanderbilt.edu>

Charilaos Skiadas wrote:
> On Feb 24, 2007, at 11:37 AM, Gabor Grothendieck wrote:
> 
>> The _question_ assumed that, which is why the answers did too.
> 
> Oh yes, I totally agree, the file snippet the OP provided did indeed  
> assume that, though nothing in the text of his question did, so I  
> wasn't entirely clear whether the actual file that is going to be  
> processed has this form or not. So I just wanted to make sure the OP  
> is aware of this limitation, in case the actual file is more  
> problematic.
> 
> But most importantly, I wanted to suggest a reevaluation, if  
> possible, of the process that generates these XML's, and perhaps  
> fixing that, instead of patching the problem after it has been created.

Also, I wouldn't tolerate R *crashing* in package code on malformed xml 
input.

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From deepayan.sarkar at gmail.com  Sat Feb 24 18:40:36 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 24 Feb 2007 11:40:36 -0600
Subject: [R] barchart (lattice) with text labels
In-Reply-To: <45DFF5C3.60806@xmission.com>
References: <45DFF5C3.60806@xmission.com>
Message-ID: <eb555e660702240940v76bb600dscc70a51fa266279d@mail.gmail.com>

On 2/24/07, Mark and Heather Lyman <lymanmh at xmission.com> wrote:
> I would like to place the value for each bar in barchart (lattice) at
> the top of each bar. Something like the following code produces.
>
> library(lattice)
>
> mypanelfunc <- function(x, y, ...)
> {
>   panel.barchart(x, y, ...)
>   panel.text(x, y, labels=as.character(round(x,2)), ...)
>   }
>
> myprepanelfunc <- function(x, y, ...) list(xlim=c(0, max(x)+.1))
>
> mydata <- expand.grid(a=factor(1:5), b=factor(1:3), c=factor(1:2))
> mydata$x <- runif(nrow(mydata))
>
> barchart(a~x|b, mydata, groups=c, panel=mypanelfunc,
> prepanel=myprepanelfunc, adj=c(-0.1,0.5))
>
> However, I cannot figure out how to shift the values to correspond with
> their respective grouped bar.

You should look at panel.barchart and try to reproduce the
calculations done there.

Deepayan


From duncan at wald.ucdavis.edu  Sat Feb 24 19:01:19 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Sat, 24 Feb 2007 10:01:19 -0800
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
In-Reply-To: <loom.20070224T123203-607@post.gmane.org>
References: <loom.20070224T123203-607@post.gmane.org>
Message-ID: <20070224180119.GA5577@wald.ucdavis.edu>

Ulrich Keller wrote:
> I am trying to read a number of XML files using xmlTreeParse(). Unfortunately,
> some of them are malformed in a way that makes R crash. The problem is that
> closing tags are sometimes repeated like this:
> 
> <tag>value1</tag><tag>value2</tag>some garbage</tag></tag><tag>value3</tag>
> 
> I want to preprocess the contents of the XML file using gsub() before feeding
> them to xmlTreeParse() to clean them up, but I can't figure out how to do it.
> What I need is something that transforms the example above into:
> 
> <tag>value1</tag><tag>value2</tag><tag>value3</tag>
> 
> Some kind of "</tag>.*</tag>" that only matches if there is no "<tag>" in ".*".
> 
> Thanks in advance for you ideas,


Instead of using xmlTreeParse() which really expects well-formed XML,
and assuming you cannot have the XML generation mechanism fixed, you might
try to use htmlTreeParse().
While the name suggests it is for HTML, it is really a "relaxed"
XML parser that is capable of handling malformed XML.  This typically
occurs in HTML and hence the name.
Of course, since the XML is malformed, the results will be hard to predict
as it is hard to make sense of "non-sense".


If xmlTreeParse() is actually causing R to exit (i.e. what some people
refer to as crashing), as Jeff (Horner) said, we would like to be able
to stop this. We will need the actual text/file passed to
xmlTreeParse(), version information of operating system, R and the XML
package and any locale information.  However, if by crashing you mean
generates an error, then that is expected on malformed XML inputs.

HTH,
 D.

> 
> Uli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
4210 Mathematical Sciences Bldg.  fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA



-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070224/8edba5e8/attachment.bin 

From francogrex at mail.com  Sat Feb 24 19:30:46 2007
From: francogrex at mail.com (francogrex)
Date: Sat, 24 Feb 2007 10:30:46 -0800 (PST)
Subject: [R] Woolf's test, Odds ratio, stratification
Message-ID: <9136458.post@talk.nabble.com>


Just a general question concerning the woolf test (package vcd), when we have
stratified data (2x2 tables) and  when the p.value of the woolf-test is
below 0.05 then we assume that there is a heterogeneity and a common odds
ratio cannot be computed?
Does this mean that we have to try to add more stratification variables
(stratify more) to make the woolf-test p.value insignificant? 
Also in the same package there is an oddsratio function that seems to be
calculating the odds ratio even when the values are non-integers and even
when one cell has a null value (in contrast to the mantel_hanszel or the
fisher exact test that do not admit zero values or non-integers) does anyone
know what's the difference and how to optain a p.value on that odds ratio?
Thanks
-- 
View this message in context: http://www.nabble.com/Woolf%27s-test%2C-Odds-ratio%2C-stratification-tf3284589.html#a9136458
Sent from the R help mailing list archive at Nabble.com.


From ulrich.keller at emacs.lu  Sat Feb 24 19:48:15 2007
From: ulrich.keller at emacs.lu (Ulrich Keller)
Date: Sat, 24 Feb 2007 18:48:15 +0000 (UTC)
Subject: [R] gsub: replacing a.*a if no occurence of b in .*
References: <loom.20070224T123203-607@post.gmane.org>
	<20070224180119.GA5577@wald.ucdavis.edu>
Message-ID: <loom.20070224T191607-781@post.gmane.org>

Duncan Temple Lang <duncan <at> wald.ucdavis.edu> writes:
> If xmlTreeParse() is actually causing R to exit (i.e. what some people
> refer to as crashing), as Jeff (Horner) said, we would like to be able
> to stop this. We will need the actual text/file passed to
> xmlTreeParse(), version information of operating system, R and the XML
> package and any locale information.  However, if by crashing you mean
> generates an error, then that is expected on malformed XML inputs.


Thanks Duncan and all the other helpful people. I will try the suggestions
tomorrow. Of course it would be best to fix the generation mechanism, I'm going
to notify the responsible developer as soon as I can, but unfortunately I
already have a few thousand files that have to be parsed.

R (2.4.1) really exits, on Linux (Ubuntu Edgy) I get an endless series of
segfault errors like this one:

 *** caught segfault ***
address 0x75716e6d, cause 'memory not mapped'

until I kill the process. On Windows (XP SP2), R exits too. Note that this only
happens when I read the file to a character vector (of length one) and then pass
this to xmlTreeParse(). When I let xmlTreeParse() read the file directly, it
prints an error message and everything is fine. I just remember that I had a
similar problem with XML files that contained ISO8859-encoded text. Reading the
files directly caused an error message, passing them as a character vector
caused a crash.

I will send you one of the offending files by mail, plus one that is well-formed.

Thanks again,

Uli


From lymanmh at xmission.com  Sat Feb 24 20:20:37 2007
From: lymanmh at xmission.com (Mark and Heather Lyman)
Date: Sat, 24 Feb 2007 12:20:37 -0700
Subject: [R] barchart (lattice) with text labels
In-Reply-To: <eb555e660702240940v76bb600dscc70a51fa266279d@mail.gmail.com>
References: <45DFF5C3.60806@xmission.com>
	<eb555e660702240940v76bb600dscc70a51fa266279d@mail.gmail.com>
Message-ID: <45E09005.40701@xmission.com>

Deepayan Sarkar wrote:
> On 2/24/07, Mark and Heather Lyman <lymanmh at xmission.com> wrote:
>> I would like to place the value for each bar in barchart (lattice) at
>> the top of each bar. Something like the following code produces.
>>
>> library(lattice)
>>
>> mypanelfunc <- function(x, y, ...)
>> {
>>   panel.barchart(x, y, ...)
>>   panel.text(x, y, labels=as.character(round(x,2)), ...)
>>   }
>>
>> myprepanelfunc <- function(x, y, ...) list(xlim=c(0, max(x)+.1))
>>
>> mydata <- expand.grid(a=factor(1:5), b=factor(1:3), c=factor(1:2))
>> mydata$x <- runif(nrow(mydata))
>>
>> barchart(a~x|b, mydata, groups=c, panel=mypanelfunc,
>> prepanel=myprepanelfunc, adj=c(-0.1,0.5))
>>
>> However, I cannot figure out how to shift the values to correspond with
>> their respective grouped bar.
>
> You should look at panel.barchart and try to reproduce the
> calculations done there.
>
> Deepayan
>
That's what I was afraid of. I made a half-hearted attempt at that, but 
when I didn't work, I hoped there was an easier way. Well, I will give 
it another try.

Mark Lyman


From A.Robinson at ms.unimelb.edu.au  Sat Feb 24 21:30:36 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 25 Feb 2007 07:30:36 +1100
Subject: [R] problem with weights on lmer function
In-Reply-To: <40e66e0b0702240546v7fa25418v61fe14bca29cb93e@mail.gmail.com>
References: <200702221822.04001.chrysopa@gmail.com>
	<20070222223621.GV24437@ms.unimelb.edu.au>
	<200702231718.56551.chrysopa@gmail.com>
	<40e66e0b0702240546v7fa25418v61fe14bca29cb93e@mail.gmail.com>
Message-ID: <20070224203036.GH55494@ms.unimelb.edu.au>

Hi Ronaldo,

Thanks, that's helpful!  I also don't get an error.  Mind you, I added

data=test

to the model call.  This is my system:

> sessionInfo()
R version 2.4.1 Patched (2006-12-30 r40330) 
i386-unknown-freebsd6.1 

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
"methods"  
[7] "base"     

other attached packages:
       lme4      Matrix     lattice 
"0.9975-13" "0.9975-11"   "0.14-16" 


Cheers,

Andrew

On Sat, Feb 24, 2007 at 07:46:09AM -0600, Douglas Bates wrote:
> On 2/23/07, Ronaldo Reis Junior <chrysopa at gmail.com> wrote:
> > Em Quinta 22 Fevereiro 2007 20:36, Andrew Robinson escreveu:
> > > Hi Ronaldo,
> > >
> > > I suggest that you send us a small, well-documented, code example that
> > > we can reproduce.  It certainly looks as though there is a problem,
> > > but given this information it's hard to know what it is!
> > >
> > > Cheers
> > >
> > > Andrew
> >
> > Andrew and all R users.
> >
> > Look this example:
> >
> > test<-structure(list(subject = structure(c(1, 1, 1, 1, 1, 1, 1, 1,
> > 2, 2, 2, 2, 2, 2, 2, 2), .Label = c("S1", "S2"), class = "factor"),
> >     time = c(0, 7, 15, 22, 32, 39, 46, 53, 0, 7, 14, 24, 28,
> >     34, 41, 48), noccup = c(0, 1, 2, 1, 6, 4, 3, 3, 0, 18,
> >     21, 14, 7, 14, 12, 8), ntotal = c(100, 100, 100, 100, 100,
> >     100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100)), .Names =
> > c("subject",
> > "time", "noccup", "ntotal"), class = "data.frame", row.names = c("1",
> > "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> > "14", "15", "16"))
> >
> > I have 2 subject over 8 times. Each subject is compound by 100 pieces. I
> > measure the number of piece occuped in any time.
> >
> > I try this:
> >
> > m1<-lmer(noccup/ntotal~time+(time|subject),family=binomial,weights=ntotal)
> > Error in lmer(noccup/ntotal ~ time + (time | subject), family = binomial,  :
> >         object `weights' of incorrect type
> >
> > I dont understand why this error.
> >
> > m1<-lmer(noccup/ntotal~1+(time|subject),family=binomial,weights=ntotal)
> > Error in lmer(noccup/ntotal ~ time + (time | subject), family = binomial,  :
> >         object `weights' of incorrect type
> >
> > Why weights is of incorrect type? In glm this is correct type.
> 
> I don't get such an error using lme4 0.9975-10 under R-2.4.1 on
> i386-apple-darwin8.8.1
> 
> Of course the model fit is singular because you are attempting to
> estimate 3 variance-covariance components from 2 groups.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From s-dhar at northwestern.edu  Sat Feb 24 21:52:11 2007
From: s-dhar at northwestern.edu (Sumitrajit Dhar)
Date: Sat, 24 Feb 2007 14:52:11 -0600
Subject: [R] independent text in panels
Message-ID: <26A17F9A-EDA1-40DD-90D8-2C9D912EC533@northwestern.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/440edc14/attachment.pl 

From maitra at iastate.edu  Sat Feb 24 21:54:33 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Sat, 24 Feb 2007 14:54:33 -0600
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <XFMail.070224134556.ted.harding@nessie.mcc.ac.uk>
References: <Pine.LNX.4.44.0702221311250.14521-100000@reclus.nhh.no>
	<XFMail.070224134556.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070224145433.07c2b1e1@triveni.stat.iastate.edu>

Hi,

Sorry for being a late entrant to this thread, but let me see if I understand the problem.

The poster wants to sample from an ellipsoid. Let us call this ellipsoid X'\Gamma X - d^2= 0.

There is no loss in assuming that the center is zero, otherwise the same can be done.

Let us consider the case when Gamma = I first. 

Then, let X \sim N_p(0, I) (any radially symmetric distribution will
do here), then d*X/||X|| is uniform on the sphere of radius d. 

How about imitating the same?

Let X \sim N_p(0, \Sigma), where \Sigma = \Gamma^{-1} then X restricted to X'\Gamma X = d^2 gives the required uniform density on the ellipsoid.

How do we get this easily? I don't think rejection works or is even necessary.

Isn't d*X / ||\Gamma^{1/2}X|| enough? Here \Gamma^{1/2} is the square root matrix of \Gamma.

Note that any distribution of the kind f(X'\Gamma X) would work, but the multivariate Gaussian is a convenient tool, for which two-lines of R code should be enough.


Many thanks and best wishes,
Ranjan




On Sat, 24 Feb 2007 13:45:56 -0000 (GMT) (Ted Harding) <ted.harding at nessie.mcc.ac.uk> wrote:

> [Apologies if this is a repeated posting for you. Something seems
>  to have gone amiss with my previous attempts to post this reply,
>  as seen from my end]
> 
> On 22-Feb-07 Roger Bivand wrote:
> > On 21 Feb 2007, Russell Senior wrote:
> > 
> >> 
> >> I am interested in making a random sample from a uniform distribution
> >> of points over the surface of the earth, using the WGS84 ellipsoid as
> >> a model for the earth.  I know how to do this for a sphere, but would
> >> like to do better.  I can supply random numbers, want latitude
> >> longitude pairs out.
> >> 
> >> Can anyone point me at a solution?  Thanks very much.
> >> 
> > 
> > http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.html
> > 
> > looks promising, untried.
> 
> Hmmm ... That page didn't seem to be directly useful, since
> on my understanding of the code (and comments) listed under
> "subroutine uniform_on_ellipsoid_map(dim_num, n, a, r, seed, x)"
> "UNIFORM_ON_ELLIPSOID_MAP maps uniform points onto an ellipsoid."
> in
> 
> http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.f90
> 
> it takes points uniformly distributed on a sphere and then
> linearly transforms these onto an ellipsoid. This will not
> give unform density over the surface of the ellipsoid: indeed
> the example graph they show of points on an ellipse generated
> in this way clearly appear to be more dense at the "ends" of
> the ellipse, and less dense on its "sides". See:
> 
> http://www.csit.fsu.edu/~burkardt/f_src/random_data/
> uniform_on_ellipsoid_map.png
> [all one line]
> 
> Indeed, if I understand their method correctly, in the case
> of a horizontal ellipse it is equivalent (modulo rotating
> the result) to distributing the points uniformly over a circle,
> and then stretching the circle sideways. This will preserve
> the vertical distribution (so at the two ends of the major axis
> it has the same density as on the circle) but diluting the
> horizontal distribution (so that at the two ends of the minor
> axis the density isless than on the circle).
> 
> I did have a notion about this, but sat on it expecting that
> someone would come up with a slick solution -- which hasn't
> happened yet.
> 
> For the application you have in hand, uniform distribution
> over a sphere is a fairly close approximation to uniform
> distriobution over the ellipspoid -- but not quite.
> 
> But a rejection method, applied to points uniform on the sphere,
> can give you points uniform on the ellipsoid and, because of
> the close approximation of the sphere to the ellipsoid, you
> would not be rejecting many points.
> 
> The outline strategy I had in mind (I haven't worked out details)
> is based on the following.
> 
> Consider a point X0 on the sphere, at radial distance r0 from
> the centre of the sphere (same as the centre of the ellipsoid).
> Let the radius through that point meet the ellipsoid at a point
> X1, at radial distance R1.
> 
> Let dS0 be an element of area at X0 on the sphere, which projects
> radially onto an element of area dS1 on the ellipsoid. You want
> all elements dS1 of equal size to be equally likely to receive
> a random point.
> 
> Let the angle between the tangent plane to the ellipsoid at X1,
> and the tangent plane to the sphere at X0, be phi.
> 
> The the ratio of areas dS1/dS0 is R(X0), say, where
> 
>   R(X0) = dS1/dS0 = r1^2/(r0^2 * cos(phi))
> 
> and the smaller this ratio, the less likely you want a point
> u.d. on the sphere to give rise to a point on the ellipsoid.
> 
> Now define an acceptance probability P(X0) by
> 
>   P(X0) = R(X0)/sup[R(X)]
> 
> taking the supremum over X on the sphere. Then sample points X0
> unformly on the sphere, accepting each one with probability
> P(X0), and continue sampling until you have the number of
> points that you need.
> 
> Maybe someone has a better idea ... (or code for the above!)
> 
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 24-Feb-07                                       Time: 13:45:50
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From klaster at karlin.mff.cuni.cz  Sat Feb 24 22:08:30 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Sat, 24 Feb 2007 22:08:30 +0100
Subject: [R] recovering collums of DF using a text var.list
In-Reply-To: <537792.40150.qm@web56602.mail.re3.yahoo.com>
References: <537792.40150.qm@web56602.mail.re3.yahoo.com>
Message-ID: <45E0A94E.2010306@karlin.mff.cuni.cz>

Milton Cezar Ribeiro napsal(a):
> Hello people,
> 
> I would like to know how can I use a list of variables (a char list) to have access to the collums from a dataframe to be used in some analysis like, just as example, a ploting task on a "for()" loop. Of course the code below is just to understand the way. In this example I have a dataframe with several collumns (more then fifty in my case) and I would like do use only some of them. I really need use a "var.list"! 
> 
> a<-seq(1,100,1)
> b<-c(rep(c(1,2,3,4,5),20))
> c<-rnorm(100,0,1)
> d<-runif(100,0,1)
> e<-c^2
> f<-c/d
> g<-c-d
> df<-data.frame(cbind(a,b,c,d,e,f,g))
> var.list<-c("c","f","g")
> for (myvar in var.list) 
>  {
>  plot(density(df$myvar))   # here I need recover df$c , df$f and df$g
>  }
> 
> Kind regards
> Miltinho 
> Brazil
> 
> __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

Is this what you expect?
for (myvar in var.list) {
plot(density(df[[paste(myvar)]]), main=paste('Density of',myvar))
}

It may not be bad idea to make a structure (list, dataframe) consisting 
of only those variables you want and use some variant of apply() instead 
of the for-loop. See ?apply, ?tapply, ?sapply

Petr
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From marc_schwartz at comcast.net  Sat Feb 24 23:44:35 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 24 Feb 2007 16:44:35 -0600
Subject: [R] Woolf's test, Odds ratio, stratification
In-Reply-To: <9136458.post@talk.nabble.com>
References: <9136458.post@talk.nabble.com>
Message-ID: <1172357075.4865.69.camel@localhost.localdomain>

On Sat, 2007-02-24 at 10:30 -0800, francogrex wrote:
> Just a general question concerning the woolf test (package vcd), when we have
> stratified data (2x2 tables) and  when the p.value of the woolf-test is
> below 0.05 then we assume that there is a heterogeneity and a common odds
> ratio cannot be computed?
> Does this mean that we have to try to add more stratification variables
> (stratify more) to make the woolf-test p.value insignificant? 
> Also in the same package there is an oddsratio function that seems to be
> calculating the odds ratio even when the values are non-integers and even
> when one cell has a null value (in contrast to the mantel_hanszel or the
> fisher exact test that do not admit zero values or non-integers) does anyone
> know what's the difference and how to optain a p.value on that odds ratio?
> Thanks

I'll defer comments on your 'vcd' function specific questions to the
package authors.

However, from a general perspective the Woolf Test, or for that matter,
the Breslow-Day Test (+/- Tarone Correction), are tests that are akin to
running tests for the homogeneity of variances prior to performing an
ANOVA. You are testing to see if the underlying assumptions of the
latter test have been violated. 

If the tests are "significant", it is not an _absolute_ contraindication
to continuing, but the result should give you pause to consider what
else is going on in your data. 

These tests are commonly applied, for example, in multi-center clinical
trials to test for the 'poolability' of the data from the sites into a
common data set for analysis.

In the case of the ANOVA, having markedly different variances suggests
that the differences in the means may not be the most 'interesting' of
findings in your data.

Similarly, with stratified 2x2 tables, you can perform a CMH test using
mantelhaen.test() and you will get a common odds ratio and CI's.
However, the common odds ratio may be at best, less interesting than
other underlying characteristics and at worst perhaps, misleading.

For an example, let's use the UCBAdmissions dataset and some of the
examples in ?mantelhaen.test. For specific information on the dataset,
see ?UCBAdmissions.  In fact, reading the Details on that help page and
running the examples, especially the series of 6 mosaicplot()s, should
provide additional insight into the findings I present below.

So, let's start:

First, let's charge ahead and run the CMH test on the data:

> mantelhaen.test(UCBAdmissions)

	Mantel-Haenszel chi-squared test with continuity correction

data:  UCBAdmissions 
Mantel-Haenszel X-squared = 1.4269, df = 1, p-value = 0.2323
alternative hypothesis: true common odds ratio is not equal to 1 
95 percent confidence interval:
 0.7719074 1.0603298 
sample estimates:
common odds ratio 
        0.9046968 


So, we can see that the resultant test statistic is not significant, yet
we do get a common OR and CI's.


Now, let's do the Woolf test, using the code provided
in ?mantelhaen.test:

> woolf(UCBAdmissions)
[1] 0.003427200


This suggests that the assumption of common odds ratios in the CMH test
is violated and indeed if we run:

> apply(UCBAdmissions, 3, function(x) (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))
        A         B         C         D         E         F 
0.3492120 0.8025007 1.1330596 0.9212838 1.2216312 0.8278727 


We can see that the OR's range from 0.3492120 to 1.2216312.


So, let's take a look at the 'raw' data and do some further testing.  A
while ago, I wrote (and believe posted) code to take a data.frame.table
object and convert it back to it's raw structure as a non-summarized
data frame. Let's do that with UCBAdmissions, since UCBAdmissions is a
summarized 3D table:

> str(UCBAdmissions)
 table [1:2, 1:2, 1:6] 512 313 89 19 353 207 17 8 120 205 ...
 - attr(*, "dimnames")=List of 3
  ..$ Admit : chr [1:2] "Admitted" "Rejected"
  ..$ Gender: chr [1:2] "Male" "Female"
  ..$ Dept  : chr [1:6] "A" "B" "C" "D" ...


Here is the code:

expand.dft <- function(x, na.strings = "NA", as.is = FALSE, dec = ".")
{
  DF <- sapply(1:nrow(x), function(i) x[rep(i, each = x$Freq[i]), ],
               simplify = FALSE)

  DF <- subset(do.call("rbind", DF), select = -Freq)

  for (i in 1:ncol(DF))
  {
    DF[[i]] <- type.convert(as.character(DF[[i]]),
                            na.strings = na.strings,
                            as.is = as.is, dec = dec)
                                           
  }
    
  DF
}             



So:

DF <- expand.dft(as.data.frame.table(UCBAdmissions))

> str(DF)
'data.frame':	4526 obs. of  3 variables:
 $ Admit : Factor w/ 2 levels "Admitted","Rejected": 1 1 1 1 1 1 1 1 1 1 ...
 $ Gender: Factor w/ 2 levels "Female","Male": 2 2 2 2 2 2 2 2 2 2 ...
 $ Dept  : Factor w/ 6 levels "A","B","C","D",..: 1 1 1 1 1 1 1 1 1 1 ...


Now, let's create logistic regression models on the raw data, first
using just the two covariates of Gender and Dept:

fit <- glm(Admit ~ Gender + Dept, family = binomial, data = DF)

> summary(fit)

Call:
glm(formula = Admit ~ Gender + Dept, family = binomial, data = DF)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.3613  -0.9588   0.3741   0.9306   1.4773  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.68192    0.09911  -6.880 5.97e-12 ***
GenderMale   0.09987    0.08085   1.235    0.217    
DeptB        0.04340    0.10984   0.395    0.693    
DeptC        1.26260    0.10663  11.841  < 2e-16 ***
DeptD        1.29461    0.10582  12.234  < 2e-16 ***
DeptE        1.73931    0.12611  13.792  < 2e-16 ***
DeptF        3.30648    0.16998  19.452  < 2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6044.3  on 4525  degrees of freedom
Residual deviance: 5187.5  on 4519  degrees of freedom
AIC: 5201.5

Number of Fisher Scoring iterations: 5




Now, for comparison, let's create the same model, but this time
including interaction terms for Gender*Dept:

fitIT <- glm(Admit ~ Gender * Dept, family = binomial, data = DF)

> summary(fitIT)

Call:
glm(formula = Admit ~ Gender * Dept, family = binomial, data = DF)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.3793  -0.9768   0.3820   0.9127   1.8642  

Coefficients:
                 Estimate Std. Error z value Pr(>|z|)    
(Intercept)       -1.5442     0.2527  -6.110 9.94e-10 ***
GenderMale         1.0521     0.2627   4.005 6.21e-05 ***
DeptB              0.7904     0.4977   1.588  0.11224    
DeptC              2.2046     0.2672   8.252  < 2e-16 ***
DeptD              2.1662     0.2750   7.878 3.32e-15 ***
DeptE              2.7013     0.2790   9.682  < 2e-16 ***
DeptF              4.1250     0.3297  12.512  < 2e-16 ***
GenderMale:DeptB  -0.8321     0.5104  -1.630  0.10306    
GenderMale:DeptC  -1.1770     0.2996  -3.929 8.53e-05 ***
GenderMale:DeptD  -0.9701     0.3026  -3.206  0.00135 ** 
GenderMale:DeptE  -1.2523     0.3303  -3.791  0.00015 ***
GenderMale:DeptF  -0.8632     0.4027  -2.144  0.03206 *  
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6044.3  on 4525  degrees of freedom
Residual deviance: 5167.3  on 4514  degrees of freedom
AIC: 5191.3

Number of Fisher Scoring iterations: 5


Two key things to notice:

1. Note that the coefficients of the main effects of Gender and Dept
change materially from the first model to the second, demonstrating the
impact of the interaction terms.

2. The interaction terms themselves are generally quite significant.



Further, running:

> anova(fit, fitIT,  test = "Chisq")
Analysis of Deviance Table

Model 1: Admit ~ Gender + Dept
Model 2: Admit ~ Gender * Dept
  Resid. Df Resid. Dev   Df Deviance P(>|Chi|)
1      4519     5187.5                        
2      4514     5167.3    5     20.2  0.001144


pretty well confirms that the impact of including the interaction terms
in the model is quite significant and that there is confounding going on
between Gender and Department, which is arguably, more interesting than
the isolated impact of Gender on Admissions.

You might want to look further at the help page for the dataset as I
noted above, to gain further graphical insight into the data.


To your query, the goal is not to 'make' the Woolf test non-significant,
but to better understand why it is significant to start with and dig
deeper into your data to gain further insight.

HTH,

Marc Schwartz


From ted.harding at nessie.mcc.ac.uk  Sat Feb 24 23:49:25 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 24 Feb 2007 22:49:25 -0000 (GMT)
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <20070224145433.07c2b1e1@triveni.stat.iastate.edu>
Message-ID: <XFMail.070224224925.ted.harding@nessie.mcc.ac.uk>

On 24-Feb-07 Ranjan Maitra wrote:
> Hi,
> Sorry for being a late entrant to this thread, but let me see if I
> understand the problem.
> 
> The poster wants to sample from an ellipsoid. Let us call this
> ellipsoid X'\Gamma X - d^2= 0.
> 
> There is no loss in assuming that the center is zero, otherwise the
> same can be done.
> 
> Let us consider the case when Gamma = I first. 
> 
> Then, let X \sim N_p(0, I) (any radially symmetric distribution will
> do here), then d*X/||X|| is uniform on the sphere of radius d. 
> 
> How about imitating the same?
> 
> Let X \sim N_p(0, \Sigma), where \Sigma = \Gamma^{-1} then X restricted
> to X'\Gamma X = d^2 gives the required uniform density on the
> ellipsoid.
> 
> How do we get this easily? I don't think rejection works or is even
> necessary.
> 
> Isn't d*X / ||\Gamma^{1/2}X|| enough? Here \Gamma^{1/2} is the square
> root matrix of \Gamma.
> 
> Note that any distribution of the kind f(X'\Gamma X) would work, but
> the multivariate Gaussian is a convenient tool, for which two-lines of
> R code should be enough.
> 
> 
> Many thanks and best wishes,
> Ranjan

As I understand Rusell Senior's original query, he wants to
generate a uniform distribution on the surface of the ellipsoid,
not over its interior:

  "I am interested in making a random sample from a uniform
   distribution of points over the surface of the earth,
   using the WGS84 ellipsoid as a model for the earth."

Your solution, and the solution given in Roger Bivand's reference

Section "UNIFORM_IN_ELLIPSOID_MAP maps uniform points into an ellipsoid"
in:
http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.f90

is valid for uniform points in the interior of an ellipsoid, I think.

But, since it is a linear transformation, it is not valid for the
points on the surface, as I explain for the case of an ellipse
(in particular, it results in higher linear density along the
circumference of an ellipse near the ends of the major axis
than near the ends of the minor axis).

It is also the method adopted for points on the surface in the
Section "UNIFORM_ON_ELLIPSOID_MAP maps uniform points onto an ellipsoid",
and I think this is wrong, as I explained.

Ted.

> On Sat, 24 Feb 2007 13:45:56 -0000 (GMT) (Ted Harding)
> <ted.harding at nessie.mcc.ac.uk> wrote:
> 
>> [Apologies if this is a repeated posting for you. Something seems
>>  to have gone amiss with my previous attempts to post this reply,
>>  as seen from my end]
>> 
>> On 22-Feb-07 Roger Bivand wrote:
>> > On 21 Feb 2007, Russell Senior wrote:
>> > 
>> >> 
>> >> I am interested in making a random sample from a uniform
>> >> distribution
>> >> of points over the surface of the earth, using the WGS84 ellipsoid
>> >> as
>> >> a model for the earth.  I know how to do this for a sphere, but
>> >> would
>> >> like to do better.  I can supply random numbers, want latitude
>> >> longitude pairs out.
>> >> 
>> >> Can anyone point me at a solution?  Thanks very much.
>> >> 
>> > 
>> > http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.html
>> > 
>> > looks promising, untried.
>> 
>> Hmmm ... That page didn't seem to be directly useful, since
>> on my understanding of the code (and comments) listed under
>> "subroutine uniform_on_ellipsoid_map(dim_num, n, a, r, seed, x)"
>> "UNIFORM_ON_ELLIPSOID_MAP maps uniform points onto an ellipsoid."
>> in
>> 
>> http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.f90
>> 
>> it takes points uniformly distributed on a sphere and then
>> linearly transforms these onto an ellipsoid. This will not
>> give unform density over the surface of the ellipsoid: indeed
>> the example graph they show of points on an ellipse generated
>> in this way clearly appear to be more dense at the "ends" of
>> the ellipse, and less dense on its "sides". See:
>> 
>> http://www.csit.fsu.edu/~burkardt/f_src/random_data/
>> uniform_on_ellipsoid_map.png
>> [all one line]
>> 
>> Indeed, if I understand their method correctly, in the case
>> of a horizontal ellipse it is equivalent (modulo rotating
>> the result) to distributing the points uniformly over a circle,
>> and then stretching the circle sideways. This will preserve
>> the vertical distribution (so at the two ends of the major axis
>> it has the same density as on the circle) but diluting the
>> horizontal distribution (so that at the two ends of the minor
>> axis the density isless than on the circle).
>> 
>> I did have a notion about this, but sat on it expecting that
>> someone would come up with a slick solution -- which hasn't
>> happened yet.
>> 
>> For the application you have in hand, uniform distribution
>> over a sphere is a fairly close approximation to uniform
>> distriobution over the ellipspoid -- but not quite.
>> 
>> But a rejection method, applied to points uniform on the sphere,
>> can give you points uniform on the ellipsoid and, because of
>> the close approximation of the sphere to the ellipsoid, you
>> would not be rejecting many points.
>> 
>> The outline strategy I had in mind (I haven't worked out details)
>> is based on the following.
>> 
>> Consider a point X0 on the sphere, at radial distance r0 from
>> the centre of the sphere (same as the centre of the ellipsoid).
>> Let the radius through that point meet the ellipsoid at a point
>> X1, at radial distance R1.
>> 
>> Let dS0 be an element of area at X0 on the sphere, which projects
>> radially onto an element of area dS1 on the ellipsoid. You want
>> all elements dS1 of equal size to be equally likely to receive
>> a random point.
>> 
>> Let the angle between the tangent plane to the ellipsoid at X1,
>> and the tangent plane to the sphere at X0, be phi.
>> 
>> The the ratio of areas dS1/dS0 is R(X0), say, where
>> 
>>   R(X0) = dS1/dS0 = r1^2/(r0^2 * cos(phi))
>> 
>> and the smaller this ratio, the less likely you want a point
>> u.d. on the sphere to give rise to a point on the ellipsoid.
>> 
>> Now define an acceptance probability P(X0) by
>> 
>>   P(X0) = R(X0)/sup[R(X)]
>> 
>> taking the supremum over X on the sphere. Then sample points X0
>> unformly on the sphere, accepting each one with probability
>> P(X0), and continue sampling until you have the number of
>> points that you need.
>> 
>> Maybe someone has a better idea ... (or code for the above!)
>> 
>> Ted.
>> 
>> 
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 24-Feb-07                                       Time: 13:45:50
>> ------------------------------ XFMail ------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 24-Feb-07                                       Time: 22:49:09
------------------------------ XFMail ------------------------------


From maitra at iastate.edu  Sun Feb 25 01:37:39 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Sat, 24 Feb 2007 18:37:39 -0600
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <XFMail.070224224925.ted.harding@nessie.mcc.ac.uk>
References: <20070224145433.07c2b1e1@triveni.stat.iastate.edu>
	<XFMail.070224224925.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070224183739.00ac6395@triveni.stat.iastate.edu>

"My" method is for the surface, not for the interior. The constraint d*X/|| \Gamma^{-1/2}X ||ensures the constraint, no? The uniformity is ensured by the density restricted to satisfy the constraint which makes it a constant.

Ranjan


On Sat, 24 Feb 2007 22:49:25 -0000 (GMT) (Ted Harding) <ted.harding at nessie.mcc.ac.uk> wrote:

> On 24-Feb-07 Ranjan Maitra wrote:
> > Hi,
> > Sorry for being a late entrant to this thread, but let me see if I
> > understand the problem.
> > 
> > The poster wants to sample from an ellipsoid. Let us call this
> > ellipsoid X'\Gamma X - d^2= 0.
> > 
> > There is no loss in assuming that the center is zero, otherwise the
> > same can be done.
> > 
> > Let us consider the case when Gamma = I first. 
> > 
> > Then, let X \sim N_p(0, I) (any radially symmetric distribution will
> > do here), then d*X/||X|| is uniform on the sphere of radius d. 
> > 
> > How about imitating the same?
> > 
> > Let X \sim N_p(0, \Sigma), where \Sigma = \Gamma^{-1} then X restricted
> > to X'\Gamma X = d^2 gives the required uniform density on the
> > ellipsoid.
> > 
> > How do we get this easily? I don't think rejection works or is even
> > necessary.
> > 
> > Isn't d*X / ||\Gamma^{1/2}X|| enough? Here \Gamma^{1/2} is the square
> > root matrix of \Gamma.
> > 
> > Note that any distribution of the kind f(X'\Gamma X) would work, but
> > the multivariate Gaussian is a convenient tool, for which two-lines of
> > R code should be enough.
> > 
> > 
> > Many thanks and best wishes,
> > Ranjan
> 
> As I understand Rusell Senior's original query, he wants to
> generate a uniform distribution on the surface of the ellipsoid,
> not over its interior:
> 
>   "I am interested in making a random sample from a uniform
>    distribution of points over the surface of the earth,
>    using the WGS84 ellipsoid as a model for the earth."
> 
> Your solution, and the solution given in Roger Bivand's reference
> 
> Section "UNIFORM_IN_ELLIPSOID_MAP maps uniform points into an ellipsoid"
> in:
> http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.f90
> 
> is valid for uniform points in the interior of an ellipsoid, I think.
> 
> But, since it is a linear transformation, it is not valid for the
> points on the surface, as I explain for the case of an ellipse
> (in particular, it results in higher linear density along the
> circumference of an ellipse near the ends of the major axis
> than near the ends of the minor axis).
> 
> It is also the method adopted for points on the surface in the
> Section "UNIFORM_ON_ELLIPSOID_MAP maps uniform points onto an ellipsoid",
> and I think this is wrong, as I explained.
> 
> Ted.
> 
> > On Sat, 24 Feb 2007 13:45:56 -0000 (GMT) (Ted Harding)
> > <ted.harding at nessie.mcc.ac.uk> wrote:
> > 
> >> [Apologies if this is a repeated posting for you. Something seems
> >>  to have gone amiss with my previous attempts to post this reply,
> >>  as seen from my end]
> >> 
> >> On 22-Feb-07 Roger Bivand wrote:
> >> > On 21 Feb 2007, Russell Senior wrote:
> >> > 
> >> >> 
> >> >> I am interested in making a random sample from a uniform
> >> >> distribution
> >> >> of points over the surface of the earth, using the WGS84 ellipsoid
> >> >> as
> >> >> a model for the earth.  I know how to do this for a sphere, but
> >> >> would
> >> >> like to do better.  I can supply random numbers, want latitude
> >> >> longitude pairs out.
> >> >> 
> >> >> Can anyone point me at a solution?  Thanks very much.
> >> >> 
> >> > 
> >> > http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.html
> >> > 
> >> > looks promising, untried.
> >> 
> >> Hmmm ... That page didn't seem to be directly useful, since
> >> on my understanding of the code (and comments) listed under
> >> "subroutine uniform_on_ellipsoid_map(dim_num, n, a, r, seed, x)"
> >> "UNIFORM_ON_ELLIPSOID_MAP maps uniform points onto an ellipsoid."
> >> in
> >> 
> >> http://www.csit.fsu.edu/~burkardt/f_src/random_data/random_data.f90
> >> 
> >> it takes points uniformly distributed on a sphere and then
> >> linearly transforms these onto an ellipsoid. This will not
> >> give unform density over the surface of the ellipsoid: indeed
> >> the example graph they show of points on an ellipse generated
> >> in this way clearly appear to be more dense at the "ends" of
> >> the ellipse, and less dense on its "sides". See:
> >> 
> >> http://www.csit.fsu.edu/~burkardt/f_src/random_data/
> >> uniform_on_ellipsoid_map.png
> >> [all one line]
> >> 
> >> Indeed, if I understand their method correctly, in the case
> >> of a horizontal ellipse it is equivalent (modulo rotating
> >> the result) to distributing the points uniformly over a circle,
> >> and then stretching the circle sideways. This will preserve
> >> the vertical distribution (so at the two ends of the major axis
> >> it has the same density as on the circle) but diluting the
> >> horizontal distribution (so that at the two ends of the minor
> >> axis the density isless than on the circle).
> >> 
> >> I did have a notion about this, but sat on it expecting that
> >> someone would come up with a slick solution -- which hasn't
> >> happened yet.
> >> 
> >> For the application you have in hand, uniform distribution
> >> over a sphere is a fairly close approximation to uniform
> >> distriobution over the ellipspoid -- but not quite.
> >> 
> >> But a rejection method, applied to points uniform on the sphere,
> >> can give you points uniform on the ellipsoid and, because of
> >> the close approximation of the sphere to the ellipsoid, you
> >> would not be rejecting many points.
> >> 
> >> The outline strategy I had in mind (I haven't worked out details)
> >> is based on the following.
> >> 
> >> Consider a point X0 on the sphere, at radial distance r0 from
> >> the centre of the sphere (same as the centre of the ellipsoid).
> >> Let the radius through that point meet the ellipsoid at a point
> >> X1, at radial distance R1.
> >> 
> >> Let dS0 be an element of area at X0 on the sphere, which projects
> >> radially onto an element of area dS1 on the ellipsoid. You want
> >> all elements dS1 of equal size to be equally likely to receive
> >> a random point.
> >> 
> >> Let the angle between the tangent plane to the ellipsoid at X1,
> >> and the tangent plane to the sphere at X0, be phi.
> >> 
> >> The the ratio of areas dS1/dS0 is R(X0), say, where
> >> 
> >>   R(X0) = dS1/dS0 = r1^2/(r0^2 * cos(phi))
> >> 
> >> and the smaller this ratio, the less likely you want a point
> >> u.d. on the sphere to give rise to a point on the ellipsoid.
> >> 
> >> Now define an acceptance probability P(X0) by
> >> 
> >>   P(X0) = R(X0)/sup[R(X)]
> >> 
> >> taking the supremum over X on the sphere. Then sample points X0
> >> unformly on the sphere, accepting each one with probability
> >> P(X0), and continue sampling until you have the number of
> >> points that you need.
> >> 
> >> Maybe someone has a better idea ... (or code for the above!)
> >> 
> >> Ted.
> >> 
> >> 
> >> --------------------------------------------------------------------
> >> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> >> Fax-to-email: +44 (0)870 094 0861
> >> Date: 24-Feb-07                                       Time: 13:45:50
> >> ------------------------------ XFMail ------------------------------
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 24-Feb-07                                       Time: 22:49:09
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mark.lyman at gmail.com  Sun Feb 25 10:17:37 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Sun, 25 Feb 2007 02:17:37 -0700
Subject: [R] barchart (lattice) with text labels
In-Reply-To: <eb555e660702240940v76bb600dscc70a51fa266279d@mail.gmail.com>
References: <45DFF5C3.60806@xmission.com>
	<eb555e660702240940v76bb600dscc70a51fa266279d@mail.gmail.com>
Message-ID: <45E15431.7050608@gmail.com>

Deepayan Sarkar wrote:
> On 2/24/07, Mark and Heather Lyman <lymanmh at xmission.com> wrote:
>> I would like to place the value for each bar in barchart (lattice) at
>> the top of each bar. Something like the following code produces.
>>
>> library(lattice)
>>
>> mypanelfunc <- function(x, y, ...)
>> {
>>   panel.barchart(x, y, ...)
>>   panel.text(x, y, labels=as.character(round(x,2)), ...)
>>   }
>>
>> myprepanelfunc <- function(x, y, ...) list(xlim=c(0, max(x)+.1))
>>
>> mydata <- expand.grid(a=factor(1:5), b=factor(1:3), c=factor(1:2))
>> mydata$x <- runif(nrow(mydata))
>>
>> barchart(a~x|b, mydata, groups=c, panel=mypanelfunc,
>> prepanel=myprepanelfunc, adj=c(-0.1,0.5))
>>
>> However, I cannot figure out how to shift the values to correspond with
>> their respective grouped bar.
>
> You should look at panel.barchart and try to reproduce the
> calculations done there.
>
> Deepayan
>
This is the panel function that I ended up using. As Deepayan suggested, 
I borrowed heavily from panel.barchart.

mypanelfunc <- function(x, y, groups, box.ratio, ...)
{
  panel.barchart(x, y, groups=groups, box.ratio=box.ratio, ...)
  origin <- current.panel.limits()$xlim[1]
  groupSub <- function(groups, subscripts, ...) groups[subscripts]
  groups <- as.numeric(groupSub(groups, ...))
  vals <- sort(unique(groups))
  nvals <- length(vals)
  height <- box.ratio/(1+ nvals * box.ratio)
  for (i in unique(y)) {
    ok <- y == levels(y)[i]
    nok <- sum(ok, na.rm = TRUE)
    panel.text(x = x[ok], y = (i + height * (groups[ok] - (nvals + 1)/2)),
      labels=as.character(signif(x[ok],2)), ...)
  }
}

Mark Lyman


From anup_nandialath at yahoo.com  Sun Feb 25 07:51:17 2007
From: anup_nandialath at yahoo.com (Anup Nandialath)
Date: Sat, 24 Feb 2007 22:51:17 -0800 (PST)
Subject: [R] Random Integers
Message-ID: <546455.26953.qm@web53302.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/6a1bbaa2/attachment.pl 

From dfarrar at newrvana.com  Sat Feb 24 19:57:25 2007
From: dfarrar at newrvana.com (David Farrar)
Date: Sat, 24 Feb 2007 10:57:25 -0800 (PST)
Subject: [R] rpart with overdispersed count data?
Message-ID: <20070224185725.64204.qmail@web805.biz.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/8822becb/attachment.pl 

From joe_retzer at yahoo.com  Sat Feb 24 22:48:21 2007
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Sat, 24 Feb 2007 13:48:21 -0800 (PST)
Subject: [R] Bold Substring in mtext (newbie question)
Message-ID: <296155.30347.qm@web60317.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/bc9ae5dd/attachment.pl 

From joe_retzer at yahoo.com  Sun Feb 25 00:12:52 2007
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Sat, 24 Feb 2007 15:12:52 -0800 (PST)
Subject: [R] mtext bold font problem
Message-ID: <814845.81329.qm@web60312.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070224/0319445b/attachment.pl 

From phhs80 at gmail.com  Sun Feb 25 15:24:06 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Sun, 25 Feb 2007 14:24:06 +0000
Subject: [R] Random Integers
In-Reply-To: <546455.26953.qm@web53302.mail.yahoo.com>
References: <546455.26953.qm@web53302.mail.yahoo.com>
Message-ID: <6ade6f6c0702250624y4a89e66ay6c24552d3533a16f@mail.gmail.com>

> Is there an R function to generate random integers?  Thanks in advance.

For example, if you want 5 random integers from the sequence 1:50, you
can do the following:

sample(1:50,5)

Paul


From a.fugard at ed.ac.uk  Sun Feb 25 15:27:57 2007
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Sun, 25 Feb 2007 14:27:57 +0000
Subject: [R] Random Integers
In-Reply-To: <546455.26953.qm@web53302.mail.yahoo.com>
References: <546455.26953.qm@web53302.mail.yahoo.com>
Message-ID: <5622A512-63AE-484C-B8AD-B865A4EBA4A2@ed.ac.uk>



On 25 Feb 2007, at 06:51, Anup Nandialath wrote:

>
> Is there an R function to generate random integers?  Thanks in  
> advance.

The package Random does what you want, but requires a net connection.

http://cran.r-project.org/src/contrib/Descriptions/random.html

"This package provides an interface to the true random number service  
provided by the random.org website created by Mads Haahr. The  
random.org web service samples atmospheric noise via radio tuned to  
an unused broadcasting frequency together with a skew correction  
algorithm due to John von Neumann."

install.packages("random")
library(random)
?random

 > randomNumbers(10,1,6,1)
    V1
1   5
2   3
3   6
4   3
5   3
6   1
7   6
8   4
9   5
10  3

Andy

--
Andy Fugard, Postgraduate Research Student
Psychology (Room F15), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk


From Charles.Annis at StatisticalEngineering.com  Sun Feb 25 15:32:33 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sun, 25 Feb 2007 09:32:33 -0500
Subject: [R] Random Integers
In-Reply-To: <546455.26953.qm@web53302.mail.yahoo.com>
References: <546455.26953.qm@web53302.mail.yahoo.com>
Message-ID: <05c501c758e9$ca14e620$6400a8c0@DD4XFW31>

Sure.

rpois(n, lambda)

... will do it.  But you should tell us something about how you want your
numbers to be distributed, since rpois() produces integers having a Poisson
distribution.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anup Nandialath
Sent: Sunday, February 25, 2007 1:51 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Random Integers

Hi all,

Is there an R function to generate random integers?  Thanks in advance.

Sincerely

Anup

 
---------------------------------
Now that's room service! Choose from over 150,000 hotels 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Sun Feb 25 15:44:32 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 25 Feb 2007 08:44:32 -0600
Subject: [R] Double-banger function names: preferences and suggestions
Message-ID: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>

What do you prefer/recommend for double-banger function names:

 1 scale.colour
 2 scale_colour
 3 scaleColour

1 is more R-like, but conflicts with S3.  2 is a modern version of
number 1, but not many packages use it.  Number 3 is more java-like.
(I like number 2 best)

Any suggestions?

Thanks,

Hadley


From f.harrell at vanderbilt.edu  Sun Feb 25 16:24:10 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 25 Feb 2007 09:24:10 -0600
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
Message-ID: <45E1AA1A.60403@vanderbilt.edu>

hadley wickham wrote:
> What do you prefer/recommend for double-banger function names:
> 
>  1 scale.colour
>  2 scale_colour
>  3 scaleColour
> 
> 1 is more R-like, but conflicts with S3.  2 is a modern version of
> number 1, but not many packages use it.  Number 3 is more java-like.
> (I like number 2 best)
> 
> Any suggestions?
> 
> Thanks,
> 
> Hadley

Personal preference is for 3.  I wish I had used that throughout the 
Hmisc and Design packages for non-generics.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ggrothendieck at gmail.com  Sun Feb 25 16:32:31 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Feb 2007 10:32:31 -0500
Subject: [R] Bold Substring in mtext (newbie question)
In-Reply-To: <296155.30347.qm@web60317.mail.yahoo.com>
References: <296155.30347.qm@web60317.mail.yahoo.com>
Message-ID: <971536df0702250732u6b274777x1a0be1c2cf4f6f77@mail.gmail.com>

Try this:

plot(1)
mtext(quote(A ~ bold(bold) ~ word), cex = 1.3)

On 2/24/07, Joseph Retzer <joe_retzer at yahoo.com> wrote:
> Hi everyone,
>  I suspect this is an easy task however I've not been able to accomplish it. I'd like to create an mtext title which has certain words bold, the rest not bold. So far I've been able to create one which is all bold, one which is all not bold and one which has bold and not bold superimposed on one another. Any suggestion would be appreciated.
> Many thanks,
> Joe Retzer
>
> # Not bold
> mtext(paste("Overall Satisfaction Call Difference vs. Overall Satisfaction Rep. Difference"), outer = TRUE, cex = 1.3)
>
> # Bold
>  mtext(expression(bold("Overall Satisfaction Call Difference vs. Overall Satisfaction Rep. Difference")), outer = TRUE, cex = 1.3)
>
> # I'd like to have just the 2 occurrences of the word "Difference" being bold
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Sun Feb 25 16:33:06 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 25 Feb 2007 10:33:06 -0500 (EST)
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
Message-ID: <606471.61904.qm@web32810.mail.mud.yahoo.com>


--- hadley wickham <h.wickham at gmail.com> wrote:

> What do you prefer/recommend for double-banger
> function names:
> 
>  1 scale.colour
>  2 scale_colour
>  3 scaleColour
> 
> 1 is more R-like, but conflicts with S3.  2 is a
> modern version of
> number 1, but not many packages use it.  Number 3 is
> more java-like.
> (I like number 2 best)

I prefer 1 but if it is in conflict with S2 then I go
with 3. I just don't like underscores. :)


From albmont at centroin.com.br  Sun Feb 25 16:52:18 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sun, 25 Feb 2007 15:52:18 +0000
Subject: [R] Random Integers
In-Reply-To: <05c501c758e9$ca14e620$6400a8c0@DD4XFW31>
References: <546455.26953.qm@web53302.mail.yahoo.com>
	<05c501c758e9$ca14e620$6400a8c0@DD4XFW31>
Message-ID: <200702251552.18503.albmont@centroin.com.br>

Charles Annis, P.E. wrote:
>
> rpois(n, lambda)
>
> ... will do it.  But you should tell us something about how you want your
> numbers to be distributed, since rpois() produces integers having a Poisson
> distribution.
>
<nitpick>
rpois does not generate random _integers_, it generates random 
_natural numbers_.
</nitpick>

The question should be more descriptive. "Random" is half of the things
we need to know, the other half is how deterministic you want your integers.

For example, if you want to generate random integers in such a way that
all integers have the same probability, then this can't be done. OTOH, if
you want to simulate random integers that distribute "like integers appear
in Nature", then it's still not precise, but there are serious attempts 
to reproduce this behaviour. Check in the wikipedia (www.wikipedia.org)
those distributions: Zip's law, Zeta distribution, Benford's law, 
Zipf-Mandelbrot law. The problem is that all of them generate positive
random integers, but it's not difficult to extrapolate them to integers.

Alberto Monteiro


From ggrothendieck at gmail.com  Sun Feb 25 16:53:20 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Feb 2007 10:53:20 -0500
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
Message-ID: <971536df0702250753u259515b4s4da122f42b10c5eb@mail.gmail.com>

There is a fourth possibility too:

4. `scale colour`

I guess my preference is #1, #2, #3 and #4 in that order with #1 best.  Even
though #1 can conflict with S3 it usually does not and its historically what R
used so I usually just stay consistent with historical precedent.  #2 is
otherwise best and has the advantage of no conflict with S3.  #3 is too hard
to read due to the lack of spacing and is probably motivated by misguided
Java programmers.   The introduction of such names into R was and continues
to be a mistake in my opinion.  #4 takes up too much screen real estate and
the backquotes look too busy.

On 2/25/07, hadley wickham <h.wickham at gmail.com> wrote:
> What do you prefer/recommend for double-banger function names:
>
>  1 scale.colour
>  2 scale_colour
>  3 scaleColour
>
> 1 is more R-like, but conflicts with S3.  2 is a modern version of
> number 1, but not many packages use it.  Number 3 is more java-like.
> (I like number 2 best)
>
> Any suggestions?
>
> Thanks,
>
> Hadley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Sun Feb 25 16:56:32 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sun, 25 Feb 2007 15:56:32 +0000
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
Message-ID: <200702251556.33019.albmont@centroin.com.br>

hadley wickham wrote:
>
> What do you prefer/recommend for double-banger function names:
>
>  1 scale.colour
>  2 scale_colour
>  3 scaleColour
>
> 1 is more R-like, but conflicts with S3.  2 is a modern version of
> number 1, but not many packages use it.  Number 3 is more java-like.
> (I like number 2 best)
>
> Any suggestions?
>
I always prefer 2, but this would make it non-portable to S-Plus. S-Plus
has a bug, where _ is the equivalent to <- (why would they do this? I
prefer to think it's stupidity and not villainy)

I guess most (if not all) functions in the libraries use 1.

Alberto Monteiro


From anup_nandialath at yahoo.com  Sun Feb 25 17:10:18 2007
From: anup_nandialath at yahoo.com (Anup Nandialath)
Date: Sun, 25 Feb 2007 08:10:18 -0800 (PST)
Subject: [R] Random Integers
In-Reply-To: <05c501c758e9$ca14e620$6400a8c0@DD4XFW31>
Message-ID: <785185.60620.qm@web53307.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/924e2ffc/attachment.pl 

From glubke at nd.edu  Sun Feb 25 17:24:19 2007
From: glubke at nd.edu (Gitta Lubke)
Date: Sun, 25 Feb 2007 10:24:19 -0600
Subject: [R] rotating labels
Message-ID: <45E1B833.3000203@nd.edu>

I'd like to rotate x-axis labels to get a diagonal orientation (rather 
than horizontal or vertical), this would result in an easier read, any 
ideas?
thanks, Gitta

From nono.231 at gmail.com  Sun Feb 25 17:26:40 2007
From: nono.231 at gmail.com (I. Soumpasis)
Date: Sun, 25 Feb 2007 18:26:40 +0200
Subject: [R] dev.print and postscript device problem
Message-ID: <3ff92a550702250826q5be9435bice36dd3c43c5b2da@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/9dc86e0d/attachment.pl 

From b.jacobs at pandora.be  Sun Feb 25 17:27:25 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Sun, 25 Feb 2007 17:27:25 +0100
Subject: [R] Combining Dataframes
In-Reply-To: <200702251552.18503.albmont@centroin.com.br>
Message-ID: <20070225162729.992D5D4170@asia.telenet-ops.be>

Hi,

What is the best way to combine several dataframes (approx a dozen, all
having one column) into one? All dataframes have a different rowlength, and
do not contain numbers.
As this new dataframe should have the length of the dataframe with the most
rows, the difference in rows with the other dataframes can be filled with
the value NA.

I've tried merge (only possible with 2 df) and cbind (gives error)

Thx for helping me out.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alberto Vieira
Ferreira Monteiro
Sent: 25 February 2007 16:52
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Random Integers

Charles Annis, P.E. wrote:
>
> rpois(n, lambda)
>
> ... will do it.  But you should tell us something about how you want your
> numbers to be distributed, since rpois() produces integers having a
Poisson
> distribution.
>
<nitpick>
rpois does not generate random _integers_, it generates random 
_natural numbers_.
</nitpick>

The question should be more descriptive. "Random" is half of the things
we need to know, the other half is how deterministic you want your integers.

For example, if you want to generate random integers in such a way that
all integers have the same probability, then this can't be done. OTOH, if
you want to simulate random integers that distribute "like integers appear
in Nature", then it's still not precise, but there are serious attempts 
to reproduce this behaviour. Check in the wikipedia (www.wikipedia.org)
those distributions: Zip's law, Zeta distribution, Benford's law, 
Zipf-Mandelbrot law. The problem is that all of them generate positive
random integers, but it's not difficult to extrapolate them to integers.

Alberto Monteiro

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Sun Feb 25 17:28:22 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 25 Feb 2007 10:28:22 -0600
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <200702251556.33019.albmont@centroin.com.br>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
	<200702251556.33019.albmont@centroin.com.br>
Message-ID: <1172420902.4865.86.camel@localhost.localdomain>

On Sun, 2007-02-25 at 15:56 +0000, Alberto Vieira Ferreira Monteiro
wrote:
> hadley wickham wrote:
> >
> > What do you prefer/recommend for double-banger function names:
> >
> >  1 scale.colour
> >  2 scale_colour
> >  3 scaleColour
> >
> > 1 is more R-like, but conflicts with S3.  2 is a modern version of
> > number 1, but not many packages use it.  Number 3 is more java-like.
> > (I like number 2 best)
> >
> > Any suggestions?
> >
> I always prefer 2, but this would make it non-portable to S-Plus. S-Plus
> has a bug, where _ is the equivalent to <- (why would they do this? I
> prefer to think it's stupidity and not villainy)

That's not a bug. If you search the archives of both the S-PLUS list and
the R lists, you will see highly energized discussion on the use of the
underscore operator.

In R, the use of '_' was allowed for assignment up until version 1.8.0
when:

DEPRECATED & DEFUNCT

    o	The assignment operator `_' has been removed.


and subsequently allowed in names in version 1.9.0 when:

 o	Underscore '_' is now allowed in syntactically valid names, and
	make.names() no longer changes underscores.  Very old code
	that makes use of underscore for assignment may now give
	confusing error messages.


Not to further contribute to the dialog on 'style', but to further
contribute ;-), for those who have coded in the Windows environment (ie.
C, VBA, etc.) the extension of sorts to number 3 is of course "Hungarian
Notation", named after Charles Simonyi, originally at Xerox PARC and
later senior developer/architect at MS. The extension was the inclusion
of the data type prefix, such as fnScaleColour to indicate that this was
a function, with the name using caps to make words more distinct.

And no, I'm not advocating that use...I have been guilty myself of using
variants of 1 and 3, perhaps driven by my circulating caffeine levels as
much as anything else.

HTH,

Marc Schwartz
<Off to go remove 12 inches of snow from the driveway and sidewalk...oy>


From maitra at iastate.edu  Sun Feb 25 17:36:05 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Sun, 25 Feb 2007 10:36:05 -0600
Subject: [R] Combining Dataframes
In-Reply-To: <20070225162729.992D5D4170@asia.telenet-ops.be>
References: <200702251552.18503.albmont@centroin.com.br>
	<20070225162729.992D5D4170@asia.telenet-ops.be>
Message-ID: <20070225103605.2a9c95ce@triveni.stat.iastate.edu>

Hi,

This question has no connection with the original thread. Please do not post like this since it messes up threads since making searching by thread topics in archives useless.

Thank you,
Ranjan

On Sun, 25 Feb 2007 17:27:25 +0100 "Bert Jacobs" <b.jacobs at pandora.be> wrote:

> Hi,
> 
> What is the best way to combine several dataframes (approx a dozen, all
> having one column) into one? All dataframes have a different rowlength, and
> do not contain numbers.
> As this new dataframe should have the length of the dataframe with the most
> rows, the difference in rows with the other dataframes can be filled with
> the value NA.
> 
> I've tried merge (only possible with 2 df) and cbind (gives error)
> 
> Thx for helping me out.
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alberto Vieira
> Ferreira Monteiro
> Sent: 25 February 2007 16:52
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Random Integers
> 
> Charles Annis, P.E. wrote:
> >
> > rpois(n, lambda)
> >
> > ... will do it.  But you should tell us something about how you want your
> > numbers to be distributed, since rpois() produces integers having a
> Poisson
> > distribution.
> >
> <nitpick>
> rpois does not generate random _integers_, it generates random 
> _natural numbers_.
> </nitpick>
> 
> The question should be more descriptive. "Random" is half of the things
> we need to know, the other half is how deterministic you want your integers.
> 
> For example, if you want to generate random integers in such a way that
> all integers have the same probability, then this can't be done. OTOH, if
> you want to simulate random integers that distribute "like integers appear
> in Nature", then it's still not precise, but there are serious attempts 
> to reproduce this behaviour. Check in the wikipedia (www.wikipedia.org)
> those distributions: Zip's law, Zeta distribution, Benford's law, 
> Zipf-Mandelbrot law. The problem is that all of them generate positive
> random integers, but it's not difficult to extrapolate them to integers.
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From amnakhan493 at gmail.com  Sun Feb 25 17:37:35 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 25 Feb 2007 08:37:35 -0800
Subject: [R] RFA and nsRFA
Message-ID: <3ffd3bb60702250837w440481a1m2c55cf885da1839d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/6ce56531/attachment.pl 

From amnakhan493 at gmail.com  Sun Feb 25 17:37:35 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 25 Feb 2007 08:37:35 -0800
Subject: [R] RFA and nsRFA
Message-ID: <3ffd3bb60702250837w440481a1m2c55cf885da1839d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/6ce56531/attachment-0001.pl 

From amnakhan493 at gmail.com  Sun Feb 25 17:44:29 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 25 Feb 2007 08:44:29 -0800
Subject: [R] nsRFA
Message-ID: <3ffd3bb60702250844t15a3b6acve4fcfedfc7ed7736@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/449503ef/attachment.pl 

From amnakhan493 at gmail.com  Sun Feb 25 17:44:29 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 25 Feb 2007 08:44:29 -0800
Subject: [R] nsRFA
Message-ID: <3ffd3bb60702250844t15a3b6acve4fcfedfc7ed7736@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/449503ef/attachment-0001.pl 

From amnakhan493 at gmail.com  Sun Feb 25 17:51:10 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 25 Feb 2007 08:51:10 -0800
Subject: [R] RFA
Message-ID: <3ffd3bb60702250851w575442cat6b16d1aefb290f8f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/bbf24446/attachment.pl 

From amnakhan493 at gmail.com  Sun Feb 25 17:51:10 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 25 Feb 2007 08:51:10 -0800
Subject: [R] RFA
Message-ID: <3ffd3bb60702250851w575442cat6b16d1aefb290f8f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/bbf24446/attachment-0001.pl 

From jafarikia at gmail.com  Sun Feb 25 18:20:17 2007
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Sun, 25 Feb 2007 12:20:17 -0500
Subject: [R] Writing integers in "write.matrix" function
Message-ID: <e3f2a5ab0702250920n2acf1d3ai97eed65213a8c347@mail.gmail.com>

Hello everyone,

I am using the following program to get the p-value of some numbers
(column 'LR' of the data.dat file). I want to write the 1st and 2nd
column of the output file (data.out) as an integer while the program
change them. Could anybody please tell me how I can write
the code which writes the values of the first two columns as integer?

For your convenience, I have attached the input and output files.
I should indicate that I have already sent this message but I could not
apply your comments.
Thanks

library ('MASS')
MP<-read.table(file='data.dat')
names(MP)<-c('B','R','S','L','LR','Q')
a<-as.matrix((1-pchisq(MP$LR, df=1)))
b<-cbind(MP$B,MP$R,a,MP$S,MP$L,MP$LR,MP$Q)
write.matrix(b,'data.out')



These are the files if you don't like to open the attached file!



data.dat:

  1   7   1.704094  10.61   0.874286  0.00

  2   7   1.955271   2.82   1.001429  0.00

  3   6   2.960459   3.74   1.548667  0.00

  4   4   6.120956   5.83   3.929000  1.92



data.out:

c d

1.000000e+00 7.000000e+00 3.497715e-01 1.704094e+00 1.061000e+01
8.742860e-01 0.000000e+00

2.000000e+00 7.000000e+00 3.169650e-01 1.955271e+00 2.820000e+00
1.001429e+00 0.000000e+00

3.000000e+00 6.000000e+00 2.133323e-01 2.960459e+00 3.740000e+00
1.548667e+00 0.000000e+00

4.000000e+00 4.000000e+00 4.746016e-02 6.120956e+00 5.830000e+00
3.929000e+00 1.920000e+00
-------------- next part --------------
  1   7   1.704094  10.61   0.874286  0.00
  2   7   1.955271   2.82   1.001429  0.00
  3   6   2.960459   3.74   1.548667  0.00
  4   4   6.120956   5.83   3.929000  1.92



-------------- next part --------------
c d     
1.000000e+00 7.000000e+00 3.497715e-01 1.704094e+00 1.061000e+01 8.742860e-01 0.000000e+00
2.000000e+00 7.000000e+00 3.169650e-01 1.955271e+00 2.820000e+00 1.001429e+00 0.000000e+00
3.000000e+00 6.000000e+00 2.133323e-01 2.960459e+00 3.740000e+00 1.548667e+00 0.000000e+00
4.000000e+00 4.000000e+00 4.746016e-02 6.120956e+00 5.830000e+00 3.929000e+00 1.920000e+00

From ccleland at optonline.net  Sun Feb 25 18:48:05 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 25 Feb 2007 12:48:05 -0500
Subject: [R] rotating labels
In-Reply-To: <45E1B833.3000203@nd.edu>
References: <45E1B833.3000203@nd.edu>
Message-ID: <45E1CBD5.2040205@optonline.net>

Gitta Lubke wrote:
> I'd like to rotate x-axis labels to get a diagonal orientation (rather
> than horizontal or vertical), this would result in an easier read, any
> ideas?
> thanks, Gitta

  This is FAQ 7.27

http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-create-rotated-axis-labels_003f

  And for more examples in the archives, RSiteSearch("rotate label")

> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From albmont at centroin.com.br  Sun Feb 25 19:11:12 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sun, 25 Feb 2007 18:11:12 +0000
Subject: [R] RSPython
In-Reply-To: <200702251556.33019.albmont@centroin.com.br>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
	<200702251556.33019.albmont@centroin.com.br>
Message-ID: <200702251811.14087.albmont@centroin.com.br>

Any hints on how to make RSPython work? I downloaded and installed it, but
I can't use it neither from python nor from R!

Alberto Monteiro


From Serguei.Kaniovski at wifo.ac.at  Sun Feb 25 19:14:22 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Sun, 25 Feb 2007 19:14:22 +0100
Subject: [R] Overdispersion in a GLM binomial model
Message-ID: <OFD529DF90.5CD64148-ONC125728D.00643145-C125728D.0064314B@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/9a686c7b/attachment.pl 

From sfalcon at fhcrc.org  Sun Feb 25 19:16:22 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Sun, 25 Feb 2007 10:16:22 -0800
Subject: [R] help with RMySQL
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E8C618@BAN-MAILSRV03.Amba.com>
	(Ravi S. Shankar's message of "Fri, 23 Feb 2007 14:06:20 +0530")
References: <A36876D3F8A5734FA84A4338135E7CC3E8C618@BAN-MAILSRV03.Amba.com>
Message-ID: <m2k5y6m7bd.fsf@ziti.local>

"Ravi S. Shankar" <ravis at ambaresearch.com> writes:

> Hi R users,
>
> I am using RMySQL to connect to a database in MySQL.
>
> I have 3 questions.
>
>  
>
> 1)When I give the following command
>
> dbListTables(con)
>
>  
>
> I get the output 
>
>  
>
> stack imbalance in .Call, 142 then 143
>
> stack imbalance in <-, 140 then 141
>
> stack imbalance in {, 138 then 139
>
> stack imbalance in standardGeneric, 126 then 127
>
> stack imbalance in class, 121 then 122
>
> stack imbalance in <-, 119 then 120
>
> stack imbalance in {, 117 then 118
>
> stack imbalance in <-, 111 then 112
>
> stack imbalance in {, 109 then 110
>
>  
>
> [1] "newtable" "ravi"    
>
>  
>
> Could somebody tell me why it shows a stack imbalance?

It most likely indicates a bug in the package's C code.  But you need
to tell us the output of sessionInfo() and the version of MySQLite you
are using.

You might also want to send such a message to the r-sig-db list.

>
>  
>
> 2)For this command
>
> testcmd1<-fetch(dbSendQuery(con,"select ticker from ravi where
> banks>=100"))   
>
>> dim(testcmd1)
>
> The output is
>
> [1] 500   1
>
> However when I give the same command directly in MySQL command prompt I
> get an output greater than 2000!! (Does fetch have a limitation?)

Read the doc for fetch.

> 3) Also is it possible to get output where the NA values are
>    removed?

Not sure what you mean, but you can always remove them afterwards.
nulls in the DB should get pulled across as NA and so if your query
excludes nulls...

+ seth


From duncan at wald.ucdavis.edu  Sun Feb 25 19:21:56 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Sun, 25 Feb 2007 10:21:56 -0800
Subject: [R] RSPython
In-Reply-To: <200702251811.14087.albmont@centroin.com.br>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>	<200702251556.33019.albmont@centroin.com.br>
	<200702251811.14087.albmont@centroin.com.br>
Message-ID: <45E1D3C4.10409@wald.ucdavis.edu>

Well, we'll need to know in what ways it doesn't work and
what operating system you are using, etc.

If you want to call R from Python, RPy is probably more straightforward.

But RSPython works fine on Unix machines. Not on Windows at this point.


 D.

Alberto Vieira Ferreira Monteiro wrote:
> Any hints on how to make RSPython work? I downloaded and installed it, but
> I can't use it neither from python nor from R!
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From a.fugard at ed.ac.uk  Sun Feb 25 19:35:50 2007
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Sun, 25 Feb 2007 18:35:50 +0000
Subject: [R] Combining Dataframes
In-Reply-To: <20070225162729.992D5D4170@asia.telenet-ops.be>
References: <20070225162729.992D5D4170@asia.telenet-ops.be>
Message-ID: <CCA5DA75-B00B-42B5-A87E-BFA4C2638A82@ed.ac.uk>

Would need more info.  Merge could still do the job; you might just  
have to call it approx a dozen - 1 times!

Andy


On 25 Feb 2007, at 16:27, Bert Jacobs wrote:

> Hi,
>
> What is the best way to combine several dataframes (approx a dozen,  
> all
> having one column) into one? All dataframes have a different  
> rowlength, and
> do not contain numbers.
> As this new dataframe should have the length of the dataframe with  
> the most
> rows, the difference in rows with the other dataframes can be  
> filled with
> the value NA.
>
> I've tried merge (only possible with 2 df) and cbind (gives error)
>
> Thx for helping me out.
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alberto Vieira
> Ferreira Monteiro
> Sent: 25 February 2007 16:52
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Random Integers
>
> Charles Annis, P.E. wrote:
>>
>> rpois(n, lambda)
>>
>> ... will do it.  But you should tell us something about how you  
>> want your
>> numbers to be distributed, since rpois() produces integers having a
> Poisson
>> distribution.
>>
> <nitpick>
> rpois does not generate random _integers_, it generates random
> _natural numbers_.
> </nitpick>
>
> The question should be more descriptive. "Random" is half of the  
> things
> we need to know, the other half is how deterministic you want your  
> integers.
>
> For example, if you want to generate random integers in such a way  
> that
> all integers have the same probability, then this can't be done.  
> OTOH, if
> you want to simulate random integers that distribute "like integers  
> appear
> in Nature", then it's still not precise, but there are serious  
> attempts
> to reproduce this behaviour. Check in the wikipedia  
> (www.wikipedia.org)
> those distributions: Zip's law, Zeta distribution, Benford's law,
> Zipf-Mandelbrot law. The problem is that all of them generate positive
> random integers, but it's not difficult to extrapolate them to  
> integers.
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


--
Andy Fugard, Postgraduate Research Student
Psychology (Room F15), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk


From francogrex at mail.com  Sun Feb 25 19:54:12 2007
From: francogrex at mail.com (francogrex)
Date: Sun, 25 Feb 2007 10:54:12 -0800 (PST)
Subject: [R] Woolf's test, Odds ratio, stratification
In-Reply-To: <1172357075.4865.69.camel@localhost.localdomain>
References: <9136458.post@talk.nabble.com>
	<1172357075.4865.69.camel@localhost.localdomain>
Message-ID: <9146699.post@talk.nabble.com>


Thanks Mark for taking the time to provide me with a very well detailed reply
and explanation. It helps a lot. Regards.
-- 
View this message in context: http://www.nabble.com/Woolf%27s-test%2C-Odds-ratio%2C-stratification-tf3284589.html#a9146699
Sent from the R help mailing list archive at Nabble.com.


From jholtman at gmail.com  Sun Feb 25 20:05:37 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 25 Feb 2007 14:05:37 -0500
Subject: [R] Writing integers in "write.matrix" function
In-Reply-To: <e3f2a5ab0702250920n2acf1d3ai97eed65213a8c347@mail.gmail.com>
References: <e3f2a5ab0702250920n2acf1d3ai97eed65213a8c347@mail.gmail.com>
Message-ID: <644e1f320702251105l47d650f8pc7a7c13bf64c6988@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/b09405e9/attachment.pl 

From julien at no-log.org  Sun Feb 25 20:51:19 2007
From: julien at no-log.org (Julien Barnier)
Date: Sun, 25 Feb 2007 20:51:19 +0100
Subject: [R] If you had just one book on R to buy...
Message-ID: <87r6seaudk.fsf@gnugnus.org>

Hi,

I am starting a new job as a study analyst for a social science
research unit. I would really like to use R as my main tool for data
manipulation and analysis. So I'd like to ask you, if you had just one
book on R to buy (or to keep), which one would it be ? I already
bought the Handbook of Statistical Analysis Using R, but I'd like to
have something more complete, both on the statistical point of view
and on R usage.

I thought that "Modern applied statistics with S-Plus" would be a good
choice, but maybe some of you could have interesting suggestions ?

Thanks in advance,

-- 
Julien


From a.fugard at ed.ac.uk  Sun Feb 25 20:58:09 2007
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Sun, 25 Feb 2007 19:58:09 +0000
Subject: [R] Repeated measures logistic regression
Message-ID: <7AA2D066-4D55-49FC-BEB9-993E3690C4CA@ed.ac.uk>

Dear all,

I'm struggling to find the best (set of?) function(s) to do repeated  
measures logistic regression on some data from a psychology experiment.

An artificial version of the data I've got is as follows.  Firstly,  
each participant filled in a questionnaire, the result of which is a  
score.

 > questionnaire
    ID Score
1   1     6
2   2     5
3   3     6
4   4     2
...

Secondly, each participant did a task which required a series of  
button-pushes.  The response is binary.  The factors CondA and CondB  
describe the structure of the stimulus:

 > experiment
     ID CondA CondB Response
1    1    a1    b1        1
2    1    a2    b2        0
3    1    a3    b1        0
4    1    a4    b2        0
5    1    a1    b1        1
6    1    a2    b2        0
7    1    a3    b1        0
8    1    a4    b2        0
9    2    a1    b1        1
10   2    a2    b2        0
11   2    a3    b1        0
12   2    a4    b2        0
13   2    a1    b1        1
14   2    a2    b2        0
15   2    a3    b1        0
16   2    a4    b2        0

I would like to model how someone's score on the questionnaire  
relates to the responses they give in the button-pushing.  I'm  
particularly interested in interactions between the type of the  
stimulus and the score.

I combined the experiment and the questionnaire dataframe with a  
merge so now there an additional column.

 > exp.q
     ID Score CondA CondB Response
1    1     6    a1    b1        1
2    1     6    a2    b2        0
3    1     6    a3    b1        0
4    1     6    a4    b2        0
5    1     6    a1    b1        1
6    1     6    a2    b2        0
7    1     6    a3    b1        0
8    1     6    a4    b2        0
9    2     5    a1    b1        1
10   2     5    a2    b2        0
11   2     5    a3    b1        0
12   2     5    a4    b2        0
...

Eventually, via glm, glmmPQL, and a few others, I ended up with  
lmer.  My questions follow.  I suspect (or hope) that I need to be  
pointed towards the relevant literature.  I own Faraway's "Extending  
the Linear Model with R" and Crawley's "Statistics: An Introduction  
using R".

1. Is the way I've combined the tables okay?  I'm concerned that the  
repetition of the score is Bad but can't think of any other way to  
code things.

2. Is lmer the most appropriate function to use?

3. If so, does the following call capture what I'm trying to model?

model1 = lmer(Response ~ CondA * CondB * Score + (1|Subject),
               data =exp.q,
               family = binomial)

I just want to tell lmer, "Look, this set of responses all comes from  
the same person: tell me the within-subject stuff that's going on and  
how that's affected by their score!"

4. Is there any way to do stepwise model simplification?  In the real  
data I have, there are several more predictors, including more than  
one questionnaire score and subscores.  I have specific hypotheses  
about what could be going on, so I can live with manual editing of  
the formulae, but it's nice for exploratory purposes to do stepwise  
simplification.

5. What's the best way to discover and report the relative  
contribution of each predictor?  I'm after an analogue of  
standardized betas (though I recently learned that they're thoroughly  
evil).

6. Is there anyway to get a p-value for goodness of fit?

Many thanks for any help,

Andy

--
Andy Fugard, Postgraduate Research Student
Psychology (Room F15), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk


From PAlspach at hortresearch.co.nz  Sun Feb 25 21:24:34 2007
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Mon, 26 Feb 2007 09:24:34 +1300
Subject: [R] If you had just one book on R to buy...
Message-ID: <EC0F8FF776F3F74E9C63CE16641C962801D9FF9F@AKLEXB02.hort.net.nz>


Julien

This is quite a common question.  Within R, try

RSiteSearch('one good book for R') 

and follow the threads ......

Peter Alspach


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Julien Barnier
> Sent: Monday, 26 February 2007 8:51 a.m.
> To: r-help at stat.math.ethz.ch
> Subject: [R] If you had just one book on R to buy...
> 
> Hi,
> 
> I am starting a new job as a study analyst for a social 
> science research unit. I would really like to use R as my 
> main tool for data manipulation and analysis. So I'd like to 
> ask you, if you had just one book on R to buy (or to keep), 
> which one would it be ? I already bought the Handbook of 
> Statistical Analysis Using R, but I'd like to have something 
> more complete, both on the statistical point of view and on R usage.
> 
> I thought that "Modern applied statistics with S-Plus" would 
> be a good choice, but maybe some of you could have 
> interesting suggestions ?
> 
> Thanks in advance,
> 
> --
> Julien
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}


From albmont at centroin.com.br  Sun Feb 25 21:45:50 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sun, 25 Feb 2007 20:45:50 +0000
Subject: [R] RSPython
In-Reply-To: <45E1D3C4.10409@wald.ucdavis.edu>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
	<200702251811.14087.albmont@centroin.com.br>
	<45E1D3C4.10409@wald.ucdavis.edu>
Message-ID: <200702252045.50757.albmont@centroin.com.br>

Duncan Temple Lang wrote:
>
> Well, we'll need to know in what ways it doesn't work and
> what operating system you are using, etc.
>
In python:

>>> import RS
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
ImportError: No module named RS

In R:

> library(RSPython)
Erro em ifelse(R.version$os == "Win32", ";", ":", ) :
        unused argument(s) ()
Erro em library(RSPython) : .First.lib falhou para 'RSPython'
>

Why they chose to give a partial translation of the error messages
in beyond my comprehension... A translation is:

> library(RSPython)
Error in ifelse(R.version$os == "Win32", ";", ":", ) :
        unused argument(s) ()
Error in library(RSPython) : .First.lib failed for 'RSPython'
>

Operating system is Linux.

> If you want to call R from Python, RPy is probably more straightforward.
>
Ok, I will try to find it.

Alberto Monteiro


From kubovy at virginia.edu  Sun Feb 25 21:45:24 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 25 Feb 2007 15:45:24 -0500
Subject: [R] mtext bold font problem
In-Reply-To: <814845.81329.qm@web60312.mail.yahoo.com>
References: <814845.81329.qm@web60312.mail.yahoo.com>
Message-ID: <38239A47-ACB0-4BB7-86A7-65A3FCF083CB@virginia.edu>

On Feb 24, 2007, at 6:12 PM, Joseph Retzer wrote:

> I'd like to create an mtext title which has certain words bold, the  
> rest not bold.

x <- 1:10
y <- x^2
plot(x, y, main = expression(paste(
     "Overall Satisfaction Call",
     bold(" Difference "),
     "vs. Overall Satisfaction Rep.",
     bold(" Difference")))
)

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From mkerekes at web.de  Sun Feb 25 17:02:52 2007
From: mkerekes at web.de (Monika Kerekes)
Date: Sun, 25 Feb 2007 17:02:52 +0100
Subject: [R] Macros in R
Message-ID: <E1HLLq2-0004T5-00@smtp05.web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/879d18f4/attachment.pl 

From albmont at centroin.com.br  Sun Feb 25 22:22:14 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sun, 25 Feb 2007 21:22:14 +0000
Subject: [R] RPy and the evil underscore
In-Reply-To: <200702251811.14087.albmont@centroin.com.br>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
	<200702251556.33019.albmont@centroin.com.br>
	<200702251811.14087.albmont@centroin.com.br>
Message-ID: <200702252122.15587.albmont@centroin.com.br>

I seems like I will join two threads :-)

Ok, RPy was installed (in Fedora Core 4, yum -y install rpy), and it 
is running. However, I have a doubt, and the (meagre) documentation
doesn't seem to address it.

In python, when I do this:

>>> import rpy
>>> rpy.r.setwd("/mypath")
>>> rpy.r.source("myfile.r")

Everything happens as expected. But now, there's
a problem if I try to use a function in myfile:

>>> x = my_function(1)
>>> x = r.my_function(1)
>>> x = rpy.my_function(1)
>>> x = rpy.r.my_function(1)

None of them work: the problem is that the _ is mistreated.
If the function has "." instead of "_", it works:

>>> x = rpy.r.my_function(1)

This is weird: I must write the R soutine with a ".", but then
rpy translates it to "_"!

Alberto Monteiro


From liuwensui at gmail.com  Sun Feb 25 22:22:09 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 25 Feb 2007 16:22:09 -0500
Subject: [R] If you had just one book on R to buy...
In-Reply-To: <87r6seaudk.fsf@gnugnus.org>
References: <87r6seaudk.fsf@gnugnus.org>
Message-ID: <1115a2b00702251322t736f1ee2o9901a8b9019d63dc@mail.gmail.com>

I have both handbook and MASS and think MASS is much better. But of
course, it also depends on how you want to use R or your previous
exposure to R.


On 2/25/07, Julien Barnier <julien at no-log.org> wrote:
> Hi,
>
> I am starting a new job as a study analyst for a social science
> research unit. I would really like to use R as my main tool for data
> manipulation and analysis. So I'd like to ask you, if you had just one
> book on R to buy (or to keep), which one would it be ? I already
> bought the Handbook of Statistical Analysis Using R, but I'd like to
> have something more complete, both on the statistical point of view
> and on R usage.
>
> I thought that "Modern applied statistics with S-Plus" would be a good
> choice, but maybe some of you could have interesting suggestions ?
>
> Thanks in advance,
>
> --
> Julien
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From wht_crl at yahoo.com  Sun Feb 25 22:33:47 2007
From: wht_crl at yahoo.com (carol white)
Date: Sun, 25 Feb 2007 13:33:47 -0800 (PST)
Subject: [R] patient risk score by cox regression
Message-ID: <394275.16539.qm@web62005.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070225/6f9b72e1/attachment.pl 

From tchur at optushome.com.au  Sun Feb 25 22:47:58 2007
From: tchur at optushome.com.au (Tim Churches)
Date: Mon, 26 Feb 2007 08:47:58 +1100
Subject: [R] RPy and the evil underscore
In-Reply-To: <200702252122.15587.albmont@centroin.com.br>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>	<200702251556.33019.albmont@centroin.com.br>	<200702251811.14087.albmont@centroin.com.br>
	<200702252122.15587.albmont@centroin.com.br>
Message-ID: <45E2040E.6050207@optushome.com.au>

Alberto Vieira Ferreira Monteiro wrote:
> I seems like I will join two threads :-)

Please address RPy-specific questions to the Rpy mailing list, where
they will be answered swiftly and without annoyance to everyone else on
this general r-help mailing list.

> Ok, RPy was installed (in Fedora Core 4, yum -y install rpy), and it 
> is running. However, I have a doubt, and the (meagre) documentation
> doesn't seem to address it.
>
> In python, when I do this:
> 
>>>> import rpy
>>>> rpy.r.setwd("/mypath")
>>>> rpy.r.source("myfile.r")
> 
> Everything happens as expected. But now, there's
> a problem if I try to use a function in myfile:
> 
>>>> x = my_function(1)
>>>> x = r.my_function(1)
>>>> x = rpy.my_function(1)
>>>> x = rpy.r.my_function(1)
> 
> None of them work: the problem is that the _ is mistreated.
> If the function has "." instead of "_", it works:
> 
>>>> x = rpy.r.my_function(1)
> 
> This is weird: I must write the R soutine with a ".", but then
> rpy translates it to "_"!

Object identifiers cannot begin with an underscore in R, but they can in
Python. To avoid having to confusingly special-case this difference, the
RPy designers elected to translate underscores in Python object names to
dots in R object names.

All this is clearly documented in the RPy manual at
http://rpy.sourceforge.net/rpy/doc/rpy_html/R-objects-look-up.html#R-objects-look-up

Tim C


From ggrothendieck at gmail.com  Sun Feb 25 22:54:55 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Feb 2007 16:54:55 -0500
Subject: [R] Macros in R
In-Reply-To: <E1HLLq2-0004T5-00@smtp05.web.de>
References: <E1HLLq2-0004T5-00@smtp05.web.de>
Message-ID: <971536df0702251354u538e3b8bra48165782acf6120@mail.gmail.com>

Its a FAQ.

http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-turn-a-string-into-a-variable_003f

On 2/25/07, Monika Kerekes <mkerekes at web.de> wrote:
> Dear members,
>
>
>
> I have started to work with R recently and there is one thing which I could
> not solve so far. I don't know how to define macros in R. The problem at
> hand is the following: I want R to go through a list of 1:54 and create the
> matrices input1, input2, input3 up to input54. I have tried the following:
>
>
>
> for ( i in 1:54) {
>
>      input[i] = matrix(nrow = 1, ncol = 107)
>
>      input[i][1,]=datset$variable
>
> }
>
>
>
> However, R never creates the required matrices. I have also tried to type
> input'i' and input$i, none of which worked. I would be very grateful for
> help as this is a basic question the answer of which is paramount to any
> further usage of the software.
>
>
>
> Thank you very much
>
>
>
> Monika
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ted.harding at nessie.mcc.ac.uk  Sun Feb 25 23:09:53 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 25 Feb 2007 22:09:53 -0000 (GMT)
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <20070224183739.00ac6395@triveni.stat.iastate.edu>
Message-ID: <XFMail.070225220953.ted.harding@nessie.mcc.ac.uk>


On 25-Feb-07 Ranjan Maitra wrote:
> "My" method is for the surface, not for the interior. The
> constraint d*X/|| \Gamma^{-1/2}X ||ensures the constraint, no?
> The uniformity is ensured by the density restricted to satisfy
> the constraint which makes it a constant.
> 
> Ranjan

OK -- I admit that I misread your notation in the first instance.
I now see that it generates points which lie on the surface of
the ellipsoid. Apologies!

However, your suggested procedure (reprodiced below) does not do
the required job of generating points which are uniformly
distributed over the surface/circumference of the ellipsoid/ellipse.

I have verified this empirically and mathematically for the ellipse
(see below).

> On Sat, 24 Feb 2007 22:49:25 -0000 (GMT) (Ted Harding)
> <ted.harding at nessie.mcc.ac.uk> wrote:
> 
>> On 24-Feb-07 Ranjan Maitra wrote:
>> > Hi,
>> > Sorry for being a late entrant to this thread, but let me see
>> > if I understand the problem.
>> > 
>> > The poster wants to sample from an ellipsoid. Let us call this
>> > ellipsoid X'\Gamma X - d^2= 0.
>> > 
>> > There is no loss in assuming that the center is zero, otherwise
>> > the same can be done.
>> > 
>> > Let us consider the case when Gamma = I first. 
>> > 
>> > Then, let X \sim N_p(0, I) (any radially symmetric distribution
>> > will do here), then d*X/||X|| is uniform on the sphere of
>> > radius d. 
>> > 
>> > How about imitating the same?
>> > 
>> > Let X \sim N_p(0, \Sigma), where \Sigma = \Gamma^{-1} then X
>> > restricted to X'\Gamma X = d^2 gives the required uniform
>> > density on the ellipsoid.
>> > 
>> > How do we get this easily? I don't think rejection works or
>> > is even necessary.
>> > 
>> > Isn't d*X / ||\Gamma^{1/2}X|| enough? Here \Gamma^{1/2} is
>> > the square root matrix of \Gamma.
>> > 
>> > Note that any distribution of the kind f(X'\Gamma X) would work,
>> > but the multivariate Gaussian is a convenient tool, for which
>> > two-lines of R code should be enough.

A. Verification that it does not generate uniformly distributed
   points along the circumference.

I am considering an ellipse, with vertical minor axis of length 2,
and horizontal minor axis of length 2*a = 10 (so a = 5).

Its equation is (x^2)/(a^2) + y^2 = 1 (so your d^2 = 1). Hence
(in your notation)

  Gamma = matrix(c(1/(a^2),0,0,1),nrow=2),

and

  Sigma = matrix(c(a^2,0,0,1), nrow=2).

The following R code generates the bivariate Z (I'm using Z instead
of your X in X ~ N(0,Sigma) because I want to discuss Z = (X,Y) later):

library(MASS)

a <- 5
Gamma    <- matrix(c(1(a^2),0,0,1),nrow=2)
Gamma0.5 <- matrix(c(1/a   ,0,0,1),nrow=2)
Sigma <- matrix(c(a^2,0,0,1),nrow=2) ## Same as Gamma^(-1)

plota <- function(u,v,a){
  print(plot(u,v,pch="+",xlim=c(-a,a),ylim=c(-a,a)))
}

N <- 20000
X <- mvrnorm(N,c(0,0),Sigma)

### Now I implement your "X restricted to X'*Gamma*X = d^2"
### leading up to your "d*X / ||\Gamma^{1/2}X||":

Y <- X %*% Gamma0.5
NormY <- sqrt(Y[,1]^2 + Y[,2]^2)
Z     <- cbind(X[,1]/NormY,X[,2]/NormY)

### And now it can be verified that these points indeed lie
### on the ellipsoid:

plota(Z[,1],Z[,2],a)
min((Z[,1]^2)/(a^2) + Z[,2]^2)
### [1] 1
max((Z[,1]^2)/(a^2) + Z[,2]^2)
### [1] 1

### Now, however, I dissect the circumference of the ellipsoid
### into segments of equal angular increment relative to the centre,
### count the numbers of the above points in each segment, and
### compute the resulting estimate of density in each segment
### by dividing the number by the length of the segment (computed
### assuming the segment is approximately a straight line):

M <- 500
t <- 2*pi*(1/M)*(0:M)
dN <- numeric(M)
ds <- numeric(M)

for(i in (1:M)){
  x0    <- a*cos(t[i]); x1 <- a*cos(t[i+1]);
  y0    <-   sin(t[i]); y1 <-   sin(t[i+1]);
  if(x1 < x0){tmp<-x1; x1 <- x0; x0 <- tmp}
  if(y1 < y0){tmp<-y1; y1 <- y0; y0 <- tmp}
  ds[i] <- sqrt((x1-x0)^2 + (y1-y0)^2);
  ix    <- ( (x0 < Z[,1])&(Z[,1] < x1) )&( (y0 < Z[,2])&(Z[,2] < y1) );
  dN[i] <- sum(ix)
}

X11()
plot(t[1:M],dN/ds,pch="+",ylim=c(0,max(dN/ds)))

### showing that there are very clear peaks at t [theta] = 0,
### pi/2, and again at pi == 0 -- i.e. the points are concentrated
### at the ends of the long axis (exactly as was the case with the
### method given on the URL that Roger Bivand suggested ("untested"!).
### The height of the peaks is 5 times the minimum, which is
### compatible with a uniform distribution on a circle which has
### been stretched sideways by a linear factor of 5 (see previously
### posted comments).

B. Theoretically wrong.

[NOTE: For the theoretical result used at the end, below, there is
 a useful note by Dave Rusin on the Web at

   http://www.math.niu.edu/~rusin/known-math/95/ellipse.rand

 I have made a PDF version of this, which is easier to read,
 if anyone would like a copy]

Suppose we generate Z = [X,Y] ~ N(0,Sigma) as above. Then

  "Gamma^{1/2}X" = [X/a, Y]

and

  "||\Gamma^{1/2}X||" = sqrt((X/a)^2 + Y^2) = K, say

and (since your d = 1)

  "d*X / ||\Gamma^{1/2}X||" = [X/K, Y/K]

Hence, if theta is the angular coordinate of [X,Y],

  tan(theta) = Y/X = (1/a)*Y/(X/a)

But now X/a, and Y, can be considered independent N(0,1) random
variables, and the angular coordinate, phi say, of [X/a,Y] will
be uniformaly distributed over [0, 1*pi].

**>Hence a*tan(theta) = tan(phi) where phi is uniformaly distributed
in your construction.<**

Now the NOTE given above derives a method of sampling an angle phi
from a non-uniform distribution such that X = a*cos(phi) and
Y = sin(phi) is uniformly distributed on the circumference of an
ellipse. For the present ellipse (X^2)/(a(^2) + Y^2 = 1, a direct
derivation is easy.

Let X = a*cos(phi), Y = sin(phi), so that (X^2(/(a^2) + Y^2 = 1.
Note that phi is not the angular coordinate of (X,Y), but it is
the angular coordiinate  of (X/a,Y), just like phi for your
method above.

For an element of length ds along the circumference,

  ds = sqrt(dx^2 + dy^2) = sqrt(a^2 * sin(phi)^2 + cos(phi)^2)*d.phi

     = sqrt(a^2 - (a^2 - 1)*cos(phi)^2)*d.phi

Since s is uniformly distributed, the probability density of phi
is given by

   f(phi) = sqrt( a^2 - (a^2 - 1)*cos(phi)^2 ) [****]

a non-uniform distribution, which is incompatible with the uniform
distribution of phi (see **>...<** above) in your method.


C. The flaw in your argument?

I think this arises when you say "The uniformity is ensured by the
density restricted to satisfy the constraint which makes it a constant".
Your construction does not restrict (condition) the point to lie
on the circumference: it projects the points from the interior
onto the circumference, which is a quite different matter.


D. How to proceed?

It is clear that this is not a straightforward thing to achieve.
Adopting the Dave Rusin method of sampling from the distribution
[***] of phi requires implementing a routine to geretae random
numbers from what one might call an "incomplete elliptic integral"
distribution. According to my search today of the R site, there
is as yet no explicit reference anywhere to "elliptic integral".
Something relevant may occur under some other name, of course;
but I have not located it.

And this is only for the ellipse! Next comes the ellpsoid ... !!

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Feb-07                                       Time: 22:06:13
------------------------------ XFMail ------------------------------


From rpichlova at yahoo.com  Mon Feb 26 00:32:45 2007
From: rpichlova at yahoo.com (Radka Ptacnikova)
Date: Sun, 25 Feb 2007 15:32:45 -0800 (PST)
Subject: [R] nested design in lme, need help with specifying model
Message-ID: <320609.29391.qm@web51302.mail.yahoo.com>

Hi,

I wonder if anyone can help me with specifying a right model for my analysis. I am a beginner to lme methods and though have spent already many hours studying from various books an on-line helps, I was unfortunately not able to find a solution to my problem on my own. 

Data structure:
I studied escape behavior of three species of a prey to a predator. The prey specimens (many) were in a vessel, together with one predator. Escape responses were video-recorded when a prey approached the predator close enough and jumped consequently away. Each set was run twice with a fresh predator and a fresh set of the prey specimens, leading to two replicates per treatment. Unequal number of shots (i.e. prey specimens) were analyzed in each of the two replicates for each of the three prey species (range 11-19). The data are therefore unbalanced and also variance for treatments/replicates is far from being homogeneous, so that a nested anova is not a good choice here. As the number of prey specimens was rather high, I assume that each shot represents a different prey individual. 

My questions:
1) Do the three prey species significantly differ in their escape response?
2) What was variability between replicates within a species and how much did it contribute to overall variability?

Now, to my best understanding, the model should be:

mod1<-lme(Escape.parameter~Species, random=~1|Species/Replicate)

as I am interested in Species as fixed effects and want to know variability caused by Replicates nested within Species as random effects. However, when running this model, I get 

Random effects:
 Formula: ~1 | Species
        (Intercept)
StdDev:    2.937479

 Formula: ~1 | Replicate %in% Species
        (Intercept) Residual
StdDev:    4.973931 4.266302

Fixed effects: Max_speed ~ Species 
                Value Std.Error        DF   t-value p-value
(Intercept) 23.792040  4.798143 39  4.958593       0
Spec2     -7.121766  6.747930   0 -1.055400     NaN
Spec3      -9.779830  6.725391  0 -1.454165     NaN

So I get variance within species and within replicates, but what the hell are these zero DF's, leading to zero p's and how should I interpret them?


Another model I tried was:
mod2<-lme(Escape.parameter~Species, random=~1|Replicate)

Random effects:
 Formula: ~1 | Replicate
         (Intercept) Residual
StdDev: 0.0002733313 5.180472

Fixed effects: Max_speed ~ Species
                              Value Std.Error        DF   t-value       p-value
(Intercept)                26.00364  1.561971  41  16.647963   0e+00
SpeciesSpec2          -7.93297  2.056430  41  -3.857641   4e-04
SpeciesSpec3        -11.81048  1.962713  41  -6.017425   0e+00

Alright, I get the among species differences, but I am confused here with the very low StdDev of Replicate as a random effect, since I know f.ex. from a plot, that it is relatively high. Which leads me to thinking, that something is wrong here. 

I'd appreciate any hints and suggestions.

Radka






 
____________________________________________________________________________________
Never Miss an Email


From albmont at centroin.com.br  Mon Feb 26 00:48:15 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sun, 25 Feb 2007 23:48:15 +0000
Subject: [R]  random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <XFMail.070225220953.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070225220953.ted.harding@nessie.mcc.ac.uk>
Message-ID: <200702252348.15167.albmont@centroin.com.br>

I guess this sample is required for some practical application, say a
simulation for something done over the Earth. Then, I also guess that
the sample does not have to be _absolutely_ exact, but a reasonable
approximation can do it. And the ellipsoid is a rotation ellipsoid.

This is my suggestion:

(1) Divide the Ellipsoid by latitudes in _n_ horizontal slices in such
a way that each slice can be considered "almost" spherical. Of course
here lies the problem: depending on the purpose of the simulation,
_n_ would be so big as to make it impractical

(2) Compute the area of each slice (there are formulas for that, whose
error is not very big - again, we rely on the purpose of the simulation)

(3) Chose a random slice based on weight = area

(4) Chose the random latitude by a uniform from the minimum to the
maximum latitude (a much better approximation would give
higher weight to the latitude closer to the equator)

(5) lon = 2 pi runif(1) # :-)

Now the question is: do you know the formulas to compute the area in (2)?
I know these formulas exist, I learned them in the last century, but I can't
remember them and I don't know how to find them.

Alberto Monteiro


From Scott.Williams at petermac.org  Mon Feb 26 08:55:11 2007
From: Scott.Williams at petermac.org (Williams Scott)
Date: Mon, 26 Feb 2007 18:55:11 +1100
Subject: [R] how to fill between 2 stair plots
Message-ID: <46B75B4A4A45914ABB0901364EFF4A209B0C45@PMC-EMAIL.petermac.org.au>


Hi all,

I want to create a simple plot with 2 type='s' lines on it:

plot(a, b, type='s')
lines(x, y, type='s') 

I wish to then fill the area between the curves with a colour to
accentuate the differences eg col=gray(0.95). I cant seem to come up
with a simple method for this. Any pointers in the right direction much
appreciated.

Cheers

Scott
_____________________________

Dr. Scott Williams

MBBS BScMed FRANZCR

Peter MacCallum Cancer Centre

Melbourne, Australia

scott.williams at petermac.org


From sergeyg at gmail.com  Mon Feb 26 09:14:17 2007
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Mon, 26 Feb 2007 09:14:17 +0100
Subject: [R] Automated figure production
Message-ID: <7cb007bd0702260014x6c960fc8o3fb6441d6936b444@mail.gmail.com>

Hello, everybody

Two questions:

1) I am new to maillists, and particularly to r-help maillist. Where
specifically do I go online to see my question and answers to it??? If
I use the searchable archives, or archives by Robert King, I see my
question but not the answers, though I know that at least one persion
posted the answer to r-help. Why do not I see the answers?

2)

I need to produce 104 figures. In each graphic window I want to plot 8
figures. How do I automate the process so that it opens 13 separate
graphic windows in R Gui and plots the figures? (I then paste them in
Word, one by one). Also, how do I save all 104 figures to one PDF
file?

Here is the code to produce the figures, and at this point I change
the j-index in the for-loop by hand, produce 8 figures, copy them to
Word, then change j to 9:16 etc...

#HOW TO PRODUCE ALL 104 GRAPHS AT ONCE

old.par<-par(no.readonly=TRUE)
par(mfrow=c(4,2))

for(j in 1:8) {
plot(Cleaned[zz[,j],4], type="b", main=long.names[j], xlab="Month",
ylab="MFR", col="blue")
abline(h=0, col="red")
}
par(old.par)


Thanks for your help!
Sergey

-- 
Laziness is nothing more than the habit of resting before you get tired.
- Jules Renard (writer)

Experience is one thing you can't get for nothing.
- Oscar Wilde (writer)

When you are finished changing, you're finished.
- Benjamin Franklin (President)


From P.Dalgaard at biostat.ku.dk  Mon Feb 26 09:41:48 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 26 Feb 2007 09:41:48 +0100
Subject: [R] how to fill between 2 stair plots
In-Reply-To: <46B75B4A4A45914ABB0901364EFF4A209B0C45@PMC-EMAIL.petermac.org.au>
References: <46B75B4A4A45914ABB0901364EFF4A209B0C45@PMC-EMAIL.petermac.org.au>
Message-ID: <45E29D4C.7040704@biostat.ku.dk>

Williams Scott wrote:
> Hi all,
>
> I want to create a simple plot with 2 type='s' lines on it:
>
> plot(a, b, type='s')
> lines(x, y, type='s') 
>
> I wish to then fill the area between the curves with a colour to
> accentuate the differences eg col=gray(0.95). I cant seem to come up
> with a simple method for this. Any pointers in the right direction much
> appreciated.
>
>   
I don't think there is a really simple method for this. I'd start with
converting the two 's' lines to ordinary lines along the lines of

N <- length(a)
a1 <- c(a[1],rep(a[-1],each=2),a[N]) # possibly a[N]+a_bit for the final
step)
b1 <- rep(b,each=2)

x1, y1 similarly, then

polygon(c(a1,rev(x1)),c(b1,rev(y1), col="grey")

(Did I confuse 's' and 'S'? Anyways, you get the idea)

> Cheers
>
> Scott
> _____________________________
>
> Dr. Scott Williams
>
> MBBS BScMed FRANZCR
>
> Peter MacCallum Cancer Centre
>
> Melbourne, Australia
>
> scott.williams at petermac.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From klaster at karlin.mff.cuni.cz  Mon Feb 26 09:49:42 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Mon, 26 Feb 2007 09:49:42 +0100
Subject: [R] how to fill between 2 stair plots
In-Reply-To: <46B75B4A4A45914ABB0901364EFF4A209B0C45@PMC-EMAIL.petermac.org.au>
References: <46B75B4A4A45914ABB0901364EFF4A209B0C45@PMC-EMAIL.petermac.org.au>
Message-ID: <45E29F26.5080103@karlin.mff.cuni.cz>

Williams Scott napsal(a):
> Hi all,
> 
> I want to create a simple plot with 2 type='s' lines on it:
> 
> plot(a, b, type='s')
> lines(x, y, type='s') 
> 
> I wish to then fill the area between the curves with a colour to
> accentuate the differences eg col=gray(0.95). I cant seem to come up
> with a simple method for this. Any pointers in the right direction much
> appreciated.
> 
> Cheers
> 
> Scott
> _____________________________
> 
> Dr. Scott Williams
> 
> MBBS BScMed FRANZCR
> 
> Peter MacCallum Cancer Centre
> 
> Melbourne, Australia
> 
> scott.williams at petermac.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
?polygon might be useful. See also demo(graphics)
Petr
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From petr.pikal at precheza.cz  Mon Feb 26 09:52:51 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 26 Feb 2007 09:52:51 +0100
Subject: [R] Automated figure production
In-Reply-To: <7cb007bd0702260014x6c960fc8o3fb6441d6936b444@mail.gmail.com>
Message-ID: <45E2ADF3.10065.8C0F39@localhost>



On 26 Feb 2007 at 9:14, Sergey Goriatchev wrote:

Date sent:      	Mon, 26 Feb 2007 09:14:17 +0100
From:           	"Sergey Goriatchev" <sergeyg at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Automated figure production

> Hello, everybody
> 
> Two questions:
> 
> 1) I am new to maillists, and particularly to r-help maillist. Where
> specifically do I go online to see my question and answers to it??? If
> I use the searchable archives, or archives by Robert King, I see my
> question but not the answers, though I know that at least one persion
> posted the answer to r-help. Why do not I see the answers?

You is close. Try click on latest R-help in Searchable R-list archive

> 
> 2)
> 
> I need to produce 104 figures. In each graphic window I want to plot 8
> figures. How do I automate the process so that it opens 13 separate
> graphic windows in R Gui and plots the figures? (I then paste them in
> Word, one by one). Also, how do I save all 104 figures to one PDF
> file?

see ?pdf, ?png 

> 
> Here is the code to produce the figures, and at this point I change
> the j-index in the for-loop by hand, produce 8 figures, copy them to
> Word, then change j to 9:16 etc...
> 
> #HOW TO PRODUCE ALL 104 GRAPHS AT ONCE
> 

e.g.
cycle
png(name.based.on.cycle.pointer, 800,800)
> old.par<-par(no.readonly=TRUE)
> par(mfrow=c(4,2))
> 
> for(j in 1:8) {
> plot(Cleaned[zz[,j],4], type="b", main=long.names[j], xlab="Month",
> ylab="MFR", col="blue") abline(h=0, col="red") } par(old.par)
> 

dev.off()
endcycle

or similar without cycle using pdf device see onefile=T option

HTH
Petr


> 
> Thanks for your help!
> Sergey
> 
> -- 
> Laziness is nothing more than the habit of resting before you get
> tired. - Jules Renard (writer)
> 
> Experience is one thing you can't get for nothing.
> - Oscar Wilde (writer)
> 
> When you are finished changing, you're finished.
> - Benjamin Franklin (President)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From carina.brehony at zoology.oxford.ac.uk  Mon Feb 26 11:06:01 2007
From: carina.brehony at zoology.oxford.ac.uk (Carina Brehony)
Date: Mon, 26 Feb 2007 10:06:01 -0000
Subject: [R] Chi Square with two tab-delimited text files
Message-ID: <002f01c7598d$b828f060$124901a3@eos>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/50c18fc3/attachment.pl 

From b.rowlingson at lancaster.ac.uk  Mon Feb 26 11:21:54 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 26 Feb 2007 10:21:54 +0000
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
Message-ID: <45E2B4C2.90406@lancaster.ac.uk>

hadley wickham wrote:
> What do you prefer/recommend for double-banger function names:
> 
>  1 scale.colour
>  2 scale_colour
>  3 scaleColour
> 
> 1 is more R-like, but conflicts with S3.  2 is a modern version of
> number 1, but not many packages use it.  Number 3 is more java-like.
> (I like number 2 best)

  Or you can be lisp-ish and use hyphens (or many other symbols) by quoting:

  > "scale-colour"=2
  > ls()
  [1] "scale-colour"

  but that requires further perversions:

  > get("scale-colour")
  [1] 2

  I like (3), aka camelCase - aka about a dozen other names: 
http://en.wikipedia.org/wiki/Camel_case - but that's mainly because its 
widely used in Python, and Python syntax is just marvellous. The more R 
syntax tends to Python syntax the better. Let's get rid of curly 
brackets and make whitespace significant...

  But I digress. As usual.

  ANytHiNg bUt sTUdLY cApS: http://en.wikipedia.org/wiki/StudlyCaps

Barry


From nilsson.henric at gmail.com  Mon Feb 26 11:40:43 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Mon, 26 Feb 2007 11:40:43 +0100 (CET)
Subject: [R] some caracter dont work with JGR
In-Reply-To: <200702231621.11884.chrysopa@gmail.com>
References: <200702231621.11884.chrysopa@gmail.com>
Message-ID: <19724.212.209.13.15.1172486443.squirrel@www.sorch.se>

Den Fr, 2007-02-23, 19:21 skrev Ronaldo Reis Junior:
> Hi,
>
> I testing JGR and I like, but my ~ caracter dont work. My keyboard is
> Brazilian ABNT2.
>
> The key is OK, only in JGR it dont work.
>
> Anybody have any idea about this?

Yes, and it's known problem -- see e.g.
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6371251.

It has been discussed on more than one occasion over at the RoSuDa devel
list (http://www.rosuda.org/lists.shtml) for which questions on JGR and
related projects are more appropriate than R-help.


HTH,
Henric



>
> Thanks
> Ronaldo
> --
> Mais variado que baldea??o em Cacequi.
> --
>> Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
> | `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
> |   `- Fone: (38) 3229-8190 | ronaldo.reis at unimontes.br |
> chrysopa at gmail.com
> | ICQ#: 5692561 | LinuxUser#: 205366
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From mdu at ceh.ac.uk  Mon Feb 26 12:12:36 2007
From: mdu at ceh.ac.uk (Mike Dunbar)
Date: Mon, 26 Feb 2007 11:12:36 +0000
Subject: [R] nested design in lme, need help with specifying model
Message-ID: <s5e2c0b4.073@wpo.nerc.ac.uk>

Dear Radka

I'm not sure I quite understand your design and quite where the nesting comes in.

But a quick suggestion is why are you adding species as random as well as fixed? I don't think you can do this or indeed should do it. I think this is why you get problems with your fixed effects. If you have 3 species then species ought to be fixed. Replicate is more the sort of effect that ought to be random, this ought to pick up the fact that prey within one run of the experiment won't be independent. But if you only have two replicates per treatment (species of prey?), then this will limit your ability to detect differences between species of prey, unless your within-replicate variation is very low. You can look at this very simply but not quite as powerfully by averaging the responses for each replicate and doing a non nested anova.

Re your second analysis, this seems along the right lines. How have you coded replicate? This may explain your results. Without more details on the plot you did its difficult to help further.

regards

Mike



>>> Radka Ptacnikova <rpichlova at yahoo.com> 25/02/2007 23:32 >>>
Hi,

I wonder if anyone can help me with specifying a right model for my analysis. I am a beginner to lme methods and though have spent already many hours studying from various books an on-line helps, I was unfortunately not able to find a solution to my problem on my own. 

Data structure:
I studied escape behavior of three species of a prey to a predator. The prey specimens (many) were in a vessel, together with one predator. Escape responses were video-recorded when a prey approached the predator close enough and jumped consequently away. Each set was run twice with a fresh predator and a fresh set of the prey specimens, leading to two replicates per treatment. Unequal number of shots (i.e. prey specimens) were analyzed in each of the two replicates for each of the three prey species (range 11-19). The data are therefore unbalanced and also variance for treatments/replicates is far from being homogeneous, so that a nested anova is not a good choice here. As the number of prey specimens was rather high, I assume that each shot represents a different prey individual. 

My questions:
1) Do the three prey species significantly differ in their escape response?
2) What was variability between replicates within a species and how much did it contribute to overall variability?

Now, to my best understanding, the model should be:

mod1<-lme(Escape.parameter~Species, random=~1|Species/Replicate)

as I am interested in Species as fixed effects and want to know variability caused by Replicates nested within Species as random effects. However, when running this model, I get 

Random effects:
 Formula: ~1 | Species
        (Intercept)
StdDev:    2.937479

 Formula: ~1 | Replicate %in% Species
        (Intercept) Residual
StdDev:    4.973931 4.266302

Fixed effects: Max_speed ~ Species 
                Value Std.Error        DF   t-value p-value
(Intercept) 23.792040  4.798143 39  4.958593       0
Spec2     -7.121766  6.747930   0 -1.055400     NaN
Spec3      -9.779830  6.725391  0 -1.454165     NaN

So I get variance within species and within replicates, but what the hell are these zero DF's, leading to zero p's and how should I interpret them?


Another model I tried was:
mod2<-lme(Escape.parameter~Species, random=~1|Replicate)

Random effects:
 Formula: ~1 | Replicate
         (Intercept) Residual
StdDev: 0.0002733313 5.180472

Fixed effects: Max_speed ~ Species
                              Value Std.Error        DF   t-value       p-value
(Intercept)                26.00364  1.561971  41  16.647963   0e+00
SpeciesSpec2          -7.93297  2.056430  41  -3.857641   4e-04
SpeciesSpec3        -11.81048  1.962713  41  -6.017425   0e+00

Alright, I get the among species differences, but I am confused here with the very low StdDev of Replicate as a random effect, since I know f.ex. from a plot, that it is relatively high. Which leads me to thinking, that something is wrong here. 

I'd appreciate any hints and suggestions.

Radka






 
____________________________________________________________________________________
Never Miss an Email

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

-- 
This message (and any attachments) is for the recipient only...{{dropped}}


From Serguei.Kaniovski at wifo.ac.at  Mon Feb 26 12:13:43 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Mon, 26 Feb 2007 12:13:43 +0100
Subject: [R] Add-up duplicates and merge
Message-ID: <OFC964EC63.26948770-ONC125728E.003DAE63-C125728E.003DAE6D@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/d2f88195/attachment.pl 

From anthony.brooks at csc.mrc.ac.uk  Mon Feb 26 12:35:01 2007
From: anthony.brooks at csc.mrc.ac.uk (Brooks, Anthony B)
Date: Mon, 26 Feb 2007 11:35:01 -0000
Subject: [R] PlotAffyRNAdeg on Estrogen Data
Message-ID: <320C29F789759249967AA22AA131694A971CFC@icex6.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/9ac6f75b/attachment.pl 

From Winfried.Theis at unilever.com  Mon Feb 26 12:36:59 2007
From: Winfried.Theis at unilever.com (Theis, Winfried)
Date: Mon, 26 Feb 2007 11:36:59 -0000
Subject: [R] Add-up duplicates and merge
In-Reply-To: <OFC964EC63.26948770-ONC125728E.003DAE63-C125728E.003DAE6D@wsr.ac.at>
Message-ID: <9AE166DBB565F746B1F5BFA7369EF8F71748BC@BRBSEVS20006.S2.MS.UNILEVER.COM>

Hello,

 for the first task change the names if they are character into a factor
and have a look at ?by 
For the merge you will need strsplit() on character vectors (so be
carefull to change the factor back to character) and merge() 

Regards,
Winfried 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Serguei Kaniovski
Sent: Monday, February 26, 2007 12:14 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Add-up duplicates and merge


Hello,

a have two matrices of data as below. I would like to add-up the
duplicate in terms of pair of names in rows, and then merge the values
in the second matrix to the pairs as two new variables x3 and x4.

Input

,x1,x2
jane.mike,31,43
jane.steve,32,2
jane.steve,5,3
jim.mike,76,5
jane.steve,4,4
mike.steve,54,7
mike.steve,5,7
jane.mike,7,8

and

,y
jane,0.3
jim,0.4
mike,0.1
carl,0.5
john,0.9
steve,0.4
dirk,0.2

Output:

,x1,x2,x3,x4
jane.mike,38,51,0.3,0.1
jane.steve,41,9,0.3,0.4
jim.mike,76,5,0.4,0.1
mike.steve,59,14,0.1,0.4

Any help appreciated,
Serguei
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From klaster at karlin.mff.cuni.cz  Mon Feb 26 12:50:28 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Mon, 26 Feb 2007 12:50:28 +0100
Subject: [R] Chi Square with two tab-delimited text files
In-Reply-To: <002f01c7598d$b828f060$124901a3@eos>
References: <002f01c7598d$b828f060$124901a3@eos>
Message-ID: <45E2C984.5070806@karlin.mff.cuni.cz>

Carina Brehony napsal(a):
> Hi,
> I want to do a chi square test and I have two tab delimited text files with
> Expected and Observed values to compare.  Each file contains only the values
<snip>

There are a lot of chi^2 tests, most of them compare O&E quantities and 
it is not clear which one you want to use. I'd guess a goodness of fit 
test, but who knows? See ?chisq.test and the examples given there. It 
also tells you that the y-argument is ignored if x is a matrix (that's 
probably the reason why you get different results using read.table and 
scan).
Petr
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From carina.brehony at zoology.oxford.ac.uk  Mon Feb 26 12:57:44 2007
From: carina.brehony at zoology.oxford.ac.uk (Carina Brehony)
Date: Mon, 26 Feb 2007 11:57:44 -0000
Subject: [R] Chi Square with two tab-delimited text files
In-Reply-To: <45E2C984.5070806@karlin.mff.cuni.cz>
References: <002f01c7598d$b828f060$124901a3@eos>
	<45E2C984.5070806@karlin.mff.cuni.cz>
Message-ID: <004201c7599d$539268b0$124901a3@eos>

Yes, I would like to do a goodness-of-fit test.

-----Original Message-----
From: Petr Klasterecky [mailto:klaster at karlin.mff.cuni.cz] 
Sent: 26 February 2007 11:50
To: Carina Brehony
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Chi Square with two tab-delimited text files

Carina Brehony napsal(a):
> Hi,
> I want to do a chi square test and I have two tab delimited text files
with
> Expected and Observed values to compare.  Each file contains only the
values
<snip>

There are a lot of chi^2 tests, most of them compare O&E quantities and 
it is not clear which one you want to use. I'd guess a goodness of fit 
test, but who knows? See ?chisq.test and the examples given there. It 
also tells you that the y-argument is ignored if x is a matrix (that's 
probably the reason why you get different results using read.table and 
scan).
Petr
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From rdiaz02 at gmail.com  Mon Feb 26 13:09:14 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Mon, 26 Feb 2007 13:09:14 +0100
Subject: [R] If you had just one book on R to buy...
In-Reply-To: <87r6seaudk.fsf@gnugnus.org>
References: <87r6seaudk.fsf@gnugnus.org>
Message-ID: <624934630702260409u2166e6bpb642f5fb5889eb04@mail.gmail.com>

On 2/25/07, Julien Barnier <julien at no-log.org> wrote:
> Hi,
>
> I am starting a new job as a study analyst for a social science
> research unit. I would really like to use R as my main tool for data
> manipulation and analysis. So I'd like to ask you, if you had just one
> book on R to buy (or to keep), which one would it be ? I already
> bought the Handbook of Statistical Analysis Using R, but I'd like to
> have something more complete, both on the statistical point of view
> and on R usage.
>
> I thought that "Modern applied statistics with S-Plus" would be a good
> choice, but maybe some of you could have interesting suggestions ?
>


Dear Julien,

I'd definitely go for MASS if you already have Handbook. MASS is an
awesome book, but you did not tell us anything about your background
(stats begginners, for instance, sometimes get lost in MASS, because
that is not the target audience). In terms of books of this level,
MASS is unique. (There are more specific books for certain topics,
such as mixed models, etc; but for a wide coverage, I'd go with MASS).

HTH,

R.



> Thanks in advance,
>
> --
> Julien
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From mothsailor at googlemail.com  Mon Feb 26 13:11:53 2007
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 26 Feb 2007 12:11:53 +0000
Subject: [R] Chi Square with two tab-delimited text files
In-Reply-To: <004201c7599d$539268b0$124901a3@eos>
References: <002f01c7598d$b828f060$124901a3@eos>
	<45E2C984.5070806@karlin.mff.cuni.cz>
	<004201c7599d$539268b0$124901a3@eos>
Message-ID: <815b70590702260411i43529590q9373189db157a445@mail.gmail.com>

It's a bit difficult to advise without knowing what the rows and
columns represent, but why not just calculate the statistic yourself,
given that you already have observed and expected values?  For
example:

chi2 <- sum((y-x)^2/x)



On 26/02/07, Carina Brehony <carina.brehony at zoology.oxford.ac.uk> wrote:
> Yes, I would like to do a goodness-of-fit test.
>
> -----Original Message-----
> From: Petr Klasterecky [mailto:klaster at karlin.mff.cuni.cz]
> Sent: 26 February 2007 11:50
> To: Carina Brehony
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Chi Square with two tab-delimited text files
>
> Carina Brehony napsal(a):
> > Hi,
> > I want to do a chi square test and I have two tab delimited text files
> with
> > Expected and Observed values to compare.  Each file contains only the
> values
> <snip>
>
> There are a lot of chi^2 tests, most of them compare O&E quantities and
> it is not clear which one you want to use. I'd guess a goodness of fit
> test, but who knows? See ?chisq.test and the examples given there. It
> also tells you that the y-argument is ignored if x is a matrix (that's
> probably the reason why you get different results using read.table and
> scan).
> Petr
> --
> Petr Klasterecky
> Dept. of Probability and Statistics
> Charles University in Prague
> Czech Republic
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From carina.brehony at zoology.oxford.ac.uk  Mon Feb 26 13:28:43 2007
From: carina.brehony at zoology.oxford.ac.uk (Carina Brehony)
Date: Mon, 26 Feb 2007 12:28:43 -0000
Subject: [R] Chi Square with two tab-delimited text files
In-Reply-To: <815b70590702260411i43529590q9373189db157a445@mail.gmail.com>
References: <002f01c7598d$b828f060$124901a3@eos>
	<45E2C984.5070806@karlin.mff.cuni.cz>
	<004201c7599d$539268b0$124901a3@eos>
	<815b70590702260411i43529590q9373189db157a445@mail.gmail.com>
Message-ID: <004f01c759a1$a799a6e0$124901a3@eos>

Hi,
The files look like below and the rows and columns are numbers of genetic
types e.g. row1 is type 4; column1 is type A. So for, row1:column1 cell
there are 78 type 4/type A combinations.  I hope this makes sense!



	78	500	18	6	0	4	0	1	6
1	1	0	0	0	1	0	0	0	0
0	1	0	0	0	0	0	2	1	0
0	0	1	0	0	0	0	23	0	0
0	7	0	0	7	0	0	0	6	0
8	0	0	0	0	0	0	14	0	0
0	0	0	0	0	0	0	5	0	0
0	0	0	0	45	0	0	0	0	0
0	0	0	0	0	0	0	3	0	40
0	0	0	0	0	0	0	0	0	0
0	0	0	12	0	0	0	0	8	4
0	0	0	0	0	0	....etc...	





-----Original Message-----
From: David Barron [mailto:mothsailor at googlemail.com] 
Sent: 26 February 2007 12:12
To: Carina Brehony; r-help
Subject: Re: [R] Chi Square with two tab-delimited text files

It's a bit difficult to advise without knowing what the rows and
columns represent, but why not just calculate the statistic yourself,
given that you already have observed and expected values?  For
example:

chi2 <- sum((y-x)^2/x)



On 26/02/07, Carina Brehony <carina.brehony at zoology.oxford.ac.uk> wrote:
> Yes, I would like to do a goodness-of-fit test.
>
> -----Original Message-----
> From: Petr Klasterecky [mailto:klaster at karlin.mff.cuni.cz]
> Sent: 26 February 2007 11:50
> To: Carina Brehony
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Chi Square with two tab-delimited text files
>
> Carina Brehony napsal(a):
> > Hi,
> > I want to do a chi square test and I have two tab delimited text files
> with
> > Expected and Observed values to compare.  Each file contains only the
> values
> <snip>
>
> There are a lot of chi^2 tests, most of them compare O&E quantities and
> it is not clear which one you want to use. I'd guess a goodness of fit
> test, but who knows? See ?chisq.test and the examples given there. It
> also tells you that the y-argument is ignored if x is a matrix (that's
> probably the reason why you get different results using read.table and
> scan).
> Petr
> --
> Petr Klasterecky
> Dept. of Probability and Statistics
> Charles University in Prague
> Czech Republic
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mothsailor at googlemail.com  Mon Feb 26 14:38:48 2007
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 26 Feb 2007 13:38:48 +0000
Subject: [R] Chi Square with two tab-delimited text files
In-Reply-To: <004f01c759a1$a799a6e0$124901a3@eos>
References: <002f01c7598d$b828f060$124901a3@eos>
	<45E2C984.5070806@karlin.mff.cuni.cz>
	<004201c7599d$539268b0$124901a3@eos>
	<815b70590702260411i43529590q9373189db157a445@mail.gmail.com>
	<004f01c759a1$a799a6e0$124901a3@eos>
Message-ID: <815b70590702260538k5ebf6c1alfae136986e680b37@mail.gmail.com>

In that case, you can just ignore the expected values and use the
observed values in the chisq.test.  The reason you got a p value of 1
before is because the second argument was ignored, and so you did a
chi square test on the expected values alone.

If you have loaded the obseved values into a matrix y using read.table
as in your first example, then just use chisq.test(y).  But you should
notice that you have a lot of zero cells and so probably lots of small
expected values, which is a problem for the chi square test.



On 26/02/07, Carina Brehony <carina.brehony at zoology.oxford.ac.uk> wrote:
> Hi,
> The files look like below and the rows and columns are numbers of genetic
> types e.g. row1 is type 4; column1 is type A. So for, row1:column1 cell
> there are 78 type 4/type A combinations.  I hope this makes sense!
>
>
>
>         78      500     18      6       0       4       0       1       6
> 1       1       0       0       0       1       0       0       0       0
> 0       1       0       0       0       0       0       2       1       0
> 0       0       1       0       0       0       0       23      0       0
> 0       7       0       0       7       0       0       0       6       0
> 8       0       0       0       0       0       0       14      0       0
> 0       0       0       0       0       0       0       5       0       0
> 0       0       0       0       45      0       0       0       0       0
> 0       0       0       0       0       0       0       3       0       40
> 0       0       0       0       0       0       0       0       0       0
> 0       0       0       12      0       0       0       0       8       4
> 0       0       0       0       0       0       ....etc...
>
>
>
>
>
> -----Original Message-----
> From: David Barron [mailto:mothsailor at googlemail.com]
> Sent: 26 February 2007 12:12
> To: Carina Brehony; r-help
> Subject: Re: [R] Chi Square with two tab-delimited text files
>
> It's a bit difficult to advise without knowing what the rows and
> columns represent, but why not just calculate the statistic yourself,
> given that you already have observed and expected values?  For
> example:
>
> chi2 <- sum((y-x)^2/x)
>
>
>
> On 26/02/07, Carina Brehony <carina.brehony at zoology.oxford.ac.uk> wrote:
> > Yes, I would like to do a goodness-of-fit test.
> >
> > -----Original Message-----
> > From: Petr Klasterecky [mailto:klaster at karlin.mff.cuni.cz]
> > Sent: 26 February 2007 11:50
> > To: Carina Brehony
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Chi Square with two tab-delimited text files
> >
> > Carina Brehony napsal(a):
> > > Hi,
> > > I want to do a chi square test and I have two tab delimited text files
> > with
> > > Expected and Observed values to compare.  Each file contains only the
> > values
> > <snip>
> >
> > There are a lot of chi^2 tests, most of them compare O&E quantities and
> > it is not clear which one you want to use. I'd guess a goodness of fit
> > test, but who knows? See ?chisq.test and the examples given there. It
> > also tells you that the y-argument is ignored if x is a matrix (that's
> > probably the reason why you get different results using read.table and
> > scan).
> > Petr
> > --
> > Petr Klasterecky
> > Dept. of Probability and Statistics
> > Charles University in Prague
> > Czech Republic
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
>
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From carina.brehony at zoology.oxford.ac.uk  Mon Feb 26 15:24:41 2007
From: carina.brehony at zoology.oxford.ac.uk (Carina Brehony)
Date: Mon, 26 Feb 2007 14:24:41 -0000
Subject: [R] Chi Square with two tab-delimited text files
In-Reply-To: <815b70590702260538k5ebf6c1alfae136986e680b37@mail.gmail.com>
References: <002f01c7598d$b828f060$124901a3@eos>
	<45E2C984.5070806@karlin.mff.cuni.cz>
	<004201c7599d$539268b0$124901a3@eos>
	<815b70590702260411i43529590q9373189db157a445@mail.gmail.com>
	<004f01c759a1$a799a6e0$124901a3@eos>
	<815b70590702260538k5ebf6c1alfae136986e680b37@mail.gmail.com>
Message-ID: <005001c759b1$dab27380$124901a3@eos>

Hi,
Thanks for the input.  I have tried the test again just using the Observed
values and the read.table() function and get this:

data:  y 
X-squared = NaN, df = 5405, p-value = NA

Warning message:
Chi-squared approximation may be incorrect in: chisq.test(y)


So it doesn't seem to like it!  I guess the zeroes are a problem for it.  Is
there another way around? Do I need to have the totals of each column and
row in the file also?

Thanks,
Carina


From jonathan.lees at unc.edu  Mon Feb 26 15:25:36 2007
From: jonathan.lees at unc.edu (Jonathan Lees)
Date: Mon, 26 Feb 2007 09:25:36 -0500
Subject: [R] PLotting R graphics/symbols without user x-y scaling
Message-ID: <45E2EDE0.5060305@unc.edu>


Is it possible to add lines or other
user defined graphics
to a plot in R that does not depend on
the user scale for the plot?

For example I have a plot
plot(x,y)
and I want to add some graphic that is
scaled in inches or cm but I do not want the
graphic to change when the x-y scales are
changed - like a thermometer, scale bar or
other symbol -
How does one do this?

I want to build my own library of glyphs to add to plots
but I do not know how to plot them when their
size is independent of the device/user coordinates.

Is it possible to add to the list
of symbols in the function symbols()
other than:
  _circles_, _squares_, _rectangles_, _stars_, _thermometers_, and
      _boxplots_

can I make my own symbols and have symbols call these?


Thanks-


-- 
Jonathan M. Lees
Professor
THE UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL
Department of Geological Sciences
Campus Box #3315
Chapel Hill, NC  27599-3315
TEL: (919) 962-0695
FAX: (919) 966-4519
jonathan.lees at unc.edu
http://www.unc.edu/~leesj


From jmacdon at med.umich.edu  Mon Feb 26 15:07:15 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Mon, 26 Feb 2007 09:07:15 -0500
Subject: [R] PlotAffyRNAdeg on Estrogen Data
In-Reply-To: <320C29F789759249967AA22AA131694A971CFC@icex6.ic.ac.uk>
References: <320C29F789759249967AA22AA131694A971CFC@icex6.ic.ac.uk>
Message-ID: <45E2E993.8090004@med.umich.edu>

Hi Tony,

This question concerns a Bioconductor package, so is best asked on the 
BioC mailing list instead of R-help.

Best,

Jim

Brooks, Anthony B wrote:
> Hi everyone,
> 
> I'm trying to generate an RNA degradation plot of the Estrogen example
> data plot, but seem to get an error. I've tried defining an ylim value,
> ylim=c(0,30) , but it doesn't seem to work either.
> 
>  
> 
> My code is as follows:
> 
>  
> 
> 
>>RNAdeg<-AffyRNAdeg(Data)
> 
> 
>>png(DegLoc, width=720, height=720)
> 
> 
>>par(ann=FALSE)
> 
> 
>>par(mar=c(3,3,0.1,0.1))
> 
> 
>>plotAffyRNAdeg(RNAdeg,col=cols, cex.axis=1.2)
> 
> 
> Error in plot.window(xlim, ylim, log, asp, ...) : 
> 
>         need finite 'ylim' values
> 
> 
>>dev.off()
> 
> 
> null device 
> 
>           1
> 
>  
> 
> Thanks in advance
> 
> Tony
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From johannes_graumann at web.de  Mon Feb 26 16:25:21 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 26 Feb 2007 16:25:21 +0100
Subject: [R] Test of Presence Matrix HOWTO?
Message-ID: <eruu51$6rq$1@sea.gmane.org>

Hello,

Imagine 3 lists like so:

> a <- list("A","B","C","D")
> b <- list("A","B","E","F")
> c <- list("A","C","E","G")

What I need (vennDiagram) is a matrix characterizing with 1 or 0 whether any
given member is present or not like so:
     x1 x2 x3
[1,]  1  1  1
[2,]  1  1  0
[3,]  1  0  1
[4,]  1  0  0
[5,]  0  1  1
[6,]  0  1  0
[7,]  0  0  1

(where the rows represent "A"-"G" and the columns a-c, respectively).

> table(c(a,b,c))
will give me a quick answer for the "1 1 1" case, but how to deal with the
other cases efficiently without looping over each string and looking for
membership %in% each list?

Thanks for enlightening the learning,

Joh


From ThadenJohnJ at uams.edu  Mon Feb 26 16:35:39 2007
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Mon, 26 Feb 2007 09:35:39 -0600
Subject: [R] someattributes
References: <mailman.9.1172401202.464.r-help@stat.math.ethz.ch>
Message-ID: <B1614B0C915A654A9C29BB71DA80E0DD1E968A@MAIL2.ad.uams.edu>

I'd like to use someattributes(), as described 
in documentation for R version 2.4.1 (windows build)  

>help("attributes")

however, someattributes() does not seem to exist.

> someattributes()
Error: could not find function "someattributes"

Is this true or am I doing something wrong?

-John

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From Thierry.ONKELINX at inbo.be  Mon Feb 26 16:38:25 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 26 Feb 2007 16:38:25 +0100
Subject: [R] Test of Presence Matrix HOWTO?
In-Reply-To: <eruu51$6rq$1@sea.gmane.org>
Message-ID: <2E9C414912813E4EB981326983E0A104029C406F@inboexch.inbo.be>

> a <- c("A","B","C","D")
> b <- c("A","B","E","F")
> c <- c("A","C","E","G")
> Df <- cbind(a, b, c)
> apply(Df, 2, function(x)(LETTERS[1:7] %in% x))
         a     b     c
[1,]  TRUE  TRUE  TRUE
[2,]  TRUE  TRUE FALSE
[3,]  TRUE FALSE  TRUE
[4,]  TRUE FALSE FALSE
[5,] FALSE  TRUE  TRUE
[6,] FALSE  TRUE FALSE
[7,] FALSE FALSE  TRUE
> 
> apply(Df, 2, function(x)(as.numeric(LETTERS[1:7] %in% x)))
     a b c
[1,] 1 1 1
[2,] 1 1 0
[3,] 1 0 1
[4,] 1 0 0
[5,] 0 1 1
[6,] 0 1 0
[7,] 0 0 1


Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch [mailto:r-help-
> bounces op stat.math.ethz.ch] Namens Johannes Graumann
> Verzonden: maandag 26 februari 2007 16:25
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] Test of Presence Matrix HOWTO?
> 
> Hello,
> 
> Imagine 3 lists like so:
> 
> > a <- list("A","B","C","D")
> > b <- list("A","B","E","F")
> > c <- list("A","C","E","G")
> 
> What I need (vennDiagram) is a matrix characterizing with 1 or 0
whether
> any
> given member is present or not like so:
>      x1 x2 x3
> [1,]  1  1  1
> [2,]  1  1  0
> [3,]  1  0  1
> [4,]  1  0  0
> [5,]  0  1  1
> [6,]  0  1  0
> [7,]  0  0  1
> 
> (where the rows represent "A"-"G" and the columns a-c, respectively).
> 
> > table(c(a,b,c))
> will give me a quick answer for the "1 1 1" case, but how to deal with
the
> other cases efficiently without looping over each string and looking
for
> membership %in% each list?
> 
> Thanks for enlightening the learning,
> 
> Joh
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Mon Feb 26 16:45:13 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 26 Feb 2007 16:45:13 +0100
Subject: [R] Test of Presence Matrix HOWTO?
References: <eruu51$6rq$1@sea.gmane.org>
Message-ID: <00ab01c759bd$1a7ed840$0540210a@www.domain>

you can use something like the following:

a <- list("A","B","C","D")
b <- list("A","B","E","F")
c <- list("A","C","E","G")

#####################

abc <- list(a, b, c)
unq.abc <- unique(unlist(abc))

out.lis <- lapply(abc, "%in%", x = unq.abc)
out.lis
lapply(out.lis, as.numeric)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Johannes Graumann" <johannes_graumann at web.de>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, February 26, 2007 4:25 PM
Subject: [R] Test of Presence Matrix HOWTO?


> Hello,
>
> Imagine 3 lists like so:
>
>> a <- list("A","B","C","D")
>> b <- list("A","B","E","F")
>> c <- list("A","C","E","G")
>
> What I need (vennDiagram) is a matrix characterizing with 1 or 0 
> whether any
> given member is present or not like so:
>     x1 x2 x3
> [1,]  1  1  1
> [2,]  1  1  0
> [3,]  1  0  1
> [4,]  1  0  0
> [5,]  0  1  1
> [6,]  0  1  0
> [7,]  0  0  1
>
> (where the rows represent "A"-"G" and the columns a-c, 
> respectively).
>
>> table(c(a,b,c))
> will give me a quick answer for the "1 1 1" case, but how to deal 
> with the
> other cases efficiently without looping over each string and looking 
> for
> membership %in% each list?
>
> Thanks for enlightening the learning,
>
> Joh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From dieter.menne at menne-biomed.de  Mon Feb 26 17:27:51 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 26 Feb 2007 17:27:51 +0100
Subject: [R] LD50 contrasts with lmer/lme4
Message-ID: <LPEJLJACLINDNMBMFAFIMEFKCHAA.dieter.menne@menne-biomed.de>

Dear R-list,

I have a data set from 20 pigs, each of which is tested at crossed 9 doses
(logdose -4:4) and 3 skin treatment substances when exposed to a standard
polluted environment. So there are 27 patches on each pig. The response is
irritation=yes/no.

I want to determine "equally effective 50% doses" (similar to old LD50), and
to test the treatments against each other. I am looking for something like
dose.p in MASS generalized to lmer (or glmmPQL or whatever). The direct as
output by lmer are not useful, because saying "30% irritation with A and 40%
with B at dose xx" has less meaning than giving "equivalent effective
doses".

Dieter

----- Simulated data -----
library(lme4)
animal = data.frame(ID = as.factor(1:20), da = rnorm(1:20))
treat = data.frame(treat=c('A','B','C'), treatoff=c(1,2,1.5),
       treatslope = c(0.5,0.6,0.7))

gr = expand.grid(animal=animal$ID,treat=treat$treat,logdose=c(-4:4))
gr$resp = as.integer(treat$treatoff[gr$treat]+
  treat$treatslope[gr$treat]*gr$logdose+
  animal$da[gr$animal] +  rnorm(nrow(gr),0,2) >0)

gr.lmer = lmer(resp ~ treat*logdose+(1|animal),data=gr,family=binomial)
summary(gr.lmer)

------- Output
Fixed effects:
               Estimate Std. Error z value Pr(>|z|)
(Intercept)      0.9553     0.3074    3.11   0.0019 **
treatB           0.8793     0.3313    2.65   0.0079 **
treatC           0.5516     0.3077    1.79   0.0730 .
logdose          0.3733     0.0774    4.82  1.4e-06 ***
treatB:logdose   0.3081     0.1323    2.33   0.0198 *
treatC:logdose   0.2666     0.1249    2.13   0.0328 *

----- Goal
                Value SD  p
50% logdose (A-B)  xx   xx  xx
50% logdose (A-C)  yy   yy  yy


From h.wickham at gmail.com  Mon Feb 26 17:28:06 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 26 Feb 2007 10:28:06 -0600
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
Message-ID: <f8e6ff050702260828h49de771fgd38deb679de8e4c8@mail.gmail.com>

Thanks to every one who contributed - there definitely isn't a
consensus, but perhaps a slight preference towards number 3
(camelCase).  I'm not sure yet if this beats out my personal
preference for number 2.

Hadley

On 2/25/07, hadley wickham <h.wickham at gmail.com> wrote:
> What do you prefer/recommend for double-banger function names:
>
>  1 scale.colour
>  2 scale_colour
>  3 scaleColour
>
> 1 is more R-like, but conflicts with S3.  2 is a modern version of
> number 1, but not many packages use it.  Number 3 is more java-like.
> (I like number 2 best)
>
> Any suggestions?
>
> Thanks,
>
> Hadley
>


From skiadas at hanover.edu  Mon Feb 26 17:31:55 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 26 Feb 2007 11:31:55 -0500
Subject: [R] someattributes
In-Reply-To: <B1614B0C915A654A9C29BB71DA80E0DD1E968A@MAIL2.ad.uams.edu>
References: <mailman.9.1172401202.464.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD1E968A@MAIL2.ad.uams.edu>
Message-ID: <E55D5743-F26E-4113-9203-21EAA6C422D2@hanover.edu>


On Feb 26, 2007, at 10:35 AM, Thaden, John J wrote:

> I'd like to use someattributes(), as described
> in documentation for R version 2.4.1 (windows build)
>
>> help("attributes")
>
> however, someattributes() does not seem to exist.
>
>> someattributes()
> Error: could not find function "someattributes"
>
> Is this true or am I doing something wrong?

My help shows it as "moreattributes", not "someattributes". (MacOSX,  
though doesn't sound like it should be platform-specific).
> -John

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From jafarikia at gmail.com  Mon Feb 26 18:11:54 2007
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Mon, 26 Feb 2007 12:11:54 -0500
Subject: [R] Barplot Graph
Message-ID: <e3f2a5ab0702260911m5bd90166o5f982a682a9c0cde@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/fe5eae9e/attachment.pl 

From Serguei.Kaniovski at wifo.ac.at  Mon Feb 26 18:18:20 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Mon, 26 Feb 2007 18:18:20 +0100
Subject: [R] Adding duplicates by rows
Message-ID: <OF64CFF31A.26B0B0FF-ONC125728E.005F1049-C125728E.005F1050@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/c05b752d/attachment.pl 

From Andy.Bunn at wwu.edu  Mon Feb 26 18:29:39 2007
From: Andy.Bunn at wwu.edu (Andy Bunn)
Date: Mon, 26 Feb 2007 09:29:39 -0800
Subject: [R] Partial whitening of time series?
Message-ID: <B786254B2435F94E808B17CEC2A432F70737D577@EVS1.univ.dir.wwu.edu>

I have a time series with a one year lag, ar=0.5. The series has some
interesting events that disappear when the series is whitened (i.e.,
fitting an AR process and looking at the residuals). I'd like to remove
the autocorrelation in stages to see the effect on the time series. Is
there a way to specify the autocorrelation term while fitting an AR
process? 

For instance, given the following:

x <- arima.sim(model = list(order = c(1,0,0), ar = 0.5), n = 500,
sd=0.25)

Can I filter x in a way that the autocorrelation at lag one is 0.4, then
0.3, 0.2, 0.1, until I get to a clean series equivalent to:

y <- arima(x, order = c(1,0,0))$resid

Thanks in advance, 
Andy


From walter345 at yahoo.com  Mon Feb 26 18:37:56 2007
From: walter345 at yahoo.com (Walter345)
Date: Mon, 26 Feb 2007 09:37:56 -0800 (PST)
Subject: [R] survival analysis using rpart
Message-ID: <9163329.post@talk.nabble.com>


Hello,

I use rpart to predict survival time and have a problem in interpreting the
output of ?estimated rate?. Here is an example of what I do:

> stagec <-
> read.table("http://www.stanford.edu/class/stats202/DATA/stagec.data", 
> col.names=c("pgtime", "pgstat", "age","eet", "g2", "grade", "gleason",
> "ploidy"))

> fit <- rpart(Surv(pgtime, pgstat) ~ age + eet + g2 + grade + gleason +
> ploidy, data=stagec)


Result:

1) root 146 195.411600 1.0000000  
   2) grade< 2.5 61  45.021520 0.3624701  
     4) g2< 11.36 33   9.120116 0.1225562 *
     5) g2>=11.36 28  27.804100 0.7335298  
      10) gleason< 5.5 20  14.376900 0.5292190 *
      11) gleason>=5.5 8  11.201470 1.3083680 *
   3) grade>=2.5 85 125.327400 1.6190620  
     6) age>=56.5 75 104.154700 1.4287310  
      12) gleason< 7.5 50  66.701410 1.1431320 *
      13) gleason>=7.5 25  33.993130 2.0355220  
        26) g2>=15.29 13  16.555970 1.3494740 *
        27) g2< 15.29 12  14.220260 2.9210480 *
     7) age< 56.5 10  15.522810 3.1977430 *

Let?s look at the terminal node 4:

#	PGTIME	PGSTAGE	AGE	EET	G2	GRADE	GLEASON	PLOIDY
1	8.657084	0	70	1	4.43	1	3	1
2	16.70088	0	56	2	5.29	1	3	1
3	3.162217	1	62	2	3.57	2	4	1
4	10.20123	0	63	2	5.14	2	5	1
5	4.479124	0	63	2	5.75	2	5	1
6	6.516084	0	66	2	5.92	2	5	1
7	4.936345	0	67	2	6.41	2	5	1
8	10.79808	0	72	1	6.68	2	NA	1
9	9.174537	0	62	1	6.74	2	5	1
10	10.87474	0	72	2	6.8	2	5	1
11	7.028062	0	52	2	7.15	2	7	1
12	11.36481	0	59	2	7.61	2	5	1
13	10.17659	0	64	1	7.61	2	NA	1
14	6.96783	0	67	2	7.78	2	6	1
15	10.61738	0	55	2	7.81	2	5	1
16	6.510609	0	70	1	7.88	2	6	1
17	10.36276	0	55	2	8.1	2	5	1
18	6.694045	0	54	2	8.11	2	4	1
19	11.718	0	61	2	8.4	2	5	1
20	7.301847	0	69	2	8.46	2	5	1
21	6.067077	0	69	2	8.58	2	6	1
22	8.353182	0	59	2	8.76	2	6	1
23	5.541409	0	59	1	9.01	2	5	1
24	5.492128	0	61	2	9.42	2	5	1
25	7.208761	0	63	1	9.76	2	5	1
26	6.004106	0	52	2	9.9	2	4	1
27	5.664613	0	71	1	10.16	2	6	1
28	6.130047	0	64	2	10.26	2	4	1
29	9.812457	0	64	1	10.51	2	5	1
30	6.275154	0	62	2	10.82	2	6	1
31	9.253935	0	61	2	11.23	2	5	1
32	5.201916	0	54	2	11.35	2	6	1
33	6.22861	0	65	2	11.35	2	5	1

Here we have 33 observations and 1 event. The ?estimated rate? is 0.1225562.
My questions are:

(1) Is the ?estimated rate? the estimated hazard rate ratio? 
(2) How does rpart calculate this rate?
(3) Suppose I use xpred.rpart(fit, xval=10) to perform 10-fold
cross-validation using (a) the complete stagec data set and (b) only a
subset of it, say, using the columns Age, EET, and G2 only. For the i-th
patient, I am likely to obtain a different estimated rate. How can I
meaningfully compare both rates? How can say which one is ?better?? 

Thanks a lot for all comments!
Walter





-- 
View this message in context: http://www.nabble.com/survival-analysis-using-rpart-tf3294276.html#a9163329
Sent from the R help mailing list archive at Nabble.com.


From ccleland at optonline.net  Mon Feb 26 18:51:06 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 26 Feb 2007 12:51:06 -0500
Subject: [R] Adding duplicates by rows
In-Reply-To: <OF64CFF31A.26B0B0FF-ONC125728E.005F1049-C125728E.005F1050@wsr.ac.at>
References: <OF64CFF31A.26B0B0FF-ONC125728E.005F1049-C125728E.005F1050@wsr.ac.at>
Message-ID: <45E31E0A.7070400@optonline.net>

Serguei Kaniovski wrote:
> Hi,
> 
> I am trying to add duplicates of matrix "mat" by row. Commands
> 
> subset(mat,duplicated(rownames(mat)))
> 
> or
> 
> mat[which(duplicated(rownames(mat))),]
> 
> return only half of the required indices. How can I find the remaining
> ones, ie the matches, so that I can add them up?

mat <- matrix(runif(70), ncol=5)
rownames(mat) <- c("Z", rep(LETTERS[1:6], each=2), "G")

  There is probably a more elegant way, but this seems to do what you want:

mat[rownames(mat) %in% names(which(table(rownames(mat)) > 1)),]

  Also, have you considered aggregate()?

aggregate(mat, list(ROW = rownames(mat)), sum)

> Thanks,
> Serguei
> 
> ___________________________________________________________________
> 
> Austrian Institute of Economic Research (WIFO)
> 
> Name: Serguei Kaniovski       P.O.Box 91
> Tel.: +43-1-7982601-231             Arsenal Objekt 20
> Fax: +43-1-7989386                  1103 Vienna, Austria
> Mail: Serguei.Kaniovski at wifo.ac.at  A-1030 Wien
> 
> http://www.wifo.ac.at/Serguei.Kaniovski
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From liuwensui at gmail.com  Mon Feb 26 19:14:35 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 26 Feb 2007 13:14:35 -0500
Subject: [R] Partial whitening of time series?
In-Reply-To: <B786254B2435F94E808B17CEC2A432F70737D577@EVS1.univ.dir.wwu.edu>
References: <B786254B2435F94E808B17CEC2A432F70737D577@EVS1.univ.dir.wwu.edu>
Message-ID: <1115a2b00702261014r26ba7a5ic088691b0785b9ec@mail.gmail.com>

andy,

if your model is Xt = 0.5 * Xt-1 + e, then it should have
Xt = 0.1 * Xt-1 + 0.4 * Xt-1 + e
(Xt - 0.1*Xt-1) = 0.4 * Xt-1 + e

so what you need to do is to substract part of lag from your series.
it is just my $0.02.

On 2/26/07, Andy Bunn <Andy.Bunn at wwu.edu> wrote:
> I have a time series with a one year lag, ar=0.5. The series has some
> interesting events that disappear when the series is whitened (i.e.,
> fitting an AR process and looking at the residuals). I'd like to remove
> the autocorrelation in stages to see the effect on the time series. Is
> there a way to specify the autocorrelation term while fitting an AR
> process?
>
> For instance, given the following:
>
> x <- arima.sim(model = list(order = c(1,0,0), ar = 0.5), n = 500,
> sd=0.25)
>
> Can I filter x in a way that the autocorrelation at lag one is 0.4, then
> 0.3, 0.2, 0.1, until I get to a clean series equivalent to:
>
> y <- arima(x, order = c(1,0,0))$resid
>
> Thanks in advance,
> Andy
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From kaustubhp_in at yahoo.com  Mon Feb 26 19:43:21 2007
From: kaustubhp_in at yahoo.com (Kaustubh Patil)
Date: Mon, 26 Feb 2007 10:43:21 -0800 (PST)
Subject: [R] eigenvalue ordering
Message-ID: <404546.22722.qm@web61121.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/b844d4c1/attachment.pl 

From albmont at centroin.com.br  Mon Feb 26 19:58:58 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Mon, 26 Feb 2007 16:58:58 -0200
Subject: [R] eigenvalue ordering
In-Reply-To: <404546.22722.qm@web61121.mail.yahoo.com>
References: <404546.22722.qm@web61121.mail.yahoo.com>
Message-ID: <20070226185649.M83387@centroin.com.br>

Kaustubh Patil wrote:
> 
>  Is it possible to get unordered eigenvalues and eigenvectors of a 
> symmetric matrix in R?
> 
Yes, see help("eigen").

If you are strict about the unordered part, do a sample(set, size)
to randomize the eigenvalues.

Alberto Monteiro


From p.dalgaard at biostat.ku.dk  Mon Feb 26 20:22:50 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 26 Feb 2007 20:22:50 +0100
Subject: [R] eigenvalue ordering
In-Reply-To: <20070226185649.M83387@centroin.com.br>
References: <404546.22722.qm@web61121.mail.yahoo.com>
	<20070226185649.M83387@centroin.com.br>
Message-ID: <45E3338A.9050101@biostat.ku.dk>

Alberto Monteiro wrote:
> Kaustubh Patil wrote:
>   
>>  Is it possible to get unordered eigenvalues and eigenvectors of a 
>> symmetric matrix in R?
> Yes, see help("eigen").
>   
Er, where do you see anything about (un)order? As far as I know, there's 
no "natural" ordering of eigenvalues and eigenvalue algorithms generally 
find them  in  either increasing or decreasing order (or closest to 
specified value).
> If you are strict about the unordered part, do a sample(set, size)
> to randomize the eigenvalues.
>


From macq at llnl.gov  Mon Feb 26 20:19:50 2007
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 26 Feb 2007 11:19:50 -0800
Subject: [R] Macros in R
In-Reply-To: <E1HLLq2-0004T5-00@smtp05.web.de>
References: <E1HLLq2-0004T5-00@smtp05.web.de>
Message-ID: <p0623090fc208e0f592d1@[128.115.153.6]>

If I understand the question correctly, I would do this:


for (i in 1:54)   assign(  paste('input',i,sep='') ,  matrix( 
dataset$variable, nrow=1)   )


You now have 54 matrices, named input1, input2, ... input54, each 
having 1 row and as many columns as dataset$variable is long.
(also, they're identical, since all are created from the same object, 
dataset$variable)

See,  of course, the help page for assign() to see why this works.

However, I do wonder, in the bigger picture of what you're trying to 
do, whether there isn't a better way. For example, why matrices, 
since they all have only one row?

-Don

At 5:02 PM +0100 2/25/07, Monika Kerekes wrote:
>Dear members,
>
>
>
>I have started to work with R recently and there is one thing which I could
>not solve so far. I don't know how to define macros in R. The problem at
>hand is the following: I want R to go through a list of 1:54 and create the
>matrices input1, input2, input3 up to input54. I have tried the following:
>
>
>
>for ( i in 1:54) {
>
>       input[i] = matrix(nrow = 1, ncol = 107)
>
>       input[i][1,]=datset$variable
>
>}
>
>
>
>However, R never creates the required matrices. I have also tried to type
>input'i' and input$i, none of which worked. I would be very grateful for
>help as this is a basic question the answer of which is paramount to any
>further usage of the software.
>
>
>
>Thank you very much
>
>
>
>Monika
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From albmont at centroin.com.br  Mon Feb 26 20:33:31 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Mon, 26 Feb 2007 17:33:31 -0200
Subject: [R] eigenvalue ordering
In-Reply-To: <45E3338A.9050101@biostat.ku.dk>
References: <404546.22722.qm@web61121.mail.yahoo.com>
	<20070226185649.M83387@centroin.com.br>
	<45E3338A.9050101@biostat.ku.dk>
Message-ID: <20070226193055.M88239@centroin.com.br>


Peter Dalgaard wrote:
>
>>>  Is it possible to get unordered eigenvalues and eigenvectors of a 
>>> symmetric matrix in R?
>>
>> Yes, see help("eigen").
>   
> Er, where do you see anything about (un)order? As far as I know, 
> there's no "natural" ordering of eigenvalues and eigenvalue 
> algorithms generally find them  in  either increasing or decreasing 
> order (or closest to specified value).
>
"eigen" orders the values. From help("eigen"):

  values: a vector containing the p eigenvalues of 'x', sorted in
          _decreasing_ order, according to 'Mod(values)' in the
          asymmetric case when they might be complex (even for real
          matrices).  For real asymmetric matrices the vector will be
          complex only if complex conjugate pairs of eigenvalues are
          detected. 

So, if you are strict about getting unordered eigenvalues,
you must shuffle them :-)

Alberto Monteiro


From jrkrideau at yahoo.ca  Mon Feb 26 20:56:07 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 26 Feb 2007 14:56:07 -0500 (EST)
Subject: [R] 2 data frames - list in one out put  , matrix in another ??
Message-ID: <504697.36932.qm@web32808.mail.mud.yahoo.com>

I have two more or less parallel dataframes that are
giving me different results on one subset of
variables.  I know that I assembled the 2 dataframes
slightly differently but I don't see why I am getting
this result because one set of variables are labelled
and the other is not. Variable names are the same,
etc.  as far as I can acertain.  The only diffference
seems to be that bdata variables are labelled.  

About now I really don't care which I get but I would
like them to be the same.  Can anyone suggest what I
am doing wrong or should be looking at?

Windows XP , R 2.4.1 Using Hmisc and gtools as well as
the basic R installation.  

Problem

load(adata)
fn1 <- function(x) {table(x)}
jj <-apply(adata[,110:127], 2, fn1)

OUTPUT jj is aa list of 18 tables
Examine a variable:
  >  typeof(adata$act.toy)
[1] "integer"
>     class(adata$act.toy)
[1] "integer"


load(bdata
fn1 <- function(x) {table(x)}
kk <-apply(bdata[,94:111], 2, fn1)

OUTPUT jj is a matrix 2 X 18
>   class(bdata$act.toy)
[1] "labelled"
>    typeof(bdata$act.toy)
[1] "integer"


From ThadenJohnJ at uams.edu  Mon Feb 26 21:00:27 2007
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Mon, 26 Feb 2007 14:00:27 -0600
Subject: [R] someattributes
References: <mailman.9.1172401202.464.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD1E968A@MAIL2.ad.uams.edu>
	<E55D5743-F26E-4113-9203-21EAA6C422D2@hanover.edu>
Message-ID: <B1614B0C915A654A9C29BB71DA80E0DD1E968C@MAIL2.ad.uams.edu>

I had written

>> ...someattributes() does not seem to exist.

And Haris Skiadas replied

> My help shows it as "moreattributes", not 
> "someattributes". (MacOSX), though doesn't 
> sound like it should be platform-specific).

Thanks for correcting me.  Actually, my windows
R documentation says "mostattributes()", but it
makes no difference -- none of the three show
up as function names or R objects.

-John 

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From ahailu9 at gmail.com  Mon Feb 26 21:03:31 2007
From: ahailu9 at gmail.com (A Hailu)
Date: Tue, 27 Feb 2007 05:03:31 +0900
Subject: [R] returns from dnorm and dmvnorm
Message-ID: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/1b3ef5d4/attachment.pl 

From dingjun_cn at yahoo.com  Mon Feb 26 21:03:48 2007
From: dingjun_cn at yahoo.com (Jun Ding)
Date: Mon, 26 Feb 2007 12:03:48 -0800 (PST)
Subject: [R] hierarchical clustering using cutree
Message-ID: <772898.56375.qm@web81015.mail.mud.yahoo.com>

Hi Everyone, 

I am doing hierarchical clustering analysis and have a
question regarding "cutree". 

I am doing things like this:

hc <- hclust(dist(X))
a <- cutree(hc, k=2)

Basically "a" is a vector containing the assignments
of 1 or 2 for each sample. May I know how "cutree"
decides to assign 1 and 2's to each sample (in other
words, how clusters 1 and 2 are decided)? I am having
the feeling that the first sample will always be
assigned to Cluster 1, but I am not sure about this. 

Thank you!

Best,
Jun 


 
____________________________________________________________________________________
Looking for earth-friendly autos? 
Browse Top Cars by "Green Rating" at Yahoo! Autos' Green Center.


From skiadas at hanover.edu  Mon Feb 26 21:09:15 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 26 Feb 2007 15:09:15 -0500
Subject: [R] someattributes
In-Reply-To: <B1614B0C915A654A9C29BB71DA80E0DD1E968C@MAIL2.ad.uams.edu>
References: <mailman.9.1172401202.464.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD1E968A@MAIL2.ad.uams.edu>
	<E55D5743-F26E-4113-9203-21EAA6C422D2@hanover.edu>
	<B1614B0C915A654A9C29BB71DA80E0DD1E968C@MAIL2.ad.uams.edu>
Message-ID: <9E6A6648-6114-44DD-A632-2D7B3DBD1243@hanover.edu>

On Feb 26, 2007, at 3:00 PM, Thaden, John J wrote:

> Thanks for correcting me.  Actually, my windows
> R documentation says "mostattributes()", but it
> makes no difference -- none of the three show
> up as function names or R objects.

That's because there is no "mostattributes" function, it only works  
as an assignment:

?"mostattributes<-"

Example:

 > x <- c(2,3,4)
 > mostattributes(x) <- list(foo="bar")
 > x
[1] 2 3 4
attr(,"foo")
[1] "bar"

> -John

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From bcarvalh at jhsph.edu  Mon Feb 26 21:13:05 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 26 Feb 2007 15:13:05 -0500
Subject: [R] returns from dnorm and dmvnorm
In-Reply-To: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>
References: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>
Message-ID: <46532646-C3A6-4F63-8B07-FF0C0402267D@jhsph.edu>

well, nobody said that the density must be smaller than 1, right? :-)

it's just the value of the normal density function at the point you  
asked. you may try doing that by hand and, with the correct math,  
you'll get the same thing.

b

On Feb 26, 2007, at 3:03 PM, A Hailu wrote:

> Hi All,
> Why would calls to dnorm and dmvnorm return values that are above  
> 1? For
> example,
>> dnorm(0.00003,mean=0, sd=0.1)
> [1] 3.989423
>
> This is happening on two different installations of R that I have.
>
> Thank you.
>
> Hailu


From ggrothendieck at gmail.com  Mon Feb 26 21:23:56 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Feb 2007 15:23:56 -0500
Subject: [R] someattributes
In-Reply-To: <B1614B0C915A654A9C29BB71DA80E0DD1E968C@MAIL2.ad.uams.edu>
References: <mailman.9.1172401202.464.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD1E968A@MAIL2.ad.uams.edu>
	<E55D5743-F26E-4113-9203-21EAA6C422D2@hanover.edu>
	<B1614B0C915A654A9C29BB71DA80E0DD1E968C@MAIL2.ad.uams.edu>
Message-ID: <971536df0702261223i2bfd28c6y5553f0c747082254@mail.gmail.com>

On 2/26/07, Thaden, John J <ThadenJohnJ at uams.edu> wrote:
> I had written
>
> >> ...someattributes() does not seem to exist.
>
> And Haris Skiadas replied
>
> > My help shows it as "moreattributes", not
> > "someattributes". (MacOSX), though doesn't
> > sound like it should be platform-specific).
>
> Thanks for correcting me.  Actually, my windows
> R documentation says "mostattributes()", but it
> makes no difference -- none of the three show
> up as function names or R objects.

Try:

?"mostattributes<-"


From ThadenJohnJ at uams.edu  Mon Feb 26 21:24:30 2007
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Mon, 26 Feb 2007 14:24:30 -0600
Subject: [R] someattributes (actually, mostattributes)
References: <mailman.9.1172401202.464.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD1E968A@MAIL2.ad.uams.edu>
	<E55D5743-F26E-4113-9203-21EAA6C422D2@hanover.edu>
	<B1614B0C915A654A9C29BB71DA80E0DD1E968C@MAIL2.ad.uams.edu>
	<9E6A6648-6114-44DD-A632-2D7B3DBD1243@hanover.edu>
Message-ID: <B1614B0C915A654A9C29BB71DA80E0DD1E968F@MAIL2.ad.uams.edu>

Haris Skiadas replied

>> Thanks for correcting me.  Actually, my windows
>> R documentation says "mostattributes()", but it
>> makes no difference -- none of the three show
>> up as function names or R objects.

> That's because there is no "mostattributes" function,
> it only works as an assignment:
> 
> ?"mostattributes<-"

Thanks. Obviously I need to learn about assignments that
are not R objects.

-John





Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From rvaradhan at jhmi.edu  Mon Feb 26 21:26:59 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 26 Feb 2007 15:26:59 -0500
Subject: [R] returns from dnorm and dmvnorm
In-Reply-To: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>
References: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>
Message-ID: <000601c759e4$783e41b0$7c94100a@win.ad.jhu.edu>

I guarantee that it would also happen on all future versions of R.

Why would you expect density to be smaller than 1?  The only constraints on
density are that (a) it is non-negative and (b) it integrates to one. The
smaller the variance, the greater the density is around its center.  Density
can be made to become arbitrarily large by letting the variance gets close
to zero, and in the limit you will obtain Dirac's delta function.  


Ravi. 

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of A Hailu
Sent: Monday, February 26, 2007 3:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] returns from dnorm and dmvnorm

Hi All,
Why would calls to dnorm and dmvnorm return values that are above 1? For
example,
> dnorm(0.00003,mean=0, sd=0.1)
[1] 3.989423

This is happening on two different installations of R that I have.

Thank you.

Hailu

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From skiadas at hanover.edu  Mon Feb 26 21:36:38 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 26 Feb 2007 15:36:38 -0500
Subject: [R] returns from dnorm and dmvnorm
In-Reply-To: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>
References: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>
Message-ID: <4F1BC63E-871C-4D64-AA29-F374DB5BB50D@hanover.edu>

On Feb 26, 2007, at 3:03 PM, A Hailu wrote:
> Hi All,
> Why would calls to dnorm and dmvnorm return values that are above  
> 1? For
> example,
>> dnorm(0.00003,mean=0, sd=0.1)
> [1] 3.989423

Because dnorm gives you the density function, whose integral is the  
distribution function, which is likely what you want. Try:

pnorm(0.00003,mean=0, sd=0.1)

> This is happening on two different installations of R that I have.
>
> Thank you.
>
> Hailu

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From ahailu9 at gmail.com  Mon Feb 26 21:44:57 2007
From: ahailu9 at gmail.com (A Hailu)
Date: Tue, 27 Feb 2007 05:44:57 +0900
Subject: [R] returns from dnorm and dmvnorm
In-Reply-To: <46532646-C3A6-4F63-8B07-FF0C0402267D@jhsph.edu>
References: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>
	<46532646-C3A6-4F63-8B07-FF0C0402267D@jhsph.edu>
Message-ID: <e75403920702261244q31fd124eyb38cfe29629a8a21@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/7ddccfdb/attachment.pl 

From Andy.Bunn at wwu.edu  Mon Feb 26 21:45:01 2007
From: Andy.Bunn at wwu.edu (Andy Bunn)
Date: Mon, 26 Feb 2007 12:45:01 -0800
Subject: [R] Partial whitening of time series?
In-Reply-To: <1115a2b00702261014r26ba7a5ic088691b0785b9ec@mail.gmail.com>
References: <B786254B2435F94E808B17CEC2A432F70737D577@EVS1.univ.dir.wwu.edu>
	<1115a2b00702261014r26ba7a5ic088691b0785b9ec@mail.gmail.com>
Message-ID: <B786254B2435F94E808B17CEC2A432F70737D8E8@EVS1.univ.dir.wwu.edu>

Thanks, I wasn't thinking real clearly when I pressed 'send'. All
figured out now. -A

-----Original Message-----
From: Wensui Liu [mailto:liuwensui at gmail.com] 
Sent: Monday, February 26, 2007 10:15 AM
To: Andy Bunn
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Partial whitening of time series?

andy,

if your model is Xt = 0.5 * Xt-1 + e, then it should have
Xt = 0.1 * Xt-1 + 0.4 * Xt-1 + e
(Xt - 0.1*Xt-1) = 0.4 * Xt-1 + e

so what you need to do is to substract part of lag from your series.
it is just my $0.02.

On 2/26/07, Andy Bunn <Andy.Bunn at wwu.edu> wrote:
> I have a time series with a one year lag, ar=0.5. The series has some
> interesting events that disappear when the series is whitened (i.e.,
> fitting an AR process and looking at the residuals). I'd like to
remove
> the autocorrelation in stages to see the effect on the time series. Is
> there a way to specify the autocorrelation term while fitting an AR
> process?
>
> For instance, given the following:
>
> x <- arima.sim(model = list(order = c(1,0,0), ar = 0.5), n = 500,
> sd=0.25)
>
> Can I filter x in a way that the autocorrelation at lag one is 0.4,
then
> 0.3, 0.2, 0.1, until I get to a clean series equivalent to:
>
> y <- arima(x, order = c(1,0,0))$resid
>
> Thanks in advance,
> Andy
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From ahailu9 at gmail.com  Mon Feb 26 21:49:25 2007
From: ahailu9 at gmail.com (A Hailu)
Date: Tue, 27 Feb 2007 05:49:25 +0900
Subject: [R] returns from dnorm and dmvnorm
In-Reply-To: <4F1BC63E-871C-4D64-AA29-F374DB5BB50D@hanover.edu>
References: <e75403920702261203u62b938bh43ce6cef5f04926e@mail.gmail.com>
	<4F1BC63E-871C-4D64-AA29-F374DB5BB50D@hanover.edu>
Message-ID: <e75403920702261249t298faf69u7c8e71e1358e38e3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/3412020f/attachment.pl 

From smckinney at bccrc.ca  Mon Feb 26 22:27:25 2007
From: smckinney at bccrc.ca (Steven McKinney)
Date: Mon, 26 Feb 2007 13:27:25 -0800
Subject: [R] Double-banger function names: preferences and suggestions
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com><200702251556.33019.albmont@centroin.com.br>
	<1172420902.4865.86.camel@localhost.localdomain>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0235FB23@crcmail1.BCCRC.CA>


The underscore versus left-arrow 
conundrum has its roots in the evolution
of ASCII during the middle of the last
century. 

Some Teletype machines in the 1970s (when the
S language was being developed) still had a
left arrow, and its ASCII code was used in S
as a one keystroke convenience for the assignment
operator.  The left arrow symbol was then
removed from most keyboards/printers/fontsets
and replaced by the underscore.  Thus the
underscore remained as a one keystroke assignment
operator.


See e.g.

http://www.wps.com/projects/codes/index.html#GRPH


LEFT-ARROW, ?
UNDERSCORE, _

One of the graphical codes, left-arrow mutated to the 
underscore of ASCII-1967. It may have had earlier, 
or other, meanings, but for some early programming 
languages it was "assignment", eg.

      c ? b + a

"C is assigned the sum of B and A".




Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney at bccrc.ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch on behalf of Marc Schwartz
Sent: Sun 2/25/2007 8:28 AM
To: Alberto Vieira Ferreira Monteiro
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Double-banger function names: preferences and suggestions
 
On Sun, 2007-02-25 at 15:56 +0000, Alberto Vieira Ferreira Monteiro
wrote:
> hadley wickham wrote:
> >
> > What do you prefer/recommend for double-banger function names:
> >
> >  1 scale.colour
> >  2 scale_colour
> >  3 scaleColour
> >
> > 1 is more R-like, but conflicts with S3.  2 is a modern version of
> > number 1, but not many packages use it.  Number 3 is more java-like.
> > (I like number 2 best)
> >
> > Any suggestions?
> >
> I always prefer 2, but this would make it non-portable to S-Plus. S-Plus
> has a bug, where _ is the equivalent to <- (why would they do this? I
> prefer to think it's stupidity and not villainy)

That's not a bug. If you search the archives of both the S-PLUS list and
the R lists, you will see highly energized discussion on the use of the
underscore operator.

In R, the use of '_' was allowed for assignment up until version 1.8.0
when:

DEPRECATED & DEFUNCT

    o	The assignment operator `_' has been removed.


and subsequently allowed in names in version 1.9.0 when:

 o	Underscore '_' is now allowed in syntactically valid names, and
	make.names() no longer changes underscores.  Very old code
	that makes use of underscore for assignment may now give
	confusing error messages.


Not to further contribute to the dialog on 'style', but to further
contribute ;-), for those who have coded in the Windows environment (ie.
C, VBA, etc.) the extension of sorts to number 3 is of course "Hungarian
Notation", named after Charles Simonyi, originally at Xerox PARC and
later senior developer/architect at MS. The extension was the inclusion
of the data type prefix, such as fnScaleColour to indicate that this was
a function, with the name using caps to make words more distinct.

And no, I'm not advocating that use...I have been guilty myself of using
variants of 1 and 3, perhaps driven by my circulating caffeine levels as
much as anything else.

HTH,

Marc Schwartz
<Off to go remove 12 inches of snow from the driveway and sidewalk...oy>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Sixtease at gmail.com  Mon Feb 26 23:25:56 2007
From: Sixtease at gmail.com (Sixtease)
Date: Mon, 26 Feb 2007 14:25:56 -0800 (PST)
Subject: [R] training svm
Message-ID: <9170716.post@talk.nabble.com>


Hello. I'm new to R and I'm trying to solve a classification problem. I have
a training dataset of about 40,000 rows and 50 columns. When I try to train
support vector machine, it gives me this error after a few seconds:

 Error in predict.svm(ret, xhold) : Model is empty!

This is the code I use:

 ne_span_data <- as.matrix(read.table('ne_span.data.R.txt', header=TRUE,
row.names='id'))
 library('e1071')
 svm_ne_span_model <- svm(NE_type ~ . , ne_span_data)

it gives me:
Error in predict.svm(ret, xhold) : Model is empty!

A line from the ne_span.data.R.txt file:
 svt OTHER N N I S 2 NA NA NA NA NA A NA NA 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 train-s1m2

Any idea what's wrong here?
-- 
View this message in context: http://www.nabble.com/training-svm-tf3296613.html#a9170716
Sent from the R help mailing list archive at Nabble.com.


From np at alambic.org  Mon Feb 26 17:51:52 2007
From: np at alambic.org (Nicolas Prune)
Date: Mon, 26 Feb 2007 17:51:52 +0100
Subject: [R] match() function with a little enhancement
Message-ID: <1172508712.45e31028d7496@imp2.online.net>

Dear R users,

I was wondering if R has a built-in function doing the following :

my_match(values_vector,lookup_vector)
{
for each value of values_vector :

if value %in% lookup_vector, then value is unchanged
else, value is changed the the closest element of lookup_vector, "closest"
meaning "the one that would come just after if we sorted them using order()"
}

For example :

values <- c("Kiwis", "Bananas", "Ananas", "Cherries", "Peer")
vector <- c("Oranges", "Bananas", "Apples", "Cherries", "Lemons")

my_match(values, vector) should return :

c("Lemons","Bananas","Apples","Cherries",NA)

I currently use a home-made function for this, but it is quite slow on large
sets, msotly because I did not manage to avoid using a loop.

Many thanks for your ideas,
Nicolas


From jholtman at gmail.com  Tue Feb 27 00:06:15 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 26 Feb 2007 18:06:15 -0500
Subject: [R] match() function with a little enhancement
In-Reply-To: <1172508712.45e31028d7496@imp2.online.net>
References: <1172508712.45e31028d7496@imp2.online.net>
Message-ID: <644e1f320702261506i280a7e50u3171ca8ec5556ffd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/ceaecba0/attachment.pl 

From p.murrell at auckland.ac.nz  Tue Feb 27 00:14:52 2007
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 27 Feb 2007 12:14:52 +1300
Subject: [R] PLotting R graphics/symbols without user x-y scaling
In-Reply-To: <45E2EDE0.5060305@unc.edu>
References: <45E2EDE0.5060305@unc.edu>
Message-ID: <45E369EC.4050100@stat.auckland.ac.nz>

Hi


Jonathan Lees wrote:
> Is it possible to add lines or other
> user defined graphics
> to a plot in R that does not depend on
> the user scale for the plot?
> 
> For example I have a plot
> plot(x,y)
> and I want to add some graphic that is
> scaled in inches or cm but I do not want the
> graphic to change when the x-y scales are
> changed - like a thermometer, scale bar or
> other symbol -
> How does one do this?
> 
> I want to build my own library of glyphs to add to plots
> but I do not know how to plot them when their
> size is independent of the device/user coordinates.
> 
> Is it possible to add to the list
> of symbols in the function symbols()
> other than:
>   _circles_, _squares_, _rectangles_, _stars_, _thermometers_, and
>       _boxplots_
> 
> can I make my own symbols and have symbols call these?


There is currently no mechanism for defining your own additions to
symbols(), but this sort of thing is easily doable using the grid
graphics system, and the resulting symbols would be easy to add to
lattice plots.  See ...
http://www.stat.auckland.ac.nz/~paul/RGraphics/chapter4.pdf
http://www.stat.auckland.ac.nz/~paul/RGraphics/chapter5.pdf

There is also an example of how to do this sort of thing using the
grImport package (and grid and lattice) in
http://www.stat.auckland.ac.nz/~paul/Talks/import.pdf
The complete code for the example is ...

library(grImport)
hourglass <-
     new("Picture",
         paths=
         list(new("PictureFill",
                  x=c(0, 1, 0, 1),
                  y=c(0, 0, 1, 1),
                  rgb="black"),
              new("PictureStroke",
                  x=c(0, 1, 0, 1, 0),
                  y=c(0, 0, 1, 1, 0),
                  rgb="grey")),
         summary=
         new("PictureSummary",
             numPaths=1,
             xscale=c(0, 1),
             yscale=c(0, 1)))
dotplot(variety ~ yield | year, data=barley,
         panel=function(x, y, type, ...) {
             panel.dotplot(x, y, type="n", ...)
             grid.symbols(hourglass,
                          x=unit(as.numeric(x), "native"),
                          y=unit(as.numeric(y), "native"),
                          size=unit(5, "mm"))
         })

Paul


> Thanks-
> 
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From wexler at yahoo.com  Tue Feb 27 00:27:10 2007
From: wexler at yahoo.com (Michael Wexler)
Date: Mon, 26 Feb 2007 15:27:10 -0800 (PST)
Subject: [R] Crosstabbing multiple response data
Message-ID: <60272.97701.qm@web30806.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/a9f07d38/attachment.pl 

From nhepburn at ualberta.ca  Tue Feb 27 01:11:08 2007
From: nhepburn at ualberta.ca (Neil Hepburn)
Date: Mon, 26 Feb 2007 17:11:08 -0700
Subject: [R] looping
Message-ID: <200702270011.l1R0BAjf004764@pilsener.srv.ualberta.ca>


Greetings:

I am looking for some help (probably really basic) with looping. What I want
to do is repeatedly sample observations (about 100 per sample) from a large
dataset (100,000 observations).  I would like the samples labelled sample.1,
sample.2, and so on (or some other suitably simple naming scheme).  To do
this manually I would 

>smp.1 <- sample(100000, 100)
>sample.1 <- dataset[smp.1,]
>smp.2 <- sample(100000, 100)
>sample.2 <- dataset[smp.2,]
.
.
.
>smp.50 <- sample(100000, 100)
>sample.50 <- dataset[smp.50,]

and so on.

I tried the following loop code to generate 100 samples:

>for (i in 1:50){
>+ smp.[i] <- sample(100000, 100)
>+ sample.[i] <- dataset[smp.[i],]}

Unfortunately, that does not work -- specifying the looping variable i in
the way that I have does not work since R uses that to reference places in a
vector (x[i] would be the ith element in the vector x)

Is it possible to assign the value of the looping variable in a name within
the loop structure?

Cheers,
Neil Hepburn

===========================================
Neil Hepburn, Economics Instructor
Social Sciences Department,
The University of Alberta Augustana Campus
4901 - 46 Avenue 
Camrose, Alberta
T4V 2R3

Phone (780) 697-1588
email nhepburn at augustana.ca


From morlon.helene at gmail.com  Tue Feb 27 01:14:04 2007
From: morlon.helene at gmail.com (MORLON)
Date: Mon, 26 Feb 2007 16:14:04 -0800
Subject: [R] RDA and trend surface regression
Message-ID: <003301c75a04$31b89d60$c08aeca9@MORLONH>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070226/046cb525/attachment.pl 

From Max.Kuhn at pfizer.com  Tue Feb 27 01:49:40 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Mon, 26 Feb 2007 19:49:40 -0500
Subject: [R] RDA and trend surface regression
In-Reply-To: <003301c75a04$31b89d60$c08aeca9@MORLONH>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307A24DC4@groamrexm03.amer.pfizer.com>

Helene,

You will have to give us more information, such as your system/versions
and a small reproducible example. We try to stress that questions are
more easily answered when there are a lot of specific details given and
a reproducible case can be tested.

Here are two comments though:

 1. The quadratic terms probably are not showing up because you are not
using a proper model formula for the task. See:

http://cran.r-project.org/doc/manuals/R-intro.html#Formulae-for-statisti
cal-models

Specifically, the part that says

"I(M): Insulate M. Inside M all operators have their normal arithmetic
meaning, and that term appears in the model matrix. "

is important. So, as an example from ?rda:

 x <- rda(Species ~ (Sepal.Length+Sepal.Width)^2 + Sepal.Width^2, data =
iris)

would not work for the squared term, but

 x <- rda(Species ~ (Sepal.Length+Sepal.Width)^2 + I(Sepal.Width^2),
data = iris)

would.

2. RDA is fitting models at or between LDA and QDA. So a QDA model with
quadratic terms would be quartic discriminant analysis. Of course, there
are no rules against this, but high order polynomials can do weird
things in the tail (which would be the edges of the space defined by
your training data). If your data are that nonlinear, there are much
better ways of classifying data. I'd suggests getting a copy of Hastie
et all (2001) or MASS.

Max



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of MORLON
Sent: Monday, February 26, 2007 7:14 PM
To: r-help at stat.math.ethz.ch
Subject: [R] RDA and trend surface regression

Dear all,

 

I'm performing RDA on plant presence/absence data, constrained by
geographical locations. I'd like to constrain the RDA by the "extended
matrix of geographical coordinates" -ie the matrix of geographical
coordinates completed by adding all terms of a cubic trend surface
regression- . 

This is the command I use (package vegan):

 

>rda(Helling ~ x+y+x*y+x^2+y^2+x*y^2+y*x^2+x^3+y^3) 

 

where Helling is the matrix of Hellinger-transformed presence/absence
data

The result returned by R is exactly the same as the one given by:

 

>anova(rda(Helling ~ x+y)

 

Ie the quadratic and cubic terms are not taken into account

 

I hope you can help me with that: "how can I perform a RDA on an
extended
matrix of geographical coordinates in R?".

 

Thank you very much in advance,

 

Helene Morlon

University of California, Merced

morlon.helene at gmail.com

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From dylan.beaudette at gmail.com  Tue Feb 27 02:36:30 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Mon, 26 Feb 2007 17:36:30 -0800
Subject: [R] prop.test or chisq.test ..?
Message-ID: <200702261736.31001.dylan.beaudette@gmail.com>

Hi everyone,

Suppose I have a count the occurrences of positive results, and the total 
number of occurrences:


pos <- 14
total <- 15

testing that the proportion of positive occurrences is greater than 0.5 gives 
a p-value and confidence interval:

prop.test( pos, total, p=0.5, alternative='greater')

        1-sample proportions test with continuity correction

data:  14 out of 15, null probability 0.5 
X-squared = 9.6, df = 1, p-value = 0.0009729
alternative hypothesis: true p is greater than 0.5 
95 percent confidence interval:
 0.706632 1.000000 
sample estimates:
        p 
0.9333333


My question is how does the use of chisq.test() differ from the above 
operation. For example:

chisq.test(table( c(rep('pos', 14), rep('neg', 1)) ))

        Chi-squared test for given probabilities

data:  table(c(rep("pos", 14), rep("neg", 1))) 
X-squared = 11.2667, df = 1, p-value = 0.0007891

... gives slightly different results. I am corrent in interpreting that the 
chisq.test() function in this case is giving me a p-value associated with the 
test that the probabilities of pos are *different* than the probabilities of 
neg -- and thus a larger p-value than the prop.test(... , p=0.5, 
alternative='greater') ? 

I realize that this is a rather elementary question, and references to a text 
would be just as helpful. Ideally, I would like a measure of how much I 
can 'trust' that a larger proportion is also statistically meaningful. Thus 
far the results from prop.test() match my intuition, but affirmation would be 
great.

Cheers,


-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341


From ggrothendieck at gmail.com  Tue Feb 27 02:00:53 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Feb 2007 20:00:53 -0500
Subject: [R] PLotting R graphics/symbols without user x-y scaling
In-Reply-To: <45E2EDE0.5060305@unc.edu>
References: <45E2EDE0.5060305@unc.edu>
Message-ID: <971536df0702261700t98ec0d7x45ae905fd7174e3e@mail.gmail.com>

You can use par("usr") to get the min/max coords of the plot and then
use that.  For example, this will plot a red dot in the middle of the
plot area regardless of the coordinates:

plot(1:10)  # sample plot
usr <- par("usr")
points(mean(usr[1:2]), mean(usr[3:4]), pch = 20, col = "red") # red dot

See ?par

On 2/26/07, Jonathan Lees <jonathan.lees at unc.edu> wrote:
>
> Is it possible to add lines or other
> user defined graphics
> to a plot in R that does not depend on
> the user scale for the plot?
>
> For example I have a plot
> plot(x,y)
> and I want to add some graphic that is
> scaled in inches or cm but I do not want the
> graphic to change when the x-y scales are
> changed - like a thermometer, scale bar or
> other symbol -
> How does one do this?
>
> I want to build my own library of glyphs to add to plots
> but I do not know how to plot them when their
> size is independent of the device/user coordinates.
>
> Is it possible to add to the list
> of symbols in the function symbols()
> other than:
>  _circles_, _squares_, _rectangles_, _stars_, _thermometers_, and
>      _boxplots_
>
> can I make my own symbols and have symbols call these?
>
>
> Thanks-
>
>
> --
> Jonathan M. Lees
> Professor
> THE UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL
> Department of Geological Sciences
> Campus Box #3315
> Chapel Hill, NC  27599-3315
> TEL: (919) 962-0695
> FAX: (919) 966-4519
> jonathan.lees at unc.edu
> http://www.unc.edu/~leesj
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gunter.berton at gene.com  Tue Feb 27 02:02:34 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 26 Feb 2007 17:02:34 -0800
Subject: [R] looping
In-Reply-To: <200702270011.l1R0BAjf004764@pilsener.srv.ualberta.ca>
Message-ID: <010801c75a0a$f7824130$4d908980@gne.windows.gene.com>

You do not say -- and I am unable to divine -- whether you wish to sample
with or without replacement: each time or as a whole.

In general, when you want to do this sort of thing, the fastest way to do it
is just to sample everything you need at once and then form it into a list
or matrix or whatever. For example, for sampling 100 each time with
replacement 200 times:

mySamples <- matrix(sample(yourDatavector, 100*200,replace=FALSE),ncol=200)

will give you a 100 row by 200 column matrix of samples without replacement
from yourDatavector. I hope that you can adapt this to suit your needs.

 
Bert Gunter
Nonclinical Statistics
7-7374

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Neil Hepburn
Sent: Monday, February 26, 2007 4:11 PM
To: r-help at stat.math.ethz.ch
Subject: [R] looping


Greetings:

I am looking for some help (probably really basic) with looping. What I want
to do is repeatedly sample observations (about 100 per sample) from a large
dataset (100,000 observations).  I would like the samples labelled sample.1,
sample.2, and so on (or some other suitably simple naming scheme).  To do
this manually I would 

>smp.1 <- sample(100000, 100)
>sample.1 <- dataset[smp.1,]
>smp.2 <- sample(100000, 100)
>sample.2 <- dataset[smp.2,]
.
.
.
>smp.50 <- sample(100000, 100)
>sample.50 <- dataset[smp.50,]

and so on.

I tried the following loop code to generate 100 samples:

>for (i in 1:50){
>+ smp.[i] <- sample(100000, 100)
>+ sample.[i] <- dataset[smp.[i],]}

Unfortunately, that does not work -- specifying the looping variable i in
the way that I have does not work since R uses that to reference places in a
vector (x[i] would be the ith element in the vector x)

Is it possible to assign the value of the looping variable in a name within
the loop structure?

Cheers,
Neil Hepburn

===========================================
Neil Hepburn, Economics Instructor
Social Sciences Department,
The University of Alberta Augustana Campus
4901 - 46 Avenue 
Camrose, Alberta
T4V 2R3

Phone (780) 697-1588
email nhepburn at augustana.ca

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From tkobayas at indiana.edu  Tue Feb 27 02:56:18 2007
From: tkobayas at indiana.edu (Takatsugu Kobayashi)
Date: Mon, 26 Feb 2007 20:56:18 -0500
Subject: [R] Optimizing the loop for large data
Message-ID: <45E38FC2.6050700@indiana.edu>

Rusers:

I am trying to apply a quadratic discriminant function to find the best 
classification outcomes.
1 is assigned to the values greater than a threshold value; and 0 otherwise.
I would like to see how the apparent error rates and the optimal error 
rate change with increasing threshold values.

I have a 1000*10 data matrix: n=1000 and p=10.

Here is what I wrote so far, but seems to be inefficient. I appreciate 
if someone help me out.

library(foreign)
library(MASS)

D<-read.dbf('data/Indianapolis015.dbf') # import a data

# data looks like this

          LONGLAB         X        Y Perimeter       Area    X_UTM   
Y_UTM   F0 F1  F2
1  TAZ 18011:1000 -86.25985 39.95286  2.061630  0.1862549 50600.38 
4435792  235  0  35
2  TAZ 18011:1001 -86.31030 39.97591  3.657006  0.7305006 46440.80 
4438608    0  0   0
3  TAZ 18011:1002 -86.29542 39.97054  3.516089  0.6408084 47677.31 
4437936  155  0  15
4  TAZ 18011:1003 -86.27574 39.97294  5.000185  1.2592142 49374.91 
4438102  835  0  55
5  TAZ 18011:1004 -86.25967 39.97197  4.788531  1.1930984 50741.38 
4437913  425  0  80
6  TAZ 18011:1005 -86.29245 39.98580  6.189141  1.6734483 48031.44 
4439616  185  0  35
7  TAZ 18011:1006 -86.24899 39.98259  7.525633  2.0564466 51723.80 
4439040  505  0  45
8  TAZ 18011:1007 -86.30974 39.99014  3.773037  0.7790234 46583.20 
4440186   30  0  10
9  TAZ 18011:1008 -86.27151 39.99040  4.589226  1.2212674 49850.92 
4440021   40  0   0
10 TAZ 18011:9215 -86.58085 40.13588 37.278521 69.6681954 24438.13 
4457794 2095 85 200

thrs<-seq(1000,10000,length=50)
ED<-D[,383]/D[,5] # employment density
CBDx<-D[,6]-58277.194363 # convert a coordinate for x
CBDy<-D[,7]-4414486.03135 # convert a coordinate for y

AER<-vector("numeric",length(thrs))
OER<-vector("numeric",length(thrs))
MER<-vector("numeric",length(thrs))

# compute the apparent error rates for each threshold value
for (j in 1:length(thrs)){
ctgy<-ifelse(ED>thrs[j],2,1) # 2 categories are created by the threshold
test1<-qda(cbind(ED,CBDx,CBDy),ctgy)
est1<-cbind(ctgy,predict(test1)$class)
AER[j]<-sum((est1[,1]-est1[2])==0)/dim(D)[1]
}

# OER computation for ith location taken out for the thresholds
for (k in 1:dim(D)[1]){
for (j in 1:length(thrs)){
ctgy<-ifelse(ED>thrs[j],2,1)
test2<-qda(cbind(ED[-k],CBDx[-k],CBDy[-k]),ctgy[-k])
est2<-cbind(ctgy[-k],predict(test2)$class)
OER[j]<-mean(sum((est2[,1]-est2[2])==0)/(dim(D)[1]-1))
}}


From mwtoews at sfu.ca  Mon Feb 26 20:33:49 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Mon, 26 Feb 2007 11:33:49 -0800
Subject: [R] exact matching of names in attr
Message-ID: <45E3361D.9080103@sfu.ca>

In R 2.5.0 (r40806), one of the change is to allow partial matching of 
"name" in the attr function. However, how can I tell if I have an exact 
match or not?

For example, checking to see if an object has a "name" attribute, then 
giving it one if it doesn't:

dat <- data.frame(x=1:10,y=rnorm(10))
if(is.null(attr(dat,"name")))
    attr(dat,"name") <- "Site 1"
str(dat)

(This example works in R < 2.5) Although there is no "name" attribute to 
the data.frame, it partially matches to "names", resulting in not 
setting the attribute. (Personally, I think this change in the "attr" 
function is not desirable, and much prefer exact matches to avoid 
unintentional errors).

How can I tell if this is an exact match? Is there a way to force an 
exact match?

Thanks.

+mt


From mwtoews at sfu.ca  Tue Feb 27 06:51:13 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Mon, 26 Feb 2007 21:51:13 -0800
Subject: [R]  looping
References: 200702270011.l1R0BAjf004764@pilsener.srv.ualberta.ca
Message-ID: <45E3C6D1.4040306@sfu.ca>

Another way is to use an indexed list, which is far more tidier than 
your method. If you mean "about 100" as in an irregular number, then a 
list is your friend (i.e., a ragged array, that can have sometimes 97 
samples, sometime 105 samples, etc.). Similar to your example:

dat <- runif(100000,0,100) # fake dataset
smp <- list() # need an empty list first
for(i in 1:1000)
    smp[[i]] <- sample(dat,100)

However, if you are new to R/S, the best advice is to learn to _not_ use 
the for loop (because it is slow, and there are "vectorized" ways). For 
example, if we want to find the mean of each sample, then return a tidy 
result:

sapply(samp,mean)

or a crazy new analysis you might be working on:

crazy <- function(x,y) (sum(x>y)^2)/sum(x)
sapply(smp,crazy,10)

etc.
+mt


From wwguocn at gmail.com  Tue Feb 27 07:05:09 2007
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Tue, 27 Feb 2007 14:05:09 +0800
Subject: [R] How to use bash command in R script?
Message-ID: <d3677d7d0702262205q448a9153j1b797b34b437aa4d@mail.gmail.com>

Dear All:

Maybe it is a too basic question, but I don't how to find the answer.
Sorry for that.

What I want to do is call a shell command, which will provide two
numbers, and assign those numbers to a vector. For example:

The following command:

$mxresult.sh ABC.mx

mxresult.sh is a script written by myself and ABC.mx is a Mx script.
I can get two numbers, 126.128 and 29, with this command.

Is there any way to do it like this:

c <- somefunction("mxresult.sh ABC.mx")

Or is their any other way to fulfill the function?

Thanks in advance!

Best washes,
Wei-Wei


From p.dalgaard at biostat.ku.dk  Tue Feb 27 07:37:38 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 27 Feb 2007 07:37:38 +0100
Subject: [R] How to use bash command in R script?
In-Reply-To: <d3677d7d0702262205q448a9153j1b797b34b437aa4d@mail.gmail.com>
References: <d3677d7d0702262205q448a9153j1b797b34b437aa4d@mail.gmail.com>
Message-ID: <45E3D1B2.2060707@biostat.ku.dk>

Guo Wei-Wei wrote:
> Dear All:
>
> Maybe it is a too basic question, but I don't how to find the answer.
> Sorry for that.
>
> What I want to do is call a shell command, which will provide two
> numbers, and assign those numbers to a vector. For example:
>
> The following command:
>
> $mxresult.sh ABC.mx
>
> mxresult.sh is a script written by myself and ABC.mx is a Mx script.
> I can get two numbers, 126.128 and 29, with this command.
>
> Is there any way to do it like this:
>
> c <- somefunction("mxresult.sh ABC.mx")
>
> Or is their any other way to fulfill the function?
>   
txt <- system(""mxresult.sh ABC.mx", intern=TRUE)

is the first step.  Then you need to get the numbers using either a 
scan() on a textConnection (see its help page) or something like

mynum <- as.numeric(strsplit(txt, "  *")[[1]])


> Thanks in advance!
>
> Best washes,
> Wei-Wei
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From petr.pikal at precheza.cz  Tue Feb 27 08:00:07 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 27 Feb 2007 08:00:07 +0100
Subject: [R] 2 data frames - list in one out put  , matrix in another ??
In-Reply-To: <504697.36932.qm@web32808.mail.mud.yahoo.com>
Message-ID: <45E3E507.21353.2B49F8@localhost>

Hi

as I do not know what class is labelled and not knowing about your 
data I just try to give you some hints.

What says str(adata) and str(bdata) about structure of your data?

It can be possible that in second case the structure of resulting 
table can be formed into matrix and apply probably does this coercion 
in accordance with Details section of help page.

But this is only a guess.

HTH
Petr



On 26 Feb 2007 at 14:56, John Kane wrote:

Date sent:      	Mon, 26 Feb 2007 14:56:07 -0500 (EST)
From:           	John Kane <jrkrideau at yahoo.ca>
To:             	R R-help <r-help at stat.math.ethz.ch>
Subject:        	[R] 2 data frames - list in one out put  , matrix in another ??

> I have two more or less parallel dataframes that are
> giving me different results on one subset of
> variables.  I know that I assembled the 2 dataframes
> slightly differently but I don't see why I am getting
> this result because one set of variables are labelled
> and the other is not. Variable names are the same,
> etc.  as far as I can acertain.  The only diffference
> seems to be that bdata variables are labelled.  
> 
> About now I really don't care which I get but I would
> like them to be the same.  Can anyone suggest what I
> am doing wrong or should be looking at?
> 
> Windows XP , R 2.4.1 Using Hmisc and gtools as well as
> the basic R installation.  
> 
> Problem
> 
> load(adata)
> fn1 <- function(x) {table(x)}
> jj <-apply(adata[,110:127], 2, fn1)
> 
> OUTPUT jj is aa list of 18 tables
> Examine a variable:
>   >  typeof(adata$act.toy)
> [1] "integer"
> >     class(adata$act.toy)
> [1] "integer"
> 
> 
> load(bdata
> fn1 <- function(x) {table(x)}
> kk <-apply(bdata[,94:111], 2, fn1)
> 
> OUTPUT jj is a matrix 2 X 18
> >   class(bdata$act.toy)
> [1] "labelled"
> >    typeof(bdata$act.toy)
> [1] "integer"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From hb at stat.berkeley.edu  Tue Feb 27 08:01:11 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Mon, 26 Feb 2007 23:01:11 -0800
Subject: [R] exact matching of names in attr
In-Reply-To: <45E3361D.9080103@sfu.ca>
References: <45E3361D.9080103@sfu.ca>
Message-ID: <59d7961d0702262301g6e10ad1by5d832fc691c33eff@mail.gmail.com>

if (!"name" %in% names(attributes(dat))) {
  ...
}

/Henrik

On 2/26/07, Michael Toews <mwtoews at sfu.ca> wrote:
> In R 2.5.0 (r40806), one of the change is to allow partial matching of
> "name" in the attr function. However, how can I tell if I have an exact
> match or not?
>
> For example, checking to see if an object has a "name" attribute, then
> giving it one if it doesn't:
>
> dat <- data.frame(x=1:10,y=rnorm(10))
> if(is.null(attr(dat,"name")))
>     attr(dat,"name") <- "Site 1"
> str(dat)
>
> (This example works in R < 2.5) Although there is no "name" attribute to
> the data.frame, it partially matches to "names", resulting in not
> setting the attribute. (Personally, I think this change in the "attr"
> function is not desirable, and much prefer exact matches to avoid
> unintentional errors).
>
> How can I tell if this is an exact match? Is there a way to force an
> exact match?
>
> Thanks.
>
> +mt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From A.Robinson at ms.unimelb.edu.au  Tue Feb 27 08:02:05 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 27 Feb 2007 18:02:05 +1100
Subject: [R] How to use bash command in R script?
In-Reply-To: <d3677d7d0702262205q448a9153j1b797b34b437aa4d@mail.gmail.com>
References: <d3677d7d0702262205q448a9153j1b797b34b437aa4d@mail.gmail.com>
Message-ID: <20070227070205.GJ55494@ms.unimelb.edu.au>

?system.

Cheers,

Andrew

On Tue, Feb 27, 2007 at 02:05:09PM +0800, Guo Wei-Wei wrote:
> Dear All:
> 
> Maybe it is a too basic question, but I don't how to find the answer.
> Sorry for that.
> 
> What I want to do is call a shell command, which will provide two
> numbers, and assign those numbers to a vector. For example:
> 
> The following command:
> 
> $mxresult.sh ABC.mx
> 
> mxresult.sh is a script written by myself and ABC.mx is a Mx script.
> I can get two numbers, 126.128 and 29, with this command.
> 
> Is there any way to do it like this:
> 
> c <- somefunction("mxresult.sh ABC.mx")
> 
> Or is their any other way to fulfill the function?
> 
> Thanks in advance!
> 
> Best washes,
> Wei-Wei
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From jarioksa at sun3.oulu.fi  Tue Feb 27 08:27:08 2007
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 27 Feb 2007 09:27:08 +0200
Subject: [R]  RDA and trend surface regression
Message-ID: <1172561228.12881.7.camel@biol102145.oulu.fi>


> 'm performing RDA on plant presence/absence data, constrained by
> geographical locations. I'd like to constrain the RDA by the "extended
> matrix of geographical coordinates" -ie the matrix of geographical
> coordinates completed by adding all terms of a cubic trend surface
> regression- . 
> 
> This is the command I use (package vegan):
> 
>  
> 
> >rda(Helling ~ x+y+x*y+x^2+y^2+x*y^2+y*x^2+x^3+y^3) 
> 
>  
> 
> where Helling is the matrix of Hellinger-transformed presence/absence data
> 
> The result returned by R is exactly the same as the one given by:
> 
>  
> 
> >anova(rda(Helling ~ x+y)
> 
>  
> 
> Ie the quadratic and cubic terms are not taken into account
> 

You must *I*solate the polynomial terms with function I ("AsIs") so that
they are not interpreted as formula operators:

rda(Helling ~ x + y + I(x*y) + I(x^2) + I(y^2) + I(x*y^2) + I(y*x^2) +
I(x^3) + I(y^3))

If you don't have the interaction terms, then it is easier and better
(numerically) to use poly():

rda(Helling ~ poly(x, 3) + poly(y, 3))

Another issue is that in my opinion using polynomial constraints is an
Extremely Bad Idea(TM).

cheers, Jari Oksanen


From Lukas.Indermaur at eawag.ch  Tue Feb 27 08:46:08 2007
From: Lukas.Indermaur at eawag.ch (Indermaur Lukas)
Date: Tue, 27 Feb 2007 08:46:08 +0100
Subject: [R] fitting of all possible models
Message-ID: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>

Hi,
Fitting all possible models (GLM) with 10 predictors will result in loads of (2^10 - 1) models. I want to do that in order to get the importance of variables (having an unbalanced variable design) by summing the up the AIC-weights of models including the same variable, for every variable separately. It's time consuming and annoying to define all possible models by hand. 
 
Is there a command, or easy solution to let R define the set of all possible models itself? I defined models in the following way to process them with a batch job:
 
# e.g. model 1
preference<- formula(Y~Lwd + N + Sex + YY)                                                
# e.g. model 2
preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY)  
etc.
etc.
 
 
I appreciate any hint
Cheers
Lukas
 
 
 
 
 
??? 
Lukas Indermaur, PhD student 
eawag / Swiss Federal Institute of Aquatic Science and Technology 
ECO - Department of Aquatic Ecology
?berlandstrasse 133
CH-8600 D?bendorf
Switzerland
 
Phone: +41 (0) 71 220 38 25
Fax    : +41 (0) 44 823 53 15 
Email: lukas.indermaur at eawag.ch
www.lukasindermaur.ch


From wwguocn at gmail.com  Tue Feb 27 08:52:05 2007
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Tue, 27 Feb 2007 15:52:05 +0800
Subject: [R] How to use bash command in R script?
In-Reply-To: <20070227070205.GJ55494@ms.unimelb.edu.au>
References: <d3677d7d0702262205q448a9153j1b797b34b437aa4d@mail.gmail.com>
	<20070227070205.GJ55494@ms.unimelb.edu.au>
Message-ID: <d3677d7d0702262352r53dafa56yd2b438bccdd61393@mail.gmail.com>

Thank you all! I solved my problem with your help.

Best wishes,
Wei-Wei


From P.Dalgaard at biostat.ku.dk  Tue Feb 27 09:01:58 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 27 Feb 2007 09:01:58 +0100
Subject: [R] How to use bash command in R script?
In-Reply-To: <d3677d7d0702262352r53dafa56yd2b438bccdd61393@mail.gmail.com>
References: <d3677d7d0702262205q448a9153j1b797b34b437aa4d@mail.gmail.com>	<20070227070205.GJ55494@ms.unimelb.edu.au>
	<d3677d7d0702262352r53dafa56yd2b438bccdd61393@mail.gmail.com>
Message-ID: <45E3E576.60503@biostat.ku.dk>

Guo Wei-Wei wrote:
> Thank you all! I solved my problem with your help.
>   
Come to think of it, it might be more to the point to  use scan() on a
pipe():

con <- pipe("mxresult.sh ABC.mx", "r")
mynum <- scan(con)
close(con)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From amnakhan493 at gmail.com  Tue Feb 27 09:57:15 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 27 Feb 2007 13:57:15 +0500
Subject: [R] Fwd: RFA
In-Reply-To: <3ffd3bb60702250851w575442cat6b16d1aefb290f8f@mail.gmail.com>
References: <3ffd3bb60702250851w575442cat6b16d1aefb290f8f@mail.gmail.com>
Message-ID: <3ffd3bb60702270057n32af60e3g2c0d3dfeea3e79c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/8cc53e39/attachment.pl 

From amnakhan493 at gmail.com  Tue Feb 27 09:57:15 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 27 Feb 2007 13:57:15 +0500
Subject: [R] Fwd: RFA
In-Reply-To: <3ffd3bb60702250851w575442cat6b16d1aefb290f8f@mail.gmail.com>
References: <3ffd3bb60702250851w575442cat6b16d1aefb290f8f@mail.gmail.com>
Message-ID: <3ffd3bb60702270057n32af60e3g2c0d3dfeea3e79c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/8cc53e39/attachment-0001.pl 

From amnakhan493 at gmail.com  Tue Feb 27 09:58:47 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 27 Feb 2007 13:58:47 +0500
Subject: [R] Fwd: nsRFA
In-Reply-To: <3ffd3bb60702250844t15a3b6acve4fcfedfc7ed7736@mail.gmail.com>
References: <3ffd3bb60702250844t15a3b6acve4fcfedfc7ed7736@mail.gmail.com>
Message-ID: <3ffd3bb60702270058k40a24c85y8c167806e9a1146b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/9023cecf/attachment.pl 

From amnakhan493 at gmail.com  Tue Feb 27 09:58:47 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 27 Feb 2007 13:58:47 +0500
Subject: [R] Fwd: nsRFA
In-Reply-To: <3ffd3bb60702250844t15a3b6acve4fcfedfc7ed7736@mail.gmail.com>
References: <3ffd3bb60702250844t15a3b6acve4fcfedfc7ed7736@mail.gmail.com>
Message-ID: <3ffd3bb60702270058k40a24c85y8c167806e9a1146b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/9023cecf/attachment-0001.pl 

From amnakhan493 at gmail.com  Tue Feb 27 09:59:24 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 27 Feb 2007 13:59:24 +0500
Subject: [R] Fwd: RFA and nsRFA
In-Reply-To: <3ffd3bb60702250837w440481a1m2c55cf885da1839d@mail.gmail.com>
References: <3ffd3bb60702250837w440481a1m2c55cf885da1839d@mail.gmail.com>
Message-ID: <3ffd3bb60702270059oa1ac0f9v754c21e75a8181a3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/65a1607b/attachment.pl 

From amnakhan493 at gmail.com  Tue Feb 27 09:59:24 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 27 Feb 2007 13:59:24 +0500
Subject: [R] Fwd: RFA and nsRFA
In-Reply-To: <3ffd3bb60702250837w440481a1m2c55cf885da1839d@mail.gmail.com>
References: <3ffd3bb60702250837w440481a1m2c55cf885da1839d@mail.gmail.com>
Message-ID: <3ffd3bb60702270059oa1ac0f9v754c21e75a8181a3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/65a1607b/attachment-0001.pl 

From wwguocn at gmail.com  Tue Feb 27 10:12:06 2007
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Tue, 27 Feb 2007 17:12:06 +0800
Subject: [R] How to use bash command in R script?
In-Reply-To: <45E3E576.60503@biostat.ku.dk>
References: <d3677d7d0702262205q448a9153j1b797b34b437aa4d@mail.gmail.com>
	<20070227070205.GJ55494@ms.unimelb.edu.au>
	<d3677d7d0702262352r53dafa56yd2b438bccdd61393@mail.gmail.com>
	<45E3E576.60503@biostat.ku.dk>
Message-ID: <d3677d7d0702270112y347b4228wea8fccc38a512263@mail.gmail.com>

It's a great way. You told a lot of things that I need to ask.
Thank you very much!

Best wishes,
Wei-Wei

2007/2/27, Peter Dalgaard <P.Dalgaard at biostat.ku.dk>:
> Guo Wei-Wei wrote:
> > Thank you all! I solved my problem with your help.
> >
> Come to think of it, it might be more to the point to  use scan() on a
> pipe():
>
> con <- pipe("mxresult.sh ABC.mx", "r")
> mynum <- scan(con)
> close(con)
>
> --
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
>
>


From sergeyg at gmail.com  Tue Feb 27 10:49:22 2007
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Tue, 27 Feb 2007 10:49:22 +0100
Subject: [R] Additional args to fun in integrate() not found?
Message-ID: <7cb007bd0702270149o4021e0e5u11fafd878e099b38@mail.gmail.com>

Hello, fellow Rdicts,

I have  the code for the program below. I need to integrate a function
of "x" and "p". I use integrate to integrate over "x" and pass "p" as
an additional argument. "p" is specified and given default value in
the argument list. Still, integrate() cannot read "p", unless I
explicitly insert a numeric value in the integrate() argument list.
And when I do that, I get the right result, but still some warnings.

Please, help me with these problems:
1) why is "p" not recognized?
2) what are these warning messages?

PROGRAM CODE:
---------------------------

#THIS LIBRARY IS NEEDED FOR THE INCOMPLETE GAMMA FUNCTION

library(zipfR)

#------------------------------------------------------------------------

gedCDF = function(yvec, p=2, mu=0, scale=1, numint=0)
{

#-------------------------------------------------------------------------
#Setting k to sqrt(2) and the GED with p=2 coincides with standard normal.
#Set k=1 and GED with p=1 coincides with Laplace.

	k<-sqrt(2)
	#k<-1
	scale<-scale*k
	
	zvec<-(yvec-mu)/scale
	cdf<-matrix(0, length(zvec),1)

	for(i in 1:length(zvec))
	{
		z<-zvec[i]

		if(numint==0)
		{
		if(z<=0)
		{
		t<-0.5*(1-1/gamma(1/p)*Igamma((1/p),(-z)^p,lower=TRUE))
		}
		else
		{
		t<-1-(0.5*(1-1/gamma(1/p)*Igamma((1/p),(z)^p,lower=TRUE )))
		}
		}
		else
		{
		t<-integrate(geddenstandard, -35, z, subdivisions=1000,
		rel.tol=100*.Machine$double.eps, abs.tol=rel.tol,
		stop.on.error=TRUE, keep.xy=FALSE, aux=NULL,p)
		}
	cdf[i]<-t
	}
cdf
}

#-------------------------------------------------------------------------
geddenstandard = function(z,p)
{
f<-p/(2*gamma(1/p))*exp(-abs(z)^p)
}
-----------------------------------------------------------------------------------------------------------------------

If I run with this definition I get the following error message and abort:
> gedCDF(c(1,2,3,4,5), numint=1)
Error in eval(expr, envir, enclos) : ..1 used in an incorrect context,
no ... to look in

If I replace "p" in integrate() with 2, I get correct answers, but
still some warning messages:

> gedCDF(c(1,2,3,4,5), numint=1)
[[1]]
[1] 0.8413447

[[2]]
[1] 0.9772499

[[3]]
[1] 0.9986501

[[4]]
[1] 0.9999683

[[5]]
[1] 0.9999997

Warning messages:
1: number of items to replace is not a multiple of replacement length
2: number of items to replace is not a multiple of replacement length
3: number of items to replace is not a multiple of replacement length
4: number of items to replace is not a multiple of replacement length
5: number of items to replace is not a multiple of replacement length

-------------------------------
Do I get these warnings because I define cdf as a matrix and the
output-cdf is a list?

Please, help me with these!
Email to my gmail account, please: sergeyg at gmail.com

THanks in advance
Sergey


From sergeyg at gmail.com  Tue Feb 27 11:05:32 2007
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Tue, 27 Feb 2007 11:05:32 +0100
Subject: [R] Additional: to integrate()
Message-ID: <7cb007bd0702270205w615d78dfyf2d1af523db8d51@mail.gmail.com>

Hi again, people

I found that there is some problem with rel.tol agrument in the
integrate() function.
It is not getting found. Maybe because it is a call to .Machine$double.eps?
If I specify rel.tol as an argument in gedCDF, everything works,
except for those warning messages.

Strange.

-- 
Laziness is nothing more than the habit of resting before you get tired.
- Jules Renard (writer)

Experience is one thing you can't get for nothing.
- Oscar Wilde (writer)

When you are finished changing, you're finished.
- Benjamin Franklin (President)


From dimitris.rizopoulos at med.kuleuven.be  Tue Feb 27 11:10:49 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 27 Feb 2007 11:10:49 +0100
Subject: [R] Additional args to fun in integrate() not found?
References: <7cb007bd0702270149o4021e0e5u11fafd878e099b38@mail.gmail.com>
Message-ID: <010801c75a57$8dd52840$0540210a@www.domain>

you need 'p = p' in the integrate() call; try this version:

gedCDF <- function (yvec, p = 2, mu = 0, scale = 1, numint = 0) {
    k <- sqrt(2)
    # k <- 1
    scale <- scale*k
    zvec <- (yvec - mu) / scale
    cdf <- numeric(length(zvec))
    for (i in seq(zvec)) {
        z <- zvec[i]
        cdf[i] <- if (numint == 0) {
            if (z <= 0) {
                0.5 * (1 - 1/gamma(1/p) * Igamma((1/p), (-z)^p,
                    lower = TRUE))
            } else {
                1 - (0.5 * (1 - 1/gamma(1/p) * Igamma((1/p), (z)^p,
                    lower = TRUE)))
            }
        } else {
            integrate(geddenstandard, lower = -35, upper = z,
                rel.tol = 100*.Machine$double.eps, p = p)$value
        }
    }
    cdf
}

geddenstandard <- function (z, p) {
    p / (2 * gamma(1/p)) * exp(-abs(z)^p)
}

###########

gedCDF(c(1,2,3,4,5), numint=1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sergey Goriatchev" <sergeyg at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, February 27, 2007 10:49 AM
Subject: [R] Additional args to fun in integrate() not found?


> Hello, fellow Rdicts,
>
> I have  the code for the program below. I need to integrate a 
> function
> of "x" and "p". I use integrate to integrate over "x" and pass "p" 
> as
> an additional argument. "p" is specified and given default value in
> the argument list. Still, integrate() cannot read "p", unless I
> explicitly insert a numeric value in the integrate() argument list.
> And when I do that, I get the right result, but still some warnings.
>
> Please, help me with these problems:
> 1) why is "p" not recognized?
> 2) what are these warning messages?
>
> PROGRAM CODE:
> ---------------------------
>
> #THIS LIBRARY IS NEEDED FOR THE INCOMPLETE GAMMA FUNCTION
>
> library(zipfR)
>
> #------------------------------------------------------------------------
>
> gedCDF = function(yvec, p=2, mu=0, scale=1, numint=0)
> {
>
> #-------------------------------------------------------------------------
> #Setting k to sqrt(2) and the GED with p=2 coincides with standard 
> normal.
> #Set k=1 and GED with p=1 coincides with Laplace.
>
> k<-sqrt(2)
> #k<-1
> scale<-scale*k
>
> zvec<-(yvec-mu)/scale
> cdf<-matrix(0, length(zvec),1)
>
> for(i in 1:length(zvec))
> {
> z<-zvec[i]
>
> if(numint==0)
> {
> if(z<=0)
> {
> t<-0.5*(1-1/gamma(1/p)*Igamma((1/p),(-z)^p,lower=TRUE))
> }
> else
> {
> t<-1-(0.5*(1-1/gamma(1/p)*Igamma((1/p),(z)^p,lower=TRUE )))
> }
> }
> else
> {
> t<-integrate(geddenstandard, -35, z, subdivisions=1000,
> rel.tol=100*.Machine$double.eps, abs.tol=rel.tol,
> stop.on.error=TRUE, keep.xy=FALSE, aux=NULL,p)
> }
> cdf[i]<-t
> }
> cdf
> }
>
> #-------------------------------------------------------------------------
> geddenstandard = function(z,p)
> {
> f<-p/(2*gamma(1/p))*exp(-abs(z)^p)
> }
> -----------------------------------------------------------------------------------------------------------------------
>
> If I run with this definition I get the following error message and 
> abort:
>> gedCDF(c(1,2,3,4,5), numint=1)
> Error in eval(expr, envir, enclos) : ..1 used in an incorrect 
> context,
> no ... to look in
>
> If I replace "p" in integrate() with 2, I get correct answers, but
> still some warning messages:
>
>> gedCDF(c(1,2,3,4,5), numint=1)
> [[1]]
> [1] 0.8413447
>
> [[2]]
> [1] 0.9772499
>
> [[3]]
> [1] 0.9986501
>
> [[4]]
> [1] 0.9999683
>
> [[5]]
> [1] 0.9999997
>
> Warning messages:
> 1: number of items to replace is not a multiple of replacement 
> length
> 2: number of items to replace is not a multiple of replacement 
> length
> 3: number of items to replace is not a multiple of replacement 
> length
> 4: number of items to replace is not a multiple of replacement 
> length
> 5: number of items to replace is not a multiple of replacement 
> length
>
> -------------------------------
> Do I get these warnings because I define cdf as a matrix and the
> output-cdf is a list?
>
> Please, help me with these!
> Email to my gmail account, please: sergeyg at gmail.com
>
> THanks in advance
> Sergey
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From florent.baty at unibas.ch  Tue Feb 27 12:10:09 2007
From: florent.baty at unibas.ch (Florent Baty)
Date: Tue, 27 Feb 2007 12:10:09 +0100
Subject: [R] Coxph and ordered factors
Message-ID: <45E41191.8050108@unibas.ch>

Dear Terry,

Thanks for your answer. I realised that depending on whether or not I am 
using an ordered factor I have some substantial changes in the output of 
function coxph:

Here is the output I obtain when using variable 'stage2' as an ordered 
factor:

Call:
coxph(formula = Surv(survTime, status) ~ stage2, data = survData)


            coef exp(coef) se(coef)       z       p
stage2.L  2.3430    10.413    0.709  3.3033 0.00096
stage2.Q -0.0498     0.951    0.629 -0.0792 0.94000
stage2.C -0.0468     0.954    0.541 -0.0866 0.93000

Likelihood ratio test=30.2  on 3 df, p=1.25e-06  n= 61

Here is the output when using 'stage2' without ordering:

Call:
coxph(formula = Surv(survTime, status) ~ stage2, data = survData)


        coef exp(coef) se(coef)     z      p
stage22 1.06      2.87     1.16 0.914 0.3600
stage23 2.17      8.73     1.10 1.975 0.0480
stage24 3.12     22.70     1.03 3.037 0.0024

Likelihood ratio test=30.2  on 3 df, p=1.25e-06  n= 61

Finally here is the output when coding 'stage2' as a numerical variable:

Call:
coxph(formula = Surv(survTime, status) ~ stage2, data = survData)


       coef exp(coef) se(coef)    z       p
stage2 1.03      2.79    0.226 4.53 5.8e-06

Likelihood ratio test=30.2  on 1 df, p=3.94e-08  n= 61


It seems that when using ordered factors a model with linear, quadratic 
and cubic terms is fitted. Is that something expected? Can't we 
specifically focus on a simpler model including only linear terms? I am 
not sure to have a full understanding of what is actually done when 
factors are ordered.

Best wishes,

Florent

-- 
--------------------------------------------------
		Dr Florent BATY
Pulmonary Gene Research, Universit?tsspital Basel
   Petersgraben 4, CH-4031 Basel, Switzerland
 tel: +41 61 265 57 27 - fax: +41 61 265 45 87


From aiminy at iastate.edu  Tue Feb 27 12:31:13 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Tue, 27 Feb 2007 05:31:13 -0600
Subject: [R] merge two colums
Message-ID: <6.2.3.4.2.20070227052733.03e8e8b0@aiminy.mail.iastate.edu>

I have a data set like this

 > head(data.1A24_aa_model)
     V1    V2 V3  V4 V5     V6
1 1A24 MODEL  1 ALA  1  84.47
2 1A24 MODEL  1 GLN  2  63.06
3 1A24 MODEL  1 TYR  3 107.72
4 1A24 MODEL  1 GLU  4  54.36
5 1A24 MODEL  1 ASP  5  67.01
6 1A24 MODEL  1 GLY  6 999.00

I want to change this to the following format:

    V1 V2 V3  V4 V5
1 1A24 MODEL  1 ALA _1  84.47
2 1A24 MODEL  1 GLN _2  63.06
3 1A24 MODEL  1 TYR _3 107.72
4 1A24 MODEL  1 GLU _4  54.36
5 1A24 MODEL  1 ASP _5  67.01
6 1A24 MODEL  1 GLY _6 999.00

anyone know how to do this?

Aimin


From r.hankin at noc.soton.ac.uk  Tue Feb 27 12:51:32 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 27 Feb 2007 11:51:32 +0000
Subject: [R] merge two colums
In-Reply-To: <6.2.3.4.2.20070227052733.03e8e8b0@aiminy.mail.iastate.edu>
References: <6.2.3.4.2.20070227052733.03e8e8b0@aiminy.mail.iastate.edu>
Message-ID: <A4CD729A-883D-4A89-A675-4728BD2F912E@soc.soton.ac.uk>

Hi.

Use paste(... , sep="_") to create a new variable, then
data.frame()  to join them.

Note that you have to name the columns explicitly; the
default names are long for my tastes.


HTH

rksh



 > a <- data.frame(V1=1:3,V2=letters[1:3],V3=3:1,V4=100:102)
 > a
   V1 V2 V3  V4
1  1  a  3 100
2  2  b  2 101
3  3  c  1 102
 > data.frame(n1=a$V1,n2=paste(a$V2,a$V3,sep="_"),n4=a$V4)
   n1  n2  n4
1  1 a_3 100
2  2 b_2 101
3  3 c_1 102
 >



On 27 Feb 2007, at 11:31, Aimin Yan wrote:

> I have a data set like this
>
>> head(data.1A24_aa_model)
>      V1    V2 V3  V4 V5     V6
> 1 1A24 MODEL  1 ALA  1  84.47
> 2 1A24 MODEL  1 GLN  2  63.06
> 3 1A24 MODEL  1 TYR  3 107.72
> 4 1A24 MODEL  1 GLU  4  54.36
> 5 1A24 MODEL  1 ASP  5  67.01
> 6 1A24 MODEL  1 GLY  6 999.00
>
> I want to change this to the following format:
>
>     V1 V2 V3  V4 V5
> 1 1A24 MODEL  1 ALA _1  84.47
> 2 1A24 MODEL  1 GLN _2  63.06
> 3 1A24 MODEL  1 TYR _3 107.72
> 4 1A24 MODEL  1 GLU _4  54.36
> 5 1A24 MODEL  1 ASP _5  67.01
> 6 1A24 MODEL  1 GLY _6 999.00
>
> anyone know how to do this?
>
> Aimin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From david.meyer at wu-wien.ac.at  Tue Feb 27 13:43:29 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Tue, 27 Feb 2007 13:43:29 +0100
Subject: [R]  training svm
Message-ID: <45E42771.50205@wu-wien.ac.at>

Hello (whoever you are),

your data looks problematic. What does

head(ne_span_data)

reveal?

BTW, svm() will not handle NA values.

Best
David

---------------------

Hello. I'm new to R and I'm trying to solve a classification problem. I have
a training dataset of about 40,000 rows and 50 columns. When I try to train
support vector machine, it gives me this error after a few seconds:

  Error in predict.svm(ret, xhold) : Model is empty!

This is the code I use:

  ne_span_data <- as.matrix(read.table('ne_span.data.R.txt', header=TRUE,
row.names='id'))
  library('e1071')
  svm_ne_span_model <- svm(NE_type ~ . , ne_span_data)

it gives me:
Error in predict.svm(ret, xhold) : Model is empty!

A line from the ne_span.data.R.txt file:
  svt OTHER N N I S 2 NA NA NA NA NA A NA NA 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 train-s1m2


From f.harrell at vanderbilt.edu  Tue Feb 27 14:13:40 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 27 Feb 2007 07:13:40 -0600
Subject: [R] fitting of all possible models
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <45E42E84.6050509@vanderbilt.edu>

Indermaur Lukas wrote:
> Hi,
> Fitting all possible models (GLM) with 10 predictors will result in loads of (2^10 - 1) models. I want to do that in order to get the importance of variables (having an unbalanced variable design) by summing the up the AIC-weights of models including the same variable, for every variable separately. It's time consuming and annoying to define all possible models by hand. 
>  
> Is there a command, or easy solution to let R define the set of all possible models itself? I defined models in the following way to process them with a batch job:
>  
> # e.g. model 1
> preference<- formula(Y~Lwd + N + Sex + YY)                                                
> # e.g. model 2
> preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY)  
> etc.
> etc.
>  
>  
> I appreciate any hint
> Cheers
> Lukas

If you choose the model from amount 2^10 -1 having best AIC, that model 
will be badly biased.  Why look at so many?  Pre-specification of 
models, or fitting full models with penalization, or using data 
reduction (masked to Y) may work better.

Frank

>  
>  
>  
>  
>  
> ??? 
> Lukas Indermaur, PhD student 
> eawag / Swiss Federal Institute of Aquatic Science and Technology 
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
>  
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15 
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From h.wickham at gmail.com  Tue Feb 27 14:22:42 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 27 Feb 2007 07:22:42 -0600
Subject: [R] fitting of all possible models
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <f8e6ff050702270522y5a327f1at7ae8a9118b0936f8@mail.gmail.com>

Hi Lukas,

You may find my meifly package helpful.  It provides functions to
generate ensembles of models (eg. fitall) and then extract all the
coefficients, residuals etc (coef, summary, residual etc).  The main
point of the package is to visualise all these models, and I second
Frank's comment that merely selecting the best model will be perilous.

Unfortunately, the package is not on CRAN yet, but if you are
interested, contact me off list with your OS and I can email you the
package, and accompanying paper.

Regards,

Hadley

On 2/27/07, Indermaur Lukas <Lukas.Indermaur at eawag.ch> wrote:
> Hi,
> Fitting all possible models (GLM) with 10 predictors will result in loads of (2^10 - 1) models. I want to do that in order to get the importance of variables (having an unbalanced variable design) by summing the up the AIC-weights of models including the same variable, for every variable separately. It's time consuming and annoying to define all possible models by hand.
>
> Is there a command, or easy solution to let R define the set of all possible models itself? I defined models in the following way to process them with a batch job:
>
> # e.g. model 1
> preference<- formula(Y~Lwd + N + Sex + YY)
> # e.g. model 2
> preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY)
> etc.
> etc.
>
>
> I appreciate any hint
> Cheers
> Lukas
>
>
>
>
>
> ???
> Lukas Indermaur, PhD student
> eawag / Swiss Federal Institute of Aquatic Science and Technology
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
>
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Tue Feb 27 13:58:55 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 27 Feb 2007 07:58:55 -0500 (EST)
Subject: [R] Crosstabbing multiple response data
In-Reply-To: <60272.97701.qm@web30806.mail.mud.yahoo.com>
Message-ID: <427592.49015.qm@web32802.mail.mud.yahoo.com>

Thanks to everyone for this.  I was looking at the
same problem last night and just was going to write a
posting to R-help when I saw this.  


--- Michael Wexler <wexler at yahoo.com> wrote:

> 
> Thanks to Charles, Gabor, and a private message from
> Frank E Harrell with some good ideas and help.  This
> crossprod approach was very clever, I would never
> have thought of it.
> 
> Best, Michael
> 
> 
> ----- Original Message ----
> From: Charles C. Berry <cberry at tajo.ucsd.edu>
> To: Michael Wexler <wexler at yahoo.com>
> Cc: r-help at stat.math.ethz.ch
> Sent: Thursday, February 22, 2007 1:17:44 PM
> Subject: Re: [R] Crosstabbing multiple response data
> 
> 
> > res <- crossprod( as.matrix( ratings[ , -1] ) )
> > diag(res) <- ""
> > print(res, quote=F)
>       att1 att2 att3
> att1      2    1
> att2 2         2
> att3 1    2
> > 
> > res2 <- crossprod(as.matrix( ratings[ , -1])) *
> 100 / nrow( ratings )
> > res2[] <- paste( res2, "%", sep="" )
> > diag(res2) <- ""
> > print(res2, quote=F)
>       att1 att2 att3
> att1      50%  25%
> att2 50%       50%
> att3 25%  50%
> >
> 
> Be sure to bone up on format and sprintf before
> taking this into 
> production.
> 
> On Thu, 22 Feb 2007, Michael Wexler wrote:
> 
> > Using R version 2.4.1 (2006-12-18) on Windows, I
> have a dataset which resembles this:
> >
> > id    att1    att2    att3
> > 1    1        1        0
> > 2    1        0        0
> > 3    0        1        1
> > 4    1        1        1
> >
> > ratings <- data.frame(id = c(1,2,3,4), att1 =
> c(1,1,0,1), att2 = c(1,0,0,1), att3 = c(0,1,1,1))
> >
> > I would like to get a cross tab of counts of
> co-ocurrence, which might resemble this:
> >
> >    att1    att2    att3
> > att1         2       1
> > att2    2            2
> > att3    1    2
> >
> > with the hope of understanding, at least pairwise,
> what things "hang together".   (Yes, there are much,
> much better ways to do this statistically including
> clustering and binary corrected correlation, but the
> audience I am working with asked for this version
> for a specific reason.)
> >
> > (Later on, I would also like to convert to
> percentages of the total unique pop, so the final
> version of the table would be
> >
> >
> >    att1    att2    att3
> >
> > att1         50%       25%
> >
> > att2    50%            50%
> >
> > att3    25%    50%
> >
> >
> > But I can do this in excel if I can get the first
> table out.)
> >
> > I have tried the reshape library, but could not
> get anything resembling this (both on its own, as
> well as feeding in to table()).  (I have also played
> with transposing and using some comments from this
> list from 2002 and 2004, but the questioners appear
> to assume more knowledge than I have in use of R;
> the example in the posting guide was also more
> complex than I was ready for, I'm afraid.)
> >
> > Sample of some of my efforts:
> > library(reshape)
> > melt(ratings,id=c("id"))
> >
> > ds1 <- melt(ratings,id=c("id"))
> > table(ds1$variable, ds1$variable) # returns only
> rowcounts, 3 along diagonal
> > xtabs(formula = value ~ ds1$variable +
> ds1$variable , data=ds1) # returns only a single row
> of collapsed counts, appears to not allow 1 variable
> in multiple uses
> >
> > I suspect I am close, so any nudges in the right
> direction would be helpful.
> >
> > Thanks much, Michael
> >
> > PS: www.rseek.org is very impressive, I heartily
> encourage its use.
> >
> >
> >     [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> Charles C. Berry                        (858)
> 534-2098
>                                           Dept of
> Family/Preventive Medicine
> E mailto:cberry at tajo.ucsd.edu             UC San
> Diego
> http://biostat.ucsd.edu/~cberry/         La Jolla,
> San Diego 92093-0901
> 
> 
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jrkrideau at yahoo.ca  Tue Feb 27 14:33:18 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 27 Feb 2007 08:33:18 -0500 (EST)
Subject: [R] Crosstabbing multiple response data
Message-ID: <39385.60330.qm@web32802.mail.mud.yahoo.com>


--- John Kane <jrkrideau at yahoo.ca> wrote:

> Thanks to everyone for this.  I was looking at the
> same problem last night and just was going to write
> a
> posting to R-help when I saw this.  
> 
> 
> --- Michael Wexler <wexler at yahoo.com> wrote:
> 
> > 
> > Thanks to Charles, Gabor, and a private message
> from
> > Frank E Harrell with some good ideas and help. 
> This
> > crossprod approach was very clever, I would never
> > have thought of it.
> > 
> > Best, Michael
> > 
> > 
> > ----- Original Message ----
> > From: Charles C. Berry <cberry at tajo.ucsd.edu>
> > To: Michael Wexler <wexler at yahoo.com>
> > Cc: r-help at stat.math.ethz.ch
> > Sent: Thursday, February 22, 2007 1:17:44 PM
> > Subject: Re: [R] Crosstabbing multiple response
> data
> > 
> > 
> > > res <- crossprod( as.matrix( ratings[ , -1] ) )
> > > diag(res) <- ""
> > > print(res, quote=F)
> >       att1 att2 att3
> > att1      2    1
> > att2 2         2
> > att3 1    2
> > > 
> > > res2 <- crossprod(as.matrix( ratings[ , -1])) *
> > 100 / nrow( ratings )
> > > res2[] <- paste( res2, "%", sep="" )
> > > diag(res2) <- ""
> > > print(res2, quote=F)
> >       att1 att2 att3
> > att1      50%  25%
> > att2 50%       50%
> > att3 25%  50%
> > >
> > 
> > Be sure to bone up on format and sprintf before
> > taking this into 
> > production.
> > 
> > On Thu, 22 Feb 2007, Michael Wexler wrote:
> > 
> > > Using R version 2.4.1 (2006-12-18) on Windows, I
> > have a dataset which resembles this:
> > >
> > > id    att1    att2    att3
> > > 1    1        1        0
> > > 2    1        0        0
> > > 3    0        1        1
> > > 4    1        1        1
> > >
> > > ratings <- data.frame(id = c(1,2,3,4), att1 =
> > c(1,1,0,1), att2 = c(1,0,0,1), att3 = c(0,1,1,1))
> > >
> > > I would like to get a cross tab of counts of
> > co-ocurrence, which might resemble this:
> > >
> > >    att1    att2    att3
> > > att1         2       1
> > > att2    2            2
> > > att3    1    2
> > >
> > > with the hope of understanding, at least
> pairwise,
> > what things "hang together".   (Yes, there are
> much,
> > much better ways to do this statistically
> including
> > clustering and binary corrected correlation, but
> the
> > audience I am working with asked for this version
> > for a specific reason.)
> > >
> > > (Later on, I would also like to convert to
> > percentages of the total unique pop, so the final
> > version of the table would be
> > >
> > >
> > >    att1    att2    att3
> > >
> > > att1         50%       25%
> > >
> > > att2    50%            50%
> > >
> > > att3    25%    50%
> > >
> > >
> > > But I can do this in excel if I can get the
> first
> > table out.)
> > >
> > > I have tried the reshape library, but could not
> > get anything resembling this (both on its own, as
> > well as feeding in to table()).  (I have also
> played
> > with transposing and using some comments from this
> > list from 2002 and 2004, but the questioners
> appear
> > to assume more knowledge than I have in use of R;
> > the example in the posting guide was also more
> > complex than I was ready for, I'm afraid.)
> > >
> > > Sample of some of my efforts:
> > > library(reshape)
> > > melt(ratings,id=c("id"))
> > >
> > > ds1 <- melt(ratings,id=c("id"))
> > > table(ds1$variable, ds1$variable) # returns only
> > rowcounts, 3 along diagonal
> > > xtabs(formula = value ~ ds1$variable +
> > ds1$variable , data=ds1) # returns only a single
> row
> > of collapsed counts, appears to not allow 1
> variable
> > in multiple uses
> > >
> > > I suspect I am close, so any nudges in the right
> > direction would be helpful.
> > >
> > > Thanks much, Michael
> > >
> > > PS: www.rseek.org is very impressive, I heartily
> > encourage its use.
> > >
> > >
> > >     [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> > reproducible code.
> > >
> > 
> > Charles C. Berry                        (858)
> > 534-2098
> >                                           Dept of
> > Family/Preventive Medicine
> > E mailto:cberry at tajo.ucsd.edu             UC San
> > Diego
> > http://biostat.ucsd.edu/~cberry/         La Jolla,
> > San Diego 92093-0901
> > 
> > 
> > 
> > 
> > 
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> > reproducible code.
> > 
> 
> 
> __________________________________________________
> Do You Yahoo!?

> protection around 
> http://mail.yahoo.com 
>


From Serguei.Kaniovski at wifo.ac.at  Tue Feb 27 14:36:45 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Tue, 27 Feb 2007 14:36:45 +0100
Subject: [R] How to put the dependent variable in GLM proportion model
Message-ID: <OFE1A218E1.33D08AEF-ONC125728F.004AC6D8-C125728F.004AC6E1@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/919c259d/attachment.pl 

From timb at metrumrg.com  Tue Feb 27 15:29:51 2007
From: timb at metrumrg.com (Tim Bergsma)
Date: Tue, 27 Feb 2007 09:29:51 -0500
Subject: [R] fitting the gamma cumulative distribution function
Message-ID: <45E4405F.8040204@metrumrg.com>

Hi.

I have a vector of quantiles and a vector of probabilites that, when 
plotted, look very like the gamma cumulative distribution function.  I 
can guess some shape and scale parameters that give a similar result, 
but I'd rather let the parameters be estimated.  Is there a direct way 
to do this in R?

Thanks,

Tim.

week <- c(0,5,6,7,9,11,14,19,39)
fraction <- c(0,0.23279,0.41093,0.58198,0.77935,0.88057,0.94231,0.98583,1)
weeks <- 1:160/4
plot(weeks, pgamma(weeks,shape=6,scale=1.15),type="l")
points(week,fraction,col="red")


From anja.eggert at uni-rostock.de  Tue Feb 27 15:55:15 2007
From: anja.eggert at uni-rostock.de (Anja Eggert)
Date: Tue, 27 Feb 2007 15:55:15 +0100
Subject: [R] stl function
Message-ID: <45E44653.2020604@biologie.uni-rostock.de>

I want to apply the stl-function to decompose a time series (daily 
measurements over 22 years) into seasonal component, trend and 
residuals. I was able to get the diagrams.
However, I could not find out what are the equations behind it. I.e. it 
is probably not an additive or multiplicative combination of season (as 
sin and cos-functions) and a linear trend?
Furthermore, what are the grey bars on the right hand side of the diagrams?
I would appreciate very much to receive some information or maybe a good 
reference.

Thank you very much,
Anja

-- 
*************************************************************
Dr. Anja Eggert						
Universit?t Rostock
Institut f?r Biowissenschaften
AG Angewandte ?kologie

Albert-Einstein-Str. 3
18059 Rostock

T: ++49 381 498 6094
F: ++49 381 498 6072

e-mail: anja.eggert at uni-rostock.de


From brown_emu at yahoo.com  Tue Feb 27 15:59:51 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 27 Feb 2007 06:59:51 -0800 (PST)
Subject: [R] fitting the gamma cumulative distribution function
In-Reply-To: <45E4405F.8040204@metrumrg.com>
Message-ID: <134506.23515.qm@web39703.mail.mud.yahoo.com>

Hi Tim,

I believe fitdistr() in the MASS package is the function you are looking for.
(example given in help page)...

Best regards,
ST

--- Tim Bergsma <timb at metrumrg.com> wrote:

> Hi.
> 
> I have a vector of quantiles and a vector of probabilites that, when 
> plotted, look very like the gamma cumulative distribution function.  I 
> can guess some shape and scale parameters that give a similar result, 
> but I'd rather let the parameters be estimated.  Is there a direct way 
> to do this in R?
> 
> Thanks,
> 
> Tim.
> 
> week <- c(0,5,6,7,9,11,14,19,39)
> fraction <- c(0,0.23279,0.41093,0.58198,0.77935,0.88057,0.94231,0.98583,1)
> weeks <- 1:160/4
> plot(weeks, pgamma(weeks,shape=6,scale=1.15),type="l")
> points(week,fraction,col="red")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From amy.uhrin at noaa.gov  Tue Feb 27 16:13:16 2007
From: amy.uhrin at noaa.gov (Amy Uhrin)
Date: Tue, 27 Feb 2007 10:13:16 -0500
Subject: [R] rpart minimum sample size
Message-ID: <45E44A8C.1060809@noaa.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/96f61b89/attachment.pl 

From therneau at mayo.edu  Tue Feb 27 16:30:09 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Tue, 27 Feb 2007 09:30:09 -0600 (CST)
Subject: [R] survival analysis using rpart
Message-ID: <200702271530.l1RFU9l07629@hsrnfs-101.mayo.edu>

> I use rpart to predict survival time and have a problem in interpreting the
> output of ?estimated rate?

> (1) Is the ?estimated rate? the estimated hazard rate ratio? 
> (2) How does rpart calculate this rate?
> (3) Suppose I use xpred.rpart(fit, xval=10) to perform 10-fold
> cross-validation using (a) the complete stagec data set and (b) only a
> subset of it, say, using the columns Age, EET, and G2 only. For the i-th
> patient, I am likely to obtain a different estimated rate. How can I
> meaningfully compare both rates? How can say which one is ?better?? 

For questions 1 and 2, you need to read the documentation.
   www.mayo.edu/biostatistics , get technical report #61.  (We should bundle
 this with the package, I suspect)
    or the appropriate chapter in Venables and Ripley, Modern Applied Statistics
 with S, 4th edition.
 
 For question 3, rpart does not have the usual "nested model" likelihood
 ratio tests. I don't know how to say which model is better.
 
 	Terry Therneau


From herrdittmann at yahoo.co.uk  Tue Feb 27 16:39:49 2007
From: herrdittmann at yahoo.co.uk (Bernd Dittmann)
Date: Tue, 27 Feb 2007 15:39:49 +0000
Subject: [R]  garch and extra explanatory variable
Message-ID: <45E450C5.9070507@yahoo.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/e811fb47/attachment.pl 

From azzalini at stat.unipd.it  Tue Feb 27 16:44:03 2007
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Tue, 27 Feb 2007 16:44:03 +0100
Subject: [R] fitting the gamma cumulative distribution function
In-Reply-To: <134506.23515.qm@web39703.mail.mud.yahoo.com>
References: <45E4405F.8040204@metrumrg.com>
	<134506.23515.qm@web39703.mail.mud.yahoo.com>
Message-ID: <20070227164403.aeb91a2f.azzalini@stat.unipd.it>

On Tue, 27 Feb 2007 06:59:51 -0800 (PST), Stephen Tucker wrote:

ST> Hi Tim,
ST> 
ST> I believe fitdistr() in the MASS package is the function you are looking
ST> for. (example given in help page)...
ST> 
ST> Best regards,
ST> ST
ST> 
ST> --- Tim Bergsma <timb a metrumrg.com> wrote:
ST> 
ST> > Hi.
ST> > 
ST> > I have a vector of quantiles and a vector of probabilites that, when 
ST> > plotted, look very like the gamma cumulative distribution function.  I 
ST> > can guess some shape and scale parameters that give a similar result, 
ST> > but I'd rather let the parameters be estimated.  Is there a direct way 
ST> > to do this in R?
ST> > 
ST> > Thanks,
ST> > 
ST> > Tim.
ST> > 
ST> > week <- c(0,5,6,7,9,11,14,19,39)
ST> > fraction <- c
ST> > (0,0.23279,0.41093,0.58198,0.77935,0.88057,0.94231,0.98583,1) weeks <-
ST> > 1:160/4 plot(weeks, pgamma(weeks,shape=6,scale=1.15),type="l")
ST> > points(week,fraction,col="red")
ST> > 

you can decide a "distance" criterion and select the paramers which
minimize that distance, something like
 
  criterion  <- function(param, week, fraction){
                  cdf <- pgamma(week, param[1], param[2])
                  p <- diff(cdf)
                  sum((diff(fraction)-p)^2/p) # or some other function
                  }
              
and then minimize this criterion with respect to the parameters
using optim() or nlminb().

You cannot use fitdistr() because it requires the individual sample values.
In fact you cannot even use MLE for grouped data or X^2, since the sample
size is not  known (at least not reported), hence we do not have the absolute 
frequencies. If the sample size was known, then the problem would change.
  
-- 
Adelchi Azzalini  <azzalini a stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/


From liuwensui at gmail.com  Tue Feb 27 16:44:11 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 27 Feb 2007 10:44:11 -0500
Subject: [R] rpart minimum sample size
In-Reply-To: <45E44A8C.1060809@noaa.gov>
References: <45E44A8C.1060809@noaa.gov>
Message-ID: <1115a2b00702270744n289317eo736666e133e109ea@mail.gmail.com>

amy,
without looking at your actual code, i would suggest you to take a
look at rpart.control()

On 2/27/07, Amy Uhrin <amy.uhrin at noaa.gov> wrote:
> Is there an optimal / minimum sample size for attempting to construct a
> classification tree using /rpart/?
>
> I have 27 seagrass disturbance sites (boat groundings) that have been
> monitored for a number of years.  The monitoring protocol for each site
> is identical.  From the monitoring data, I am able to determine the
> level of recovery that each site has experienced.  Recovery is our
> categorical dependent variable with values of none, low, medium, high
> which are based upon percent seagrass regrowth into the injury over
> time.  I wish to be able to predict the level of recovery of future
> vessel grounding sites based upon a number of categorical / continuous
> predictor variables used here including (but not limited to) such
> parameters as:  sediment grain size, wave exposure, original size
> (volume) of the injury, injury age, injury location.
>
> When I run /rpart/, the data is split into only two terminal nodes based
> solely upon values of the original volume of each injury.  No other
> predictor variables are considered, even though I have included about
> six of them in the model.  When I remove volume from the model the same
> thing happens but with injury area - two terminal nodes are formed based
> upon area values and no other variables appear.  I was hoping that this
> was a programming issue, me being a newbie and all, but I really think
> I've got the code right.  Now I am beginning to wonder if my N is too
> small for this method?
>
> --
> Amy V. Uhrin, Research Ecologist
>
> NOAA, National Ocean Service
> Center for Coastal Fisheries and Habitat Research
> 101 Pivers Island Road
> Beaufort, NC 28516
> (252) 728-8778
> (252) 728-8784 (fax)
> Amy.Uhrin at noaa.gov
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>  \!/ \!/   <:}}}}}><   \!/ \!/  >^<**>^<  \!/ \!/
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From S.Pickett at exeter.ac.uk  Tue Feb 27 16:52:54 2007
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Tue, 27 Feb 2007 15:52:54 -0000 (GMT)
Subject: [R] str() to extract components
Message-ID: <1646.144.173.76.117.1172591574.squirrel@www.webmail.ex.ac.uk>

Hi,

I have been dabbling with str() to extract values from outputs such as
lmer etc and have found it very helpful sometimes.

but only seem to manage to extract the values when the output is one
simple table, any more complicated and I'm stumped :-(

take this example of the extracted coeficients from a lmer analysis...

using str(coef(lmer(resp3~b$age+b$size+b$pcfat+(1|sex), data=b))) yields

Formal class 'lmer.coef' [package "Matrix"] with 3 slots
  ..@ .Data :List of 1
  .. ..$ :`data.frame': 2 obs. of  4 variables:
  .. .. ..$ (Intercept): num [1:2] 1.07 1.13
  .. .. ..$ b$age      : num [1:2] 0.00702 0.00702
  .. .. ..$ b$size     : num [1:2] 0.0343 0.0343
  .. .. ..$ b$pcfat    : num [1:2] 0.0451 0.0451
  ..@ varFac: list()
  ..@ stdErr: num(0)

how do I "get inside" the first table to get the value 1.07 for instance?

Any help much appreciated.


Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From Lukas.Indermaur at eawag.ch  Tue Feb 27 17:04:59 2007
From: Lukas.Indermaur at eawag.ch (Indermaur Lukas)
Date: Tue, 27 Feb 2007 17:04:59 +0100
Subject: [R] fitting of all possible models
References: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
	<45E42E84.6050509@vanderbilt.edu>
Message-ID: <FE8C160D1505B24497FA7C78D4DADACA0478FE@EA-MAIL.eawag.wroot.emp-eaw.ch>

Hi Frank
I fitted a set of 12 candidate models and evaluated the importance of variables based on model averaged coefficients and SE (model weights >=0.9). Variables in my models were not distributed in equal numbers across all models thus I was not able to evaluate the importance of variables just by summing up the AIC-weights of models including a specific variable. Now, why so many models to fit: I was curious, if the ranking in the importance of variables is similar, when just summing up the AIC-weights over an all-possible-models set and looking at the ordered model averaged coefficients (order of CV=SE/coefficient).
 
Any hint for me?
Cheers
Lukas

 

Indermaur Lukas wrote:
> Hi,
> Fitting all possible models (GLM) with 10 predictors will result in loads of (2^10 - 1) models. I want to do that in order to get the importance of variables (having an unbalanced variable design) by summing the up the AIC-weights of models including the same variable, for every variable separately. It's time consuming and annoying to define all possible models by hand.
> 
> Is there a command, or easy solution to let R define the set of all possible models itself? I defined models in the following way to process them with a batch job:
> 
> # e.g. model 1
> preference<- formula(Y~Lwd + N + Sex + YY)                                               
> # e.g. model 2
> preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY) 
> etc.
> etc.
> 
> 
> I appreciate any hint
> Cheers
> Lukas

If you choose the model from amount 2^10 -1 having best AIC, that model
will be badly biased.  Why look at so many?  Pre-specification of
models, or fitting full models with penalization, or using data
reduction (masked to Y) may work better.

Frank

> 
> 
> 
> 
> 
> ???
> Lukas Indermaur, PhD student
> eawag / Swiss Federal Institute of Aquatic Science and Technology
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
> 
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.harrell at vanderbilt.edu  Tue Feb 27 17:08:51 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 27 Feb 2007 10:08:51 -0600
Subject: [R] rpart minimum sample size
In-Reply-To: <45E44A8C.1060809@noaa.gov>
References: <45E44A8C.1060809@noaa.gov>
Message-ID: <45E45793.3030804@vanderbilt.edu>

Amy Uhrin wrote:
> Is there an optimal / minimum sample size for attempting to construct a 
> classification tree using /rpart/?
> 
> I have 27 seagrass disturbance sites (boat groundings) that have been 
> monitored for a number of years.  The monitoring protocol for each site 
> is identical.  From the monitoring data, I am able to determine the 
> level of recovery that each site has experienced.  Recovery is our 
> categorical dependent variable with values of none, low, medium, high 
> which are based upon percent seagrass regrowth into the injury over 
> time.  I wish to be able to predict the level of recovery of future 
> vessel grounding sites based upon a number of categorical / continuous 
> predictor variables used here including (but not limited to) such 
> parameters as:  sediment grain size, wave exposure, original size 
> (volume) of the injury, injury age, injury location.
> 
> When I run /rpart/, the data is split into only two terminal nodes based 
> solely upon values of the original volume of each injury.  No other 
> predictor variables are considered, even though I have included about 
> six of them in the model.  When I remove volume from the model the same 
> thing happens but with injury area - two terminal nodes are formed based 
> upon area values and no other variables appear.  I was hoping that this 
> was a programming issue, me being a newbie and all, but I really think 
> I've got the code right.  Now I am beginning to wonder if my N is too 
> small for this method?
> 

In my experience N needs to be around 20,000 to get both good accuracy 
and replicability of patterns if the number of potential predictors is 
not tiny.  In general, the R^2 from rpart is not competitive with that 
from an intelligently fitted regression model.  It's just a difficult 
problem, when relying on a single tree (hence the popularity of random 
forests, bagging, boosting).

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From gunter.berton at gene.com  Tue Feb 27 17:12:13 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Tue, 27 Feb 2007 08:12:13 -0800
Subject: [R] fitting of all possible models
In-Reply-To: <45E42E84.6050509@vanderbilt.edu>
Message-ID: <002301c75a8a$0ad338f0$4d908980@gne.windows.gene.com>

... Below

-- Bert 

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank E Harrell Jr
Sent: Tuesday, February 27, 2007 5:14 AM
To: Indermaur Lukas
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] fitting of all possible models

Indermaur Lukas wrote:
> Hi,
> Fitting all possible models (GLM) with 10 predictors will result in loads
of (2^10 - 1) models. I want to do that in order to get the importance of
variables (having an unbalanced variable design) by summing the up the
AIC-weights of models including the same variable, for every variable
separately. It's time consuming and annoying to define all possible models
by hand. 
>  
> Is there a command, or easy solution to let R define the set of all
possible models itself? I defined models in the following way to process
them with a batch job:
>  
> # e.g. model 1
> preference<- formula(Y~Lwd + N + Sex + YY)

> # e.g. model 2
> preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY)  
> etc.
> etc.
>  
>  
> I appreciate any hint
> Cheers
> Lukas

If you choose the model from amount 2^10 -1 having best AIC, that model 
will be badly biased.  Why look at so many?  Pre-specification of 
models, or fitting full models with penalization, 

--- ...the rub being how much to penalize. My impression from what I've read
is, for prediction, close to "the more, the better is the predictor..." .
Nature rewards parsimony.

Cheers,
Bert


Frank

>  
>  
>  
>  
>  
> ??? 
> Lukas Indermaur, PhD student 
> eawag / Swiss Federal Institute of Aquatic Science and Technology 
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
>  
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15 
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From j.van_den_hoff at fzd.de  Tue Feb 27 17:28:20 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Tue, 27 Feb 2007 17:28:20 +0100
Subject: [R] str() to extract components
In-Reply-To: <1646.144.173.76.117.1172591574.squirrel@www.webmail.ex.ac.uk>
References: <1646.144.173.76.117.1172591574.squirrel@www.webmail.ex.ac.uk>
Message-ID: <20070227162820.GA27453@marco.fz-rossendorf.de>

On Tue, Feb 27, 2007 at 03:52:54PM -0000, Simon Pickett wrote:
> Hi,
> 
> I have been dabbling with str() to extract values from outputs such as
> lmer etc and have found it very helpful sometimes.
> 
> but only seem to manage to extract the values when the output is one
> simple table, any more complicated and I'm stumped :-(
> 
> take this example of the extracted coeficients from a lmer analysis...
> 
> using str(coef(lmer(resp3~b$age+b$size+b$pcfat+(1|sex), data=b))) yields
> 
> Formal class 'lmer.coef' [package "Matrix"] with 3 slots
>   ..@ .Data :List of 1
>   .. ..$ :`data.frame': 2 obs. of  4 variables:
>   .. .. ..$ (Intercept): num [1:2] 1.07 1.13
>   .. .. ..$ b$age      : num [1:2] 0.00702 0.00702
>   .. .. ..$ b$size     : num [1:2] 0.0343 0.0343
>   .. .. ..$ b$pcfat    : num [1:2] 0.0451 0.0451
>   ..@ varFac: list()
>   ..@ stdErr: num(0)
> 
> how do I "get inside" the first table to get the value 1.07 for instance?
> 
> Any help much appreciated.
> 
may `unlist' would be enough?


From andy1983 at excite.com  Tue Feb 27 17:38:00 2007
From: andy1983 at excite.com (andy1983)
Date: Tue, 27 Feb 2007 08:38:00 -0800 (PST)
Subject: [R] read.csv size limits
Message-ID: <9186136.post@talk.nabble.com>


I have been using the read.csv function for a while now without any problems.
My files are usually 20-50 MBs and they take up to a minute to import. They
have all been under 50,000 rows and under 100 columns.

Recently, I tried importing a file of a similar size (which means about the
same amount of data), but with ~500,000 columns and ~20 rows. The process is
taking forever (~1 hour so far). In Task Manager, I see the CPU is at max,
but memory slows down to a halt at around 50 MBs (far below memory limit).

Is this normal? Is there a way to optimize this operation or at least check
the progress? Will this take 2 hours or 200 hours?

All I was trying to do is transpose my extra-wide table with a process that
I assumed would take 5 minutes. Maybe R is not the solution I am looking
for?

Thanks.

-- 
View this message in context: http://www.nabble.com/read.csv-size-limits-tf3302366.html#a9186136
Sent from the R help mailing list archive at Nabble.com.


From gavin.simpson at ucl.ac.uk  Tue Feb 27 17:47:35 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 27 Feb 2007 16:47:35 +0000
Subject: [R] stl function
In-Reply-To: <45E44653.2020604@biologie.uni-rostock.de>
References: <45E44653.2020604@biologie.uni-rostock.de>
Message-ID: <1172594855.19931.3.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-02-27 at 15:55 +0100, Anja Eggert wrote:
> I want to apply the stl-function to decompose a time series (daily 
> measurements over 22 years) into seasonal component, trend and 
> residuals. I was able to get the diagrams.
> However, I could not find out what are the equations behind it. I.e. it 
> is probably not an additive or multiplicative combination of season (as 
> sin and cos-functions) and a linear trend?
> Furthermore, what are the grey bars on the right hand side of the diagrams?
> I would appreciate very much to receive some information or maybe a good 
> reference.
> 
> Thank you very much,
> Anja
> 

?stl tells you all you need to know to answer this, including the
reference to the academic publication that describes the method.

?plot.stl tells you that the grey bars are range bars - they are used to
assess the relative magnitude of various decomposed components.

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From rsb at wsu.edu  Tue Feb 27 17:48:04 2007
From: rsb at wsu.edu (Bricklemyer, Ross S)
Date: Tue, 27 Feb 2007 08:48:04 -0800
Subject: [R] compiling issues with Mandriva Linux 2007 Discovery
Message-ID: <2FC987BC0B90B24786CAF43DD3F5719C814299@CRU105.cahe.ad.wsu.edu>

All,

I am a new user to Linux but I am familiar with R.  I have previously
used and installed R on a Windows platform without problems.  I recently
set up a dual boot system (XP_64, Mandriva) to run R on a Linux platform
in order to more efficiently handle large datasets.  I have not done
compiling before, but read the R instructions and followed to my best
ability.  I downloaded the most recent tar and unpacked it into
/usr/local/R_HOME.  I was able to run ./configure and added the
additional Linux packages necessary to compile.  The problem arises
using the 'make' command.  When I run make I get an error that there is
not a target or a makefile.  There is, however, Makefile.in in the
directory.  I think I am close to getting this installed, but I am
stuck.  Any help would be greatly appreciated.  I do not suppose that
any of the pre compiled Linux versions (i.e. Debian, SuSe) available on
CRAN mirrors would run on a Mandriva distro?

Best,
Ross

*******************************************************************
Ross Bricklemyer
Dept. of Crop and Soil Sciences
Washington State University
201 Johnson Hall
Pullman, WA 99164-6420
Work: 509.335.3661
Cell/Home: 406.570.8576
Fax: 509.335.8674
Email: rsb at wsu.edu


From f.harrell at vanderbilt.edu  Tue Feb 27 17:55:43 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 27 Feb 2007 10:55:43 -0600
Subject: [R] fitting of all possible models
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478FE@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
	<45E42E84.6050509@vanderbilt.edu>
	<FE8C160D1505B24497FA7C78D4DADACA0478FE@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <45E4628F.2050202@vanderbilt.edu>

Indermaur Lukas wrote:
> Hi Frank
> I fitted a set of 12 candidate models and evaluated the importance of variables based on model averaged coefficients and SE (model weights >=0.9). Variables in my models were not distributed in equal numbers across all models thus I was not able to evaluate the importance of variables just by summing up the AIC-weights of models including a specific variable. Now, why so many models to fit: I was curious, if the ranking in the importance of variables is similar, when just summing up the AIC-weights over an all-possible-models set and looking at the ordered model averaged coefficients (order of CV=SE/coefficient).
>  
> Any hint for me?
> Cheers
> Lukas

I have seen the literature on Bayesian model averaging which uses 
weights from Bayes factors, related to BIC, but not the approach you are 
using.

Frank

> 
>  
> 
> Indermaur Lukas wrote:
>> Hi,
>> Fitting all possible models (GLM) with 10 predictors will result in loads of (2^10 - 1) models. I want to do that in order to get the importance of variables (having an unbalanced variable design) by summing the up the AIC-weights of models including the same variable, for every variable separately. It's time consuming and annoying to define all possible models by hand.
>>
>> Is there a command, or easy solution to let R define the set of all possible models itself? I defined models in the following way to process them with a batch job:
>>
>> # e.g. model 1
>> preference<- formula(Y~Lwd + N + Sex + YY)                                               
>> # e.g. model 2
>> preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY) 
>> etc.
>> etc.
>>
>>
>> I appreciate any hint
>> Cheers
>> Lukas
> 
> If you choose the model from amount 2^10 -1 having best AIC, that model
> will be badly biased.  Why look at so many?  Pre-specification of
> models, or fitting full models with penalization, or using data
> reduction (masked to Y) may work better.
> 
> Frank
> 
>>
>>
>>
>>
>> ???
>> Lukas Indermaur, PhD student
>> eawag / Swiss Federal Institute of Aquatic Science and Technology
>> ECO - Department of Aquatic Ecology
>> ?berlandstrasse 133
>> CH-8600 D?bendorf
>> Switzerland
>>
>> Phone: +41 (0) 71 220 38 25
>> Fax    : +41 (0) 44 823 53 15
>> Email: lukas.indermaur at eawag.ch
>> www.lukasindermaur.ch


From f.harrell at vanderbilt.edu  Tue Feb 27 17:58:12 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 27 Feb 2007 10:58:12 -0600
Subject: [R] fitting of all possible models
In-Reply-To: <002301c75a8a$0ad338f0$4d908980@gne.windows.gene.com>
References: <002301c75a8a$0ad338f0$4d908980@gne.windows.gene.com>
Message-ID: <45E46324.9070404@vanderbilt.edu>

Bert Gunter wrote:
> ... Below
> 
> -- Bert 
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank E Harrell Jr
> Sent: Tuesday, February 27, 2007 5:14 AM
> To: Indermaur Lukas
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] fitting of all possible models
> 
> Indermaur Lukas wrote:
>> Hi,
>> Fitting all possible models (GLM) with 10 predictors will result in loads
> of (2^10 - 1) models. I want to do that in order to get the importance of
> variables (having an unbalanced variable design) by summing the up the
> AIC-weights of models including the same variable, for every variable
> separately. It's time consuming and annoying to define all possible models
> by hand. 
>>  
>> Is there a command, or easy solution to let R define the set of all
> possible models itself? I defined models in the following way to process
> them with a batch job:
>>  
>> # e.g. model 1
>> preference<- formula(Y~Lwd + N + Sex + YY)
> 
>> # e.g. model 2
>> preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY)  
>> etc.
>> etc.
>>  
>>  
>> I appreciate any hint
>> Cheers
>> Lukas
> 
> If you choose the model from amount 2^10 -1 having best AIC, that model 
> will be badly biased.  Why look at so many?  Pre-specification of 
> models, or fitting full models with penalization, 
> 
> --- ...the rub being how much to penalize. My impression from what I've read
> is, for prediction, close to "the more, the better is the predictor..." .
> Nature rewards parsimony.
> 
> Cheers,
> Bert

Bert,

In my experience nature rewards complexity, if done right.  See Savage's 
antiparsimony principle  -Frank

@Article{gre00whe,
   author =               {Greenland, Sander},
   title =                {When should epidemiologic regressions use 
random coeff
icients?},
   journal =      Biometrics,
   year =                 2000,
   volume =               56,
   pages =                {915-921},
   annote =               {Bayesian methods;causal inference;empirical Bayes
estimators;epidemiologic method;hierarchical regression;mixed
models;multilevel modeling;random-coefficient
regression;shrinkage;variance components;use of statistics in
epidemiology is largely primitive;stepwise variable selection on
confounders leaves important confounders uncontrolled;composition
matrix;example with far too many significant predictors with many
regression coefficients absurdly inflated when
overfit;lack of evidence for dietary effects mediated through
constituents;shrinkage instead of variable selection;larger effect on
confidence interval width than on point estimates with variable
selection;uncertainty about variance of random effects is just
uncertainty about prior opinion;estimation of variance is
pointless;instead the analysis shuld be repeated using different
values;"if one feels compelled to estimate $\tau^2$, I would recommend
giving it a proper prior concentrated amount contextually reasonable
values";claim about ordinary MLE being unbiased is misleading because
it assumes the model is correct and is the only model
entertained;shrinkage towards compositional model;"models need to be
complex to capture uncertainty about the relations...an honest
uncertainty assessment requires parameters for all effects that we
know may be present.  This advice is implicit in an antiparsimony
principle often attributed to L. J. Savage 'All models should be as
big as an elephant (see Draper, 1995)'".  See also gus06per.}
}

> 
> 
> Frank
> 
>>  
>>  
>>  
>>  
>>  
>> ??? 
>> Lukas Indermaur, PhD student 
>> eawag / Swiss Federal Institute of Aquatic Science and Technology 
>> ECO - Department of Aquatic Ecology
>> ?berlandstrasse 133
>> CH-8600 D?bendorf
>> Switzerland
>>  
>> Phone: +41 (0) 71 220 38 25
>> Fax    : +41 (0) 44 823 53 15 
>> Email: lukas.indermaur at eawag.ch
>> www.lukasindermaur.ch
>>


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From joris.dewolf at cropdesign.com  Tue Feb 27 18:11:24 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Tue, 27 Feb 2007 18:11:24 +0100
Subject: [R] str() to extract components
In-Reply-To: <1646.144.173.76.117.1172591574.squirrel@www.webmail.ex.ac.uk>
Message-ID: <OF13B5BE1C.1A68BA4A-ONC125728F.005DF0ED-C125728F.005E6DCD@basf-c-s.be>


mod <- lmer(resp3~b$age+b$size+b$pcfat+(1|sex), data=b)
coef(mod)[1]$Subject[1,1]


r-help-bounces at stat.math.ethz.ch wrote on 27/02/2007 16:52:54:

> Hi,
>
> I have been dabbling with str() to extract values from outputs such as
> lmer etc and have found it very helpful sometimes.
>
> but only seem to manage to extract the values when the output is one
> simple table, any more complicated and I'm stumped :-(
>
> take this example of the extracted coeficients from a lmer analysis...
>
> using str(coef(lmer(resp3~b$age+b$size+b$pcfat+(1|sex), data=b))) yields
>
> Formal class 'lmer.coef' [package "Matrix"] with 3 slots
>   ..@ .Data :List of 1
>   .. ..$ :`data.frame': 2 obs. of  4 variables:
>   .. .. ..$ (Intercept): num [1:2] 1.07 1.13
>   .. .. ..$ b$age      : num [1:2] 0.00702 0.00702
>   .. .. ..$ b$size     : num [1:2] 0.0343 0.0343
>   .. .. ..$ b$pcfat    : num [1:2] 0.0451 0.0451
>   ..@ varFac: list()
>   ..@ stdErr: num(0)
>
> how do I "get inside" the first table to get the value 1.07 for instance?
>
> Any help much appreciated.
>
>
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From albmont at centroin.com.br  Tue Feb 27 18:24:35 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Tue, 27 Feb 2007 15:24:35 -0200
Subject: [R] ts; decompose; plot and title
Message-ID: <20070227171810.M38943@centroin.com.br>

Is there any way to give a "decent" title after I plot something
generated by decompose?

For example:

# generate something with period 12
x <- rnorm(600) + sin(2 * pi * (1:600) / 12)

# transform to a monthy time series
y <- ts(x, frequency=12, start=c(1950,1))

# decompose
z <- decompose(y)

# plot
plot(z)

Now, the title is the ugly "Decomposition of additive time series".
How can do this with a decent title, like "Analysis of UFO abductions"?

Alberto Monteiro


From ggrothendieck at gmail.com  Tue Feb 27 18:34:12 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Feb 2007 12:34:12 -0500
Subject: [R] ts; decompose; plot and title
In-Reply-To: <20070227171810.M38943@centroin.com.br>
References: <20070227171810.M38943@centroin.com.br>
Message-ID: <971536df0702270934k277d7633xe4d01a92a97ba894@mail.gmail.com>

Try this:

plot(cbind(observed = z$random +
    z$trend * z$seasonal, trend = z$trend, seasonal = z$seasonal,
        random = z$random), main = "My title")

Change the * to + if your setup is additive.

On 2/27/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> Is there any way to give a "decent" title after I plot something
> generated by decompose?
>
> For example:
>
> # generate something with period 12
> x <- rnorm(600) + sin(2 * pi * (1:600) / 12)
>
> # transform to a monthy time series
> y <- ts(x, frequency=12, start=c(1950,1))
>
> # decompose
> z <- decompose(y)
>
> # plot
> plot(z)
>
> Now, the title is the ugly "Decomposition of additive time series".
> How can do this with a decent title, like "Analysis of UFO abductions"?
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jpfededa at gmail.com  Tue Feb 27 18:46:59 2007
From: jpfededa at gmail.com (Juan Pablo Fededa)
Date: Tue, 27 Feb 2007 14:46:59 -0300
Subject: [R] ordered matrix question
Message-ID: <7bff68f40702270946x65769440l47b6f8fcade0f0f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/b24bb23c/attachment.pl 

From morlon.helene at gmail.com  Tue Feb 27 18:52:50 2007
From: morlon.helene at gmail.com (MORLON)
Date: Tue, 27 Feb 2007 09:52:50 -0800
Subject: [R] RDA and trend surface regression
In-Reply-To: <1172561228.12881.7.camel@biol102145.oulu.fi>
References: <1172561228.12881.7.camel@biol102145.oulu.fi>
Message-ID: <004901c75a98$1a0989b0$c08aeca9@MORLONH>

Thanks a lot for your answers,

I am concerned by your advice not to use polynomial constraints, or to use
QDA instead of RDA. My final goal is to perform variation partitioning using
partial RDA to assess the relative importance of environmental vs spatial
variables. For the spatial analyses, trend surface analysis (polynomial
constraints) is recommended in Legendre and Legendre 1998 (p739). Is there a
better method to integrate space as an explanatory variable in a variation
partitioning analyses? 

Also, I don't understand this: when I test for the significant contribution
of monomials (forward elimination)

>anova(rda(Helling ~ I(x^2)+Condition(x)+Condition(y)))

performs the permutation test as expected, whereas 

>anova(rda(Helling ~ I(y^2)+Condition(x)+Condition(y)))

Returns this error message:

Error in "names<-.default"(`*tmp*`, value = "Model") : 
        attempt to set an attribute on NULL

Thanks again for your help
Kind regards,
Helene

Helene MORLON
University of California, Merced

-----Original Message-----
From: Jari Oksanen [mailto:jarioksa at sun3.oulu.fi] 
Sent: Monday, February 26, 2007 11:27 PM
To: r-help at stat.math.ethz.ch
Cc: morlon.helene at gmail.com
Subject: [R] RDA and trend surface regression


> 'm performing RDA on plant presence/absence data, constrained by
> geographical locations. I'd like to constrain the RDA by the "extended
> matrix of geographical coordinates" -ie the matrix of geographical
> coordinates completed by adding all terms of a cubic trend surface
> regression- . 
> 
> This is the command I use (package vegan):
> 
>  
> 
> >rda(Helling ~ x+y+x*y+x^2+y^2+x*y^2+y*x^2+x^3+y^3) 
> 
>  
> 
> where Helling is the matrix of Hellinger-transformed presence/absence data
> 
> The result returned by R is exactly the same as the one given by:
> 
>  
> 
> >anova(rda(Helling ~ x+y)
> 
>  
> 
> Ie the quadratic and cubic terms are not taken into account
> 

You must *I*solate the polynomial terms with function I ("AsIs") so that
they are not interpreted as formula operators:

rda(Helling ~ x + y + I(x*y) + I(x^2) + I(y^2) + I(x*y^2) + I(y*x^2) +
I(x^3) + I(y^3))

If you don't have the interaction terms, then it is easier and better
(numerically) to use poly():

rda(Helling ~ poly(x, 3) + poly(y, 3))

Another issue is that in my opinion using polynomial constraints is an
Extremely Bad Idea(TM).

cheers, Jari Oksanen


From gavin.simpson at ucl.ac.uk  Tue Feb 27 18:53:17 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 27 Feb 2007 17:53:17 +0000
Subject: [R] ts; decompose; plot and title
In-Reply-To: <20070227171810.M38943@centroin.com.br>
References: <20070227171810.M38943@centroin.com.br>
Message-ID: <1172598797.19931.58.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-02-27 at 15:24 -0200, Alberto Monteiro wrote:
> Is there any way to give a "decent" title after I plot something
> generated by decompose?
> 
> For example:
> 
> # generate something with period 12
> x <- rnorm(600) + sin(2 * pi * (1:600) / 12)
> 
> # transform to a monthy time series
> y <- ts(x, frequency=12, start=c(1950,1))
> 
> # decompose
> z <- decompose(y)
> 
> # plot
> plot(z)
> 
> Now, the title is the ugly "Decomposition of additive time series".
> How can do this with a decent title, like "Analysis of UFO abductions"?
> 
> Alberto Monteiro

It is because plot.decompose.ts decides to impose it's own title for
some reason (using getAnywhere(plot.decompose.ts) to get the function
definition):

function (x, ...)
{
    plot(cbind(observed = x$random + if (x$type == "additive")
        x$trend + x$seasonal
    else x$trend * x$seasonal, trend = x$trend, seasonal = x$seasonal,
        random = x$random), main = paste("Decomposition of",
        x$type, "time series"), ...)
}

I'd just write your own wrapper instead, using plot.decompose.ts, along
the lines of:

decomp.plot <- function(x, main = NULL, ...)
{
    if(is.null(main))
	main <- paste("Decomposition of", x$type, "time series")
    plot(cbind(observed = x$random + if (x$type == "additive")
        x$trend + x$seasonal
    else x$trend * x$seasonal, trend = x$trend, seasonal = x$seasonal,
        random = x$random), main = main, ...)
}

#then to complete your example:

# generate something with period 12
x <- rnorm(600) + sin(2 * pi * (1:600) / 12)

# transform to a monthy time series
y <- ts(x, frequency=12, start=c(1950,1))

# decompose
z <- decompose(y)

# plot
decomp.plot(z, main = "Analysis of UFO abductions")

Perhaps you could also file a bug report under the wish list category,
showing your example and the fact that

plot(z, main = "Analysis of UFO abductions") 

gives this error:

Error in plotts(x = x, y = y, plot.type = plot.type, xy.labels =
xy.labels,  :
        formal argument "main" matched by multiple actual arguments

It isn't really a bug, but an infelicity in the way the function
currently works - my decomp.plot function may even be a suitable patch
or maybe the following is better:

decomp.plot2 <- function(x, main, ...)
{
    if(missing(main))
	main <- paste("Decomposition of", x$type, "time series")
    plot(cbind(observed = x$random + if (x$type == "additive")
        x$trend + x$seasonal
    else x$trend * x$seasonal, trend = x$trend, seasonal = x$seasonal,
        random = x$random), main = main, ...)
}

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ibanez at bioef.org  Tue Feb 27 18:58:48 2007
From: ibanez at bioef.org (Berta)
Date: Tue, 27 Feb 2007 18:58:48 +0100
Subject: [R] sample size for 2-sample proportion tests
Message-ID: <015101c75a98$efdf5bf0$6601a8c0@BIOEF.ORG>

Hi R-users,
I want to calculate the sample size needed to carry out a  2-sample 
proprotion test.
I have the hypotesized treatment probability of success (0.80), the 
hypotesized control probability of success (0.05), and also de proportion of 
the sample devoted to treated group (5%), (fraction=rho=0.05, n2/n1=19). 
Using the Hsmisc library, it seemss that I can use bsamsize (option 1) or 
samplesize.bin (option 2, alpha=0.05 or option 3 alpha=0.05/2, I am not sure 
after reading the help page) and I can use STATA (option 4).

library(Hmisc)

#OPTION 1:
bsamsize(p1=0.8, p2=0.05, fraction=0.05, alpha=.05, power=.9)
#          n1  =2.09,       n2=39.7,  TOTAL=42

#OPTION 2:
samplesize.bin(alpha=0.05, beta=0.9, pit=0.8, pic=0.05, rho=0.05)
#  n= 58, TOTAL= 58

#OPTION 3:
samplesize.bin(alpha=0.025, beta=0.9, pit=0.8, pic=0.05, rho=0.05)
# n= 72, TOTAL= 72

#OPTION 4:
sampsi 0.8 0.05, p(0.90) a(0.05) r(19)
#  n1=4,  n2 = 76   TOTAL=80

Can the method used produces the differences (42 vs 72 vs 80)? Can somebody 
give me hints about the possible reasons (asymptotic-exact distribution- 
continuity correction-my own error)? Which method would be recomended?

Thanks a lot in advance,

Berta


From gavin.simpson at ucl.ac.uk  Tue Feb 27 19:09:25 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 27 Feb 2007 18:09:25 +0000
Subject: [R] ordered matrix question
In-Reply-To: <7bff68f40702270946x65769440l47b6f8fcade0f0f7@mail.gmail.com>
References: <7bff68f40702270946x65769440l47b6f8fcade0f0f7@mail.gmail.com>
Message-ID: <1172599765.19931.62.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-02-27 at 14:46 -0300, Juan Pablo Fededa wrote:
> Hi all,
> 
> Is there an easy way to generate an object wich will be the same matrix, but
> ordered by de cfp value?
> The data frame consists of numeric columns:
> "Block"    "X"    "Y"    "cfp"    "yfp"    "ID"
> 0    524    244    213.41795    7.18482109    1
> 0    556    270    65.383904    9.568372661    2
> 0    528    316    40.789474    5.573732175    3
> 0    642    432    135.81734    12.40134427    4
> 0    716    430    34.359135    3.944923077    5
> 0    894    362    109.63158    3.197160316    6
> 0    958    130    63.98452    3.396452004    7
> 0    506    92    83.513931    9.105496856    8
> 0    476    464    91.674919    9.178089414    9
> 0    364    426    44.13932    2.068334364    10
> 
> Thanks in advance,

Yes, see ?order. E.g.:

mat <- scan()
0    524    244    213.41795    7.18482109    1
0    556    270    65.383904    9.568372661    2
0    528    316    40.789474    5.573732175    3
0    642    432    135.81734    12.40134427    4
0    716    430    34.359135    3.944923077    5
0    894    362    109.63158    3.197160316    6
0    958    130    63.98452    3.396452004    7
0    506    92    83.513931    9.105496856    8
0    476    464    91.674919    9.178089414    9
0    364    426    44.13932    2.068334364    10


mat <- data.frame(matrix(mat, ncol = 6, byrow = TRUE))
names(mat) <- c("Block", "X", "Y", "cfp", "yfp", "ID")
mat
mat[order(mat$cfp), ]

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From Max.Kuhn at pfizer.com  Tue Feb 27 19:13:08 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Tue, 27 Feb 2007 13:13:08 -0500
Subject: [R] RDA and trend surface regression
In-Reply-To: <004901c75a98$1a0989b0$c08aeca9@MORLONH>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307A25406@groamrexm03.amer.pfizer.com>

Helene,

My point was only that RDA may fit a quadratic model for the terms
specified in your model. The terms that you had specified were already
higher order polynomials (some cubic). So a QDA classifier with the
model terms that you specified my be a fifth order polynomial in the
original data. I don't know the reference you cite or even the
subject-matter specifics. I'm just a simple cave man (for you SNL fans).
But I do know that there are more reliable ways to get nonlinear
classification boundaries than using x^5. 

If you want a quadratic model, I would suggest that you use QDA with the
predictors in the original units (or see Hastie's book for a good
example of using higher order terms with LDA). 

Looking at your email, you want a "a variation partitioning analyses".
RDA works best as a classification technique. Perhaps a multivariate
ANOVA model may be a more direct way to meet your needs. There is a
connection between LDA and some multivariate linear models, but I don't
know of a similar connection to RDA.

Max

-----Original Message-----
From: MORLON [mailto:morlon.helene at gmail.com] 
Sent: Tuesday, February 27, 2007 12:53 PM
To: 'Jari Oksanen'; r-help at stat.math.ethz.ch
Cc: Kuhn, Max
Subject: RE: [R] RDA and trend surface regression

Thanks a lot for your answers,

I am concerned by your advice not to use polynomial constraints, or to
use
QDA instead of RDA. My final goal is to perform variation partitioning
using
partial RDA to assess the relative importance of environmental vs
spatial
variables. For the spatial analyses, trend surface analysis (polynomial
constraints) is recommended in Legendre and Legendre 1998 (p739). Is
there a
better method to integrate space as an explanatory variable in a
variation
partitioning analyses? 

Also, I don't understand this: when I test for the significant
contribution
of monomials (forward elimination)

>anova(rda(Helling ~ I(x^2)+Condition(x)+Condition(y)))

performs the permutation test as expected, whereas 

>anova(rda(Helling ~ I(y^2)+Condition(x)+Condition(y)))

Returns this error message:

Error in "names<-.default"(`*tmp*`, value = "Model") : 
        attempt to set an attribute on NULL

Thanks again for your help
Kind regards,
Helene

Helene MORLON
University of California, Merced

-----Original Message-----
From: Jari Oksanen [mailto:jarioksa at sun3.oulu.fi] 
Sent: Monday, February 26, 2007 11:27 PM
To: r-help at stat.math.ethz.ch
Cc: morlon.helene at gmail.com
Subject: [R] RDA and trend surface regression


> 'm performing RDA on plant presence/absence data, constrained by
> geographical locations. I'd like to constrain the RDA by the "extended
> matrix of geographical coordinates" -ie the matrix of geographical
> coordinates completed by adding all terms of a cubic trend surface
> regression- . 
> 
> This is the command I use (package vegan):
> 
>  
> 
> >rda(Helling ~ x+y+x*y+x^2+y^2+x*y^2+y*x^2+x^3+y^3) 
> 
>  
> 
> where Helling is the matrix of Hellinger-transformed presence/absence
data
> 
> The result returned by R is exactly the same as the one given by:
> 
>  
> 
> >anova(rda(Helling ~ x+y)
> 
>  
> 
> Ie the quadratic and cubic terms are not taken into account
> 

You must *I*solate the polynomial terms with function I ("AsIs") so that
they are not interpreted as formula operators:

rda(Helling ~ x + y + I(x*y) + I(x^2) + I(y^2) + I(x*y^2) + I(y*x^2) +
I(x^3) + I(y^3))

If you don't have the interaction terms, then it is easier and better
(numerically) to use poly():

rda(Helling ~ poly(x, 3) + poly(y, 3))

Another issue is that in my opinion using polynomial constraints is an
Extremely Bad Idea(TM).

cheers, Jari Oksanen

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From F.MENDIBURU at CGIAR.ORG  Tue Feb 27 19:13:18 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Tue, 27 Feb 2007 13:13:18 -0500
Subject: [R] ordered matrix question
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A2014069CD@webmail.cip.cgiar.org>

Juan Pablo,

X is data.frame or matrix
X <- X[order(X[,4]),]
options see help(order)

Felipe de Mendiburu

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Juan Pablo Fededa
Sent: Tuesday, February 27, 2007 12:47 PM
To: R-help at stat.math.ethz.ch
Subject: [R] ordered matrix question


Hi all,

Is there an easy way to generate an object wich will be the same matrix, but
ordered by de cfp value?
The data frame consists of numeric columns:
"Block"    "X"    "Y"    "cfp"    "yfp"    "ID"
0    524    244    213.41795    7.18482109    1
0    556    270    65.383904    9.568372661    2
0    528    316    40.789474    5.573732175    3
0    642    432    135.81734    12.40134427    4
0    716    430    34.359135    3.944923077    5
0    894    362    109.63158    3.197160316    6
0    958    130    63.98452    3.396452004    7
0    506    92    83.513931    9.105496856    8
0    476    464    91.674919    9.178089414    9
0    364    426    44.13932    2.068334364    10

Thanks in advance,


Juan Pablo

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From res90sx5 at verizon.net  Tue Feb 27 19:13:23 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Tue, 27 Feb 2007 10:13:23 -0800
Subject: [R] ordered matrix question
In-Reply-To: <7bff68f40702270946x65769440l47b6f8fcade0f0f7@mail.gmail.com>
Message-ID: <006301c75a9a$fb2b97b0$0201a8c0@Aragorn>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Juan Pablo Fededa
> Sent: Tuesday, February 27, 2007 9:47 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] ordered matrix question
> 
> Hi all,
> 
> Is there an easy way to generate an object wich will be the same matrix, but
> ordered by de cfp value?
> The data frame consists of numeric columns:
> "Block"    "X"    "Y"    "cfp"    "yfp"    "ID"
> 0    524    244    213.41795    7.18482109    1
> 0    556    270    65.383904    9.568372661    2
> 0    528    316    40.789474    5.573732175    3
> 0    642    432    135.81734    12.40134427    4
> 0    716    430    34.359135    3.944923077    5
> 0    894    362    109.63158    3.197160316    6
> 0    958    130    63.98452    3.396452004    7
> 0    506    92    83.513931    9.105496856    8
> 0    476    464    91.674919    9.178089414    9
> 0    364    426    44.13932    2.068334364    10
> 
> Thanks in advance,
> 
> 
> Juan Pablo

Juan,

Look at ?order.  Something like this should work

your.df[order(your.df$cfp, decreasing=TRUE), ]

Hope this is helpful,

Dan

Daniel Nordlund
Bothell, WA  USA


From lochapoka at web.de  Tue Feb 27 19:24:31 2007
From: lochapoka at web.de (Volker Bahn)
Date: Tue, 27 Feb 2007 13:24:31 -0500
Subject: [R] plotting rpart objects - fancy option
Message-ID: <45E4775F.7020209@web.de>

Hi all,

I'm trying to create nice plots of rpart objects. In particular, I'd 
like to use the "fancy" option to text() that creates ellipses and 
rectangles at the splits and endnotes, respectively. This worked fine in 
the past, but now the ellipses do not interrupt the original tree lines 
anymore but overlay them (see attached ps file). I'd like it to look the 
way Figure 18 on page 49 of the rpart report "An Introduction to 
Recursive Partitioning Using the RPART Routines" looks and can't figure 
out why it doesn't.

I created the attached ps with the command:

 > post(tree)

which according to the report is pretty much equivalent to:

 > plot(tree, uniform = T, branch = 0.2, compress = T, margin = 0.1)
 > text(tree, all = T, use.n=T, fancy = T)

As for my system info:

 > sessionInfo()
R version 2.4.1 (2006-12-18)
i386-pc-mingw32

locale:
LC_COLLATE=English_Canada.1252;LC_CTYPE=English_Canada.1252;LC_MONETARY=English_Canada.1252;LC_NUMERIC=C;LC_TIME=English_Canada.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods" 
[7] "base"    

other attached packages:
   rpart
"3.1-34"


Thank you,

Volker
-------------- next part --------------
A non-text attachment was scrubbed...
Name: temptree.pr.ps
Type: application/postscript
Size: 10594 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070227/c773102b/attachment.ps 

From bunny at lautloscrew.com  Tue Feb 27 19:25:04 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Tue, 27 Feb 2007 19:25:04 +0100
Subject: [R] Multiple conditional without if
Message-ID: <8A85DA2D-33F8-4381-A139-21C75D7C9B98@lautloscrew.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/8472efa5/attachment.pl 

From efg at stowers-institute.org  Tue Feb 27 19:25:01 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 27 Feb 2007 12:25:01 -0600
Subject: [R] ordered matrix question
References: <7bff68f40702270946x65769440l47b6f8fcade0f0f7@mail.gmail.com>
Message-ID: <es1t1v$nli$1@sea.gmane.org>

"Juan Pablo Fededa" <jpfededa at gmail.com> wrote in message 
news:7bff68f40702270946x65769440l47b6f8fcade0f0f7 at mail.gmail.com...
> Hi all,
>
> Is there an easy way to generate an object wich will be the same matrix, 
> but
> ordered by de cfp value?

Does this help?

> RawData <- "Block    X    Y    cfp    yfp    ID
+ 0    524    244    213.41795    7.18482109    1
+ 0    556    270    65.383904    9.568372661    2
+ 0    528    316    40.789474    5.573732175    3
+ 0    642    432    135.81734    12.40134427    4
+ 0    716    430    34.359135    3.944923077    5
+ 0    894    362    109.63158    3.197160316    6
+ 0    958    130    63.98452    3.396452004    7
+ 0    506    92    83.513931    9.105496856    8
+ 0    476    464    91.674919    9.178089414    9
+ 0    364    426    44.13932    2.068334364    10"
> d <- read.table(textConnection(RawData), header=TRUE)

> d.ordered <- data.matrix( d[order(d$cfp),] )

> d.ordered
   Block   X   Y       cfp       yfp ID
5      0 716 430  34.35914  3.944923  5
3      0 528 316  40.78947  5.573732  3
10     0 364 426  44.13932  2.068334 10
7      0 958 130  63.98452  3.396452  7
2      0 556 270  65.38390  9.568373  2
8      0 506  92  83.51393  9.105497  8
9      0 476 464  91.67492  9.178089  9
6      0 894 362 109.63158  3.197160  6
4      0 642 432 135.81734 12.401344  4
1      0 524 244 213.41795  7.184821  1


efg

Earl F. Glynn
Stowers Institute for Medical Research


From dreiss.isb at gmail.com  Tue Feb 27 19:43:26 2007
From: dreiss.isb at gmail.com (David Reiss)
Date: Tue, 27 Feb 2007 10:43:26 -0800
Subject: [R] Function to do multiple named lookups faster?
Message-ID: <fd913b0d0702271043u4dc34295pa820838a44819326@mail.gmail.com>

Hi,
I apologize if this topic has been discussed - I could not figure out
a good search phrase for this question.

I have a named vector x, with multiple (duplicate) names, and I would
like to obtain a (shorter) vector with non-duplicate names in which
the values are the means of the values of the duplicated indexes in x.
My best (fastest) solution to this was this code:

nms <- names( x )
x.uniq <- sapply( unique( nms ), function( i ) mean( subtracted[ nms == i ] ) )

However, this takes forever on my beefy Mac Pro. Is there a faster way
to this using pre-written functions in R?

Thanks a lot for any advice.
-David


From gavin.simpson at ucl.ac.uk  Tue Feb 27 19:55:08 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 27 Feb 2007 18:55:08 +0000
Subject: [R] RDA and trend surface regression
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D307A25406@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D307A25406@groamrexm03.amer.pfizer.com>
Message-ID: <1172602508.19931.71.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-02-27 at 13:13 -0500, Kuhn, Max wrote:
> Helene,
> 
> My point was only that RDA may fit a quadratic model for the terms
> specified in your model. The terms that you had specified were already
> higher order polynomials (some cubic). So a QDA classifier with the
> model terms that you specified my be a fifth order polynomial in the
> original data. I don't know the reference you cite or even the
> subject-matter specifics. I'm just a simple cave man (for you SNL fans).
> But I do know that there are more reliable ways to get nonlinear
> classification boundaries than using x^5. 

I doubt that Helene is trying to do a classification - unless you
consider classification to mean that all rows/samples are in different
groups (i.e. n samples therefore n groups) - which is how RDA
(Redundancy Analysis) is used in ecology.

You could take a look at multispati in package ade4 for a different way
to handle spatial constraints. There is also the principle coordinates
analysis of neighbour matrices (PCNM) method - not sure this is coded
anywhere in R yet though. Here are two references that may be useful:

Dray, S., P. Legendre, and P. R. Peres- Neto. 2006. Spatial modeling: a
comprehensive framework for principal coordinate analysis of neighbor
matrices (PCNM). Ecological Modelling, in press.

Griffith, D. A., and P. R. Peres- Neto. 2006. Spatial modeling in
ecology: the flexibility of eigenfunction spatial analyses. Ecology, in
press.

HTH

G

> 
> If you want a quadratic model, I would suggest that you use QDA with the
> predictors in the original units (or see Hastie's book for a good
> example of using higher order terms with LDA). 
> 
> Looking at your email, you want a "a variation partitioning analyses".
> RDA works best as a classification technique. Perhaps a multivariate
> ANOVA model may be a more direct way to meet your needs. There is a
> connection between LDA and some multivariate linear models, but I don't
> know of a similar connection to RDA.
> 
> Max
> 
> -----Original Message-----
> From: MORLON [mailto:morlon.helene at gmail.com] 
> Sent: Tuesday, February 27, 2007 12:53 PM
> To: 'Jari Oksanen'; r-help at stat.math.ethz.ch
> Cc: Kuhn, Max
> Subject: RE: [R] RDA and trend surface regression
> 
> Thanks a lot for your answers,
> 
> I am concerned by your advice not to use polynomial constraints, or to
> use
> QDA instead of RDA. My final goal is to perform variation partitioning
> using
> partial RDA to assess the relative importance of environmental vs
> spatial
> variables. For the spatial analyses, trend surface analysis (polynomial
> constraints) is recommended in Legendre and Legendre 1998 (p739). Is
> there a
> better method to integrate space as an explanatory variable in a
> variation
> partitioning analyses? 
> 
> Also, I don't understand this: when I test for the significant
> contribution
> of monomials (forward elimination)
> 
> >anova(rda(Helling ~ I(x^2)+Condition(x)+Condition(y)))
> 
> performs the permutation test as expected, whereas 
> 
> >anova(rda(Helling ~ I(y^2)+Condition(x)+Condition(y)))
> 
> Returns this error message:
> 
> Error in "names<-.default"(`*tmp*`, value = "Model") : 
>         attempt to set an attribute on NULL
> 
> Thanks again for your help
> Kind regards,
> Helene
> 
> Helene MORLON
> University of California, Merced
> 
> -----Original Message-----
> From: Jari Oksanen [mailto:jarioksa at sun3.oulu.fi] 
> Sent: Monday, February 26, 2007 11:27 PM
> To: r-help at stat.math.ethz.ch
> Cc: morlon.helene at gmail.com
> Subject: [R] RDA and trend surface regression
> 
> 
> > 'm performing RDA on plant presence/absence data, constrained by
> > geographical locations. I'd like to constrain the RDA by the "extended
> > matrix of geographical coordinates" -ie the matrix of geographical
> > coordinates completed by adding all terms of a cubic trend surface
> > regression- . 
> > 
> > This is the command I use (package vegan):
> > 
> >  
> > 
> > >rda(Helling ~ x+y+x*y+x^2+y^2+x*y^2+y*x^2+x^3+y^3) 
> > 
> >  
> > 
> > where Helling is the matrix of Hellinger-transformed presence/absence
> data
> > 
> > The result returned by R is exactly the same as the one given by:
> > 
> >  
> > 
> > >anova(rda(Helling ~ x+y)
> > 
> >  
> > 
> > Ie the quadratic and cubic terms are not taken into account
> > 
> 
> You must *I*solate the polynomial terms with function I ("AsIs") so that
> they are not interpreted as formula operators:
> 
> rda(Helling ~ x + y + I(x*y) + I(x^2) + I(y^2) + I(x*y^2) + I(y*x^2) +
> I(x^3) + I(y^3))
> 
> If you don't have the interaction terms, then it is easier and better
> (numerically) to use poly():
> 
> rda(Helling ~ poly(x, 3) + poly(y, 3))
> 
> Another issue is that in my opinion using polynomial constraints is an
> Extremely Bad Idea(TM).
> 
> cheers, Jari Oksanen
> 
> ----------------------------------------------------------------------
> LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From jg_liao at yahoo.com  Tue Feb 27 19:58:06 2007
From: jg_liao at yahoo.com (Jason Liao)
Date: Tue, 27 Feb 2007 10:58:06 -0800 (PST)
Subject: [R] how much performance penalty does this incur,
	scalar as a vector of one element?
Message-ID: <20070227185806.95310.qmail@web53711.mail.yahoo.com>

Dear Prof.  Tierney, thank you very much to answer my question. It is good to know that the loss of efficiency can be small.

I came to this question after using R to implement a few low level algorithm: KD-tree and recursive algorithm for conditional Poisson binomial. The R's speed has been slow and even much slower than Ruby. 

I love R dearly and always tell my students that it is the best thing that ever happened to statistics. R is much more elegant than C or Fortran. Unfortunately Fortran or C is still needed when speed is a concern and a statistician has then to confront the ugly and complex large world. A huge gain in productivity and reduction in mental anguish can be achieved If R's speed can be improved via compilation.

I did a little research. The following tool claims to make Python as fast as C

http://www-128.ibm.com/developerworks/linux/library/l-psyco.html

Recently, a new Ruby implementation makes it several times faster:

http://www.antoniocangiano.com/articles/2007/02/19/ruby-implementations-shootout-ruby-vs-yarv-vs-jruby-vs-gardens-point-ruby-net-vs-rubinius-vs-cardinal
 
Jason Liao, http://www.geocities.com/jg_liao    
Associate Professor of Biostatistics
Drexel University School of Public Health
245 N. 15th Street, Mail Stop 660
Philadelphia, PA 19102-1192
phone 215-762-3934





 
____________________________________________________________________________________
Expecting? Get great news right away with email Auto-Check.


From Greg.Snow at intermountainmail.org  Tue Feb 27 20:07:43 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 27 Feb 2007 12:07:43 -0700
Subject: [R] Macros in R
In-Reply-To: <E1HLLq2-0004T5-00@smtp05.web.de>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB83CAAD@LP-EXCHVS07.CO.IHC.COM>

Others have pointed you to the answer to your question, but both FAQ
7.21 and the assign help page should really have a big banner at the top
saying "Here Be Dragons".

Using a loop or other automated procedure to create variables in the
main namespace can cause hard to find bugs, accidentally clobber
existing variables, and other non-fun things.

For this type of thing it is usually best to use a list (or an
environment, but I am more comforatable with lists).

For your example you could do something like:

> mymats <- list()
> for (i in 1:54){
+   myname <- paste('mymatrix',i,sep='')
+   mymats[[myname]] <- matrix( # insert whatever code you want here
+ }

A big advantage of this approach is that you can then deal with your
list of matricies as a single unit.  If you want to delete them, you
just delete the list rather than having to delete 54 individual
matricies.  The list can also be saved as a single unit to a file,
passed to another function, etc.

To access a single matrix (for example 'mymatrix5' which is in position
5) you have several options:

> mean( mymats[[5]] )
> mean( mymats[['mymatrix5']] )
> with( mymats, mean(mymatrix5) )
> attach(mymats)
> mean(mymatrix5) # as long as there is not a mymatrix 5 in the global
environment
> detach()

And probably others.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Monika Kerekes
> Sent: Sunday, February 25, 2007 9:03 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Macros in R
> 
> Dear members,
> 
>  
> 
> I have started to work with R recently and there is one thing 
> which I could not solve so far. I don't know how to define 
> macros in R. The problem at hand is the following: I want R 
> to go through a list of 1:54 and create the matrices input1, 
> input2, input3 up to input54. I have tried the following:
> 
>  
> 
> for ( i in 1:54) {
> 
>       input[i] = matrix(nrow = 1, ncol = 107)
> 
>       input[i][1,]=datset$variable
> 
> }
> 
>  
> 
> However, R never creates the required matrices. I have also 
> tried to type input'i' and input$i, none of which worked. I 
> would be very grateful for help as this is a basic question 
> the answer of which is paramount to any further usage of the software.
> 
>  
> 
> Thank you very much
> 
>  
> 
> Monika
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Tue Feb 27 20:11:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Feb 2007 14:11:15 -0500
Subject: [R] Macros in R
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB83CAAD@LP-EXCHVS07.CO.IHC.COM>
References: <E1HLLq2-0004T5-00@smtp05.web.de>
	<07E228A5BE53C24CAD490193A7381BBB83CAAD@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <971536df0702271111q46d05f03sd1b646c02ed98a3d@mail.gmail.com>

The FAQ does mention your point already.

On 2/27/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> Others have pointed you to the answer to your question, but both FAQ
> 7.21 and the assign help page should really have a big banner at the top
> saying "Here Be Dragons".
>
> Using a loop or other automated procedure to create variables in the
> main namespace can cause hard to find bugs, accidentally clobber
> existing variables, and other non-fun things.
>
> For this type of thing it is usually best to use a list (or an
> environment, but I am more comforatable with lists).
>
> For your example you could do something like:
>
> > mymats <- list()
> > for (i in 1:54){
> +   myname <- paste('mymatrix',i,sep='')
> +   mymats[[myname]] <- matrix( # insert whatever code you want here
> + }
>
> A big advantage of this approach is that you can then deal with your
> list of matricies as a single unit.  If you want to delete them, you
> just delete the list rather than having to delete 54 individual
> matricies.  The list can also be saved as a single unit to a file,
> passed to another function, etc.
>
> To access a single matrix (for example 'mymatrix5' which is in position
> 5) you have several options:
>
> > mean( mymats[[5]] )
> > mean( mymats[['mymatrix5']] )
> > with( mymats, mean(mymatrix5) )
> > attach(mymats)
> > mean(mymatrix5) # as long as there is not a mymatrix 5 in the global
> environment
> > detach()
>
> And probably others.
>
> Hope this helps,
>
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Monika Kerekes
> > Sent: Sunday, February 25, 2007 9:03 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Macros in R
> >
> > Dear members,
> >
> >
> >
> > I have started to work with R recently and there is one thing
> > which I could not solve so far. I don't know how to define
> > macros in R. The problem at hand is the following: I want R
> > to go through a list of 1:54 and create the matrices input1,
> > input2, input3 up to input54. I have tried the following:
> >
> >
> >
> > for ( i in 1:54) {
> >
> >       input[i] = matrix(nrow = 1, ncol = 107)
> >
> >       input[i][1,]=datset$variable
> >
> > }
> >
> >
> >
> > However, R never creates the required matrices. I have also
> > tried to type input'i' and input$i, none of which worked. I
> > would be very grateful for help as this is a basic question
> > the answer of which is paramount to any further usage of the software.
> >
> >
> >
> > Thank you very much
> >
> >
> >
> > Monika
> >
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mrennie at utm.utoronto.ca  Tue Feb 27 20:20:51 2007
From: mrennie at utm.utoronto.ca (Michael D. Rennie)
Date: Tue, 27 Feb 2007 14:20:51 -0500
Subject: [R] recording graphics going from lattice to traditional plots,
 and issues with log axes
Message-ID: <45E48493.6070509@utm.utoronto.ca>


Hi there

I am using the options(graphics.record = TRUE) command to keep track of 
my plots as I execute them. However, if I do a couple of plots using the 
lattice package (i.e., xyplots(y ~ x, ...), then go back to using the 
traditional graphics state (i.e., plot(y, x, etc...), then I can't go 
back to see the lattice plots.

Is the answer here "just stick with one graphics platform"? The reason 
I'm switching back and forth is that I am plotting logarithmic axes. In 
both the traditional state (plot(y, x,....)) and in the lattice plots 
(xyplot(y~x,... ,)) the arguments

...xlog = TRUE, ylog = TRUE, .......

don't do anything, and the only way I can get my log log axes is to do a 
traditional plot with the argument

log = "xy"

which doesn't do anything in the lattice framework.

Any suggestions on either of these issues (recording and log axes)?

Cheers,

Mike

-- 
Michael D. Rennie
Ph.D. Candidate
University of Toronto at Mississauga
3359 Missisagua Rd. N. 
Mississauga, ON L5L 1C6
Ph: 905-828-5452 Fax: 905-828-3792
www.utm.utoronto.ca/~w3rennie


From jholtman at gmail.com  Tue Feb 27 20:41:20 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 27 Feb 2007 14:41:20 -0500
Subject: [R] read.csv size limits
In-Reply-To: <9186136.post@talk.nabble.com>
References: <9186136.post@talk.nabble.com>
Message-ID: <644e1f320702271141t4f4c4037o8730ccf61f349880@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/d3730f98/attachment.pl 

From F.MENDIBURU at CGIAR.ORG  Tue Feb 27 20:42:09 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Tue, 27 Feb 2007 14:42:09 -0500
Subject: [R] Multiple conditional without if
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A2014069CE@webmail.cip.cgiar.org>

Dear matthias,

newmatrix = oldmatrix[ (oldmatrix[,5]==4 & oldmatrix[,6]==1) , ]

Felipe

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of bunny ,
lautloscrew.com
Sent: Tuesday, February 27, 2007 1:25 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Multiple conditional without if


Dear all,

i am stuck with a syntax problem.

i have a matrix which has about 500 rows and 6 columns.
now i want to kick some data out.
i want create a new matrix which is basically the old one except for all
entries which have a 4 in the 5 column AND a 1 in the 6th column.

i tried the following but couldn?t get a new matrix, just some wierd  
errors:

newmatrix=oldmatrix[,2][oldmatrix[,5]==4]&&oldmatrix[,2][oldmatrix[,6] 
==1]

all i get is:
numeric(0)

does anybody have an idea how to fix this one ?

thx in advance

matthias
	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Tue Feb 27 20:54:38 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 27 Feb 2007 20:54:38 +0100 (CET)
Subject: [R] RDA and trend surface regression
In-Reply-To: <1172602508.19931.71.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0702272047230.15125-100000@reclus.nhh.no>

On Tue, 27 Feb 2007, Gavin Simpson wrote:

> On Tue, 2007-02-27 at 13:13 -0500, Kuhn, Max wrote:
> > Helene,
> > 
> > My point was only that RDA may fit a quadratic model for the terms
> > specified in your model. The terms that you had specified were already
> > higher order polynomials (some cubic). So a QDA classifier with the
> > model terms that you specified my be a fifth order polynomial in the
> > original data. I don't know the reference you cite or even the
> > subject-matter specifics. I'm just a simple cave man (for you SNL fans).
> > But I do know that there are more reliable ways to get nonlinear
> > classification boundaries than using x^5. 
> 
> I doubt that Helene is trying to do a classification - unless you
> consider classification to mean that all rows/samples are in different
> groups (i.e. n samples therefore n groups) - which is how RDA
> (Redundancy Analysis) is used in ecology.
> 
> You could take a look at multispati in package ade4 for a different way
> to handle spatial constraints. There is also the principle coordinates
> analysis of neighbour matrices (PCNM) method - not sure this is coded
> anywhere in R yet though. Here are two references that may be useful:
> 
> Dray, S., P. Legendre, and P. R. Peres- Neto. 2006. Spatial modeling: a
> comprehensive framework for principal coordinate analysis of neighbor
> matrices (PCNM). Ecological Modelling, in press.
> 
> Griffith, D. A., and P. R. Peres- Neto. 2006. Spatial modeling in
> ecology: the flexibility of eigenfunction spatial analyses. Ecology, in
> press.

Pedro Peres-Neto helped cast his matlab original to R as function ME() in
the spdep package, at least partly to see if it worked like
SpatialFiltering() in spdep, based on a forthcoming paper by Tiefelsdorf
and Griffith; code suggestions from St?phane Dray were also used. For
sample data sets, including those used in the papers, ME() reproduces the
original results, and reproduces results from the matlab code from which
it was derived with possible differences for the stopping rule of the
semi-parametric stage - the Oribatid mites data set is in ade4.

Roger

> 
> HTH
> 
> G
> 
> > 
> > If you want a quadratic model, I would suggest that you use QDA with the
> > predictors in the original units (or see Hastie's book for a good
> > example of using higher order terms with LDA). 
> > 
> > Looking at your email, you want a "a variation partitioning analyses".
> > RDA works best as a classification technique. Perhaps a multivariate
> > ANOVA model may be a more direct way to meet your needs. There is a
> > connection between LDA and some multivariate linear models, but I don't
> > know of a similar connection to RDA.
> > 
> > Max
> > 
> > -----Original Message-----
> > From: MORLON [mailto:morlon.helene at gmail.com] 
> > Sent: Tuesday, February 27, 2007 12:53 PM
> > To: 'Jari Oksanen'; r-help at stat.math.ethz.ch
> > Cc: Kuhn, Max
> > Subject: RE: [R] RDA and trend surface regression
> > 
> > Thanks a lot for your answers,
> > 
> > I am concerned by your advice not to use polynomial constraints, or to
> > use
> > QDA instead of RDA. My final goal is to perform variation partitioning
> > using
> > partial RDA to assess the relative importance of environmental vs
> > spatial
> > variables. For the spatial analyses, trend surface analysis (polynomial
> > constraints) is recommended in Legendre and Legendre 1998 (p739). Is
> > there a
> > better method to integrate space as an explanatory variable in a
> > variation
> > partitioning analyses? 
> > 
> > Also, I don't understand this: when I test for the significant
> > contribution
> > of monomials (forward elimination)
> > 
> > >anova(rda(Helling ~ I(x^2)+Condition(x)+Condition(y)))
> > 
> > performs the permutation test as expected, whereas 
> > 
> > >anova(rda(Helling ~ I(y^2)+Condition(x)+Condition(y)))
> > 
> > Returns this error message:
> > 
> > Error in "names<-.default"(`*tmp*`, value = "Model") : 
> >         attempt to set an attribute on NULL
> > 
> > Thanks again for your help
> > Kind regards,
> > Helene
> > 
> > Helene MORLON
> > University of California, Merced
> > 
> > -----Original Message-----
> > From: Jari Oksanen [mailto:jarioksa at sun3.oulu.fi] 
> > Sent: Monday, February 26, 2007 11:27 PM
> > To: r-help at stat.math.ethz.ch
> > Cc: morlon.helene at gmail.com
> > Subject: [R] RDA and trend surface regression
> > 
> > 
> > > 'm performing RDA on plant presence/absence data, constrained by
> > > geographical locations. I'd like to constrain the RDA by the "extended
> > > matrix of geographical coordinates" -ie the matrix of geographical
> > > coordinates completed by adding all terms of a cubic trend surface
> > > regression- . 
> > > 
> > > This is the command I use (package vegan):
> > > 
> > >  
> > > 
> > > >rda(Helling ~ x+y+x*y+x^2+y^2+x*y^2+y*x^2+x^3+y^3) 
> > > 
> > >  
> > > 
> > > where Helling is the matrix of Hellinger-transformed presence/absence
> > data
> > > 
> > > The result returned by R is exactly the same as the one given by:
> > > 
> > >  
> > > 
> > > >anova(rda(Helling ~ x+y)
> > > 
> > >  
> > > 
> > > Ie the quadratic and cubic terms are not taken into account
> > > 
> > 
> > You must *I*solate the polynomial terms with function I ("AsIs") so that
> > they are not interpreted as formula operators:
> > 
> > rda(Helling ~ x + y + I(x*y) + I(x^2) + I(y^2) + I(x*y^2) + I(y*x^2) +
> > I(x^3) + I(y^3))
> > 
> > If you don't have the interaction terms, then it is easier and better
> > (numerically) to use poly():
> > 
> > rda(Helling ~ poly(x, 3) + poly(y, 3))
> > 
> > Another issue is that in my opinion using polynomial constraints is an
> > Extremely Bad Idea(TM).
> > 
> > cheers, Jari Oksanen
> > 
> > ----------------------------------------------------------------------
> > LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From mamoralesri at telecom.com.co  Tue Feb 27 20:59:46 2007
From: mamoralesri at telecom.com.co (Mario A. Morales R. )
Date: Tue, 27 Feb 2007 14:59:46 -0500
Subject: [R] matplot on lattice graphics
Message-ID: <45E48DB2.6080601@telecom.com.co>

can I  use matplot on each panel of a lattice graphic? How?   
 <https://academico.unicordoba.edu.co/squirrelmail/src/compose.php?send_to=r-help%40stat.math.ethz.ch>


From andy_liaw at merck.com  Tue Feb 27 21:14:38 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 27 Feb 2007 15:14:38 -0500
Subject: [R] Multiple conditional without if  [Broadcast]
In-Reply-To: <8A85DA2D-33F8-4381-A139-21C75D7C9B98@lautloscrew.com>
References: <8A85DA2D-33F8-4381-A139-21C75D7C9B98@lautloscrew.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03C94BAF@usctmx1106.merck.com>

From: bunny, lautloscrew.com
> 
> Dear all,
> 
> i am stuck with a syntax problem.
> 
> i have a matrix which has about 500 rows and 6 columns.
> now i want to kick some data out.
> i want create a new matrix which is basically the old one 
> except for all entries which have a 4 in the 5 column AND a 1 
> in the 6th column.
> 
> i tried the following but couldn?t get a new matrix, just some wierd
> errors:
> 
> newmatrix=oldmatrix[,2][oldmatrix[,5]==4]&&oldmatrix[,2][oldmatrix[,6]
> ==1]
> 
> all i get is:
> numeric(0)

That's not a `weird error', but a numeric vector of length 0.
 
> does anybody have an idea how to fix this one ?

Try:

newmatrix = oldmatrix[oldmatrix[, 5]==4 & oldmatrix[, 6] == 1, 2, drop=FALSE]

If you just want a subset of column 2 as a vector, you can leave off the drop=FALSE part.

Reading "An Introduction to R" should have save you some trouble in the first place.

Andy
 
> thx in advance
> 
> matthias
> 	[[alternative HTML version deleted]]
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From klaster at karlin.mff.cuni.cz  Tue Feb 27 21:15:05 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Tue, 27 Feb 2007 21:15:05 +0100
Subject: [R] Multiple conditional without if
In-Reply-To: <8A85DA2D-33F8-4381-A139-21C75D7C9B98@lautloscrew.com>
References: <8A85DA2D-33F8-4381-A139-21C75D7C9B98@lautloscrew.com>
Message-ID: <45E49149.50307@karlin.mff.cuni.cz>

bunny , lautloscrew.com napsal(a):
> Dear all,
> 
> i am stuck with a syntax problem.
> 
> i have a matrix which has about 500 rows and 6 columns.
> now i want to kick some data out.
> i want create a new matrix which is basically the old one except for all
> entries which have a 4 in the 5 column AND a 1 in the 6th column.
> 
> i tried the following but couldn?t get a new matrix, just some wierd  
> errors:
> 
> newmatrix=oldmatrix[,2][oldmatrix[,5]==4]&&oldmatrix[,2][oldmatrix[,6] 
> ==1]
This is nonsense.

 > m <- matrix(rep(1:12,3),ncol=6)
 > m[1,6] <-1
 > m
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    7    1    7    1    1
[2,]    2    8    2    8    2    8
[3,]    3    9    3    9    3    9
[4,]    4   10    4   10    4   10
[5,]    5   11    5   11    5   11
[6,]    6   12    6   12    6   12

 > m[!((m[,5]==4)|(m[,6]==1)),]
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    2    8    2    8    2    8
[2,]    3    9    3    9    3    9
[3,]    5   11    5   11    5   11
[4,]    6   12    6   12    6   12

Please read the appropriate chapter in R-intro to become familiar with 
vector, matrix and list indexing.
Petr


> 
> all i get is:
> numeric(0)
> 
> does anybody have an idea how to fix this one ?
> 
> thx in advance
> 
> matthias
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From kalyansikha at yahoo.com  Tue Feb 27 15:11:27 2007
From: kalyansikha at yahoo.com (Bhanu Kalyan.K)
Date: Tue, 27 Feb 2007 06:11:27 -0800 (PST)
Subject: [R] Problem with R interface termination
Message-ID: <20070227141127.28245.qmail@web34309.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/af1516bd/attachment.pl 

From laurent.beaulaton at Bordeaux.Cemagref.fr  Tue Feb 27 21:10:57 2007
From: laurent.beaulaton at Bordeaux.Cemagref.fr (Beaulaton Laurent)
Date: Tue, 27 Feb 2007 21:10:57 +0100
Subject: [R] interactions and GAM
Message-ID: <182D8C7FAD4DCE499BF5AD749B3AA06404C233@hermes.bordeaux.cemagref.fr>

Dear R-users,

I have 1 remark and 1 question on the inclusion of interactions in the gam function from the gam package.

I need to fit quantitative predictors in interactions with factors. You can see an example of what I need in fig 9.13 p265  from Hastie and Tibshirani book (1990). 
It's clearly stated that in ?gam  "Interactions with nonparametric smooth terms are not fully supported".
I have found a trick in a former http://www.math.yorku.ca/Who/Faculty/Monette/S-news/2284.html, using NAs and na.gam.replace argument, but some points are still unclear for me.

First the prediction of new data (using predict function) is not so easy (see script below), and need a close reading from section 7.3.2 of the Chambers and Hastie (1992).

Second I need to have the same intercept for all levels of factor and this not achievable with this trick. My question is : why not replacing NA by 0 (or any other particular value) ?

Here is a quite long (sorry for that) script with a generated dataset to better undestand my question.
in this script the model to fit is (in a GLM-like writing) : y~s(x2):x1
the generated dataset follows this model and y(x2=0)=10 whatever x1.

########################
#start of script
########################

#data construction  (with deliberately very small noise)
data1=data.frame(x1=rep(NA,27),x2=NA,y=NA)

data1$x1=factor(c(rep(1,11),rep(2,11),rep(3,5)))
data1$x2=c(rep(0:10,2),0:4)

data1[data1$x1==1,"y"]=data1[data1$x1==1,"x2"]^4*5+rnorm(11)+10000
data1[data1$x1==2,"y"]=data1[data1$x1==2,"x2"]^4*(-3)+rnorm(11)+10000
data1[data1$x1==3,"y"]=10000*data1[data1$x1==3,"x2"]+rnorm(5)+10000

library(lattice)
xyplot(data1$y~data1$x2,groups=data1$x1)

#creation of dummy variables for interactions
data1$x2_1=ifelse(data1$x1=="1",data1$x2,NA)
data1$x2_2=ifelse(data1$x1=="2",data1$x2,NA)
data1$x2_3=ifelse(data1$x1=="3",data1$x2,NA)

#model fitting
library(gam)
model=gam(y~s(x2_1)+s(x2_2)+s(x2_3)+x1,data=data1,na=na.gam.replace)

#prediction fit well data :
summary(model)
plot(data1$x2,data1$y)
points(data1$x2,model$fitted.value,col="red",pch="+")

#trying to see prediction:
predict(model) #does work
predict(model,newdata=data1) #produce NA

#trying to replace NA in data1 by mean, to mimic na.gam.replace:
Ndata=data1
Ndata$x2_1=ifelse(data1$x1=="1",data1$x2,mean(data1$x2_1,na.rm=TRUE))
Ndata$x2_2=ifelse(data1$x1=="2",data1$x2,mean(data1$x2_2,na.rm=TRUE))
Ndata$x2_3=ifelse(data1$x1=="3",data1$x2,mean(data1$x2_3,na.rm=TRUE))

predict(model,Ndata)-predict(model) #as you can see there is a systematic biais

#correct way to predict (=returned 0 for terms with NA value):
p=predict(model,data1,type="term")
rowSums(cbind(p,attr(p,"constant")),na.rm=TRUE)-predict(model)

#alternative solution, 0 instead of NA
data1$v1=ifelse(data1$x1=="1",data1$x2,0)
data1$v2=ifelse(data1$x1=="2",data1$x2,0)
data1$v3=ifelse(data1$x1=="3",data1$x2,0)

model1=gam(y~s(v1)+s(v2)+s(v3),data=data1)
summary(model1)
points(data1$x2,predict(model1,data1),col="green",pch="X")
#no particular problem with predict function

#what's happened in x2=0 ?
predict(model)[data1$x2==0]
predict(model1)[data1$x2==0]

########################
#end of script
########################

thanks in advance
best regards
Laurent Beaulaton

---------------------------------------------
Laurent Beaulaton
###############################
# NEW !!!!                                       #
#  http://www.laurent-beaulaton.fr/    #
# Tel + 33 (0)5 57 89 27 17             #
###############################
---------------------------------------------
Cemagref (French Institute of Agricultural and Environmental Engineering Research )
Unit? "Ecosyst?mes estuariens et poissons migrateurs amphihalins"
(anciennement Unit? "Ressources aquatiques continentales")
50 avenue de Verdun
F 33612 Cestas Cedex

Tel + 33 (0)5 57 89 27 17
Fax + 33 (0)5 57 89 08 01
mailto:laurent.beaulaton at bordeaux.cemagref.fr

http://www.laurent-beaulaton.fr/ 
http://www.bordeaux.cemagref.fr/rabx/


From jafarikia at gmail.com  Tue Feb 27 21:35:36 2007
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Tue, 27 Feb 2007 15:35:36 -0500
Subject: [R] Angle of Bar Plot
Message-ID: <e3f2a5ab0702271235w1050b90u66e8f348eb3ffaa8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/fe94be78/attachment.pl 

From bates at stat.wisc.edu  Tue Feb 27 21:36:30 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 27 Feb 2007 14:36:30 -0600
Subject: [R] compiling issues with Mandriva Linux 2007 Discovery
In-Reply-To: <2FC987BC0B90B24786CAF43DD3F5719C814299@CRU105.cahe.ad.wsu.edu>
References: <2FC987BC0B90B24786CAF43DD3F5719C814299@CRU105.cahe.ad.wsu.edu>
Message-ID: <40e66e0b0702271236u4f1fccd8i9a8f40a9b4c83b77@mail.gmail.com>

On 2/27/07, Bricklemyer, Ross S <rsb at wsu.edu> wrote:
> All,
>
> I am a new user to Linux but I am familiar with R.  I have previously
> used and installed R on a Windows platform without problems.  I recently
> set up a dual boot system (XP_64, Mandriva) to run R on a Linux platform
> in order to more efficiently handle large datasets.  I have not done
> compiling before, but read the R instructions and followed to my best
> ability.  I downloaded the most recent tar and unpacked it into
> /usr/local/R_HOME.  I was able to run ./configure and added the
> additional Linux packages necessary to compile.  The problem arises
> using the 'make' command.  When I run make I get an error that there is
> not a target or a makefile.  There is, however, Makefile.in in the
> directory.  I think I am close to getting this installed, but I am
> stuck.  Any help would be greatly appreciated.  I do not suppose that
> any of the pre compiled Linux versions (i.e. Debian, SuSe) available on
> CRAN mirrors would run on a Mandriva distro?

You skipped a step.  Unpack then run configure then run make.


From Greg.Snow at intermountainmail.org  Tue Feb 27 21:37:30 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 27 Feb 2007 13:37:30 -0700
Subject: [R] PLotting R graphics/symbols without user x-y scaling
In-Reply-To: <45E2EDE0.5060305@unc.edu>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB83CAF2@LP-EXCHVS07.CO.IHC.COM>

Beyond the other replies that you have received, here are a couple more
ideas.

The cnvrt.coords function in the TeachingDemos package can aid in
switching between the coordinate systems so you could use this to
convert your x,y coordinates from the user system to the plot
coordinates (0 to 1), add values to these to represent the graphic you
want to plot, then convert these coordinates back to user coordinates
and use the lines function to do the plotting.

You could also do the conversion, then change the user plotting
coordinates rather than converting back to user coordinates.

You can also use the subplot function in the TeachingDemos package to
add small plots to an existing plot (see the examples).

Here is a quick stab at a function that uses subplot internally to do
similar to symbols, you give it the x and y coordinates where the center
of your symbol should be, then the 'symb' argument can either be a
matrix with points that define the shape of the symbol, or a function
that creates a plot of the symbol.

my.symbols <- function(x, y=NULL, symb, inches=1, add=FALSE,
	xlab=deparse(substitute(x)), ylab=deparse(substitute(y)),
main=NULL,
	xlim=NULL, ylim=NULL, vadj=0.5, hadj=0.5, pars=NULL, ...){

  if(!require(TeachingDemos)) stop('The TeachingDemos package is
required')

  if(!add){
	plot(x,y, type='n', xlab=xlab,ylab=ylab,xlim=xlim,ylim=ylim)
  }

  if(is.function(symb)){
    symb2 <- symb
  } else {
    symb2 <- function(...){
      plot( rbind(symb, symb[1,]), xlab='',ylab='', xaxs='i',yaxs='i',
          ann=FALSE, axes=FALSE, type='l'   )
    }
  }

  for (i in seq(along=x)){
	subplot(symb2(i,...),x[i],y[i], size=c(inches,inches),
vadj=vadj, 
       hadj=hadj, pars=pars)
  }

}

Here are a couple of quick examples that use the above function:

tmp.hex <- rbind( c(0,0), c(0,1), c(1,2), c(2,2), c(2,1), c(1,0) )

my.symbols( runif(10), runif(10), tmp.hex, inches=.25 )

tmp.fun <- function(which, r, ...){
  r <- r[which]
  plot( 0.5, 0.5, xlim=c(0,1), ylim=c(0,1), xaxs='i', yaxs='i', 
    ann=FALSE, axes=FALSE, cex=r)
}

my.symbols( runif(10), runif(10), tmp.fun, r=sample(1:10) )


I will polish and document this function and add it to the next release
of the TeachingDemos package (when I get enough time to put out a new
release, hopefully soon, but don't hold your breath).

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathan Lees
> Sent: Monday, February 26, 2007 7:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] PLotting R graphics/symbols without user x-y scaling
> 
> 
> Is it possible to add lines or other
> user defined graphics
> to a plot in R that does not depend on
> the user scale for the plot?
> 
> For example I have a plot
> plot(x,y)
> and I want to add some graphic that is
> scaled in inches or cm but I do not want the graphic to 
> change when the x-y scales are changed - like a thermometer, 
> scale bar or other symbol - How does one do this?
> 
> I want to build my own library of glyphs to add to plots but 
> I do not know how to plot them when their size is independent 
> of the device/user coordinates.
> 
> Is it possible to add to the list
> of symbols in the function symbols()
> other than:
>   _circles_, _squares_, _rectangles_, _stars_, _thermometers_, and
>       _boxplots_
> 
> can I make my own symbols and have symbols call these?
> 
> 
> Thanks-
> 
> 
> --
> Jonathan M. Lees
> Professor
> THE UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL Department of 
> Geological Sciences Campus Box #3315 Chapel Hill, NC  27599-3315
> TEL: (919) 962-0695
> FAX: (919) 966-4519
> jonathan.lees at unc.edu
> http://www.unc.edu/~leesj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jarioksa at sun3.oulu.fi  Tue Feb 27 21:38:15 2007
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 27 Feb 2007 22:38:15 +0200
Subject: [R] RDA and trend surface regression
In-Reply-To: <1172602508.19931.71.camel@gsimpson.geog.ucl.ac.uk>
References: <71257D09F114DA4A8E134DEAC70F25D307A25406@groamrexm03.amer.pfizer.com>
	<1172602508.19931.71.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <d4bc47d41f3a6171ff60fed5290e68ed@sun3.oulu.fi>


On 27 Feb 2007, at 20:55, Gavin Simpson wrote:

> On Tue, 2007-02-27 at 13:13 -0500, Kuhn, Max wrote:
>> Helene,
>>
>> My point was only that RDA may fit a quadratic model for the terms
>> specified in your model. The terms that you had specified were already
>> higher order polynomials (some cubic). So a QDA classifier with the
>> model terms that you specified my be a fifth order polynomial in the
>> original data. I don't know the reference you cite or even the
>> subject-matter specifics. I'm just a simple cave man (for you SNL 
>> fans).
>> But I do know that there are more reliable ways to get nonlinear
>> classification boundaries than using x^5.
>
> I doubt that Helene is trying to do a classification - unless you
> consider classification to mean that all rows/samples are in different
> groups (i.e. n samples therefore n groups) - which is how RDA
> (Redundancy Analysis) is used in ecology.
>
> You could take a look at multispati in package ade4 for a different way
> to handle spatial constraints. There is also the principle coordinates
> analysis of neighbour matrices (PCNM) method - not sure this is coded
> anywhere in R yet though. Here are two references that may be useful:
>
St?phane Dray has R code for finding PCNM matrices. Google for his 
name: it's not that common. I also have a copy of his function and can 
send it if really needed, but it may be better to check Dray's page 
first. St?phane Dray says think that not all functions need be in CRAN. 
May be true, but I think it might help many people.

There are at least three reasons why not use polynomial constraints in 
RDA. Max Kuhn mentioned one: polynomials typically flip wildly at 
margins (or they are unstable in more neutral speech). Second reason is 
that they are almost impossible to interpret in ordination display. The 
third reason is that RDA (or CCA) avoid some ordination artefacts 
(curvature, horseshoe, arc etc.) just because the constraints are 
linear: allowing them to be curved allows curved solutions. These 
arguments are not necessarily valid if you only want to have variance 
partitioning, or if you use polynomial conditions ("partial out" 
polynomial effects in Canoco language). In that case it may make sense 
to use quadratic (or polynomial) constraints or conditions.

cheers, Jari Oksanen


From F.MENDIBURU at CGIAR.ORG  Tue Feb 27 21:42:21 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Tue, 27 Feb 2007 15:42:21 -0500
Subject: [R] Multiple conditional without if
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A2014069CF@webmail.cip.cgiar.org>

Matthias,

According to the logic,

New matrix which is basically the old one except for all
entries which have a 4 in the 5 column AND a 1 in the 6th column

newmatrix <- oldmatrix[ (oldmatrix[,5]!=4 & oldmatrix[,6]!=1) , ]

---

newmatrix <- oldmatrix[ !(oldmatrix[,5]==4 & oldmatrix[,6]==1) , ]

by,

Felipe

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of bunny ,
lautloscrew.com
Sent: Tuesday, February 27, 2007 1:25 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Multiple conditional without if


Dear all,

i am stuck with a syntax problem.

i have a matrix which has about 500 rows and 6 columns.
now i want to kick some data out.
i want create a new matrix which is basically the old one except for all
entries which have a 4 in the 5 column AND a 1 in the 6th column.

i tried the following but couldn?t get a new matrix, just some wierd  
errors:

newmatrix=oldmatrix[,2][oldmatrix[,5]==4]&&oldmatrix[,2][oldmatrix[,6] 
==1]

all i get is:
numeric(0)

does anybody have an idea how to fix this one ?

thx in advance

matthias
	[[alternative HTML version deleted]]


From Greg.Snow at intermountainmail.org  Tue Feb 27 21:47:17 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 27 Feb 2007 13:47:17 -0700
Subject: [R] looping
In-Reply-To: <200702270011.l1R0BAjf004764@pilsener.srv.ualberta.ca>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB83CAF6@LP-EXCHVS07.CO.IHC.COM>

For the example that you give, using lapply, sapply, or replicate may be
the better way to go:

> mysample <- replicate( 50, dataset[ sample(100000,100), ] )

If you really want to use a loop, then use a list:

> mysamples <- list()
> mysampdata <- list()
> for (i in 1:50){
+   mysamples[[i]] <- sample(100000, 100)
+   mysampdata[[i]] <- dataset[ mysamples[[i]], ]
+ }

Then you can use lapply or sapply to do something with each sampled
dataset:

> sapply( mysampdata, summary )

Or you can access individual elements in a number of ways:

> summary( mysampdata[[1]] )
> names(mysampdata) <- paste('d',1:50, sep='')
> with(mysampdata, summary(d2))
> summary( mysampdata$d3 )
> attach(mysampdata)
> summary(d4)
> detach()

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Neil Hepburn
> Sent: Monday, February 26, 2007 5:11 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] looping
> 
> 
> Greetings:
> 
> I am looking for some help (probably really basic) with 
> looping. What I want to do is repeatedly sample observations 
> (about 100 per sample) from a large dataset (100,000 
> observations).  I would like the samples labelled sample.1, 
> sample.2, and so on (or some other suitably simple naming 
> scheme).  To do this manually I would 
> 
> >smp.1 <- sample(100000, 100)
> >sample.1 <- dataset[smp.1,]
> >smp.2 <- sample(100000, 100)
> >sample.2 <- dataset[smp.2,]
> .
> .
> .
> >smp.50 <- sample(100000, 100)
> >sample.50 <- dataset[smp.50,]
> 
> and so on.
> 
> I tried the following loop code to generate 100 samples:
> 
> >for (i in 1:50){
> >+ smp.[i] <- sample(100000, 100)
> >+ sample.[i] <- dataset[smp.[i],]}
> 
> Unfortunately, that does not work -- specifying the looping 
> variable i in the way that I have does not work since R uses 
> that to reference places in a vector (x[i] would be the ith 
> element in the vector x)
> 
> Is it possible to assign the value of the looping variable in 
> a name within the loop structure?
> 
> Cheers,
> Neil Hepburn
> 
> ===========================================
> Neil Hepburn, Economics Instructor
> Social Sciences Department,
> The University of Alberta Augustana Campus
> 4901 - 46 Avenue
> Camrose, Alberta
> T4V 2R3
> 
> Phone (780) 697-1588
> email nhepburn at augustana.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From NordlDJ at dshs.wa.gov  Tue Feb 27 21:59:24 2007
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Tue, 27 Feb 2007 12:59:24 -0800
Subject: [R] Multiple conditional without if
In-Reply-To: <45E49149.50307@karlin.mff.cuni.cz>
Message-ID: <941871A13165C2418EC144ACB212BDB0078D54@dshsmxoly1504g.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Petr 
> Klasterecky
> Sent: Tuesday, February 27, 2007 12:15 PM
> To: bunny , lautloscrew.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Multiple conditional without if
> 
> bunny , lautloscrew.com napsal(a):
> > Dear all,
> > 
> > i am stuck with a syntax problem.
> > 
> > i have a matrix which has about 500 rows and 6 columns.
> > now i want to kick some data out.
> > i want create a new matrix which is basically the old one 
> except for all
> > entries which have a 4 in the 5 column AND a 1 in the 6th column.
> > 
> > i tried the following but couldn?t get a new matrix, just 
> some wierd  
> > errors:
> > 
> > 
> newmatrix=oldmatrix[,2][oldmatrix[,5]==4]&&oldmatrix[,2][oldma
> trix[,6] 
> > ==1]
> This is nonsense.
> 
>  > m <- matrix(rep(1:12,3),ncol=6)
>  > m[1,6] <-1
>  > m
>       [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    1    7    1    7    1    1
> [2,]    2    8    2    8    2    8
> [3,]    3    9    3    9    3    9
> [4,]    4   10    4   10    4   10
> [5,]    5   11    5   11    5   11
> [6,]    6   12    6   12    6   12
> 
>  > m[!((m[,5]==4)|(m[,6]==1)),]

I think what was requested was to exclude rows where column 5 was 4 AND column 6 was 1, so,

m[((m[,5]!=4)|(m[,6]!=1)),]

<<<snip>>>

Hope this is helpful,

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


From Greg.Snow at intermountainmail.org  Tue Feb 27 21:58:59 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 27 Feb 2007 13:58:59 -0700
Subject: [R] fitting of all possible models
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB83CB01@LP-EXCHVS07.CO.IHC.COM>

You may want to look at the packages 'leaps'.  I don't think it does glm's, but possibly you could modify it to.

Otherwise here is one quick approach (though there are probably better ones):

> apply( expand.grid( c(TRUE,FALSE),c(TRUE,FALSE),c(TRUE,FALSE) ),
+ 1, function(x) as.formula(paste(c('y~1', c('x1','x2','x3')[x]), collapse='+')))

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Indermaur Lukas
> Sent: Tuesday, February 27, 2007 12:46 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] fitting of all possible models
> 
> Hi,
> Fitting all possible models (GLM) with 10 predictors will 
> result in loads of (2^10 - 1) models. I want to do that in 
> order to get the importance of variables (having an 
> unbalanced variable design) by summing the up the AIC-weights 
> of models including the same variable, for every variable 
> separately. It's time consuming and annoying to define all 
> possible models by hand. 
>  
> Is there a command, or easy solution to let R define the set 
> of all possible models itself? I defined models in the 
> following way to process them with a batch job:
>  
> # e.g. model 1
> preference<- formula(Y~Lwd + N + Sex + YY)                    
>                             
> # e.g. model 2
> preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY) etc.
> etc.
>  
>  
> I appreciate any hint
> Cheers
> Lukas
>  
>  
>  
>  
>  
> ??? 
> Lukas Indermaur, PhD student 
> eawag / Swiss Federal Institute of Aquatic Science and Technology 
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
>  
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15 
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Tue Feb 27 22:06:43 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 27 Feb 2007 14:06:43 -0700
Subject: [R] How to put the dependent variable in GLM proportion model
In-Reply-To: <OFE1A218E1.33D08AEF-ONC125728F.004AC6D8-C125728F.004AC6E1@wsr.ac.at>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB83CB16@LP-EXCHVS07.CO.IHC.COM>



-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

The first one should be:
> n <- (S+F)
> share <- S/(S+F)
> glm(share~x, family=quasibinomial, weights=n)

This should give you results more comparable to the second one.  Either
way is acceptable.  

Hope this helps,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Serguei Kaniovski
> Sent: Tuesday, February 27, 2007 6:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to put the dependent variable in GLM proportion model
> 
> 
> Hello everyone,
> 
> I am confused about how the dependent variable should be 
> specified, e.g.
> say S and F denote series of successes and failures. Is it
> 
> share<-S/(S+F)
> glm(share~x,family=quasibinomial)
> 
> or
> 
> glm(cbind(S,F)~x,family=quasibinomial)
> 
> The two variants produce very different dispersion parameter 
> and deviances.
> The book by Crawley, the only one R-book a have, says the 
> second variant is correct for proportions data.
> 
> Serguei
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Tue Feb 27 22:36:18 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 27 Feb 2007 14:36:18 -0700
Subject: [R] Macros in R
In-Reply-To: <971536df0702271111q46d05f03sd1b646c02ed98a3d@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB83CB2D@LP-EXCHVS07.CO.IHC.COM>

The FAQ does mention using a list (and I did not mean to imply that it
did not).  Personally I think it is a little soft on this point for the
following reasons:

1. It mentions lists at the very end, some users may read about the
assign function, think that that answers the question that they think
they have, and never read on to the end.
2. The phrase "often easier" seems a soft sell to me, like "consider
this", not "DO IT THIS WAY".
3. It does not point out any of the dangers of using assign and the
additional benefits of using lists.
4. The phrase "Here Be Dragons" is fun to say, and seeing it grabs
attention and gets people thinking (except maybe on September 19th,
International Talk Like a Pirate Day).

And the parenthetical remark on number 4 brings up the obvious addition
to the R Infrequently Asked Questions list:

Q: Aye, mateys, what be a pirate's favorite statistical package?
A: R, of course (but you need to pronounce it AAAARRRRR :-).

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
> Sent: Tuesday, February 27, 2007 12:11 PM
> To: Greg Snow
> Cc: Monika Kerekes; r-help at stat.math.ethz.ch
> Subject: Re: [R] Macros in R
> 
> The FAQ does mention your point already.
> 
> On 2/27/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> > Others have pointed you to the answer to your question, but both FAQ
> > 7.21 and the assign help page should really have a big 
> banner at the 
> > top saying "Here Be Dragons".
[snip]


From aiminy at iastate.edu  Tue Feb 27 22:38:13 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Tue, 27 Feb 2007 15:38:13 -0600
Subject: [R] R in linux
Message-ID: <6.2.3.4.2.20070227153255.03e0e6d0@aiminy.mail.iastate.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/b7fb4362/attachment.pl 

From aiminy at iastate.edu  Tue Feb 27 23:14:00 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Tue, 27 Feb 2007 16:14:00 -0600
Subject: [R] R in linux
Message-ID: <6.2.3.4.2.20070227161337.03912610@aiminy.mail.iastate.edu>

I used to use R-WinEdt in Windows. I know R-Winedt is only for R in Windows.
Now I try to use R in linux. I am wondering if there  is something 
that can do same thing as R-Winedt for R in linux?

Aimin


From eleonorademaria at yahoo.com  Wed Feb 28 00:06:42 2007
From: eleonorademaria at yahoo.com (Eleonora Demaria)
Date: Tue, 27 Feb 2007 15:06:42 -0800 (PST)
Subject: [R] help with NSST models in GaussRf
Message-ID: <804982.92391.qm@web33710.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/0ef3d071/attachment.pl 

From geoffrey.russell at gmail.com  Wed Feb 28 00:11:19 2007
From: geoffrey.russell at gmail.com (Geoff Russell)
Date: Wed, 28 Feb 2007 09:41:19 +1030
Subject: [R] factor documentation issue
Message-ID: <93c3eada0702271511g3989ed3fme371f32559209441@mail.gmail.com>

There is a warning in the documentation for ?factor  (R version 2.3.0)
as follows:

" The interpretation of a factor depends on both the codes and the
  '"levels"' attribute.  Be careful only to compare factors with the
  same set of levels (in the same order).  In particular,
  'as.numeric' applied to a factor is meaningless, and may happen by
  implicit coercion.  To "revert" a factor 'f' to its original
  numeric values, 'as.numeric(levels(f))[f]' is recommended and
  slightly more efficient than 'as.numeric(as.character(f))'.


But as.numeric seems to work fine whereas as.numeric(levels(f))[f] doesn't
always do anything useful.

For example:

> f<-factor(1:3,labels=c("A","B","C"))
> f
[1] A B C
Levels: A B C
> as.numeric(f)
[1] 1 2 3
> as.numeric(levels(f))[f]
[1] NA NA NA
Warning message:
NAs introduced by coercion

And also,

> f<-factor(1:3,labels=c(1,5,6))
> f
[1] 1 5 6
Levels: 1 5 6
> as.numeric(f)
[1] 1 2 3
> as.numeric(levels(f))[f]
[1] 1 5 6

Is the documentation wrong, or is the code wrong, or have I missed
something?

Cheers,
Geoff Russell


From bolker at zoo.ufl.edu  Wed Feb 28 00:24:29 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Tue, 27 Feb 2007 23:24:29 +0000 (UTC)
Subject: [R] Angle of Bar Plot
References: <e3f2a5ab0702271235w1050b90u66e8f348eb3ffaa8@mail.gmail.com>
Message-ID: <loom.20070228T002239-426@post.gmane.org>

Mohsen Jafarikia <jafarikia <at> gmail.com> writes:

> 
> Hello everyone,
> 
> I want to use 'angle' in Bar Plot with different angles for different bars.
> 
> For example if the 3th column of the following data is '0.0', I want the
> angle to be '45' degrees, if it is '1.92', I want '65' and 


[snip]

 couldn't read the end of your e-mail but chose to set angle=80
for the third value ...

z <- matrix(c(3.74,0,6.12,1.92,9.71,0,1.32,1.92,8.24,4.48),byrow=TRUE,
+ ncol=2)
> z
     [,1] [,2]
[1,] 3.74 0.00
[2,] 6.12 1.92
[3,] 9.71 0.00
[4,] 1.32 1.92
[5,] 8.24 4.48

> barplot(z[,1],angle=c(45,65,80)[as.numeric(factor(z[,2]))],density=5)

  I will point out that this is pretty ugly though ...

  Ben Bolker


From ssj1364 at gmail.com  Wed Feb 28 00:32:31 2007
From: ssj1364 at gmail.com (sj)
Date: Tue, 27 Feb 2007 16:32:31 -0700
Subject: [R] .C HoltWinters
Message-ID: <1c6126db0702271532t45877628p88537b40fb6cd791@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/ccc1639a/attachment.pl 

From researchjj at gmail.com  Wed Feb 28 02:22:48 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 28 Feb 2007 09:22:48 +0800
Subject: [R] Datamining-package-?
Message-ID: <b4485c4c0702271722k19e49523ic4615f9825d0b324@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/391e1a7e/attachment.pl 

From liuwensui at gmail.com  Wed Feb 28 02:34:32 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 27 Feb 2007 20:34:32 -0500
Subject: [R] Datamining-package-?
In-Reply-To: <b4485c4c0702271722k19e49523ic4615f9825d0b324@mail.gmail.com>
References: <b4485c4c0702271722k19e49523ic4615f9825d0b324@mail.gmail.com>
Message-ID: <1115a2b00702271734g74fc6d1awdab2adc413de2181@mail.gmail.com>

what do you mean by data preprocessing? there are tons of R functions
that you can use to process data and do data mining.

On 2/27/07, j.joshua thomas <researchjj at gmail.com> wrote:
> Dear Group,
>
> I am looking for a package that is going to help me on Data preprocessing
> methods in Datamining.
>
> Is there any package in R2.4.0 to support DM? or what is the suitable
> package that i can adopt do the work?
>
> Kindly need your assistance.
>
> Thanks & Regards
>
>
>
>
> JJ
> ---
>
> --
> Lecturer J. Joshua Thomas
> KDU College Penang Campus
> Research Student,
> University Sains Malaysia
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From researchjj at gmail.com  Wed Feb 28 02:52:07 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 28 Feb 2007 09:52:07 +0800
Subject: [R] Datamining-package-?
In-Reply-To: <1115a2b00702271734g74fc6d1awdab2adc413de2181@mail.gmail.com>
References: <b4485c4c0702271722k19e49523ic4615f9825d0b324@mail.gmail.com>
	<1115a2b00702271734g74fc6d1awdab2adc413de2181@mail.gmail.com>
Message-ID: <b4485c4c0702271752l463c6e99k83a26974eaab6e0e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/a137d032/attachment.pl 

From bridgman29 at cp.fuller.edu  Wed Feb 28 03:04:08 2007
From: bridgman29 at cp.fuller.edu (Matthew Bridgman)
Date: Tue, 27 Feb 2007 18:04:08 -0800
Subject: [R] lm ANOVA vs. AOV
Message-ID: <67D566B6-4624-435F-9B23-600816D5F2C5@cp.fuller.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/451cf8be/attachment.pl 

From res90sx5 at verizon.net  Wed Feb 28 03:01:39 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Tue, 27 Feb 2007 18:01:39 -0800
Subject: [R] Datamining-package-?
In-Reply-To: <b4485c4c0702271752l463c6e99k83a26974eaab6e0e@mail.gmail.com>
Message-ID: <004601c75adc$632d3300$0201a8c0@Aragorn>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of j.joshua thomas
> Sent: Tuesday, February 27, 2007 5:52 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Datamining-package-?
> 
> Hi again,
> The idea of preprocessing is mainly based on the need to prepare the data
> before they are actually used in pattern extraction.or feed the data
> into EA's (Genetic Algorithm) There are no standard practice yet however,
> the frequently used on are
> 
> 1. the extraction of derived attributes that is quantities that accompany
> but not directly related to the data patterns and may prove meaningful or
> increase the understanding of the patterns
> 
> 2. the removal of some existing attributes that should be of no concern to
> the mining process and its insignificance
> 
> So i looking for a package that can do this two above mentioned points....
> 
> Initially i would like to visualize the data into pattern and understand the
> patterns.
> 
> 
<<<snip>>>

Joshua,

You might take a look at the package rattle on CRAN for initially looking at your data and doing some basic data mining.

Hope this is helpful,

Dan

Daniel Nordlund
Bothell, WA, USA


From hb at stat.berkeley.edu  Wed Feb 28 03:15:08 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 27 Feb 2007 18:15:08 -0800
Subject: [R] Problem with R interface termination
In-Reply-To: <20070227141127.28245.qmail@web34309.mail.mud.yahoo.com>
References: <20070227141127.28245.qmail@web34309.mail.mud.yahoo.com>
Message-ID: <59d7961d0702271815s7ed8f2a5ud17963529d1ff64e@mail.gmail.com>

Hi,

this is due to the R.utils library.  Interestingly, others reported
this yesterday.  There are three different ways to get around the
problem right now:  1) Exit R so it does not call .Last() by
quit(runLast=FALSE),  2) load the R.utils package and then exit via
quit() as usual, or 3) remove the faulty .Last() function by
rm(.Last).  (Thus, you don't have kill the R process).

Brief explanation: If you load the R.utils package, it will modify or
create .Last() so that  finalizeSession() (defined in R.utils) is
called, which in turn calls so called registered hook functions
allowing optional code to be evaluated "whenever" R exists, cf.
http://tolstoy.newcastle.edu.au/R/devel/05/06/1206.html.  So, if you
exit R and save the session, this .Last() function will be saved to
your .RData file.  If you then start a new R session and load the
saved .RData, that modified .Last() function will also be loaded.
However, if you now try to exit R again, .Last() will be called which
in turn calls finalizeSession() which is undefined since R.utils is
not loaded and you will get the error that you reported.

I'm testing out an updated version of R.utils (v 0.8.6), which will
not cause this problem in the first place, and will fix the problem in
"faulty" .RData.  I will put in on CRAN as soon as I know it has been
tested thoroughly.  In the meanwhile, you can install it by:

source("http://www.braju.com/R/hbLite.R")
hbLite("R.utils")

To fix an old faulty .RData file, start R in the same directory, then
load the updated R.utils (which will replace that faulty .Last() with
a better one), and then save the session again when you exit R.  When
you do this you will see "Warning message: 'package:R.utils' may not
be available when loading", which is ok.  That should fix your
problems.

Let me know if it works

Henrik (author of R.utils)

On 2/27/07, Bhanu Kalyan.K <kalyansikha at yahoo.com> wrote:
> Dear Sir,
>
> The R interface that i am currently using is R 2.4.1( in Win XP). I am not able to terminate the program by giving the q() command. Each time I pass this command,
>
> > q()
> Error in .Last() : could not find function "finalizeSession"
>
> this error creeps in and it neither allows me to save my workspace nor come out of R. Therefore, whenever this happens, I am forced to end the Rgui.exe process from the task manager.
> Kindly help me fix this problem.
>
> Regards,
>
> Bhanu Kalyan K
>
>
> Bhanu Kalyan K
> B.Tech Final Year, CSE
>
> Tel: +91-9885238228
>
> Alternate E-Mail:
> reach4kalyan at gmail.com
>
>
> ---------------------------------
> Need Mail bonding?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From researchjj at gmail.com  Wed Feb 28 03:18:04 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 28 Feb 2007 10:18:04 +0800
Subject: [R] Datamining-package-?
In-Reply-To: <004601c75adc$632d3300$0201a8c0@Aragorn>
References: <b4485c4c0702271752l463c6e99k83a26974eaab6e0e@mail.gmail.com>
	<004601c75adc$632d3300$0201a8c0@Aragorn>
Message-ID: <b4485c4c0702271818p27b29d6fn723ebd99eeed10d3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/96207fb9/attachment.pl 

From researchjj at gmail.com  Wed Feb 28 03:29:40 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 28 Feb 2007 10:29:40 +0800
Subject: [R] Datamining-package-?
In-Reply-To: <b4485c4c0702271818p27b29d6fn723ebd99eeed10d3@mail.gmail.com>
References: <b4485c4c0702271752l463c6e99k83a26974eaab6e0e@mail.gmail.com>
	<004601c75adc$632d3300$0201a8c0@Aragorn>
	<b4485c4c0702271818p27b29d6fn723ebd99eeed10d3@mail.gmail.com>
Message-ID: <b4485c4c0702271829y447c156n80c19a013f7f5de7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/e13357d9/attachment.pl 

From jholtman at gmail.com  Wed Feb 28 03:32:23 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 27 Feb 2007 21:32:23 -0500
Subject: [R] Datamining-package-?
In-Reply-To: <b4485c4c0702271818p27b29d6fn723ebd99eeed10d3@mail.gmail.com>
References: <b4485c4c0702271752l463c6e99k83a26974eaab6e0e@mail.gmail.com>
	<004601c75adc$632d3300$0201a8c0@Aragorn>
	<b4485c4c0702271818p27b29d6fn723ebd99eeed10d3@mail.gmail.com>
Message-ID: <644e1f320702271832n1e41fab9uf9bd128fceb1da08@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/926febca/attachment.pl 

From roberto.perdisci at gmail.com  Wed Feb 28 03:40:55 2007
From: roberto.perdisci at gmail.com (Roberto Perdisci)
Date: Tue, 27 Feb 2007 21:40:55 -0500
Subject: [R] Datamining-package-?
In-Reply-To: <b4485c4c0702271829y447c156n80c19a013f7f5de7@mail.gmail.com>
References: <b4485c4c0702271752l463c6e99k83a26974eaab6e0e@mail.gmail.com>
	<004601c75adc$632d3300$0201a8c0@Aragorn>
	<b4485c4c0702271818p27b29d6fn723ebd99eeed10d3@mail.gmail.com>
	<b4485c4c0702271829y447c156n80c19a013f7f5de7@mail.gmail.com>
Message-ID: <cf94d0090702271840g272c2f2bt2da6922c8c6cd922@mail.gmail.com>

Hi,
  out of curiosity, what is the name of the package you found?

Roberto

On 2/27/07, j.joshua thomas <researchjj at gmail.com> wrote:
> Dear Group,
>
> I have found the package.
>
> Thanks very much
>
>
> JJ
> ---
>
>
> On 2/28/07, j.joshua thomas <researchjj at gmail.com> wrote:
> >
> >
> > I couldn't locate package rattle?  Need some one's help.
> >
> >
> > JJ
> > ---
> >
> >
> >
> > On 2/28/07, Daniel Nordlund <res90sx5 at verizon.net> wrote:
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch [mailto:
> > > r-help-bounces at stat.math.ethz.ch]
> > > > On Behalf Of j.joshua thomas
> > > > Sent: Tuesday, February 27, 2007 5:52 PM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: Re: [R] Datamining-package-?
> > > >
> > > > Hi again,
> > > > The idea of preprocessing is mainly based on the need to prepare the
> > > data
> > > > before they are actually used in pattern extraction.or feed the data
> > > > into EA's (Genetic Algorithm) There are no standard practice yet
> > > however,
> > > > the frequently used on are
> > > >
> > > > 1. the extraction of derived attributes that is quantities that
> > > accompany
> > > > but not directly related to the data patterns and may prove meaningful
> > > or
> > > > increase the understanding of the patterns
> > > >
> > > > 2. the removal of some existing attributes that should be of no
> > > concern to
> > > > the mining process and its insignificance
> > > >
> > > > So i looking for a package that can do this two above mentioned
> > > points....
> > > >
> > > > Initially i would like to visualize the data into pattern and
> > > understand the
> > > > patterns.
> > > >
> > > >
> > > <<<snip>>>
> > >
> > > Joshua,
> > >
> > > You might take a look at the package rattle on CRAN for initially
> > > looking at your data and doing some basic data mining.
> > >
> > > Hope this is helpful,
> > >
> > > Dan
> > >
> > > Daniel Nordlund
> > > Bothell, WA, USA
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> >
> > --
> > Lecturer J. Joshua Thomas
> > KDU College Penang Campus
> > Research Student,
> > University Sains Malaysia
> >
>
>
>
> --
> Lecturer J. Joshua Thomas
> KDU College Penang Campus
> Research Student,
> University Sains Malaysia
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From qijcey at fbz-metaal.be  Wed Feb 28 03:44:41 2007
From: qijcey at fbz-metaal.be (Magdalena Lott)
Date: Wed, 28 Feb 2007 04:44:41 +0200
Subject: [R] Be true
Message-ID: <00cc01c75b65$3cca66e0$2370c2a0@ashakoc>

(Watch Michael J. Fox back McCaskill on stem cells -- :32 ) Democrats say they are ahead

Critical Care Inc, 
symbl: .CTCX.
This one is an easy doubler
@ 65 cents wont last long
Expected target : $ 3.00 
 Critical Care Announces Expansion of Cost Containment Activities 
Business Wire (Fri, Feb 16)  
 Critical Care to Acquire Health Care Company 
Business Wire (Wed, Jan 31)  

GET IN TOMORROW, This one is shoo in to DOUBLE

evening. "It's unfortunate that Talent is one of the only Republicans who agrees."


From researchjj at gmail.com  Wed Feb 28 03:54:27 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 28 Feb 2007 10:54:27 +0800
Subject: [R] Datamining-package rattle() Errors
Message-ID: <b4485c4c0702271854k7448ff2bqd490f1f70b65ef16@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/c3011de3/attachment.pl 

From jholtman at gmail.com  Wed Feb 28 03:56:49 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 27 Feb 2007 21:56:49 -0500
Subject: [R] Function to do multiple named lookups faster?
In-Reply-To: <fd913b0d0702271043u4dc34295pa820838a44819326@mail.gmail.com>
References: <fd913b0d0702271043u4dc34295pa820838a44819326@mail.gmail.com>
Message-ID: <644e1f320702271856l4d819f81q7313a3995df2fcd4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070227/02deb53c/attachment.pl 

From tchur at optushome.com.au  Wed Feb 28 04:02:37 2007
From: tchur at optushome.com.au (Tim Churches)
Date: Wed, 28 Feb 2007 14:02:37 +1100
Subject: [R] Datamining-package rattle() Errors
In-Reply-To: <b4485c4c0702271854k7448ff2bqd490f1f70b65ef16@mail.gmail.com>
References: <b4485c4c0702271854k7448ff2bqd490f1f70b65ef16@mail.gmail.com>
Message-ID: <45E4F0CD.2030907@optushome.com.au>

j.joshua thomas wrote:
> Dear Group
> 
> I have few errors while installing package rattle from CRAN
> 
> i do the installing from the local zip files...
> 
>  I am using R 2.4.0 do i have to upgrade to R2.4.1 ?

You *do* have to read the r-help posting guide and take exact heed of
what it suggests: http://www.r-project.org/posting-guide.html

Tim C

> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> utils:::menuInstallLocal()
> package 'rattle' successfully unpacked and MD5 sums checked
> updating HTML package descriptions
>> help(rattle)
> No documentation for 'rattle' in specified packages and libraries:
> you could try 'help.search("rattle")'
>> library(rattle)
> Rattle, Graphical interface for data mining using R, Version 2.2.0.
> Copyright (C) 2006 Graham.Williams at togaware.com, GPL
> Type "rattle()" to shake, rattle, and roll your data.
> Warning message:
> package 'rattle' was built under R version 2.4.1
>> rattle()
> Error in rattle() : could not find function "gladeXMLNew"
> In addition: Warning message:
> there is no package called 'RGtk2' in: library(package, lib.loc = lib.loc,
> character.only = TRUE, logical = TRUE,
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> update.packages(ask='graphics')
> 
> 
> On 2/28/07, Roberto Perdisci <roberto.perdisci at gmail.com> wrote:
>> Hi,
>> out of curiosity, what is the name of the package you found?
>>
>> Roberto
>>
>> On 2/27/07, j.joshua thomas <researchjj at gmail.com> wrote:
>>> Dear Group,
>>>
>>> I have found the package.
>>>
>>> Thanks very much
>>>
>>>
>>> JJ
>>> ---
>>>
>>>
>>> On 2/28/07, j.joshua thomas <researchjj at gmail.com> wrote:
>>>>
>>>> I couldn't locate package rattle?  Need some one's help.
>>>>
>>>>
>>>> JJ
>>>> ---
>>>>
>>>>
>>>>
>>>> On 2/28/07, Daniel Nordlund <res90sx5 at verizon.net> wrote:
>>>>>> -----Original Message-----
>>>>>> From: r-help-bounces at stat.math.ethz.ch [mailto:
>>>>> r-help-bounces at stat.math.ethz.ch]
>>>>>> On Behalf Of j.joshua thomas
>>>>>> Sent: Tuesday, February 27, 2007 5:52 PM
>>>>>> To: r-help at stat.math.ethz.ch
>>>>>> Subject: Re: [R] Datamining-package-?
>>>>>>
>>>>>> Hi again,
>>>>>> The idea of preprocessing is mainly based on the need to prepare
>> the
>>>>> data
>>>>>> before they are actually used in pattern extraction.or feed the
>> data
>>>>>> into EA's (Genetic Algorithm) There are no standard practice yet
>>>>> however,
>>>>>> the frequently used on are
>>>>>>
>>>>>> 1. the extraction of derived attributes that is quantities that
>>>>> accompany
>>>>>> but not directly related to the data patterns and may prove
>> meaningful
>>>>> or
>>>>>> increase the understanding of the patterns
>>>>>>
>>>>>> 2. the removal of some existing attributes that should be of no
>>>>> concern to
>>>>>> the mining process and its insignificance
>>>>>>
>>>>>> So i looking for a package that can do this two above mentioned
>>>>> points....
>>>>>> Initially i would like to visualize the data into pattern and
>>>>> understand the
>>>>>> patterns.
>>>>>>
>>>>>>
>>>>> <<<snip>>>
>>>>>
>>>>> Joshua,
>>>>>
>>>>> You might take a look at the package rattle on CRAN for initially
>>>>> looking at your data and doing some basic data mining.
>>>>>
>>>>> Hope this is helpful,
>>>>>
>>>>> Dan
>>>>>
>>>>> Daniel Nordlund
>>>>> Bothell, WA, USA
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html<
>> http://www.r-project.org/posting-guide.html>
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>>
>>>> --
>>>> Lecturer J. Joshua Thomas
>>>> KDU College Penang Campus
>>>> Research Student,
>>>> University Sains Malaysia
>>>>
>>>
>>>
>>> --
>>> Lecturer J. Joshua Thomas
>>> KDU College Penang Campus
>>> Research Student,
>>> University Sains Malaysia
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
> 
> 
>


From NordlDJ at dshs.wa.gov  Wed Feb 28 04:19:24 2007
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Tue, 27 Feb 2007 19:19:24 -0800
Subject: [R] Datamining-package rattle() Errors
In-Reply-To: <b4485c4c0702271854k7448ff2bqd490f1f70b65ef16@mail.gmail.com>
References: <b4485c4c0702271854k7448ff2bqd490f1f70b65ef16@mail.gmail.com>
Message-ID: <941871A13165C2418EC144ACB212BDB0078D55@dshsmxoly1504g.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of j.joshua thomas
> Sent: Tuesday, February 27, 2007 6:54 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Datamining-package rattle() Errors
> 
> Dear Group
> 
> I have few errors while installing package rattle from CRAN
> 
> i do the installing from the local zip files...
> 
>  I am using R 2.4.0 do i have to upgrade to R2.4.1 ?
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> utils:::menuInstallLocal()
> package 'rattle' successfully unpacked and MD5 sums checked
> updating HTML package descriptions
> > help(rattle)
> No documentation for 'rattle' in specified packages and libraries:
> you could try 'help.search("rattle")'
> > library(rattle)
> Rattle, Graphical interface for data mining using R, Version 2.2.0.
> Copyright (C) 2006 Graham.Williams at togaware.com, GPL
> Type "rattle()" to shake, rattle, and roll your data.
> Warning message:
> package 'rattle' was built under R version 2.4.1
> > rattle()
> Error in rattle() : could not find function "gladeXMLNew"
> In addition: Warning message:
> there is no package called 'RGtk2' in: library(package, 
> lib.loc = lib.loc,
> character.only = TRUE, logical = TRUE,
> > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> > update.packages(ask='graphics')
> 
> 
> On 2/28/07, Roberto Perdisci <roberto.perdisci at gmail.com> wrote:
> >
> > Hi,
> > out of curiosity, what is the name of the package you found?
> >
> > Roberto
> >
> > On 2/27/07, j.joshua thomas <researchjj at gmail.com> wrote:
> > > Dear Group,
> > >
> > > I have found the package.
> > >
> > > Thanks very much
> > >
> > >
> > > JJ
> > > ---
> > >
> > >
> > > On 2/28/07, j.joshua thomas <researchjj at gmail.com> wrote:
> > > >
> > > >
> > > > I couldn't locate package rattle?  Need some one's help.
> > > >
> > > >
> > > > JJ
> > > > ---
> > > >
> > > >
> > > >
> > > > On 2/28/07, Daniel Nordlund <res90sx5 at verizon.net> wrote:
> > > > >
> > > > > > -----Original Message-----
> > > > > > From: r-help-bounces at stat.math.ethz.ch [mailto:
> > > > > r-help-bounces at stat.math.ethz.ch]
> > > > > > On Behalf Of j.joshua thomas
> > > > > > Sent: Tuesday, February 27, 2007 5:52 PM
> > > > > > To: r-help at stat.math.ethz.ch
> > > > > > Subject: Re: [R] Datamining-package-?
> > > > > >
> > > > > > Hi again,
> > > > > > The idea of preprocessing is mainly based on the 
> need to prepare
> > the
> > > > > data
> > > > > > before they are actually used in pattern 
> extraction.or feed the
> > data
> > > > > > into EA's (Genetic Algorithm) There are no standard 
> practice yet
> > > > > however,
> > > > > > the frequently used on are
> > > > > >
> > > > > > 1. the extraction of derived attributes that is 
> quantities that
> > > > > accompany
> > > > > > but not directly related to the data patterns and may prove
> > meaningful
> > > > > or
> > > > > > increase the understanding of the patterns
> > > > > >
> > > > > > 2. the removal of some existing attributes that 
> should be of no
> > > > > concern to
> > > > > > the mining process and its insignificance
> > > > > >
> > > > > > So i looking for a package that can do this two 
> above mentioned
> > > > > points....
> > > > > >
> > > > > > Initially i would like to visualize the data into 
> pattern and
> > > > > understand the
> > > > > > patterns.
> > > > > >
> > > > > >
> > > > > <<<snip>>>
> > > > >
> > > > > Joshua,
> > > > >
> > > > > You might take a look at the package rattle on CRAN 
> for initially
> > > > > looking at your data and doing some basic data mining.
> > > > >
> > > > > Hope this is helpful,
> > > > >
> > > > > Dan
> > > > >
> > > > > Daniel Nordlund
> > > > > Bothell, WA, USA
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html<
> > http://www.r-project.org/posting-guide.html>
> > > > > and provide commented, minimal, self-contained, 
> reproducible code.
> > > > >
> > > >
> > > >
> > > >
> > > > --
> > > > Lecturer J. Joshua Thomas
> > > > KDU College Penang Campus
> > > > Research Student,
> > > > University Sains Malaysia
> > > >
> > >
> > >
> > >
> > > --
> > > Lecturer J. Joshua Thomas
> > > KDU College Penang Campus
> > > Research Student,
> > > University Sains Malaysia
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> 
> 
> 
> -- 
> Lecturer J. Joshua Thomas
> KDU College Penang Campus
> Research Student,
> University Sains Malaysia
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

Joshua,

Tim Churches has given you some good advice.  Let me add that I probably should have pointed you to the URL below in my original post.  You will need to install a couple other pieces of software to use rattle.  Take a look at this guide, and in particular, the installation section.  At the Togaware home page (I think) there is also some information about a Rattle mailing list.  If you can't find it, you can email me and I will send you the URL.  Good luck with your data mining.

http://datamining.togaware.com/survivor/index.html 

Hope this is of some additional help,

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


From mamoralesri at telecom.com.co  Wed Feb 28 04:22:22 2007
From: mamoralesri at telecom.com.co (Mario A. Morales R. )
Date: Tue, 27 Feb 2007 22:22:22 -0500
Subject: [R] matplot on lattice graphics
Message-ID: <45E4F56E.6090705@telecom.com.co>

can I  use matplot on each panel of a lattice graphic? How?


From f.harrell at vanderbilt.edu  Wed Feb 28 05:04:21 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 27 Feb 2007 22:04:21 -0600
Subject: [R] bootcov and cph error
In-Reply-To: <46B75B4A4A45914ABB0901364EFF4A209B0275@PMC-EMAIL.petermac.org.au>
References: <46B75B4A4A45914ABB0901364EFF4A209B0275@PMC-EMAIL.petermac.org.au>
Message-ID: <45E4FF45.7030100@vanderbilt.edu>

Williams Scott wrote:
> Hi all,
> I am trying to get bootstrap resampled estimates of covariates in a Cox
> model using cph (Design library).
> 
> Using the following I get the error:
> 
>> ddist2.abr <- datadist(data2.abr)
>> options(datadist='ddist2.abr') 
>> cph1.abr <- cph(Surv(strt3.abr,loc3.abr)~cov.a.abr+cov.b.abr,
> data=data2.abr, x=T, y=T) 
>> boot.cph1 <- bootcov(cph1.abr, B=100, coef.reps=TRUE, pr=T)
> 1 Error in oosl(f, matxv(X, cof), Y) : not implemented for cph models
> 
> Removing coef.reps argument works fine, but I really need the
> coefficients if at all possible. I cant find anything in the help files
> suggesting that I cant use coef.reps in a cph model. Any help
> appreciated.
> 
> Cheers
> 
> Scott

Sorry it's taken so long to get to this.  The documentation needs to be 
clarified.  Add loglik=FALSE to allow coef.reps=TRUE to work for cph models.

Frank

> 
> _____________________________
> 
>  
> 
> Dr. Scott Williams MD
> 
> Peter MacCallum Cancer Centre
> 
> Melbourne, Australia
> 
> scott.williams at petermac.org


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From Friedrich.Leisch at stat.uni-muenchen.de  Wed Feb 28 15:13:56 2007
From: Friedrich.Leisch at stat.uni-muenchen.de (Friedrich Leisch)
Date: Wed, 28 Feb 2007 15:13:56 +0100
Subject: [R] cluster analysis under contiguity constraints with R ?
In-Reply-To: <45D59F44.8050807@univ-nantes.fr>
References: <45D59F44.8050807@univ-nantes.fr>
Message-ID: <17893.36388.600577.911391@celebrian.ci.tuwien.ac.at>

>>>>> On Fri, 16 Feb 2007 13:10:44 +0100,
>>>>> Bellanger Lise (BL) wrote:

  > Hello,
  >     I would like to know if there is a function in an R library that 
  > allows to do cluster analysis under contiguity constraints ?
 
I don't know what exactly you mean by "contiguity constraints", but
package flexclust can cluster with group constraints, see

	http://www.ci.tuwien.ac.at/papers/Leisch+Gruen-2006.pdf

Hope this helps,
Fritz Leisch

-- 
-----------------------------------------------------------------------
Prof. Dr. Friedrich Leisch 

Institut f?r Statistik                          Tel: (+49 89) 2180 3165
Ludwig-Maximilians-Universit?t                  Fax: (+49 89) 2180 5308
Ludwigstra?e 33
D-80539 M?nchen                 http://www.stat.uni-muenchen.de/~leisch


From rmh at temple.edu  Wed Feb 28 05:20:05 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 27 Feb 2007 23:20:05 -0500 (EST)
Subject: [R] lm ANOVA vs. AOV
Message-ID: <20070227232005.BVN92529@po-d.temple.edu>

The underlying least squares arithmetic of aov and lm is identical.
In R, the QR algorithm is used.  The difference between the two is
intent of the analysis and the default presentation of the results.

With lm [Linear Model], the focus is on the effect of the individual
columns of the predictor matrix.  The columns are usually interpreted
as values of real-valued observations.  The regression coefficients
are usually meaningful and interesting.

With aov [Analysis Of Variance], the focus is on the effects of
factors.  These are multi-degree of freedom effects associated with
categorical variables.  The arithmetic is based on a set of dummy
variables constructed from a contrast matrix.  The individual
regression coefficients themselves are not easily interpretable.

You can pursue the details of this summary in any good statistical
methods book.

Rich


From p.dalgaard at biostat.ku.dk  Wed Feb 28 06:56:35 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 28 Feb 2007 06:56:35 +0100
Subject: [R] factor documentation issue
In-Reply-To: <93c3eada0702271511g3989ed3fme371f32559209441@mail.gmail.com>
References: <93c3eada0702271511g3989ed3fme371f32559209441@mail.gmail.com>
Message-ID: <45E51993.9000509@biostat.ku.dk>

Geoff Russell wrote:
> There is a warning in the documentation for ?factor  (R version 2.3.0)
> as follows:
>
> " The interpretation of a factor depends on both the codes and the
>   '"levels"' attribute.  Be careful only to compare factors with the
>   same set of levels (in the same order).  In particular,
>   'as.numeric' applied to a factor is meaningless, and may happen by
>   implicit coercion.  To "revert" a factor 'f' to its original
>   numeric values, 'as.numeric(levels(f))[f]' is recommended and
>   slightly more efficient than 'as.numeric(as.character(f))'.
>
>
> But as.numeric seems to work fine whereas as.numeric(levels(f))[f] doesn't
> always do anything useful.
>
> For example:
>
>   
>> f<-factor(1:3,labels=c("A","B","C"))
>> f
>>     
> [1] A B C
> Levels: A B C
>   
>> as.numeric(f)
>>     
> [1] 1 2 3
>   
>> as.numeric(levels(f))[f]
>>     
> [1] NA NA NA
> Warning message:
> NAs introduced by coercion
>
> And also,
>
>   
>> f<-factor(1:3,labels=c(1,5,6))
>> f
>>     
> [1] 1 5 6
> Levels: 1 5 6
>   
>> as.numeric(f)
>>     
> [1] 1 2 3
>   
>> as.numeric(levels(f))[f]
>>     
> [1] 1 5 6
>
> Is the documentation wrong, or is the code wrong, or have I missed
> something?
>   

The documentation is somewhat unclear: The last sentence presupposes 
that the factor was generated from numeric data, i.e. the 
factor(c(7,9,13)) syndrome:

 > f <- factor (c(7,9,13))
 > f
[1] 7  9  13
Levels: 7 9 13
 > as.numeric(f)
[1] 1 2 3

Also, the statement that as.numeric(f) is meaningless is a bit strong. 
Probably should say "meaningless without knowledge of the levels and 
their order". And you can actually compare factors with their levels in 
different order:

 > g <- factor (c("7",9,13))
 > g
[1] 7  9  13
Levels: 13 7 9
 > f==g
[1] TRUE TRUE TRUE
 > as.numeric(f)==as.numeric(g)
[1] FALSE FALSE FALSE

Where you need to be careful is that if you do things like
   sexsymbols <- c(16, 19)
   plot(x, y, pch=sexsymbols[sex]),
then you should also do
   legend(x0, y0, legend=levels(sex), pch=sexsymbols)
in order to be sure the symbols match the legend. (Notice that indexing 
with  [sex] implicitly coerces sex to numeric).


From researchjj at gmail.com  Wed Feb 28 06:58:16 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 28 Feb 2007 13:58:16 +0800
Subject: [R] Datamining-package rattle() Errors
In-Reply-To: <b4485c4c0702271959w43d74b8an1ae64e0c4896a93c@mail.gmail.com>
References: <b4485c4c0702271854k7448ff2bqd490f1f70b65ef16@mail.gmail.com>
	<941871A13165C2418EC144ACB212BDB0078D55@dshsmxoly1504g.dshs.wa.lcl>
	<b4485c4c0702271959w43d74b8an1ae64e0c4896a93c@mail.gmail.com>
Message-ID: <b4485c4c0702272158s6a458a05sdbe55fecf12ac1bc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/56820251/attachment.pl 

From p.dalgaard at biostat.ku.dk  Wed Feb 28 07:07:42 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 28 Feb 2007 07:07:42 +0100
Subject: [R] lm ANOVA vs. AOV
In-Reply-To: <67D566B6-4624-435F-9B23-600816D5F2C5@cp.fuller.edu>
References: <67D566B6-4624-435F-9B23-600816D5F2C5@cp.fuller.edu>
Message-ID: <45E51C2E.3050403@biostat.ku.dk>

Matthew Bridgman wrote:
> Why would someone use lm and ANOVA (anova(lm(x))) instead of AOV (or  
> the other way around)?
> The mean squares and sum of squares are the same, but the F values  
> and p-values are slightly different.
>
>   
Crudely put, aov() is effectively useless on unbalanced designs. On the 
other hand, it will allow you to handle models with multistratum error 
structure.

I am somewhat at a loss as to how you manage to get the same MS and SS 
but different F. Presumably, the denominator is different, but if you're 
not messing with Error() terms, then I believe both aov() and 
anova(lm()) would use the residual MS. An example might help.

> I am modeling a dependent~independent1*independent2.
>
> Thanks,
> Matt Bridgman
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From researchjj at gmail.com  Wed Feb 28 07:42:42 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 28 Feb 2007 14:42:42 +0800
Subject: [R] package rattle() - libatk-1.0.0.dll Error
Message-ID: <b4485c4c0702272242n2937f4ebjac2871ae4a86fcac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/3768ba40/attachment.pl 

From hb at stat.berkeley.edu  Wed Feb 28 07:58:12 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 27 Feb 2007 22:58:12 -0800
Subject: [R] Problem with R interface termination
In-Reply-To: <59d7961d0702271815s7ed8f2a5ud17963529d1ff64e@mail.gmail.com>
References: <20070227141127.28245.qmail@web34309.mail.mud.yahoo.com>
	<59d7961d0702271815s7ed8f2a5ud17963529d1ff64e@mail.gmail.com>
Message-ID: <59d7961d0702272258v39caf51am2d23515383bebf82@mail.gmail.com>

For the completion of this thread; the below update was confirmed to
solve the problem.  /HB

On 2/27/07, Henrik Bengtsson <hb at stat.berkeley.edu> wrote:
> Hi,
>
> this is due to the R.utils library.  Interestingly, others reported
> this yesterday.  There are three different ways to get around the
> problem right now:  1) Exit R so it does not call .Last() by
> quit(runLast=FALSE),  2) load the R.utils package and then exit via
> quit() as usual, or 3) remove the faulty .Last() function by
> rm(.Last).  (Thus, you don't have kill the R process).
>
> Brief explanation: If you load the R.utils package, it will modify or
> create .Last() so that  finalizeSession() (defined in R.utils) is
> called, which in turn calls so called registered hook functions
> allowing optional code to be evaluated "whenever" R exists, cf.
> http://tolstoy.newcastle.edu.au/R/devel/05/06/1206.html.  So, if you
> exit R and save the session, this .Last() function will be saved to
> your .RData file.  If you then start a new R session and load the
> saved .RData, that modified .Last() function will also be loaded.
> However, if you now try to exit R again, .Last() will be called which
> in turn calls finalizeSession() which is undefined since R.utils is
> not loaded and you will get the error that you reported.
>
> I'm testing out an updated version of R.utils (v 0.8.6), which will
> not cause this problem in the first place, and will fix the problem in
> "faulty" .RData.  I will put in on CRAN as soon as I know it has been
> tested thoroughly.  In the meanwhile, you can install it by:
>
> source("http://www.braju.com/R/hbLite.R")
> hbLite("R.utils")
>
> To fix an old faulty .RData file, start R in the same directory, then
> load the updated R.utils (which will replace that faulty .Last() with
> a better one), and then save the session again when you exit R.  When
> you do this you will see "Warning message: 'package:R.utils' may not
> be available when loading", which is ok.  That should fix your
> problems.
>
> Let me know if it works
>
> Henrik (author of R.utils)
>
> On 2/27/07, Bhanu Kalyan.K <kalyansikha at yahoo.com> wrote:
> > Dear Sir,
> >
> > The R interface that i am currently using is R 2.4.1( in Win XP). I am not able to terminate the program by giving the q() command. Each time I pass this command,
> >
> > > q()
> > Error in .Last() : could not find function "finalizeSession"
> >
> > this error creeps in and it neither allows me to save my workspace nor come out of R. Therefore, whenever this happens, I am forced to end the Rgui.exe process from the task manager.
> > Kindly help me fix this problem.
> >
> > Regards,
> >
> > Bhanu Kalyan K
> >
> >
> > Bhanu Kalyan K
> > B.Tech Final Year, CSE
> >
> > Tel: +91-9885238228
> >
> > Alternate E-Mail:
> > reach4kalyan at gmail.com
> >
> >
> > ---------------------------------
> > Need Mail bonding?
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From researchjj at gmail.com  Wed Feb 28 08:13:39 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 28 Feb 2007 15:13:39 +0800
Subject: [R] Package RGtk2, rattle, libatk-1.0.0.dll Errors
Message-ID: <b4485c4c0702272313l727904b3r1ae02f0a01f0cb5b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/3edc5bef/attachment.pl 

From ligges at statistik.uni-dortmund.de  Wed Feb 28 08:39:35 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Feb 2007 08:39:35 +0100
Subject: [R] Datamining-package rattle() Errors
In-Reply-To: <b4485c4c0702272158s6a458a05sdbe55fecf12ac1bc@mail.gmail.com>
References: <b4485c4c0702271854k7448ff2bqd490f1f70b65ef16@mail.gmail.com>	<941871A13165C2418EC144ACB212BDB0078D55@dshsmxoly1504g.dshs.wa.lcl>	<b4485c4c0702271959w43d74b8an1ae64e0c4896a93c@mail.gmail.com>
	<b4485c4c0702272158s6a458a05sdbe55fecf12ac1bc@mail.gmail.com>
Message-ID: <45E531B7.8020800@statistik.uni-dortmund.de>

Obviously the rattle folks a) have a not that up to date version of 
rattle on CRAN and b) failed to set up the Windows repository correctly 
on http://rattle.togaware.com/

Uwe Ligges


j.joshua thomas wrote:
> I browse through the web link http://rattle.togaware.com/ and followed the
> instructions
> 
> Installation on RGtk2 has been done without errors. however i have problem
> with rattle.
> 
> So i download rattle 2.2.4.zip and unpack it. earlier i have tried from CRAN
> rattle 2.2.0.zip it showed some warning messages please see the following
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> install.packages("rattle", repos="http://rattle.togaware.com")
> Warning: unable to access index for repository
> http://rattle.togaware.com/bin/windows/contrib/2.4
> Warning in download.packages(pkgs, destdir = tmpd, available = available,  :
> 
>          no package 'rattle' at the repositories
>> library(rattle)
> Rattle, Graphical interface for data mining using R, Version 2.2.0.
> Copyright (C) 2006 Graham.Williams at togaware.com, GPL
> Type "rattle()" to shake, rattle, and roll your data.
> Warning message:
> package 'rattle' was built under R version 2.4.1
>> utils:::menuInstallLocal()
> Warning: package 'rattle' is in use and will not be installed
> updating HTML package descriptions
> JJ
> ---
> 
> 
> 
> 
> On 2/28/07, j.joshua thomas <researchjj at gmail.com> wrote:
>> Dan,
>> I browse through the web link http://rattle.togaware.com/ and followed the
>> instructions
>>
>> Installation on RGtk2 has been done without errors. however i have problem
>> with rattle.
>>
>> So i download rattle 2.2.4.zip and unpack it. earlier i have tried from
>> CRAN rattle 2.2.0.zip it showed some warning messages please see the
>> following
>>
>>
>>
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>> install.packages("rattle", repos="http://rattle.togaware.com")
>> Warning: unable to access index for repository
>> http://rattle.togaware.com/bin/windows/contrib/2.4
>> Warning in download.packages(pkgs, destdir = tmpd, available = available,
>> :
>>          no package 'rattle' at the repositories
>>> library(rattle)
>> Rattle, Graphical interface for data mining using R, Version 2.2.0.
>> Copyright (C) 2006 Graham.Williams at togaware.com, GPL
>> Type "rattle()" to shake, rattle, and roll your data.
>> Warning message:
>> package 'rattle' was built under R version 2.4.1
>>> utils:::menuInstallLocal()
>> Warning: package 'rattle' is in use and will not be installed
>> updating HTML package descriptions
>>
>>
>>  On 2/28/07, Nordlund, Dan (DSHS/RDA) <NordlDJ at dshs.wa.gov> wrote:
>>>> -----Original Message-----
>>>> From: r-help-bounces at stat.math.ethz.ch
>>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of j.joshua thomas
>>>> Sent: Tuesday, February 27, 2007 6:54 PM
>>>> To: r-help at stat.math.ethz.ch
>>>> Subject: [R] Datamining-package rattle() Errors
>>>>
>>>> Dear Group
>>>>
>>>> I have few errors while installing package rattle from CRAN
>>>>
>>>> i do the installing from the local zip files...
>>>>
>>>>  I am using R 2.4.0 do i have to upgrade to R2.4.1 ?
>>>>
>>>>
>>>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>>>
>>>> utils:::menuInstallLocal()
>>>> package 'rattle' successfully unpacked and MD5 sums checked
>>>> updating HTML package descriptions
>>>>> help(rattle)
>>>> No documentation for 'rattle' in specified packages and libraries:
>>>> you could try 'help.search("rattle")'
>>>>> library(rattle)
>>>> Rattle, Graphical interface for data mining using R, Version 2.2.0.
>>>> Copyright (C) 2006 Graham.Williams at togaware.com, GPL
>>>> Type "rattle()" to shake, rattle, and roll your data.
>>>> Warning message:
>>>> package 'rattle' was built under R version 2.4.1
>>>>> rattle()
>>>> Error in rattle() : could not find function "gladeXMLNew"
>>>> In addition: Warning message:
>>>> there is no package called 'RGtk2' in: library(package,
>>>> lib.loc = lib.loc,
>>>> character.only = TRUE, logical = TRUE,
>>>>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>>>> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>>>>> update.packages(ask='graphics')
>>>>
>>>> On 2/28/07, Roberto Perdisci <roberto.perdisci at gmail.com > wrote:
>>>>> Hi,
>>>>> out of curiosity, what is the name of the package you found?
>>>>>
>>>>> Roberto
>>>>>
>>>>> On 2/27/07, j.joshua thomas < researchjj at gmail.com> wrote:
>>>>>> Dear Group,
>>>>>>
>>>>>> I have found the package.
>>>>>>
>>>>>> Thanks very much
>>>>>>
>>>>>>
>>>>>> JJ
>>>>>> ---
>>>>>>
>>>>>>
>>>>>> On 2/28/07, j.joshua thomas <researchjj at gmail.com> wrote:
>>>>>>>
>>>>>>> I couldn't locate package rattle?  Need some one's help.
>>>>>>>
>>>>>>>
>>>>>>> JJ
>>>>>>> ---
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On 2/28/07, Daniel Nordlund <res90sx5 at verizon.net> wrote:
>>>>>>>>> -----Original Message-----
>>>>>>>>> From: r-help-bounces at stat.math.ethz.ch [mailto:
>>>>>>>> r-help-bounces at stat.math.ethz.ch ]
>>>>>>>>> On Behalf Of j.joshua thomas
>>>>>>>>> Sent: Tuesday, February 27, 2007 5:52 PM
>>>>>>>>> To: r-help at stat.math.ethz.ch
>>>>>>>>> Subject: Re: [R] Datamining-package-?
>>>>>>>>>
>>>>>>>>> Hi again,
>>>>>>>>> The idea of preprocessing is mainly based on the
>>>> need to prepare
>>>>> the
>>>>>>>> data
>>>>>>>>> before they are actually used in pattern
>>>> extraction.or feed the
>>>>> data
>>>>>>>>> into EA's (Genetic Algorithm) There are no standard
>>>> practice yet
>>>>>>>> however,
>>>>>>>>> the frequently used on are
>>>>>>>>>
>>>>>>>>> 1. the extraction of derived attributes that is
>>>> quantities that
>>>>>>>> accompany
>>>>>>>>> but not directly related to the data patterns and may prove
>>>>> meaningful
>>>>>>>> or
>>>>>>>>> increase the understanding of the patterns
>>>>>>>>>
>>>>>>>>> 2. the removal of some existing attributes that
>>>> should be of no
>>>>>>>> concern to
>>>>>>>>> the mining process and its insignificance
>>>>>>>>>
>>>>>>>>> So i looking for a package that can do this two
>>>> above mentioned
>>>>>>>> points....
>>>>>>>>> Initially i would like to visualize the data into
>>>> pattern and
>>>>>>>> understand the
>>>>>>>>> patterns.
>>>>>>>>>
>>>>>>>>>
>>>>>>>> <<<snip>>>
>>>>>>>>
>>>>>>>> Joshua,
>>>>>>>>
>>>>>>>> You might take a look at the package rattle on CRAN
>>>> for initially
>>>>>>>> looking at your data and doing some basic data mining.
>>>>>>>>
>>>>>>>> Hope this is helpful,
>>>>>>>>
>>>>>>>> Dan
>>>>>>>>
>>>>>>>> Daniel Nordlund
>>>>>>>> Bothell, WA, USA
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>> <
>>>>> http://www.r-project.org/posting-guide.html>
>>>>>>>> and provide commented, minimal, self-contained,
>>>> reproducible code.
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Lecturer J. Joshua Thomas
>>>>>>> KDU College Penang Campus
>>>>>>> Research Student,
>>>>>>> University Sains Malaysia
>>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Lecturer J. Joshua Thomas
>>>>>> KDU College Penang Campus
>>>>>> Research Student,
>>>>>> University Sains Malaysia
>>>>>>
>>>>>>         [[alternative HTML version deleted]]
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>> --
>>>> Lecturer J. Joshua Thomas
>>>> KDU College Penang Campus
>>>> Research Student,
>>>> University Sains Malaysia
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>> Joshua,
>>>
>>> Tim Churches has given you some good advice.  Let me add that I probably
>>> should have pointed you to the URL below in my original post.  You will need
>>> to install a couple other pieces of software to use rattle.  Take a look at
>>> this guide, and in particular, the installation section.  At the Togaware
>>> home page (I think) there is also some information about a Rattle mailing
>>> list.  If you can't find it, you can email me and I will send you the
>>> URL.  Good luck with your data mining.
>>>
>>> http://datamining.togaware.com/survivor/index.html
>>>
>>> Hope this is of some additional help,
>>>
>>> Dan
>>>
>>> Daniel J. Nordlund
>>> Research and Data Analysis
>>> Washington State Department of Social and Health Services
>>> Olympia, WA  98504-5204
>>>
>>>
>>>
>>>
>>
>> --
>> Lecturer J. Joshua Thomas
>> KDU College Penang Campus
>> Research Student,
>> University Sains Malaysia
>>
> 
> 
>


From ahimsa at camposarceiz.com  Wed Feb 28 09:23:53 2007
From: ahimsa at camposarceiz.com (ahimsa campos-arceiz)
Date: Wed, 28 Feb 2007 17:23:53 +0900
Subject: [R] no df to test the effect of an interaccion on a lmer mixed model
Message-ID: <45e920ef0702280023p7757ee7bw972b8bacae6103b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/4c4e2b52/attachment.pl 

From gavin.simpson at ucl.ac.uk  Wed Feb 28 09:29:24 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 28 Feb 2007 08:29:24 +0000
Subject: [R] cluster analysis under contiguity constraints with R ?
In-Reply-To: <45D59F44.8050807@univ-nantes.fr>
References: <45D59F44.8050807@univ-nantes.fr>
Message-ID: <1172651364.3024.16.camel@dhcppc2.my.nat.localnet>

On Fri, 2007-02-16 at 13:10 +0100, Bellanger Lise wrote:
> Hello,
>  
>     I would like to know if there is a function in an R library that 
> allows to do cluster analysis under contiguity constraints ?
>  
> 
>     Thank you very much for your answer !
> 
>     Lise Bellanger

Lise, 

One option might be some non-CRAN code from Steve Juggins. He has a
modified hclust (see ?hclust for details on this) that is designed to
work with samples that are ordered in some way. The type-use example
that Steve wrote it for is clustering samples in a lake sediment core,
where the samples are strictly ordered in time (depth) and you can only
fuse adjacent samples/groups.

If this is the sort of thing you meant, try:

http://www.campus.ncl.ac.uk/staff/Stephen.Juggins/analysis.htm

The function is chclust() in his in-development palaeo package.

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From res90sx5 at verizon.net  Wed Feb 28 09:29:17 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Wed, 28 Feb 2007 00:29:17 -0800
Subject: [R] Package RGtk2, rattle, libatk-1.0.0.dll Errors
In-Reply-To: <b4485c4c0702272313l727904b3r1ae02f0a01f0cb5b@mail.gmail.com>
Message-ID: <005901c75b12$89ee8e40$0201a8c0@Aragorn>

Joshua,

I have a few questions about the problems you are having which I will intersperse in your email below.  I recall having some of the same difficulties you are having, which for me, were at least in part due to not following the installation directions exactly, in the correct order. 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of j.joshua thomas
> Sent: Tuesday, February 27, 2007 11:14 PM
> To: r-help at stat.math.ethz.ch; rattle at togaware.com
> Subject: [R] Package RGtk2, rattle, libatk-1.0.0.dll Errors
> 
> Dear Group,
> 
> I have followed the instructions  from the link
> http://datamining.togaware.com/survivor/Installing_GTK.html
> However i couldn't fix the libatk01.0.0.dll application error
> 
Did you follow the instructions about installing the Glade package before doing any of the rest of the installation?  From the URL I pointed you to:

Installing GTK, R, and Rattle

1. Install the GTK+ libraries 
  
    ...

For MS/Windows, install the latest version of the Glade package from the http://gladewin32.sourceforge.net/modules/news/Glade for Windows website. Download the self-installing package (e.g., gtk-win32-devel-2.8.10-rc1.exe) and open it to install the libraries: 

MS/Windows: run gtk-win32-devel-2.8.10-rc1.exe

> Here, i did uninstall R-Gui-2.4.0 then did the fresh installation and still
> facing the same problem
> I am using Windows- XP

If you were going to reinstall, why not install the current R-2.4.1?  The FAQ and the posting guide say that you should always install the latest version of R before requesting help (especially since one of your error messages listed below pointed out that the RGtk2 package you were trying to install was built under R-2.4.1).

> 
> *Please find the following*
> 
> 
> R version 2.4.0 (2006-10-03)
> Copyright (C) 2006 The R Foundation for Statistical Computing
> ISBN 3-900051-07-0
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
>   Natural language support but running in an English locale
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
> 
> [Previously saved workspace restored]
> 
> > install.packages("RGtk2")
> --- Please select a CRAN mirror for use in this session ---
> trying URL '
> http://cran.au.r-project.org/bin/windows/contrib/2.4/RGtk2_2.8.7.zip'
> Content type 'application/zip' length 13050736 bytes
> opened URL
> downloaded 12744Kb
> 
> package 'RGtk2' successfully unpacked and MD5 sums checked
> 
> The downloaded packages are in
>         C:\Documents and Settings\jjoshua\Local
> Settings\Temp\RtmpLetwrb\downloaded_packages
> updating HTML package descriptions
> > install.packages("rattle")
> trying URL '
> http://cran.au.r-project.org/bin/windows/contrib/2.4/rattle_2.2.0.zip'
> Content type 'application/zip' length 340875 bytes
> opened URL
> downloaded 332Kb
> 
> package 'rattle' successfully unpacked and MD5 sums checked
> 
> The downloaded packages are in
>         C:\Documents and Settings\jjoshua\Local
> Settings\Temp\RtmpLetwrb\downloaded_packages
> updating HTML package descriptions
> > library(RGtk2)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> 'C:/PROGRA~1/R/R-24~1.0/library/RGtk2/libs/RGtk2.dll':

I'm not sure where this path came from, but it doesn't look right to me.  To install RGtk2 and rattle (and any other packages from CRAN), I would use the Packages/Install package(s)... item from the RGui menu bar.

>   LoadLibrary failure:  The specified module could not be found.
> 
> In addition: Warning message:
> package 'RGtk2' was built under R version 2.4.1
> Error: package/namespace load failed for 'RGtk2'
> >
> 

Installing rattle requires more work than the average package.  You need to make sure that you have the required software (external to R) installed before installing rattle.  I would suggest again that you subscribe to the Rattle mailing list.  You might get better help there.  I am willing to continue trying to help you either on or off list, but I am no expert, just a fellow traveler with you.

Dan

Daniel Nordlund
Bothell, WA  USA


From gregor.gorjanc at bfro.uni-lj.si  Wed Feb 28 09:48:48 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Wed, 28 Feb 2007 08:48:48 +0000 (UTC)
Subject: [R] .C HoltWinters
References: <1c6126db0702271532t45877628p88537b40fb6cd791@mail.gmail.com>
Message-ID: <loom.20070228T094810-455@post.gmane.org>

sj <ssj1364 <at> gmail.com> writes:

> Hello,
> 
> I would like to look at the compiled C code behind HoltWinters from the
> stats package. Is that possible? If so where do I find it?

Get the R source tarball from CRAN unpack it and search for the C code.

Gregor


From buser at stat.math.ethz.ch  Wed Feb 28 10:07:22 2007
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 28 Feb 2007 10:07:22 +0100
Subject: [R] prop.test or chisq.test ..?
In-Reply-To: <200702261736.31001.dylan.beaudette@gmail.com>
References: <200702261736.31001.dylan.beaudette@gmail.com>
Message-ID: <17893.17994.538502.77917@stat.math.ethz.ch>

Hi

Some comments are inside.

Dylan Beaudette writes:
 > Hi everyone,
 > 
 > Suppose I have a count the occurrences of positive results, and the total 
 > number of occurrences:
 > 
 > 
 > pos <- 14
 > total <- 15
 > 
 > testing that the proportion of positive occurrences is greater than 0.5 gives 
 > a p-value and confidence interval:
 > 
 > prop.test( pos, total, p=0.5, alternative='greater')
 > 
 >         1-sample proportions test with continuity correction
 > 
 > data:  14 out of 15, null probability 0.5 
 > X-squared = 9.6, df = 1, p-value = 0.0009729
 > alternative hypothesis: true p is greater than 0.5 
 > 95 percent confidence interval:
 >  0.706632 1.000000 
 > sample estimates:
 >         p 
 > 0.9333333
 > 

First of all by default there is a continuity correction in
prop.test(). If you use 

prop.test(pos, total, p=0.5, alternative="greater", correct = FALSE)

	1-sample proportions test without continuity correction

data:  pos out of total, null probability 0.5 
X-squared = 11.2667, df = 1, p-value = 0.0003946
alternative hypothesis: true p is greater than 0.5 
95 percent confidence interval:
 0.7492494 1.0000000 
sample estimates:
        p 
0.9333333 

Remark that know the X-squared is identical to  your result from
chisq.test(), but the p-value is 0.0007891/2

The reason is that you are testing here the against the
alternative "greater"

If you use a two sided test 

prop.test(pos, total, p=0.5, alternative="two.sided", correct = FALSE)

then you reporduce the results form chisq.test().


 > 
 > My question is how does the use of chisq.test() differ from the above 
 > operation. For example:
 > 
 > chisq.test(table( c(rep('pos', 14), rep('neg', 1)) ))
 > 
 >         Chi-squared test for given probabilities
 > 
 > data:  table(c(rep("pos", 14), rep("neg", 1))) 
 > X-squared = 11.2667, df = 1, p-value = 0.0007891
 > 
 > ... gives slightly different results. I am corrent in interpreting that the 
 > chisq.test() function in this case is giving me a p-value associated with the 
 > test that the probabilities of pos are *different* than the probabilities of 
 > neg -- and thus a larger p-value than the prop.test(... , p=0.5, 
 > alternative='greater') ? 
 > 

Yes. In your example chisq.test() tests the null hypothesis if
all population probabilities are equal

?chisq.test says:
"In this case, the hypothesis tested is whether the population
probabilities equal those in 'p', or are all equal if 'p' is not
given." 

which means p1 = p2 = 0.5 in your two population case against
the alternative p1 != p2.

This is similar to the test in prop.test() p=0.5 against p!=0.5 
and therefore you get identical results if you choose
alternative="two.sided" in prop.test().


 > I realize that this is a rather elementary question, and references to a text 
 > would be just as helpful. Ideally, I would like a measure of how much I 
 > can 'trust' that a larger proportion is also statistically meaningful. Thus 
 > far the results from prop.test() match my intuition, but
 > affirmation would be 

Your intuition was correct. Nevertheless in your original
results (with the continuity correction), the p-value of
prop.test()  (0.0009729) was larger than the p-value of
chisq.test() (0.0007891) and therefore against your intuition. 

 > great.
 > 
 > Cheers,
 > 
 > 
 > -- 
 > Dylan Beaudette
 > Soils and Biogeochemistry Graduate Group
 > University of California at Davis
 > 530.754.7341
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


Hope this helps

Christoph Buser

--------------------------------------------------------------

Credit and Surety PML study: visit our web page www.cs-pml.org

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/


From robert.lischke at web.de  Wed Feb 28 10:42:39 2007
From: robert.lischke at web.de (Robert Lischke)
Date: Wed, 28 Feb 2007 10:42:39 +0100
Subject: [R] R in linux
In-Reply-To: <6.2.3.4.2.20070227161337.03912610@aiminy.mail.iastate.edu>
References: <6.2.3.4.2.20070227161337.03912610@aiminy.mail.iastate.edu>
Message-ID: <7ee9d37a0702280142w56104bbeh8f8535b56d295511@mail.gmail.com>

hi,

there are a couple of alternatives, i'm sure someone is going to bring
up vi/emacs ;)

i'd suggest you take a look at rkward (http://rkward.sourceforge.net/)
and eclipse with the statet-plugin (http://www.eclipse.org/,
http://www.walware.de/goto/statet).

depending on which desktop your working kate from kde is also quite nice.

greetings,



rob


2007/2/27, Aimin Yan <aiminy at iastate.edu>:
> I used to use R-WinEdt in Windows. I know R-Winedt is only for R in Windows.
> Now I try to use R in linux. I am wondering if there  is something
> that can do same thing as R-Winedt for R in linux?
>
> Aimin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From giampi at kth.se  Wed Feb 28 10:49:23 2007
From: giampi at kth.se (Giampiero Salvi)
Date: Wed, 28 Feb 2007 10:49:23 +0100 (CET)
Subject: [R] adding graphics after dev.copy
Message-ID: <Pine.LNX.4.58.0702281037500.21779@ctt14.speech.kth.se>

Hi,
I am trying to create a plot and save it as pdf to the file
file1.pdf, then add some details and save the new plot as file2.pdf.
I would like to avoid repeating all the instructions needed to create
file1.pdf when I create file2.pdf.

This is what I have tried:

pdf(file="file1.pdf")
plot(1:10)
dev.copy(pdf,file="file2.pdf")
points(2,3)
dev.off()
dev.off()

When I run points I get the error:
Error in plot.xy(xy.coords(x, y), type = type, ...) :
        plot.new has not been called yet

When I try to do the same with the x11 device, it works:
x11()
plot(1:10)
dev.copy(x11)
points(2,3)
dev.off()
dev.off()

Can someone tell me what goes wrong in the first case and why the x11
and pdf devices behave differently?

Thank you!
Giampiero


From tuechler at gmx.at  Wed Feb 28 10:49:52 2007
From: tuechler at gmx.at (Heinz Tuechler)
Date: Wed, 28 Feb 2007 10:49:52 +0100
Subject: [R] factor documentation issue
In-Reply-To: <93c3eada0702271511g3989ed3fme371f32559209441@mail.gmail.co
 m>
Message-ID: <3.0.6.32.20070228104952.00b16a00@pop.gmx.net>

At 09:41 28.02.2007 +1030, Geoff Russell wrote:
>There is a warning in the documentation for ?factor  (R version 2.3.0)
>as follows:
>
>" The interpretation of a factor depends on both the codes and the
>  '"levels"' attribute.  Be careful only to compare factors with the
>  same set of levels (in the same order).  In particular,
>  'as.numeric' applied to a factor is meaningless, and may happen by
>  implicit coercion.  To "revert" a factor 'f' to its original
>  numeric values, 'as.numeric(levels(f))[f]' is recommended and
>  slightly more efficient than 'as.numeric(as.character(f))'.
>
>
>But as.numeric seems to work fine whereas as.numeric(levels(f))[f] doesn't
>always do anything useful.
>
>For example:
>
>> f<-factor(1:3,labels=c("A","B","C"))
>> f
>[1] A B C
>Levels: A B C
>> as.numeric(f)
>[1] 1 2 3
>> as.numeric(levels(f))[f]
>[1] NA NA NA
>Warning message:
>NAs introduced by coercion
>
>And also,
>
>> f<-factor(1:3,labels=c(1,5,6))
>> f
>[1] 1 5 6
>Levels: 1 5 6
>> as.numeric(f)
>[1] 1 2 3
>> as.numeric(levels(f))[f]
>[1] 1 5 6
>
>Is the documentation wrong, or is the code wrong, or have I missed
>something?
>
>Cheers,
>Geoff Russell
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>From "R Language Definition"

"2.3.1 Factors

Factors are used to describe items that can have a finite number of values
(gender, social class, etc.). 
...
Factors are currently implemented using an integer array to specify the
actual levels and a second array of names that are mapped to the integers.
Rather unfortunately users often make use of the implementation in order to
make some calculations easier. This, however, is an implementation issue
and is not guaranteed to hold in all implementations of R."

In my view factors are (miss)used in different, not necessarily connected
ways.
A factor may represent a statistical concept i.e. a categorical variable.
Further it may be an (internal) way of data reduction or some method for
labelling values.
In my view these concepts should not be mixed up and would I recommend to
avoid factors for data reduction and labelling.

Heinz


From paul.bivand at gmail.com  Wed Feb 28 11:18:35 2007
From: paul.bivand at gmail.com (Paul Bivand)
Date: Wed, 28 Feb 2007 10:18:35 +0000
Subject: [R] compiling issues with Mandriva Linux 2007 Discovery
In-Reply-To: <40e66e0b0702271236u4f1fccd8i9a8f40a9b4c83b77@mail.gmail.com>
References: <2FC987BC0B90B24786CAF43DD3F5719C814299@CRU105.cahe.ad.wsu.edu>
	<40e66e0b0702271236u4f1fccd8i9a8f40a9b4c83b77@mail.gmail.com>
Message-ID: <93c5133f0702280218p6489c081l7752b88633f539ed@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/af05b1b7/attachment.pl 

From pburns at pburns.seanet.com  Wed Feb 28 11:20:13 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 28 Feb 2007 10:20:13 +0000
Subject: [R] sort of OT: bootstrap tutorial
Message-ID: <45E5575D.8060700@pburns.seanet.com>

There is now a tutorial on bootstrapping and other resampling
methods at:

http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html

Corrections and other suggestions are welcome.

The project started because a novice asked me about bootstrapping.
My response was, "How dare you bug me while I'm playing with my
cats, just google for it."  My correspondent was not very impressed
with my advice.  Once I actually followed my strategy, I saw how
awful it was.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


From borchers at decrc.abb.de  Wed Feb 28 11:35:48 2007
From: borchers at decrc.abb.de (Hans W. Borchers)
Date: Wed, 28 Feb 2007 10:35:48 +0000 (UTC)
Subject: [R] Package RGtk2, rattle, libatk-1.0.0.dll Errors
References: <b4485c4c0702272313l727904b3r1ae02f0a01f0cb5b@mail.gmail.com>
Message-ID: <loom.20070228T112435-850@post.gmane.org>

j.joshua thomas <researchjj <at> gmail.com> writes:

> 
> Dear Group,
> 
> I have followed the instructions  from the link
> http://datamining.togaware.com/survivor/Installing_GTK.html
> However i couldn't fix the libatk01.0.0.dll application error
> 

It is important that libatk-1.0.0.dll is in your path, that is 
under Windows your Path variable must include <GTK-HOME>/bin, 
where <RGtK-HOME> is the directory you installed GTK to. (Or,
search for libatk-1.0.0.dll on your whole disk.

You can check this also by typing 'gtk-demo' into a DOS window,
then the GTK+ Code Demos will start in a new window if the path
is set correctly. If not, set it manually, as reinstalling will
usually not help (it's Windows, man).

Hans Werner


From Roger.Bivand at nhh.no  Wed Feb 28 11:41:10 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 28 Feb 2007 11:41:10 +0100 (CET)
Subject: [R] .C HoltWinters
In-Reply-To: <1c6126db0702271532t45877628p88537b40fb6cd791@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0702281136150.15830-100000@reclus.nhh.no>

On Tue, 27 Feb 2007, sj wrote:

> Hello,
> 
> I would like to look at the compiled C code behind HoltWinters from the
> stats package. Is that possible? If so where do I find it?

Please see the R help desk note in R News 6(4): 43-45, October 2006, by 
Uwe Ligges and Duncan Murdoch: "Accessing the sources" for full details.

In this case, you can browse the SVN trunk (current development version) 
at:

https://svn.r-project.org/R/trunk/src/library/stats/src/HoltWinters.c

> 
> thanks,
> 
> Spencer
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From giampi at kth.se  Wed Feb 28 11:47:44 2007
From: giampi at kth.se (Giampiero Salvi)
Date: Wed, 28 Feb 2007 11:47:44 +0100 (CET)
Subject: [R] adding graphics after dev.copy
In-Reply-To: <Pine.LNX.4.58.0702281037500.21779@ctt14.speech.kth.se>
References: <Pine.LNX.4.58.0702281037500.21779@ctt14.speech.kth.se>
Message-ID: <Pine.LNX.4.58.0702281143530.21779@ctt14.speech.kth.se>

Hi,
now I see the problem is that it is not possible to run dev.copy from
non-interactive devices (see the post in r-devel: "[Rd] feature
enhancement request & patch: dev.control(displaylist='en (PR#3424)"
Tue 08 Jul 2003 - 03:50:38 EST).

I'll move this question to the r-devel list.

Giampiero


On Wed, 28 Feb 2007, Giampiero Salvi wrote:

> Hi,
> I am trying to create a plot and save it as pdf to the file
> file1.pdf, then add some details and save the new plot as file2.pdf.
> I would like to avoid repeating all the instructions needed to create
> file1.pdf when I create file2.pdf.
>
> This is what I have tried:
>
> pdf(file="file1.pdf")
> plot(1:10)
> dev.copy(pdf,file="file2.pdf")
> points(2,3)
> dev.off()
> dev.off()
>
> When I run points I get the error:
> Error in plot.xy(xy.coords(x, y), type = type, ...) :
>         plot.new has not been called yet
>
> When I try to do the same with the x11 device, it works:
> x11()
> plot(1:10)
> dev.copy(x11)
> points(2,3)
> dev.off()
> dev.off()
>
> Can someone tell me what goes wrong in the first case and why the x11
> and pdf devices behave differently?
>
> Thank you!
> Giampiero
>


From giampi at kth.se  Wed Feb 28 11:52:16 2007
From: giampi at kth.se (Giampiero Salvi)
Date: Wed, 28 Feb 2007 11:52:16 +0100 (CET)
Subject: [R] adding graphics after dev.copy
In-Reply-To: <Pine.LNX.4.58.0702281143530.21779@ctt14.speech.kth.se>
References: <Pine.LNX.4.58.0702281037500.21779@ctt14.speech.kth.se>
	<Pine.LNX.4.58.0702281143530.21779@ctt14.speech.kth.se>
Message-ID: <Pine.LNX.4.58.0702281150570.21779@ctt14.speech.kth.se>

Hi,
sorry for the multiple posting ;)
Here is the way to do it:

pdf(file="file1.pdf")
dev.control(displaylist='enable')
plot(1:10)
dev.copy(pdf,file="file2.pdf")
points(2,3)
dev.off()
dev.off()

Giampiero


From Lukas.Indermaur at eawag.ch  Wed Feb 28 12:02:45 2007
From: Lukas.Indermaur at eawag.ch (Indermaur Lukas)
Date: Wed, 28 Feb 2007 12:02:45 +0100
Subject: [R] bootstrap
Message-ID: <FE8C160D1505B24497FA7C78D4DADACA047903@EA-MAIL.eawag.wroot.emp-eaw.ch>

Hi,
I would like to evaluate the frequency of the variables within the best selected model by AIC among a set of 12 competing models (I fit them with GLM) with a bootstrap procedure  to get unbiased results. So I would ike to do the ranking of the 12-model-set 10'000 times separately and calculate the frequency of variables of the 10'000 best ranked models. I wrote a script doing the model ranking (with some difficulty) for the pre-defined 12-model-set, that works. My model results (one row per model) are stored in a dataframe called "mydataframe". How can I implement my script simply into a bootstrap and sum up the frequencies of variables of the 10'000 bootstraped results?
 
I am not so good in R and would appreciate any hint.
 
Regards
Lukas
 
 
??? 
Lukas Indermaur, PhD student 
eawag / Swiss Federal Institute of Aquatic Science and Technology 
ECO - Department of Aquatic Ecology
?berlandstrasse 133
CH-8600 D?bendorf
Switzerland
 
Phone: +41 (0) 71 220 38 25
Fax    : +41 (0) 44 823 53 15 
Email: lukas.indermaur at eawag.ch
www.lukasindermaur.ch


From sommerfeld at mpil-ploen.mpg.de  Wed Feb 28 12:57:04 2007
From: sommerfeld at mpil-ploen.mpg.de (Ralf Sommerfeld)
Date: Wed, 28 Feb 2007 12:57:04 +0100
Subject: [R] LME without convergence
Message-ID: <45E56E10.50805@mpil-ploen.mpg.de>

Dear R-help list readers,

I am fitting a mixed model using the lme function (R V 2.3.1 for 
Windows). This is an example:

dep<-c(25,40,33.33,60,70.83,72,71.43,50,40,53.33,64,54.17,60,53.57)
yes<-c(0,1,2,3,4,5,6,0,1,2,3,4,5,6)
treat<-c(1,1,1,1,1,1,1,0,0,0,0,0,0,0) #factor

If I now fit a model with random slopes as well as intercepts:

model1<-lme(dep~yes,random=yes|treat)

R encounters the following problem:

Fehler in lme.formula(dep ~ yes, random = ~yes | treat) :
         iteration limit reached without convergence (9)

Following other instructions on that problem, I tried to adjust the 
iteration limits and tolerances using lmeControl(), however this did not 
work.

Can someone help me?

Best regards,
Ralf


From jrkrideau at yahoo.ca  Wed Feb 28 13:36:34 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 28 Feb 2007 07:36:34 -0500 (EST)
Subject: [R] sort of OT: bootstrap tutorial
In-Reply-To: <45E5575D.8060700@pburns.seanet.com>
Message-ID: <20070228123635.80834.qmail@web32811.mail.mud.yahoo.com>

Thank you Patrick. That looks very useful.
--- Patrick Burns <pburns at pburns.seanet.com> wrote:

> There is now a tutorial on bootstrapping and other
> resampling
> methods at:
> 
>
http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html
> 
> Corrections and other suggestions are welcome.
> 
> The project started because a novice asked me about
> bootstrapping.
> My response was, "How dare you bug me while I'm
> playing with my
> cats, just google for it."  My correspondent was not
> very impressed
> with my advice.  Once I actually followed my
> strategy, I saw how
> awful it was.
> 
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S
> User")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From mswierniak at o2.pl  Wed Feb 28 14:12:46 2007
From: mswierniak at o2.pl (jastar)
Date: Wed, 28 Feb 2007 05:12:46 -0800 (PST)
Subject: [R] delete selecting rows and columns
Message-ID: <9203533.post@talk.nabble.com>


Hi,
I'm working with a big square matrix (15k x 15k) and I have some trouble.
I want to deleting selecting rows and columns.
I use something like this:

> sel_r=c(15,34,384,985,4302,6213)
> sel_c=c(3,151,324,3384,7985,14302)
> matrix=matrix[-sel_r,-sel_c]

but it works very slow.
Does anybody know how to make it in faster way?
Thank's
-- 
View this message in context: http://www.nabble.com/delete-selecting-rows-and-columns-tf3308726.html#a9203533
Sent from the R help mailing list archive at Nabble.com.


From mswierniak at o2.pl  Wed Feb 28 14:12:46 2007
From: mswierniak at o2.pl (jastar)
Date: Wed, 28 Feb 2007 05:12:46 -0800 (PST)
Subject: [R] delete selecting rows and columns
Message-ID: <9203533.post@talk.nabble.com>


Hi,
I'm working with a big square matrix (15k x 15k) and I have some trouble.
I want to delete selecting rows and columns.
I'm using something like this:

> sel_r=c(15,34,384,985,4302,6213)
> sel_c=c(3,151,324,3384,7985,14302)
> matrix=matrix[-sel_r,-sel_c]

but it works very slow.
Does anybody know how to make it in faster way?
Thank's
-- 
View this message in context: http://www.nabble.com/delete-selecting-rows-and-columns-tf3308726.html#a9203533
Sent from the R help mailing list archive at Nabble.com.


From f.harrell at vanderbilt.edu  Wed Feb 28 14:15:47 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 28 Feb 2007 07:15:47 -0600
Subject: [R] bootstrap
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA047903@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA047903@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <45E58083.1000802@vanderbilt.edu>

Indermaur Lukas wrote:
> Hi,
> I would like to evaluate the frequency of the variables within the best selected model by AIC among a set of 12 competing models (I fit them with GLM) with a bootstrap procedure  to get unbiased results. So I would ike to do the ranking of the 12-model-set 10'000 times separately and calculate the frequency of variables of the 10'000 best ranked models. I wrote a script doing the model ranking (with some difficulty) for the pre-defined 12-model-set, that works. My model results (one row per model) are stored in a dataframe called "mydataframe". How can I implement my script simply into a bootstrap and sum up the frequencies of variables of the 10'000 bootstraped results?
>  
> I am not so good in R and would appreciate any hint.
>  
> Regards
> Lukas

I'm not entirely clear on your goals but the bootstrap selection 
frequency is mainly a replay of the significance of variables in the 
full model.  I don't see that it provides new information.  And 
selection frequency is marred by collinearity.

Frank

>  
>  
> ??? 
> Lukas Indermaur, PhD student 
> eawag / Swiss Federal Institute of Aquatic Science and Technology 
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
>  
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15 
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From bates at stat.wisc.edu  Wed Feb 28 15:02:24 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 28 Feb 2007 08:02:24 -0600
Subject: [R] LME without convergence
In-Reply-To: <45E56E10.50805@mpil-ploen.mpg.de>
References: <45E56E10.50805@mpil-ploen.mpg.de>
Message-ID: <40e66e0b0702280602g18b8962frce04adb7ea02a198@mail.gmail.com>

On 2/28/07, Ralf Sommerfeld <sommerfeld at mpil-ploen.mpg.de> wrote:
> Dear R-help list readers,
>
> I am fitting a mixed model using the lme function (R V 2.3.1 for
> Windows). This is an example:
>
> dep<-c(25,40,33.33,60,70.83,72,71.43,50,40,53.33,64,54.17,60,53.57)
> yes<-c(0,1,2,3,4,5,6,0,1,2,3,4,5,6)
> treat<-c(1,1,1,1,1,1,1,0,0,0,0,0,0,0) #factor
>
> If I now fit a model with random slopes as well as intercepts:
>
> model1<-lme(dep~yes,random=yes|treat)
>
> R encounters the following problem:
>
> Fehler in lme.formula(dep ~ yes, random = ~yes | treat) :
>          iteration limit reached without convergence (9)
>
> Following other instructions on that problem, I tried to adjust the
> iteration limits and tolerances using lmeControl(), however this did not
> work.
>
> Can someone help me?

Not really.  You're trying to estimate three variance-covariance
parameters from two groups and just about any way you try to do that
will encounter difficulties.


From timothyholland at gmail.com  Wed Feb 28 06:34:51 2007
From: timothyholland at gmail.com (Tim Holland)
Date: Wed, 28 Feb 2007 00:34:51 -0500
Subject: [R] SEM - standardized path coefficients?
Message-ID: <1dba89130702272134t12c46290rdc5a972cb4a16e5c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/cd7e79e3/attachment.pl 

From toby909 at gmail.com  Wed Feb 28 07:26:32 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Tue, 27 Feb 2007 22:26:32 -0800
Subject: [R] D() and deriv() accessing components of returned expression
Message-ID: <es37ap$ppc$1@sea.gmane.org>

Hi All

I like the D() function since it directly gives me the derivative. D() however 
does not take multiple arguments like c("x1", "x2"), but deriv() does, but 
deriv() I am having trouble accessing the actual derivative function. So 
strange, its sitting direct in front of me but I cant access it:

 > deriv(fct, c("x", "c"))
expression({
     .value <- x^2 - 5 * x - 2 + 5 * c
     .grad <- array(0, c(length(.value), 2), list(NULL, c("x",
         "c")))
     .grad[, "x"] <- 2 * x - 5
     .grad[, "c"] <- 5
     attr(.value, "gradient") <- .grad
     .value
})

I want to plug in the derivative expression into an eval() function. I guess 
this is a more general question of how to access parts of a "list"?

Thanks for your hint.

Toby




de = 1
x = -4
D(fct, "x")
dfct = deriv(fct, c("x", "c"))
while (abs(de)>0.000000001) {
de = solve(-eval(fct),eval(dfct))
x = de+x
print(x)
}


From valderama at gmail.com  Wed Feb 28 15:09:47 2007
From: valderama at gmail.com (Laurent Valdes)
Date: Wed, 28 Feb 2007 15:09:47 +0100
Subject: [R] PROC TABULATE with R
Message-ID: <3ef00e160702280609h5f0c0e4o1d60199706b30d4e@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070228/6ebce030/attachment.pl 

From Roger.Vallejo at ARS.USDA.GOV  Wed Feb 28 15:17:56 2007
From: Roger.Vallejo at ARS.USDA.GOV (Vallejo, Roger)
Date: Wed, 28 Feb 2007 09:17:56 -0500
Subject: [R] topTable function from LIMMA
Message-ID: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDC0@MD-MAIL-01.ARSNET.ARS.USDA.GOV>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/7c2d619f/attachment.pl 

From amsegura at fcien.edu.uy  Wed Feb 28 15:20:03 2007
From: amsegura at fcien.edu.uy (amsegura at fcien.edu.uy)
Date: Wed, 28 Feb 2007 12:20:03 -0200
Subject: [R] Piecewise linear regression with RMA
Message-ID: <1172672403.45e58f935398b@webmail.fcien.edu.uy>



Hi,
I'm working in crustacean maturity, using breakpoint analysis.
I used total length and Carapace length (linearly related) to find a break
point, wich is actually the size at maturity. Both sizes are measured with
error, so Modell II aplies.
Piecewise regression uses model I to find breakpoints.
Do anybody nows a way of finding the break point using a model II regression?
Any feedback on this is highly appreciated.

Angel Segura
Instituto de Investigaciones Pesqueras
Facultad de Veterinaria
Montevideo-Uruguay


From sue at xlsolutions-corp.com  Wed Feb 28 15:27:53 2007
From: sue at xlsolutions-corp.com (Sue Turner)
Date: Wed, 28 Feb 2007 07:27:53 -0700
Subject: [R] 2 Courses: (1) R/Splus Advanced Programming - San Francisco -
	(2) Data Mining: Practical Tools and Techniques in R/Splus -
	Salt Lake City
Message-ID: <20070228072753.9f08cc34deb45d78e54b3b5664e21546.fae7ac9fdc.wbe@email.secureserver.net>


XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce
our*** R/S Advanced Programming ***course in San Francisco on March
15-16 and
*** Data Mining: Practical Tools and Techniques in R/Splus *** course in
Salt Lake City on March 26-27


Ask for group discount and reserve your seat Now - Earlybird Rates.
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com


(1) R/Splus Advanced Programming  - San Francisco, March 15-16
http://www.xlsolutions-corp.com/Radv.htm  

Course Outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function

Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578

(2) Data Mining: Practical Tools and Techniques in R/Splus - Salt Lake
City, March 26-27
http://www.xlsolutions-corp.com/RSMining.htm



Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat!

Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com


From jmacdon at med.umich.edu  Wed Feb 28 15:30:35 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Wed, 28 Feb 2007 09:30:35 -0500
Subject: [R] topTable function from LIMMA
In-Reply-To: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDC0@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
References: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDC0@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
Message-ID: <45E5920B.30004@med.umich.edu>

Hi Roger,

Vallejo, Roger wrote:
> Dear R-Help,
> 
>  
> 
> I am using the function "topTable" from the LIMMA package.  To estimate
> adjusted P-values there are several options (adjust="fdr" , adjust="BH")
> as shown below:
> 
>  
> 
> topTable(fit, number = 10, adjust = "BH", fit$Name)   
> 
>  
> 
> I guess any of these options (fdr, BH, etc.) is using a default of
> FDR=0.05 which is quite conservative (i.e., very likely none of the
> tested genes will be ranked as differentially expressed at 5% FDR). I
> would argue that for an exploratory study it would be okay to use a 0.05
> < FDR <0.20. So, I was wondering if we can change that default of
> FDR=0.0.5 with a less stringent FDR value (0.10, 0.20, etc.) in the
> "topTable" function.  How do I change it?

The topTable() function doesn't have a default cutoff based on the 
p-value (adjusted or otherwise). The cutoff is based on the 'number' 
argument, which has a default of 10, so regardless of the p-value 
adjustment method used, you only get 10 genes if you don't change this 
default.

You can either increase the number to something arbitrarily high and 
then subset the resulting data.frame based on the p-value, or just use 
write.fit() to ouput the whole table and do the work in e.g., Excel.

Best,

Jim


> 
>  
> 
> Thanks a lot for the help on the use of "topTable" function from the
> LIMMA package.
> 
>  
> 
> Roger
> 
>  
> 
>  
> 
> Roger L. Vallejo, Ph.D.
> 
> Computational Biologist & Geneticist
> 
> U.S. Department of Agriculture, ARS          
> 
> National Center for Cool & Cold Water Aquaculture
> 
> 11861 Leetown Road
> 
> Kearneysville, WV 25430
> 
> Voice:    (304) 724-8340 Ext. 2141
> 
> Email:   roger.vallejo at ars.usda.gov <mailto:roger.vallejo at ars.usda.gov> 
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From albmont at centroin.com.br  Wed Feb 28 15:35:43 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 28 Feb 2007 12:35:43 -0200
Subject: [R] get more than get
Message-ID: <20070228143434.M47940@centroin.com.br>

This must be a stupid question, but is there any "extension" of get?

For example:
x <- 10
get("x") # gives me 10
get("x^2") # gives me an error

Alberto Monteiro


From jfox at mcmaster.ca  Wed Feb 28 15:37:22 2007
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 28 Feb 2007 09:37:22 -0500
Subject: [R] SEM - standardized path coefficients?
In-Reply-To: <1dba89130702272134t12c46290rdc5a972cb4a16e5c@mail.gmail.com>
Message-ID: <20070228143718.HSFF1767.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Tim,

See ?standardized.coefficients (after loading the sem package).

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tim Holland
> Sent: Wednesday, February 28, 2007 12:35 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] SEM - standardized path coefficients?
> 
> Hello -
> 
> Does anybody know how to get the SEM package in R to return 
> standardized path coefficients instead of unstandardized 
> ones?  Does this involve changing the covariance matrix, or 
> is there an argument in the SEM itself that can be changed?
> 
> Thank you,
> Tim
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From therneau at mayo.edu  Wed Feb 28 15:59:44 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Wed, 28 Feb 2007 08:59:44 -0600 (CST)
Subject: [R] rpart minimum sample size
Message-ID: <200702281459.l1SExil07037@hsrnfs-101.mayo.edu>

  Look at rpart.control.  Rpart has two "advisory" parameters that control
the tree size at the smallest nodes:
	minsplit (default 20): a node with less than this many subjects will
	not be worth splitting
	
	minbucket (default 7) : don't create any final nodes with <7 
	observations
	
As I said, these are advisory, and reflect that these final splits are usually
not worthwhile.  They lead to a little faster run time, but mostly to a less
complex plotted model.

  I am not nearly as pessimistic as Frank Harrell ("need 20,000 observations").
Rpart often gives a good model -- one that predicts the outcome, and I find
the intermediate steps that it takes informative.  However, there are often many
trees with similar predictive ability, but a very different "look" in terms
of splitpoints and variables.  Saying that any given rpart model is THE best
is perilous.
	Terry T.


From perezperezmm at yahoo.es  Wed Feb 28 16:04:33 2007
From: perezperezmm at yahoo.es (M Perez)
Date: Wed, 28 Feb 2007 16:04:33 +0100 (CET)
Subject: [R] topTable function from LIMMA
In-Reply-To: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDC0@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
Message-ID: <187690.89702.qm@web25507.mail.ukl.yahoo.com>

Dear Roger,

I think there is no a default value for the FDR in the
toptable funtion. You will get a p-values corrected by
the method you choose(fdr, bh). 

Briefly the shorted list of p-values will give you the
FDR. It is up to you choose what is the estimated
number of false positives you want to have.

HTH
Manuel   


--- "Vallejo, Roger" <Roger.Vallejo at ARS.USDA.GOV>
escribi?:

> Dear R-Help,
> 
>  
> 
> I am using the function "topTable" from the LIMMA
> package.  To estimate
> adjusted P-values there are several options
> (adjust="fdr" , adjust="BH")
> as shown below:
> 
>  
> 
> topTable(fit, number = 10, adjust = "BH", fit$Name) 
>  
> 
>  
> 
> I guess any of these options (fdr, BH, etc.) is
> using a default of
> FDR=0.05 which is quite conservative (i.e., very
> likely none of the
> tested genes will be ranked as differentially
> expressed at 5% FDR). I
> would argue that for an exploratory study it would
> be okay to use a 0.05
> < FDR <0.20. So, I was wondering if we can change
> that default of
> FDR=0.0.5 with a less stringent FDR value (0.10,
> 0.20, etc.) in the
> "topTable" function.  How do I change it?
> 
>  
> 
> Thanks a lot for the help on the use of "topTable"
> function from the
> LIMMA package.
> 
>  
> 
> Roger
> 
>  
> 
>  
> 
> Roger L. Vallejo, Ph.D.
> 
> Computational Biologist & Geneticist
> 
> U.S. Department of Agriculture, ARS          
> 
> National Center for Cool & Cold Water Aquaculture
> 
> 11861 Leetown Road
> 
> Kearneysville, WV 25430
> 
> Voice:    (304) 724-8340 Ext. 2141
> 
> Email:   roger.vallejo at ars.usda.gov
> <mailto:roger.vallejo at ars.usda.gov> 
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From ligges at statistik.uni-dortmund.de  Wed Feb 28 16:10:02 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Feb 2007 16:10:02 +0100
Subject: [R] get more than get
In-Reply-To: <20070228143434.M47940@centroin.com.br>
References: <20070228143434.M47940@centroin.com.br>
Message-ID: <45E59B4A.7060407@statistik.uni-dortmund.de>



Alberto Monteiro wrote:
> This must be a stupid question, but is there any "extension" of get?
> 
> For example:
> x <- 10
> get("x") # gives me 10
> get("x^2") # gives me an error


There is no object called "x^2" in the environments within your search 
path. If you want to calculate x^2, then type

x^2
or
get("x")^2


Uwe Ligges


> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From i.visser at uva.nl  Wed Feb 28 16:11:31 2007
From: i.visser at uva.nl (Ingmar Visser)
Date: Wed, 28 Feb 2007 16:11:31 +0100
Subject: [R] get more than get
In-Reply-To: <20070228143434.M47940@centroin.com.br>
References: <20070228143434.M47940@centroin.com.br>
Message-ID: <C2366A1A-6ED0-45CA-AEE7-722FFC75F6D6@uva.nl>

it's unclear what you want ...
but
get("x")^2
does not give an error
hth, Ingmar

On 28 Feb 2007, at 15:35, Alberto Monteiro wrote:

> This must be a stupid question, but is there any "extension" of get?
>
> For example:
> x <- 10
> get("x") # gives me 10
> get("x^2") # gives me an error
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From b.rowlingson at lancaster.ac.uk  Wed Feb 28 16:18:17 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 28 Feb 2007 15:18:17 +0000
Subject: [R] get more than get
In-Reply-To: <20070228143434.M47940@centroin.com.br>
References: <20070228143434.M47940@centroin.com.br>
Message-ID: <45E59D39.7020909@lancaster.ac.uk>

Alberto Monteiro wrote:
> This must be a stupid question, but is there any "extension" of get?
> 
> For example:
> x <- 10
> get("x") # gives me 10
> get("x^2") # gives me an error

  'get' really only gets R objects - you want to evaluate an expression 
- like this:

 > x=2
 > eval(parse(text="x^2"))
[1] 4

Barry


From dacha.atienza at gmail.com  Wed Feb 28 16:21:20 2007
From: dacha.atienza at gmail.com (Dacha Atienza)
Date: Wed, 28 Feb 2007 16:21:20 +0100
Subject: [R] Help on GAM
Message-ID: <358012680702280721g27db0186l4919afb57784df9a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/d68b1de2/attachment.pl 

From walter345 at yahoo.com  Wed Feb 28 16:23:21 2007
From: walter345 at yahoo.com (Walter345)
Date: Wed, 28 Feb 2007 07:23:21 -0800 (PST)
Subject: [R] survival analysis using rpart
In-Reply-To: <9163329.post@talk.nabble.com>
References: <9163329.post@talk.nabble.com>
Message-ID: <9205816.post@talk.nabble.com>



Thanks a lot for the reply, Terry!

Concerning my question #3, I was thinking about the following scenario:

Suppose that you have a data set of survival time data. We use rpart on two
different subsets of the data set, where the columns contain the covariates.
Say, for instance, we build the model first using columns #1 to #10 (call it
model A), and second using columns #11 to #20 (call it model B). For both
subsets, we perform 10-fold cross-validation. 

Let?s consider one case only. Let this case be censored after 30 time units.
Model A puts this case into a leaf with an estimated event rate of 3.56
while model B puts this case into a leaf with an estimated rate of 0.12. I
wish to use this rate to predict the outcome of the case (outcome = event or
outcome = non-event). A rate > 1 is associated with higher chance of an
event, whereas a rate smaller than 1 is associated with a lower chance.
Hence, model A predicts an event for this case, while model B does not.
Thus, model B makes a better prediction than A (for this case). 

Does this make sense or do I misinterpret the event rate? And is it
reasonable to choose a threshold of 1? 

Thanks a lot for all comments!
Walter






Walter345 wrote:
> 
> Hello,
> 
> I use rpart to predict survival time and have a problem in interpreting
> the output of ?estimated rate?. Here is an example of what I do:
> 
>> stagec <-
>> read.table("http://www.stanford.edu/class/stats202/DATA/stagec.data", 
>> col.names=c("pgtime", "pgstat", "age","eet", "g2", "grade", "gleason",
>> "ploidy"))
> 
>> fit <- rpart(Surv(pgtime, pgstat) ~ age + eet + g2 + grade + gleason +
>> ploidy, data=stagec)
> 
> 
> Result:
> 
> 1) root 146 195.411600 1.0000000  
>    2) grade< 2.5 61  45.021520 0.3624701  
>      4) g2< 11.36 33   9.120116 0.1225562 *
>      5) g2>=11.36 28  27.804100 0.7335298  
>       10) gleason< 5.5 20  14.376900 0.5292190 *
>       11) gleason>=5.5 8  11.201470 1.3083680 *
>    3) grade>=2.5 85 125.327400 1.6190620  
>      6) age>=56.5 75 104.154700 1.4287310  
>       12) gleason< 7.5 50  66.701410 1.1431320 *
>       13) gleason>=7.5 25  33.993130 2.0355220  
>         26) g2>=15.29 13  16.555970 1.3494740 *
>         27) g2< 15.29 12  14.220260 2.9210480 *
>      7) age< 56.5 10  15.522810 3.1977430 *
> 
> Let?s look at the terminal node 4:
> 
> #	PGTIME	PGSTAGE	AGE	EET	G2	GRADE	GLEASON	PLOIDY
> 1	8.657084	0	70	1	4.43	1	3	1
> 2	16.70088	0	56	2	5.29	1	3	1
> 3	3.162217	1	62	2	3.57	2	4	1
> 4	10.20123	0	63	2	5.14	2	5	1
> 5	4.479124	0	63	2	5.75	2	5	1
> 6	6.516084	0	66	2	5.92	2	5	1
> 7	4.936345	0	67	2	6.41	2	5	1
> 8	10.79808	0	72	1	6.68	2	NA	1
> 9	9.174537	0	62	1	6.74	2	5	1
> 10	10.87474	0	72	2	6.8	2	5	1
> 11	7.028062	0	52	2	7.15	2	7	1
> 12	11.36481	0	59	2	7.61	2	5	1
> 13	10.17659	0	64	1	7.61	2	NA	1
> 14	6.96783	0	67	2	7.78	2	6	1
> 15	10.61738	0	55	2	7.81	2	5	1
> 16	6.510609	0	70	1	7.88	2	6	1
> 17	10.36276	0	55	2	8.1	2	5	1
> 18	6.694045	0	54	2	8.11	2	4	1
> 19	11.718	0	61	2	8.4	2	5	1
> 20	7.301847	0	69	2	8.46	2	5	1
> 21	6.067077	0	69	2	8.58	2	6	1
> 22	8.353182	0	59	2	8.76	2	6	1
> 23	5.541409	0	59	1	9.01	2	5	1
> 24	5.492128	0	61	2	9.42	2	5	1
> 25	7.208761	0	63	1	9.76	2	5	1
> 26	6.004106	0	52	2	9.9	2	4	1
> 27	5.664613	0	71	1	10.16	2	6	1
> 28	6.130047	0	64	2	10.26	2	4	1
> 29	9.812457	0	64	1	10.51	2	5	1
> 30	6.275154	0	62	2	10.82	2	6	1
> 31	9.253935	0	61	2	11.23	2	5	1
> 32	5.201916	0	54	2	11.35	2	6	1
> 33	6.22861	0	65	2	11.35	2	5	1
> 
> Here we have 33 observations and 1 event. The ?estimated rate? is
> 0.1225562. My questions are:
> 
> (1) Is the ?estimated rate? the estimated hazard rate ratio? 
> (2) How does rpart calculate this rate?
> (3) Suppose I use xpred.rpart(fit, xval=10) to perform 10-fold
> cross-validation using (a) the complete stagec data set and (b) only a
> subset of it, say, using the columns Age, EET, and G2 only. For the i-th
> patient, I am likely to obtain a different estimated rate. How can I
> meaningfully compare both rates? How can say which one is ?better?? 
> 
> Thanks a lot for all comments!
> Walter
> 
> 
> 
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/survival-analysis-using-rpart-tf3294276.html#a9205816
Sent from the R help mailing list archive at Nabble.com.


From dacha.atienza at gmail.com  Wed Feb 28 16:31:09 2007
From: dacha.atienza at gmail.com (Dacha Atienza)
Date: Wed, 28 Feb 2007 16:31:09 +0100
Subject: [R] Fwd: How to do GAM
Message-ID: <358012680702280731n4d29a504v76dca3dd7d0fae83@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/482c4f17/attachment.pl 

From h.wickham at gmail.com  Wed Feb 28 16:38:05 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 28 Feb 2007 09:38:05 -0600
Subject: [R] PROC TABULATE with R
In-Reply-To: <3ef00e160702280609h5f0c0e4o1d60199706b30d4e@mail.gmail.com>
References: <3ef00e160702280609h5f0c0e4o1d60199706b30d4e@mail.gmail.com>
Message-ID: <f8e6ff050702280738x2b357ed6jf19c59fff88a146c@mail.gmail.com>

You might want to have a look at the reshape package,
http://had.co.nz/reshape, which can create multi-dimensional cubes and
perform some OLAP type functionality (although it's likely a lot
slower than a fully-fledged OLAP, it will probably do the job)

Hadley

On 2/28/07, Laurent Valdes <valderama at gmail.com> wrote:
> Hi !
>
> with apply or tapply-like functions, is it possible to create
> multidimensional cubes with R ? Like with SAS and its function PROC TABULATE
> or OLAP ?
> Is there some functions or modules to access OLAP databases with R ?
>
> My idea is to create a package for that, since XMLA and JOLAP specifications
> should able us to do so !
>
> Thanks for your help,
>
> Laurent
>
>
> --
> ?? attendre que l'herbe pousse, le boeuf meurt de faim?
> ?Le boeuf? @<http://www.le-valdo.com>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From liuwensui at gmail.com  Wed Feb 28 16:44:24 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 28 Feb 2007 10:44:24 -0500
Subject: [R] Help on GAM
In-Reply-To: <358012680702280721g27db0186l4919afb57784df9a@mail.gmail.com>
References: <358012680702280721g27db0186l4919afb57784df9a@mail.gmail.com>
Message-ID: <1115a2b00702280744q530c681ey90644fc5db337e88@mail.gmail.com>

which library are you using for gam?

On 2/28/07, Dacha Atienza <dacha.atienza at gmail.com> wrote:
> 1) I have a semiparametric model, like
> *Y~x1+s(x2)+s(x3)*
> When I rum gam package I only obtained the estimates and the statistics of
> the nonparametric part. How can I get the parametric part? Please could you
> give me the complete comand to do it.
>
> 2) How are the negative coefficients identified. I run different examples
> and I never got any negative parameters.
>
> Thank you,
>
> Dacha
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From Emili.Tortosa at eco.uji.es  Wed Feb 28 17:06:18 2007
From: Emili.Tortosa at eco.uji.es (Emili Tortosa-Ausina)
Date: Wed, 28 Feb 2007 17:06:18 +0100
Subject: [R] legend question
Message-ID: <20070228160616.C549A40096@anubis.uji.es>

Hi to all,

I'm sorry for posting this question, I am sure I am missing something 
important but after reading the documentation I cannot find where the 
problem is.

I want to add a legend to a figure. If I use a simple example drawn 
from the R Reference Manual such as, for instance:

x <- seq(-pi, pi, len = 65)
plot(x, sin(x), type="l", col = 2)
legend(x = -3, y = .9, "legend text", pch = 1, xjust = 0.5)

then everything works just fine.

However, if I use other data such as, for instance:

y<-c(1960, 1965, 1970, 1975)
z<-c(1, 2, 3, 4)
plot(y, z, type="l", col = 2)
legend(x = -3, y = .9, "legend text", pch = 1, xjust = 0.5)

then the legend is not shown.

Any hints?

Thanks in advance,

Emili


From b.rowlingson at lancaster.ac.uk  Wed Feb 28 16:47:05 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 28 Feb 2007 15:47:05 +0000
Subject: [R] get more than get
In-Reply-To: <C2366A1A-6ED0-45CA-AEE7-722FFC75F6D6@uva.nl>
References: <20070228143434.M47940@centroin.com.br>
	<C2366A1A-6ED0-45CA-AEE7-722FFC75F6D6@uva.nl>
Message-ID: <45E5A3F9.306@lancaster.ac.uk>

Ingmar Visser wrote:
> it's unclear what you want ...
> but
> get("x")^2
> does not give an error

  Neither does get("x^2") if you actually have an object called "x^2":

  > "x^2"=4.000001
  > get("x^2")
  [1] 4.000001

  but that would be a perverse thing to do.

Barry


From maitra at iastate.edu  Wed Feb 28 17:21:50 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 28 Feb 2007 10:21:50 -0600
Subject: [R] legend question
In-Reply-To: <20070228160616.C549A40096@anubis.uji.es>
References: <20070228160616.C549A40096@anubis.uji.es>
Message-ID: <20070228102150.2be2c9e5@triveni.stat.iastate.edu>

On Wed, 28 Feb 2007 17:06:18 +0100 Emili Tortosa-Ausina <Emili.Tortosa at eco.uji.es> wrote:

> y<-c(1960, 1965, 1970, 1975)
> z<-c(1, 2, 3, 4)
> plot(y, z, type="l", col = 2)
> legend(x = -3, y = .9, "legend text", pch = 1, xjust = 0.5)

your x and y are outside the plotting area. try using a different set, or better still use locator() to specify x, y interactively.

hth,
ranjan


From brown_emu at yahoo.com  Wed Feb 28 17:32:20 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 28 Feb 2007 08:32:20 -0800 (PST)
Subject: [R] legend question
In-Reply-To: <20070228160616.C549A40096@anubis.uji.es>
Message-ID: <992145.19477.qm@web39707.mail.mud.yahoo.com>

Hi Emili,

Even though you are calling your horizontal coordinate y, and vertical
coordinate z, the first and second arguments to legend(), namely x and y,
should be the horizontal and vertical coordinates, respectively; and they are
given in user coordinates (e.g., legend()'s x should be between 1960 and 1975
and legend()'s y should be between 1 and 4).

If you want to use normalized coordinates (i.e. 0 to 1), you can scale as in
this example:

legend(x = par("usr")[1] + diff(par("usr")[1:2])*normalizedCoordX,
       y = par("usr")[3] + diff(par("usr")[3:4])*normalizedCoordY,
       ...)

where normalizedCoordX and Y go from 0 to 1 (see ?par, par("usr") returns
vector of c(xmin,xmax,ymin,ymax) of user coordinates on a plot)

You can alternatively use legend(x = "topleft",...) or "bottomright", and so
on to place your legend.

If you want to add your legend outside of the plot, you should consider
increasing the margins using the 'mar' argument in par(), and also setting
par(xpd=TRUE) (so stuff can show up outside of the plotting region).

Best regards,
ST


> y<-c(1960, 1965, 1970, 1975)
> z<-c(1, 2, 3, 4)
within the data limits of your x and y)





--- Emili Tortosa-Ausina <Emili.Tortosa at eco.uji.es> wrote:

> Hi to all,
> 
> I'm sorry for posting this question, I am sure I am missing something 
> important but after reading the documentation I cannot find where the 
> problem is.
> 
> I want to add a legend to a figure. If I use a simple example drawn 
> from the R Reference Manual such as, for instance:
> 
> x <- seq(-pi, pi, len = 65)
> plot(x, sin(x), type="l", col = 2)
> legend(x = -3, y = .9, "legend text", pch = 1, xjust = 0.5)
> 
> then everything works just fine.
> 
> However, if I use other data such as, for instance:
> 
> y<-c(1960, 1965, 1970, 1975)
> z<-c(1, 2, 3, 4)
> plot(y, z, type="l", col = 2)
> legend(x = -3, y = .9, "legend text", pch = 1, xjust = 0.5)
> 
> then the legend is not shown.
> 
> Any hints?
> 
> Thanks in advance,
> 
> Emili
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Food fight? Enjoy some healthy debate


From cromero at botany.ufl.edu  Wed Feb 28 17:48:59 2007
From: cromero at botany.ufl.edu (Claudia Romero)
Date: Wed, 28 Feb 2007 11:48:59 -0500
Subject: [R] question regression trees
Message-ID: <45E5B27B.2040800@botany.ufl.edu>

Hello,
This is my first time addressing such a big audience so apologies in 
advance in case I fail to formulate this question.

I am working with 13 species of trees, and the data I have are:
  1 continuous (phenolic concentration in xylem and in phloem) and 2 
categorical variables: lineage (3 subclades) and habitat (fire and non 
fire).

I am trying to see how species can be splitted 'objectively' based on 
these variables. I tried to do a regression tree using the rpart 
library, but repeatedly got the following answer, even when I tried to 
run it using ONLY the categorical variables:

 > plot(fit, compress=TRUE)
Error in plot.rpart(fit, compress = TRUE) :
         fit is not a tree, just a root

Can anyone please help me think about this?

Many thanks,
claudia romero


From jholtman at gmail.com  Wed Feb 28 17:50:18 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 28 Feb 2007 11:50:18 -0500
Subject: [R] legend question
In-Reply-To: <20070228160616.C549A40096@anubis.uji.es>
References: <20070228160616.C549A40096@anubis.uji.es>
Message-ID: <644e1f320702280850m2c2d0602j8003c13096de305d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/476e7b26/attachment.pl 

From liuwensui at gmail.com  Wed Feb 28 17:53:45 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 28 Feb 2007 11:53:45 -0500
Subject: [R] question regression trees
In-Reply-To: <45E5B27B.2040800@botany.ufl.edu>
References: <45E5B27B.2040800@botany.ufl.edu>
Message-ID: <1115a2b00702280853t6cda2cbycfdaa38eafed9554@mail.gmail.com>

with seeing more code and output, i guess your tree fails to grow.

On 2/28/07, Claudia Romero <cromero at botany.ufl.edu> wrote:
> Hello,
> This is my first time addressing such a big audience so apologies in
> advance in case I fail to formulate this question.
>
> I am working with 13 species of trees, and the data I have are:
>   1 continuous (phenolic concentration in xylem and in phloem) and 2
> categorical variables: lineage (3 subclades) and habitat (fire and non
> fire).
>
> I am trying to see how species can be splitted 'objectively' based on
> these variables. I tried to do a regression tree using the rpart
> library, but repeatedly got the following answer, even when I tried to
> run it using ONLY the categorical variables:
>
>  > plot(fit, compress=TRUE)
> Error in plot.rpart(fit, compress = TRUE) :
>          fit is not a tree, just a root
>
> Can anyone please help me think about this?
>
> Many thanks,
> claudia romero
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From jmb at mssl.ucl.ac.uk  Wed Feb 28 17:52:05 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Wed, 28 Feb 2007 16:52:05 +0000 (GMT)
Subject: [R] legend question
Message-ID: <200702281652.l1SGq5tv003033@msslhb.mssl.ucl.ac.uk>

Hi folks,

Do you mind if I ask a related question that I have been having trouble with - 
how do you put the legend outside of the plot area (to the bottom of the area - 
below the x-axis title)? Could anybody show me using the example given below:

x <- seq(-pi, pi, len = 65)
plot(x, sin(x), type="l", col = 2)
legend(x = -3, y = .9, "legend text", pch = 1, xjust = 0.5)

Thank you, I've not been able to do this simple bit of programming and it is 
very frustrating not to be able to add a simple key.

Best Wishes,

Jenny

Hi Emili,

Even though you are calling your horizontal coordinate y, and vertical
coordinate z, the first and second arguments to legend(), namely x and y,
should be the horizontal and vertical coordinates, respectively; and they are
given in user coordinates (e.g., legend()'s x should be between 1960 and 1975
and legend()'s y should be between 1 and 4).

If you want to use normalized coordinates (i.e. 0 to 1), you can scale as in
this example:

legend(x = par("usr")[1] + diff(par("usr")[1:2])*normalizedCoordX,
       y = par("usr")[3] + diff(par("usr")[3:4])*normalizedCoordY,
       ...)

where normalizedCoordX and Y go from 0 to 1 (see ?par, par("usr") returns
vector of c(xmin,xmax,ymin,ymax) of user coordinates on a plot)

You can alternatively use legend(x = "topleft",...) or "bottomright", and so
on to place your legend.

If you want to add your legend outside of the plot, you should consider
increasing the margins using the 'mar' argument in par(), and also setting
par(xpd=TRUE) (so stuff can show up outside of the plotting region).

Best regards,
ST


> y<-c(1960, 1965, 1970, 1975)
> z<-c(1, 2, 3, 4)
within the data limits of your x and y)





--- Emili Tortosa-Ausina <Emili.Tortosa at eco.uji.es> wrote:

> Hi to all,
> 
> I'm sorry for posting this question, I am sure I am missing something 
> important but after reading the documentation I cannot find where the 
> problem is.
> 
> I want to add a legend to a figure. If I use a simple example drawn 
> from the R Reference Manual such as, for instance:
> 
> x <- seq(-pi, pi, len = 65)
> plot(x, sin(x), type="l", col = 2)
> legend(x = -3, y = .9, "legend text", pch = 1, xjust = 0.5)
> 
> then everything works just fine.
> 
> However, if I use other data such as, for instance:
> 
> y<-c(1960, 1965, 1970, 1975)
> z<-c(1, 2, 3, 4)
> plot(y, z, type="l", col = 2)
> legend(x = -3, y = .9, "legend text", pch = 1, xjust = 0.5)
> 
> then the legend is not shown.
> 
> Any hints?
> 
> Thanks in advance,
> 
> Emili
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
________________________________________________________________________________
____
Food fight? Enjoy some healthy debate

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

------------- End Forwarded Message -------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student: long range drought prediction 
Climate Extremes Group
Department of Space and Climate Physics
University College London
Holmbury St Mary 
Dorking, Surrey, RH5 6NT
Tel: 01483 204149
Mob: 07916 139187
Web: http://climate.mssl.ucl.ac.uk


From aiminy at iastate.edu  Wed Feb 28 18:17:53 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Wed, 28 Feb 2007 11:17:53 -0600
Subject: [R] use tapply in a list which contains many data.frame
Message-ID: <6.2.3.4.2.20070228104714.03917c10@aiminy.mail.iastate.edu>

I have a list like the following:

 > class(temp)
[1] "list"
 > names(temp)
  [1] "1A24" "1A57" "1A5J" "1A6X" "1AB7" "1AF8" "1AFI" "1AGG" "1AH9" "1AHL"
[11] "1AJ3" "1AJW" "1AK7" "1ALG" "1AOY" "1APF" "1AUZ" "1AZJ" "1AZK"
 > head(temp[[1]])
     pr   model    aa  omega
1 1A24 MODEL_1 1_ALA  84.47
2 1A24 MODEL_1 2_GLN  63.06
3 1A24 MODEL_1 3_TYR 107.72
4 1A24 MODEL_1 4_GLU  54.36
5 1A24 MODEL_1 5_ASP  67.01
6 1A24 MODEL_1 6_GLY 999.00

This Iist have 19 data.frame that has their name such as 1A24,1A57....
The above is the structure for  data.frame 1A24, pr have 1 level, 
model has 20 levels,aa has 189 levels

I want to get the following matrix for each data.frame in this list like this

1A24
              MODEL_1 MODEL_2 .....     MODEL_20   Mean_of_20_models 
Sd_of_20_models
1_ALA    84.47
2_GLN    63.06
3_TYR    107.72
4_GLU    54.36
5_ASP
6_GLY

same thing for other data.frame in this list

1A57
.....

1A5J
.....

Means_of_20_models is average of omega for 1_ALA in 20 models
Sd_of_20_models is standard deviation of omega for 1_ALA between 20 models


Does anyone has some advice on how to do this?

thanks

Aimin


From spluque at gmail.com  Wed Feb 28 18:21:38 2007
From: spluque at gmail.com (Sebastian P. Luque)
Date: Wed, 28 Feb 2007 11:21:38 -0600
Subject: [R] legend question
References: <200702281652.l1SGq5tv003033@msslhb.mssl.ucl.ac.uk>
Message-ID: <87irdmfba5.fsf@patagonia.sebmags.homelinux.org>

On Wed, 28 Feb 2007 16:52:05 +0000 (GMT),
Jenny Barnes <jmb at mssl.ucl.ac.uk> wrote:

> Hi folks, Do you mind if I ask a related question that I have been
> having trouble with - how do you put the legend outside of the plot area
> (to the bottom of the area - below the x-axis title)? Could anybody show
> me using the example given below:

> x <- seq(-pi, pi, len = 65) plot(x, sin(x), type="l", col = 2) legend(x
> = -3, y = .9, "legend text", pch = 1, xjust = 0.5)

> Thank you, I've not been able to do this simple bit of programming and
> it is very frustrating not to be able to add a simple key.

Have a look at ?par and argument 'inset' in ?legend itself.  Here's one
way:


x <- seq(-pi, pi, len=65)
par(mar=c(par("mar")[1] + 2, par("mar")[-1]))
plot(x, sin(x), type="l", col=2)
par(xpd=TRUE)
legend("bottom", "legend text", pch=1, inset=-0.3)


-- 
Seb


From anup_nandialath at yahoo.com  Wed Feb 28 18:28:33 2007
From: anup_nandialath at yahoo.com (Anup Nandialath)
Date: Wed, 28 Feb 2007 09:28:33 -0800 (PST)
Subject: [R] matrix manipulations
Message-ID: <974067.7746.qm@web53308.mail.yahoo.com>

Dear friends,

I have a basic question with R. I'm generating a set
of random variables and then combining them using the
cbind statement. The code for that is given below.

for (i in 1:100)
  {
    y <- rpois(i,lambda=10)
    X0 <- seq(1,1,length=i)
    X1 <- rnorm(i,mean=5,sd=10)
    X2 <- rnorm(i,mean=17,sd=12)
    X3 <- rnorm(i,mean=3, sd=24)
    ind <- rep(1:5,20)
  }
  
data100 <- cbind(y,X0,X1,X2,X3,ind)

but when i look at the data100 table, the y values now
take the observation count. (ie) the data under Y is
not the poisson random generates but the observation
number. Hence the last vector (ind) does not have a
header. Is there any way i can drop the number of
observation counts being added into the matrix.

Thanks in advance for your help.

Sincerely

Anup



"However bad life may seem, there is always something you can do and succeed at. While there is life, there is hope." Stephen Hawking

Anup Menon Nandialath
*********************************************************
http://www.soundclick.com/bands/7/tailgunner_music.htm  *
*********************************************************


 
____________________________________________________________________________________
Looking for earth-friendly autos? 
Browse Top Cars by "Green Rating" at Yahoo! Autos' Green Center.


From p.dalgaard at biostat.ku.dk  Wed Feb 28 18:57:34 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 28 Feb 2007 18:57:34 +0100
Subject: [R] matrix manipulations
In-Reply-To: <974067.7746.qm@web53308.mail.yahoo.com>
References: <974067.7746.qm@web53308.mail.yahoo.com>
Message-ID: <45E5C28E.4040501@biostat.ku.dk>

Anup Nandialath wrote:
> Dear friends,
>
> I have a basic question with R. I'm generating a set
> of random variables and then combining them using the
> cbind statement. The code for that is given below.
>
> for (i in 1:100)
>   {
>     y <- rpois(i,lambda=10)
>     X0 <- seq(1,1,length=i)
>     X1 <- rnorm(i,mean=5,sd=10)
>     X2 <- rnorm(i,mean=17,sd=12)
>     X3 <- rnorm(i,mean=3, sd=24)
>     ind <- rep(1:5,20)
>   }
>   
> data100 <- cbind(y,X0,X1,X2,X3,ind)
>
> but when i look at the data100 table, the y values now
> take the observation count. (ie) the data under Y is
> not the poisson random generates but the observation
> number. Hence the last vector (ind) does not have a
> header. Is there any way i can drop the number of
> observation counts being added into the matrix.
>
>   
That is not what is going on. Sounds like you have your column labels 
misaligned with the column contents.

Take a look at

m <- matrix(rnorm(4), 2, 2)
m
colnames(m) <- c("a", "b")
m
dim(m)

(what was that for loop supposed to be good for, by the way?)


From albmont at centroin.com.br  Wed Feb 28 19:00:29 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 28 Feb 2007 16:00:29 -0200
Subject: [R] matrix manipulations
In-Reply-To: <974067.7746.qm@web53308.mail.yahoo.com>
References: <974067.7746.qm@web53308.mail.yahoo.com>
Message-ID: <20070228175621.M84666@centroin.com.br>


Anup Menon Nandialath wrote:
> 
> I have a basic question with R. I'm generating a set
> of random variables and then combining them using the
> cbind statement. The code for that is given below.
> 
> for (i in 1:100)
>   {
>     y <- rpois(i,lambda=10)
>     X0 <- seq(1,1,length=i)
>     X1 <- rnorm(i,mean=5,sd=10)
>     X2 <- rnorm(i,mean=17,sd=12)
>     X3 <- rnorm(i,mean=3, sd=24)
>     ind <- rep(1:5,20)
>   }
> 
> data100 <- cbind(y,X0,X1,X2,X3,ind)
> 
First, why the loop? For i in 1:99, this code is a waste
of computer time. The code should be:

  i <- 100
  y <- rpois(i,lambda=10)
  X0 <- seq(1,1,length=i)
  X1 <- rnorm(i,mean=5,sd=10)
  X2 <- rnorm(i,mean=17,sd=12)
  X3 <- rnorm(i,mean=3, sd=24)
  ind <- rep(1:5,20)
  data100 <- cbind(y,X0,X1,X2,X3,ind)

> but when i look at the data100 table, the y values now
> take the observation count. 

The y values should be the same as y. y is a (random)
array of integers.

Alberto Monteiro


From lalithaviswanath at yahoo.com  Wed Feb 28 19:03:46 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Wed, 28 Feb 2007 10:03:46 -0800 (PST)
Subject: [R] Packages in R for least median squares regression and computing
	outliers (thompson tau technique etc.)
Message-ID: <916363.92662.qm@web43135.mail.sp1.yahoo.com>

Hi
I am looking for suitable packages in R that do
regression analyses using least median squares method
(or better). Additionally, I am also looking for
packages that implement algorithms/methods for
detecting outliers that can be discarded before doing
the regression analyses.

Although some websites refer to "lms" method under
package "lps" in R, I am unable to find such a package
on CRAN.

I would greatly appreciate any pointers to suitable
functions/packages for doing the above analyses.

Thanks
Lalitha


 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From rsb at wsu.edu  Wed Feb 28 19:01:58 2007
From: rsb at wsu.edu (Bricklemyer, Ross S)
Date: Wed, 28 Feb 2007 10:01:58 -0800
Subject: [R] configure error on Mandriva 2007
Message-ID: <2FC987BC0B90B24786CAF43DD3F5719C86B06F@CRU105.cahe.ad.wsu.edu>


All,

I am having difficulty installing R-2.4.1 on Mandriva Linux 2007 Discovery.  Thanks to the help of Doug Bates I got further along the path (Thanks Doug!!).  I now cannot figure out which line of code needs modified in order for configure to complete.  Please see the attached output script to see where configure is stopping.

Best,
Ross Bricklemyer
Dept. of Crop and Soil Sciences
Washington State university
rsb at wsu.edu

From andy_liaw at merck.com  Wed Feb 28 19:13:55 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 28 Feb 2007 13:13:55 -0500
Subject: [R] Package RGtk2, rattle, libatk-1.0.0.dll Errors
In-Reply-To: <b4485c4c0702272313l727904b3r1ae02f0a01f0cb5b@mail.gmail.com>
References: <b4485c4c0702272313l727904b3r1ae02f0a01f0cb5b@mail.gmail.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03C94FA5@usctmx1106.merck.com>

The way I have that problem resolved is by installing the rggobi package
using the command shown on http://www.ggobi.org/rggobi/, which is

  source("http://www.ggobi.org/download/install.r")

That will install all the things that Ggobi needs.  Since rattle depends
on rggobi, it's probably a good idea to do this anyway.  After that,
rattle will start just fine.  I have not actually use rattle, though.

Andy 

From: j.joshua thomas
> 
> Dear Group,
> 
> I have followed the instructions  from the link 
> http://datamining.togaware.com/survivor/Installing_GTK.html
> However i couldn't fix the libatk01.0.0.dll application error
> 
> Here, i did uninstall R-Gui-2.4.0 then did the fresh 
> installation and still facing the same problem I am using Windows- XP
> 
> *Please find the following*
> 
> 
> R version 2.4.0 (2006-10-03)
> Copyright (C) 2006 The R Foundation for Statistical Computing 
> ISBN 3-900051-07-0
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
>   Natural language support but running in an English locale
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and 'citation()' 
> on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or 
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
> 
> [Previously saved workspace restored]
> 
> > install.packages("RGtk2")
> --- Please select a CRAN mirror for use in this session --- 
> trying URL '
> http://cran.au.r-project.org/bin/windows/contrib/2.4/RGtk2_2.8.7.zip'
> Content type 'application/zip' length 13050736 bytes opened 
> URL downloaded 12744Kb
> 
> package 'RGtk2' successfully unpacked and MD5 sums checked
> 
> The downloaded packages are in
>         C:\Documents and Settings\jjoshua\Local 
> Settings\Temp\RtmpLetwrb\downloaded_packages
> updating HTML package descriptions
> > install.packages("rattle")
> trying URL '
> http://cran.au.r-project.org/bin/windows/contrib/2.4/rattle_2.2.0.zip'
> Content type 'application/zip' length 340875 bytes opened URL 
> downloaded 332Kb
> 
> package 'rattle' successfully unpacked and MD5 sums checked
> 
> The downloaded packages are in
>         C:\Documents and Settings\jjoshua\Local 
> Settings\Temp\RtmpLetwrb\downloaded_packages
> updating HTML package descriptions
> > library(RGtk2)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> 'C:/PROGRA~1/R/R-24~1.0/library/RGtk2/libs/RGtk2.dll':
>   LoadLibrary failure:  The specified module could not be found.
> 
> In addition: Warning message:
> package 'RGtk2' was built under R version 2.4.1
> Error: package/namespace load failed for 'RGtk2'
> >
> 
> 
> --
> Lecturer J. Joshua Thomas
> KDU College Penang Campus
> Research Student,
> University Sains Malaysia
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From klaster at karlin.mff.cuni.cz  Wed Feb 28 19:16:16 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Wed, 28 Feb 2007 19:16:16 +0100
Subject: [R] matrix manipulations
In-Reply-To: <974067.7746.qm@web53308.mail.yahoo.com>
References: <974067.7746.qm@web53308.mail.yahoo.com>
Message-ID: <45E5C6F0.7040909@karlin.mff.cuni.cz>

Don't know what's wrong - it works:

 > data100
         y X0           X1           X2          X3 ind
   [1,] 11  1   2.79581511 -23.33335477 -33.6123061   1
   [2,]  8  1  21.43289242  21.52826214   3.8415209   2
   [3,]  6  1   6.18688631  21.51057247 -50.5547410   3
   [4,] 12  1  -5.95172686  13.74167916 -16.1798745   4
  <snip>

 > data100[y]
   [1]  9 10  8 18 10 10  9  7 11 11  7 10  9 14  7 18  9  7 10 10 18 10 
  9 11  9 10 11 11 11 10  8  9  8  7 18  9 11 15  7
  [40] 10  7  9  7 18 11 11 18 11 11 18  7 10 11  7 11 10 18  7  9  9  9 
  7  7  7 14 18 18 14 11 12 11  8 15  7  7  9 18 14
  [79] 10  7  7 12  7  7  9  8  7  8 11  7 15 18 18  7 15  7  7 11 11 10

These can pretty well be Poisson(10) variates... Remind that R is 
case-sensitive in names, y and Y is not the same.

However, I really hope you are not using the code given below since it 
is, well... very original... to generate something 100 times and to keep 
just the last value at the end.

Petr

Anup Nandialath napsal(a):
> Dear friends,
> 
> I have a basic question with R. I'm generating a set
> of random variables and then combining them using the
> cbind statement. The code for that is given below.
> 
> for (i in 1:100)
>   {
>     y <- rpois(i,lambda=10)
>     X0 <- seq(1,1,length=i)
>     X1 <- rnorm(i,mean=5,sd=10)
>     X2 <- rnorm(i,mean=17,sd=12)
>     X3 <- rnorm(i,mean=3, sd=24)
>     ind <- rep(1:5,20)
>   }
>   
> data100 <- cbind(y,X0,X1,X2,X3,ind)
> 
> but when i look at the data100 table, the y values now
> take the observation count. (ie) the data under Y is
> not the poisson random generates but the observation
> number. Hence the last vector (ind) does not have a
> header. Is there any way i can drop the number of
> observation counts being added into the matrix.
> 
> Thanks in advance for your help.
> 
> Sincerely
> 
> Anup

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From gunter.berton at gene.com  Wed Feb 28 19:25:47 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 28 Feb 2007 10:25:47 -0800
Subject: [R] Packages in R for least median squares regression and
	computingoutliers (thompson tau technique etc.)
In-Reply-To: <916363.92662.qm@web43135.mail.sp1.yahoo.com>
Message-ID: <003c01c75b65$de0ae930$4d908980@gne.windows.gene.com>

Packages MASS and robustbase both have this functionality. There may also be
others.

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of lalitha viswanath
Sent: Wednesday, February 28, 2007 10:04 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Packages in R for least median squares regression and
computingoutliers (thompson tau technique etc.)

Hi
I am looking for suitable packages in R that do
regression analyses using least median squares method
(or better). Additionally, I am also looking for
packages that implement algorithms/methods for
detecting outliers that can be discarded before doing
the regression analyses.

Although some websites refer to "lms" method under
package "lps" in R, I am unable to find such a package
on CRAN.

I would greatly appreciate any pointers to suitable
functions/packages for doing the above analyses.

Thanks
Lalitha


 
____________________________________________________________________________
________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From aiminy at iastate.edu  Wed Feb 28 19:38:08 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Wed, 28 Feb 2007 12:38:08 -0600
Subject: [R] use tapply in a list which contains many data.frame
Message-ID: <6.2.3.4.2.20070228123403.03e17720@aiminy.mail.iastate.edu>

In order to make this question be clear, Here is my code

 >source("http://omega.psi.iastate.edu/data/nmr_19.R")
 > class(temp)
[1] "list"
 > names(temp)
  [1] "1A24" "1A57" "1A5J" "1A6X" "1AB7" "1AF8" "1AFI" "1AGG" "1AH9" "1AHL"
[11] "1AJ3" "1AJW" "1AK7" "1ALG" "1AOY" "1APF" "1AUZ" "1AZJ" "1AZK"
 > head(temp[[1]])
     pr   model    aa  omega
1 1A24 MODEL_1 1_ALA  84.47
2 1A24 MODEL_1 2_GLN  63.06
3 1A24 MODEL_1 3_TYR 107.72
4 1A24 MODEL_1 4_GLU  54.36
5 1A24 MODEL_1 5_ASP  67.01
6 1A24 MODEL_1 6_GLY 999.00

This Iist have 19 data.frame that has their name such as 1A24,1A57....
The above is the structure for  data.frame 1A24, pr have 1 level, 
model has 20 levels,aa has 189 levels

I want to get the following matrix for each data.frame in this list like this

1A24
              MODEL_1 MODEL_2 .....     MODEL_20   Mean_of_20_models 
Sd_of_20_models
1_ALA    84.47
2_GLN    63.06
3_TYR    107.72
4_GLU    54.36
5_ASP
6_GLY

same thing for other data.frame in this list

1A57
.....

1A5J
.....

Means_of_20_models is average of omega for 1_ALA in 20 models
Sd_of_20_models is standard deviation of omega for 1_ALA between 20 models


Does anyone has some advice on how to do this?

thanks

Aimin


From england at cs.umn.edu  Wed Feb 28 19:48:57 2007
From: england at cs.umn.edu (Darin A. England)
Date: Wed, 28 Feb 2007 12:48:57 -0600
Subject: [R] question regression trees
In-Reply-To: <1115a2b00702280853t6cda2cbycfdaa38eafed9554@mail.gmail.com>
References: <45E5B27B.2040800@botany.ufl.edu>
	<1115a2b00702280853t6cda2cbycfdaa38eafed9554@mail.gmail.com>
Message-ID: <20070228184857.GA16665@cs.umn.edu>

Claudia,
Can you send us the actual call you are making to rpart()?
The call to plot() doesn't really help.
Darin

On Wed, Feb 28, 2007 at 11:53:45AM -0500, Wensui Liu wrote:
> with seeing more code and output, i guess your tree fails to grow.
> 
> On 2/28/07, Claudia Romero <cromero at botany.ufl.edu> wrote:
> > Hello,
> > This is my first time addressing such a big audience so apologies in
> > advance in case I fail to formulate this question.
> >
> > I am working with 13 species of trees, and the data I have are:
> >   1 continuous (phenolic concentration in xylem and in phloem) and 2
> > categorical variables: lineage (3 subclades) and habitat (fire and non
> > fire).
> >
> > I am trying to see how species can be splitted 'objectively' based on
> > these variables. I tried to do a regression tree using the rpart
> > library, but repeatedly got the following answer, even when I tried to
> > run it using ONLY the categorical variables:
> >
> >  > plot(fit, compress=TRUE)
> > Error in plot.rpart(fit, compress = TRUE) :
> >          fit is not a tree, just a root
> >
> > Can anyone please help me think about this?
> >
> > Many thanks,
> > claudia romero
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> -- 
> WenSui Liu
> A lousy statistician who happens to know a little programming
> (http://spaces.msn.com/statcompute/blog)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Barbara.Diane-Spillmann at agrar.uni-giessen.de  Wed Feb 28 15:34:04 2007
From: Barbara.Diane-Spillmann at agrar.uni-giessen.de (Barbara Diane-Spillmann)
Date: Wed, 28 Feb 2007 15:34:04 +0100
Subject: [R] map population density
Message-ID: <45E592DC.8050809@agrar.uni-giessen.de>

dear all,

i?m trying to plot a map of population density in german communities by 
using shape files.
to plot the polygons is not the problem but as soon as I want to add the 
population data to the map I get the following message:

Fehler in if (attr(theMap$Shapes[[ii]], "nParts") == 1) { :
Argument hat L?nge 0

My population data are either in form of a csv file or a column of the 
dbf file. either possibility gives me the message above.
what does R want to tell me?

thank you for your help

barbara


From ggrothendieck at gmail.com  Wed Feb 28 20:24:20 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Feb 2007 14:24:20 -0500
Subject: [R] Packages in R for least median squares regression and
	computing outliers (thompson tau technique etc.)
In-Reply-To: <916363.92662.qm@web43135.mail.sp1.yahoo.com>
References: <916363.92662.qm@web43135.mail.sp1.yahoo.com>
Message-ID: <971536df0702281124k1921afcdw7cbeea658c4bea04@mail.gmail.com>

Try rq in quantreg using the default value for tau.

On 2/28/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
> Hi
> I am looking for suitable packages in R that do
> regression analyses using least median squares method
> (or better). Additionally, I am also looking for
> packages that implement algorithms/methods for
> detecting outliers that can be discarded before doing
> the regression analyses.
>
> Although some websites refer to "lms" method under
> package "lps" in R, I am unable to find such a package
> on CRAN.
>
> I would greatly appreciate any pointers to suitable
> functions/packages for doing the above analyses.
>
> Thanks
> Lalitha
>
>
>
> ____________________________________________________________________________________
> TV dinner still cooling?
> Check out "Tonight's Picks" on Yahoo! TV.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rkoenker at uiuc.edu  Wed Feb 28 20:37:29 2007
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 28 Feb 2007 13:37:29 -0600
Subject: [R] Packages in R for least median squares regression and
	computing outliers (thompson tau technique etc.)
In-Reply-To: <971536df0702281124k1921afcdw7cbeea658c4bea04@mail.gmail.com>
References: <916363.92662.qm@web43135.mail.sp1.yahoo.com>
	<971536df0702281124k1921afcdw7cbeea658c4bea04@mail.gmail.com>
Message-ID: <12A97944-D63F-478E-9ECC-2B0E0274E82D@uiuc.edu>

It's not often one gets needs to correct Gabor, but no, ....

least median of squares  is not the same as least absolute error  
regression.

Take a look at the package robust if you want the lms.

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Feb 28, 2007, at 1:24 PM, Gabor Grothendieck wrote:

> Try rq in quantreg using the default value for tau.
>
> On 2/28/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
>> Hi
>> I am looking for suitable packages in R that do
>> regression analyses using least median squares method
>> (or better). Additionally, I am also looking for
>> packages that implement algorithms/methods for
>> detecting outliers that can be discarded before doing
>> the regression analyses.
>>
>> Although some websites refer to "lms" method under
>> package "lps" in R, I am unable to find such a package
>> on CRAN.
>>
>> I would greatly appreciate any pointers to suitable
>> functions/packages for doing the above analyses.
>>
>> Thanks
>> Lalitha
>>
>>
>>
>> _____________________________________________________________________ 
>> _______________
>> TV dinner still cooling?
>> Check out "Tonight's Picks" on Yahoo! TV.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Roger.Bivand at nhh.no  Wed Feb 28 22:01:24 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 28 Feb 2007 22:01:24 +0100 (CET)
Subject: [R] map population density
In-Reply-To: <45E592DC.8050809@agrar.uni-giessen.de>
Message-ID: <Pine.LNX.4.44.0702282141380.15830-100000@reclus.nhh.no>

On Wed, 28 Feb 2007, Barbara Diane-Spillmann wrote:

> dear all,
> 
> i?m trying to plot a map of population density in german communities by 
> using shape files.
> to plot the polygons is not the problem but as soon as I want to add the 
> population data to the map I get the following message:
> 
> Fehler in if (attr(theMap$Shapes[[ii]], "nParts") == 1) { :
> Argument hat L?nge 0
> 
> My population data are either in form of a csv file or a column of the 
> dbf file. either possibility gives me the message above.
> what does R want to tell me?

I assume that you are using read.shape() from the maptools package,
followed by plot.Map(), a deprecated function from the same package. It is
possible that you also have a poorly written command, which you have not
shown. If your second argument was not named explicitly, and is assumed to
be recs, mahem may break loose (and has here - function most likely
looking for out-of-range geometries indexed by population numbers).

The currently recommended approach is to read your shapefile into an 
object specifically designed to keep the geometries and their attribute 
data correctly associated. The function readShapePoly() in maptools will 
do this. The plot method for the object you read will by default only show 
the boundaries, but if you are willing to use lattice graphics, you can 
get there in two lines:

x <- readShapePoly(<my_file.shp>)
spplot(x, "chosen_column_name")

for variable "chosen_column_name" in the shapefile DBF.

You can also use base graphics, but then need to pass a vector of colours 
through the col= argument, usually by look-up in a short vector of colours 
indexed by findInterval() on the variable of interest.

Please consider posting follow-ups to the R-sig-geo list, which is more 
focussed.


> 
> thank you for your help
> 
> barbara
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From albmont at centroin.com.br  Wed Feb 28 22:02:46 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 28 Feb 2007 19:02:46 -0200
Subject: [R] What is a expression good for?
Message-ID: <20070228205814.M50062@centroin.com.br>

I mean, I can generate a expression, for example, with:

z <- expression(x+y)

But then how can I _use_ it? Is it possible to retrieve
information from it, for example, that z is a sum, its
first argument is x (or expression(x)) and its second
argument is y?

Alberto Monteiro


From jholtman at gmail.com  Wed Feb 28 22:10:34 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 28 Feb 2007 16:10:34 -0500
Subject: [R] use tapply in a list which contains many data.frame
In-Reply-To: <6.2.3.4.2.20070228123403.03e17720@aiminy.mail.iastate.edu>
References: <6.2.3.4.2.20070228123403.03e17720@aiminy.mail.iastate.edu>
Message-ID: <644e1f320702281310l3c41052bia23f7914ffdc1146@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/2a1d466f/attachment.pl 

From dfarrar at newrvana.com  Wed Feb 28 22:26:01 2007
From: dfarrar at newrvana.com (David Farrar)
Date: Wed, 28 Feb 2007 13:26:01 -0800 (PST)
Subject: [R] question regression trees
In-Reply-To: <1115a2b00702280853t6cda2cbycfdaa38eafed9554@mail.gmail.com>
Message-ID: <20070228212601.75011.qmail@web802.biz.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070228/20a4350d/attachment.pl 

From gunter.berton at gene.com  Wed Feb 28 22:54:39 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 28 Feb 2007 13:54:39 -0800
Subject: [R] What is a expression good for?
In-Reply-To: <20070228205814.M50062@centroin.com.br>
Message-ID: <006601c75b83$0bb25b30$4d908980@gne.windows.gene.com>

See V&R's S PROGRAMMING, esp. section 3.5; and section 6.1 and subsequent of
the "R Language Definition."

An expression object is the output of parse(), and so is R's representation
of a parsed expression. It is a type of list -- a parse tree for the
expression. This means that you can actually find the sorts of things you
mention by taking it apart as a list:

> ex <- parse(text = "x + y")
> ex
expression(x + y)
> class(ex)
[1] "expression"
> ex[[1]]
x + y
> ex[[c(1,1)]]
`+`
> ex[[c(1,2)]]
x
> ex[[c(1,3)]]
y


There are few if any circumstances when one should do this: this is the job
of the evaluator. There are also special tools available for when you really
might want to do this sort of thing   -- eg. ?formula, ?terms for altering
model specifications. But it is tricky to do right and in full generality --
e.g. ?eval and the above references for some of the issues. 

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alberto Monteiro
Sent: Wednesday, February 28, 2007 1:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] What is a expression good for?

I mean, I can generate a expression, for example, with:

z <- expression(x+y)

But then how can I _use_ it? Is it possible to retrieve
information from it, for example, that z is a sum, its
first argument is x (or expression(x)) and its second
argument is y?

Alberto Monteiro

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From albmont at centroin.com.br  Wed Feb 28 23:40:11 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Wed, 28 Feb 2007 22:40:11 +0000
Subject: [R] What is a expression good for?
In-Reply-To: <006601c75b83$0bb25b30$4d908980@gne.windows.gene.com>
References: <006601c75b83$0bb25b30$4d908980@gne.windows.gene.com>
Message-ID: <200702282240.11751.albmont@centroin.com.br>

Bert Gunter wrote:
>
> An expression object is the output of parse(), and so is R's representation
> of a parsed expression. It is a type of list -- a parse tree for the
> expression. This means that you can actually find the sorts of things you
> mention by taking it apart as a list:
>
>> ex <- parse(text = "x + y")
>> ex
>
> expression(x + y)
>
>> class(ex)
>
> [1] "expression"
>
>> ex[[1]]
>
> x + y
>
Ok so far. But which magic did you use to infer that the next
instructions return something useful?

>> ex[[c(1,1)]]
>
> `+`
>
>> ex[[c(1,2)]]
>
> x
>
>> ex[[c(1,3)]]
>
> y
>
>
> There are few if any circumstances when one should do this: this is the job
> of the evaluator. There are also special tools available for when you
> really might want to do this sort of thing   -- eg. ?formula, ?terms for
> altering model specifications. But it is tricky to do right and in full
> generality -- e.g. ?eval and the above references for some of the issues.
>
I was thinking about doing symbolic algebra - or some tiny part of it.
R does not have a symbolic library, does it?

Alberto Monteiro


From drnevich at uiuc.edu  Wed Feb 28 23:41:18 2007
From: drnevich at uiuc.edu (drnevich at uiuc.edu)
Date: Wed, 28 Feb 2007 16:41:18 -0600 (CST)
Subject: [R] problem with help.start and ?somefunction
Message-ID: <20070228164118.ALN76279@expms1.cites.uiuc.edu>

Hi all,

I am going to be teaching a workshop next week using R and Bioconductor in one of our university's computer labs. They have recently installed R 2.4.1 for me, and I'm checking all my scripts. I just noticed that using the ?somefunction call to access the documentation for that function is not working. On my own PC, the ? call output changed between R 2.3 and 2.4; before it would open some sort of plain text file and now it opens a nice browser with all the functions of that package listed in a side frame and the function documentation listed in the main window. At the computer lab, calling ?somefunction opens the browser with the functions in the side frame, but the main window says "The page cannot be displayed"

If I try help.start(), I get the warnings below, but it does open the html search page. I'm guessing the two are related, and it might have something to do with this message from 2004:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/30360.html

What do I do? I'm not sure what to tell the sys admin about how to fix the problem... I should mention that I used this same lab over a year ago with R 2.2.1, and didn't have the ? help problem - probably because it wasn't the new browser version of help.

Thanks,
Jenny

> help.start()
updating HTML package listing
updating HTML search index
Warning messages:
1: cannot update HTML package index in: make.packages.html(.libPaths()) 
2: cannot open file 'P:\xpapps\R\R-2.4.1/doc/html/search/index.txt', reason 'Permission denied' 
If nothing happens, you should open 'P:\xpapps\R\R-2.4.1\doc\html\index.html' yourself
Warning message:
cannot update HTML search index in: make.search.html(.libPaths()) 

> sessionInfo()
R version 2.4.1 (2006-12-18) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "splines"   "tools"     "stats"     "graphics"  "grDevices" "utils"    
[7] "datasets"  "methods"   "base"     

other attached packages:
        made4 scatterplot3d          ade4 affycoretools       annaffy 
      "1.8.0"      "0.3-24"       "1.4-2"       "1.7.5"       "1.6.1" 
       xtable         gcrma   matchprobes       biomaRt         RCurl 
      "1.4-3"       "2.6.0"       "1.6.0"       "1.8.1"       "0.8-0" 
          XML       GOstats      Category    genefilter      survival 
      "1.4-0"       "2.0.4"       "2.0.3"      "1.12.0"        "2.30" 
         KEGG          RBGL      annotate            GO         graph 
     "1.14.1"      "1.10.0"      "1.12.1"      "1.14.1"      "1.12.0" 
        limma          affy        affyio       Biobase 
      "2.9.8"      "1.12.2"       "1.2.0"      "1.12.2" 
> 
Jenny Drnevich, Ph.D.
Functional Genomics Bioinformatics Specialist
Roy J. Carver Biotechnology Center
University of Illinois, Urbana-Champaign

330 ERML
1201 W. Gregory Dr.
Urbana, IL 61801

ph: 217-244-7355
fax: 217-265-5066 
e-mail: drnevich at uiuc.edu


From toby909 at gmail.com  Wed Feb 28 20:41:12 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Wed, 28 Feb 2007 11:41:12 -0800
Subject: [R] does function deriv take a vector of expressions?
Message-ID: <es4lsn$rg7$1@sea.gmane.org>

Hi All

I was just wondering if the function deriv also takes a vector of expressions If 
someone knows I would appreciate letting me know.

Thanks Toby


