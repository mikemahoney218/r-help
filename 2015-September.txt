From dwinsemius at comcast.net  Tue Sep  1 01:34:10 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 31 Aug 2015 16:34:10 -0700
Subject: [R] modify strip labels with given text using lattice package
In-Reply-To: <CAMk+s2RKfDsNeQM30dvrEkrST6i+t2URWvbAsoirWES+UYbsPw@mail.gmail.com>
References: <CAMk+s2RKfDsNeQM30dvrEkrST6i+t2URWvbAsoirWES+UYbsPw@mail.gmail.com>
Message-ID: <C1183D3A-5DEA-4E52-80F0-E71E84EA76F1@comcast.net>


On Aug 31, 2015, at 1:17 PM, Luigi Marongiu wrote:

> Dear all,
> I am drawing a barchart plot with lattice and the resulting strips are
> taking the value of the variable being compared (in this example
> "assay"). However I would like to write myself the value to place into
> the strips, let's say I want to call the variables as "molecular test"
> and "serological test" the values "a" and "b" respectively within
> "assay". I have tried different approaches taken from the web but
> nothing worked.

Look at ?barchart in the section on arguments to strip and then at ?strip.default for how to use `strip.custom`. If you needed finer control there are functions that return the panel, but this request did not require using it:

barchart(
   test ~ count|assay,
   df,
   groups = res,
   stack = TRUE,
   main = "Comparison of test results",
   xlab = "Count",
   col = c("yellow", "blue"),
   par.settings = list(
       strip.background = list(col="light grey"),
       superpose.polygon=list(col= c("yellow", "blue"))
   ),
   strip = strip.custom( factor.levels=c("molecular test","serological test") ),

   scales = list(
       alternating = FALSE
   ),
   key = list(
       space="top",
       columns=2,
       text=list(c("Negative", "Positive"), col="black"),
       rectangles=list(col=c("yellow", "blue"))
   )
)

> Would you have any tip?
> Best regards
> Luigi
> 
>>>> 
> test <- rep(c("Adenovirus", "Rotavirus", "Norovirus", "Rotarix",
> "Sapovirus"), 2)
> res <- c(0, 1, 0, 1,0, 1,0, 1,0, 1, 0, 1, 0, 1,0, 1,0, 1,0, 1)
> count <- rnorm(20)
> assay <- c(rep("a", 10), rep("b", 10))
> 
> df <- data.frame(test, res, count, assay, stringsAsFactors = FALSE)
> 
> library(lattice)
> barchart(
>    test ~ count|assay,
>    df,
>    groups = res,
>    stack = TRUE,
>    main = "Comparison of test results",
>    xlab = "Count",
>    col = c("yellow", "blue"),
>    par.settings = list(
>        strip.background = list(col="light grey"),
>        superpose.polygon=list(col= c("yellow", "blue"))
>    ),
>    scales = list(
>        alternating = FALSE
>    ),
>    key = list(
>        space="top",
>        columns=2,
>        text=list(c("Negative", "Positive"), col="black"),
>        rectangles=list(col=c("yellow", "blue"))
>    )
> )
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Tue Sep  1 01:48:15 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 31 Aug 2015 16:48:15 -0700
Subject: [R] Conditional replacement and removal of data frame values
In-Reply-To: <CAMk+s2R_yPO3ygNFLUnaM9TVZy7ZB7neO-3o_=4s2jRGACHZqg@mail.gmail.com>
References: <CAMk+s2R_yPO3ygNFLUnaM9TVZy7ZB7neO-3o_=4s2jRGACHZqg@mail.gmail.com>
Message-ID: <A7D83169-C0AC-446D-9417-A44337677896@comcast.net>


On Aug 31, 2015, at 1:49 PM, Luigi Marongiu wrote:

> Dear all,
> I have a data frame and I would like to do the following:
> a) replace value of one variable "a" according to the value of another one "b"
> b) remove all the instances of the variable "b"
> 
> For the sake of argument, let's say I have the following data frame:
> test <- rep(c("Adenovirus", "Rotavirus", "Norovirus", "Rotarix",
> "Sapovirus"), 3)
> res <- c(0, 1, 0, 0, 1,
>         1, 0, 1, 1, 0,
>         0, 1, 0, 1, 0)
> samp <- c(rep(1, 5), rep(2, 5), rep(3, 5))
> df <- data.frame(test, res, samp, stringsAsFactors = FALSE)
> 
> The task I need is to coerce the results of the "Rotavirus" to
> negative (0) if and only if "Rotarix" is positive (1). In this
> example, the results shows that for "samp" 3 "Rotavirus" should be 0:
>    test           res samp
> 2  Rotavirus   1    1
> 4  Rotarix       0    1
> 7  Rotavirus    0    2
> 9  Rotarix       1    2
> 12 Rotavirus   1    3
> 14 Rotarix       1    3
> 
> I can't use the subset function because then I would work on a
> separate object and I don't know how to implement the conditions for
> the replacements.
> Finally, all the "Rotarix" entries should be removed from the data frame.

From context it appears you want to do this testing within groups determined by 'samp', so you might choose to use an lapply-split approach:

lapply( split(df, df$samp), 
       FUN=function(d) if ( d[d$test =="Rotarix", "res"] ) { d$res[d$test=="Rotavirus"] <- 0 ; return( d[!d$test=="Rotarix", ] ) } else { d[!d$test=="Rotarix", ]} )
$`1`
        test res samp
1 Adenovirus   0    1
2  Rotavirus   1    1
3  Norovirus   0    1
5  Sapovirus   1    1

$`2`
         test res samp
6  Adenovirus   1    2
7   Rotavirus   0    2
8   Norovirus   1    2
10  Sapovirus   0    2

$`3`
         test res samp
11 Adenovirus   0    3
12  Rotavirus   0    3
13  Norovirus   0    3
15  Sapovirus   0    3

It's pretty easy to rbind.data.frame those together

> do.call( rbind.data.frame,  lapply( split(df, df$samp), FUN=function(d) if ( d[d$test =="Rotarix", "res"] ) { d$res[d$test=="Rotavirus"] <- 0 ; return( d[!d$test=="Rotarix", ] ) } else { d[!d$test=="Rotarix", ]} ) )
           test res samp
1.1  Adenovirus   0    1
1.2   Rotavirus   1    1
1.3   Norovirus   0    1
1.5   Sapovirus   1    1
2.6  Adenovirus   1    2
2.7   Rotavirus   0    2
2.8   Norovirus   1    2
2.10  Sapovirus   0    2
3.11 Adenovirus   0    3
3.12  Rotavirus   0    3
3.13  Norovirus   0    3
3.15  Sapovirus   0    3



> Thank you.
> Best regards,
> Luigi
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From wdunlap at tibco.com  Tue Sep  1 03:24:33 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 31 Aug 2015 18:24:33 -0700
Subject: [R] Non zero padding dates
In-Reply-To: <CALvVS-HZ3QUXYT+cDqedxJTTtzQTLfSJRoYGbO9DhvWyx4E86w@mail.gmail.com>
References: <CALvVS-HZ3QUXYT+cDqedxJTTtzQTLfSJRoYGbO9DhvWyx4E86w@mail.gmail.com>
Message-ID: <CAF8bMcaHM0ms-qtnQdKg4u_ACdmZmb-7_vgz3kFY=1dQy2LLuA@mail.gmail.com>

> For example: "2015119_06"  ("2015-01-19 06:00")

How would the ninth of November be represented?

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Aug 31, 2015 at 1:11 PM, Dominic Roye <dominic.roye at gmail.com>
wrote:

> Hello,
>
> How can I convert date-time in which month and day have non zero padding?
>
> For example: "2015119_06"  ("2015-01-19 06:00")
>
> Thanks
>
> Dominic
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Tue Sep  1 03:28:22 2015
From: jholtman at gmail.com (jim holtman)
Date: Mon, 31 Aug 2015 21:28:22 -0400
Subject: [R] Non zero padding dates
In-Reply-To: <CALvVS-HZ3QUXYT+cDqedxJTTtzQTLfSJRoYGbO9DhvWyx4E86w@mail.gmail.com>
References: <CALvVS-HZ3QUXYT+cDqedxJTTtzQTLfSJRoYGbO9DhvWyx4E86w@mail.gmail.com>
Message-ID: <CAAxdm-5HWUBdnE=hGhk5BDW6wDgBXwUcfsX5t4Dt4BEhnkOiXg@mail.gmail.com>

you will probably have to take a guess since your example is also
 2015-11-09 06:00; so how do you determine which one.  come up with an
algorithm and the solution is easy.


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Mon, Aug 31, 2015 at 4:11 PM, Dominic Roye <dominic.roye at gmail.com>
wrote:

> Hello,
>
> How can I convert date-time in which month and day have non zero padding?
>
> For example: "2015119_06"  ("2015-01-19 06:00")
>
> Thanks
>
> Dominic
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Sep  1 03:34:06 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 31 Aug 2015 18:34:06 -0700
Subject: [R] Non zero padding dates
In-Reply-To: <CALvVS-HZ3QUXYT+cDqedxJTTtzQTLfSJRoYGbO9DhvWyx4E86w@mail.gmail.com>
References: <CALvVS-HZ3QUXYT+cDqedxJTTtzQTLfSJRoYGbO9DhvWyx4E86w@mail.gmail.com>
Message-ID: <CAGxFJbSpYhhv8VA41oojLpJSjoO1FO2FWpgXSP1v-P-AMD_0Zw@mail.gmail.com>

In your example, how would you know that it is not 2015-11-9 06:00. ?

Cheers,
Bert



On Monday, August 31, 2015, Dominic Roye <dominic.roye at gmail.com> wrote:

> Hello,
>
> How can I convert date-time in which month and day have non zero padding?
>
> For example: "2015119_06"  ("2015-01-19 06:00")
>
> Thanks
>
> Dominic
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge is
certainly not wisdom."
   -- Clifford Stoll

	[[alternative HTML version deleted]]


From matthew.david.pickard at gmail.com  Tue Sep  1 06:14:10 2015
From: matthew.david.pickard at gmail.com (Matt Pickard)
Date: Mon, 31 Aug 2015 22:14:10 -0600
Subject: [R] reshape: melt and cast
Message-ID: <CANbrOWd0mGGatb=9WY8PfWPYJXPCGcL5AG-B=+=vU1+P4pLTHQ@mail.gmail.com>

Hi,

I have data that looks like this:









*> head(ratings)  QCode  PID  RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7 SI8 SI9
SI10 SI111 GUILT 1123 cwormhoudt   2   2   3   1   1   1   3   3   3
2    12  LOVE 1123 cwormhoudt   1   2   3   2   1   1   1   1   1    1
33 GUILT 1136 cwormhoudt   1   2   3   1   1   1   2   3   2    2    14
LOVE 1136 cwormhoudt   1   2   3   1   1   1   1   1   1    1    25 GUILT
1137 cwormhoudt   2   2   2   1   1   1   2   3   1    2    16  LOVE 1137
cwormhoudt   1   3   4   1   1   1   1   1   1    1    4*








*> tail(ratings)      QCode  PID RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7 SI8
SI9 SI10 SI112456    FUN 1555  zspeidel   1   3   3   1   1   1   1   1
1    1    12457    FUN 1556  zspeidel   1   2   2   1   1   1   1   1
1    1    12458    FUN 1558  zspeidel   1   2   3   1   1   1   1   1
1    1    12459 APPEAR 1558  zspeidel   1   3   3   1   1   1   2   1
1    2    12460 APPEAR 1559  zspeidel   1   3   3   1   1   1   2   1
1    2    12461    FUN 1559  zspeidel   1   2   2   1   1   1   1   1
1    1    1*
I am trying to use the melt and cast functions to re-arrange it to look
like this:








*   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123   SI1
1        12 APPEAR 1123   SI2          4        13 APPEAR 1123
SI3          1        24 APPEAR 1123   SI4          3        15 APPEAR
1123   SI5          1        16 APPEAR 1123   SI6          1        3*
So, I melt the data like this:



*mratings = melt(ratings, variable_name="sItem")*
Then cast the data like this:


*> outData = cast(mratings, QCode + PID + sItem ~ RaterName)Aggregation
requires fun.aggregate: length used as default*

But the value columns appear to be displaying counts and not the original
values:













*> head(outData)   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123
SI1          1        12 APPEAR 1123   SI2          1        13 APPEAR
1123   SI3          1        14 APPEAR 1123   SI4          1        15
APPEAR 1123   SI5          1        16 APPEAR 1123   SI6          1
1> which(outData$zpeidel==3)integer(0)*
How to I prevent cast from aggregating the data according to counts?  Am I
doing something wrong?

Thanks in advance.

MP

	[[alternative HTML version deleted]]


From ssefick at gmail.com  Tue Sep  1 07:32:12 2015
From: ssefick at gmail.com (stephen sefick)
Date: Tue, 1 Sep 2015 00:32:12 -0500
Subject: [R] reshape: melt and cast
In-Reply-To: <CANbrOWd0mGGatb=9WY8PfWPYJXPCGcL5AG-B=+=vU1+P4pLTHQ@mail.gmail.com>
References: <CANbrOWd0mGGatb=9WY8PfWPYJXPCGcL5AG-B=+=vU1+P4pLTHQ@mail.gmail.com>
Message-ID: <CADKEMqi1QSuvN7ZZAPHnX5RsRzUJ9-GE-0R1yVALJHH43rBGhA@mail.gmail.com>

This is very hard to read. Please use dput to provide data. I believe the
answer is in the manual. Look at the aggregation function argument.

Please excuse my brevity; this message was sent from my telephone.
On Sep 1, 2015 12:11 AM, "Matt Pickard" <matthew.david.pickard at gmail.com>
wrote:

> Hi,
>
> I have data that looks like this:
>
>
>
>
>
>
>
>
>
> *> head(ratings)  QCode  PID  RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7 SI8 SI9
> SI10 SI111 GUILT 1123 cwormhoudt   2   2   3   1   1   1   3   3   3
> 2    12  LOVE 1123 cwormhoudt   1   2   3   2   1   1   1   1   1    1
> 33 GUILT 1136 cwormhoudt   1   2   3   1   1   1   2   3   2    2    14
> LOVE 1136 cwormhoudt   1   2   3   1   1   1   1   1   1    1    25 GUILT
> 1137 cwormhoudt   2   2   2   1   1   1   2   3   1    2    16  LOVE 1137
> cwormhoudt   1   3   4   1   1   1   1   1   1    1    4*
>
>
>
>
>
>
>
>
> *> tail(ratings)      QCode  PID RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7 SI8
> SI9 SI10 SI112456    FUN 1555  zspeidel   1   3   3   1   1   1   1   1
> 1    1    12457    FUN 1556  zspeidel   1   2   2   1   1   1   1   1
> 1    1    12458    FUN 1558  zspeidel   1   2   3   1   1   1   1   1
> 1    1    12459 APPEAR 1558  zspeidel   1   3   3   1   1   1   2   1
> 1    2    12460 APPEAR 1559  zspeidel   1   3   3   1   1   1   2   1
> 1    2    12461    FUN 1559  zspeidel   1   2   2   1   1   1   1   1
> 1    1    1*
> I am trying to use the melt and cast functions to re-arrange it to look
> like this:
>
>
>
>
>
>
>
>
> *   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123   SI1
> 1        12 APPEAR 1123   SI2          4        13 APPEAR 1123
> SI3          1        24 APPEAR 1123   SI4          3        15 APPEAR
> 1123   SI5          1        16 APPEAR 1123   SI6          1        3*
> So, I melt the data like this:
>
>
>
> *mratings = melt(ratings, variable_name="sItem")*
> Then cast the data like this:
>
>
> *> outData = cast(mratings, QCode + PID + sItem ~ RaterName)Aggregation
> requires fun.aggregate: length used as default*
>
> But the value columns appear to be displaying counts and not the original
> values:
>
>
>
>
>
>
>
>
>
>
>
>
>
> *> head(outData)   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123
> SI1          1        12 APPEAR 1123   SI2          1        13 APPEAR
> 1123   SI3          1        14 APPEAR 1123   SI4          1        15
> APPEAR 1123   SI5          1        16 APPEAR 1123   SI6          1
> 1> which(outData$zpeidel==3)integer(0)*
> How to I prevent cast from aggregating the data according to counts?  Am I
> doing something wrong?
>
> Thanks in advance.
>
> MP
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dulcalma at bigpond.com  Tue Sep  1 07:40:53 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Tue, 1 Sep 2015 15:40:53 +1000
Subject: [R] modify strip labels with given text using lattice package
In-Reply-To: <CAMk+s2RKfDsNeQM30dvrEkrST6i+t2URWvbAsoirWES+UYbsPw@mail.gmail.com>
References: <CAMk+s2RKfDsNeQM30dvrEkrST6i+t2URWvbAsoirWES+UYbsPw@mail.gmail.com>
Message-ID: <000601d0e478$cb30dd10$61929730$@bigpond.com>

Hi Luigi

add strip.custom argument

barchart(test ~ count|assay, df,
      groups = res,
      stack = TRUE,
      main = "Comparison of test results",
      xlab = "Count",
      col = c("yellow", "blue"),
      strip    = strip.custom(factor.levels = c("molecular
test","serological test"),
                              par.strip.text = list(cex = 1) ),
       par.settings = list(
          strip.background = list(col="light grey"),
          superpose.polygon=list(col= c("yellow", "blue"))
      ),
      scales = list(alternating = FALSE),
     key = list(
        space="top",
        columns=2,
        text=list(c("Negative", "Positive"), col="black"),
        rectangles=list(col=c("yellow", "blue"))
    )
 )


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Luigi
Marongiu
Sent: Tuesday, 1 September 2015 06:18
To: r-help
Subject: [R] modify strip labels with given text using lattice package

Dear all,
I am drawing a barchart plot with lattice and the resulting strips are
taking the value of the variable being compared (in this example
"assay"). However I would like to write myself the value to place into
the strips, let's say I want to call the variables as "molecular test"
and "serological test" the values "a" and "b" respectively within
"assay". I have tried different approaches taken from the web but
nothing worked.
Would you have any tip?
Best regards
Luigi

>>>
test <- rep(c("Adenovirus", "Rotavirus", "Norovirus", "Rotarix",
"Sapovirus"), 2)
res <- c(0, 1, 0, 1,0, 1,0, 1,0, 1, 0, 1, 0, 1,0, 1,0, 1,0, 1)
count <- rnorm(20)
assay <- c(rep("a", 10), rep("b", 10))

df <- data.frame(test, res, count, assay, stringsAsFactors = FALSE)

library(lattice)
barchart(
    test ~ count|assay,
    df,
    groups = res,
    stack = TRUE,
    main = "Comparison of test results",
    xlab = "Count",
    col = c("yellow", "blue"),
    par.settings = list(
        strip.background = list(col="light grey"),
        superpose.polygon=list(col= c("yellow", "blue"))
    ),
    scales = list(
        alternating = FALSE
    ),
    key = list(
        space="top",
        columns=2,
        text=list(c("Negative", "Positive"), col="black"),
        rectangles=list(col=c("yellow", "blue"))
    )
)

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From lists at dewey.myzen.co.uk  Tue Sep  1 09:40:02 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 1 Sep 2015 08:40:02 +0100
Subject: [R] Median on second group of CSV file produce Na
In-Reply-To: <CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
References: <1441049748473-4711689.post@n4.nabble.com>
	<CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
Message-ID: <55E55652.4020702@dewey.myzen.co.uk>

Dear Shawin

You probably did not get an answer because
1 - you seem to have posted in HTML which mangled your post into 
unreadability
2 - there seem to be lots of lines which do not do anything germane to 
the problem.

Why not try summary(y) or str(y) before you convert it to a matrix and 
see what it says?

On 31/08/2015 20:46, shawin wrote:
> I have an issue ans i posted it , so i would like to receive a solution
> please
>
> On Mon, Aug 31, 2015 at 8:35 PM, shawin [via R] <
> ml-node+s789695n4711689h58 at n4.nabble.com> wrote:
>
>> I have a data frame  csv file and I'm trying to calculate median for each
>> group separately row by row . When I separate the data frame in two groups
>> and calculate the median for each one, I am getting an NA result for the
>> second group :
>> the data
>>    x1  x2  x3  x4  x5  x6  x7  y1  y2  y3  y4  y5  y6  y7  y8
>> 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045 9.640816474
>> 9.606262272   9.329651027 9.434541611 9.473922432 9.311412966 9.3154885
>> 9.434977488 9.470895414 9.764258059
>> 8.630629966 8.55831075  8.788391003 8.576231135 8.671587906 8.842979993
>> 8.861958856 8.58330436  8.603596508 8.570129609 8.59798922  8.572686772
>> 8.679751791 8.663950953 8.432875347
>> 9.354748885 9.367668838 9.259952558 9.421538213 9.554635162 9.603744578
>> 9.452197983 9.284228877 9.404607878 9.317737979 9.343115301 9.310644266
>> 9.27227486  9.360337823 9.44706281
>> 9.944863964 9.950427516 10.19101759 10.07350804 10.03269879 10.1307908
>>   10.03487287 9.74609383  9.886379007 9.775472567 10.036596   9.544738458
>> 9.699611598 9.911962567
>> 9.625804277
>>
>>
>>                                     Code:
>>
>>         rowN <- nrow(AT1)
>>         MD1<-vector(length=rowN)
>>        MD2<-vector(length=rowN)
>>
>>            MD1[1:rowN]<-NA
>>            MD2[1:rowN]<-NA
>>
>>
>>           x<- AT1[,c(2,3,4,5,6,7,8) ]
>>          write.csv(x,"x.csv",row.names=TRUE)
>>           x<-as.matrix(x)
>>          for(i in 2:rowN) {
>>         MD1[i]=median(x[i,])
>>            }
>>           write.csv(MD1,"MD1.csv",row.names=TRUE)
>>
>>           y<- AT1[,c(9,10,11,12,13,14,15,16)]
>>           write.csv(y,"y.csv",row.names=TRUE)
>>           y<-as.matrix(y)
>>           for(j in 2:rowN) {
>>           MD2[j]=median(y[j,])
>>            }
>>           write.csv(MD2,"MD2.csv",row.names=TRUE)
>>
>>
>> ------------------------------
>> If you reply to this email, your message will be added to the discussion
>> below:
>>
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689.html
>> To start a new topic under R help, email
>> ml-node+s789695n789696h75 at n4.nabble.com
>> To unsubscribe from R, click here
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
>> .
>> NAML
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711690.html
> Sent from the R help mailing list archive at Nabble.com.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From lists at dewey.myzen.co.uk  Tue Sep  1 09:43:45 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 1 Sep 2015 08:43:45 +0100
Subject: [R] Metafor and forest(); not showing 'ilab' and text
In-Reply-To: <CAM1GnGNPE5QOWk_NZMjrG0WGHufKnLcbo17-sN-N7bP68yhkRg@mail.gmail.com>
References: <55DC5018.8040000@dewey.myzen.co.uk>
	<077E31A57DA26E46AB0D493C9966AC730F1AA922CF@UM-MAIL4112.unimaas.nl>
	<CALxyAHQu6Q=grjPfVZL8=9mJNHxY1pe9wwn6tjj38-YJBW7c5A@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F1AA92353@UM-MAIL4112.unimaas.nl>
	<CALxyAHQ=O__2U+p16N=3VaTdd9_+QZBT5NrBPijzX0RQmKVjog@mail.gmail.com>
	<55DCA4E6.40204@dewey.myzen.co.uk>
	<CALxyAHQ-9Q-fAuAX1MPxOw9o8=4DyJzkKDngcrdC1Ory-_pMTg@mail.gmail.com>
	<55E48218.9090008@dewey.myzen.co.uk>
	<CALxyAHTKPoyHpxD-Z+BgGZrF+4GHZsQOt1J_dn2xxfVoqHVhgA@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F1ADBC6F2@UM-MAIL4112.unimaas.nl>
	<CAM1GnGNPE5QOWk_NZMjrG0WGHufKnLcbo17-sN-N7bP68yhkRg@mail.gmail.com>
Message-ID: <55E55731.6050409@dewey.myzen.co.uk>

Dear Navien

If anyone on the list is going to help you you need to

1 - provide a minimal, self-contained and reproducible example of your 
problem
2 - start a new thread

Note that in general list members provide help about R rather than 
statistics for which there are other lists.

On 31/08/2015 18:29, Navien wrote:
> Dear Wolfgang,
>
> Kindly please i have an issue with R code could you please help me.
>
> Best Regards
>
> On Mon, Aug 31, 2015 at 6:24 PM, Viechtbauer Wolfgang (STAT)-2 [via R] <
> ml-node+s789695n4711682h19 at n4.nabble.com> wrote:
>
>> Have you read help(rma.mv)? It describes in detail what "random = ~ 1 |
>> author" does. Also, I think you may find some of these useful:
>>
>>
>> http://www.metafor-project.org/doku.php/analyses#multivariate_multilevel_meta-analysis_models
>>
>> Especially:
>> http://www.metafor-project.org/doku.php/analyses:konstantopoulos2011
>>
>> Using "random = ~ 1 | author" is likely to be insufficient. You also need
>> to add random effects at the observation level.
>>
>> Best,
>> Wolfgang
>>
>> --
>> Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and
>>
>> Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD
>>
>> Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com
>>
>>
>>> -----Original Message-----
>>> From: Marco Colagrossi [mailto:[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711682&i=0>]
>>> Sent: Monday, August 31, 2015 18:37
>>> To: Michael Dewey
>>> Cc: Viechtbauer Wolfgang (STAT); [hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711682&i=1>
>>> Subject: Re: [R] Metafor and forest(); not showing 'ilab' and text
>>>
>>> The solution that you proposed works perfectly, thank you very much.
>>>
>>> I'll wait for Wolfgang answer as I'm having few doubts about the models.
>>>
>>> Thanks
>>>
>>> On 31 August 2015 at 18:34, Michael Dewey <[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711682&i=2>>
>>> wrote:
>>>> Comments in line
>>>>
>>>> On 31/08/2015 16:08, Marco Colagrossi wrote:
>>>>>
>>>>> Thanks for your help,
>>>>>
>>>>> I got the mistake I was making and I managed to find a solution
>>>>> regarding those graphs; I don't want to abuse of your patience but I
>>>>> have three further questions:
>>>>>
>>>>> 1. Always regarding the forest plots, it is possible to make a
>>>>> cross-subset? I try to explain my self better; I have one dummy
>>>>> variable called pub and another variable called SIMiv that can take
>>>>> the values of "share", "loan", "number" and "duration". How can I
>>>>> subset my sample so that the forest shows only (for example) studies
>>>>> when the dummy takes the value of 1 and the SIMiv variable takes the
>>>>> values of "share" and "loan"?
>>>>> Something like this:
>>>>> forest(pc, var, ci95m, ci95p, slab = authoryear2, psize=1,
>>>>> subset=(pub==1, SIMiv=("share", "loan", "duration"))
>>>>>
>>>>
>>>> Do you not want something like
>>>> (pub == 1) & (SIMIv %in% c("share", "loan", "duration"))
>>>>
>>>>
>>>>> 2. I have few doubts regarding the multilevel modeling;
>>>>>       rma.mv(pc, var, random = ~ 1 | author, data=codebook)
>>>>>      if I'm correct this should be a multilevel model nested at
>>> "author"
>>>>> level; what I cannot understand If it is a varying intercept
>>>>> (Y=A+BjX), a varying slope (Y=Aj+BX) or a varying intercept&slope
>>>>> model (Y=Aj+BjX). Are there the formulas for it somewhere? So far I
>>>>> only found the formulas for the estimators included in the metafor
>>>>> package.
>>>>>
>>>>
>>>> I think it a random intercept but Wolfgang may correct me there.
>>>>
>>>>
>>>>> 3. metareg1 <- rma.mv(pc, var, random = ~ 1 | author, mods = ~ pub +
>>>>> SIMiv, data=codebook)
>>>>> Again, if I'm correct this should be a multilevel meta regression
>>>>> (correct me if I'm wrong); I have the same doubts as before.
>>>>>
>>>>> Thank you again
>>>>>
>>>>> Marco
>>>>>
>>>>> On 25 August 2015 at 19:24, Michael Dewey <[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711682&i=3>>
>>> wrote:
>>>>>>
>>>>>> Dear Marco
>>>>>>
>>>>>> When you change xlim it increases the width of the forest plot in
>> the
>>>>>> sense
>>>>>> you describe. It does not push your text out of the way to make
>> space
>>> for
>>>>>> it
>>>>>> but instead overprints it. You may like to use alim to truncate your
>>>>>> confidence interval whiskers to fit within the space you see or make
>>> your
>>>>>> labels shorter.
>>>>>>
>>>>>>
>>>>>> On 25/08/2015 17:25, Marco Colagrossi wrote:
>>>>>>>
>>>>>>>
>>>>>>> I think I've not explained myself well. When I say "the width of
>> the
>>>>>>> forest plot" I mean the region above the observed outcome, the
>>>>>>> "actual" forest plot, not the plot as a whole. Even if I change
>>> values
>>>>>>> for Xlim, cex or ilab.xpos the width of that particular region
>>> within
>>>>>>> the plot doesn't change.
>>>>>>>
>>>>>>> Best,
>>>>>>>
>>>>>>> Marco
>>>>>>>
>>>>>>> On 25 August 2015 at 18:11, Viechtbauer Wolfgang (STAT)
>>>>>>> <[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711682&i=4>> wrote:
>>>>>>>>
>>>>>>>>
>>>>>>>> The 'xlim' argument does not change the actual width of the
>>> plotting
>>>>>>>> device. For that, you need to use the 'width' argument with
>>> whatever
>>>>>>>> device
>>>>>>>> you are actually using. You can then use the 'xlim' argument to
>>> create
>>>>>>>> appropriate spacing to the left/right of the part of the plot that
>>>>>>>> shows the
>>>>>>>> estimates and their CIs. Within that space, you can then add
>>> additional
>>>>>>>> columns with the 'ilab' argument. It's up to you to find an
>>> appropriate
>>>>>>>> combination of plotting device width, character/symbol expansion
>>> factor
>>>>>>>> ('cex' argument), 'xlim' values, and 'ilab.xpos' values to create
>> a
>>>>>>>> nice
>>>>>>>> looking plot that has no overlapping text and no excessive white
>>> space.
>>>>>>>> An
>>>>>>>> example is this:
>>>>>>>>
>>>>>>>>
>>>>>>>> http://www.metafor-
>>> project.org/doku.php/plots:forest_plot_with_subgroups
>>>>>>>>
>>>>>>>> Note that it took me dozens of iterations to create that plot. You
>>> just
>>>>>>>> have to start experimenting.
>>>>>>>>
>>>>>>>> Best,
>>>>>>>> Wolfgang
>>>>>>>>
>>>>>>>>> -----Original Message-----
>>>>>>>>> From: Marco Colagrossi [mailto:[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711682&i=5>]
>>>>>>>>> Sent: Tuesday, August 25, 2015 17:59
>>>>>>>>> To: Viechtbauer Wolfgang (STAT)
>>>>>>>>> Cc: [hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711682&i=6>; Michael Dewey
>>>>>>>>> Subject: Re: [R] Metafor and forest(); not showing 'ilab' and
>> text
>>>>>>>>>
>>>>>>>>> Thanks again for your help. I'm sorry to bother you but I don't
>>> get
>>>>>>>>> how to widen the forest plot; if I try to change the values of
>>> xlim or
>>>>>>>>> the ilab.xpos values the width of the forest plot region does not
>>>>>>>>> change, but only moves on the graphs. What I'm I missing?
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> forest(pc, var, ci95m, ci95p, slab = authoryear, psize=1,
>>>>>>>>> subset=(pub==1),
>>>>>>>>>           xlim = c(-16, 6),
>>>>>>>>>           ilab = data.frame(SIMdv, SIMiv),
>>>>>>>>>           ilab.xpos = c(-7.5, -5.5), cex = 0.75)
>>>>>>>>> op <- par(cex=.75, font=2)
>>>>>>>>>          text(c(-7.5, -5.5), 54, c("DV", "IV"))
>>>>>>>>>          text(-16,                54, "Author(s) and Year",
>>> pos=4)
>>>>>>>>>          text(6,                  54, "Outcome [95% CI]", pos=2)
>>>>>>>>> par(op)
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> par("usr")[1:2]
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> [1] -16   6
>> ______________________________________________
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711682&i=7>
>> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> ------------------------------
>> If you reply to this email, your message will be added to the discussion
>> below:
>>
>> http://r.789695.n4.nabble.com/Metafor-and-forest-not-showing-ilab-and-text-tp4711432p4711682.html
>> To start a new topic under R help, email
>> ml-node+s789695n789696h53 at n4.nabble.com
>> To unsubscribe from R, click here
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=bmF2aW5nMnVrQGdtYWlsLmNvbXw3ODk2OTV8LTczNzQxMTY0Ng==>
>> .
>> NAML
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Metafor-and-forest-not-showing-ilab-and-text-tp4711432p4711683.html
> Sent from the R help mailing list archive at Nabble.com.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From petr.pikal at precheza.cz  Tue Sep  1 10:03:29 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 1 Sep 2015 08:03:29 +0000
Subject: [R] Median on second group of CSV file produce Na
In-Reply-To: <CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
References: <1441049748473-4711689.post@n4.nabble.com>
	<CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DA33@SRVEXCHMBX.precheza.cz>

Hi

Your code is highly complicated and difficult to understand without example data. Perhaps you want row medians for two groups x and y which are located in different columns of original data.

MD1 <- apply(AT1[,c(2,3,4,5,6,7,8) ], 1, median)

shall give you desired result without need of cycle and almost all other stuff.

If there is NA in result, some of the value can be NA so after median you have to put na.rm=TRUE.

consult
?apply
?summary
?str

for explanation

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of shawin
> Sent: Monday, August 31, 2015 9:46 PM
> To: r-help at r-project.org
> Subject: Re: [R] Median on second group of CSV file produce Na
>
> I have an issue ans i posted it , so i would like to receive a solution
> please
>
> On Mon, Aug 31, 2015 at 8:35 PM, shawin [via R] <
> ml-node+s789695n4711689h58 at n4.nabble.com> wrote:
>
> > I have a data frame  csv file and I'm trying to calculate median for
> > each group separately row by row . When I separate the data frame in
> > two groups and calculate the median for each one, I am getting an NA
> > result for the second group :
> > the data
> >   x1  x2  x3  x4  x5  x6  x7  y1  y2  y3  y4  y5  y6  y7  y8
> > 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> 9.640816474
> > 9.606262272   9.329651027 9.434541611 9.473922432 9.311412966
> 9.3154885
> > 9.434977488 9.470895414 9.764258059
> > 8.630629966 8.55831075  8.788391003 8.576231135 8.671587906
> > 8.842979993
> > 8.861958856 8.58330436  8.603596508 8.570129609 8.59798922
> > 8.572686772
> > 8.679751791 8.663950953 8.432875347
> > 9.354748885 9.367668838 9.259952558 9.421538213 9.554635162
> > 9.603744578
> > 9.452197983 9.284228877 9.404607878 9.317737979 9.343115301
> > 9.310644266
> > 9.27227486  9.360337823 9.44706281
> > 9.944863964 9.950427516 10.19101759 10.07350804 10.03269879
> 10.1307908
> >  10.03487287 9.74609383  9.886379007 9.775472567 10.036596
> 9.544738458
> > 9.699611598 9.911962567
> > 9.625804277
> >
> >
> >                                    Code:
> >
> >        rowN <- nrow(AT1)
> >        MD1<-vector(length=rowN)
> >       MD2<-vector(length=rowN)
> >
> >           MD1[1:rowN]<-NA
> >           MD2[1:rowN]<-NA
> >
> >
> >          x<- AT1[,c(2,3,4,5,6,7,8) ]
> >         write.csv(x,"x.csv",row.names=TRUE)
> >          x<-as.matrix(x)
> >         for(i in 2:rowN) {
> >        MD1[i]=median(x[i,])
> >           }
> >          write.csv(MD1,"MD1.csv",row.names=TRUE)
> >
> >          y<- AT1[,c(9,10,11,12,13,14,15,16)]
> >          write.csv(y,"y.csv",row.names=TRUE)
> >          y<-as.matrix(y)
> >          for(j in 2:rowN) {
> >          MD2[j]=median(y[j,])
> >           }
> >          write.csv(MD2,"MD2.csv",row.names=TRUE)
> >
> >
> > ------------------------------
> > If you reply to this email, your message will be added to the
> > discussion
> > below:
> >
> > http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-
> produ
> > ce-Na-tp4711689.html To start a new topic under R help, email
> > ml-node+s789695n789696h75 at n4.nabble.com
> > To unsubscribe from R, click here
> >
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscr
> >
> ibe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtM
> > jQ0MzkwMjQ1>
> > .
> > NAML
> >
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_vi
> >
> ewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces
> > .BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.naml.n
> > amespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabb
> >
> le.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21na
> > bble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> send_instant_em
> > ail%21nabble%3Aemail.naml>
> >
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Median-on-
> second-group-of-CSV-file-produce-Na-tp4711689p4711690.html
> Sent from the R help mailing list archive at Nabble.com.
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From loris.bennett at fu-berlin.de  Tue Sep  1 12:11:51 2015
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Tue, 1 Sep 2015 12:11:51 +0200
Subject: [R] igraph tree: increase vertex separation within tier
Message-ID: <87twre78k8.fsf@hornfels.zedat.fu-berlin.de>

Hi,

I am collecting data about network errors and would like to visualise
the results in some sort of graph which reflects the hierarchy of the
components in the network (i.e. core switches are connected to leaf
switches and nodes are connected to leaf switches).

The errors are in file which looks something like this:

--------
2015-07-15;coreswitch01;1;0
2015-07-15;coreswitch01;2;0
2015-07-15;leafswitch01;1;0
2015-07-15;leafswitch01;2;0
2015-07-15;leafswitch01;3;0
2015-07-15;leafswitch01;4;0
2015-07-15;leafswitch01;5;0
2015-07-15;leafswitch02;1;0
2015-07-15;leafswitch02;2;0
2015-07-15;leafswitch02;3;0
2015-07-15;leafswitch02;4;0
2015-07-15;leafswitch02;5;0
2015-07-15;leafswitch02;2;561
2015-07-15;node001;1;0
2015-07-15;node002;1;0
2015-07-15;node003;1;0
2015-07-15;node004;1;3
2015-07-15;node005;1;10529
2015-07-15;node007;1;0
2015-07-15;node008;1;2081
--------

and the topology file, which relates ports to ports and ports to
multiport components, looks something like this:

--------
coreswitch01;coreswitch01_01
coreswitch01;coreswitch01_02
leafswitch01;leafswitch01_01
leafswitch01;leafswitch01_02
leafswitch01;leafswitch01_03
leafswitch01;leafswitch01_04
leafswitch01;leafswitch01_05
leafswitch02;leafswitch02_01
leafswitch02;leafswitch02_02
leafswitch02;leafswitch02_03
leafswitch02;leafswitch02_04
leafswitch02;leafswitch02_05
coreswitch01_01;leafswitch01_01
coreswitch01_02;leafswitch02_01
leafswitch01_02;node001_01
leafswitch01_03;node002_01
leafswitch01_04;node003_01
leafswitch01_05;node004_01
leafswitch02_02;node005_01
leafswitch02_03;node006_01
leafswitch02_04;node007_01
leafswitch02_05;node008_01
--------


I'm plotting the data with the following:

--------
library("igraph")

error_data <- read.csv(file="errors.csv",head=FALSE,sep=";")
colnames(error_data) <- c("datetime","name","portnumber","generic_error");

topo_data <- read.csv(file="topology.csv",head=FALSE,sep=";")
G <-graph.data.frame(topo_data, directed=F)

error_counter <- 'generic_error'
error_counter_max <- max(error_data[,error_counter])

vcolours <- 100 - round(99*error_data[,error_counter]/error_counter_max)
hc100 <- heat.colors(100)
vcolour_values <- hc100[vcolours]

l <- layout_(G,as_tree(root=c(1),rootlevel=c(1)))

plot(G, layout = l, vertex.shape = "rectangle", vertex.color=vcolour_values)
--------

This produces roughly what I want.  However, even with this subset of
the full network, there is quite a lot of overlapping of vertex labels
within the lowest tier.  The full data set has over 100 vertices on the
lowest tier, which causes the labels to become illegible.  I can adjust
the aspect ratio of the plot and/or the canvas, but this doesn't affect
the separation between the vertices.

Does anyone have any advice about how to display such information?
(Completely different approaches also welcome.)

Cheers,

Loris

-- 
This signature is currently under construction.


From naving2uk at gmail.com  Tue Sep  1 09:57:16 2015
From: naving2uk at gmail.com (Navien)
Date: Tue, 1 Sep 2015 00:57:16 -0700 (PDT)
Subject: [R] Median on second group of CSV file produce Na
In-Reply-To: <55E55652.4020702@dewey.myzen.co.uk>
References: <1441049748473-4711689.post@n4.nabble.com>
	<CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
	<55E55652.4020702@dewey.myzen.co.uk>
Message-ID: <CAM1GnGNQn0=5Nha-AMy70x_Rjgi5JFi7j073xYApZLUFW8GNiw@mail.gmail.com>

Dear Michael ,

Thank you very much for your reply , kindly  please can i send you the
program and data please , y is shown a normal csv file no issue with it :(


Kind Regards

On Tue, Sep 1, 2015 at 8:36 AM, Michael Dewey-3 [via R] <
ml-node+s789695n4711707h94 at n4.nabble.com> wrote:

> Dear Shawin
>
> You probably did not get an answer because
> 1 - you seem to have posted in HTML which mangled your post into
> unreadability
> 2 - there seem to be lots of lines which do not do anything germane to
> the problem.
>
> Why not try summary(y) or str(y) before you convert it to a matrix and
> see what it says?
>
> On 31/08/2015 20:46, shawin wrote:
>
> > I have an issue ans i posted it , so i would like to receive a solution
> > please
> >
> > On Mon, Aug 31, 2015 at 8:35 PM, shawin [via R] <
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=0>>
> wrote:
> >
> >> I have a data frame  csv file and I'm trying to calculate median for
> each
> >> group separately row by row . When I separate the data frame in two
> groups
> >> and calculate the median for each one, I am getting an NA result for
> the
> >> second group :
> >> the data
> >>    x1  x2  x3  x4  x5  x6  x7  y1  y2  y3  y4  y5  y6  y7  y8
> >> 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045 9.640816474
> >> 9.606262272   9.329651027 9.434541611 9.473922432 9.311412966 9.3154885
> >> 9.434977488 9.470895414 9.764258059
> >> 8.630629966 8.55831075  8.788391003 8.576231135 8.671587906 8.842979993
> >> 8.861958856 8.58330436  8.603596508 8.570129609 8.59798922  8.572686772
> >> 8.679751791 8.663950953 8.432875347
> >> 9.354748885 9.367668838 9.259952558 9.421538213 9.554635162 9.603744578
> >> 9.452197983 9.284228877 9.404607878 9.317737979 9.343115301 9.310644266
> >> 9.27227486  9.360337823 9.44706281
> >> 9.944863964 9.950427516 10.19101759 10.07350804 10.03269879 10.1307908
> >>   10.03487287 9.74609383  9.886379007 9.775472567 10.036596
> 9.544738458
> >> 9.699611598 9.911962567
> >> 9.625804277
> >>
> >>
> >>                                     Code:
> >>
> >>         rowN <- nrow(AT1)
> >>         MD1<-vector(length=rowN)
> >>        MD2<-vector(length=rowN)
> >>
> >>            MD1[1:rowN]<-NA
> >>            MD2[1:rowN]<-NA
> >>
> >>
> >>           x<- AT1[,c(2,3,4,5,6,7,8) ]
> >>          write.csv(x,"x.csv",row.names=TRUE)
> >>           x<-as.matrix(x)
> >>          for(i in 2:rowN) {
> >>         MD1[i]=median(x[i,])
> >>            }
> >>           write.csv(MD1,"MD1.csv",row.names=TRUE)
> >>
> >>           y<- AT1[,c(9,10,11,12,13,14,15,16)]
> >>           write.csv(y,"y.csv",row.names=TRUE)
> >>           y<-as.matrix(y)
> >>           for(j in 2:rowN) {
> >>           MD2[j]=median(y[j,])
> >>            }
> >>           write.csv(MD2,"MD2.csv",row.names=TRUE)
> >>
> >>
> >> ------------------------------
> >> If you reply to this email, your message will be added to the
> discussion
> >> below:
> >>
> >>
> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689.html
> >> To start a new topic under R help, email
> >> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=1>
> >> To unsubscribe from R, click here
> >> <
> >> .
> >> NAML
> >> <
> http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>
> >>
> >
> >
> >
> >
> > --
> > View this message in context:
> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711690.html
> > Sent from the R help mailing list archive at Nabble.com.
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=2>
> mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
> ______________________________________________
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=3>
> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711707.html
> To start a new topic under R help, email
> ml-node+s789695n789696h53 at n4.nabble.com
> To unsubscribe from R, click here
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=bmF2aW5nMnVrQGdtYWlsLmNvbXw3ODk2OTV8LTczNzQxMTY0Ng==>
> .
> NAML
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711709.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From axionator at gmail.com  Tue Sep  1 13:34:41 2015
From: axionator at gmail.com (axionator)
Date: Tue, 1 Sep 2015 13:34:41 +0200
Subject: [R] automatically calling dev.off()
Message-ID: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>

Hi,
is there a way to automatically call dev.off()?
I use options(device="myfunc") to automatically open a device to print to.
Afterwards, I would like to close it (so that the file is actually written).
I tried to do it via addTaskCallback, but unfortunately, I have to use
Rserve and (for any reason), the callback does not work (in the default R
console on win7 it works, however).

So, the following runs in the R console (but not via Rserve):

mypng <- function(filename = "test.png", ...)
{
    times <- function(total = 2, str = "Task") {
        ctr <- 0
        function(expr, value, ok, visible) {
            ctr <<- ctr + 1
            keep.me <- (ctr < total)
            if (ctr == total) {
                cat("ENDPRINT")
                dev.off()
            }
            keep.me
        }
    }
    png(filename, ...)
    n <- addTaskCallback(times(1))
}
options(device="mypng")
plot(rnorm(333))

Any hints?

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Tue Sep  1 15:36:31 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 1 Sep 2015 14:36:31 +0100
Subject: [R] Median on second group of CSV file produce Na
In-Reply-To: <CAM1GnGNQn0=5Nha-AMy70x_Rjgi5JFi7j073xYApZLUFW8GNiw@mail.gmail.com>
References: <1441049748473-4711689.post@n4.nabble.com>
	<CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
	<55E55652.4020702@dewey.myzen.co.uk>
	<CAM1GnGNQn0=5Nha-AMy70x_Rjgi5JFi7j073xYApZLUFW8GNiw@mail.gmail.com>
Message-ID: <55E5A9DF.8060201@dewey.myzen.co.uk>

What happened when you did summary(y) or str(y)?

On 01/09/2015 08:57, Navien wrote:
> Dear Michael ,
>
> Thank you very much for your reply , kindly  please can i send you the
> program and data please , y is shown a normal csv file no issue with it :(
>
>
> Kind Regards
>
> On Tue, Sep 1, 2015 at 8:36 AM, Michael Dewey-3 [via R] <
> ml-node+s789695n4711707h94 at n4.nabble.com> wrote:
>
>> Dear Shawin
>>
>> You probably did not get an answer because
>> 1 - you seem to have posted in HTML which mangled your post into
>> unreadability
>> 2 - there seem to be lots of lines which do not do anything germane to
>> the problem.
>>
>> Why not try summary(y) or str(y) before you convert it to a matrix and
>> see what it says?
>>
>> On 31/08/2015 20:46, shawin wrote:
>>
>>> I have an issue ans i posted it , so i would like to receive a solution
>>> please
>>>
>>> On Mon, Aug 31, 2015 at 8:35 PM, shawin [via R] <
>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=0>>
>> wrote:
>>>
>>>> I have a data frame  csv file and I'm trying to calculate median for
>> each
>>>> group separately row by row . When I separate the data frame in two
>> groups
>>>> and calculate the median for each one, I am getting an NA result for
>> the
>>>> second group :
>>>> the data
>>>>     x1  x2  x3  x4  x5  x6  x7  y1  y2  y3  y4  y5  y6  y7  y8
>>>> 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045 9.640816474
>>>> 9.606262272   9.329651027 9.434541611 9.473922432 9.311412966 9.3154885
>>>> 9.434977488 9.470895414 9.764258059
>>>> 8.630629966 8.55831075  8.788391003 8.576231135 8.671587906 8.842979993
>>>> 8.861958856 8.58330436  8.603596508 8.570129609 8.59798922  8.572686772
>>>> 8.679751791 8.663950953 8.432875347
>>>> 9.354748885 9.367668838 9.259952558 9.421538213 9.554635162 9.603744578
>>>> 9.452197983 9.284228877 9.404607878 9.317737979 9.343115301 9.310644266
>>>> 9.27227486  9.360337823 9.44706281
>>>> 9.944863964 9.950427516 10.19101759 10.07350804 10.03269879 10.1307908
>>>>    10.03487287 9.74609383  9.886379007 9.775472567 10.036596
>> 9.544738458
>>>> 9.699611598 9.911962567
>>>> 9.625804277
>>>>
>>>>
>>>>                                      Code:
>>>>
>>>>          rowN <- nrow(AT1)
>>>>          MD1<-vector(length=rowN)
>>>>         MD2<-vector(length=rowN)
>>>>
>>>>             MD1[1:rowN]<-NA
>>>>             MD2[1:rowN]<-NA
>>>>
>>>>
>>>>            x<- AT1[,c(2,3,4,5,6,7,8) ]
>>>>           write.csv(x,"x.csv",row.names=TRUE)
>>>>            x<-as.matrix(x)
>>>>           for(i in 2:rowN) {
>>>>          MD1[i]=median(x[i,])
>>>>             }
>>>>            write.csv(MD1,"MD1.csv",row.names=TRUE)
>>>>
>>>>            y<- AT1[,c(9,10,11,12,13,14,15,16)]
>>>>            write.csv(y,"y.csv",row.names=TRUE)
>>>>            y<-as.matrix(y)
>>>>            for(j in 2:rowN) {
>>>>            MD2[j]=median(y[j,])
>>>>             }
>>>>            write.csv(MD2,"MD2.csv",row.names=TRUE)
>>>>
>>>>
>>>> ------------------------------
>>>> If you reply to this email, your message will be added to the
>> discussion
>>>> below:
>>>>
>>>>
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689.html
>>>> To start a new topic under R help, email
>>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=1>
>>>> To unsubscribe from R, click here
>>>> <
>>>> .
>>>> NAML
>>>> <
>> http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711690.html
>>> Sent from the R help mailing list archive at Nabble.com.
>>> [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=2>
>> mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>>
>> ______________________________________________
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=3>
>> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> ------------------------------
>> If you reply to this email, your message will be added to the discussion
>> below:
>>
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711707.html
>> To start a new topic under R help, email
>> ml-node+s789695n789696h53 at n4.nabble.com
>> To unsubscribe from R, click here
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=bmF2aW5nMnVrQGdtYWlsLmNvbXw3ODk2OTV8LTczNzQxMTY0Ng==>
>> .
>> NAML
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711709.html
> Sent from the R help mailing list archive at Nabble.com.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From jdnewmil at dcn.davis.CA.us  Tue Sep  1 15:42:21 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 01 Sep 2015 06:42:21 -0700
Subject: [R] automatically calling dev.off()
In-Reply-To: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
References: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
Message-ID: <35CDE9DD-1474-4A34-92D8-F86962FE121E@dcn.davis.CA.us>

Don't use options to do this in batch mode. Open and close the file as you make the plot.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 1, 2015 4:34:41 AM PDT, axionator <axionator at gmail.com> wrote:
>Hi,
>is there a way to automatically call dev.off()?
>I use options(device="myfunc") to automatically open a device to print
>to.
>Afterwards, I would like to close it (so that the file is actually
>written).
>I tried to do it via addTaskCallback, but unfortunately, I have to use
>Rserve and (for any reason), the callback does not work (in the default
>R
>console on win7 it works, however).
>
>So, the following runs in the R console (but not via Rserve):
>
>mypng <- function(filename = "test.png", ...)
>{
>    times <- function(total = 2, str = "Task") {
>        ctr <- 0
>        function(expr, value, ok, visible) {
>            ctr <<- ctr + 1
>            keep.me <- (ctr < total)
>            if (ctr == total) {
>                cat("ENDPRINT")
>                dev.off()
>            }
>            keep.me
>        }
>    }
>    png(filename, ...)
>    n <- addTaskCallback(times(1))
>}
>options(device="mypng")
>plot(rnorm(333))
>
>Any hints?
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Tue Sep  1 16:04:21 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 1 Sep 2015 10:04:21 -0400
Subject: [R] automatically calling dev.off()
In-Reply-To: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
References: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
Message-ID: <55E5B065.60806@gmail.com>

On 01/09/2015 7:34 AM, axionator wrote:
> Hi,
> is there a way to automatically call dev.off()?
> I use options(device="myfunc") to automatically open a device to print to.
> Afterwards, I would like to close it (so that the file is actually written).
> I tried to do it via addTaskCallback, but unfortunately, I have to use
> Rserve and (for any reason), the callback does not work (in the default R
> console on win7 it works, however).
> 
> So, the following runs in the R console (but not via Rserve):
> 
> mypng <- function(filename = "test.png", ...)
> {
>     times <- function(total = 2, str = "Task") {
>         ctr <- 0
>         function(expr, value, ok, visible) {
>             ctr <<- ctr + 1
>             keep.me <- (ctr < total)
>             if (ctr == total) {
>                 cat("ENDPRINT")
>                 dev.off()
>             }
>             keep.me
>         }
>     }
>     png(filename, ...)
>     n <- addTaskCallback(times(1))
> }
> options(device="mypng")
> plot(rnorm(333))
> 

It wouldn't make sense to call dev.off() after the plot() command,
because the user might be planning to add something to it.  You need to
tell R that you are done, and that's what dev.off() is for.

You can call it automatically in a function by using

on.exit(dev.off())

and it should happen automatically at the end of an R session, but it
*shouldn't* happen after every plotting call.

Duncan Murdoch


From jdnewmil at dcn.davis.CA.us  Tue Sep  1 16:21:30 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 01 Sep 2015 07:21:30 -0700
Subject: [R] automatically calling dev.off()
In-Reply-To: <CAGopM691A9UHfwG7fOFEMjDE2civ7c2cdXCjhcrxii--6pexXg@mail.gmail.com>
References: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
	<35CDE9DD-1474-4A34-92D8-F86962FE121E@dcn.davis.CA.us>
	<CAGopM691A9UHfwG7fOFEMjDE2civ7c2cdXCjhcrxii--6pexXg@mail.gmail.com>
Message-ID: <4F53E70E-C781-4EA3-AFF8-36A204A571DE@dcn.davis.CA.us>

If you are implementing your own interactive graphics device by creating files, then I would expect you to want to leave all files closed between graphics operations so your widget can interactively update as the user makes graphics calls. However, I have not done this, so could only suggest that you look at some example client code (RStudio? RGui?) and perhaps search r-devel.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 1, 2015 6:54:33 AM PDT, axionator <axionator at gmail.com> wrote:
>I did it on the console only for testing. I have a program that uses
>Rserve
>and would like to have it therefore. (the program basically provides a
>terminal where you can type in R commands. Since plotting is only
>possible
>via files (when using Rserve) that are then displayed in a widget, I
>like
>to avoid the overhead of writing png() ... dev.off()  for each plot.)
>
>On Tue, Sep 1, 2015 at 3:42 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> Don't use options to do this in batch mode. Open and close the file
>as you
>> make the plot.
>>
>---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go
>Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                       Live:   OO#.. Dead: OO#.. 
>Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#. 
>rocks...1k
>>
>---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>>
>> On September 1, 2015 4:34:41 AM PDT, axionator <axionator at gmail.com>
>> wrote:
>> >Hi,
>> >is there a way to automatically call dev.off()?
>> >I use options(device="myfunc") to automatically open a device to
>print
>> >to.
>> >Afterwards, I would like to close it (so that the file is actually
>> >written).
>> >I tried to do it via addTaskCallback, but unfortunately, I have to
>use
>> >Rserve and (for any reason), the callback does not work (in the
>default
>> >R
>> >console on win7 it works, however).
>> >
>> >So, the following runs in the R console (but not via Rserve):
>> >
>> >mypng <- function(filename = "test.png", ...)
>> >{
>> >    times <- function(total = 2, str = "Task") {
>> >        ctr <- 0
>> >        function(expr, value, ok, visible) {
>> >            ctr <<- ctr + 1
>> >            keep.me <- (ctr < total)
>> >            if (ctr == total) {
>> >                cat("ENDPRINT")
>> >                dev.off()
>> >            }
>> >            keep.me
>> >        }
>> >    }
>> >    png(filename, ...)
>> >    n <- addTaskCallback(times(1))
>> >}
>> >options(device="mypng")
>> >plot(rnorm(333))
>> >
>> >Any hints?
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>>


From thierry.onkelinx at inbo.be  Tue Sep  1 16:27:29 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 1 Sep 2015 16:27:29 +0200
Subject: [R] automatically calling dev.off()
In-Reply-To: <4F53E70E-C781-4EA3-AFF8-36A204A571DE@dcn.davis.CA.us>
References: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
	<35CDE9DD-1474-4A34-92D8-F86962FE121E@dcn.davis.CA.us>
	<CAGopM691A9UHfwG7fOFEMjDE2civ7c2cdXCjhcrxii--6pexXg@mail.gmail.com>
	<4F53E70E-C781-4EA3-AFF8-36A204A571DE@dcn.davis.CA.us>
Message-ID: <CAJuCY5ybrHi3bZ3UBCTMoW67z1e2_3L6VHj=3_x5Za0kE9GmnQ@mail.gmail.com>

Have a look at the implementation of ggsave in the ggplot2 package.
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no
more than asking him to perform a post-mortem examination: he may be
able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does
not ensure that a reasonable answer can be extracted from a given body
of data. ~ John Tukey


2015-09-01 16:21 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
> If you are implementing your own interactive graphics device by creating files, then I would expect you to want to leave all files closed between graphics operations so your widget can interactively update as the user makes graphics calls. However, I have not done this, so could only suggest that you look at some example client code (RStudio? RGui?) and perhaps search r-devel.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 1, 2015 6:54:33 AM PDT, axionator <axionator at gmail.com> wrote:
>>I did it on the console only for testing. I have a program that uses
>>Rserve
>>and would like to have it therefore. (the program basically provides a
>>terminal where you can type in R commands. Since plotting is only
>>possible
>>via files (when using Rserve) that are then displayed in a widget, I
>>like
>>to avoid the overhead of writing png() ... dev.off()  for each plot.)
>>
>>On Tue, Sep 1, 2015 at 3:42 PM, Jeff Newmiller
>><jdnewmil at dcn.davis.ca.us>
>>wrote:
>>
>>> Don't use options to do this in batch mode. Open and close the file
>>as you
>>> make the plot.
>>>
>>---------------------------------------------------------------------------
>>> Jeff Newmiller                        The     .....       .....  Go
>>Live...
>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>> Go...
>>>                                       Live:   OO#.. Dead: OO#..
>>Playing
>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>rocks...1k
>>>
>>---------------------------------------------------------------------------
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On September 1, 2015 4:34:41 AM PDT, axionator <axionator at gmail.com>
>>> wrote:
>>> >Hi,
>>> >is there a way to automatically call dev.off()?
>>> >I use options(device="myfunc") to automatically open a device to
>>print
>>> >to.
>>> >Afterwards, I would like to close it (so that the file is actually
>>> >written).
>>> >I tried to do it via addTaskCallback, but unfortunately, I have to
>>use
>>> >Rserve and (for any reason), the callback does not work (in the
>>default
>>> >R
>>> >console on win7 it works, however).
>>> >
>>> >So, the following runs in the R console (but not via Rserve):
>>> >
>>> >mypng <- function(filename = "test.png", ...)
>>> >{
>>> >    times <- function(total = 2, str = "Task") {
>>> >        ctr <- 0
>>> >        function(expr, value, ok, visible) {
>>> >            ctr <<- ctr + 1
>>> >            keep.me <- (ctr < total)
>>> >            if (ctr == total) {
>>> >                cat("ENDPRINT")
>>> >                dev.off()
>>> >            }
>>> >            keep.me
>>> >        }
>>> >    }
>>> >    png(filename, ...)
>>> >    n <- addTaskCallback(times(1))
>>> >}
>>> >options(device="mypng")
>>> >plot(rnorm(333))
>>> >
>>> >Any hints?
>>> >
>>> >       [[alternative HTML version deleted]]
>>> >
>>> >______________________________________________
>>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >https://stat.ethz.ch/mailman/listinfo/r-help
>>> >PLEASE do read the posting guide
>>> >http://www.R-project.org/posting-guide.html
>>> >and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marco.colagrossi at gmail.com  Tue Sep  1 16:49:03 2015
From: marco.colagrossi at gmail.com (Marco Colagrossi)
Date: Tue, 1 Sep 2015 16:49:03 +0200
Subject: [R] Metafor and forest(); not showing 'ilab' and text
In-Reply-To: <077E31A57DA26E46AB0D493C9966AC730F1ADBC6F2@UM-MAIL4112.unimaas.nl>
References: <CALxyAHTr2xjF9ZYcH2fKVe0-JuCtVV1GU+Nt2Qr5yLrj5+FKLQ@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F1AA92004@UM-MAIL4112.unimaas.nl>
	<CALxyAHTKXM6zw+BZhG3OHpn9ONGj+DRSAq3hNd0O_TZAVRy+YQ@mail.gmail.com>
	<55DC5018.8040000@dewey.myzen.co.uk>
	<077E31A57DA26E46AB0D493C9966AC730F1AA922CF@UM-MAIL4112.unimaas.nl>
	<CALxyAHQu6Q=grjPfVZL8=9mJNHxY1pe9wwn6tjj38-YJBW7c5A@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F1AA92353@UM-MAIL4112.unimaas.nl>
	<CALxyAHQ=O__2U+p16N=3VaTdd9_+QZBT5NrBPijzX0RQmKVjog@mail.gmail.com>
	<55DCA4E6.40204@dewey.myzen.co.uk>
	<CALxyAHQ-9Q-fAuAX1MPxOw9o8=4DyJzkKDngcrdC1Ory-_pMTg@mail.gmail.com>
	<55E48218.9090008@dewey.myzen.co.uk>
	<CALxyAHTKPoyHpxD-Z+BgGZrF+4GHZsQOt1J_dn2xxfVoqHVhgA@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F1ADBC6F2@UM-MAIL4112.unimaas.nl>
Message-ID: <CALxyAHRzYb1qfOLycJAFeKg88prZeqGFX7ZJUq8ZQFo+K64vSw@mail.gmail.com>

I did read the help(rma.mv) and I also had look at the analysis by
Konstantopoulos (2011) in the past few days. You have to apologize me
but is the first meta analysis I'm trying to carry on, it is the first
I'm working on R and moreover the terminology here is somehow
different (and confusing) with respect to the terminology I was used
in panel data analysis.

It looks to me - correct me if I'm wrong - that a model such:

     rma.mv(pc, var, random = ~ 1 | author, data=codebook)

or

     rma.mv(pc, var, random = ~ 1 | author, mods = ~ pub + SIMiv, data=codebook)

it is a varying intercept model (using Gelman-Hill Yi = Aji+BXi+Ei).
Why do you say that "Using "random = ~ 1 | author" is likely to be
insufficient. You also need to add random effects at the observation
level"? Could you please walk me through what you mean by that?

I'll (try to) explain to you what I'm doing here so you might be able
to help me out.
I'm carrying on a meta-analysis (and ultimately, few meta-regressions)
of the relationship between firm performances and bank-firm
relationship. Since I have different proxies for the latter, I
computed as effect size a raw partial correlation, a continuous
Fisher?s z-score and the one-tail p-value as a continuous
interpretation of the direction and the significance of an effect
size.

The number of studies in my meta-analysis is 29, but most of them have
multiple cases so, ultimately, I have 98 different cases. All of the
29 studies have repeated yearly observation in the same country for
different time span; let's say one from 1985 to 1991 in Spain, one
from 1987 to 1999 in China, one from 1981 to 2002 in the US and so on.
Even in case in which two different study investigates the same
country and some of the years in the time span overlaps I'm sure that
their population is drawn from different samples.

My idea was too confront what in econometrics is called a study-fixed
effect (or author fixed effect since authors have no more than one
study in my analysis), that is:

       rma(pc, var, mods = ~ I(study), data=codebook)

With a multilevel model, in order to account for the fact that
observation from the same study are not independent.

May I have your opinion on what I'm trying to do here guys?
Do you think I should take into account also a autoregressive
structure over time and correct that with struct="HAR" ?

Sorry for the long mail, I wanted to explain as better as possible,
and thank you again for your help, It is incredibly appreciated.

Marco

On 31 August 2015 at 19:28, Viechtbauer Wolfgang (STAT)
<wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> Have you read help(rma.mv)? It describes in detail what "random = ~ 1 | author" does. Also, I think you may find some of these useful:
>
> http://www.metafor-project.org/doku.php/analyses#multivariate_multilevel_meta-analysis_models
>
> Especially: http://www.metafor-project.org/doku.php/analyses:konstantopoulos2011
>
> Using "random = ~ 1 | author" is likely to be insufficient. You also need to add random effects at the observation level.
>
> Best,
> Wolfgang
>
> --
> Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and
> Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD
> Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com
>
>> -----Original Message-----
>> From: Marco Colagrossi [mailto:marco.colagrossi at gmail.com]
>> Sent: Monday, August 31, 2015 18:37
>> To: Michael Dewey
>> Cc: Viechtbauer Wolfgang (STAT); r-help at r-project.org
>> Subject: Re: [R] Metafor and forest(); not showing 'ilab' and text
>>
>> The solution that you proposed works perfectly, thank you very much.
>>
>> I'll wait for Wolfgang answer as I'm having few doubts about the models.
>>
>> Thanks
>>
>> On 31 August 2015 at 18:34, Michael Dewey <lists at dewey.myzen.co.uk>
>> wrote:
>> > Comments in line
>> >
>> > On 31/08/2015 16:08, Marco Colagrossi wrote:
>> >>
>> >> Thanks for your help,
>> >>
>> >> I got the mistake I was making and I managed to find a solution
>> >> regarding those graphs; I don't want to abuse of your patience but I
>> >> have three further questions:
>> >>
>> >> 1. Always regarding the forest plots, it is possible to make a
>> >> cross-subset? I try to explain my self better; I have one dummy
>> >> variable called pub and another variable called SIMiv that can take
>> >> the values of "share", "loan", "number" and "duration". How can I
>> >> subset my sample so that the forest shows only (for example) studies
>> >> when the dummy takes the value of 1 and the SIMiv variable takes the
>> >> values of "share" and "loan"?
>> >> Something like this:
>> >> forest(pc, var, ci95m, ci95p, slab = authoryear2, psize=1,
>> >> subset=(pub==1, SIMiv=("share", "loan", "duration"))
>> >>
>> >
>> > Do you not want something like
>> > (pub == 1) & (SIMIv %in% c("share", "loan", "duration"))
>> >
>> >
>> >> 2. I have few doubts regarding the multilevel modeling;
>> >>      rma.mv(pc, var, random = ~ 1 | author, data=codebook)
>> >>     if I'm correct this should be a multilevel model nested at
>> "author"
>> >> level; what I cannot understand If it is a varying intercept
>> >> (Y=A+BjX), a varying slope (Y=Aj+BX) or a varying intercept&slope
>> >> model (Y=Aj+BjX). Are there the formulas for it somewhere? So far I
>> >> only found the formulas for the estimators included in the metafor
>> >> package.
>> >>
>> >
>> > I think it a random intercept but Wolfgang may correct me there.
>> >
>> >
>> >> 3. metareg1 <- rma.mv(pc, var, random = ~ 1 | author, mods = ~ pub +
>> >> SIMiv, data=codebook)
>> >> Again, if I'm correct this should be a multilevel meta regression
>> >> (correct me if I'm wrong); I have the same doubts as before.
>> >>
>> >> Thank you again
>> >>
>> >> Marco
>> >>
>> >> On 25 August 2015 at 19:24, Michael Dewey <lists at dewey.myzen.co.uk>
>> wrote:
>> >>>
>> >>> Dear Marco
>> >>>
>> >>> When you change xlim it increases the width of the forest plot in the
>> >>> sense
>> >>> you describe. It does not push your text out of the way to make space
>> for
>> >>> it
>> >>> but instead overprints it. You may like to use alim to truncate your
>> >>> confidence interval whiskers to fit within the space you see or make
>> your
>> >>> labels shorter.
>> >>>
>> >>>
>> >>> On 25/08/2015 17:25, Marco Colagrossi wrote:
>> >>>>
>> >>>>
>> >>>> I think I've not explained myself well. When I say "the width of the
>> >>>> forest plot" I mean the region above the observed outcome, the
>> >>>> "actual" forest plot, not the plot as a whole. Even if I change
>> values
>> >>>> for Xlim, cex or ilab.xpos the width of that particular region
>> within
>> >>>> the plot doesn't change.
>> >>>>
>> >>>> Best,
>> >>>>
>> >>>> Marco
>> >>>>
>> >>>> On 25 August 2015 at 18:11, Viechtbauer Wolfgang (STAT)
>> >>>> <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
>> >>>>>
>> >>>>>
>> >>>>> The 'xlim' argument does not change the actual width of the
>> plotting
>> >>>>> device. For that, you need to use the 'width' argument with
>> whatever
>> >>>>> device
>> >>>>> you are actually using. You can then use the 'xlim' argument to
>> create
>> >>>>> appropriate spacing to the left/right of the part of the plot that
>> >>>>> shows the
>> >>>>> estimates and their CIs. Within that space, you can then add
>> additional
>> >>>>> columns with the 'ilab' argument. It's up to you to find an
>> appropriate
>> >>>>> combination of plotting device width, character/symbol expansion
>> factor
>> >>>>> ('cex' argument), 'xlim' values, and 'ilab.xpos' values to create a
>> >>>>> nice
>> >>>>> looking plot that has no overlapping text and no excessive white
>> space.
>> >>>>> An
>> >>>>> example is this:
>> >>>>>
>> >>>>>
>> >>>>> http://www.metafor-
>> project.org/doku.php/plots:forest_plot_with_subgroups
>> >>>>>
>> >>>>> Note that it took me dozens of iterations to create that plot. You
>> just
>> >>>>> have to start experimenting.
>> >>>>>
>> >>>>> Best,
>> >>>>> Wolfgang
>> >>>>>
>> >>>>>> -----Original Message-----
>> >>>>>> From: Marco Colagrossi [mailto:marco.colagrossi at gmail.com]
>> >>>>>> Sent: Tuesday, August 25, 2015 17:59
>> >>>>>> To: Viechtbauer Wolfgang (STAT)
>> >>>>>> Cc: r-help at r-project.org; Michael Dewey
>> >>>>>> Subject: Re: [R] Metafor and forest(); not showing 'ilab' and text
>> >>>>>>
>> >>>>>> Thanks again for your help. I'm sorry to bother you but I don't
>> get
>> >>>>>> how to widen the forest plot; if I try to change the values of
>> xlim or
>> >>>>>> the ilab.xpos values the width of the forest plot region does not
>> >>>>>> change, but only moves on the graphs. What I'm I missing?
>> >>>>>>
>> >>>>>>
>> >>>>>> forest(pc, var, ci95m, ci95p, slab = authoryear, psize=1,
>> >>>>>> subset=(pub==1),
>> >>>>>>          xlim = c(-16, 6),
>> >>>>>>          ilab = data.frame(SIMdv, SIMiv),
>> >>>>>>          ilab.xpos = c(-7.5, -5.5), cex = 0.75)
>> >>>>>> op <- par(cex=.75, font=2)
>> >>>>>>         text(c(-7.5, -5.5), 54, c("DV", "IV"))
>> >>>>>>         text(-16,                54, "Author(s) and Year",
>> pos=4)
>> >>>>>>         text(6,                  54, "Outcome [95% CI]", pos=2)
>> >>>>>> par(op)
>> >>>>>>>
>> >>>>>>>
>> >>>>>>> par("usr")[1:2]
>> >>>>>>
>> >>>>>>
>> >>>>>> [1] -16   6


From murdoch.duncan at gmail.com  Tue Sep  1 17:00:20 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 1 Sep 2015 11:00:20 -0400
Subject: [R] automatically calling dev.off()
In-Reply-To: <CAGopM68UZoPBwsXL31kPE1zLNqcwYxaOiyWq91qFC-Km0=BDuw@mail.gmail.com>
References: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
	<55E5B065.60806@gmail.com>
	<CAGopM68UZoPBwsXL31kPE1zLNqcwYxaOiyWq91qFC-Km0=BDuw@mail.gmail.com>
Message-ID: <55E5BD84.2040906@gmail.com>

On 01/09/2015 10:13 AM, axionator wrote:
> Yes, the user could add something. Here, In about 90% of the time, he
> won't. So it would be nice to have a dev.off() as default (and some
> extra handling if more plot commands will follow).

That's a different mental model of how graphics should work, and it's
not compatible with standard R graphics.  However, it would be pretty
easy to get this to work with a grid graphics based systems (e.g.
ggplot2, lattice) since they don't draw anything until you print the
object:  you'd just need to override the print method.

Duncan Murdoch


> 
> On Tue, Sep 1, 2015 at 4:04 PM, Duncan Murdoch <murdoch.duncan at gmail.com
> <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>     On 01/09/2015 7:34 AM, axionator wrote:
>     > Hi,
>     > is there a way to automatically call dev.off()?
>     > I use options(device="myfunc") to automatically open a device to print to.
>     > Afterwards, I would like to close it (so that the file is actually written).
>     > I tried to do it via addTaskCallback, but unfortunately, I have to use
>     > Rserve and (for any reason), the callback does not work (in the default R
>     > console on win7 it works, however).
>     >
>     > So, the following runs in the R console (but not via Rserve):
>     >
>     > mypng <- function(filename = "test.png", ...)
>     > {
>     >     times <- function(total = 2, str = "Task") {
>     >         ctr <- 0
>     >         function(expr, value, ok, visible) {
>     >             ctr <<- ctr + 1
>     >             keep.me <http://keep.me> <- (ctr < total)
>     >             if (ctr == total) {
>     >                 cat("ENDPRINT")
>     >                 dev.off()
>     >             }
>     >             keep.me <http://keep.me>
>     >         }
>     >     }
>     >     png(filename, ...)
>     >     n <- addTaskCallback(times(1))
>     > }
>     > options(device="mypng")
>     > plot(rnorm(333))
>     >
> 
>     It wouldn't make sense to call dev.off() after the plot() command,
>     because the user might be planning to add something to it.  You need to
>     tell R that you are done, and that's what dev.off() is for.
> 
>     You can call it automatically in a function by using
> 
>     on.exit(dev.off())
> 
>     and it should happen automatically at the end of an R session, but it
>     *shouldn't* happen after every plotting call.
> 
>     Duncan Murdoch
> 
>


From wewolski at gmail.com  Tue Sep  1 17:31:27 2015
From: wewolski at gmail.com (Witold E Wolski)
Date: Tue, 1 Sep 2015 17:31:27 +0200
Subject: [R] data - stringAsFactors = FALSE
Message-ID: <CAAjnpdjxVi9evtFR13DetGzHHoXFeMg5P5UmjPDAHNgMj8QLfA@mail.gmail.com>

I have a tab delimited table in the data directory of a package.

I would like that when loading this data with
data(tablename)

in the example section the strings are not coerced to factors.

How can I achieve it? Or should I move this tables to the inst/extdata
directory and load them with read.table?

regards



-- 
Witold Eryk Wolski

	[[alternative HTML version deleted]]


From ssefick at gmail.com  Tue Sep  1 17:36:46 2015
From: ssefick at gmail.com (stephen sefick)
Date: Tue, 1 Sep 2015 10:36:46 -0500
Subject: [R] reshape: melt and cast
In-Reply-To: <CAC_TuLpX5fJA3EoMHetyz_5Jn=MZY_4_ywB60YRt5mS1a2LMgg@mail.gmail.com>
References: <CANbrOWd0mGGatb=9WY8PfWPYJXPCGcL5AG-B=+=vU1+P4pLTHQ@mail.gmail.com>
	<CADKEMqi1QSuvN7ZZAPHnX5RsRzUJ9-GE-0R1yVALJHH43rBGhA@mail.gmail.com>
	<CAC_TuLpX5fJA3EoMHetyz_5Jn=MZY_4_ywB60YRt5mS1a2LMgg@mail.gmail.com>
Message-ID: <CADKEMqjEkyctgXGckgjr6wZ1CLj3t69cUHXHFQREoqO0y__3TQ@mail.gmail.com>

I would make this minimal. In other words, use an example data set, dput,
and use output of dput in a block of reproducible code. I don't understand
exactly what you want, but does sum work? If there is more than one record
for a given set of factors the sum is the sum of the counts. If only one
record, then the sum is the same as the original number.

On Tue, Sep 1, 2015 at 10:00 AM, Matthew Pickard <
matthew.david.pickard at gmail.com> wrote:

> Thanks, Stephen. I've looked into the fun.aggregate argument. I don't want
> to aggregate, so I thought leaving it blank (allowing it to default to
> NULL) would do that.
>
>
> Here's a corrected post (with further explanation):
>
> Hi,
>
> I have data that looks like this:
>
> >dput(head(ratings))
> structure(list(QCode = structure(c(5L, 7L, 5L, 7L, 5L, 7L), .Label =
> c("APPEAR",
> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
> "factor"),
>     PID = structure(c(1L, 1L, 2L, 2L, 3L, 3L), .Label = c("1123",
>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>     "1556", "1558", "1559"), class = "factor"), RaterName =
> structure(c(1L,
>     1L, 1L, 1L, 1L, 1L), .Label = c("cwormhoudt", "zspeidel"), class =
> "factor"),
>     SI1 = c(2L, 1L, 1L, 1L, 2L, 1L), SI2 = c(2L, 2L, 2L, 2L,
>     2L, 3L), SI3 = c(3L, 3L, 3L, 3L, 2L, 4L), SI4 = c(1L, 2L,
>     1L, 1L, 1L, 1L), SI5 = c(1L, 1L, 1L, 1L, 1L, 1L), SI6 = c(1L,
>     1L, 1L, 1L, 1L, 1L), SI7 = c(3L, 1L, 2L, 1L, 2L, 1L), SI8 = c(3L,
>     1L, 3L, 1L, 3L, 1L), SI9 = c(3L, 1L, 2L, 1L, 1L, 1L), SI10 = c(2L,
>     1L, 2L, 1L, 2L, 1L), SI11 = c(1L, 3L, 1L, 2L, 1L, 4L)), .Names =
> c("QCode",
> "PID", "RaterName", "SI1", "SI2", "SI3", "SI4", "SI5", "SI6",
> "SI7", "SI8", "SI9", "SI10", "SI11"), row.names = c(NA, 6L), class =
> "data.frame")
>
>
> > dput(tail(ratings))
> structure(list(QCode = structure(c(3L, 3L, 3L, 1L, 1L, 3L), .Label =
> c("APPEAR",
> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
> "factor"),
>     PID = structure(c(161L, 162L, 163L, 163L, 164L, 164L), .Label =
> c("1123",
>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>     "1556", "1558", "1559"), class = "factor"), RaterName =
> structure(c(2L,
>     2L, 2L, 2L, 2L, 2L), .Label = c("cwormhoudt", "zspeidel"), class =
> "factor"),
>     SI1 = c(1L, 1L, 1L, 1L, 1L, 1L), SI2 = c(3L, 2L, 2L, 3L,
>     3L, 2L), SI3 = c(3L, 2L, 3L, 3L, 3L, 2L), SI4 = c(1L, 1L,
>     1L, 1L, 1L, 1L), SI5 = c(1L, 1L, 1L, 1L, 1L, 1L), SI6 = c(1L,
>     1L, 1L, 1L, 1L, 1L), SI7 = c(1L, 1L, 1L, 2L, 2L, 1L), SI8 = c(1L,
>     1L, 1L, 1L, 1L, 1L), SI9 = c(1L, 1L, 1L, 1L, 1L, 1L), SI10 = c(1L,
>     1L, 1L, 2L, 2L, 1L), SI11 = c(1L, 1L, 1L, 1L, 1L, 1L)), .Names =
> c("QCode",
> "PID", "RaterName", "SI1", "SI2", "SI3", "SI4", "SI5", "SI6",
> "SI7", "SI8", "SI9", "SI10", "SI11"), row.names = 2456:2461, class =
> "data.frame")
>
>
> I am trying to use the melt and cast functions to re-arrange to have
> column names QCode, PID, sItem, cwormhoudt, zpeidel.  Under each of the
> last two columns I want the values that correspond to each of RaterNames.
>
> So, I melt the data like this:
>
> mratings = melt(ratings, variable_name="sItem")
>
> Then cast the data like this:
>
> > outData = cast(mratings, QCode + PID + sItem ~ RaterName)
> Aggregation requires fun.aggregate: length used as default
>
> But the value columns appear to be displaying counts and not the original
> values.
>
> > dput(head(outData))
> structure(list(QCode = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label =
> c("APPEAR",
> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
> "factor"),
>     PID = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = c("1123",
>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>     "1556", "1558", "1559"), class = "factor"), sItem = structure(1:6,
> .Label = c("SI1",
>     "SI2", "SI3", "SI4", "SI5", "SI6", "SI7", "SI8", "SI9", "SI10",
>     "SI11"), class = "factor"), cwormhoudt = c(1L, 1L, 1L, 1L,
>     1L, 1L), zspeidel = c(1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("QCode",
> "PID", "sItem", "cwormhoudt", "zspeidel"), row.names = c(NA,
> 6L), class = "data.frame")
>
> The correct output would have 3s, 4s, and 5s in the RaterName columns:
>
> > which(outData$zpeidel==3)
> integer(0)
>
> I don't want it to aggregate the data, I simply want it to insert the
> original values into the two RaterName columns.  In cast, the fun.aggregate
> defaults to NULL, which I assumed was no function.  But the output says it
> uses length as default.
>
> How to I prevent cast from aggregating the data according to counts?  Am I
> doing something wrong?
>
> Thanks in advance.
>
>
> On Mon, Aug 31, 2015 at 11:32 PM, stephen sefick <ssefick at gmail.com>
> wrote:
>
>> This is very hard to read. Please use dput to provide data. I believe the
>> answer is in the manual. Look at the aggregation function argument.
>>
>> Please excuse my brevity; this message was sent from my telephone.
>> On Sep 1, 2015 12:11 AM, "Matt Pickard" <matthew.david.pickard at gmail.com>
>> wrote:
>>
>>> Hi,
>>>
>>> I have data that looks like this:
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> *> head(ratings)  QCode  PID  RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7 SI8
>>> SI9
>>> SI10 SI111 GUILT 1123 cwormhoudt   2   2   3   1   1   1   3   3   3
>>> 2    12  LOVE 1123 cwormhoudt   1   2   3   2   1   1   1   1   1    1
>>> 33 GUILT 1136 cwormhoudt   1   2   3   1   1   1   2   3   2    2    14
>>> LOVE 1136 cwormhoudt   1   2   3   1   1   1   1   1   1    1    25 GUILT
>>> 1137 cwormhoudt   2   2   2   1   1   1   2   3   1    2    16  LOVE 1137
>>> cwormhoudt   1   3   4   1   1   1   1   1   1    1    4*
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> *> tail(ratings)      QCode  PID RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7
>>> SI8
>>> SI9 SI10 SI112456    FUN 1555  zspeidel   1   3   3   1   1   1   1   1
>>> 1    1    12457    FUN 1556  zspeidel   1   2   2   1   1   1   1   1
>>> 1    1    12458    FUN 1558  zspeidel   1   2   3   1   1   1   1   1
>>> 1    1    12459 APPEAR 1558  zspeidel   1   3   3   1   1   1   2   1
>>> 1    2    12460 APPEAR 1559  zspeidel   1   3   3   1   1   1   2   1
>>> 1    2    12461    FUN 1559  zspeidel   1   2   2   1   1   1   1   1
>>> 1    1    1*
>>> I am trying to use the melt and cast functions to re-arrange it to look
>>> like this:
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> *   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123   SI1
>>> 1        12 APPEAR 1123   SI2          4        13 APPEAR 1123
>>> SI3          1        24 APPEAR 1123   SI4          3        15 APPEAR
>>> 1123   SI5          1        16 APPEAR 1123   SI6          1        3*
>>> So, I melt the data like this:
>>>
>>>
>>>
>>> *mratings = melt(ratings, variable_name="sItem")*
>>> Then cast the data like this:
>>>
>>>
>>> *> outData = cast(mratings, QCode + PID + sItem ~ RaterName)Aggregation
>>> requires fun.aggregate: length used as default*
>>>
>>> But the value columns appear to be displaying counts and not the original
>>> values:
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> *> head(outData)   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123
>>> SI1          1        12 APPEAR 1123   SI2          1        13 APPEAR
>>> 1123   SI3          1        14 APPEAR 1123   SI4          1        15
>>> APPEAR 1123   SI5          1        16 APPEAR 1123   SI6          1
>>> 1> which(outData$zpeidel==3)integer(0)*
>>> How to I prevent cast from aggregating the data according to counts?  Am
>>> I
>>> doing something wrong?
>>>
>>> Thanks in advance.
>>>
>>> MP
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>


-- 
Stephen Sefick
**************************************************
Auburn University
Biological Sciences
331 Funchess Hall
Auburn, Alabama
36849
**************************************************
sas0025 at auburn.edu
http://www.auburn.edu/~sas0025
**************************************************

Let's not spend our time and resources thinking about things that are so
little or so large that all they really do for us is puff us up and make us
feel like gods.  We are mammals, and have not exhausted the annoying little
problems of being mammals.

                                -K. Mullis

"A big computer, a complex algorithm and a long time does not equal
science."

                              -Robert Gentleman

	[[alternative HTML version deleted]]


From ssefick at gmail.com  Tue Sep  1 17:46:04 2015
From: ssefick at gmail.com (stephen sefick)
Date: Tue, 1 Sep 2015 10:46:04 -0500
Subject: [R] reshape: melt and cast
In-Reply-To: <CAC_TuLrr2WHRDL3F2hOtTDXp+9QOMh4u8W-DJxLxWCoMjFvFjQ@mail.gmail.com>
References: <CANbrOWd0mGGatb=9WY8PfWPYJXPCGcL5AG-B=+=vU1+P4pLTHQ@mail.gmail.com>
	<CADKEMqi1QSuvN7ZZAPHnX5RsRzUJ9-GE-0R1yVALJHH43rBGhA@mail.gmail.com>
	<CAC_TuLpX5fJA3EoMHetyz_5Jn=MZY_4_ywB60YRt5mS1a2LMgg@mail.gmail.com>
	<CADKEMqjEkyctgXGckgjr6wZ1CLj3t69cUHXHFQREoqO0y__3TQ@mail.gmail.com>
	<CAC_TuLrr2WHRDL3F2hOtTDXp+9QOMh4u8W-DJxLxWCoMjFvFjQ@mail.gmail.com>
Message-ID: <CADKEMqjS+VPmfCvTUY8VPrW-2pApJs7dQ+B_+noWfF4+VhR6gg@mail.gmail.com>

You are welcome.

On Tue, Sep 1, 2015 at 10:44 AM, Matthew Pickard <
matthew.david.pickard at gmail.com> wrote:

> Yep, that works. Thanks, Stephen. I should have drawn the parallel with
> Excel Pivot tables sooner.
>
> On Tue, Sep 1, 2015 at 9:36 AM, stephen sefick <ssefick at gmail.com> wrote:
>
>> I would make this minimal. In other words, use an example data set, dput,
>> and use output of dput in a block of reproducible code. I don't understand
>> exactly what you want, but does sum work? If there is more than one record
>> for a given set of factors the sum is the sum of the counts. If only one
>> record, then the sum is the same as the original number.
>>
>> On Tue, Sep 1, 2015 at 10:00 AM, Matthew Pickard <
>> matthew.david.pickard at gmail.com> wrote:
>>
>>> Thanks, Stephen. I've looked into the fun.aggregate argument. I don't
>>> want to aggregate, so I thought leaving it blank (allowing it to default to
>>> NULL) would do that.
>>>
>>>
>>> Here's a corrected post (with further explanation):
>>>
>>> Hi,
>>>
>>> I have data that looks like this:
>>>
>>> >dput(head(ratings))
>>> structure(list(QCode = structure(c(5L, 7L, 5L, 7L, 5L, 7L), .Label =
>>> c("APPEAR",
>>> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
>>> "factor"),
>>>     PID = structure(c(1L, 1L, 2L, 2L, 3L, 3L), .Label = c("1123",
>>>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>>>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>>>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>>>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>>>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>>>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>>>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>>>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>>>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>>>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>>>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>>>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>>>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>>>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>>>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>>>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>>>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>>>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>>>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>>>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>>>     "1556", "1558", "1559"), class = "factor"), RaterName =
>>> structure(c(1L,
>>>     1L, 1L, 1L, 1L, 1L), .Label = c("cwormhoudt", "zspeidel"), class =
>>> "factor"),
>>>     SI1 = c(2L, 1L, 1L, 1L, 2L, 1L), SI2 = c(2L, 2L, 2L, 2L,
>>>     2L, 3L), SI3 = c(3L, 3L, 3L, 3L, 2L, 4L), SI4 = c(1L, 2L,
>>>     1L, 1L, 1L, 1L), SI5 = c(1L, 1L, 1L, 1L, 1L, 1L), SI6 = c(1L,
>>>     1L, 1L, 1L, 1L, 1L), SI7 = c(3L, 1L, 2L, 1L, 2L, 1L), SI8 = c(3L,
>>>     1L, 3L, 1L, 3L, 1L), SI9 = c(3L, 1L, 2L, 1L, 1L, 1L), SI10 = c(2L,
>>>     1L, 2L, 1L, 2L, 1L), SI11 = c(1L, 3L, 1L, 2L, 1L, 4L)), .Names =
>>> c("QCode",
>>> "PID", "RaterName", "SI1", "SI2", "SI3", "SI4", "SI5", "SI6",
>>> "SI7", "SI8", "SI9", "SI10", "SI11"), row.names = c(NA, 6L), class =
>>> "data.frame")
>>>
>>>
>>> > dput(tail(ratings))
>>> structure(list(QCode = structure(c(3L, 3L, 3L, 1L, 1L, 3L), .Label =
>>> c("APPEAR",
>>> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
>>> "factor"),
>>>     PID = structure(c(161L, 162L, 163L, 163L, 164L, 164L), .Label =
>>> c("1123",
>>>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>>>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>>>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>>>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>>>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>>>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>>>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>>>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>>>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>>>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>>>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>>>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>>>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>>>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>>>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>>>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>>>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>>>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>>>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>>>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>>>     "1556", "1558", "1559"), class = "factor"), RaterName =
>>> structure(c(2L,
>>>     2L, 2L, 2L, 2L, 2L), .Label = c("cwormhoudt", "zspeidel"), class =
>>> "factor"),
>>>     SI1 = c(1L, 1L, 1L, 1L, 1L, 1L), SI2 = c(3L, 2L, 2L, 3L,
>>>     3L, 2L), SI3 = c(3L, 2L, 3L, 3L, 3L, 2L), SI4 = c(1L, 1L,
>>>     1L, 1L, 1L, 1L), SI5 = c(1L, 1L, 1L, 1L, 1L, 1L), SI6 = c(1L,
>>>     1L, 1L, 1L, 1L, 1L), SI7 = c(1L, 1L, 1L, 2L, 2L, 1L), SI8 = c(1L,
>>>     1L, 1L, 1L, 1L, 1L), SI9 = c(1L, 1L, 1L, 1L, 1L, 1L), SI10 = c(1L,
>>>     1L, 1L, 2L, 2L, 1L), SI11 = c(1L, 1L, 1L, 1L, 1L, 1L)), .Names =
>>> c("QCode",
>>> "PID", "RaterName", "SI1", "SI2", "SI3", "SI4", "SI5", "SI6",
>>> "SI7", "SI8", "SI9", "SI10", "SI11"), row.names = 2456:2461, class =
>>> "data.frame")
>>>
>>>
>>> I am trying to use the melt and cast functions to re-arrange to have
>>> column names QCode, PID, sItem, cwormhoudt, zpeidel.  Under each of the
>>> last two columns I want the values that correspond to each of RaterNames.
>>>
>>> So, I melt the data like this:
>>>
>>> mratings = melt(ratings, variable_name="sItem")
>>>
>>> Then cast the data like this:
>>>
>>> > outData = cast(mratings, QCode + PID + sItem ~ RaterName)
>>> Aggregation requires fun.aggregate: length used as default
>>>
>>> But the value columns appear to be displaying counts and not the
>>> original values.
>>>
>>> > dput(head(outData))
>>> structure(list(QCode = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label =
>>> c("APPEAR",
>>> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
>>> "factor"),
>>>     PID = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = c("1123",
>>>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>>>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>>>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>>>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>>>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>>>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>>>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>>>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>>>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>>>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>>>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>>>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>>>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>>>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>>>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>>>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>>>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>>>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>>>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>>>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>>>     "1556", "1558", "1559"), class = "factor"), sItem = structure(1:6,
>>> .Label = c("SI1",
>>>     "SI2", "SI3", "SI4", "SI5", "SI6", "SI7", "SI8", "SI9", "SI10",
>>>     "SI11"), class = "factor"), cwormhoudt = c(1L, 1L, 1L, 1L,
>>>     1L, 1L), zspeidel = c(1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("QCode",
>>> "PID", "sItem", "cwormhoudt", "zspeidel"), row.names = c(NA,
>>> 6L), class = "data.frame")
>>>
>>> The correct output would have 3s, 4s, and 5s in the RaterName columns:
>>>
>>> > which(outData$zpeidel==3)
>>> integer(0)
>>>
>>> I don't want it to aggregate the data, I simply want it to insert the
>>> original values into the two RaterName columns.  In cast, the fun.aggregate
>>> defaults to NULL, which I assumed was no function.  But the output says it
>>> uses length as default.
>>>
>>> How to I prevent cast from aggregating the data according to counts?  Am
>>> I doing something wrong?
>>>
>>> Thanks in advance.
>>>
>>>
>>> On Mon, Aug 31, 2015 at 11:32 PM, stephen sefick <ssefick at gmail.com>
>>> wrote:
>>>
>>>> This is very hard to read. Please use dput to provide data. I believe
>>>> the answer is in the manual. Look at the aggregation function argument.
>>>>
>>>> Please excuse my brevity; this message was sent from my telephone.
>>>> On Sep 1, 2015 12:11 AM, "Matt Pickard" <
>>>> matthew.david.pickard at gmail.com> wrote:
>>>>
>>>>> Hi,
>>>>>
>>>>> I have data that looks like this:
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> *> head(ratings)  QCode  PID  RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7
>>>>> SI8 SI9
>>>>> SI10 SI111 GUILT 1123 cwormhoudt   2   2   3   1   1   1   3   3   3
>>>>> 2    12  LOVE 1123 cwormhoudt   1   2   3   2   1   1   1   1   1    1
>>>>> 33 GUILT 1136 cwormhoudt   1   2   3   1   1   1   2   3   2    2    14
>>>>> LOVE 1136 cwormhoudt   1   2   3   1   1   1   1   1   1    1    25
>>>>> GUILT
>>>>> 1137 cwormhoudt   2   2   2   1   1   1   2   3   1    2    16  LOVE
>>>>> 1137
>>>>> cwormhoudt   1   3   4   1   1   1   1   1   1    1    4*
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> *> tail(ratings)      QCode  PID RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7
>>>>> SI8
>>>>> SI9 SI10 SI112456    FUN 1555  zspeidel   1   3   3   1   1   1   1   1
>>>>> 1    1    12457    FUN 1556  zspeidel   1   2   2   1   1   1   1   1
>>>>> 1    1    12458    FUN 1558  zspeidel   1   2   3   1   1   1   1   1
>>>>> 1    1    12459 APPEAR 1558  zspeidel   1   3   3   1   1   1   2   1
>>>>> 1    2    12460 APPEAR 1559  zspeidel   1   3   3   1   1   1   2   1
>>>>> 1    2    12461    FUN 1559  zspeidel   1   2   2   1   1   1   1   1
>>>>> 1    1    1*
>>>>> I am trying to use the melt and cast functions to re-arrange it to look
>>>>> like this:
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> *   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123   SI1
>>>>> 1        12 APPEAR 1123   SI2          4        13 APPEAR 1123
>>>>> SI3          1        24 APPEAR 1123   SI4          3        15 APPEAR
>>>>> 1123   SI5          1        16 APPEAR 1123   SI6          1        3*
>>>>> So, I melt the data like this:
>>>>>
>>>>>
>>>>>
>>>>> *mratings = melt(ratings, variable_name="sItem")*
>>>>> Then cast the data like this:
>>>>>
>>>>>
>>>>> *> outData = cast(mratings, QCode + PID + sItem ~ RaterName)Aggregation
>>>>> requires fun.aggregate: length used as default*
>>>>>
>>>>> But the value columns appear to be displaying counts and not the
>>>>> original
>>>>> values:
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> *> head(outData)   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123
>>>>> SI1          1        12 APPEAR 1123   SI2          1        13 APPEAR
>>>>> 1123   SI3          1        14 APPEAR 1123   SI4          1        15
>>>>> APPEAR 1123   SI5          1        16 APPEAR 1123   SI6          1
>>>>> 1> which(outData$zpeidel==3)integer(0)*
>>>>> How to I prevent cast from aggregating the data according to counts?
>>>>> Am I
>>>>> doing something wrong?
>>>>>
>>>>> Thanks in advance.
>>>>>
>>>>> MP
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>
>>
>>
>> --
>> Stephen Sefick
>> **************************************************
>> Auburn University
>> Biological Sciences
>> 331 Funchess Hall
>> Auburn, Alabama
>> 36849
>> **************************************************
>> sas0025 at auburn.edu
>> http://www.auburn.edu/~sas0025
>> **************************************************
>>
>> Let's not spend our time and resources thinking about things that are so
>> little or so large that all they really do for us is puff us up and make us
>> feel like gods.  We are mammals, and have not exhausted the annoying little
>> problems of being mammals.
>>
>>                                 -K. Mullis
>>
>> "A big computer, a complex algorithm and a long time does not equal
>> science."
>>
>>                               -Robert Gentleman
>>
>>
>


-- 
Stephen Sefick
**************************************************
Auburn University
Biological Sciences
331 Funchess Hall
Auburn, Alabama
36849
**************************************************
sas0025 at auburn.edu
http://www.auburn.edu/~sas0025
**************************************************

Let's not spend our time and resources thinking about things that are so
little or so large that all they really do for us is puff us up and make us
feel like gods.  We are mammals, and have not exhausted the annoying little
problems of being mammals.

                                -K. Mullis

"A big computer, a complex algorithm and a long time does not equal
science."

                              -Robert Gentleman

	[[alternative HTML version deleted]]


From shawinkarim at gmail.com  Tue Sep  1 15:48:33 2015
From: shawinkarim at gmail.com (shawin)
Date: Tue, 1 Sep 2015 06:48:33 -0700 (PDT)
Subject: [R] Median on second group of CSV file produce Na
In-Reply-To: <55E5A9DF.8060201@dewey.myzen.co.uk>
References: <1441049748473-4711689.post@n4.nabble.com>
	<CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
	<55E55652.4020702@dewey.myzen.co.uk>
	<CAM1GnGNQn0=5Nha-AMy70x_Rjgi5JFi7j073xYApZLUFW8GNiw@mail.gmail.com>
	<55E5A9DF.8060201@dewey.myzen.co.uk>
Message-ID: <CAPR_rLEfyWFPAynnzXjeVn6eSqnOos1RnZQunOcvgEoL7AmP5g@mail.gmail.com>

The str() out come is :

chr [1:31099, 1:8] " 9.329651" " 8.583304" " 9.284229" " 9.746094"
"10.200084" " 9.249863" " 7.979661" "11.271934" ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:8] "GSM215579.CEL" "GSM215580.CEL" "GSM215581.CEL"
"GSM215582.CEL" ...

On Tue, Sep 1, 2015 at 2:32 PM, Michael Dewey-3 [via R] <
ml-node+s789695n4711716h95 at n4.nabble.com> wrote:

> What happened when you did summary(y) or str(y)?
>
> On 01/09/2015 08:57, Navien wrote:
>
> > Dear Michael ,
> >
> > Thank you very much for your reply , kindly  please can i send you the
> > program and data please , y is shown a normal csv file no issue with it
> :(
> >
> >
> > Kind Regards
> >
> > On Tue, Sep 1, 2015 at 8:36 AM, Michael Dewey-3 [via R] <
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711716&i=0>>
> wrote:
> >
> >> Dear Shawin
> >>
> >> You probably did not get an answer because
> >> 1 - you seem to have posted in HTML which mangled your post into
> >> unreadability
> >> 2 - there seem to be lots of lines which do not do anything germane to
> >> the problem.
> >>
> >> Why not try summary(y) or str(y) before you convert it to a matrix and
> >> see what it says?
> >>
> >> On 31/08/2015 20:46, shawin wrote:
> >>
> >>> I have an issue ans i posted it , so i would like to receive a
> solution
> >>> please
> >>>
> >>> On Mon, Aug 31, 2015 at 8:35 PM, shawin [via R] <
> >>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=0>>
>
> >> wrote:
> >>>
> >>>> I have a data frame  csv file and I'm trying to calculate median for
> >> each
> >>>> group separately row by row . When I separate the data frame in two
> >> groups
> >>>> and calculate the median for each one, I am getting an NA result for
> >> the
> >>>> second group :
> >>>> the data
> >>>>     x1  x2  x3  x4  x5  x6  x7  y1  y2  y3  y4  y5  y6  y7  y8
> >>>> 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> 9.640816474
> >>>> 9.606262272   9.329651027 9.434541611 9.473922432 9.311412966
> 9.3154885
> >>>> 9.434977488 9.470895414 9.764258059
> >>>> 8.630629966 8.55831075  8.788391003 8.576231135 8.671587906
> 8.842979993
> >>>> 8.861958856 8.58330436  8.603596508 8.570129609 8.59798922
>  8.572686772
> >>>> 8.679751791 8.663950953 8.432875347
> >>>> 9.354748885 9.367668838 9.259952558 9.421538213 9.554635162
> 9.603744578
> >>>> 9.452197983 9.284228877 9.404607878 9.317737979 9.343115301
> 9.310644266
> >>>> 9.27227486  9.360337823 9.44706281
> >>>> 9.944863964 9.950427516 10.19101759 10.07350804 10.03269879
> 10.1307908
> >>>>    10.03487287 9.74609383  9.886379007 9.775472567 10.036596
> >> 9.544738458
> >>>> 9.699611598 9.911962567
> >>>> 9.625804277
> >>>>
> >>>>
> >>>>                                      Code:
> >>>>
> >>>>          rowN <- nrow(AT1)
> >>>>          MD1<-vector(length=rowN)
> >>>>         MD2<-vector(length=rowN)
> >>>>
> >>>>             MD1[1:rowN]<-NA
> >>>>             MD2[1:rowN]<-NA
> >>>>
> >>>>
> >>>>            x<- AT1[,c(2,3,4,5,6,7,8) ]
> >>>>           write.csv(x,"x.csv",row.names=TRUE)
> >>>>            x<-as.matrix(x)
> >>>>           for(i in 2:rowN) {
> >>>>          MD1[i]=median(x[i,])
> >>>>             }
> >>>>            write.csv(MD1,"MD1.csv",row.names=TRUE)
> >>>>
> >>>>            y<- AT1[,c(9,10,11,12,13,14,15,16)]
> >>>>            write.csv(y,"y.csv",row.names=TRUE)
> >>>>            y<-as.matrix(y)
> >>>>            for(j in 2:rowN) {
> >>>>            MD2[j]=median(y[j,])
> >>>>             }
> >>>>            write.csv(MD2,"MD2.csv",row.names=TRUE)
> >>>>
> >>>>
> >>>> ------------------------------
> >>>> If you reply to this email, your message will be added to the
> >> discussion
> >>>> below:
> >>>>
> >>>>
> >>
> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689.html
> >>>> To start a new topic under R help, email
> >>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=1>
>
> >>>> To unsubscribe from R, click here
> >>>> <
> >>>> .
> >>>> NAML
> >>>> <
> >>
> http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>
> >>
> >>>>
> >>>
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
> >>
> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711690.html
> >>> Sent from the R help mailing list archive at Nabble.com.
> >>> [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=2>
>
> >> mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >> --
> >> Michael
> >> http://www.dewey.myzen.co.uk/home.html
> >>
> >> ______________________________________________
> >> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=3>
> >> mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >> ------------------------------
> >> If you reply to this email, your message will be added to the
> discussion
> >> below:
> >>
> >>
> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711707.html
> >> To start a new topic under R help, email
> >> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711716&i=1>
> >> To unsubscribe from R, click here
> >> <
> >> .
> >> NAML
> >> <
> http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>
> >>
> >
> >
> >
> >
> > --
> > View this message in context:
> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711709.html
> > Sent from the R help mailing list archive at Nabble.com.
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711716&i=2>
> mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
> ______________________________________________
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711716&i=3>
> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711716.html
> To start a new topic under R help, email
> ml-node+s789695n789696h75 at n4.nabble.com
> To unsubscribe from R, click here
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
> .
> NAML
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711718.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From axionator at gmail.com  Tue Sep  1 15:54:33 2015
From: axionator at gmail.com (axionator)
Date: Tue, 1 Sep 2015 15:54:33 +0200
Subject: [R] automatically calling dev.off()
In-Reply-To: <35CDE9DD-1474-4A34-92D8-F86962FE121E@dcn.davis.CA.us>
References: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
	<35CDE9DD-1474-4A34-92D8-F86962FE121E@dcn.davis.CA.us>
Message-ID: <CAGopM691A9UHfwG7fOFEMjDE2civ7c2cdXCjhcrxii--6pexXg@mail.gmail.com>

I did it on the console only for testing. I have a program that uses Rserve
and would like to have it therefore. (the program basically provides a
terminal where you can type in R commands. Since plotting is only possible
via files (when using Rserve) that are then displayed in a widget, I like
to avoid the overhead of writing png() ... dev.off()  for each plot.)

On Tue, Sep 1, 2015 at 3:42 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Don't use options to do this in batch mode. Open and close the file as you
> make the plot.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 1, 2015 4:34:41 AM PDT, axionator <axionator at gmail.com>
> wrote:
> >Hi,
> >is there a way to automatically call dev.off()?
> >I use options(device="myfunc") to automatically open a device to print
> >to.
> >Afterwards, I would like to close it (so that the file is actually
> >written).
> >I tried to do it via addTaskCallback, but unfortunately, I have to use
> >Rserve and (for any reason), the callback does not work (in the default
> >R
> >console on win7 it works, however).
> >
> >So, the following runs in the R console (but not via Rserve):
> >
> >mypng <- function(filename = "test.png", ...)
> >{
> >    times <- function(total = 2, str = "Task") {
> >        ctr <- 0
> >        function(expr, value, ok, visible) {
> >            ctr <<- ctr + 1
> >            keep.me <- (ctr < total)
> >            if (ctr == total) {
> >                cat("ENDPRINT")
> >                dev.off()
> >            }
> >            keep.me
> >        }
> >    }
> >    png(filename, ...)
> >    n <- addTaskCallback(times(1))
> >}
> >options(device="mypng")
> >plot(rnorm(333))
> >
> >Any hints?
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From axionator at gmail.com  Tue Sep  1 16:13:16 2015
From: axionator at gmail.com (axionator)
Date: Tue, 1 Sep 2015 16:13:16 +0200
Subject: [R] automatically calling dev.off()
In-Reply-To: <55E5B065.60806@gmail.com>
References: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
	<55E5B065.60806@gmail.com>
Message-ID: <CAGopM68UZoPBwsXL31kPE1zLNqcwYxaOiyWq91qFC-Km0=BDuw@mail.gmail.com>

Yes, the user could add something. Here, In about 90% of the time, he
won't. So it would be nice to have a dev.off() as default (and some extra
handling if more plot commands will follow).

On Tue, Sep 1, 2015 at 4:04 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 01/09/2015 7:34 AM, axionator wrote:
> > Hi,
> > is there a way to automatically call dev.off()?
> > I use options(device="myfunc") to automatically open a device to print
> to.
> > Afterwards, I would like to close it (so that the file is actually
> written).
> > I tried to do it via addTaskCallback, but unfortunately, I have to use
> > Rserve and (for any reason), the callback does not work (in the default R
> > console on win7 it works, however).
> >
> > So, the following runs in the R console (but not via Rserve):
> >
> > mypng <- function(filename = "test.png", ...)
> > {
> >     times <- function(total = 2, str = "Task") {
> >         ctr <- 0
> >         function(expr, value, ok, visible) {
> >             ctr <<- ctr + 1
> >             keep.me <- (ctr < total)
> >             if (ctr == total) {
> >                 cat("ENDPRINT")
> >                 dev.off()
> >             }
> >             keep.me
> >         }
> >     }
> >     png(filename, ...)
> >     n <- addTaskCallback(times(1))
> > }
> > options(device="mypng")
> > plot(rnorm(333))
> >
>
> It wouldn't make sense to call dev.off() after the plot() command,
> because the user might be planning to add something to it.  You need to
> tell R that you are done, and that's what dev.off() is for.
>
> You can call it automatically in a function by using
>
> on.exit(dev.off())
>
> and it should happen automatically at the end of an R session, but it
> *shouldn't* happen after every plotting call.
>
> Duncan Murdoch
>
>

	[[alternative HTML version deleted]]


From axionator at gmail.com  Tue Sep  1 17:36:34 2015
From: axionator at gmail.com (axionator)
Date: Tue, 1 Sep 2015 17:36:34 +0200
Subject: [R] automatically calling dev.off()
In-Reply-To: <55E5BD84.2040906@gmail.com>
References: <CAGopM6_85hQs+VT8uoJnUsdv5MRsChfL2FbSQQdFQn24yaSa3A@mail.gmail.com>
	<55E5B065.60806@gmail.com>
	<CAGopM68UZoPBwsXL31kPE1zLNqcwYxaOiyWq91qFC-Km0=BDuw@mail.gmail.com>
	<55E5BD84.2040906@gmail.com>
Message-ID: <CAGopM68Kp83i=hfB6=gx4Ld4F+jQMN6FLdQne=mkiL8Jip0Wxw@mail.gmail.com>

Yes, it's a different model, but maybe my main problem is that Rserve does
not handle addTaskCallback as in the console (and thus no dev.off() is
called).

On Tue, Sep 1, 2015 at 5:00 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 01/09/2015 10:13 AM, axionator wrote:
> > Yes, the user could add something. Here, In about 90% of the time, he
> > won't. So it would be nice to have a dev.off() as default (and some
> > extra handling if more plot commands will follow).
>
> That's a different mental model of how graphics should work, and it's
> not compatible with standard R graphics.  However, it would be pretty
> easy to get this to work with a grid graphics based systems (e.g.
> ggplot2, lattice) since they don't draw anything until you print the
> object:  you'd just need to override the print method.
>
> Duncan Murdoch
>
>
> >
> > On Tue, Sep 1, 2015 at 4:04 PM, Duncan Murdoch <murdoch.duncan at gmail.com
> > <mailto:murdoch.duncan at gmail.com>> wrote:
> >
> >     On 01/09/2015 7:34 AM, axionator wrote:
> >     > Hi,
> >     > is there a way to automatically call dev.off()?
> >     > I use options(device="myfunc") to automatically open a device to
> print to.
> >     > Afterwards, I would like to close it (so that the file is actually
> written).
> >     > I tried to do it via addTaskCallback, but unfortunately, I have to
> use
> >     > Rserve and (for any reason), the callback does not work (in the
> default R
> >     > console on win7 it works, however).
> >     >
> >     > So, the following runs in the R console (but not via Rserve):
> >     >
> >     > mypng <- function(filename = "test.png", ...)
> >     > {
> >     >     times <- function(total = 2, str = "Task") {
> >     >         ctr <- 0
> >     >         function(expr, value, ok, visible) {
> >     >             ctr <<- ctr + 1
> >     >             keep.me <http://keep.me> <- (ctr < total)
> >     >             if (ctr == total) {
> >     >                 cat("ENDPRINT")
> >     >                 dev.off()
> >     >             }
> >     >             keep.me <http://keep.me>
> >     >         }
> >     >     }
> >     >     png(filename, ...)
> >     >     n <- addTaskCallback(times(1))
> >     > }
> >     > options(device="mypng")
> >     > plot(rnorm(333))
> >     >
> >
> >     It wouldn't make sense to call dev.off() after the plot() command,
> >     because the user might be planning to add something to it.  You need
> to
> >     tell R that you are done, and that's what dev.off() is for.
> >
> >     You can call it automatically in a function by using
> >
> >     on.exit(dev.off())
> >
> >     and it should happen automatically at the end of an R session, but it
> >     *shouldn't* happen after every plotting call.
> >
> >     Duncan Murdoch
> >
> >
>
>

	[[alternative HTML version deleted]]


From matthew.david.pickard at gmail.com  Tue Sep  1 17:44:12 2015
From: matthew.david.pickard at gmail.com (Matthew Pickard)
Date: Tue, 1 Sep 2015 09:44:12 -0600
Subject: [R] reshape: melt and cast
In-Reply-To: <CADKEMqjEkyctgXGckgjr6wZ1CLj3t69cUHXHFQREoqO0y__3TQ@mail.gmail.com>
References: <CANbrOWd0mGGatb=9WY8PfWPYJXPCGcL5AG-B=+=vU1+P4pLTHQ@mail.gmail.com>
	<CADKEMqi1QSuvN7ZZAPHnX5RsRzUJ9-GE-0R1yVALJHH43rBGhA@mail.gmail.com>
	<CAC_TuLpX5fJA3EoMHetyz_5Jn=MZY_4_ywB60YRt5mS1a2LMgg@mail.gmail.com>
	<CADKEMqjEkyctgXGckgjr6wZ1CLj3t69cUHXHFQREoqO0y__3TQ@mail.gmail.com>
Message-ID: <CAC_TuLrr2WHRDL3F2hOtTDXp+9QOMh4u8W-DJxLxWCoMjFvFjQ@mail.gmail.com>

Yep, that works. Thanks, Stephen. I should have drawn the parallel with
Excel Pivot tables sooner.

On Tue, Sep 1, 2015 at 9:36 AM, stephen sefick <ssefick at gmail.com> wrote:

> I would make this minimal. In other words, use an example data set, dput,
> and use output of dput in a block of reproducible code. I don't understand
> exactly what you want, but does sum work? If there is more than one record
> for a given set of factors the sum is the sum of the counts. If only one
> record, then the sum is the same as the original number.
>
> On Tue, Sep 1, 2015 at 10:00 AM, Matthew Pickard <
> matthew.david.pickard at gmail.com> wrote:
>
>> Thanks, Stephen. I've looked into the fun.aggregate argument. I don't
>> want to aggregate, so I thought leaving it blank (allowing it to default to
>> NULL) would do that.
>>
>>
>> Here's a corrected post (with further explanation):
>>
>> Hi,
>>
>> I have data that looks like this:
>>
>> >dput(head(ratings))
>> structure(list(QCode = structure(c(5L, 7L, 5L, 7L, 5L, 7L), .Label =
>> c("APPEAR",
>> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
>> "factor"),
>>     PID = structure(c(1L, 1L, 2L, 2L, 3L, 3L), .Label = c("1123",
>>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>>     "1556", "1558", "1559"), class = "factor"), RaterName =
>> structure(c(1L,
>>     1L, 1L, 1L, 1L, 1L), .Label = c("cwormhoudt", "zspeidel"), class =
>> "factor"),
>>     SI1 = c(2L, 1L, 1L, 1L, 2L, 1L), SI2 = c(2L, 2L, 2L, 2L,
>>     2L, 3L), SI3 = c(3L, 3L, 3L, 3L, 2L, 4L), SI4 = c(1L, 2L,
>>     1L, 1L, 1L, 1L), SI5 = c(1L, 1L, 1L, 1L, 1L, 1L), SI6 = c(1L,
>>     1L, 1L, 1L, 1L, 1L), SI7 = c(3L, 1L, 2L, 1L, 2L, 1L), SI8 = c(3L,
>>     1L, 3L, 1L, 3L, 1L), SI9 = c(3L, 1L, 2L, 1L, 1L, 1L), SI10 = c(2L,
>>     1L, 2L, 1L, 2L, 1L), SI11 = c(1L, 3L, 1L, 2L, 1L, 4L)), .Names =
>> c("QCode",
>> "PID", "RaterName", "SI1", "SI2", "SI3", "SI4", "SI5", "SI6",
>> "SI7", "SI8", "SI9", "SI10", "SI11"), row.names = c(NA, 6L), class =
>> "data.frame")
>>
>>
>> > dput(tail(ratings))
>> structure(list(QCode = structure(c(3L, 3L, 3L, 1L, 1L, 3L), .Label =
>> c("APPEAR",
>> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
>> "factor"),
>>     PID = structure(c(161L, 162L, 163L, 163L, 164L, 164L), .Label =
>> c("1123",
>>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>>     "1556", "1558", "1559"), class = "factor"), RaterName =
>> structure(c(2L,
>>     2L, 2L, 2L, 2L, 2L), .Label = c("cwormhoudt", "zspeidel"), class =
>> "factor"),
>>     SI1 = c(1L, 1L, 1L, 1L, 1L, 1L), SI2 = c(3L, 2L, 2L, 3L,
>>     3L, 2L), SI3 = c(3L, 2L, 3L, 3L, 3L, 2L), SI4 = c(1L, 1L,
>>     1L, 1L, 1L, 1L), SI5 = c(1L, 1L, 1L, 1L, 1L, 1L), SI6 = c(1L,
>>     1L, 1L, 1L, 1L, 1L), SI7 = c(1L, 1L, 1L, 2L, 2L, 1L), SI8 = c(1L,
>>     1L, 1L, 1L, 1L, 1L), SI9 = c(1L, 1L, 1L, 1L, 1L, 1L), SI10 = c(1L,
>>     1L, 1L, 2L, 2L, 1L), SI11 = c(1L, 1L, 1L, 1L, 1L, 1L)), .Names =
>> c("QCode",
>> "PID", "RaterName", "SI1", "SI2", "SI3", "SI4", "SI5", "SI6",
>> "SI7", "SI8", "SI9", "SI10", "SI11"), row.names = 2456:2461, class =
>> "data.frame")
>>
>>
>> I am trying to use the melt and cast functions to re-arrange to have
>> column names QCode, PID, sItem, cwormhoudt, zpeidel.  Under each of the
>> last two columns I want the values that correspond to each of RaterNames.
>>
>> So, I melt the data like this:
>>
>> mratings = melt(ratings, variable_name="sItem")
>>
>> Then cast the data like this:
>>
>> > outData = cast(mratings, QCode + PID + sItem ~ RaterName)
>> Aggregation requires fun.aggregate: length used as default
>>
>> But the value columns appear to be displaying counts and not the original
>> values.
>>
>> > dput(head(outData))
>> structure(list(QCode = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label =
>> c("APPEAR",
>> "FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
>> "factor"),
>>     PID = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = c("1123",
>>     "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
>>     "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
>>     "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
>>     "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
>>     "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
>>     "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
>>     "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
>>     "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
>>     "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
>>     "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
>>     "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
>>     "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
>>     "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
>>     "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
>>     "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
>>     "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
>>     "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
>>     "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
>>     "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
>>     "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
>>     "1556", "1558", "1559"), class = "factor"), sItem = structure(1:6,
>> .Label = c("SI1",
>>     "SI2", "SI3", "SI4", "SI5", "SI6", "SI7", "SI8", "SI9", "SI10",
>>     "SI11"), class = "factor"), cwormhoudt = c(1L, 1L, 1L, 1L,
>>     1L, 1L), zspeidel = c(1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("QCode",
>> "PID", "sItem", "cwormhoudt", "zspeidel"), row.names = c(NA,
>> 6L), class = "data.frame")
>>
>> The correct output would have 3s, 4s, and 5s in the RaterName columns:
>>
>> > which(outData$zpeidel==3)
>> integer(0)
>>
>> I don't want it to aggregate the data, I simply want it to insert the
>> original values into the two RaterName columns.  In cast, the fun.aggregate
>> defaults to NULL, which I assumed was no function.  But the output says it
>> uses length as default.
>>
>> How to I prevent cast from aggregating the data according to counts?  Am
>> I doing something wrong?
>>
>> Thanks in advance.
>>
>>
>> On Mon, Aug 31, 2015 at 11:32 PM, stephen sefick <ssefick at gmail.com>
>> wrote:
>>
>>> This is very hard to read. Please use dput to provide data. I believe
>>> the answer is in the manual. Look at the aggregation function argument.
>>>
>>> Please excuse my brevity; this message was sent from my telephone.
>>> On Sep 1, 2015 12:11 AM, "Matt Pickard" <matthew.david.pickard at gmail.com>
>>> wrote:
>>>
>>>> Hi,
>>>>
>>>> I have data that looks like this:
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> *> head(ratings)  QCode  PID  RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7 SI8
>>>> SI9
>>>> SI10 SI111 GUILT 1123 cwormhoudt   2   2   3   1   1   1   3   3   3
>>>> 2    12  LOVE 1123 cwormhoudt   1   2   3   2   1   1   1   1   1    1
>>>> 33 GUILT 1136 cwormhoudt   1   2   3   1   1   1   2   3   2    2    14
>>>> LOVE 1136 cwormhoudt   1   2   3   1   1   1   1   1   1    1    25
>>>> GUILT
>>>> 1137 cwormhoudt   2   2   2   1   1   1   2   3   1    2    16  LOVE
>>>> 1137
>>>> cwormhoudt   1   3   4   1   1   1   1   1   1    1    4*
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> *> tail(ratings)      QCode  PID RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7
>>>> SI8
>>>> SI9 SI10 SI112456    FUN 1555  zspeidel   1   3   3   1   1   1   1   1
>>>> 1    1    12457    FUN 1556  zspeidel   1   2   2   1   1   1   1   1
>>>> 1    1    12458    FUN 1558  zspeidel   1   2   3   1   1   1   1   1
>>>> 1    1    12459 APPEAR 1558  zspeidel   1   3   3   1   1   1   2   1
>>>> 1    2    12460 APPEAR 1559  zspeidel   1   3   3   1   1   1   2   1
>>>> 1    2    12461    FUN 1559  zspeidel   1   2   2   1   1   1   1   1
>>>> 1    1    1*
>>>> I am trying to use the melt and cast functions to re-arrange it to look
>>>> like this:
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> *   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123   SI1
>>>> 1        12 APPEAR 1123   SI2          4        13 APPEAR 1123
>>>> SI3          1        24 APPEAR 1123   SI4          3        15 APPEAR
>>>> 1123   SI5          1        16 APPEAR 1123   SI6          1        3*
>>>> So, I melt the data like this:
>>>>
>>>>
>>>>
>>>> *mratings = melt(ratings, variable_name="sItem")*
>>>> Then cast the data like this:
>>>>
>>>>
>>>> *> outData = cast(mratings, QCode + PID + sItem ~ RaterName)Aggregation
>>>> requires fun.aggregate: length used as default*
>>>>
>>>> But the value columns appear to be displaying counts and not the
>>>> original
>>>> values:
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> *> head(outData)   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123
>>>> SI1          1        12 APPEAR 1123   SI2          1        13 APPEAR
>>>> 1123   SI3          1        14 APPEAR 1123   SI4          1        15
>>>> APPEAR 1123   SI5          1        16 APPEAR 1123   SI6          1
>>>> 1> which(outData$zpeidel==3)integer(0)*
>>>> How to I prevent cast from aggregating the data according to counts?
>>>> Am I
>>>> doing something wrong?
>>>>
>>>> Thanks in advance.
>>>>
>>>> MP
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>
>
>
> --
> Stephen Sefick
> **************************************************
> Auburn University
> Biological Sciences
> 331 Funchess Hall
> Auburn, Alabama
> 36849
> **************************************************
> sas0025 at auburn.edu
> http://www.auburn.edu/~sas0025
> **************************************************
>
> Let's not spend our time and resources thinking about things that are so
> little or so large that all they really do for us is puff us up and make us
> feel like gods.  We are mammals, and have not exhausted the annoying little
> problems of being mammals.
>
>                                 -K. Mullis
>
> "A big computer, a complex algorithm and a long time does not equal
> science."
>
>                               -Robert Gentleman
>
>

	[[alternative HTML version deleted]]


From matthew.david.pickard at gmail.com  Tue Sep  1 17:04:15 2015
From: matthew.david.pickard at gmail.com (Matt Pickard)
Date: Tue, 1 Sep 2015 09:04:15 -0600
Subject: [R] reshape: melt and cast
In-Reply-To: <CANbrOWd0mGGatb=9WY8PfWPYJXPCGcL5AG-B=+=vU1+P4pLTHQ@mail.gmail.com>
References: <CANbrOWd0mGGatb=9WY8PfWPYJXPCGcL5AG-B=+=vU1+P4pLTHQ@mail.gmail.com>
Message-ID: <CANbrOWcMcbUo-79Yf3aWNEFq+m7b9320rb8GX1QkZhaxN18P4w@mail.gmail.com>

Thanks, Stephen. I've looked into the fun.aggregate argument. I don't want
to aggregate, so I thought leaving it blank (allowing it to default to
NULL) would do that.


Here's a corrected post (with further explanation):

Hi,

I have data that looks like this:

>dput(head(ratings))
structure(list(QCode = structure(c(5L, 7L, 5L, 7L, 5L, 7L), .Label =
c("APPEAR",
"FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
"factor"),
    PID = structure(c(1L, 1L, 2L, 2L, 3L, 3L), .Label = c("1123",
    "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
    "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
    "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
    "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
    "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
    "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
    "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
    "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
    "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
    "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
    "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
    "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
    "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
    "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
    "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
    "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
    "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
    "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
    "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
    "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
    "1556", "1558", "1559"), class = "factor"), RaterName = structure(c(1L,
    1L, 1L, 1L, 1L, 1L), .Label = c("cwormhoudt", "zspeidel"), class =
"factor"),
    SI1 = c(2L, 1L, 1L, 1L, 2L, 1L), SI2 = c(2L, 2L, 2L, 2L,
    2L, 3L), SI3 = c(3L, 3L, 3L, 3L, 2L, 4L), SI4 = c(1L, 2L,
    1L, 1L, 1L, 1L), SI5 = c(1L, 1L, 1L, 1L, 1L, 1L), SI6 = c(1L,
    1L, 1L, 1L, 1L, 1L), SI7 = c(3L, 1L, 2L, 1L, 2L, 1L), SI8 = c(3L,
    1L, 3L, 1L, 3L, 1L), SI9 = c(3L, 1L, 2L, 1L, 1L, 1L), SI10 = c(2L,
    1L, 2L, 1L, 2L, 1L), SI11 = c(1L, 3L, 1L, 2L, 1L, 4L)), .Names =
c("QCode",
"PID", "RaterName", "SI1", "SI2", "SI3", "SI4", "SI5", "SI6",
"SI7", "SI8", "SI9", "SI10", "SI11"), row.names = c(NA, 6L), class =
"data.frame")


> dput(tail(ratings))
structure(list(QCode = structure(c(3L, 3L, 3L, 1L, 1L, 3L), .Label =
c("APPEAR",
"FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
"factor"),
    PID = structure(c(161L, 162L, 163L, 163L, 164L, 164L), .Label =
c("1123",
    "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
    "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
    "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
    "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
    "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
    "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
    "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
    "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
    "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
    "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
    "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
    "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
    "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
    "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
    "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
    "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
    "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
    "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
    "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
    "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
    "1556", "1558", "1559"), class = "factor"), RaterName = structure(c(2L,
    2L, 2L, 2L, 2L, 2L), .Label = c("cwormhoudt", "zspeidel"), class =
"factor"),
    SI1 = c(1L, 1L, 1L, 1L, 1L, 1L), SI2 = c(3L, 2L, 2L, 3L,
    3L, 2L), SI3 = c(3L, 2L, 3L, 3L, 3L, 2L), SI4 = c(1L, 1L,
    1L, 1L, 1L, 1L), SI5 = c(1L, 1L, 1L, 1L, 1L, 1L), SI6 = c(1L,
    1L, 1L, 1L, 1L, 1L), SI7 = c(1L, 1L, 1L, 2L, 2L, 1L), SI8 = c(1L,
    1L, 1L, 1L, 1L, 1L), SI9 = c(1L, 1L, 1L, 1L, 1L, 1L), SI10 = c(1L,
    1L, 1L, 2L, 2L, 1L), SI11 = c(1L, 1L, 1L, 1L, 1L, 1L)), .Names =
c("QCode",
"PID", "RaterName", "SI1", "SI2", "SI3", "SI4", "SI5", "SI6",
"SI7", "SI8", "SI9", "SI10", "SI11"), row.names = 2456:2461, class =
"data.frame")


I am trying to use the melt and cast functions to re-arrange to have column
names QCode, PID, sItem, cwormhoudt, zpeidel.  Under each of the last two
columns I want the values that correspond to each of RaterNames.

So, I melt the data like this:

mratings = melt(ratings, variable_name="sItem")

Then cast the data like this:

> outData = cast(mratings, QCode + PID + sItem ~ RaterName)
Aggregation requires fun.aggregate: length used as default

But the value columns appear to be displaying counts and not the original
values.

> dput(head(outData))
structure(list(QCode = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label =
c("APPEAR",
"FEAR", "FUN", "GRAT", "GUILT", "Joy", "LOVE", "UNGRAT"), class =
"factor"),
    PID = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = c("1123",
    "1136", "1137", "1142", "1146", "1147", "1148", "1149", "1152",
    "1153", "1154", "1156", "1158", "1161", "1164", "1179", "1182",
    "1183", "1191", "1196", "1197", "1198", "1199", "1200", "1201",
    "1203", "1205", "1207", "1208", "1209", "1214", "1216", "1219",
    "1220", "1222", "1223", "1224", "1225", "1226", "1229", "1236",
    "1237", "1238", "1240", "1241", "1243", "1245", "1246", "1248",
    "1254", "1255", "1256", "1257", "1260", "1262", "1264", "1268",
    "1270", "1272", "1278", "1279", "1280", "1282", "1283", "1287",
    "1288", "1292", "1293", "1297", "1310", "1311", "1315", "1329",
    "1332", "1333", "1343", "1346", "1347", "1352", "1354", "1355",
    "1356", "1360", "1368", "1369", "1370", "1378", "1398", "1400",
    "1403", "1404", "1411", "1412", "1420", "1421", "1423", "1424",
    "1426", "1428", "1432", "1433", "1435", "1436", "1438", "1439",
    "1440", "1441", "1443", "1444", "1446", "1447", "1448", "1449",
    "1450", "1453", "1454", "1456", "1459", "1460", "1461", "1462",
    "1463", "1468", "1471", "1475", "1478", "1481", "1482", "1487",
    "1488", "1490", "1493", "1495", "1497", "1503", "1504", "1508",
    "1509", "1511", "1513", "1514", "1515", "1522", "1524", "1525",
    "1526", "1527", "1528", "1529", "1532", "1534", "1536", "1538",
    "1539", "1540", "1543", "1550", "1551", "1552", "1554", "1555",
    "1556", "1558", "1559"), class = "factor"), sItem = structure(1:6,
.Label = c("SI1",
    "SI2", "SI3", "SI4", "SI5", "SI6", "SI7", "SI8", "SI9", "SI10",
    "SI11"), class = "factor"), cwormhoudt = c(1L, 1L, 1L, 1L,
    1L, 1L), zspeidel = c(1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("QCode",
"PID", "sItem", "cwormhoudt", "zspeidel"), row.names = c(NA,
6L), class = "data.frame")

The correct output would have 3s, 4s, and 5s in the RaterName columns:

> which(outData$zpeidel==3)
integer(0)

I don't want it to aggregate the data, I simply want it to insert the
original values into the two RaterName columns.  In cast, the fun.aggregate
defaults to NULL, which I assumed was no function.  But the output says it
uses length as default.

On Mon, Aug 31, 2015 at 10:14 PM, Matt Pickard <
matthew.david.pickard at gmail.com> wrote:

> Hi,
>
> I have data that looks like this:
>
>
>
>
>
>
>
>
>
> *> head(ratings)  QCode  PID  RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7 SI8
> SI9 SI10 SI111 GUILT 1123 cwormhoudt   2   2   3   1   1   1   3   3   3
> 2    12  LOVE 1123 cwormhoudt   1   2   3   2   1   1   1   1   1    1
> 33 GUILT 1136 cwormhoudt   1   2   3   1   1   1   2   3   2    2    14
> LOVE 1136 cwormhoudt   1   2   3   1   1   1   1   1   1    1    25 GUILT
> 1137 cwormhoudt   2   2   2   1   1   1   2   3   1    2    16  LOVE 1137
> cwormhoudt   1   3   4   1   1   1   1   1   1    1    4*
>
>
>
>
>
>
>
>
> *> tail(ratings)      QCode  PID RaterName SI1 SI2 SI3 SI4 SI5 SI6 SI7 SI8
> SI9 SI10 SI112456    FUN 1555  zspeidel   1   3   3   1   1   1   1   1
> 1    1    12457    FUN 1556  zspeidel   1   2   2   1   1   1   1   1
> 1    1    12458    FUN 1558  zspeidel   1   2   3   1   1   1   1   1
> 1    1    12459 APPEAR 1558  zspeidel   1   3   3   1   1   1   2   1
> 1    2    12460 APPEAR 1559  zspeidel   1   3   3   1   1   1   2   1
> 1    2    12461    FUN 1559  zspeidel   1   2   2   1   1   1   1   1
> 1    1    1*
> I am trying to use the melt and cast functions to re-arrange it to look
> like this:
>
>
>
>
>
>
>
>
> *   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123   SI1
> 1        12 APPEAR 1123   SI2          4        13 APPEAR 1123
> SI3          1        24 APPEAR 1123   SI4          3        15 APPEAR
> 1123   SI5          1        16 APPEAR 1123   SI6          1        3*
> So, I melt the data like this:
>
>
>
> *mratings = melt(ratings, variable_name="sItem")*
> Then cast the data like this:
>
>
> *> outData = cast(mratings, QCode + PID + sItem ~ RaterName)Aggregation
> requires fun.aggregate: length used as default*
>
> But the value columns appear to be displaying counts and not the original
> values:
>
>
>
>
>
>
>
>
>
>
>
>
>
> *> head(outData)   QCode  PID sItem cwormhoudt zspeidel1 APPEAR 1123
> SI1          1        12 APPEAR 1123   SI2          1        13 APPEAR
> 1123   SI3          1        14 APPEAR 1123   SI4          1        15
> APPEAR 1123   SI5          1        16 APPEAR 1123   SI6          1
> 1> which(outData$zpeidel==3)integer(0)*
> How to I prevent cast from aggregating the data according to counts?  Am I
> doing something wrong?
>
> Thanks in advance.
>
> MP
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Sep  1 20:12:41 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 1 Sep 2015 11:12:41 -0700
Subject: [R] Median on second group of CSV file produce Na
In-Reply-To: <CAPR_rLEfyWFPAynnzXjeVn6eSqnOos1RnZQunOcvgEoL7AmP5g@mail.gmail.com>
References: <1441049748473-4711689.post@n4.nabble.com>
	<CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
	<55E55652.4020702@dewey.myzen.co.uk>
	<CAM1GnGNQn0=5Nha-AMy70x_Rjgi5JFi7j073xYApZLUFW8GNiw@mail.gmail.com>
	<55E5A9DF.8060201@dewey.myzen.co.uk>
	<CAPR_rLEfyWFPAynnzXjeVn6eSqnOos1RnZQunOcvgEoL7AmP5g@mail.gmail.com>
Message-ID: <3FD0145C-F61D-42F6-967A-C4615EF32097@comcast.net>


On Sep 1, 2015, at 6:48 AM, shawin wrote:

> The str() out come is :
> 
> chr [1:31099, 1:8] " 9.329651" " 8.583304" " 9.284229" " 9.746094"
> "10.200084" " 9.249863" " 7.979661" "11.271934" ...
> - attr(*, "dimnames")=List of 2
>  ..$ : NULL
>  ..$ : chr [1:8] "GSM215579.CEL" "GSM215580.CEL" "GSM215581.CEL"
> "GSM215582.CEL" ...
> 

So the matrix you thought was numeric is actually a character-object. The median function returns NA for character arguments. I'm seeing spaces in front of the decimal numbers so appears your input statements need to be somehow adjusted, or you need to coerce the matrix to numeric. The colClasses argument to read.table and cousins should be used more often.

-- 
David.


> On Tue, Sep 1, 2015 at 2:32 PM, Michael Dewey-3 [via R] <
> ml-node+s789695n4711716h95 at n4.nabble.com> wrote:
> 
>> What happened when you did summary(y) or str(y)?
>> 
>> On 01/09/2015 08:57, Navien wrote:
>> 
>>> Dear Michael ,
>>> 
>>> Thank you very much for your reply , kindly  please can i send you the
>>> program and data please , y is shown a normal csv file no issue with it
>> :(
>>> 
>>> 
>>> Kind Regards
>>> 
>>> On Tue, Sep 1, 2015 at 8:36 AM, Michael Dewey-3 [via R] <
>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711716&i=0>>
>> wrote:
>>> 
>>>> Dear Shawin
>>>> 
>>>> You probably did not get an answer because
>>>> 1 - you seem to have posted in HTML which mangled your post into
>>>> unreadability
>>>> 2 - there seem to be lots of lines which do not do anything germane to
>>>> the problem.
>>>> 
>>>> Why not try summary(y) or str(y) before you convert it to a matrix and
>>>> see what it says?
>>>> 
>>>> On 31/08/2015 20:46, shawin wrote:
>>>> 
>>>>> I have an issue ans i posted it , so i would like to receive a
>> solution
>>>>> please
>>>>> 
>>>>> On Mon, Aug 31, 2015 at 8:35 PM, shawin [via R] <
>>>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=0>>
>> 
>>>> wrote:
>>>>> 
>>>>>> I have a data frame  csv file and I'm trying to calculate median for
>>>> each
>>>>>> group separately row by row . When I separate the data frame in two
>>>> groups
>>>>>> and calculate the median for each one, I am getting an NA result for
>>>> the
>>>>>> second group :
>>>>>> the data
>>>>>>    x1  x2  x3  x4  x5  x6  x7  y1  y2  y3  y4  y5  y6  y7  y8
>>>>>> 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
>> 9.640816474
>>>>>> 9.606262272   9.329651027 9.434541611 9.473922432 9.311412966
>> 9.3154885
>>>>>> 9.434977488 9.470895414 9.764258059
>>>>>> 8.630629966 8.55831075  8.788391003 8.576231135 8.671587906
>> 8.842979993
>>>>>> 8.861958856 8.58330436  8.603596508 8.570129609 8.59798922
>> 8.572686772
>>>>>> 8.679751791 8.663950953 8.432875347
>>>>>> 9.354748885 9.367668838 9.259952558 9.421538213 9.554635162
>> 9.603744578
>>>>>> 9.452197983 9.284228877 9.404607878 9.317737979 9.343115301
>> 9.310644266
>>>>>> 9.27227486  9.360337823 9.44706281
>>>>>> 9.944863964 9.950427516 10.19101759 10.07350804 10.03269879
>> 10.1307908
>>>>>>   10.03487287 9.74609383  9.886379007 9.775472567 10.036596
>>>> 9.544738458
>>>>>> 9.699611598 9.911962567
>>>>>> 9.625804277
>>>>>> 
>>>>>> 
>>>>>>                                     Code:
>>>>>> 
>>>>>>         rowN <- nrow(AT1)
>>>>>>         MD1<-vector(length=rowN)
>>>>>>        MD2<-vector(length=rowN)
>>>>>> 
>>>>>>            MD1[1:rowN]<-NA
>>>>>>            MD2[1:rowN]<-NA
>>>>>> 
>>>>>> 
>>>>>>           x<- AT1[,c(2,3,4,5,6,7,8) ]
>>>>>>          write.csv(x,"x.csv",row.names=TRUE)
>>>>>>           x<-as.matrix(x)
>>>>>>          for(i in 2:rowN) {
>>>>>>         MD1[i]=median(x[i,])
>>>>>>            }
>>>>>>           write.csv(MD1,"MD1.csv",row.names=TRUE)
>>>>>> 
>>>>>>           y<- AT1[,c(9,10,11,12,13,14,15,16)]
>>>>>>           write.csv(y,"y.csv",row.names=TRUE)
>>>>>>           y<-as.matrix(y)
>>>>>>           for(j in 2:rowN) {
>>>>>>           MD2[j]=median(y[j,])
>>>>>>            }
>>>>>>           write.csv(MD2,"MD2.csv",row.names=TRUE)
>>>>>> 
>>>>>> 
>>>>>> ------------------------------
>>>>>> If you reply to this email, your message will be added to the
>>>> discussion
>>>>>> below:
>>>>>> 
>>>>>> 
>>>> 
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689.html
>>>>>> To start a new topic under R help, email
>>>>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=1>
>> 
>>>>>> To unsubscribe from R, click here
>>>>>> <
>>>>>> .
>>>>>> NAML
>>>>>> <
>>>> 
>> http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>> 
>>>> 
>>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> --
>>>>> View this message in context:
>>>> 
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711690.html
>>>>> Sent from the R help mailing list archive at Nabble.com.
>>>>> [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=2>
>> 
>>>> mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>> 
>>>> --
>>>> Michael
>>>> http://www.dewey.myzen.co.uk/home.html
>>>> 
>>>> ______________________________________________
>>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711707&i=3>
>>>> mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>>>> ------------------------------
>>>> If you reply to this email, your message will be added to the
>> discussion
>>>> below:
>>>> 
>>>> 
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711707.html
>>>> To start a new topic under R help, email
>>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711716&i=1>
>>>> To unsubscribe from R, click here
>>>> <
>>>> .
>>>> NAML
>>>> <
>> http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>> 
>>>> 
>>> 
>>> 
>>> 
>>> 
>>> --
>>> View this message in context:
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711709.html
>>> Sent from the R help mailing list archive at Nabble.com.
>>> [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711716&i=2>
>> mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>> 
>> ______________________________________________
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711716&i=3>
>> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> ------------------------------
>> If you reply to this email, your message will be added to the discussion
>> below:
>> 
>> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711716.html
>> To start a new topic under R help, email
>> ml-node+s789695n789696h75 at n4.nabble.com
>> To unsubscribe from R, click here
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
>> .
>> NAML
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>> 
> 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-produce-Na-tp4711689p4711718.html
> Sent from the R help mailing list archive at Nabble.com.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From Wang.Xue at mayo.edu  Tue Sep  1 23:24:46 2015
From: Wang.Xue at mayo.edu (Wang, Xue, Ph.D.)
Date: Tue, 1 Sep 2015 21:24:46 +0000
Subject: [R] lsqlin in R package pracma
In-Reply-To: <CAML4n3O1+__MzeD=Rq97zoo=Xdv0hok+RJcjRWZeg1vjQw7OLA@mail.gmail.com>
References: <CAML4n3OeKiQBQVPCz-OdmSTdWwG766FeUvuwRJmBzdaX6FMDtw@mail.gmail.com>
	<2f3a88$19rlff@ironport10.mayo.edu>
	<CAML4n3O1+__MzeD=Rq97zoo=Xdv0hok+RJcjRWZeg1vjQw7OLA@mail.gmail.com>
Message-ID: <2f3a88$1b7arn@ironport10.mayo.edu>


slsqp in R seems quite slow. Does anyone have some suggestion as how to speed up this?




-----Original Message-----
From: Hans W Borchers [mailto:hwborchers at gmail.com] 
Sent: Wednesday, August 26, 2015 10:11 AM
To: r-help at stat.math.ethz.ch
Cc: Wang, Xue, Ph.D.
Subject: Re: [R] lsqlin in R package pracma

I am a strong advocate of the *nloptr* package; and "sequential quadratic programming", i.e. slsqp(), should be a good choice for least-squares problems. Note that you still have to provide a starting point.

BUT: this point does not need to lie in the interior of the feasible region.
So you can start with a point that solves the problem on the boundary (with lsqlin()) and then continue with slsqp().

For the example mentioned above, it looks like this:

    library(pracma)
    x0 <- lsqlin(C, d, A, b)  # starting point on the boundary

    library(nloptr)
    f <- function(x) sum((C %*% x - d)^2)  # objective
    hin <- function(x) -A %*% x + b        # constraint, w/  A x >= 0 !

    slsqp(x0, f, hin = hin)
    # $par
    # [1]  0.1298620 -0.5756944  0.4251035  0.2438448
    #
    # $value
    # [1] 0.01758538
    # ...

The solution is slightly better than the MATLAB one or the constrOptim one.
Of course, all these can be improved by changing some optional parameters.


On Wed, Aug 26, 2015 at 2:18 PM, Wang, Xue, Ph.D. <Wang.Xue at mayo.edu> wrote:
> Hi Hans,
>
> Thanks for your comments!
>
> I need the linear inequality constraints so nlsLM is not a candidate.
>
> I am also looking at the functions mma and slsqp in R package nloptr. Compared with constrOptim(), which approach would you recommend?
>
> Thanks,
>
> Xue

From orlinskipawel at wp.pl  Tue Sep  1 23:29:40 2015
From: orlinskipawel at wp.pl (=?ISO-8859-2?Q?Pawe=B3_Orli=F1ski?=)
Date: Tue, 01 Sep 2015 23:29:40 +0200
Subject: [R] Inference on parameters with package "PhaseType"
Message-ID: <55e618c419c376.21668371@wp.pl>

Hi everyone,

I am using package "PhaseType" to approximate given sample of absorption times with a phase-type distribution. The function I am using is called "phtMCMC".
The problem is that this function does not return parameters of starting state distribution. It return only parameters of subintensity matrix.
Hence, the inference is not possible.
Does anyone used this function and struggled with this problem?
Or did I misunderstand something?

Thanks in adnvance.
Pawe? Orli?ski


From aslett at stats.ox.ac.uk  Wed Sep  2 00:37:39 2015
From: aslett at stats.ox.ac.uk (Louis Aslett)
Date: Tue, 1 Sep 2015 23:37:39 +0100
Subject: [R] Inference on parameters with package "PhaseType"
In-Reply-To: <55e618c419c376.21668371@wp.pl>
References: <55e618c419c376.21668371@wp.pl>
Message-ID: <CADTpfYnOukZhUya9_gPuUdfJ+DqgcTHynb6nm5TQfXFSTWjmZA@mail.gmail.com>

Hi Pawe?,

I replied to your direct email to me a month or so ago but perhaps your
spam filter ate it?  I'll repeat here:

Indeed, apologies the documentation could perhaps be a little clearer on
this point: the phtMCMC function is only sampling the generator matrix, not
the initial state probabilities.  My own research was motivated by problems
in reliability theory where there is no uncertainty in the initial state of
the system, so when implementing this I treated these as known (and fixed)
and only sampled from the posterior of the generator matrix entries.
Posterior sampling when the initial distribution is unknown is perfectly
possible, it's just not implemented in full generality in the package.

Adding this capability to the code should not be too big a job if this is
something you need?  Please touch base at this email address if this aspect
would be a big help and I will add to my to-do list ... would be happy to
discuss in more detail off-list.

All the best,

Louis



On 1 September 2015 at 22:29, Pawe? Orli?ski <orlinskipawel at wp.pl> wrote:

> Hi everyone,
>
> I am using package "PhaseType" to approximate given sample of absorption
> times with a phase-type distribution. The function I am using is called
> "phtMCMC".
> The problem is that this function does not return parameters of starting
> state distribution. It return only parameters of subintensity matrix.
> Hence, the inference is not possible.
> Does anyone used this function and struggled with this problem?
> Or did I misunderstand something?
>
> Thanks in adnvance.
> Pawe? Orli?ski
>
>
>

	[[alternative HTML version deleted]]


From otoomet at gmail.com  Wed Sep  2 02:16:24 2015
From: otoomet at gmail.com (Ott Toomet)
Date: Tue, 1 Sep 2015 17:16:24 -0700
Subject: [R] Parallel: initializing SOCKET cluster
Message-ID: <CAMMJQ0bB9bYcjavFe5-8qmHdGmsMQTvg+rXnaAae-1aKVKjJnw@mail.gmail.com>

I am running R on TORQUE/MOAB cluster (I guess---as the documentation is
terrific).

My problem is that I cannot start a slave node (using 'Rscript') before I
load the necessary modules first.  Just executing

cl <- makePSOCKcluster("n0470")

results in error:
/sw/r-3.2.0/lib64/R/bin/Rscript: error while loading shared libraries:
libiomp5.so: cannot open shared object file: No such file or directory

However, I can start it with the manual option and thereafter execute

[otoomet at n0470 ~]$ module load icc_15.0
[otoomet at n0470 ~]$ '/sw/r-3.2.0/lib64/R/bin/Rscript' -e
'parallel:::.slaveRSOCK()' MASTER=n0470 PORT=11985 TIMEOUT=2592000
METHODS=TRUE XDR=TRUE

and voila, it works.

Is there a way to execute initialization script on the nodes _before_
Rscript is started?  Or is there a better way to make cluster on such
systems?

Cheers,
Ott

-- 
Ott Toomet

Visiting Researcher
School of Information
Mary Gates Hall, Suite 095
University of Washington
Seattle, WA 98195

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Sep  2 08:04:54 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 2 Sep 2015 06:04:54 +0000
Subject: [R] Median on second group of CSV file produce Na
In-Reply-To: <3FD0145C-F61D-42F6-967A-C4615EF32097@comcast.net>
References: <1441049748473-4711689.post@n4.nabble.com>
	<CAPR_rLFANxncOh2xP7QjDVd7X6Qc2ptmwi8a=4+hf3oPi=Br0g@mail.gmail.com>
	<55E55652.4020702@dewey.myzen.co.uk>
	<CAM1GnGNQn0=5Nha-AMy70x_Rjgi5JFi7j073xYApZLUFW8GNiw@mail.gmail.com>
	<55E5A9DF.8060201@dewey.myzen.co.uk>
	<CAPR_rLEfyWFPAynnzXjeVn6eSqnOos1RnZQunOcvgEoL7AmP5g@mail.gmail.com>
	<3FD0145C-F61D-42F6-967A-C4615EF32097@comcast.net>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DBBB@SRVEXCHMBX.precheza.cz>

Hi

and if you are in learning R you can help yourself a lot reading chapters 2-7 from R intro manual, which you can probably find in doc folder within your R installation.

It is only about 30 pages and it gives you concise insight into R objects and how to manipulate with them.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> Winsemius
> Sent: Tuesday, September 01, 2015 8:13 PM
> To: shawin
> Cc: r-help at r-project.org
> Subject: Re: [R] Median on second group of CSV file produce Na
>
>
> On Sep 1, 2015, at 6:48 AM, shawin wrote:
>
> > The str() out come is :
> >
> > chr [1:31099, 1:8] " 9.329651" " 8.583304" " 9.284229" " 9.746094"
> > "10.200084" " 9.249863" " 7.979661" "11.271934" ...
> > - attr(*, "dimnames")=List of 2
> >  ..$ : NULL
> >  ..$ : chr [1:8] "GSM215579.CEL" "GSM215580.CEL" "GSM215581.CEL"
> > "GSM215582.CEL" ...
> >
>
> So the matrix you thought was numeric is actually a character-object.
> The median function returns NA for character arguments. I'm seeing
> spaces in front of the decimal numbers so appears your input statements
> need to be somehow adjusted, or you need to coerce the matrix to
> numeric. The colClasses argument to read.table and cousins should be
> used more often.
>
> --
> David.
>
>
> > On Tue, Sep 1, 2015 at 2:32 PM, Michael Dewey-3 [via R] <
> > ml-node+s789695n4711716h95 at n4.nabble.com> wrote:
> >
> >> What happened when you did summary(y) or str(y)?
> >>
> >> On 01/09/2015 08:57, Navien wrote:
> >>
> >>> Dear Michael ,
> >>>
> >>> Thank you very much for your reply , kindly  please can i send you
> the
> >>> program and data please , y is shown a normal csv file no issue
> with it
> >> :(
> >>>
> >>>
> >>> Kind Regards
> >>>
> >>> On Tue, Sep 1, 2015 at 8:36 AM, Michael Dewey-3 [via R] <
> >>> [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711716&i=0>>
> >> wrote:
> >>>
> >>>> Dear Shawin
> >>>>
> >>>> You probably did not get an answer because
> >>>> 1 - you seem to have posted in HTML which mangled your post into
> >>>> unreadability
> >>>> 2 - there seem to be lots of lines which do not do anything
> germane to
> >>>> the problem.
> >>>>
> >>>> Why not try summary(y) or str(y) before you convert it to a matrix
> and
> >>>> see what it says?
> >>>>
> >>>> On 31/08/2015 20:46, shawin wrote:
> >>>>
> >>>>> I have an issue ans i posted it , so i would like to receive a
> >> solution
> >>>>> please
> >>>>>
> >>>>> On Mon, Aug 31, 2015 at 8:35 PM, shawin [via R] <
> >>>>> [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711707&i=0>>
> >>
> >>>> wrote:
> >>>>>
> >>>>>> I have a data frame  csv file and I'm trying to calculate median
> for
> >>>> each
> >>>>>> group separately row by row . When I separate the data frame in
> two
> >>>> groups
> >>>>>> and calculate the median for each one, I am getting an NA result
> for
> >>>> the
> >>>>>> second group :
> >>>>>> the data
> >>>>>>    x1  x2  x3  x4  x5  x6  x7  y1  y2  y3  y4  y5  y6  y7  y8
> >>>>>> 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> >> 9.640816474
> >>>>>> 9.606262272   9.329651027 9.434541611 9.473922432 9.311412966
> >> 9.3154885
> >>>>>> 9.434977488 9.470895414 9.764258059
> >>>>>> 8.630629966 8.55831075  8.788391003 8.576231135 8.671587906
> >> 8.842979993
> >>>>>> 8.861958856 8.58330436  8.603596508 8.570129609 8.59798922
> >> 8.572686772
> >>>>>> 8.679751791 8.663950953 8.432875347
> >>>>>> 9.354748885 9.367668838 9.259952558 9.421538213 9.554635162
> >> 9.603744578
> >>>>>> 9.452197983 9.284228877 9.404607878 9.317737979 9.343115301
> >> 9.310644266
> >>>>>> 9.27227486  9.360337823 9.44706281
> >>>>>> 9.944863964 9.950427516 10.19101759 10.07350804 10.03269879
> >> 10.1307908
> >>>>>>   10.03487287 9.74609383  9.886379007 9.775472567 10.036596
> >>>> 9.544738458
> >>>>>> 9.699611598 9.911962567
> >>>>>> 9.625804277
> >>>>>>
> >>>>>>
> >>>>>>                                     Code:
> >>>>>>
> >>>>>>         rowN <- nrow(AT1)
> >>>>>>         MD1<-vector(length=rowN)
> >>>>>>        MD2<-vector(length=rowN)
> >>>>>>
> >>>>>>            MD1[1:rowN]<-NA
> >>>>>>            MD2[1:rowN]<-NA
> >>>>>>
> >>>>>>
> >>>>>>           x<- AT1[,c(2,3,4,5,6,7,8) ]
> >>>>>>          write.csv(x,"x.csv",row.names=TRUE)
> >>>>>>           x<-as.matrix(x)
> >>>>>>          for(i in 2:rowN) {
> >>>>>>         MD1[i]=median(x[i,])
> >>>>>>            }
> >>>>>>           write.csv(MD1,"MD1.csv",row.names=TRUE)
> >>>>>>
> >>>>>>           y<- AT1[,c(9,10,11,12,13,14,15,16)]
> >>>>>>           write.csv(y,"y.csv",row.names=TRUE)
> >>>>>>           y<-as.matrix(y)
> >>>>>>           for(j in 2:rowN) {
> >>>>>>           MD2[j]=median(y[j,])
> >>>>>>            }
> >>>>>>           write.csv(MD2,"MD2.csv",row.names=TRUE)
> >>>>>>
> >>>>>>
> >>>>>> ------------------------------
> >>>>>> If you reply to this email, your message will be added to the
> >>>> discussion
> >>>>>> below:
> >>>>>>
> >>>>>>
> >>>>
> >> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-
> produce-Na-tp4711689.html
> >>>>>> To start a new topic under R help, email
> >>>>>> [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711707&i=1>
> >>
> >>>>>> To unsubscribe from R, click here
> >>>>>> <
> >>>>>> .
> >>>>>> NAML
> >>>>>> <
> >>>>
> >>
> http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_view
> er&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.Ba
> sicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.naml.namespaces.BasicNamespace-
> nabble.view.web.template.NabbleNamespace-
> nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%2
> 1nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> send_instant_email%21nabble%3Aemail.naml>
> >>
> >>>>
> >>>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>> --
> >>>>> View this message in context:
> >>>>
> >> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-
> produce-Na-tp4711689p4711690.html
> >>>>> Sent from the R help mailing list archive at Nabble.com.
> >>>>> [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711707&i=2>
> >>
> >>>> mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible
> code.
> >>>>>
> >>>>
> >>>> --
> >>>> Michael
> >>>> http://www.dewey.myzen.co.uk/home.html
> >>>>
> >>>> ______________________________________________
> >>>> [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711707&i=3>
> >>>> mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>>
> >>>> ------------------------------
> >>>> If you reply to this email, your message will be added to the
> >> discussion
> >>>> below:
> >>>>
> >>>>
> >> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-
> produce-Na-tp4711689p4711707.html
> >>>> To start a new topic under R help, email
> >>>> [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711716&i=1>
> >>>> To unsubscribe from R, click here
> >>>> <
> >>>> .
> >>>> NAML
> >>>> <
> >>
> http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_view
> er&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.Ba
> sicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.naml.namespaces.BasicNamespace-
> nabble.view.web.template.NabbleNamespace-
> nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%2
> 1nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> send_instant_email%21nabble%3Aemail.naml>
> >>
> >>>>
> >>>
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
> >> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-
> produce-Na-tp4711689p4711709.html
> >>> Sent from the R help mailing list archive at Nabble.com.
> >>> [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711716&i=2>
> >> mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >> --
> >> Michael
> >> http://www.dewey.myzen.co.uk/home.html
> >>
> >> ______________________________________________
> >> [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711716&i=3>
> >> mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >> ------------------------------
> >> If you reply to this email, your message will be added to the
> discussion
> >> below:
> >>
> >> http://r.789695.n4.nabble.com/Median-on-second-group-of-CSV-file-
> produce-Na-tp4711689p4711716.html
> >> To start a new topic under R help, email
> >> ml-node+s789695n789696h75 at n4.nabble.com
> >> To unsubscribe from R, click here
> >>
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscri
> be_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ
> 0MzkwMjQ1>
> >> .
> >> NAML
> >>
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_vie
> wer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.B
> asicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.naml.namespaces.BasicNamespace-
> nabble.view.web.template.NabbleNamespace-
> nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%2
> 1nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> send_instant_email%21nabble%3Aemail.naml>
> >>
> >
> >
> >
> >
> > --
> > View this message in context: http://r.789695.n4.nabble.com/Median-
> on-second-group-of-CSV-file-produce-Na-tp4711689p4711718.html
> > Sent from the R help mailing list archive at Nabble.com.
> >     [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From wewolski at gmail.com  Wed Sep  2 14:39:38 2015
From: wewolski at gmail.com (Witold E Wolski)
Date: Wed, 2 Sep 2015 14:39:38 +0200
Subject: [R] registerDoParallel and R CMD check --as-cran errors
Message-ID: <CAAjnpdiCFjS_XjyrOuhsof+aB9KmT8mfkb7MOQ+Rwhb9rWB7gg@mail.gmail.com>

I am testing a package with

R CMD check
and
R CMD check --as-cran

some code which is run in the examples section uses the foreach and
doParallel package.


When run with --as-cran I have an error.
registerDoParallel function causes an error.

Error in .check_ncores(length(names)) : 6 simultaneous processes spawned
Calls: annotatePeptides ... registerDoParallel -> makeCluster ->
makePSOCKcluster -> .check_ncores


What should can I do to prevent this error and pas the --as-cran check?

best regards
Witold


-- 
Witold Eryk Wolski

	[[alternative HTML version deleted]]


From mylisttech at gmail.com  Wed Sep  2 18:22:11 2015
From: mylisttech at gmail.com (mylisttech at gmail.com)
Date: Wed, 2 Sep 2015 21:52:11 +0530
Subject: [R] Is it possible to run factor analysis on categorical data.
Message-ID: <4C6B905D-3939-4078-AF94-D3ABC19A13E5@gmail.com>

Dear Rexperts,

I tried running factanal on a group and variables ( all categorical datas ) and I learnt that's not possible , at least by using factanal. Can I use FactoMineR for MFA ? Does it do the same thing as factor analysis.

Thanks in advance for the help,
Harmeet

From bgunter.4567 at gmail.com  Wed Sep  2 18:33:36 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 2 Sep 2015 09:33:36 -0700
Subject: [R] Is it possible to run factor analysis on categorical data.
In-Reply-To: <4C6B905D-3939-4078-AF94-D3ABC19A13E5@gmail.com>
References: <4C6B905D-3939-4078-AF94-D3ABC19A13E5@gmail.com>
Message-ID: <CAGxFJbRUetdd=u2R6J_-RSE2+NB88W-=pCVgVEzRhrZr58uEjQ@mail.gmail.com>

No.

Rhetorical question: You appear not to understand what factor analysis
does, so why are you trying to use it?

In any case, these are statistical questions, and therefore largely
off topic here, which is about R programming. I suggest you:

1) Consult a local statistical expert (your advisor?) for statistical help;

2) and/or post on a statistical site, like stats.stackexchange.com for
further advice and, in particular alternatives for what you might do.
Then you can search R's packages (google generally does pretty well at
this, as does R's "sos" package) for an implementation of the methods
you decide on.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 2, 2015 at 9:22 AM,  <mylisttech at gmail.com> wrote:
> Dear Rexperts,
>
> I tried running factanal on a group and variables ( all categorical datas ) and I learnt that's not possible , at least by using factanal. Can I use FactoMineR for MFA ? Does it do the same thing as factor analysis.
>
> Thanks in advance for the help,
> Harmeet
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fabrizio.renzi84 at gmail.com  Wed Sep  2 10:38:15 2015
From: fabrizio.renzi84 at gmail.com (fabrizio renzi)
Date: Wed, 2 Sep 2015 10:38:15 +0200
Subject: [R] spatial estimation
Message-ID: <CAGq2xcRvPHYzTL=qV7NUBMf8fG8c0stLJkwNEsj2574rqKL3uw@mail.gmail.com>

I am using spdep package to do spatial estimations in R.

Anyway I was not able to find out how to compute the loglikelihood (and a
pseudo-R^2) of a 2sls spatial model, estimated through the command stsls. I
need some fitting indicator to compare the 2sls model with the one obtained
through maxL (command lagsarlm).

Thanks for your answer,

Regards


-- 
*Fabrizio*

	[[alternative HTML version deleted]]


From galavizk at gmail.com  Wed Sep  2 17:27:18 2015
From: galavizk at gmail.com (Karla Galaviz)
Date: Wed, 2 Sep 2015 11:27:18 -0400
Subject: [R] pcnetmeta help: initial values
Message-ID: <005401d0e593$db1257b0$91370710$@gmail.com>

Good morning,

 

I hope this message finds you well.

I am writing because I have been trying to run a Bayesian network
meta-analysis for 2 weeks now using the pcnetmeta package and have not been
able to do it. I have looked for help online and could not find what my
problem is.

 

My code is: 

#### Bayesian Network Meta-analysis with follow up times

LR <- read.csv("C:/Users/Galanova/Box Sync/IHME/EF meta/R
analysis/diabetes.csv", row.names=NULL, quote="")

View(LR)

nma.ab.followup(s.id = s.id, t.id = t.id, event.n = event.n, total.n =
total.n, followup = follow.up, 

        data=LR, param = c("lograte", "rank.prob"),

        model = "het_eqcor", prior.type="invgamma", a = 0.001, b = 0.001,

        c = 10, higher.better = FALSE, digits = 4, n.adapt = 5000,

        n.iter = 100000, n.burnin = floor(n.iter/2), n.chains = 3,

        n.thin = max(1, floor((n.iter - n.burnin)/100000)),

        conv.diag = FALSE, trace = "lograte", dic = FALSE,

        postdens = FALSE, mcmc.samples = FALSE)

 

My data is:

'data.frame':  128 obs. of  11 variables:
 $ Author   : Factor w/ 124 levels "Ali (2011)-1000 mg",..: 70 69 39 38 102
101 104 103 106 105 ...
 $ s.id     : int  1 1 2 2 3 3 4 4 5 5 ...
 $ t.id     : int  1 0 1 0 1 0 1 0 1 0 ...
 $ trtname  : Factor w/ 5 levels "ALT","Control",..: 3 2 3 2 3 2 3 2 3 2 ...
 $ event.n  : num  NA NA 34 55 8 18 35 55 15 36 ...
 $ total.n  : int  108 80 143 142 52 54 300 321 103 110 ...
 $ follow.up: num  1 1 3.1 3.1 3 3 3 3 3 3 ...
 $ X1000.py : num  NA NA 80 132 60 127 42 58 27 51 ...
 $ percent  : num  NA NA 24 39 18 38 12 17 15 8 ...
 $ rr       : num  NA NA 0.58 NA 0.42 NA 0.56 NA 0.53 NA ...
 $ rr.se    : num  NA NA 0.55 NA -0.06 NA 0.14 NA -0.0325 NA ...

 

When I run it, I get this message:

 Error in jags.model(file = textConnection(modelstring), data = data.jags,
: 

  Error in node y[17]

Observed node inconsistent with unobserved parents at initialization.

Try setting appropriate initial values.

 

I have tried using different initial values and that does not work either. I
am a beginner in R and would really appreciate some help on this.

Thank you for taking the time to read this message.

 

Karla

 


	[[alternative HTML version deleted]]


From romana.stats at gmail.com  Wed Sep  2 17:21:39 2015
From: romana.stats at gmail.com (Romana Shehla)
Date: Wed, 2 Sep 2015 20:51:39 +0530
Subject: [R] Urgent query
Message-ID: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>

Sir/Madam,

          I am a scholar at Aligarh Muslim University and have been
continuously using R language. Recently, I am stuck on labeling axis of the
graphic, I have made. Actually, I want to use mathematical notation
 0<beta<1 at the axis. I know how to write the greek letter but I am unable
to show the interval (0,1) .  I would be really grateful if you could help
me with this problem.
Waiting for your reply

With regards,
Romana Shehla
Research Scholar
AMU

	[[alternative HTML version deleted]]


From voxcoelestis at googlemail.com  Wed Sep  2 08:46:35 2015
From: voxcoelestis at googlemail.com (Voxcoelestis)
Date: Wed, 2 Sep 2015 08:46:35 +0200
Subject: [R] Visualization of people's interactions by participation to
	parties
Message-ID: <55E69B4B.9@googlemail.com>

Dear all,

I have a long list of parties and participants over many years and want 
to extract network relations between people to identify groups of 
friends. My list looks like this:

Party 1; date party 1; first name 1 last name 1; first name 2 last name 
2; first name 3 last name 3;
Party 2; date party 2; first name 1 last name 1; first name 3 last name 
3; first name 4 last name 4;
Party 3; date party 3; first name 3 last name 3; first name 5 last name 5;
Party 4; date party 4; first name 2 last name 2; first name 6 last name 
6; first name 3 last name 3; first name 1 last name 1;
Party 5; date party 5; first name 5 last name 5; first name 4 last name 4;
....

Obviously the amount and the order of names is not regular. The list is 
far too long to count co-appearances for each person-person combination 
by hand.

What I would like to do is first of all create a network with individual 
persons as nodes and the co-appearances as edges and the number of 
co-appearances as strenght of interactions clustering closesly related 
people.

In a second step it would be beneficial to extract information on the 
durability of these interactions by including the time difference 
between first and last interaction.

Do you have any ideas or hints how to approach this problem?

Thank you so much,

Hendrik


From dwinsemius at comcast.net  Wed Sep  2 20:44:04 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 2 Sep 2015 11:44:04 -0700
Subject: [R] Urgent query
In-Reply-To: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>
References: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>
Message-ID: <523213FF-3258-4058-8831-4BA0DF502671@comcast.net>


On Sep 2, 2015, at 8:21 AM, Romana Shehla wrote:

> Sir/Madam,
> 
>          I am a scholar at Aligarh Muslim University and have been
> continuously using R language. Recently, I am stuck on labeling axis of the
> graphic, I have made. Actually, I want to use mathematical notation
> 0<beta<1 at the axis. I know how to write the greek letter but I am unable
> to show the interval (0,1) .  I would be really grateful if you could help
> me with this problem.
> Waiting for your reply

plot(1,1, xlab=expression( 0 *"<"* beta  *"<"* 1  ) )

I originally tried:

plot(1,1, xlab=expression( 0 < beta  < 1  ) )

.... But it throws an error because the is not a syntactically correct R expression, so I switched to using text versions of the symbols.

-- 

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Wed Sep  2 20:45:19 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 2 Sep 2015 11:45:19 -0700
Subject: [R] Urgent query
In-Reply-To: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>
References: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>
Message-ID: <CAGxFJbSFhX1tvX7Nynk-JgNL5pCXr0GgfJsqn9rPc_MOznBFzQ@mail.gmail.com>

Please show us the code that you used that failed. There are several
different graphics systems in R, and the answer depends on which
system you use.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 2, 2015 at 8:21 AM, Romana Shehla <romana.stats at gmail.com> wrote:
> Sir/Madam,
>
>           I am a scholar at Aligarh Muslim University and have been
> continuously using R language. Recently, I am stuck on labeling axis of the
> graphic, I have made. Actually, I want to use mathematical notation
>  0<beta<1 at the axis. I know how to write the greek letter but I am unable
> to show the interval (0,1) .  I would be really grateful if you could help
> me with this problem.
> Waiting for your reply
>
> With regards,
> Romana Shehla
> Research Scholar
> AMU
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fisher at plessthan.com  Wed Sep  2 21:02:14 2015
From: fisher at plessthan.com (Fisher Dennis)
Date: Wed, 2 Sep 2015 12:02:14 -0700
Subject: [R] Use of contrasts in ANOVA
Message-ID: <ED5C6251-9B12-4491-BFF9-10E2F3CB551A@plessthan.com>

R 3.2.0
OS X

Colleagues,

I am confused about the implementation of contrasts in ANOVA.  I have 4 groups (BATCH) in DATA.  My code reads:
	contrasts(DATA$BATCH)	<- c(-1, -1, 1, 1)/2
	LM						<- lm(VALUES ~ SIDN + BATCH, data=DATA)
where both SIDN and BATCH are factors and VALUES is a real number

When I run this code, I get the exact results that a colleague obtains in SAS.  So far, so good.  Also, when I examine DATA$BATCH, i get:
> DATA$BATCH
 [1] P R S Q R P S Q R P S Q P R Q S R P S Q R P Q S P R Q S P R Q S P R S Q P R
[39] S Q P R Q S R P Q S R P S Q R P Q S P R S Q R P Q S R P S Q R P Q S P R S Q
[77] P R S Q R P S Q P R Q S R P Q S P R Q S
attr(,"contrasts")
  [,1] [,2] [,3]
P -0.5 -0.5 -0.5
Q -0.5  0.5  0.5
R  0.5  0.5 -0.5
S  0.5 -0.5  0.5
Levels: P Q R S

I noted that columns 2 and 3 did not seem relevant.  Out of curiosity, I modified the contrasts code:
	contrasts(DATA$BATCH, how.many=1)	<- c(-1, -1, 1, 1)/2
Now when I examine DATA$BATCH, I get:
> DATA$BATCH
 [1] P R S Q R P S Q R P S Q P R Q S R P S Q R P Q S P R Q S P R Q S P R S Q P R
[39] S Q P R Q S R P Q S R P S Q R P Q S P R S Q R P Q S R P S Q R P Q S P R S Q
[77] P R S Q R P S Q P R Q S R P Q S P R Q S
attr(,"contrasts")
  [,1]
P -0.5
Q -0.5
R  0.5
S  0.5
Levels: P Q R S

This matches the previous output except for the disappearance of columns 2 and 3.  I thought that would be desirable.
When I now run the code, the estimate for BATCH did not change ? however, its SE (and the resulting t value) changed 10%.  

I don?t understand how the 
	how.many 
option in contrasts works.  Can anyone offer an explanation?

Thanks in advance.

Dennis	

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From dwinsemius at comcast.net  Wed Sep  2 21:53:32 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 2 Sep 2015 12:53:32 -0700
Subject: [R] Use of contrasts in ANOVA
In-Reply-To: <ED5C6251-9B12-4491-BFF9-10E2F3CB551A@plessthan.com>
References: <ED5C6251-9B12-4491-BFF9-10E2F3CB551A@plessthan.com>
Message-ID: <FFB9BE5F-8A40-4B00-8C26-4AD59FE68EA0@comcast.net>


On Sep 2, 2015, at 12:02 PM, Fisher Dennis wrote:

> R 3.2.0
> OS X
> 
> Colleagues,
> 
> I am confused about the implementation of contrasts in ANOVA.  I have 4 groups (BATCH) in DATA.  My code reads:
> 	contrasts(DATA$BATCH)	<- c(-1, -1, 1, 1)/2
> 	LM						<- lm(VALUES ~ SIDN + BATCH, data=DATA)
> where both SIDN and BATCH are factors and VALUES is a real number
> 
> When I run this code, I get the exact results that a colleague obtains in SAS.  So far, so good.  Also, when I examine DATA$BATCH, i get:
>> DATA$BATCH
> [1] P R S Q R P S Q R P S Q P R Q S R P S Q R P Q S P R Q S P R Q S P R S Q P R
> [39] S Q P R Q S R P Q S R P S Q R P Q S P R S Q R P Q S R P S Q R P Q S P R S Q
> [77] P R S Q R P S Q P R Q S R P Q S P R Q S
> attr(,"contrasts")
>  [,1] [,2] [,3]
> P -0.5 -0.5 -0.5
> Q -0.5  0.5  0.5
> R  0.5  0.5 -0.5
> S  0.5 -0.5  0.5
> Levels: P Q R S
> 
> I noted that columns 2 and 3 did not seem relevant.

The help page for ?contrasts and ?contrasts<- says:

how.many	
How many contrasts should be made. Defaults to one less than the number of levels of x.

So it made 3 contrasts.


>  Out of curiosity, I modified the contrasts code:
> 	contrasts(DATA$BATCH, how.many=1)	<- c(-1, -1, 1, 1)/2
> Now when I examine DATA$BATCH, I get:
>> DATA$BATCH
> [1] P R S Q R P S Q R P S Q P R Q S R P S Q R P Q S P R Q S P R Q S P R S Q P R
> [39] S Q P R Q S R P Q S R P S Q R P Q S P R S Q R P Q S R P S Q R P Q S P R S Q
> [77] P R S Q R P S Q P R Q S R P Q S P R Q S
> attr(,"contrasts")
>  [,1]
> P -0.5
> Q -0.5
> R  0.5
> S  0.5
> Levels: P Q R S
> 
> This matches the previous output except for the disappearance of columns 2 and 3.  I thought that would be desirable.
> When I now run the code, the estimate for BATCH did not change ? however, its SE (and the resulting t value) changed 10%.  

Wouldn't the number of degrees of freedom change when the contrasts changed? Specifics would depend on specifics. The implicit model would be different. When you run the first version with lm you see 3 non-Intercept parameters, while you only see 1 with the second model.

-- 
David.


> 
> I don?t understand how the 
> 	how.many 
> option in contrasts works.  Can anyone offer an explanation?
> 
> Thanks in advance.
> 
> Dennis	
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From tanasa at gmail.com  Thu Sep  3 00:11:22 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Wed, 2 Sep 2015 15:11:22 -0700
Subject: [R] reading files with name columns and row columns
Message-ID: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>

Dear all,

would appreciate a piece of help with a simple question: I am reading in R
a file that is formatted as a matrix (an example is shown below, although
it is more complex, a matrix of 1000 * 1000 ):

the names of the columns are 0, 10000, 40000, 80000, etc
the names of the rows are 0, 10000, 40000, 80000, etc

           0 200000 400000
0          0       0       0
200000  0       0       0
400000  0       0       0

shall I use the command :

y <- read.table("file",row.names=1, header=T)

the results is :

> y[1:3,1:3]
       X0 X200000 X400000
0       0       0       0
200000  0       0       0
400000  0       0       0

The question is : why R adds an X to the names of the columns eg X0,
X20000, X40000, when it shall be only 0, 20000, 40000 ? thanks !

-- bogdan

	[[alternative HTML version deleted]]


From shawinkarim at gmail.com  Wed Sep  2 21:08:18 2015
From: shawinkarim at gmail.com (shawin)
Date: Wed, 2 Sep 2015 12:08:18 -0700 (PDT)
Subject: [R] Converting CSV file to numeric and calculate ttest
In-Reply-To: <1441220823200-4711770.post@n4.nabble.com>
References: <1441220823200-4711770.post@n4.nabble.com>
Message-ID: <CAPR_rLGgd4sUiKboDvrtFqasLRHzhsdrMqAmwj=oFLcBfhUdJA@mail.gmail.com>

p value should be 0.3

On Wed, Sep 2, 2015 at 8:07 PM, Navien [via R] <
ml-node+s789695n4711770h69 at n4.nabble.com> wrote:

> I have a csv file i convert it to numeric value , then i apply t-test ,
> but t-test result is not correct: please could you guide me. the data :
> ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15
> 1367452_at 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> 9.640816474 9.606262272 9.329651027 9.434541611 9.473922432 9.311412966
> 9.3154885 9.434977488 9.470895414 9.764258059
> 1367453_at 8.630629966 8.55831075 8.788391003 8.576231135 8.671587906
> 8.842979993 8.861958856 8.58330436 8.603596508 8.570129609 8.59798922
> 8.572686772 8.679751791 8.663950953 8.432875347
> 1367454_at 9.354748885 9.367668838 9.259952558 9.421538213 9.554635162
> 9.603744578 9.452197983 9.284228877 9.404607878 9.317737979 9.343115301
> 9.310644266 9.27227486 9.360337823 9.44706281
> 1367455_at 9.944863964 9.950427516 10.19101759 10.07350804 10.03269879
> 10.1307908 10.03487287 9.74609383 9.886379007 9.775472567 10.036596
> 9.544738458 9.699611598 9.911962567 9.625804277
>           AT1 <-
> read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/AT1.csv",
>  stringsAsFactors=TRUE)
>
>
> str(AT1)
> matAT = data.frame(AT1[,-1])
> str(matAT)
> write.csv(matAT,"matAT.csv",row.names=FALSE)
> #convert the csv file to numeric
> matAT<-as.matrix(sapply(matAT,as.numeric))
> str(matAT)
> group<-factor(rep(c("Ob","Ln"),times=c(7,8)))
> t.test(matAT[1,]~group)
>
>
>
>
>
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-calculate-ttest-tp4711770.html
> To start a new topic under R help, email
> ml-node+s789695n789696h75 at n4.nabble.com
> To unsubscribe from R, click here
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
> .
> NAML
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711771.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Sep  3 00:59:04 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 2 Sep 2015 15:59:04 -0700
Subject: [R] reading files with name columns and row columns
In-Reply-To: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>
References: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>
Message-ID: <CAGxFJbScPcQeWUmDiXjTAGipiJ+Q1s6Ez6NbkM7Xm64ffXQ=8g@mail.gmail.com>

Please read the Help file carefully before posting:

"read.table is not the right tool for reading large matrices,
especially those with many columns: it is designed to read data frames
which may have columns of very different classes. Use scan instead for
matrices."

But the answer to your question can be found in

?make.names

for what constitutes a syntactically valid name in R.


Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 2, 2015 at 3:11 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
> Dear all,
>
> would appreciate a piece of help with a simple question: I am reading in R
> a file that is formatted as a matrix (an example is shown below, although
> it is more complex, a matrix of 1000 * 1000 ):
>
> the names of the columns are 0, 10000, 40000, 80000, etc
> the names of the rows are 0, 10000, 40000, 80000, etc
>
>            0 200000 400000
> 0          0       0       0
> 200000  0       0       0
> 400000  0       0       0
>
> shall I use the command :
>
> y <- read.table("file",row.names=1, header=T)
>
> the results is :
>
>> y[1:3,1:3]
>        X0 X200000 X400000
> 0       0       0       0
> 200000  0       0       0
> 400000  0       0       0
>
> The question is : why R adds an X to the names of the columns eg X0,
> X20000, X40000, when it shall be only 0, 20000, 40000 ? thanks !
>
> -- bogdan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tanasa at gmail.com  Thu Sep  3 01:08:54 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Wed, 2 Sep 2015 16:08:54 -0700
Subject: [R] reading files with name columns and row columns
In-Reply-To: <CAGxFJbScPcQeWUmDiXjTAGipiJ+Q1s6Ez6NbkM7Xm64ffXQ=8g@mail.gmail.com>
References: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>
	<CAGxFJbScPcQeWUmDiXjTAGipiJ+Q1s6Ez6NbkM7Xm64ffXQ=8g@mail.gmail.com>
Message-ID: <CA+JEM020O7=R6RHEH_8eNQ-F8vtGX_UbaBRc-6j8VmJZOVU3wQ@mail.gmail.com>

Thanks, Bert ! I solved the situation in the meanwhile, by using :

y <- as.matrix(read.table("FILE_NAME",header=T,row.names=1))

colnames(y) <- gsub("X","", colnames(y))


On Wed, Sep 2, 2015 at 3:59 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Please read the Help file carefully before posting:
>
> "read.table is not the right tool for reading large matrices,
> especially those with many columns: it is designed to read data frames
> which may have columns of very different classes. Use scan instead for
> matrices."
>
> But the answer to your question can be found in
>
> ?make.names
>
> for what constitutes a syntactically valid name in R.
>
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Wed, Sep 2, 2015 at 3:11 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
> > Dear all,
> >
> > would appreciate a piece of help with a simple question: I am reading in
> R
> > a file that is formatted as a matrix (an example is shown below, although
> > it is more complex, a matrix of 1000 * 1000 ):
> >
> > the names of the columns are 0, 10000, 40000, 80000, etc
> > the names of the rows are 0, 10000, 40000, 80000, etc
> >
> >            0 200000 400000
> > 0          0       0       0
> > 200000  0       0       0
> > 400000  0       0       0
> >
> > shall I use the command :
> >
> > y <- read.table("file",row.names=1, header=T)
> >
> > the results is :
> >
> >> y[1:3,1:3]
> >        X0 X200000 X400000
> > 0       0       0       0
> > 200000  0       0       0
> > 400000  0       0       0
> >
> > The question is : why R adds an X to the names of the columns eg X0,
> > X20000, X40000, when it shall be only 0, 20000, 40000 ? thanks !
> >
> > -- bogdan
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Sep  3 01:36:47 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 2 Sep 2015 16:36:47 -0700
Subject: [R] reading files with name columns and row columns
In-Reply-To: <CA+JEM020O7=R6RHEH_8eNQ-F8vtGX_UbaBRc-6j8VmJZOVU3wQ@mail.gmail.com>
References: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>
	<CAGxFJbScPcQeWUmDiXjTAGipiJ+Q1s6Ez6NbkM7Xm64ffXQ=8g@mail.gmail.com>
	<CA+JEM020O7=R6RHEH_8eNQ-F8vtGX_UbaBRc-6j8VmJZOVU3wQ@mail.gmail.com>
Message-ID: <CAF8bMcZzNGUYPK1SBgFGrqBVvHE76x6d35=zU+X3N0CB8cANyg@mail.gmail.com>

  y <- as.matrix(read.table("FILE_NAME",header=T,row.names=1))
  colnames(y) <- gsub("X","", colnames(y))

Use read.table's check.names=FALSE argument so it won't mangle
the column names instead of trying to demangle them with gsub() afterwards.

E.g.,
  txt <- "   50%  100%\nA   5     8\nB  13    14\n"
  cat(txt)
  #   50%  100%
  #A   5     8
  #B  13    14
  read.table(text=txt, head=TRUE, row.names=1)
  #  X50. X100.
  #A    5     8
  #B   13    14
  read.table(text=txt, head=TRUE, row.names=1, check.names=FALSE)
  #  50% 100%
  #A   5    8
  #B  13   14


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Sep 2, 2015 at 4:08 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:

> Thanks, Bert ! I solved the situation in the meanwhile, by using :
>
> y <- as.matrix(read.table("FILE_NAME",header=T,row.names=1))
>
> colnames(y) <- gsub("X","", colnames(y))
>
>
> On Wed, Sep 2, 2015 at 3:59 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
> > Please read the Help file carefully before posting:
> >
> > "read.table is not the right tool for reading large matrices,
> > especially those with many columns: it is designed to read data frames
> > which may have columns of very different classes. Use scan instead for
> > matrices."
> >
> > But the answer to your question can be found in
> >
> > ?make.names
> >
> > for what constitutes a syntactically valid name in R.
> >
> >
> > Cheers,
> > Bert
> >
> > Bert Gunter
> >
> > "Data is not information. Information is not knowledge. And knowledge
> > is certainly not wisdom."
> >    -- Clifford Stoll
> >
> >
> > On Wed, Sep 2, 2015 at 3:11 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
> > > Dear all,
> > >
> > > would appreciate a piece of help with a simple question: I am reading
> in
> > R
> > > a file that is formatted as a matrix (an example is shown below,
> although
> > > it is more complex, a matrix of 1000 * 1000 ):
> > >
> > > the names of the columns are 0, 10000, 40000, 80000, etc
> > > the names of the rows are 0, 10000, 40000, 80000, etc
> > >
> > >            0 200000 400000
> > > 0          0       0       0
> > > 200000  0       0       0
> > > 400000  0       0       0
> > >
> > > shall I use the command :
> > >
> > > y <- read.table("file",row.names=1, header=T)
> > >
> > > the results is :
> > >
> > >> y[1:3,1:3]
> > >        X0 X200000 X400000
> > > 0       0       0       0
> > > 200000  0       0       0
> > > 400000  0       0       0
> > >
> > > The question is : why R adds an X to the names of the columns eg X0,
> > > X20000, X40000, when it shall be only 0, 20000, 40000 ? thanks !
> > >
> > > -- bogdan
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tanasa at gmail.com  Thu Sep  3 01:42:16 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Wed, 2 Sep 2015 16:42:16 -0700
Subject: [R] reading files with name columns and row columns
In-Reply-To: <CAF8bMcZzNGUYPK1SBgFGrqBVvHE76x6d35=zU+X3N0CB8cANyg@mail.gmail.com>
References: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>
	<CAGxFJbScPcQeWUmDiXjTAGipiJ+Q1s6Ez6NbkM7Xm64ffXQ=8g@mail.gmail.com>
	<CA+JEM020O7=R6RHEH_8eNQ-F8vtGX_UbaBRc-6j8VmJZOVU3wQ@mail.gmail.com>
	<CAF8bMcZzNGUYPK1SBgFGrqBVvHE76x6d35=zU+X3N0CB8cANyg@mail.gmail.com>
Message-ID: <CA+JEM007K=62EB+SAuDdk3ZQ4W+sOXvxEGJUg98tEdjsuBcRZA@mail.gmail.com>

that is great, thank you Bill for time and help ;) !

On Wed, Sep 2, 2015 at 4:36 PM, William Dunlap <wdunlap at tibco.com> wrote:

>   y <- as.matrix(read.table("FILE_NAME",header=T,row.names=1))
>   colnames(y) <- gsub("X","", colnames(y))
>
> Use read.table's check.names=FALSE argument so it won't mangle
> the column names instead of trying to demangle them with gsub()
> afterwards.
>
> E.g.,
>   txt <- "   50%  100%\nA   5     8\nB  13    14\n"
>   cat(txt)
>   #   50%  100%
>   #A   5     8
>   #B  13    14
>   read.table(text=txt, head=TRUE, row.names=1)
>   #  X50. X100.
>   #A    5     8
>   #B   13    14
>   read.table(text=txt, head=TRUE, row.names=1, check.names=FALSE)
>   #  50% 100%
>   #A   5    8
>   #B  13   14
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Wed, Sep 2, 2015 at 4:08 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
>
>> Thanks, Bert ! I solved the situation in the meanwhile, by using :
>>
>> y <- as.matrix(read.table("FILE_NAME",header=T,row.names=1))
>>
>> colnames(y) <- gsub("X","", colnames(y))
>>
>>
>> On Wed, Sep 2, 2015 at 3:59 PM, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>>
>> > Please read the Help file carefully before posting:
>> >
>> > "read.table is not the right tool for reading large matrices,
>> > especially those with many columns: it is designed to read data frames
>> > which may have columns of very different classes. Use scan instead for
>> > matrices."
>> >
>> > But the answer to your question can be found in
>> >
>> > ?make.names
>> >
>> > for what constitutes a syntactically valid name in R.
>> >
>> >
>> > Cheers,
>> > Bert
>> >
>> > Bert Gunter
>> >
>> > "Data is not information. Information is not knowledge. And knowledge
>> > is certainly not wisdom."
>> >    -- Clifford Stoll
>> >
>> >
>> > On Wed, Sep 2, 2015 at 3:11 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
>> > > Dear all,
>> > >
>> > > would appreciate a piece of help with a simple question: I am reading
>> in
>> > R
>> > > a file that is formatted as a matrix (an example is shown below,
>> although
>> > > it is more complex, a matrix of 1000 * 1000 ):
>> > >
>> > > the names of the columns are 0, 10000, 40000, 80000, etc
>> > > the names of the rows are 0, 10000, 40000, 80000, etc
>> > >
>> > >            0 200000 400000
>> > > 0          0       0       0
>> > > 200000  0       0       0
>> > > 400000  0       0       0
>> > >
>> > > shall I use the command :
>> > >
>> > > y <- read.table("file",row.names=1, header=T)
>> > >
>> > > the results is :
>> > >
>> > >> y[1:3,1:3]
>> > >        X0 X200000 X400000
>> > > 0       0       0       0
>> > > 200000  0       0       0
>> > > 400000  0       0       0
>> > >
>> > > The question is : why R adds an X to the names of the columns eg X0,
>> > > X20000, X40000, when it shall be only 0, 20000, 40000 ? thanks !
>> > >
>> > > -- bogdan
>> > >
>> > >         [[alternative HTML version deleted]]
>> > >
>> > > ______________________________________________
>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From shawinkarim at gmail.com  Thu Sep  3 00:47:34 2015
From: shawinkarim at gmail.com (shawin)
Date: Wed, 2 Sep 2015 15:47:34 -0700 (PDT)
Subject: [R] reading files with name columns and row columns
In-Reply-To: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>
References: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>
Message-ID: <CAPR_rLG_ZC92P-TQ8aY3YkA96dT_j=3hFvORjnnshA8HYEmvmA@mail.gmail.com>

also the header =0

On Wed, Sep 2, 2015 at 11:52 PM, Shawin Karim <shawinkarim at gmail.com> wrote:

> make row.name=0 instead to 1
>
> On Wed, Sep 2, 2015 at 11:07 PM, Bogdan Tanasa [via R] <
> ml-node+s789695n4711774h93 at n4.nabble.com> wrote:
>
>> Dear all,
>>
>> would appreciate a piece of help with a simple question: I am reading in
>> R
>> a file that is formatted as a matrix (an example is shown below, although
>> it is more complex, a matrix of 1000 * 1000 ):
>>
>> the names of the columns are 0, 10000, 40000, 80000, etc
>> the names of the rows are 0, 10000, 40000, 80000, etc
>>
>>            0 200000 400000
>> 0          0       0       0
>> 200000  0       0       0
>> 400000  0       0       0
>>
>> shall I use the command :
>>
>> y <- read.table("file",row.names=1, header=T)
>>
>> the results is :
>>
>> > y[1:3,1:3]
>>        X0 X200000 X400000
>> 0       0       0       0
>> 200000  0       0       0
>> 400000  0       0       0
>>
>> The question is : why R adds an X to the names of the columns eg X0,
>> X20000, X40000, when it shall be only 0, 20000, 40000 ? thanks !
>>
>> -- bogdan
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711774&i=0>
>> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> ------------------------------
>> If you reply to this email, your message will be added to the discussion
>> below:
>>
>> http://r.789695.n4.nabble.com/reading-files-with-name-columns-and-row-columns-tp4711774.html
>> To start a new topic under R help, email
>> ml-node+s789695n789696h75 at n4.nabble.com
>> To unsubscribe from R, click here
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
>> .
>> NAML
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>
>




--
View this message in context: http://r.789695.n4.nabble.com/reading-files-with-name-columns-and-row-columns-tp4711774p4711776.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From shawinkarim at gmail.com  Thu Sep  3 01:37:16 2015
From: shawinkarim at gmail.com (shawin)
Date: Wed, 2 Sep 2015 16:37:16 -0700 (PDT)
Subject: [R] reading files with name columns and row columns
In-Reply-To: <CAF8bMcZzNGUYPK1SBgFGrqBVvHE76x6d35=zU+X3N0CB8cANyg@mail.gmail.com>
References: <CA+JEM03kVcjaxipK7QRBZGZkeWs+6NngcDYhpuampOJEcXZPiQ@mail.gmail.com>
	<CAGxFJbScPcQeWUmDiXjTAGipiJ+Q1s6Ez6NbkM7Xm64ffXQ=8g@mail.gmail.com>
	<CA+JEM020O7=R6RHEH_8eNQ-F8vtGX_UbaBRc-6j8VmJZOVU3wQ@mail.gmail.com>
	<CAF8bMcZzNGUYPK1SBgFGrqBVvHE76x6d35=zU+X3N0CB8cANyg@mail.gmail.com>
Message-ID: <CAPR_rLFc5asSGXQBmLi21zmzhuqBeD1AkYhV=UXcZ4L3AWg+dA@mail.gmail.com>

dear William ,

I have an issue with R code which is :


FCP<-as.matrix(sapply(FCPval,as.numeric))
for (i in 1:rowN){if (FCP$FC[i] >= 1.5 & FCP$FC[i]<=-1.5 & FCP$p[i]<=0.05){
dfrmPFC=data.frame(matrix(Fc=FC,p=p))}
}

the error is :Error in FCP$FC : $ operator is invalid for atomic vectors
could you please help me

On Thu, Sep 3, 2015 at 12:32 AM, William Dunlap [via R] <
ml-node+s789695n4711779h48 at n4.nabble.com> wrote:

>   y <- as.matrix(read.table("FILE_NAME",header=T,row.names=1))
>   colnames(y) <- gsub("X","", colnames(y))
>
> Use read.table's check.names=FALSE argument so it won't mangle
> the column names instead of trying to demangle them with gsub()
> afterwards.
>
> E.g.,
>   txt <- "   50%  100%\nA   5     8\nB  13    14\n"
>   cat(txt)
>   #   50%  100%
>   #A   5     8
>   #B  13    14
>   read.table(text=txt, head=TRUE, row.names=1)
>   #  X50. X100.
>   #A    5     8
>   #B   13    14
>   read.table(text=txt, head=TRUE, row.names=1, check.names=FALSE)
>   #  50% 100%
>   #A   5    8
>   #B  13   14
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Wed, Sep 2, 2015 at 4:08 PM, Bogdan Tanasa <[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711779&i=0>> wrote:
>
> > Thanks, Bert ! I solved the situation in the meanwhile, by using :
> >
> > y <- as.matrix(read.table("FILE_NAME",header=T,row.names=1))
> >
> > colnames(y) <- gsub("X","", colnames(y))
> >
> >
> > On Wed, Sep 2, 2015 at 3:59 PM, Bert Gunter <[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711779&i=1>>
> > wrote:
> >
> > > Please read the Help file carefully before posting:
> > >
> > > "read.table is not the right tool for reading large matrices,
> > > especially those with many columns: it is designed to read data frames
> > > which may have columns of very different classes. Use scan instead for
> > > matrices."
> > >
> > > But the answer to your question can be found in
> > >
> > > ?make.names
> > >
> > > for what constitutes a syntactically valid name in R.
> > >
> > >
> > > Cheers,
> > > Bert
> > >
> > > Bert Gunter
> > >
> > > "Data is not information. Information is not knowledge. And knowledge
> > > is certainly not wisdom."
> > >    -- Clifford Stoll
> > >
> > >
> > > On Wed, Sep 2, 2015 at 3:11 PM, Bogdan Tanasa <[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711779&i=2>> wrote:
> > > > Dear all,
> > > >
> > > > would appreciate a piece of help with a simple question: I am
> reading
> > in
> > > R
> > > > a file that is formatted as a matrix (an example is shown below,
> > although
> > > > it is more complex, a matrix of 1000 * 1000 ):
> > > >
> > > > the names of the columns are 0, 10000, 40000, 80000, etc
> > > > the names of the rows are 0, 10000, 40000, 80000, etc
> > > >
> > > >            0 200000 400000
> > > > 0          0       0       0
> > > > 200000  0       0       0
> > > > 400000  0       0       0
> > > >
> > > > shall I use the command :
> > > >
> > > > y <- read.table("file",row.names=1, header=T)
> > > >
> > > > the results is :
> > > >
> > > >> y[1:3,1:3]
> > > >        X0 X200000 X400000
> > > > 0       0       0       0
> > > > 200000  0       0       0
> > > > 400000  0       0       0
> > > >
> > > > The question is : why R adds an X to the names of the columns eg X0,
> > > > X20000, X40000, when it shall be only 0, 20000, 40000 ? thanks !
> > > >
> > > > -- bogdan
> > > >
> > > >         [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711779&i=3> mailing list --
> To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711779&i=4>
> mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711779&i=5>
> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://r.789695.n4.nabble.com/reading-files-with-name-columns-and-row-columns-tp4711774p4711779.html
> To start a new topic under R help, email
> ml-node+s789695n789696h75 at n4.nabble.com
> To unsubscribe from R, click here
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
> .
> NAML
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://r.789695.n4.nabble.com/reading-files-with-name-columns-and-row-columns-tp4711774p4711780.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Thu Sep  3 09:14:29 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 3 Sep 2015 07:14:29 +0000
Subject: [R] Converting CSV file to numeric and calculate ttest
In-Reply-To: <CAPR_rLGgd4sUiKboDvrtFqasLRHzhsdrMqAmwj=oFLcBfhUdJA@mail.gmail.com>
References: <1441220823200-4711770.post@n4.nabble.com>
	<CAPR_rLGgd4sUiKboDvrtFqasLRHzhsdrMqAmwj=oFLcBfhUdJA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DEE6@SRVEXCHMBX.precheza.cz>

Hi

p value of what? How do you know that it should be 0.3?

Your HTML post is mangled and almost unreadable.

Your code is not reproducible, we do not have AT1.csv

AT1 shall be numeric so your reading process is wrong and following conversion to numeric shall be umnnecessary if you read your data properly.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of shawin
> Sent: Wednesday, September 02, 2015 9:08 PM
> To: r-help at r-project.org
> Subject: Re: [R] Converting CSV file to numeric and calculate ttest
>
> p value should be 0.3
>
> On Wed, Sep 2, 2015 at 8:07 PM, Navien [via R] <
> ml-node+s789695n4711770h69 at n4.nabble.com> wrote:
>
> > I have a csv file i convert it to numeric value , then i apply t-test
> > , but t-test result is not correct: please could you guide me. the
> data :
> > ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 1367452_at
> > 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> > 9.640816474 9.606262272 9.329651027 9.434541611 9.473922432
> > 9.311412966
> > 9.3154885 9.434977488 9.470895414 9.764258059 1367453_at 8.630629966
> > 8.55831075 8.788391003 8.576231135 8.671587906
> > 8.842979993 8.861958856 8.58330436 8.603596508 8.570129609 8.59798922
> > 8.572686772 8.679751791 8.663950953 8.432875347 1367454_at
> 9.354748885
> > 9.367668838 9.259952558 9.421538213 9.554635162
> > 9.603744578 9.452197983 9.284228877 9.404607878 9.317737979
> > 9.343115301
> > 9.310644266 9.27227486 9.360337823 9.44706281 1367455_at 9.944863964
> > 9.950427516 10.19101759 10.07350804 10.03269879
> > 10.1307908 10.03487287 9.74609383 9.886379007 9.775472567 10.036596
> > 9.544738458 9.699611598 9.911962567 9.625804277
> >           AT1 <-
> > read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/AT1.csv",
> >  stringsAsFactors=TRUE)
> >
> >
> > str(AT1)
> > matAT = data.frame(AT1[,-1])
> > str(matAT)
> > write.csv(matAT,"matAT.csv",row.names=FALSE)
> > #convert the csv file to numeric
> > matAT<-as.matrix(sapply(matAT,as.numeric))
> > str(matAT)
> > group<-factor(rep(c("Ob","Ln"),times=c(7,8)))
> > t.test(matAT[1,]~group)
> >
> >
> >
> >
> >
> >
> >
> > ------------------------------
> > If you reply to this email, your message will be added to the
> > discussion
> > below:
> >
> > http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-
> calcu
> > late-ttest-tp4711770.html To start a new topic under R help, email
> > ml-node+s789695n789696h75 at n4.nabble.com
> > To unsubscribe from R, click here
> >
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscr
> >
> ibe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtM
> > jQ0MzkwMjQ1>
> > .
> > NAML
> >
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_vi
> >
> ewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces
> > .BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.naml.n
> > amespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabb
> >
> le.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21na
> > bble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> send_instant_em
> > ail%21nabble%3Aemail.naml>
> >
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Converting-
> CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711771.html
> Sent from the R help mailing list archive at Nabble.com.
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From petr.pikal at precheza.cz  Thu Sep  3 09:47:51 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 3 Sep 2015 07:47:51 +0000
Subject: [R] Converting CSV file to numeric and calculate ttest
In-Reply-To: <CAPR_rLGo-t_tLoBUB3GcbkKXn_TdAvVkuk685_PztHPz1n-WBA@mail.gmail.com>
References: <1441220823200-4711770.post@n4.nabble.com>
	<CAPR_rLGgd4sUiKboDvrtFqasLRHzhsdrMqAmwj=oFLcBfhUdJA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DEE6@SRVEXCHMBX.precheza.cz>
	<CAPR_rLGo-t_tLoBUB3GcbkKXn_TdAvVkuk685_PztHPz1n-WBA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DF1B@SRVEXCHMBX.precheza.cz>

Hi

what is F?

F>2 & F<2
this is FALSE for all values of F

What value you want to extract? All rows corresponding to condition or just values from specific column.

For selection values you can use e.g. [
data(iris)
sel<-iris$Sepal.Width>4
iris[sel,] or iris[sel, c(1,3,5)]

Cheers
Petr

From: Shawin Karim [mailto:shawinkarim at gmail.com]
Sent: Thursday, September 03, 2015 9:17 AM
To: PIKAL Petr
Subject: Re: [R] Converting CSV file to numeric and calculate ttest

I solve it , thanks  but i have an issue please :


have a csv file and i would like to extract the value of it with specific range fof the last column for example; F>2 & F<2 & P=1, and then write the result to csv file.

the data is :
x1

x2

x3

x4

x5

x6

9.488404

9.470895

9.282434

9.366707

9.955383

9.640816

8.63063

8.558311

8.788391

8.576231

8.671588

8.84298

9.354749

9.367669

9.259953

9.421538

9.554635

9.603745

9.944864

9.950428

10.19102

10.07351

10.0327

10.13079


Fc

p

1.037883

0.320095

1.057708

0.060132

1.065191

0.111192




I tried the code bellow but , it does not work?

FCPval <- read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv")

 c=as.data.frame(FCPval)

for (i in 1:rowN){if (C$F[i] >= 2 && C$F[i]<=-2&& C$p[i]<=3){

dfrmPFC=data.frame(Fc=FC,p=p)}
}

the error is :

Error in C$FC : object of type 'closure' is not subsettable.

can i  extract the value according to the condition

On Thu, Sep 3, 2015 at 8:14 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

p value of what? How do you know that it should be 0.3?

Your HTML post is mangled and almost unreadable.

Your code is not reproducible, we do not have AT1.csv

AT1 shall be numeric so your reading process is wrong and following conversion to numeric shall be umnnecessary if you read your data properly.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of shawin
> Sent: Wednesday, September 02, 2015 9:08 PM
> To: r-help at r-project.org<mailto:r-help at r-project.org>
> Subject: Re: [R] Converting CSV file to numeric and calculate ttest
>
> p value should be 0.3
>
> On Wed, Sep 2, 2015 at 8:07 PM, Navien [via R] <
> ml-node+s789695n4711770h69 at n4.nabble.com<mailto:ml-node%2Bs789695n4711770h69 at n4.nabble.com>> wrote:
>
> > I have a csv file i convert it to numeric value , then i apply t-test
> > , but t-test result is not correct: please could you guide me. the
> data :
> > ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 1367452_at
> > 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> > 9.640816474 9.606262272 9.329651027 9.434541611 9.473922432
> > 9.311412966
> > 9.3154885 9.434977488 9.470895414 9.764258059 1367453_at 8.630629966
> > 8.55831075 8.788391003 8.576231135 8.671587906
> > 8.842979993 8.861958856 8.58330436 8.603596508 8.570129609 8.59798922
> > 8.572686772 8.679751791 8.663950953 8.432875347 1367454_at
> 9.354748885
> > 9.367668838 9.259952558 9.421538213 9.554635162
> > 9.603744578 9.452197983 9.284228877 9.404607878 9.317737979
> > 9.343115301
> > 9.310644266 9.27227486 9.360337823 9.44706281 1367455_at 9.944863964
> > 9.950427516 10.19101759 10.07350804 10.03269879
> > 10.1307908 10.03487287 9.74609383 9.886379007 9.775472567 10.036596
> > 9.544738458 9.699611598 9.911962567 9.625804277
> >           AT1 <-
> > read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/AT1.csv",
> >  stringsAsFactors=TRUE)
> >
> >
> > str(AT1)
> > matAT = data.frame(AT1[,-1])
> > str(matAT)
> > write.csv(matAT,"matAT.csv",row.names=FALSE)
> > #convert the csv file to numeric
> > matAT<-as.matrix(sapply(matAT,as.numeric))
> > str(matAT)
> > group<-factor(rep(c("Ob","Ln"),times=c(7,8)))
> > t.test(matAT[1,]~group)
> >
> >
> >
> >
> >
> >
> >
> > ------------------------------
> > If you reply to this email, your message will be added to the
> > discussion
> > below:
> >
> > http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-
> calcu
> > late-ttest-tp4711770.html To start a new topic under R help, email
> > ml-node+s789695n789696h75 at n4.nabble.com<mailto:ml-node%2Bs789695n789696h75 at n4.nabble.com>
> > To unsubscribe from R, click here
> >
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscr
> >
> ibe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtM
> > jQ0MzkwMjQ1>
> > .
> > NAML
> >
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_vi
> >
> ewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces
> > .BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.naml.n
> > amespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabb
> >
> le.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21na
> > bble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> send_instant_em
> > ail%21nabble%3Aemail.naml>
> >
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Converting-
> CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711771.html
> Sent from the R help mailing list archive at Nabble.com.
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Thu Sep  3 12:18:12 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 3 Sep 2015 10:18:12 +0000
Subject: [R] spatial estimation
References: <CAGq2xcRvPHYzTL=qV7NUBMf8fG8c0stLJkwNEsj2574rqKL3uw@mail.gmail.com>
Message-ID: <loom.20150903T121339-119@post.gmane.org>

fabrizio renzi <fabrizio.renzi84 <at> gmail.com> writes:

> 
> I am using spdep package to do spatial estimations in R.
> 
> Anyway I was not able to find out how to compute the loglikelihood (and a
> pseudo-R^2) of a 2sls spatial model, estimated through the command stsls. I
> need some fitting indicator to compare the 2sls model with the one obtained
> through maxL (command lagsarlm).
> 

When the poster wrote to me off-list a week ago, I replied:

"Please *do* write to the R-sig-geo list rather than to me directly -
others can answer your question as well, perhaps better, and in a more
timely way than I can. In addition, threads in the list can be searched in
the archives, so others can avoid the same problem later.

Please when you post indicate the published paper defining the LL of this
GMM-estimated model."

It is instructive to run:

library(sem)
logLik(tsls(Q ~ P + D, ~ D + F + A, data=Kmenta))

which explains why I asked for a publication providing something which
doesn't spring naturally from the estimation method.

Roger Bivand



> Thanks for your answer,
> 
> Regards
>


From bgunter.4567 at gmail.com  Thu Sep  3 16:38:51 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 3 Sep 2015 07:38:51 -0700
Subject: [R] Urgent query
In-Reply-To: <CAPoOT9odELqeX0bwaKPy=OTecTrAKdjYL-F3NZYcYycOVtsxvQ@mail.gmail.com>
References: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>
	<CAGxFJbSFhX1tvX7Nynk-JgNL5pCXr0GgfJsqn9rPc_MOznBFzQ@mail.gmail.com>
	<CAPoOT9odELqeX0bwaKPy=OTecTrAKdjYL-F3NZYcYycOVtsxvQ@mail.gmail.com>
Message-ID: <CAGxFJbTJHFK8dYYdXOJUYgiYjJnLE7otvp+b5KMnZw3sQVGCaQ@mail.gmail.com>

1. Always reply to the list, not just to 1 person.

2. I don't do private consulting.

3. Read the posting guide linked below for how to properly post code.

Cheers,

Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 2, 2015 at 11:59 PM, Romana Shehla <romana.stats at gmail.com> wrote:
> Sir , I have attached the pdf file of codes.
>
> With regards,
> Romana Shehla
> Research Scholar
> AMU
>
> On Thu, Sep 3, 2015 at 12:15 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> Please show us the code that you used that failed. There are several
>> different graphics systems in R, and the answer depends on which
>> system you use.
>>
>> Cheers,
>> Bert
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>    -- Clifford Stoll
>>
>>
>> On Wed, Sep 2, 2015 at 8:21 AM, Romana Shehla <romana.stats at gmail.com>
>> wrote:
>> > Sir/Madam,
>> >
>> >           I am a scholar at Aligarh Muslim University and have been
>> > continuously using R language. Recently, I am stuck on labeling axis of
>> > the
>> > graphic, I have made. Actually, I want to use mathematical notation
>> >  0<beta<1 at the axis. I know how to write the greek letter but I am
>> > unable
>> > to show the interval (0,1) .  I would be really grateful if you could
>> > help
>> > me with this problem.
>> > Waiting for your reply
>> >
>> > With regards,
>> > Romana Shehla
>> > Research Scholar
>> > AMU
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>
>


From canamika at gmail.com  Thu Sep  3 15:29:24 2015
From: canamika at gmail.com (Anamika Chaudhuri)
Date: Thu, 3 Sep 2015 09:29:24 -0400
Subject: [R] How to use dwish in MCMCpack in R
Message-ID: <CALv--dYJXv7fCfnu+c=5h9o_QDKPeWFpSsav-HXKepq2d4KkAA@mail.gmail.com>

Hi All:

I am trying to use the dwish function in the MCMCpack in R

dwish(W, v, S) where

Arguments
W-Positive definite matrix W
v-Degrees of freedom (scalar).
S-Inverse scale matrix

How do I determine W, the positive definite matrix. The matrix provided in
the documentation doesnot help.

Thanks
Anamika

	[[alternative HTML version deleted]]


From joyaa at goldcoastosteopathy.com.au  Thu Sep  3 14:31:12 2015
From: joyaa at goldcoastosteopathy.com.au (Joyaa Antares)
Date: Thu, 3 Sep 2015 22:31:12 +1000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 10
Message-ID: <823426D10F76410EB7709DB1CE9FEF61@Zeus>

Hello r-help,

I am quite new to R.  I downloaded R 3.2.2 for Windows to use with Windows 10.   

On attempting to load the RcmdrPlugin.survival plug-in I got this error message:

Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") : 
  [tcl] invalid command name "configure".

The lead-up to this was:

* I uninstalled R 3.2.2 and deleted all R files I could find.
* Downloaded R 3.2.2 to do a fresh install
* Opened the .exe file as Administrator
* Ran 64-bit R as Administrator
* Updated all packages 
* Installed Rcmdr and RcmdrPlugin.survival
* Ran Rcmdr using library(Rcmdr) from the console
* This required an install of a host of packages, which I installed
* Loaded RcmdrPlugin.survival from the Tools menu
* I was asked to restart R commander, and when I tried it didn?t open properly (showing only File/Edit/Data in the menu bar), and this wouldn?t close properly either.  The error message as above showed in the console.

I haven?t posted to the list before and would be very grateful for help.  Thank you.   Joyaa
	[[alternative HTML version deleted]]


From oleg.portnoy75 at gmail.com  Thu Sep  3 15:34:40 2015
From: oleg.portnoy75 at gmail.com (Oleg Portnoy)
Date: Thu, 3 Sep 2015 16:34:40 +0300
Subject: [R] R programming question
Message-ID: <CAN4Z=h4dqPV5LxGKP1hLh0SdHQokZm2dPD5KB0P5XAqqB+bz8w@mail.gmail.com>

I have two vectors of probability distribution of the same length n for
random variable X and Y. I want to built matrix of two-dimension
distribution for X and Y with assumption of independence.
P(X=i,Y=j)=P(X=i)*P(Y=j)
I want to do this very fast for n=120 and big amount of different
distribution of X and Y.
Thanks.

	[[alternative HTML version deleted]]


From p.jagadish at inrhythm-inc.com  Thu Sep  3 09:39:09 2015
From: p.jagadish at inrhythm-inc.com (jagadishpchary)
Date: Thu, 3 Sep 2015 00:39:09 -0700 (PDT)
Subject: [R] Unable to display titles in final merged SPSS output file -
 when merged the two SPSS data sets in R
Message-ID: <1441265949485-4711786.post@n4.nabble.com>

I have two large data sets in SPSS which I am trying to merge the files using
R - (for test purpose I included only two variables with 4 cases) and below
is the R syntax that I am using for merging the files.

data <- read.spss("D:/R_merge/Data.sav")
data1 <- read.spss("D:/R_merge/Data1.sav")
mydata <- smartbind(data, data1)
write.foreign(mydata, "D:/R_merge/data.txt", "D:/R_merge/data.sps",  
package="SPSS")

The issue is: when I export the data from R to SPSS the value labels
(titles) are not shown in the final merged SPSS data set - only the variable
name is shown at the title. 

I have also tried with "haven" package but the smartbind & rbind are working
properly

data <- read_sav("D:/R_merge/Data.sav")
data1 <- read_sav("D:/R_merge/Data1.sav")
mydata <- rbind(data, data1)
write_sav(data.frame(mydata), "D:/R_merge/test2.sav")

Could anyone please help me out. I am attaching 2 spss files. DATA.sav
<http://r.789695.n4.nabble.com/file/n4711786/DATA.sav>  
DATA1.sav <http://r.789695.n4.nabble.com/file/n4711786/DATA1.sav>  



--
View this message in context: http://r.789695.n4.nabble.com/Unable-to-display-titles-in-final-merged-SPSS-output-file-when-merged-the-two-SPSS-data-sets-in-R-tp4711786.html
Sent from the R help mailing list archive at Nabble.com.


From thierry.onkelinx at inbo.be  Thu Sep  3 17:01:50 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 3 Sep 2015 17:01:50 +0200
Subject: [R] R programming question
In-Reply-To: <CAN4Z=h4dqPV5LxGKP1hLh0SdHQokZm2dPD5KB0P5XAqqB+bz8w@mail.gmail.com>
References: <CAN4Z=h4dqPV5LxGKP1hLh0SdHQokZm2dPD5KB0P5XAqqB+bz8w@mail.gmail.com>
Message-ID: <CAJuCY5yeezu84pCvNGZm7D9sdX82z78CbcOB+DEtPf+dxL6eTw@mail.gmail.com>

Have a look at ?outer

outer(1:10, 10:1)

Best regards,
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no
more than asking him to perform a post-mortem examination: he may be
able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does
not ensure that a reasonable answer can be extracted from a given body
of data. ~ John Tukey


2015-09-03 15:34 GMT+02:00 Oleg Portnoy <oleg.portnoy75 at gmail.com>:
> I have two vectors of probability distribution of the same length n for
> random variable X and Y. I want to built matrix of two-dimension
> distribution for X and Y with assumption of independence.
> P(X=i,Y=j)=P(X=i)*P(Y=j)
> I want to do this very fast for n=120 and big amount of different
> distribution of X and Y.
> Thanks.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Thu Sep  3 17:29:52 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Thu, 3 Sep 2015 15:29:52 +0000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
 10
In-Reply-To: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>

Dear Joyaa,

I too run R 3.2.2 on a Windows 10 system. I uninstalled R, deleted my package library, and then reinstalled R and the Rcmdr and RcmdrPlugin.survival packages. I took all defaults, except that I selected the SDI rather than the default MDI for Rgui, but I seriously doubt that this is the source of your problem. I used the 0-Cloud mirror, both for R 3.2.2 and for packages.

I'm afraid that I can't duplicate your problem -- everything works perfectly fine for me.

You provided a reasonable amount of detail, but is there anything else you can add?

Best,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
web: socserv.mcmaster.ca/jfox


________________________________________
From: R-help [r-help-bounces at r-project.org] on behalf of Joyaa Antares [joyaa at goldcoastosteopathy.com.au]
Sent: September 3, 2015 8:31 AM
To: r-help at r-project.org
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 10

Hello r-help,

I am quite new to R.  I downloaded R 3.2.2 for Windows to use with Windows 10.

On attempting to load the RcmdrPlugin.survival plug-in I got this error message:

Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
  [tcl] invalid command name "configure".

The lead-up to this was:

* I uninstalled R 3.2.2 and deleted all R files I could find.
* Downloaded R 3.2.2 to do a fresh install
* Opened the .exe file as Administrator
* Ran 64-bit R as Administrator
* Updated all packages
* Installed Rcmdr and RcmdrPlugin.survival
* Ran Rcmdr using library(Rcmdr) from the console
* This required an install of a host of packages, which I installed
* Loaded RcmdrPlugin.survival from the Tools menu
* I was asked to restart R commander, and when I tried it didn?t open properly (showing only File/Edit/Data in the menu bar), and this wouldn?t close properly either.  The error message as above showed in the console.

I haven?t posted to the list before and would be very grateful for help.  Thank you.   Joyaa
        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Thu Sep  3 18:28:20 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 3 Sep 2015 12:28:20 -0400
Subject: [R] How to use dwish in MCMCpack in R
In-Reply-To: <CALv--dYJXv7fCfnu+c=5h9o_QDKPeWFpSsav-HXKepq2d4KkAA@mail.gmail.com>
References: <CALv--dYJXv7fCfnu+c=5h9o_QDKPeWFpSsav-HXKepq2d4KkAA@mail.gmail.com>
Message-ID: <55E87524.3050906@gmail.com>

On 03/09/2015 9:29 AM, Anamika Chaudhuri wrote:
> Hi All:
> 
> I am trying to use the dwish function in the MCMCpack in R
> 
> dwish(W, v, S) where
> 
> Arguments
> W-Positive definite matrix W
> v-Degrees of freedom (scalar).
> S-Inverse scale matrix
> 
> How do I determine W, the positive definite matrix. The matrix provided in
> the documentation doesnot help.

Your question doesn't make sense.  You want to compute the density of a
Wishart distribution.  That's a distribution of matrices.  So you just
put in the matrix at which you want to compute the density.

Duncan Murdoch


From murdoch.duncan at gmail.com  Thu Sep  3 18:47:45 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 3 Sep 2015 12:47:45 -0400
Subject: [R] How to use dwish in MCMCpack in R
In-Reply-To: <CALv--dakoEseg1H1AWu4Nh3zFCxd0xNDTu_nWNzbE=K5SSLDYA@mail.gmail.com>
References: <CALv--dYJXv7fCfnu+c=5h9o_QDKPeWFpSsav-HXKepq2d4KkAA@mail.gmail.com>
	<55E87524.3050906@gmail.com>
	<CALv--dakoEseg1H1AWu4Nh3zFCxd0xNDTu_nWNzbE=K5SSLDYA@mail.gmail.com>
Message-ID: <55E879B1.4050802@gmail.com>

On 03/09/2015 12:41 PM, Anamika Chaudhuri wrote:
> I put in the matrix at which I want the density as S. I was wondering
> what is W then?

You need to read the help page.  The matrix at which you want the
density is W.  S is a parameter of the distribution.

Duncan Murdoch

> 
> Thanks
> Anamika
> 
> On Thu, Sep 3, 2015 at 12:28 PM, Duncan Murdoch
> <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>     On 03/09/2015 9:29 AM, Anamika Chaudhuri wrote:
>     > Hi All:
>     >
>     > I am trying to use the dwish function in the MCMCpack in R
>     >
>     > dwish(W, v, S) where
>     >
>     > Arguments
>     > W-Positive definite matrix W
>     > v-Degrees of freedom (scalar).
>     > S-Inverse scale matrix
>     >
>     > How do I determine W, the positive definite matrix. The matrix provided in
>     > the documentation doesnot help.
> 
>     Your question doesn't make sense.  You want to compute the density of a
>     Wishart distribution.  That's a distribution of matrices.  So you just
>     put in the matrix at which you want to compute the density.
> 
>     Duncan Murdoch
> 
>


From bgunter.4567 at gmail.com  Thu Sep  3 19:34:29 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 3 Sep 2015 10:34:29 -0700
Subject: [R] Urgent query
In-Reply-To: <CAPoOT9osKbkvdzuaTAR+F5_fy2zxMh2jivfpMLyTVcCCqswFhQ@mail.gmail.com>
References: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>
	<CAGxFJbSFhX1tvX7Nynk-JgNL5pCXr0GgfJsqn9rPc_MOznBFzQ@mail.gmail.com>
	<CAPoOT9odELqeX0bwaKPy=OTecTrAKdjYL-F3NZYcYycOVtsxvQ@mail.gmail.com>
	<CAGxFJbTJHFK8dYYdXOJUYgiYjJnLE7otvp+b5KMnZw3sQVGCaQ@mail.gmail.com>
	<CAPoOT9osKbkvdzuaTAR+F5_fy2zxMh2jivfpMLyTVcCCqswFhQ@mail.gmail.com>
Message-ID: <CAGxFJbSX1F=B28p+W1y2-mmN-FUopMih4PA49kLAW8+rmSDEAw@mail.gmail.com>

You still failed to post to the list! Please learn how to use your
mail client properly and **always** cc the list.

Anyway, here is an answer (there may be other ways):

> curve(dweibull(x,shape=0.8,scale=1),mgp=c(2,0.3,0),cex.lab=1.2,xlab="t",ylab="f(t)")
> mtext(expression(italic(paste(alpha==1,","," ",0<phantom(),beta<1))),side=1,line=4,cex=1)



Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Sep 3, 2015 at 10:16 AM, Romana Shehla <romana.stats at gmail.com> wrote:
> Dear R mailing list,
>              I apologise for not being correct at posting my problem. I read
> the posting guide and got somewhat confused. I am really scared if I am
> still correct or not. If I am not, please pardon me.
> Following are the commands. Could you please let me know about how the limit
> 0 < ? < 1 can be introduced at x-axis instead of ? < 1, which I have
> written.
>
>
>
>
>>curve(dweibull(x,shape=0.8,scale=1),mgp=c(2,0.3,0),cex.lab=1.2,xlab="t",ylab="f(t)")
>
>>mtext(expression(italic(paste(alpha==1,",","
>> ",beta<1))),side=1,line=4,cex=1)
>
>
>
> With regards,
> Romana Shehla
> Research Scholar
> AMU
>
> On Thu, Sep 3, 2015 at 8:08 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> 1. Always reply to the list, not just to 1 person.
>>
>> 2. I don't do private consulting.
>>
>> 3. Read the posting guide linked below for how to properly post code.
>>
>> Cheers,
>>
>> Bert
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>    -- Clifford Stoll
>>
>>
>> On Wed, Sep 2, 2015 at 11:59 PM, Romana Shehla <romana.stats at gmail.com>
>> wrote:
>> > Sir , I have attached the pdf file of codes.
>> >
>> > With regards,
>> > Romana Shehla
>> > Research Scholar
>> > AMU
>> >
>> > On Thu, Sep 3, 2015 at 12:15 AM, Bert Gunter <bgunter.4567 at gmail.com>
>> > wrote:
>> >>
>> >> Please show us the code that you used that failed. There are several
>> >> different graphics systems in R, and the answer depends on which
>> >> system you use.
>> >>
>> >> Cheers,
>> >> Bert
>> >> Bert Gunter
>> >>
>> >> "Data is not information. Information is not knowledge. And knowledge
>> >> is certainly not wisdom."
>> >>    -- Clifford Stoll
>> >>
>> >>
>> >> On Wed, Sep 2, 2015 at 8:21 AM, Romana Shehla <romana.stats at gmail.com>
>> >> wrote:
>> >> > Sir/Madam,
>> >> >
>> >> >           I am a scholar at Aligarh Muslim University and have been
>> >> > continuously using R language. Recently, I am stuck on labeling axis
>> >> > of
>> >> > the
>> >> > graphic, I have made. Actually, I want to use mathematical notation
>> >> >  0<beta<1 at the axis. I know how to write the greek letter but I am
>> >> > unable
>> >> > to show the interval (0,1) .  I would be really grateful if you could
>> >> > help
>> >> > me with this problem.
>> >> > Waiting for your reply
>> >> >
>> >> > With regards,
>> >> > Romana Shehla
>> >> > Research Scholar
>> >> > AMU
>> >> >
>> >> >         [[alternative HTML version deleted]]
>> >> >
>> >> > ______________________________________________
>> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > PLEASE do read the posting guide
>> >> > http://www.R-project.org/posting-guide.html
>> >> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>
>


From davidsmi at microsoft.com  Thu Sep  3 19:41:07 2015
From: davidsmi at microsoft.com (David Smith)
Date: Thu, 3 Sep 2015 17:41:07 +0000
Subject: [R] Revolutions blog: August 2015 roundup
Message-ID: <B7C74A84-F4DE-4B46-A752-3A4CC941378B@microsoft.com>

Since 2008, Revolution Analytics (and now Microsoft) staff and guests have written about R every weekday at the Revolutions blog:
 http://blog.revolutionanalytics.com
and every month I post a summary of articles from the previous month of particular interest to readers of r-help. 

In case you missed them, here are some articles related to R from the month of August:

Creating interactive time series charts of financial data in R: http://blog.revolutionanalytics.com/2015/08/plotting-time-series-in-r.html

Many R books have been translated into Chinese: http://blog.revolutionanalytics.com/2015/08/new-r-titles-available-in-chinese.html

A tutorial on visualizing current-events geographic data with choropleths: http://blog.revolutionanalytics.com/2015/08/following-uo-on-the-news-with-choroplether-and-r.html

Revolution R Enterprise 7.4.1 is now available on Windows and Linux servers http://blog.revolutionanalytics.com/2015/08/rre-741.html and in the Azure Marketplace
http://blog.revolutionanalytics.com/2015/08/revolution-r-enterprise-now-available-in-the-cloud-on-azure-marketplace.html

Zillow uses R to estimate the value of houses and rental properties: http://blog.revolutionanalytics.com/2015/08/how-r-is-used-at-zillow-to-estimate-housing-values.html

There?s a new (and free) online course on edX for R beginners, sponsored by Microsoft and presented by DataCamp: http://blog.revolutionanalytics.com/2015/08/free-edx-course-for-r-beginners.html
 

Mini-reviews of 5 new R packages: AzureML, distcomp, rotationForest, rpca, and SwarmSVM: http://blog.revolutionanalytics.com/2015/08/5-new-r-packages-for-data-scientists.html

The R Consortium?s best practices for secure use of R: http://blog.revolutionanalytics.com/2015/08/good-advice-for-security-with-r.html

How to extract data from a SQL Server database in Azure to an R client running Linux: http://blog.revolutionanalytics.com/2015/08/using-azure-as-an-r-datasource-part-4-pulling-data-from-sql-server-to-linux.html 

DeployR Open 7.4.1, the open-source server-based framework for simple and secure R integration for application developers, is now available: http://blog.revolutionanalytics.com/2015/08/deployr-741-released.html

R 3.2.2 is now available: http://blog.revolutionanalytics.com/2015/08/r-322-released.html

A review of the JSM 2015 conference and the prevalence of R there: http://blog.revolutionanalytics.com/2015/08/r-news-from-jsm-2015.html

R is available with Cortana Analytics, which you can learn about in upcoming workshops and webinars: http://blog.revolutionanalytics.com/2015/08/get-to-know-cortana-analytics-workshop-and-webinars.html

A comparison of the network structure of the CRAN and Bioconductor repositories: http://blog.revolutionanalytics.com/2015/08/differences-in-the-network-structure-of-cran-and-bioconductor.html

Using R to find signal in noisy data: http://blog.revolutionanalytics.com/2015/08/how-do-you-know-if-yourdata-has-signal.html

I discussed the R Consortium in the inaugural episode of the R Talk podcast: http://blog.revolutionanalytics.com/2015/08/r-talk.html

An exponential random graph model of connections between CRAN packages: http://blog.revolutionanalytics.com/2015/08/a-simple-statnet-model-of-cran.html

Using the igraph package to simplify a network graph: http://blog.revolutionanalytics.com/2015/08/contracting-and-simplifying-a-network-graph.html

An introductory guide to the Bioconductor project: http://blog.revolutionanalytics.com/2015/08/a-short-introduction-to-bioconductor.html 

An animation shows every commit to the R source code over 18 years: http://blog.revolutionanalytics.com/2015/08/watch-18-years-of-r-development-in-15-minutes.html

General interest stories (not related to R) in the past month included: Macklemore on mopeds (http://blog.revolutionanalytics.com/2015/08/because-its-friday-downtown-spokane.html), reconstructed timelapses (http://blog.revolutionanalytics.com/2015/08/timelapses.html), a moving visualization of WW2 fatalities (http://blog.revolutionanalytics.com/2015/08/a-moving-vision-of-ww2-fatalities.html), and the Magnus Effect (http://blog.revolutionanalytics.com/2015/08/magnus-effect.html).

Meeting times for local R user groups (http://blog.revolutionanalytics.com/local-r-groups.html) can be found on the updated R Community Calendar at: http://blog.revolutionanalytics.com/calendar.html

If you're looking for more articles about R, you can find summaries from previous months at http://blog.revolutionanalytics.com/roundups/. You can receive daily blog posts via email using services like blogtrottr.com, or join the Revolution Analytics mailing list at http://revolutionanalytics.com/newsletter to be alerted to new articles on a monthly basis.

As always, thanks for the comments and please keep sending suggestions to me at davidsmi at microsoft.com or via Twitter (I'm @revodavid).

Cheers,
# David


-- 
David M Smith <davidsmi at microsoft.com>
R Community Lead, Microsoft
Twitter: @revodavid
Blog:  http://blog.revolutionanalytics.com <http://blog.revolutionanalytics.com/>
We?re hiring! http://azuremljobs.github.io/

From aline.andrey1 at gmail.com  Thu Sep  3 17:57:12 2015
From: aline.andrey1 at gmail.com (Aline Andrey)
Date: Thu, 3 Sep 2015 17:57:12 +0200
Subject: [R] urgent question about Lmer models
Message-ID: <CAA+PV2Suck-SBQ0hstjnpkFDJD4HFkqKGVCNfbMoNvAW_=6K6Q@mail.gmail.com>

Dear r-help,

I have a question about the std error of a lmer model with library
(LmerTest).

I have 12 sites with 6 treatments over each site. I measured some response
variable (biomass_of_insects) (with a gaussian distribution).

I did :
library (LmerTest)
model<- lmer((biomass_of_insects) ~ as.factor(treatments) + (1 | sites))

However, the response of the model show always the same std error (see
below):

Fixed effects:
                       Estimate Std. Error      df t value Pr(>|t|)
(Intercept)             501.333     80.656  66.000   6.216 3.91e-08 ***
as.factor(treat)F       126.667    114.065  66.000   1.110    0.271
as.factor(treat)I        -8.333    114.065  66.000  -0.073    0.942
as.factor(treat)I+F1/3  -75.000    114.065  66.000  -0.658    0.513
as.factor(treat)I+F2/3   18.333    114.065  66.000   0.161    0.873
as.factor(treat)I+F3/3   15.917    114.065  66.000   0.140    0.889



Do you know why std.error is always the same ?

Thank you very much,

Aline

	[[alternative HTML version deleted]]


From canamika at gmail.com  Thu Sep  3 18:41:34 2015
From: canamika at gmail.com (Anamika Chaudhuri)
Date: Thu, 3 Sep 2015 12:41:34 -0400
Subject: [R] How to use dwish in MCMCpack in R
In-Reply-To: <55E87524.3050906@gmail.com>
References: <CALv--dYJXv7fCfnu+c=5h9o_QDKPeWFpSsav-HXKepq2d4KkAA@mail.gmail.com>
	<55E87524.3050906@gmail.com>
Message-ID: <CALv--dakoEseg1H1AWu4Nh3zFCxd0xNDTu_nWNzbE=K5SSLDYA@mail.gmail.com>

I put in the matrix at which I want the density as S. I was wondering what
is W then?

Thanks
Anamika

On Thu, Sep 3, 2015 at 12:28 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 03/09/2015 9:29 AM, Anamika Chaudhuri wrote:
> > Hi All:
> >
> > I am trying to use the dwish function in the MCMCpack in R
> >
> > dwish(W, v, S) where
> >
> > Arguments
> > W-Positive definite matrix W
> > v-Degrees of freedom (scalar).
> > S-Inverse scale matrix
> >
> > How do I determine W, the positive definite matrix. The matrix provided
> in
> > the documentation doesnot help.
>
> Your question doesn't make sense.  You want to compute the density of a
> Wishart distribution.  That's a distribution of matrices.  So you just
> put in the matrix at which you want to compute the density.
>
> Duncan Murdoch
>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Sep  3 20:39:43 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 3 Sep 2015 11:39:43 -0700
Subject: [R] urgent question about Lmer models
In-Reply-To: <CAA+PV2Suck-SBQ0hstjnpkFDJD4HFkqKGVCNfbMoNvAW_=6K6Q@mail.gmail.com>
References: <CAA+PV2Suck-SBQ0hstjnpkFDJD4HFkqKGVCNfbMoNvAW_=6K6Q@mail.gmail.com>
Message-ID: <CAGxFJbSPdsKhfQ0kAoQMt4VsJMqnjJTZwqoVxqLG6C_UfXJRTg@mail.gmail.com>

You need to consult a local statistical expert or post on a statistics
list like stats.stackexchange.com .  This is a statistical question
not an R question.

The answer is: Because the design is balanced and the treatment error
is obtained from the (within site) rsd, but you will probably need
more explanation than this.

BTW, if you haven't already done so, try making some informative
trellised plots to understand what is going on. Formal statistical
analysis alone can be very misleading.

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Sep 3, 2015 at 8:57 AM, Aline Andrey <aline.andrey1 at gmail.com> wrote:
> Dear r-help,
>
> I have a question about the std error of a lmer model with library
> (LmerTest).
>
> I have 12 sites with 6 treatments over each site. I measured some response
> variable (biomass_of_insects) (with a gaussian distribution).
>
> I did :
> library (LmerTest)
> model<- lmer((biomass_of_insects) ~ as.factor(treatments) + (1 | sites))
>
> However, the response of the model show always the same std error (see
> below):
>
> Fixed effects:
>                        Estimate Std. Error      df t value Pr(>|t|)
> (Intercept)             501.333     80.656  66.000   6.216 3.91e-08 ***
> as.factor(treat)F       126.667    114.065  66.000   1.110    0.271
> as.factor(treat)I        -8.333    114.065  66.000  -0.073    0.942
> as.factor(treat)I+F1/3  -75.000    114.065  66.000  -0.658    0.513
> as.factor(treat)I+F2/3   18.333    114.065  66.000   0.161    0.873
> as.factor(treat)I+F3/3   15.917    114.065  66.000   0.140    0.889
>
>
>
> Do you know why std.error is always the same ?
>
> Thank you very much,
>
> Aline
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bbolker at gmail.com  Thu Sep  3 22:04:39 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 3 Sep 2015 20:04:39 +0000
Subject: [R] urgent question about Lmer models
References: <CAA+PV2Suck-SBQ0hstjnpkFDJD4HFkqKGVCNfbMoNvAW_=6K6Q@mail.gmail.com>
	<CAGxFJbSPdsKhfQ0kAoQMt4VsJMqnjJTZwqoVxqLG6C_UfXJRTg@mail.gmail.com>
Message-ID: <loom.20150903T213844-803@post.gmane.org>

Bert Gunter <bgunter.4567 <at> gmail.com> writes:

> 
> You need to consult a local statistical expert or post on a statistics
> list like stats.stackexchange.com .  This is a statistical question
> not an R question.
> 
> The answer is: Because the design is balanced and the treatment error
> is obtained from the (within site) rsd, but you will probably need
> more explanation than this.
> 
> BTW, if you haven't already done so, try making some informative
> trellised plots to understand what is going on. Formal statistical
> analysis alone can be very misleading.
> 
> Cheers,
> Bert
> 
> Bert Gunter

   That's a good explanation, very similar to the one I posted
at http://stackoverflow.com/questions/32383259/
   lmertest-package-standard-errors  (broken URL for Gmane).

  I know most people who read this message will already know this,
but (to the original poster) *please don't cross-post to R lists and
StackOverflow*; it causes wasted effort, as in this case.

  Ben Bolker


From wjhopper510 at gmail.com  Thu Sep  3 17:14:16 2015
From: wjhopper510 at gmail.com (Will Hopper)
Date: Thu, 3 Sep 2015 11:14:16 -0400
Subject: [R] optimx parameter scale warning
Message-ID: <CAGTXQPtUhYx=fgc5BBSNW3_Mqx52cVQJiccvEq=Wi4O5dU3dng@mail.gmail.com>

 Hi all,

I'm using optimx (version 2013.8.7) to perform parameter estimation with
the nelder-mead method, and received a warning that the parameters are on
different scales, which can hurt optimization performance on derivative
free methods. This warning is accurate as some parameters are small
(between 0 and 1) and others can be quite large.

So, I tried using the parscale option in the control list to put them on
more equal scaling. There are 8 parameters, so I supplied a vector of
length 8 to the parscale option, which if I understand correctly, would
scale the large parameters down by their starting values so everything will
start out near 1 Here is the call to optimx itself I used:

    fit <- optimx(par = c(ER=.6,LR=.035,TR
=.05,FR=.1,alpha=50,lambda=.4,Tmin=.5,Tmax=40),
                  fn = RT_ErrorFcn,
                  method = "Nelder-Mead"
                  control = list(maxit=1000,
                                     parscale = c(1,1,1,1,50, 1,1,40)),
                  fcn = model$fn, # passed to RT_ErrorFcn
                  fix = model$fixed,  #RT_ErrorFcn
                  obs = data) # passed to RT_ErrorFcn


However, the warning about different parameter scales is still given. Is it
intended behavior for the warning to be given even when the parscale option
is used? Or am I misunderstanding the point of the parscale option, or
implementing it wrong?

Thanks for any info on this topic.

- Will

	[[alternative HTML version deleted]]


From glennmschultz at me.com  Thu Sep  3 17:23:16 2015
From: glennmschultz at me.com (Glenn Schultz)
Date: Thu, 03 Sep 2015 15:23:16 +0000 (GMT)
Subject: [R] Extracting XML value
Message-ID: <797dc3bb-1818-4d77-9f51-9cd1c158bc98@me.com>

All,

I have made it as far as generating an api call which returns the following xml
[1] "<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<observations realtime_start=\"2015-09-03\" realtime_end=\"2015-09-03\" observation_start=\"2015-09-01\" observation_end=\"2015-09-01\" units=\"lin\" output_type=\"1\" file_type=\"xml\" order_by=\"observation_date\" sort_order=\"asc\" count=\"1\" offset=\"0\" limit=\"100000\">\n ?<observation realtime_start=\"2015-09-03\" realtime_end=\"2015-09-03\" date=\"2015-09-01\" value=\"0.46\"/>\n</observations>\n\n\n\n"
attr(,"Content-Type")
? ? ? ? ? ? ? charset?
"text/xml" ? ?"UTF-8"?

following DTL's presentation on the Berkley site and the package help I parsed the xml

doc = xmlTreeParse(USSW10, asText = TRUE, useInternal = TRUE)

which gives
<?xml version="1.0" encoding="utf-8"?>
<observations realtime_start="2015-09-03" realtime_end="2015-09-03" observation_start="2015-09-01" observation_end="2015-09-01" units="lin" output_type="1" file_type="xml" order_by="observation_date" sort_order="asc" count="1" offset="0" limit="100000">
? <observation realtime_start="2015-09-03" realtime_end="2015-09-03" date="2015-09-01" value="0.46"/>
</observations>

finally I try to extract the value 0.46 using the xmlValue function. ?I have lost something in translation and I am unable to extract the value. ?my understanding is I have one node with no children, correct?

-Glenn



From jeff at trefftzs.org  Thu Sep  3 21:03:19 2015
From: jeff at trefftzs.org (Jeff Trefftzs)
Date: Thu, 03 Sep 2015 12:03:19 -0700
Subject: [R] ggplot2 will not install after system upgrade
Message-ID: <1441306999.3562.3.camel@trefftzs.org>

I just upgraded my laptop from Fedora Core 20 to Fedora Core 22, and
after the upgrade R can no longer use the ggplot2 library.  The
principal complaint seems to be that libicui18n.so.50 is not found. The
version of libicu that is installed is version 54.  On the other hand,
the same environment exists on my desktop computer, also with version
54 of libicu and all works just fine.  
Any hints?  Has anyone else seen this?

Thanks,

-- 
Jeff Trefftzs
http://www.trefftzs.org


From marti3sa at dukes.jmu.edu  Thu Sep  3 20:25:46 2015
From: marti3sa at dukes.jmu.edu (Martinez, Stephen Anthony - marti3sa)
Date: Thu, 3 Sep 2015 18:25:46 +0000
Subject: [R] R on Windows 10
Message-ID: <DM2PR02MB334943BFE99A5EAF8232329D2680@DM2PR02MB334.namprd02.prod.outlook.com>

Please forgive me if this is the wrong place to ask or if I missed somewhere that it mentions this but I just wanted to make sure, does the most recent version of R work on windows 10? I know under the FAQ it says XP or newer but just wanted to confirm since windows 10 is new.

Thank you! 
-Stephen


From shawinkarim at gmail.com  Thu Sep  3 08:58:52 2015
From: shawinkarim at gmail.com (shawin)
Date: Wed, 2 Sep 2015 23:58:52 -0700 (PDT)
Subject: [R] Extracting column value from csv file according to if else
 and & operator
In-Reply-To: <1441263444171-4711782.post@n4.nabble.com>
References: <1441263444171-4711782.post@n4.nabble.com>
Message-ID: <CAPR_rLHqo9as1UKoK=oTNWwv0kGEyFsQ636QbVoZoUPqF+0Faw@mail.gmail.com>

I should extract the value according to the condition above

On Thu, Sep 3, 2015 at 7:57 AM, Navien [via R] <
ml-node+s789695n4711782h41 at n4.nabble.com> wrote:

> Hi all
> I have a csv file and i would like to extract the value of it with
> specific range fof the last column for example; F>2 & F<2 & P=1, and then
> write the result to csv file.
>
> the data is :
>
> x1 x2 x3 x4 x5 x6
> 9.488404 9.470895 9.282434 9.366707 9.955383 9.640816
> 8.63063 8.558311 8.788391 8.576231 8.671588 8.84298
> 9.354749 9.367669 9.259953 9.421538 9.554635 9.603745
> 9.944864 9.950428 10.19102 10.07351 10.0327 10.13079
> Fc p
> 1.037883 0.320095
> 1.057708 0.060132
> 1.065191 0.111192
>
>
>
> I tried the code bellow but , it does not work?
>
> FCPval <-
> read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv")
>
>  c=as.data.frame(FCPval)
>
> for (i in 1:rowN){if (C$F[i] >= 2 && C$F[i]<=-2&& C$p[i]<=3){
>
> dfrmPFC=data.frame(Fc=FC,p=p)}
> }
>
> the error is :
>
> Error in C$FC : object of type 'closure' is not subsettable.
>
> can i  extract the value according to the condition
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://r.789695.n4.nabble.com/Extracting-column-value-from-csv-file-according-to-if-else-and-operator-tp4711782.html
> To start a new topic under R help, email
> ml-node+s789695n789696h75 at n4.nabble.com
> To unsubscribe from R, click here
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
> .
> NAML
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://r.789695.n4.nabble.com/Extracting-column-value-from-csv-file-according-to-if-else-and-operator-tp4711782p4711783.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From shawinkarim at gmail.com  Thu Sep  3 09:14:26 2015
From: shawinkarim at gmail.com (shawin)
Date: Thu, 3 Sep 2015 00:14:26 -0700 (PDT)
Subject: [R] Converting CSV file to numeric and calculate ttest
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DEE6@SRVEXCHMBX.precheza.cz>
References: <1441220823200-4711770.post@n4.nabble.com>
	<CAPR_rLGgd4sUiKboDvrtFqasLRHzhsdrMqAmwj=oFLcBfhUdJA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DEE6@SRVEXCHMBX.precheza.cz>
Message-ID: <CAPR_rLGKOmxhC3w4BPPwVzObhCA2VVNL7T8dQhDZHTGzhaOtaQ@mail.gmail.com>

dear Pikal , did you receive my last mail ?

On Thu, Sep 3, 2015 at 8:11 AM, PIKAL Petr [via R] <
ml-node+s789695n4711784h3 at n4.nabble.com> wrote:

> Hi
>
> p value of what? How do you know that it should be 0.3?
>
> Your HTML post is mangled and almost unreadable.
>
> Your code is not reproducible, we do not have AT1.csv
>
> AT1 shall be numeric so your reading process is wrong and following
> conversion to numeric shall be umnnecessary if you read your data properly.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help [mailto:[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711784&i=0>] On Behalf Of
> shawin
> > Sent: Wednesday, September 02, 2015 9:08 PM
> > To: [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711784&i=1>
> > Subject: Re: [R] Converting CSV file to numeric and calculate ttest
> >
> > p value should be 0.3
> >
> > On Wed, Sep 2, 2015 at 8:07 PM, Navien [via R] <
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711784&i=2>>
> wrote:
> >
> > > I have a csv file i convert it to numeric value , then i apply t-test
> > > , but t-test result is not correct: please could you guide me. the
> > data :
> > > ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 1367452_at
> > > 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> > > 9.640816474 9.606262272 9.329651027 9.434541611 9.473922432
> > > 9.311412966
> > > 9.3154885 9.434977488 9.470895414 9.764258059 1367453_at 8.630629966
> > > 8.55831075 8.788391003 8.576231135 8.671587906
> > > 8.842979993 8.861958856 8.58330436 8.603596508 8.570129609 8.59798922
> > > 8.572686772 8.679751791 8.663950953 8.432875347 1367454_at
> > 9.354748885
> > > 9.367668838 9.259952558 9.421538213 9.554635162
> > > 9.603744578 9.452197983 9.284228877 9.404607878 9.317737979
> > > 9.343115301
> > > 9.310644266 9.27227486 9.360337823 9.44706281 1367455_at 9.944863964
> > > 9.950427516 10.19101759 10.07350804 10.03269879
> > > 10.1307908 10.03487287 9.74609383 9.886379007 9.775472567 10.036596
> > > 9.544738458 9.699611598 9.911962567 9.625804277
> > >           AT1 <-
> > > read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/AT1.csv",
> > >  stringsAsFactors=TRUE)
> > >
> > >
> > > str(AT1)
> > > matAT = data.frame(AT1[,-1])
> > > str(matAT)
> > > write.csv(matAT,"matAT.csv",row.names=FALSE)
> > > #convert the csv file to numeric
> > > matAT<-as.matrix(sapply(matAT,as.numeric))
> > > str(matAT)
> > > group<-factor(rep(c("Ob","Ln"),times=c(7,8)))
> > > t.test(matAT[1,]~group)
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > ------------------------------
> > > If you reply to this email, your message will be added to the
> > > discussion
> > > below:
> > >
> > > http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-
> > calcu
> > > late-ttest-tp4711770.html To start a new topic under R help, email
> > > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711784&i=3>
> > > To unsubscribe from R, click here
> > >
> > <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscr
> > >
> > ibe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtM
> > > jQ0MzkwMjQ1>
> > > .
> > > NAML
> > >
> > <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_vi
> > >
> > ewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces
> > > .BasicNamespace-nabble.view.web.template.NabbleNamespace-
> > nabble.naml.n
> > > amespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-
> > nabb
> > >
> > le.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21na
> > > bble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> > send_instant_em
> > > ail%21nabble%3Aemail.naml>
> > >
> >
> >
> >
> >
> > --
> > View this message in context: http://r.789695.n4.nabble.com/Converting-
> > CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711771.html
> > Sent from the R help mailing list archive at Nabble.com.
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711784&i=4>
> mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
> ______________________________________________
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711784&i=5>
> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711784.html
> To start a new topic under R help, email
> ml-node+s789695n789696h75 at n4.nabble.com
> To unsubscribe from R, click here
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
> .
> NAML
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711785.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From shawinkarim at gmail.com  Thu Sep  3 09:50:27 2015
From: shawinkarim at gmail.com (shawin)
Date: Thu, 3 Sep 2015 00:50:27 -0700 (PDT)
Subject: [R] Converting CSV file to numeric and calculate ttest
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DF1B@SRVEXCHMBX.precheza.cz>
References: <1441220823200-4711770.post@n4.nabble.com>
	<CAPR_rLGgd4sUiKboDvrtFqasLRHzhsdrMqAmwj=oFLcBfhUdJA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DEE6@SRVEXCHMBX.precheza.cz>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DF1B@SRVEXCHMBX.precheza.cz>
Message-ID: <CAPR_rLHShq0OR7pR-tBS1CO4Wx3e9BnPQJN0gbx+aWu2A=j_PQ@mail.gmail.com>

Oh sorry F<2 & F<-2 ,  i need to extract the row value according to the
last two column F,P and using condition ,

I attached  an test file to see the data

On Thu, Sep 3, 2015 at 8:43 AM, PIKAL Petr [via R] <
ml-node+s789695n4711787h96 at n4.nabble.com> wrote:

> Hi
>
> what is F?
>
> F>2 & F<2
> this is FALSE for all values of F
>
> What value you want to extract? All rows corresponding to condition or
> just values from specific column.
>
> For selection values you can use e.g. [
> data(iris)
> sel<-iris$Sepal.Width>4
> iris[sel,] or iris[sel, c(1,3,5)]
>
> Cheers
> Petr
>
> From: Shawin Karim [mailto:[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=0>]
> Sent: Thursday, September 03, 2015 9:17 AM
> To: PIKAL Petr
> Subject: Re: [R] Converting CSV file to numeric and calculate ttest
>
> I solve it , thanks  but i have an issue please :
>
>
> have a csv file and i would like to extract the value of it with specific
> range fof the last column for example; F>2 & F<2 & P=1, and then write the
> result to csv file.
>
> the data is :
> x1
>
> x2
>
> x3
>
> x4
>
> x5
>
> x6
>
> 9.488404
>
> 9.470895
>
> 9.282434
>
> 9.366707
>
> 9.955383
>
> 9.640816
>
> 8.63063
>
> 8.558311
>
> 8.788391
>
> 8.576231
>
> 8.671588
>
> 8.84298
>
> 9.354749
>
> 9.367669
>
> 9.259953
>
> 9.421538
>
> 9.554635
>
> 9.603745
>
> 9.944864
>
> 9.950428
>
> 10.19102
>
> 10.07351
>
> 10.0327
>
> 10.13079
>
>
> Fc
>
> p
>
> 1.037883
>
> 0.320095
>
> 1.057708
>
> 0.060132
>
> 1.065191
>
> 0.111192
>
>
>
>
> I tried the code bellow but , it does not work?
>
> FCPval <-
> read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv")
>
>  c=as.data.frame(FCPval)
>
> for (i in 1:rowN){if (C$F[i] >= 2 && C$F[i]<=-2&& C$p[i]<=3){
>
> dfrmPFC=data.frame(Fc=FC,p=p)}
> }
>
> the error is :
>
> Error in C$FC : object of type 'closure' is not subsettable.
>
> can i  extract the value according to the condition
>
> On Thu, Sep 3, 2015 at 8:14 AM, PIKAL Petr <[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=1><mailto:[hidden
> email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=2>>> wrote:
> Hi
>
> p value of what? How do you know that it should be 0.3?
>
> Your HTML post is mangled and almost unreadable.
>
> Your code is not reproducible, we do not have AT1.csv
>
> AT1 shall be numeric so your reading process is wrong and following
> conversion to numeric shall be umnnecessary if you read your data properly.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help [mailto:[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=3><mailto:[hidden
> email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=4>>] On
> Behalf Of shawin
> > Sent: Wednesday, September 02, 2015 9:08 PM
> > To: [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=5><mailto:[hidden
> email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=6>>
> > Subject: Re: [R] Converting CSV file to numeric and calculate ttest
> >
> > p value should be 0.3
> >
> > On Wed, Sep 2, 2015 at 8:07 PM, Navien [via R] <
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=7>
> <mailto:ml-node%[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=8>>> wrote:
> >
> > > I have a csv file i convert it to numeric value , then i apply t-test
> > > , but t-test result is not correct: please could you guide me. the
> > data :
> > > ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 1367452_at
> > > 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> > > 9.640816474 9.606262272 9.329651027 9.434541611 9.473922432
> > > 9.311412966
> > > 9.3154885 9.434977488 9.470895414 9.764258059 1367453_at 8.630629966
> > > 8.55831075 8.788391003 8.576231135 8.671587906
> > > 8.842979993 8.861958856 8.58330436 8.603596508 8.570129609 8.59798922
> > > 8.572686772 8.679751791 8.663950953 8.432875347 1367454_at
> > 9.354748885
> > > 9.367668838 9.259952558 9.421538213 9.554635162
> > > 9.603744578 9.452197983 9.284228877 9.404607878 9.317737979
> > > 9.343115301
> > > 9.310644266 9.27227486 9.360337823 9.44706281 1367455_at 9.944863964
> > > 9.950427516 10.19101759 10.07350804 10.03269879
> > > 10.1307908 10.03487287 9.74609383 9.886379007 9.775472567 10.036596
> > > 9.544738458 9.699611598 9.911962567 9.625804277
> > >           AT1 <-
> > > read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/AT1.csv",
> > >  stringsAsFactors=TRUE)
> > >
> > >
> > > str(AT1)
> > > matAT = data.frame(AT1[,-1])
> > > str(matAT)
> > > write.csv(matAT,"matAT.csv",row.names=FALSE)
> > > #convert the csv file to numeric
> > > matAT<-as.matrix(sapply(matAT,as.numeric))
> > > str(matAT)
> > > group<-factor(rep(c("Ob","Ln"),times=c(7,8)))
> > > t.test(matAT[1,]~group)
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > ------------------------------
> > > If you reply to this email, your message will be added to the
> > > discussion
> > > below:
> > >
> > > http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-
> > calcu
> > > late-ttest-tp4711770.html To start a new topic under R help, email
> > > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=9>
> <mailto:ml-node%[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=10>>
> > > To unsubscribe from R, click here
> > >
> > <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscr
> > >
> > ibe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtM
> > > jQ0MzkwMjQ1>
> > > .
> > > NAML
> > >
> > <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_vi
> > >
> > ewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces
> > > .BasicNamespace-nabble.view.web.template.NabbleNamespace-
> > nabble.naml.n
> > > amespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-
> > nabb
> > >
> > le.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21na
> > > bble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> > send_instant_em
> > > ail%21nabble%3Aemail.naml>
> > >
> >
> >
> >
> >
> > --
> > View this message in context: http://r.789695.n4.nabble.com/Converting-
> > CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711771.html
> > Sent from the R help mailing list archive at Nabble.com.
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=11>
> <mailto:[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=12>> mailing list --
> To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=13>
> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711787.html
> To start a new topic under R help, email
> ml-node+s789695n789696h75 at n4.nabble.com
> To unsubscribe from R, click here
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
> .
> NAML
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>


test.csv (1K) <http://r.789695.n4.nabble.com/attachment/4711788/0/test.csv>




--
View this message in context: http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711788.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From shawinkarim at gmail.com  Thu Sep  3 13:55:11 2015
From: shawinkarim at gmail.com (shawin)
Date: Thu, 3 Sep 2015 04:55:11 -0700 (PDT)
Subject: [R] Converting CSV file to numeric and calculate ttest
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DF1B@SRVEXCHMBX.precheza.cz>
References: <1441220823200-4711770.post@n4.nabble.com>
	<CAPR_rLGgd4sUiKboDvrtFqasLRHzhsdrMqAmwj=oFLcBfhUdJA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DEE6@SRVEXCHMBX.precheza.cz>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DF1B@SRVEXCHMBX.precheza.cz>
Message-ID: <CAPR_rLEgEjYniB3jZ0HH6P7rw1TF_Nej8mzmPAEq-X7deJX0bA@mail.gmail.com>

Dear Petr,

This code does not work ?
For selection values you can use e.g. [
data(iris)
sel<-iris$Sepal.Width>4
iris[sel,] or iris[sel, c(1,3,5)]


the data is csv file which is :
FCPval <- read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv")

;

Fc2<-(FCPVal$Fc>=2 | FCPval<=-2);  this will just come out , true false
 not extracting the Fc  ,
#####################################################################################################
FCP<-as.matrix(sapply(FCPval,as.numeric))
A=as.data.frame(FCP)
Fc2<-(A$Fc>=1.3| A$Fc<=-1.3) & A$p=0.05;  // has error when i combine
condition

Error in (Fc2 <- (A$Fc >= 1.3 | A$Fc <= -1.3) & A$p) = 0.05 :
  could not find function "<-<-"

write.csv(Fc2,"Fc2.csv",row.names =FALSE )

please need help


On Thu, Sep 3, 2015 at 8:56 AM, Shawin Karim <shawinkarim at gmail.com> wrote:

> Oh sorry F<2 & F<-2 ,  i need to extract the row value according to the
> last two column F,P and using condition ,
>
> I attached  an test file to see the data
>
> On Thu, Sep 3, 2015 at 8:43 AM, PIKAL Petr [via R] <
> ml-node+s789695n4711787h96 at n4.nabble.com> wrote:
>
>> Hi
>>
>> what is F?
>>
>> F>2 & F<2
>> this is FALSE for all values of F
>>
>> What value you want to extract? All rows corresponding to condition or
>> just values from specific column.
>>
>> For selection values you can use e.g. [
>> data(iris)
>> sel<-iris$Sepal.Width>4
>> iris[sel,] or iris[sel, c(1,3,5)]
>>
>> Cheers
>> Petr
>>
>> From: Shawin Karim [mailto:[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711787&i=0>]
>> Sent: Thursday, September 03, 2015 9:17 AM
>> To: PIKAL Petr
>> Subject: Re: [R] Converting CSV file to numeric and calculate ttest
>>
>> I solve it , thanks  but i have an issue please :
>>
>>
>> have a csv file and i would like to extract the value of it with specific
>> range fof the last column for example; F>2 & F<2 & P=1, and then write the
>> result to csv file.
>>
>> the data is :
>> x1
>>
>> x2
>>
>> x3
>>
>> x4
>>
>> x5
>>
>> x6
>>
>> 9.488404
>>
>> 9.470895
>>
>> 9.282434
>>
>> 9.366707
>>
>> 9.955383
>>
>> 9.640816
>>
>> 8.63063
>>
>> 8.558311
>>
>> 8.788391
>>
>> 8.576231
>>
>> 8.671588
>>
>> 8.84298
>>
>> 9.354749
>>
>> 9.367669
>>
>> 9.259953
>>
>> 9.421538
>>
>> 9.554635
>>
>> 9.603745
>>
>> 9.944864
>>
>> 9.950428
>>
>> 10.19102
>>
>> 10.07351
>>
>> 10.0327
>>
>> 10.13079
>>
>>
>> Fc
>>
>> p
>>
>> 1.037883
>>
>> 0.320095
>>
>> 1.057708
>>
>> 0.060132
>>
>> 1.065191
>>
>> 0.111192
>>
>>
>>
>>
>> I tried the code bellow but , it does not work?
>>
>> FCPval <-
>> read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv")
>>
>>  c=as.data.frame(FCPval)
>>
>> for (i in 1:rowN){if (C$F[i] >= 2 && C$F[i]<=-2&& C$p[i]<=3){
>>
>> dfrmPFC=data.frame(Fc=FC,p=p)}
>> }
>>
>> the error is :
>>
>> Error in C$FC : object of type 'closure' is not subsettable.
>>
>> can i  extract the value according to the condition
>>
>> On Thu, Sep 3, 2015 at 8:14 AM, PIKAL Petr <[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711787&i=1><mailto:[hidden
>> email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=2>>> wrote:
>> Hi
>>
>> p value of what? How do you know that it should be 0.3?
>>
>> Your HTML post is mangled and almost unreadable.
>>
>> Your code is not reproducible, we do not have AT1.csv
>>
>> AT1 shall be numeric so your reading process is wrong and following
>> conversion to numeric shall be umnnecessary if you read your data properly.
>>
>> Cheers
>> Petr
>>
>> > -----Original Message-----
>> > From: R-help [mailto:[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711787&i=3><mailto:[hidden
>> email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=4>>] On
>> Behalf Of shawin
>> > Sent: Wednesday, September 02, 2015 9:08 PM
>> > To: [hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711787&i=5><mailto:[hidden
>> email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=6>>
>> > Subject: Re: [R] Converting CSV file to numeric and calculate ttest
>> >
>> > p value should be 0.3
>> >
>> > On Wed, Sep 2, 2015 at 8:07 PM, Navien [via R] <
>> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=7>
>> <mailto:ml-node%[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711787&i=8>>> wrote:
>> >
>> > > I have a csv file i convert it to numeric value , then i apply t-test
>> > > , but t-test result is not correct: please could you guide me. the
>> > data :
>> > > ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 1367452_at
>> > > 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
>> > > 9.640816474 9.606262272 9.329651027 9.434541611 9.473922432
>> > > 9.311412966
>> > > 9.3154885 9.434977488 9.470895414 9.764258059 1367453_at 8.630629966
>> > > 8.55831075 8.788391003 8.576231135 8.671587906
>> > > 8.842979993 8.861958856 8.58330436 8.603596508 8.570129609 8.59798922
>> > > 8.572686772 8.679751791 8.663950953 8.432875347 1367454_at
>> > 9.354748885
>> > > 9.367668838 9.259952558 9.421538213 9.554635162
>> > > 9.603744578 9.452197983 9.284228877 9.404607878 9.317737979
>> > > 9.343115301
>> > > 9.310644266 9.27227486 9.360337823 9.44706281 1367455_at 9.944863964
>> > > 9.950427516 10.19101759 10.07350804 10.03269879
>> > > 10.1307908 10.03487287 9.74609383 9.886379007 9.775472567 10.036596
>> > > 9.544738458 9.699611598 9.911962567 9.625804277
>> > >           AT1 <-
>> > > read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/AT1.csv",
>> > >  stringsAsFactors=TRUE)
>> > >
>> > >
>> > > str(AT1)
>> > > matAT = data.frame(AT1[,-1])
>> > > str(matAT)
>> > > write.csv(matAT,"matAT.csv",row.names=FALSE)
>> > > #convert the csv file to numeric
>> > > matAT<-as.matrix(sapply(matAT,as.numeric))
>> > > str(matAT)
>> > > group<-factor(rep(c("Ob","Ln"),times=c(7,8)))
>> > > t.test(matAT[1,]~group)
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > > ------------------------------
>> > > If you reply to this email, your message will be added to the
>> > > discussion
>> > > below:
>> > >
>> > > http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-
>> > calcu
>> > > late-ttest-tp4711770.html To start a new topic under R help, email
>> > > [hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711787&i=9><mailto:ml-node%[hidden
>> email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=10>>
>> > > To unsubscribe from R, click here
>> > >
>> > <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscr
>> > >
>> > ibe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtM
>> > > jQ0MzkwMjQ1>
>> > > .
>> > > NAML
>> > >
>> > <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_vi
>> > >
>> > ewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces
>> > > .BasicNamespace-nabble.view.web.template.NabbleNamespace-
>> > nabble.naml.n
>> > > amespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-
>> > nabb
>> > >
>> > le.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21na
>> > > bble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
>> > send_instant_em
>> > > ail%21nabble%3Aemail.naml>
>> > >
>> >
>> >
>> >
>> >
>> > --
>> > View this message in context: http://r.789695.n4.nabble.com/Converting-
>> > CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711771.html
>> > Sent from the R help mailing list archive at Nabble.com.
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=11>
>> <mailto:[hidden email]
>> <http:///user/SendEmail.jtp?type=node&node=4711787&i=12>> mailing list
>> -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-
>> > guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>> ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
>> vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
>> ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
>> strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
>> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
>> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
>> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are
>> intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its
>> sender. Delete the contents of this e-mail with all attachments and its
>> copies from your system.
>> If you are not the intended recipient of this e-mail, you are not
>> authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage
>> caused by modifications of the e-mail or by delay with transfer of the
>> email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a
>> contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to
>> immediately accept such offer; The sender of this e-mail (offer) excludes
>> any acceptance of the offer on the part of the recipient containing any
>> amendment or variation.
>> - the sender insists on that the respective contract is concluded only
>> upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to
>> enter into any contracts on behalf of the company except for cases in which
>> he/she is expressly authorized to do so in writing, and such authorization
>> or power of attorney is submitted to the recipient or the person
>> represented by the recipient, or the existence of such authorization is
>> known to the recipient of the person represented by the recipient.
>>
>>
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>> ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
>> vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
>> ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
>> strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
>> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
>> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
>> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are
>> intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its
>> sender. Delete the contents of this e-mail with all attachments and its
>> copies from your system.
>> If you are not the intended recipient of this e-mail, you are not
>> authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage
>> caused by modifications of the e-mail or by delay with transfer of the
>> email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a
>> contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to
>> immediately accept such offer; The sender of this e-mail (offer) excludes
>> any acceptance of the offer on the part of the recipient containing any
>> amendment or variation.
>> - the sender insists on that the respective contract is concluded only
>> upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to
>> enter into any contracts on behalf of the company except for cases in which
>> he/she is expressly authorized to do so in writing, and such authorization
>> or power of attorney is submitted to the recipient or the person
>> represented by the recipient, or the existence of such authorization is
>> known to the recipient of the person represented by the recipient.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=13>
>> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ------------------------------
>> If you reply to this email, your message will be added to the discussion
>> below:
>>
>> http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711787.html
>> To start a new topic under R help, email
>> ml-node+s789695n789696h75 at n4.nabble.com
>> To unsubscribe from R, click here
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NXwtMjQ0MzkwMjQ1>
>> .
>> NAML
>> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>
>




--
View this message in context: http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711792.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From btupper at bigelow.org  Thu Sep  3 22:41:14 2015
From: btupper at bigelow.org (Ben Tupper)
Date: Thu, 3 Sep 2015 16:41:14 -0400
Subject: [R] Extracting XML value
In-Reply-To: <797dc3bb-1818-4d77-9f51-9cd1c158bc98@me.com>
References: <797dc3bb-1818-4d77-9f51-9cd1c158bc98@me.com>
Message-ID: <2215727F-14F1-4903-BAA0-24940F119E15@bigelow.org>

Hi,

You are very close and your understanding is correct - you need to extract the root node from the XMLDocument returned from xmlTreeParse.

library(XML)

txt <-  "<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<observations realtime_start=\"2015-09-03\" realtime_end=\"2015-09-03\" observation_start=\"2015-09-01\" observation_end=\"2015-09-01\" units=\"lin\" output_type=\"1\" file_type=\"xml\" order_by=\"observation_date\" sort_order=\"asc\" count=\"1\" offset=\"0\" limit=\"100000\">\n  <observation realtime_start=\"2015-09-03\" realtime_end=\"2015-09-03\" date=\"2015-09-01\" value=\"0.46\"/>\n</observations>\n\n\n\n"

# parse the text tree and extract the root node
obs <- xmlRoot(xmlTreeParse(txt, useInternalNodes = TRUE, asText = TRUE))

# get the first child node of 'observation' name.  Yes, there is just one.
obs1 <- obs['observation'][[1]]

# it has no value, just attributes of which 'value' is one
xmlAttrs(obs1)[['value']]


Cheers,
Ben



On Sep 3, 2015, at 11:23 AM, Glenn Schultz <glennmschultz at me.com> wrote:

> All,
> 
> I have made it as far as generating an api call which returns the following xml
> [1] "<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<observations realtime_start=\"2015-09-03\" realtime_end=\"2015-09-03\" observation_start=\"2015-09-01\" observation_end=\"2015-09-01\" units=\"lin\" output_type=\"1\" file_type=\"xml\" order_by=\"observation_date\" sort_order=\"asc\" count=\"1\" offset=\"0\" limit=\"100000\">\n  <observation realtime_start=\"2015-09-03\" realtime_end=\"2015-09-03\" date=\"2015-09-01\" value=\"0.46\"/>\n</observations>\n\n\n\n"
> attr(,"Content-Type")
>               charset 
> "text/xml"    "UTF-8" 
> 
> following DTL's presentation on the Berkley site and the package help I parsed the xml
> 
> doc = xmlTreeParse(USSW10, asText = TRUE, useInternal = TRUE)
> 
> which gives
> <?xml version="1.0" encoding="utf-8"?>
> <observations realtime_start="2015-09-03" realtime_end="2015-09-03" observation_start="2015-09-01" observation_end="2015-09-01" units="lin" output_type="1" file_type="xml" order_by="observation_date" sort_order="asc" count="1" offset="0" limit="100000">
>   <observation realtime_start="2015-09-03" realtime_end="2015-09-03" date="2015-09-01" value="0.46"/>
> </observations>
> 
> finally I try to extract the value 0.46 using the xmlValue function.  I have lost something in translation and I am unable to extract the value.  my understanding is I have one node with no children, correct?
> 
> -Glenn
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From jdnewmil at dcn.davis.CA.us  Thu Sep  3 22:45:44 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 03 Sep 2015 13:45:44 -0700
Subject: [R] R on Windows 10
In-Reply-To: <DM2PR02MB334943BFE99A5EAF8232329D2680@DM2PR02MB334.namprd02.prod.outlook.com>
References: <DM2PR02MB334943BFE99A5EAF8232329D2680@DM2PR02MB334.namprd02.prod.outlook.com>
Message-ID: <32B2A0D6-48DF-4EAD-BD1D-C8566765834E@dcn.davis.CA.us>

Apparently yes. Just this morning John Fox said he was using R 3.2.2 on Windows 10. To be fair, he was responding to someone who was having trouble, but as typical there were no details.

The posting guide does recommend searching the archives before posting.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 3, 2015 11:25:46 AM PDT, "Martinez, Stephen Anthony - marti3sa" <marti3sa at dukes.jmu.edu> wrote:
>Please forgive me if this is the wrong place to ask or if I missed
>somewhere that it mentions this but I just wanted to make sure, does
>the most recent version of R work on windows 10? I know under the FAQ
>it says XP or newer but just wanted to confirm since windows 10 is new.
>
>Thank you! 
>-Stephen
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From istazahn at gmail.com  Thu Sep  3 22:47:29 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Thu, 3 Sep 2015 16:47:29 -0400
Subject: [R] ggplot2 will not install after system upgrade
In-Reply-To: <1441306999.3562.3.camel@trefftzs.org>
References: <1441306999.3562.3.camel@trefftzs.org>
Message-ID: <CA+vqiLETyCwDHs+XGutSgz3QH5KAfKt0PTEYkgSrPZL2UyNGFg@mail.gmail.com>

Hi Jeff,
Your chances of getting a useful response will increase if you provide
some additional information. For example, which version of R? Which
version of ggplot2? What sequence of commands produces the error? What
_exactly_ does the error message say?

Does

update.packages(ask=FALSE, checkBuilt=TRUE)
install.packages("ggplot2")

help?

Best,
Ista

On Thu, Sep 3, 2015 at 3:03 PM, Jeff Trefftzs <jeff at trefftzs.org> wrote:
> I just upgraded my laptop from Fedora Core 20 to Fedora Core 22, and
> after the upgrade R can no longer use the ggplot2 library.  The
> principal complaint seems to be that libicui18n.so.50 is not found. The
> version of libicu that is installed is version 54.  On the other hand,
> the same environment exists on my desktop computer, also with version
> 54 of libicu and all works just fine.
> Any hints?  Has anyone else seen this?
>
> Thanks,
>
> --
> Jeff Trefftzs
> http://www.trefftzs.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wewolski at gmail.com  Thu Sep  3 22:53:05 2015
From: wewolski at gmail.com (Witold E Wolski)
Date: Thu, 3 Sep 2015 22:53:05 +0200
Subject: [R] registerDoParallel and R CMD check --as-cran errors
In-Reply-To: <CAAjnpdiCFjS_XjyrOuhsof+aB9KmT8mfkb7MOQ+Rwhb9rWB7gg@mail.gmail.com>
References: <CAAjnpdiCFjS_XjyrOuhsof+aB9KmT8mfkb7MOQ+Rwhb9rWB7gg@mail.gmail.com>
Message-ID: <CAAjnpdg4vdvZAM5+dL=3DTYZBXxsXadWaexj46GZXXRx42dy5A@mail.gmail.com>

To answer my own question. Dono if it's the right way to do... In the
example code I am using a single CPU and it works.

regards

PS. Should I have asked this question on the devel list?



On 2 September 2015 at 14:39, Witold E Wolski <wewolski at gmail.com> wrote:

> I am testing a package with
>
> R CMD check
> and
> R CMD check --as-cran
>
> some code which is run in the examples section uses the foreach and
> doParallel package.
>
>
> When run with --as-cran I have an error.
> registerDoParallel function causes an error.
>
> Error in .check_ncores(length(names)) : 6 simultaneous processes spawned
> Calls: annotatePeptides ... registerDoParallel -> makeCluster ->
> makePSOCKcluster -> .check_ncores
>
>
> What should can I do to prevent this error and pas the --as-cran check?
>
> best regards
> Witold
>
>
> --
> Witold Eryk Wolski
>
>


-- 
Witold Eryk Wolski

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Thu Sep  3 23:04:43 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Thu, 3 Sep 2015 21:04:43 +0000
Subject: [R] R on Windows 10
In-Reply-To: <32B2A0D6-48DF-4EAD-BD1D-C8566765834E@dcn.davis.CA.us>
References: <DM2PR02MB334943BFE99A5EAF8232329D2680@DM2PR02MB334.namprd02.prod.outlook.com>
	<32B2A0D6-48DF-4EAD-BD1D-C8566765834E@dcn.davis.CA.us>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC8A473ED@FHSDB4H16-2.csu.mcmaster.ca>

Dear Jeff and Stephen,

I can confirm that I'm using R 3.2.2 (and R-devel) on two Windows 10 systems with no problems whatsoever. As well, although I can't be sure, I doubt that the problem reported earlier today (concerning the RcmdrPlugin.survival package) was related to use of Windows 10.

I hope this helps,
 John

-----------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.socsci.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jeff
> Newmiller
> Sent: Thursday, September 3, 2015 4:46 PM
> To: Martinez, Stephen Anthony - marti3sa; r-help at r-project.org
> Subject: Re: [R] R on Windows 10
> 
> Apparently yes. Just this morning John Fox said he was using R 3.2.2 on
> Windows 10. To be fair, he was responding to someone who was having
> trouble, but as typical there were no details.
> 
> The posting guide does recommend searching the archives before posting.
> ------------------------------------------------------------------------
> ---
> Jeff Newmiller                        The     .....       .....  Go
> Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> ------------------------------------------------------------------------
> ---
> Sent from my phone. Please excuse my brevity.
> 
> On September 3, 2015 11:25:46 AM PDT, "Martinez, Stephen Anthony -
> marti3sa" <marti3sa at dukes.jmu.edu> wrote:
> >Please forgive me if this is the wrong place to ask or if I missed
> >somewhere that it mentions this but I just wanted to make sure, does
> >the most recent version of R work on windows 10? I know under the FAQ
> >it says XP or newer but just wanted to confirm since windows 10 is new.
> >
> >Thank you!
> >-Stephen
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Thu Sep  3 23:24:26 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Thu, 03 Sep 2015 16:24:26 -0500
Subject: [R] ggplot2 will not install after system upgrade
In-Reply-To: <1441306999.3562.3.camel@trefftzs.org>
References: <1441306999.3562.3.camel@trefftzs.org>
Message-ID: <8F24EED9-ED5C-4327-A1FB-61AC421A7196@me.com>


> On Sep 3, 2015, at 2:03 PM, Jeff Trefftzs <jeff at trefftzs.org> wrote:
> 
> I just upgraded my laptop from Fedora Core 20 to Fedora Core 22, and
> after the upgrade R can no longer use the ggplot2 library.  The
> principal complaint seems to be that libicui18n.so.50 is not found. The
> version of libicu that is installed is version 54.  On the other hand,
> the same environment exists on my desktop computer, also with version
> 54 of libicu and all works just fine.  
> Any hints?  Has anyone else seen this?
> 
> Thanks,


Jeff,

How did you install R (RPM or compiled source) and did you re-install after the F22 upgrade, or just upgrade from F20 to F22 in place?

Since F20 EOL?d this summer, you would likely need to install current R RPMs built for F22, after the upgrade and/or rebuild R from source post the F22 upgrade.

My guess is a version incompatibility issue, where you are running a version of R built for F20 on F22.

Regards,

Marc Schwartz


From zadig_1 at excite.com  Fri Sep  4 02:40:36 2015
From: zadig_1 at excite.com (ce)
Date: Thu, 03 Sep 2015 20:40:36 -0400
Subject: [R] merge xts objects with different data types ?
Message-ID: <20150903204036.5853@web002.roc2.bluetie.com>


Hello

Let's say some questions about merging  xts variables :

a<- xts("abc", Sys.Date())
b <- xts("def", Sys.Date())
c <- xts(1, Sys.Date())

> merge(a,b)
           a     b    
2015-09-03 "abc" "def"
> merge(a,b,c)
            a  b c
2015-09-03 NA NA 1
Warning messages:
1: In merge.xts(a, b, c) : NAs introduced by coercion
2: In merge.xts(a, b, c) : NAs introduced by coercion
3: In merge.xts(a, b, c) : NAs introduced by coercion
4: In merge.xts(a, b, c) : NAs introduced by coercion

How I can merge  a, b ,c correctly ? Another example is with Binary variables :

> e<- xts(TRUE, Sys.Date())
> e
           [,1]
2015-09-03 TRUE
> merge(e,b)
           e  b
2015-09-03 1 NA
Warning message:
In merge.xts(e, b) : NAs introduced by coercion


My second question is how I can convert an xts object to factor :

> d <- merge(a,b)
> d
           a     b    
2015-09-03 "abc" "def"
> factor(d, levels = c("abc","def"))
  a   b 
abc def 
Levels: abc def

Date disappears here?

Thanks for your help
ce


From josh.m.ulrich at gmail.com  Fri Sep  4 03:43:08 2015
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 3 Sep 2015 20:43:08 -0500
Subject: [R] merge xts objects with different data types ?
In-Reply-To: <20150903204036.5853@web002.roc2.bluetie.com>
References: <20150903204036.5853@web002.roc2.bluetie.com>
Message-ID: <CAPPM_gRzkSpvBpLR=qrzSZ8oCLLF7am=yCpCuyfLPzk8UqesoQ@mail.gmail.com>

On Thu, Sep 3, 2015 at 7:40 PM, ce <zadig_1 at excite.com> wrote:
>
> Hello
>
> Let's say some questions about merging  xts variables :
>
> a<- xts("abc", Sys.Date())
> b <- xts("def", Sys.Date())
> c <- xts(1, Sys.Date())
>
>> merge(a,b)
>            a     b
> 2015-09-03 "abc" "def"
>> merge(a,b,c)
>             a  b c
> 2015-09-03 NA NA 1
> Warning messages:
> 1: In merge.xts(a, b, c) : NAs introduced by coercion
> 2: In merge.xts(a, b, c) : NAs introduced by coercion
> 3: In merge.xts(a, b, c) : NAs introduced by coercion
> 4: In merge.xts(a, b, c) : NAs introduced by coercion
>
> How I can merge  a, b ,c correctly ? Another example is with Binary variables :
>
>> e<- xts(TRUE, Sys.Date())
>> e
>            [,1]
> 2015-09-03 TRUE
>> merge(e,b)
>            e  b
> 2015-09-03 1 NA
> Warning message:
> In merge.xts(e, b) : NAs introduced by coercion
>
xts objects are a matrix with an index attribute, and you can't mix
types in a matrix.  So all the objects you merge need to be the same
type.  For objects a, b, and c: you need to convert c to character:
storage.mode(c) <- "character"

Also, merge.xts currently only supports n-way merges integer, numeric,
and logical types (see https://github.com/joshuaulrich/xts/issues/44).
So you need to merge a and b first, then merge that result with c.
You can do that by calling merge.xts many times, or via Reduce:
merge(merge(a,b),c)
Reduce(merge, list(a,b,c))

>
> My second question is how I can convert an xts object to factor :
>
>> d <- merge(a,b)
>> d
>            a     b
> 2015-09-03 "abc" "def"
>> factor(d, levels = c("abc","def"))
>   a   b
> abc def
> Levels: abc def
>
> Date disappears here?
>
I'm not sure what you expected; factors don't have dates.  It's not
clear what you're trying to do here.

> Thanks for your help
> ce
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From zadig_1 at excite.com  Fri Sep  4 03:52:19 2015
From: zadig_1 at excite.com (ce)
Date: Thu, 03 Sep 2015 21:52:19 -0400
Subject: [R] merge xts objects with different data types ?
Message-ID: <20150903215219.1961@web010.roc2.bluetie.com>


Thanks a lot Jeff and Joshua,
I can see why data.table can be used with dates ( still more work to make it work :)  )

For factors, I was hoping to get a time series of a factor ( values can be only "abc" or "def" category ) :

2015-09-04 "abc"
2015-09-05 "abc"
2015-09-06 "def"
2015-09-07 "abc"


-----Original Message-----
From: "Joshua Ulrich" [josh.m.ulrich at gmail.com]
Date: 09/03/2015 09:43 PM
To: "ce" <zadig_1 at excite.com>
CC: "R-Help" <r-help at r-project.org>
Subject: Re: [R] merge xts objects with different data types ?

On Thu, Sep 3, 2015 at 7:40 PM, ce <zadig_1 at excite.com> wrote:
>
> Hello
>
> Let's say some questions about merging  xts variables :
>
> a<- xts("abc", Sys.Date())
> b <- xts("def", Sys.Date())
> c <- xts(1, Sys.Date())
>
>> merge(a,b)
>            a     b
> 2015-09-03 "abc" "def"
>> merge(a,b,c)
>             a  b c
> 2015-09-03 NA NA 1
> Warning messages:
> 1: In merge.xts(a, b, c) : NAs introduced by coercion
> 2: In merge.xts(a, b, c) : NAs introduced by coercion
> 3: In merge.xts(a, b, c) : NAs introduced by coercion
> 4: In merge.xts(a, b, c) : NAs introduced by coercion
>
> How I can merge  a, b ,c correctly ? Another example is with Binary variables :
>
>> e<- xts(TRUE, Sys.Date())
>> e
>            [,1]
> 2015-09-03 TRUE
>> merge(e,b)
>            e  b
> 2015-09-03 1 NA
> Warning message:
> In merge.xts(e, b) : NAs introduced by coercion
>
xts objects are a matrix with an index attribute, and you can't mix
types in a matrix.  So all the objects you merge need to be the same
type.  For objects a, b, and c: you need to convert c to character:
storage.mode(c) <- "character"

Also, merge.xts currently only supports n-way merges integer, numeric,
and logical types (see https://github.com/joshuaulrich/xts/issues/44).
So you need to merge a and b first, then merge that result with c.
You can do that by calling merge.xts many times, or via Reduce:
merge(merge(a,b),c)
Reduce(merge, list(a,b,c))

>
> My second question is how I can convert an xts object to factor :
>
>> d <- merge(a,b)
>> d
>            a     b
> 2015-09-03 "abc" "def"
>> factor(d, levels = c("abc","def"))
>   a   b
> abc def
> Levels: abc def
>
> Date disappears here?
>
I'm not sure what you expected; factors don't have dates.  It's not
clear what you're trying to do here.

> Thanks for your help
> ce
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From jfox at mcmaster.ca  Fri Sep  4 05:24:54 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Fri, 4 Sep 2015 03:24:54 +0000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
 10
In-Reply-To: <8CB19DFF334846FEA36C098039F1A46C@Zeus>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
	<8CB19DFF334846FEA36C098039F1A46C@Zeus>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca>

Dear Joyaa,

> -----Original Message-----
> From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> Sent: September 3, 2015 9:37 PM
> To: Fox, John <jfox at mcmaster.ca>
> Cc: r-help at r-project.org
> Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 10
> 
> Dear John, Dear List,
> 
> Thank you for responding.  Like you, I have tried uninstalling R and reinstalling
> again.  Question 1: having uninstalled R, how do you delete the package library?

You say below that you installed R packages when R was run as administrator and that you took all defaults. In that case,  the installed packages will be in C:\Program Files\R\R-3.2.2\library . Just delete the library directory, which will still be there after R 3.2.2 is uninstalled. I do recommend, by the way, that, for using the Rcmdr, you install R with the SDI rather than the default MDI, but that doesn't explain your difficulties.

> 
> I have used defaults throughout the process, except for running the whole things
> as Administrator rather than a User.  For the mirror, my first two install attempts
> were via a mirror in Germany.  My 3rd and 4th attempts via Cambridge, UK.
> 
> Perhaps importantly: the first time I ran the RcmdrPlugin.survival plug-in it
> worked fine.  

If you said that before, I missed it. 

> I got half way through the exercise I was attempting, then closed
> the programs for the night.  The next day, when I tried to reload the plugin from
> R Commander, the plugin asked me to restart R Commander, but on opting for
> "Yes", this is when R Commander does not open properly.

Here's my hypothesis: When you "closed" R, you saved the R workspace. My guess is that this is the source of your problem. If so, the saved workspace is in the file .RData in your home directory; delete it and try again.

If this works, in future, exit from the Rcmdr and R via the Rcmdr menus (or if closing R directly, don't save the workspace).

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox


> 
> My thanks to you and the list for your time.  Joyaa (Gold Coast, Queensland,
> Australia)
> 
> -----Original Message-----
> From: Fox, John
> Sent: Friday, September 4, 2015 1:29 AM
> To: Joyaa Antares
> Cc: r-help at r-project.org
> Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
> 10
> 
> Dear Joyaa,
> 
> I too run R 3.2.2 on a Windows 10 system. I uninstalled R, deleted my package
> library, and then reinstalled R and the Rcmdr and RcmdrPlugin.survival packages.
> I took all defaults, except that I selected the SDI rather than the default MDI for
> Rgui, but I seriously doubt that this is the source of your problem. I used the 0-
> Cloud mirror, both for R
> 3.2.2 and for packages.
> 
> I'm afraid that I can't duplicate your problem -- everything works perfectly fine
> for me.
> 
> You provided a reasonable amount of detail, but is there anything else you can
> add?
> 
> Best,
> John
> 
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> web: socserv.mcmaster.ca/jfox
> 
> 
> ________________________________________
> From: R-help [r-help-bounces at r-project.org] on behalf of Joyaa Antares
> [joyaa at goldcoastosteopathy.com.au]
> Sent: September 3, 2015 8:31 AM
> To: r-help at r-project.org
> Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 10
> 
> Hello r-help,
> 
> I am quite new to R.  I downloaded R 3.2.2 for Windows to use with Windows
> 10.
> 
> On attempting to load the RcmdrPlugin.survival plug-in I got this error
> message:
> 
> Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
>   [tcl] invalid command name "configure".
> 
> The lead-up to this was:
> 
> * I uninstalled R 3.2.2 and deleted all R files I could find.
> * Downloaded R 3.2.2 to do a fresh install
> * Opened the .exe file as Administrator
> * Ran 64-bit R as Administrator
> * Updated all packages
> * Installed Rcmdr and RcmdrPlugin.survival
> * Ran Rcmdr using library(Rcmdr) from the console
> * This required an install of a host of packages, which I installed
> * Loaded RcmdrPlugin.survival from the Tools menu
> * I was asked to restart R commander, and when I tried it didn't open properly
> (showing only File/Edit/Data in the menu bar), and this wouldn't close properly
> either.  The error message as above showed in the console.
> 
> I haven't posted to the list before and would be very grateful for help.
> Thank you.   Joyaa
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> =


From milos.zarkovic at gmail.com  Fri Sep  4 07:01:25 2015
From: milos.zarkovic at gmail.com (=?UTF-8?B?TWlsb8WhIMW9YXJrb3ZpxIc=?=)
Date: Fri, 4 Sep 2015 07:01:25 +0200
Subject: [R] R on Windows 10
In-Reply-To: <DM2PR02MB334943BFE99A5EAF8232329D2680@DM2PR02MB334.namprd02.prod.outlook.com>
References: <DM2PR02MB334943BFE99A5EAF8232329D2680@DM2PR02MB334.namprd02.prod.outlook.com>
Message-ID: <CANgWSHBVeqaheduOjg4dM_ZMUxszETcUdB5=sb6skGufKKmj3A@mail.gmail.com>

R, Rstudio and Rcommander work on windows 10 without problems
On 3 Sep 2015 22:11, "Martinez, Stephen Anthony - marti3sa" <
marti3sa at dukes.jmu.edu> wrote:

> Please forgive me if this is the wrong place to ask or if I missed
> somewhere that it mentions this but I just wanted to make sure, does the
> most recent version of R work on windows 10? I know under the FAQ it says
> XP or newer but just wanted to confirm since windows 10 is new.
>
> Thank you!
> -Stephen
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Fri Sep  4 01:00:51 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Thu, 3 Sep 2015 16:00:51 -0700 (PDT)
Subject: [R] file.show cleanup
Message-ID: <1441321251687-4711824.post@n4.nabble.com>

The file.show() function seems to be exactly what I'm needing for displaying
file contents for users, but I need something like "file.close()" to close
the "R Information" window to clean up afterwards. Does anyone know if there
is such a thing?

-Dan



--
View this message in context: http://r.789695.n4.nabble.com/file-show-cleanup-tp4711824.html
Sent from the R help mailing list archive at Nabble.com.


From ddalthorp at usgs.gov  Fri Sep  4 02:12:38 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Thu, 3 Sep 2015 17:12:38 -0700 (PDT)
Subject: [R] edit GUI preferences for current session?
Message-ID: <1441325558276-4711825.post@n4.nabble.com>

I'd like to edit the GUI preferences for the current (Windows) session via
command line rather than the Edit|GUI preferences menu. In particular, is
there a way to change the pagerstyle to singlewindow without using the menu
or editing the RConsole file?

Any ideas?

Thanks!

-Dan



--
View this message in context: http://r.789695.n4.nabble.com/edit-GUI-preferences-for-current-session-tp4711825.html
Sent from the R help mailing list archive at Nabble.com.


From hazel_knipe at hotmail.co.uk  Thu Sep  3 23:09:39 2015
From: hazel_knipe at hotmail.co.uk (Hazel Knipe)
Date: Thu, 3 Sep 2015 22:09:39 +0100
Subject: [R]  Pairwise comparisons, adonis (Vegan)
Message-ID: <DUB119-W839C9A7CE3A750E2E77C1CE680@phx.gbl>

Hello all, 
 
I am an undergraduate, so bare with me if I don't make complete sense! 
 
I have performed a permanova using the function Adonis (vegan), to test for significant difference in OTU datasets from different species (n=5) at different sites (n=6) with a total of 20 libraries (unbalanced). I have got a significant result, but I would like to know which species/ sites are causing this result.
 
adonis(formula = vegdist(comm, method = "bray") ~ Species, data = metadata, strata = Site) 
 
I have heard you can run multiple adonis tests, and then adjust the P value (which correction?), although I am not sure my reduced datasets meet the assumptions, or if the permutations produce a meaningful result at such a low number.  
 
Does anyone know how to get around this, or if I should be using a different test for this data? 
 
Thank you in advance, 
 
Hazel Knipe
 		 	   		  
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Fri Sep  4 03:21:38 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 03 Sep 2015 18:21:38 -0700
Subject: [R] merge xts objects with different data types ?
In-Reply-To: <20150903204036.5853@web002.roc2.bluetie.com>
References: <20150903204036.5853@web002.roc2.bluetie.com>
Message-ID: <83582090-0657-4041-AC0C-4BCC81A98EB7@dcn.davis.CA.us>

The root of your problems lie in your assumption that xts variables act like data frames. Instead they are matrices with an index attribute. All values in a matrix must be of the same storage mode. 

You might want to investigate the data.table package. It is not a time series object but you can still handle time series operations and you can have any type of data.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 3, 2015 5:40:36 PM PDT, ce <zadig_1 at excite.com> wrote:
>
>Hello
>
>Let's say some questions about merging  xts variables :
>
>a<- xts("abc", Sys.Date())
>b <- xts("def", Sys.Date())
>c <- xts(1, Sys.Date())
>
>> merge(a,b)
>           a     b    
>2015-09-03 "abc" "def"
>> merge(a,b,c)
>            a  b c
>2015-09-03 NA NA 1
>Warning messages:
>1: In merge.xts(a, b, c) : NAs introduced by coercion
>2: In merge.xts(a, b, c) : NAs introduced by coercion
>3: In merge.xts(a, b, c) : NAs introduced by coercion
>4: In merge.xts(a, b, c) : NAs introduced by coercion
>
>How I can merge  a, b ,c correctly ? Another example is with Binary
>variables :
>
>> e<- xts(TRUE, Sys.Date())
>> e
>           [,1]
>2015-09-03 TRUE
>> merge(e,b)
>           e  b
>2015-09-03 1 NA
>Warning message:
>In merge.xts(e, b) : NAs introduced by coercion
>
>
>My second question is how I can convert an xts object to factor :
>
>> d <- merge(a,b)
>> d
>           a     b    
>2015-09-03 "abc" "def"
>> factor(d, levels = c("abc","def"))
>  a   b 
>abc def 
>Levels: abc def
>
>Date disappears here?
>
>Thanks for your help
>ce
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jeff at trefftzs.org  Thu Sep  3 23:53:08 2015
From: jeff at trefftzs.org (Jeff Trefftzs)
Date: Thu, 03 Sep 2015 14:53:08 -0700
Subject: [R] ggplot2 will not install after system upgrade
In-Reply-To: <CA+vqiLETyCwDHs+XGutSgz3QH5KAfKt0PTEYkgSrPZL2UyNGFg@mail.gmail.com>
References: <1441306999.3562.3.camel@trefftzs.org>
	<CA+vqiLETyCwDHs+XGutSgz3QH5KAfKt0PTEYkgSrPZL2UyNGFg@mail.gmail.com>
Message-ID: <1441317188.3562.15.camel@trefftzs.org>

On Thu, 2015-09-03 at 16:47 -0400, Ista Zahn wrote:
> Hi Jeff,
> Your chances of getting a useful response will increase if you
> provide
> some additional information. For example, which version of R? Which
> version of ggplot2? 


Sorry.  R was version 3.2.1
ggplot2 1.0.1

> What sequence of commands produces the error?

install.packages("ggplot2") (or various equivalents while in R-Studio

> What
> _exactly_ does the error message say?

I was working on my laptop, where I didn't have email enabled, so I was
unable to cut & paste all the output.  The last bit of the error
messages boiled down to "unable to find libicui18n.so.50.  No such file
or directory"

Does
> update.packages(ask=FALSE, checkBuilt=TRUE)
> install.packages("ggplot2")

I hadn't tried that.  

Follow-up:  On the laptop I downgraded to R-3.1.3 and things worked
again.  The various error messages I got were confusing.  When I tried
to install ggplot2 from the first US mirror the https server at
Berkeley), it told me "gplot2 not available for R 3.2.1".  When I tried
one of the other servers (e.g., other Berkeley server, or the UCLA
server) it would download, and come to grief with the libicu message.

But downgrading to R 3.1.3 seems to have cured things.

I'm still baffled, however, since I'm writing this on my desktop
computer which has R version 3.2.1 and a successful install of ggplot2
-1.0.1 actually working.

-- 
Jeff Trefftzs
http://www.trefftzs.org


From joyaa at goldcoastosteopathy.com.au  Fri Sep  4 03:36:44 2015
From: joyaa at goldcoastosteopathy.com.au (Joyaa Antares)
Date: Fri, 4 Sep 2015 11:36:44 +1000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
	10
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
Message-ID: <8CB19DFF334846FEA36C098039F1A46C@Zeus>

Dear John, Dear List,

Thank you for responding.  Like you, I have tried uninstalling R and 
reinstalling again.  Question 1: having uninstalled R, how do you delete the 
package library?

I have used defaults throughout the process, except for running the whole 
things as Administrator rather than a User.  For the mirror, my first two 
install attempts were via a mirror in Germany.  My 3rd and 4th attempts via 
Cambridge, UK.

Perhaps importantly: the first time I ran the RcmdrPlugin.survival plug-in 
it worked fine.  I got half way through the exercise I was attempting, then 
closed the programs for the night.  The next day, when I tried to reload the 
plugin from R Commander, the plugin asked me to restart R Commander, but on 
opting for "Yes", this is when R Commander does not open properly.

My thanks to you and the list for your time.  Joyaa (Gold Coast, Queensland, 
Australia)

-----Original Message----- 
From: Fox, John
Sent: Friday, September 4, 2015 1:29 AM
To: Joyaa Antares
Cc: r-help at r-project.org
Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 
10

Dear Joyaa,

I too run R 3.2.2 on a Windows 10 system. I uninstalled R, deleted my 
package library, and then reinstalled R and the Rcmdr and 
RcmdrPlugin.survival packages. I took all defaults, except that I selected 
the SDI rather than the default MDI for Rgui, but I seriously doubt that 
this is the source of your problem. I used the 0-Cloud mirror, both for R 
3.2.2 and for packages.

I'm afraid that I can't duplicate your problem -- everything works perfectly 
fine for me.

You provided a reasonable amount of detail, but is there anything else you 
can add?

Best,
John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
web: socserv.mcmaster.ca/jfox


________________________________________
From: R-help [r-help-bounces at r-project.org] on behalf of Joyaa Antares 
[joyaa at goldcoastosteopathy.com.au]
Sent: September 3, 2015 8:31 AM
To: r-help at r-project.org
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 10

Hello r-help,

I am quite new to R.  I downloaded R 3.2.2 for Windows to use with Windows 
10.

On attempting to load the RcmdrPlugin.survival plug-in I got this error 
message:

Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
  [tcl] invalid command name "configure".

The lead-up to this was:

* I uninstalled R 3.2.2 and deleted all R files I could find.
* Downloaded R 3.2.2 to do a fresh install
* Opened the .exe file as Administrator
* Ran 64-bit R as Administrator
* Updated all packages
* Installed Rcmdr and RcmdrPlugin.survival
* Ran Rcmdr using library(Rcmdr) from the console
* This required an install of a host of packages, which I installed
* Loaded RcmdrPlugin.survival from the Tools menu
* I was asked to restart R commander, and when I tried it didn?t open 
properly (showing only File/Edit/Data in the menu bar), and this wouldn?t 
close properly either.  The error message as above showed in the console.

I haven?t posted to the list before and would be very grateful for help. 
Thank you.   Joyaa
        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
=


From pywang61 at yahoo.com.tw  Fri Sep  4 03:01:06 2015
From: pywang61 at yahoo.com.tw (=?utf-8?B?546L5p+P5YWD?=)
Date: Fri, 4 Sep 2015 09:01:06 +0800
Subject: [R] How to get filter probabilities from msmFit() of package MSwM?
Message-ID: <1441328466.17847.YahooMailBasic@web75103.mail.tw1.yahoo.com>


Dear all:
    I am a rookie in using R. I have a question: How to get filter probabilities from msmFit() of package MSwM?

following are my code....

#Markov Switch Model
library(MSwM)
data(example)
mod<-lm(y~1,example)
mod.msm<-msmFit(mod,k=2,sw=c(T,T))
summary(mod.msm)
plotProb(mod.msm,which=1)
plotProb(mod.msm,which=2)


?
??? 
Paul Wang
pywang61 at yahoo.com.tw


From pywang61 at yahoo.com.tw  Fri Sep  4 07:47:30 2015
From: pywang61 at yahoo.com.tw (=?utf-8?B?546L5p+P5YWD?=)
Date: Fri, 4 Sep 2015 13:47:30 +0800
Subject: [R] =?utf-8?b?5Zue6KaG77mVICBIb3cgdG8gZ2V0IGZpbHRlciBwcm9iYWJp?=
 =?utf-8?q?lities_from_msmFit=28=29_of_package_MSwM=3F?=
In-Reply-To: <1441328466.17847.YahooMailBasic@web75103.mail.tw1.yahoo.com>
Message-ID: <1441345650.8051.YahooMailBasic@web75103.mail.tw1.yahoo.com>

By using  plotProb(mod.msm,which=2), I could only show the graph of smoothed probabilities. May I extract smoothed probabilities and filter probabilities from the msmFit()?  thank you very much.
--------------------------------------------
15/9/4 (?)???? <pywang61 at yahoo.com.tw> ???

 ??: [R] How to get filter probabilities from msmFit() of package MSwM?
 ???: r-help at r-project.org
 ??: 2015?9?4?,?,??9:01


 Dear all:
 ? ? I am a rookie in using R. I have a question:
 How to get filter probabilities from msmFit() of package
 MSwM?

 following are my code....

 #Markov Switch Model
 library(MSwM)
 data(example)
 mod<-lm(y~1,example)
 mod.msm<-msmFit(mod,k=2,sw=c(T,T))
 summary(mod.msm)
 plotProb(mod.msm,which=1)
 plotProb(mod.msm,which=2)


 ?
 ??? 
 Paul Wang
 pywang61 at yahoo.com.tw

 ______________________________________________
 R-help at r-project.org
 mailing list -- To UNSUBSCRIBE and more, see
 https://stat.ethz.ch/mailman/listinfo/r-help
 PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 and provide commented, minimal, self-contained, reproducible
 code.


From petr.pikal at precheza.cz  Fri Sep  4 12:39:39 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 4 Sep 2015 10:39:39 +0000
Subject: [R] Converting CSV file to numeric and calculate ttest
In-Reply-To: <CAPR_rLEgEjYniB3jZ0HH6P7rw1TF_Nej8mzmPAEq-X7deJX0bA@mail.gmail.com>
References: <1441220823200-4711770.post@n4.nabble.com>
	<CAPR_rLGgd4sUiKboDvrtFqasLRHzhsdrMqAmwj=oFLcBfhUdJA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DEE6@SRVEXCHMBX.precheza.cz>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3DF1B@SRVEXCHMBX.precheza.cz>
	<CAPR_rLEgEjYniB3jZ0HH6P7rw1TF_Nej8mzmPAEq-X7deJX0bA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E394@SRVEXCHMBX.precheza.cz>

Hi

Well, you want us to help you but you do not provide any relevant information

This is a result of first line of your code on my computer

> FCPval <- read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv")
Error in file(file, "rt") : cannot open the connection
In addition: Warning message:
In file(file, "rt") :
  cannot open file 'C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv': No such file or directory

So nobody knows what is FCPval. If you read it correctly it shall be data frame, from your previous post I presume it is numeric.


> Fc2<-(FCPVal$Fc>=2 | FCPval<=-2);  this will just come out , true false
not extracting the Fc  ,

you does not **extract** anything, you did not ask for extraction.

> FCP<-as.matrix(sapply(FCPval,as.numeric))
> A=as.data.frame(FCP)

Above lines are difficult to grasp. If csv file was numeric why do you want to transfer it to numeric. You shall ***read*** it as numeric at first instance. If the file is not numeric what do you expect as.numeric will do to it. It just put numeric values where they are not appropriate.

Here is an example

> lev
  animals animalYears ind
1    bird           1   2
2     cat           1   2
3     dog           1   2
> as.matrix(sapply(lev,as.numeric))
     animals animalYears ind
[1,]       1           1   2
[2,]       2           1   2
[3,]       3           1   2
>

I will not go deep into source of your this error, it is probably due to properties of your object A

> Fc2<-(A$Fc>=1.3| A$Fc<=-1.3) & A$p=0.05;  // has error when i combine
> condition
>
> Error in (Fc2 <- (A$Fc >= 1.3 | A$Fc <= -1.3) & A$p) = 0.05 :
>   could not find function "<-<-"
>

Here is how you shall do a selection, based on inbuilt iris data set

data(iris)
> dim(iris)
[1] 150   5 # so iris has 150 rows

sel <- (iris$Sepal.Width>4 | iris$Sepal.Width<3) & iris$Petal.Width==1.3
iris[sel,]
    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
54           5.5         2.3          4.0         1.3 versicolor
56           5.7         2.8          4.5         1.3 versicolor
59           6.6         2.9          4.6         1.3 versicolor
65           5.6         2.9          3.6         1.3 versicolor
72           6.1         2.8          4.0         1.3 versicolor
75           6.4         2.9          4.3         1.3 versicolor
88           6.3         2.3          4.4         1.3 versicolor
90           5.5         2.5          4.0         1.3 versicolor
95           5.6         2.7          4.2         1.3 versicolor
97           5.7         2.9          4.2         1.3 versicolor
98           6.2         2.9          4.3         1.3 versicolor
100          5.7         2.8          4.1         1.3 versicolor

Only 12 are extracted so with proper data everything works. If you want better answer please provide at least result of

str(yourdata)

but preferably the data itself e.g. by

dput(head(FCPval, 20))

Petr

PS. And read R intro as it seems you do not understand R basics.


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of shawin
> Sent: Thursday, September 03, 2015 1:55 PM
> To: r-help at r-project.org
> Subject: Re: [R] Converting CSV file to numeric and calculate ttest
>
> Dear Petr,
>
> This code does not work ?
> For selection values you can use e.g. [
> data(iris)
> sel<-iris$Sepal.Width>4
> iris[sel,] or iris[sel, c(1,3,5)]
>
>
> the data is csv file which is :
> FCPval <-
> read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv")
>
> ;
>
> Fc2<-(FCPVal$Fc>=2 | FCPval<=-2);  this will just come out , true false
> not extracting the Fc  ,
> #######################################################################
> ##############################
> FCP<-as.matrix(sapply(FCPval,as.numeric))
> A=as.data.frame(FCP)
> Fc2<-(A$Fc>=1.3| A$Fc<=-1.3) & A$p=0.05;  // has error when i combine
> condition
>
> Error in (Fc2 <- (A$Fc >= 1.3 | A$Fc <= -1.3) & A$p) = 0.05 :
>   could not find function "<-<-"
>
> write.csv(Fc2,"Fc2.csv",row.names =FALSE )
>
> please need help
>
>
> On Thu, Sep 3, 2015 at 8:56 AM, Shawin Karim <shawinkarim at gmail.com>
> wrote:
>
> > Oh sorry F<2 & F<-2 ,  i need to extract the row value according to
> > the last two column F,P and using condition ,
> >
> > I attached  an test file to see the data
> >
> > On Thu, Sep 3, 2015 at 8:43 AM, PIKAL Petr [via R] <
> > ml-node+s789695n4711787h96 at n4.nabble.com> wrote:
> >
> >> Hi
> >>
> >> what is F?
> >>
> >> F>2 & F<2
> >> this is FALSE for all values of F
> >>
> >> What value you want to extract? All rows corresponding to condition
> >> or just values from specific column.
> >>
> >> For selection values you can use e.g. [
> >> data(iris)
> >> sel<-iris$Sepal.Width>4
> >> iris[sel,] or iris[sel, c(1,3,5)]
> >>
> >> Cheers
> >> Petr
> >>
> >> From: Shawin Karim [mailto:[hidden email]
> >> <http:///user/SendEmail.jtp?type=node&node=4711787&i=0>]
> >> Sent: Thursday, September 03, 2015 9:17 AM
> >> To: PIKAL Petr
> >> Subject: Re: [R] Converting CSV file to numeric and calculate ttest
> >>
> >> I solve it , thanks  but i have an issue please :
> >>
> >>
> >> have a csv file and i would like to extract the value of it with
> >> specific range fof the last column for example; F>2 & F<2 & P=1, and
> >> then write the result to csv file.
> >>
> >> the data is :
> >> x1
> >>
> >> x2
> >>
> >> x3
> >>
> >> x4
> >>
> >> x5
> >>
> >> x6
> >>
> >> 9.488404
> >>
> >> 9.470895
> >>
> >> 9.282434
> >>
> >> 9.366707
> >>
> >> 9.955383
> >>
> >> 9.640816
> >>
> >> 8.63063
> >>
> >> 8.558311
> >>
> >> 8.788391
> >>
> >> 8.576231
> >>
> >> 8.671588
> >>
> >> 8.84298
> >>
> >> 9.354749
> >>
> >> 9.367669
> >>
> >> 9.259953
> >>
> >> 9.421538
> >>
> >> 9.554635
> >>
> >> 9.603745
> >>
> >> 9.944864
> >>
> >> 9.950428
> >>
> >> 10.19102
> >>
> >> 10.07351
> >>
> >> 10.0327
> >>
> >> 10.13079
> >>
> >>
> >> Fc
> >>
> >> p
> >>
> >> 1.037883
> >>
> >> 0.320095
> >>
> >> 1.057708
> >>
> >> 0.060132
> >>
> >> 1.065191
> >>
> >> 0.111192
> >>
> >>
> >>
> >>
> >> I tried the code bellow but , it does not work?
> >>
> >> FCPval <-
> >> read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/FCPval.csv")
> >>
> >>  c=as.data.frame(FCPval)
> >>
> >> for (i in 1:rowN){if (C$F[i] >= 2 && C$F[i]<=-2&& C$p[i]<=3){
> >>
> >> dfrmPFC=data.frame(Fc=FC,p=p)}
> >> }
> >>
> >> the error is :
> >>
> >> Error in C$FC : object of type 'closure' is not subsettable.
> >>
> >> can i  extract the value according to the condition
> >>
> >> On Thu, Sep 3, 2015 at 8:14 AM, PIKAL Petr <[hidden email]
> >>
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=1><mailto:[hidde
> >> n email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=2>>>
> >> wrote:
> >> Hi
> >>
> >> p value of what? How do you know that it should be 0.3?
> >>
> >> Your HTML post is mangled and almost unreadable.
> >>
> >> Your code is not reproducible, we do not have AT1.csv
> >>
> >> AT1 shall be numeric so your reading process is wrong and following
> >> conversion to numeric shall be umnnecessary if you read your data
> properly.
> >>
> >> Cheers
> >> Petr
> >>
> >> > -----Original Message-----
> >> > From: R-help [mailto:[hidden email]
> >>
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=3><mailto:[hidde
> >> n email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=4>>]
> On
> >> Behalf Of shawin
> >> > Sent: Wednesday, September 02, 2015 9:08 PM
> >> > To: [hidden email]
> >>
> <http:///user/SendEmail.jtp?type=node&node=4711787&i=5><mailto:[hidde
> >> n email] <http:///user/SendEmail.jtp?type=node&node=4711787&i=6>>
> >> > Subject: Re: [R] Converting CSV file to numeric and calculate
> ttest
> >> >
> >> > p value should be 0.3
> >> >
> >> > On Wed, Sep 2, 2015 at 8:07 PM, Navien [via R] < [hidden email]
> >> > <http:///user/SendEmail.jtp?type=node&node=4711787&i=7>
> >> <mailto:ml-node%[hidden email]
> >> <http:///user/SendEmail.jtp?type=node&node=4711787&i=8>>> wrote:
> >> >
> >> > > I have a csv file i convert it to numeric value , then i apply
> >> > > t-test , but t-test result is not correct: please could you
> guide
> >> > > me. the
> >> > data :
> >> > > ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 1367452_at
> >> > > 9.488404158 9.470895414 9.282433728 9.366707445 9.955383045
> >> > > 9.640816474 9.606262272 9.329651027 9.434541611 9.473922432
> >> > > 9.311412966
> >> > > 9.3154885 9.434977488 9.470895414 9.764258059 1367453_at
> >> > > 8.630629966
> >> > > 8.55831075 8.788391003 8.576231135 8.671587906
> >> > > 8.842979993 8.861958856 8.58330436 8.603596508 8.570129609
> >> > > 8.59798922
> >> > > 8.572686772 8.679751791 8.663950953 8.432875347 1367454_at
> >> > 9.354748885
> >> > > 9.367668838 9.259952558 9.421538213 9.554635162
> >> > > 9.603744578 9.452197983 9.284228877 9.404607878 9.317737979
> >> > > 9.343115301
> >> > > 9.310644266 9.27227486 9.360337823 9.44706281 1367455_at
> >> > > 9.944863964
> >> > > 9.950427516 10.19101759 10.07350804 10.03269879
> >> > > 10.1307908 10.03487287 9.74609383 9.886379007 9.775472567
> >> > > 10.036596
> >> > > 9.544738458 9.699611598 9.911962567 9.625804277
> >> > >           AT1 <-
> >> > > read.csv("C:/Users/shawin/Desktop/RProgramms/RAdipose/AT1.csv",
> >> > >  stringsAsFactors=TRUE)
> >> > >
> >> > >
> >> > > str(AT1)
> >> > > matAT = data.frame(AT1[,-1])
> >> > > str(matAT)
> >> > > write.csv(matAT,"matAT.csv",row.names=FALSE)
> >> > > #convert the csv file to numeric
> >> > > matAT<-as.matrix(sapply(matAT,as.numeric))
> >> > > str(matAT)
> >> > > group<-factor(rep(c("Ob","Ln"),times=c(7,8)))
> >> > > t.test(matAT[1,]~group)
> >> > >
> >> > >
> >> > >
> >> > >
> >> > >
> >> > >
> >> > >
> >> > > ------------------------------
> >> > > If you reply to this email, your message will be added to the
> >> > > discussion
> >> > > below:
> >> > >
> >> > > http://r.789695.n4.nabble.com/Converting-CSV-file-to-numeric-
> and-
> >> > calcu
> >> > > late-ttest-tp4711770.html To start a new topic under R help,
> >> > > email [hidden email]
> >> <http:///user/SendEmail.jtp?type=node&node=4711787&i=9><mailto:ml-
> nod
> >> e%[hidden email]
> >> <http:///user/SendEmail.jtp?type=node&node=4711787&i=10>>
> >> > > To unsubscribe from R, click here
> >> > >
> >> >
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsub
> >> > scr
> >> > >
> >> >
> ibe_by_code&node=789695&code=c2hhd2lua2FyaW1AZ21haWwuY29tfDc4OTY5NX
> >> > wtM
> >> > > jQ0MzkwMjQ1>
> >> > > .
> >> > > NAML
> >> > >
> >> >
> <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro
> >> > _vi
> >> > >
> >> >
> ewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespa
> >> > ces
> >> > > .BasicNamespace-nabble.view.web.template.NabbleNamespace-
> >> > nabble.naml.n
> >> > > amespaces.BasicNamespace-
> nabble.view.web.template.NabbleNamespace
> >> > > -
> >> > nabb
> >> > >
> >> >
> le.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%2
> >> > 1na
> >> > > bble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> >> > send_instant_em
> >> > > ail%21nabble%3Aemail.naml>
> >> > >
> >> >
> >> >
> >> >
> >> >
> >> > --
> >> > View this message in context:
> >> > http://r.789695.n4.nabble.com/Converting-
> >> > CSV-file-to-numeric-and-calculate-ttest-tp4711770p4711771.html
> >> > Sent from the R help mailing list archive at Nabble.com.
> >> >       [[alternative HTML version deleted]]
> >> >

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From hwborchers at gmail.com  Fri Sep  4 13:46:00 2015
From: hwborchers at gmail.com (Hans W Borchers)
Date: Fri, 4 Sep 2015 13:46:00 +0200
Subject: [R] lsqlin in R package pracma
In-Reply-To: <2f3a88$1b7arm@ironport10.mayo.edu>
References: <CAML4n3OeKiQBQVPCz-OdmSTdWwG766FeUvuwRJmBzdaX6FMDtw@mail.gmail.com>
	<2f3a88$19rlff@ironport10.mayo.edu>
	<CAML4n3O1+__MzeD=Rq97zoo=Xdv0hok+RJcjRWZeg1vjQw7OLA@mail.gmail.com>
	<2f3a88$1b7arm@ironport10.mayo.edu>
Message-ID: <CAML4n3NNM9efDwSKenO0wZfqq=Kr08xQdZ2MqgLYnNBGH0XRAw@mail.gmail.com>

On Tue, Sep 1, 2015 at 11:24 PM, Wang, Xue, Ph.D. <Wang.Xue at mayo.edu> wrote:
>
> slsqp in R seems quite slow. Does anyone have some suggestion as how to speed up this?
>

It is no surprise that a general solver like slsqp() takes longer than
specialized quadratic solvers such as solve.QP, ipop(), or
lowRankQP(). You got good advice in this thread, even with code; so
why don't you use what has been suggested in this thread?

If in your timings you include calling lsqlin() in pracma, then this
will increase the time by at least 0.4 ms.

NLopt is an external, compiled program that cannot be accelerated from within R.

I wrote a small wrapper for all the different approaches proposed.
Here is a rough timing estimate (with microbenchmark, in
milliseconds), all derived from the MATLAB example:

                    with constraints    w/o constraints
    ---------------------------------------------------
    qr.solve        --                  0.06 ms
    slsqp           5.5   ms            5.25 ms     [*]
    solve.QP        0.045 ms            --
    ipop            5.0   ms            --
    lowRankQP       0.16  ms            --
    ---------------------------------------------------
    [*] plus 0.4 ms when using lsqlin for start values.

Because solve.QP is fastest, I append a function mimicking the MATLAB
API of lsqlin (without error handling for now) that can be used with
and without equality constraints, utilizing Berend's contribution. If
this is too slow, I guess you will have to look outside R, for example
Julia provides convex solvers that may be fast.

--- cut ----------------------------------------------------------------------
require(quadprog)
constrLsqlin <- function(C, d, A, b, # linear inequalities required
                         Aeq = NULL, beq = NULL, lb = NULL, ub = NULL) {
    m <- nrow(C); n <- ncol(C)
    meq <- nrow(Aeq); neq <- ncol(Aeq)
    if (is.null(meq)) meq = 0

    Dmat <- t(C) %*% C
    dvec <- t(C) %*% d

    if (is.null(Aeq)) {
        Amat <- -A
        bvec <- -b
    } else {
        Amat <- rbind(Aeq, -A)
        bvec <- c(beq, -b)
    }
    if (!is.null(lb)) {
        Amat <- rbind(Amat, diag(n))
        bvec <- c(bvec, lb)
    }
    if (!is.null(ub)) {
        Amat <- rbind(Amat, -diag(n))
        bvec <- c(bvec, -ub)
    }
    rslt <- solve.QP(Dmat, dvec, t(Amat), bvec, meq=meq)
    rslt$solution
}


From chelseaedwards92 at gmail.com  Fri Sep  4 14:20:32 2015
From: chelseaedwards92 at gmail.com (Chelsea Edwards)
Date: Fri, 4 Sep 2015 13:20:32 +0100
Subject: [R] Plotting a fourth variable on a persp plot
Message-ID: <CADF50GhAynY4emBSPgpsLnTjJgVK_gWxB5hCODL2N2s2NCwjiw@mail.gmail.com>

Hi all,

I'm trying to plot colour as a fourth variable onto a persp plot to
highlight the multiple modes in my dataset.

A plot with 3 variables (frequency, time and depth) has been created using
the code below:

require(MASS)
require(graphics)
require(plot3D)
require(pdfCluster)

## density estimate and persp plot with 3 variables
f1 <- kde2d(Max_d, Bottom_t, n=124, lims=c(0, 90, 0, 140))
persp(f1, phi = 40, theta = 50, d = 5, col="lightblue", xlab="Maximum
diving depth, m", ylab="Bottom time, m", zlab="Number of dives")

# f1$x represents depth - vector - length 124
# f1$y represents bottom time (duration) - vector - length 124
# f1$z is the frequency matrix - length 15375 (124 by 124 Matrix)


The colour variable has been produced as a vector (length 124) using a
cluster analysis
## Colour variable -  representing 3 modes within the data (1,2&3 in
g_class1)
test1 <- Penguins[,c(16, 11)]
test1a <- pdfCluster(test1, graphtype="delaunay", h=h.norm(test1),
hmult=1.2, options="Qt", kernel="gaussian", bwtype="fixed")
g_class1 <-groups(test1a, stage=5)
g1 <- data.frame(g_class1)
Penguins3 <- data.frame(c(Penguins, g1, f1))

## using persp3D and colvar to get colour plotted
persp3D(z=f1$z, x=f1$x, y=f1$y, bty="bl2", axes=T,  facets=NA,
colvar=Penguins3$g_class1, lighting=T, colkey=TRUE, xlab="Maximum depth",
ylab="Bottom_time", zlab="Number of dives",xlim=c(0,90), ylim=c(0,140),
ticktype="detailed", border=NA)

# error - as colvar is not equal in dimensions to matrix Z.

ggg1 <- matrix(Penguins3$g_class1, nrow=124, ncol=124)
persp3D(z=f1$z, x=f1$x, y=f1$y, bty="b2", axes=T, colvar=ggg1, colkey=TRUE,
xlab="Maximum depth", ylab="Bottom_time", zlab="Number of
dives",xlim=c(0,90), ylim=c(0,140), ticktype="detailed", border=NA)

# produces the chart but values are (obviously) wrong

I understand that my current colour variable is a vector when I actually
need a matrix, but I'm unsure how to get around this issue as the vector
length is 124 and the matrix I need would be 124X124.

Is there anyway to plot the colour as a variable over the perspective plot?

Thanks in advance,

Chelsea

	[[alternative HTML version deleted]]


From joyaa at goldcoastosteopathy.com.au  Fri Sep  4 13:07:19 2015
From: joyaa at goldcoastosteopathy.com.au (Joyaa Antares)
Date: Fri, 4 Sep 2015 21:07:19 +1000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
	10
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
	<8CB19DFF334846FEA36C098039F1A46C@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca>
Message-ID: <B7588D35DAE94C89A2D783C46E699A15@Zeus>

Dear John,

Very many thanks.  Your hypothesis was completely correct - I did save the 
workspace, and this is what created the problem.   As a newbie, being part 
way through an "exercise", saving my workspace intuitively seemed the right 
thing to do.   Clearly not!    So now, before I closed R and Rcmdr, I copied 
and pasted from Rcmdr's script file into a MS Word document, added a few 
comments (with preceding #), and then closed without otherwise saving R's 
script, output, workspace or markdown.  I hope this is reasonable practice. 
If not, I would be grateful for guidance on this.

Once again, very many thanks.  Joyaa

-----Original Message----- 
From: Fox, John
Sent: Friday, September 4, 2015 1:24 PM
To: Joyaa Antares
Cc: r-help at r-project.org
Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 
10

Dear Joyaa,

> -----Original Message-----
> From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> Sent: September 3, 2015 9:37 PM
> To: Fox, John <jfox at mcmaster.ca>
> Cc: r-help at r-project.org
> Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with 
> Windows 10
>
> Dear John, Dear List,
>
> Thank you for responding.  Like you, I have tried uninstalling R and 
> reinstalling
> again.  Question 1: having uninstalled R, how do you delete the package 
> library?

You say below that you installed R packages when R was run as administrator 
and that you took all defaults. In that case,  the installed packages will 
be in C:\Program Files\R\R-3.2.2\library . Just delete the library 
directory, which will still be there after R 3.2.2 is uninstalled. I do 
recommend, by the way, that, for using the Rcmdr, you install R with the SDI 
rather than the default MDI, but that doesn't explain your difficulties.

>
> I have used defaults throughout the process, except for running the whole 
> things
> as Administrator rather than a User.  For the mirror, my first two install 
> attempts
> were via a mirror in Germany.  My 3rd and 4th attempts via Cambridge, UK.
>
> Perhaps importantly: the first time I ran the RcmdrPlugin.survival plug-in 
> it
> worked fine.

If you said that before, I missed it.

> I got half way through the exercise I was attempting, then closed
> the programs for the night.  The next day, when I tried to reload the 
> plugin from
> R Commander, the plugin asked me to restart R Commander, but on opting for
> "Yes", this is when R Commander does not open properly.

Here's my hypothesis: When you "closed" R, you saved the R workspace. My 
guess is that this is the source of your problem. If so, the saved workspace 
is in the file .RData in your home directory; delete it and try again.

If this works, in future, exit from the Rcmdr and R via the Rcmdr menus (or 
if closing R directly, don't save the workspace).

I hope this helps,
John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox


>
> My thanks to you and the list for your time.  Joyaa (Gold Coast, 
> Queensland,
> Australia)
>
> -----Original Message-----
> From: Fox, John
> Sent: Friday, September 4, 2015 1:29 AM
> To: Joyaa Antares
> Cc: r-help at r-project.org
> Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with 
> Windows
> 10
>
> Dear Joyaa,
>
> I too run R 3.2.2 on a Windows 10 system. I uninstalled R, deleted my 
> package
> library, and then reinstalled R and the Rcmdr and RcmdrPlugin.survival 
> packages.
> I took all defaults, except that I selected the SDI rather than the 
> default MDI for
> Rgui, but I seriously doubt that this is the source of your problem. I 
> used the 0-
> Cloud mirror, both for R
> 3.2.2 and for packages.
>
> I'm afraid that I can't duplicate your problem -- everything works 
> perfectly fine
> for me.
>
> You provided a reasonable amount of detail, but is there anything else you 
> can
> add?
>
> Best,
> John
>
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> web: socserv.mcmaster.ca/jfox
>
>
> ________________________________________
> From: R-help [r-help-bounces at r-project.org] on behalf of Joyaa Antares
> [joyaa at goldcoastosteopathy.com.au]
> Sent: September 3, 2015 8:31 AM
> To: r-help at r-project.org
> Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 
> 10
>
> Hello r-help,
>
> I am quite new to R.  I downloaded R 3.2.2 for Windows to use with Windows
> 10.
>
> On attempting to load the RcmdrPlugin.survival plug-in I got this error
> message:
>
> Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
>   [tcl] invalid command name "configure".
>
> The lead-up to this was:
>
> * I uninstalled R 3.2.2 and deleted all R files I could find.
> * Downloaded R 3.2.2 to do a fresh install
> * Opened the .exe file as Administrator
> * Ran 64-bit R as Administrator
> * Updated all packages
> * Installed Rcmdr and RcmdrPlugin.survival
> * Ran Rcmdr using library(Rcmdr) from the console
> * This required an install of a host of packages, which I installed
> * Loaded RcmdrPlugin.survival from the Tools menu
> * I was asked to restart R commander, and when I tried it didn't open 
> properly
> (showing only File/Edit/Data in the menu bar), and this wouldn't close 
> properly
> either.  The error message as above showed in the console.
>
> I haven't posted to the list before and would be very grateful for help.
> Thank you.   Joyaa
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> =


From mensahaziz at yahoo.com  Fri Sep  4 08:53:49 2015
From: mensahaziz at yahoo.com (Aziz Mensah)
Date: Fri, 4 Sep 2015 06:53:49 +0000 (UTC)
Subject: [R] simulation in vector autoregressive model (VAR)
Message-ID: <686716862.1252905.1441349629475.JavaMail.yahoo@mail.yahoo.com>

I have a data from 4 variables ( STOCK, CPI, EXC, and CCI) from 1980 to 2012. I want to do a forecast using VAR(12) model with a simulation of 100,000 for 5 years. And also estimate the RMSE, MAPE, and Theil Inequality. Can anyone help me with this problem in R? Thanks so much.?

	[[alternative HTML version deleted]]


From callyak4real at yahoo.com  Fri Sep  4 14:40:56 2015
From: callyak4real at yahoo.com (callyak4real)
Date: Fri, 04 Sep 2015 13:40:56 +0100
Subject: [R] Gamma count
Message-ID: <dd85a3whaily88u4eke2jwy8.1441370456254@email.android.com>


I intend to simulate data from gamma distribution. I plan rounding the observations into counts. What I want to know is how I will manipulate the parameters of gamma to have both over and under dispersed count scenarios.
	[[alternative HTML version deleted]]


From jwd at surewest.net  Fri Sep  4 16:09:29 2015
From: jwd at surewest.net (jwd)
Date: Fri, 4 Sep 2015 07:09:29 -0700
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
 10
In-Reply-To: <B7588D35DAE94C89A2D783C46E699A15@Zeus>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
	<8CB19DFF334846FEA36C098039F1A46C@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca>
	<B7588D35DAE94C89A2D783C46E699A15@Zeus>
Message-ID: <20150904070929.6aa1327a@Draco.site>

On Fri, 4 Sep 2015 21:07:19 +1000
"Joyaa Antares" <joyaa at goldcoastosteopathy.com.au> wrote:

> Dear John,
> 
> Very many thanks.  Your hypothesis was completely correct - I did
> save the workspace, and this is what created the problem.   As a
> newbie, being part way through an "exercise", saving my workspace
> intuitively seemed the right thing to do.   Clearly not!    So now,
> before I closed R and Rcmdr, I copied and pasted from Rcmdr's script
> file into a MS Word document, added a few comments (with preceding
> #), and then closed without otherwise saving R's script, output,
> workspace or markdown.  I hope this is reasonable practice. If not, I
> would be grateful for guidance on this.
> 
> Once again, very many thanks.  Joyaa
> 
A potentially useful piece of advise is to avoid using MSWord as a text
editor.  I don't use Rcmdr, but text files, particularly CSVs are
extremely useful in R as data files.  Scripts should be pure text as
well.  Word demands that you keep this in mind at all times.  It won't
default to a simple text mode.  You can use notepad, which MS provides
as a stopgap, though I would recommend a more capable programmer's text
editor such as Notepad++, a very useful GPL-licensed text editor for
Windows, which can be downloaded for free.    

JWDougherty


From ragia11 at hotmail.com  Fri Sep  4 16:12:30 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Fri, 4 Sep 2015 17:12:30 +0300
Subject: [R] groups Rank
In-Reply-To: <DUB125-W49BC987BD7C478DCA7220FB3700@phx.gbl>
References: <DUB125-W10C5D212C9BD3CF6775318B3A00@phx.gbl>,
	<DUB125-W91C972100E24493CABA989B3700@phx.gbl>,
	<DUB125-W49BC987BD7C478DCA7220FB3700@phx.gbl>
Message-ID: <DUB125-W12FF665BD9DF1AD4630B40B3570@phx.gbl>

Dear Group,kinldy, I have the following data frame df
         id value1           1  4   2           1  4   3           1  6   4           1  6   5           2  1.5   6           2  2.5  7           2  2.5   8           2  2.5  

add rank column regarding id coulmn where rank for the highest value would be 1, others rank would be the (value/ value of heighest)/ number of rows that took the same value
thus the data frame should be
        id value                    Rank1           1  4                    0.332           1  4                    0.333           1  6                    0.54           1  6                    0.55           2  1.5                  0.6   6           2  2.5                  0.337           2  2.5                  0.338           2  2.5                  0.33

how to reach this resultthanks in advanceRagia 		 	   		   		 	   		   		 	   		  
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Fri Sep  4 16:17:58 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 04 Sep 2015 07:17:58 -0700
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
	10
In-Reply-To: <20150904070929.6aa1327a@Draco.site>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
	<8CB19DFF334846FEA36C098039F1A46C@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca>
	<B7588D35DAE94C89A2D783C46E699A15@Zeus>
	<20150904070929.6aa1327a@Draco.site>
Message-ID: <00765A09-9BA6-4A74-AC66-73ABE8388063@dcn.davis.CA.us>

This is a major point... do not make the mistake that you can "get by" using MSWord as a text editor because it actively changes your code in ways that make sense in a word processor but change code in ways that corrupt it.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 4, 2015 7:09:29 AM PDT, jwd <jwd at surewest.net> wrote:
>On Fri, 4 Sep 2015 21:07:19 +1000
>"Joyaa Antares" <joyaa at goldcoastosteopathy.com.au> wrote:
>
>> Dear John,
>> 
>> Very many thanks.  Your hypothesis was completely correct - I did
>> save the workspace, and this is what created the problem.   As a
>> newbie, being part way through an "exercise", saving my workspace
>> intuitively seemed the right thing to do.   Clearly not!    So now,
>> before I closed R and Rcmdr, I copied and pasted from Rcmdr's script
>> file into a MS Word document, added a few comments (with preceding
>> #), and then closed without otherwise saving R's script, output,
>> workspace or markdown.  I hope this is reasonable practice. If not, I
>> would be grateful for guidance on this.
>> 
>> Once again, very many thanks.  Joyaa
>> 
>A potentially useful piece of advise is to avoid using MSWord as a text
>editor.  I don't use Rcmdr, but text files, particularly CSVs are
>extremely useful in R as data files.  Scripts should be pure text as
>well.  Word demands that you keep this in mind at all times.  It won't
>default to a simple text mode.  You can use notepad, which MS provides
>as a stopgap, though I would recommend a more capable programmer's text
>editor such as Notepad++, a very useful GPL-licensed text editor for
>Windows, which can be downloaded for free.    
>
>JWDougherty
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From sarah.goslee at gmail.com  Fri Sep  4 16:19:35 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 4 Sep 2015 10:19:35 -0400
Subject: [R] groups Rank
In-Reply-To: <DUB125-W12FF665BD9DF1AD4630B40B3570@phx.gbl>
References: <DUB125-W10C5D212C9BD3CF6775318B3A00@phx.gbl>
	<DUB125-W91C972100E24493CABA989B3700@phx.gbl>
	<DUB125-W49BC987BD7C478DCA7220FB3700@phx.gbl>
	<DUB125-W12FF665BD9DF1AD4630B40B3570@phx.gbl>
Message-ID: <CAM_vjunHM8iAHisTytkn-EWjq42V4oo0NqOtQCbL3RvmXLqsZA@mail.gmail.com>

Hi Ragia,

I can't make out your data or desired result, but it sounds like
aggregate() might get you started. If you need more help, please
repost your data using dput() and do NOT post in HTML so that we can
see what your desired result looks like.

Sarah

On Fri, Sep 4, 2015 at 10:12 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
> Dear Group,kinldy, I have the following data frame df
>          id value1           1  4   2           1  4   3           1  6   4           1  6   5           2  1.5   6           2  2.5  7           2  2.5   8           2  2.5
>
> add rank column regarding id coulmn where rank for the highest value would be 1, others rank would be the (value/ value of heighest)/ number of rows that took the same value
> thus the data frame should be
>         id value                    Rank1           1  4                    0.332           1  4                    0.333           1  6                    0.54           1  6                    0.55           2  1.5                  0.6   6           2  2.5                  0.337           2  2.5                  0.338           2  2.5                  0.33
>
> how to reach this resultthanks in advanceRagia
>         [[alternative HTML version deleted]]
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From jfox at mcmaster.ca  Fri Sep  4 18:07:45 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Fri, 4 Sep 2015 16:07:45 +0000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
 10
In-Reply-To: <B7588D35DAE94C89A2D783C46E699A15@Zeus>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
	<8CB19DFF334846FEA36C098039F1A46C@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca>
	<B7588D35DAE94C89A2D783C46E699A15@Zeus>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC8A477EC@FHSDB4H16-2.csu.mcmaster.ca>

Dear Joyaa,

> -----Original Message-----
> From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> Sent: Friday, September 4, 2015 7:07 AM
> To: Fox, John
> Cc: r-help at r-project.org
> Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> Windows 10
> 
> Dear John,
> 
> Very many thanks.  Your hypothesis was completely correct - I did save
> the
> workspace, and this is what created the problem.   As a newbie, being
> part
> way through an "exercise", saving my workspace intuitively seemed the
> right
> thing to do.   Clearly not!    

It wouldn't have seemed the correct thing to do had you read the Rcmdr manual (accessible via "Help > Introduction to the R Commander"). See Section 6.5.

I understand why saving the workspace was made the default on exiting the R Console -- after all, it can help users to avoid inadvertently losing work -- but it my experience it is a frequent source of confusion among new users of R, whether or not they're using the R Commander.

> So now, before I closed R and Rcmdr, I
> copied
> and pasted from Rcmdr's script file into a MS Word document, added a few
> comments (with preceding #), and then closed without otherwise saving
> R's
> script, output, workspace or markdown.  I hope this is reasonable
> practice.
> If not, I would be grateful for guidance on this.

You've already received good advice from others about *not* using Word as a programming editor. I think that it's safe to say that the most popular programming editor for R (and for good reason) is RStudio. The Rcmdr script tab has basic editing capabilities, including saving and opening R scripts, and it offers to save the script when you exit.

You can, however, paste input and output, including graphs, from the Rcmdr into Word to create a report. It would be better, however, to use the Rcmdr Markdown tab to create reports in the form of HTML, PDF, or Word files. The Rcmdr includes an editor for R Markdown documents.  See Sections 6.1 and 6.2 of the manual.

Best,
 John

> 
> Once again, very many thanks.  Joyaa
> 
> -----Original Message-----
> From: Fox, John
> Sent: Friday, September 4, 2015 1:24 PM
> To: Joyaa Antares
> Cc: r-help at r-project.org
> Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> Windows
> 10
> 
> Dear Joyaa,
> 
> > -----Original Message-----
> > From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> > Sent: September 3, 2015 9:37 PM
> > To: Fox, John <jfox at mcmaster.ca>
> > Cc: r-help at r-project.org
> > Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > Windows 10
> >
> > Dear John, Dear List,
> >
> > Thank you for responding.  Like you, I have tried uninstalling R and
> > reinstalling
> > again.  Question 1: having uninstalled R, how do you delete the
> package
> > library?
> 
> You say below that you installed R packages when R was run as
> administrator
> and that you took all defaults. In that case,  the installed packages
> will
> be in C:\Program Files\R\R-3.2.2\library . Just delete the library
> directory, which will still be there after R 3.2.2 is uninstalled. I do
> recommend, by the way, that, for using the Rcmdr, you install R with the
> SDI
> rather than the default MDI, but that doesn't explain your difficulties.
> 
> >
> > I have used defaults throughout the process, except for running the
> whole
> > things
> > as Administrator rather than a User.  For the mirror, my first two
> install
> > attempts
> > were via a mirror in Germany.  My 3rd and 4th attempts via Cambridge,
> UK.
> >
> > Perhaps importantly: the first time I ran the RcmdrPlugin.survival
> plug-in
> > it
> > worked fine.
> 
> If you said that before, I missed it.
> 
> > I got half way through the exercise I was attempting, then closed
> > the programs for the night.  The next day, when I tried to reload the
> > plugin from
> > R Commander, the plugin asked me to restart R Commander, but on opting
> for
> > "Yes", this is when R Commander does not open properly.
> 
> Here's my hypothesis: When you "closed" R, you saved the R workspace. My
> guess is that this is the source of your problem. If so, the saved
> workspace
> is in the file .RData in your home directory; delete it and try again.
> 
> If this works, in future, exit from the Rcmdr and R via the Rcmdr menus
> (or
> if closing R directly, don't save the workspace).
> 
> I hope this helps,
> John
> 
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> Web: socserv.mcmaster.ca/jfox
> 
> 
> >
> > My thanks to you and the list for your time.  Joyaa (Gold Coast,
> > Queensland,
> > Australia)
> >
> > -----Original Message-----
> > From: Fox, John
> > Sent: Friday, September 4, 2015 1:29 AM
> > To: Joyaa Antares
> > Cc: r-help at r-project.org
> > Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > Windows
> > 10
> >
> > Dear Joyaa,
> >
> > I too run R 3.2.2 on a Windows 10 system. I uninstalled R, deleted my
> > package
> > library, and then reinstalled R and the Rcmdr and RcmdrPlugin.survival
> > packages.
> > I took all defaults, except that I selected the SDI rather than the
> > default MDI for
> > Rgui, but I seriously doubt that this is the source of your problem. I
> > used the 0-
> > Cloud mirror, both for R
> > 3.2.2 and for packages.
> >
> > I'm afraid that I can't duplicate your problem -- everything works
> > perfectly fine
> > for me.
> >
> > You provided a reasonable amount of detail, but is there anything else
> you
> > can
> > add?
> >
> > Best,
> > John
> >
> > -----------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > web: socserv.mcmaster.ca/jfox
> >
> >
> > ________________________________________
> > From: R-help [r-help-bounces at r-project.org] on behalf of Joyaa Antares
> > [joyaa at goldcoastosteopathy.com.au]
> > Sent: September 3, 2015 8:31 AM
> > To: r-help at r-project.org
> > Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> Windows
> > 10
> >
> > Hello r-help,
> >
> > I am quite new to R.  I downloaded R 3.2.2 for Windows to use with
> Windows
> > 10.
> >
> > On attempting to load the RcmdrPlugin.survival plug-in I got this
> error
> > message:
> >
> > Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
> >   [tcl] invalid command name "configure".
> >
> > The lead-up to this was:
> >
> > * I uninstalled R 3.2.2 and deleted all R files I could find.
> > * Downloaded R 3.2.2 to do a fresh install
> > * Opened the .exe file as Administrator
> > * Ran 64-bit R as Administrator
> > * Updated all packages
> > * Installed Rcmdr and RcmdrPlugin.survival
> > * Ran Rcmdr using library(Rcmdr) from the console
> > * This required an install of a host of packages, which I installed
> > * Loaded RcmdrPlugin.survival from the Tools menu
> > * I was asked to restart R commander, and when I tried it didn't open
> > properly
> > (showing only File/Edit/Data in the menu bar), and this wouldn't close
> > properly
> > either.  The error message as above showed in the console.
> >
> > I haven't posted to the list before and would be very grateful for
> help.
> > Thank you.   Joyaa
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > =


From ddalthorp at usgs.gov  Fri Sep  4 18:09:43 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Fri, 4 Sep 2015 09:09:43 -0700 (PDT)
Subject: [R] Gamma count
In-Reply-To: <dd85a3whaily88u4eke2jwy8.1441370456254@email.android.com>
References: <dd85a3whaily88u4eke2jwy8.1441370456254@email.android.com>
Message-ID: <1441382983785-4711853.post@n4.nabble.com>

Pick the mean (mu) and variance (sig2) you want. Then, shape = mu^2/sig2 and
scale = sig2/mu. This should work fine if your mean is large enough so that
p(x = 0 or 1) is small.



--
View this message in context: http://r.789695.n4.nabble.com/Gamma-count-tp4711845p4711853.html
Sent from the R help mailing list archive at Nabble.com.


From jvadams at usgs.gov  Fri Sep  4 18:17:45 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Fri, 4 Sep 2015 11:17:45 -0500
Subject: [R] Visualization of people's interactions by participation to
	parties
In-Reply-To: <55E69B4B.9@googlemail.com>
References: <55E69B4B.9@googlemail.com>
Message-ID: <CAN5YmCEpf8k6qdJQcmPvCkf7uO+7sAf-e8QO2cT3sAHZftxBAA@mail.gmail.com>

Hendrik,

It's not clear to me what kind of R help you are looking for.  I suggest
you provide more information on the data that you have and the questions
that you want answered.  Is it in an external file?  Is it an R object?
What code have you written or tried?  Including example data, for example
the output from dput(), is very helpful.

Jean

On Wed, Sep 2, 2015 at 1:46 AM, Voxcoelestis via R-help <
r-help at r-project.org> wrote:

> Dear all,
>
> I have a long list of parties and participants over many years and want to
> extract network relations between people to identify groups of friends. My
> list looks like this:
>
> Party 1; date party 1; first name 1 last name 1; first name 2 last name 2;
> first name 3 last name 3;
> Party 2; date party 2; first name 1 last name 1; first name 3 last name 3;
> first name 4 last name 4;
> Party 3; date party 3; first name 3 last name 3; first name 5 last name 5;
> Party 4; date party 4; first name 2 last name 2; first name 6 last name 6;
> first name 3 last name 3; first name 1 last name 1;
> Party 5; date party 5; first name 5 last name 5; first name 4 last name 4;
> ....
>
> Obviously the amount and the order of names is not regular. The list is
> far too long to count co-appearances for each person-person combination by
> hand.
>
> What I would like to do is first of all create a network with individual
> persons as nodes and the co-appearances as edges and the number of
> co-appearances as strenght of interactions clustering closesly related
> people.
>
> In a second step it would be beneficial to extract information on the
> durability of these interactions by including the time difference between
> first and last interaction.
>
> Do you have any ideas or hints how to approach this problem?
>
> Thank you so much,
>
> Hendrik
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From clint at ecy.wa.gov  Fri Sep  4 18:41:49 2015
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 4 Sep 2015 09:41:49 -0700 (PDT)
Subject: [R] Visualization of people's interactions by participation to
 parties
In-Reply-To: <CAN5YmCEpf8k6qdJQcmPvCkf7uO+7sAf-e8QO2cT3sAHZftxBAA@mail.gmail.com>
References: <55E69B4B.9@googlemail.com>
	<CAN5YmCEpf8k6qdJQcmPvCkf7uO+7sAf-e8QO2cT3sAHZftxBAA@mail.gmail.com>
Message-ID: <alpine.LRH.2.20.1509040936320.5633@aeolus.ecy.wa.gov>

Hendrik,

As a start, I'd make a matrix of zeros with the parties in rows 
and all of the participants as columns and put a one for each participant 
at each party.  The matrix will consist of a vector for each 
participant showing the parties attended. The pattern may suggest the next 
step.  Similarity of vectors could be an indication of interaction.

Clint

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600

         USPS:           PO Box 47600, Olympia, WA 98504-7600
         Parcels:        300 Desmond Drive, Lacey, WA 98503-1274

On Fri, 4 Sep 2015, Adams, Jean wrote:

> Hendrik,
>
> It's not clear to me what kind of R help you are looking for.  I suggest
> you provide more information on the data that you have and the questions
> that you want answered.  Is it in an external file?  Is it an R object?
> What code have you written or tried?  Including example data, for example
> the output from dput(), is very helpful.
>
> Jean
>
> On Wed, Sep 2, 2015 at 1:46 AM, Voxcoelestis via R-help <
> r-help at r-project.org> wrote:
>
>> Dear all,
>>
>> I have a long list of parties and participants over many years and want to
>> extract network relations between people to identify groups of friends. My
>> list looks like this:
>>
>> Party 1; date party 1; first name 1 last name 1; first name 2 last name 2;
>> first name 3 last name 3;
>> Party 2; date party 2; first name 1 last name 1; first name 3 last name 3;
>> first name 4 last name 4;
>> Party 3; date party 3; first name 3 last name 3; first name 5 last name 5;
>> Party 4; date party 4; first name 2 last name 2; first name 6 last name 6;
>> first name 3 last name 3; first name 1 last name 1;
>> Party 5; date party 5; first name 5 last name 5; first name 4 last name 4;
>> ....
>>
>> Obviously the amount and the order of names is not regular. The list is
>> far too long to count co-appearances for each person-person combination by
>> hand.
>>
>> What I would like to do is first of all create a network with individual
>> persons as nodes and the co-appearances as edges and the number of
>> co-appearances as strenght of interactions clustering closesly related
>> people.
>>
>> In a second step it would be beneficial to extract information on the
>> durability of these interactions by including the time difference between
>> first and last interaction.
>>
>> Do you have any ideas or hints how to approach this problem?
>>
>> Thank you so much,
>>
>> Hendrik
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kittlein at mdp.edu.ar  Fri Sep  4 13:51:02 2015
From: kittlein at mdp.edu.ar (Marcelo Kittlein)
Date: Fri, 4 Sep 2015 11:51:02 +0000
Subject: [R] getting a vector of unown size
In-Reply-To: <55E98137.8070007@mdp.edu.ar>
References: <55E98137.8070007@mdp.edu.ar>
Message-ID: <55E985A6.3020900@mdp.edu.ar>

Hi all!

I have written some code in C for simulating the fate of population 
using a dll loaded in R. During this simulation the population may go 
extinct such that the length of its trajectory is unknown beforehand. I 
wonder there is a way to define the size of the result in C and make it 
known to R to get the vector appropriately.

I have defined the output size very much larger than necessary to hold 
the result and then shrink the vector to the size where meaningful 
values occur, but this waste much resources and time unnecessarily.

The C code


#include <stdlib.h>
#include <math.h>


double min(double a, double b)
{
if(a>b)
return b;
else
return a;
}

double unirand()
{
return (rand()+1.0)/(RAND_MAX+1.0);
}

void RndSBDdemography(double *b, double *d, double *a, double *c, double 
*No, double *tmax, double *tiempo, double *Nind)
{
double tb, td, n;
int bc=0;
tiempo[0]=1.0;
Nind[0] = *No;

// This loop generates output of variable and a priori unknown size

while (Nind[bc] >= 2 && tiempo[bc] <= *tmax)
{
bc = bc + 1;
tb = -(1 / (*b - *a * Nind[bc-1])) * log(unirand());
td = -(1 / (*d + *c * Nind[bc-1])) * log(unirand());

tiempo[bc]= tiempo[bc-1] + min(tb, td)/ Nind[bc-1];

if (tb < td) n=1.0 ; else n= -1.0;

Nind[bc] = Nind[bc-1] + n;

}

}


The definition of the function call in R


RndSBDdemography = function(b=b,d=d,a=a,c=c, No=No, tmax=tmax){
tiempo=numeric(100000);
Nind=numeric(100000);
out=.C("RndSBDdemography",
b=as.double(b),
d=as.double(d),
a=as.double(a),
c=as.double(c),
No=as.double(No),
tmax=as.double(tmax),
tiempo=as.double(tiempo),
Nind=as.double(Nind)
)

# trim the output to a size where meaningful values occur
indi=which(out$tiempo!=0)
return(data.frame(tiempo=out$tiempo[indi], Nind=out$Nind[indi]))
}

Hope someone can give ahint on this.

Best Regards

Marcelo Kittlein


From dwinsemius at comcast.net  Fri Sep  4 20:44:21 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 4 Sep 2015 11:44:21 -0700
Subject: [R] simulation in vector autoregressive model (VAR)
In-Reply-To: <686716862.1252905.1441349629475.JavaMail.yahoo@mail.yahoo.com>
References: <686716862.1252905.1441349629475.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <A505F872-32CF-4FB8-846E-658675C5CC4C@comcast.net>

Before you post on Rhelp you should first read the Posting Guide. It tells you that requests for statistical advice might be answered but are not really on-topic. Furthermore requests for tutorials or extended worked examples should probably be accompanied by evidence of searching using Google or equivalent:

https://www.google.com/search?q=r+vector+auto-regressive+model&ie=utf-8&oe=utf-8

-- 
David
On Sep 3, 2015, at 11:53 PM, Aziz Mensah via R-help wrote:

> I have a data from 4 variables ( STOCK, CPI, EXC, and CCI) from 1980 to 2012. I want to do a forecast using VAR(12) model with a simulation of 100,000 for 5 years. And also estimate the RMSE, MAPE, and Theil Inequality. Can anyone help me with this problem in R? Thanks so much. 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.CA.us  Fri Sep  4 20:51:06 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 04 Sep 2015 11:51:06 -0700
Subject: [R] getting a vector of unown size
In-Reply-To: <55E985A6.3020900@mdp.edu.ar>
References: <55E98137.8070007@mdp.edu.ar> <55E985A6.3020900@mdp.edu.ar>
Message-ID: <3EF28DE0-DEF3-4470-A535-6DB148BA3EDD@dcn.davis.CA.us>

If you had read the Posting Guide as every posting admonishes you in the footer, you would know that this question about C belongs on R-devel, not R-help. I think a lot of C questions are addressed in the Writing R Extensions document also, so be sure to convey how the documentation failed to communicate an answer to you when you do post again.

If this question were about how to handle returning vectors of unknown size in an R function, I would point out that one end of the spectrum is your over allocation strategy, and the other end is building an intermediate list of atomic data values (rows of data on a file?) and in between those would be a list of small blocks of memory that you allocate as each "current" chunk is filled up. Such a list can be measured after all data are in memory so a block of exactly the right size can be allocated and the data transferred and the list can then be discarded/freed. Which direction to go depends strongly on the variability/size of your particular data and how much time you have to optimize it.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 4, 2015 4:51:02 AM PDT, Marcelo Kittlein <kittlein at mdp.edu.ar> wrote:
>Hi all!
>
>I have written some code in C for simulating the fate of population 
>using a dll loaded in R. During this simulation the population may go 
>extinct such that the length of its trajectory is unknown beforehand. I
>
>wonder there is a way to define the size of the result in C and make it
>
>known to R to get the vector appropriately.
>
>I have defined the output size very much larger than necessary to hold 
>the result and then shrink the vector to the size where meaningful 
>values occur, but this waste much resources and time unnecessarily.
>
>The C code
>
>
>#include <stdlib.h>
>#include <math.h>
>
>
>double min(double a, double b)
>{
>if(a>b)
>return b;
>else
>return a;
>}
>
>double unirand()
>{
>return (rand()+1.0)/(RAND_MAX+1.0);
>}
>
>void RndSBDdemography(double *b, double *d, double *a, double *c,
>double 
>*No, double *tmax, double *tiempo, double *Nind)
>{
>double tb, td, n;
>int bc=0;
>tiempo[0]=1.0;
>Nind[0] = *No;
>
>// This loop generates output of variable and a priori unknown size
>
>while (Nind[bc] >= 2 && tiempo[bc] <= *tmax)
>{
>bc = bc + 1;
>tb = -(1 / (*b - *a * Nind[bc-1])) * log(unirand());
>td = -(1 / (*d + *c * Nind[bc-1])) * log(unirand());
>
>tiempo[bc]= tiempo[bc-1] + min(tb, td)/ Nind[bc-1];
>
>if (tb < td) n=1.0 ; else n= -1.0;
>
>Nind[bc] = Nind[bc-1] + n;
>
>}
>
>}
>
>
>The definition of the function call in R
>
>
>RndSBDdemography = function(b=b,d=d,a=a,c=c, No=No, tmax=tmax){
>tiempo=numeric(100000);
>Nind=numeric(100000);
>out=.C("RndSBDdemography",
>b=as.double(b),
>d=as.double(d),
>a=as.double(a),
>c=as.double(c),
>No=as.double(No),
>tmax=as.double(tmax),
>tiempo=as.double(tiempo),
>Nind=as.double(Nind)
>)
>
># trim the output to a size where meaningful values occur
>indi=which(out$tiempo!=0)
>return(data.frame(tiempo=out$tiempo[indi], Nind=out$Nind[indi]))
>}
>
>Hope someone can give ahint on this.
>
>Best Regards
>
>Marcelo Kittlein
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Fri Sep  4 23:57:49 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 4 Sep 2015 23:57:49 +0200
Subject: [R] Urgent query
In-Reply-To: <CAGxFJbSX1F=B28p+W1y2-mmN-FUopMih4PA49kLAW8+rmSDEAw@mail.gmail.com>
References: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>
	<CAGxFJbSFhX1tvX7Nynk-JgNL5pCXr0GgfJsqn9rPc_MOznBFzQ@mail.gmail.com>
	<CAPoOT9odELqeX0bwaKPy=OTecTrAKdjYL-F3NZYcYycOVtsxvQ@mail.gmail.com>
	<CAGxFJbTJHFK8dYYdXOJUYgiYjJnLE7otvp+b5KMnZw3sQVGCaQ@mail.gmail.com>
	<CAPoOT9osKbkvdzuaTAR+F5_fy2zxMh2jivfpMLyTVcCCqswFhQ@mail.gmail.com>
	<CAGxFJbSX1F=B28p+W1y2-mmN-FUopMih4PA49kLAW8+rmSDEAw@mail.gmail.com>
Message-ID: <F051C9A6-981E-42A0-A0C8-A16099462C04@gmail.com>


> On 03 Sep 2015, at 19:34 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> You still failed to post to the list! Please learn how to use your
> mail client properly and **always** cc the list.
> 
> Anyway, here is an answer (there may be other ways):
> 
>> curve(dweibull(x,shape=0.8,scale=1),mgp=c(2,0.3,0),cex.lab=1.2,xlab="t",ylab="f(t)")
>> mtext(expression(italic(paste(alpha==1,","," ",0<phantom(),beta<1))),side=1,line=4,cex=1)
> 

I think I prefer:

mtext(expression({alpha==1}*","~0<{beta<1}),side=1,line=4,cex=1)
-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From bgunter.4567 at gmail.com  Sat Sep  5 00:40:05 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 4 Sep 2015 15:40:05 -0700
Subject: [R] Urgent query
In-Reply-To: <F051C9A6-981E-42A0-A0C8-A16099462C04@gmail.com>
References: <CAPoOT9rFbeAt_gUP2gVYqs0x9g4_3ztpZFxTUPUKWC8C4BocoQ@mail.gmail.com>
	<CAGxFJbSFhX1tvX7Nynk-JgNL5pCXr0GgfJsqn9rPc_MOznBFzQ@mail.gmail.com>
	<CAPoOT9odELqeX0bwaKPy=OTecTrAKdjYL-F3NZYcYycOVtsxvQ@mail.gmail.com>
	<CAGxFJbTJHFK8dYYdXOJUYgiYjJnLE7otvp+b5KMnZw3sQVGCaQ@mail.gmail.com>
	<CAPoOT9osKbkvdzuaTAR+F5_fy2zxMh2jivfpMLyTVcCCqswFhQ@mail.gmail.com>
	<CAGxFJbSX1F=B28p+W1y2-mmN-FUopMih4PA49kLAW8+rmSDEAw@mail.gmail.com>
	<F051C9A6-981E-42A0-A0C8-A16099462C04@gmail.com>
Message-ID: <CAGxFJbR2ijyuPHEBk_LLSXZwUhaZhkYWm78t_Sc2YTohYcw9-g@mail.gmail.com>

Yup. Me too. Much better.

-- Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Sep 4, 2015 at 2:57 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>
>> On 03 Sep 2015, at 19:34 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> You still failed to post to the list! Please learn how to use your
>> mail client properly and **always** cc the list.
>>
>> Anyway, here is an answer (there may be other ways):
>>
>>> curve(dweibull(x,shape=0.8,scale=1),mgp=c(2,0.3,0),cex.lab=1.2,xlab="t",ylab="f(t)")
>>> mtext(expression(italic(paste(alpha==1,","," ",0<phantom(),beta<1))),side=1,line=4,cex=1)
>>
>
> I think I prefer:
>
> mtext(expression({alpha==1}*","~0<{beta<1}),side=1,line=4,cex=1)
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>


From rxu823 at gmail.com  Fri Sep  4 23:18:22 2015
From: rxu823 at gmail.com (Roger Xu)
Date: Fri, 4 Sep 2015 17:18:22 -0400
Subject: [R] Manova: Data similarly generated are significantly different.
Message-ID: <CAFuMVsGPwBhDN3bF9mvh8OC5pp+xhu7g6bc4fyLxZQCftxo1nA@mail.gmail.com>

Dear R users,

Hi. I don't know if my understanding of Manova test is correct. So I test
with the following code and got strange results.

Any help would be appreciated.

y0, y1, and y2 are independently generated by the same method.

They are each split into 20 groups by the same method.
The summary of the Manova test (stored in variable s) says:

The Pr (>F) value is less than 2.2e-16.

y0 <- runif(100, 0, 1)
y1 <- runif(100, 0, 1)
y2 <- runif(100, 0, 1)

y0 <- c(y0, runif(100, 0, 10) )
y1 <- c(y1, runif(100, 0, 10) )
y2 <- c(y2, runif(100, 0, 10) )

y0=as.numeric(unlist(y0))
y1=as.numeric(unlist(y1))
y2=as.numeric(unlist(y2))

b=10
a=length(y0)/b
g=rep(1:a,rep(b,a))

m1 <- manova(cbind(y0, y1, y2) ~ g)
s=summary(m1, test = "Wilks")

a = s$stats
a = a[11]
s
a

The summary is here:

       Df   Wilks approx F num Df den Df    Pr(>F)
       g           1 0.37069   110.91      3    196 < 2.2e-16 ***
       Residuals 198
       ---
       Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

	[[alternative HTML version deleted]]


From nick.petschek at gmail.com  Fri Sep  4 23:27:46 2015
From: nick.petschek at gmail.com (Nick Petschek)
Date: Fri, 4 Sep 2015 17:27:46 -0400
Subject: [R] Composite index reliability questions - cronbach()
Message-ID: <CAGzwxb58_sUwir6u-AuTz1Qmbtnf0mu=ptX1OPkL02gFPXPA-g@mail.gmail.com>

Hi All,

I have two questions on using cronbach() from the psy() package.

My simplified situation is the following: I have a survey of 10 questions
(column names are "Q1", "Q2", etc.) that went out to 100+ people. I have
the responses to the questions, plus additional variables (demographics,
location, etc.) in a data frame (named "JDC"). I want to build composite
indices from these 10 questions. I plan to use two steps to create the
indices. First, grouping the questions by what makes intuitive sense given
what they ask, and second, by testing the reliability of these groupings
using cronbach().

QUESTION 1

Let's say I think Q1, Q3, and Q5 will make a good index. With my limited
knowledge of R, I would think there's a way to say "run the reliability on
these three variables in this dataframe". However, I have so far only been
able to test the reliability of *adjacent *variables. For example, I could
do:

*cronbach(jdc[,1:3])*

to test Q1, Q2, and Q3. Is there a way to test non-adjacent variables?

I realize I could do something like:

*trust <- jdc[, c("Q2", "Q7", "Q8")]*
*cronbach(trust)*

but that adds a few extra steps, and I have tons of questions and indices
which would make that very cumbersome, especially since I will go through
several iterations in testing potential indices.


QUESTION 2

Is there a way to refer to the column name when using cronbach(), instead
of just the location of the variable? For example:

This works:   *cronbach(jdc[,1:3])*
This doesn't:* cronbach(jdc[Q1, Q2, Q3])*


Thanks in advance for any insights, answers, words of encouragement, or
alternate ways I could solve this puzzle.

Nick

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sat Sep  5 02:04:30 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 4 Sep 2015 20:04:30 -0400
Subject: [R] Manova: Data similarly generated are significantly
	different.
In-Reply-To: <CAFuMVsGPwBhDN3bF9mvh8OC5pp+xhu7g6bc4fyLxZQCftxo1nA@mail.gmail.com>
References: <CAFuMVsGPwBhDN3bF9mvh8OC5pp+xhu7g6bc4fyLxZQCftxo1nA@mail.gmail.com>
Message-ID: <55EA318E.6000106@gmail.com>

On 04/09/2015 5:18 PM, Roger Xu wrote:
> y0 <- runif(100, 0, 1)
> y1 <- runif(100, 0, 1)
> y2 <- runif(100, 0, 1)
> 
> y0 <- c(y0, runif(100, 0, 10) )
> y1 <- c(y1, runif(100, 0, 10) )
> y2 <- c(y2, runif(100, 0, 10) )
> 
> y0=as.numeric(unlist(y0))
> y1=as.numeric(unlist(y1))
> y2=as.numeric(unlist(y2))
> 
> b=10
> a=length(y0)/b
> g=rep(1:a,rep(b,a))
> 
> m1 <- manova(cbind(y0, y1, y2) ~ g)
> s=summary(m1, test = "Wilks")
> 
> a = s$stats
> a = a[11]
> s
> a

If you plot any of the y columns vs g you see a huge effect.  You aren't
testing whether the y's are equal, you are testing whether the triplets
depend on g, and they do.

Duncan Murdoch


From drjimlemon at gmail.com  Sat Sep  5 13:23:30 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 5 Sep 2015 21:23:30 +1000
Subject: [R] Composite index reliability questions - cronbach()
In-Reply-To: <CAGzwxb58_sUwir6u-AuTz1Qmbtnf0mu=ptX1OPkL02gFPXPA-g@mail.gmail.com>
References: <CAGzwxb58_sUwir6u-AuTz1Qmbtnf0mu=ptX1OPkL02gFPXPA-g@mail.gmail.com>
Message-ID: <CA+8X3fXNQgCF7quAnHvcWCp2xXuZi-Ah8kngLmq0G+6G-rqEww@mail.gmail.com>

Hi Nick,
If you haven't just made a typo on your example in QUESTION 2, the "This
doesn't" line should read:

cronbach(jdc[,c("Q1","Q2","Q3")])

Without the quotes, R looks for three objects named Q1, Q2 and Q3 and
probably doesn't find them.

Jim


On Sat, Sep 5, 2015 at 7:27 AM, Nick Petschek <nick.petschek at gmail.com>
wrote:

> Hi All,
>
> I have two questions on using cronbach() from the psy() package.
>
> My simplified situation is the following: I have a survey of 10 questions
> (column names are "Q1", "Q2", etc.) that went out to 100+ people. I have
> the responses to the questions, plus additional variables (demographics,
> location, etc.) in a data frame (named "JDC"). I want to build composite
> indices from these 10 questions. I plan to use two steps to create the
> indices. First, grouping the questions by what makes intuitive sense given
> what they ask, and second, by testing the reliability of these groupings
> using cronbach().
>
> QUESTION 1
>
> Let's say I think Q1, Q3, and Q5 will make a good index. With my limited
> knowledge of R, I would think there's a way to say "run the reliability on
> these three variables in this dataframe". However, I have so far only been
> able to test the reliability of *adjacent *variables. For example, I could
> do:
>
> *cronbach(jdc[,1:3])*
>
> to test Q1, Q2, and Q3. Is there a way to test non-adjacent variables?
>
> I realize I could do something like:
>
> *trust <- jdc[, c("Q2", "Q7", "Q8")]*
> *cronbach(trust)*
>
> but that adds a few extra steps, and I have tons of questions and indices
> which would make that very cumbersome, especially since I will go through
> several iterations in testing potential indices.
>
>
> QUESTION 2
>
> Is there a way to refer to the column name when using cronbach(), instead
> of just the location of the variable? For example:
>
> This works:   *cronbach(jdc[,1:3])*
> This doesn't:* cronbach(jdc[Q1, Q2, Q3])*
>
>
> Thanks in advance for any insights, answers, words of encouragement, or
> alternate ways I could solve this puzzle.
>
> Nick
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sergio.fonda99 at gmail.com  Sat Sep  5 14:29:11 2015
From: sergio.fonda99 at gmail.com (Sergio Fonda)
Date: Sat, 5 Sep 2015 14:29:11 +0200
Subject: [R] factor interaction boxplot ordering by median
Message-ID: <CAJRuHopV_h=A8AopTcm1YJfLR6Cohon+OsJoKAscPs_QC7JMTg@mail.gmail.com>

 I would to visualize in boxplot a data frame with two factors ordering one
factor with the median.
As example,suppose to have the InsectSprays dataframe, where an "operator"
factor with two levels, op1 and op2, has been added as shown at bottom here.
How may be generated a boxplot showing boxes for the interaction spray*op,
ordered according to the operators' count median for every spray ?
Thanks in advance for any help!
Sergio
________________________________
Modified InsectSprays dataframe:
   count spray  op
1     10     A op1
2      7     A op1
3     20     A op1
4     14     A op1
5     14     A op1
6     12     A op1
7     10     A op2
8     23     A op2
9     17     A op2
10    20     A op2
11    14     A op2
12    13     A op2
13    11     B op1
14    17     B op1
15    21     B op1
16    11     B op1
17    16     B op1
18    14     B op1
19    17     B op2
20    17     B op2
21    19     B op2
22    21     B op2
23     7     B op2
24    13     B op2
25     0     C op1
26     1     C op1
27     7     C op1
28     2     C op1
29     3     C op1
30     1     C op1
31     2     C op2
32     1     C op2
33     3     C op2
34     0     C op2
35     1     C op2
36     4     C op2
37     3     D op1
38     5     D op1
39    12     D op1
40     6     D op1
41     4     D op1
42     3     D op1
43     5     D op2
44     5     D op2
45     5     D op2
46     5     D op2
47     2     D op2
48     4     D op2
49     3     E op1
50     5     E op1
51     3     E op1
52     5     E op1
53     3     E op1
54     6     E op1
55     1     E op2
56     1     E op2
57     3     E op2
58     2     E op2
59     6     E op2
60     4     E op2
61    11     F op1
62     9     F op1
63    15     F op1
64    22     F op1
65    15     F op1
66    16     F op1
67    13     F op2
68    10     F op2
69    26     F op2
70    26     F op2
71    24     F op2
72    13     F op2

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Sat Sep  5 14:33:04 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Sat, 5 Sep 2015 12:33:04 +0000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
 10
In-Reply-To: <0A9E7DB7EE7844B19D8114959045695D@Zeus>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
	<8CB19DFF334846FEA36C098039F1A46C@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca>
	<B7588D35DAE94C89A2D783C46E699A15@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A477EC@FHSDB4H16-2.csu.mcmaster.ca>
	<0A9E7DB7EE7844B19D8114959045695D@Zeus>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC8A47D77@FHSDB4H16-2.csu.mcmaster.ca>

Dear Joyaa,

> -----Original Message-----
> From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> Sent: September 5, 2015 2:07 AM
> To: Fox, John <jfox at mcmaster.ca>
> Cc: r-help at r-project.org
> Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 10
> 
> Dear John,
> 
> You mention, "the Rcmdr manual (accessible via "Help > Introduction to the R
> Commander")".
> 
> Apologies for my dumbness, but the manual I did read - and found helpful -
> before posting is clearly a different one.   A 43 page pdf  with no section
> numbers nor any reference to the Workspace.   (Package ?Rcmdr?    August 19,
> 2015     Version 2.2-0    Date 2015-08-14    Title R Commander ....//  snip
> //...   Author John Fox [aut, cre], Milan Bouchet-Valat [aut],  ..... //
> snip //").

I'm at a loss to explain why your copy of the manual has no section numbers, particularly since the version and date you cite are correct; as I've just confirmed, the PDF manual distributed with version 2.2-0 of the Rcmdr does indeed have section numbers, starting with "1 Introduction" near the top of the first page. In case it's somehow ambiguous, this is section 1. I'll send you a copy of the PDF separately, so as not to waste bandwidth on the r-help list.

> 
> Since the Help page you refer to doesn't seem to be here

I'm sorry, but I don't recall referring to a help page and I don't see a reference to a help page in this thread. Can you remind me what I apparently said?

Best,
 John

> https://stat.ethz.ch/mailman/listinfo/r-help  or here
> http://finzi.psych.upenn.edu/,  I would be very grateful for an extra direction
> and apologise for getting a bit lost amongst all the possibilities.
> 
> Kind regards, Joyaa  (feeling like a fish in a new ocean).
> 
> 
> -----Original Message-----
> From: Fox, John
> Sent: Saturday, September 5, 2015 2:07 AM
> To: Joyaa Antares
> Cc: r-help at r-project.org
> Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
> 10
> 
> Dear Joyaa,
> 
> > -----Original Message-----
> > From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> > Sent: Friday, September 4, 2015 7:07 AM
> > To: Fox, John
> > Cc: r-help at r-project.org
> > Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > Windows 10
> >
> > Dear John,
> >
> > Very many thanks.  Your hypothesis was completely correct - I did save
> > the
> > workspace, and this is what created the problem.   As a newbie, being
> > part
> > way through an "exercise", saving my workspace intuitively seemed the
> > right
> > thing to do.   Clearly not!
> 
> It wouldn't have seemed the correct thing to do had you read the Rcmdr manual
> (accessible via "Help > Introduction to the R Commander"). See Section 6.5.
> 
> I understand why saving the workspace was made the default on exiting the R
> Console -- after all, it can help users to avoid inadvertently losing work -- but it
> my experience it is a frequent source of confusion among new users of R,
> whether or not they're using the R Commander.
> 
> > So now, before I closed R and Rcmdr, I copied and pasted from Rcmdr's
> > script file into a MS Word document, added a few comments (with
> > preceding #), and then closed without otherwise saving R's script,
> > output, workspace or markdown.  I hope this is reasonable practice.
> > If not, I would be grateful for guidance on this.
> 
> You've already received good advice from others about *not* using Word as a
> programming editor. I think that it's safe to say that the most popular
> programming editor for R (and for good reason) is RStudio. The Rcmdr script tab
> has basic editing capabilities, including saving and opening R scripts, and it offers
> to save the script when you exit.
> 
> You can, however, paste input and output, including graphs, from the Rcmdr
> into Word to create a report. It would be better, however, to use the Rcmdr
> Markdown tab to create reports in the form of HTML, PDF, or Word files. The
> Rcmdr includes an editor for R Markdown documents.  See Sections 6.1 and 6.2
> of the manual.
> 
> Best,
> John
> 
> >
> > Once again, very many thanks.  Joyaa
> >
> > -----Original Message-----
> > From: Fox, John
> > Sent: Friday, September 4, 2015 1:24 PM
> > To: Joyaa Antares
> > Cc: r-help at r-project.org
> > Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > Windows
> > 10
> >
> > Dear Joyaa,
> >
> > > -----Original Message-----
> > > From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> > > Sent: September 3, 2015 9:37 PM
> > > To: Fox, John <jfox at mcmaster.ca>
> > > Cc: r-help at r-project.org
> > > Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > > Windows 10
> > >
> > > Dear John, Dear List,
> > >
> > > Thank you for responding.  Like you, I have tried uninstalling R and
> > > reinstalling again.  Question 1: having uninstalled R, how do you
> > > delete the
> > package
> > > library?
> >
> > You say below that you installed R packages when R was run as
> > administrator and that you took all defaults. In that case,  the
> > installed packages will be in C:\Program Files\R\R-3.2.2\library .
> > Just delete the library directory, which will still be there after R
> > 3.2.2 is uninstalled. I do recommend, by the way, that, for using the
> > Rcmdr, you install R with the SDI rather than the default MDI, but
> > that doesn't explain your difficulties.
> >
> > >
> > > I have used defaults throughout the process, except for running the
> > whole
> > > things
> > > as Administrator rather than a User.  For the mirror, my first two
> > install
> > > attempts
> > > were via a mirror in Germany.  My 3rd and 4th attempts via
> > > Cambridge,
> > UK.
> > >
> > > Perhaps importantly: the first time I ran the RcmdrPlugin.survival
> > plug-in
> > > it
> > > worked fine.
> >
> > If you said that before, I missed it.
> >
> > > I got half way through the exercise I was attempting, then closed
> > > the programs for the night.  The next day, when I tried to reload
> > > the plugin from R Commander, the plugin asked me to restart R
> > > Commander, but on opting
> > for
> > > "Yes", this is when R Commander does not open properly.
> >
> > Here's my hypothesis: When you "closed" R, you saved the R workspace.
> > My guess is that this is the source of your problem. If so, the saved
> > workspace is in the file .RData in your home directory; delete it and
> > try again.
> >
> > If this works, in future, exit from the Rcmdr and R via the Rcmdr
> > menus (or if closing R directly, don't save the workspace).
> >
> > I hope this helps,
> > John
> >
> > -----------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > Web: socserv.mcmaster.ca/jfox
> >
> >
> > >
> > > My thanks to you and the list for your time.  Joyaa (Gold Coast,
> > > Queensland,
> > > Australia)
> > >
> > > -----Original Message-----
> > > From: Fox, John
> > > Sent: Friday, September 4, 2015 1:29 AM
> > > To: Joyaa Antares
> > > Cc: r-help at r-project.org
> > > Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > > Windows
> > > 10
> > >
> > > Dear Joyaa,
> > >
> > > I too run R 3.2.2 on a Windows 10 system. I uninstalled R, deleted
> > > my package library, and then reinstalled R and the Rcmdr and
> > > RcmdrPlugin.survival packages.
> > > I took all defaults, except that I selected the SDI rather than the
> > > default MDI for Rgui, but I seriously doubt that this is the source
> > > of your problem. I used the 0- Cloud mirror, both for R
> > > 3.2.2 and for packages.
> > >
> > > I'm afraid that I can't duplicate your problem -- everything works
> > > perfectly fine for me.
> > >
> > > You provided a reasonable amount of detail, but is there anything
> > > else
> > you
> > > can
> > > add?
> > >
> > > Best,
> > > John
> > >
> > > -----------------------------
> > > John Fox, Professor
> > > McMaster University
> > > Hamilton, Ontario
> > > Canada L8S 4M4
> > > web: socserv.mcmaster.ca/jfox
> > >
> > >
> > > ________________________________________
> > > From: R-help [r-help-bounces at r-project.org] on behalf of Joyaa
> > > Antares [joyaa at goldcoastosteopathy.com.au]
> > > Sent: September 3, 2015 8:31 AM
> > > To: r-help at r-project.org
> > > Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > Windows
> > > 10
> > >
> > > Hello r-help,
> > >
> > > I am quite new to R.  I downloaded R 3.2.2 for Windows to use with
> > Windows
> > > 10.
> > >
> > > On attempting to load the RcmdrPlugin.survival plug-in I got this
> > error
> > > message:
> > >
> > > Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
> > >   [tcl] invalid command name "configure".
> > >
> > > The lead-up to this was:
> > >
> > > * I uninstalled R 3.2.2 and deleted all R files I could find.
> > > * Downloaded R 3.2.2 to do a fresh install
> > > * Opened the .exe file as Administrator
> > > * Ran 64-bit R as Administrator
> > > * Updated all packages
> > > * Installed Rcmdr and RcmdrPlugin.survival
> > > * Ran Rcmdr using library(Rcmdr) from the console
> > > * This required an install of a host of packages, which I installed
> > > * Loaded RcmdrPlugin.survival from the Tools menu
> > > * I was asked to restart R commander, and when I tried it didn't
> > > open properly (showing only File/Edit/Data in the menu bar), and
> > > this wouldn't close properly either.  The error message as above
> > > showed in the console.
> > >
> > > I haven't posted to the list before and would be very grateful for
> > help.
> > > Thank you.   Joyaa
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > > =


From marc_schwartz at me.com  Sat Sep  5 15:08:15 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Sat, 05 Sep 2015 08:08:15 -0500
Subject: [R] factor interaction boxplot ordering by median
In-Reply-To: <CAJRuHopV_h=A8AopTcm1YJfLR6Cohon+OsJoKAscPs_QC7JMTg@mail.gmail.com>
References: <CAJRuHopV_h=A8AopTcm1YJfLR6Cohon+OsJoKAscPs_QC7JMTg@mail.gmail.com>
Message-ID: <122369DF-537A-4D98-A626-1289C0CB4447@me.com>


> On Sep 5, 2015, at 7:29 AM, Sergio Fonda <sergio.fonda99 at gmail.com> wrote:
> 
> I would to visualize in boxplot a data frame with two factors ordering one
> factor with the median.
> As example,suppose to have the InsectSprays dataframe, where an "operator"
> factor with two levels, op1 and op2, has been added as shown at bottom here.
> How may be generated a boxplot showing boxes for the interaction spray*op,
> ordered according to the operators' count median for every spray ?
> Thanks in advance for any help!
> Sergio
> ________________________________
> Modified InsectSprays dataframe:

<snip>

Hi,

There is actually an example of reordering factor levels by a calculated numeric value using the InsectSprays data frame in ?boxplot using ?reorder. An interaction can be created by using ?interaction.

Given your data above, in a data frame ?DF?:

DF <- structure(list(count = c(10L, 7L, 20L, 14L, 14L, 12L, 10L, 23L, 
17L, 20L, 14L, 13L, 11L, 17L, 21L, 11L, 16L, 14L, 17L, 17L, 19L, 
21L, 7L, 13L, 0L, 1L, 7L, 2L, 3L, 1L, 2L, 1L, 3L, 0L, 1L, 4L, 
3L, 5L, 12L, 6L, 4L, 3L, 5L, 5L, 5L, 5L, 2L, 4L, 3L, 5L, 3L, 
5L, 3L, 6L, 1L, 1L, 3L, 2L, 6L, 4L, 11L, 9L, 15L, 22L, 15L, 16L, 
13L, 10L, 26L, 26L, 24L, 13L), spray = structure(c(1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L), .Label = c("A", "B", "C", "D", "E", "F"), class = "factor"), 
    op = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
    2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
    1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
    1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
    2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
    2L, 2L), .Label = c("op1", "op2"), class = "factor")), .Names = c("count", 
"spray", "op"), class = "data.frame", row.names = c("1", "2", 
"3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", 
"15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", 
"26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", 
"37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", 
"48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", 
"59", "60", "61", "62", "63", "64", "65", "66", "67", "68", "69", 
"70", "71", "72"))


# Modify the ?boxplot example
bymedian <- with(DF, reorder(interaction(spray, op), count, median))

> bymedian
 [1] A.op1 A.op1 A.op1 A.op1 A.op1 A.op1 A.op2 A.op2 A.op2 A.op2 A.op2
[12] A.op2 B.op1 B.op1 B.op1 B.op1 B.op1 B.op1 B.op2 B.op2 B.op2 B.op2
[23] B.op2 B.op2 C.op1 C.op1 C.op1 C.op1 C.op1 C.op1 C.op2 C.op2 C.op2
[34] C.op2 C.op2 C.op2 D.op1 D.op1 D.op1 D.op1 D.op1 D.op1 D.op2 D.op2
[45] D.op2 D.op2 D.op2 D.op2 E.op1 E.op1 E.op1 E.op1 E.op1 E.op1 E.op2
[56] E.op2 E.op2 E.op2 E.op2 E.op2 F.op1 F.op1 F.op1 F.op1 F.op1 F.op1
[67] F.op2 F.op2 F.op2 F.op2 F.op2 F.op2
attr(,"scores")
A.op1 B.op1 C.op1 D.op1 E.op1 F.op1 A.op2 B.op2 C.op2 D.op2 E.op2 F.op2 
 13.0  15.0   1.5   4.5   4.0  15.0  15.5  17.0   1.5   5.0   2.5  18.5 
12 Levels: C.op1 C.op2 E.op2 E.op1 D.op1 D.op2 A.op1 B.op1 ... F.op2


boxplot(count ~ bymedian, data = DF,
        xlab = "Interaction of spray and op", ylab = "Insect count",
        main = "Modified InsectSprays Data", varwidth = TRUE,
        col = "lightgray")


Regards,

Marc Schwartz


From sergio.fonda99 at gmail.com  Sat Sep  5 15:17:25 2015
From: sergio.fonda99 at gmail.com (Sergio Fonda)
Date: Sat, 5 Sep 2015 15:17:25 +0200
Subject: [R] factor interaction boxplot ordering by median
In-Reply-To: <122369DF-537A-4D98-A626-1289C0CB4447@me.com>
References: <CAJRuHopV_h=A8AopTcm1YJfLR6Cohon+OsJoKAscPs_QC7JMTg@mail.gmail.com>
	<122369DF-537A-4D98-A626-1289C0CB4447@me.com>
Message-ID: <CAJRuHophqWb7b86XQzYuwS17JjMuWquWMALSc+PJDD8D-+onvw@mail.gmail.com>

Great! Thank you
All the best
SF
Il 05/set/2015 15:08, "Marc Schwartz" <marc_schwartz at me.com> ha scritto:

>
> > On Sep 5, 2015, at 7:29 AM, Sergio Fonda <sergio.fonda99 at gmail.com>
> wrote:
> >
> > I would to visualize in boxplot a data frame with two factors ordering
> one
> > factor with the median.
> > As example,suppose to have the InsectSprays dataframe, where an
> "operator"
> > factor with two levels, op1 and op2, has been added as shown at bottom
> here.
> > How may be generated a boxplot showing boxes for the interaction
> spray*op,
> > ordered according to the operators' count median for every spray ?
> > Thanks in advance for any help!
> > Sergio
> > ________________________________
> > Modified InsectSprays dataframe:
>
> <snip>
>
> Hi,
>
> There is actually an example of reordering factor levels by a calculated
> numeric value using the InsectSprays data frame in ?boxplot using ?reorder.
> An interaction can be created by using ?interaction.
>
> Given your data above, in a data frame ?DF?:
>
> DF <- structure(list(count = c(10L, 7L, 20L, 14L, 14L, 12L, 10L, 23L,
> 17L, 20L, 14L, 13L, 11L, 17L, 21L, 11L, 16L, 14L, 17L, 17L, 19L,
> 21L, 7L, 13L, 0L, 1L, 7L, 2L, 3L, 1L, 2L, 1L, 3L, 0L, 1L, 4L,
> 3L, 5L, 12L, 6L, 4L, 3L, 5L, 5L, 5L, 5L, 2L, 4L, 3L, 5L, 3L,
> 5L, 3L, 6L, 1L, 1L, 3L, 2L, 6L, 4L, 11L, 9L, 15L, 22L, 15L, 16L,
> 13L, 10L, 26L, 26L, 24L, 13L), spray = structure(c(1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L,
> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
> 6L, 6L, 6L, 6L, 6L), .Label = c("A", "B", "C", "D", "E", "F"), class =
> "factor"),
>     op = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,
>     2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,
>     1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L,
>     1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,
>     2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,
>     2L, 2L), .Label = c("op1", "op2"), class = "factor")), .Names =
> c("count",
> "spray", "op"), class = "data.frame", row.names = c("1", "2",
> "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14",
> "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25",
> "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36",
> "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47",
> "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58",
> "59", "60", "61", "62", "63", "64", "65", "66", "67", "68", "69",
> "70", "71", "72"))
>
>
> # Modify the ?boxplot example
> bymedian <- with(DF, reorder(interaction(spray, op), count, median))
>
> > bymedian
>  [1] A.op1 A.op1 A.op1 A.op1 A.op1 A.op1 A.op2 A.op2 A.op2 A.op2 A.op2
> [12] A.op2 B.op1 B.op1 B.op1 B.op1 B.op1 B.op1 B.op2 B.op2 B.op2 B.op2
> [23] B.op2 B.op2 C.op1 C.op1 C.op1 C.op1 C.op1 C.op1 C.op2 C.op2 C.op2
> [34] C.op2 C.op2 C.op2 D.op1 D.op1 D.op1 D.op1 D.op1 D.op1 D.op2 D.op2
> [45] D.op2 D.op2 D.op2 D.op2 E.op1 E.op1 E.op1 E.op1 E.op1 E.op1 E.op2
> [56] E.op2 E.op2 E.op2 E.op2 E.op2 F.op1 F.op1 F.op1 F.op1 F.op1 F.op1
> [67] F.op2 F.op2 F.op2 F.op2 F.op2 F.op2
> attr(,"scores")
> A.op1 B.op1 C.op1 D.op1 E.op1 F.op1 A.op2 B.op2 C.op2 D.op2 E.op2 F.op2
>  13.0  15.0   1.5   4.5   4.0  15.0  15.5  17.0   1.5   5.0   2.5  18.5
> 12 Levels: C.op1 C.op2 E.op2 E.op1 D.op1 D.op2 A.op1 B.op1 ... F.op2
>
>
> boxplot(count ~ bymedian, data = DF,
>         xlab = "Interaction of spray and op", ylab = "Insect count",
>         main = "Modified InsectSprays Data", varwidth = TRUE,
>         col = "lightgray")
>
>
> Regards,
>
> Marc Schwartz
>
>

	[[alternative HTML version deleted]]


From ragia11 at hotmail.com  Sat Sep  5 16:44:13 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Sat, 5 Sep 2015 17:44:13 +0300
Subject: [R] groups Rank
In-Reply-To: <CAM_vjunHM8iAHisTytkn-EWjq42V4oo0NqOtQCbL3RvmXLqsZA@mail.gmail.com>
References: <DUB125-W10C5D212C9BD3CF6775318B3A00@phx.gbl>,
	<DUB125-W91C972100E24493CABA989B3700@phx.gbl>,
	<DUB125-W49BC987BD7C478DCA7220FB3700@phx.gbl>,
	<DUB125-W12FF665BD9DF1AD4630B40B3570@phx.gbl>,
	<CAM_vjunHM8iAHisTytkn-EWjq42V4oo0NqOtQCbL3RvmXLqsZA@mail.gmail.com>
Message-ID: <DUB125-W27A4D5CE21C3CF6038C503B3560@phx.gbl>

thanks for replying, I attached the data frame for source "i" I want to sum the values and get the max value then add a new column called rank . That new column cell value for each source i and for specific id would be (value/max value) * count of rows that have the same criteria "same i and same id"
many thanksRagia

> Date: Fri, 4 Sep 2015 10:19:35 -0400
> Subject: Re: [R] groups Rank
> From: sarah.goslee at gmail.com
> To: ragia11 at hotmail.com
> CC: r-help at r-project.org
> 
> Hi Ragia,
> 
> I can't make out your data or desired result, but it sounds like
> aggregate() might get you started. If you need more help, please
> repost your data using dput() and do NOT post in HTML so that we can
> see what your desired result looks like.
> 
> Sarah
> 
> On Fri, Sep 4, 2015 at 10:12 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
> > Dear Group,kinldy, I have the following data frame df
> >          id value1           1  4   2           1  4   3           1  6   4           1  6   5           2  1.5   6           2  2.5  7           2  2.5   8           2  2.5
> >
> > add rank column regarding id coulmn where rank for the highest value would be 1, others rank would be the (value/ value of heighest)/ number of rows that took the same value
> > thus the data frame should be
> >         id value                    Rank1           1  4                    0.332           1  4                    0.333           1  6                    0.54           1  6                    0.55           2  1.5                  0.6   6           2  2.5                  0.337           2  2.5                  0.338           2  2.5                  0.33
> >
> > how to reach this resultthanks in advanceRagia
> >         [[alternative HTML version deleted]]
> >
> 
> -- 
> Sarah Goslee
> http://www.functionaldiversity.org
 		 	   		  
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: measures.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150905/18b6e044/attachment.txt>

From ddalthorp at usgs.gov  Sat Sep  5 17:21:29 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Sat, 5 Sep 2015 08:21:29 -0700 (PDT)
Subject: [R] For Loops please help!!
In-Reply-To: <1441461490096-4711882.post@n4.nabble.com>
References: <1441461490096-4711882.post@n4.nabble.com>
Message-ID: <1441466489528-4711884.post@n4.nabble.com>

The code has an error so it won't run as written. 

Instead of:
infectrate[n]= (400)(1.1)^(n); 

try:
infectrate[n]= 400*1.1^n;

What I get after making this change looks right.



--
View this message in context: http://r.789695.n4.nabble.com/For-Loops-please-help-tp4711882p4711884.html
Sent from the R help mailing list archive at Nabble.com.


From ddalthorp at usgs.gov  Sat Sep  5 17:31:15 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Sat, 5 Sep 2015 08:31:15 -0700 (PDT)
Subject: [R] For Loops please help!!
In-Reply-To: <1441461490096-4711882.post@n4.nabble.com>
References: <1441461490096-4711882.post@n4.nabble.com>
Message-ID: <1441467075582-4711885.post@n4.nabble.com>

Also, any time you write "for" in R, you pay a steep price in performance. In
a short, simple loop it may not be noticeable, but in a more challenging
problem it can be a huge issue.

A more efficient way to write your loop would be:
infectrate = 400*1.1^(1:30) # calculation
cbind(1:30,log(infectrate))    # display

-Dan



--
View this message in context: http://r.789695.n4.nabble.com/For-Loops-please-help-tp4711882p4711885.html
Sent from the R help mailing list archive at Nabble.com.


From jdnewmil at dcn.davis.CA.us  Sat Sep  5 18:13:39 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sat, 05 Sep 2015 09:13:39 -0700
Subject: [R] For Loops please help!!
In-Reply-To: <1441467075582-4711885.post@n4.nabble.com>
References: <1441461490096-4711882.post@n4.nabble.com>
	<1441467075582-4711885.post@n4.nabble.com>
Message-ID: <201C385B-DE47-4D6F-9234-B42DB2AB1DEF@dcn.davis.CA.us>

This is not true. The steep price has to do with memory use patterns like result <- c( result, new value ). Vectorization is cleaner, easier to read, and somewhat faster, but for loops are not the monster that they have a reputation for being if the memory is allocated before the loop and elements are updated in the loop. In particular the more complicated the algorithm inside the loop, the less incremental overhead the for loop will introduce.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 5, 2015 8:31:15 AM PDT, Dan D <ddalthorp at usgs.gov> wrote:
>Also, any time you write "for" in R, you pay a steep price in
>performance. In
>a short, simple loop it may not be noticeable, but in a more
>challenging
>problem it can be a huge issue.
>
>A more efficient way to write your loop would be:
>infectrate = 400*1.1^(1:30) # calculation
>cbind(1:30,log(infectrate))    # display
>
>-Dan
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/For-Loops-please-help-tp4711882p4711885.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Sat Sep  5 18:31:40 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Sat, 5 Sep 2015 09:31:40 -0700 (PDT)
Subject: [R] For Loops please help!!
In-Reply-To: <201C385B-DE47-4D6F-9234-B42DB2AB1DEF@dcn.davis.CA.us>
References: <1441461490096-4711882.post@n4.nabble.com>
	<1441467075582-4711885.post@n4.nabble.com>
	<201C385B-DE47-4D6F-9234-B42DB2AB1DEF@dcn.davis.CA.us>
Message-ID: <1441470700594-4711887.post@n4.nabble.com>

Yes, the cause is memory use patterns, but the price is steep nonetheless.

E.g.:

rate<-log(400*1.1^(1:30)) # runs about 27x times as fast as the following
(test via 'microbenchmark') 

rate<-numeric(30)
for (i in 1:30){
   rate[i]<-log(400*1.1^i)
}

When manipulating large arrays, the difference can easily be a few seconds
vs. an hour or more. And if many such arrays need to be run, the difference
is between "difficult" and "not feasible". 

-Dan



--
View this message in context: http://r.789695.n4.nabble.com/For-Loops-please-help-tp4711882p4711887.html
Sent from the R help mailing list archive at Nabble.com.


From joyaa at goldcoastosteopathy.com.au  Sat Sep  5 07:52:59 2015
From: joyaa at goldcoastosteopathy.com.au (Joyaa Antares)
Date: Sat, 5 Sep 2015 15:52:59 +1000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
	10
In-Reply-To: <20150904070929.6aa1327a@Draco.site>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus><ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca><8CB19DFF334846FEA36C098039F1A46C@Zeus><ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca><B7588D35DAE94C89A2D783C46E699A15@Zeus>
	<20150904070929.6aa1327a@Draco.site>
Message-ID: <F342E21D2719443888F99E7957CDFE8D@Zeus>

Der JWD, Dear Jeff Newmiller,

Many thanks for your useful advice.     JWD - I have notepad++ and I don't 
know why I didn't think of that!     Having said that, I will take a look at 
RStudio that John mentions too.

Thanks again.  Much appreciated.  Joyaa

-----Original Message----- 
From: jwd
Sent: Saturday, September 5, 2015 12:09 AM
To: r-help at r-project.org
Cc: Joyaa Antares
Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 
10

On Fri, 4 Sep 2015 21:07:19 +1000
"Joyaa Antares" <joyaa at goldcoastosteopathy.com.au> wrote:

> Dear John,
>
> Very many thanks.  Your hypothesis was completely correct - I did
> save the workspace, and this is what created the problem.   As a
> newbie, being part way through an "exercise", saving my workspace
> intuitively seemed the right thing to do.   Clearly not!    So now,
> before I closed R and Rcmdr, I copied and pasted from Rcmdr's script
> file into a MS Word document, added a few comments (with preceding
> #), and then closed without otherwise saving R's script, output,
> workspace or markdown.  I hope this is reasonable practice. If not, I
> would be grateful for guidance on this.
>
> Once again, very many thanks.  Joyaa
>
A potentially useful piece of advise is to avoid using MSWord as a text
editor.  I don't use Rcmdr, but text files, particularly CSVs are
extremely useful in R as data files.  Scripts should be pure text as
well.  Word demands that you keep this in mind at all times.  It won't
default to a simple text mode.  You can use notepad, which MS provides
as a stopgap, though I would recommend a more capable programmer's text
editor such as Notepad++, a very useful GPL-licensed text editor for
Windows, which can be downloaded for free.

JWDougherty


From joyaa at goldcoastosteopathy.com.au  Sat Sep  5 08:07:10 2015
From: joyaa at goldcoastosteopathy.com.au (Joyaa Antares)
Date: Sat, 5 Sep 2015 16:07:10 +1000
Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows
	10
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC8A477EC@FHSDB4H16-2.csu.mcmaster.ca>
References: <823426D10F76410EB7709DB1CE9FEF61@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A4719F@FHSDB4H16-2.csu.mcmaster.ca>
	<8CB19DFF334846FEA36C098039F1A46C@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A475A4@FHSDB4H16-2.csu.mcmaster.ca>
	<B7588D35DAE94C89A2D783C46E699A15@Zeus>
	<ACD1644AA6C67E4FBD0C350625508EC8A477EC@FHSDB4H16-2.csu.mcmaster.ca>
Message-ID: <0A9E7DB7EE7844B19D8114959045695D@Zeus>

Dear John,

You mention, "the Rcmdr manual (accessible via "Help > Introduction to the R 
Commander")".

Apologies for my dumbness, but the manual I did read - and found helpful - 
before posting is clearly a different one.   A 43 page pdf  with no section 
numbers nor any reference to the Workspace.   (Package ?Rcmdr?    August 19, 
2015     Version 2.2-0    Date 2015-08-14    Title R Commander ....//  snip 
//...   Author John Fox [aut, cre], Milan Bouchet-Valat [aut],  ..... // 
snip //").

Since the Help page you refer to doesn't seem to be here 
https://stat.ethz.ch/mailman/listinfo/r-help  or here 
http://finzi.psych.upenn.edu/,  I would be very grateful for an extra 
direction and apologise for getting a bit lost amongst all the 
possibilities.

Kind regards, Joyaa  (feeling like a fish in a new ocean).


-----Original Message----- 
From: Fox, John
Sent: Saturday, September 5, 2015 2:07 AM
To: Joyaa Antares
Cc: r-help at r-project.org
Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with Windows 
10

Dear Joyaa,

> -----Original Message-----
> From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> Sent: Friday, September 4, 2015 7:07 AM
> To: Fox, John
> Cc: r-help at r-project.org
> Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> Windows 10
>
> Dear John,
>
> Very many thanks.  Your hypothesis was completely correct - I did save
> the
> workspace, and this is what created the problem.   As a newbie, being
> part
> way through an "exercise", saving my workspace intuitively seemed the
> right
> thing to do.   Clearly not!

It wouldn't have seemed the correct thing to do had you read the Rcmdr 
manual (accessible via "Help > Introduction to the R Commander"). See 
Section 6.5.

I understand why saving the workspace was made the default on exiting the R 
Console -- after all, it can help users to avoid inadvertently losing 
work -- but it my experience it is a frequent source of confusion among new 
users of R, whether or not they're using the R Commander.

> So now, before I closed R and Rcmdr, I
> copied
> and pasted from Rcmdr's script file into a MS Word document, added a few
> comments (with preceding #), and then closed without otherwise saving
> R's
> script, output, workspace or markdown.  I hope this is reasonable
> practice.
> If not, I would be grateful for guidance on this.

You've already received good advice from others about *not* using Word as a 
programming editor. I think that it's safe to say that the most popular 
programming editor for R (and for good reason) is RStudio. The Rcmdr script 
tab has basic editing capabilities, including saving and opening R scripts, 
and it offers to save the script when you exit.

You can, however, paste input and output, including graphs, from the Rcmdr 
into Word to create a report. It would be better, however, to use the Rcmdr 
Markdown tab to create reports in the form of HTML, PDF, or Word files. The 
Rcmdr includes an editor for R Markdown documents.  See Sections 6.1 and 6.2 
of the manual.

Best,
John

>
> Once again, very many thanks.  Joyaa
>
> -----Original Message-----
> From: Fox, John
> Sent: Friday, September 4, 2015 1:24 PM
> To: Joyaa Antares
> Cc: r-help at r-project.org
> Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> Windows
> 10
>
> Dear Joyaa,
>
> > -----Original Message-----
> > From: Joyaa Antares [mailto:joyaa at goldcoastosteopathy.com.au]
> > Sent: September 3, 2015 9:37 PM
> > To: Fox, John <jfox at mcmaster.ca>
> > Cc: r-help at r-project.org
> > Subject: Re: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > Windows 10
> >
> > Dear John, Dear List,
> >
> > Thank you for responding.  Like you, I have tried uninstalling R and
> > reinstalling
> > again.  Question 1: having uninstalled R, how do you delete the
> package
> > library?
>
> You say below that you installed R packages when R was run as
> administrator
> and that you took all defaults. In that case,  the installed packages
> will
> be in C:\Program Files\R\R-3.2.2\library . Just delete the library
> directory, which will still be there after R 3.2.2 is uninstalled. I do
> recommend, by the way, that, for using the Rcmdr, you install R with the
> SDI
> rather than the default MDI, but that doesn't explain your difficulties.
>
> >
> > I have used defaults throughout the process, except for running the
> whole
> > things
> > as Administrator rather than a User.  For the mirror, my first two
> install
> > attempts
> > were via a mirror in Germany.  My 3rd and 4th attempts via Cambridge,
> UK.
> >
> > Perhaps importantly: the first time I ran the RcmdrPlugin.survival
> plug-in
> > it
> > worked fine.
>
> If you said that before, I missed it.
>
> > I got half way through the exercise I was attempting, then closed
> > the programs for the night.  The next day, when I tried to reload the
> > plugin from
> > R Commander, the plugin asked me to restart R Commander, but on opting
> for
> > "Yes", this is when R Commander does not open properly.
>
> Here's my hypothesis: When you "closed" R, you saved the R workspace. My
> guess is that this is the source of your problem. If so, the saved
> workspace
> is in the file .RData in your home directory; delete it and try again.
>
> If this works, in future, exit from the Rcmdr and R via the Rcmdr menus
> (or
> if closing R directly, don't save the workspace).
>
> I hope this helps,
> John
>
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> Web: socserv.mcmaster.ca/jfox
>
>
> >
> > My thanks to you and the list for your time.  Joyaa (Gold Coast,
> > Queensland,
> > Australia)
> >
> > -----Original Message-----
> > From: Fox, John
> > Sent: Friday, September 4, 2015 1:29 AM
> > To: Joyaa Antares
> > Cc: r-help at r-project.org
> > Subject: RE: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> > Windows
> > 10
> >
> > Dear Joyaa,
> >
> > I too run R 3.2.2 on a Windows 10 system. I uninstalled R, deleted my
> > package
> > library, and then reinstalled R and the Rcmdr and RcmdrPlugin.survival
> > packages.
> > I took all defaults, except that I selected the SDI rather than the
> > default MDI for
> > Rgui, but I seriously doubt that this is the source of your problem. I
> > used the 0-
> > Cloud mirror, both for R
> > 3.2.2 and for packages.
> >
> > I'm afraid that I can't duplicate your problem -- everything works
> > perfectly fine
> > for me.
> >
> > You provided a reasonable amount of detail, but is there anything else
> you
> > can
> > add?
> >
> > Best,
> > John
> >
> > -----------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > web: socserv.mcmaster.ca/jfox
> >
> >
> > ________________________________________
> > From: R-help [r-help-bounces at r-project.org] on behalf of Joyaa Antares
> > [joyaa at goldcoastosteopathy.com.au]
> > Sent: September 3, 2015 8:31 AM
> > To: r-help at r-project.org
> > Subject: [R] Unable to run RcmdrPlugin.survival using 3.2.2 with
> Windows
> > 10
> >
> > Hello r-help,
> >
> > I am quite new to R.  I downloaded R 3.2.2 for Windows to use with
> Windows
> > 10.
> >
> > On attempting to load the RcmdrPlugin.survival plug-in I got this
> error
> > message:
> >
> > Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
> >   [tcl] invalid command name "configure".
> >
> > The lead-up to this was:
> >
> > * I uninstalled R 3.2.2 and deleted all R files I could find.
> > * Downloaded R 3.2.2 to do a fresh install
> > * Opened the .exe file as Administrator
> > * Ran 64-bit R as Administrator
> > * Updated all packages
> > * Installed Rcmdr and RcmdrPlugin.survival
> > * Ran Rcmdr using library(Rcmdr) from the console
> > * This required an install of a host of packages, which I installed
> > * Loaded RcmdrPlugin.survival from the Tools menu
> > * I was asked to restart R commander, and when I tried it didn't open
> > properly
> > (showing only File/Edit/Data in the menu bar), and this wouldn't close
> > properly
> > either.  The error message as above showed in the console.
> >
> > I haven't posted to the list before and would be very grateful for
> help.
> > Thank you.   Joyaa
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > =


From joyaa at goldcoastosteopathy.com.au  Sat Sep  5 08:23:00 2015
From: joyaa at goldcoastosteopathy.com.au (Joyaa)
Date: Fri, 4 Sep 2015 23:23:00 -0700 (PDT)
Subject: [R] R on Windows 10
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC8A473ED@FHSDB4H16-2.csu.mcmaster.ca>
References: <DM2PR02MB334943BFE99A5EAF8232329D2680@DM2PR02MB334.namprd02.prod.outlook.com>
	<32B2A0D6-48DF-4EAD-BD1D-C8566765834E@dcn.davis.CA.us>
	<ACD1644AA6C67E4FBD0C350625508EC8A473ED@FHSDB4H16-2.csu.mcmaster.ca>
Message-ID: <1441434180648-4711867.post@n4.nabble.com>

Dear Stephen, 

I was the new R user having trouble "with Windows 10", but it turns out my
issues were due to saving the workspace, rather than anything to do with
Windows 10.  That problem is now sorted and so far all looks good.

HTH, Joyaa


John Fox wrote
> I can confirm that I'm using R 3.2.2 (and R-devel) on two Windows 10
> systems with no problems whatsoever. As well, although I can't be sure, I
> doubt that the problem reported earlier today (concerning the
> RcmdrPlugin.survival package) was related to use of Windows 10.
> 
> I hope this helps,
>  John
> 
> -----------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.socsci.mcmaster.ca/jfox/
> 
> 
> 
>> -----Original Message-----
>> From: R-help [mailto:

> r-help-bounces@

> ] On Behalf Of Jeff
>> Newmiller
>> Sent: Thursday, September 3, 2015 4:46 PM
>> To: Martinez, Stephen Anthony - marti3sa; 

> r-help@

>> Subject: Re: [R] R on Windows 10
>> 
>> Apparently yes. Just this morning John Fox said he was using R 3.2.2 on
>> Windows 10. To be fair, he was responding to someone who was having
>> trouble, but as typical there were no details.
>> 
>> The posting guide does recommend searching the archives before posting.
>> ------------------------------------------------------------------------
>> ---
>> Jeff Newmiller                        The     .....       .....  Go





--
View this message in context: http://r.789695.n4.nabble.com/R-on-Windows-10-tp4711815p4711867.html
Sent from the R help mailing list archive at Nabble.com.


From nick.petschek at gmail.com  Sat Sep  5 18:09:41 2015
From: nick.petschek at gmail.com (Nick Petschek)
Date: Sat, 5 Sep 2015 12:09:41 -0400
Subject: [R] Composite index reliability questions - cronbach()
In-Reply-To: <CA+8X3fXNQgCF7quAnHvcWCp2xXuZi-Ah8kngLmq0G+6G-rqEww@mail.gmail.com>
References: <CAGzwxb58_sUwir6u-AuTz1Qmbtnf0mu=ptX1OPkL02gFPXPA-g@mail.gmail.com>
	<CA+8X3fXNQgCF7quAnHvcWCp2xXuZi-Ah8kngLmq0G+6G-rqEww@mail.gmail.com>
Message-ID: <CAGzwxb5BB5w3Bu3TDseXet1my-m1w=F_SddNZ1BAO_WEib-gQA@mail.gmail.com>

Thanks, Jim!

The lack of quotes was a typo, but what was not was my forgetting to
include the "c(" function... Thanks!



On Sat, Sep 5, 2015 at 7:23 AM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Nick,
> If you haven't just made a typo on your example in QUESTION 2, the "This
> doesn't" line should read:
>
> cronbach(jdc[,c("Q1","Q2","Q3")])
>
> Without the quotes, R looks for three objects named Q1, Q2 and Q3 and
> probably doesn't find them.
>
> Jim
>
>
> On Sat, Sep 5, 2015 at 7:27 AM, Nick Petschek <nick.petschek at gmail.com>
> wrote:
>
>> Hi All,
>>
>> I have two questions on using cronbach() from the psy() package.
>>
>> My simplified situation is the following: I have a survey of 10 questions
>> (column names are "Q1", "Q2", etc.) that went out to 100+ people. I have
>> the responses to the questions, plus additional variables (demographics,
>> location, etc.) in a data frame (named "JDC"). I want to build composite
>> indices from these 10 questions. I plan to use two steps to create the
>> indices. First, grouping the questions by what makes intuitive sense given
>> what they ask, and second, by testing the reliability of these groupings
>> using cronbach().
>>
>> QUESTION 1
>>
>> Let's say I think Q1, Q3, and Q5 will make a good index. With my limited
>> knowledge of R, I would think there's a way to say "run the reliability on
>> these three variables in this dataframe". However, I have so far only been
>> able to test the reliability of *adjacent *variables. For example, I could
>> do:
>>
>> *cronbach(jdc[,1:3])*
>>
>> to test Q1, Q2, and Q3. Is there a way to test non-adjacent variables?
>>
>> I realize I could do something like:
>>
>> *trust <- jdc[, c("Q2", "Q7", "Q8")]*
>> *cronbach(trust)*
>>
>> but that adds a few extra steps, and I have tons of questions and indices
>> which would make that very cumbersome, especially since I will go through
>> several iterations in testing potential indices.
>>
>>
>> QUESTION 2
>>
>> Is there a way to refer to the column name when using cronbach(), instead
>> of just the location of the variable? For example:
>>
>> This works:   *cronbach(jdc[,1:3])*
>> This doesn't:* cronbach(jdc[Q1, Q2, Q3])*
>>
>>
>> Thanks in advance for any insights, answers, words of encouragement, or
>> alternate ways I could solve this puzzle.
>>
>> Nick
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From suharto_anggono at yahoo.com  Sat Sep  5 19:02:05 2015
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Sat, 5 Sep 2015 10:02:05 -0700
Subject: [R] For logical i and length(i) > length(x),
	x[i] <- value makes length(x) == length(i)
Message-ID: <1441472525.27020.YahooMailBasic@web125101.mail.ne1.yahoo.com>

I came across this behavior when I followed the code of function 'rank' in R.

It seems that subassignment of a vector by a logical index vector that is longer than the original vector always results in expanding the original vector to the length of the index vector.

The resulting length may be different from the result of subassignment by the equivalent numeric vector. For subassignment of a vector by a numeric index vector, the original vector is expanded to the maximum index, if it is larger than the length of the original vector.

This is an example.

> x <- NA
> x[c(FALSE,TRUE,FALSE)] <- 1
> x
[1] NA  1 NA

Compare to this.

> x <- NA
> x[which(c(FALSE,TRUE,FALSE))] <- 1
> x
[1] NA  1

Does S exhibit the same behavior?

Currently, if there is NA and na.last = "keep", function 'rank' in R relies on this behavior to give correct result length.

In "R Language Definition", "3.4.1 Indexing by vectors" says: "Logical. The indexing i should generally have the same length as x. .... If it is longer, then x is conceptually extended with NAs. ...." The statement can be taught to support the observed behavior.

> sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows XP (build 2600) Service Pack 2

locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base


From ddalthorp at usgs.gov  Sat Sep  5 23:14:18 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Sat, 5 Sep 2015 14:14:18 -0700 (PDT)
Subject: [R] Help with vectors!
In-Reply-To: <1441295060538-4711801.post@n4.nabble.com>
References: <1441295060538-4711801.post@n4.nabble.com>
Message-ID: <1441487658926-4711895.post@n4.nabble.com>

# your data
VAS<-c("Green","Green","Black","Green","White","Yellow","Yellow","Black","Green","Black")

# declare the new vector
New_Vector<-numeric(length(VAS))

# brute force:
New_Vector[VAS=="White"]<-1
New_Vector[VAS=="Yellow"]<-2
New_Vector[VAS=="Green"]<-3
New_Vector[VAS=="Black"]<-4

# a little more subtle
cols<-c("White","Yellow","Green","Black")
for (i in 1:length(cols))  New_Vector[VAS==cols[i]]<-i

# and a general approach (that may give a different indexing, but can be
used for any array)
for (i in 1:length(unique(VAS))) New_Vector[VAS==unique(VAS)[i]]<-i
cbind(1:length(unique(VAS)),unique(VAS)) # a decoding key for the color
index




--
View this message in context: http://r.789695.n4.nabble.com/Help-with-vectors-tp4711801p4711895.html
Sent from the R help mailing list archive at Nabble.com.


From ddalthorp at usgs.gov  Sat Sep  5 23:18:57 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Sat, 5 Sep 2015 14:18:57 -0700 (PDT)
Subject: [R] Plots Help
In-Reply-To: <1441486071255-4711894.post@n4.nabble.com>
References: <1441486071255-4711894.post@n4.nabble.com>
Message-ID: <1441487937706-4711896.post@n4.nabble.com>

As written, the code does not run because you are trying to plot post vs. t.
Try instead:
plot(t, post(t)), or, more simply, plot(t, dbeta(t,1.05,30))

-Dan



--
View this message in context: http://r.789695.n4.nabble.com/Plots-Help-tp4711894p4711896.html
Sent from the R help mailing list archive at Nabble.com.


From ddalthorp at usgs.gov  Sat Sep  5 23:21:13 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Sat, 5 Sep 2015 14:21:13 -0700 (PDT)
Subject: [R] Plots Help
In-Reply-To: <1441486071255-4711894.post@n4.nabble.com>
References: <1441486071255-4711894.post@n4.nabble.com>
Message-ID: <1441488073653-4711897.post@n4.nabble.com>

length(post) is 1 (i.e., it is just a single function), but length(post(t))
== length(t)



--
View this message in context: http://r.789695.n4.nabble.com/Plots-Help-tp4711894p4711897.html
Sent from the R help mailing list archive at Nabble.com.


From jdnewmil at dcn.davis.CA.us  Sun Sep  6 00:49:52 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sat, 05 Sep 2015 15:49:52 -0700
Subject: [R] For logical i and length(i) > length(x),
	x[i] <- value makes length(x) == length(i)
In-Reply-To: <1441472525.27020.YahooMailBasic@web125101.mail.ne1.yahoo.com>
References: <1441472525.27020.YahooMailBasic@web125101.mail.ne1.yahoo.com>
Message-ID: <58D305CE-D9B9-4563-8620-B65E92860F90@dcn.davis.CA.us>

I think this behavior is consistent with typical indexing behaviour in R... I would ask you what result you thought you should get? I, for one, can think of all sorts of uses for numeric indexes that have different lengths than the vector, but am stumped to think of any use for what you are proposing.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 5, 2015 10:02:05 AM PDT, Suharto Anggono Suharto Anggono via R-help <r-help at r-project.org> wrote:
>I came across this behavior when I followed the code of function 'rank'
>in R.
>
>It seems that subassignment of a vector by a logical index vector that
>is longer than the original vector always results in expanding the
>original vector to the length of the index vector.
>
>The resulting length may be different from the result of subassignment
>by the equivalent numeric vector. For subassignment of a vector by a
>numeric index vector, the original vector is expanded to the maximum
>index, if it is larger than the length of the original vector.
>
>This is an example.
>
>> x <- NA
>> x[c(FALSE,TRUE,FALSE)] <- 1
>> x
>[1] NA  1 NA
>
>Compare to this.
>
>> x <- NA
>> x[which(c(FALSE,TRUE,FALSE))] <- 1
>> x
>[1] NA  1
>
>Does S exhibit the same behavior?
>
>Currently, if there is NA and na.last = "keep", function 'rank' in R
>relies on this behavior to give correct result length.
>
>In "R Language Definition", "3.4.1 Indexing by vectors" says: "Logical.
>The indexing i should generally have the same length as x. .... If it
>is longer, then x is conceptually extended with NAs. ...." The
>statement can be taught to support the observed behavior.
>
>> sessionInfo()
>R version 3.2.2 (2015-08-14)
>Platform: i386-w64-mingw32/i386 (32-bit)
>Running under: Windows XP (build 2600) Service Pack 2
>
>locale:
>[1] LC_COLLATE=English_United States.1252
>[2] LC_CTYPE=English_United States.1252
>[3] LC_MONETARY=English_United States.1252
>[4] LC_NUMERIC=C
>[5] LC_TIME=English_United States.1252
>
>attached base packages:
>[1] stats     graphics  grDevices utils     datasets  methods   base
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Sun Sep  6 02:47:38 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Sat, 5 Sep 2015 17:47:38 -0700 (PDT)
Subject: [R] [Bayesian Methods] Riemann Sums for Posterior expected loss
In-Reply-To: <1441497459284-4711900.post@n4.nabble.com>
References: <1441497459284-4711900.post@n4.nabble.com>
Message-ID: <1441500458796-4711901.post@n4.nabble.com>

Does this get you started?

f<-function(t,cx){
  (abs(t-cx)*(t >= cx)+10*abs(t-cx)*(t < cx))*dbeta(t,1.05,30)
}

integrate(f,lower=0,upper=1,cx=0.4)$val # e.g., for c = 0.4

The "integrate" function integrates over the first parameter. Other
parameters can be entered as needed.

[Note: I used cx as the parameter name rather than c because 'c' has a
special meaning in R.]

-Dan



--
View this message in context: http://r.789695.n4.nabble.com/Bayesian-Methods-Riemann-Sums-for-Posterior-expected-loss-tp4711900p4711901.html
Sent from the R help mailing list archive at Nabble.com.


From dan.abner99 at gmail.com  Sun Sep  6 04:09:28 2015
From: dan.abner99 at gmail.com (Dan Abner)
Date: Sat, 5 Sep 2015 22:09:28 -0400
Subject: [R] Exporting column names with spaces with write.csv() XXXX
Message-ID: <CAPRGo-m--PnaEjdRMgf0_icXWqVHPx=eGUOVN7A0YGny9My+Tw@mail.gmail.com>

Hi all,

I have a number of columns with spaces in the column names exactly as
I want them in R. When I go to export the data table to csv using
write.csv(), the fn adds periods in place of the spaces.

Is there a way to suppress the addition of the periods?

Thanks,

Dan


From jdnewmil at dcn.davis.CA.us  Sun Sep  6 04:32:44 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sat, 05 Sep 2015 19:32:44 -0700
Subject: [R] Exporting column names with spaces with write.csv() XXXX
In-Reply-To: <CAPRGo-m--PnaEjdRMgf0_icXWqVHPx=eGUOVN7A0YGny9My+Tw@mail.gmail.com>
References: <CAPRGo-m--PnaEjdRMgf0_icXWqVHPx=eGUOVN7A0YGny9My+Tw@mail.gmail.com>
Message-ID: <D87C922E-B0F0-4E93-BBD4-6FE30274DEDA@dcn.davis.CA.us>

Reproducible example, please.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 5, 2015 7:09:28 PM PDT, Dan Abner <dan.abner99 at gmail.com> wrote:
>Hi all,
>
>I have a number of columns with spaces in the column names exactly as
>I want them in R. When I go to export the data table to csv using
>write.csv(), the fn adds periods in place of the spaces.
>
>Is there a way to suppress the addition of the periods?
>
>Thanks,
>
>Dan
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Sun Sep  6 04:57:58 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 6 Sep 2015 12:57:58 +1000
Subject: [R] Exporting column names with spaces with write.csv() XXXX
In-Reply-To: <CAPRGo-m--PnaEjdRMgf0_icXWqVHPx=eGUOVN7A0YGny9My+Tw@mail.gmail.com>
References: <CAPRGo-m--PnaEjdRMgf0_icXWqVHPx=eGUOVN7A0YGny9My+Tw@mail.gmail.com>
Message-ID: <CA+8X3fX1cuh7GkOHPzkE1QqzsiU+wi8GfobEunQBstodOF-Gag@mail.gmail.com>

Hi Dan,
I don't get this behavior with R-3.2.2:

da.df<-data.frame(1:3,4:6)
names(da.df)<-c("no good","no good at all")
> da.df
  no good no good at all
1       1              4
2       2              5
3       3              6
write.csv(da.df,file="gloop.csv",row.names=FALSE)

gives me this:

"no good","no good at all"
1,4
2,5
3,6

Are you sure that R didn't add the periods when you created the data frame?
They would then be written into the CSV file.

Jim



On Sun, Sep 6, 2015 at 12:09 PM, Dan Abner <dan.abner99 at gmail.com> wrote:

> Hi all,
>
> I have a number of columns with spaces in the column names exactly as
> I want them in R. When I go to export the data table to csv using
> write.csv(), the fn adds periods in place of the spaces.
>
> Is there a way to suppress the addition of the periods?
>
> Thanks,
>
> Dan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From djmuser at gmail.com  Sun Sep  6 05:36:43 2015
From: djmuser at gmail.com (Dennis Murphy)
Date: Sat, 5 Sep 2015 20:36:43 -0700
Subject: [R] Exporting column names with spaces with write.csv() XXXX
In-Reply-To: <CA+8X3fX1cuh7GkOHPzkE1QqzsiU+wi8GfobEunQBstodOF-Gag@mail.gmail.com>
References: <CAPRGo-m--PnaEjdRMgf0_icXWqVHPx=eGUOVN7A0YGny9My+Tw@mail.gmail.com>
	<CA+8X3fX1cuh7GkOHPzkE1QqzsiU+wi8GfobEunQBstodOF-Gag@mail.gmail.com>
Message-ID: <CADv2QyFeU16jJp2M2QXEqpWQxHPhPM76GyeAhFMax2nKk9YH4A@mail.gmail.com>

...or, when trying to read it back into R,

read.csv(header = TRUE, text = '
"no good","no good at all"
1,4
2,5
3,6')

  no.good no.good.at.all
1       1              4
2       2              5
3       3              6


read.csv(header = TRUE, check.names = FALSE, text = '
"no good","no good at all"
1,4
2,5
3,6')

  no good no good at all
1       1              4
2       2              5
3       3              6


Thanks for the MWE, Jim.

Dennis

On Sat, Sep 5, 2015 at 7:57 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi Dan,
> I don't get this behavior with R-3.2.2:
>
> da.df<-data.frame(1:3,4:6)
> names(da.df)<-c("no good","no good at all")
>> da.df
>   no good no good at all
> 1       1              4
> 2       2              5
> 3       3              6
> write.csv(da.df,file="gloop.csv",row.names=FALSE)
>
> gives me this:
>
> "no good","no good at all"
> 1,4
> 2,5
> 3,6
>
> Are you sure that R didn't add the periods when you created the data frame?
> They would then be written into the CSV file.
>
> Jim
>
>
>
> On Sun, Sep 6, 2015 at 12:09 PM, Dan Abner <dan.abner99 at gmail.com> wrote:
>
>> Hi all,
>>
>> I have a number of columns with spaces in the column names exactly as
>> I want them in R. When I go to export the data table to csv using
>> write.csv(), the fn adds periods in place of the spaces.
>>
>> Is there a way to suppress the addition of the periods?
>>
>> Thanks,
>>
>> Dan
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sergio.fonda99 at gmail.com  Sun Sep  6 17:08:29 2015
From: sergio.fonda99 at gmail.com (Sergio Fonda)
Date: Sun, 6 Sep 2015 17:08:29 +0200
Subject: [R] factor interaction boxplot ordering by median
In-Reply-To: <122369DF-537A-4D98-A626-1289C0CB4447@me.com>
References: <CAJRuHopV_h=A8AopTcm1YJfLR6Cohon+OsJoKAscPs_QC7JMTg@mail.gmail.com>
	<122369DF-537A-4D98-A626-1289C0CB4447@me.com>
Message-ID: <CAJRuHopEE3ZbrfjD0VQMcQDobYn9LO8ki-i-cp-DzDjgnjkiQA@mail.gmail.com>

Thanks to Marc Schwartz I was able to order a "two factors"
interaction boxplot with median associated to one factor alone.
I tried further to generate facets plot (3x2 boxplots in ggplot2) for the
dataframe reported at bottom and I'm not able to reach a correct plot.
The dataframe is a simulation of genes expressions in five conditions of
patiens.
I would like that each of the 5 boxplots were ordered with median of values
associated to the combination "gene*pat.cond".
Any help is much appreciated,
Sergio
 ____________________________
Example Dataframe:
    pat.cond     gene        value
1          N  ADAMTS1 -5.557194292
2          N  ADAMTS1 -2.576700157
3          E  ADAMTS1 -6.629356620
4          E  ADAMTS1 -6.629356620
5          E  ADAMTS1 -6.629356620
6          E  ADAMTS1 -6.629356620
7          M  ADAMTS1 -6.629356620
8          M  ADAMTS1 -6.629356620
9          M  ADAMTS1 -6.629356620
10         M  ADAMTS1 -2.742163709
11         N ADAMTS12 -1.084104611
12         E ADAMTS12  1.710871953
13         E ADAMTS12  6.629356620
14         M ADAMTS12  6.629356620
15         M ADAMTS12  1.711422682
16         E      APC -3.665169008
17         E      APC -1.192183578
18         M      APC -6.629356620
19         M      APC  3.696542194
20         M      APC -3.533991641
21         R      APC -4.267887134
22         N    BORIS  6.629356620
23         E    BORIS  6.629356620
24         M    BORIS  6.629356620
25         M    BORIS  6.629356620
26         M    BORIS  6.629356620
27         S    BORIS  6.629356620
28         E    BRCA1 -4.260347862
29         E    BRCA1 -6.629356620
30         E    BRCA1 -6.629356620
31         M    BRCA1 -6.443174967
32         M    BRCA1 -6.629356620
33         M    BRCA1 -6.629356620
34         M    BRCA1 -2.525972281
35         N      P16 -1.698706675
36         M      P16 -6.629356620
37         M      P16 -6.629356620
38         M      P16 -6.629356620
39         S      P16 -6.629356620
40         S      P16 -6.629356620
41         S      P16  0.518269571
42         N     DAX1  0.708010228
43         E     DAX1 -0.337455350
44         E     DAX1  2.803374605
45         E     DAX1 -1.407226228
46         M     DAX1  6.629356620
47         S     DAX1  2.022408499
48         S     DAX1  6.629356620
49         S     DAX1  6.629356620
50         S     DAX1  6.629356620
51         N     DKK1 -0.876905559
52         N     DKK1  0.008176565
53         E     DKK1 -3.057081024
54         M     DKK1  0.804046915
55         N     DKK2 -4.677880676
56         N     DKK2 -3.264182143
57         M     DKK2  0.219772061
58         R     DKK2 -3.205760419
59         R     DKK2 -3.567799537
60         R     DKK2 -1.687113091
61         S     DKK2 -6.629356620
62         S     DKK2 -6.629356620
63         N     ESR1  1.479810020
64         M     ESR1  1.861011014
65         N     FBP1 -0.110196473
66         N     FBP1  0.721286184
67         E     FBP1 -5.107943868
68         E     FBP1 -4.593812366
69         M     FBP1 -1.622176688
70         S     FBP1 -3.097791525
71         E    FOXL2 -4.564123239
72         E    FOXL2  3.111180437
73         M    FOXL2  0.323703764
74         M    FOXL2 -3.851350485
75         R    FOXL2 -3.324087523
76         S    FOXL2  0.739408989
77         M    GATA5 -1.171932246
78         M    GATA5 -3.466054731
79         M    GATA5 -1.283038699
80         S    GATA5 -2.778390690
81         S    GATA5 -2.014479273
82         S    GATA5 -3.015234172
83         E     GPX3 -6.629356620
84         E     GPX3 -1.942190735
85         M     GPX3  6.629356620
86         M     GPX3 -6.629356620
87         M     GPX3 -2.615982450
88         M     GPX3 -6.629356620
89         M     GPX3  0.128743354
90         S     GPX3 -6.629356620
91         E      MAL -6.629356620
92         E      MAL -4.825545452
93         E      MAL -6.629356620
94         E      MAL -6.629356620
95         E      MAL -6.629356620
96         M      MAL -0.419005364
97         M      MAL -0.923667455
98         M      MAL  6.629356620
99         M      MAL  3.371740196
100        S      MAL -6.629356620
101        N     MGMT -6.629356620
102        E     MGMT  1.115112556
103        M     MGMT  4.030893797
104        S     MGMT -6.629356620
105        N     MLH1 -0.519875304
106        N     MLH1 -1.352872084
107        E     MLH1 -0.777864442
108        E     MLH1  1.105073029
109        E     MLH1  5.758699199
110        E     MLH1 -1.498072236
111        S     MLH1 -1.630362301
112        M    MYOD1 -6.629356620
113        M    MYOD1  6.629356620
114        M    MYOD1 -6.629356620
115        M    MYOD1 -6.629356620
116        R    MYOD1 -6.629356620
117        R    MYOD1 -6.629356620
118        S    MYOD1 -6.629356620
119        S    MYOD1 -6.629356620
120        S    MYOD1 -4.645053781
121        N    NELL1 -6.629356620
122        N    NELL1 -5.536591557
123        N    NELL1 -5.903856552
124        E    NELL1 -6.629356620
125        E    NELL1 -6.629356620
126        M    NELL1 -6.629356620
127        S    NELL1 -6.629356620
128        N     OSMR -6.629356620
129        M     OSMR  4.407821839
130        M     OSMR  0.364604851
131        M     OSMR -6.629356620
132        S     OSMR -6.629356620
133        N     PAX6  6.629356620
134        E     PAX6 -3.401030959
135        M     PAX6 -0.946878855
136        S     PAX6  0.964721065
137        E    PTGS2 -3.749795240
138        M    PTGS2 -6.629356620
139        S    PTGS2 -2.076356656
140        S    PTGS2 -3.961164916
141        N     RARB -4.611429173
142        E     RARB -1.706166432
143        S     RARB -3.697493231
144        N  RASSF1A -1.505128642
145        E  RASSF1A  0.808179628
146        E  RASSF1A -6.629356620
147        E  RASSF1A -0.694997778
148        M  RASSF1A -0.332307577
149        S  RASSF1A -6.629356620
150        S  RASSF1A  6.629356620
151        N     RPRM -4.023523466
152        E     RPRM  0.443243995
153        S     RPRM -6.629356620
154        S     RPRM -4.019270038
155        E    RSPOI -1.735307607
156        E    RSPOI -1.296083205
157        M    RSPOI -3.003752756
158        S    RSPOI -3.156564936
159        S    RSPOI -2.928871731
160        N  SEPTIN9 -6.629356620
161        E  SEPTIN9 -2.585469731
162        E  SEPTIN9 -2.525798264
163        M  SEPTIN9  6.629356620
164        R  SEPTIN9 -3.071222253
165        S  SEPTIN9 -6.629356620
166        M    SFRP1 -1.426465815
167        R    SFRP1 -6.629356620
168        S    SFRP1 -2.348648751
169        S    SFRP1 -6.629356620
170        S    SFRP1 -2.304295273
171        N    SFRP4 -2.315044901
172        E    SFRP4 -2.940158139
173        M    SFRP4  6.629356620
174        S    SFRP4 -6.629356620
175        S    SFRP4  6.629356620
176        N    SFRP5 -6.629356620
177        E    SFRP5 -5.833523393
178        E    SFRP5  2.765666156
179        E    SFRP5  2.707734922
180        E    SFRP5  6.629356620
181        M    SMAD4 -6.629356620
182        M    SMAD4 -6.629356620
183        M    SMAD4 -6.629356620
184        R    SMAD4 -6.629356620
185        S    SMAD4 -6.629356620
186        S    SMAD4 -6.629356620
187        S    SMAD4 -6.629356620
188        N    SOCS3 -0.604614698
189        N    SOCS3  0.097268646
190        E    SOCS3 -1.505450218
191        E    SOCS3  6.629356620
192        N    SPARC -3.865639988
193        N    SPARC -1.868782928
194        E    SPARC  2.821046068
195        E    SPARC  6.629356620
196        S    SPARC -6.629356620
197        S    SPARC  0.202017887
198        S    SPARC  6.629356620
199        S    SPARC  6.629356620
200        N     TAC1 -1.528221578
201        M     TAC1  6.629356620
202        M     TAC1  6.629356620
203        R     TAC1 -2.294267250
204        S     TAC1  2.624594634
205        N     TERT -2.985641232
206        E     TERT  2.809747729
207        E     TERT  6.629356620
208        E     TERT -3.164404633
209        M     TERT  6.629356620
210        M     TERT -2.766881108
211        S     TERT  0.875337372
212        S     TERT  3.175095461
213        N    TIMP3 -2.046729298
214        E    TIMP3 -3.461408230
215        E    TIMP3 -2.996720557
216        M    TIMP3 -6.629356620
217        M    TIMP3 -1.527333149
218        R    TIMP3 -3.933657283
219        S    TIMP3 -1.020144976
220        S    TIMP3 -2.357112874
221        S    TIMP3  0.806736362
222        M   TMEFF2 -1.160275850
223        M   TMEFF2 -2.712025806
224        R   TMEFF2 -3.961478237
225        R   TMEFF2 -6.510953714
226        S   TMEFF2 -0.619555286
227        S   TMEFF2  2.344341057
228        E     WIF1 -6.629356620
229        E     WIF1 -3.806114522
230        E     WIF1 -2.318313158
231        M     WIF1  1.102899897
232        M     WIF1  6.629356620
233        M     WIF1 -6.629356620
234        N     WNT4 -6.629356620
235        E     WNT4 -3.474027185
236        E     WNT4 -6.629356620
237        S     WNT4 -6.629356620
238        S     WNT4 -6.629356620
239        E      WRN -6.629356620
240        M      WRN -4.148942985
241        S      WRN -1.855994142
242        R      WT1 -0.824982393
243        S      WT1  1.236129501
244        S      WT1  1.088540877


2015-09-05 15:08 GMT+02:00 Marc Schwartz <marc_schwartz at me.com>:

>
> > On Sep 5, 2015, at 7:29 AM, Sergio Fonda <sergio.fonda99 at gmail.com>
> wrote:
> >
> > I would to visualize in boxplot a data frame with two factors ordering
> one
> > factor with the median.
> > As example,suppose to have the InsectSprays dataframe, where an
> "operator"
> > factor with two levels, op1 and op2, has been added as shown at bottom
> here.
> > How may be generated a boxplot showing boxes for the interaction
> spray*op,
> > ordered according to the operators' count median for every spray ?
> > Thanks in advance for any help!
> > Sergio
> > ________________________________
> > Modified InsectSprays dataframe:
>
> <snip>
>
> Hi,
>
> There is actually an example of reordering factor levels by a calculated
> numeric value using the InsectSprays data frame in ?boxplot using ?reorder.
> An interaction can be created by using ?interaction.
>
> Given your data above, in a data frame ?DF?:
>
> DF <- structure(list(count = c(10L, 7L, 20L, 14L, 14L, 12L, 10L, 23L,
> 17L, 20L, 14L, 13L, 11L, 17L, 21L, 11L, 16L, 14L, 17L, 17L, 19L,
> 21L, 7L, 13L, 0L, 1L, 7L, 2L, 3L, 1L, 2L, 1L, 3L, 0L, 1L, 4L,
> 3L, 5L, 12L, 6L, 4L, 3L, 5L, 5L, 5L, 5L, 2L, 4L, 3L, 5L, 3L,
> 5L, 3L, 6L, 1L, 1L, 3L, 2L, 6L, 4L, 11L, 9L, 15L, 22L, 15L, 16L,
> 13L, 10L, 26L, 26L, 24L, 13L), spray = structure(c(1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L,
> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
> 6L, 6L, 6L, 6L, 6L), .Label = c("A", "B", "C", "D", "E", "F"), class =
> "factor"),
>     op = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,
>     2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,
>     1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L,
>     1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,
>     2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,
>     2L, 2L), .Label = c("op1", "op2"), class = "factor")), .Names =
> c("count",
> "spray", "op"), class = "data.frame", row.names = c("1", "2",
> "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14",
> "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25",
> "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36",
> "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47",
> "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58",
> "59", "60", "61", "62", "63", "64", "65", "66", "67", "68", "69",
> "70", "71", "72"))
>
>
> # Modify the ?boxplot example
> bymedian <- with(DF, reorder(interaction(spray, op), count, median))
>
> > bymedian
>  [1] A.op1 A.op1 A.op1 A.op1 A.op1 A.op1 A.op2 A.op2 A.op2 A.op2 A.op2
> [12] A.op2 B.op1 B.op1 B.op1 B.op1 B.op1 B.op1 B.op2 B.op2 B.op2 B.op2
> [23] B.op2 B.op2 C.op1 C.op1 C.op1 C.op1 C.op1 C.op1 C.op2 C.op2 C.op2
> [34] C.op2 C.op2 C.op2 D.op1 D.op1 D.op1 D.op1 D.op1 D.op1 D.op2 D.op2
> [45] D.op2 D.op2 D.op2 D.op2 E.op1 E.op1 E.op1 E.op1 E.op1 E.op1 E.op2
> [56] E.op2 E.op2 E.op2 E.op2 E.op2 F.op1 F.op1 F.op1 F.op1 F.op1 F.op1
> [67] F.op2 F.op2 F.op2 F.op2 F.op2 F.op2
> attr(,"scores")
> A.op1 B.op1 C.op1 D.op1 E.op1 F.op1 A.op2 B.op2 C.op2 D.op2 E.op2 F.op2
>  13.0  15.0   1.5   4.5   4.0  15.0  15.5  17.0   1.5   5.0   2.5  18.5
> 12 Levels: C.op1 C.op2 E.op2 E.op1 D.op1 D.op2 A.op1 B.op1 ... F.op2
>
>
> boxplot(count ~ bymedian, data = DF,
>         xlab = "Interaction of spray and op", ylab = "Insect count",
>         main = "Modified InsectSprays Data", varwidth = TRUE,
>         col = "lightgray")
>
>
> Regards,
>
> Marc Schwartz
>
>

	[[alternative HTML version deleted]]


From priyanka.garg112 at gmail.com  Sat Sep  5 21:12:31 2015
From: priyanka.garg112 at gmail.com (Priyanka Garg)
Date: Sun, 6 Sep 2015 00:42:31 +0530
Subject: [R] Predict method of J48 in RWeka (Urgent)
Message-ID: <CAA5eUmFye1mQgg=x1XXt3fssPQJUcWKyJLCeS1xPQ8_L+GAYAA@mail.gmail.com>

hi,

I am new to R using package RWeka.Currently i am using J48 classifier for
classification and prediction. My doubt is after building tree using J48,
when i use the same for prediction on my test set it gives the predicted
class names. How could i calculate accuracy, precision, recall and also
obtain laplace smoothing values ??


Regards
Priyanka Garg
School of Computers & Information Sciences
University Of Hyderabad

	[[alternative HTML version deleted]]


From evan.cooch at gmail.com  Sat Sep  5 22:59:54 2015
From: evan.cooch at gmail.com (Evan Cooch)
Date: Sat, 5 Sep 2015 16:59:54 -0400
Subject: [R] extracting every nth character from a string...
Message-ID: <55EB57CA.2090909@gmail.com>

Suppose I had the following string, which has length of integer multiple 
of some value n. So, say n=2, and the example string has a length of  
(2x4) = 8 characters.

str <- "ABCDEFGH"

What I'm trying to figure out is a simple, base-R coded way (which I 
heuristically call StrSubset in the following) to extract every nth 
character from the string, to generate a new string.

So

str <- "ABCDEFGH"

new_str <- StrSubset(str);

print(new_str)

which would yield

"ACEG"


Best I could come up with is something like the following, where I 
extract every odd character from the string:

StrSubset <- function(string)
       { 
paste(unlist(strsplit(string,""))[seq(1,nchar(string),2)],collapse="") }


Anything more elegant come to mind? Trying to avoid regex if possible 
(harder to explain to end-users), but if that meets the 'more elegant' 
sniff test, happy to consider...

Thanks in advance...


From allennugent at hotmail.com  Sun Sep  6 13:54:45 2015
From: allennugent at hotmail.com (AltShift)
Date: Sun, 6 Sep 2015 04:54:45 -0700 (PDT)
Subject: [R] Is there a time series resampling function ?
Message-ID: <1441540485695-4711907.post@n4.nabble.com>

I need a function for regularising the time base of electronically acquired
signals (i.e. vectors of samples with a nominally constant time base). 

For example, the accelerometer in my smartphone can deliver data at about 50
Hz, but the sampling rate varies by about 5% throughout a recording. I have
developed signal processing functions that assume a constant sample rate, so
I need to modify my input data vectors so that this assumption becomes true.
Furthermore, if I want to combine signals from different sources that have
different sampling rates, I will be obliged to harmonise the time bases,
somehow.

I have written routines in other languages to interpolate (generate a larger
number of regularly spaced samples from the input series) or, what I call,
"intrapolate" (generate a smaller number of regularly spaced samples from
the input series), so it wouldn't be too hard form me to write a function
that covers both, but I prefer not to reinvent wheels (unless my wheel is
markedly better!)

Can anyone tell me if there is already a package that might cover what I
need?




--
View this message in context: http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907.html
Sent from the R help mailing list archive at Nabble.com.


From rosita21 at gmail.com  Sun Sep  6 16:06:56 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Sun, 6 Sep 2015 15:06:56 +0100
Subject: [R] split.screen to draw graphs - ggplot2 and lattice (can't slip
	in 4 cells)
Message-ID: <C14B6C56-82E6-4300-B056-647B0AD74A35@gmail.com>

Dear all,

 

I need your urgent help J

 

I?m na?ve, and I?m pretty sure my doubt is very simple to solve, but I?m not getting it.

 

I used the following code to produce my research graphs, nonetheless, is this problem, I do not have 6 graphs (1 ? 6),

 

#   3   4   5

#                2

#   6   7   8

 

 

but only 4,instead.  So, I need to reformulate the code, just to

 

#   3   4  

#                2

#   6   7  

 

Can you please help me reformulating?

I suppose I have to change something in the split.screen code, because nowadays, my ?third? column is blank J

 

 

I attach the code:

 

library(ggplot2)

library(reshape)

library(lattice)

 

mean.alpha1<-read.csv("graphs_mean_alpha1.csv")

mean.alpha2<-read.csv("graphs_mean_alpha2.csv")

quartz(width=10,height=5)

split.screen(figs=matrix(c(0,0.4,0.5,1,

                           0.4,0.7,0.5,1,

                           0.7,1,0.5,1,

                           0,0.4,0,0.5,

                           0.4,0.7,0,0.5,

                           0.7,1,0,0.5),nrow=6,byrow=TRUE),

             screen=1)

 

 

 

screen(3)

par(mar=c(0,3.5,3,0))

# now the second set

n250<-mean.alpha1$nsample==250

matplot(x=mean.alpha1$lambda[n250],y=mean.alpha1[n250,3:5],

        type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-1.2, -0.25),main="nsample=250",ylab="", cex.main=1)

abline(h = -1, col = "gray60")

mtext(expression(paste("mean av. for  ",alpha[1])),side=2,line=2, cex.main=1)

 

 

 

screen(4)

par(mar=c(0,0,3,0))

# now the second set

n250<-mean.alpha1$nsample==1000

matplot(x=mean.alpha1$lambda[n1000],y=mean.alpha2[n1000,3:5],

        type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-1.2, -0.25),main="nsample=1000", cex.main=1)

abline(h = -1, col = "gray60")

 

 

 

screen(6)

par(mar=c(3,3.5,0,0))

# now the second set

n250<-mean.alpha2$nsample==250

matplot(x=mean.alpha2$lambda[n250],y=mean.alpha2[n250,3:5],

        type="l",pch=1:3,col=c(4,2,3),ylim=c(-.6, -.35),ylab="")

abline(h = -.5, col = "gray60")

mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)

mtext(expression(paste("mean av. for  ",alpha[2])),side=2,line=2, cex.main=1.5)

 

 

 

screen(7)

par(mar=c(3,0,0,0))

# now the second set

n1000<-mean.alpha2$nsample==1000

matplot(x=mean.alpha2$lambda[n1000],y=mean.alpha2[n1000,3:5],

type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.6, -.35))

abline(h = -.5, col = "gray60")

mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)

 

 

screen(2)

par(mar=c(0,0,0,0))

# plot an empty plot to get the coordinates

plot(0:1,0:1,type="n",axes=FALSE)

legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "true coefficient"),bty = "n", lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)

 

 

close.screen(all=TRUE)

 

 

BEST,

RO





Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates


From ggrothendieck at gmail.com  Sun Sep  6 17:26:55 2015
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 Sep 2015 11:26:55 -0400
Subject: [R] extracting every nth character from a string...
In-Reply-To: <55EB57CA.2090909@gmail.com>
References: <55EB57CA.2090909@gmail.com>
Message-ID: <CAP01uRk10oVH1168G2fGcGGVr27Ak44CtEWk5UxAnQwSLJ781Q@mail.gmail.com>

This uses a regular expression but is shorter:

> gsub("(.).", "\\1", "ABCDEFG")
[1] "ACEG"

It replaces each successive pair of characters with the first of that
pair.  If there is an odd number of characters then the last character is
not matched and therefore kept -- thus it works properly for both even and
odd.


On Sat, Sep 5, 2015 at 4:59 PM, Evan Cooch <evan.cooch at gmail.com> wrote:

> Suppose I had the following string, which has length of integer multiple
> of some value n. So, say n=2, and the example string has a length of  (2x4)
> = 8 characters.
>
> str <- "ABCDEFGH"
>
> What I'm trying to figure out is a simple, base-R coded way (which I
> heuristically call StrSubset in the following) to extract every nth
> character from the string, to generate a new string.
>
> So
>
> str <- "ABCDEFGH"
>
> new_str <- StrSubset(str);
>
> print(new_str)
>
> which would yield
>
> "ACEG"
>
>
> Best I could come up with is something like the following, where I extract
> every odd character from the string:
>
> StrSubset <- function(string)
>       {
> paste(unlist(strsplit(string,""))[seq(1,nchar(string),2)],collapse="") }
>
>
> Anything more elegant come to mind? Trying to avoid regex if possible
> (harder to explain to end-users), but if that meets the 'more elegant'
> sniff test, happy to consider...
>
> Thanks in advance...
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From sergio.fonda99 at gmail.com  Sun Sep  6 17:37:56 2015
From: sergio.fonda99 at gmail.com (Sergio Fonda)
Date: Sun, 6 Sep 2015 17:37:56 +0200
Subject: [R] boxplots facets in ggplot,
	generated with two factors interaction mediane-ordered
Message-ID: <CAJRuHopv0BPXuNqaix_LXo2A3aG5ZBG6bDAa7cRLeco=kimR9g@mail.gmail.com>

Thanks to Marc Schwartz I was able to order a "two factors"
interaction boxplot with median associated to one factor alone.
I tried further to generate facets plot (3x2 boxplots in ggplot2) for the
dataframe reported at bottom and I'm not able to reach a correct plot.
The dataframe is a simulation of genes expressions in five conditions of
patients.
I would like that each of the 5 boxplots were ordered with median of values
associated to the combination "gene*pat.cond".
Any help is much appreciated,
Sergio
 ____________________________
Example Dataframe:
    pat.cond     gene        value
1          N  ADAMTS1 -5.557194292
2          N  ADAMTS1 -2.576700157
3          E  ADAMTS1 -6.629356620
4          E  ADAMTS1 -6.629356620
5          E  ADAMTS1 -6.629356620
6          E  ADAMTS1 -6.629356620
7          M  ADAMTS1 -6.629356620
8          M  ADAMTS1 -6.629356620
9          M  ADAMTS1 -6.629356620
10         M  ADAMTS1 -2.742163709
11         N ADAMTS12 -1.084104611
12         E ADAMTS12  1.710871953
13         E ADAMTS12  6.629356620
14         M ADAMTS12  6.629356620
15         M ADAMTS12  1.711422682
16         E      APC -3.665169008
17         E      APC -1.192183578
18         M      APC -6.629356620
19         M      APC  3.696542194
20         M      APC -3.533991641
21         R      APC -4.267887134
22         N    BORIS  6.629356620
23         E    BORIS  6.629356620
24         M    BORIS  6.629356620
25         M    BORIS  6.629356620
26         M    BORIS  6.629356620
27         S    BORIS  6.629356620
28         E    BRCA1 -4.260347862
29         E    BRCA1 -6.629356620
30         E    BRCA1 -6.629356620
31         M    BRCA1 -6.443174967
32         M    BRCA1 -6.629356620
33         M    BRCA1 -6.629356620
34         M    BRCA1 -2.525972281
35         N      P16 -1.698706675
36         M      P16 -6.629356620
37         M      P16 -6.629356620
38         M      P16 -6.629356620
39         S      P16 -6.629356620
40         S      P16 -6.629356620
41         S      P16  0.518269571
42         N     DAX1  0.708010228
43         E     DAX1 -0.337455350
44         E     DAX1  2.803374605
45         E     DAX1 -1.407226228
46         M     DAX1  6.629356620
47         S     DAX1  2.022408499
48         S     DAX1  6.629356620
49         S     DAX1  6.629356620
50         S     DAX1  6.629356620
51         N     DKK1 -0.876905559
52         N     DKK1  0.008176565
53         E     DKK1 -3.057081024
54         M     DKK1  0.804046915
55         N     DKK2 -4.677880676
56         N     DKK2 -3.264182143
57         M     DKK2  0.219772061
58         R     DKK2 -3.205760419
59         R     DKK2 -3.567799537
60         R     DKK2 -1.687113091
61         S     DKK2 -6.629356620
62         S     DKK2 -6.629356620
63         N     ESR1  1.479810020
64         M     ESR1  1.861011014
65         N     FBP1 -0.110196473
66         N     FBP1  0.721286184
67         E     FBP1 -5.107943868
68         E     FBP1 -4.593812366
69         M     FBP1 -1.622176688
70         S     FBP1 -3.097791525
71         E    FOXL2 -4.564123239
72         E    FOXL2  3.111180437
73         M    FOXL2  0.323703764
74         M    FOXL2 -3.851350485
75         R    FOXL2 -3.324087523
76         S    FOXL2  0.739408989
77         M    GATA5 -1.171932246
78         M    GATA5 -3.466054731
79         M    GATA5 -1.283038699
80         S    GATA5 -2.778390690
81         S    GATA5 -2.014479273
82         S    GATA5 -3.015234172
83         E     GPX3 -6.629356620
84         E     GPX3 -1.942190735
85         M     GPX3  6.629356620
86         M     GPX3 -6.629356620
87         M     GPX3 -2.615982450
88         M     GPX3 -6.629356620
89         M     GPX3  0.128743354
90         S     GPX3 -6.629356620
91         E      MAL -6.629356620
92         E      MAL -4.825545452
93         E      MAL -6.629356620
94         E      MAL -6.629356620
95         E      MAL -6.629356620
96         M      MAL -0.419005364
97         M      MAL -0.923667455
98         M      MAL  6.629356620
99         M      MAL  3.371740196
100        S      MAL -6.629356620
101        N     MGMT -6.629356620
102        E     MGMT  1.115112556
103        M     MGMT  4.030893797
104        S     MGMT -6.629356620
105        N     MLH1 -0.519875304
106        N     MLH1 -1.352872084
107        E     MLH1 -0.777864442
108        E     MLH1  1.105073029
109        E     MLH1  5.758699199
110        E     MLH1 -1.498072236
111        S     MLH1 -1.630362301
112        M    MYOD1 -6.629356620
113        M    MYOD1  6.629356620
114        M    MYOD1 -6.629356620
115        M    MYOD1 -6.629356620
116        R    MYOD1 -6.629356620
117        R    MYOD1 -6.629356620
118        S    MYOD1 -6.629356620
119        S    MYOD1 -6.629356620
120        S    MYOD1 -4.645053781
121        N    NELL1 -6.629356620
122        N    NELL1 -5.536591557
123        N    NELL1 -5.903856552
124        E    NELL1 -6.629356620
125        E    NELL1 -6.629356620
126        M    NELL1 -6.629356620
127        S    NELL1 -6.629356620
128        N     OSMR -6.629356620
129        M     OSMR  4.407821839
130        M     OSMR  0.364604851
131        M     OSMR -6.629356620
132        S     OSMR -6.629356620
133        N     PAX6  6.629356620
134        E     PAX6 -3.401030959
135        M     PAX6 -0.946878855
136        S     PAX6  0.964721065
137        E    PTGS2 -3.749795240
138        M    PTGS2 -6.629356620
139        S    PTGS2 -2.076356656
140        S    PTGS2 -3.961164916
141        N     RARB -4.611429173
142        E     RARB -1.706166432
143        S     RARB -3.697493231
144        N  RASSF1A -1.505128642
145        E  RASSF1A  0.808179628
146        E  RASSF1A -6.629356620
147        E  RASSF1A -0.694997778
148        M  RASSF1A -0.332307577
149        S  RASSF1A -6.629356620
150        S  RASSF1A  6.629356620
151        N     RPRM -4.023523466
152        E     RPRM  0.443243995
153        S     RPRM -6.629356620
154        S     RPRM -4.019270038
155        E    RSPOI -1.735307607
156        E    RSPOI -1.296083205
157        M    RSPOI -3.003752756
158        S    RSPOI -3.156564936
159        S    RSPOI -2.928871731
160        N  SEPTIN9 -6.629356620
161        E  SEPTIN9 -2.585469731
162        E  SEPTIN9 -2.525798264
163        M  SEPTIN9  6.629356620
164        R  SEPTIN9 -3.071222253
165        S  SEPTIN9 -6.629356620
166        M    SFRP1 -1.426465815
167        R    SFRP1 -6.629356620
168        S    SFRP1 -2.348648751
169        S    SFRP1 -6.629356620
170        S    SFRP1 -2.304295273
171        N    SFRP4 -2.315044901
172        E    SFRP4 -2.940158139
173        M    SFRP4  6.629356620
174        S    SFRP4 -6.629356620
175        S    SFRP4  6.629356620
176        N    SFRP5 -6.629356620
177        E    SFRP5 -5.833523393
178        E    SFRP5  2.765666156
179        E    SFRP5  2.707734922
180        E    SFRP5  6.629356620
181        M    SMAD4 -6.629356620
182        M    SMAD4 -6.629356620
183        M    SMAD4 -6.629356620
184        R    SMAD4 -6.629356620
185        S    SMAD4 -6.629356620
186        S    SMAD4 -6.629356620
187        S    SMAD4 -6.629356620
188        N    SOCS3 -0.604614698
189        N    SOCS3  0.097268646
190        E    SOCS3 -1.505450218
191        E    SOCS3  6.629356620
192        N    SPARC -3.865639988
193        N    SPARC -1.868782928
194        E    SPARC  2.821046068
195        E    SPARC  6.629356620
196        S    SPARC -6.629356620
197        S    SPARC  0.202017887
198        S    SPARC  6.629356620
199        S    SPARC  6.629356620
200        N     TAC1 -1.528221578
201        M     TAC1  6.629356620
202        M     TAC1  6.629356620
203        R     TAC1 -2.294267250
204        S     TAC1  2.624594634
205        N     TERT -2.985641232
206        E     TERT  2.809747729
207        E     TERT  6.629356620
208        E     TERT -3.164404633
209        M     TERT  6.629356620
210        M     TERT -2.766881108
211        S     TERT  0.875337372
212        S     TERT  3.175095461
213        N    TIMP3 -2.046729298
214        E    TIMP3 -3.461408230
215        E    TIMP3 -2.996720557
216        M    TIMP3 -6.629356620
217        M    TIMP3 -1.527333149
218        R    TIMP3 -3.933657283
219        S    TIMP3 -1.020144976
220        S    TIMP3 -2.357112874
221        S    TIMP3  0.806736362
222        M   TMEFF2 -1.160275850
223        M   TMEFF2 -2.712025806
224        R   TMEFF2 -3.961478237
225        R   TMEFF2 -6.510953714
226        S   TMEFF2 -0.619555286
227        S   TMEFF2  2.344341057
228        E     WIF1 -6.629356620
229        E     WIF1 -3.806114522
230        E     WIF1 -2.318313158
231        M     WIF1  1.102899897
232        M     WIF1  6.629356620
233        M     WIF1 -6.629356620
234        N     WNT4 -6.629356620
235        E     WNT4 -3.474027185
236        E     WNT4 -6.629356620
237        S     WNT4 -6.629356620
238        S     WNT4 -6.629356620
239        E      WRN -6.629356620
240        M      WRN -4.148942985
241        S      WRN -1.855994142
242        R      WT1 -0.824982393
243        S      WT1  1.236129501
244        S      WT1  1.088540877

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Sun Sep  6 18:49:52 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sun, 06 Sep 2015 09:49:52 -0700
Subject: [R] Is there a time series resampling function ?
In-Reply-To: <1441540485695-4711907.post@n4.nabble.com>
References: <1441540485695-4711907.post@n4.nabble.com>
Message-ID: <2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>

There are lots of them. You might be having trouble searching because you don't know how to spell "interpolate". 

?approx
?spline

Also look at the Time Series task view on CRAN.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 6, 2015 4:54:45 AM PDT, AltShift <allennugent at hotmail.com> wrote:
>I need a function for regularising the time base of electronically
>acquired
>signals (i.e. vectors of samples with a nominally constant time base). 
>
>For example, the accelerometer in my smartphone can deliver data at
>about 50
>Hz, but the sampling rate varies by about 5% throughout a recording. I
>have
>developed signal processing functions that assume a constant sample
>rate, so
>I need to modify my input data vectors so that this assumption becomes
>true.
>Furthermore, if I want to combine signals from different sources that
>have
>different sampling rates, I will be obliged to harmonise the time
>bases,
>somehow.
>
>I have written routines in other languages to interpolate (generate a
>larger
>number of regularly spaced samples from the input series) or, what I
>call,
>"intrapolate" (generate a smaller number of regularly spaced samples
>from
>the input series), so it wouldn't be too hard form me to write a
>function
>that covers both, but I prefer not to reinvent wheels (unless my wheel
>is
>markedly better!)
>
>Can anyone tell me if there is already a package that might cover what
>I
>need?
>
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From sergio.fonda99 at gmail.com  Sun Sep  6 18:50:25 2015
From: sergio.fonda99 at gmail.com (Sergio Fonda)
Date: Sun, 6 Sep 2015 18:50:25 +0200
Subject: [R] Is there a time series resampling function ?
In-Reply-To: <1441540485695-4711907.post@n4.nabble.com>
References: <1441540485695-4711907.post@n4.nabble.com>
Message-ID: <CAJRuHor9mqpqbtf9va0sOOucJDtdyxSjcUGnTg8F_gzbw2BmeQ@mail.gmail.com>

Have you tried signal package?
SF
Il 06/set/2015 17:14, "AltShift" <allennugent at hotmail.com> ha scritto:

> I need a function for regularising the time base of electronically acquired
> signals (i.e. vectors of samples with a nominally constant time base).
>
> For example, the accelerometer in my smartphone can deliver data at about
> 50
> Hz, but the sampling rate varies by about 5% throughout a recording. I have
> developed signal processing functions that assume a constant sample rate,
> so
> I need to modify my input data vectors so that this assumption becomes
> true.
> Furthermore, if I want to combine signals from different sources that have
> different sampling rates, I will be obliged to harmonise the time bases,
> somehow.
>
> I have written routines in other languages to interpolate (generate a
> larger
> number of regularly spaced samples from the input series) or, what I call,
> "intrapolate" (generate a smaller number of regularly spaced samples from
> the input series), so it wouldn't be too hard form me to write a function
> that covers both, but I prefer not to reinvent wheels (unless my wheel is
> markedly better!)
>
> Can anyone tell me if there is already a package that might cover what I
> need?
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sergio.fonda99 at gmail.com  Sun Sep  6 19:02:31 2015
From: sergio.fonda99 at gmail.com (Sergio Fonda)
Date: Sun, 6 Sep 2015 19:02:31 +0200
Subject: [R] Is there a time series resampling function ?
In-Reply-To: <2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>
References: <1441540485695-4711907.post@n4.nabble.com>
	<2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>
Message-ID: <CAJRuHop-Nki7K7ZAv22dEv2PBNMF4SH4sXGUGdSQY3y0NA40-w@mail.gmail.com>

I insist: as a trial, apply function "resample" (package signal) I think
it's worth doing a test ( it performs resampling through bandlimited
interpolation)
SF
Il 06/set/2015 18:52, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> ha
scritto:

> There are lots of them. You might be having trouble searching because you
> don't know how to spell "interpolate".
>
> ?approx
> ?spline
>
> Also look at the Time Series task view on CRAN.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 6, 2015 4:54:45 AM PDT, AltShift <allennugent at hotmail.com>
> wrote:
> >I need a function for regularising the time base of electronically
> >acquired
> >signals (i.e. vectors of samples with a nominally constant time base).
> >
> >For example, the accelerometer in my smartphone can deliver data at
> >about 50
> >Hz, but the sampling rate varies by about 5% throughout a recording. I
> >have
> >developed signal processing functions that assume a constant sample
> >rate, so
> >I need to modify my input data vectors so that this assumption becomes
> >true.
> >Furthermore, if I want to combine signals from different sources that
> >have
> >different sampling rates, I will be obliged to harmonise the time
> >bases,
> >somehow.
> >
> >I have written routines in other languages to interpolate (generate a
> >larger
> >number of regularly spaced samples from the input series) or, what I
> >call,
> >"intrapolate" (generate a smaller number of regularly spaced samples
> >from
> >the input series), so it wouldn't be too hard form me to write a
> >function
> >that covers both, but I prefer not to reinvent wheels (unless my wheel
> >is
> >markedly better!)
> >
> >Can anyone tell me if there is already a package that might cover what
> >I
> >need?
> >
> >
> >
> >
> >--
> >View this message in context:
> >
> http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907.html
> >Sent from the R help mailing list archive at Nabble.com.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ragia11 at hotmail.com  Sun Sep  6 19:41:18 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Sun, 6 Sep 2015 20:41:18 +0300
Subject: [R] dplyr question
Message-ID: <DUB125-W54AF9A3065F7C2B5294121B3550@phx.gbl>

Dear group,
I have the following data frame df
 
   Measure_id i j id value1           1 5 1  1   2.02           1 5 2  1   2.03           1 5 1  2   1.54           1 5 2  2   1.55           1 5 1  3   0.06           1 5 2  3   0.07           1 5 1  4   0.08           1 5 2  4   1.09           1 5 1  5   1.010          1 5 2  5   2.0..        ... . . ..   ...I want to add a probability column,  the prob column depends on id grouped by for each ithe rank will be current (value / max value ) for the same id for specific i, it would be
 Measure_id i j id value    prob1           1 5 1  1   2.0          2/2  2           1 5 2  1   2.0          2/2  3           1 5 1  2   1.5          1.5/1.5   4           1 5 2  2   1.5          1.5/1.5 5           1 5 1  3   0.0          06           1 5 2  3   0.0          07           1 5 1  4   0.0          0/1  8           1 5 2  4   1.0          1/1  9           1 5 1  5   1.0          1/210          1 5 2  5   2.0          2/3..        ... . . ..   ...
then I want to add a rank coulmn that rank regarding probability, if the probability equal they took the same rank for thesame id belongs to the same i, otherwize lower probability took higher rank for examole if we have three values for i=7 and for the three values the id is 1 and the probability is ( .2,.4,.5) the rank should be 3,2,1
replying  highly appreciatedRagia 		 	   		  
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Sep  7 00:09:14 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 6 Sep 2015 15:09:14 -0700
Subject: [R] boxplots facets in ggplot,
 generated with two factors interaction mediane-ordered
In-Reply-To: <CAJRuHopv0BPXuNqaix_LXo2A3aG5ZBG6bDAa7cRLeco=kimR9g@mail.gmail.com>
References: <CAJRuHopv0BPXuNqaix_LXo2A3aG5ZBG6bDAa7cRLeco=kimR9g@mail.gmail.com>
Message-ID: <CAGxFJbRqJ8RbaEhMg2VCqB7xv9pyqzJBg+JwnLw=w+z-JRH8wg@mail.gmail.com>

Please stop posting in HTML and use dput() to provide data (as the
posting guide requests).

Cheers,
Bert Gunter


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sun, Sep 6, 2015 at 8:37 AM, Sergio Fonda <sergio.fonda99 at gmail.com> wrote:
> Thanks to Marc Schwartz I was able to order a "two factors"
> interaction boxplot with median associated to one factor alone.
> I tried further to generate facets plot (3x2 boxplots in ggplot2) for the
> dataframe reported at bottom and I'm not able to reach a correct plot.
> The dataframe is a simulation of genes expressions in five conditions of
> patients.
> I would like that each of the 5 boxplots were ordered with median of values
> associated to the combination "gene*pat.cond".
> Any help is much appreciated,
> Sergio
>  ____________________________
> Example Dataframe:
>     pat.cond     gene        value
> 1          N  ADAMTS1 -5.557194292
> 2          N  ADAMTS1 -2.576700157
> 3          E  ADAMTS1 -6.629356620
> 4          E  ADAMTS1 -6.629356620
> 5          E  ADAMTS1 -6.629356620
> 6          E  ADAMTS1 -6.629356620
> 7          M  ADAMTS1 -6.629356620
> 8          M  ADAMTS1 -6.629356620
> 9          M  ADAMTS1 -6.629356620
> 10         M  ADAMTS1 -2.742163709
> 11         N ADAMTS12 -1.084104611
> 12         E ADAMTS12  1.710871953
> 13         E ADAMTS12  6.629356620
> 14         M ADAMTS12  6.629356620
> 15         M ADAMTS12  1.711422682
> 16         E      APC -3.665169008
> 17         E      APC -1.192183578
> 18         M      APC -6.629356620
> 19         M      APC  3.696542194
> 20         M      APC -3.533991641
> 21         R      APC -4.267887134
> 22         N    BORIS  6.629356620
> 23         E    BORIS  6.629356620
> 24         M    BORIS  6.629356620
> 25         M    BORIS  6.629356620
> 26         M    BORIS  6.629356620
> 27         S    BORIS  6.629356620
> 28         E    BRCA1 -4.260347862
> 29         E    BRCA1 -6.629356620
> 30         E    BRCA1 -6.629356620
> 31         M    BRCA1 -6.443174967
> 32         M    BRCA1 -6.629356620
> 33         M    BRCA1 -6.629356620
> 34         M    BRCA1 -2.525972281
> 35         N      P16 -1.698706675
> 36         M      P16 -6.629356620
> 37         M      P16 -6.629356620
> 38         M      P16 -6.629356620
> 39         S      P16 -6.629356620
> 40         S      P16 -6.629356620
> 41         S      P16  0.518269571
> 42         N     DAX1  0.708010228
> 43         E     DAX1 -0.337455350
> 44         E     DAX1  2.803374605
> 45         E     DAX1 -1.407226228
> 46         M     DAX1  6.629356620
> 47         S     DAX1  2.022408499
> 48         S     DAX1  6.629356620
> 49         S     DAX1  6.629356620
> 50         S     DAX1  6.629356620
> 51         N     DKK1 -0.876905559
> 52         N     DKK1  0.008176565
> 53         E     DKK1 -3.057081024
> 54         M     DKK1  0.804046915
> 55         N     DKK2 -4.677880676
> 56         N     DKK2 -3.264182143
> 57         M     DKK2  0.219772061
> 58         R     DKK2 -3.205760419
> 59         R     DKK2 -3.567799537
> 60         R     DKK2 -1.687113091
> 61         S     DKK2 -6.629356620
> 62         S     DKK2 -6.629356620
> 63         N     ESR1  1.479810020
> 64         M     ESR1  1.861011014
> 65         N     FBP1 -0.110196473
> 66         N     FBP1  0.721286184
> 67         E     FBP1 -5.107943868
> 68         E     FBP1 -4.593812366
> 69         M     FBP1 -1.622176688
> 70         S     FBP1 -3.097791525
> 71         E    FOXL2 -4.564123239
> 72         E    FOXL2  3.111180437
> 73         M    FOXL2  0.323703764
> 74         M    FOXL2 -3.851350485
> 75         R    FOXL2 -3.324087523
> 76         S    FOXL2  0.739408989
> 77         M    GATA5 -1.171932246
> 78         M    GATA5 -3.466054731
> 79         M    GATA5 -1.283038699
> 80         S    GATA5 -2.778390690
> 81         S    GATA5 -2.014479273
> 82         S    GATA5 -3.015234172
> 83         E     GPX3 -6.629356620
> 84         E     GPX3 -1.942190735
> 85         M     GPX3  6.629356620
> 86         M     GPX3 -6.629356620
> 87         M     GPX3 -2.615982450
> 88         M     GPX3 -6.629356620
> 89         M     GPX3  0.128743354
> 90         S     GPX3 -6.629356620
> 91         E      MAL -6.629356620
> 92         E      MAL -4.825545452
> 93         E      MAL -6.629356620
> 94         E      MAL -6.629356620
> 95         E      MAL -6.629356620
> 96         M      MAL -0.419005364
> 97         M      MAL -0.923667455
> 98         M      MAL  6.629356620
> 99         M      MAL  3.371740196
> 100        S      MAL -6.629356620
> 101        N     MGMT -6.629356620
> 102        E     MGMT  1.115112556
> 103        M     MGMT  4.030893797
> 104        S     MGMT -6.629356620
> 105        N     MLH1 -0.519875304
> 106        N     MLH1 -1.352872084
> 107        E     MLH1 -0.777864442
> 108        E     MLH1  1.105073029
> 109        E     MLH1  5.758699199
> 110        E     MLH1 -1.498072236
> 111        S     MLH1 -1.630362301
> 112        M    MYOD1 -6.629356620
> 113        M    MYOD1  6.629356620
> 114        M    MYOD1 -6.629356620
> 115        M    MYOD1 -6.629356620
> 116        R    MYOD1 -6.629356620
> 117        R    MYOD1 -6.629356620
> 118        S    MYOD1 -6.629356620
> 119        S    MYOD1 -6.629356620
> 120        S    MYOD1 -4.645053781
> 121        N    NELL1 -6.629356620
> 122        N    NELL1 -5.536591557
> 123        N    NELL1 -5.903856552
> 124        E    NELL1 -6.629356620
> 125        E    NELL1 -6.629356620
> 126        M    NELL1 -6.629356620
> 127        S    NELL1 -6.629356620
> 128        N     OSMR -6.629356620
> 129        M     OSMR  4.407821839
> 130        M     OSMR  0.364604851
> 131        M     OSMR -6.629356620
> 132        S     OSMR -6.629356620
> 133        N     PAX6  6.629356620
> 134        E     PAX6 -3.401030959
> 135        M     PAX6 -0.946878855
> 136        S     PAX6  0.964721065
> 137        E    PTGS2 -3.749795240
> 138        M    PTGS2 -6.629356620
> 139        S    PTGS2 -2.076356656
> 140        S    PTGS2 -3.961164916
> 141        N     RARB -4.611429173
> 142        E     RARB -1.706166432
> 143        S     RARB -3.697493231
> 144        N  RASSF1A -1.505128642
> 145        E  RASSF1A  0.808179628
> 146        E  RASSF1A -6.629356620
> 147        E  RASSF1A -0.694997778
> 148        M  RASSF1A -0.332307577
> 149        S  RASSF1A -6.629356620
> 150        S  RASSF1A  6.629356620
> 151        N     RPRM -4.023523466
> 152        E     RPRM  0.443243995
> 153        S     RPRM -6.629356620
> 154        S     RPRM -4.019270038
> 155        E    RSPOI -1.735307607
> 156        E    RSPOI -1.296083205
> 157        M    RSPOI -3.003752756
> 158        S    RSPOI -3.156564936
> 159        S    RSPOI -2.928871731
> 160        N  SEPTIN9 -6.629356620
> 161        E  SEPTIN9 -2.585469731
> 162        E  SEPTIN9 -2.525798264
> 163        M  SEPTIN9  6.629356620
> 164        R  SEPTIN9 -3.071222253
> 165        S  SEPTIN9 -6.629356620
> 166        M    SFRP1 -1.426465815
> 167        R    SFRP1 -6.629356620
> 168        S    SFRP1 -2.348648751
> 169        S    SFRP1 -6.629356620
> 170        S    SFRP1 -2.304295273
> 171        N    SFRP4 -2.315044901
> 172        E    SFRP4 -2.940158139
> 173        M    SFRP4  6.629356620
> 174        S    SFRP4 -6.629356620
> 175        S    SFRP4  6.629356620
> 176        N    SFRP5 -6.629356620
> 177        E    SFRP5 -5.833523393
> 178        E    SFRP5  2.765666156
> 179        E    SFRP5  2.707734922
> 180        E    SFRP5  6.629356620
> 181        M    SMAD4 -6.629356620
> 182        M    SMAD4 -6.629356620
> 183        M    SMAD4 -6.629356620
> 184        R    SMAD4 -6.629356620
> 185        S    SMAD4 -6.629356620
> 186        S    SMAD4 -6.629356620
> 187        S    SMAD4 -6.629356620
> 188        N    SOCS3 -0.604614698
> 189        N    SOCS3  0.097268646
> 190        E    SOCS3 -1.505450218
> 191        E    SOCS3  6.629356620
> 192        N    SPARC -3.865639988
> 193        N    SPARC -1.868782928
> 194        E    SPARC  2.821046068
> 195        E    SPARC  6.629356620
> 196        S    SPARC -6.629356620
> 197        S    SPARC  0.202017887
> 198        S    SPARC  6.629356620
> 199        S    SPARC  6.629356620
> 200        N     TAC1 -1.528221578
> 201        M     TAC1  6.629356620
> 202        M     TAC1  6.629356620
> 203        R     TAC1 -2.294267250
> 204        S     TAC1  2.624594634
> 205        N     TERT -2.985641232
> 206        E     TERT  2.809747729
> 207        E     TERT  6.629356620
> 208        E     TERT -3.164404633
> 209        M     TERT  6.629356620
> 210        M     TERT -2.766881108
> 211        S     TERT  0.875337372
> 212        S     TERT  3.175095461
> 213        N    TIMP3 -2.046729298
> 214        E    TIMP3 -3.461408230
> 215        E    TIMP3 -2.996720557
> 216        M    TIMP3 -6.629356620
> 217        M    TIMP3 -1.527333149
> 218        R    TIMP3 -3.933657283
> 219        S    TIMP3 -1.020144976
> 220        S    TIMP3 -2.357112874
> 221        S    TIMP3  0.806736362
> 222        M   TMEFF2 -1.160275850
> 223        M   TMEFF2 -2.712025806
> 224        R   TMEFF2 -3.961478237
> 225        R   TMEFF2 -6.510953714
> 226        S   TMEFF2 -0.619555286
> 227        S   TMEFF2  2.344341057
> 228        E     WIF1 -6.629356620
> 229        E     WIF1 -3.806114522
> 230        E     WIF1 -2.318313158
> 231        M     WIF1  1.102899897
> 232        M     WIF1  6.629356620
> 233        M     WIF1 -6.629356620
> 234        N     WNT4 -6.629356620
> 235        E     WNT4 -3.474027185
> 236        E     WNT4 -6.629356620
> 237        S     WNT4 -6.629356620
> 238        S     WNT4 -6.629356620
> 239        E      WRN -6.629356620
> 240        M      WRN -4.148942985
> 241        S      WRN -1.855994142
> 242        R      WT1 -0.824982393
> 243        S      WT1  1.236129501
> 244        S      WT1  1.088540877
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From oluola2011 at yahoo.com  Mon Sep  7 00:22:32 2015
From: oluola2011 at yahoo.com (Olu Ola)
Date: Sun, 6 Sep 2015 15:22:32 -0700
Subject: [R] Handling "NA" in summation
Message-ID: <1441578152.25851.YahooMailBasic@web161605.mail.bf1.yahoo.com>

Hello,
I am currently working with a dataframe which has some missing values represented by "NA". whenever, I add two columns in which at least one of the pair of an observation is "NA", the sum returns zero. That is for the same observation, if 

dataframe$A = 20
dataframe$B = NA

dataframe$A + dataframe$B  returns zero.

I do not want to delete the observations with the NA's. How do I go about carrying out the necessary operations without deleting the observations with the NA's

Thank you


From sergio.fonda99 at gmail.com  Mon Sep  7 00:36:34 2015
From: sergio.fonda99 at gmail.com (Sergio Fonda)
Date: Mon, 7 Sep 2015 00:36:34 +0200
Subject: [R] boxplots facets in ggplot,
	generated with two factors interaction mediane-ordered
Message-ID: <CAJRuHoozLzB9cuP78MDuNRpa0JFw+0e7COdDMPttav6PsQHHuw@mail.gmail.com>

 Sorry for repeating the message owing to previous uncorrect html
version delivered (Thank you bert Gunter for the alert).
Marc Schwartz enabled me to order a "two factors" interaction boxplot
with median associated to one factor alone: thanks.
I tried further to generate facets plot (3x2 boxplots in ggplot2) for
the dataframe reported at bottom and I'm not able to reach a correct
plot.
The dataframe is a simulation of genes expressions in five conditions
of patients.
I would like that each of the 5 boxplots were ordered with median of
values associated to the combination "gene*pat.cond".
Any help is much appreciated,
Sergio
 ____________________________
Example Dataframe:
DF <-

structure(list(pat.cond = structure(c(3L, 3L, 1L, 1L, 1L, 1L,
2L, 2L, 2L, 2L, 3L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 4L, 3L,
1L, 2L, 2L, 2L, 5L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 3L, 2L, 2L, 2L,
5L, 5L, 5L, 3L, 1L, 1L, 1L, 2L, 5L, 5L, 5L, 5L, 3L, 3L, 1L, 2L,
3L, 3L, 2L, 4L, 4L, 4L, 5L, 5L, 3L, 2L, 3L, 3L, 1L, 1L, 2L, 5L,
1L, 1L, 2L, 2L, 4L, 5L, 2L, 2L, 2L, 5L, 5L, 5L, 1L, 1L, 2L, 2L,
2L, 2L, 2L, 5L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 5L, 3L, 1L,
2L, 5L, 3L, 3L, 1L, 1L, 1L, 1L, 5L, 2L, 2L, 2L, 2L, 4L, 4L, 5L,
5L, 5L, 3L, 3L, 3L, 1L, 1L, 2L, 5L, 3L, 2L, 2L, 2L, 5L, 3L, 1L,
2L, 5L, 1L, 2L, 5L, 5L, 3L, 1L, 5L, 3L, 1L, 1L, 1L, 2L, 5L, 5L,
3L, 1L, 5L, 5L, 1L, 1L, 2L, 5L, 5L, 3L, 1L, 1L, 2L, 4L, 5L, 2L,
4L, 5L, 5L, 5L, 3L, 1L, 2L, 5L, 5L, 3L, 1L, 1L, 1L, 1L, 2L, 2L,
2L, 4L, 5L, 5L, 5L, 3L, 3L, 1L, 1L, 3L, 3L, 1L, 1L, 5L, 5L, 5L,
5L, 3L, 2L, 2L, 4L, 5L, 3L, 1L, 1L, 1L, 2L, 2L, 5L, 5L, 3L, 1L,
1L, 2L, 2L, 4L, 5L, 5L, 5L, 2L, 2L, 4L, 4L, 5L, 5L, 1L, 1L, 1L,
2L, 2L, 2L, 3L, 1L, 1L, 5L, 5L, 1L, 2L, 5L, 4L, 5L, 5L), .Label = c("E",
"M", "N", "R", "S"), class = "factor"), gene = structure(c(1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L,
11L, 11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 12L, 12L, 13L,
13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L, 16L,
16L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L,
18L, 18L, 18L, 18L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 20L, 20L,
20L, 20L, 20L, 21L, 21L, 21L, 21L, 22L, 22L, 22L, 22L, 23L, 23L,
23L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 25L, 25L, 25L, 25L, 26L,
26L, 26L, 26L, 26L, 27L, 27L, 27L, 27L, 27L, 27L, 28L, 28L, 28L,
28L, 28L, 29L, 29L, 29L, 29L, 29L, 30L, 30L, 30L, 30L, 30L, 31L,
31L, 31L, 31L, 31L, 31L, 31L, 32L, 32L, 32L, 32L, 33L, 33L, 33L,
33L, 33L, 33L, 33L, 33L, 34L, 34L, 34L, 34L, 34L, 35L, 35L, 35L,
35L, 35L, 35L, 35L, 35L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L,
36L, 37L, 37L, 37L, 37L, 37L, 37L, 38L, 38L, 38L, 38L, 38L, 38L,
39L, 39L, 39L, 39L, 39L, 40L, 40L, 40L, 41L, 41L, 41L), .Label = c("ADAMTS1",
"ADAMTS12", "APC", "BORIS", "BRCA1", "P16", "DAX1", "DKK1", "DKK2",
"ESR1", "FBP1", "FOXL2", "GATA5", "GPX3", "MAL", "MGMT", "MLH1",
"MYOD1", "NELL1", "OSMR", "PAX6", "PTGS2", "RARB", "RASSF1A",
"RPRM", "RSPOI", "SEPTIN9", "SFRP1", "SFRP4", "SFRP5", "SMAD4",
"SOCS3", "SPARC", "TAC1", "TERT", "TIMP3", "TMEFF2", "WIF1",
"WNT4", "WRN", "WT1"), class = "factor"), value = c(-5.55719429171322,
-2.57670015708011, -6.62935662007961, -6.62935662007961, -6.62935662007961,
-6.62935662007961, -6.62935662007961, -6.62935662007961, -6.62935662007961,
-2.74216370891365, -1.08410461110653, 1.71087195329919, 6.62935662007961,
6.62935662007961, 1.71142268170053, -3.66516900838688, -1.1921835776928,
-6.62935662007961, 3.69654219362645, -3.53399164077955, -4.26788713377028,
6.62935662007961, 6.62935662007961, 6.62935662007961, 6.62935662007961,
6.62935662007961, 6.62935662007961, -4.26034786170766, -6.62935662007961,
-6.62935662007961, -6.44317496711901, -6.62935662007961, -6.62935662007961,
-2.52597228116937, -1.69870667463154, -6.62935662007961, -6.62935662007961,
-6.62935662007961, -6.62935662007961, -6.62935662007961, 0.518269571084176,
0.708010227963537, -0.337455349570156, 2.80337460503403, -1.40722622802079,
6.62935662007961, 2.0224084987157, 6.62935662007961, 6.62935662007961,
6.62935662007961, -0.876905559086274, 0.00817656528033632, -3.0570810238408,
0.804046915404625, -4.67788067608942, -3.26418214278398, 0.219772060806496,
-3.20576041923257, -3.56779953667954, -1.68711309148568, -6.62935662007961,
-6.62935662007961, 1.47981002017674, 1.86101101355619, -0.110196473063222,
0.721286184467319, -5.10794386813511, -4.59381236574539, -1.6221766881308,
-3.09779152534191, -4.56412323856937, 3.11118043673057, 0.323703764100918,
-3.85135048527067, -3.32408752349639, 0.73940898934279, -1.17193224621231,
-3.46605473088182, -1.28303869901795, -2.77839069037272, -2.01447927302819,
-3.01523417249907, -6.62935662007961, -1.94219073489774, 6.62935662007961,
-6.62935662007961, -2.61598244990042, -6.62935662007961, 0.128743354288953,
-6.62935662007961, -6.62935662007961, -4.82554545191733, -6.62935662007961,
-6.62935662007961, -6.62935662007961, -0.41900536366504, -0.923667454545306,
6.62935662007961, 3.37174019620549, -6.62935662007961, -6.62935662007961,
1.11511255641923, 4.03089379683332, -6.62935662007961, -0.51987530372271,
-1.35287208430557, -0.777864442364311, 1.10507302913784, 5.75869919923622,
-1.49807223582966, -1.63036230103387, -6.62935662007961, 6.62935662007961,
-6.62935662007961, -6.62935662007961, -6.62935662007961, -6.62935662007961,
-6.62935662007961, -6.62935662007961, -4.64505378091392, -6.62935662007961,
-5.53659155714965, -5.90385655188777, -6.62935662007961, -6.62935662007961,
-6.62935662007961, -6.62935662007961, -6.62935662007961, 4.40782183888056,
0.364604850758639, -6.62935662007961, -6.62935662007961, 6.62935662007961,
-3.40103095864142, -0.946878855241565, 0.964721064911626, -3.74979523988984,
-6.62935662007961, -2.07635665640007, -3.96116491608577, -4.61142917326893,
-1.70616643169626, -3.69749323087937, -1.5051286418556, 0.808179628116493,
-6.62935662007961, -0.694997777544086, -0.33230757657502, -6.62935662007961,
6.62935662007961, -4.0235234663823, 0.443243994976371, -6.62935662007961,
-4.01927003836633, -1.73530760747145, -1.29608320459239, -3.00375275640445,
-3.15656493554918, -2.92887173087699, -6.62935662007961, -2.58546973110719,
-2.52579826365207, 6.62935662007961, -3.07122225341412, -6.62935662007961,
-1.42646581480733, -6.62935662007961, -2.34864875081226, -6.62935662007961,
-2.30429527278787, -2.3150449014819, -2.94015813861698, 6.62935662007961,
-6.62935662007961, 6.62935662007961, -6.62935662007961, -5.83352339310267,
2.76566615565279, 2.7077349216041, 6.62935662007961, -6.62935662007961,
-6.62935662007961, -6.62935662007961, -6.62935662007961, -6.62935662007961,
-6.62935662007961, -6.62935662007961, -0.604614698290898, 0.0972686463523985,
-1.50545021800149, 6.62935662007961, -3.86563998837838, -1.86878292774122,
2.82104606819918, 6.62935662007961, -6.62935662007961, 0.20201788697105,
6.62935662007961, 6.62935662007961, -1.52822157760479, 6.62935662007961,
6.62935662007961, -2.29426725030903, 2.6245946340759, -2.98564123198612,
2.80974772888437, 6.62935662007961, -3.16440463278017, 6.62935662007961,
-2.76688110755984, 0.875337371866448, 3.17509546138607, -2.04672929806323,
-3.46140822952959, -2.99672055669611, -6.62935662007961, -1.52733314896727,
-3.93365728287806, -1.0201449757102, -2.35711287350022, 0.806736362269386,
-1.16027585023965, -2.71202580562016, -3.96147823714849, -6.51095371428348,
-0.619555285633288, 2.344341056524, -6.62935662007961, -3.80611452228589,
-2.31831315765999, 1.10289989684782, 6.62935662007961, -6.62935662007961,
-6.62935662007961, -3.47402718515842, -6.62935662007961, -6.62935662007961,
-6.62935662007961, -6.62935662007961, -4.14894298549979, -1.85599414152503,
-0.824982393234853, 1.23612950069026, 1.08854087705027)), .Names = c("pat.cond",
"gene", "value"), row.names = c(NA, -244L), class = "data.frame")


From zadig_1 at excite.com  Mon Sep  7 00:48:19 2015
From: zadig_1 at excite.com (ce)
Date: Sun, 06 Sep 2015 18:48:19 -0400
Subject: [R] Handling "NA" in summation
Message-ID: <20150906184819.11374@web007.roc2.bluetie.com>



I use something like :

dataframe[ is.na(dataframe) ] <- 0 
dataframe[ is.nan(dataframe) ] <- 0 
dataframe[ is.infinite(dataframe) ] <- 0 

-----Original Message-----
From: "Olu Ola via R-help" [r-help at r-project.org]
Date: 09/06/2015 06:24 PM
To: r-help at r-project.org
Subject: [R] Handling &quot;NA&quot; in summation

Hello,
I am currently working with a dataframe which has some missing values represented by "NA". whenever, I add two columns in which at least one of the pair of an observation is "NA", the sum returns zero. That is for the same observation, if 

dataframe$A = 20
dataframe$B = NA

dataframe$A + dataframe$B  returns zero.

I do not want to delete the observations with the NA's. How do I go about carrying out the necessary operations without deleting the observations with the NA's

Thank you

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Mon Sep  7 00:58:04 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sun, 06 Sep 2015 15:58:04 -0700
Subject: [R] Handling "NA" in summation
In-Reply-To: <1441578152.25851.YahooMailBasic@web161605.mail.bf1.yahoo.com>
References: <1441578152.25851.YahooMailBasic@web161605.mail.bf1.yahoo.com>
Message-ID: <6B8C83B6-D464-4DB4-94A6-0B4080F23BD5@dcn.davis.CA.us>

That is not how R works. 20+NA is NA, which is not the same as zero. This is not optional behaviour.

I notice that you put quotes around the NA.... if those really are there then you should be getting an error.

You need to assemble a reproducible example, such as is described at [1]. By doing so you will either see your mistake or have something we can help you debug. Be sure to give one or more examples of results you expect to obtain, since your email below does not indicate what your desired result is.

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 6, 2015 3:22:32 PM PDT, Olu Ola via R-help <r-help at r-project.org> wrote:
>Hello,
>I am currently working with a dataframe which has some missing values
>represented by "NA". whenever, I add two columns in which at least one
>of the pair of an observation is "NA", the sum returns zero. That is
>for the same observation, if 
>
>dataframe$A = 20
>dataframe$B = NA
>
>dataframe$A + dataframe$B  returns zero.
>
>I do not want to delete the observations with the NA's. How do I go
>about carrying out the necessary operations without deleting the
>observations with the NA's
>
>Thank you
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From sarah.goslee at gmail.com  Mon Sep  7 00:59:38 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Sun, 6 Sep 2015 18:59:38 -0400
Subject: [R] Handling "NA" in summation
In-Reply-To: <20150906184819.11374@web007.roc2.bluetie.com>
References: <20150906184819.11374@web007.roc2.bluetie.com>
Message-ID: <CAM_vjun5RC8P3y+Cvw6_q_myqRtfGroVcBPpafS1_SSf9c4_Pw@mail.gmail.com>

I'm not quite sure how you get zero from that situation. Do you expect
the answer to be 20?

How about:

> dataframe <- data.frame(A=20, B=NA)

> dataframe$A + dataframe$B

[1] NA

> ?sum

> sum(dataframe$A, dataframe$B, na.rm=TRUE)

[1] 20

Sarah

On Sun, Sep 6, 2015 at 6:48 PM, ce <zadig_1 at excite.com> wrote:
>
>
> I use something like :
>
> dataframe[ is.na(dataframe) ] <- 0
> dataframe[ is.nan(dataframe) ] <- 0
> dataframe[ is.infinite(dataframe) ] <- 0
>
> -----Original Message-----
> From: "Olu Ola via R-help" [r-help at r-project.org]
> Date: 09/06/2015 06:24 PM
> To: r-help at r-project.org
> Subject: [R] Handling &quot;NA&quot; in summation
>
> Hello,
> I am currently working with a dataframe which has some missing values represented by "NA". whenever, I add two columns in which at least one of the pair of an observation is "NA", the sum returns zero. That is for the same observation, if
>
> dataframe$A = 20
> dataframe$B = NA
>
> dataframe$A + dataframe$B  returns zero.
>
> I do not want to delete the observations with the NA's. How do I go about carrying out the necessary operations without deleting the observations with the NA's
>
> Thank you
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From sarah.goslee at gmail.com  Mon Sep  7 01:02:02 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Sun, 6 Sep 2015 19:02:02 -0400
Subject: [R] groups Rank
In-Reply-To: <DUB125-W27A4D5CE21C3CF6038C503B3560@phx.gbl>
References: <DUB125-W10C5D212C9BD3CF6775318B3A00@phx.gbl>
	<DUB125-W91C972100E24493CABA989B3700@phx.gbl>
	<DUB125-W49BC987BD7C478DCA7220FB3700@phx.gbl>
	<DUB125-W12FF665BD9DF1AD4630B40B3570@phx.gbl>
	<CAM_vjunHM8iAHisTytkn-EWjq42V4oo0NqOtQCbL3RvmXLqsZA@mail.gmail.com>
	<DUB125-W27A4D5CE21C3CF6038C503B3560@phx.gbl>
Message-ID: <CAM_vju=FpeLo-fmhdk2vJ8thmxeAbcHhKXwk9yOWYDuXnsrL+A@mail.gmail.com>

Please use dput() to provide data, rather than expecting people to
open random attachments. Besides, there are multiple options for
getting data into R, and we need to know exactly what you did. dput()
is faster and easier.

What have you tried? Did you look at aggregate() as I suggested?

Sarah

On Sat, Sep 5, 2015 at 10:44 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
> thanks for replying, I attached the data frame
>  for source "i" I want to sum the values and get the max value then add a
> new column called rank . That new column cell value for each source i and
> for specific id would be (value/max value) * count of rows that have the
> same criteria "same i and same id"
>
> many thanks
> Ragia
>
>> Date: Fri, 4 Sep 2015 10:19:35 -0400
>> Subject: Re: [R] groups Rank
>> From: sarah.goslee at gmail.com
>> To: ragia11 at hotmail.com
>> CC: r-help at r-project.org
>>
>> Hi Ragia,
>>
>> I can't make out your data or desired result, but it sounds like
>> aggregate() might get you started. If you need more help, please
>> repost your data using dput() and do NOT post in HTML so that we can
>> see what your desired result looks like.
>>
>> Sarah
>>
>> On Fri, Sep 4, 2015 at 10:12 AM, Ragia Ibrahim <ragia11 at hotmail.com>
>> wrote:
>> > Dear Group,kinldy, I have the following data frame df
>> > id value1 1 4 2 1 4 3 1 6 4 1 6 5 2 1.5 6 2 2.5 7 2 2.5 8 2 2.5
>> >
>> > add rank column regarding id coulmn where rank for the highest value
>> > would be 1, others rank would be the (value/ value of heighest)/ number of
>> > rows that took the same value
>> > thus the data frame should be
>> > id value Rank1 1 4 0.332 1 4 0.333 1 6 0.54 1 6 0.55 2 1.5 0.6 6 2 2.5
>> > 0.337 2 2.5 0.338 2 2.5 0.33
>> >
>> > how to reach this resultthanks in advanceRagia
>> > [[alternative HTML version deleted]]
>> >


From r.turner at auckland.ac.nz  Mon Sep  7 01:16:05 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 7 Sep 2015 11:16:05 +1200
Subject: [R] [FORGED]  Handling "NA" in summation
In-Reply-To: <1441578152.25851.YahooMailBasic@web161605.mail.bf1.yahoo.com>
References: <1441578152.25851.YahooMailBasic@web161605.mail.bf1.yahoo.com>
Message-ID: <55ECC935.6010205@auckland.ac.nz>

On 07/09/15 10:22, Olu Ola via R-help wrote:
> Hello, I am currently working with a dataframe which has some missing
> values represented by "NA". whenever, I add two columns in which at
> least one of the pair of an observation is "NA", the sum returns
> zero. That is for the same observation, if
>
> dataframe$A = 20 dataframe$B = NA
>
> dataframe$A + dataframe$B  returns zero.

No it does not.  It returns NA.  As it should.

> I do not want to delete the observations with the NA's. How do I go
> about carrying out the necessary operations without deleting the
> observations with the NA's.

Your question seems to demonstrate a substantial amount of confusion.

Amongst other things you probably want to deal with vectors (or perhaps 
matrices) rather than data frames.

To sum a numeric vector, ignoring missing values, you can use the sum() 
function, setting the argument "na.rm" to TRUE.  E.g.

    v <- c(1,NA,2,NA,3,NA,4,NA)
    sum(v,na.rm=TRUE) # Gives 10.

Ignore other advice that you were given, to replace NAs in your data 
frame (???) by zeroes.  That is very dangerous, misleading and 
confusing.  "Missing" and "zero" are *VERY* different concepts.

cheers,

Rolf Turner


-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jdnewmil at dcn.davis.CA.us  Mon Sep  7 01:20:46 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sun, 06 Sep 2015 16:20:46 -0700
Subject: [R] Handling "NA" in summation
In-Reply-To: <20150906184819.11374@web007.roc2.bluetie.com>
References: <20150906184819.11374@web007.roc2.bluetie.com>
Message-ID: <1568924B-A508-417C-8535-CA56A2196CEE@dcn.davis.CA.us>

So you have decided that NA==0 and Inf == 0... if that is really what you want then it looks like that is what you got. If you don't like the fact that you are mucking with your data, then make a copy of the data first and muck with that.

Blegh.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 6, 2015 3:48:19 PM PDT, ce <zadig_1 at excite.com> wrote:
>
>
>I use something like :
>
>dataframe[ is.na(dataframe) ] <- 0 
>dataframe[ is.nan(dataframe) ] <- 0 
>dataframe[ is.infinite(dataframe) ] <- 0 
>
>-----Original Message-----
>From: "Olu Ola via R-help" [r-help at r-project.org]
>Date: 09/06/2015 06:24 PM
>To: r-help at r-project.org
>Subject: [R] Handling &quot;NA&quot; in summation
>
>Hello,
>I am currently working with a dataframe which has some missing values
>represented by "NA". whenever, I add two columns in which at least one
>of the pair of an observation is "NA", the sum returns zero. That is
>for the same observation, if 
>
>dataframe$A = 20
>dataframe$B = NA
>
>dataframe$A + dataframe$B  returns zero.
>
>I do not want to delete the observations with the NA's. How do I go
>about carrying out the necessary operations without deleting the
>observations with the NA's
>
>Thank you
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From zadig_1 at excite.com  Mon Sep  7 01:30:36 2015
From: zadig_1 at excite.com (ce)
Date: Sun, 06 Sep 2015 19:30:36 -0400
Subject: [R] Handling "NA" in summation
Message-ID: <20150906193036.21969@web010.roc2.bluetie.com>


Sorry I misunderstood questions .
I think original poster wants result to be 0 , right ? 
Then 

dataframe$A[ is.na(dataframe$B) ] <- 0
dataframe$B[ is.na(dataframe$B) ] <- 0

is this case you would lose  dataframe$A data or 

dataframe$A + ( dataframe$B[ is.na(dataframe$B) ] <- -1*dataframe$A )



-----Original Message-----
From: "Sarah Goslee" [sarah.goslee at gmail.com]
Date: 09/06/2015 07:00 PM
CC: "r-help" <r-help at r-project.org>
Subject: Re: [R] Handling &quot;NA&quot; in summation

I'm not quite sure how you get zero from that situation. Do you expect
the answer to be 20?

How about:

> dataframe <- data.frame(A=20, B=NA)

> dataframe$A + dataframe$B

[1] NA

> ?sum

> sum(dataframe$A, dataframe$B, na.rm=TRUE)

[1] 20

Sarah

On Sun, Sep 6, 2015 at 6:48 PM, ce <zadig_1 at excite.com> wrote:
>
>
> I use something like :
>
> dataframe[ is.na(dataframe) ] <- 0
> dataframe[ is.nan(dataframe) ] <- 0
> dataframe[ is.infinite(dataframe) ] <- 0
>
> -----Original Message-----
> From: "Olu Ola via R-help" [r-help at r-project.org]
> Date: 09/06/2015 06:24 PM
> To: r-help at r-project.org
> Subject: [R] Handling "NA" in summation
>
> Hello,
> I am currently working with a dataframe which has some missing values represented by "NA". whenever, I add two columns in which at least one of the pair of an observation is "NA", the sum returns zero. That is for the same observation, if
>
> dataframe$A = 20
> dataframe$B = NA
>
> dataframe$A + dataframe$B  returns zero.
>
> I do not want to delete the observations with the NA's. How do I go about carrying out the necessary operations without deleting the observations with the NA's
>
> Thank you
>

-- 
Sarah Goslee
http://www.functionaldiversity.org

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Mon Sep  7 01:50:38 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Sun, 6 Sep 2015 16:50:38 -0700 (PDT)
Subject: [R] Handling "NA" in summation
In-Reply-To: <1441578152.25851.YahooMailBasic@web161605.mail.bf1.yahoo.com>
References: <1441578152.25851.YahooMailBasic@web161605.mail.bf1.yahoo.com>
Message-ID: <1441583438824-4711932.post@n4.nabble.com>

# it's not clear what your question is, but here's a stab in the dark at a
solution!

ind<- !is.na(dataframe$A) & !is.na(dataframe$B)
dataframe$A[ind] + dataframe$B[ind] 

- Dan

P.S. I'm sure there are ways to do this using one of R's functions for
automatically removing NA's (na.rm = T), but unless you use them all the
time, their behavior is not always predictable (e.g., sometimes it ignores a
whole row with one NA in it; sometimes it treats NAs as zeros). Explicitly
defining  the indices that you want to exclude may make it easier to avoid
difficult-to-find errors.
 



--
View this message in context: http://r.789695.n4.nabble.com/Handling-NA-in-summation-tp4711923p4711932.html
Sent from the R help mailing list archive at Nabble.com.


From pywang61 at yahoo.com.tw  Mon Sep  7 03:14:40 2015
From: pywang61 at yahoo.com.tw (=?utf-8?B?546L5p+P5YWD?=)
Date: Mon, 7 Sep 2015 09:14:40 +0800
Subject: [R] =?utf-8?b?5Zue6KaG77mVICDlm57opobvuZUgIEhvdyB0byBnZXQgZmls?=
 =?utf-8?q?ter_probabilities_from_msmFit=28=29_of_package_MSwM=3F?=
In-Reply-To: <1441345650.8051.YahooMailBasic@web75103.mail.tw1.yahoo.com>
Message-ID: <1441588480.12317.YahooMailBasic@web75103.mail.tw1.yahoo.com>

I think I find the way to extract smoothed probabilities from msmFit().

In the MSM.lm-class, we could use  mod.msm at Fit@smoProb to extract the smoothed probabilities, also, we could use  mod.msm at Fit@filtProb to extract the filter probabilities.

Best Regards

  Paul Wang
  pywang61 at yahoo.com.tw

--------------------------------------------
15/9/4 (?)???? <pywang61 at yahoo.com.tw> ???

 ??: [R] ???  How to get filter probabilities from msmFit() of package MSwM?
 ???: r-help at r-project.org
 ??: 2015?9?4?,?,??1:47

 By using?
 plotProb(mod.msm,which=2), I could only show the graph of
 smoothed probabilities. May I extract smoothed probabilities
 and filter probabilities from the msmFit()?? thank you very
 much.
 --------------------------------------------
 15/9/4 (?)???? <pywang61 at yahoo.com.tw>
 ???

  ??: [R] How
 to get filter probabilities from msmFit() of package
 MSwM?
  ???: r-help at r-project.org
  ??: 2015?9?4?,?,??9:01


  Dear all:
  ? ? I am a rookie in using R. I have a
 question:
  How to get filter probabilities
 from msmFit() of package
  MSwM?

  following are my code....

  #Markov Switch Model
  library(MSwM)

 data(example)
  mod<-lm(y~1,example)
  mod.msm<-msmFit(mod,k=2,sw=c(T,T))
  summary(mod.msm)

 plotProb(mod.msm,which=1)

 plotProb(mod.msm,which=2)


  ?
  ???

  Paul Wang
  pywang61 at yahoo.com.tw


 ______________________________________________
  R-help at r-project.org
  mailing list -- To UNSUBSCRIBE and more,
 see
  https://stat.ethz.ch/mailman/listinfo/r-help
  PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
  and provide commented, minimal,
 self-contained, reproducible
  code.

 ______________________________________________
 R-help at r-project.org
 mailing list -- To UNSUBSCRIBE and more, see
 https://stat.ethz.ch/mailman/listinfo/r-help
 PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 and provide commented, minimal, self-contained,
 reproducible code.


From oluola2011 at yahoo.com  Mon Sep  7 04:35:08 2015
From: oluola2011 at yahoo.com (Olu Ola)
Date: Sun, 6 Sep 2015 19:35:08 -0700
Subject: [R] [FORGED]  Handling "NA" in summation
In-Reply-To: <55ECC935.6010205@auckland.ac.nz>
Message-ID: <1441593308.65267.YahooMailBasic@web161605.mail.bf1.yahoo.com>

Hello,
I need to first apologize for the error in my first question
 dataframe$A = 20 dataframe$B = NA
 
 dataframe$A + dataframe$B actually? returns NA

You quite understand my point of view. This is a household level data where you need to compute the total income of each household member before aggregating by household. 
Assume you have a household with 5 members
4 out of the 5 household members do have a full-time job
3 of the household members do not have a part-time job so that the column for these part-time job records NA for these three household members.
1 of the household members neither has a full-time nor part-time job
When I add the column for the full-time job and the part-time job for the five household members, it returns NA as the total income for the two household members who at least should have their total income equal to their full-time job income. 
Based on the scenario described above, only one of the household members should have NA for the total income but R returns NA as the total income for the two household members who at least should have their total income equal to their full-time job income. 
  
This is just the first step because subsequently, I will need to compute mean. If I go ahead to replace the NA's with zeros it will bias my mean. 

So all I need is a way to still retain my NA so that my mean and other relevant computations will not be biased.

Thank you
 
--------------------------------------------
On Sun, 9/6/15, Rolf Turner <r.turner at auckland.ac.nz> wrote:

 Subject: Re: [FORGED] [R] Handling "NA" in summation

 Date: Sunday, September 6, 2015, 7:16 PM

 On 07/09/15 10:22, Olu
 Ola via R-help wrote:
 > Hello, I am
 currently working with a dataframe which has some missing
 > values represented by "NA".
 whenever, I add two columns in which at
 >
 least one of the pair of an observation is "NA",
 the sum returns
 > zero. That is for the
 same observation, if
 >
 > dataframe$A = 20 dataframe$B = NA
 >
 > dataframe$A +
 dataframe$B? returns zero.

 No it does not.? It returns NA.? As it
 should.

 > I do not want to delete
 the observations with the NA's. How do I go
 > about carrying out the necessary
 operations without deleting the
 >
 observations with the NA's.

 Your question seems to demonstrate a
 substantial amount of confusion.

 Amongst other things you probably want to deal
 with vectors (or perhaps 
 matrices) rather
 than data frames.

 To sum a
 numeric vector, ignoring missing values, you can use the
 sum() 
 function, setting the argument
 "na.rm" to TRUE.? E.g.

 ? ? v <- c(1,NA,2,NA,3,NA,4,NA)
 ? ? sum(v,na.rm=TRUE) # Gives 10.

 Ignore other advice that you
 were given, to replace NAs in your data 
 frame (???) by zeroes.? That is very
 dangerous, misleading and 
 confusing.?
 "Missing" and "zero" are *VERY*
 different concepts.

 cheers,

 Rolf
 Turner


 --

 Technical Editor ANZJS
 Department of Statistics
 University of Auckland
 Phone:
 +64-9-373-7599 ext. 88276


From sarah.goslee at gmail.com  Mon Sep  7 05:02:48 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Sun, 6 Sep 2015 23:02:48 -0400
Subject: [R] [FORGED] Handling "NA" in summation
In-Reply-To: <1441593308.65267.YahooMailBasic@web161605.mail.bf1.yahoo.com>
References: <55ECC935.6010205@auckland.ac.nz>
	<1441593308.65267.YahooMailBasic@web161605.mail.bf1.yahoo.com>
Message-ID: <CAM_vjuk7JH2v6a=sP+YgD=sGGxSZOwypuSpAxDjWqZ33QFvMAw@mail.gmail.com>

I think given the previous confusion and lack of clarity in this
description as well, you really need to give us
a. sample data using dput()
b. what you expect the result to be
c. what code you've tried, including how you've attempted using the
sum() function that several of us have recommended.

Sarah

On Sun, Sep 6, 2015 at 10:35 PM, Olu Ola via R-help
<r-help at r-project.org> wrote:
> Hello,
> I need to first apologize for the error in my first question
>  dataframe$A = 20 dataframe$B = NA
>
>  dataframe$A + dataframe$B actually  returns NA
>
> You quite understand my point of view. This is a household level data where you need to compute the total income of each household member before aggregating by household.
> Assume you have a household with 5 members
> 4 out of the 5 household members do have a full-time job
> 3 of the household members do not have a part-time job so that the column for these part-time job records NA for these three household members.
> 1 of the household members neither has a full-time nor part-time job
> When I add the column for the full-time job and the part-time job for the five household members, it returns NA as the total income for the two household members who at least should have their total income equal to their full-time job income.
> Based on the scenario described above, only one of the household members should have NA for the total income but R returns NA as the total income for the two household members who at least should have their total income equal to their full-time job income.
>
> This is just the first step because subsequently, I will need to compute mean. If I go ahead to replace the NA's with zeros it will bias my mean.
>
> So all I need is a way to still retain my NA so that my mean and other relevant computations will not be biased.
>
> Thank you
>
> --------------------------------------------
> On Sun, 9/6/15, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
>  Subject: Re: [FORGED] [R] Handling "NA" in summation
>
>  Date: Sunday, September 6, 2015, 7:16 PM
>
>  On 07/09/15 10:22, Olu
>  Ola via R-help wrote:
>  > Hello, I am
>  currently working with a dataframe which has some missing
>  > values represented by "NA".
>  whenever, I add two columns in which at
>  >
>  least one of the pair of an observation is "NA",
>  the sum returns
>  > zero. That is for the
>  same observation, if
>  >
>  > dataframe$A = 20 dataframe$B = NA
>  >
>  > dataframe$A +
>  dataframe$B  returns zero.
>
>  No it does not.  It returns NA.  As it
>  should.
>
>  > I do not want to delete
>  the observations with the NA's. How do I go
>  > about carrying out the necessary
>  operations without deleting the
>  >
>  observations with the NA's.
>
>  Your question seems to demonstrate a
>  substantial amount of confusion.
>
>  Amongst other things you probably want to deal
>  with vectors (or perhaps
>  matrices) rather
>  than data frames.
>
>  To sum a
>  numeric vector, ignoring missing values, you can use the
>  sum()
>  function, setting the argument
>  "na.rm" to TRUE.  E.g.
>
>      v <- c(1,NA,2,NA,3,NA,4,NA)
>      sum(v,na.rm=TRUE) # Gives 10.
>
>  Ignore other advice that you
>  were given, to replace NAs in your data
>  frame (???) by zeroes.  That is very
>  dangerous, misleading and
>  confusing.
>  "Missing" and "zero" are *VERY*
>  different concepts.
>
>  cheers,
>
>  Rolf
>  Turner
>


From jdnewmil at dcn.davis.CA.us  Mon Sep  7 06:27:55 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sun, 06 Sep 2015 21:27:55 -0700
Subject: [R] [FORGED]  Handling "NA" in summation
In-Reply-To: <1441593308.65267.YahooMailBasic@web161605.mail.bf1.yahoo.com>
References: <1441593308.65267.YahooMailBasic@web161605.mail.bf1.yahoo.com>
Message-ID: <4CC1C733-357B-47F1-8D0A-693D85A5D137@dcn.davis.CA.us>

That you choose to interpret an income level of NA as zero could mean that the value was encoded incorrectly or could mean that that the income of that person was never reviewed and theirs might have represented a significant additional amount of income.

If you believe it was encoded improperly, you could use the technique suggested by zadig_1 at excite.com on a copy of your data for the purposes of this calculation.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 6, 2015 7:35:08 PM PDT, Olu Ola via R-help <r-help at r-project.org> wrote:
>Hello,
>I need to first apologize for the error in my first question
> dataframe$A = 20 dataframe$B = NA
> 
> dataframe$A + dataframe$B actually? returns NA
>
>You quite understand my point of view. This is a household level data
>where you need to compute the total income of each household member
>before aggregating by household. 
>Assume you have a household with 5 members
>4 out of the 5 household members do have a full-time job
>3 of the household members do not have a part-time job so that the
>column for these part-time job records NA for these three household
>members.
>1 of the household members neither has a full-time nor part-time job
>When I add the column for the full-time job and the part-time job for
>the five household members, it returns NA as the total income for the
>two household members who at least should have their total income equal
>to their full-time job income. 
>Based on the scenario described above, only one of the household
>members should have NA for the total income but R returns NA as the
>total income for the two household members who at least should have
>their total income equal to their full-time job income. 
>  
>This is just the first step because subsequently, I will need to
>compute mean. If I go ahead to replace the NA's with zeros it will bias
>my mean. 
>
>So all I need is a way to still retain my NA so that my mean and other
>relevant computations will not be biased.
>
>Thank you
> 
>--------------------------------------------
>On Sun, 9/6/15, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
> Subject: Re: [FORGED] [R] Handling "NA" in summation
>
> Date: Sunday, September 6, 2015, 7:16 PM
>
> On 07/09/15 10:22, Olu
> Ola via R-help wrote:
> > Hello, I am
> currently working with a dataframe which has some missing
> > values represented by "NA".
> whenever, I add two columns in which at
> >
> least one of the pair of an observation is "NA",
> the sum returns
> > zero. That is for the
> same observation, if
> >
> > dataframe$A = 20 dataframe$B = NA
> >
> > dataframe$A +
> dataframe$B? returns zero.
>
> No it does not.? It returns NA.? As it
> should.
>
> > I do not want to delete
> the observations with the NA's. How do I go
> > about carrying out the necessary
> operations without deleting the
> >
> observations with the NA's.
>
> Your question seems to demonstrate a
> substantial amount of confusion.
>
> Amongst other things you probably want to deal
> with vectors (or perhaps 
> matrices) rather
> than data frames.
>
> To sum a
> numeric vector, ignoring missing values, you can use the
> sum() 
> function, setting the argument
> "na.rm" to TRUE.? E.g.
>
> ? ? v <- c(1,NA,2,NA,3,NA,4,NA)
> ? ? sum(v,na.rm=TRUE) # Gives 10.
>
> Ignore other advice that you
> were given, to replace NAs in your data 
> frame (???) by zeroes.? That is very
> dangerous, misleading and 
> confusing.?
> "Missing" and "zero" are *VERY*
> different concepts.
>
> cheers,
>
> Rolf
> Turner
>
>
> --
>
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone:
> +64-9-373-7599 ext. 88276
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From tanasa at gmail.com  Mon Sep  7 08:00:14 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Sun, 6 Sep 2015 23:00:14 -0700
Subject: [R] scaling loess curves
Message-ID: <CA+JEM03QfeZeA4ResFMS+2bcCJMxR9o0zjRj+m8sFBiqjEEiLg@mail.gmail.com>

Dear all,

please could you advise about a method to scale 2 plots of LOESS curves.
More specifically, we do have 2 sets of 5C data, and the loess plots
reflect the relationship between INTENSITY and DISTANCE (please see the R
code below).

I am looking for a method/formula to scale these 2 LOESS plots and make
them directly comparable.

many thanks,

-- bogdan



-------------- the R code ------------------



a <- read.delim("a",header=T)
qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size = 1,
span=0.01)+xlab("distance")+ylab("intensity")



b <- read.delim("b",header=T)
qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size = 1,
span=0.01)+xlab("distance")+ylab("intensity")

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Sep  7 09:35:31 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 7 Sep 2015 07:35:31 +0000
Subject: [R] dplyr question
In-Reply-To: <DUB125-W54AF9A3065F7C2B5294121B3550@phx.gbl>
References: <DUB125-W54AF9A3065F7C2B5294121B3550@phx.gbl>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E69D@SRVEXCHMBX.precheza.cz>

Hi

Why do you post the same question without following Sarah's advice?

You can clearly see that due your HTML posting your data are terrible mess and we do not have slightest idea what you really want.

So again. Please copy to next mail result of

dput(head(df, 20))

and explain what shall be the result.

And change your mail client to post in plain text.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ragia
> Ibrahim
> Sent: Sunday, September 06, 2015 7:41 PM
> To: r-help at r-project.org
> Subject: [R] dplyr question
>
> Dear group,
> I have the following data frame df
>
>    Measure_id i j id value1           1 5 1  1   2.02           1 5 2
> 1   2.03           1 5 1  2   1.54           1 5 2  2   1.55
> 1 5 1  3   0.06           1 5 2  3   0.07           1 5 1  4   0.08
> 1 5 2  4   1.09           1 5 1  5   1.010          1 5 2  5   2.0..
> ... . . ..   ...I want to add a probability column,  the prob column
> depends on id grouped by for each ithe rank will be current (value /
> max value ) for the same id for specific i, it would be
>  Measure_id i j id value    prob1           1 5 1  1   2.0          2/2
> 2           1 5 2  1   2.0          2/2  3           1 5 1  2   1.5
> 1.5/1.5   4           1 5 2  2   1.5          1.5/1.5 5           1 5 1
> 3   0.0          06           1 5 2  3   0.0          07           1 5
> 1  4   0.0          0/1  8           1 5 2  4   1.0          1/1  9
> 1 5 1  5   1.0          1/210          1 5 2  5   2.0          2/3..
> ... . . ..   ...
> then I want to add a rank coulmn that rank regarding probability, if
> the probability equal they took the same rank for thesame id belongs to
> the same i, otherwize lower probability took higher rank for examole if
> we have three values for i=7 and for the three values the id is 1 and
> the probability is ( .2,.4,.5) the rank should be 3,2,1
> replying  highly appreciatedRagia
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ragia11 at hotmail.com  Mon Sep  7 10:11:49 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Mon, 7 Sep 2015 11:11:49 +0300
Subject: [R] groups Rank
In-Reply-To: <DUB125-W330E69EE3D349200682C6CB3540@phx.gbl>
References: <DUB125-W10C5D212C9BD3CF6775318B3A00@phx.gbl>,
	<DUB125-W91C972100E24493CABA989B3700@phx.gbl>,
	<DUB125-W49BC987BD7C478DCA7220FB3700@phx.gbl>,
	<DUB125-W12FF665BD9DF1AD4630B40B3570@phx.gbl>,
	<CAM_vjunHM8iAHisTytkn-EWjq42V4oo0NqOtQCbL3RvmXLqsZA@mail.gmail.com>,
	<DUB125-W27A4D5CE21C3CF6038C503B3560@phx.gbl>,
	<CAM_vju=FpeLo-fmhdk2vJ8thmxeAbcHhKXwk9yOWYDuXnsrL+A@mail.gmail.com>,
	<DUB125-W330E69EE3D349200682C6CB3540@phx.gbl>
Message-ID: <DUB125-W800D17CA67EC7748B3503CB3540@phx.gbl>

apology for re sending the Email, I changed the format to plain text as I have been advised
the?data ?is as follow??

thanks Sarah,?
I used pdut, and here is the data as written on R..I attached the dput result
structure(list(Measure_id = c(1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3,?
3, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2,?
3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3), i = c(5, 5,?
5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,?
5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,?
7, 7, 7, 7), j = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,?
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,?
3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5), id = c(1, 2, 3, 4, 5,?
6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,?
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8,?
9, 10, 11, 12), value = c(2, 1.5, 0, 0, 1, 0.5, 0, 0, 0, 0, 0.5,?
2, 2, 1.5, 0, 1, 2, 0, 0.5, 1.44269504088896, 0, 0, 0, 0, 1,?
1.5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0,?
0, 0, 0)), .Names = c("Measure_id", "i", "j", "id", "value"), row.names = c(NA,?
48L), class = "data.frame")

the?data ?is as follow??:

? ?Measure_id i j id value
1 ? ? ? ? ? 1 5 1 ?1 ? 2.0
2 ? ? ? ? ? 1 5 2 ?1 ? 2.0
3 ? ? ? ? ? 1 5 1 ?2 ? 1.5
4 ? ? ? ? ? 1 5 2 ?2 ? 1.5
5 ? ? ? ? ? 1 5 1 ?3 ? 0.0
6 ? ? ? ? ? 1 5 2 ?3 ? 0.0
7 ? ? ? ? ? 1 5 1 ?4 ? 0.0
8 ? ? ? ? ? 1 5 2 ?4 ? 1.0
9 ? ? ? ? ? 1 5 1 ?5 ? 1.0
10 ? ? ? ? ?1 5 2 ?5 ? 2.0
.. ? ? ? ?... . . .. ? ...
I want to add a probability column, ?the prob column depends on id grouped by for each i
the rank will be current (value / max value ) for the same id for specific i, it would be

?Measure_id i j id value ? ?prob
1 ? ? ? ? ? 1 5 1 ?1 ? 2.0 ? ? ? ? ?2/2 ?
2 ? ? ? ? ? 1 5 2 ?1 ? 2.0 ? ? ? ? ?2/2 ?
3 ? ? ? ? ? 1 5 1 ?2 ? 1.5 ? ? ? ? ?1.5/1.5 ??
4 ? ? ? ? ? 1 5 2 ?2 ? 1.5 ? ? ? ? ?1.5/1.5?
5 ? ? ? ? ? 1 5 1 ?3 ? 0.0 ? ? ? ? ?0
6 ? ? ? ? ? 1 5 2 ?3 ? 0.0 ? ? ? ? ?0
7 ? ? ? ? ? 1 5 1 ?4 ? 0.0 ? ? ? ? ?0/1 ?
8 ? ? ? ? ? 1 5 2 ?4 ? 1.0 ? ? ? ? ?1/1 ?
9 ? ? ? ? ? 1 5 1 ?5 ? 1.0 ? ? ? ? ?1/2
10 ? ? ? ? ?1 5 2 ?5 ? 2.0 ? ? ? ? ?2/3
.. ? ? ? ?... . . .. ? ...

then I want to add a rank column that rank regarding probability, if the probability equal they took the same rank for the
same id belongs to the same i, otherwize lower probability took higher rank for examole if we have three values for i=7 and for the three values the id is 1 and the probability is ( .2,.4,.5) the rank should be 3,2,1



I looked at aggregate and dplyr...should I use for loop and subset each i and id rows do calculations and then group them again ??
is there easier way?

replying ?highly appreciated
> 
> 
> 
>> Date: Sun, 6 Sep 2015 19:02:02 -0400 
>> Subject: Re: [R] groups Rank 
>> From: sarah.goslee at gmail.com 
>> To: ragia11 at hotmail.com 
>> CC: r-help at r-project.org 
>> 
>> Please use dput() to provide data, rather than expecting people to 
>> open random attachments. Besides, there are multiple options for 
>> getting data into R, and we need to know exactly what you did. dput() 
>> is faster and easier. 
>> 
>> What have you tried? Did you look at aggregate() as I suggested? 
>> 
>> Sarah 
>> 
>> On Sat, Sep 5, 2015 at 10:44 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote: 
>>> thanks for replying, I attached the data frame 
>>> for source "i" I want to sum the values and get the max value then add a 
>>> new column called rank . That new column cell value for each source i and 
>>> for specific id would be (value/max value) * count of rows that have the 
>>> same criteria "same i and same id" 
>>> 
>>> many thanks 
>>> Ragia 
>>> 
>>>> Date: Fri, 4 Sep 2015 10:19:35 -0400 
>>>> Subject: Re: [R] groups Rank 
>>>> From: sarah.goslee at gmail.com 
>>>> To: ragia11 at hotmail.com 
>>>> CC: r-help at r-project.org 
>>>> 
>>>> Hi Ragia, 
>>>> 
>>>> I can't make out your data or desired result, but it sounds like 
>>>> aggregate() might get you started. If you need more help, please 
>>>> repost your data using dput() and do NOT post in HTML so that we can 
>>>> see what your desired result looks like. 
>>>> 
>>>> Sarah 
>>>> 
>>>> On Fri, Sep 4, 2015 at 10:12 AM, Ragia Ibrahim <ragia11 at hotmail.com> 
>>>> wrote: 
>>>>> Dear Group,kinldy, I have the following data frame df 
>>>>> id value1 1 4 2 1 4 3 1 6 4 1 6 5 2 1.5 6 2 2.5 7 2 2.5 8 2 2.5 
>>>>> 
>>>>> add rank column regarding id coulmn where rank for the highest value 
>>>>> would be 1, others rank would be the (value/ value of heighest)/ 
> number of 
>>>>> rows that took the same value 
>>>>> thus the data frame should be 
>>>>> id value Rank1 1 4 0.332 1 4 0.333 1 6 0.54 1 6 0.55 2 1.5 0.6 6 2 2.5 
>>>>> 0.337 2 2.5 0.338 2 2.5 0.33 
>>>>> 
>>>>> how to reach this resultthanks in advanceRagia 
>>>>> [[alternative HTML version deleted]] 
>>>>> 
 		 	   		  

From petr.pikal at precheza.cz  Mon Sep  7 11:29:21 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 7 Sep 2015 09:29:21 +0000
Subject: [R] groups Rank
In-Reply-To: <DUB125-W800D17CA67EC7748B3503CB3540@phx.gbl>
References: <DUB125-W10C5D212C9BD3CF6775318B3A00@phx.gbl>,
	<DUB125-W91C972100E24493CABA989B3700@phx.gbl>,
	<DUB125-W49BC987BD7C478DCA7220FB3700@phx.gbl>,
	<DUB125-W12FF665BD9DF1AD4630B40B3570@phx.gbl>,
	<CAM_vjunHM8iAHisTytkn-EWjq42V4oo0NqOtQCbL3RvmXLqsZA@mail.gmail.com>,
	<DUB125-W27A4D5CE21C3CF6038C503B3560@phx.gbl>,
	<CAM_vju=FpeLo-fmhdk2vJ8thmxeAbcHhKXwk9yOWYDuXnsrL+A@mail.gmail.com>,
	<DUB125-W330E69EE3D349200682C6CB3540@phx.gbl>
	<DUB125-W800D17CA67EC7748B3503CB3540@phx.gbl>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E800@SRVEXCHMBX.precheza.cz>

Hi

OK, thanks for sending dput result.

I am still not sure what exactly you want. Using ?ave you can get result of x/max(x)

dat$prob <- ave(dat$value, paste(dat$id, dat$i), FUN= function(x) x/max(x))

however in case max(x) is zero the result is NA

You can change it to zero

dat$prob[is.nan(dat$prob)] <- 0

and compute rank value by similar process.

dat$rankvalue <- ave(dat$prob, paste(dat$id, dat$i), FUN = rank)

But I am not sure if this is the desired result.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ragia
> Ibrahim
> Sent: Monday, September 07, 2015 10:12 AM
> To: Sarah Goslee; r-help at r-project.org
> Subject: Re: [R] groups Rank
>
> apology for re sending the Email, I changed the format to plain text as
> I have been advised the data  is as follow
>
> thanks Sarah,
> I used pdut, and here is the data as written on R..I attached the dput
> result structure(list(Measure_id = c(1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3,
> 3, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3,
> 3, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3), i = c(5, 5, 5, 5, 5, 5, 5, 5,
> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7,
> 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7), j = c(1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3,
> 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5), id = c(1, 2,
> 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
> 11, 12), value = c(2, 1.5, 0, 0, 1, 0.5, 0, 0, 0, 0, 0.5, 2, 2, 1.5, 0,
> 1, 2, 0, 0.5, 1.44269504088896, 0, 0, 0, 0, 1, 1.5, 0, 0, 1, 0, 0, 0,
> 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)), .Names =
> c("Measure_id", "i", "j", "id", "value"), row.names = c(NA, 48L), class
> = "data.frame")
>
> the data  is as follow  :
>
>    Measure_id i j id value
> 1           1 5 1  1   2.0
> 2           1 5 2  1   2.0
> 3           1 5 1  2   1.5
> 4           1 5 2  2   1.5
> 5           1 5 1  3   0.0
> 6           1 5 2  3   0.0
> 7           1 5 1  4   0.0
> 8           1 5 2  4   1.0
> 9           1 5 1  5   1.0
> 10          1 5 2  5   2.0
> ..        ... . . ..   ...
> I want to add a probability column,  the prob column depends on id
> grouped by for each i the rank will be current (value / max value ) for
> the same id for specific i, it would be
>
>  Measure_id i j id value    prob
> 1           1 5 1  1   2.0          2/2
> 2           1 5 2  1   2.0          2/2
> 3           1 5 1  2   1.5          1.5/1.5
> 4           1 5 2  2   1.5          1.5/1.5
> 5           1 5 1  3   0.0          0
> 6           1 5 2  3   0.0          0
> 7           1 5 1  4   0.0          0/1
> 8           1 5 2  4   1.0          1/1
> 9           1 5 1  5   1.0          1/2
> 10          1 5 2  5   2.0          2/3
> ..        ... . . ..   ...
>
> then I want to add a rank column that rank regarding probability, if
> the probability equal they took the same rank for the same id belongs
> to the same i, otherwize lower probability took higher rank for examole
> if we have three values for i=7 and for the three values the id is 1
> and the probability is ( .2,.4,.5) the rank should be 3,2,1
>
>
>
> I looked at aggregate and dplyr...should I use for loop and subset each
> i and id rows do calculations and then group them again ??
> is there easier way?
>
> replying  highly appreciated
> >
> >
> >
> >> Date: Sun, 6 Sep 2015 19:02:02 -0400
> >> Subject: Re: [R] groups Rank
> >> From: sarah.goslee at gmail.com
> >> To: ragia11 at hotmail.com
> >> CC: r-help at r-project.org
> >>
> >> Please use dput() to provide data, rather than expecting people to
> >> open random attachments. Besides, there are multiple options for
> >> getting data into R, and we need to know exactly what you did.
> dput()
> >> is faster and easier.
> >>
> >> What have you tried? Did you look at aggregate() as I suggested?
> >>
> >> Sarah
> >>
> >> On Sat, Sep 5, 2015 at 10:44 AM, Ragia Ibrahim <ragia11 at hotmail.com>
> wrote:
> >>> thanks for replying, I attached the data frame for source "i" I
> want
> >>> to sum the values and get the max value then add a new column
> called
> >>> rank . That new column cell value for each source i and for
> specific
> >>> id would be (value/max value) * count of rows that have the same
> >>> criteria "same i and same id"
> >>>
> >>> many thanks
> >>> Ragia
> >>>
> >>>> Date: Fri, 4 Sep 2015 10:19:35 -0400
> >>>> Subject: Re: [R] groups Rank
> >>>> From: sarah.goslee at gmail.com
> >>>> To: ragia11 at hotmail.com
> >>>> CC: r-help at r-project.org
> >>>>
> >>>> Hi Ragia,
> >>>>
> >>>> I can't make out your data or desired result, but it sounds like
> >>>> aggregate() might get you started. If you need more help, please
> >>>> repost your data using dput() and do NOT post in HTML so that we
> >>>> can see what your desired result looks like.
> >>>>
> >>>> Sarah
> >>>>
> >>>> On Fri, Sep 4, 2015 at 10:12 AM, Ragia Ibrahim
> >>>> <ragia11 at hotmail.com>
> >>>> wrote:
> >>>>> Dear Group,kinldy, I have the following data frame df id value1 1
> >>>>> 4 2 1 4 3 1 6 4 1 6 5 2 1.5 6 2 2.5 7 2 2.5 8 2 2.5
> >>>>>
> >>>>> add rank column regarding id coulmn where rank for the highest
> >>>>> value would be 1, others rank would be the (value/ value of
> >>>>> heighest)/
> > number of
> >>>>> rows that took the same value
> >>>>> thus the data frame should be
> >>>>> id value Rank1 1 4 0.332 1 4 0.333 1 6 0.54 1 6 0.55 2 1.5 0.6 6
> 2
> >>>>> 2.5
> >>>>> 0.337 2 2.5 0.338 2 2.5 0.33
> >>>>>
> >>>>> how to reach this resultthanks in advanceRagia [[alternative HTML
> >>>>> version deleted]]
> >>>>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From drjimlemon at gmail.com  Mon Sep  7 12:23:48 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 7 Sep 2015 20:23:48 +1000
Subject: [R] split.screen to draw graphs - ggplot2 and lattice (can't
 slip in 4 cells)
In-Reply-To: <C14B6C56-82E6-4300-B056-647B0AD74A35@gmail.com>
References: <C14B6C56-82E6-4300-B056-647B0AD74A35@gmail.com>
Message-ID: <CA+8X3fX4ti1cDDNyovhCXhjobp6yrRzEOU5sRcTUFs60mqbSaA@mail.gmail.com>

Hi Rosa,
I think all you need is the split.screen commands but this will show you
where each screen number is located:

split.screen(figs=matrix(c(0,0.8,0,1,0.8,1,0,1),ncol=4,byrow=TRUE))
split.screen(figs=matrix(c(0,0.5,0.5,1,0.5,1,0.5,1,0,0.5,0,0.5,0.5,1,0,0.5),
 ncol=4,byrow=TRUE),screen=1)
screen(3)
plot(0:1,0:1,type="n",xlab="",ylab="",axes=FALSE)
box()
text(0.5,0.5,3)
screen(4)
plot(0:1,0:1,type="n",xlab="",ylab="",axes=FALSE)
box()
text(0.5,0.5,4)
screen(5)
plot(0:1,0:1,type="n",xlab="",ylab="",axes=FALSE)
box()
text(0.5,0.5,5)
screen(6)
plot(0:1,0:1,type="n",xlab="",ylab="",axes=FALSE)
box()
text(0.5,0.5,6)
screen(2)
plot(0:1,0:1,type="n",xlab="",ylab="",axes=FALSE)
box()
text(0.5,0.5,2)
close.screen(all=TRUE)

I used width=10,height=6 in the device call.

Jim

On Mon, Sep 7, 2015 at 12:06 AM, Rosa Oliveira <rosita21 at gmail.com> wrote:

> Dear all,
>
>
>
> I need your urgent help J
>
>
>
> I?m na?ve, and I?m pretty sure my doubt is very simple to solve, but I?m
> not getting it.
>
>
>
> I used the following code to produce my research graphs, nonetheless, is
> this problem, I do not have 6 graphs (1 ? 6),
>
>
>
> #   3   4   5
>
> #                2
>
> #   6   7   8
>
>
>
>
>
> but only 4,instead.  So, I need to reformulate the code, just to
>
>
>
> #   3   4
>
> #                2
>
> #   6   7
>
>
>
> Can you please help me reformulating?
>
> I suppose I have to change something in the split.screen code, because
> nowadays, my ?third? column is blank J
>
>
>
>
>
> I attach the code:
>
>
>
> library(ggplot2)
>
> library(reshape)
>
> library(lattice)
>
>
>
> mean.alpha1<-read.csv("graphs_mean_alpha1.csv")
>
> mean.alpha2<-read.csv("graphs_mean_alpha2.csv")
>
> quartz(width=10,height=5)
>
> split.screen(figs=matrix(c(0,0.4,0.5,1,
>
>                            0.4,0.7,0.5,1,
>
>                            0.7,1,0.5,1,
>
>                            0,0.4,0,0.5,
>
>                            0.4,0.7,0,0.5,
>
>                            0.7,1,0,0.5),nrow=6,byrow=TRUE),
>
>              screen=1)
>
>
>
>
>
>
>
> screen(3)
>
> par(mar=c(0,3.5,3,0))
>
> # now the second set
>
> n250<-mean.alpha1$nsample==250
>
> matplot(x=mean.alpha1$lambda[n250],y=mean.alpha1[n250,3:5],
>
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-1.2,
> -0.25),main="nsample=250",ylab="", cex.main=1)
>
> abline(h = -1, col = "gray60")
>
> mtext(expression(paste("mean av. for  ",alpha[1])),side=2,line=2,
> cex.main=1)
>
>
>
>
>
>
>
> screen(4)
>
> par(mar=c(0,0,3,0))
>
> # now the second set
>
> n250<-mean.alpha1$nsample==1000
>
> matplot(x=mean.alpha1$lambda[n1000],y=mean.alpha2[n1000,3:5],
>
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-1.2,
> -0.25),main="nsample=1000", cex.main=1)
>
> abline(h = -1, col = "gray60")
>
>
>
>
>
>
>
> screen(6)
>
> par(mar=c(3,3.5,0,0))
>
> # now the second set
>
> n250<-mean.alpha2$nsample==250
>
> matplot(x=mean.alpha2$lambda[n250],y=mean.alpha2[n250,3:5],
>
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.6, -.35),ylab="")
>
> abline(h = -.5, col = "gray60")
>
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>
> mtext(expression(paste("mean av. for  ",alpha[2])),side=2,line=2,
> cex.main=1.5)
>
>
>
>
>
>
>
> screen(7)
>
> par(mar=c(3,0,0,0))
>
> # now the second set
>
> n1000<-mean.alpha2$nsample==1000
>
> matplot(x=mean.alpha2$lambda[n1000],y=mean.alpha2[n1000,3:5],
>
> type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.6, -.35))
>
> abline(h = -.5, col = "gray60")
>
> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
>
>
>
>
>
> screen(2)
>
> par(mar=c(0,0,0,0))
>
> # plot an empty plot to get the coordinates
>
> plot(0:1,0:1,type="n",axes=FALSE)
>
> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "true coefficient"),bty = "n",
> lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
>
>
>
>
>
> close.screen(all=TRUE)
>
>
>
>
>
> BEST,
>
> RO
>
>
>
>
>
> Atenciosamente,
> Rosa Oliveira
>
> --
>
> ____________________________________________________________________________
>
>
> Rosa Celeste dos Santos Oliveira,
>
> E-mail: rosita21 at gmail.com
> Tlm: +351 939355143
> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>
> ____________________________________________________________________________
> "Many admire, few know"
> Hippocrates
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From cjendres1 at gmail.com  Fri Sep  4 16:42:30 2015
From: cjendres1 at gmail.com (Christopher Endres)
Date: Fri, 4 Sep 2015 10:42:30 -0400
Subject: [R] [R-pkgs] nhanesA - easy retrieval and import of NHANES data
Message-ID: <CA+uw+qAeXzVP4dMY9AZyT=8_w4V0RiCssLLFBDZiyWYigEVGQQ@mail.gmail.com>

Dear R enthusiasts, I would like to announce nhanesA, a package that
enables easy retrieval of the data tables that are available at the
National Health and Nutritional Examination Survey (NHANES).
NHANES data are used in over 10,000 peer-reviewed journal publications
every year. In addition to easy table download, nhanesA features several
functions that implement web scraping (using rvest) to display table names,
table variables, and table code translations.
For more information, Here's a link to the package:
https://cran.r-project.ohg/web/packages/nhanesA/
<https://cran.r-project.org/web/packages/nhanesA/>
install.packages("nhanesA") will get you started.

Hopefully it will be useful for clinical research and general data
exploration.

Sincerely,
Christopher Endres

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From Luis.Diaz at tecnocom.es  Mon Sep  7 11:30:36 2015
From: Luis.Diaz at tecnocom.es (Diaz Garcia, Luis Carlos)
Date: Mon, 7 Sep 2015 09:30:36 +0000
Subject: [R] get data from pl sql block
Message-ID: <DB5PR03MB12535A071DB138340CDAEE62EB540@DB5PR03MB1253.eurprd03.prod.outlook.com>

Hello all

last week I create a script with R
This script connect to Oracle database and retreave some data.

This is a sample of the code

dbName <- sqlQuery(con, "SELECT instance_name, host_name from v$instance",errors=FALSE)
title (main = paste0("Mapa de los dblinks del entorno: ", dbName$INSTANCE_NAME, "_", 
       dbName$HOST_NAME),sub="Luis Diaz - Emergencies & improvments")

This code works fine, but now I need to get data from a pl sql block, such like this:

	DECLARE
	v_result number;

	BEGIN
	EXECUTE IMMEDIATE 'select count(1) from dual at db_link'';
	v_result:=0; 
	DBMS_OUTPUT.PUT_LINE(v_result);
	
	EXCEPTION 
	WHEN OTHERS THEN 
	v_result:=1; 
	DBMS_OUTPUT.PUT_LINE(v_result);

	END; 
/

This code return 0 if the db link works and 1 if not...
I try tis way:

isDead <- sqlQuery(con,"
	set serveroutput on
	DECLARE
	v_result number;

	BEGIN
	EXECUTE IMMEDIATE 'select count(1) from dual at DB_LINK';
	v_result:=0; 
	EXCEPTION 
	WHEN OTHERS THEN 
	v_result:=1; 
END; 
/
", errors=FALSE)
print(isDead)

The result of this isDead variable is always : -1
I expect a value 0 or 1 depending of the db link result.
Do you have any idea ?

Thanks a los


Tecnocom
Luis Diaz
Arquitecto Bases de Datos Oracle
Email: luis.diaz at tecnocom.es
http://www.tecnocom.es


From wewolski at gmail.com  Mon Sep  7 12:34:53 2015
From: wewolski at gmail.com (Witold E Wolski)
Date: Mon, 7 Sep 2015 12:34:53 +0200
Subject: [R] names in R list's
Message-ID: <CAAjnpdgHgYG3xUOECypMOV4aP6M=R6DgDuqmDmJJ6wm79fzjcQ@mail.gmail.com>

What is the access time for R lists given a name of list element, is it
linear, log, or constant?

Than what are to rules for names in R-lists

That reusing names is possible makes me wonder.

tmp <- as.list(c(1,2,3,4))
names(tmp) = c("a","a","b","b")
tmp
tmp$a


What I am looking for is a standard R data structure which will allow me
for fast and name lookup.




-- 
Witold Eryk Wolski

	[[alternative HTML version deleted]]


From janka.vanschoenwinkel at uhasselt.be  Mon Sep  7 13:18:25 2015
From: janka.vanschoenwinkel at uhasselt.be (Janka VANSCHOENWINKEL)
Date: Mon, 7 Sep 2015 13:18:25 +0200
Subject: [R] Lag variable by group
Message-ID: <CAHymutLSjzMxVdKSA0eJ8DvYXfXUY1Zn_obcf8_VSHuc_=-CHA@mail.gmail.com>

Hi!

I have the following dataset with the variables ID (this is a unique ID per
farmer), year, and another variable t1.
I now would like to have a fourth variable which is the lag value of t1 for
each farm ID.

I found a code on the internet that does exactly what I need, but it does
not work for this dataset. Does anyone have suggestions about how I can
make this work?

Thanks a lot!

Janka

data<-structure(list(year = c(2007, 2005, 2008, 2006, 2005, 2007, 2006,
2008, 2007, 2005, 2007, 2007, 2005, 2006, 2005, 2006, 2005, 2006,
2007, 2007, 2005, 2008, 2007, 2008, 2005, 2005, 2006, 2008, 2007,
2007, 2008, 2008, 2006, 2005, 2007, 2006, 2008, 2008, 2007, 2007,
2007, 2006, 2006, 2008, 2006, 2008, 2008, 2008, 2006, 2007, 2008,
2007, 2005, 2007, 2008, 2005, 2007, 2005, 2005, 2008, 2005, 2006,
2005, 2006, 2008, 2006, 2008, 2006, 2007, 2006, 2005, 2008, 2006,
2007, 2008, 2006, 2006, 2006, 2005, 2008, 2006, 2008, 2006, 2006,
2006, 2007, 2008, 2005, 2007, 2006, 2007, 2008, 2006, 2008, 2005,
2007, 2005, 2007, 2006, 2006), id = c(28958L, 28962L,
28962L, 28965L, 28960L, 28962L, 28964L, 28970L, 28961L, 28965L,
78458L, 28960L, 28961L, 28961L, 28969L, 28962L, 28959L, 28959L,
58845L, 28965L, 28963L, 78459L, 28967L, 28957L, 28964L, 28966L,
28958L, 28960L, 28969L, 28959L, 28958L, 28969L, 58845L, 28958L,
28954L, 28963L, 78458L, 28965L, 28966L, 28963L, 28970L, 28970L,
28960L, 28959L, 28954L, 28954L, 58845L, 28967L, 28966L, 78459L,
28956L, 28964L, 28956L, 28957L, 28961L, 28970L, 28968L, 28954L,
28955L, 28968L, 28968L, 28967L, 28967L, 28957L, 28966L, 28956L,
28964L, 28969L, 28955L, 28955L, 28957L, 28955L, 28968L, 28956L,
28963L, 29004L, 58848L, 29005L, 28974L, 29005L, 28974L, 29006L,
28981L, 29007L, 29002L, 28980L, 29001L, 29006L, 29005L, 28989L,
28989L, 58846L, 28980L, 28981L, 78467L, 28990L, 28973L, 29004L,
28972L, 29006L), t1 = c(-1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513, -1.43884992599487, -1.43884992599487, -1.43884992599487,
-1.43884992599487, -1.43884992599487, -1.43884992599487, -1.43884992599487,
-1.43884992599487, -1.43884992599487, -1.43884992599487, -1.43884992599487,
-1.43884992599487, -1.43884992599487, -1.43884992599487, -1.43884992599487,
-1.43884992599487, -1.43884992599487, -1.43884992599487, -1.43884992599487,
-1.43884992599487, -1.43884992599487, -1.43884992599487, -1.43884992599487,
-1.43884992599487, -1.43884992599487)), .Names = c("year", "id",
"t1"), row.names = c(NA, 100L), class = "data.frame")

library(data.table)
data[, lag.t1:=c(NA, t1[-.N]), by=id]


Thank you very much!

Janka

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Sep  7 13:34:20 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 7 Sep 2015 11:34:20 +0000
Subject: [R] scaling loess curves
In-Reply-To: <CA+JEM03QfeZeA4ResFMS+2bcCJMxR9o0zjRj+m8sFBiqjEEiLg@mail.gmail.com>
References: <CA+JEM03QfeZeA4ResFMS+2bcCJMxR9o0zjRj+m8sFBiqjEEiLg@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E8AE@SRVEXCHMBX.precheza.cz>

Hi

what about xlim or ylim?

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bogdan
> Tanasa
> Sent: Monday, September 07, 2015 8:00 AM
> To: r-help
> Subject: [R] scaling loess curves
>
> Dear all,
>
> please could you advise about a method to scale 2 plots of LOESS
> curves.
> More specifically, we do have 2 sets of 5C data, and the loess plots
> reflect the relationship between INTENSITY and DISTANCE (please see the
> R code below).
>
> I am looking for a method/formula to scale these 2 LOESS plots and make
> them directly comparable.
>
> many thanks,
>
> -- bogdan
>
>
>
> -------------- the R code ------------------
>
>
>
> a <- read.delim("a",header=T)
> qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size =
> 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
>
>
> b <- read.delim("b",header=T)
> qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size =
> 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From b.rowlingson at lancaster.ac.uk  Mon Sep  7 15:25:45 2015
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 7 Sep 2015 14:25:45 +0100
Subject: [R] names in R list's
In-Reply-To: <c286e75f74a049128f6f4ae207bf4f0c@EX-1-HT0.lancs.local>
References: <c286e75f74a049128f6f4ae207bf4f0c@EX-1-HT0.lancs.local>
Message-ID: <CANVKczMxV2MY+emcKLvNdS3iPmMcwaxnP3qk9P_=7j5_ntiYrA@mail.gmail.com>

On Mon, Sep 7, 2015 at 11:34 AM, Witold E Wolski <wewolski at gmail.com> wrote:
> What is the access time for R lists given a name of list element, is it
> linear, log, or constant?

 Try it and see?

> Than what are to rules for names in R-lists
>
> That reusing names is possible makes me wonder.
>
> tmp <- as.list(c(1,2,3,4))
> names(tmp) = c("a","a","b","b")
> tmp
> tmp$a
>
>
> What I am looking for is a standard R data structure which will allow me
> for fast and name lookup.

 Depends what you mean by "standard"?

 There's a `hash` package that implements what are variously known as
hash tables or associative arrays (in perl) and dictionaries (in
Python)

 > require(hash)
 > z=hash("a",99)
 > z
<hash> containing 1 key-value pair(s).
  a : 99
 > z$b=123

There's no ordering so numeric indexing fails:

 > z[1]
Error in get(k, x) : invalid first argument
 > z[[1]]
Error in z[[1]] : wrong arguments for subsetting an environment
 > z
<hash> containing 2 key-value pair(s).
  a : 99
  b : 123

and the keys are unique, even if you try:

 > z=hash(c("a","b","b","c"), 1:4)
 > z
 <hash> containing 3 key-value pair(s).
  a : 1
  b : 3
  c : 4

I guess the lookup uses the usual fast hash lookup algorithms, but
you'd have to check the docs and source for details.

Barry


From anassaldanha at gmail.com  Mon Sep  7 12:37:23 2015
From: anassaldanha at gmail.com (Sofia Saldanha)
Date: Mon, 07 Sep 2015 10:37:23 +0000
Subject: [R] Help
Message-ID: <CAD3K6dQdsg9CjDT9J2W5G5JHhZXPhuFUvFcXops2GpCmi5Bjjw@mail.gmail.com>

Hi,
I started using R recently and everything was going perfect until I stoped
seeing my output results in the R console.
Now, instead of showing ">" on the console before the command, it shows
"+". It doesn't even close the program now. I'm using the 64bit 3.2.2
version.
E.g.:
+ 1
+ q()

Thanks!

	[[alternative HTML version deleted]]


From randolf at balasus.eu  Mon Sep  7 13:19:20 2015
From: randolf at balasus.eu (rbalasus)
Date: Mon, 7 Sep 2015 04:19:20 -0700 (PDT)
Subject: [R] values in date format from .xls as label on x-line
Message-ID: <1441624760027-4711949.post@n4.nabble.com>


hello , I am very new to R, till now I was working with lineplots reading
their data from .xls files. All was working as expected, 
now I want to label the data-values with "date"-values on the x line. But I
have absolute no clue how to read and interprete these values as date-values
not just only integer, I was testing and searching solutions with the
convert function. I hop I just  need only one or two modifications in my
code, and all is working. Any Ideas?


this is my testcode, and the example values in the .xls, (with integer
values, all is working correctly.)


textcode.r
require(XLConnect)
wb = loadWorkbook("C:/xlsfile.xls")
impdata = readWorksheet(wb, sheet = "Data", header = TRUE)
datec(c)  <- as.Date(impdata$date, "%d.%m.%y")
plot(datec,impdata$amount, main="amount year to date",type="n", ylab="Eur",
xlab="year to date",ylim=c(1,6000))
lines(datec,impdata$amountmount, col = "steelblue")

xls-file
date        amount
30.01.2015  1000
30.02.2015  2000
15.03.2015  2800

THANX in advance for your help.





--
View this message in context: http://r.789695.n4.nabble.com/values-in-date-format-from-xls-as-label-on-x-line-tp4711949.html
Sent from the R help mailing list archive at Nabble.com.


From susrutha.gongalla at gmail.com  Mon Sep  7 14:34:47 2015
From: susrutha.gongalla at gmail.com (Susrutha Gongalla)
Date: Mon, 7 Sep 2015 08:34:47 -0400
Subject: [R] Maxent package - Restriction on number of unique class labels
Message-ID: <CAFH1dhXc==0g+3QPXHwK8LiAptBjvPWnvoZfpqNTUnFfEptUgg@mail.gmail.com>

Hi everyone,

I am developing a text classification code in R using 'maxent' package. I
see that there is a limit on number of unique class labels that can be
modeled, which is 255. Can someone please help me understand why this limit
is present?
The data I am working on has about 400 labels (data consists of thousands
of records). Can anybody please suggest which package to use to be able to
achieve maximum entropy based text classification on this?
I have tried using 'RTextTools' package as well. It seems to be internally
using 'maxent' package in return and  gives me the same error.

Would really appreciate any help!

Thanks.

Susrutha Gongalla

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Sep  7 16:06:16 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 7 Sep 2015 14:06:16 +0000
Subject: [R] values in date format from .xls as label on x-line
In-Reply-To: <1441624760027-4711949.post@n4.nabble.com>
References: <1441624760027-4711949.post@n4.nabble.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E940@SRVEXCHMBX.precheza.cz>

Hi

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> rbalasus
> Sent: Monday, September 07, 2015 1:19 PM
> To: r-help at r-project.org
> Subject: [R] values in date format from .xls as label on x-line
>
>
> hello , I am very new to R, till now I was working with lineplots
> reading their data from .xls files. All was working as expected, now I
> want to label the data-values with "date"-values on the x line. But I
> have absolute no clue how to read and interprete these values as date-
> values not just only integer, I was testing and searching solutions
> with the convert function. I hop I just  need only one or two
> modifications in my code, and all is working. Any Ideas?
>
>
> this is my testcode, and the example values in the .xls, (with integer
> values, all is working correctly.)
>
>
> textcode.r
> require(XLConnect)
> wb = loadWorkbook("C:/xlsfile.xls")
> impdata = readWorksheet(wb, sheet = "Data", header = TRUE)
> datec(c)  <- as.Date(impdata$date, "%d.%m.%y")
^^^^^^^^^^^
What is this? If you meant

datec  <- as.Date(impdata$date, "%d.%m.%y")

datec shall be class Date, you can check it by

class(datec)

Maybe there shall be class mentioned on help page for mode and/or typeof  in See Also section.

As you did not provide any data here is toy example

datum <- structure(c(16359, 16361, 16362, 16363, 16364, 16365, 16366,
16367, 16368), class = "Date")

plot(datum, 1:9)

gives you a plot with x axes labeled by date. You can change format of x axis

plot(datum, 1:9, format="%d.%m.")

which gives you expected formatting although it issues set of warnings. I did not found any plotting method for Date class although it (probably) works as expected.

Cheers
Petr

> plot(datec,impdata$amount, main="amount year to date",type="n",
> ylab="Eur",
> xlab="year to date",ylim=c(1,6000))
> lines(datec,impdata$amountmount, col = "steelblue")
>
> xls-file
> date        amount
> 30.01.2015  1000
> 30.02.2015  2000
> 15.03.2015  2800
>
> THANX in advance for your help.
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/values-in-
> date-format-from-xls-as-label-on-x-line-tp4711949.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From petr.pikal at precheza.cz  Mon Sep  7 16:09:59 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 7 Sep 2015 14:09:59 +0000
Subject: [R] Help
In-Reply-To: <CAD3K6dQdsg9CjDT9J2W5G5JHhZXPhuFUvFcXops2GpCmi5Bjjw@mail.gmail.com>
References: <CAD3K6dQdsg9CjDT9J2W5G5JHhZXPhuFUvFcXops2GpCmi5Bjjw@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E953@SRVEXCHMBX.precheza.cz>

Hi

You probably already found what is wrong. You entered syntactically correct expression but without proper closing part. R expects this closing part of an expression. You can stop it by menu Misc/Stop all computation or simply pressing ESC.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sofia
> Saldanha
> Sent: Monday, September 07, 2015 12:37 PM
> To: r-help at r-project.org
> Subject: [R] Help
>
> Hi,
> I started using R recently and everything was going perfect until I
> stoped seeing my output results in the R console.
> Now, instead of showing ">" on the console before the command, it shows
> "+". It doesn't even close the program now. I'm using the 64bit 3.2.2
> version.
> E.g.:
> + 1
> + q()
>
> Thanks!
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From jdnewmil at dcn.davis.CA.us  Mon Sep  7 16:37:05 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 07 Sep 2015 07:37:05 -0700
Subject: [R] names in R list's
In-Reply-To: <CAAjnpdgHgYG3xUOECypMOV4aP6M=R6DgDuqmDmJJ6wm79fzjcQ@mail.gmail.com>
References: <CAAjnpdgHgYG3xUOECypMOV4aP6M=R6DgDuqmDmJJ6wm79fzjcQ@mail.gmail.com>
Message-ID: <2D0B5742-2595-4837-8371-BF5895C6B6D2@dcn.davis.CA.us>

You puzzle me. Why does someone who cannot figure out how to post an email in plain text after so many messages on this mailing list get all worried about access time for string indexing?

Environment objects have those properties. They do not solve all problems though, because they are rather heavyweight... you need a lot of lookups to pay for their overhead. R5 objects and the hash package both use them, but I have never found three need to use them. Yes, I do program in Perl so I know where you are coming from, but the vector-based name lookup used in R works quite effectively for data where the number of list items is short or where I plan to access every element as part of my data processing anyway.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 7, 2015 3:34:53 AM PDT, Witold E Wolski <wewolski at gmail.com> wrote:
>What is the access time for R lists given a name of list element, is it
>linear, log, or constant?
>
>Than what are to rules for names in R-lists
>
>That reusing names is possible makes me wonder.
>
>tmp <- as.list(c(1,2,3,4))
>names(tmp) = c("a","a","b","b")
>tmp
>tmp$a
>
>
>What I am looking for is a standard R data structure which will allow
>me
>for fast and name lookup.


From dcarlson at tamu.edu  Mon Sep  7 16:41:09 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 7 Sep 2015 14:41:09 +0000
Subject: [R] extracting every nth character from a string...
In-Reply-To: <CAP01uRk10oVH1168G2fGcGGVr27Ak44CtEWk5UxAnQwSLJ781Q@mail.gmail.com>
References: <55EB57CA.2090909@gmail.com>
	<CAP01uRk10oVH1168G2fGcGGVr27Ak44CtEWk5UxAnQwSLJ781Q@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6BDFEB@mb02.ads.tamu.edu>

No rex, but not much less complicated, than your original but a different approach:

> i <- seq(1, nchar(str), 2)
> paste0(mapply(substr, str, i, i), collapse="")
[1] "ACEG"

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Gabor Grothendieck
Sent: Sunday, September 6, 2015 10:27 AM
To: Evan Cooch
Cc: r-help at r-project.org
Subject: Re: [R] extracting every nth character from a string...

This uses a regular expression but is shorter:

> gsub("(.).", "\\1", "ABCDEFG")
[1] "ACEG"

It replaces each successive pair of characters with the first of that
pair.  If there is an odd number of characters then the last character is
not matched and therefore kept -- thus it works properly for both even and
odd.


On Sat, Sep 5, 2015 at 4:59 PM, Evan Cooch <evan.cooch at gmail.com> wrote:

> Suppose I had the following string, which has length of integer multiple
> of some value n. So, say n=2, and the example string has a length of  (2x4)
> = 8 characters.
>
> str <- "ABCDEFGH"
>
> What I'm trying to figure out is a simple, base-R coded way (which I
> heuristically call StrSubset in the following) to extract every nth
> character from the string, to generate a new string.
>
> So
>
> str <- "ABCDEFGH"
>
> new_str <- StrSubset(str);
>
> print(new_str)
>
> which would yield
>
> "ACEG"
>
>
> Best I could come up with is something like the following, where I extract
> every odd character from the string:
>
> StrSubset <- function(string)
>       {
> paste(unlist(strsplit(string,""))[seq(1,nchar(string),2)],collapse="") }
>
>
> Anything more elegant come to mind? Trying to avoid regex if possible
> (harder to explain to end-users), but if that meets the 'more elegant'
> sniff test, happy to consider...
>
> Thanks in advance...
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Mon Sep  7 16:53:57 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 07 Sep 2015 07:53:57 -0700
Subject: [R] get data from pl sql block
In-Reply-To: <DB5PR03MB12535A071DB138340CDAEE62EB540@DB5PR03MB1253.eurprd03.prod.outlook.com>
References: <DB5PR03MB12535A071DB138340CDAEE62EB540@DB5PR03MB1253.eurprd03.prod.outlook.com>
Message-ID: <9F5B911F-3D00-4843-9A65-CC33A49E0F51@dcn.davis.CA.us>

You will get better answers on the R-sig-db mailing list.


You don't say which package you are using, but I don't think RODBC supports non-SELECT statements.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 7, 2015 2:30:36 AM PDT, "Diaz Garcia, Luis Carlos" <Luis.Diaz at tecnocom.es> wrote:
>Hello all
>
>last week I create a script with R
>This script connect to Oracle database and retreave some data.
>
>This is a sample of the code
>
>dbName <- sqlQuery(con, "SELECT instance_name, host_name from
>v$instance",errors=FALSE)
>title (main = paste0("Mapa de los dblinks del entorno: ",
>dbName$INSTANCE_NAME, "_", 
>       dbName$HOST_NAME),sub="Luis Diaz - Emergencies & improvments")
>
>This code works fine, but now I need to get data from a pl sql block,
>such like this:
>
>	DECLARE
>	v_result number;
>
>	BEGIN
>	EXECUTE IMMEDIATE 'select count(1) from dual at db_link'';
>	v_result:=0; 
>	DBMS_OUTPUT.PUT_LINE(v_result);
>	
>	EXCEPTION 
>	WHEN OTHERS THEN 
>	v_result:=1; 
>	DBMS_OUTPUT.PUT_LINE(v_result);
>
>	END; 
>/
>
>This code return 0 if the db link works and 1 if not...
>I try tis way:
>
>isDead <- sqlQuery(con,"
>	set serveroutput on
>	DECLARE
>	v_result number;
>
>	BEGIN
>	EXECUTE IMMEDIATE 'select count(1) from dual at DB_LINK';
>	v_result:=0; 
>	EXCEPTION 
>	WHEN OTHERS THEN 
>	v_result:=1; 
>END; 
>/
>", errors=FALSE)
>print(isDead)
>
>The result of this isDead variable is always : -1
>I expect a value 0 or 1 depending of the db link result.
>Do you have any idea ?
>
>Thanks a los
>
>
>Tecnocom
>Luis Diaz
>Arquitecto Bases de Datos Oracle
>Email: luis.diaz at tecnocom.es
>http://www.tecnocom.es
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From Luis.Diaz at tecnocom.es  Mon Sep  7 17:03:12 2015
From: Luis.Diaz at tecnocom.es (Diaz Garcia, Luis Carlos)
Date: Mon, 7 Sep 2015 15:03:12 +0000
Subject: [R] get data from pl sql block
In-Reply-To: <9F5B911F-3D00-4843-9A65-CC33A49E0F51@dcn.davis.CA.us>
References: <DB5PR03MB12535A071DB138340CDAEE62EB540@DB5PR03MB1253.eurprd03.prod.outlook.com>,
	<9F5B911F-3D00-4843-9A65-CC33A49E0F51@dcn.davis.CA.us>
Message-ID: <DB5PR03MB12537F18F60E35DE02551D9DEB540@DB5PR03MB1253.eurprd03.prod.outlook.com>

Hi Jeff
yes I use RODBC, and I think you're write... Only "select" statments... I have a solution I think.
When I'll be sure, I'll share the info into the list.

Thanks a lot !


Tecnocom
Luis Diaz
Arquitecto Bases de Datos Oracle
Email: luis.diaz at tecnocom.es
http://www.tecnocom.es

________________________________________
De: Jeff Newmiller [jdnewmil at dcn.davis.CA.us]
Enviado: lunes, 7 de septiembre de 2015 16:53
Para: Diaz Garcia, Luis Carlos; R-help at r-project.org
Asunto: Re: [R] get data from pl sql block

You will get better answers on the R-sig-db mailing list.


You don't say which package you are using, but I don't think RODBC supports non-SELECT statements.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
---------------------------------------------------------------------------
Sent from my phone. Please excuse my brevity.

On September 7, 2015 2:30:36 AM PDT, "Diaz Garcia, Luis Carlos" <Luis.Diaz at tecnocom.es> wrote:
>Hello all
>
>last week I create a script with R
>This script connect to Oracle database and retreave some data.
>
>This is a sample of the code
>
>dbName <- sqlQuery(con, "SELECT instance_name, host_name from
>v$instance",errors=FALSE)
>title (main = paste0("Mapa de los dblinks del entorno: ",
>dbName$INSTANCE_NAME, "_",
>       dbName$HOST_NAME),sub="Luis Diaz - Emergencies & improvments")
>
>This code works fine, but now I need to get data from a pl sql block,
>such like this:
>
>       DECLARE
>       v_result number;
>
>       BEGIN
>       EXECUTE IMMEDIATE 'select count(1) from dual at db_link'';
>       v_result:=0;
>       DBMS_OUTPUT.PUT_LINE(v_result);
>
>       EXCEPTION
>       WHEN OTHERS THEN
>       v_result:=1;
>       DBMS_OUTPUT.PUT_LINE(v_result);
>
>       END;
>/
>
>This code return 0 if the db link works and 1 if not...
>I try tis way:
>
>isDead <- sqlQuery(con,"
>       set serveroutput on
>       DECLARE
>       v_result number;
>
>       BEGIN
>       EXECUTE IMMEDIATE 'select count(1) from dual at DB_LINK';
>       v_result:=0;
>       EXCEPTION
>       WHEN OTHERS THEN
>       v_result:=1;
>END;
>/
>", errors=FALSE)
>print(isDead)
>
>The result of this isDead variable is always : -1
>I expect a value 0 or 1 depending of the db link result.
>Do you have any idea ?
>
>Thanks a los
>
>
>Tecnocom
>Luis Diaz
>Arquitecto Bases de Datos Oracle
>Email: luis.diaz at tecnocom.es
>http://www.tecnocom.es
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Mon Sep  7 17:40:54 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Mon, 7 Sep 2015 08:40:54 -0700 (PDT)
Subject: [R] Help
In-Reply-To: <CAD3K6dQdsg9CjDT9J2W5G5JHhZXPhuFUvFcXops2GpCmi5Bjjw@mail.gmail.com>
References: <CAD3K6dQdsg9CjDT9J2W5G5JHhZXPhuFUvFcXops2GpCmi5Bjjw@mail.gmail.com>
Message-ID: <1441640454657-4711960.post@n4.nabble.com>

press ESC.

You may have entered a command that was missing a parenthesis or something
else that R needs before it can make sense out of your code (e.g., entering
"sum(X" without the closing paren will give you that pattern). ESC brings
back the command line and you can try again.

-Dan 



--
View this message in context: http://r.789695.n4.nabble.com/Help-tp4711952p4711960.html
Sent from the R help mailing list archive at Nabble.com.


From ddalthorp at usgs.gov  Mon Sep  7 19:15:58 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Mon, 7 Sep 2015 10:15:58 -0700 (PDT)
Subject: [R] extracting every nth character from a string...
In-Reply-To: <55EB57CA.2090909@gmail.com>
References: <55EB57CA.2090909@gmail.com>
Message-ID: <1441646158099-4711962.post@n4.nabble.com>

# or:
strsplit("junk",split=NULL)[[1]][(1:nchar("junk"))%%2==1]
strsplit("junk",split=NULL)[[1]][(1:nchar("junk"))%%2==0]





--
View this message in context: http://r.789695.n4.nabble.com/extracting-every-nth-character-from-a-string-tp4711908p4711962.html
Sent from the R help mailing list archive at Nabble.com.


From ddalthorp at usgs.gov  Mon Sep  7 19:42:34 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Mon, 7 Sep 2015 10:42:34 -0700 (PDT)
Subject: [R] extracting every nth character from a string...
In-Reply-To: <55EB57CA.2090909@gmail.com>
References: <55EB57CA.2090909@gmail.com>
Message-ID: <1441647754557-4711963.post@n4.nabble.com>

# as a complete function
StrSubset<-function(junk,n){
   ifelse(n==1,junk,
paste(strsplit(junk,split=NULL)[[1]][(1:nchar(junk))%%n==1],collapse=''))
}




--
View this message in context: http://r.789695.n4.nabble.com/extracting-every-nth-character-from-a-string-tp4711908p4711963.html
Sent from the R help mailing list archive at Nabble.com.


From ghada.f.mm at gmail.com  Mon Sep  7 15:57:19 2015
From: ghada.f.mm at gmail.com (Ghada Almousa)
Date: Mon, 7 Sep 2015 16:57:19 +0300
Subject: [R] please help me for my project
Message-ID: <CADG8gkuStCnHpiwKs8reZP6CfaeCOopRMMYmH-jL1_DmG-kNLg@mail.gmail.com>

Hello dears member
I have project to analysis clusters algorithm in R
"K-mean, Hierarchical, Density based and EM"
I want to calculate
Cluster instance , number of iteration , sum of squared error SSE and the
accuracy for each cluster algorithms that i mention above
And the log likelihood for EM and DBSCAN

	[[alternative HTML version deleted]]


From vdemart at gmail.com  Mon Sep  7 16:16:59 2015
From: vdemart at gmail.com (Victor)
Date: Mon, 7 Sep 2015 16:16:59 +0200
Subject: [R] Compiling R 3.2.2 under FreeBSD
Message-ID: <9251004B-F7DA-4F38-8D2A-729EA90E4ED7@gmail.com>

The R-CRAN project under FreeBSD 10.2 ports is quite obsolete being blocked at version 3.0.2 while at the moment the project is at version 3.2.2.
Modifying the /usr/ports/math/R port  I'm trying to upgrade R to that last version.  When I issue the 'make' command I get the following error
.....................................
installing etc ...
installing share ...
/usr/ports/math/R-new/work/stage/usr/local/lib/R/lib/libRblas.so is unchanged
gcc48 -std=gnu99 -I. -I../../src/include -I../../src/include -I/usr/local/include -I/usr/local/include -DLIBICONV_PLUG -DHAVE_CONFIG_H  -fopenmp -fpic  -O2 -pipe  -fno-builtin-coshl -fno-builtin-erfcl -fno-builtin-erfl -fno-builtin-lgammal -fno-builtin-powl -fno-builtin-sinhl -fno-builtin-tanhl -fno-builtin-tgammal -DLIBICONV_PLUG -fstack-protector -Wl,-rpath=/usr/local/lib/gcc48 -fno-strict-aliasing -L/usr/local/lib -fno-builtin-coshl -fno-builtin-erfcl -fno-builtin-erfl -fno-builtin-lgammal -fno-builtin-powl -fno-builtin-sinhl -fno-builtin-tanhl -fno-builtin-tgammal -Wl,-rpath=/usr/local/lib/gcc48  -L/usr/local/lib/gcc48 -B/usr/local/bin -fstack-protector -Wl,-rpath=/usr/local/lib/gcc48 -L/usr/local/lib/gcc48 -DR_HOME='"/usr/local/lib/R"'  -o Rscript ./Rscript.c
/usr/ports/math/R-new/work/stage/usr/local/lib/R/bin/exec/R is unchanged
/usr/ports/math/R-new/work/stage/usr/local/lib/R/lib/libR.so is unchanged
/usr/ports/math/R-new/work/stage/usr/local/lib/R/modules/internet.so is unchanged
/usr/ports/math/R-new/work/stage/usr/local/lib/R/modules/lapack.so is unchanged
/usr/ports/math/R-new/work/stage/usr/local/lib/R/lib/libRlapack.so is unchanged
/usr/ports/math/R-new/work/stage/usr/local/lib/R/modules/R_X11.so is unchanged
/usr/ports/math/R-new/work/stage/usr/local/lib/R/modules/R_de.so is unchanged
installing packages ...
  building HTML index ...
install: R-FAQ.info*: No such file or directory
*** Error code 71

Stop.
make[1]: stopped in /usr/ports/math/R-new
*** Error code 1

Stop.
make: stopped in /usr/ports/math/R-new
................................................



What should I do?

Ciao
Vittorio

From bennet at umich.edu  Mon Sep  7 17:08:13 2015
From: bennet at umich.edu (Bennet Fauber)
Date: Mon, 7 Sep 2015 11:08:13 -0400
Subject: [R] update.packages() behavior
Message-ID: <CAB2ovov55qTiDxnuhXMdBeP47U1g8WhQ7C4Fs3iRCAWvJcE1QQ@mail.gmail.com>

I recently compiled and installed R 3.2.2 on an RHEL 6.5 system.  Upon
installation, I tried

$ R-3.2.2/bin/R

R version 3.2.2 (2015-08-14) -- "Fire Safety"
....

> update.packages()
--- Please select a CRAN mirror for use in this session ---
Error in download.file(url, destfile = f, quiet = TRUE) :
  unsupported URL scheme
HTTPS CRAN mirror
....
Selection: 14
Warning: unable to access index for repository https://cran.mtu.edu/src/contrib

However, choosing '18: (HTTP mirrors)', which then presents a
different menu of download location choices, and selecting one of
those seems to work, as does specifically naming a http URL, e.g.,
update.packages(repos = "http://cran.mtu.edu/").

Is the defaulting to the https URL-type a new behavior?  Did I miss
including some library, so https is not included?

My configure line was

$ ./configure --prefix=/tmp/bennet/local --mandir=/tmp/bennet/local/man \
   --enable-R-shlib --without-x \
   --with-blas="-L/usr/cac/rhel6/intel-xe-2015/mkl/lib/intel64
-lmkl_gf_lp64 -lmkl_sequential -lmkl_core"

and I have

$ rpm -qa | grep curl
curl-7.19.7-46.el6.x86_64
libcurl-devel-7.19.7-46.el6.x86_64
libcurl-7.19.7-46.el6.x86_64

installed.

If https is the default URL-type, is there some configuration I need
to do to not get the unsupported URL scheme message?  Is there a way
to globally set the URL-type to http instead?

I recompiled R 3.1.1 to check, and it doesn't seem to default to https.

Sorry if this is covered somewhere, pointers to appropriate
documentation will be appreciated.

Thanks,  -- bennet


From jrkrideau at inbox.com  Mon Sep  7 21:17:50 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 7 Sep 2015 11:17:50 -0800
Subject: [R] please help me for my project
In-Reply-To: <CADG8gkuStCnHpiwKs8reZP6CfaeCOopRMMYmH-jL1_DmG-kNLg@mail.gmail.com>
Message-ID: <775F7C331C8.000005CAjrkrideau@inbox.com>

Some suggestions on how to ask a question on the R-help list

John Kane
Kingston ON Canada


> -----Original Message-----
> From: ghada.f.mm at gmail.com
> Sent: Mon, 7 Sep 2015 16:57:19 +0300
> To: r-help at r-project.org
> Subject: [R] please help me for my project
> 
> Hello dears member
> I have project to analysis clusters algorithm in R
> "K-mean, Hierarchical, Density based and EM"
> I want to calculate
> Cluster instance , number of iteration , sum of squared error SSE and the
> accuracy for each cluster algorithms that i mention above
> And the log likelihood for EM and DBSCAN
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From jonsleepy at gmail.com  Mon Sep  7 21:27:05 2015
From: jonsleepy at gmail.com (Jon BR)
Date: Mon, 7 Sep 2015 15:27:05 -0400
Subject: [R] Reformatting text inside a data frame
Message-ID: <CA+d7zeSh5Or+CRrspdcczhQWvBLSZjeRCu4yq0Qy=j=OvjnS5g@mail.gmail.com>

Hi all,
    I've read in a large data frame that has formatting similar to the one
in the small example below:

df <-
data.frame(c(1,2,3),c(NA,"AD=2;BA=8","AD=9;BA=1"),c("AD=13;BA=49","AD=1;BA=2",NA));
names(df) <- c("rowNum","first","second")

> df
  rowNum     first      second
1      1      <NA> AD=13;BA=49
2      2 AD=2;BA=8   AD=1;BA=2
3      3 AD=9;BA=1        <NA>


I'd like to reformat all of the non-NA entries in df from "first" and
"second" and so-on such that "AD=13;BA=49" will be replaced by the
following string: "13_13-49".

So applied to df, the output would be the following:

  rowNum     first      second
1      1      <NA> 13_13-49
2      2 2_2-8   1_1-2
3      3 9_9-1        <NA>


I'm generally a big proponent of shell scripting with awk, but I'd prefer
an all-R solution if one exists (and also to learn how to do this more
generally).

Could someone point out an appropriate paradigm or otherwise point me in
the right direction?

Best,
Jonathan

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Mon Sep  7 21:48:11 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 7 Sep 2015 11:48:11 -0800
Subject: [R] Reformatting text inside a data frame
In-Reply-To: <CA+d7zeSh5Or+CRrspdcczhQWvBLSZjeRCu4yq0Qy=j=OvjnS5g@mail.gmail.com>
Message-ID: <77A34DCF4B5.0000061Cjrkrideau@inbox.com>

I'm not making a lot of sense of the data, it looks like you want more recodes than you have mentioned  but in any case  you might want to look at the recode function in the car package.  It "should" do what you want thought there may be faster ways to do it.

BTW, for supplying sample data have a look at ?dput . Using dput() means that we see exactly the same data as you do.

Sorry not to be of more help
John Kane
Kingston ON Canada


> -----Original Message-----
> From: jonsleepy at gmail.com
> Sent: Mon, 7 Sep 2015 15:27:05 -0400
> To: r-help at r-project.org
> Subject: [R] Reformatting text inside a data frame
> 
> Hi all,
>     I've read in a large data frame that has formatting similar to the
> one
> in the small example below:
> 
> df <-
> data.frame(c(1,2,3),c(NA,"AD=2;BA=8","AD=9;BA=1"),c("AD=13;BA=49","AD=1;BA=2",NA));
> names(df) <- c("rowNum","first","second")
> 
>> df
>   rowNum     first      second
> 1      1      <NA> AD=13;BA=49
> 2      2 AD=2;BA=8   AD=1;BA=2
> 3      3 AD=9;BA=1        <NA>
> 
> 
> I'd like to reformat all of the non-NA entries in df from "first" and
> "second" and so-on such that "AD=13;BA=49" will be replaced by the
> following string: "13_13-49".
> 
> So applied to df, the output would be the following:
> 
>   rowNum     first      second
> 1      1      <NA> 13_13-49
> 2      2 2_2-8   1_1-2
> 3      3 9_9-1        <NA>
> 
> 
> I'm generally a big proponent of shell scripting with awk, but I'd prefer
> an all-R solution if one exists (and also to learn how to do this more
> generally).
> 
> Could someone point out an appropriate paradigm or otherwise point me in
> the right direction?
> 
> Best,
> Jonathan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From jonsleepy at gmail.com  Mon Sep  7 22:20:51 2015
From: jonsleepy at gmail.com (Jon BR)
Date: Mon, 7 Sep 2015 16:20:51 -0400
Subject: [R] Reformatting text inside a data frame
In-Reply-To: <77A34DCF4B5.0000061Cjrkrideau@inbox.com>
References: <CA+d7zeSh5Or+CRrspdcczhQWvBLSZjeRCu4yq0Qy=j=OvjnS5g@mail.gmail.com>
	<77A34DCF4B5.0000061Cjrkrideau@inbox.com>
Message-ID: <CA+d7zeRUJ46iTfHQrJyCC70ccBh1_=TzyrWeXBPPCS4ZexN8dA@mail.gmail.com>

Hi John,
     Thanks for the reply; I'm pasting here the output from dput, with a
'df <-' added in front:

df <- structure(list(rowNum = c(1, 2, 3), first = structure(c(NA, 1L,
2L), .Label = c("AD=2;BA=8", "AD=9;BA=1"), class = "factor"),
    second = structure(c(2L, 1L, NA), .Label = c("AD=1;BA=2",
    "AD=13;BA=49"), class = "factor")), .Names = c("rowNum",
"first", "second"), row.names = c(NA, -3L), class = "data.frame")




To add more specifics, about what I would like; each value to be adjusted
has the following general format:

"AD=X;BA=Y"

I would like to extract the values of X and Y and format them as a string
as such:

"X_X-Y"


Here's how I would handle a specific instance using awk in a shell script:

echo  "AD=X;BA=Y" | awk '{split($1,a,"AD="); split(a[2],b,";");
split(b[2],c,"BA="); print b[1]"_"b[1]"-"c[2]}'
X_X-Y

I'd like this to apply for all the entries that aren't NA to the right of
column 1.

Hoping this adds clarity for any others who also didn't follow my example.

Thanks in advance for any tips-

Best,
Jonathan

On Mon, Sep 7, 2015 at 3:48 PM, John Kane <jrkrideau at inbox.com> wrote:

> I'm not making a lot of sense of the data, it looks like you want more
> recodes than you have mentioned  but in any case  you might want to look at
> the recode function in the car package.  It "should" do what you want
> thought there may be faster ways to do it.
>
> BTW, for supplying sample data have a look at ?dput . Using dput() means
> that we see exactly the same data as you do.
>
> Sorry not to be of more help
> John Kane
> Kingston ON Canada
>
>
> > -----Original Message-----
> > From: jonsleepy at gmail.com
> > Sent: Mon, 7 Sep 2015 15:27:05 -0400
> > To: r-help at r-project.org
> > Subject: [R] Reformatting text inside a data frame
> >
> > Hi all,
> >     I've read in a large data frame that has formatting similar to the
> > one
> > in the small example below:
> >
> > df <-
> >
> data.frame(c(1,2,3),c(NA,"AD=2;BA=8","AD=9;BA=1"),c("AD=13;BA=49","AD=1;BA=2",NA));
> > names(df) <- c("rowNum","first","second")
> >
> >> df
> >   rowNum     first      second
> > 1      1      <NA> AD=13;BA=49
> > 2      2 AD=2;BA=8   AD=1;BA=2
> > 3      3 AD=9;BA=1        <NA>
> >
> >
> > I'd like to reformat all of the non-NA entries in df from "first" and
> > "second" and so-on such that "AD=13;BA=49" will be replaced by the
> > following string: "13_13-49".
> >
> > So applied to df, the output would be the following:
> >
> >   rowNum     first      second
> > 1      1      <NA> 13_13-49
> > 2      2 2_2-8   1_1-2
> > 3      3 9_9-1        <NA>
> >
> >
> > I'm generally a big proponent of shell scripting with awk, but I'd prefer
> > an all-R solution if one exists (and also to learn how to do this more
> > generally).
> >
> > Could someone point out an appropriate paradigm or otherwise point me in
> > the right direction?
> >
> > Best,
> > Jonathan
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
> Check it out at http://www.inbox.com/earth
>
>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Sep  7 22:45:51 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 7 Sep 2015 13:45:51 -0700
Subject: [R] please help me for my project
In-Reply-To: <775F7C331C8.000005CAjrkrideau@inbox.com>
References: <CADG8gkuStCnHpiwKs8reZP6CfaeCOopRMMYmH-jL1_DmG-kNLg@mail.gmail.com>
	<775F7C331C8.000005CAjrkrideau@inbox.com>
Message-ID: <CAGxFJbRANbNn+kTZP0SGkTpyoNMfDQq+jjUqevO7Wrc+cTcgZQ@mail.gmail.com>

... But This list has a *no homework* policy, and this sounds like homework.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Sep 7, 2015 at 12:17 PM, John Kane <jrkrideau at inbox.com> wrote:
> Some suggestions on how to ask a question on the R-help list
>
> John Kane
> Kingston ON Canada
>
>
>> -----Original Message-----
>> From: ghada.f.mm at gmail.com
>> Sent: Mon, 7 Sep 2015 16:57:19 +0300
>> To: r-help at r-project.org
>> Subject: [R] please help me for my project
>>
>> Hello dears member
>> I have project to analysis clusters algorithm in R
>> "K-mean, Hierarchical, Density based and EM"
>> I want to calculate
>> Cluster instance , number of iteration , sum of squared error SSE and the
>> accuracy for each cluster algorithms that i mention above
>> And the log likelihood for EM and DBSCAN
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
> Visit http://www.inbox.com/photosharing to find out more!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.tu-dortmund.de  Mon Sep  7 23:09:07 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Mon, 7 Sep 2015 23:09:07 +0200
Subject: [R] update.packages() behavior
In-Reply-To: <CAB2ovov55qTiDxnuhXMdBeP47U1g8WhQ7C4Fs3iRCAWvJcE1QQ@mail.gmail.com>
References: <CAB2ovov55qTiDxnuhXMdBeP47U1g8WhQ7C4Fs3iRCAWvJcE1QQ@mail.gmail.com>
Message-ID: <55EDFCF3.8030209@statistik.tu-dortmund.de>

Right R-3.2.2 has the new default, but requires, for example, libcurl 
suppport in order to be able to deal with https. See the reelase Notes. 
Otherwise, http works as before.

Best,
Uwe Ligges


On 07.09.2015 17:08, Bennet Fauber wrote:
> I recently compiled and installed R 3.2.2 on an RHEL 6.5 system.  Upon
> installation, I tried
>
> $ R-3.2.2/bin/R
>
> R version 3.2.2 (2015-08-14) -- "Fire Safety"
> ....
>
>> update.packages()
> --- Please select a CRAN mirror for use in this session ---
> Error in download.file(url, destfile = f, quiet = TRUE) :
>    unsupported URL scheme
> HTTPS CRAN mirror
> ....
> Selection: 14
> Warning: unable to access index for repository https://cran.mtu.edu/src/contrib
>
> However, choosing '18: (HTTP mirrors)', which then presents a
> different menu of download location choices, and selecting one of
> those seems to work, as does specifically naming a http URL, e.g.,
> update.packages(repos = "http://cran.mtu.edu/").
>
> Is the defaulting to the https URL-type a new behavior?  Did I miss
> including some library, so https is not included?
>
> My configure line was
>
> $ ./configure --prefix=/tmp/bennet/local --mandir=/tmp/bennet/local/man \
>     --enable-R-shlib --without-x \
>     --with-blas="-L/usr/cac/rhel6/intel-xe-2015/mkl/lib/intel64
> -lmkl_gf_lp64 -lmkl_sequential -lmkl_core"
>
> and I have
>
> $ rpm -qa | grep curl
> curl-7.19.7-46.el6.x86_64
> libcurl-devel-7.19.7-46.el6.x86_64
> libcurl-7.19.7-46.el6.x86_64
>
> installed.
>
> If https is the default URL-type, is there some configuration I need
> to do to not get the unsupported URL scheme message?  Is there a way
> to globally set the URL-type to http instead?
>
> I recompiled R 3.1.1 to check, and it doesn't seem to default to https.
>
> Sorry if this is covered somewhere, pointers to appropriate
> documentation will be appreciated.
>
> Thanks,  -- bennet
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Mon Sep  7 23:25:59 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 7 Sep 2015 14:25:59 -0700
Subject: [R] Reformatting text inside a data frame
In-Reply-To: <CA+d7zeRUJ46iTfHQrJyCC70ccBh1_=TzyrWeXBPPCS4ZexN8dA@mail.gmail.com>
References: <CA+d7zeSh5Or+CRrspdcczhQWvBLSZjeRCu4yq0Qy=j=OvjnS5g@mail.gmail.com>
	<77A34DCF4B5.0000061Cjrkrideau@inbox.com>
	<CA+d7zeRUJ46iTfHQrJyCC70ccBh1_=TzyrWeXBPPCS4ZexN8dA@mail.gmail.com>
Message-ID: <0C7F38D9-E33E-4856-8F79-E7D84A80300E@comcast.net>


> On Sep 7, 2015, at 1:20 PM, Jon BR <jonsleepy at gmail.com> wrote:
> 
> Hi John,
>     Thanks for the reply; I'm pasting here the output from dput, with a
> 'df <-' added in front:
> 
> df <- structure(list(rowNum = c(1, 2, 3), first = structure(c(NA, 1L,
> 2L), .Label = c("AD=2;BA=8", "AD=9;BA=1"), class = "factor"),
>    second = structure(c(2L, 1L, NA), .Label = c("AD=1;BA=2",
>    "AD=13;BA=49"), class = "factor")), .Names = c("rowNum",
> "first", "second"), row.names = c(NA, -3L), class = "data.frame")
> 
> 
> 
> 
> To add more specifics, about what I would like; each value to be adjusted
> has the following general format:
> 
> "AD=X;BA=Y"
> 
> I would like to extract the values of X and Y and format them as a string
> as such:
> 
> "X_X-Y"
> 
> 
> Here's how I would handle a specific instance using awk in a shell script:
> 
> echo  "AD=X;BA=Y" | awk '{split($1,a,"AD="); split(a[2],b,";");
> split(b[2],c,"BA="); print b[1]"_"b[1]"-"c[2]}'
> X_X-Y
> 
> I'd like this to apply for all the entries that aren't NA to the right of
> column 1.

df[2:3] <- lapply(df[2:3], sub, patt="(AD\\=)(.+)(;BA\\=)(.+)?,
                                repl="\\2_\\2-\\4? )

> df
  rowNum first   second
1      1  <NA> 13_13-49
2      2 2_2-8    1_1-2
3      3 9_9-1     <NA>

> 
> Hoping this adds clarity for any others who also didn't follow my example.
> 
> Thanks in advance for any tips-
> 
> Best,
> Jonathan
> 
> On Mon, Sep 7, 2015 at 3:48 PM, John Kane <jrkrideau at inbox.com> wrote:
> 
>> I'm not making a lot of sense of the data, it looks like you want more
>> recodes than you have mentioned  but in any case  you might want to look at
>> the recode function in the car package.  It "should" do what you want
>> thought there may be faster ways to do it.
>> 
>> BTW, for supplying sample data have a look at ?dput . Using dput() means
>> that we see exactly the same data as you do.
>> 
>> Sorry not to be of more help
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: jonsleepy at gmail.com
>>> Sent: Mon, 7 Sep 2015 15:27:05 -0400
>>> To: r-help at r-project.org
>>> Subject: [R] Reformatting text inside a data frame
>>> 
>>> Hi all,
>>>    I've read in a large data frame that has formatting similar to the
>>> one
>>> in the small example below:
>>> 
>>> df <-
>>> 
>> data.frame(c(1,2,3),c(NA,"AD=2;BA=8","AD=9;BA=1"),c("AD=13;BA=49","AD=1;BA=2",NA));
>>> names(df) <- c("rowNum","first","second")
>>> 
>>>> df
>>>  rowNum     first      second
>>> 1      1      <NA> AD=13;BA=49
>>> 2      2 AD=2;BA=8   AD=1;BA=2
>>> 3      3 AD=9;BA=1        <NA>
>>> 
>>> 
>>> I'd like to reformat all of the non-NA entries in df from "first" and
>>> "second" and so-on such that "AD=13;BA=49" will be replaced by the
>>> following string: "13_13-49".
>>> 
>>> So applied to df, the output would be the following:
>>> 
>>>  rowNum     first      second
>>> 1      1      <NA> 13_13-49
>>> 2      2 2_2-8   1_1-2
>>> 3      3 9_9-1        <NA>
>>> 
>>> 
>>> I'm generally a big proponent of shell scripting with awk, but I'd prefer
>>> an all-R solution if one exists (and also to learn how to do this more
>>> generally).
>>> 
>>> Could someone point out an appropriate paradigm or otherwise point me in
>>> the right direction?
>>> 
>>> Best,
>>> Jonathan
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ____________________________________________________________
>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>> Check it out at http://www.inbox.com/earth
>> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Mon Sep  7 23:34:34 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 7 Sep 2015 13:34:34 -0800
Subject: [R] Maxent package - Restriction on number of unique class
 labels
In-Reply-To: <CAFH1dhXc==0g+3QPXHwK8LiAptBjvPWnvoZfpqNTUnFfEptUgg@mail.gmail.com>
Message-ID: <78911CC2AF0.000006CDjrkrideau@inbox.com>


I have never even heard of the package but it might be just that that is the default maximum number of labels programmed in.

Have a look at the manual and/or the actual function.  If you don't get an answer in a day or so, email the author or maintainer. 
John Kane
Kingston ON Canada


> -----Original Message-----
> From: susrutha.gongalla at gmail.com
> Sent: Mon, 7 Sep 2015 08:34:47 -0400
> To: r-help at r-project.org
> Subject: [R] Maxent package - Restriction on number of unique class
> labels
> 
> Hi everyone,
> 
> I am developing a text classification code in R using 'maxent' package. I
> see that there is a limit on number of unique class labels that can be
> modeled, which is 255. Can someone please help me understand why this
> limit
> is present?
> The data I am working on has about 400 labels (data consists of thousands
> of records). Can anybody please suggest which package to use to be able
> to
> achieve maximum entropy based text classification on this?
> I have tried using 'RTextTools' package as well. It seems to be
> internally
> using 'maxent' package in return and  gives me the same error.
> 
> Would really appreciate any help!
> 
> Thanks.
> 
> Susrutha Gongalla
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Publish your photos in seconds for FREE
TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if4


From allennugent at hotmail.com  Tue Sep  8 05:04:38 2015
From: allennugent at hotmail.com (AltShift)
Date: Mon, 7 Sep 2015 20:04:38 -0700 (PDT)
Subject: [R] Is there a time series resampling function ?
In-Reply-To: <2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>
References: <1441540485695-4711907.post@n4.nabble.com>
	<2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>
Message-ID: <1441681478400-4711977.post@n4.nabble.com>


If
Jeff Newmiller wrote
> There are lots of them. You might be having trouble searching because you
> don't know how to spell "interpolate". 

Hi Jeff,

If you re-read my original post you will see the word interpolate, spelled
correctly. I also used the made-up word "intrapolate" (I thought that using
quotes in the post would make this obvious) because I am describing a
process that is complementary to interpolation. 

My actual problem is that I don't know how to search CRAN effectively. I'm
trying to address this deficiency now.




--
View this message in context: http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907p4711977.html
Sent from the R help mailing list archive at Nabble.com.


From bennet at umich.edu  Tue Sep  8 03:43:11 2015
From: bennet at umich.edu (Bennet Fauber)
Date: Mon, 7 Sep 2015 21:43:11 -0400
Subject: [R] update.packages() behavior
In-Reply-To: <55EDFCF3.8030209@statistik.tu-dortmund.de>
References: <CAB2ovov55qTiDxnuhXMdBeP47U1g8WhQ7C4Fs3iRCAWvJcE1QQ@mail.gmail.com>
	<55EDFCF3.8030209@statistik.tu-dortmund.de>
Message-ID: <CAB2ovouyZZysurqPiobpLu88Gcka-eFszBYHHQC3qSOnToCZoQ@mail.gmail.com>

I am not seeing an option with

$ ./configure --help

to include libcurl.  The Release Notes say:

=====
It is now easier to use secure downloads from https:// URLs on builds
which support them: no longer do non-default options need to be
selected to do so. In particular, packages can be installed from
repositories which offer https:// URLs, and those listed by
setRepositories() now do so (for some of their mirrors).

Support for https:// URLs is available on Windows, and on other
platforms if support for libcurl was compiled in and if that supports
the https protocol (system installations can be expected to do). So
https:// support can be expected except on rather old OSes (an example
being OS X ?Snow Leopard?, where a non-system version of libcurl can
be used).
=====

Is RHEL6 a 'rather old OS'?  Do I need to compile and include a newer
version of libcurl?

Or, perhaps, I am using a mirror that doesn't support this, even
though I chose one from the presented list?

Thanks,  -- bennet



On Mon, Sep 7, 2015 at 5:09 PM, Uwe Ligges
<ligges at statistik.tu-dortmund.de> wrote:
> Right R-3.2.2 has the new default, but requires, for example, libcurl
> suppport in order to be able to deal with https. See the reelase Notes.
> Otherwise, http works as before.
>
> Best,
> Uwe Ligges
>
>
> On 07.09.2015 17:08, Bennet Fauber wrote:
>>
>> I recently compiled and installed R 3.2.2 on an RHEL 6.5 system.  Upon
>> installation, I tried
>>
>> $ R-3.2.2/bin/R
>>
>> R version 3.2.2 (2015-08-14) -- "Fire Safety"
>> ....
>>
>>> update.packages()
>>
>> --- Please select a CRAN mirror for use in this session ---
>> Error in download.file(url, destfile = f, quiet = TRUE) :
>>    unsupported URL scheme
>> HTTPS CRAN mirror
>> ....
>> Selection: 14
>> Warning: unable to access index for repository
>> https://cran.mtu.edu/src/contrib
>>
>> However, choosing '18: (HTTP mirrors)', which then presents a
>> different menu of download location choices, and selecting one of
>> those seems to work, as does specifically naming a http URL, e.g.,
>> update.packages(repos = "http://cran.mtu.edu/").
>>
>> Is the defaulting to the https URL-type a new behavior?  Did I miss
>> including some library, so https is not included?
>>
>> My configure line was
>>
>> $ ./configure --prefix=/tmp/bennet/local --mandir=/tmp/bennet/local/man \
>>     --enable-R-shlib --without-x \
>>     --with-blas="-L/usr/cac/rhel6/intel-xe-2015/mkl/lib/intel64
>> -lmkl_gf_lp64 -lmkl_sequential -lmkl_core"
>>
>> and I have
>>
>> $ rpm -qa | grep curl
>> curl-7.19.7-46.el6.x86_64
>> libcurl-devel-7.19.7-46.el6.x86_64
>> libcurl-7.19.7-46.el6.x86_64
>>
>> installed.
>>
>> If https is the default URL-type, is there some configuration I need
>> to do to not get the unsupported URL scheme message?  Is there a way
>> to globally set the URL-type to http instead?
>>
>> I recompiled R 3.1.1 to check, and it doesn't seem to default to https.
>>
>> Sorry if this is covered somewhere, pointers to appropriate
>> documentation will be appreciated.
>>
>> Thanks,  -- bennet
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From jdnewmil at dcn.davis.CA.us  Tue Sep  8 07:09:03 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 07 Sep 2015 22:09:03 -0700
Subject: [R] Is there a time series resampling function ?
In-Reply-To: <1441681478400-4711977.post@n4.nabble.com>
References: <1441540485695-4711907.post@n4.nabble.com>
	<2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>
	<1441681478400-4711977.post@n4.nabble.com>
Message-ID: <FF5B2F5F-CC12-4C94-BCBF-4B3395125199@dcn.davis.CA.us>

Sorry, I did not see your use of the correct term, and I did not see any distinction between interpolate and the process you were describing so "intrapolate" just looked out of place.

If Google isn't helping, try

library(sos)
findFn("interpolate")

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 7, 2015 8:04:38 PM PDT, AltShift <allennugent at hotmail.com> wrote:
>
>If
>Jeff Newmiller wrote
>> There are lots of them. You might be having trouble searching because
>you
>> don't know how to spell "interpolate". 
>
>Hi Jeff,
>
>If you re-read my original post you will see the word interpolate,
>spelled
>correctly. I also used the made-up word "intrapolate" (I thought that
>using
>quotes in the post would make this obvious) because I am describing a
>process that is complementary to interpolation. 
>
>My actual problem is that I don't know how to search CRAN effectively.
>I'm
>trying to address this deficiency now.
>
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907p4711977.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Tue Sep  8 08:37:28 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 8 Sep 2015 06:37:28 +0000
Subject: [R] Lag variable by group
In-Reply-To: <CAHymutLSjzMxVdKSA0eJ8DvYXfXUY1Zn_obcf8_VSHuc_=-CHA@mail.gmail.com>
References: <CAHymutLSjzMxVdKSA0eJ8DvYXfXUY1Zn_obcf8_VSHuc_=-CHA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EA67@SRVEXCHMBX.precheza.cz>

Hi

Thanks for providing data. I did not see any response and frankly speaking I do not use data.table so I am not sure what do you mean by lagging t1.

I would start with ordering data.
ooo<-order(data$id, data$year)
data <- data[ooo,]

Then you can split data according to id.

datas<-split(data[,c(1,3)], data$id)

dput(head(datas))
structure(list(`28954` = structure(list(year = c(2005, 2006,
2007, 2008), t1 = c(-1.81807494163513, -1.81807494163513, -1.81807494163513,
-1.81807494163513)), .Names = c("year", "t1"), row.names = c(58L,
45L, 35L, 46L), class = "data.frame"), `28955` = structure(list(
    year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
    -1.81807494163513, -1.81807494163513, -1.81807494163513)), .Names = c("year",
"t1"), row.names = c(59L, 70L, 69L, 72L), class = "data.frame"),
    `28956` = structure(list(year = c(2005, 2006, 2007, 2008),
        t1 = c(-1.81807494163513, -1.81807494163513, -1.81807494163513,
        -1.81807494163513)), .Names = c("year", "t1"), row.names = c(53L,
    66L, 74L, 51L), class = "data.frame"), `28957` = structure(list(
        year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
        -1.81807494163513, -1.81807494163513, -1.81807494163513
        )), .Names = c("year", "t1"), row.names = c(71L, 64L,
    54L, 24L), class = "data.frame"), `28958` = structure(list(
        year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
        -1.81807494163513, -1.81807494163513, -1.81807494163513
        )), .Names = c("year", "t1"), row.names = c(34L, 27L,
    1L, 31L), class = "data.frame"), `28959` = structure(list(
        year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
        -1.81807494163513, -1.81807494163513, -1.81807494163513
        )), .Names = c("year", "t1"), row.names = c(17L, 18L,
    30L, 44L), class = "data.frame")), .Names = c("28954", "28955",
"28956", "28957", "28958", "28959"))

But now I am lost what result you expect. Can you explain it on this smaller data set?

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Janka
> VANSCHOENWINKEL
> Sent: Monday, September 07, 2015 1:18 PM
> To: r-help at r-project.org
> Subject: [R] Lag variable by group
>
> Hi!
>
> I have the following dataset with the variables ID (this is a unique ID
> per farmer), year, and another variable t1.
> I now would like to have a fourth variable which is the lag value of t1
> for each farm ID.
>
> I found a code on the internet that does exactly what I need, but it
> does not work for this dataset. Does anyone have suggestions about how
> I can make this work?
>
> Thanks a lot!
>
> Janka
>
> data<-structure(list(year = c(2007, 2005, 2008, 2006, 2005, 2007, 2006,
> 2008, 2007, 2005, 2007, 2007, 2005, 2006, 2005, 2006, 2005, 2006, 2007,
> 2007, 2005, 2008, 2007, 2008, 2005, 2005, 2006, 2008, 2007, 2007, 2008,
> 2008, 2006, 2005, 2007, 2006, 2008, 2008, 2007, 2007, 2007, 2006, 2006,
> 2008, 2006, 2008, 2008, 2008, 2006, 2007, 2008, 2007, 2005, 2007, 2008,
> 2005, 2007, 2005, 2005, 2008, 2005, 2006, 2005, 2006, 2008, 2006, 2008,
> 2006, 2007, 2006, 2005, 2008, 2006, 2007, 2008, 2006, 2006, 2006, 2005,
> 2008, 2006, 2008, 2006, 2006, 2006, 2007, 2008, 2005, 2007, 2006, 2007,
> 2008, 2006, 2008, 2005, 2007, 2005, 2007, 2006, 2006), id = c(28958L,
> 28962L, 28962L, 28965L, 28960L, 28962L, 28964L, 28970L, 28961L, 28965L,
> 78458L, 28960L, 28961L, 28961L, 28969L, 28962L, 28959L, 28959L, 58845L,
> 28965L, 28963L, 78459L, 28967L, 28957L, 28964L, 28966L, 28958L, 28960L,
> 28969L, 28959L, 28958L, 28969L, 58845L, 28958L, 28954L, 28963L, 78458L,
> 28965L, 28966L, 28963L, 28970L, 28970L, 28960L, 28959L, 28954L, 28954L,
> 58845L, 28967L, 28966L, 78459L, 28956L, 28964L, 28956L, 28957L, 28961L,
> 28970L, 28968L, 28954L, 28955L, 28968L, 28968L, 28967L, 28967L, 28957L,
> 28966L, 28956L, 28964L, 28969L, 28955L, 28955L, 28957L, 28955L, 28968L,
> 28956L, 28963L, 29004L, 58848L, 29005L, 28974L, 29005L, 28974L, 29006L,
> 28981L, 29007L, 29002L, 28980L, 29001L, 29006L, 29005L, 28989L, 28989L,
> 58846L, 28980L, 28981L, 78467L, 28990L, 28973L, 29004L, 28972L,
> 29006L), t1 = c(-1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> 1.81807494163513, -1.43884992599487, -1.43884992599487, -
> 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> 1.43884992599487, -1.43884992599487)), .Names = c("year", "id", "t1"),
> row.names = c(NA, 100L), class = "data.frame")
>
> library(data.table)
> data[, lag.t1:=c(NA, t1[-.N]), by=id]
>
>
> Thank you very much!
>
> Janka
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From janka.vanschoenwinkel at uhasselt.be  Tue Sep  8 08:47:38 2015
From: janka.vanschoenwinkel at uhasselt.be (Janka VANSCHOENWINKEL)
Date: Tue, 8 Sep 2015 08:47:38 +0200
Subject: [R] Lag variable by group
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EA67@SRVEXCHMBX.precheza.cz>
References: <CAHymutLSjzMxVdKSA0eJ8DvYXfXUY1Zn_obcf8_VSHuc_=-CHA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EA67@SRVEXCHMBX.precheza.cz>
Message-ID: <CAHymutJvB+8XfkQOnKkVYNhR+vMFJuga09OvX_8er0RPBqAsqw@mail.gmail.com>

Hi Petr and other member who can use this post,

Somebody gave me an answer in a private email which worked for me!

The only thing I needed to do was to make first a data.table object of my
data. Then the code works!

library(data.table)
data <- data.table(data, key = "id")
data[, lag.t1:=c(NA, t1[-.N]), by=id]

Thank you very much for your help Petr!

I really appreciate it!

Janka



2015-09-08 8:37 GMT+02:00 PIKAL Petr <petr.pikal at precheza.cz>:

> Hi
>
> Thanks for providing data. I did not see any response and frankly speaking
> I do not use data.table so I am not sure what do you mean by lagging t1.
>
> I would start with ordering data.
> ooo<-order(data$id, data$year)
> data <- data[ooo,]
>
> Then you can split data according to id.
>
> datas<-split(data[,c(1,3)], data$id)
>
> dput(head(datas))
> structure(list(`28954` = structure(list(year = c(2005, 2006,
> 2007, 2008), t1 = c(-1.81807494163513, -1.81807494163513,
> -1.81807494163513,
> -1.81807494163513)), .Names = c("year", "t1"), row.names = c(58L,
> 45L, 35L, 46L), class = "data.frame"), `28955` = structure(list(
>     year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
>     -1.81807494163513, -1.81807494163513, -1.81807494163513)), .Names =
> c("year",
> "t1"), row.names = c(59L, 70L, 69L, 72L), class = "data.frame"),
>     `28956` = structure(list(year = c(2005, 2006, 2007, 2008),
>         t1 = c(-1.81807494163513, -1.81807494163513, -1.81807494163513,
>         -1.81807494163513)), .Names = c("year", "t1"), row.names = c(53L,
>     66L, 74L, 51L), class = "data.frame"), `28957` = structure(list(
>         year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
>         -1.81807494163513, -1.81807494163513, -1.81807494163513
>         )), .Names = c("year", "t1"), row.names = c(71L, 64L,
>     54L, 24L), class = "data.frame"), `28958` = structure(list(
>         year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
>         -1.81807494163513, -1.81807494163513, -1.81807494163513
>         )), .Names = c("year", "t1"), row.names = c(34L, 27L,
>     1L, 31L), class = "data.frame"), `28959` = structure(list(
>         year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
>         -1.81807494163513, -1.81807494163513, -1.81807494163513
>         )), .Names = c("year", "t1"), row.names = c(17L, 18L,
>     30L, 44L), class = "data.frame")), .Names = c("28954", "28955",
> "28956", "28957", "28958", "28959"))
>
> But now I am lost what result you expect. Can you explain it on this
> smaller data set?
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Janka
> > VANSCHOENWINKEL
> > Sent: Monday, September 07, 2015 1:18 PM
> > To: r-help at r-project.org
> > Subject: [R] Lag variable by group
> >
> > Hi!
> >
> > I have the following dataset with the variables ID (this is a unique ID
> > per farmer), year, and another variable t1.
> > I now would like to have a fourth variable which is the lag value of t1
> > for each farm ID.
> >
> > I found a code on the internet that does exactly what I need, but it
> > does not work for this dataset. Does anyone have suggestions about how
> > I can make this work?
> >
> > Thanks a lot!
> >
> > Janka
> >
> > data<-structure(list(year = c(2007, 2005, 2008, 2006, 2005, 2007, 2006,
> > 2008, 2007, 2005, 2007, 2007, 2005, 2006, 2005, 2006, 2005, 2006, 2007,
> > 2007, 2005, 2008, 2007, 2008, 2005, 2005, 2006, 2008, 2007, 2007, 2008,
> > 2008, 2006, 2005, 2007, 2006, 2008, 2008, 2007, 2007, 2007, 2006, 2006,
> > 2008, 2006, 2008, 2008, 2008, 2006, 2007, 2008, 2007, 2005, 2007, 2008,
> > 2005, 2007, 2005, 2005, 2008, 2005, 2006, 2005, 2006, 2008, 2006, 2008,
> > 2006, 2007, 2006, 2005, 2008, 2006, 2007, 2008, 2006, 2006, 2006, 2005,
> > 2008, 2006, 2008, 2006, 2006, 2006, 2007, 2008, 2005, 2007, 2006, 2007,
> > 2008, 2006, 2008, 2005, 2007, 2005, 2007, 2006, 2006), id = c(28958L,
> > 28962L, 28962L, 28965L, 28960L, 28962L, 28964L, 28970L, 28961L, 28965L,
> > 78458L, 28960L, 28961L, 28961L, 28969L, 28962L, 28959L, 28959L, 58845L,
> > 28965L, 28963L, 78459L, 28967L, 28957L, 28964L, 28966L, 28958L, 28960L,
> > 28969L, 28959L, 28958L, 28969L, 58845L, 28958L, 28954L, 28963L, 78458L,
> > 28965L, 28966L, 28963L, 28970L, 28970L, 28960L, 28959L, 28954L, 28954L,
> > 58845L, 28967L, 28966L, 78459L, 28956L, 28964L, 28956L, 28957L, 28961L,
> > 28970L, 28968L, 28954L, 28955L, 28968L, 28968L, 28967L, 28967L, 28957L,
> > 28966L, 28956L, 28964L, 28969L, 28955L, 28955L, 28957L, 28955L, 28968L,
> > 28956L, 28963L, 29004L, 58848L, 29005L, 28974L, 29005L, 28974L, 29006L,
> > 28981L, 29007L, 29002L, 28980L, 29001L, 29006L, 29005L, 28989L, 28989L,
> > 58846L, 28980L, 28981L, 78467L, 28990L, 28973L, 29004L, 28972L,
> > 29006L), t1 = c(-1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487)), .Names = c("year", "id", "t1"),
> > row.names = c(NA, 100L), class = "data.frame")
> >
> > library(data.table)
> > data[, lag.t1:=c(NA, t1[-.N]), by=id]
> >
> >
> > Thank you very much!
> >
> > Janka
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>



-- 

[image: Logo UHasselt]Mevrouw Janka Vanschoenwinkel
*Doctoraatsbursaal - PhD *
Milieueconomie - Environmental economics

T +32(0)11 26 87 42 | GSM +32(0)476 28 21 40

www.uhasselt.be/eec

Universiteit Hasselt | Campus Diepenbeek
Agoralaan Gebouw D | B-3590 Diepenbeek
Kantoor F11

Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt

P Please consider the environment before printing this e-mail

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Tue Sep  8 08:56:36 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 8 Sep 2015 08:56:36 +0200
Subject: [R] Help
In-Reply-To: <1441640454657-4711960.post@n4.nabble.com>
References: <CAD3K6dQdsg9CjDT9J2W5G5JHhZXPhuFUvFcXops2GpCmi5Bjjw@mail.gmail.com>
	<1441640454657-4711960.post@n4.nabble.com>
Message-ID: <80FD2CD9-875A-4531-9248-30BDEBE13149@gmail.com>

Please avoid Nabble.... For starters, your mail has lost the original poster and all context.

Anyways,

(A) in this particular case it wouldn't be the sum(x issue since it would stop with a syntax error:

> sum(x
+ 1
Error: unexpected numeric constant in:
"sum(x
1"
> 

More commonly, miscounting quote characters (' or ") is the culprit, or maybe a curly brace {. The latter would stop at any  syntax error, though:

> {
+ 1
+ q()
+ a b
Error: unexpected symbol in:
"q()
a b"

With quotes it will go on until a matching quote is found, since character constants can be multi-line.

(B) More importantly, ESC is not invariably the ticket out. It is the case in the Mac GUI and in RStudio, and presumably in RGui on Windows too, but in a Terminal application on OSX or Linux, it is Ctrl-C.  

-pd


On 07 Sep 2015, at 17:40 , Dan D <ddalthorp at usgs.gov> wrote:

> press ESC.
> 
> You may have entered a command that was missing a parenthesis or something
> else that R needs before it can make sense out of your code (e.g., entering
> "sum(X" without the closing paren will give you that pattern). ESC brings
> back the command line and you can try again.
> 
> -Dan 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Help-tp4711952p4711960.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From janka.vanschoenwinkel at uhasselt.be  Tue Sep  8 09:32:34 2015
From: janka.vanschoenwinkel at uhasselt.be (Janka VANSCHOENWINKEL)
Date: Tue, 8 Sep 2015 09:32:34 +0200
Subject: [R] Lag variable by group
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EA92@SRVEXCHMBX.precheza.cz>
References: <CAHymutLSjzMxVdKSA0eJ8DvYXfXUY1Zn_obcf8_VSHuc_=-CHA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EA67@SRVEXCHMBX.precheza.cz>
	<CAHymutJvB+8XfkQOnKkVYNhR+vMFJuga09OvX_8er0RPBqAsqw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EA92@SRVEXCHMBX.precheza.cz>
Message-ID: <CAHymutKPaTxAJiX4ESPKeRtfc8Z3Lqd5BaXTNy_1cJL3459YYQ@mail.gmail.com>

Wow! Thanks for pointing that out! And thanks for testing it out as well!

It is always the first year available (unbalanced panel) that should get NA.

So using the code line you provided earlier, this should work:

library(data.table)
data <- data.table(newdata, key = "id")
ooo<-order(data$id, data$year)
data <- data[ooo,]
data$lagvar<-data[, lag.t1:=c(NA, t1[-.N]), by=id]

Thank you very much for pointing that out!



2015-09-08 9:05 GMT+02:00 PIKAL Petr <petr.pikal at precheza.cz>:

> Hm. I tried your example but what puzzles me is that your data are not
> sorted by year and therefore sometimes the first year is changed to NA but
> sometimes any arbitrary year is changed to NA.
>
>
>
> > head(data)
>
>    year    id        t1    lag.t1
>
> 1: 2007 28954 -1.818075        NA
>
> 2: 2006 28954 -1.818075 -1.818075
>
> 3: 2008 28954 -1.818075 -1.818075
>
> 4: 2005 28954 -1.818075 -1.818075
>
> 5: 2005 28955 -1.818075        NA
>
> 6: 2007 28955 -1.818075 -1.818075
>
>
>
> Is it what you intended?
>
> Cheers
>
> Petr
>
>
>
>
>
> *From:* Janka VANSCHOENWINKEL [mailto:janka.vanschoenwinkel at uhasselt.be]
> *Sent:* Tuesday, September 08, 2015 8:48 AM
> *To:* PIKAL Petr
> *Cc:* r-help at r-project.org
> *Subject:* Re: [R] Lag variable by group
>
>
>
> Hi Petr and other member who can use this post,
>
>
>
> Somebody gave me an answer in a private email which worked for me!
>
>
>
> The only thing I needed to do was to make first a data.table object of my
> data. Then the code works!
>
>
>
> library(data.table)
> data <- data.table(data, key = "id")
> data[, lag.t1:=c(NA, t1[-.N]), by=id]
>
>
>
> Thank you very much for your help Petr!
>
>
>
> I really appreciate it!
>
>
>
> Janka
>
>
>
>
>
>
>
> 2015-09-08 8:37 GMT+02:00 PIKAL Petr <petr.pikal at precheza.cz>:
>
> Hi
>
> Thanks for providing data. I did not see any response and frankly speaking
> I do not use data.table so I am not sure what do you mean by lagging t1.
>
> I would start with ordering data.
> ooo<-order(data$id, data$year)
> data <- data[ooo,]
>
> Then you can split data according to id.
>
> datas<-split(data[,c(1,3)], data$id)
>
> dput(head(datas))
> structure(list(`28954` = structure(list(year = c(2005, 2006,
> 2007, 2008), t1 = c(-1.81807494163513, -1.81807494163513,
> -1.81807494163513,
> -1.81807494163513)), .Names = c("year", "t1"), row.names = c(58L,
> 45L, 35L, 46L), class = "data.frame"), `28955` = structure(list(
>     year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
>     -1.81807494163513, -1.81807494163513, -1.81807494163513)), .Names =
> c("year",
> "t1"), row.names = c(59L, 70L, 69L, 72L), class = "data.frame"),
>     `28956` = structure(list(year = c(2005, 2006, 2007, 2008),
>         t1 = c(-1.81807494163513, -1.81807494163513, -1.81807494163513,
>         -1.81807494163513)), .Names = c("year", "t1"), row.names = c(53L,
>     66L, 74L, 51L), class = "data.frame"), `28957` = structure(list(
>         year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
>         -1.81807494163513, -1.81807494163513, -1.81807494163513
>         )), .Names = c("year", "t1"), row.names = c(71L, 64L,
>     54L, 24L), class = "data.frame"), `28958` = structure(list(
>         year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
>         -1.81807494163513, -1.81807494163513, -1.81807494163513
>         )), .Names = c("year", "t1"), row.names = c(34L, 27L,
>     1L, 31L), class = "data.frame"), `28959` = structure(list(
>         year = c(2005, 2006, 2007, 2008), t1 = c(-1.81807494163513,
>         -1.81807494163513, -1.81807494163513, -1.81807494163513
>         )), .Names = c("year", "t1"), row.names = c(17L, 18L,
>     30L, 44L), class = "data.frame")), .Names = c("28954", "28955",
> "28956", "28957", "28958", "28959"))
>
> But now I am lost what result you expect. Can you explain it on this
> smaller data set?
>
> Cheers
> Petr
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Janka
> > VANSCHOENWINKEL
> > Sent: Monday, September 07, 2015 1:18 PM
> > To: r-help at r-project.org
> > Subject: [R] Lag variable by group
> >
> > Hi!
> >
> > I have the following dataset with the variables ID (this is a unique ID
> > per farmer), year, and another variable t1.
> > I now would like to have a fourth variable which is the lag value of t1
> > for each farm ID.
> >
> > I found a code on the internet that does exactly what I need, but it
> > does not work for this dataset. Does anyone have suggestions about how
> > I can make this work?
> >
> > Thanks a lot!
> >
> > Janka
> >
> > data<-structure(list(year = c(2007, 2005, 2008, 2006, 2005, 2007, 2006,
> > 2008, 2007, 2005, 2007, 2007, 2005, 2006, 2005, 2006, 2005, 2006, 2007,
> > 2007, 2005, 2008, 2007, 2008, 2005, 2005, 2006, 2008, 2007, 2007, 2008,
> > 2008, 2006, 2005, 2007, 2006, 2008, 2008, 2007, 2007, 2007, 2006, 2006,
> > 2008, 2006, 2008, 2008, 2008, 2006, 2007, 2008, 2007, 2005, 2007, 2008,
> > 2005, 2007, 2005, 2005, 2008, 2005, 2006, 2005, 2006, 2008, 2006, 2008,
> > 2006, 2007, 2006, 2005, 2008, 2006, 2007, 2008, 2006, 2006, 2006, 2005,
> > 2008, 2006, 2008, 2006, 2006, 2006, 2007, 2008, 2005, 2007, 2006, 2007,
> > 2008, 2006, 2008, 2005, 2007, 2005, 2007, 2006, 2006), id = c(28958L,
> > 28962L, 28962L, 28965L, 28960L, 28962L, 28964L, 28970L, 28961L, 28965L,
> > 78458L, 28960L, 28961L, 28961L, 28969L, 28962L, 28959L, 28959L, 58845L,
> > 28965L, 28963L, 78459L, 28967L, 28957L, 28964L, 28966L, 28958L, 28960L,
> > 28969L, 28959L, 28958L, 28969L, 58845L, 28958L, 28954L, 28963L, 78458L,
> > 28965L, 28966L, 28963L, 28970L, 28970L, 28960L, 28959L, 28954L, 28954L,
> > 58845L, 28967L, 28966L, 78459L, 28956L, 28964L, 28956L, 28957L, 28961L,
> > 28970L, 28968L, 28954L, 28955L, 28968L, 28968L, 28967L, 28967L, 28957L,
> > 28966L, 28956L, 28964L, 28969L, 28955L, 28955L, 28957L, 28955L, 28968L,
> > 28956L, 28963L, 29004L, 58848L, 29005L, 28974L, 29005L, 28974L, 29006L,
> > 28981L, 29007L, 29002L, 28980L, 29001L, 29006L, 29005L, 28989L, 28989L,
> > 58846L, 28980L, 28981L, 78467L, 28990L, 28973L, 29004L, 28972L,
> > 29006L), t1 = c(-1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.81807494163513, -1.81807494163513, -
> > 1.81807494163513, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487, -1.43884992599487, -
> > 1.43884992599487, -1.43884992599487)), .Names = c("year", "id", "t1"),
> > row.names = c(NA, 100L), class = "data.frame")
> >
> > library(data.table)
> > data[, lag.t1:=c(NA, t1[-.N]), by=id]
> >
> >
> > Thank you very much!
> >
> > Janka
> >
>
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
>
>
>
>
> -
>
>
>
> ------------------------------
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>



-- 

[image: Logo UHasselt]Mevrouw Janka Vanschoenwinkel
*Doctoraatsbursaal - PhD *
Milieueconomie - Environmental economics

T +32(0)11 26 87 42 | GSM +32(0)476 28 21 40

www.uhasselt.be/eec

Universiteit Hasselt | Campus Diepenbeek
Agoralaan Gebouw D | B-3590 Diepenbeek
Kantoor F11

Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt

P Please consider the environment before printing this e-mail

	[[alternative HTML version deleted]]


From ivan.calandra at univ-reims.fr  Tue Sep  8 12:33:14 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Tue, 8 Sep 2015 12:33:14 +0200
Subject: [R] help with reshape
Message-ID: <55EEB96A.8020504@univ-reims.fr>

Dear users,

I'm having troubles with reshaping a data.frame from long to wide format.
I copy the output of dput() at the end of the mail because it is quite long.

Each row of the column "Elem" should be transposed to a new column. All 
variables "Etape", "Ech", "repet", "dilution", "Rincage" define the 
samples. Meaning that for each unique combination of these variables, I 
want a single row, and as many columns as elements in "Elem".

So I tried:
reshape(mydata, timevar="Elem", 
idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide", 
drop=c("ID","Nom_ech"))

The problem is that some columns are not used at all for defining 
samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing, but I 
don't understand why.

Can you help me with that? I have no idea what I am doing wrong...

Thanks in advance,
Ivan



mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549, 550,
551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 11574,
11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583,
683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
696, 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
"Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
"Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
"Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
"R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
"R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
"R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
"S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
"S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
"S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
"S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
"S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
"S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
"S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
"S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
"S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
"S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2",
"B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
"B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3",
"F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
"F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
"F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
"F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1), dilution = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100,
100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
100, 100, 100, 100), Rincage = c("non", "non", "non", "non",
"non", "non", "non", "non", "non", "non", "non", "non", "non",
"non", "non", "non", "non", "non", "non", "non", "oui", "oui",
"oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "non",
"non", "non", "non", "non", "non", "non", "non", "non", "non",
"non", "non", "non", "non", "non", "non", "non", "non", "non",
"non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
"Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247",
"Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605",
"Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288",
"Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890", "As1937",
"Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
"Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852",
"Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455, 
596.166666666667,
578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859, 1.96766666666667,
NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875, 0.2877, 0.2395,
5732.33333333333, 3.9615, 3.48833333333333, 337.433333333333,
323.533333333333, 0.877166666666667, 0.292466666666667, NA, 
1.79566666666667,
NA, 106.326666666667, NA, 2.2755, 291.933333333333, 278.833333333333,
0.819, NA, 3.946, NA, NA, 1.47766666666667, 1.63266666666667,
40.44, 40.2533333333333, 128.9, 50.11, 49.02, 37.4733333333333,
37.775, 0.3764)), .Names = c("ID", "Nom_ech", "Etape", "Ech",
"repet", "dilution", "Rincage", "Elem", "Moyenne"), row.names = c(1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L,
37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L, 118L, 119L, 120L,
121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L
), class = c("tbl_df", "tbl", "data.frame"))

-- 

Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra


From jessicalavabre at gmail.com  Tue Sep  8 11:01:14 2015
From: jessicalavabre at gmail.com (Jessica Lavabre)
Date: Tue, 8 Sep 2015 16:01:14 +0700
Subject: [R] Modelling categorical variables
Message-ID: <CAGCTXOji9z4v6OG7JB6ur-2oZXWYZjDjU-1AHuf0baCzjBU7MA@mail.gmail.com>

Hi,

I am a beginner with statistics and R and have no clue on how to model my
data. I have collected information on seed traps (ID) that includes the
habitat type (Hab) and different measures of distances. Also I have applied
a modularity analysis, so that the seeds traps are grouped into modules. My
dataset is as follow:



*ID    Hab  Module    DistEdge    MeanDist1    MeanDist2    MeanDist3
F48    F       A         21.768       24.941          6.033
27.642    F50    F       E         35.666**       60.505          149.927 *
*       48.582   F52    F       B**         12.243**       103.041
72.908  *
*        102.375    N02    N      B **        58.681**
129.59          127.344   *
*     131.383   N17    N      B**         62.829**       72.827 **
76.736  *
*        77.644  N22    N      B**         89.207**       78.719  **
75.005   *
*       81.176N33    N      A**         23.288**       35.48    **
25.317 *
*         36.931    N40    N      B**         36.734**       62.234 **
30.68   *
*         61.885    N47    N      E  **       60.443**       66.367  **
150.892 **       55.097   *

I am looking for a way to analyze if there is any correlation between the
Module classification and the other variables. My difficulties here are:
1 - is there a way to model my data where Module is the response variable
(something like Module~Hab*DistEdge*MeanDist1) ? If so, which model should
I use (I only have a bit of experience with glm) and which distribution?
2 - Is that a problem if I have different types of predictor variable
(factor and numerical)?

Any help would be greatly appreciated,

-- Jessica Lavabre-Micas

	[[alternative HTML version deleted]]


From miguel.lurgirivera at adelaide.edu.au  Tue Sep  8 11:03:02 2015
From: miguel.lurgirivera at adelaide.edu.au (Miguel Lurgi Rivera)
Date: Tue, 8 Sep 2015 09:03:02 +0000
Subject: [R] help with the by term in the smoother of gam - mgcv
Message-ID: <30117563EEC9E04E8C82048581020F8D15770A16@mailmb10.ad.adelaide.edu.au>


Hi,

I have written a GAM model with two predictor variables, one of which is a factor (with 7 levels). I want to model an interaction between x0 and x1, so I'm using a ti smoother because I think that is the appropriate way to model interactions.

The general form of the model is y ~ ti(x0, by=x1), where x0 is a continuous numeric variable and x1 is the factor (with 7 levels).

However, when I run the model, the outcome is not as expected (the fit was not great). When I look at plots of the smoothers for each level of the factor, they are terrible and don?t appear to follow the shape of the data points at all (see attached plots - Figure 1).

When searching over the help pages I found in page 56 of the manual that it says: 'Note that when using factor by variables, centering constraints are applied to the smooths, which usually means that the by variable should be included as a parametric term, as well'.

Even though I wasn't sure what that meant (!), I proceeded to add my categorical by variable (x1) as a parametric term and ended up with the following model:

y ~ x1 + ti(x0, by=x1)

When doing this results improved greatly (see attached plots - Figure 2) and I don't quite understand why. I suspect it is because of the 'centering constraints' mentioned in the manual, but I am not sure.

So, my questions are:
- Is the second model the correct formulation?
- What is (in plainer terms) the difference between using those two model formulations and why should I include the by factor also as a parametric term? I thought that usually any covariates listed in an interaction were also automatically modelled as main effects too?.

On a different topic, as you can see from Figure 2, curves fit to the data is still not ideal. Do you have any suggestions on how to improve this? I have lowered k to 3 in the smoother (less than that prevents model convergence) and have used bs = 'fs' as a baseline for the smoother because the manual says is more penalising. But none of these tricks have actually worked.

Thanks in advance!

Cheers,

Miguel.-


-------------- next part --------------
A non-text attachment was scrubbed...
Name: Figure-1.png
Type: image/png
Size: 116451 bytes
Desc: Figure-1.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150908/74d33a9b/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Figure-2.png
Type: image/png
Size: 152094 bytes
Desc: Figure-2.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150908/74d33a9b/attachment-0001.png>

From allennugent at hotmail.com  Tue Sep  8 12:15:00 2015
From: allennugent at hotmail.com (AltShift)
Date: Tue, 8 Sep 2015 03:15:00 -0700 (PDT)
Subject: [R] Is there a time series resampling function ?
In-Reply-To: <FF5B2F5F-CC12-4C94-BCBF-4B3395125199@dcn.davis.CA.us>
References: <1441540485695-4711907.post@n4.nabble.com>
	<2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>
	<1441681478400-4711977.post@n4.nabble.com>
	<FF5B2F5F-CC12-4C94-BCBF-4B3395125199@dcn.davis.CA.us>
Message-ID: <1441707300774-4711986.post@n4.nabble.com>

Cheers, mate!

I thought there must be some obscure way to search from within CRAN.  After
reading your reply I tried Googling "CRAN signal processing".  Much better. 
(D'oh!)




--
View this message in context: http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907p4711986.html
Sent from the R help mailing list archive at Nabble.com.


From allennugent at hotmail.com  Tue Sep  8 12:16:42 2015
From: allennugent at hotmail.com (AltShift)
Date: Tue, 8 Sep 2015 03:16:42 -0700 (PDT)
Subject: [R] Is there a time series resampling function ?
In-Reply-To: <CAJRuHop-Nki7K7ZAv22dEv2PBNMF4SH4sXGUGdSQY3y0NA40-w@mail.gmail.com>
References: <1441540485695-4711907.post@n4.nabble.com>
	<2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>
	<CAJRuHop-Nki7K7ZAv22dEv2PBNMF4SH4sXGUGdSQY3y0NA40-w@mail.gmail.com>
Message-ID: <1441707402995-4711987.post@n4.nabble.com>

Thanks, Sergio.

Yes, resample looks promising. I'll try it out now.




--
View this message in context: http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907p4711987.html
Sent from the R help mailing list archive at Nabble.com.


From wewolski at gmail.com  Tue Sep  8 13:53:13 2015
From: wewolski at gmail.com (Witold E Wolski)
Date: Tue, 8 Sep 2015 13:53:13 +0200
Subject: [R] names in R list's
In-Reply-To: <2D0B5742-2595-4837-8371-BF5895C6B6D2@dcn.davis.CA.us>
References: <CAAjnpdgHgYG3xUOECypMOV4aP6M=R6DgDuqmDmJJ6wm79fzjcQ@mail.gmail.com>
	<2D0B5742-2595-4837-8371-BF5895C6B6D2@dcn.davis.CA.us>
Message-ID: <CAAjnpdiSqTiVXoz6Ksob_G+ZrB2ZAzbdNZXSWrQ_MH9QQ9AGzw@mail.gmail.com>

Hi Jeff,

Indeed there was something about plain-text in the r-help posting
guide although I can't find it there anymore.
https://www.r-project.org/posting-guide.html

Is it still an requirement?

Jeff, thanks for you constructive contribution ;) . Glad that you know
about plain text mode in e-mails, beside doing some perl programming.
I forgot about both. Even the linux admin's I know use python and
thunderbird or some webmail nowadays not pine and perl, but I do not
much networking, so what do I know.

I think the question I am asking is legitimate. The access complexity
of datastructures is specified in the documentation in case of python
datastructures,  java collections or stl containers.
I guess this information is available for name access on R-list but I
just can't find it.




regards

On 7 September 2015 at 16:37, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> You puzzle me. Why does someone who cannot figure out how to post an email in plain text after so many messages on this mailing list get all worried about access time for string indexing?
>
> Environment objects have those properties. They do not solve all problems though, because they are rather heavyweight... you need a lot of lookups to pay for their overhead. R5 objects and the hash package both use them, but I have never found three need to use them. Yes, I do program in Perl so I know where you are coming from, but the vector-based name lookup used in R works quite effectively for data where the number of list items is short or where I plan to access every element as part of my data processing anyway.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 7, 2015 3:34:53 AM PDT, Witold E Wolski <wewolski at gmail.com> wrote:
>>What is the access time for R lists given a name of list element, is it
>>linear, log, or constant?
>>
>>Than what are to rules for names in R-lists
>>
>>That reusing names is possible makes me wonder.
>>
>>tmp <- as.list(c(1,2,3,4))
>>names(tmp) = c("a","a","b","b")
>>tmp
>>tmp$a
>>
>>
>>What I am looking for is a standard R data structure which will allow
>>me
>>for fast and name lookup.
>



-- 
Witold Eryk Wolski


From marc_schwartz at me.com  Tue Sep  8 14:03:31 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 08 Sep 2015 07:03:31 -0500
Subject: [R] names in R list's
In-Reply-To: <CAAjnpdiSqTiVXoz6Ksob_G+ZrB2ZAzbdNZXSWrQ_MH9QQ9AGzw@mail.gmail.com>
References: <CAAjnpdgHgYG3xUOECypMOV4aP6M=R6DgDuqmDmJJ6wm79fzjcQ@mail.gmail.com>
	<2D0B5742-2595-4837-8371-BF5895C6B6D2@dcn.davis.CA.us>
	<CAAjnpdiSqTiVXoz6Ksob_G+ZrB2ZAzbdNZXSWrQ_MH9QQ9AGzw@mail.gmail.com>
Message-ID: <ECBB4C96-3F84-47ED-ADFD-7733B2D90177@me.com>


> On Sep 8, 2015, at 6:53 AM, Witold E Wolski <wewolski at gmail.com> wrote:
> 
> Hi Jeff,
> 
> Indeed there was something about plain-text in the r-help posting
> guide although I can't find it there anymore.
> https://www.r-project.org/posting-guide.html
> 
> Is it still an requirement?

<snip>

Witold,

See the first bullet in the ?Technical details of posting? section:

"No HTML posting (harder to detect spam) (note that this is the default in some mail clients - you may have to turn it off). Note that chances have become relatively high for ?HTMLified? e-mails to be completely intercepted (without notice to the sender).?


Regards,

Marc Schwartz


From petr.pikal at precheza.cz  Tue Sep  8 14:04:00 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 8 Sep 2015 12:04:00 +0000
Subject: [R] help with reshape
In-Reply-To: <55EEB96A.8020504@univ-reims.fr>
References: <55EEB96A.8020504@univ-reims.fr>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>

Hi

I am not sure if I got it

library(reshape2)
mm<-melt(mydata, measure.vars="Moyenne")
test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem+value)

gives me 3 rows but names need some tweaking afterwards.

nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)], "_"), "[", 1)), sep=".")
names(test)[-(1:5)]<-nn

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
> Calandra
> Sent: Tuesday, September 08, 2015 12:33 PM
> To: R list
> Subject: [R] help with reshape
>
> Dear users,
>
> I'm having troubles with reshaping a data.frame from long to wide
> format.
> I copy the output of dput() at the end of the mail because it is quite
> long.
>
> Each row of the column "Elem" should be transposed to a new column. All
> variables "Etape", "Ech", "repet", "dilution", "Rincage" define the
> samples. Meaning that for each unique combination of these variables, I
> want a single row, and as many columns as elements in "Elem".
>
> So I tried:
> reshape(mydata, timevar="Elem",
> idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide",
> drop=c("ID","Nom_ech"))
>
> The problem is that some columns are not used at all for defining
> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing, but
> I don't understand why.
>
> Can you help me with that? I have no idea what I am doing wrong...
>
> Thanks in advance,
> Ivan
>
>
>
> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549, 550,
> 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 11574,
> 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583, 683,
> 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697,
> 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100",
> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1", "S1", "S1", "S1", "S1"), Ech = c("B2", "B2", "B2", "B2", "B2",
> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
> "B2", "B2", "B2", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution = c(1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
> 100, 100, 100, 100, 100, 100, 100, 100, 100), Rincage = c("non", "non",
> "non", "non", "non", "non", "non", "non", "non", "non", "non", "non",
> "non", "non", "non", "non", "non", "non", "non", "non", "oui", "oui",
> "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "non", "non",
> "non", "non", "non", "non", "non", "non", "non", "non", "non", "non",
> "non", "non", "non", "non", "non", "non", "non", "non"), Elem =
> c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286",
> "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", "Fe2382", "Fe2599",
> "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605", "Ni2216", "Al1670",
> "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378",
> "Cr2055", "Cr2835", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247",
> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605",
> "Ni2216"), Moyenne = c(NA, NA, 3.7455, 596.166666666667, 578.2, 0.1514,
> NA, 1.87225, NA, 0.3664, 1.859, 1.96766666666667, NA, NA,
> 9.29566666666667, 13.08, 12.69, 0.26875, 0.2877, 0.2395,
> 5732.33333333333, 3.9615, 3.48833333333333, 337.433333333333,
> 323.533333333333, 0.877166666666667, 0.292466666666667, NA,
> 1.79566666666667, NA, 106.326666666667, NA, 2.2755, 291.933333333333,
> 278.833333333333, 0.819, NA, 3.946, NA, NA, 1.47766666666667,
> 1.63266666666667, 40.44, 40.2533333333333, 128.9, 50.11, 49.02,
> 37.4733333333333, 37.775, 0.3764)), .Names = c("ID", "Nom_ech",
> "Etape", "Ech", "repet", "dilution", "Rincage", "Elem", "Moyenne"),
> row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
> 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L, 32L, 33L, 34L, 35L,
> 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L, 118L, 119L, 120L,
> 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L ),
> class = c("tbl_df", "tbl", "data.frame"))
>
> --
>
> Ivan Calandra, PhD
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ivan.calandra at univ-reims.fr  Tue Sep  8 14:17:41 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Tue, 8 Sep 2015 14:17:41 +0200
Subject: [R] help with reshape
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
Message-ID: <55EED1E5.1080703@univ-reims.fr>

Thank you Petr,

It kinda works, but not completely. The problem is that it produces a 
column for each value ("Moyenne"), and not each element of "Elem". That 
means I have only one value per column, instead of up to 3.
For example, I have 3 columns for Al1670 instead of just one, and each 
column contains maximum one value (the others being NA).

Not sure I am being clear...

By the way, I don't understand why my solution did not work; what is 
wrong there?

Thank you again!
Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 08/09/15 14:04, PIKAL Petr a ?crit :
> Hi
>
> I am not sure if I got it
>
> library(reshape2)
> mm<-melt(mydata, measure.vars="Moyenne")
> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem+value)
>
> gives me 3 rows but names need some tweaking afterwards.
>
> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)], "_"), "[", 1)), sep=".")
> names(test)[-(1:5)]<-nn
>
> Cheers
> Petr
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>> Calandra
>> Sent: Tuesday, September 08, 2015 12:33 PM
>> To: R list
>> Subject: [R] help with reshape
>>
>> Dear users,
>>
>> I'm having troubles with reshaping a data.frame from long to wide
>> format.
>> I copy the output of dput() at the end of the mail because it is quite
>> long.
>>
>> Each row of the column "Elem" should be transposed to a new column. All
>> variables "Etape", "Ech", "repet", "dilution", "Rincage" define the
>> samples. Meaning that for each unique combination of these variables, I
>> want a single row, and as many columns as elements in "Elem".
>>
>> So I tried:
>> reshape(mydata, timevar="Elem",
>> idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide",
>> drop=c("ID","Nom_ech"))
>>
>> The problem is that some columns are not used at all for defining
>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing, but
>> I don't understand why.
>>
>> Can you help me with that? I have no idea what I am doing wrong...
>>
>> Thanks in advance,
>> Ivan
>>
>>
>>
>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549, 550,
>> 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 11574,
>> 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583, 683,
>> 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697,
>> 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100",
>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1", "S1", "S1", "S1", "S1"), Ech = c("B2", "B2", "B2", "B2", "B2",
>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>> "B2", "B2", "B2", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1,
>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution = c(1,
>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>> 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
>> 100, 100, 100, 100, 100, 100, 100, 100, 100), Rincage = c("non", "non",
>> "non", "non", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non", "non", "non", "non", "non", "non", "non", "non", "oui", "oui",
>> "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "non", "non",
>> "non", "non", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non", "non", "non", "non", "non", "non", "non", "non"), Elem =
>> c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286",
>> "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", "Fe2382", "Fe2599",
>> "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605", "Ni2216", "Al1670",
>> "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378",
>> "Cr2055", "Cr2835", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247",
>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605",
>> "Ni2216"), Moyenne = c(NA, NA, 3.7455, 596.166666666667, 578.2, 0.1514,
>> NA, 1.87225, NA, 0.3664, 1.859, 1.96766666666667, NA, NA,
>> 9.29566666666667, 13.08, 12.69, 0.26875, 0.2877, 0.2395,
>> 5732.33333333333, 3.9615, 3.48833333333333, 337.433333333333,
>> 323.533333333333, 0.877166666666667, 0.292466666666667, NA,
>> 1.79566666666667, NA, 106.326666666667, NA, 2.2755, 291.933333333333,
>> 278.833333333333, 0.819, NA, 3.946, NA, NA, 1.47766666666667,
>> 1.63266666666667, 40.44, 40.2533333333333, 128.9, 50.11, 49.02,
>> 37.4733333333333, 37.775, 0.3764)), .Names = c("ID", "Nom_ech",
>> "Etape", "Ech", "repet", "dilution", "Rincage", "Elem", "Moyenne"),
>> row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
>> 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L, 32L, 33L, 34L, 35L,
>> 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L, 118L, 119L, 120L,
>> 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L ),
>> class = c("tbl_df", "tbl", "data.frame"))
>>
>> --
>>
>> Ivan Calandra, PhD
>> University of Reims Champagne-Ardenne
>> GEGENAA - EA 3795
>> CREA - 2 esplanade Roland Garros
>> 51100 Reims, France
>> +33(0)3 26 77 36 89
>> ivan.calandra at univ-reims.fr
>> https://www.researchgate.net/profile/Ivan_Calandra
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From petr.pikal at precheza.cz  Tue Sep  8 14:25:19 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 8 Sep 2015 12:25:19 +0000
Subject: [R] Modelling categorical variables
In-Reply-To: <CAGCTXOji9z4v6OG7JB6ur-2oZXWYZjDjU-1AHuf0baCzjBU7MA@mail.gmail.com>
References: <CAGCTXOji9z4v6OG7JB6ur-2oZXWYZjDjU-1AHuf0baCzjBU7MA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC9F@SRVEXCHMBX.precheza.cz>

Hi

You probably wont get many answers because:

1 -you post in HTML (post in plain text)
2 -you provide data which are unreadable (copy output of dput(yourdata) instead)
3 -you ask statistical question which are rarely answered here (they are better suited to stackexchange list)

Regarding your models nothing prevents you to test any of them - lm, glm, ...
Or go through some available documents on CRAN like e.g.

Using R for Data Analysis and Graphics - Introduction, Examples and Commentary? by John Maindonald (PDF, data sets and scripts are available at JM's homepage).
?Practical Regression and Anova using R? by Julian Faraway (PDF, data sets and scripts are available at the book homepage).

among many others to learn how to use R for modelling.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jessica
> Lavabre
> Sent: Tuesday, September 08, 2015 11:01 AM
> To: r-help at r-project.org
> Subject: [R] Modelling categorical variables
>
> Hi,
>
> I am a beginner with statistics and R and have no clue on how to model
> my data. I have collected information on seed traps (ID) that includes
> the habitat type (Hab) and different measures of distances. Also I have
> applied a modularity analysis, so that the seeds traps are grouped into
> modules. My dataset is as follow:
>
>
>
> *ID    Hab  Module    DistEdge    MeanDist1    MeanDist2    MeanDist3
> F48    F       A         21.768       24.941          6.033
> 27.642    F50    F       E         35.666**       60.505
> 149.927 *
> *       48.582   F52    F       B**         12.243**       103.041
> 72.908  *
> *        102.375    N02    N      B **        58.681**
> 129.59          127.344   *
> *     131.383   N17    N      B**         62.829**       72.827 **
> 76.736  *
> *        77.644  N22    N      B**         89.207**       78.719  **
> 75.005   *
> *       81.176N33    N      A**         23.288**       35.48    **
> 25.317 *
> *         36.931    N40    N      B**         36.734**       62.234 **
> 30.68   *
> *         61.885    N47    N      E  **       60.443**       66.367  **
> 150.892 **       55.097   *
>
> I am looking for a way to analyze if there is any correlation between
> the Module classification and the other variables. My difficulties here
> are:
> 1 - is there a way to model my data where Module is the response
> variable (something like Module~Hab*DistEdge*MeanDist1) ? If so, which
> model should I use (I only have a bit of experience with glm) and which
> distribution?
> 2 - Is that a problem if I have different types of predictor variable
> (factor and numerical)?
>
> Any help would be greatly appreciated,
>
> -- Jessica Lavabre-Micas
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From petr.pikal at precheza.cz  Tue Sep  8 14:45:56 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 8 Sep 2015 12:45:56 +0000
Subject: [R] help with reshape
In-Reply-To: <55EED1E5.1080703@univ-reims.fr>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>

Hi

I looked into docs to reshape2 and played around a bit and by some magical feature

test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)

probably works as you expect.

I cannot comment your solution as I use reshape only sparsely.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
> Calandra
> Sent: Tuesday, September 08, 2015 2:18 PM
> To: R list
> Subject: Re: [R] help with reshape
>
> Thank you Petr,
>
> It kinda works, but not completely. The problem is that it produces a
> column for each value ("Moyenne"), and not each element of "Elem". That
> means I have only one value per column, instead of up to 3.
> For example, I have 3 columns for Al1670 instead of just one, and each
> column contains maximum one value (the others being NA).
>
> Not sure I am being clear...
>
> By the way, I don't understand why my solution did not work; what is
> wrong there?
>
> Thank you again!
> Ivan
>
> --
> Ivan Calandra, PhD
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
>
> Le 08/09/15 14:04, PIKAL Petr a ?crit :
> > Hi
> >
> > I am not sure if I got it
> >
> > library(reshape2)
> > mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
> > Etape+Ech+repet+dilution+Rincage~Elem+value)
> >
> > gives me 3 rows but names need some tweaking afterwards.
> >
> > nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
> "_"),
> > "[", 1)), sep=".") names(test)[-(1:5)]<-nn
> >
> > Cheers
> > Petr
> >
> >
> >> -----Original Message-----
> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
> >> Calandra
> >> Sent: Tuesday, September 08, 2015 12:33 PM
> >> To: R list
> >> Subject: [R] help with reshape
> >>
> >> Dear users,
> >>
> >> I'm having troubles with reshaping a data.frame from long to wide
> >> format.
> >> I copy the output of dput() at the end of the mail because it is
> >> quite long.
> >>
> >> Each row of the column "Elem" should be transposed to a new column.
> >> All variables "Etape", "Ech", "repet", "dilution", "Rincage" define
> >> the samples. Meaning that for each unique combination of these
> >> variables, I want a single row, and as many columns as elements in
> "Elem".
> >>
> >> So I tried:
> >> reshape(mydata, timevar="Elem",
> >> idvar=c("Etape","Ech","repet","dilution","Rincage"),
> >> direction="wide",
> >> drop=c("ID","Nom_ech"))
> >>
> >> The problem is that some columns are not used at all for defining
> >> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing,
> >> but I don't understand why.
> >>
> >> Can you help me with that? I have no idea what I am doing wrong...
> >>
> >> Thanks in advance,
> >> Ivan
> >>
> >>
> >>
> >> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549,
> >> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
> >> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
> 11583,
> >> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
> 696,
> >> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
> >> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
> >> B2",
> >> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
> >> B2",
> >> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
> >> B2",
> >> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> >> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> >> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> >> *100",
> >> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
> >> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2", "B2",
> >> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
> >> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3", "F10-3",
> >> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-
> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
> >> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> >> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1,
> 1,
> >> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
> >> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> >> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100, 100,
> >> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), Rincage =
> >> c("non", "non", "non", "non", "non", "non", "non", "non", "non",
> >> "non", "non", "non", "non", "non", "non", "non", "non", "non",
> "non",
> >> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
> "oui",
> >> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
> "non",
> >> "non", "non", "non", "non", "non", "non", "non", "non", "non",
> "non",
> >> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> >> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
> "Cu3247",
> >> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
> "Mn2605",
> >> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> "Cd2288",
> >> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
> "As1937",
> >> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
> "Cr2835",
> >> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
> "Mg2852",
> >> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
> >> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
> >> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875,
> >> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
> >> 337.433333333333, 323.533333333333, 0.877166666666667,
> >> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667, NA,
> >> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
> NA,
> >> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333, 128.9,
> >> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names = c("ID",
> >> "Nom_ech", "Etape", "Ech", "repet", "dilution", "Rincage", "Elem",
> >> "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
> >> 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L,
> 32L,
> >> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
> >> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
> >> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
> >>
> >> --
> >>
> >> Ivan Calandra, PhD
> >> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
> >> esplanade Roland Garros 51100 Reims, France
> >> +33(0)3 26 77 36 89
> >> ivan.calandra at univ-reims.fr
> >> https://www.researchgate.net/profile/Ivan_Calandra
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-
> >> guide.html and provide commented, minimal, self-contained,
> >> reproducible code.
> > ________________________________

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ivan.calandra at univ-reims.fr  Tue Sep  8 14:55:34 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Tue, 8 Sep 2015 14:55:34 +0200
Subject: [R] help with reshape
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
Message-ID: <55EEDAC6.9020202@univ-reims.fr>

Thanks Petr,
It looks good, but I have to check in more details.

Can anyone help me with my original solution using reshape()? I'd like 
to understand what I did wrong.
reshape(mydata, timevar="Elem", 
idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide", 
drop=c("ID","Nom_ech"))

Thank you in advance
Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 08/09/15 14:45, PIKAL Petr a ?crit :
> Hi
>
> I looked into docs to reshape2 and played around a bit and by some magical feature
>
> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
>
> probably works as you expect.
>
> I cannot comment your solution as I use reshape only sparsely.
>
> Cheers
> Petr
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>> Calandra
>> Sent: Tuesday, September 08, 2015 2:18 PM
>> To: R list
>> Subject: Re: [R] help with reshape
>>
>> Thank you Petr,
>>
>> It kinda works, but not completely. The problem is that it produces a
>> column for each value ("Moyenne"), and not each element of "Elem". That
>> means I have only one value per column, instead of up to 3.
>> For example, I have 3 columns for Al1670 instead of just one, and each
>> column contains maximum one value (the others being NA).
>>
>> Not sure I am being clear...
>>
>> By the way, I don't understand why my solution did not work; what is
>> wrong there?
>>
>> Thank you again!
>> Ivan
>>
>> --
>> Ivan Calandra, PhD
>> University of Reims Champagne-Ardenne
>> GEGENAA - EA 3795
>> CREA - 2 esplanade Roland Garros
>> 51100 Reims, France
>> +33(0)3 26 77 36 89
>> ivan.calandra at univ-reims.fr
>> https://www.researchgate.net/profile/Ivan_Calandra
>>
>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
>>> Hi
>>>
>>> I am not sure if I got it
>>>
>>> library(reshape2)
>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
>>>
>>> gives me 3 rows but names need some tweaking afterwards.
>>>
>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
>> "_"),
>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
>>>
>>> Cheers
>>> Petr
>>>
>>>
>>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>>>> Calandra
>>>> Sent: Tuesday, September 08, 2015 12:33 PM
>>>> To: R list
>>>> Subject: [R] help with reshape
>>>>
>>>> Dear users,
>>>>
>>>> I'm having troubles with reshaping a data.frame from long to wide
>>>> format.
>>>> I copy the output of dput() at the end of the mail because it is
>>>> quite long.
>>>>
>>>> Each row of the column "Elem" should be transposed to a new column.
>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage" define
>>>> the samples. Meaning that for each unique combination of these
>>>> variables, I want a single row, and as many columns as elements in
>> "Elem".
>>>> So I tried:
>>>> reshape(mydata, timevar="Elem",
>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>>> direction="wide",
>>>> drop=c("ID","Nom_ech"))
>>>>
>>>> The problem is that some columns are not used at all for defining
>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing,
>>>> but I don't understand why.
>>>>
>>>> Can you help me with that? I have no idea what I am doing wrong...
>>>>
>>>> Thanks in advance,
>>>> Ivan
>>>>
>>>>
>>>>
>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549,
>>>> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
>>>> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
>> 11583,
>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
>> 696,
>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>> B2",
>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>> B2",
>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>> B2",
>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>>>> *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2", "B2",
>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3", "F10-3",
>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-
>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1,
>> 1,
>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>>>> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100, 100,
>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), Rincage =
>>>> c("non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non",
>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
>> "oui",
>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non",
>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non",
>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
>> "Cu3247",
>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
>> "Mn2605",
>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>> "Cd2288",
>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
>> "As1937",
>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
>> "Cr2835",
>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>> "Mg2852",
>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875,
>>>> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
>>>> 337.433333333333, 323.533333333333, 0.877166666666667,
>>>> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667, NA,
>>>> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
>> NA,
>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333, 128.9,
>>>> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names = c("ID",
>>>> "Nom_ech", "Etape", "Ech", "repet", "dilution", "Rincage", "Elem",
>>>> "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>>>> 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L,
>> 32L,
>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
>>>>
>>>> --
>>>>
>>>> Ivan Calandra, PhD
>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>> esplanade Roland Garros 51100 Reims, France
>>>> +33(0)3 26 77 36 89
>>>> ivan.calandra at univ-reims.fr
>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>> guide.html and provide commented, minimal, self-contained,
>>>> reproducible code.
>>> ________________________________
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From petr.pikal at precheza.cz  Tue Sep  8 15:12:00 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 8 Sep 2015 13:12:00 +0000
Subject: [R] help with reshape
In-Reply-To: <55EEDAC6.9020202@univ-reims.fr>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECE1@SRVEXCHMBX.precheza.cz>

Hi

based on your data I got 3 lines with similar results as I got from melt/dcast only the rows are not in the same order.

> version
               _
platform       i386-w64-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status         Under development (unstable)
major          3
minor          3.0
year           2015
month          06
day            15
svn rev        68521
language       R
version.string R Under development (unstable) (2015-06-15 r68521)
nickname       Unsuffered Consequences
>

> sessionInfo()
R Under development (unstable) (2015-06-15 r68521)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows XP (build 2600) Service Pack 3

locale:
[1] LC_COLLATE=Czech_Czech Republic.1250  LC_CTYPE=Czech_Czech Republic.1250    LC_MONETARY=Czech_Czech Republic.1250
[4] LC_NUMERIC=C                          LC_TIME=Czech_Czech Republic.1250

attached base packages:
[1] stats     datasets  utils     grDevices graphics  methods   base

other attached packages:
[1] reshape2_1.4.1  lattice_0.20-31 fun_0.1

loaded via a namespace (and not attached):
 [1] Rcpp_0.11.6      digest_0.6.8     MASS_7.3-40      grid_3.3.0       plyr_1.8.3       nlme_3.1-120     gtable_0.1.2     magrittr_1.5
 [9] scales_0.2.5     ggplot2_1.0.1    stringi_0.4-1    proto_0.3-10     tools_3.3.0      stringr_1.0.0    munsell_0.4.2    colorspace_1.2-6
>

test2<-reshape(mydata, timevar="Elem", idvar=c("Etape","Ech","repet","dilution","Rincage"),
direction="wide", drop=c("ID","Nom_ech"))

> dput(test2)
structure(list(Etape = c("S1", "S1", "S1"), Ech = c("B2", "F10-3",
"F10-\n 3"), repet = c(NA, 1, 1), dilution = c(1, 100, 100),
    Rincage = c("non", "non", "non"), Moyenne.Al1670 = c(NA,
    106.326666666667, NA), Moyenne.As1890 = c(NA_real_, NA_real_,
    NA_real_), Moyenne.As1937 = c(3.7455, 2.2755, NA), Moyenne.Ca1840 = c(596.166666666667,
    291.933333333333, NA), Moyenne.Ca3158 = c(578.2, 278.833333333333,
    NA), Moyenne.Cd2288 = c(0.1514, 0.819, NA), Moyenne.Co2286 = c(NA_real_,
    NA_real_, NA_real_), Moyenne.Co2378 = c(1.87225, 3.946, NA
    ), Moyenne.Cr2055 = c(NA_real_, NA_real_, NA_real_), Moyenne.Cr2835 = c(0.3664,
    NA, NA), Moyenne.Cu2247 = c(1.859, 1.47766666666667, NA),
    Moyenne.Cu3247 = c(1.96766666666667, 1.63266666666667, NA
    ), Moyenne.Fe2382 = c(NA, 40.44, NA), Moyenne.Fe2599 = c(NA,
    40.2533333333333, NA), Moyenne.K_7664 = c(9.29566666666667,
    NA, 128.9), Moyenne.Mg2795 = c(13.08, 50.11, NA), Moyenne.Mg2852 = c(12.69,
    49.02, NA), Moyenne.Mn2576 = c(0.26875, 37.4733333333333,
    NA), Moyenne.Mn2605 = c(0.2877, 37.775, NA), Moyenne.Ni2216 = c(0.2395,
    0.3764, NA)), .Names = c("Etape", "Ech", "repet", "dilution",
"Rincage", "Moyenne.Al1670", "Moyenne.As1890", "Moyenne.As1937",
"Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288", "Moyenne.Co2286",
"Moyenne.Co2378", "Moyenne.Cr2055", "Moyenne.Cr2835", "Moyenne.Cu2247",
"Moyenne.Cu3247", "Moyenne.Fe2382", "Moyenne.Fe2599", "Moyenne.K_7664",
"Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576", "Moyenne.Mn2605",
"Moyenne.Ni2216"), row.names = c(1L, 112L, 126L), class = c("tbl_df",
"tbl", "data.frame"), reshapeWide = structure(list(v.names = NULL,
    timevar = "Elem", idvar = c("Etape", "Ech", "repet", "dilution",
    "Rincage"), times = c("Al1670", "As1890", "As1937", "Ca1840",
    "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
    "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
    "Mg2852", "Mn2576", "Mn2605", "Ni2216"), varying = structure(c("Moyenne.Al1670",
    "Moyenne.As1890", "Moyenne.As1937", "Moyenne.Ca1840", "Moyenne.Ca3158",
    "Moyenne.Cd2288", "Moyenne.Co2286", "Moyenne.Co2378", "Moyenne.Cr2055",
    "Moyenne.Cr2835", "Moyenne.Cu2247", "Moyenne.Cu3247", "Moyenne.Fe2382",
    "Moyenne.Fe2599", "Moyenne.K_7664", "Moyenne.Mg2795", "Moyenne.Mg2852",
    "Moyenne.Mn2576", "Moyenne.Mn2605", "Moyenne.Ni2216"), .Dim = c(1L,
    20L))), .Names = c("v.names", "timevar", "idvar", "times",
"varying")))
>

dput(test)
structure(list(Etape = c("S1", "S1", "S1", "S1"), Ech = c("B2",
"F10-\n 3", "F10-3", "F10-3"), repet = c(NA, 1, 1, NA), dilution = c(1,
100, 100, 1), Rincage = c("non", "non", "non", "oui"), Al1670 = c(NA,
NA, 106.326666666667, 5732.33333333333), As1890 = c(NA, NA, NA,
3.9615), As1937 = c(3.7455, NA, 2.2755, 3.48833333333333), Ca1840 = c(596.166666666667,
NA, 291.933333333333, 337.433333333333), Ca3158 = c(578.2, NA,
278.833333333333, 323.533333333333), Cd2288 = c(0.1514, NA, 0.819,
0.877166666666667), Co2286 = c(NA, NA, NA, 0.292466666666667),
    Co2378 = c(1.87225, NA, 3.946, NA), Cr2055 = c(NA, NA, NA,
    1.79566666666667), Cr2835 = c(0.3664, NA, NA, NA), Cu2247 = c(1.859,
    NA, 1.47766666666667, NA), Cu3247 = c(1.96766666666667, NA,
    1.63266666666667, NA), Fe2382 = c(NA, NA, 40.44, NA), Fe2599 = c(NA,
    NA, 40.2533333333333, NA), K_7664 = c(9.29566666666667, 128.9,
    NA, NA), Mg2795 = c(13.08, NA, 50.11, NA), Mg2852 = c(12.69,
    NA, 49.02, NA), Mn2576 = c(0.26875, NA, 37.4733333333333,
    NA), Mn2605 = c(0.2877, NA, 37.775, NA), Ni2216 = c(0.2395,
    NA, 0.3764, NA)), .Names = c("Etape", "Ech", "repet", "dilution",
"Rincage", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
"Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247",
"Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605",
"Ni2216"), row.names = c(NA, -4L), class = "data.frame")
>

Cheers
Petr

> -----Original Message-----
> From: Ivan Calandra [mailto:ivan.calandra at univ-reims.fr]
> Sent: Tuesday, September 08, 2015 2:56 PM
> To: PIKAL Petr; R list
> Subject: Re: [R] help with reshape
>
> Thanks Petr,
> It looks good, but I have to check in more details.
>
> Can anyone help me with my original solution using reshape()? I'd like
> to understand what I did wrong.
> reshape(mydata, timevar="Elem",
> idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide",
> drop=c("ID","Nom_ech"))
>
> Thank you in advance
> Ivan
>
> --
> Ivan Calandra, PhD
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
>
> Le 08/09/15 14:45, PIKAL Petr a ?crit :
> > Hi
> >
> > I looked into docs to reshape2 and played around a bit and by some
> > magical feature
> >
> > test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
> >
> > probably works as you expect.
> >
> > I cannot comment your solution as I use reshape only sparsely.
> >
> > Cheers
> > Petr
> >
> >> -----Original Message-----
> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
> >> Calandra
> >> Sent: Tuesday, September 08, 2015 2:18 PM
> >> To: R list
> >> Subject: Re: [R] help with reshape
> >>
> >> Thank you Petr,
> >>
> >> It kinda works, but not completely. The problem is that it produces
> a
> >> column for each value ("Moyenne"), and not each element of "Elem".
> >> That means I have only one value per column, instead of up to 3.
> >> For example, I have 3 columns for Al1670 instead of just one, and
> >> each column contains maximum one value (the others being NA).
> >>
> >> Not sure I am being clear...
> >>
> >> By the way, I don't understand why my solution did not work; what is
> >> wrong there?
> >>
> >> Thank you again!
> >> Ivan
> >>
> >> --
> >> Ivan Calandra, PhD
> >> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
> >> esplanade Roland Garros 51100 Reims, France
> >> +33(0)3 26 77 36 89
> >> ivan.calandra at univ-reims.fr
> >> https://www.researchgate.net/profile/Ivan_Calandra
> >>
> >> Le 08/09/15 14:04, PIKAL Petr a ?crit :
> >>> Hi
> >>>
> >>> I am not sure if I got it
> >>>
> >>> library(reshape2)
> >>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
> >>> Etape+Ech+repet+dilution+Rincage~Elem+value)
> >>>
> >>> gives me 3 rows but names need some tweaking afterwards.
> >>>
> >>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
> >> "_"),
> >>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
> >>>
> >>> Cheers
> >>> Petr
> >>>
> >>>
> >>>> -----Original Message-----
> >>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> >>>> Ivan Calandra
> >>>> Sent: Tuesday, September 08, 2015 12:33 PM
> >>>> To: R list
> >>>> Subject: [R] help with reshape
> >>>>
> >>>> Dear users,
> >>>>
> >>>> I'm having troubles with reshaping a data.frame from long to wide
> >>>> format.
> >>>> I copy the output of dput() at the end of the mail because it is
> >>>> quite long.
> >>>>
> >>>> Each row of the column "Elem" should be transposed to a new
> column.
> >>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage"
> define
> >>>> the samples. Meaning that for each unique combination of these
> >>>> variables, I want a single row, and as many columns as elements in
> >> "Elem".
> >>>> So I tried:
> >>>> reshape(mydata, timevar="Elem",
> >>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
> >>>> direction="wide",
> >>>> drop=c("ID","Nom_ech"))
> >>>>
> >>>> The problem is that some columns are not used at all for defining
> >>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing,
> >>>> but I don't understand why.
> >>>>
> >>>> Can you help me with that? I have no idea what I am doing wrong...
> >>>>
> >>>> Thanks in advance,
> >>>> Ivan
> >>>>
> >>>>
> >>>>
> >>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549,
> >>>> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
> >>>> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
> >> 11583,
> >>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
> >> 696,
> >>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
> >>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
> >>>> B2",
> >>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
> >>>> B2",
> >>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
> >>>> B2",
> >>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> >>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> >>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3
> >>>> (1) *100",
> >>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
> >>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2",
> >>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
> >>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3",
> >>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>> "F10-3", "F10-
> >> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
> >>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> >>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> NA,
> >>>> 1,
> >> 1,
> >>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
> >>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1,
> >>>> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100,
> >>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100),
> >>>> Rincage = c("non", "non", "non", "non", "non", "non", "non",
> "non",
> >>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
> >>>> "non",
> >> "non",
> >>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
> >> "oui",
> >>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
> >> "non",
> >>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
> >> "non",
> >>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> >>>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
> >> "Cu3247",
> >>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
> >> "Mn2605",
> >>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> >> "Cd2288",
> >>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
> >> "As1937",
> >>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
> >> "Cr2835",
> >>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
> >> "Mg2852",
> >>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
> >>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
> >>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875,
> >>>> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
> >>>> 337.433333333333, 323.533333333333, 0.877166666666667,
> >>>> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667, NA,
> >>>> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
> >> NA,
> >>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333,
> 128.9,
> >>>> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names = c("ID",
> >>>> "Nom_ech", "Etape", "Ech", "repet", "dilution", "Rincage", "Elem",
> >>>> "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
> >>>> 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L,
> >> 32L,
> >>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
> >>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
> >>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
> >>>>
> >>>>


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ivan.calandra at univ-reims.fr  Tue Sep  8 15:25:25 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Tue, 8 Sep 2015 15:25:25 +0200
Subject: [R] help with reshape
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECE1@SRVEXCHMBX.precheza.cz>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECE1@SRVEXCHMBX.precheza.cz>
Message-ID: <55EEE1C5.4020604@univ-reims.fr>

I do get only 2 lines:

mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549, 550, 
551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 11574, 
11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583, 683, 684, 
685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 
699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2", "Step1 B2", 
"Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", 
"Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", 
"Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "R1 S1 F1 
0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", 
"R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 
F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) 
*100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
"S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 
0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 
(1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) 
*100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", 
"S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"), Etape = c("S1", "S1", "S1", 
"S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", 
"S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", 
"S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", 
"S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = 
c("B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", 
"B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3", 
"F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", 
"F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", 
"F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", 
"F10-3", "F10-3", "F10-3", "F10-3"), repet = c(NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1), dilution = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 
100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 
100), Rincage = c("non", "non", "non", "non", "non", "non", "non", 
"non", "non", "non", "non", "non", "non", "non", "non", "non", "non", 
"non", "non", "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", 
"oui", "oui", "oui", "non", "non", "non", "non", "non", "non", "non", 
"non", "non", "non", "non", "non", "non", "non", "non", "non", "non", 
"non", "non", "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", 
"Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", 
"Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", 
"Mn2605", "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158", 
"Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890", 
"As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", 
"Cr2835", "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795", 
"Mg2852", "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455, 
596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859, 
1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875, 
0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333, 
337.433333333333, 323.533333333333, 0.877166666666667, 
0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667, NA, 
2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA, NA, 
1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333, 128.9, 
50.11, 49.02, 37.4733333333333,
37.775, 0.3764)), .Names = c("ID", "Nom_ech", "Etape", "Ech", "repet", 
"dilution", "Rincage", "Elem", "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 
20L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 
115L, 116L, 117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 
127L, 128L, 129L, 130L, 131L), class = c("tbl_df", "tbl", "data.frame"))


test2<-reshape(mydata, timevar="Elem", 
idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide", 
drop=c("ID","Nom_ech"))


dput(test2)
structure(list(Etape = c("S1", "S1"), Ech = c("B2", "F10-3"), repet = 
c(NA, 1), dilution = c(1, 100), Rincage = c("non", "non"), 
Moyenne.Al1670 = c(NA, 106.326666666667), Moyenne.As1890 = c(NA_real_, 
NA_real_), Moyenne.As1937 = c(3.7455, 2.2755), Moyenne.Ca1840 = 
c(596.166666666667, 291.933333333333), Moyenne.Ca3158 = c(578.2, 
278.833333333333), Moyenne.Cd2288 = c(0.1514, 0.819), Moyenne.Co2286 = 
c(NA_real_, NA_real_), Moyenne.Co2378 = c(1.87225, 3.946), 
Moyenne.Cr2055 = c(NA_real_, NA_real_), Moyenne.Cr2835 = c(0.3664, NA), 
Moyenne.Cu2247 = c(1.859, 1.47766666666667), Moyenne.Cu3247 = 
c(1.96766666666667, 1.63266666666667), Moyenne.Fe2382 = c(NA, 40.44), 
Moyenne.Fe2599 = c(NA, 40.2533333333333), Moyenne.K_7664 = 
c(9.29566666666667, 128.9), Moyenne.Mg2795 = c(13.08, 50.11), 
Moyenne.Mg2852 = c(12.69, 49.02), Moyenne.Mn2576 = c(0.26875, 
37.4733333333333), Moyenne.Mn2605 = c(0.2877, 37.775), Moyenne.Ni2216 = 
c(0.2395, 0.3764)), .Names = c("Etape", "Ech", "repet", "dilution", 
"Rincage", "Moyenne.Al1670", "Moyenne.As1890", "Moyenne.As1937", 
"Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288", "Moyenne.Co2286", 
"Moyenne.Co2378", "Moyenne.Cr2055", "Moyenne.Cr2835",
"Moyenne.Cu2247", "Moyenne.Cu3247", "Moyenne.Fe2382", "Moyenne.Fe2599", 
"Moyenne.K_7664", "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576", 
"Moyenne.Mn2605", "Moyenne.Ni2216"), row.names = c(1L, 112L), class = 
c("tbl_df", "tbl", "data.frame"), reshapeWide = structure(list(v.names = 
NULL, timevar = "Elem", idvar = c("Etape", "Ech", "repet", "dilution", 
"Rincage"), times = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158", 
"Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", 
"Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605", 
"Ni2216"), varying = structure(c("Moyenne.Al1670", "Moyenne.As1890", 
"Moyenne.As1937", "Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288", 
"Moyenne.Co2286", "Moyenne.Co2378", "Moyenne.Cr2055", "Moyenne.Cr2835", 
"Moyenne.Cu2247", "Moyenne.Cu3247", "Moyenne.Fe2382", "Moyenne.Fe2599", 
"Moyenne.K_7664", "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576", 
"Moyenne.Mn2605", "Moyenne.Ni2216"), .Dim = c(1L, 20L))), .Names = 
c("v.names", "timevar", "idvar", "times",
"varying")))


sessionInfo()
R version 3.2.1 (2015-06-18)
Platform: x86_64-apple-darwin10.8.0 (64-bit)
Running under: OS X 10.6.8 (Snow Leopard)
locale:
[1] fr_FR.UTF-8/fr_FR.UTF-8/fr_FR.UTF-8/C/fr_FR.UTF-8/fr_FR.UTF-8
attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base


I do not have the latest version installed, because there is no 3.2.2 
for Snow Leopard... Could this be the reason?

Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 08/09/15 15:12, PIKAL Petr a ?crit :
> Hi
>
> based on your data I got 3 lines with similar results as I got from melt/dcast only the rows are not in the same order.
>
>> version
>                 _
> platform       i386-w64-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status         Under development (unstable)
> major          3
> minor          3.0
> year           2015
> month          06
> day            15
> svn rev        68521
> language       R
> version.string R Under development (unstable) (2015-06-15 r68521)
> nickname       Unsuffered Consequences
>> sessionInfo()
> R Under development (unstable) (2015-06-15 r68521)
> Platform: i386-w64-mingw32/i386 (32-bit)
> Running under: Windows XP (build 2600) Service Pack 3
>
> locale:
> [1] LC_COLLATE=Czech_Czech Republic.1250  LC_CTYPE=Czech_Czech Republic.1250    LC_MONETARY=Czech_Czech Republic.1250
> [4] LC_NUMERIC=C                          LC_TIME=Czech_Czech Republic.1250
>
> attached base packages:
> [1] stats     datasets  utils     grDevices graphics  methods   base
>
> other attached packages:
> [1] reshape2_1.4.1  lattice_0.20-31 fun_0.1
>
> loaded via a namespace (and not attached):
>   [1] Rcpp_0.11.6      digest_0.6.8     MASS_7.3-40      grid_3.3.0       plyr_1.8.3       nlme_3.1-120     gtable_0.1.2     magrittr_1.5
>   [9] scales_0.2.5     ggplot2_1.0.1    stringi_0.4-1    proto_0.3-10     tools_3.3.0      stringr_1.0.0    munsell_0.4.2    colorspace_1.2-6
> test2<-reshape(mydata, timevar="Elem", idvar=c("Etape","Ech","repet","dilution","Rincage"),
> direction="wide", drop=c("ID","Nom_ech"))
>
>> dput(test2)
> structure(list(Etape = c("S1", "S1", "S1"), Ech = c("B2", "F10-3",
> "F10-\n 3"), repet = c(NA, 1, 1), dilution = c(1, 100, 100),
>      Rincage = c("non", "non", "non"), Moyenne.Al1670 = c(NA,
>      106.326666666667, NA), Moyenne.As1890 = c(NA_real_, NA_real_,
>      NA_real_), Moyenne.As1937 = c(3.7455, 2.2755, NA), Moyenne.Ca1840 = c(596.166666666667,
>      291.933333333333, NA), Moyenne.Ca3158 = c(578.2, 278.833333333333,
>      NA), Moyenne.Cd2288 = c(0.1514, 0.819, NA), Moyenne.Co2286 = c(NA_real_,
>      NA_real_, NA_real_), Moyenne.Co2378 = c(1.87225, 3.946, NA
>      ), Moyenne.Cr2055 = c(NA_real_, NA_real_, NA_real_), Moyenne.Cr2835 = c(0.3664,
>      NA, NA), Moyenne.Cu2247 = c(1.859, 1.47766666666667, NA),
>      Moyenne.Cu3247 = c(1.96766666666667, 1.63266666666667, NA
>      ), Moyenne.Fe2382 = c(NA, 40.44, NA), Moyenne.Fe2599 = c(NA,
>      40.2533333333333, NA), Moyenne.K_7664 = c(9.29566666666667,
>      NA, 128.9), Moyenne.Mg2795 = c(13.08, 50.11, NA), Moyenne.Mg2852 = c(12.69,
>      49.02, NA), Moyenne.Mn2576 = c(0.26875, 37.4733333333333,
>      NA), Moyenne.Mn2605 = c(0.2877, 37.775, NA), Moyenne.Ni2216 = c(0.2395,
>      0.3764, NA)), .Names = c("Etape", "Ech", "repet", "dilution",
> "Rincage", "Moyenne.Al1670", "Moyenne.As1890", "Moyenne.As1937",
> "Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288", "Moyenne.Co2286",
> "Moyenne.Co2378", "Moyenne.Cr2055", "Moyenne.Cr2835", "Moyenne.Cu2247",
> "Moyenne.Cu3247", "Moyenne.Fe2382", "Moyenne.Fe2599", "Moyenne.K_7664",
> "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576", "Moyenne.Mn2605",
> "Moyenne.Ni2216"), row.names = c(1L, 112L, 126L), class = c("tbl_df",
> "tbl", "data.frame"), reshapeWide = structure(list(v.names = NULL,
>      timevar = "Elem", idvar = c("Etape", "Ech", "repet", "dilution",
>      "Rincage"), times = c("Al1670", "As1890", "As1937", "Ca1840",
>      "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
>      "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>      "Mg2852", "Mn2576", "Mn2605", "Ni2216"), varying = structure(c("Moyenne.Al1670",
>      "Moyenne.As1890", "Moyenne.As1937", "Moyenne.Ca1840", "Moyenne.Ca3158",
>      "Moyenne.Cd2288", "Moyenne.Co2286", "Moyenne.Co2378", "Moyenne.Cr2055",
>      "Moyenne.Cr2835", "Moyenne.Cu2247", "Moyenne.Cu3247", "Moyenne.Fe2382",
>      "Moyenne.Fe2599", "Moyenne.K_7664", "Moyenne.Mg2795", "Moyenne.Mg2852",
>      "Moyenne.Mn2576", "Moyenne.Mn2605", "Moyenne.Ni2216"), .Dim = c(1L,
>      20L))), .Names = c("v.names", "timevar", "idvar", "times",
> "varying")))
> dput(test)
> structure(list(Etape = c("S1", "S1", "S1", "S1"), Ech = c("B2",
> "F10-\n 3", "F10-3", "F10-3"), repet = c(NA, 1, 1, NA), dilution = c(1,
> 100, 100, 1), Rincage = c("non", "non", "non", "oui"), Al1670 = c(NA,
> NA, 106.326666666667, 5732.33333333333), As1890 = c(NA, NA, NA,
> 3.9615), As1937 = c(3.7455, NA, 2.2755, 3.48833333333333), Ca1840 = c(596.166666666667,
> NA, 291.933333333333, 337.433333333333), Ca3158 = c(578.2, NA,
> 278.833333333333, 323.533333333333), Cd2288 = c(0.1514, NA, 0.819,
> 0.877166666666667), Co2286 = c(NA, NA, NA, 0.292466666666667),
>      Co2378 = c(1.87225, NA, 3.946, NA), Cr2055 = c(NA, NA, NA,
>      1.79566666666667), Cr2835 = c(0.3664, NA, NA, NA), Cu2247 = c(1.859,
>      NA, 1.47766666666667, NA), Cu3247 = c(1.96766666666667, NA,
>      1.63266666666667, NA), Fe2382 = c(NA, NA, 40.44, NA), Fe2599 = c(NA,
>      NA, 40.2533333333333, NA), K_7664 = c(9.29566666666667, 128.9,
>      NA, NA), Mg2795 = c(13.08, NA, 50.11, NA), Mg2852 = c(12.69,
>      NA, 49.02, NA), Mn2576 = c(0.26875, NA, 37.4733333333333,
>      NA), Mn2605 = c(0.2877, NA, 37.775, NA), Ni2216 = c(0.2395,
>      NA, 0.3764, NA)), .Names = c("Etape", "Ech", "repet", "dilution",
> "Rincage", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247",
> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605",
> "Ni2216"), row.names = c(NA, -4L), class = "data.frame")
> Cheers
> Petr
>
>> -----Original Message-----
>> From: Ivan Calandra [mailto:ivan.calandra at univ-reims.fr]
>> Sent: Tuesday, September 08, 2015 2:56 PM
>> To: PIKAL Petr; R list
>> Subject: Re: [R] help with reshape
>>
>> Thanks Petr,
>> It looks good, but I have to check in more details.
>>
>> Can anyone help me with my original solution using reshape()? I'd like
>> to understand what I did wrong.
>> reshape(mydata, timevar="Elem",
>> idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide",
>> drop=c("ID","Nom_ech"))
>>
>> Thank you in advance
>> Ivan
>>
>> --
>> Ivan Calandra, PhD
>> University of Reims Champagne-Ardenne
>> GEGENAA - EA 3795
>> CREA - 2 esplanade Roland Garros
>> 51100 Reims, France
>> +33(0)3 26 77 36 89
>> ivan.calandra at univ-reims.fr
>> https://www.researchgate.net/profile/Ivan_Calandra
>>
>> Le 08/09/15 14:45, PIKAL Petr a ?crit :
>>> Hi
>>>
>>> I looked into docs to reshape2 and played around a bit and by some
>>> magical feature
>>>
>>> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
>>>
>>> probably works as you expect.
>>>
>>> I cannot comment your solution as I use reshape only sparsely.
>>>
>>> Cheers
>>> Petr
>>>
>>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>>>> Calandra
>>>> Sent: Tuesday, September 08, 2015 2:18 PM
>>>> To: R list
>>>> Subject: Re: [R] help with reshape
>>>>
>>>> Thank you Petr,
>>>>
>>>> It kinda works, but not completely. The problem is that it produces
>> a
>>>> column for each value ("Moyenne"), and not each element of "Elem".
>>>> That means I have only one value per column, instead of up to 3.
>>>> For example, I have 3 columns for Al1670 instead of just one, and
>>>> each column contains maximum one value (the others being NA).
>>>>
>>>> Not sure I am being clear...
>>>>
>>>> By the way, I don't understand why my solution did not work; what is
>>>> wrong there?
>>>>
>>>> Thank you again!
>>>> Ivan
>>>>
>>>> --
>>>> Ivan Calandra, PhD
>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>> esplanade Roland Garros 51100 Reims, France
>>>> +33(0)3 26 77 36 89
>>>> ivan.calandra at univ-reims.fr
>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>
>>>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
>>>>> Hi
>>>>>
>>>>> I am not sure if I got it
>>>>>
>>>>> library(reshape2)
>>>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
>>>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
>>>>>
>>>>> gives me 3 rows but names need some tweaking afterwards.
>>>>>
>>>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
>>>> "_"),
>>>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
>>>>>
>>>>> Cheers
>>>>> Petr
>>>>>
>>>>>
>>>>>> -----Original Message-----
>>>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>>>>> Ivan Calandra
>>>>>> Sent: Tuesday, September 08, 2015 12:33 PM
>>>>>> To: R list
>>>>>> Subject: [R] help with reshape
>>>>>>
>>>>>> Dear users,
>>>>>>
>>>>>> I'm having troubles with reshaping a data.frame from long to wide
>>>>>> format.
>>>>>> I copy the output of dput() at the end of the mail because it is
>>>>>> quite long.
>>>>>>
>>>>>> Each row of the column "Elem" should be transposed to a new
>> column.
>>>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage"
>> define
>>>>>> the samples. Meaning that for each unique combination of these
>>>>>> variables, I want a single row, and as many columns as elements in
>>>> "Elem".
>>>>>> So I tried:
>>>>>> reshape(mydata, timevar="Elem",
>>>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>>>>> direction="wide",
>>>>>> drop=c("ID","Nom_ech"))
>>>>>>
>>>>>> The problem is that some columns are not used at all for defining
>>>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing,
>>>>>> but I don't understand why.
>>>>>>
>>>>>> Can you help me with that? I have no idea what I am doing wrong...
>>>>>>
>>>>>> Thanks in advance,
>>>>>> Ivan
>>>>>>
>>>>>>
>>>>>>
>>>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549,
>>>>>> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
>>>>>> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
>>>> 11583,
>>>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
>>>> 696,
>>>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>>> B2",
>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>>> B2",
>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>>> B2",
>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3
>>>>>> (1) *100",
>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
>>>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2",
>>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3",
>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>> "F10-3", "F10-
>>>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
>>>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA,
>>>>>> 1,
>>>> 1,
>>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
>>>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>> 1,
>>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100,
>>>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100),
>>>>>> Rincage = c("non", "non", "non", "non", "non", "non", "non",
>> "non",
>>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>>> "non",
>>>> "non",
>>>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
>>>> "oui",
>>>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
>>>> "non",
>>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>> "non",
>>>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>>>>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
>>>> "Cu3247",
>>>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
>>>> "Mn2605",
>>>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>>> "Cd2288",
>>>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
>>>> "As1937",
>>>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
>>>> "Cr2835",
>>>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>>>> "Mg2852",
>>>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
>>>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
>>>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875,
>>>>>> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
>>>>>> 337.433333333333, 323.533333333333, 0.877166666666667,
>>>>>> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667, NA,
>>>>>> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
>>>> NA,
>>>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333,
>> 128.9,
>>>>>> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names = c("ID",
>>>>>> "Nom_ech", "Etape", "Ech", "repet", "dilution", "Rincage", "Elem",
>>>>>> "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>>>>>> 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L,
>>>> 32L,
>>>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
>>>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
>>>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
>>>>>>
>>>>>>
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From jdnewmil at dcn.davis.CA.us  Tue Sep  8 15:30:53 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 08 Sep 2015 06:30:53 -0700
Subject: [R] names in R list's
In-Reply-To: <ECBB4C96-3F84-47ED-ADFD-7733B2D90177@me.com>
References: <CAAjnpdgHgYG3xUOECypMOV4aP6M=R6DgDuqmDmJJ6wm79fzjcQ@mail.gmail.com>
	<2D0B5742-2595-4837-8371-BF5895C6B6D2@dcn.davis.CA.us>
	<CAAjnpdiSqTiVXoz6Ksob_G+ZrB2ZAzbdNZXSWrQ_MH9QQ9AGzw@mail.gmail.com>
	<ECBB4C96-3F84-47ED-ADFD-7733B2D90177@me.com>
Message-ID: <8945DB2F-D3FD-4F74-847F-9294707636CD@dcn.davis.CA.us>

Which answers why the list strips HTML out, but the reason we should compose in plain text is so we see what our readers will see. The stripping sometimes makes the result nearly impossible to read and deters people from wading in to give an answer. In addition, some HTML editors act like word processors and do things like substitute curly quotes in place of normal quotes, which gives R fits in examples.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 8, 2015 5:03:31 AM PDT, Marc Schwartz <marc_schwartz at me.com> wrote:
>
>> On Sep 8, 2015, at 6:53 AM, Witold E Wolski <wewolski at gmail.com>
>wrote:
>> 
>> Hi Jeff,
>> 
>> Indeed there was something about plain-text in the r-help posting
>> guide although I can't find it there anymore.
>> https://www.r-project.org/posting-guide.html
>> 
>> Is it still an requirement?
>
><snip>
>
>Witold,
>
>See the first bullet in the ?Technical details of posting? section:
>
>"No HTML posting (harder to detect spam) (note that this is the default
>in some mail clients - you may have to turn it off). Note that chances
>have become relatively high for ?HTMLified? e-mails to be completely
>intercepted (without notice to the sender).?
>
>
>Regards,
>
>Marc Schwartz


From petr.pikal at precheza.cz  Tue Sep  8 15:42:56 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 8 Sep 2015 13:42:56 +0000
Subject: [R] help with reshape
In-Reply-To: <55EEE1C5.4020604@univ-reims.fr>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECE1@SRVEXCHMBX.precheza.cz>
	<55EEE1C5.4020604@univ-reims.fr>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ED1D@SRVEXCHMBX.precheza.cz>

Hi


Well, I cannot help you any further. It can be platform or version issue. You are using Mac, I work with PC, your R version is 3.2.1 mine is 3.3.0 (devel). Wizards from R core are appropriate for answering this. You can transfer our conversation to R-sig-Mac, if people using Mac found similar issue as you.

If there is R-devel for Mac you can try to install it and test your code there. If you still get the same result you can check it on different computer, maybe with some other OS.

Sorry that I cannot give you any positive answer. I hope that melt/dcast code can give you desired result, though.

Cheers
Petr

> -----Original Message-----
> From: Ivan Calandra [mailto:ivan.calandra at univ-reims.fr]
> Sent: Tuesday, September 08, 2015 3:25 PM
> To: PIKAL Petr; R list
> Subject: Re: [R] help with reshape
>
> I do get only 2 lines:
>
> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549, 550,
> 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 11574,
> 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583, 683,
> 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697,
> 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2", "Step1
> B2",
> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "R1 S1 F1
> 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1
> F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1
> F1
> 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3
> (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100",
> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"), Etape = c("S1", "S1",
> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1"), Ech = c("B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-
> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"), repet = c(NA,
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution = c(1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 100,
> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
> 100, 100, 100, 100, 100), Rincage = c("non", "non", "non", "non",
> "non", "non", "non", "non", "non", "non", "non", "non", "non", "non",
> "non", "non", "non", "non", "non", "non", "oui", "oui", "oui", "oui",
> "oui", "oui", "oui", "oui", "oui", "oui", "non", "non", "non", "non",
> "non", "non", "non", "non", "non", "non", "non", "non", "non", "non",
> "non", "non", "non", "non", "non", "non"), Elem = c("Al1670", "As1890",
> "As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
> "Cr2835", "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
> "Mg2852", "Mn2576", "Mn2605", "Ni2216", "Al1670", "As1890", "As1937",
> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
> "Al1670", "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286",
> "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", "Fe2382", "Fe2599",
> "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605", "Ni2216"), Moyenne =
> c(NA, NA, 3.7455, 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA,
> 0.3664, 1.859, 1.96766666666667, NA, NA, 9.29566666666667, 13.08,
> 12.69, 0.26875, 0.2877, 0.2395, 5732.33333333333, 3.9615,
> 3.48833333333333, 337.433333333333, 323.533333333333,
> 0.877166666666667, 0.292466666666667, NA, 1.79566666666667, NA,
> 106.326666666667, NA, 2.2755, 291.933333333333, 278.833333333333,
> 0.819, NA, 3.946, NA, NA, 1.47766666666667, 1.63266666666667, 40.44,
> 40.2533333333333, 128.9, 50.11, 49.02, 37.4733333333333, 37.775,
> 0.3764)), .Names = c("ID", "Nom_ech", "Etape", "Ech", "repet",
> "dilution", "Rincage", "Elem", "Moyenne"), row.names = c(1L, 2L, 3L,
> 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
> 19L, 20L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L,
> 114L, 115L, 116L, 117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L,
> 126L, 127L, 128L, 129L, 130L, 131L), class = c("tbl_df", "tbl",
> "data.frame"))
>
>
> test2<-reshape(mydata, timevar="Elem",
> idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide",
> drop=c("ID","Nom_ech"))
>
>
> dput(test2)
> structure(list(Etape = c("S1", "S1"), Ech = c("B2", "F10-3"), repet =
> c(NA, 1), dilution = c(1, 100), Rincage = c("non", "non"),
> Moyenne.Al1670 = c(NA, 106.326666666667), Moyenne.As1890 = c(NA_real_,
> NA_real_), Moyenne.As1937 = c(3.7455, 2.2755), Moyenne.Ca1840 =
> c(596.166666666667, 291.933333333333), Moyenne.Ca3158 = c(578.2,
> 278.833333333333), Moyenne.Cd2288 = c(0.1514, 0.819), Moyenne.Co2286 =
> c(NA_real_, NA_real_), Moyenne.Co2378 = c(1.87225, 3.946),
> Moyenne.Cr2055 = c(NA_real_, NA_real_), Moyenne.Cr2835 = c(0.3664, NA),
> Moyenne.Cu2247 = c(1.859, 1.47766666666667), Moyenne.Cu3247 =
> c(1.96766666666667, 1.63266666666667), Moyenne.Fe2382 = c(NA, 40.44),
> Moyenne.Fe2599 = c(NA, 40.2533333333333), Moyenne.K_7664 =
> c(9.29566666666667, 128.9), Moyenne.Mg2795 = c(13.08, 50.11),
> Moyenne.Mg2852 = c(12.69, 49.02), Moyenne.Mn2576 = c(0.26875,
> 37.4733333333333), Moyenne.Mn2605 = c(0.2877, 37.775), Moyenne.Ni2216 =
> c(0.2395, 0.3764)), .Names = c("Etape", "Ech", "repet", "dilution",
> "Rincage", "Moyenne.Al1670", "Moyenne.As1890", "Moyenne.As1937",
> "Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288", "Moyenne.Co2286",
> "Moyenne.Co2378", "Moyenne.Cr2055", "Moyenne.Cr2835", "Moyenne.Cu2247",
> "Moyenne.Cu3247", "Moyenne.Fe2382", "Moyenne.Fe2599", "Moyenne.K_7664",
> "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576", "Moyenne.Mn2605",
> "Moyenne.Ni2216"), row.names = c(1L, 112L), class = c("tbl_df", "tbl",
> "data.frame"), reshapeWide = structure(list(v.names = NULL, timevar =
> "Elem", idvar = c("Etape", "Ech", "repet", "dilution", "Rincage"),
> times = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288",
> "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", "Fe2382",
> "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605", "Ni2216"),
> varying = structure(c("Moyenne.Al1670", "Moyenne.As1890",
> "Moyenne.As1937", "Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288",
> "Moyenne.Co2286", "Moyenne.Co2378", "Moyenne.Cr2055", "Moyenne.Cr2835",
> "Moyenne.Cu2247", "Moyenne.Cu3247", "Moyenne.Fe2382", "Moyenne.Fe2599",
> "Moyenne.K_7664", "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576",
> "Moyenne.Mn2605", "Moyenne.Ni2216"), .Dim = c(1L, 20L))), .Names =
> c("v.names", "timevar", "idvar", "times",
> "varying")))
>
>
> sessionInfo()
> R version 3.2.1 (2015-06-18)
> Platform: x86_64-apple-darwin10.8.0 (64-bit) Running under: OS X 10.6.8
> (Snow Leopard)
> locale:
> [1] fr_FR.UTF-8/fr_FR.UTF-8/fr_FR.UTF-8/C/fr_FR.UTF-8/fr_FR.UTF-8
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
> I do not have the latest version installed, because there is no 3.2.2
> for Snow Leopard... Could this be the reason?
>
> Ivan
>
> --
> Ivan Calandra, PhD
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
>
> Le 08/09/15 15:12, PIKAL Petr a ?crit :
> > Hi
> >
> > based on your data I got 3 lines with similar results as I got from
> melt/dcast only the rows are not in the same order.
> >
> >> version
> >                 _
> > platform       i386-w64-mingw32
> > arch           i386
> > os             mingw32
> > system         i386, mingw32
> > status         Under development (unstable)
> > major          3
> > minor          3.0
> > year           2015
> > month          06
> > day            15
> > svn rev        68521
> > language       R
> > version.string R Under development (unstable) (2015-06-15 r68521)
> > nickname       Unsuffered Consequences
> >> sessionInfo()
> > R Under development (unstable) (2015-06-15 r68521)
> > Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows XP
> > (build 2600) Service Pack 3
> >
> > locale:
> > [1] LC_COLLATE=Czech_Czech Republic.1250  LC_CTYPE=Czech_Czech
> Republic.1250    LC_MONETARY=Czech_Czech Republic.1250
> > [4] LC_NUMERIC=C                          LC_TIME=Czech_Czech
> Republic.1250
> >
> > attached base packages:
> > [1] stats     datasets  utils     grDevices graphics  methods   base
> >
> > other attached packages:
> > [1] reshape2_1.4.1  lattice_0.20-31 fun_0.1
> >
> > loaded via a namespace (and not attached):
> >   [1] Rcpp_0.11.6      digest_0.6.8     MASS_7.3-40      grid_3.3.0
> plyr_1.8.3       nlme_3.1-120     gtable_0.1.2     magrittr_1.5
> >   [9] scales_0.2.5     ggplot2_1.0.1    stringi_0.4-1    proto_0.3-10
> tools_3.3.0      stringr_1.0.0    munsell_0.4.2    colorspace_1.2-6
> > test2<-reshape(mydata, timevar="Elem",
> > idvar=c("Etape","Ech","repet","dilution","Rincage"),
> > direction="wide", drop=c("ID","Nom_ech"))
> >
> >> dput(test2)
> > structure(list(Etape = c("S1", "S1", "S1"), Ech = c("B2", "F10-3",
> > "F10-\n 3"), repet = c(NA, 1, 1), dilution = c(1, 100, 100),
> >      Rincage = c("non", "non", "non"), Moyenne.Al1670 = c(NA,
> >      106.326666666667, NA), Moyenne.As1890 = c(NA_real_, NA_real_,
> >      NA_real_), Moyenne.As1937 = c(3.7455, 2.2755, NA),
> Moyenne.Ca1840 = c(596.166666666667,
> >      291.933333333333, NA), Moyenne.Ca3158 = c(578.2,
> 278.833333333333,
> >      NA), Moyenne.Cd2288 = c(0.1514, 0.819, NA), Moyenne.Co2286 =
> c(NA_real_,
> >      NA_real_, NA_real_), Moyenne.Co2378 = c(1.87225, 3.946, NA
> >      ), Moyenne.Cr2055 = c(NA_real_, NA_real_, NA_real_),
> Moyenne.Cr2835 = c(0.3664,
> >      NA, NA), Moyenne.Cu2247 = c(1.859, 1.47766666666667, NA),
> >      Moyenne.Cu3247 = c(1.96766666666667, 1.63266666666667, NA
> >      ), Moyenne.Fe2382 = c(NA, 40.44, NA), Moyenne.Fe2599 = c(NA,
> >      40.2533333333333, NA), Moyenne.K_7664 = c(9.29566666666667,
> >      NA, 128.9), Moyenne.Mg2795 = c(13.08, 50.11, NA), Moyenne.Mg2852
> = c(12.69,
> >      49.02, NA), Moyenne.Mn2576 = c(0.26875, 37.4733333333333,
> >      NA), Moyenne.Mn2605 = c(0.2877, 37.775, NA), Moyenne.Ni2216 =
> c(0.2395,
> >      0.3764, NA)), .Names = c("Etape", "Ech", "repet", "dilution",
> > "Rincage", "Moyenne.Al1670", "Moyenne.As1890", "Moyenne.As1937",
> > "Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288",
> > "Moyenne.Co2286", "Moyenne.Co2378", "Moyenne.Cr2055",
> > "Moyenne.Cr2835", "Moyenne.Cu2247", "Moyenne.Cu3247",
> > "Moyenne.Fe2382", "Moyenne.Fe2599", "Moyenne.K_7664",
> > "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576",
> > "Moyenne.Mn2605", "Moyenne.Ni2216"), row.names = c(1L, 112L, 126L),
> class = c("tbl_df", "tbl", "data.frame"), reshapeWide =
> structure(list(v.names = NULL,
> >      timevar = "Elem", idvar = c("Etape", "Ech", "repet", "dilution",
> >      "Rincage"), times = c("Al1670", "As1890", "As1937", "Ca1840",
> >      "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
> >      "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
> >      "Mg2852", "Mn2576", "Mn2605", "Ni2216"), varying =
> structure(c("Moyenne.Al1670",
> >      "Moyenne.As1890", "Moyenne.As1937", "Moyenne.Ca1840",
> "Moyenne.Ca3158",
> >      "Moyenne.Cd2288", "Moyenne.Co2286", "Moyenne.Co2378",
> "Moyenne.Cr2055",
> >      "Moyenne.Cr2835", "Moyenne.Cu2247", "Moyenne.Cu3247",
> "Moyenne.Fe2382",
> >      "Moyenne.Fe2599", "Moyenne.K_7664", "Moyenne.Mg2795",
> "Moyenne.Mg2852",
> >      "Moyenne.Mn2576", "Moyenne.Mn2605", "Moyenne.Ni2216"), .Dim =
> c(1L,
> >      20L))), .Names = c("v.names", "timevar", "idvar", "times",
> > "varying")))
> > dput(test)
> > structure(list(Etape = c("S1", "S1", "S1", "S1"), Ech = c("B2",
> > "F10-\n 3", "F10-3", "F10-3"), repet = c(NA, 1, 1, NA), dilution =
> > c(1, 100, 100, 1), Rincage = c("non", "non", "non", "oui"), Al1670 =
> > c(NA, NA, 106.326666666667, 5732.33333333333), As1890 = c(NA, NA, NA,
> > 3.9615), As1937 = c(3.7455, NA, 2.2755, 3.48833333333333), Ca1840 =
> > c(596.166666666667, NA, 291.933333333333, 337.433333333333), Ca3158 =
> > c(578.2, NA, 278.833333333333, 323.533333333333), Cd2288 = c(0.1514,
> > NA, 0.819, 0.877166666666667), Co2286 = c(NA, NA, NA,
> 0.292466666666667),
> >      Co2378 = c(1.87225, NA, 3.946, NA), Cr2055 = c(NA, NA, NA,
> >      1.79566666666667), Cr2835 = c(0.3664, NA, NA, NA), Cu2247 =
> c(1.859,
> >      NA, 1.47766666666667, NA), Cu3247 = c(1.96766666666667, NA,
> >      1.63266666666667, NA), Fe2382 = c(NA, NA, 40.44, NA), Fe2599 =
> c(NA,
> >      NA, 40.2533333333333, NA), K_7664 = c(9.29566666666667, 128.9,
> >      NA, NA), Mg2795 = c(13.08, NA, 50.11, NA), Mg2852 = c(12.69,
> >      NA, 49.02, NA), Mn2576 = c(0.26875, NA, 37.4733333333333,
> >      NA), Mn2605 = c(0.2877, NA, 37.775, NA), Ni2216 = c(0.2395,
> >      NA, 0.3764, NA)), .Names = c("Etape", "Ech", "repet",
> "dilution",
> > "Rincage", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> "Cd2288",
> > "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", "Fe2382",
> > "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605",
> "Ni2216"),
> > row.names = c(NA, -4L), class = "data.frame") Cheers Petr
> >
> >> -----Original Message-----
> >> From: Ivan Calandra [mailto:ivan.calandra at univ-reims.fr]
> >> Sent: Tuesday, September 08, 2015 2:56 PM
> >> To: PIKAL Petr; R list
> >> Subject: Re: [R] help with reshape
> >>
> >> Thanks Petr,
> >> It looks good, but I have to check in more details.
> >>
> >> Can anyone help me with my original solution using reshape()? I'd
> >> like to understand what I did wrong.
> >> reshape(mydata, timevar="Elem",
> >> idvar=c("Etape","Ech","repet","dilution","Rincage"),
> >> direction="wide",
> >> drop=c("ID","Nom_ech"))
> >>
> >> Thank you in advance
> >> Ivan
> >>
> >> --
> >> Ivan Calandra, PhD
> >> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
> >> esplanade Roland Garros 51100 Reims, France
> >> +33(0)3 26 77 36 89
> >> ivan.calandra at univ-reims.fr
> >> https://www.researchgate.net/profile/Ivan_Calandra
> >>
> >> Le 08/09/15 14:45, PIKAL Petr a ?crit :
> >>> Hi
> >>>
> >>> I looked into docs to reshape2 and played around a bit and by some
> >>> magical feature
> >>>
> >>> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
> >>>
> >>> probably works as you expect.
> >>>
> >>> I cannot comment your solution as I use reshape only sparsely.
> >>>
> >>> Cheers
> >>> Petr
> >>>
> >>>> -----Original Message-----
> >>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> >>>> Ivan Calandra
> >>>> Sent: Tuesday, September 08, 2015 2:18 PM
> >>>> To: R list
> >>>> Subject: Re: [R] help with reshape
> >>>>
> >>>> Thank you Petr,
> >>>>
> >>>> It kinda works, but not completely. The problem is that it
> produces
> >> a
> >>>> column for each value ("Moyenne"), and not each element of "Elem".
> >>>> That means I have only one value per column, instead of up to 3.
> >>>> For example, I have 3 columns for Al1670 instead of just one, and
> >>>> each column contains maximum one value (the others being NA).
> >>>>
> >>>> Not sure I am being clear...
> >>>>
> >>>> By the way, I don't understand why my solution did not work; what
> >>>> is wrong there?
> >>>>
> >>>> Thank you again!
> >>>> Ivan
> >>>>
> >>>> --
> >>>> Ivan Calandra, PhD
> >>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
> >>>> esplanade Roland Garros 51100 Reims, France
> >>>> +33(0)3 26 77 36 89
> >>>> ivan.calandra at univ-reims.fr
> >>>> https://www.researchgate.net/profile/Ivan_Calandra
> >>>>
> >>>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
> >>>>> Hi
> >>>>>
> >>>>> I am not sure if I got it
> >>>>>
> >>>>> library(reshape2)
> >>>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
> >>>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
> >>>>>
> >>>>> gives me 3 rows but names need some tweaking afterwards.
> >>>>>
> >>>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
> >>>> "_"),
> >>>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
> >>>>>
> >>>>> Cheers
> >>>>> Petr
> >>>>>
> >>>>>
> >>>>>> -----Original Message-----
> >>>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> >>>>>> Ivan Calandra
> >>>>>> Sent: Tuesday, September 08, 2015 12:33 PM
> >>>>>> To: R list
> >>>>>> Subject: [R] help with reshape
> >>>>>>
> >>>>>> Dear users,
> >>>>>>
> >>>>>> I'm having troubles with reshaping a data.frame from long to
> wide
> >>>>>> format.
> >>>>>> I copy the output of dput() at the end of the mail because it is
> >>>>>> quite long.
> >>>>>>
> >>>>>> Each row of the column "Elem" should be transposed to a new
> >> column.
> >>>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage"
> >> define
> >>>>>> the samples. Meaning that for each unique combination of these
> >>>>>> variables, I want a single row, and as many columns as elements
> >>>>>> in
> >>>> "Elem".
> >>>>>> So I tried:
> >>>>>> reshape(mydata, timevar="Elem",
> >>>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
> >>>>>> direction="wide",
> >>>>>> drop=c("ID","Nom_ech"))
> >>>>>>
> >>>>>> The problem is that some columns are not used at all for
> defining
> >>>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is
> >>>>>> missing, but I don't understand why.
> >>>>>>
> >>>>>> Can you help me with that? I have no idea what I am doing
> wrong...
> >>>>>>
> >>>>>> Thanks in advance,
> >>>>>> Ivan
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548,
> >>>>>> 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561,
> >>>>>> 562, 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581,
> >>>>>> 11582,
> >>>> 11583,
> >>>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
> >>>> 696,
> >>>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1
> >>>>>> B2",
> >>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> >>>>>> "Step1 B2",
> >>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> >>>>>> "Step1 B2",
> >>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> >>>>>> "Step1 B2",
> >>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> >>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> >>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3
> >>>>>> (1) *100",
> >>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100",
> >>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100",
> >>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100",
> >>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100",
> >>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
> *100",
> >>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
> >>>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1",
> >>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1",
> >>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1",
> >>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech =
> >>>>>> c("B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
> >>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
> >>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>>>> "F10-3", "F10-3", "F10-3", "F10-
> >>>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
> >>>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> >>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> >> NA,
> >>>>>> 1,
> >>>> 1,
> >>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution
> =
> >>>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> >> 1,
> >>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100,
> >>>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100),
> >>>>>> Rincage = c("non", "non", "non", "non", "non", "non", "non",
> >> "non",
> >>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
> >>>>>> "non",
> >>>> "non",
> >>>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
> >>>> "oui",
> >>>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
> >>>> "non",
> >>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
> >>>> "non",
> >>>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840",
> >>>>>> "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
> >>>>>> "Cu2247",
> >>>> "Cu3247",
> >>>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
> >>>> "Mn2605",
> >>>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> >>>> "Cd2288",
> >>>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
> >>>> "As1937",
> >>>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
> >>>> "Cr2835",
> >>>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
> >>>> "Mg2852",
> >>>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
> >>>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
> >>>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69,
> >>>>>> 0.26875, 0.2877, 0.2395, 5732.33333333333, 3.9615,
> >>>>>> 3.48833333333333, 337.433333333333, 323.533333333333,
> >>>>>> 0.877166666666667, 0.292466666666667, NA, 1.79566666666667, NA,
> >>>>>> 106.326666666667, NA, 2.2755, 291.933333333333,
> 278.833333333333,
> >>>>>> 0.819, NA, 3.946, NA,
> >>>> NA,
> >>>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333,
> >> 128.9,
> >>>>>> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names =
> >>>>>> c("ID", "Nom_ech", "Etape", "Ech", "repet", "dilution",
> >>>>>> "Rincage", "Elem", "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L,
> >>>>>> 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
> 19L,
> >>>>>> 20L, 29L, 30L, 31L,
> >>>> 32L,
> >>>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L,
> 117L,
> >>>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L,
> 128L,
> >>>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
> >>>>>>
> >>>>>>
> >


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ivan.calandra at univ-reims.fr  Tue Sep  8 15:46:14 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Tue, 8 Sep 2015 15:46:14 +0200
Subject: [R] help with reshape
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ED1D@SRVEXCHMBX.precheza.cz>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECE1@SRVEXCHMBX.precheza.cz>
	<55EEE1C5.4020604@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ED1D@SRVEXCHMBX.precheza.cz>
Message-ID: <55EEE6A6.7040108@univ-reims.fr>

Dear Petr,

Thank you for your help. I might look into this issue later (with the 
latest version of R), but for now, your solution with package reshape2 
works perfectly!

Bests,
Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 08/09/15 15:42, PIKAL Petr a ?crit :
> Hi
>
>
> Well, I cannot help you any further. It can be platform or version issue. You are using Mac, I work with PC, your R version is 3.2.1 mine is 3.3.0 (devel). Wizards from R core are appropriate for answering this. You can transfer our conversation to R-sig-Mac, if people using Mac found similar issue as you.
>
> If there is R-devel for Mac you can try to install it and test your code there. If you still get the same result you can check it on different computer, maybe with some other OS.
>
> Sorry that I cannot give you any positive answer. I hope that melt/dcast code can give you desired result, though.
>
> Cheers
> Petr
>
>> -----Original Message-----
>> From: Ivan Calandra [mailto:ivan.calandra at univ-reims.fr]
>> Sent: Tuesday, September 08, 2015 3:25 PM
>> To: PIKAL Petr; R list
>> Subject: Re: [R] help with reshape
>>
>> I do get only 2 lines:
>>
>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549, 550,
>> 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 11574,
>> 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583, 683,
>> 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697,
>> 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2", "Step1
>> B2",
>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "R1 S1 F1
>> 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1
>> F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1
>> F1
>> 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3
>> (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100",
>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"), Etape = c("S1", "S1",
>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1"), Ech = c("B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-
>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"), repet = c(NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution = c(1, 1, 1, 1, 1, 1, 1, 1,
>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 100,
>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
>> 100, 100, 100, 100, 100), Rincage = c("non", "non", "non", "non",
>> "non", "non", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non", "non", "non", "non", "non", "non", "oui", "oui", "oui", "oui",
>> "oui", "oui", "oui", "oui", "oui", "oui", "non", "non", "non", "non",
>> "non", "non", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non", "non", "non", "non", "non", "non"), Elem = c("Al1670", "As1890",
>> "As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
>> "Cr2835", "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>> "Mg2852", "Mn2576", "Mn2605", "Ni2216", "Al1670", "As1890", "As1937",
>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
>> "Al1670", "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288", "Co2286",
>> "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", "Fe2382", "Fe2599",
>> "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605", "Ni2216"), Moyenne =
>> c(NA, NA, 3.7455, 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA,
>> 0.3664, 1.859, 1.96766666666667, NA, NA, 9.29566666666667, 13.08,
>> 12.69, 0.26875, 0.2877, 0.2395, 5732.33333333333, 3.9615,
>> 3.48833333333333, 337.433333333333, 323.533333333333,
>> 0.877166666666667, 0.292466666666667, NA, 1.79566666666667, NA,
>> 106.326666666667, NA, 2.2755, 291.933333333333, 278.833333333333,
>> 0.819, NA, 3.946, NA, NA, 1.47766666666667, 1.63266666666667, 40.44,
>> 40.2533333333333, 128.9, 50.11, 49.02, 37.4733333333333, 37.775,
>> 0.3764)), .Names = c("ID", "Nom_ech", "Etape", "Ech", "repet",
>> "dilution", "Rincage", "Elem", "Moyenne"), row.names = c(1L, 2L, 3L,
>> 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
>> 19L, 20L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L,
>> 114L, 115L, 116L, 117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L,
>> 126L, 127L, 128L, 129L, 130L, 131L), class = c("tbl_df", "tbl",
>> "data.frame"))
>>
>>
>> test2<-reshape(mydata, timevar="Elem",
>> idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide",
>> drop=c("ID","Nom_ech"))
>>
>>
>> dput(test2)
>> structure(list(Etape = c("S1", "S1"), Ech = c("B2", "F10-3"), repet =
>> c(NA, 1), dilution = c(1, 100), Rincage = c("non", "non"),
>> Moyenne.Al1670 = c(NA, 106.326666666667), Moyenne.As1890 = c(NA_real_,
>> NA_real_), Moyenne.As1937 = c(3.7455, 2.2755), Moyenne.Ca1840 =
>> c(596.166666666667, 291.933333333333), Moyenne.Ca3158 = c(578.2,
>> 278.833333333333), Moyenne.Cd2288 = c(0.1514, 0.819), Moyenne.Co2286 =
>> c(NA_real_, NA_real_), Moyenne.Co2378 = c(1.87225, 3.946),
>> Moyenne.Cr2055 = c(NA_real_, NA_real_), Moyenne.Cr2835 = c(0.3664, NA),
>> Moyenne.Cu2247 = c(1.859, 1.47766666666667), Moyenne.Cu3247 =
>> c(1.96766666666667, 1.63266666666667), Moyenne.Fe2382 = c(NA, 40.44),
>> Moyenne.Fe2599 = c(NA, 40.2533333333333), Moyenne.K_7664 =
>> c(9.29566666666667, 128.9), Moyenne.Mg2795 = c(13.08, 50.11),
>> Moyenne.Mg2852 = c(12.69, 49.02), Moyenne.Mn2576 = c(0.26875,
>> 37.4733333333333), Moyenne.Mn2605 = c(0.2877, 37.775), Moyenne.Ni2216 =
>> c(0.2395, 0.3764)), .Names = c("Etape", "Ech", "repet", "dilution",
>> "Rincage", "Moyenne.Al1670", "Moyenne.As1890", "Moyenne.As1937",
>> "Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288", "Moyenne.Co2286",
>> "Moyenne.Co2378", "Moyenne.Cr2055", "Moyenne.Cr2835", "Moyenne.Cu2247",
>> "Moyenne.Cu3247", "Moyenne.Fe2382", "Moyenne.Fe2599", "Moyenne.K_7664",
>> "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576", "Moyenne.Mn2605",
>> "Moyenne.Ni2216"), row.names = c(1L, 112L), class = c("tbl_df", "tbl",
>> "data.frame"), reshapeWide = structure(list(v.names = NULL, timevar =
>> "Elem", idvar = c("Etape", "Ech", "repet", "dilution", "Rincage"),
>> times = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158", "Cd2288",
>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", "Fe2382",
>> "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605", "Ni2216"),
>> varying = structure(c("Moyenne.Al1670", "Moyenne.As1890",
>> "Moyenne.As1937", "Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288",
>> "Moyenne.Co2286", "Moyenne.Co2378", "Moyenne.Cr2055", "Moyenne.Cr2835",
>> "Moyenne.Cu2247", "Moyenne.Cu3247", "Moyenne.Fe2382", "Moyenne.Fe2599",
>> "Moyenne.K_7664", "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576",
>> "Moyenne.Mn2605", "Moyenne.Ni2216"), .Dim = c(1L, 20L))), .Names =
>> c("v.names", "timevar", "idvar", "times",
>> "varying")))
>>
>>
>> sessionInfo()
>> R version 3.2.1 (2015-06-18)
>> Platform: x86_64-apple-darwin10.8.0 (64-bit) Running under: OS X 10.6.8
>> (Snow Leopard)
>> locale:
>> [1] fr_FR.UTF-8/fr_FR.UTF-8/fr_FR.UTF-8/C/fr_FR.UTF-8/fr_FR.UTF-8
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>
>> I do not have the latest version installed, because there is no 3.2.2
>> for Snow Leopard... Could this be the reason?
>>
>> Ivan
>>
>> --
>> Ivan Calandra, PhD
>> University of Reims Champagne-Ardenne
>> GEGENAA - EA 3795
>> CREA - 2 esplanade Roland Garros
>> 51100 Reims, France
>> +33(0)3 26 77 36 89
>> ivan.calandra at univ-reims.fr
>> https://www.researchgate.net/profile/Ivan_Calandra
>>
>> Le 08/09/15 15:12, PIKAL Petr a ?crit :
>>> Hi
>>>
>>> based on your data I got 3 lines with similar results as I got from
>> melt/dcast only the rows are not in the same order.
>>>> version
>>>                  _
>>> platform       i386-w64-mingw32
>>> arch           i386
>>> os             mingw32
>>> system         i386, mingw32
>>> status         Under development (unstable)
>>> major          3
>>> minor          3.0
>>> year           2015
>>> month          06
>>> day            15
>>> svn rev        68521
>>> language       R
>>> version.string R Under development (unstable) (2015-06-15 r68521)
>>> nickname       Unsuffered Consequences
>>>> sessionInfo()
>>> R Under development (unstable) (2015-06-15 r68521)
>>> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows XP
>>> (build 2600) Service Pack 3
>>>
>>> locale:
>>> [1] LC_COLLATE=Czech_Czech Republic.1250  LC_CTYPE=Czech_Czech
>> Republic.1250    LC_MONETARY=Czech_Czech Republic.1250
>>> [4] LC_NUMERIC=C                          LC_TIME=Czech_Czech
>> Republic.1250
>>> attached base packages:
>>> [1] stats     datasets  utils     grDevices graphics  methods   base
>>>
>>> other attached packages:
>>> [1] reshape2_1.4.1  lattice_0.20-31 fun_0.1
>>>
>>> loaded via a namespace (and not attached):
>>>    [1] Rcpp_0.11.6      digest_0.6.8     MASS_7.3-40      grid_3.3.0
>> plyr_1.8.3       nlme_3.1-120     gtable_0.1.2     magrittr_1.5
>>>    [9] scales_0.2.5     ggplot2_1.0.1    stringi_0.4-1    proto_0.3-10
>> tools_3.3.0      stringr_1.0.0    munsell_0.4.2    colorspace_1.2-6
>>> test2<-reshape(mydata, timevar="Elem",
>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>> direction="wide", drop=c("ID","Nom_ech"))
>>>
>>>> dput(test2)
>>> structure(list(Etape = c("S1", "S1", "S1"), Ech = c("B2", "F10-3",
>>> "F10-\n 3"), repet = c(NA, 1, 1), dilution = c(1, 100, 100),
>>>       Rincage = c("non", "non", "non"), Moyenne.Al1670 = c(NA,
>>>       106.326666666667, NA), Moyenne.As1890 = c(NA_real_, NA_real_,
>>>       NA_real_), Moyenne.As1937 = c(3.7455, 2.2755, NA),
>> Moyenne.Ca1840 = c(596.166666666667,
>>>       291.933333333333, NA), Moyenne.Ca3158 = c(578.2,
>> 278.833333333333,
>>>       NA), Moyenne.Cd2288 = c(0.1514, 0.819, NA), Moyenne.Co2286 =
>> c(NA_real_,
>>>       NA_real_, NA_real_), Moyenne.Co2378 = c(1.87225, 3.946, NA
>>>       ), Moyenne.Cr2055 = c(NA_real_, NA_real_, NA_real_),
>> Moyenne.Cr2835 = c(0.3664,
>>>       NA, NA), Moyenne.Cu2247 = c(1.859, 1.47766666666667, NA),
>>>       Moyenne.Cu3247 = c(1.96766666666667, 1.63266666666667, NA
>>>       ), Moyenne.Fe2382 = c(NA, 40.44, NA), Moyenne.Fe2599 = c(NA,
>>>       40.2533333333333, NA), Moyenne.K_7664 = c(9.29566666666667,
>>>       NA, 128.9), Moyenne.Mg2795 = c(13.08, 50.11, NA), Moyenne.Mg2852
>> = c(12.69,
>>>       49.02, NA), Moyenne.Mn2576 = c(0.26875, 37.4733333333333,
>>>       NA), Moyenne.Mn2605 = c(0.2877, 37.775, NA), Moyenne.Ni2216 =
>> c(0.2395,
>>>       0.3764, NA)), .Names = c("Etape", "Ech", "repet", "dilution",
>>> "Rincage", "Moyenne.Al1670", "Moyenne.As1890", "Moyenne.As1937",
>>> "Moyenne.Ca1840", "Moyenne.Ca3158", "Moyenne.Cd2288",
>>> "Moyenne.Co2286", "Moyenne.Co2378", "Moyenne.Cr2055",
>>> "Moyenne.Cr2835", "Moyenne.Cu2247", "Moyenne.Cu3247",
>>> "Moyenne.Fe2382", "Moyenne.Fe2599", "Moyenne.K_7664",
>>> "Moyenne.Mg2795", "Moyenne.Mg2852", "Moyenne.Mn2576",
>>> "Moyenne.Mn2605", "Moyenne.Ni2216"), row.names = c(1L, 112L, 126L),
>> class = c("tbl_df", "tbl", "data.frame"), reshapeWide =
>> structure(list(v.names = NULL,
>>>       timevar = "Elem", idvar = c("Etape", "Ech", "repet", "dilution",
>>>       "Rincage"), times = c("Al1670", "As1890", "As1937", "Ca1840",
>>>       "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
>>>       "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>>>       "Mg2852", "Mn2576", "Mn2605", "Ni2216"), varying =
>> structure(c("Moyenne.Al1670",
>>>       "Moyenne.As1890", "Moyenne.As1937", "Moyenne.Ca1840",
>> "Moyenne.Ca3158",
>>>       "Moyenne.Cd2288", "Moyenne.Co2286", "Moyenne.Co2378",
>> "Moyenne.Cr2055",
>>>       "Moyenne.Cr2835", "Moyenne.Cu2247", "Moyenne.Cu3247",
>> "Moyenne.Fe2382",
>>>       "Moyenne.Fe2599", "Moyenne.K_7664", "Moyenne.Mg2795",
>> "Moyenne.Mg2852",
>>>       "Moyenne.Mn2576", "Moyenne.Mn2605", "Moyenne.Ni2216"), .Dim =
>> c(1L,
>>>       20L))), .Names = c("v.names", "timevar", "idvar", "times",
>>> "varying")))
>>> dput(test)
>>> structure(list(Etape = c("S1", "S1", "S1", "S1"), Ech = c("B2",
>>> "F10-\n 3", "F10-3", "F10-3"), repet = c(NA, 1, 1, NA), dilution =
>>> c(1, 100, 100, 1), Rincage = c("non", "non", "non", "oui"), Al1670 =
>>> c(NA, NA, 106.326666666667, 5732.33333333333), As1890 = c(NA, NA, NA,
>>> 3.9615), As1937 = c(3.7455, NA, 2.2755, 3.48833333333333), Ca1840 =
>>> c(596.166666666667, NA, 291.933333333333, 337.433333333333), Ca3158 =
>>> c(578.2, NA, 278.833333333333, 323.533333333333), Cd2288 = c(0.1514,
>>> NA, 0.819, 0.877166666666667), Co2286 = c(NA, NA, NA,
>> 0.292466666666667),
>>>       Co2378 = c(1.87225, NA, 3.946, NA), Cr2055 = c(NA, NA, NA,
>>>       1.79566666666667), Cr2835 = c(0.3664, NA, NA, NA), Cu2247 =
>> c(1.859,
>>>       NA, 1.47766666666667, NA), Cu3247 = c(1.96766666666667, NA,
>>>       1.63266666666667, NA), Fe2382 = c(NA, NA, 40.44, NA), Fe2599 =
>> c(NA,
>>>       NA, 40.2533333333333, NA), K_7664 = c(9.29566666666667, 128.9,
>>>       NA, NA), Mg2795 = c(13.08, NA, 50.11, NA), Mg2852 = c(12.69,
>>>       NA, 49.02, NA), Mn2576 = c(0.26875, NA, 37.4733333333333,
>>>       NA), Mn2605 = c(0.2877, NA, 37.775, NA), Ni2216 = c(0.2395,
>>>       NA, 0.3764, NA)), .Names = c("Etape", "Ech", "repet",
>> "dilution",
>>> "Rincage", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>> "Cd2288",
>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247", "Cu3247", "Fe2382",
>>> "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576", "Mn2605",
>> "Ni2216"),
>>> row.names = c(NA, -4L), class = "data.frame") Cheers Petr
>>>
>>>> -----Original Message-----
>>>> From: Ivan Calandra [mailto:ivan.calandra at univ-reims.fr]
>>>> Sent: Tuesday, September 08, 2015 2:56 PM
>>>> To: PIKAL Petr; R list
>>>> Subject: Re: [R] help with reshape
>>>>
>>>> Thanks Petr,
>>>> It looks good, but I have to check in more details.
>>>>
>>>> Can anyone help me with my original solution using reshape()? I'd
>>>> like to understand what I did wrong.
>>>> reshape(mydata, timevar="Elem",
>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>>> direction="wide",
>>>> drop=c("ID","Nom_ech"))
>>>>
>>>> Thank you in advance
>>>> Ivan
>>>>
>>>> --
>>>> Ivan Calandra, PhD
>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>> esplanade Roland Garros 51100 Reims, France
>>>> +33(0)3 26 77 36 89
>>>> ivan.calandra at univ-reims.fr
>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>
>>>> Le 08/09/15 14:45, PIKAL Petr a ?crit :
>>>>> Hi
>>>>>
>>>>> I looked into docs to reshape2 and played around a bit and by some
>>>>> magical feature
>>>>>
>>>>> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
>>>>>
>>>>> probably works as you expect.
>>>>>
>>>>> I cannot comment your solution as I use reshape only sparsely.
>>>>>
>>>>> Cheers
>>>>> Petr
>>>>>
>>>>>> -----Original Message-----
>>>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>>>>> Ivan Calandra
>>>>>> Sent: Tuesday, September 08, 2015 2:18 PM
>>>>>> To: R list
>>>>>> Subject: Re: [R] help with reshape
>>>>>>
>>>>>> Thank you Petr,
>>>>>>
>>>>>> It kinda works, but not completely. The problem is that it
>> produces
>>>> a
>>>>>> column for each value ("Moyenne"), and not each element of "Elem".
>>>>>> That means I have only one value per column, instead of up to 3.
>>>>>> For example, I have 3 columns for Al1670 instead of just one, and
>>>>>> each column contains maximum one value (the others being NA).
>>>>>>
>>>>>> Not sure I am being clear...
>>>>>>
>>>>>> By the way, I don't understand why my solution did not work; what
>>>>>> is wrong there?
>>>>>>
>>>>>> Thank you again!
>>>>>> Ivan
>>>>>>
>>>>>> --
>>>>>> Ivan Calandra, PhD
>>>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>>>> esplanade Roland Garros 51100 Reims, France
>>>>>> +33(0)3 26 77 36 89
>>>>>> ivan.calandra at univ-reims.fr
>>>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>>>
>>>>>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
>>>>>>> Hi
>>>>>>>
>>>>>>> I am not sure if I got it
>>>>>>>
>>>>>>> library(reshape2)
>>>>>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
>>>>>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
>>>>>>>
>>>>>>> gives me 3 rows but names need some tweaking afterwards.
>>>>>>>
>>>>>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
>>>>>> "_"),
>>>>>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
>>>>>>>
>>>>>>> Cheers
>>>>>>> Petr
>>>>>>>
>>>>>>>
>>>>>>>> -----Original Message-----
>>>>>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>>>>>>> Ivan Calandra
>>>>>>>> Sent: Tuesday, September 08, 2015 12:33 PM
>>>>>>>> To: R list
>>>>>>>> Subject: [R] help with reshape
>>>>>>>>
>>>>>>>> Dear users,
>>>>>>>>
>>>>>>>> I'm having troubles with reshaping a data.frame from long to
>> wide
>>>>>>>> format.
>>>>>>>> I copy the output of dput() at the end of the mail because it is
>>>>>>>> quite long.
>>>>>>>>
>>>>>>>> Each row of the column "Elem" should be transposed to a new
>>>> column.
>>>>>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage"
>>>> define
>>>>>>>> the samples. Meaning that for each unique combination of these
>>>>>>>> variables, I want a single row, and as many columns as elements
>>>>>>>> in
>>>>>> "Elem".
>>>>>>>> So I tried:
>>>>>>>> reshape(mydata, timevar="Elem",
>>>>>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>>>>>>> direction="wide",
>>>>>>>> drop=c("ID","Nom_ech"))
>>>>>>>>
>>>>>>>> The problem is that some columns are not used at all for
>> defining
>>>>>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is
>>>>>>>> missing, but I don't understand why.
>>>>>>>>
>>>>>>>> Can you help me with that? I have no idea what I am doing
>> wrong...
>>>>>>>> Thanks in advance,
>>>>>>>> Ivan
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548,
>>>>>>>> 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561,
>>>>>>>> 562, 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581,
>>>>>>>> 11582,
>>>>>> 11583,
>>>>>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
>>>>>> 696,
>>>>>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1
>>>>>>>> B2",
>>>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>>>>>>>> "Step1 B2",
>>>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>>>>>>>> "Step1 B2",
>>>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>>>>>>>> "Step1 B2",
>>>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3
>>>>>>>> (1) *100",
>>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100",
>>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100",
>>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100",
>>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100",
>>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>> *100",
>>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
>>>>>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1",
>>>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1",
>>>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1",
>>>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech =
>>>>>>>> c("B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>>>>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>>>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>>>> "F10-3", "F10-3", "F10-3", "F10-
>>>>>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
>>>>>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>> NA,
>>>>>>>> 1,
>>>>>> 1,
>>>>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution
>> =
>>>>>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>>>> 1,
>>>>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100,
>>>>>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100),
>>>>>>>> Rincage = c("non", "non", "non", "non", "non", "non", "non",
>>>> "non",
>>>>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>>>>> "non",
>>>>>> "non",
>>>>>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
>>>>>> "oui",
>>>>>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>>> "non",
>>>>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>>> "non",
>>>>>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840",
>>>>>>>> "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835",
>>>>>>>> "Cu2247",
>>>>>> "Cu3247",
>>>>>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
>>>>>> "Mn2605",
>>>>>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>>>>> "Cd2288",
>>>>>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
>>>>>> "As1937",
>>>>>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
>>>>>> "Cr2835",
>>>>>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>>>>>> "Mg2852",
>>>>>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
>>>>>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
>>>>>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69,
>>>>>>>> 0.26875, 0.2877, 0.2395, 5732.33333333333, 3.9615,
>>>>>>>> 3.48833333333333, 337.433333333333, 323.533333333333,
>>>>>>>> 0.877166666666667, 0.292466666666667, NA, 1.79566666666667, NA,
>>>>>>>> 106.326666666667, NA, 2.2755, 291.933333333333,
>> 278.833333333333,
>>>>>>>> 0.819, NA, 3.946, NA,
>>>>>> NA,
>>>>>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333,
>>>> 128.9,
>>>>>>>> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names =
>>>>>>>> c("ID", "Nom_ech", "Etape", "Ech", "repet", "dilution",
>>>>>>>> "Rincage", "Elem", "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L,
>>>>>>>> 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
>> 19L,
>>>>>>>> 20L, 29L, 30L, 31L,
>>>>>> 32L,
>>>>>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L,
>> 117L,
>>>>>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L,
>> 128L,
>>>>>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
>>>>>>>>
>>>>>>>>
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From priyanka.garg112 at gmail.com  Tue Sep  8 08:10:35 2015
From: priyanka.garg112 at gmail.com (Priyanka Garg)
Date: Tue, 8 Sep 2015 11:40:35 +0530
Subject: [R] Laplace smoothing in J48
Message-ID: <CAA5eUmEVg4NGkSGZQcu2bU6BPiJ7+24OC4_VYTsVy=3Pv2y7dQ@mail.gmail.com>

Hi,

I am using J48 classifier. I want to now after i set A as TRUE in control
option of the classifier, how could i see its effect when using predict
method ?



Regards
Priyanka Garg
School of Computers & Information Sciences
University Of Hyderabad

	[[alternative HTML version deleted]]


From putra_autumn86 at yahoo.com  Tue Sep  8 08:58:58 2015
From: putra_autumn86 at yahoo.com (smart hendsome)
Date: Tue, 8 Sep 2015 06:58:58 +0000 (UTC)
Subject: [R] Counting number of rain
Message-ID: <610724885.3110482.1441695538956.JavaMail.yahoo@mail.yahoo.com>

Hello R-users,
I want to ask how to count the number of daily rain data.? My data as below:
 Year Month Day Amount 1901 1 1 0 1901 1 2 3 1901 1 3 0 1901 1 4 0.5 1901 1 5 0 1901 1 6 0  1901 1 7 0.3 1901 1 8 0 1901 1 9 0 1901 1 10 0 1901 1 11 0.5 1901 1 12 1.8 1901 1 13 0 1901 1 14 0 1901 1 15 2.5 1901 1 16 0 1901 1 17 0 1901 1 18 0 1901 1 19 0 1901 1 20 0 1901 1 21 0 1901 1 22 0 1901 1 23 0 1901 1 24 0 1901 1 25 0 1901 1 26 16.5 1901 1 27 0.3 1901 1 28 0 1901 1 29 0 1901 1 30 0 1901 1 31 0 1901 2 1 0 1901 2 2 0 1901 2 3 0 1901 2 4 0 1901 2 5 0 1901 2 6 0 1901 2 7 0 1901 2 8 0.3 1901 2 9 0 1901 2 10 0 1901 2 11 0 1901 2 12 1 1901 2 13 0.3 1901 2 14 0 1901 2 15 0 1901 2 16 0 1901 2 17 0 1901 2 18 0 1901 2 19 0 1901 2 20 0 1901 2 21 0 1901 2 22 0 1901 2 23 0.3 1901 2 24 0 1901 2 25 0 1901 2 26 0.3 1901 2 27 0 1901 2 28 0 1901 3 1 0 1901 3 2 0.8 1901 3 3 2.3 1901 3 4 0 1901 3 5 0 1901 3 6 0 1901 3 7 0 1901 3 8 0 1901 3 9 0 1901 3 10 2 1901 3 11 0 1901 3 12 0 1901 3 13 0 1901 3 14 0 1901 3 15 0 1901 3 16 0 1901 3 17 0 1901 3 18 0 1901 3 19 0 1901 3 20 0 1901 3 21 0 1901 3 22 1.5 1901 3 23 1.3 1901 3 24 0 1901 3 25 0 1901 3 26 0 1901 3 27 0 1901 3 28 0.3 1901 3 29 0.3  1901 3 30 4.6 1901 3 31 0 1901 4 1 0 1901 4 2 4.6 1901 4 3 30.7 1901 4 4 0 1901 4 5 0 1901 4 6 0 1901 4 7 0 1901 4 8 0 1901 4 9 0 1901 4 10 0 1901 4 11 0 1901 4 12 0 1901 4 13 0 1901 4 14 0 1901 4 15 0.3 1901 4 16 1.3 1901 4 17 0 1901 4 18 0 1901 4 19 0.3 1901 4 20 1 1901 4 21 9.4 1901 4 22 0.5 1901 4 23 0.3 1901 4 24 0 1901 4 25 0 1901 4 26 0 1901 4 27 0 1901 4 28 0 1901 4 29 0 1901 4 30 0 1901 5 1 0 1901 5 2 0 1901 5 3 0 1901 5 4 0 1901 5 5 0 1901  5 6 0 1901 5 7 0 1901 5 8 0.5 1901 5 9 2.3 1901 5 10 0.3 1901 5 11 0 1901 5 12 0 1901 5 13 0 1901 5 14 0 1901 5 15 0 1901 5 16 0 1901 5 17 0 1901 5 18 0 1901 5 19 0 1901 5 20 0 1901 5 21 0.5 1901 5 22 0 1901 5 23 0 1901 5 24 0 1901 5 25 0 1901 5 26 4.8 1901 5 27 10.9 1901 5 28 3.6 1901 5 29 0 1901 5 30 0 1901 5 31 5.1 1901 6 1 0.5 1901 6 2 0 1901 6 3 2 1901 6 4 0  1901 6 5 10.2 1901 6 6 33.3 1901 6 7 0.3 1901 6 8 0 1901 6 9 0 1901 6 10 0.5 1901 6 11 0.5 1901 6 12 0.3 1901 6 13 2.8 1901 6 14 5.6 1901 6 15 0.3 1901 6 16 6.6 1901 6 17 14.2 1901 6 18 4.8  1901 6 19 8.4 1901 6 20 1.8 1901 6 21 1.8 1901 6 22 0.3 1901 6 23 8.6 1901 6 24 0 1901 6 25 0  1901 6 26 0 1901 6 27 0 1901 6 28 0 1901 6 29 0 1901 6 30 0 1901 7 1 0 1901 7 2 0 1901 7 3 0 1901 7 4 0 1901 7 5 1 1901 7 6 0.5 1901 7 7 0.3 1901 7 8 0.3 1901 7 9 6.1 1901 7 10 0.3  1901 7 11 1.5 1901 7 12 0 1901 7 13 1.5 1901 7 14 0.3 1901 7 15 3.3 1901 7 16 2.3 1901 7 17 0.5  1901 7 18 0 1901 7 19 0 1901 7 20 0 1901 7 21 1.8 1901 7 22 0 1901 7 23 1 1901 7 24 0.3 1901  7 25 0.3 1901 7 26 1.3 1901 7 27 17 1901 7 28 6.6 1901 7 29 6.1 1901 7 30 0.5 1901 7 31 0.3 1901 8 1 0 1901 8 2 0 1901 8 3 0 1901 8 4 0 1901 8 5 0 1901 8 6 3.3 1901 8 7 4.1 1901 8 8 0.3  1901 8 9 0 1901 8 10 0 1901 8 11 0 1901 8 12 0 1901 8 13 0 1901 8 14 0 1901 8 15 0 1901 8 16 0 1901 8 17 0.5 1901 8 18 0 1901 8 19 0 1901 8 20 0 1901 8 21 0 1901 8 22 0 1901 8 23 0.3 1901 8 24 1 1901 8 25 0 1901 8 26 0 1901 8 27 10.2 1901 8 28 1.5 1901 8 29 0.5 1901 8 30 1.3  1901 8 31 0 1901 9 1 0 1901 9 2 3 1901 9 3 1 1901 9 4 0.5 1901 9 5 0.3 1901 9 6 0 1901 9 7 0 1901 9 8 2.3 1901 9 9 0.3 1901 9 10 0 1901 9 11 0 1901 9 12 0 1901 9 13 0 1901 9 14 0  1901 9 15 0 1901 9 16 0 1901 9 17 0 1901 9 18 1.8 1901 9 19 8.1 1901 9 20 0.3 1901 9 21 5.8 1901 9 22 4.1 1901 9 23 0.3 1901 9 24 1.8 1901 9 25 0 1901 9 26 0 1901 9 27 0 1901 9 28 0 1901  9 29 1.8 1901 9 30 0.8 1901 10 1 0 1901 10 2 0 1901 10 3 0 1901 10 4 0 1901 10 5 0.3 1901 10 6 0 1901 10 7 0 1901 10 8 0 1901 10 9 0 1901 10 10 0 1901 10 11 0.3 1901 10 12 3.8 1901 10 13 0.4 1901 10 14 9 1901 10 15 2 1901 10 16 1 1901 10 17 0 1901 10 18 0 1901 10 19 0 1901 10 20 0.3 1901 10 21 0 1901 10 22 0 1901 10 23 0 1901 10 24 0 1901 10 25 0 1901 10 26 0 1901 10 27 14.5  1901 10 28 6.4 1901 10 29 0.8 1901 10 30 0 1901 10 31 0 1901 11 1 0 1901 11 2 0 1901 11 3 0  1901 11 4 0 1901 11 5 0 1901 11 6 0 1901 11 7 0 1901 11 8 0 1901 11 9 0 1901 11 10 0 1901 11 11 0 1901 11 12 5.1 1901 11 13 0.3 1901 11 14 5.8 1901 11 15 0 1901 11 16 0 1901 11 17 1 1901 11 18 0.5 1901 11 19 0 1901 11 20 0 1901 11 21 0 1901 11 22 0 1901 11 23 0 1901 11 24 0 1901 11 25 0.3 1901 11 26 0 1901 11 27 0 1901 11 28 0 1901 11 29 0 1901 11 30 3.3 1901 12 1 0 1901 12 2 0 1901 12 3 0 1901 12 4 0 1901 12 5 0 1901 12 6 0 1901 12 7 0 1901 12 8 0 1901 12 9 0 1901 12 10 0 1901 12 11 0 1901 12 12 0 1901 12 13 0 1901 12 14 0 1901 12 15 0 1901 12 16 0 1901 12 17 0 1901 12 18 0 1901 12 19 0 1901 12 20 0 1901 12 21 6.1 1901 12 22 5.6 1901 12 23 0 1901 12 24 0 1901 12 25 0 1901 12 26 0 1901 12 27 0 1901 12 28 0 1901 12 29 0 1901 12 30 0 1901 12 31 9.9 1902 1 1 0 1902 1 2 0 1902 1 3 0 1902 1 4 4.1 1902 1 5 0 1902 1 6 0 1902 1 7 0 1902 1 8 0 1902 1 9 2.5 1902 1 10 0 1902 1 11 0 1902 1 12 0 1902 1 13 0.3 1902 1 14 0 1902 1 15 0 1902 1 16 0 1902 1 17 0 1902 1 18 0 1902 1 19 0 1902 1 20 0.3 1902 1 21 0.8 1902 1 22 1.8  1902 1 23 0 1902 1 24 0 1902 1 25 0 1902 1 26 2.8 1902 1 27 0 1902 1 28 0.3 1902 1 29 0 1902 1 30 0 1902 1 31 0 1902 2 1 2.8 1902 2 2 0 1902 2 3 0.3 1902 2 4 0 1902 2 5 0 1902 2 6 0 1902 2 7 0 1902 2 8 0 1902 2 9 0 1902 2 10 0 1902 2 11 0 1902 2 12 5.6 1902 2 13 0 1902  2 14 0 1902 2 15 0 1902 2 16 0 1902 2 17 0 1902 2 18 0 1902 2 19 0 1902 2 20 0.8 1902 2 21 0 1902 2 22 0 1902 2 23 0 1902 2 24 2.8 1902 2 25 2.8 1902 2 26 0 1902 2 27 0 1902 2 28 0 1902 3 1 0 1902 3 2 0 1902 3 3 0 1902 3 4 0 1902 3 5 0 1902 3 6 0 1902 3 7 0 1902 3 8 0  1902 3 9 0 1902 3 10 0 1902 3 11 0 1902 3 12 0 1902 3 13 0 1902 3 14 0 1902 3 15 0 1902 3 16 0 1902 3 17 0 1902 3 18 0 1902 3 19 0 1902 3 20 0 1902 3 21 0 1902 3 22 0 1902 3 23 0  1902 3 24 0 1902 3 25 4.8 1902 3 26 11.9 1902 3 27 3.8 1902 3 28 1.8 1902 3 29 0 1902 3 30 0 1902 3 31 0 1902 4 1 0 1902 4 2 0 1902 4 3 0 1902 4 4 0 1902 4 5 0 1902 4 6 0 1902 4 7 0  1902 4 8 0 1902 4 9 2.5 1902 4 10 0 1902 4 11 0 1902 4 12 0 1902 4 13 0 1902 4 14 0 1902 4 15 0 1902 4 16 0 1902 4 17 0 1902 4 18 0 1902 4 19 0 1902 4 20 0 1902 4 21 0 1902 4 22 0  1902 4 23 0 1902 4 24 0 1902 4 25 0 1902 4 26 1.3 1902 4 27 8.1 1902 4 28 0 1902 4 29 0 1902 4 30 0 1902 5 1 0 1902 5 2 0 1902 5 3 0 1902 5 4 0 1902 5 5 0 1902 5 6 3 1902 5 7 3.8  1902 5 8 4.3 1902 5 9 2.8 1902 5 10 3 1902 5 11 0 1902 5 12 0.5 1902 5 13 2 1902 5 14 0 1902 5 15 0.3 1902 5 16 0.8 1902 5 17 0.3 1902 5 18 0 1902 5 19 0 1902 5 20 0 1902 5 21 0 1902 5  22 0 1902 5 23 0 1902 5 24 0 1902 5 25 0 1902 5 26 0 1902 5 27 0 1902 5 28 0 1902 5 29 0.3  1902 5 30 1.8 1902 5 31 0 1902 6 1 5.3 1902 6 2 0 1902 6 3 0 1902 6 4 0.3 1902 6 5 0 1902 6 6 0 1902 6 7 4.6 1902 6 8 5.1 1902 6 9 12.4 1902 6 10 5.3 1902 6 11 0.3 1902 6 12 0 1902 6 13 0 1902 6 14 3 1902 6 15 4.1 1902 6 16 7.4 1902 6 17 5.1 1902 6 18 8.1 1902 6 19 18.3 1902 6 20 3.3 1902 6 21 16.5 1902 6 22 0.5 1902 6 23 0 1902 6 24 0 1902 6 25 0 1902 6 26 0 1902 6 27 0 1902 6 28 0 1902 6 29 0 1902 6 30 0 1902 7 1 0 1902 7 2 0 1902 7 3 0 1902 7 4 0 1902 7  5 0 1902 7 6 1.8 1902 7 7 0 1902 7 8 0 1902 7 9 0 1902 7 10 1 1902 7 11 0.3 1902 7 12 0  1902 7 13 0.8 1902 7 14 0 1902 7 15 0 1902 7 16 0 1902 7 17 0.8 1902 7 18 0 1902 7 19 1.3 1902 7 20 0.5 1902 7 21 0 1902 7 22 1.5 1902 7 23 1.3 1902 7 24 2.5 1902 7 25 9.4 1902 7 26 0.3 1902 7 27 0 1902 7 28 0 1902 7 29 0 1902 7 30 5.1 1902 7 31 2 1902 8 1 0 1902 8 2 3 1902 8 3 1.8 1902 8 4 0 1902 8 5 0.5 1902 8 6 3.3 1902 8 7 3.8 1902 8 8 0 1902 8 9 0 1902 8 10 0 1902 8 11 0.3 1902 8 12 0 1902 8 13 0 1902 8 14 0 1902 8 15 0 1902 8 16 0 1902 8 17 0 1902 8 18 0 1902 8 19 0 1902 8 20 0 1902 8 21 0 1902 8 22 0 1902 8 23 1.5 1902 8 24 2.8 1902 8 25 0  1902 8 26 0 1902 8 27 5.8 1902 8 28 0 1902 8 29 1.5 1902 8 30 0 1902 8 31 0 1902 9 1 0 1902 9 2 2.5 1902 9 3 0.8 1902 9 4 0.8 1902 9 5 0 1902 9 6 0 1902 9 7 0 1902 9 8 0 1902 9 9 0  1902 9 10 0 1902 9 11 9.4 1902 9 12 0.5 1902 9 13 6.6 1902 9 14 0.5 1902 9 15 2.8 1902 9 16 4.1  1902 9 17 6.9 1902 9 18 0.5 1902 9 19 0 1902 9 20 0 1902 9 21 0 1902 9 22 0 1902 9 23 0 1902 9 24 0 1902 9 25 0 1902 9 26 1.8 1902 9 27 0.3 1902 9 28 0.3 1902 9 29 0 1902 9 30 0 1902 10  1 0.8 1902 10 2 1.5 1902 10 3 2 1902 10 4 0 1902 10 5 0 1902 10 6 0.3 1902 10 7 0.5 1902 10 8  1 1902 10 9 0 1902 10 10 1 1902 10 11 10.9 1902 10 12 0.3 1902 10 13 0 1902 10 14 0 1902 10 15 0 1902 10 16 0 1902 10 17 0 1902 10 18 2.3 1902 10 19 0.3 1902 10 20 0 1902 10 21 0 1902 10 22 1.8 1902 10 23 0.5 1902 10 24 1.5 1902 10 25 0 1902 10 26 0 1902 10 27 1 1902 10 28 5.3 1902 10 29 0 1902 10 30 0 1902 10 31 0 1902 11 1 0 1902 11 2 0 1902 11 3 0 1902 11 4 0 1902 11 5 0 1902 11 6 0.8 1902 11 7 0.3 1902 11 8 0.5 1902 11 9 1 1902 11 10 0 1902 11 11 0 1902 11 12 0 1902 11 13 7.1 1902 11 14 0.3 1902 11 15 0 1902 11 16 0 1902 11 17 0 1902 11 18 0 1902 11 19 0.3 1902 11 20 1 1902 11 21 0 1902 11 22 0 1902 11 23 0 1902 11 24 0 1902 11 25 0 1902 11 26 0 1902 11 27 0 1902 11 28 0 1902 11 29 0 1902 11 30 0 1902 12 1 0 1902 12 2 0 1902 12 3 0 1902 12 4  0 1902 12 5 1.8 1902 12 6 6.9 1902 12 7 0 1902 12 8 0 1902 12 9 0 1902 12 10 0.5 1902 12 11 4.8 1902 12 12 0 1902 12 13 0 1902 12 14 0 1902 12 15 0 1902 12 16 0.8 1902 12 17 15.5 1902 12 18 11.7 1902 12 19 9.4 1902 12 20 0.3 1902 12 21 0 1902 12 22 0 1902 12 23 0 1902 12 24 0.3 1902 12 25 0.3 1902 12 26 0 1902 12 27 0 1902 12 28 0 1902 12 29 0 1902 12 30 0 1902 12 31 0 1903 1 1 0 1903 1 2 0 1903 1 3 1.3 1903 1 4 2 1903 1 5 0 1903 1 6 3.6 1903 1 7 0 1903 1 8 0 1903 1 9 0 1903 1 10 0 1903 1 11 0 1903 1 12 0 1903 1 13 0 1903 1 14 0 1903 1 15 0 1903 1 16 0  1903 1 17 0 1903 1 18 0 1903 1 19 0 1903 1 20 0 1903 1 21 10.2 1903 1 22 3 1903 1 23 0 1903 1 24 0 1903 1 25 0 1903 1 26 0 1903 1 27 0 1903 1 28 0 1903 1 29 0 1903 1 30 0 1903 1 31 0  1903 2 1 0 1903 2 2 0.3 1903 2 3 0 1903 2 4 0 1903 2 5 0 1903 2 6 0 1903 2 7 0 1903 2 8 0 1903 2 9 0 1903 2 10 0 1903 2 11 0 1903 2 12 0 1903 2 13 0 1903 2 14 10.2 1903 2 15 13.5  1903 2 16 7.9 1903 2 17 0.5 1903 2 18 0 1903 2 19 0 1903 2 20 0 1903 2 21 0 1903 2 22 0 1903 2 23 0 1903 2 24 0 1903 2 25 0 1903 2 26 0 1903 2 27 0 1903 2 28 0 1903 3 1 0 1903 3 2 0.5 1903 3 3 0 1903 3 4 21.3 1903 3 5 2 1903 3 6 0 1903 3 7 0 1903 3 8 20.1 1903 3 9 21.6 1903 3 10 0 1903 3 11 0 1903 3 12 0 1903 3 13 0 1903 3 14 3.3 1903 3 15 0 1903 3 16 0 1903 3 17 0 1903 3 18 0 1903 3 19 0 1903 3 20 0 1903 3 21 0 1903 3 22 0 1903 3 23 0 1903 3 24 2.8  1903 3 25 0 1903 3 26 0 1903 3 27 0 1903 3 28 0 1903 3 29 0 1903 3 30 0 1903 3 31 0 1903 4  1 0 1903 4 2 0 1903 4 3 0 1903 4 4 6.6 1903 4 5 17.5 1903 4 6 3.3 1903 4 7 0 1903 4 8 0  1903 4 9 0 1903 4 10 0 1903 4 11 5.8 1903 4 12 0 1903 4 13 0 1903 4 14 0 1903 4 15 0 1903 4  16 0 1903 4 17 2 1903 4 18 1.5 1903 4 19 0 1903 4 20 0 1903 4 21 0 1903 4 22 0.3 1903 4 23 0 1903 4 24 2.3 1903 4 25 8.6 1903 4 26 15.2 1903 4 27 3.3 1903 4 28 0 1903 4 29 0 1903 4 30 2  1903 5 1 1.3 1903 5 2 0 1903 5 3 0 1903 5 4 0 1903 5 5 0 1903 5 6 0 1903 5 7 0 1903 5 8  11.4 1903 5 9 0 1903 5 10 0 1903 5 11 1.3 1903 5 12 0 1903 5 13 0 1903 5 14 0 1903 5 15 0  1903 5 16 0 1903 5 17 0 1903 5 18 0 1903 5 19 0.3 1903 5 20 11.7 1903 5 21 11.2 1903 5 22 3.3 1903 5 23 0.3 1903 5 24 0 1903 5 25 0 1903 5 26 1 1903 5 27 0 1903 5 28 0 1903 5 29 0 1903 5 30 2.5 1903 5 31 0 1903 6 1 0 1903 6 2 0 1903 6 3 3 1903 6 4 5.1 1903 6 5 4.3 1903 6 6 11.7  1903 6 7 15.7 1903 6 8 10.7 1903 6 9 1.5 1903 6 10 2 1903 6 11 5.8 1903 6 12 3.3 1903 6 13 1  1903 6 14 1.5 1903 6 15 4.6 1903 6 16 5.1 1903 6 17 0.3 1903 6 18 0 1903 6 19 1.3 1903 6 20 0.5  1903 6 21 0 1903 6 22 0 1903 6 23 8.9 1903 6 24 0.3 1903 6 25 0 1903 6 26 5.1 1903 6 27 0 1903 6 28 0 1903 6 29 0 1903 6 30 0 1903 7 1 0.3 1903 7 2 7.1 1903 7 3 0.5 1903 7 4 9.4 1903 7 5 0.5 1903 7 6 4.8 1903 7 7 0.3 1903 7 8 0 1903 7 9 4.1 1903 7 10 0.3 1903 7 11 0 1903 7 12 0  1903 7 13 0 1903 7 14 3.6 1903 7 15 4.6 1903 7 16 0 1903 7 17 0 1903 7 18 0 1903 7 19 11.2 1903 7 20 0.3 1903 7 21 0 1903 7 22 0 1903 7 23 0 1903 7 24 0 1903 7 25 0 1903 7 26 0 1903 7 27 0 1903 7 28 5.1 1903 7 29 4.8 1903 7 30 0.8 1903 7 31 1.3 1903 8 1 0.3 1903 8 2 0.5 1903 8 3 0 1903 8 4 0.8 1903 8 5 0.3 1903 8 6 0 1903 8 7 0 1903 8 8 0 1903 8 9 0 1903 8 10 0 1903 8 11 2.5 1903 8 12 1.3 1903 8 13 0 1903 8 14 0 1903 8 15 0 1903 8 16 2 1903 8 17 0 1903 8 18 1.5 1903 8 19 0 1903 8 20 0.8 1903 8 21 6.1 1903 8 22 0 1903 8 23 0.5 1903 8 24 0 1903 8 25 0 1903 8 26 0.5 1903 8 27 0 1903 8 28 0 1903 8 29 0 1903 8 30 0.5 1903 8 31 34.5 1903 9 1 0.3  1903 9 2 6.1 1903 9 3 0 1903 9 4 0 1903 9 5 6.9 1903 9 6 2.5 1903 9 7 3.6 1903 9 8 0.5 1903 9 9 0 1903 9 10 0 1903 9 11 0 1903 9 12 0 1903 9 13 4.1 1903 9 14 0 1903 9 15 0 1903 9 16 0 1903 9 17 15.7 1903 9 18 4.1 1903 9 19 0.3 1903 9 20 0 1903 9 21 0 1903 9 22 0 1903 9 23 0.8  1903 9 24 1.3 1903 9 25 10.4 1903 9 26 0 1903 9 27 0 1903 9 28 0.5 1903 9 29 9.7 1903 9 30 0  1903 10 1 0.3 1903 10 2 0 1903 10 3 0.3 1903 10 4 0 1903 10 5 0 1903 10 6 0 1903 10 7 0 1903  10 8 0 1903 10 9 0 1903 10 10 2.3 1903 10 11 0.3 1903 10 12 0 1903 10 13 0 1903 10 14 0 1903 10 15 4.6 1903 10 16 1 1903 10 17 0 1903 10 18 0 1903 10 19 0 1903 10 20 5.8 1903 10 21 0 1903 10 22 0.3 1903 10 23 0 1903 10 24 0.5 1903 10 25 0 1903 10 26 0 1903 10 27 0 1903 10 28 0 1903 10  29 0 1903 10 30 0 1903 10 31 0 1903 11 1 14.5 1903 11 2 0 1903 11 3 0 1903 11 4 0 1903 11 5 0 1903 11 6 0 1903 11 7 0 1903 11 8 0 1903 11 9 0 1903 11 10 0.8 1903 11 11 0.8 1903 11 12 0  1903 11 13 0 1903 11 14 0 1903 11 15 1.7 1903 11 16 2.1 1903 11 17 0 1903 11 18 0 1903 11 19 0  1903 11 20 0 1903 11 21 0 1903 11 22 0 1903 11 23 1 1903 11 24 2.5 1903 11 25 0.3 1903 11 26 9.1  1903 11 27 2.5 1903 11 28 9.9 1903 11 29 16.8 1903 11 30 0.8 1903 12 1 0 1903 12 2 0 1903 12 3 0  1903 12 4 0 1903 12 5 0 1903 12 6 0 1903 12 7 0 1903 12 8 0 1903 12 9 0 1903 12 10 0 1903 12 11 15.5 1903 12 12 0.5 1903 12 13 1.8 1903 12 14 15.7 1903 12 15 0 1903 12 16 0 1903 12 17 0 1903 12 18 0 1903 12 19 0.3 1903 12 20 2.8 1903 12 21 0 1903 12 22 0 1903 12 23 0 1903 12 24 0 1903 12 25 0 1903 12 26 0 1903 12 27 33.5 1903 12 28 0 1903 12 29 0 1903 12 30 0 1903 12 31 0 1904  1 1 2 1904 1 2 2 1904 1 3 33.8 1904 1 4 0.3 1904 1 5 0 1904 1 6 0 1904 1 7 0 1904 1 8 0  1904 1 9 0 1904 1 10 0 1904 1 11 0 1904 1 12 0 1904 1 13 0 1904 1 14 0 1904 1 15 0 1904 1  16 0 1904 1 17 0 1904 1 18 0 1904 1 19 0 1904 1 20 0 1904 1 21 2.8 1904 1 22 18 1904 1 23 1.8 1904 1 24 0 1904 1 25 0 1904 1 26 2 1904 1 27 4.6 1904 1 28 0 1904 1 29 0 1904 1 30 0 1904 1 31 0 1904 2 1 0 1904 2 2 0 1904 2 3 0 1904 2 4 0 1904 2 5 0 1904 2 6 1.8 1904 2 7 0  1904 2 8 0 1904 2 9 0 1904 2 10 0 1904 2 11 0 1904 2 12 0.8 1904 2 13 0 1904 2 14 0 1904 2 15 0 1904 2 16 0 1904 2 17 0 1904 2 18 0 1904 2 19 1.3 1904 2 20 0 1904 2 21 0 1904 2 22 0.5 1904 2 23 0 1904 2 24 0.3 1904 2 25 0 1904 2 26 8.9 1904 2 27 1.5 1904 2 28 0 1904 2 29 0  1904 3 1 0 1904 3 2 0 1904 3 3 0 1904 3 4 0 1904 3 5 0 1904 3 6 0 1904 3 7 0 1904 3 8 0 1904 3 9 0 1904 3 10 0 1904 3 11 0 1904 3 12 0 1904 3 13 0 1904 3 14 0 1904 3 15 0 1904 3 16 6.1 1904 3 17 0 1904 3 18 0 1904 3 19 0 1904 3 20 1.5 1904 3 21 0.8 1904 3 22 0 1904 3 23 0 1904 3 24 0 1904 3 25 0 1904 3 26 0 1904 3 27 0 1904 3 28 0 1904 3 29 0 1904 3 30 0 1904 3 31 0 1904 4 1 0 1904 4 2 0 1904 4 3 0 1904 4 4 0 1904 4 5 0 1904 4 6 0 1904 4 7 0  1904 4 8 0 1904 4 9 0 1904 4 10 0 1904 4 11 0 1904 4 12 0 1904 4 13 0 1904 4 14 0 1904 4  15 0 1904 4 16 0 1904 4 17 0 1904 4 18 23.6 1904 4 19 10.9 1904 4 20 0 1904 4 21 2.5 1904 4 22 1.8 1904 4 23 0 1904 4 24 0 1904 4 25 0 1904 4 26 0 1904 4 27 2 1904 4 28 1 1904 4 29 0 1904 4 30 0 1904 5 1 0 1904 5 2 0 1904 5 3 0 1904 5 4 0.5 1904 5 5 0 1904 5 6 7.4 1904 5 7  0.3 1904 5 8 0 1904 5 9 3.8 1904 5 10 0 1904 5 11 0 1904 5 12 0 1904 5 13 0 1904 5 14 0 1904 5 15 0 1904 5 16 3.6 1904 5 17 8.6 1904 5 18 0 1904 5 19 0 1904 5 20 0 1904 5 21 5.3 1904 5 22 4.3 1904 5 23 0 1904 5 24 0 1904 5 25 0 1904 5 26 0 1904 5 27 0 1904 5 28 0 1904 5 29 0 1904 5 30 18.8 1904 5 31 5.3 1904 6 1 1 1904 6 2 0.3 1904 6 3 0 1904 6 4 0 1904 6 5 0 1904 6 6 0 1904 6 7 1.3 1904 6 8 0 1904 6 9 0 1904 6 10 0 1904 6 11 1.8 1904 6 12 8.4 1904 6 13 0 1904 6 14 0.8 1904 6 15 0 1904 6 16 0.3 1904 6 17 15.2 1904 6 18 13.7 1904 6 19 1.8 1904 6 20 0.5 1904 6 21 0 1904 6 22 7.6 1904 6 23 5.1 1904 6 24 1.3 1904 6 25 0 1904 6 26 0 1904 6 27 0 1904 6 28 0.5 1904 6 29 13 1904 6 30 30.5 1904 7 1 8.6 1904 7 2 1.5 1904 7 3 1.3 1904 7 4 0  1904 7 5 0 1904 7 6 0.8 1904 7 7 1 1904 7 8 1 1904 7 9 0 1904 7 10 0 1904 7 11 0 1904 7 12 0 1904 7 13 11.2 1904 7 14 4.8 1904 7 15 7.6 1904 7 16 3.6 1904 7 17 0 1904 7 18 0.5 1904 7 19 0 1904 7 20 0.3 1904 7 21 0 1904 7 22 0 1904 7 23 0 1904 7 24 15.5 1904 7 25 4.6 1904 7 26 1.3 1904 7 27 1 1904 7 28 0.5 1904 7 29 0 1904 7 30 0 1904 7 31 0 1904 8 1 0 1904 8 2 0 1904 8 3 0 1904 8 4 0 1904 8 5 0 1904 8 6 0 1904 8 7 3.8 1904 8 8 0 1904 8 9 0 1904 8 10 0  1904 8 11 0 1904 8 12 0 1904 8 13 9.1 1904 8 14 0 1904 8 15 0 1904 8 16 1.3 1904 8 17 0 1904 8 18 0.5 1904 8 19 2.5 1904 8 20 7.1 1904 8 21 0 1904 8 22 0 1904 8 23 2.5 1904 8 24 2.8 1904 8 25 0 1904 8 26 0.3 
i want the counting number of rain as follows:

??????????? Jan??????? Feb??????? Mar??? Apr??? May??? Jun??? Jul??? Aug??? Sep??? Oct??? Nov??? Dec
1901??????? x190219031904

x = number of rain for > 0.000001

Thanks for your help.

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Sep  8 16:23:16 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 8 Sep 2015 14:23:16 +0000
Subject: [R] help with reshape
In-Reply-To: <55EEDAC6.9020202@univ-reims.fr>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE76A@mb02.ads.tamu.edu>

I have not followed this thread closely, but this seems to work:

mydata$repet[is.na(mydata$repet)] <- 0
reshape(mydata, timevar="Elem", idvar=c("Etape","Ech", "repet", "dilution","Rincage"),
     direction="wide", drop=c("ID","Nom_ech"))

If this is the expected outcome, the problem is the NA values in repet. I changed them to 0 since you did not have any 0 entries in the data (otherwise you could use 999 or some other value that does not occur in the data). Change them back after running reshape().

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan Calandra
Sent: Tuesday, September 8, 2015 7:56 AM
To: PIKAL Petr; R list
Subject: Re: [R] help with reshape

Thanks Petr,
It looks good, but I have to check in more details.

Can anyone help me with my original solution using reshape()? I'd like 
to understand what I did wrong.
reshape(mydata, timevar="Elem", 
idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide", 
drop=c("ID","Nom_ech"))

Thank you in advance
Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 08/09/15 14:45, PIKAL Petr a ?crit :
> Hi
>
> I looked into docs to reshape2 and played around a bit and by some magical feature
>
> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
>
> probably works as you expect.
>
> I cannot comment your solution as I use reshape only sparsely.
>
> Cheers
> Petr
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>> Calandra
>> Sent: Tuesday, September 08, 2015 2:18 PM
>> To: R list
>> Subject: Re: [R] help with reshape
>>
>> Thank you Petr,
>>
>> It kinda works, but not completely. The problem is that it produces a
>> column for each value ("Moyenne"), and not each element of "Elem". That
>> means I have only one value per column, instead of up to 3.
>> For example, I have 3 columns for Al1670 instead of just one, and each
>> column contains maximum one value (the others being NA).
>>
>> Not sure I am being clear...
>>
>> By the way, I don't understand why my solution did not work; what is
>> wrong there?
>>
>> Thank you again!
>> Ivan
>>
>> --
>> Ivan Calandra, PhD
>> University of Reims Champagne-Ardenne
>> GEGENAA - EA 3795
>> CREA - 2 esplanade Roland Garros
>> 51100 Reims, France
>> +33(0)3 26 77 36 89
>> ivan.calandra at univ-reims.fr
>> https://www.researchgate.net/profile/Ivan_Calandra
>>
>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
>>> Hi
>>>
>>> I am not sure if I got it
>>>
>>> library(reshape2)
>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
>>>
>>> gives me 3 rows but names need some tweaking afterwards.
>>>
>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
>> "_"),
>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
>>>
>>> Cheers
>>> Petr
>>>
>>>
>>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>>>> Calandra
>>>> Sent: Tuesday, September 08, 2015 12:33 PM
>>>> To: R list
>>>> Subject: [R] help with reshape
>>>>
>>>> Dear users,
>>>>
>>>> I'm having troubles with reshaping a data.frame from long to wide
>>>> format.
>>>> I copy the output of dput() at the end of the mail because it is
>>>> quite long.
>>>>
>>>> Each row of the column "Elem" should be transposed to a new column.
>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage" define
>>>> the samples. Meaning that for each unique combination of these
>>>> variables, I want a single row, and as many columns as elements in
>> "Elem".
>>>> So I tried:
>>>> reshape(mydata, timevar="Elem",
>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>>> direction="wide",
>>>> drop=c("ID","Nom_ech"))
>>>>
>>>> The problem is that some columns are not used at all for defining
>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing,
>>>> but I don't understand why.
>>>>
>>>> Can you help me with that? I have no idea what I am doing wrong...
>>>>
>>>> Thanks in advance,
>>>> Ivan
>>>>
>>>>
>>>>
>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549,
>>>> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
>>>> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
>> 11583,
>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
>> 696,
>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>> B2",
>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>> B2",
>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>> B2",
>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>>>> *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2", "B2",
>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3", "F10-3",
>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-
>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1,
>> 1,
>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>>>> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100, 100,
>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), Rincage =
>>>> c("non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non",
>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
>> "oui",
>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non",
>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>> "non",
>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
>> "Cu3247",
>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
>> "Mn2605",
>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>> "Cd2288",
>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
>> "As1937",
>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
>> "Cr2835",
>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>> "Mg2852",
>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875,
>>>> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
>>>> 337.433333333333, 323.533333333333, 0.877166666666667,
>>>> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667, NA,
>>>> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
>> NA,
>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333, 128.9,
>>>> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names = c("ID",
>>>> "Nom_ech", "Etape", "Ech", "repet", "dilution", "Rincage", "Elem",
>>>> "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>>>> 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L,
>> 32L,
>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
>>>>
>>>> --
>>>>
>>>> Ivan Calandra, PhD
>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>> esplanade Roland Garros 51100 Reims, France
>>>> +33(0)3 26 77 36 89
>>>> ivan.calandra at univ-reims.fr
>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>> guide.html and provide commented, minimal, self-contained,
>>>> reproducible code.
>>> ________________________________
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From ivan.calandra at univ-reims.fr  Tue Sep  8 16:33:06 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Tue, 8 Sep 2015 16:33:06 +0200
Subject: [R] help with reshape
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE76A@mb02.ads.tamu.edu>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE76A@mb02.ads.tamu.edu>
Message-ID: <55EEF1A2.4000206@univ-reims.fr>

David,

It seems that your solution works, but why would that be? And why would 
this NA behavior be platform or version specific?

I really need to check with a newer version of R...

Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 08/09/15 16:23, David L Carlson a ?crit :
> I have not followed this thread closely, but this seems to work:
>
> mydata$repet[is.na(mydata$repet)] <- 0
> reshape(mydata, timevar="Elem", idvar=c("Etape","Ech", "repet", "dilution","Rincage"),
>       direction="wide", drop=c("ID","Nom_ech"))
>
> If this is the expected outcome, the problem is the NA values in repet. I changed them to 0 since you did not have any 0 entries in the data (otherwise you could use 999 or some other value that does not occur in the data). Change them back after running reshape().
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
> ----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan Calandra
> Sent: Tuesday, September 8, 2015 7:56 AM
> To: PIKAL Petr; R list
> Subject: Re: [R] help with reshape
>
> Thanks Petr,
> It looks good, but I have to check in more details.
>
> Can anyone help me with my original solution using reshape()? I'd like
> to understand what I did wrong.
> reshape(mydata, timevar="Elem",
> idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide",
> drop=c("ID","Nom_ech"))
>
> Thank you in advance
> Ivan
>
> --
> Ivan Calandra, PhD
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
>
> Le 08/09/15 14:45, PIKAL Petr a ?crit :
>> Hi
>>
>> I looked into docs to reshape2 and played around a bit and by some magical feature
>>
>> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
>>
>> probably works as you expect.
>>
>> I cannot comment your solution as I use reshape only sparsely.
>>
>> Cheers
>> Petr
>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>>> Calandra
>>> Sent: Tuesday, September 08, 2015 2:18 PM
>>> To: R list
>>> Subject: Re: [R] help with reshape
>>>
>>> Thank you Petr,
>>>
>>> It kinda works, but not completely. The problem is that it produces a
>>> column for each value ("Moyenne"), and not each element of "Elem". That
>>> means I have only one value per column, instead of up to 3.
>>> For example, I have 3 columns for Al1670 instead of just one, and each
>>> column contains maximum one value (the others being NA).
>>>
>>> Not sure I am being clear...
>>>
>>> By the way, I don't understand why my solution did not work; what is
>>> wrong there?
>>>
>>> Thank you again!
>>> Ivan
>>>
>>> --
>>> Ivan Calandra, PhD
>>> University of Reims Champagne-Ardenne
>>> GEGENAA - EA 3795
>>> CREA - 2 esplanade Roland Garros
>>> 51100 Reims, France
>>> +33(0)3 26 77 36 89
>>> ivan.calandra at univ-reims.fr
>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>
>>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
>>>> Hi
>>>>
>>>> I am not sure if I got it
>>>>
>>>> library(reshape2)
>>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
>>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
>>>>
>>>> gives me 3 rows but names need some tweaking afterwards.
>>>>
>>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
>>> "_"),
>>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
>>>>
>>>> Cheers
>>>> Petr
>>>>
>>>>
>>>>> -----Original Message-----
>>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>>>>> Calandra
>>>>> Sent: Tuesday, September 08, 2015 12:33 PM
>>>>> To: R list
>>>>> Subject: [R] help with reshape
>>>>>
>>>>> Dear users,
>>>>>
>>>>> I'm having troubles with reshaping a data.frame from long to wide
>>>>> format.
>>>>> I copy the output of dput() at the end of the mail because it is
>>>>> quite long.
>>>>>
>>>>> Each row of the column "Elem" should be transposed to a new column.
>>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage" define
>>>>> the samples. Meaning that for each unique combination of these
>>>>> variables, I want a single row, and as many columns as elements in
>>> "Elem".
>>>>> So I tried:
>>>>> reshape(mydata, timevar="Elem",
>>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>>>> direction="wide",
>>>>> drop=c("ID","Nom_ech"))
>>>>>
>>>>> The problem is that some columns are not used at all for defining
>>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing,
>>>>> but I don't understand why.
>>>>>
>>>>> Can you help me with that? I have no idea what I am doing wrong...
>>>>>
>>>>> Thanks in advance,
>>>>> Ivan
>>>>>
>>>>>
>>>>>
>>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549,
>>>>> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
>>>>> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
>>> 11583,
>>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
>>> 696,
>>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>> B2",
>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>> B2",
>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>> B2",
>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>>>>> *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
>>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2", "B2",
>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3", "F10-3",
>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-
>>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
>>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1,
>>> 1,
>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
>>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100, 100,
>>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), Rincage =
>>>>> c("non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>> "non",
>>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
>>> "oui",
>>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
>>> "non",
>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>> "non",
>>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>>>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
>>> "Cu3247",
>>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
>>> "Mn2605",
>>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>> "Cd2288",
>>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
>>> "As1937",
>>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
>>> "Cr2835",
>>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>>> "Mg2852",
>>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
>>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
>>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875,
>>>>> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
>>>>> 337.433333333333, 323.533333333333, 0.877166666666667,
>>>>> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667, NA,
>>>>> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
>>> NA,
>>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333, 128.9,
>>>>> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names = c("ID",
>>>>> "Nom_ech", "Etape", "Ech", "repet", "dilution", "Rincage", "Elem",
>>>>> "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>>>>> 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L,
>>> 32L,
>>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
>>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
>>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
>>>>>
>>>>> --
>>>>>
>>>>> Ivan Calandra, PhD
>>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>>> esplanade Roland Garros 51100 Reims, France
>>>>> +33(0)3 26 77 36 89
>>>>> ivan.calandra at univ-reims.fr
>>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>>> guide.html and provide commented, minimal, self-contained,
>>>>> reproducible code.
>>>> ________________________________
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
>> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
>> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Tue Sep  8 16:37:05 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Tue, 8 Sep 2015 07:37:05 -0700 (PDT)
Subject: [R] Counting number of rain
In-Reply-To: <610724885.3110482.1441695538956.JavaMail.yahoo@mail.yahoo.com>
References: <610724885.3110482.1441695538956.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1441723025370-4712011.post@n4.nabble.com>

Try the following:

## step 1: write raw data to an array
junk<-scan('clipboard') 
# entering the numbers (not the 'year' etc. labels) into R as a vector after

junk<-t(array(junk,dim=c(4,length(junk)/4))) 
# convert the vector into a 2-d array with 4 columns (year, month, day,
amount)

## step 2: create a dataframe to store and display the results
nyr<-length(unique(junk[,1]))
ans<-data.frame(array(dim=c(nyr,12))) # a dataframe for storing the results
names(ans)<-c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')
yrs<-sort(unique(junk[,1]))
row.names(ans)<-yrs

# step 3: calculate
for (yi in 1:nyr){ # loop through the years...
  for (mi in 1:12){ # ...and the months
     ans[yi,mi]<-sum(junk[junk[,1]==yrs[yi] & junk[,2]==mi,4]>0.000001) #
count the rainy days by
     # first subsetting the junk array by rows that match the given year and
month and sum
  }
}

Does that help?

- Dan



--
View this message in context: http://r.789695.n4.nabble.com/Counting-number-of-rain-tp4712007p4712011.html
Sent from the R help mailing list archive at Nabble.com.


From dcarlson at tamu.edu  Tue Sep  8 17:02:55 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 8 Sep 2015 15:02:55 +0000
Subject: [R] help with reshape
In-Reply-To: <55EEF1A2.4000206@univ-reims.fr>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE76A@mb02.ads.tamu.edu>
	<55EEF1A2.4000206@univ-reims.fr>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE85B@mb02.ads.tamu.edu>

I don't think it is platform or version specific. The purpose of the missing value, NA (i.e. Not Available), is to flag the value for special handling in some way, often by deletion. You cannot assume that NA will be treated as any other value since that would defeat the whole purpose of flagging the value as missing. Similar results occur if you try to create tables or cross-tabulations of variables that include NAs with table() and xtabs().

David

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan Calandra
Sent: Tuesday, September 8, 2015 9:33 AM
To: R list
Subject: Re: [R] help with reshape

David,

It seems that your solution works, but why would that be? And why would 
this NA behavior be platform or version specific?

I really need to check with a newer version of R...

Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 08/09/15 16:23, David L Carlson a ?crit :
> I have not followed this thread closely, but this seems to work:
>
> mydata$repet[is.na(mydata$repet)] <- 0
> reshape(mydata, timevar="Elem", idvar=c("Etape","Ech", "repet", "dilution","Rincage"),
>       direction="wide", drop=c("ID","Nom_ech"))
>
> If this is the expected outcome, the problem is the NA values in repet. I changed them to 0 since you did not have any 0 entries in the data (otherwise you could use 999 or some other value that does not occur in the data). Change them back after running reshape().
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
> ----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan Calandra
> Sent: Tuesday, September 8, 2015 7:56 AM
> To: PIKAL Petr; R list
> Subject: Re: [R] help with reshape
>
> Thanks Petr,
> It looks good, but I have to check in more details.
>
> Can anyone help me with my original solution using reshape()? I'd like
> to understand what I did wrong.
> reshape(mydata, timevar="Elem",
> idvar=c("Etape","Ech","repet","dilution","Rincage"), direction="wide",
> drop=c("ID","Nom_ech"))
>
> Thank you in advance
> Ivan
>
> --
> Ivan Calandra, PhD
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
>
> Le 08/09/15 14:45, PIKAL Petr a ?crit :
>> Hi
>>
>> I looked into docs to reshape2 and played around a bit and by some magical feature
>>
>> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
>>
>> probably works as you expect.
>>
>> I cannot comment your solution as I use reshape only sparsely.
>>
>> Cheers
>> Petr
>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>>> Calandra
>>> Sent: Tuesday, September 08, 2015 2:18 PM
>>> To: R list
>>> Subject: Re: [R] help with reshape
>>>
>>> Thank you Petr,
>>>
>>> It kinda works, but not completely. The problem is that it produces a
>>> column for each value ("Moyenne"), and not each element of "Elem". That
>>> means I have only one value per column, instead of up to 3.
>>> For example, I have 3 columns for Al1670 instead of just one, and each
>>> column contains maximum one value (the others being NA).
>>>
>>> Not sure I am being clear...
>>>
>>> By the way, I don't understand why my solution did not work; what is
>>> wrong there?
>>>
>>> Thank you again!
>>> Ivan
>>>
>>> --
>>> Ivan Calandra, PhD
>>> University of Reims Champagne-Ardenne
>>> GEGENAA - EA 3795
>>> CREA - 2 esplanade Roland Garros
>>> 51100 Reims, France
>>> +33(0)3 26 77 36 89
>>> ivan.calandra at univ-reims.fr
>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>
>>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
>>>> Hi
>>>>
>>>> I am not sure if I got it
>>>>
>>>> library(reshape2)
>>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
>>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
>>>>
>>>> gives me 3 rows but names need some tweaking afterwards.
>>>>
>>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
>>> "_"),
>>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
>>>>
>>>> Cheers
>>>> Petr
>>>>
>>>>
>>>>> -----Original Message-----
>>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>>>>> Calandra
>>>>> Sent: Tuesday, September 08, 2015 12:33 PM
>>>>> To: R list
>>>>> Subject: [R] help with reshape
>>>>>
>>>>> Dear users,
>>>>>
>>>>> I'm having troubles with reshaping a data.frame from long to wide
>>>>> format.
>>>>> I copy the output of dput() at the end of the mail because it is
>>>>> quite long.
>>>>>
>>>>> Each row of the column "Elem" should be transposed to a new column.
>>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage" define
>>>>> the samples. Meaning that for each unique combination of these
>>>>> variables, I want a single row, and as many columns as elements in
>>> "Elem".
>>>>> So I tried:
>>>>> reshape(mydata, timevar="Elem",
>>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>>>> direction="wide",
>>>>> drop=c("ID","Nom_ech"))
>>>>>
>>>>> The problem is that some columns are not used at all for defining
>>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is missing,
>>>>> but I don't understand why.
>>>>>
>>>>> Can you help me with that? I have no idea what I am doing wrong...
>>>>>
>>>>> Thanks in advance,
>>>>> Ivan
>>>>>
>>>>>
>>>>>
>>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548, 549,
>>>>> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
>>>>> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
>>> 11583,
>>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
>>> 696,
>>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1 B2",
>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>> B2",
>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>> B2",
>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1
>>>>> B2",
>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1)
>>>>> *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
>>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2", "B2",
>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3", "F10-3",
>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-
>>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
>>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1,
>>> 1,
>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
>>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100, 100, 100,
>>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), Rincage =
>>>>> c("non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>> "non",
>>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
>>> "oui",
>>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
>>> "non",
>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>> "non",
>>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>>>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
>>> "Cu3247",
>>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
>>> "Mn2605",
>>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>> "Cd2288",
>>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
>>> "As1937",
>>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
>>> "Cr2835",
>>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>>> "Mg2852",
>>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
>>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
>>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69, 0.26875,
>>>>> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
>>>>> 337.433333333333, 323.533333333333, 0.877166666666667,
>>>>> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667, NA,
>>>>> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
>>> NA,
>>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333, 128.9,
>>>>> 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names = c("ID",
>>>>> "Nom_ech", "Etape", "Ech", "repet", "dilution", "Rincage", "Elem",
>>>>> "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>>>>> 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L, 30L, 31L,
>>> 32L,
>>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
>>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
>>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
>>>>>
>>>>> --
>>>>>
>>>>> Ivan Calandra, PhD
>>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>>> esplanade Roland Garros 51100 Reims, France
>>>>> +33(0)3 26 77 36 89
>>>>> ivan.calandra at univ-reims.fr
>>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>>> guide.html and provide commented, minimal, self-contained,
>>>>> reproducible code.
>>>> ________________________________
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
>> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
>> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From jrkrideau at inbox.com  Tue Sep  8 17:05:17 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 8 Sep 2015 07:05:17 -0800
Subject: [R] Counting number of rain
In-Reply-To: <610724885.3110482.1441695538956.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <81BD9CEB1D4.000001A4jrkrideau@inbox.com>

Assuming your data is already in R format please sent it  dput() format.  See ?dput or http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html for more details.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: r-help at r-project.org
> Sent: Tue, 8 Sep 2015 06:58:58 +0000 (UTC)
> To: r-help at r-project.org
> Subject: [R] Counting number of rain
> 
> Hello R-users,
> I want to ask how to count the number of daily rain data.? My data as
> below:
>  Year Month Day Amount 1901 1 1 0 1901 1 2 3 1901 1 3 0 1901 1 4 0.5 1901
> 1 5 0 1901 1 6 0  1901 1 7 0.3 1901 1 8 0 1901 1 9 0 1901 1 10 0 1901 1
> 11 0.5 1901 1 12 1.8 1901 1 13 0 1901 1 14 0 1901 1 15 2.5 1901 1 16 0
> 1901 1 17 0 1901 1 18 0 1901 1 19 0 1901 1 20 0 1901 1 21 0 1901 1 22 0
> 1901 1 23 0 1901 1 24 0 1901 1 25 0 1901 1 26 16.5 1901 1 27 0.3 1901 1
> 28 0 1901 1 29 0 1901 1 30 0 1901 1 31 0 1901 2 1 0 1901 2 2 0 1901 2 3 0
> 1901 2 4 0 1901 2 5 0 1901 2 6 0 1901 2 7 0 1901 2 8 0.3 1901 2 9 0 1901
> 2 10 0 1901 2 11 0 1901 2 12 1 1901 2 13 0.3 1901 2 14 0 1901 2 15 0 1901
> 2 16 0 1901 2 17 0 1901 2 18 0 1901 2 19 0 1901 2 20 0 1901 2 21 0 1901 2
> 22 0 1901 2 23 0.3 1901 2 24 0 1901 2 25 0 1901 2 26 0.3 1901 2 27 0 1901
> 2 28 0 1901 3 1 0 1901 3 2 0.8 1901 3 3 2.3 1901 3 4 0 1901 3 5 0 1901 3
> 6 0 1901 3 7 0 1901 3 8 0 1901 3 9 0 1901 3 10 2 1901 3 11 0 1901 3 12 0
> 1901 3 13 0 1901 3 14 0 1901 3 15 0 1901 3 16 0 1901 3 17 0 1901 3 18 0
> 1901 3 19 0 1901 3 20 0 1901 3 21 0 1901 3 22 1.5 1901 3 23 1.3 1901 3 24
> 0 1901 3 25 0 1901 3 26 0 1901 3 27 0 1901 3 28 0.3 1901 3 29 0.3  1901 3
> 30 4.6 1901 3 31 0 1901 4 1 0 1901 4 2 4.6 1901 4 3 30.7 1901 4 4 0 1901
> 4 5 0 1901 4 6 0 1901 4 7 0 1901 4 8 0 1901 4 9 0 1901 4 10 0 1901 4 11 0
> 1901 4 12 0 1901 4 13 0 1901 4 14 0 1901 4 15 0.3 1901 4 16 1.3 1901 4 17
> 0 1901 4 18 0 1901 4 19 0.3 1901 4 20 1 1901 4 21 9.4 1901 4 22 0.5 1901
> 4 23 0.3 1901 4 24 0 1901 4 25 0 1901 4 26 0 1901 4 27 0 1901 4 28 0 1901
> 4 29 0 1901 4 30 0 1901 5 1 0 1901 5 2 0 1901 5 3 0 1901 5 4 0 1901 5 5 0
> 1901  5 6 0 1901 5 7 0 1901 5 8 0.5 1901 5 9 2.3 1901 5 10 0.3 1901 5 11
> 0 1901 5 12 0 1901 5 13 0 1901 5 14 0 1901 5 15 0 1901 5 16 0 1901 5 17 0
> 1901 5 18 0 1901 5 19 0 1901 5 20 0 1901 5 21 0.5 1901 5 22 0 1901 5 23 0
> 1901 5 24 0 1901 5 25 0 1901 5 26 4.8 1901 5 27 10.9 1901 5 28 3.6 1901 5
> 29 0 1901 5 30 0 1901 5 31 5.1 1901 6 1 0.5 1901 6 2 0 1901 6 3 2 1901 6
> 4 0  1901 6 5 10.2 1901 6 6 33.3 1901 6 7 0.3 1901 6 8 0 1901 6 9 0 1901
> 6 10 0.5 1901 6 11 0.5 1901 6 12 0.3 1901 6 13 2.8 1901 6 14 5.6 1901 6
> 15 0.3 1901 6 16 6.6 1901 6 17 14.2 1901 6 18 4.8  1901 6 19 8.4 1901 6
> 20 1.8 1901 6 21 1.8 1901 6 22 0.3 1901 6 23 8.6 1901 6 24 0 1901 6 25 0
> 1901 6 26 0 1901 6 27 0 1901 6 28 0 1901 6 29 0 1901 6 30 0 1901 7 1 0
> 1901 7 2 0 1901 7 3 0 1901 7 4 0 1901 7 5 1 1901 7 6 0.5 1901 7 7 0.3
> 1901 7 8 0.3 1901 7 9 6.1 1901 7 10 0.3  1901 7 11 1.5 1901 7 12 0 1901 7
> 13 1.5 1901 7 14 0.3 1901 7 15 3.3 1901 7 16 2.3 1901 7 17 0.5  1901 7 18
> 0 1901 7 19 0 1901 7 20 0 1901 7 21 1.8 1901 7 22 0 1901 7 23 1 1901 7 24
> 0.3 1901  7 25 0.3 1901 7 26 1.3 1901 7 27 17 1901 7 28 6.6 1901 7 29 6.1
> 1901 7 30 0.5 1901 7 31 0.3 1901 8 1 0 1901 8 2 0 1901 8 3 0 1901 8 4 0
> 1901 8 5 0 1901 8 6 3.3 1901 8 7 4.1 1901 8 8 0.3  1901 8 9 0 1901 8 10 0
> 1901 8 11 0 1901 8 12 0 1901 8 13 0 1901 8 14 0 1901 8 15 0 1901 8 16 0
> 1901 8 17 0.5 1901 8 18 0 1901 8 19 0 1901 8 20 0 1901 8 21 0 1901 8 22 0
> 1901 8 23 0.3 1901 8 24 1 1901 8 25 0 1901 8 26 0 1901 8 27 10.2 1901 8
> 28 1.5 1901 8 29 0.5 1901 8 30 1.3  1901 8 31 0 1901 9 1 0 1901 9 2 3
> 1901 9 3 1 1901 9 4 0.5 1901 9 5 0.3 1901 9 6 0 1901 9 7 0 1901 9 8 2.3
> 1901 9 9 0.3 1901 9 10 0 1901 9 11 0 1901 9 12 0 1901 9 13 0 1901 9 14 0
> 1901 9 15 0 1901 9 16 0 1901 9 17 0 1901 9 18 1.8 1901 9 19 8.1 1901 9 20
> 0.3 1901 9 21 5.8 1901 9 22 4.1 1901 9 23 0.3 1901 9 24 1.8 1901 9 25 0
> 1901 9 26 0 1901 9 27 0 1901 9 28 0 1901  9 29 1.8 1901 9 30 0.8 1901 10
> 1 0 1901 10 2 0 1901 10 3 0 1901 10 4 0 1901 10 5 0.3 1901 10 6 0 1901 10
> 7 0 1901 10 8 0 1901 10 9 0 1901 10 10 0 1901 10 11 0.3 1901 10 12 3.8
> 1901 10 13 0.4 1901 10 14 9 1901 10 15 2 1901 10 16 1 1901 10 17 0 1901
> 10 18 0 1901 10 19 0 1901 10 20 0.3 1901 10 21 0 1901 10 22 0 1901 10 23
> 0 1901 10 24 0 1901 10 25 0 1901 10 26 0 1901 10 27 14.5  1901 10 28 6.4
> 1901 10 29 0.8 1901 10 30 0 1901 10 31 0 1901 11 1 0 1901 11 2 0 1901 11
> 3 0  1901 11 4 0 1901 11 5 0 1901 11 6 0 1901 11 7 0 1901 11 8 0 1901 11
> 9 0 1901 11 10 0 1901 11 11 0 1901 11 12 5.1 1901 11 13 0.3 1901 11 14
> 5.8 1901 11 15 0 1901 11 16 0 1901 11 17 1 1901 11 18 0.5 1901 11 19 0
> 1901 11 20 0 1901 11 21 0 1901 11 22 0 1901 11 23 0 1901 11 24 0 1901 11
> 25 0.3 1901 11 26 0 1901 11 27 0 1901 11 28 0 1901 11 29 0 1901 11 30 3.3
> 1901 12 1 0 1901 12 2 0 1901 12 3 0 1901 12 4 0 1901 12 5 0 1901 12 6 0
> 1901 12 7 0 1901 12 8 0 1901 12 9 0 1901 12 10 0 1901 12 11 0 1901 12 12
> 0 1901 12 13 0 1901 12 14 0 1901 12 15 0 1901 12 16 0 1901 12 17 0 1901
> 12 18 0 1901 12 19 0 1901 12 20 0 1901 12 21 6.1 1901 12 22 5.6 1901 12
> 23 0 1901 12 24 0 1901 12 25 0 1901 12 26 0 1901 12 27 0 1901 12 28 0
> 1901 12 29 0 1901 12 30 0 1901 12 31 9.9 1902 1 1 0 1902 1 2 0 1902 1 3 0
> 1902 1 4 4.1 1902 1 5 0 1902 1 6 0 1902 1 7 0 1902 1 8 0 1902 1 9 2.5
> 1902 1 10 0 1902 1 11 0 1902 1 12 0 1902 1 13 0.3 1902 1 14 0 1902 1 15 0
> 1902 1 16 0 1902 1 17 0 1902 1 18 0 1902 1 19 0 1902 1 20 0.3 1902 1 21
> 0.8 1902 1 22 1.8  1902 1 23 0 1902 1 24 0 1902 1 25 0 1902 1 26 2.8 1902
> 1 27 0 1902 1 28 0.3 1902 1 29 0 1902 1 30 0 1902 1 31 0 1902 2 1 2.8
> 1902 2 2 0 1902 2 3 0.3 1902 2 4 0 1902 2 5 0 1902 2 6 0 1902 2 7 0 1902
> 2 8 0 1902 2 9 0 1902 2 10 0 1902 2 11 0 1902 2 12 5.6 1902 2 13 0 1902
> 2 14 0 1902 2 15 0 1902 2 16 0 1902 2 17 0 1902 2 18 0 1902 2 19 0 1902 2
> 20 0.8 1902 2 21 0 1902 2 22 0 1902 2 23 0 1902 2 24 2.8 1902 2 25 2.8
> 1902 2 26 0 1902 2 27 0 1902 2 28 0 1902 3 1 0 1902 3 2 0 1902 3 3 0 1902
> 3 4 0 1902 3 5 0 1902 3 6 0 1902 3 7 0 1902 3 8 0  1902 3 9 0 1902 3 10 0
> 1902 3 11 0 1902 3 12 0 1902 3 13 0 1902 3 14 0 1902 3 15 0 1902 3 16 0
> 1902 3 17 0 1902 3 18 0 1902 3 19 0 1902 3 20 0 1902 3 21 0 1902 3 22 0
> 1902 3 23 0  1902 3 24 0 1902 3 25 4.8 1902 3 26 11.9 1902 3 27 3.8 1902
> 3 28 1.8 1902 3 29 0 1902 3 30 0 1902 3 31 0 1902 4 1 0 1902 4 2 0 1902 4
> 3 0 1902 4 4 0 1902 4 5 0 1902 4 6 0 1902 4 7 0  1902 4 8 0 1902 4 9 2.5
> 1902 4 10 0 1902 4 11 0 1902 4 12 0 1902 4 13 0 1902 4 14 0 1902 4 15 0
> 1902 4 16 0 1902 4 17 0 1902 4 18 0 1902 4 19 0 1902 4 20 0 1902 4 21 0
> 1902 4 22 0  1902 4 23 0 1902 4 24 0 1902 4 25 0 1902 4 26 1.3 1902 4 27
> 8.1 1902 4 28 0 1902 4 29 0 1902 4 30 0 1902 5 1 0 1902 5 2 0 1902 5 3 0
> 1902 5 4 0 1902 5 5 0 1902 5 6 3 1902 5 7 3.8  1902 5 8 4.3 1902 5 9 2.8
> 1902 5 10 3 1902 5 11 0 1902 5 12 0.5 1902 5 13 2 1902 5 14 0 1902 5 15
> 0.3 1902 5 16 0.8 1902 5 17 0.3 1902 5 18 0 1902 5 19 0 1902 5 20 0 1902
> 5 21 0 1902 5  22 0 1902 5 23 0 1902 5 24 0 1902 5 25 0 1902 5 26 0 1902
> 5 27 0 1902 5 28 0 1902 5 29 0.3  1902 5 30 1.8 1902 5 31 0 1902 6 1 5.3
> 1902 6 2 0 1902 6 3 0 1902 6 4 0.3 1902 6 5 0 1902 6 6 0 1902 6 7 4.6
> 1902 6 8 5.1 1902 6 9 12.4 1902 6 10 5.3 1902 6 11 0.3 1902 6 12 0 1902 6
> 13 0 1902 6 14 3 1902 6 15 4.1 1902 6 16 7.4 1902 6 17 5.1 1902 6 18 8.1
> 1902 6 19 18.3 1902 6 20 3.3 1902 6 21 16.5 1902 6 22 0.5 1902 6 23 0
> 1902 6 24 0 1902 6 25 0 1902 6 26 0 1902 6 27 0 1902 6 28 0 1902 6 29 0
> 1902 6 30 0 1902 7 1 0 1902 7 2 0 1902 7 3 0 1902 7 4 0 1902 7  5 0 1902
> 7 6 1.8 1902 7 7 0 1902 7 8 0 1902 7 9 0 1902 7 10 1 1902 7 11 0.3 1902 7
> 12 0  1902 7 13 0.8 1902 7 14 0 1902 7 15 0 1902 7 16 0 1902 7 17 0.8
> 1902 7 18 0 1902 7 19 1.3 1902 7 20 0.5 1902 7 21 0 1902 7 22 1.5 1902 7
> 23 1.3 1902 7 24 2.5 1902 7 25 9.4 1902 7 26 0.3 1902 7 27 0 1902 7 28 0
> 1902 7 29 0 1902 7 30 5.1 1902 7 31 2 1902 8 1 0 1902 8 2 3 1902 8 3 1.8
> 1902 8 4 0 1902 8 5 0.5 1902 8 6 3.3 1902 8 7 3.8 1902 8 8 0 1902 8 9 0
> 1902 8 10 0 1902 8 11 0.3 1902 8 12 0 1902 8 13 0 1902 8 14 0 1902 8 15 0
> 1902 8 16 0 1902 8 17 0 1902 8 18 0 1902 8 19 0 1902 8 20 0 1902 8 21 0
> 1902 8 22 0 1902 8 23 1.5 1902 8 24 2.8 1902 8 25 0  1902 8 26 0 1902 8
> 27 5.8 1902 8 28 0 1902 8 29 1.5 1902 8 30 0 1902 8 31 0 1902 9 1 0 1902
> 9 2 2.5 1902 9 3 0.8 1902 9 4 0.8 1902 9 5 0 1902 9 6 0 1902 9 7 0 1902 9
> 8 0 1902 9 9 0  1902 9 10 0 1902 9 11 9.4 1902 9 12 0.5 1902 9 13 6.6
> 1902 9 14 0.5 1902 9 15 2.8 1902 9 16 4.1  1902 9 17 6.9 1902 9 18 0.5
> 1902 9 19 0 1902 9 20 0 1902 9 21 0 1902 9 22 0 1902 9 23 0 1902 9 24 0
> 1902 9 25 0 1902 9 26 1.8 1902 9 27 0.3 1902 9 28 0.3 1902 9 29 0 1902 9
> 30 0 1902 10  1 0.8 1902 10 2 1.5 1902 10 3 2 1902 10 4 0 1902 10 5 0
> 1902 10 6 0.3 1902 10 7 0.5 1902 10 8  1 1902 10 9 0 1902 10 10 1 1902 10
> 11 10.9 1902 10 12 0.3 1902 10 13 0 1902 10 14 0 1902 10 15 0 1902 10 16
> 0 1902 10 17 0 1902 10 18 2.3 1902 10 19 0.3 1902 10 20 0 1902 10 21 0
> 1902 10 22 1.8 1902 10 23 0.5 1902 10 24 1.5 1902 10 25 0 1902 10 26 0
> 1902 10 27 1 1902 10 28 5.3 1902 10 29 0 1902 10 30 0 1902 10 31 0 1902
> 11 1 0 1902 11 2 0 1902 11 3 0 1902 11 4 0 1902 11 5 0 1902 11 6 0.8 1902
> 11 7 0.3 1902 11 8 0.5 1902 11 9 1 1902 11 10 0 1902 11 11 0 1902 11 12 0
> 1902 11 13 7.1 1902 11 14 0.3 1902 11 15 0 1902 11 16 0 1902 11 17 0 1902
> 11 18 0 1902 11 19 0.3 1902 11 20 1 1902 11 21 0 1902 11 22 0 1902 11 23
> 0 1902 11 24 0 1902 11 25 0 1902 11 26 0 1902 11 27 0 1902 11 28 0 1902
> 11 29 0 1902 11 30 0 1902 12 1 0 1902 12 2 0 1902 12 3 0 1902 12 4  0
> 1902 12 5 1.8 1902 12 6 6.9 1902 12 7 0 1902 12 8 0 1902 12 9 0 1902 12
> 10 0.5 1902 12 11 4.8 1902 12 12 0 1902 12 13 0 1902 12 14 0 1902 12 15 0
> 1902 12 16 0.8 1902 12 17 15.5 1902 12 18 11.7 1902 12 19 9.4 1902 12 20
> 0.3 1902 12 21 0 1902 12 22 0 1902 12 23 0 1902 12 24 0.3 1902 12 25 0.3
> 1902 12 26 0 1902 12 27 0 1902 12 28 0 1902 12 29 0 1902 12 30 0 1902 12
> 31 0 1903 1 1 0 1903 1 2 0 1903 1 3 1.3 1903 1 4 2 1903 1 5 0 1903 1 6
> 3.6 1903 1 7 0 1903 1 8 0 1903 1 9 0 1903 1 10 0 1903 1 11 0 1903 1 12 0
> 1903 1 13 0 1903 1 14 0 1903 1 15 0 1903 1 16 0  1903 1 17 0 1903 1 18 0
> 1903 1 19 0 1903 1 20 0 1903 1 21 10.2 1903 1 22 3 1903 1 23 0 1903 1 24
> 0 1903 1 25 0 1903 1 26 0 1903 1 27 0 1903 1 28 0 1903 1 29 0 1903 1 30 0
> 1903 1 31 0  1903 2 1 0 1903 2 2 0.3 1903 2 3 0 1903 2 4 0 1903 2 5 0
> 1903 2 6 0 1903 2 7 0 1903 2 8 0 1903 2 9 0 1903 2 10 0 1903 2 11 0 1903
> 2 12 0 1903 2 13 0 1903 2 14 10.2 1903 2 15 13.5  1903 2 16 7.9 1903 2 17
> 0.5 1903 2 18 0 1903 2 19 0 1903 2 20 0 1903 2 21 0 1903 2 22 0 1903 2 23
> 0 1903 2 24 0 1903 2 25 0 1903 2 26 0 1903 2 27 0 1903 2 28 0 1903 3 1 0
> 1903 3 2 0.5 1903 3 3 0 1903 3 4 21.3 1903 3 5 2 1903 3 6 0 1903 3 7 0
> 1903 3 8 20.1 1903 3 9 21.6 1903 3 10 0 1903 3 11 0 1903 3 12 0 1903 3 13
> 0 1903 3 14 3.3 1903 3 15 0 1903 3 16 0 1903 3 17 0 1903 3 18 0 1903 3 19
> 0 1903 3 20 0 1903 3 21 0 1903 3 22 0 1903 3 23 0 1903 3 24 2.8  1903 3
> 25 0 1903 3 26 0 1903 3 27 0 1903 3 28 0 1903 3 29 0 1903 3 30 0 1903 3
> 31 0 1903 4  1 0 1903 4 2 0 1903 4 3 0 1903 4 4 6.6 1903 4 5 17.5 1903 4
> 6 3.3 1903 4 7 0 1903 4 8 0  1903 4 9 0 1903 4 10 0 1903 4 11 5.8 1903 4
> 12 0 1903 4 13 0 1903 4 14 0 1903 4 15 0 1903 4  16 0 1903 4 17 2 1903 4
> 18 1.5 1903 4 19 0 1903 4 20 0 1903 4 21 0 1903 4 22 0.3 1903 4 23 0 1903
> 4 24 2.3 1903 4 25 8.6 1903 4 26 15.2 1903 4 27 3.3 1903 4 28 0 1903 4 29
> 0 1903 4 30 2  1903 5 1 1.3 1903 5 2 0 1903 5 3 0 1903 5 4 0 1903 5 5 0
> 1903 5 6 0 1903 5 7 0 1903 5 8  11.4 1903 5 9 0 1903 5 10 0 1903 5 11 1.3
> 1903 5 12 0 1903 5 13 0 1903 5 14 0 1903 5 15 0  1903 5 16 0 1903 5 17 0
> 1903 5 18 0 1903 5 19 0.3 1903 5 20 11.7 1903 5 21 11.2 1903 5 22 3.3
> 1903 5 23 0.3 1903 5 24 0 1903 5 25 0 1903 5 26 1 1903 5 27 0 1903 5 28 0
> 1903 5 29 0 1903 5 30 2.5 1903 5 31 0 1903 6 1 0 1903 6 2 0 1903 6 3 3
> 1903 6 4 5.1 1903 6 5 4.3 1903 6 6 11.7  1903 6 7 15.7 1903 6 8 10.7 1903
> 6 9 1.5 1903 6 10 2 1903 6 11 5.8 1903 6 12 3.3 1903 6 13 1  1903 6 14
> 1.5 1903 6 15 4.6 1903 6 16 5.1 1903 6 17 0.3 1903 6 18 0 1903 6 19 1.3
> 1903 6 20 0.5  1903 6 21 0 1903 6 22 0 1903 6 23 8.9 1903 6 24 0.3 1903 6
> 25 0 1903 6 26 5.1 1903 6 27 0 1903 6 28 0 1903 6 29 0 1903 6 30 0 1903 7
> 1 0.3 1903 7 2 7.1 1903 7 3 0.5 1903 7 4 9.4 1903 7 5 0.5 1903 7 6 4.8
> 1903 7 7 0.3 1903 7 8 0 1903 7 9 4.1 1903 7 10 0.3 1903 7 11 0 1903 7 12
> 0  1903 7 13 0 1903 7 14 3.6 1903 7 15 4.6 1903 7 16 0 1903 7 17 0 1903 7
> 18 0 1903 7 19 11.2 1903 7 20 0.3 1903 7 21 0 1903 7 22 0 1903 7 23 0
> 1903 7 24 0 1903 7 25 0 1903 7 26 0 1903 7 27 0 1903 7 28 5.1 1903 7 29
> 4.8 1903 7 30 0.8 1903 7 31 1.3 1903 8 1 0.3 1903 8 2 0.5 1903 8 3 0 1903
> 8 4 0.8 1903 8 5 0.3 1903 8 6 0 1903 8 7 0 1903 8 8 0 1903 8 9 0 1903 8
> 10 0 1903 8 11 2.5 1903 8 12 1.3 1903 8 13 0 1903 8 14 0 1903 8 15 0 1903
> 8 16 2 1903 8 17 0 1903 8 18 1.5 1903 8 19 0 1903 8 20 0.8 1903 8 21 6.1
> 1903 8 22 0 1903 8 23 0.5 1903 8 24 0 1903 8 25 0 1903 8 26 0.5 1903 8 27
> 0 1903 8 28 0 1903 8 29 0 1903 8 30 0.5 1903 8 31 34.5 1903 9 1 0.3  1903
> 9 2 6.1 1903 9 3 0 1903 9 4 0 1903 9 5 6.9 1903 9 6 2.5 1903 9 7 3.6 1903
> 9 8 0.5 1903 9 9 0 1903 9 10 0 1903 9 11 0 1903 9 12 0 1903 9 13 4.1 1903
> 9 14 0 1903 9 15 0 1903 9 16 0 1903 9 17 15.7 1903 9 18 4.1 1903 9 19 0.3
> 1903 9 20 0 1903 9 21 0 1903 9 22 0 1903 9 23 0.8  1903 9 24 1.3 1903 9
> 25 10.4 1903 9 26 0 1903 9 27 0 1903 9 28 0.5 1903 9 29 9.7 1903 9 30 0
> 1903 10 1 0.3 1903 10 2 0 1903 10 3 0.3 1903 10 4 0 1903 10 5 0 1903 10 6
> 0 1903 10 7 0 1903  10 8 0 1903 10 9 0 1903 10 10 2.3 1903 10 11 0.3 1903
> 10 12 0 1903 10 13 0 1903 10 14 0 1903 10 15 4.6 1903 10 16 1 1903 10 17
> 0 1903 10 18 0 1903 10 19 0 1903 10 20 5.8 1903 10 21 0 1903 10 22 0.3
> 1903 10 23 0 1903 10 24 0.5 1903 10 25 0 1903 10 26 0 1903 10 27 0 1903
> 10 28 0 1903 10  29 0 1903 10 30 0 1903 10 31 0 1903 11 1 14.5 1903 11 2
> 0 1903 11 3 0 1903 11 4 0 1903 11 5 0 1903 11 6 0 1903 11 7 0 1903 11 8 0
> 1903 11 9 0 1903 11 10 0.8 1903 11 11 0.8 1903 11 12 0  1903 11 13 0 1903
> 11 14 0 1903 11 15 1.7 1903 11 16 2.1 1903 11 17 0 1903 11 18 0 1903 11
> 19 0  1903 11 20 0 1903 11 21 0 1903 11 22 0 1903 11 23 1 1903 11 24 2.5
> 1903 11 25 0.3 1903 11 26 9.1  1903 11 27 2.5 1903 11 28 9.9 1903 11 29
> 16.8 1903 11 30 0.8 1903 12 1 0 1903 12 2 0 1903 12 3 0  1903 12 4 0 1903
> 12 5 0 1903 12 6 0 1903 12 7 0 1903 12 8 0 1903 12 9 0 1903 12 10 0 1903
> 12 11 15.5 1903 12 12 0.5 1903 12 13 1.8 1903 12 14 15.7 1903 12 15 0
> 1903 12 16 0 1903 12 17 0 1903 12 18 0 1903 12 19 0.3 1903 12 20 2.8 1903
> 12 21 0 1903 12 22 0 1903 12 23 0 1903 12 24 0 1903 12 25 0 1903 12 26 0
> 1903 12 27 33.5 1903 12 28 0 1903 12 29 0 1903 12 30 0 1903 12 31 0 1904
> 1 1 2 1904 1 2 2 1904 1 3 33.8 1904 1 4 0.3 1904 1 5 0 1904 1 6 0 1904 1
> 7 0 1904 1 8 0  1904 1 9 0 1904 1 10 0 1904 1 11 0 1904 1 12 0 1904 1 13
> 0 1904 1 14 0 1904 1 15 0 1904 1  16 0 1904 1 17 0 1904 1 18 0 1904 1 19
> 0 1904 1 20 0 1904 1 21 2.8 1904 1 22 18 1904 1 23 1.8 1904 1 24 0 1904 1
> 25 0 1904 1 26 2 1904 1 27 4.6 1904 1 28 0 1904 1 29 0 1904 1 30 0 1904 1
> 31 0 1904 2 1 0 1904 2 2 0 1904 2 3 0 1904 2 4 0 1904 2 5 0 1904 2 6 1.8
> 1904 2 7 0  1904 2 8 0 1904 2 9 0 1904 2 10 0 1904 2 11 0 1904 2 12 0.8
> 1904 2 13 0 1904 2 14 0 1904 2 15 0 1904 2 16 0 1904 2 17 0 1904 2 18 0
> 1904 2 19 1.3 1904 2 20 0 1904 2 21 0 1904 2 22 0.5 1904 2 23 0 1904 2 24
> 0.3 1904 2 25 0 1904 2 26 8.9 1904 2 27 1.5 1904 2 28 0 1904 2 29 0  1904
> 3 1 0 1904 3 2 0 1904 3 3 0 1904 3 4 0 1904 3 5 0 1904 3 6 0 1904 3 7 0
> 1904 3 8 0 1904 3 9 0 1904 3 10 0 1904 3 11 0 1904 3 12 0 1904 3 13 0
> 1904 3 14 0 1904 3 15 0 1904 3 16 6.1 1904 3 17 0 1904 3 18 0 1904 3 19 0
> 1904 3 20 1.5 1904 3 21 0.8 1904 3 22 0 1904 3 23 0 1904 3 24 0 1904 3 25
> 0 1904 3 26 0 1904 3 27 0 1904 3 28 0 1904 3 29 0 1904 3 30 0 1904 3 31 0
> 1904 4 1 0 1904 4 2 0 1904 4 3 0 1904 4 4 0 1904 4 5 0 1904 4 6 0 1904 4
> 7 0  1904 4 8 0 1904 4 9 0 1904 4 10 0 1904 4 11 0 1904 4 12 0 1904 4 13
> 0 1904 4 14 0 1904 4  15 0 1904 4 16 0 1904 4 17 0 1904 4 18 23.6 1904 4
> 19 10.9 1904 4 20 0 1904 4 21 2.5 1904 4 22 1.8 1904 4 23 0 1904 4 24 0
> 1904 4 25 0 1904 4 26 0 1904 4 27 2 1904 4 28 1 1904 4 29 0 1904 4 30 0
> 1904 5 1 0 1904 5 2 0 1904 5 3 0 1904 5 4 0.5 1904 5 5 0 1904 5 6 7.4
> 1904 5 7  0.3 1904 5 8 0 1904 5 9 3.8 1904 5 10 0 1904 5 11 0 1904 5 12 0
> 1904 5 13 0 1904 5 14 0 1904 5 15 0 1904 5 16 3.6 1904 5 17 8.6 1904 5 18
> 0 1904 5 19 0 1904 5 20 0 1904 5 21 5.3 1904 5 22 4.3 1904 5 23 0 1904 5
> 24 0 1904 5 25 0 1904 5 26 0 1904 5 27 0 1904 5 28 0 1904 5 29 0 1904 5
> 30 18.8 1904 5 31 5.3 1904 6 1 1 1904 6 2 0.3 1904 6 3 0 1904 6 4 0 1904
> 6 5 0 1904 6 6 0 1904 6 7 1.3 1904 6 8 0 1904 6 9 0 1904 6 10 0 1904 6 11
> 1.8 1904 6 12 8.4 1904 6 13 0 1904 6 14 0.8 1904 6 15 0 1904 6 16 0.3
> 1904 6 17 15.2 1904 6 18 13.7 1904 6 19 1.8 1904 6 20 0.5 1904 6 21 0
> 1904 6 22 7.6 1904 6 23 5.1 1904 6 24 1.3 1904 6 25 0 1904 6 26 0 1904 6
> 27 0 1904 6 28 0.5 1904 6 29 13 1904 6 30 30.5 1904 7 1 8.6 1904 7 2 1.5
> 1904 7 3 1.3 1904 7 4 0  1904 7 5 0 1904 7 6 0.8 1904 7 7 1 1904 7 8 1
> 1904 7 9 0 1904 7 10 0 1904 7 11 0 1904 7 12 0 1904 7 13 11.2 1904 7 14
> 4.8 1904 7 15 7.6 1904 7 16 3.6 1904 7 17 0 1904 7 18 0.5 1904 7 19 0
> 1904 7 20 0.3 1904 7 21 0 1904 7 22 0 1904 7 23 0 1904 7 24 15.5 1904 7
> 25 4.6 1904 7 26 1.3 1904 7 27 1 1904 7 28 0.5 1904 7 29 0 1904 7 30 0
> 1904 7 31 0 1904 8 1 0 1904 8 2 0 1904 8 3 0 1904 8 4 0 1904 8 5 0 1904 8
> 6 0 1904 8 7 3.8 1904 8 8 0 1904 8 9 0 1904 8 10 0  1904 8 11 0 1904 8 12
> 0 1904 8 13 9.1 1904 8 14 0 1904 8 15 0 1904 8 16 1.3 1904 8 17 0 1904 8
> 18 0.5 1904 8 19 2.5 1904 8 20 7.1 1904 8 21 0 1904 8 22 0 1904 8 23 2.5
> 1904 8 24 2.8 1904 8 25 0 1904 8 26 0.3
> i want the counting number of rain as follows:
> 
> ??????????? Jan??????? Feb??????? Mar??? Apr??? May??? Jun??? Jul
> Aug??? Sep??? Oct??? Nov??? Dec
> 1901??????? x190219031904
> 
> x = number of rain for > 0.000001
> 
> Thanks for your help.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From sarah.goslee at gmail.com  Tue Sep  8 17:20:01 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 8 Sep 2015 11:20:01 -0400
Subject: [R] Laplace smoothing in J48
In-Reply-To: <CAA5eUmEVg4NGkSGZQcu2bU6BPiJ7+24OC4_VYTsVy=3Pv2y7dQ@mail.gmail.com>
References: <CAA5eUmEVg4NGkSGZQcu2bU6BPiJ7+24OC4_VYTsVy=3Pv2y7dQ@mail.gmail.com>
Message-ID: <CAM_vju=zrRdqJFjfiM9gHnQqAOMgpKYKD1TapwxyLUa-AXux9w@mail.gmail.com>

Without a reproducible example that includes some sample data provided
using dput() (fake is fine), the code you used, and some clear idea of
what output you expect, it's impossible to figure out how to help you.
Here are some suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

On Tue, Sep 8, 2015 at 2:10 AM, Priyanka Garg
<priyanka.garg112 at gmail.com> wrote:
> Hi,
>
> I am using J48 classifier. I want to now after i set A as TRUE in control
> option of the classifier, how could i see its effect when using predict
> method ?
>
>
>
> Regards
> Priyanka Garg
> School of Computers & Information Sciences
> University Of Hyderabad
>
>         [[alternative HTML version deleted]]


-- 
Sarah Goslee
http://www.functionaldiversity.org


From sarah.goslee at gmail.com  Tue Sep  8 17:21:37 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 8 Sep 2015 11:21:37 -0400
Subject: [R] names in R list's
In-Reply-To: <CAAjnpdiSqTiVXoz6Ksob_G+ZrB2ZAzbdNZXSWrQ_MH9QQ9AGzw@mail.gmail.com>
References: <CAAjnpdgHgYG3xUOECypMOV4aP6M=R6DgDuqmDmJJ6wm79fzjcQ@mail.gmail.com>
	<2D0B5742-2595-4837-8371-BF5895C6B6D2@dcn.davis.CA.us>
	<CAAjnpdiSqTiVXoz6Ksob_G+ZrB2ZAzbdNZXSWrQ_MH9QQ9AGzw@mail.gmail.com>
Message-ID: <CAM_vjum+qvhM-w1ECyuThspEJ1OXCch8Sk3U2E_MNrCby57M6w@mail.gmail.com>

On Tue, Sep 8, 2015 at 7:53 AM, Witold E Wolski <wewolski at gmail.com> wrote:
> Hi Jeff,
>
> Indeed there was something about plain-text in the r-help posting
> guide although I can't find it there anymore.
> https://www.r-project.org/posting-guide.html
>
> Is it still an requirement?

Yes. From that very link:

Technical details of posting: See General Instructions for more
details of the following:

No HTML posting (harder to detect spam) (note that this is the default
in some mail clients - you may have to turn it off). Note that chances
have become relatively high for ?HTMLified? e-mails to be completely
intercepted (without notice to the sender).


-- 
Sarah Goslee
http://www.functionaldiversity.org


From wdunlap at tibco.com  Tue Sep  8 18:02:35 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 8 Sep 2015 09:02:35 -0700
Subject: [R] names in R list's
In-Reply-To: <CAAjnpdiSqTiVXoz6Ksob_G+ZrB2ZAzbdNZXSWrQ_MH9QQ9AGzw@mail.gmail.com>
References: <CAAjnpdgHgYG3xUOECypMOV4aP6M=R6DgDuqmDmJJ6wm79fzjcQ@mail.gmail.com>
	<2D0B5742-2595-4837-8371-BF5895C6B6D2@dcn.davis.CA.us>
	<CAAjnpdiSqTiVXoz6Ksob_G+ZrB2ZAzbdNZXSWrQ_MH9QQ9AGzw@mail.gmail.com>
Message-ID: <CAF8bMcbceYCdp0Z6XiK8v+DLHP7wAbCM+CgN8=yzoVVMD_kcVQ@mail.gmail.com>

It is not too hard to set up some tests to show time
as a function of number of named elements for lists
and environments.  Here is one such test

test <- function (data, nToAdd, nToExtract = length(data))
{
    addTime <- {
        addedNames <- paste0("D", seq(length(data) + 1, len = nToAdd))
        system.time(for (name in addedNames) data[[name]] <- name)
    }
    extractTime <- {
        names <- sample(names(data), size = nToExtract, replace = TRUE)
        system.time(for (name in names) tmp <- data[[name]])
    }
    rbind(addTime, extractTime)[, 1:3]
}

The times for adding and extracting data is pretty linear for environments,
at least up to a size of 10^5:
  > test(new.env(), nToAdd=1e4, nToExtract=5e4)
              user.self sys.self elapsed
  addTime          1.44        0    1.44
  extractTime      9.30        0    9.30
  > test(new.env(), nToAdd=2e4, nToExtract=10e4)
              user.self sys.self elapsed
  addTime          2.87        0    2.88
  extractTime     18.53        0   18.55
  > test(new.env(), nToAdd=1e5, nToExtract=5e5)
              user.self sys.self elapsed
  addTime         14.31        0   14.32
  extractTime     91.95        0   91.96


but is noticeably quadratic for lists at 10^4 elements:
  > test(list(), nToAdd=1e4, nToExtract=5e4)
              user.self sys.self elapsed
   addTime          1.70        0    1.70
   extractTime      2.23        0    2.23
  > test(list(), nToAdd=2e4, nToExtract=10e4)
              user.self sys.self elapsed
  addTime          5.81     0.02    5.82
  extractTime      9.58     0.00    9.58
  > test(list(), nToAdd=1e5, nToExtract=5e5)
              user.self sys.self elapsed
  addTime        143.21     0.04  143.29
  extractTime    255.70     0.00  255.72

For your application you may be interested in timing replacement operations
as will.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Sep 8, 2015 at 4:53 AM, Witold E Wolski <wewolski at gmail.com> wrote:

> Hi Jeff,
>
> Indeed there was something about plain-text in the r-help posting
> guide although I can't find it there anymore.
> https://www.r-project.org/posting-guide.html
>
> Is it still an requirement?
>
> Jeff, thanks for you constructive contribution ;) . Glad that you know
> about plain text mode in e-mails, beside doing some perl programming.
> I forgot about both. Even the linux admin's I know use python and
> thunderbird or some webmail nowadays not pine and perl, but I do not
> much networking, so what do I know.
>
> I think the question I am asking is legitimate. The access complexity
> of datastructures is specified in the documentation in case of python
> datastructures,  java collections or stl containers.
> I guess this information is available for name access on R-list but I
> just can't find it.
>
>
>
>
> regards
>
> On 7 September 2015 at 16:37, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> > You puzzle me. Why does someone who cannot figure out how to post an
> email in plain text after so many messages on this mailing list get all
> worried about access time for string indexing?
> >
> > Environment objects have those properties. They do not solve all
> problems though, because they are rather heavyweight... you need a lot of
> lookups to pay for their overhead. R5 objects and the hash package both use
> them, but I have never found three need to use them. Yes, I do program in
> Perl so I know where you are coming from, but the vector-based name lookup
> used in R works quite effectively for data where the number of list items
> is short or where I plan to access every element as part of my data
> processing anyway.
> >
> ---------------------------------------------------------------------------
> > Jeff Newmiller                        The     .....       .....  Go
> Live...
> > DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
> >                                       Live:   OO#.. Dead: OO#..  Playing
> > Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> > /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> >
> ---------------------------------------------------------------------------
> > Sent from my phone. Please excuse my brevity.
> >
> > On September 7, 2015 3:34:53 AM PDT, Witold E Wolski <wewolski at gmail.com>
> wrote:
> >>What is the access time for R lists given a name of list element, is it
> >>linear, log, or constant?
> >>
> >>Than what are to rules for names in R-lists
> >>
> >>That reusing names is possible makes me wonder.
> >>
> >>tmp <- as.list(c(1,2,3,4))
> >>names(tmp) = c("a","a","b","b")
> >>tmp
> >>tmp$a
> >>
> >>
> >>What I am looking for is a standard R data structure which will allow
> >>me
> >>for fast and name lookup.
> >
>
>
>
> --
> Witold Eryk Wolski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Xochitl.Cormon at ifremer.fr  Tue Sep  8 18:40:20 2015
From: Xochitl.Cormon at ifremer.fr (Xochitl CORMON)
Date: Tue, 08 Sep 2015 18:40:20 +0200
Subject: [R] Non linear regression - Von Bertalanffy Growth Function -
 "singular gradient matrix at initial parameter estimates"
In-Reply-To: <55D4807D.4080707@gmail.com>
References: <55D3587E.7020509@ifremer.fr> <55D4807D.4080707@gmail.com>
Message-ID: <55EF0F74.7000203@ifremer.fr>

Thank you for the tip. Indeed, nlxb in nlmrt works and results are not 
crazy.

I would like however to assess goodness-of-fit (gof) and ultimately to 
compare it with gof from linear regression (fitted with same variables).

Before I used AICc to compare the nls() and lm() fit, however I get now 
an error message concerning the method loglike and its non compatibility 
with nlmrt class object. I guess it is because we use now Marquardt 
method to minimise sum-of square instead of Gauss-Newton? I am right? Or 
this is just an incompatibility coming between AICc function and nlmrt 
objects? Is there an R function to do that?

Best,

Xochitl C.


<>< <>< <>< <><

Xochitl CORMON
+33 (0)3 21 99 56 84

Doctorante en ?cologie marine et science halieutique
PhD student in marine ecology and fishery science

<>< <>< <>< <><

IFREMER
Centre Manche Mer du Nord
150 quai Gambetta
62200 Boulogne-sur-Mer

<>< <>< <>< <><



Le 19/08/2015 15:11, ProfJCNash a ?crit :
> Packages nlmrt or minpack.lm use a Marquardt method. minpack.lm won't
> proceed if the Jacobian singularity is at the starting point as far as
> I'm aware, but nlxb in nlmrt can sometimes get going. It has a policy
> that is aggressive in trying to improve the sum of squares, so will use
> more effort than nls when both work.
>
> JN
>
> On 15-08-18 12:08 PM, Xochitl CORMON wrote:
>> Dear all,
>>
>> I am trying to estimate VBGF parameters K and Linf using non linear
>> regression and nls(). First I used a classic approach where I estimate
>> both parameters together as below with "alkdyr" being a subset per year
>> of my age-length-key database and running in a loop.
>>
>> vbgf.par <- nls(Lgtcm ~  Linf *(1 - exp(-K * (Age - tzero))), start =
>> c(K= 0.07, Linf = 177.1), data=alkdyr)
>>
>> I obtain an estimation of both parameters that are strongly correlated.
>> Indeed after plotting Linf ~ K and fitting a linear regression I obtain
>> a function (Linf = a + b*K) with R2= 0.8 and a = 215, b = -763.
>>
>> In this context, to take into account explicitly correlation between
>> parameters, I decided to fit a new non linear regression derivate from
>> VBGF but where Linf is expressed depending on K (I am most interested in
>> K). To do so, I tried this model:
>> vbgf.par <- nls(Lgtcm ~  (a + (b*k)) *(1 - exp(-k * (Age - tzero))),
>> start = c(k= 0.07, a= 215, b=-763), data=alkdyr)
>>
>> Unfortunately at this point I cannot go further as I get the error
>> message "singular gradient matrix at initial parameter estimates".
>>
>> I tried to use alg= plinear (which I am not sure I understand properly
>> yet). If I give a starting value for a and b only, I have an error
>> message stating "step factor below minFactor" (even when minFactor is
>> set to 100000000000).
>>
>> Any help will be more than welcome as this is quite urgent....
>>
>> Best,
>>
>> Xochitl C.
>>
>>
>>
>>


From cdshikida at gmail.com  Tue Sep  8 18:57:17 2015
From: cdshikida at gmail.com (=?UTF-8?B?Q2xhdWRpbyBTaGlraWRhICjmlbfnlLDmsrvoqqDjgIDjgq/jg6njgqbjgrjjgqop?=)
Date: Tue, 8 Sep 2015 13:57:17 -0300
Subject: [R] mlogit data format
Message-ID: <CAHFkXVaE-BQXt=p5u+QfZ74A5Lw5mn=i4KD900H_Jze2DqgQUw@mail.gmail.com>

Ok, here is my question. I have a data frame that looks like this:

y                x1      x2 (say, age)    x3
yes             car     23                   catholic
no               bus    34                   muslim
maybe         bus    16                   jew

You see, the multinomial dependent variable is y, but I also have
multi-factors in the right side of the equation.

I want to use mlogit, but it seems to me that my database isn't like the
examples in the package's vignette. Say my dataframe is "new". I don't know
if it's just:

new1<-mlogit.data(new, choice="y", shape="long",
                  alt.levels=c("yes", "no", "maybe"))

Or should I also input levels for x1 and x3?

Any help would be appreciated.

Thanks a lot


Obrigado / Thanks for your time and attention.

Claudio D. Shikida
http://www.cdshikida.net  and http://works.bepress.com/claudio_shikida/

Esta mensagem pode conter informa??o confidencial e/ou privilegiada. Se
voc? n?o for o destinat?rio ou a pessoa autorizada a receber esta mensagem,
n?o poder? usar, copiar ou divulgar as informa??es nela contidas ou tomar
qualquer a??o baseada nessas informa??es. Se voc? recebeu esta mensagem por
engano, por favor avise imediatamente o remetente, respondendo o presente
e-mail e apague-o em seguida.
This message may contain confidential and/or privileged ...{{dropped:9}}


From ragia11 at hotmail.com  Tue Sep  8 18:58:49 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Tue, 8 Sep 2015 19:58:49 +0300
Subject: [R] groups Rank
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E800@SRVEXCHMBX.precheza.cz>
References: <DUB125-W10C5D212C9BD3CF6775318B3A00@phx.gbl>, ,
	<DUB125-W91C972100E24493CABA989B3700@phx.gbl>, ,
	<DUB125-W49BC987BD7C478DCA7220FB3700@phx.gbl>, ,
	<DUB125-W12FF665BD9DF1AD4630B40B3570@phx.gbl>, ,
	<CAM_vjunHM8iAHisTytkn-EWjq42V4oo0NqOtQCbL3RvmXLqsZA@mail.gmail.com>,
	, <DUB125-W27A4D5CE21C3CF6038C503B3560@phx.gbl>, ,
	<CAM_vju=FpeLo-fmhdk2vJ8thmxeAbcHhKXwk9yOWYDuXnsrL+A@mail.gmail.com>,
	, <DUB125-W330E69EE3D349200682C6CB3540@phx.gbl>,
	<DUB125-W800D17CA67EC7748B3503CB3540@phx.gbl>,
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E800@SRVEXCHMBX.precheza.cz>
Message-ID: <DUB125-W262D539143DD8BADCC73F2B3530@phx.gbl>


many thanks, It WORKS.

But

what If I want to add a condition that considered?Measure_id , if it is '1' ? rank reverse the probability and if it is ' 2 ' rank is ordered like probability??

Replying is highly appreciated?
Ragia
----------------------------------------
> From: petr.pikal at precheza.cz
> To: ragia11 at hotmail.com; r-help at r-project.org
> Subject: RE: [R] groups Rank
> Date: Mon, 7 Sep 2015 09:29:21 +0000
>
> Hi
>
> OK, thanks for sending dput result.
>
> I am still not sure what exactly you want. Using ?ave you can get result of x/max(x)
>
> dat$prob <- ave(dat$value, paste(dat$id, dat$i), FUN= function(x) x/max(x))
>
> however in case max(x) is zero the result is NA
>
> You can change it to zero
>
> dat$prob[is.nan(dat$prob)] <- 0
>
> and compute rank value by similar process.
>
> dat$rankvalue <- ave(dat$prob, paste(dat$id, dat$i), FUN = rank)
>
> But I am not sure if this is the desired result.
>
> Cheers
> Petr
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ragia
>> Ibrahim
>> Sent: Monday, September 07, 2015 10:12 AM
>> To: Sarah Goslee; r-help at r-project.org
>> Subject: Re: [R] groups Rank
>>
>> apology for re sending the Email, I changed the format to plain text as
>> I have been advised the data is as follow
>>
>> thanks Sarah,
>> I used pdut, and here is the data as written on R..I attached the dput
>> result structure(list(Measure_id = c(1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3,
>> 3, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3,
>> 3, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3), i = c(5, 5, 5, 5, 5, 5, 5, 5,
>> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7,
>> 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7), j = c(1, 1, 1, 1, 1,
>> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3,
>> 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5), id = c(1, 2,
>> 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
>> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
>> 11, 12), value = c(2, 1.5, 0, 0, 1, 0.5, 0, 0, 0, 0, 0.5, 2, 2, 1.5, 0,
>> 1, 2, 0, 0.5, 1.44269504088896, 0, 0, 0, 0, 1, 1.5, 0, 0, 1, 0, 0, 0,
>> 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)), .Names =
>> c("Measure_id", "i", "j", "id", "value"), row.names = c(NA, 48L), class
>> = "data.frame")
>>
>> the data is as follow :
>>
>> Measure_id i j id value
>> 1 1 5 1 1 2.0
>> 2 1 5 2 1 2.0
>> 3 1 5 1 2 1.5
>> 4 1 5 2 2 1.5
>> 5 1 5 1 3 0.0
>> 6 1 5 2 3 0.0
>> 7 1 5 1 4 0.0
>> 8 1 5 2 4 1.0
>> 9 1 5 1 5 1.0
>> 10 1 5 2 5 2.0
>> .. ... . . .. ...
>> I want to add a probability column, the prob column depends on id
>> grouped by for each i the rank will be current (value / max value ) for
>> the same id for specific i, it would be
>>
>> Measure_id i j id value prob
>> 1 1 5 1 1 2.0 2/2
>> 2 1 5 2 1 2.0 2/2
>> 3 1 5 1 2 1.5 1.5/1.5
>> 4 1 5 2 2 1.5 1.5/1.5
>> 5 1 5 1 3 0.0 0
>> 6 1 5 2 3 0.0 0
>> 7 1 5 1 4 0.0 0/1
>> 8 1 5 2 4 1.0 1/1
>> 9 1 5 1 5 1.0 1/2
>> 10 1 5 2 5 2.0 2/3
>> .. ... . . .. ...
>>
>> then I want to add a rank column that rank regarding probability, if
>> the probability equal they took the same rank for the same id belongs
>> to the same i, otherwize lower probability took higher rank for examole
>> if we have three values for i=7 and for the three values the id is 1
>> and the probability is ( .2,.4,.5) the rank should be 3,2,1
>>
>>
>>
>> I looked at aggregate and dplyr...should I use for loop and subset each
>> i and id rows do calculations and then group them again ??
>> is there easier way?
>>
>> replying highly appreciated
>>>
>>>
>>>
>>>> Date: Sun, 6 Sep 2015 19:02:02 -0400
>>>> Subject: Re: [R] groups Rank
>>>> From: sarah.goslee at gmail.com
>>>> To: ragia11 at hotmail.com
>>>> CC: r-help at r-project.org
>>>>
>>>> Please use dput() to provide data, rather than expecting people to
>>>> open random attachments. Besides, there are multiple options for
>>>> getting data into R, and we need to know exactly what you did.
>> dput()
>>>> is faster and easier.
>>>>
>>>> What have you tried? Did you look at aggregate() as I suggested?
>>>>
>>>> Sarah
>>>>
>>>> On Sat, Sep 5, 2015 at 10:44 AM, Ragia Ibrahim <ragia11 at hotmail.com>
>> wrote:
>>>>> thanks for replying, I attached the data frame for source "i" I
>> want
>>>>> to sum the values and get the max value then add a new column
>> called
>>>>> rank . That new column cell value for each source i and for
>> specific
>>>>> id would be (value/max value) * count of rows that have the same
>>>>> criteria "same i and same id"
>>>>>
>>>>> many thanks
>>>>> Ragia
>>>>>
>>>>>> Date: Fri, 4 Sep 2015 10:19:35 -0400
>>>>>> Subject: Re: [R] groups Rank
>>>>>> From: sarah.goslee at gmail.com
>>>>>> To: ragia11 at hotmail.com
>>>>>> CC: r-help at r-project.org
>>>>>>
>>>>>> Hi Ragia,
>>>>>>
>>>>>> I can't make out your data or desired result, but it sounds like
>>>>>> aggregate() might get you started. If you need more help, please
>>>>>> repost your data using dput() and do NOT post in HTML so that we
>>>>>> can see what your desired result looks like.
>>>>>>
>>>>>> Sarah
>>>>>>
>>>>>> On Fri, Sep 4, 2015 at 10:12 AM, Ragia Ibrahim
>>>>>> <ragia11 at hotmail.com>
>>>>>> wrote:
>>>>>>> Dear Group,kinldy, I have the following data frame df id value1 1
>>>>>>> 4 2 1 4 3 1 6 4 1 6 5 2 1.5 6 2 2.5 7 2 2.5 8 2 2.5
>>>>>>>
>>>>>>> add rank column regarding id coulmn where rank for the highest
>>>>>>> value would be 1, others rank would be the (value/ value of
>>>>>>> heighest)/
>>> number of
>>>>>>> rows that took the same value
>>>>>>> thus the data frame should be
>>>>>>> id value Rank1 1 4 0.332 1 4 0.333 1 6 0.54 1 6 0.55 2 1.5 0.6 6
>> 2
>>>>>>> 2.5
>>>>>>> 0.337 2 2.5 0.338 2 2.5 0.33
>>>>>>>
>>>>>>> how to reach this resultthanks in advanceRagia [[alternative HTML
>>>>>>> version deleted]]
>>>>>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
 		 	   		  

From ghada.f.mm at gmail.com  Tue Sep  8 18:21:18 2015
From: ghada.f.mm at gmail.com (Ghada Almousa)
Date: Tue, 8 Sep 2015 19:21:18 +0300
Subject: [R] Help me in cluster
Message-ID: <CADG8gku-zLsdkekcYHZYL-7N-diAYbNKiOS+hFo8GknGFuVKow@mail.gmail.com>

I have project to study and analysis clusters algorithm in R
"K-mean, Hierarchical, Density based and EM"
I want to calculate
Cluster instance , number of iteration , sum of squared error SSE and the
accuracy for each cluster algorithms that i mention above
And the log likelihood for EM and DBSCAN

I wrote code in r for
"K-mean, Hierarchical, Density based and EM
But i can't calculate
Cluster instance , number of iteration , sum of squared error SSE and the
accuracy for each cluster algorithms that i mention above
And the log likelihood for EM and DBSCAN

	[[alternative HTML version deleted]]


From schwidom at gmx.net  Tue Sep  8 19:33:16 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Tue, 8 Sep 2015 19:33:16 +0200
Subject: [R] Help with vectors!
In-Reply-To: <1441487658926-4711895.post@n4.nabble.com>
References: <1441295060538-4711801.post@n4.nabble.com>
	<1441487658926-4711895.post@n4.nabble.com>
Message-ID: <20150908173316.GA2898@debian64>

On Sat, Sep 05, 2015 at 02:14:18PM -0700, Dan D wrote:
> # your data
> VAS<-c("Green","Green","Black","Green","White","Yellow","Yellow","Black","Green","Black")
> 
> # declare the new vector
> New_Vector<-numeric(length(VAS))
> 
> # brute force:
> New_Vector[VAS=="White"]<-1
> New_Vector[VAS=="Yellow"]<-2
> New_Vector[VAS=="Green"]<-3
> New_Vector[VAS=="Black"]<-4
> 
> # a little more subtle
> cols<-c("White","Yellow","Green","Black")
> for (i in 1:length(cols))  New_Vector[VAS==cols[i]]<-i
> 
> # and a general approach (that may give a different indexing, but can be
> used for any array)
> for (i in 1:length(unique(VAS))) New_Vector[VAS==unique(VAS)[i]]<-i
> cbind(1:length(unique(VAS)),unique(VAS)) # a decoding key for the color
> index
> 

# how about:

rank( VAS, ties.method='min')

Regards


From ddalthorp at usgs.gov  Tue Sep  8 20:55:51 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Tue, 8 Sep 2015 11:55:51 -0700 (PDT)
Subject: [R] Help with vectors!
In-Reply-To: <20150908173316.GA2898@debian64>
References: <1441295060538-4711801.post@n4.nabble.com>
	<1441487658926-4711895.post@n4.nabble.com>
	<20150908173316.GA2898@debian64>
Message-ID: <1441738551536-4712023.post@n4.nabble.com>

Great!



--
View this message in context: http://r.789695.n4.nabble.com/Help-with-vectors-tp4711801p4712023.html
Sent from the R help mailing list archive at Nabble.com.


From schwidom at gmx.net  Tue Sep  8 21:33:42 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Tue, 8 Sep 2015 21:33:42 +0200
Subject: [R] Help with vectors!
In-Reply-To: <1441738551536-4712023.post@n4.nabble.com>
References: <1441295060538-4711801.post@n4.nabble.com>
	<1441487658926-4711895.post@n4.nabble.com>
	<20150908173316.GA2898@debian64>
	<1441738551536-4712023.post@n4.nabble.com>
Message-ID: <20150908193342.GB4623@debian64>

# my last one:

xtfrm( VAS)

On Tue, Sep 08, 2015 at 11:55:51AM -0700, Dan D wrote:
> Great!
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Help-with-vectors-tp4711801p4712023.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dimitri.liakhovitski at gmail.com  Tue Sep  8 22:28:58 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 8 Sep 2015 16:28:58 -0400
Subject: [R] Converting a .wav file into an mp3 file in R
Message-ID: <CAN2xGJaYeQ4OK_9519jwj=Wn=LXP2=f12-WKwTrh16wy61-mCQ@mail.gmail.com>

Hello,

I know how to read in mp3 files, e.g., using tuneR.
But is it possible to read in a .wav file - as below and then compress
it to mp3 format?

library(tuneR)
mywav <- readWave("myfile.wav")


Thanks a lot for any hints!
-- 
Dimitri Liakhovitski


From schwidom at gmx.net  Tue Sep  8 23:37:12 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Tue, 8 Sep 2015 23:37:12 +0200
Subject: [R] extracting every nth character from a string...
In-Reply-To: <55EB57CA.2090909@gmail.com>
References: <55EB57CA.2090909@gmail.com>
Message-ID: <20150908213712.GA5468@debian64>


> rawToChar( charToRaw( str)[ c( TRUE, FALSE)])
[1] "ACEG"

Regards

On Sat, Sep 05, 2015 at 04:59:54PM -0400, Evan Cooch wrote:
> Suppose I had the following string, which has length of integer multiple of
> some value n. So, say n=2, and the example string has a length of  (2x4) = 8
> characters.
> 
> str <- "ABCDEFGH"
> 
> What I'm trying to figure out is a simple, base-R coded way (which I
> heuristically call StrSubset in the following) to extract every nth
> character from the string, to generate a new string.
> 
> So
> 
> str <- "ABCDEFGH"
> 
> new_str <- StrSubset(str);
> 
> print(new_str)
> 
> which would yield
> 
> "ACEG"
> 
> 
> Best I could come up with is something like the following, where I extract
> every odd character from the string:
> 
> StrSubset <- function(string)
>       {
> paste(unlist(strsplit(string,""))[seq(1,nchar(string),2)],collapse="") }
> 
> 
> Anything more elegant come to mind? Trying to avoid regex if possible
> (harder to explain to end-users), but if that meets the 'more elegant' sniff
> test, happy to consider...
> 
> Thanks in advance...
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wdunlap at tibco.com  Tue Sep  8 23:54:03 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 8 Sep 2015 14:54:03 -0700
Subject: [R] extracting every nth character from a string...
In-Reply-To: <20150908213712.GA5468@debian64>
References: <55EB57CA.2090909@gmail.com> <20150908213712.GA5468@debian64>
Message-ID: <CAF8bMcbN+kNWGaYWznugjXtO6YDAuB8WjN30r=jDBKUpoGezyQ@mail.gmail.com>

charToRaw is not good here because it splits up multibyte characters:
 strsplit(str, "") will split str into its characters.  E.g.,

> str <- c("ggaammmmaa12:\u03b3,  OOmmeeggaa12:\u03A9...")
> rawToChar( charToRaw( str)[ c( TRUE, FALSE)])
[1] "gamma1:? Omega1:?."

> paste(collapse="", strsplit(str,split=NULL)[[1]][(1:nchar(str))%%2==1])
[1] "gamma1:, Omega2?."
> gsub("(.)(.)", "\\1", str)
[1] "gamma1:, Omega2?."





Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Sep 8, 2015 at 2:37 PM, Frank Schwidom <schwidom at gmx.net> wrote:

>
> > rawToChar( charToRaw( str)[ c( TRUE, FALSE)])
> [1] "ACEG"
>
> Regards
>
> On Sat, Sep 05, 2015 at 04:59:54PM -0400, Evan Cooch wrote:
> > Suppose I had the following string, which has length of integer multiple
> of
> > some value n. So, say n=2, and the example string has a length of  (2x4)
> = 8
> > characters.
> >
> > str <- "ABCDEFGH"
> >
> > What I'm trying to figure out is a simple, base-R coded way (which I
> > heuristically call StrSubset in the following) to extract every nth
> > character from the string, to generate a new string.
> >
> > So
> >
> > str <- "ABCDEFGH"
> >
> > new_str <- StrSubset(str);
> >
> > print(new_str)
> >
> > which would yield
> >
> > "ACEG"
> >
> >
> > Best I could come up with is something like the following, where I
> extract
> > every odd character from the string:
> >
> > StrSubset <- function(string)
> >       {
> > paste(unlist(strsplit(string,""))[seq(1,nchar(string),2)],collapse="") }
> >
> >
> > Anything more elegant come to mind? Trying to avoid regex if possible
> > (harder to explain to end-users), but if that meets the 'more elegant'
> sniff
> > test, happy to consider...
> >
> > Thanks in advance...
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From anguillaphile at outlook.com  Wed Sep  9 02:11:06 2015
From: anguillaphile at outlook.com (B Jessop)
Date: Tue, 8 Sep 2015 21:11:06 -0300
Subject: [R] Extracting case values from ancova adjusted means
Message-ID: <BLU184-W38E6B6EF2F3866697E98CAB1520@phx.gbl>







Dear Help List,
Given a standard one-way ANCOVA model for comparing k groups and with one covariate factor e.g. ANC <- lm(Y ~ X + Group) run within package "car" followed by the estimation of adjusted group means with the "effects" package e.g. Effects1 <- Effect("Group", ANC, se = TRUE) how can I extract the individual case values adjusted to the covariate grand mean that theoretically go into creating each adjusted group mean and its confidence interval?  I recognize that summary computational formulae are probably used to estimate adjusted group means and their SE and the package "effects" may not even calculate individual case adjusted values.  Use of the "fitted" command applied to the ANC model  e.g. Fitted1 <- fitted(ANC) produces case values that do not average to the adjusted mean for each group output by "Effects" while applying "fitted" to the Effects output results in NULL.  I am unsure why the first result occurs and the second outcome may result from having applied a command inappropriately.   A function may need to be created to estimate case adjusted values but that is, at this time, beyond me.  I have read the "effects" package pdf but found nothing to enlighten me on this issue.   I have estimated adjusted case values in Excel using Yij(adj) = yij - b(xij - xbar) where b is the slope of the pooled Group regression and xbar is the grand mean X but the mean of the adjusted case values for a group do not always match the adjusted group means from the Effects or even the fitted command output as closely as I think they should (to at least 3 decimals given the data are log transformed) even after allowing for decimal rounding because I used only 5 places in Excel.  
Thanks for any assistance in how to extract the case values used to create the adjusted group means from an ANCOVA by the "effects" package such that the mean of the case values for a group equals the adjusted mean for that group.
 
Regards,
B. Jessop
    


 		 	   		  
	[[alternative HTML version deleted]]


From charlotte.hurry at griffithuni.edu.au  Wed Sep  9 02:22:41 2015
From: charlotte.hurry at griffithuni.edu.au (Charlotte)
Date: Tue, 8 Sep 2015 17:22:41 -0700 (PDT)
Subject: [R] R wont accept my zero count values in the GLM with
 quasi_poisson dsitribution
In-Reply-To: <55B78255.10806@yorku.ca>
References: <1438065205709-4710462.post@n4.nabble.com>
	<55B729C2.7050904@umu.se> <55B78255.10806@yorku.ca>
Message-ID: <1441758161109-4712029.post@n4.nabble.com>

Hello

I am going to look at the ZIP (zero-inflated poisson).  I just need to ask. 
One of my variables is continuous data (hosts body length), is it ok to
include this kind of data into this analysis or do I need to turn it into
categories (29-50 mm, 51-100mm and so on)?

Many Thanks
Charlotte



--
View this message in context: http://r.789695.n4.nabble.com/R-wont-accept-my-zero-count-values-in-the-GLM-with-quasi-poisson-dsitribution-tp4710462p4712029.html
Sent from the R help mailing list archive at Nabble.com.


From liuwensui at gmail.com  Wed Sep  9 04:37:25 2015
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 8 Sep 2015 21:37:25 -0500
Subject: [R] R wont accept my zero count values in the GLM with
 quasi_poisson dsitribution
In-Reply-To: <1438065205709-4710462.post@n4.nabble.com>
References: <1438065205709-4710462.post@n4.nabble.com>
Message-ID: <CAKyN3iCpMR_D2Obvq0bBoigBtTd4okqP1k5rj-=66FJKQ4=fvA@mail.gmail.com>

based on your code "fit <-
glm(abundance~Gender,data=teminfest,family=binomial())", i don't see
anything related to quasi_poisson. are you sure what you are doing
here?

On Tue, Jul 28, 2015 at 1:33 AM, Charlotte
<charlotte.hurry at griffithuni.edu.au> wrote:
> Hello
>
> I have count values for abundance which follow a pattern of over-dispersal
> with many zero values.  I have read a number of documents which suggest that
> I don't use data transforming methods but rather than I run the GLM with the
> quasi poisson distribution.  So I have written my script and R is telling me
> that Y should be more than 0.
>
> Everything I read tells me to do it this way but I can't get R to agree.
> Did I need to add something else to my script to get it to work and keep my
> data untransformed? The script I wrote is as follows:
>
>> fit <- glm(abundance~Gender,data=teminfest,family=binomial())
>
> then I get this error
> Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1
>
> I don't use R a lot so I am having trouble figuring out what to do next.
>
> I would appreciate some help
>
> Many Thanks
> Charlotte
>
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/R-wont-accept-my-zero-count-values-in-the-GLM-with-quasi-poisson-dsitribution-tp4710462.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
WenSui Liu
https://statcompute.wordpress.com/


From pnsinha68 at gmail.com  Wed Sep  9 07:32:15 2015
From: pnsinha68 at gmail.com (Partha Sinha)
Date: Wed, 9 Sep 2015 11:02:15 +0530
Subject: [R] Task Views
Message-ID: <CADcgpJdrA6H5oLaQ-20tUeLxfRUZq6PDvPn0JM=PqR3NWQH0Qg@mail.gmail.com>

Dear All
I am using R version R version 3.2.1 (2015-06-18) in windows 7.
I have installed few task views like Timeseries and Graphics in R.

1. Is it possible to find out which "TASK Views" are installed in the system ?
2. Can in install multiple Task views using single command ?

Regards
Partha


From petr.pikal at precheza.cz  Wed Sep  9 07:59:48 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 9 Sep 2015 05:59:48 +0000
Subject: [R] help with reshape
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE85B@mb02.ads.tamu.edu>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE76A@mb02.ads.tamu.edu>
	<55EEF1A2.4000206@univ-reims.fr>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE85B@mb02.ads.tamu.edu>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EE06@SRVEXCHMBX.precheza.cz>

Hi David

But the final problem was that Ivan's code with original data (with NA) provided different result on his Mac  with R3.2.1 version and on my PC with R3.3.0 devel version.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David L
> Carlson
> Sent: Tuesday, September 08, 2015 5:03 PM
> To: Ivan Calandra; R list
> Subject: Re: [R] help with reshape
>
> I don't think it is platform or version specific. The purpose of the
> missing value, NA (i.e. Not Available), is to flag the value for
> special handling in some way, often by deletion. You cannot assume that
> NA will be treated as any other value since that would defeat the whole
> purpose of flagging the value as missing. Similar results occur if you
> try to create tables or cross-tabulations of variables that include NAs
> with table() and xtabs().
>
> David
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
> Calandra
> Sent: Tuesday, September 8, 2015 9:33 AM
> To: R list
> Subject: Re: [R] help with reshape
>
> David,
>
> It seems that your solution works, but why would that be? And why would
> this NA behavior be platform or version specific?
>
> I really need to check with a newer version of R...
>
> Ivan
>
> --
> Ivan Calandra, PhD
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
>
> Le 08/09/15 16:23, David L Carlson a ?crit :
> > I have not followed this thread closely, but this seems to work:
> >
> > mydata$repet[is.na(mydata$repet)] <- 0 reshape(mydata,
> timevar="Elem",
> > idvar=c("Etape","Ech", "repet", "dilution","Rincage"),
> >       direction="wide", drop=c("ID","Nom_ech"))
> >
> > If this is the expected outcome, the problem is the NA values in
> repet. I changed them to 0 since you did not have any 0 entries in the
> data (otherwise you could use 999 or some other value that does not
> occur in the data). Change them back after running reshape().
> >
> > -------------------------------------
> > David L Carlson
> > Department of Anthropology
> > Texas A&M University
> > College Station, TX 77840-4352
> >
> >
> > ----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
> > Calandra
> > Sent: Tuesday, September 8, 2015 7:56 AM
> > To: PIKAL Petr; R list
> > Subject: Re: [R] help with reshape
> >
> > Thanks Petr,
> > It looks good, but I have to check in more details.
> >
> > Can anyone help me with my original solution using reshape()? I'd
> like
> > to understand what I did wrong.
> > reshape(mydata, timevar="Elem",
> > idvar=c("Etape","Ech","repet","dilution","Rincage"),
> direction="wide",
> > drop=c("ID","Nom_ech"))
> >
> > Thank you in advance
> > Ivan
> >
> > --
> > Ivan Calandra, PhD
> > University of Reims Champagne-Ardenne
> > GEGENAA - EA 3795
> > CREA - 2 esplanade Roland Garros
> > 51100 Reims, France
> > +33(0)3 26 77 36 89
> > ivan.calandra at univ-reims.fr
> > https://www.researchgate.net/profile/Ivan_Calandra
> >
> > Le 08/09/15 14:45, PIKAL Petr a ?crit :
> >> Hi
> >>
> >> I looked into docs to reshape2 and played around a bit and by some
> >> magical feature
> >>
> >> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
> >>
> >> probably works as you expect.
> >>
> >> I cannot comment your solution as I use reshape only sparsely.
> >>
> >> Cheers
> >> Petr
> >>
> >>> -----Original Message-----
> >>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Ivan
> >>> Calandra
> >>> Sent: Tuesday, September 08, 2015 2:18 PM
> >>> To: R list
> >>> Subject: Re: [R] help with reshape
> >>>
> >>> Thank you Petr,
> >>>
> >>> It kinda works, but not completely. The problem is that it produces
> >>> a column for each value ("Moyenne"), and not each element of
> "Elem".
> >>> That means I have only one value per column, instead of up to 3.
> >>> For example, I have 3 columns for Al1670 instead of just one, and
> >>> each column contains maximum one value (the others being NA).
> >>>
> >>> Not sure I am being clear...
> >>>
> >>> By the way, I don't understand why my solution did not work; what
> is
> >>> wrong there?
> >>>
> >>> Thank you again!
> >>> Ivan
> >>>
> >>> --
> >>> Ivan Calandra, PhD
> >>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
> >>> esplanade Roland Garros 51100 Reims, France
> >>> +33(0)3 26 77 36 89
> >>> ivan.calandra at univ-reims.fr
> >>> https://www.researchgate.net/profile/Ivan_Calandra
> >>>
> >>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
> >>>> Hi
> >>>>
> >>>> I am not sure if I got it
> >>>>
> >>>> library(reshape2)
> >>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
> >>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
> >>>>
> >>>> gives me 3 rows but names need some tweaking afterwards.
> >>>>
> >>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
> >>> "_"),
> >>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
> >>>>
> >>>> Cheers
> >>>> Petr
> >>>>
> >>>>
> >>>>> -----Original Message-----
> >>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> >>>>> Ivan Calandra
> >>>>> Sent: Tuesday, September 08, 2015 12:33 PM
> >>>>> To: R list
> >>>>> Subject: [R] help with reshape
> >>>>>
> >>>>> Dear users,
> >>>>>
> >>>>> I'm having troubles with reshaping a data.frame from long to wide
> >>>>> format.
> >>>>> I copy the output of dput() at the end of the mail because it is
> >>>>> quite long.
> >>>>>
> >>>>> Each row of the column "Elem" should be transposed to a new
> column.
> >>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage"
> >>>>> define the samples. Meaning that for each unique combination of
> >>>>> these variables, I want a single row, and as many columns as
> >>>>> elements in
> >>> "Elem".
> >>>>> So I tried:
> >>>>> reshape(mydata, timevar="Elem",
> >>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
> >>>>> direction="wide",
> >>>>> drop=c("ID","Nom_ech"))
> >>>>>
> >>>>> The problem is that some columns are not used at all for defining
> >>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is
> missing,
> >>>>> but I don't understand why.
> >>>>>
> >>>>> Can you help me with that? I have no idea what I am doing
> wrong...
> >>>>>
> >>>>> Thanks in advance,
> >>>>> Ivan
> >>>>>
> >>>>>
> >>>>>
> >>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548,
> 549,
> >>>>> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
> >>>>> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
> >>> 11583,
> >>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
> >>> 696,
> >>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1
> B2",
> >>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> "Step1
> >>>>> B2",
> >>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> "Step1
> >>>>> B2",
> >>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
> "Step1
> >>>>> B2",
> >>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> >>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
> >>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3
> >>>>> (1) *100",
> >>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
> >>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
> >>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> "S1",
> >>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
> >>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2",
> >>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
> >>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3",
> >>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
> >>>>> "F10-3", "F10-
> >>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
> >>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> >>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> >>>>> NA, 1,
> >>> 1,
> >>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
> >>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> >>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100,
> 100,
> >>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100),
> >>>>> Rincage = c("non", "non", "non", "non", "non", "non", "non",
> >>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
> >>>>> "non", "non",
> >>> "non",
> >>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
> >>> "oui",
> >>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
> >>> "non",
> >>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
> >>> "non",
> >>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840",
> "Ca3158",
> >>>>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
> >>> "Cu3247",
> >>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
> >>> "Mn2605",
> >>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
> >>> "Cd2288",
> >>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
> >>> "As1937",
> >>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
> >>> "Cr2835",
> >>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
> >>> "Mg2852",
> >>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
> >>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
> >>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69,
> 0.26875,
> >>>>> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
> >>>>> 337.433333333333, 323.533333333333, 0.877166666666667,
> >>>>> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667,
> NA,
> >>>>> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
> >>> NA,
> >>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333,
> >>>>> 128.9, 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names =
> >>>>> c("ID", "Nom_ech", "Etape", "Ech", "repet", "dilution",
> "Rincage",
> >>>>> "Elem", "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
> >>>>> 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L,
> >>>>> 30L, 31L,
> >>> 32L,
> >>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
> >>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
> >>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
> >>>>>
> >>>>> --
> >>>>>
> >>>>> Ivan Calandra, PhD
> >>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
> >>>>> esplanade Roland Garros 51100 Reims, France
> >>>>> +33(0)3 26 77 36 89
> >>>>> ivan.calandra at univ-reims.fr
> >>>>> https://www.researchgate.net/profile/Ivan_Calandra
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-
> project.org/posting-
> >>>>> guide.html and provide commented, minimal, self-contained,
> >>>>> reproducible code.
> >>>> ________________________________
> >> ________________________________
> >> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a
> jsou ur?eny pouze jeho adres?t?m.
> >> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho
> kopie vyma?te ze sv?ho syst?mu.
> >> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> >> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
> modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> >>
> >> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> >> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> >> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky
> ze strany p??jemce s dodatkem ?i odchylkou.
> >> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> >> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
> zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
> adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
> p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen?
> zn?m?.
> >>
> >> This e-mail and any documents attached to it may be confidential and
> are intended only for its intended recipients.
> >> If you received this e-mail by mistake, please immediately inform
> its sender. Delete the contents of this e-mail with all attachments and
> its copies from your system.
> >> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any
> manner.
> >> The sender of this e-mail shall not be liable for any possible
> damage caused by modifications of the e-mail or by delay with transfer
> of the email.
> >>
> >> In case that this e-mail forms part of business dealings:
> >> - the sender reserves the right to end negotiations about entering
> into a contract in any time, for any reason, and without stating any
> reasoning.
> >> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer)
> excludes any acceptance of the offer on the part of the recipient
> containing any amendment or variation.
> >> - the sender insists on that the respective contract is concluded
> only upon an express mutual agreement on all its aspects.
> >> - the sender of this e-mail informs that he/she is not authorized to
> enter into any contracts on behalf of the company except for cases in
> which he/she is expressly authorized to do so in writing, and such
> authorization or power of attorney is submitted to the recipient or the
> person represented by the recipient, or the existence of such
> authorization is known to the recipient of the person represented by
> the recipient.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ivan.calandra at univ-reims.fr  Wed Sep  9 10:02:29 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Wed, 9 Sep 2015 10:02:29 +0200
Subject: [R] help with reshape
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EE06@SRVEXCHMBX.precheza.cz>
References: <55EEB96A.8020504@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EC2E@SRVEXCHMBX.precheza.cz>
	<55EED1E5.1080703@univ-reims.fr>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3ECB4@SRVEXCHMBX.precheza.cz>
	<55EEDAC6.9020202@univ-reims.fr>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE76A@mb02.ads.tamu.edu>
	<55EEF1A2.4000206@univ-reims.fr>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6BE85B@mb02.ads.tamu.edu>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3EE06@SRVEXCHMBX.precheza.cz>
Message-ID: <55EFE795.2090404@univ-reims.fr>

Hi David and Petr,

Just a small update: I have installed R3.2.2 for Snow Leopard (from 
http://r.research.att.com/) and I still get the same problem. There is 
no available version of 3.3.0 for Snow Leopard so I will have to wait to 
check that.

Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 09/09/15 07:59, PIKAL Petr a ?crit :
> Hi David
>
> But the final problem was that Ivan's code with original data (with NA) provided different result on his Mac  with R3.2.1 version and on my PC with R3.3.0 devel version.
>
> Cheers
> Petr
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David L
>> Carlson
>> Sent: Tuesday, September 08, 2015 5:03 PM
>> To: Ivan Calandra; R list
>> Subject: Re: [R] help with reshape
>>
>> I don't think it is platform or version specific. The purpose of the
>> missing value, NA (i.e. Not Available), is to flag the value for
>> special handling in some way, often by deletion. You cannot assume that
>> NA will be treated as any other value since that would defeat the whole
>> purpose of flagging the value as missing. Similar results occur if you
>> try to create tables or cross-tabulations of variables that include NAs
>> with table() and xtabs().
>>
>> David
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>> Calandra
>> Sent: Tuesday, September 8, 2015 9:33 AM
>> To: R list
>> Subject: Re: [R] help with reshape
>>
>> David,
>>
>> It seems that your solution works, but why would that be? And why would
>> this NA behavior be platform or version specific?
>>
>> I really need to check with a newer version of R...
>>
>> Ivan
>>
>> --
>> Ivan Calandra, PhD
>> University of Reims Champagne-Ardenne
>> GEGENAA - EA 3795
>> CREA - 2 esplanade Roland Garros
>> 51100 Reims, France
>> +33(0)3 26 77 36 89
>> ivan.calandra at univ-reims.fr
>> https://www.researchgate.net/profile/Ivan_Calandra
>>
>> Le 08/09/15 16:23, David L Carlson a ?crit :
>>> I have not followed this thread closely, but this seems to work:
>>>
>>> mydata$repet[is.na(mydata$repet)] <- 0 reshape(mydata,
>> timevar="Elem",
>>> idvar=c("Etape","Ech", "repet", "dilution","Rincage"),
>>>        direction="wide", drop=c("ID","Nom_ech"))
>>>
>>> If this is the expected outcome, the problem is the NA values in
>> repet. I changed them to 0 since you did not have any 0 entries in the
>> data (otherwise you could use 999 or some other value that does not
>> occur in the data). Change them back after running reshape().
>>> -------------------------------------
>>> David L Carlson
>>> Department of Anthropology
>>> Texas A&M University
>>> College Station, TX 77840-4352
>>>
>>>
>>> ----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ivan
>>> Calandra
>>> Sent: Tuesday, September 8, 2015 7:56 AM
>>> To: PIKAL Petr; R list
>>> Subject: Re: [R] help with reshape
>>>
>>> Thanks Petr,
>>> It looks good, but I have to check in more details.
>>>
>>> Can anyone help me with my original solution using reshape()? I'd
>> like
>>> to understand what I did wrong.
>>> reshape(mydata, timevar="Elem",
>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>> direction="wide",
>>> drop=c("ID","Nom_ech"))
>>>
>>> Thank you in advance
>>> Ivan
>>>
>>> --
>>> Ivan Calandra, PhD
>>> University of Reims Champagne-Ardenne
>>> GEGENAA - EA 3795
>>> CREA - 2 esplanade Roland Garros
>>> 51100 Reims, France
>>> +33(0)3 26 77 36 89
>>> ivan.calandra at univ-reims.fr
>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>
>>> Le 08/09/15 14:45, PIKAL Petr a ?crit :
>>>> Hi
>>>>
>>>> I looked into docs to reshape2 and played around a bit and by some
>>>> magical feature
>>>>
>>>> test <- dcast(mm, Etape+Ech+repet+dilution+Rincage~Elem)
>>>>
>>>> probably works as you expect.
>>>>
>>>> I cannot comment your solution as I use reshape only sparsely.
>>>>
>>>> Cheers
>>>> Petr
>>>>
>>>>> -----Original Message-----
>>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>> Ivan
>>>>> Calandra
>>>>> Sent: Tuesday, September 08, 2015 2:18 PM
>>>>> To: R list
>>>>> Subject: Re: [R] help with reshape
>>>>>
>>>>> Thank you Petr,
>>>>>
>>>>> It kinda works, but not completely. The problem is that it produces
>>>>> a column for each value ("Moyenne"), and not each element of
>> "Elem".
>>>>> That means I have only one value per column, instead of up to 3.
>>>>> For example, I have 3 columns for Al1670 instead of just one, and
>>>>> each column contains maximum one value (the others being NA).
>>>>>
>>>>> Not sure I am being clear...
>>>>>
>>>>> By the way, I don't understand why my solution did not work; what
>> is
>>>>> wrong there?
>>>>>
>>>>> Thank you again!
>>>>> Ivan
>>>>>
>>>>> --
>>>>> Ivan Calandra, PhD
>>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>>> esplanade Roland Garros 51100 Reims, France
>>>>> +33(0)3 26 77 36 89
>>>>> ivan.calandra at univ-reims.fr
>>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>>
>>>>> Le 08/09/15 14:04, PIKAL Petr a ?crit :
>>>>>> Hi
>>>>>>
>>>>>> I am not sure if I got it
>>>>>>
>>>>>> library(reshape2)
>>>>>> mm<-melt(mydata, measure.vars="Moyenne") test <- dcast(mm,
>>>>>> Etape+Ech+repet+dilution+Rincage~Elem+value)
>>>>>>
>>>>>> gives me 3 rows but names need some tweaking afterwards.
>>>>>>
>>>>>> nn<-paste("Moyenne", unlist(lapply(strsplit(names(test)[-(1:5)],
>>>>> "_"),
>>>>>> "[", 1)), sep=".") names(test)[-(1:5)]<-nn
>>>>>>
>>>>>> Cheers
>>>>>> Petr
>>>>>>
>>>>>>
>>>>>>> -----Original Message-----
>>>>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>>>>>> Ivan Calandra
>>>>>>> Sent: Tuesday, September 08, 2015 12:33 PM
>>>>>>> To: R list
>>>>>>> Subject: [R] help with reshape
>>>>>>>
>>>>>>> Dear users,
>>>>>>>
>>>>>>> I'm having troubles with reshaping a data.frame from long to wide
>>>>>>> format.
>>>>>>> I copy the output of dput() at the end of the mail because it is
>>>>>>> quite long.
>>>>>>>
>>>>>>> Each row of the column "Elem" should be transposed to a new
>> column.
>>>>>>> All variables "Etape", "Ech", "repet", "dilution", "Rincage"
>>>>>>> define the samples. Meaning that for each unique combination of
>>>>>>> these variables, I want a single row, and as many columns as
>>>>>>> elements in
>>>>> "Elem".
>>>>>>> So I tried:
>>>>>>> reshape(mydata, timevar="Elem",
>>>>>>> idvar=c("Etape","Ech","repet","dilution","Rincage"),
>>>>>>> direction="wide",
>>>>>>> drop=c("ID","Nom_ech"))
>>>>>>>
>>>>>>> The problem is that some columns are not used at all for defining
>>>>>>> samples. In this case, the row "S1, F10-3, NA, 1, oui" is
>> missing,
>>>>>>> but I don't understand why.
>>>>>>>
>>>>>>> Can you help me with that? I have no idea what I am doing
>> wrong...
>>>>>>> Thanks in advance,
>>>>>>> Ivan
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> mydata <- structure(list(ID = c(543, 544, 545, 546, 547, 548,
>> 549,
>>>>>>> 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,
>>>>>>> 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582,
>>>>> 11583,
>>>>>>> 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,
>>>>> 696,
>>>>>>> 697, 698, 699, 700, 701, 702), Nom_ech = c("Step1 B2", "Step1
>> B2",
>>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>> "Step1
>>>>>>> B2",
>>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>> "Step1
>>>>>>> B2",
>>>>>>> "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2", "Step1 B2",
>> "Step1
>>>>>>> B2",
>>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3", "R1 S1 F1 0-3",
>>>>>>> "R1 S1 F1 0-3", "R1 S1 F1 0-3", "S1 F1 0-3 (1) *100", "S1 F1 0-3
>>>>>>> (1) *100",
>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100",
>>>>>>> "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100", "S1 F1 0-3 (1) *100"
>>>>>>> ), Etape = c("S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>> "S1",
>>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1",
>>>>>>> "S1", "S1", "S1", "S1", "S1", "S1", "S1", "S1"), Ech = c("B2",
>>>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2",
>>>>>>> "B2", "B2", "B2", "B2", "B2", "B2", "B2", "B2", "F10-3", "F10-3",
>>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>>> "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3",
>>>>>>> "F10-3", "F10-
>>>>> 3", "F10-3", "F10-3", "F10-3", "F10-3", "F10-3"
>>>>>>> ), repet = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>>> NA, 1,
>>>>> 1,
>>>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), dilution =
>>>>>>> c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>>>>>>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 100, 100, 100, 100, 100, 100, 100,
>> 100,
>>>>>>> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100),
>>>>>>> Rincage = c("non", "non", "non", "non", "non", "non", "non",
>>>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>>>> "non", "non",
>>>>> "non",
>>>>>>> "non", "oui", "oui", "oui", "oui", "oui", "oui", "oui", "oui",
>>>>> "oui",
>>>>>>> "oui", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>> "non",
>>>>>>> "non", "non", "non", "non", "non", "non", "non", "non", "non",
>>>>> "non",
>>>>>>> "non"), Elem = c("Al1670", "As1890", "As1937", "Ca1840",
>> "Ca3158",
>>>>>>> "Cd2288", "Co2286", "Co2378", "Cr2055", "Cr2835", "Cu2247",
>>>>> "Cu3247",
>>>>>>> "Fe2382", "Fe2599", "K_7664", "Mg2795", "Mg2852", "Mn2576",
>>>>> "Mn2605",
>>>>>>> "Ni2216", "Al1670", "As1890", "As1937", "Ca1840", "Ca3158",
>>>>> "Cd2288",
>>>>>>> "Co2286", "Co2378", "Cr2055", "Cr2835", "Al1670", "As1890",
>>>>> "As1937",
>>>>>>> "Ca1840", "Ca3158", "Cd2288", "Co2286", "Co2378", "Cr2055",
>>>>> "Cr2835",
>>>>>>> "Cu2247", "Cu3247", "Fe2382", "Fe2599", "K_7664", "Mg2795",
>>>>> "Mg2852",
>>>>>>> "Mn2576", "Mn2605", "Ni2216"), Moyenne = c(NA, NA, 3.7455,
>>>>>>> 596.166666666667, 578.2, 0.1514, NA, 1.87225, NA, 0.3664, 1.859,
>>>>>>> 1.96766666666667, NA, NA, 9.29566666666667, 13.08, 12.69,
>> 0.26875,
>>>>>>> 0.2877, 0.2395, 5732.33333333333, 3.9615, 3.48833333333333,
>>>>>>> 337.433333333333, 323.533333333333, 0.877166666666667,
>>>>>>> 0.292466666666667, NA, 1.79566666666667, NA, 106.326666666667,
>> NA,
>>>>>>> 2.2755, 291.933333333333, 278.833333333333, 0.819, NA, 3.946, NA,
>>>>> NA,
>>>>>>> 1.47766666666667, 1.63266666666667, 40.44, 40.2533333333333,
>>>>>>> 128.9, 50.11, 49.02, 37.4733333333333, 37.775, 0.3764)), .Names =
>>>>>>> c("ID", "Nom_ech", "Etape", "Ech", "repet", "dilution",
>> "Rincage",
>>>>>>> "Elem", "Moyenne"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
>>>>>>> 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 29L,
>>>>>>> 30L, 31L,
>>>>> 32L,
>>>>>>> 33L, 34L, 35L, 36L, 37L, 38L, 112L, 113L, 114L, 115L, 116L, 117L,
>>>>>>> 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
>>>>>>> 129L, 130L, 131L ), class = c("tbl_df", "tbl", "data.frame"))
>>>>>>>
>>>>>>> --
>>>>>>>
>>>>>>> Ivan Calandra, PhD
>>>>>>> University of Reims Champagne-Ardenne GEGENAA - EA 3795 CREA - 2
>>>>>>> esplanade Roland Garros 51100 Reims, France
>>>>>>> +33(0)3 26 77 36 89
>>>>>>> ivan.calandra at univ-reims.fr
>>>>>>> https://www.researchgate.net/profile/Ivan_Calandra
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-
>> project.org/posting-
>>>>>>> guide.html and provide commented, minimal, self-contained,
>>>>>>> reproducible code.
>>>>>> ________________________________
>>>> ________________________________
>>>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a
>> jsou ur?eny pouze jeho adres?t?m.
>>>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho
>> kopie vyma?te ze sv?ho syst?mu.
>>>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>>>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
>> modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>>>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>>>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>>>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky
>> ze strany p??jemce s dodatkem ?i odchylkou.
>>>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>>>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
>> zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
>> adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
>> p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen?
>> zn?m?.
>>>> This e-mail and any documents attached to it may be confidential and
>> are intended only for its intended recipients.
>>>> If you received this e-mail by mistake, please immediately inform
>> its sender. Delete the contents of this e-mail with all attachments and
>> its copies from your system.
>>>> If you are not the intended recipient of this e-mail, you are not
>> authorized to use, disseminate, copy or disclose this e-mail in any
>> manner.
>>>> The sender of this e-mail shall not be liable for any possible
>> damage caused by modifications of the e-mail or by delay with transfer
>> of the email.
>>>> In case that this e-mail forms part of business dealings:
>>>> - the sender reserves the right to end negotiations about entering
>> into a contract in any time, for any reason, and without stating any
>> reasoning.
>>>> - if the e-mail contains an offer, the recipient is entitled to
>> immediately accept such offer; The sender of this e-mail (offer)
>> excludes any acceptance of the offer on the part of the recipient
>> containing any amendment or variation.
>>>> - the sender insists on that the respective contract is concluded
>> only upon an express mutual agreement on all its aspects.
>>>> - the sender of this e-mail informs that he/she is not authorized to
>> enter into any contracts on behalf of the company except for cases in
>> which he/she is expressly authorized to do so in writing, and such
>> authorization or power of attorney is submitted to the recipient or the
>> person represented by the recipient, or the existence of such
>> authorization is known to the recipient of the person represented by
>> the recipient.
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From Achim.Zeileis at uibk.ac.at  Wed Sep  9 10:43:49 2015
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Wed, 9 Sep 2015 10:43:49 +0200 (CEST)
Subject: [R] Task Views
In-Reply-To: <CADcgpJdrA6H5oLaQ-20tUeLxfRUZq6PDvPn0JM=PqR3NWQH0Qg@mail.gmail.com>
References: <CADcgpJdrA6H5oLaQ-20tUeLxfRUZq6PDvPn0JM=PqR3NWQH0Qg@mail.gmail.com>
Message-ID: <alpine.DEB.2.11.1509091041280.10252@paninaro.uibk.ac.at>

On Wed, 9 Sep 2015, Partha Sinha wrote:

> Dear All
> I am using R version R version 3.2.1 (2015-06-18) in windows 7.
> I have installed few task views like Timeseries and Graphics in R.
>
> 1. Is it possible to find out which "TASK Views" are installed in the 
> system ?

Not out of the box. The task view installation just triggers a normal 
installation of packages - once these are installed there is no way to 
know from which task views the packages came.

But you can easily run update.views("TimeSeries") which will check first 
whether all packages from the task view are already installed. And then it 
only installs those that are not installed in a current version.

> 2. Can in install multiple Task views using single command ?

Yes, just do install.views(c("TimeSeries", "Graphics")) or analogously for 
update.views().

> Regards
> Partha
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon at gmail.com  Wed Sep  9 12:16:26 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 9 Sep 2015 20:16:26 +1000
Subject: [R] Converting a .wav file into an mp3 file in R
In-Reply-To: <CAN2xGJaYeQ4OK_9519jwj=Wn=LXP2=f12-WKwTrh16wy61-mCQ@mail.gmail.com>
References: <CAN2xGJaYeQ4OK_9519jwj=Wn=LXP2=f12-WKwTrh16wy61-mCQ@mail.gmail.com>
Message-ID: <CA+8X3fVH5HD2UA0qQZ4aYK31iH+p4unPuEQijmL1Gqhv5YCUGA@mail.gmail.com>

Hi Dimitri,
Have a look at SoX:

http://sox.sourceforge.net/

Jim


On Wed, Sep 9, 2015 at 6:28 AM, Dimitri Liakhovitski <
dimitri.liakhovitski at gmail.com> wrote:

> Hello,
>
> I know how to read in mp3 files, e.g., using tuneR.
> But is it possible to read in a .wav file - as below and then compress
> it to mp3 format?
>
> library(tuneR)
> mywav <- readWave("myfile.wav")
>
>
> Thanks a lot for any hints!
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From E.Vettorazzi at uke.de  Wed Sep  9 13:31:23 2015
From: E.Vettorazzi at uke.de (Eik Vettorazzi)
Date: Wed, 9 Sep 2015 13:31:23 +0200
Subject: [R] Help with vectors!
In-Reply-To: <1441487658926-4711895.post@n4.nabble.com>
References: <1441295060538-4711801.post@n4.nabble.com>
	<1441487658926-4711895.post@n4.nabble.com>
Message-ID: <55F0188B.5050204@uke.de>

how about this:

match(VAS,unique(VAS))

#or, preserving given order

match(VAS,c("White","Yellow","Green","Black"))

Cheers.

Am 05.09.2015 um 23:14 schrieb Dan D:
> # your data
> VAS<-c("Green","Green","Black","Green","White","Yellow","Yellow","Black","Green","Black")
> 
> # declare the new vector
> New_Vector<-numeric(length(VAS))
> 
> # brute force:
> New_Vector[VAS=="White"]<-1
> New_Vector[VAS=="Yellow"]<-2
> New_Vector[VAS=="Green"]<-3
> New_Vector[VAS=="Black"]<-4
> 
> # a little more subtle
> cols<-c("White","Yellow","Green","Black")
> for (i in 1:length(cols))  New_Vector[VAS==cols[i]]<-i
> 
> # and a general approach (that may give a different indexing, but can be
> used for any array)
> for (i in 1:length(unique(VAS))) New_Vector[VAS==unique(VAS)[i]]<-i
> cbind(1:length(unique(VAS)),unique(VAS)) # a decoding key for the color
> index
> 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Help-with-vectors-tp4711801p4711895.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Eik Vettorazzi

Department of Medical Biometry and Epidemiology
University Medical Center Hamburg-Eppendorf

Martinistr. 52
20246 Hamburg

T ++49/40/7410-58243
F ++49/40/7410-57790
--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Rainer Schoppik
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From emptican at gmail.com  Wed Sep  9 14:49:48 2015
From: emptican at gmail.com (SH)
Date: Wed, 9 Sep 2015 08:49:48 -0400
Subject: [R] grid with random or clustered distribution
Message-ID: <CALSKosDDoU9iuiYMuUdaGeL8UuBzGqZKH+N9z-jwt3EDfiMdog@mail.gmail.com>

Hi R-users,

I hope this is not redundant questions.  I tried to search similar threads
relevant to my questions but could not find.  Any input would be greatly
appreciated.

I want to generate grid with binary values (1 or 0) in n1 by n2 (e.g., 100
by 100 or 200 by 500, etc.) given proportions of 1 and 0 values (e.g., 1,
5, or 10% of 1 from 100 by 100 grid).  For clustered distributed grid, I
hope to be able to define cluster size if possible.  Is there a simple way
to generate random/clustered grids with 1 and 0 values with a
pre-defined proportion?

So far, the function "EVariogram" in the "CompRandFld" package generates
clustered grid with 1 and 0.  Especially, the example #4 in the
"EVariogram" function description is a kind of what I want. Below is the
slightly modified code from the original one.  However, the code below
can't control proportion of 1 and 0 values and complicated or I have no
idea how to do it.  I believe there may be easies ways to
generate random/clustered grids with proportional 1 and 0 values.

Thank you very much in advance,

Steve


library(CompRandFld)
library(RandomFields)

x0 <- seq(1, 50, length.out=50)
y0 <- seq(1, 60, length.out=60)
d <- expand.grid(x=x0, y=y0)
dim(d)
head(d)
x <- d$x
y <- d$y
# Set the model's parameters:
corrmodel <- 'exponential'
mean <- 0
sill <- 1
nugget <- 0
scale <- 3
set.seed(1221)
# Simulation of the Binary-Gaussian random field:
data <- RFsim(x, y, corrmodel="exponential", model="BinaryGauss",
              param=list(mean=mean,sill=sill,scale=scale,nugget=nugget),
              threshold=0)$data
# Empirical lorelogram estimation:
fit <- EVariogram(data, x, y, numbins=20, maxdist=7, type="lorelogram")
# Results:
plot(fit$centers, fit$variograms, xlab='Distance', ylab="Lorelogram",
     ylim=c(min(fit$variograms), max(fit$variograms)),
     xlim=c(0, max(fit$centers)), pch=20, main="Spatial Lorelogram")
# Plotting
plot(d, type='n')
text(d, label=data)

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Wed Sep  9 15:11:28 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 9 Sep 2015 09:11:28 -0400
Subject: [R] grid with random or clustered distribution
In-Reply-To: <CALSKosDDoU9iuiYMuUdaGeL8UuBzGqZKH+N9z-jwt3EDfiMdog@mail.gmail.com>
References: <CALSKosDDoU9iuiYMuUdaGeL8UuBzGqZKH+N9z-jwt3EDfiMdog@mail.gmail.com>
Message-ID: <CAM_vjumrVxX6A+b3L4X5WiHXF4g9=V_AAgwuSyMdyUavqyjsnA@mail.gmail.com>

You can use gstat, as in:
https://www.researchgate.net/publication/43279659_Behavior_of_Vegetation_Sampling_Methods_in_the_Presence_of_Spatial_Autocorrelation

If you need more detail, I can dig up the code.

Sarah

On Wed, Sep 9, 2015 at 8:49 AM, SH <emptican at gmail.com> wrote:
> Hi R-users,
>
> I hope this is not redundant questions.  I tried to search similar threads
> relevant to my questions but could not find.  Any input would be greatly
> appreciated.
>
> I want to generate grid with binary values (1 or 0) in n1 by n2 (e.g., 100
> by 100 or 200 by 500, etc.) given proportions of 1 and 0 values (e.g., 1,
> 5, or 10% of 1 from 100 by 100 grid).  For clustered distributed grid, I
> hope to be able to define cluster size if possible.  Is there a simple way
> to generate random/clustered grids with 1 and 0 values with a
> pre-defined proportion?
>
> So far, the function "EVariogram" in the "CompRandFld" package generates
> clustered grid with 1 and 0.  Especially, the example #4 in the
> "EVariogram" function description is a kind of what I want. Below is the
> slightly modified code from the original one.  However, the code below
> can't control proportion of 1 and 0 values and complicated or I have no
> idea how to do it.  I believe there may be easies ways to
> generate random/clustered grids with proportional 1 and 0 values.
>
> Thank you very much in advance,
>
> Steve
>
>
> library(CompRandFld)
> library(RandomFields)
>
> x0 <- seq(1, 50, length.out=50)
> y0 <- seq(1, 60, length.out=60)
> d <- expand.grid(x=x0, y=y0)
> dim(d)
> head(d)
> x <- d$x
> y <- d$y
> # Set the model's parameters:
> corrmodel <- 'exponential'
> mean <- 0
> sill <- 1
> nugget <- 0
> scale <- 3
> set.seed(1221)
> # Simulation of the Binary-Gaussian random field:
> data <- RFsim(x, y, corrmodel="exponential", model="BinaryGauss",
>               param=list(mean=mean,sill=sill,scale=scale,nugget=nugget),
>               threshold=0)$data
> # Empirical lorelogram estimation:
> fit <- EVariogram(data, x, y, numbins=20, maxdist=7, type="lorelogram")
> # Results:
> plot(fit$centers, fit$variograms, xlab='Distance', ylab="Lorelogram",
>      ylim=c(min(fit$variograms), max(fit$variograms)),
>      xlim=c(0, max(fit$centers)), pch=20, main="Spatial Lorelogram")
> # Plotting
> plot(d, type='n')
> text(d, label=data)
>


-- 
Sarah Goslee
http://www.functionaldiversity.org


From emptican at gmail.com  Wed Sep  9 15:27:22 2015
From: emptican at gmail.com (SH)
Date: Wed, 9 Sep 2015 09:27:22 -0400
Subject: [R] grid with random or clustered distribution
In-Reply-To: <CAM_vjumrVxX6A+b3L4X5WiHXF4g9=V_AAgwuSyMdyUavqyjsnA@mail.gmail.com>
References: <CALSKosDDoU9iuiYMuUdaGeL8UuBzGqZKH+N9z-jwt3EDfiMdog@mail.gmail.com>
	<CAM_vjumrVxX6A+b3L4X5WiHXF4g9=V_AAgwuSyMdyUavqyjsnA@mail.gmail.com>
Message-ID: <CALSKosBgfWWknhMozjmGDE5Fvk2x0jJiMLGHOgOE1ZvGYb5C-w@mail.gmail.com>

Hi Sarah,

Thanks for your prompt responding.  The methodology in the publication is
very similar to what I plan to do.  Yes, could you be willing to share the
code if you don't mind?

Thanks a lot again,

Steve

On Wed, Sep 9, 2015 at 9:11 AM, Sarah Goslee <sarah.goslee at gmail.com> wrote:

> You can use gstat, as in:
>
> https://www.researchgate.net/publication/43279659_Behavior_of_Vegetation_Sampling_Methods_in_the_Presence_of_Spatial_Autocorrelation
>
> If you need more detail, I can dig up the code.
>
> Sarah
>
> On Wed, Sep 9, 2015 at 8:49 AM, SH <emptican at gmail.com> wrote:
> > Hi R-users,
> >
> > I hope this is not redundant questions.  I tried to search similar
> threads
> > relevant to my questions but could not find.  Any input would be greatly
> > appreciated.
> >
> > I want to generate grid with binary values (1 or 0) in n1 by n2 (e.g.,
> 100
> > by 100 or 200 by 500, etc.) given proportions of 1 and 0 values (e.g., 1,
> > 5, or 10% of 1 from 100 by 100 grid).  For clustered distributed grid, I
> > hope to be able to define cluster size if possible.  Is there a simple
> way
> > to generate random/clustered grids with 1 and 0 values with a
> > pre-defined proportion?
> >
> > So far, the function "EVariogram" in the "CompRandFld" package generates
> > clustered grid with 1 and 0.  Especially, the example #4 in the
> > "EVariogram" function description is a kind of what I want. Below is the
> > slightly modified code from the original one.  However, the code below
> > can't control proportion of 1 and 0 values and complicated or I have no
> > idea how to do it.  I believe there may be easies ways to
> > generate random/clustered grids with proportional 1 and 0 values.
> >
> > Thank you very much in advance,
> >
> > Steve
> >
> >
> > library(CompRandFld)
> > library(RandomFields)
> >
> > x0 <- seq(1, 50, length.out=50)
> > y0 <- seq(1, 60, length.out=60)
> > d <- expand.grid(x=x0, y=y0)
> > dim(d)
> > head(d)
> > x <- d$x
> > y <- d$y
> > # Set the model's parameters:
> > corrmodel <- 'exponential'
> > mean <- 0
> > sill <- 1
> > nugget <- 0
> > scale <- 3
> > set.seed(1221)
> > # Simulation of the Binary-Gaussian random field:
> > data <- RFsim(x, y, corrmodel="exponential", model="BinaryGauss",
> >               param=list(mean=mean,sill=sill,scale=scale,nugget=nugget),
> >               threshold=0)$data
> > # Empirical lorelogram estimation:
> > fit <- EVariogram(data, x, y, numbins=20, maxdist=7, type="lorelogram")
> > # Results:
> > plot(fit$centers, fit$variograms, xlab='Distance', ylab="Lorelogram",
> >      ylim=c(min(fit$variograms), max(fit$variograms)),
> >      xlim=c(0, max(fit$centers)), pch=20, main="Spatial Lorelogram")
> > # Plotting
> > plot(d, type='n')
> > text(d, label=data)
> >
>
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>

	[[alternative HTML version deleted]]


From david.ecotiere at cerema.fr  Wed Sep  9 10:19:34 2015
From: david.ecotiere at cerema.fr (DE)
Date: Wed, 9 Sep 2015 01:19:34 -0700 (PDT)
Subject: [R] Pb with date time sequence with period<1sec
Message-ID: <1441786774922-4712040.post@n4.nabble.com>

Hi,

I'd like to create a date-time seq with a period of 0.05 s, over several
days. 

# try :
start<-strptime(nom_fich,format="%y%m%d")
time<-seq(from=start, by=0.05, length.out = 86400*20*3)
print(as.POSIXlt(time[2])$sec)
# result is 0.04999995 and not 0.05 as expected

But If I am looking at the sequence, the seconds are not separated by 0.05,
but by something very close (0.04999995). Same pb if I want to add a
fraction of seconds to a date-time object :

# try :
start<-strptime(nom_fich,format="%y%m%d")
as.POSIXlt(start+0.05,origin="1970-01-01")$sec
# result is 0.04999995 and not 0.05 as expected

Any idea to solve this pb ?

Thank you in advance !




--
View this message in context: http://r.789695.n4.nabble.com/Pb-with-date-time-sequence-with-period-1sec-tp4712040.html
Sent from the R help mailing list archive at Nabble.com.


From varunmmm at gmail.com  Wed Sep  9 14:11:29 2015
From: varunmmm at gmail.com (varun yadav)
Date: Wed, 9 Sep 2015 17:41:29 +0530
Subject: [R] Seeking help
Message-ID: <CAOdZhM4ZUxoV_tWtVkS6yRP6w48HMViATmjL6Bwingn+16OvYg@mail.gmail.com>

Hi,

I am working on Predictive analytics assignment i which i need to predict
winner of state election . Data set is not very big but have multiple files
and as per my understanding only few of them are useful. Problem statement
is "* Which party should be given chance by the people to rule Bihar for
next five years in assembly election*". For better understanding, I would
like to mention few things


   - Data set do not contain any opinion poll or survey data
   - All are historical data some are from national poll results and some
   are from assembly elections which happened in 2005 and 2010
   - National election happened in 2014 in which BJP (Bhartiya Janta Party)
   did well in Bihar.
   - This election is only happening in one state Bihar not in entire
   country


*My doubts :  *


   - What should be the criteria to select the variable, what are key
   variable that we consider in any Poll if we are only having historical data
   - What model and algorithm should we use
   - Any suggestion how to approach.

I am attaching the data set as well Please help me out. I am clueless.

-- 
Thanks & Regards,
*Varun Yadav*
Department of Management Studies
IIT Roorkee,Uttarakhand
Contact: 9045999878
<https://in.linkedin.com/pub/varun-yadav/3b/87b/533>

From murdoch.duncan at gmail.com  Wed Sep  9 16:22:02 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 09 Sep 2015 10:22:02 -0400
Subject: [R] Seeking help
In-Reply-To: <CAOdZhM4ZUxoV_tWtVkS6yRP6w48HMViATmjL6Bwingn+16OvYg@mail.gmail.com>
References: <CAOdZhM4ZUxoV_tWtVkS6yRP6w48HMViATmjL6Bwingn+16OvYg@mail.gmail.com>
Message-ID: <55F0408A.6010509@gmail.com>

On 09/09/2015 8:11 AM, varun yadav wrote:
> Hi,
>
> I am working on Predictive analytics assignment

This list isn't for help with homework.  You should ask your instructor 
for help.

Duncan Murdoch


From sarah.goslee at gmail.com  Wed Sep  9 16:50:43 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 9 Sep 2015 10:50:43 -0400
Subject: [R] Pb with date time sequence with period<1sec
In-Reply-To: <1441786774922-4712040.post@n4.nabble.com>
References: <1441786774922-4712040.post@n4.nabble.com>
Message-ID: <CAM_vjukKdYF=GHoB1Ypv7RWTZMnWkKFigCLatqeysdQt5CxyMg@mail.gmail.com>

Looks like R FAQ 7.31 to me.

https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

On Wed, Sep 9, 2015 at 4:19 AM, DE <david.ecotiere at cerema.fr> wrote:
> Hi,
>
> I'd like to create a date-time seq with a period of 0.05 s, over several
> days.
>
> # try :
> start<-strptime(nom_fich,format="%y%m%d")
> time<-seq(from=start, by=0.05, length.out = 86400*20*3)
> print(as.POSIXlt(time[2])$sec)
> # result is 0.04999995 and not 0.05 as expected
>
> But If I am looking at the sequence, the seconds are not separated by 0.05,
> but by something very close (0.04999995). Same pb if I want to add a
> fraction of seconds to a date-time object :
>
> # try :
> start<-strptime(nom_fich,format="%y%m%d")
> as.POSIXlt(start+0.05,origin="1970-01-01")$sec
> # result is 0.04999995 and not 0.05 as expected
>
> Any idea to solve this pb ?
>
> Thank you in advance !
>
>


-- 
Sarah Goslee
http://www.functionaldiversity.org


From fanjianling at gmail.com  Wed Sep  9 18:04:55 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Wed, 9 Sep 2015 10:04:55 -0600
Subject: [R] How to do global curve fitting in R
Message-ID: <CAJ7mryJjqmr5bQh7nxDHw3yyPKvJ=bx6W1xRRwb1djDvr9kp_w@mail.gmail.com>

Hello all,

I am trying to fit my data to a nls model. I have many sets of data
and each can fit well for the curve. but I want to fit them at once by
sharing 2 of 3 parameters of the model. I know it is a typical global
curve fitting problem, but I don't know how to do it by R?

Does anyone know any package for this??

Thanks a lot!


Julian


From bgunter.4567 at gmail.com  Wed Sep  9 18:16:54 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 9 Sep 2015 09:16:54 -0700
Subject: [R] How to do global curve fitting in R
In-Reply-To: <CAJ7mryJjqmr5bQh7nxDHw3yyPKvJ=bx6W1xRRwb1djDvr9kp_w@mail.gmail.com>
References: <CAJ7mryJjqmr5bQh7nxDHw3yyPKvJ=bx6W1xRRwb1djDvr9kp_w@mail.gmail.com>
Message-ID: <CAGxFJbQZYGEZe0KyFBoYsKRcuSUycy5YuwR=is6GjGcmXGwRZw@mail.gmail.com>

Jianling:

1. What models are you trying to fit? Details matter, and it is
impossible to give a good answer without specifics.

2. In general terms, to do this one combines all the data and allows
for "appropriate" changes in the model parameters for the different
groups. For example, different intercepts, rate constants, etc. This
is where the specifics matter.

3. This is really more a statistics than an R question, and you would
probably do better to pursue these issues either with a local
statistical resource or on a statistics site, like
stats.stackexchange.com.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 9, 2015 at 9:04 AM, Jianling Fan <fanjianling at gmail.com> wrote:
> Hello all,
>
> I am trying to fit my data to a nls model. I have many sets of data
> and each can fit well for the curve. but I want to fit them at once by
> sharing 2 of 3 parameters of the model. I know it is a typical global
> curve fitting problem, but I don't know how to do it by R?
>
> Does anyone know any package for this??
>
> Thanks a lot!
>
>
> Julian
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fanjianling at gmail.com  Wed Sep  9 18:28:34 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Wed, 9 Sep 2015 10:28:34 -0600
Subject: [R] How to do global curve fitting in R
In-Reply-To: <CAGxFJbQZYGEZe0KyFBoYsKRcuSUycy5YuwR=is6GjGcmXGwRZw@mail.gmail.com>
References: <CAJ7mryJjqmr5bQh7nxDHw3yyPKvJ=bx6W1xRRwb1djDvr9kp_w@mail.gmail.com>
	<CAGxFJbQZYGEZe0KyFBoYsKRcuSUycy5YuwR=is6GjGcmXGwRZw@mail.gmail.com>
Message-ID: <CAJ7mryKoCYL66+cBLC3MGFL-Z5q5oq3Xt_87yj-pqAfXJ2XGrQ@mail.gmail.com>

Hi, Bert

Thanks for your reply.

I am fitting a logistic does response model with 3 parameters as :
y=a/(1+(x/x0)b), I have many sets of data. I can fit each of them for
the model. but I want them shared the parameter x0 and b, but varied
for each a.

I don't think it is a  statistics problem. It is a typical global
curve fitting problem with shared parameters. And I know how to do it
by many other software. But since I am trying to use R, I am wondering
is there any package or method to do it in R?

Thanks!

Jianling


On 9 September 2015 at 10:16, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Jianling:
>
> 1. What models are you trying to fit? Details matter, and it is
> impossible to give a good answer without specifics.
>
> 2. In general terms, to do this one combines all the data and allows
> for "appropriate" changes in the model parameters for the different
> groups. For example, different intercepts, rate constants, etc. This
> is where the specifics matter.
>
> 3. This is really more a statistics than an R question, and you would
> probably do better to pursue these issues either with a local
> statistical resource or on a statistics site, like
> stats.stackexchange.com.
>
> Cheers,
> Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Wed, Sep 9, 2015 at 9:04 AM, Jianling Fan <fanjianling at gmail.com> wrote:
>> Hello all,
>>
>> I am trying to fit my data to a nls model. I have many sets of data
>> and each can fit well for the curve. but I want to fit them at once by
>> sharing 2 of 3 parameters of the model. I know it is a typical global
>> curve fitting problem, but I don't know how to do it by R?
>>
>> Does anyone know any package for this??
>>
>> Thanks a lot!
>>
>>
>> Julian
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
Jianling Fan
???


From murdoch.duncan at gmail.com  Wed Sep  9 18:37:15 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 9 Sep 2015 12:37:15 -0400
Subject: [R] How to do global curve fitting in R
In-Reply-To: <CAJ7mryKoCYL66+cBLC3MGFL-Z5q5oq3Xt_87yj-pqAfXJ2XGrQ@mail.gmail.com>
References: <CAJ7mryJjqmr5bQh7nxDHw3yyPKvJ=bx6W1xRRwb1djDvr9kp_w@mail.gmail.com>
	<CAGxFJbQZYGEZe0KyFBoYsKRcuSUycy5YuwR=is6GjGcmXGwRZw@mail.gmail.com>
	<CAJ7mryKoCYL66+cBLC3MGFL-Z5q5oq3Xt_87yj-pqAfXJ2XGrQ@mail.gmail.com>
Message-ID: <55F0603B.5020901@gmail.com>

On 09/09/2015 12:28 PM, Jianling Fan wrote:
> Hi, Bert
> 
> Thanks for your reply.
> 
> I am fitting a logistic does response model with 3 parameters as :
> y=a/(1+(x/x0)b), I have many sets of data. I can fit each of them for
> the model. but I want them shared the parameter x0 and b, but varied
> for each a.

You might have a typo:  x0 and b aren't separately identifiable in that
model, only the ratio matters.

> 
> I don't think it is a  statistics problem. It is a typical global
> curve fitting problem with shared parameters. And I know how to do it
> by many other software. But since I am trying to use R, I am wondering
> is there any package or method to do it in R?

The nls() function should do what you want.  Your model is partially
linear ("a" enters linearly), so the "plinear" algorithm should work.
There's also an example of a "self-starting" logistic regression model
in ?selfStart.

Duncan Murdoch


> 
> Thanks!
> 
> Jianling
> 
> 
> On 9 September 2015 at 10:16, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> Jianling:
>>
>> 1. What models are you trying to fit? Details matter, and it is
>> impossible to give a good answer without specifics.
>>
>> 2. In general terms, to do this one combines all the data and allows
>> for "appropriate" changes in the model parameters for the different
>> groups. For example, different intercepts, rate constants, etc. This
>> is where the specifics matter.
>>
>> 3. This is really more a statistics than an R question, and you would
>> probably do better to pursue these issues either with a local
>> statistical resource or on a statistics site, like
>> stats.stackexchange.com.
>>
>> Cheers,
>> Bert
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>    -- Clifford Stoll
>>
>>
>> On Wed, Sep 9, 2015 at 9:04 AM, Jianling Fan <fanjianling at gmail.com> wrote:
>>> Hello all,
>>>
>>> I am trying to fit my data to a nls model. I have many sets of data
>>> and each can fit well for the curve. but I want to fit them at once by
>>> sharing 2 of 3 parameters of the model. I know it is a typical global
>>> curve fitting problem, but I don't know how to do it by R?
>>>
>>> Does anyone know any package for this??
>>>
>>> Thanks a lot!
>>>
>>>
>>> Julian
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From mxfomin at gmail.com  Wed Sep  9 18:28:02 2015
From: mxfomin at gmail.com (Maxim Fomin)
Date: Wed, 9 Sep 2015 19:28:02 +0300
Subject: [R] Doing time series markov regime switching in R
Message-ID: <CALB30JCx_Gi5y1etXuzbhNKq5=s9FXuJghsQyqsnh93-d+ZPeQ@mail.gmail.com>

Dear all,

I am doing business cycle research on industry data. One of methods to identify
cycle is Markov regime switching. As I see, there is a MSwM package
for the purposes which is pretty straightforward to use. However, some
questions for me remain:

1) Are there any packages? This is relevant because I could not couple
with 2-3-4.
2) The MSwM package provides example for regression analysis (one of
the steps is to provide regression model to msmFit). In my case I have
time series. Googling issue shows that regressing on intercept is
sufficient (i.e. 'time_data ~ 1') but I am not sure.
3) The package provides some diagnostic tools, mainly plotDiag (shows
pooled residuals, QQ plot, residual acf). However, it does not provide
tools to test a) that intercepts are same b) state variables are
independent. Accepting respective null hypotheses is evidence against
the model. How this can be done in R?
4) The package allows AR terms to be affected by unobservable state
's' through 'p' parameter. However, it is unclear when specification
should include switching in coefficients (ok, it depends on use case,
but what is general theory?).

Thanks in advance,
Best regards.


From wdunlap at tibco.com  Wed Sep  9 19:15:11 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 9 Sep 2015 10:15:11 -0700
Subject: [R] How to do global curve fitting in R
In-Reply-To: <CAJ7mryKoCYL66+cBLC3MGFL-Z5q5oq3Xt_87yj-pqAfXJ2XGrQ@mail.gmail.com>
References: <CAJ7mryJjqmr5bQh7nxDHw3yyPKvJ=bx6W1xRRwb1djDvr9kp_w@mail.gmail.com>
	<CAGxFJbQZYGEZe0KyFBoYsKRcuSUycy5YuwR=is6GjGcmXGwRZw@mail.gmail.com>
	<CAJ7mryKoCYL66+cBLC3MGFL-Z5q5oq3Xt_87yj-pqAfXJ2XGrQ@mail.gmail.com>
Message-ID: <CAF8bMcbaZKS4HmmWrSOU4kn2y8FEt0EJrMYBW11q2Q5x95VH1w@mail.gmail.com>

You can put all your data in one data.frame along with a column called, say,
'group' that says which group each row is in.  Then use the [] syntax in
nls's
formula argument to get group-specific estimates for some of the parameters.
E.g., in the following there is a global parameter 'b' and a group-specific
parameter
'p'.

> d <- transform(data.frame(x=seq(0,1,len=17),
group=rep(c("A","B","B","C"),len=17)), y =
round(1/(1.4+x^ifelse(group=="A", 2.3, ifelse(group=="B",3.1, 3.5))),2))
> str(d)
'data.frame':   17 obs. of  3 variables:
 $ x    : num  0 0.0625 0.125 0.1875 0.25 ...
 $ group: Factor w/ 3 levels "A","B","C": 1 2 2 3 1 2 2 3 1 2 ...
 $ y    : num  0.71 0.71 0.71 0.71 0.69 0.7 0.69 0.69 0.62 0.64 ...
> nls(y~1/(b+x^p[group]), data=d, start=list(b=1,
p=rep(3,length(levels(d$group)))))
Nonlinear regression model
  model: y ~ 1/(b + x^p[group])
   data: d
    b    p1    p2    p3
1.406 2.276 3.186 3.601
 residual sum-of-squares: 9.537e-05

Number of iterations to convergence: 5
Achieved convergence tolerance: 4.536e-06

(See the references listed in help(nls) to learn more about the statistical
issues.)


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Sep 9, 2015 at 9:28 AM, Jianling Fan <fanjianling at gmail.com> wrote:

> Hi, Bert
>
> Thanks for your reply.
>
> I am fitting a logistic does response model with 3 parameters as :
> y=a/(1+(x/x0)b), I have many sets of data. I can fit each of them for
> the model. but I want them shared the parameter x0 and b, but varied
> for each a.
>
> I don't think it is a  statistics problem. It is a typical global
> curve fitting problem with shared parameters. And I know how to do it
> by many other software. But since I am trying to use R, I am wondering
> is there any package or method to do it in R?
>
> Thanks!
>
> Jianling
>
>
> On 9 September 2015 at 10:16, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> > Jianling:
> >
> > 1. What models are you trying to fit? Details matter, and it is
> > impossible to give a good answer without specifics.
> >
> > 2. In general terms, to do this one combines all the data and allows
> > for "appropriate" changes in the model parameters for the different
> > groups. For example, different intercepts, rate constants, etc. This
> > is where the specifics matter.
> >
> > 3. This is really more a statistics than an R question, and you would
> > probably do better to pursue these issues either with a local
> > statistical resource or on a statistics site, like
> > stats.stackexchange.com.
> >
> > Cheers,
> > Bert
> > Bert Gunter
> >
> > "Data is not information. Information is not knowledge. And knowledge
> > is certainly not wisdom."
> >    -- Clifford Stoll
> >
> >
> > On Wed, Sep 9, 2015 at 9:04 AM, Jianling Fan <fanjianling at gmail.com>
> wrote:
> >> Hello all,
> >>
> >> I am trying to fit my data to a nls model. I have many sets of data
> >> and each can fit well for the curve. but I want to fit them at once by
> >> sharing 2 of 3 parameters of the model. I know it is a typical global
> >> curve fitting problem, but I don't know how to do it by R?
> >>
> >> Does anyone know any package for this??
> >>
> >> Thanks a lot!
> >>
> >>
> >> Julian
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Jianling Fan
> ???
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Sep  9 19:25:27 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 9 Sep 2015 10:25:27 -0700
Subject: [R] How to do global curve fitting in R
In-Reply-To: <55F0603B.5020901@gmail.com>
References: <CAJ7mryJjqmr5bQh7nxDHw3yyPKvJ=bx6W1xRRwb1djDvr9kp_w@mail.gmail.com>
	<CAGxFJbQZYGEZe0KyFBoYsKRcuSUycy5YuwR=is6GjGcmXGwRZw@mail.gmail.com>
	<CAJ7mryKoCYL66+cBLC3MGFL-Z5q5oq3Xt_87yj-pqAfXJ2XGrQ@mail.gmail.com>
	<55F0603B.5020901@gmail.com>
Message-ID: <CAGxFJbTAeJGvjbT4fiPEyTomPF-gP-EDfT+ndPgEPN3UUq40EQ@mail.gmail.com>

Well..

1) To add to Duncan's comments, if you create a new "dataset" discrete
factor column to indicate the dataset in the combined data, then you
would fit the model dataset/(corrected denominator).

2) However, there **is** a statistical issue here, for if you have
more than a "few" data sets, say more than 10 or a dozen or so, it
probably makes more sense to fit a mixed effects model where the
dataset factor is random. If so, my prior advice holds.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 9, 2015 at 9:37 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> On 09/09/2015 12:28 PM, Jianling Fan wrote:
>> Hi, Bert
>>
>> Thanks for your reply.
>>
>> I am fitting a logistic does response model with 3 parameters as :
>> y=a/(1+(x/x0)b), I have many sets of data. I can fit each of them for
>> the model. but I want them shared the parameter x0 and b, but varied
>> for each a.
>
> You might have a typo:  x0 and b aren't separately identifiable in that
> model, only the ratio matters.
>
>>
>> I don't think it is a  statistics problem. It is a typical global
>> curve fitting problem with shared parameters. And I know how to do it
>> by many other software. But since I am trying to use R, I am wondering
>> is there any package or method to do it in R?
>
> The nls() function should do what you want.  Your model is partially
> linear ("a" enters linearly), so the "plinear" algorithm should work.
> There's also an example of a "self-starting" logistic regression model
> in ?selfStart.
>
> Duncan Murdoch
>
>
>>
>> Thanks!
>>
>> Jianling
>>
>>
>> On 9 September 2015 at 10:16, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>> Jianling:
>>>
>>> 1. What models are you trying to fit? Details matter, and it is
>>> impossible to give a good answer without specifics.
>>>
>>> 2. In general terms, to do this one combines all the data and allows
>>> for "appropriate" changes in the model parameters for the different
>>> groups. For example, different intercepts, rate constants, etc. This
>>> is where the specifics matter.
>>>
>>> 3. This is really more a statistics than an R question, and you would
>>> probably do better to pursue these issues either with a local
>>> statistical resource or on a statistics site, like
>>> stats.stackexchange.com.
>>>
>>> Cheers,
>>> Bert
>>> Bert Gunter
>>>
>>> "Data is not information. Information is not knowledge. And knowledge
>>> is certainly not wisdom."
>>>    -- Clifford Stoll
>>>
>>>
>>> On Wed, Sep 9, 2015 at 9:04 AM, Jianling Fan <fanjianling at gmail.com> wrote:
>>>> Hello all,
>>>>
>>>> I am trying to fit my data to a nls model. I have many sets of data
>>>> and each can fit well for the curve. but I want to fit them at once by
>>>> sharing 2 of 3 parameters of the model. I know it is a typical global
>>>> curve fitting problem, but I don't know how to do it by R?
>>>>
>>>> Does anyone know any package for this??
>>>>
>>>> Thanks a lot!
>>>>
>>>>
>>>> Julian
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>


From fanjianling at gmail.com  Wed Sep  9 19:29:29 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Wed, 9 Sep 2015 11:29:29 -0600
Subject: [R] How to do global curve fitting in R
In-Reply-To: <CAF8bMcbaZKS4HmmWrSOU4kn2y8FEt0EJrMYBW11q2Q5x95VH1w@mail.gmail.com>
References: <CAJ7mryJjqmr5bQh7nxDHw3yyPKvJ=bx6W1xRRwb1djDvr9kp_w@mail.gmail.com>
	<CAGxFJbQZYGEZe0KyFBoYsKRcuSUycy5YuwR=is6GjGcmXGwRZw@mail.gmail.com>
	<CAJ7mryKoCYL66+cBLC3MGFL-Z5q5oq3Xt_87yj-pqAfXJ2XGrQ@mail.gmail.com>
	<CAF8bMcbaZKS4HmmWrSOU4kn2y8FEt0EJrMYBW11q2Q5x95VH1w@mail.gmail.com>
Message-ID: <CAJ7mry+AWvUfCymN3PPSAomrbH=LnD7gisrWzJruqXNchZX-sw@mail.gmail.com>

Thanks very much William,

That's exactly what I need! It is so excellent to do it by a
group-separation by [] syntax.

Thanks again!

Best regards,

Jianling






On 9 September 2015 at 11:15, William Dunlap <wdunlap at tibco.com> wrote:
> You can put all your data in one data.frame along with a column called, say,
> 'group' that says which group each row is in.  Then use the [] syntax in
> nls's
> formula argument to get group-specific estimates for some of the parameters.
> E.g., in the following there is a global parameter 'b' and a group-specific
> parameter
> 'p'.
>
>> d <- transform(data.frame(x=seq(0,1,len=17),
>> group=rep(c("A","B","B","C"),len=17)), y = round(1/(1.4+x^ifelse(group=="A",
>> 2.3, ifelse(group=="B",3.1, 3.5))),2))
>> str(d)
> 'data.frame':   17 obs. of  3 variables:
>  $ x    : num  0 0.0625 0.125 0.1875 0.25 ...
>  $ group: Factor w/ 3 levels "A","B","C": 1 2 2 3 1 2 2 3 1 2 ...
>  $ y    : num  0.71 0.71 0.71 0.71 0.69 0.7 0.69 0.69 0.62 0.64 ...
>> nls(y~1/(b+x^p[group]), data=d, start=list(b=1,
>> p=rep(3,length(levels(d$group)))))
> Nonlinear regression model
>   model: y ~ 1/(b + x^p[group])
>    data: d
>     b    p1    p2    p3
> 1.406 2.276 3.186 3.601
>  residual sum-of-squares: 9.537e-05
>
> Number of iterations to convergence: 5
> Achieved convergence tolerance: 4.536e-06
>
> (See the references listed in help(nls) to learn more about the statistical
> issues.)
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Wed, Sep 9, 2015 at 9:28 AM, Jianling Fan <fanjianling at gmail.com> wrote:
>>
>> Hi, Bert
>>
>> Thanks for your reply.
>>
>> I am fitting a logistic does response model with 3 parameters as :
>> y=a/(1+(x/x0)b), I have many sets of data. I can fit each of them for
>> the model. but I want them shared the parameter x0 and b, but varied
>> for each a.
>>
>> I don't think it is a  statistics problem. It is a typical global
>> curve fitting problem with shared parameters. And I know how to do it
>> by many other software. But since I am trying to use R, I am wondering
>> is there any package or method to do it in R?
>>
>> Thanks!
>>
>> Jianling
>>
>>
>> On 9 September 2015 at 10:16, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> > Jianling:
>> >
>> > 1. What models are you trying to fit? Details matter, and it is
>> > impossible to give a good answer without specifics.
>> >
>> > 2. In general terms, to do this one combines all the data and allows
>> > for "appropriate" changes in the model parameters for the different
>> > groups. For example, different intercepts, rate constants, etc. This
>> > is where the specifics matter.
>> >
>> > 3. This is really more a statistics than an R question, and you would
>> > probably do better to pursue these issues either with a local
>> > statistical resource or on a statistics site, like
>> > stats.stackexchange.com.
>> >
>> > Cheers,
>> > Bert
>> > Bert Gunter
>> >
>> > "Data is not information. Information is not knowledge. And knowledge
>> > is certainly not wisdom."
>> >    -- Clifford Stoll
>> >
>> >
>> > On Wed, Sep 9, 2015 at 9:04 AM, Jianling Fan <fanjianling at gmail.com>
>> > wrote:
>> >> Hello all,
>> >>
>> >> I am trying to fit my data to a nls model. I have many sets of data
>> >> and each can fit well for the curve. but I want to fit them at once by
>> >> sharing 2 of 3 parameters of the model. I know it is a typical global
>> >> curve fitting problem, but I don't know how to do it by R?
>> >>
>> >> Does anyone know any package for this??
>> >>
>> >> Thanks a lot!
>> >>
>> >>
>> >> Julian
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Jianling Fan
>> ???
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>



-- 
Jianling Fan
???


From schwidom at gmx.net  Wed Sep  9 20:30:51 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Wed, 9 Sep 2015 20:30:51 +0200
Subject: [R] Help with vectors!
In-Reply-To: <1441487658926-4711895.post@n4.nabble.com>
References: <1441295060538-4711801.post@n4.nabble.com>
	<1441487658926-4711895.post@n4.nabble.com>
Message-ID: <20150909183051.GA13162@debian64>


c( as.factor( VAS))

On Sat, Sep 05, 2015 at 02:14:18PM -0700, Dan D wrote:
> # your data
> VAS<-c("Green","Green","Black","Green","White","Yellow","Yellow","Black","Green","Black")
> 
> # declare the new vector
> New_Vector<-numeric(length(VAS))
> 
> # brute force:
> New_Vector[VAS=="White"]<-1
> New_Vector[VAS=="Yellow"]<-2
> New_Vector[VAS=="Green"]<-3
> New_Vector[VAS=="Black"]<-4
> 
> # a little more subtle
> cols<-c("White","Yellow","Green","Black")
> for (i in 1:length(cols))  New_Vector[VAS==cols[i]]<-i
> 
> # and a general approach (that may give a different indexing, but can be
> used for any array)
> for (i in 1:length(unique(VAS))) New_Vector[VAS==unique(VAS)[i]]<-i
> cbind(1:length(unique(VAS)),unique(VAS)) # a decoding key for the color
> index
> 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Help-with-vectors-tp4711801p4711895.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From oluola2011 at yahoo.com  Wed Sep  9 21:03:19 2015
From: oluola2011 at yahoo.com (Olu Ola)
Date: Wed, 9 Sep 2015 12:03:19 -0700
Subject: [R] Handling NA's when you write your own code
Message-ID: <1441825399.84942.YahooMailBasic@web161602.mail.bf1.yahoo.com>

Hello,
I have a dataset that have a couple of missing values and I DO NOT want to delete the observations with the missing values. I have read about na.action in dealing with missing values but I do not know how it applies to user-specific written code.

Is there a code you can use with your dataset so that subsequent analysis will automatically detect missing values?

The following are some of the things I will be doing so that you can advice me appropriately

Computing OLS estimates using the matrix approach
Write objective and gradient function subroutines which will now be used in the Optimx for nonlinear optimization.

Thank you


From ddalthorp at usgs.gov  Wed Sep  9 20:57:25 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Wed, 9 Sep 2015 11:57:25 -0700 (PDT)
Subject: [R] Help with vectors!
In-Reply-To: <20150909183051.GA13162@debian64>
References: <1441295060538-4711801.post@n4.nabble.com>
	<1441487658926-4711895.post@n4.nabble.com>
	<20150909183051.GA13162@debian64>
Message-ID: <1441825045692-4712061.post@n4.nabble.com>

VAS<-c("Green","Green","Black","Green","White","Yellow","Yellow","Black","Green","Black")
c(factor(VAS)) # to give integer indexing to the colors

---
This is very nice, Frank. And it can be easily adjusted to match the
original criterion that the numbers match the order of appearance of the
colors in the original vector: 

c(factor(VAS,levels=unique(VAS)))




--
View this message in context: http://r.789695.n4.nabble.com/Help-with-vectors-tp4711801p4712061.html
Sent from the R help mailing list archive at Nabble.com.


From sarah.goslee at gmail.com  Wed Sep  9 21:27:07 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 9 Sep 2015 15:27:07 -0400
Subject: [R] grid with random or clustered distribution
In-Reply-To: <CALSKosBgfWWknhMozjmGDE5Fvk2x0jJiMLGHOgOE1ZvGYb5C-w@mail.gmail.com>
References: <CALSKosDDoU9iuiYMuUdaGeL8UuBzGqZKH+N9z-jwt3EDfiMdog@mail.gmail.com>
	<CAM_vjumrVxX6A+b3L4X5WiHXF4g9=V_AAgwuSyMdyUavqyjsnA@mail.gmail.com>
	<CALSKosBgfWWknhMozjmGDE5Fvk2x0jJiMLGHOgOE1ZvGYb5C-w@mail.gmail.com>
Message-ID: <CAM_vjuk7v9q7wVn=WWASHmt3kpucPsjTAwi60sGA2J3UtrvX2A@mail.gmail.com>

########################################################
### simulate landscapes with spatial autocorrelation ###
###             Sarah Goslee 2015-09-09              ###
###     Goslee 2006 PLANT ECOLOGY 187(2):203-212     ###
########################################################

library(gstat)


## parameters
abun <- 0.2
dim1 <- 20
dim2 <- 50


## setup
xy <- expand.grid(seq_len(dim1), seq_len(dim2))
names(xy) <- c("x","y")


## three sample simulations

# no spatial autocorrelation
g.dummy <- gstat(formula = z~x+y, locations = ~x+y, dummy = TRUE, beta
= 0, model = vgm(1,"Nug", 0), nmax = 50)
sim <- predict(g.dummy, newdata = xy, nsim = 1)
random.landscape.000 <- predict(g.dummy, newdata = xy, nsim = 1)
random.landscape.000[,3] <- ifelse(random.landscape.000[,3] >
quantile(random.landscape.000[,3], abun), 0, 1)

# little spatial autocorrelation
g.dummy <- gstat(formula = z~x+y, locations = ~x+y, dummy = TRUE, beta
= 0, model = vgm(1,"Exp", 5), nmax = 50)
random.landscape.005 <- predict(g.dummy, newdata = xy, nsim = 1)
random.landscape.005[,3] <- ifelse(random.landscape.005[,3] >
quantile(random.landscape.005[,3], abun), 0, 1)

# much spatial autocorrelation
g.dummy <- gstat(formula = z~x+y, locations = ~x+y, dummy = TRUE, beta
= 0, model = vgm(1,"Exp", 250), nmax = 50)
sim <- predict(g.dummy, newdata = xy, nsim = 1)
random.landscape.250 <- predict(g.dummy, newdata = xy, nsim = 1)
random.landscape.250[,3] <- ifelse(random.landscape.250[,3] >
quantile(random.landscape.250[,3], abun), 0, 1)


# plot the simulated landscapes
par(mfrow=c(1,3))
image(random.landscape.000, main="Null", xaxt="n", yaxt="n", bty="n",
xlim=c(0,dim1), ylim=c(0, dim2), col=c("lightgray", "darkgray"))
image(random.landscape.005, main="5", xaxt="n", yaxt="n", bty="n",
xlim=c(0,dim1), ylim=c(0, dim2), col=c("lightgray", "blue"))
image(random.landscape.250, main="250", sub=paste("abun =", abun),
xaxt="n", yaxt="n", bty="n", xlim=c(0,dim1), ylim=c(0, dim2),
col=c("lightgray", "darkblue"))

########################################################
###                        end                       ###
########################################################

On Wed, Sep 9, 2015 at 9:27 AM, SH <emptican at gmail.com> wrote:
> Hi Sarah,
>
> Thanks for your prompt responding.  The methodology in the publication is
> very similar to what I plan to do.  Yes, could you be willing to share the
> code if you don't mind?
>
> Thanks a lot again,
>
> Steve
>
> On Wed, Sep 9, 2015 at 9:11 AM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>>
>> You can use gstat, as in:
>>
>> https://www.researchgate.net/publication/43279659_Behavior_of_Vegetation_Sampling_Methods_in_the_Presence_of_Spatial_Autocorrelation
>>
>> If you need more detail, I can dig up the code.
>>
>> Sarah
>>
>> On Wed, Sep 9, 2015 at 8:49 AM, SH <emptican at gmail.com> wrote:
>> > Hi R-users,
>> >
>> > I hope this is not redundant questions.  I tried to search similar
>> > threads
>> > relevant to my questions but could not find.  Any input would be greatly
>> > appreciated.
>> >
>> > I want to generate grid with binary values (1 or 0) in n1 by n2 (e.g.,
>> > 100
>> > by 100 or 200 by 500, etc.) given proportions of 1 and 0 values (e.g.,
>> > 1,
>> > 5, or 10% of 1 from 100 by 100 grid).  For clustered distributed grid, I
>> > hope to be able to define cluster size if possible.  Is there a simple
>> > way
>> > to generate random/clustered grids with 1 and 0 values with a
>> > pre-defined proportion?
>> >
>> > So far, the function "EVariogram" in the "CompRandFld" package generates
>> > clustered grid with 1 and 0.  Especially, the example #4 in the
>> > "EVariogram" function description is a kind of what I want. Below is the
>> > slightly modified code from the original one.  However, the code below
>> > can't control proportion of 1 and 0 values and complicated or I have no
>> > idea how to do it.  I believe there may be easies ways to
>> > generate random/clustered grids with proportional 1 and 0 values.
>> >
>> > Thank you very much in advance,
>> >
>> > Steve
>> >
>> >
>> > library(CompRandFld)
>> > library(RandomFields)
>> >
>> > x0 <- seq(1, 50, length.out=50)
>> > y0 <- seq(1, 60, length.out=60)
>> > d <- expand.grid(x=x0, y=y0)
>> > dim(d)
>> > head(d)
>> > x <- d$x
>> > y <- d$y
>> > # Set the model's parameters:
>> > corrmodel <- 'exponential'
>> > mean <- 0
>> > sill <- 1
>> > nugget <- 0
>> > scale <- 3
>> > set.seed(1221)
>> > # Simulation of the Binary-Gaussian random field:
>> > data <- RFsim(x, y, corrmodel="exponential", model="BinaryGauss",
>> >               param=list(mean=mean,sill=sill,scale=scale,nugget=nugget),
>> >               threshold=0)$data
>> > # Empirical lorelogram estimation:
>> > fit <- EVariogram(data, x, y, numbins=20, maxdist=7, type="lorelogram")
>> > # Results:
>> > plot(fit$centers, fit$variograms, xlab='Distance', ylab="Lorelogram",
>> >      ylim=c(min(fit$variograms), max(fit$variograms)),
>> >      xlim=c(0, max(fit$centers)), pch=20, main="Spatial Lorelogram")
>> > # Plotting
>> > plot(d, type='n')
>> > text(d, label=data)
>> >
>>
>>
>> --
>> Sarah Goslee
>> http://www.functionaldiversity.org
>
>



-- 
Sarah Goslee
http://www.stringpage.com
http://www.sarahgoslee.com
http://www.functionaldiversity.org


From lid.zigh at gmail.com  Wed Sep  9 21:39:43 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Wed, 9 Sep 2015 14:39:43 -0500
Subject: [R] intersection between two matrices based on two columns in R
Message-ID: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>

Hi there,

I want to find the intersection between two different data frame or
matrices based on two columns.
for example in matrix A I have 5 columns, the first two columns are Id1 and
Id2 and I have the same columns in the other matrix B, (Id1, Id2 ,,,)
how can I find the intersection between these two matrices based on columns
Id1 and Id2 in R?

Thanks

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Wed Sep  9 21:48:10 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 09 Sep 2015 12:48:10 -0700
Subject: [R] intersection between two matrices based on two columns in R
In-Reply-To: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>
References: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>
Message-ID: <64A6777A-BFA3-4368-AE25-0A98C3DEF2BB@dcn.davis.CA.us>

Matrices and data frames are write different. For this you most likely want to use days frames.

?merge

Using typical options merge gives you all columns from both data frames. You can choose to select specific columns by indexing the data frames before passing them to merge if you don't want that.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 9, 2015 12:39:43 PM PDT, Lida Zeighami <lid.zigh at gmail.com> wrote:
>Hi there,
>
>I want to find the intersection between two different data frame or
>matrices based on two columns.
>for example in matrix A I have 5 columns, the first two columns are Id1
>and
>Id2 and I have the same columns in the other matrix B, (Id1, Id2 ,,,)
>how can I find the intersection between these two matrices based on
>columns
>Id1 and Id2 in R?
>
>Thanks
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Wed Sep  9 21:51:42 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 9 Sep 2015 12:51:42 -0700
Subject: [R] intersection between two matrices based on two columns in R
In-Reply-To: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>
References: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>
Message-ID: <CAGxFJbS-6OVF5mevUBEFqCyRL9nAg-D3rNOUfGy2R3wOm-rFtg@mail.gmail.com>

I am not quite clear what you mean by "intersection", but I think

?merge

 is what you are looking for.

Cheers,
Bert

P.S. Please post in plain text, not HTML (though here it didn't matter).
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 9, 2015 at 12:39 PM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> Hi there,
>
> I want to find the intersection between two different data frame or
> matrices based on two columns.
> for example in matrix A I have 5 columns, the first two columns are Id1 and
> Id2 and I have the same columns in the other matrix B, (Id1, Id2 ,,,)
> how can I find the intersection between these two matrices based on columns
> Id1 and Id2 in R?
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Wed Sep  9 21:51:47 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 9 Sep 2015 15:51:47 -0400
Subject: [R] intersection between two matrices based on two columns in R
In-Reply-To: <64A6777A-BFA3-4368-AE25-0A98C3DEF2BB@dcn.davis.CA.us>
References: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>
	<64A6777A-BFA3-4368-AE25-0A98C3DEF2BB@dcn.davis.CA.us>
Message-ID: <55F08DD3.6010000@gmail.com>

On 09/09/2015 3:48 PM, Jeff Newmiller wrote:
> Matrices and data frames are write different. For this you most likely want to use days frames.
 ...
> Sent from my phone. Please excuse my brevity.

 ... and your auto-correct. ;-)

Duncan Murdoch


From dwinsemius at comcast.net  Wed Sep  9 21:53:32 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 9 Sep 2015 12:53:32 -0700
Subject: [R] intersection between two matrices based on two columns in R
In-Reply-To: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>
References: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>
Message-ID: <05886B47-40C7-4194-90DA-2279C346A853@comcast.net>


On Sep 9, 2015, at 12:39 PM, Lida Zeighami wrote:

> Hi there,
> 
> I want to find the intersection between two different data frame or
> matrices based on two columns.
> for example in matrix A I have 5 columns, the first two columns are Id1 and
> Id2 and I have the same columns in the other matrix B, (Id1, Id2 ,,,)
> how can I find the intersection between these two matrices based on columns
> Id1 and Id2 in R?
> 

It's not clear to me what you mean by the intersection of matrices, but if you want the intersection of a column vector,  A[ ,'Id1'] , with another column vector, B[ , 'Id1'], then this produces the intersection (as a vector):

intersect( A[ ,'Id1'] , B[ , 'Id1'])


I suspect, however that your native language is not R or "database" (and perhaps not English) so my alternate hypothesis is that you really intend to _merge_ these "matrices", which I suspect are really dataframes:

my_inner_join <- merge(A, B, by=c("Id1", "Id2") )


> Thanks
> 
> 	[[alternative HTML version deleted]]
> 

This is a plain text mailing list.

-- 

David Winsemius
Alameda, CA, USA


From naras at stanford.edu  Wed Sep  9 17:46:36 2015
From: naras at stanford.edu (Balasubramanian Narasimhan)
Date: Wed, 9 Sep 2015 08:46:36 -0700
Subject: [R] useR! 2016 Conference Announcement
Message-ID: <55F0545C.6030602@stanford.edu>

We are happy to announce that the R user conference

    useR! 2016

is scheduled for June 27-30, 2016, and will take place at the campus
of Stanford University, Stanford California, USA.

Following the formats of previous conferences, the program will
consist of a day of tutorials followed by three days of invited
lectures and user-contributed sessions.

The program will cover topics such as

     * History of R and computing with data
     * Bayesian Statistics
     * Bioinformatics
     * Economics, Finance and Insurance
     * High Performance Computing
     * Industrial Applications
     * Statistical Learning with Big Data
     * Spatial Statistics
     * Teaching
     * Visualization & Graphics
     * and many more.


CONFERENCE WEBPAGE

The URL for the conference web page is:

http://www.R-project.org/useR-2016

It is minimal at the moment but details of invited speakers, tutorial
sessions, registration and abstract submission process will soon
appear. A follow-up announcement will be posted to announce important
details but please feel free to check the website periodically.

CALL FOR TUTORIAL SUBMISSIONS

We invite R users to submit proposals for three hour tutorials on
special topics on R. The proposals should give a brief description of
the tutorial, including goals, detailed outline, justification why the
tutorial is important, background knowledge required and potential
attendees. The proposals should be sent before January 3, 2016 to
useR-2016 at R-project.org.


IMPORTANT DATES

    October 26, 2015    open submission of abstracts
    December 1, 2015    open registration
    January  3, 2016    tutorial submission deadline
    March    1, 2016    early registration deadline
    March    3, 2016    submission deadline for abstracts
    March   28, 2016    notification of abstract acceptance
    June     1, 2016    registration deadline
    June    20, 2016    late registration deadline (space permitting)
    June    27, 2016    tutorials
    June    28, 2016    conference start
    June    30, 2016    conference end

We hope to meet you in Stanford!

The organizing committee:
     John Chambers, Sandrine Dudoit, Trevor Hastie, Susan Holmes,
     Simon Jackman, Olivia Lau, Nicholas Lewin-Koh, Norman Matloff,
     Balasubramanian Narasimhan, Karthik Ram, Joseph Rickert,
     Duncan Temple Lang.

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From nick.petschek at gmail.com  Wed Sep  9 20:52:33 2015
From: nick.petschek at gmail.com (Nick Petschek)
Date: Wed, 9 Sep 2015 14:52:33 -0400
Subject: [R] frequency tables
Message-ID: <CAGzwxb5ka8DuWi0nOqeH3-bf-68BGtqG_ybwWkZU1TgmvrBkPw@mail.gmail.com>

Hi,

I want to run frequency tables for multiple categorical variables, ideally
one in %, and the other as a count, and am unsure how to proceed. I would
like my output to be the following:

       | favorable | unfavorable | neutral
Q1   |      80%  |         10%    |  10% |
Q2   |      70%  |         20%    |  10% |
Q3   |      60%  |         10%    |  30% |

Then the same except counts instead of %.

I've been looking into the table function, but can't quite get it to work.
Ideally I would love to export a large table to .csv.

Any direction would be great!

	[[alternative HTML version deleted]]


From otoomet at gmail.com  Wed Sep  9 21:51:31 2015
From: otoomet at gmail.com (Ott Toomet)
Date: Wed, 9 Sep 2015 12:51:31 -0700
Subject: [R] Handling NA's when you write your own code
In-Reply-To: <1441825399.84942.YahooMailBasic@web161602.mail.bf1.yahoo.com>
References: <1441825399.84942.YahooMailBasic@web161602.mail.bf1.yahoo.com>
Message-ID: <CAMMJQ0ZiEq2SHzrTodUN0rhCmmTBk-YBB8dAVb5gq01JOpB7Gg@mail.gmail.com>

Not quite sure what you mean with "dealing with NAs in use-specific code".

Some functions handle these automatically, like lm().  If you write your
own code, and want to handle NA-s somehow, you have to implement such
checks yourself.  You may also use the "complete.cases" function (but that
does not detect Inf-s).

Typically your own handling means excluding NA-s in one way or another as
most of the math operations are not defined on these.

Cheers,
Ott

On Wed, Sep 9, 2015 at 12:03 PM, Olu Ola via R-help <r-help at r-project.org>
wrote:

> Hello,
> I have a dataset that have a couple of missing values and I DO NOT want to
> delete the observations with the missing values. I have read about
> na.action in dealing with missing values but I do not know how it applies
> to user-specific written code.
>
> Is there a code you can use with your dataset so that subsequent analysis
> will automatically detect missing values?
>
> The following are some of the things I will be doing so that you can
> advice me appropriately
>
> Computing OLS estimates using the matrix approach
> Write objective and gradient function subroutines which will now be used
> in the Optimx for nonlinear optimization.
>
> Thank you
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Ott Toomet

Visiting Researcher
School of Information
Mary Gates Hall, Suite 310
University of Washington
Seattle, WA 98195

	[[alternative HTML version deleted]]


From emptican at gmail.com  Wed Sep  9 22:07:49 2015
From: emptican at gmail.com (SH)
Date: Wed, 9 Sep 2015 16:07:49 -0400
Subject: [R] grid with random or clustered distribution
In-Reply-To: <CAM_vjuk7v9q7wVn=WWASHmt3kpucPsjTAwi60sGA2J3UtrvX2A@mail.gmail.com>
References: <CALSKosDDoU9iuiYMuUdaGeL8UuBzGqZKH+N9z-jwt3EDfiMdog@mail.gmail.com>
	<CAM_vjumrVxX6A+b3L4X5WiHXF4g9=V_AAgwuSyMdyUavqyjsnA@mail.gmail.com>
	<CALSKosBgfWWknhMozjmGDE5Fvk2x0jJiMLGHOgOE1ZvGYb5C-w@mail.gmail.com>
	<CAM_vjuk7v9q7wVn=WWASHmt3kpucPsjTAwi60sGA2J3UtrvX2A@mail.gmail.com>
Message-ID: <CALSKosDgfDSnXtSF06zv9OXgLkCLZ5DCxe3RruPjOxT58u=dwA@mail.gmail.com>

Thank you so much!  I will try it!

On Wed, Sep 9, 2015 at 3:27 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:

> ########################################################
> ### simulate landscapes with spatial autocorrelation ###
> ###             Sarah Goslee 2015-09-09              ###
> ###     Goslee 2006 PLANT ECOLOGY 187(2):203-212     ###
> ########################################################
>
> library(gstat)
>
>
> ## parameters
> abun <- 0.2
> dim1 <- 20
> dim2 <- 50
>
>
> ## setup
> xy <- expand.grid(seq_len(dim1), seq_len(dim2))
> names(xy) <- c("x","y")
>
>
> ## three sample simulations
>
> # no spatial autocorrelation
> g.dummy <- gstat(formula = z~x+y, locations = ~x+y, dummy = TRUE, beta
> = 0, model = vgm(1,"Nug", 0), nmax = 50)
> sim <- predict(g.dummy, newdata = xy, nsim = 1)
> random.landscape.000 <- predict(g.dummy, newdata = xy, nsim = 1)
> random.landscape.000[,3] <- ifelse(random.landscape.000[,3] >
> quantile(random.landscape.000[,3], abun), 0, 1)
>
> # little spatial autocorrelation
> g.dummy <- gstat(formula = z~x+y, locations = ~x+y, dummy = TRUE, beta
> = 0, model = vgm(1,"Exp", 5), nmax = 50)
> random.landscape.005 <- predict(g.dummy, newdata = xy, nsim = 1)
> random.landscape.005[,3] <- ifelse(random.landscape.005[,3] >
> quantile(random.landscape.005[,3], abun), 0, 1)
>
> # much spatial autocorrelation
> g.dummy <- gstat(formula = z~x+y, locations = ~x+y, dummy = TRUE, beta
> = 0, model = vgm(1,"Exp", 250), nmax = 50)
> sim <- predict(g.dummy, newdata = xy, nsim = 1)
> random.landscape.250 <- predict(g.dummy, newdata = xy, nsim = 1)
> random.landscape.250[,3] <- ifelse(random.landscape.250[,3] >
> quantile(random.landscape.250[,3], abun), 0, 1)
>
>
> # plot the simulated landscapes
> par(mfrow=c(1,3))
> image(random.landscape.000, main="Null", xaxt="n", yaxt="n", bty="n",
> xlim=c(0,dim1), ylim=c(0, dim2), col=c("lightgray", "darkgray"))
> image(random.landscape.005, main="5", xaxt="n", yaxt="n", bty="n",
> xlim=c(0,dim1), ylim=c(0, dim2), col=c("lightgray", "blue"))
> image(random.landscape.250, main="250", sub=paste("abun =", abun),
> xaxt="n", yaxt="n", bty="n", xlim=c(0,dim1), ylim=c(0, dim2),
> col=c("lightgray", "darkblue"))
>
> ########################################################
> ###                        end                       ###
> ########################################################
>
> On Wed, Sep 9, 2015 at 9:27 AM, SH <emptican at gmail.com> wrote:
> > Hi Sarah,
> >
> > Thanks for your prompt responding.  The methodology in the publication is
> > very similar to what I plan to do.  Yes, could you be willing to share
> the
> > code if you don't mind?
> >
> > Thanks a lot again,
> >
> > Steve
> >
> > On Wed, Sep 9, 2015 at 9:11 AM, Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
> >>
> >> You can use gstat, as in:
> >>
> >>
> https://www.researchgate.net/publication/43279659_Behavior_of_Vegetation_Sampling_Methods_in_the_Presence_of_Spatial_Autocorrelation
> >>
> >> If you need more detail, I can dig up the code.
> >>
> >> Sarah
> >>
> >> On Wed, Sep 9, 2015 at 8:49 AM, SH <emptican at gmail.com> wrote:
> >> > Hi R-users,
> >> >
> >> > I hope this is not redundant questions.  I tried to search similar
> >> > threads
> >> > relevant to my questions but could not find.  Any input would be
> greatly
> >> > appreciated.
> >> >
> >> > I want to generate grid with binary values (1 or 0) in n1 by n2 (e.g.,
> >> > 100
> >> > by 100 or 200 by 500, etc.) given proportions of 1 and 0 values (e.g.,
> >> > 1,
> >> > 5, or 10% of 1 from 100 by 100 grid).  For clustered distributed
> grid, I
> >> > hope to be able to define cluster size if possible.  Is there a simple
> >> > way
> >> > to generate random/clustered grids with 1 and 0 values with a
> >> > pre-defined proportion?
> >> >
> >> > So far, the function "EVariogram" in the "CompRandFld" package
> generates
> >> > clustered grid with 1 and 0.  Especially, the example #4 in the
> >> > "EVariogram" function description is a kind of what I want. Below is
> the
> >> > slightly modified code from the original one.  However, the code below
> >> > can't control proportion of 1 and 0 values and complicated or I have
> no
> >> > idea how to do it.  I believe there may be easies ways to
> >> > generate random/clustered grids with proportional 1 and 0 values.
> >> >
> >> > Thank you very much in advance,
> >> >
> >> > Steve
> >> >
> >> >
> >> > library(CompRandFld)
> >> > library(RandomFields)
> >> >
> >> > x0 <- seq(1, 50, length.out=50)
> >> > y0 <- seq(1, 60, length.out=60)
> >> > d <- expand.grid(x=x0, y=y0)
> >> > dim(d)
> >> > head(d)
> >> > x <- d$x
> >> > y <- d$y
> >> > # Set the model's parameters:
> >> > corrmodel <- 'exponential'
> >> > mean <- 0
> >> > sill <- 1
> >> > nugget <- 0
> >> > scale <- 3
> >> > set.seed(1221)
> >> > # Simulation of the Binary-Gaussian random field:
> >> > data <- RFsim(x, y, corrmodel="exponential", model="BinaryGauss",
> >> >
>  param=list(mean=mean,sill=sill,scale=scale,nugget=nugget),
> >> >               threshold=0)$data
> >> > # Empirical lorelogram estimation:
> >> > fit <- EVariogram(data, x, y, numbins=20, maxdist=7,
> type="lorelogram")
> >> > # Results:
> >> > plot(fit$centers, fit$variograms, xlab='Distance', ylab="Lorelogram",
> >> >      ylim=c(min(fit$variograms), max(fit$variograms)),
> >> >      xlim=c(0, max(fit$centers)), pch=20, main="Spatial Lorelogram")
> >> > # Plotting
> >> > plot(d, type='n')
> >> > text(d, label=data)
> >> >
> >>
> >>
> >> --
> >> Sarah Goslee
> >> http://www.functionaldiversity.org
> >
> >
>
>
>
> --
> Sarah Goslee
> http://www.stringpage.com
> http://www.sarahgoslee.com
> http://www.functionaldiversity.org
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Wed Sep  9 22:16:49 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 09 Sep 2015 13:16:49 -0700
Subject: [R] intersection between two matrices based on two columns in R
In-Reply-To: <55F08DD3.6010000@gmail.com>
References: <CAMqbV1Cj61FKUA8z3NROrGUWMZvNem==stbxP5p9SPTPgxpsCQ@mail.gmail.com>
	<64A6777A-BFA3-4368-AE25-0A98C3DEF2BB@dcn.davis.CA.us>
	<55F08DD3.6010000@gmail.com>
Message-ID: <BC17C9FC-3729-47BC-ADBF-2DFF6079D890@dcn.davis.CA.us>

Ergh... DYAC.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 9, 2015 12:51:47 PM PDT, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>On 09/09/2015 3:48 PM, Jeff Newmiller wrote:
>> Matrices and data frames are write different. For this you most
>likely want to use days frames.
> ...
>> Sent from my phone. Please excuse my brevity.
>
> ... and your auto-correct. ;-)
>
>Duncan Murdoch


From schwidom at gmx.net  Wed Sep  9 23:02:16 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Wed, 9 Sep 2015 23:02:16 +0200
Subject: [R] Help with vectors!
In-Reply-To: <1441487658926-4711895.post@n4.nabble.com>
References: <1441295060538-4711801.post@n4.nabble.com>
	<1441487658926-4711895.post@n4.nabble.com>
Message-ID: <20150909210216.GB13162@debian64>


Just for fun:

> colSums( outer( VAS, VAS, '<')) 
 [1] 3 3 0 3 7 8 8 0 3 0


On Sat, Sep 05, 2015 at 02:14:18PM -0700, Dan D wrote:
> # your data
> VAS<-c("Green","Green","Black","Green","White","Yellow","Yellow","Black","Green","Black")
> 
> # declare the new vector
> New_Vector<-numeric(length(VAS))
> 
> # brute force:
> New_Vector[VAS=="White"]<-1
> New_Vector[VAS=="Yellow"]<-2
> New_Vector[VAS=="Green"]<-3
> New_Vector[VAS=="Black"]<-4
> 
> # a little more subtle
> cols<-c("White","Yellow","Green","Black")
> for (i in 1:length(cols))  New_Vector[VAS==cols[i]]<-i
> 
> # and a general approach (that may give a different indexing, but can be
> used for any array)
> for (i in 1:length(unique(VAS))) New_Vector[VAS==unique(VAS)[i]]<-i
> cbind(1:length(unique(VAS)),unique(VAS)) # a decoding key for the color
> index
> 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Help-with-vectors-tp4711801p4711895.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Wed Sep  9 23:16:43 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 9 Sep 2015 14:16:43 -0700
Subject: [R] frequency tables
In-Reply-To: <CAGzwxb5ka8DuWi0nOqeH3-bf-68BGtqG_ybwWkZU1TgmvrBkPw@mail.gmail.com>
References: <CAGzwxb5ka8DuWi0nOqeH3-bf-68BGtqG_ybwWkZU1TgmvrBkPw@mail.gmail.com>
Message-ID: <6E154A82-9DCA-43F8-A56C-FF86E80E592F@comcast.net>


On Sep 9, 2015, at 11:52 AM, Nick Petschek wrote:

> Hi,
> 
> I want to run frequency tables for multiple categorical variables, ideally
> one in %, and the other as a count, and am unsure how to proceed. I would
> like my output to be the following:
> 
>       | favorable | unfavorable | neutral
> Q1   |      80%  |         10%    |  10% |
> Q2   |      70%  |         20%    |  10% |
> Q3   |      60%  |         10%    |  30% |
> 
> Then the same except counts instead of %.
> 
> I've been looking into the table function, but can't quite get it to work.
> Ideally I would love to export a large table to .csv.

dat <- read.delim(text="      | favorable | unfavorable | neutral
Q1   |      80%  |         10%    |  10% 
Q2   |      70%  |         20%    |  10% 
Q3   |      60%  |         10%    |  30% 
", sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

sim <- data.frame( 
Q1 = sample(names(dat)[-1], 100, rep=TRUE, prob=as.numeric(sub("%","", dat[-1][dat$X=="Q1", ]))/100), 
Q2 = sample(names(dat)[-1], 100, rep=TRUE, prob=as.numeric(sub("%","", dat[-1][dat$X=="Q2", ]))/100),
Q3 = sample(names(dat)[-1], 100, rep=TRUE, prob=as.numeric(sub("%","", dat[-1][dat$X=="Q3", ]))/100), 
stringsAsFactors=FALSE)

Looks to me that you want to read the help pages:

?table
?proptable


> sapply(sim,table)  # trivially give percentages but wouldn't if the row N's were not 100.
            Q1 Q2 Q3
favorable   85 76 55
neutral      4  9 37
unfavorable 11 15  8

> prop.table( sapply(sim,table), 2)
              Q1   Q2   Q3
favorable   0.85 0.76 0.55
neutral     0.04 0.09 0.37
unfavorable 0.11 0.15 0.08

And if you need them in row-acroos mode learn to use the `t`-function:

> 100* t( prop.table( sapply(sim,table), 2) )
   favorable neutral unfavorable
Q1        85       4          11
Q2        76       9          15
Q3        55      37           8

You also need to read "An Introduction to R" and learn to post examples in code.


> Any direction would be great!
> 
> 	[[alternative HTML version deleted]]

And you need to read the Posting Guide and configure your mail-client to 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From r.turner at auckland.ac.nz  Thu Sep 10 00:03:48 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 10 Sep 2015 10:03:48 +1200
Subject: [R] Help with vectors!
In-Reply-To: <20150909183051.GA13162@debian64>
References: <1441295060538-4711801.post@n4.nabble.com>
	<1441487658926-4711895.post@n4.nabble.com>
	<20150909183051.GA13162@debian64>
Message-ID: <55F0ACC4.9090207@auckland.ac.nz>

On 10/09/15 06:30, Frank Schwidom wrote:

> c( as.factor( VAS))
>
> On Sat, Sep 05, 2015 at 02:14:18PM -0700, Dan D wrote:
>> # your data
>> VAS<-c("Green","Green","Black","Green","White","Yellow","Yellow",
>>        "Black","Green","Black")

<SNIP>

Better:

     as.numeric(factor(VAS))


See fortune(185).

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From pdalgd at gmail.com  Thu Sep 10 11:07:35 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 10 Sep 2015 11:07:35 +0200
Subject: [R] Pb with date time sequence with period<1sec
In-Reply-To: <CAM_vjukKdYF=GHoB1Ypv7RWTZMnWkKFigCLatqeysdQt5CxyMg@mail.gmail.com>
References: <1441786774922-4712040.post@n4.nabble.com>
	<CAM_vjukKdYF=GHoB1Ypv7RWTZMnWkKFigCLatqeysdQt5CxyMg@mail.gmail.com>
Message-ID: <75A0D6D8-B506-4016-9651-8697CDF3C446@gmail.com>

Plus, current dates are an awful lot of seconds since 1970-01-01, so the relative error on second-scale differences is bigger than you might think:

> as.numeric(Sys.time())
[1] 1441874878

and since relative representation errors are of the order 1e-16, the corresponding absolute errors are about 1e-7.

(The examples given forgot to tell us what nom_fich is supposed to be, but I assume something relatively current was meant.)

-pd

On 09 Sep 2015, at 16:50 , Sarah Goslee <sarah.goslee at gmail.com> wrote:

> Looks like R FAQ 7.31 to me.
> 
> https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f
> 
> On Wed, Sep 9, 2015 at 4:19 AM, DE <david.ecotiere at cerema.fr> wrote:
>> Hi,
>> 
>> I'd like to create a date-time seq with a period of 0.05 s, over several
>> days.
>> 
>> # try :
>> start<-strptime(nom_fich,format="%y%m%d")
>> time<-seq(from=start, by=0.05, length.out = 86400*20*3)
>> print(as.POSIXlt(time[2])$sec)
>> # result is 0.04999995 and not 0.05 as expected
>> 
>> But If I am looking at the sequence, the seconds are not separated by 0.05,
>> but by something very close (0.04999995). Same pb if I want to add a
>> fraction of seconds to a date-time object :
>> 
>> # try :
>> start<-strptime(nom_fich,format="%y%m%d")
>> as.POSIXlt(start+0.05,origin="1970-01-01")$sec
>> # result is 0.04999995 and not 0.05 as expected
>> 
>> Any idea to solve this pb ?
>> 
>> Thank you in advance !
>> 
>> 
> 
> 
> -- 
> Sarah Goslee
> http://www.functionaldiversity.org
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Thomas.Chesney at nottingham.ac.uk  Thu Sep 10 15:11:24 2015
From: Thomas.Chesney at nottingham.ac.uk (Thomas Chesney)
Date: Thu, 10 Sep 2015 13:11:24 +0000
Subject: [R] Counting occurrences of a set of values
Message-ID: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>

Can anyone suggest a way of counting how frequently sets of values occurs in a data frame? Like table() only with sets.

So for a dataset:

V1, V2, V3
1, 2, 1
1, 3, 2
1, 2, 1
1, 1, 1

The output would be something like:

1,2,1: 2
1,3,2: 1
1,1,1: 1

Thank you,

Thomas Chesney



This message and any attachment are intended solely for the addressee
and may contain confidential information. If you have received this
message in error, please send it back to me, and immediately delete it. 

Please do not use, copy or disclose the information contained in this
message or in any attachment.  Any views or opinions expressed by the
author of this email do not necessarily reflect the views of the
University of Nottingham.

This message has been checked for viruses but the contents of an
attachment may still contain software viruses which could damage your
computer system, you are advised to perform your own checks. Email
communications with the University of Nottingham may be monitored as
permitted by UK legislation.


From murdoch.duncan at gmail.com  Thu Sep 10 15:28:13 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 10 Sep 2015 09:28:13 -0400
Subject: [R] Counting occurrences of a set of values
In-Reply-To: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>
References: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>
Message-ID: <55F1856D.6090208@gmail.com>

On 10/09/2015 9:11 AM, Thomas Chesney wrote:
> Can anyone suggest a way of counting how frequently sets of values occurs in a data frame? Like table() only with sets.

Do you want 1,2,1 to be the same as 1,1,2, or different?  What about
1,2,2?  For sets, those are all the same, but for most purposes, they
aren't.  If you really want to keep the ordering, then table() does the
counting you want, it just returns it in an ugly format.

Duncan Murdoch


> 
> So for a dataset:
> 
> V1, V2, V3
> 1, 2, 1
> 1, 3, 2
> 1, 2, 1
> 1, 1, 1
> 
> The output would be something like:
> 
> 1,2,1: 2
> 1,3,2: 1
> 1,1,1: 1
> 
> Thank you,
> 
> Thomas Chesney
> 
> 
> 
> This message and any attachment are intended solely for the addressee
> and may contain confidential information. If you have received this
> message in error, please send it back to me, and immediately delete it. 
> 
> Please do not use, copy or disclose the information contained in this
> message or in any attachment.  Any views or opinions expressed by the
> author of this email do not necessarily reflect the views of the
> University of Nottingham.
> 
> This message has been checked for viruses but the contents of an
> attachment may still contain software viruses which could damage your
> computer system, you are advised to perform your own checks. Email
> communications with the University of Nottingham may be monitored as
> permitted by UK legislation.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Thu Sep 10 15:29:49 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Thu, 10 Sep 2015 13:29:49 +0000
Subject: [R] Counting occurrences of a set of values
In-Reply-To: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>
References: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC8A4D76A@FHSDB4H16-2.csu.mcmaster.ca>

Dear Thomas,

How about this?

> table(apply(Data, 1, paste, collapse=","))

1,1,1 1,2,1 1,3,2 
    1     2     1

I hope this helps,
 John

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Thomas
> Chesney
> Sent: September 10, 2015 9:11 AM
> To: r-help at r-project.org
> Subject: [R] Counting occurrences of a set of values
> 
> Can anyone suggest a way of counting how frequently sets of values occurs in a
> data frame? Like table() only with sets.
> 
> So for a dataset:
> 
> V1, V2, V3
> 1, 2, 1
> 1, 3, 2
> 1, 2, 1
> 1, 1, 1
> 
> The output would be something like:
> 
> 1,2,1: 2
> 1,3,2: 1
> 1,1,1: 1
> 
> Thank you,
> 
> Thomas Chesney
> 
> 
> 
> This message and any attachment are intended solely for the addressee and may
> contain confidential information. If you have received this message in error,
> please send it back to me, and immediately delete it.
> 
> Please do not use, copy or disclose the information contained in this message or
> in any attachment.  Any views or opinions expressed by the author of this email
> do not necessarily reflect the views of the University of Nottingham.
> 
> This message has been checked for viruses but the contents of an attachment
> may still contain software viruses which could damage your computer system,
> you are advised to perform your own checks. Email communications with the
> University of Nottingham may be monitored as permitted by UK legislation.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From thierry.onkelinx at inbo.be  Thu Sep 10 15:30:45 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 10 Sep 2015 15:30:45 +0200
Subject: [R] Counting occurrences of a set of values
In-Reply-To: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>
References: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>
Message-ID: <CAJuCY5x+Ab=P_Oj4xcOoBpqLHV4cR4LfWC0v9qjT_2sHxcaYaA@mail.gmail.com>

Have a look at the dplyr package

library(dplyr)
n <- 1000
data_frame(
  V1 = sample(0:1, n, replace = TRUE),
  V2 = sample(0:1, n, replace = TRUE),
  V3 = sample(0:1, n, replace = TRUE)
) %>%
  group_by(V1, V2, V3) %>%
  mutate(
    Freq = n()
  )


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-09-10 15:11 GMT+02:00 Thomas Chesney <Thomas.Chesney at nottingham.ac.uk>:

> Can anyone suggest a way of counting how frequently sets of values occurs
> in a data frame? Like table() only with sets.
>
> So for a dataset:
>
> V1, V2, V3
> 1, 2, 1
> 1, 3, 2
> 1, 2, 1
> 1, 1, 1
>
> The output would be something like:
>
> 1,2,1: 2
> 1,3,2: 1
> 1,1,1: 1
>
> Thank you,
>
> Thomas Chesney
>
>
>
> This message and any attachment are intended solely for the addressee
> and may contain confidential information. If you have received this
> message in error, please send it back to me, and immediately delete it.
>
> Please do not use, copy or disclose the information contained in this
> message or in any attachment.  Any views or opinions expressed by the
> author of this email do not necessarily reflect the views of the
> University of Nottingham.
>
> This message has been checked for viruses but the contents of an
> attachment may still contain software viruses which could damage your
> computer system, you are advised to perform your own checks. Email
> communications with the University of Nottingham may be monitored as
> permitted by UK legislation.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From schwidom at gmx.net  Thu Sep 10 19:35:34 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Thu, 10 Sep 2015 19:35:34 +0200
Subject: [R] Counting occurrences of a set of values
In-Reply-To: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>
References: <15F7810E-DAD9-459D-A510-F5F01AD286A0@exmail.nottingham.ac.uk>
Message-ID: <20150910173534.GA2953@debian64>



df <- data.frame( V1= 1, V2= c( 2, 3, 2, 1), V3= c( 1, 2, 1, 1))
dfO <- df[ do.call( order, df), ]
dfOD <- duplicated( dfO)
dfODTrigger <- ! c( dfOD[-1], FALSE)
dfOCounts <- diff( c( 0, which( dfODTrigger)))
cbind( dfO[ dfODTrigger, ], dfOCounts)

  V1 V2 V3 dfOCounts
4  1  1  1         1
3  1  2  1         2
2  1  3  2         1

Regards


On Thu, Sep 10, 2015 at 01:11:24PM +0000, Thomas Chesney wrote:
> Can anyone suggest a way of counting how frequently sets of values occurs in a data frame? Like table() only with sets.
> 
> So for a dataset:
> 
> V1, V2, V3
> 1, 2, 1
> 1, 3, 2
> 1, 2, 1
> 1, 1, 1
> 
> The output would be something like:
> 
> 1,2,1: 2
> 1,3,2: 1
> 1,1,1: 1
> 
> Thank you,
> 
> Thomas Chesney
> 
> 
> 
> This message and any attachment are intended solely for the addressee
> and may contain confidential information. If you have received this
> message in error, please send it back to me, and immediately delete it. 
> 
> Please do not use, copy or disclose the information contained in this
> message or in any attachment.  Any views or opinions expressed by the
> author of this email do not necessarily reflect the views of the
> University of Nottingham.
> 
> This message has been checked for viruses but the contents of an
> attachment may still contain software viruses which could damage your
> computer system, you are advised to perform your own checks. Email
> communications with the University of Nottingham may be monitored as
> permitted by UK legislation.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ragia11 at hotmail.com  Thu Sep 10 19:40:17 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Thu, 10 Sep 2015 20:40:17 +0300
Subject: [R] kendall tau distance
Message-ID: <DUB125-W94DB648B732F8E71C200C2B3510@phx.gbl>

Dear group
how to calculate ?kendall tau distance according to ?Kendall_tau_distance at wikipedia

?<a href="https&#58;&#47;&#47;en.wikipedia.org&#47;wiki&#47;Kendall_tau_distance" target="_blank" class="newlyinsertedlink">https&#58;&#47;&#47;en.wikipedia.org&#47;wiki&#47;Kendall_tau_distance</a>?


thanks in advance
Ragia 		 	   		  

From aldi at wustl.edu  Thu Sep 10 19:46:22 2015
From: aldi at wustl.edu (aldi)
Date: Thu, 10 Sep 2015 12:46:22 -0500
Subject: [R] how to split row elements [1] and [2] of a string variable A
 via srtsplit and sapply
Message-ID: <55F1C1EE.4090308@wustl.edu>

Hi,
I have a data.frame x1, of which a variable A needs to be split by 
element 1 and element 2 where separator is ":". Sometimes could be three 
elements in A, but I do not need the third element.

Since R does not have a SCAN function as in SAS, C=scan(A,1,":"); 
D=scan(A,2,":");
I am using a combination of strsplit and sapply. If I do not use the 
index [i] then R captures the full vector . Instead I need row by row 
capturing the first and the second element and from them create two new 
variables C and D.
Right now as is somehow in the loop i C is captured correctly, but D is 
missing because the variables AA does not have it. Any suggestions? 
Thank you in advance, Aldi

A          B
1:29439275 0.46773514
5:85928892 0.81283052
10:128341232 0.09332543
1:106024283:ID 0.36307805
3:62707519 0.42657952
2:80464120 0.89125094

x1<-read.table(file='./test.txt',head=T,sep='\t')
x1$A <- as.character(x1$A)

for(i in 1:length(x1$A)){

x1$AA[i] <- as.numeric(unlist(strsplit(x1$A[i],':')))

x1$C[i] <- sapply(x1$AA[i],function(x)x[1])
x1$D[i] <- sapply(x1$AA[i],function(x)x[2])
}

x1



 > x1
                A          B AA  C  D
1     1:29439275 0.46773514  1  1 NA
2     5:85928892 0.81283052  5  5 NA
3   10:128341232 0.09332543 10 10 NA
4 1:106024283:ID 0.36307805  1  1 NA
5     3:62707519 0.42657952  3  3 NA
6     2:80464120 0.89125094  2  2 NA


-- 


	[[alternative HTML version deleted]]


From jholtman at gmail.com  Thu Sep 10 20:05:01 2015
From: jholtman at gmail.com (jim holtman)
Date: Thu, 10 Sep 2015 14:05:01 -0400
Subject: [R] how to split row elements [1] and [2] of a string variable
 A via srtsplit and sapply
In-Reply-To: <55F1C1EE.4090308@wustl.edu>
References: <55F1C1EE.4090308@wustl.edu>
Message-ID: <CAAxdm-4Mu70QS1fTowW+EgaRPUGc0rTs63eAuNZCjh+ivfAn5A@mail.gmail.com>

try this:


> x <- read.table(text = "A          B
+  1:29439275 0.46773514
+  5:85928892 0.81283052
+  10:128341232 0.09332543
+  1:106024283:ID 0.36307805
+  3:62707519 0.42657952
+  2:80464120 0.89125094", header = TRUE, as.is = TRUE)
>
> temp <- strsplit(x$A, ":")
> x$C <- sapply(temp, '[[', 1)
> x$D <- sapply(temp, '[[', 2)
>
> x
               A          B  C         D
1     1:29439275 0.46773514  1  29439275
2     5:85928892 0.81283052  5  85928892
3   10:128341232 0.09332543 10 128341232
4 1:106024283:ID 0.36307805  1 106024283
5     3:62707519 0.42657952  3  62707519
6     2:80464120 0.89125094  2  80464120




Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Sep 10, 2015 at 1:46 PM, aldi <aldi at wustl.edu> wrote:

> Hi,
> I have a data.frame x1, of which a variable A needs to be split by
> element 1 and element 2 where separator is ":". Sometimes could be three
> elements in A, but I do not need the third element.
>
> Since R does not have a SCAN function as in SAS, C=scan(A,1,":");
> D=scan(A,2,":");
> I am using a combination of strsplit and sapply. If I do not use the
> index [i] then R captures the full vector . Instead I need row by row
> capturing the first and the second element and from them create two new
> variables C and D.
> Right now as is somehow in the loop i C is captured correctly, but D is
> missing because the variables AA does not have it. Any suggestions?
> Thank you in advance, Aldi
>
> A          B
> 1:29439275 0.46773514
> 5:85928892 0.81283052
> 10:128341232 0.09332543
> 1:106024283:ID 0.36307805
> 3:62707519 0.42657952
> 2:80464120 0.89125094
>
> x1<-read.table(file='./test.txt',head=T,sep='\t')
> x1$A <- as.character(x1$A)
>
> for(i in 1:length(x1$A)){
>
> x1$AA[i] <- as.numeric(unlist(strsplit(x1$A[i],':')))
>
> x1$C[i] <- sapply(x1$AA[i],function(x)x[1])
> x1$D[i] <- sapply(x1$AA[i],function(x)x[2])
> }
>
> x1
>
>
>
>  > x1
>                 A          B AA  C  D
> 1     1:29439275 0.46773514  1  1 NA
> 2     5:85928892 0.81283052  5  5 NA
> 3   10:128341232 0.09332543 10 10 NA
> 4 1:106024283:ID 0.36307805  1  1 NA
> 5     3:62707519 0.42657952  3  3 NA
> 6     2:80464120 0.89125094  2  2 NA
>
>
> --
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ajdamico at gmail.com  Thu Sep 10 20:10:18 2015
From: ajdamico at gmail.com (Anthony Damico)
Date: Thu, 10 Sep 2015 14:10:18 -0400
Subject: [R] r function idea: minimize() to turn reproducible_example.R into
	minimal_reproducible_example.R
Message-ID: <CAOwvMDxYNaMf5gBQGed2buqsoN0RcQKRsAR7GBYnnv+a5FbJBg@mail.gmail.com>

just going to throw this idea out there in case it's something that anyone
wants to pursue: if i have an R script and i'm hitting some unexpected
behavior, there should be some way to remove extraneous objects and
manipulations that never touch the line that i'm trying to reproduce.
automatically stepping through the code and removing things that never
affect the final line seems (difficult but) possible.  so if my_example.R
looks like this code and i didn't understand why i was hitting an error at
the third line..

    x <- mtcars
    y <- mean( mtcars$mpg )
    mean( x[ , "hello" ] )

..the function i am envisioning would automatically remove the `y <- mean(
mtcars$mpg )` because that object and all subsequent objects do not affect
the error resulting from the third line.  in other words, pointing this
minimize() function to the error..

    minimize( "my_example.R" , 'Error in `[.data.frame`(x, , "hello") :
undefined columns selected' )

..would find that the error happens on the third line, then follow things
backward and remove any command that does not touch the line that results
in the error.  so my reproducible example was three lines, but the minimal
reproducible example became two lines.

i understand it might be impossible to automate all of the minimizing, but
i think there might be enough low-hanging fruit here that this might be a
quick and useful debugging tool for those of us trying to create
easier-to-reproduce code for members of this list.

thanks!

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Sep 10 20:35:31 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Sep 2015 11:35:31 -0700
Subject: [R] how to split row elements [1] and [2] of a string variable
 A via srtsplit and sapply
In-Reply-To: <CAAxdm-4Mu70QS1fTowW+EgaRPUGc0rTs63eAuNZCjh+ivfAn5A@mail.gmail.com>
References: <55F1C1EE.4090308@wustl.edu>
	<CAAxdm-4Mu70QS1fTowW+EgaRPUGc0rTs63eAuNZCjh+ivfAn5A@mail.gmail.com>
Message-ID: <CAGxFJbRGd-nPpzne7uBJh5R1i04PqnBphnMfp+pNqQiyVwCycA@mail.gmail.com>

...
Alternatively, you can avoid the looping (i.e. sapply) altogether by:

do.call(rbind,strsplit(x[[1]],":"))[,-3]


     [,1] [,2]
[1,] "1"  "29439275"
[2,] "5"  "85928892"
[3,] "10" "128341232"
[4,] "1"  "106024283"
[5,] "3"  "62707519"
[6,] "2"  "80464120"

These can then be added to the existing frame, converted to numeric, etc.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Sep 10, 2015 at 11:05 AM, jim holtman <jholtman at gmail.com> wrote:
> try this:
>
>
>> x <- read.table(text = "A          B
> +  1:29439275 0.46773514
> +  5:85928892 0.81283052
> +  10:128341232 0.09332543
> +  1:106024283:ID 0.36307805
> +  3:62707519 0.42657952
> +  2:80464120 0.89125094", header = TRUE, as.is = TRUE)
>>
>> temp <- strsplit(x$A, ":")
>> x$C <- sapply(temp, '[[', 1)
>> x$D <- sapply(temp, '[[', 2)
>>
>> x
>                A          B  C         D
> 1     1:29439275 0.46773514  1  29439275
> 2     5:85928892 0.81283052  5  85928892
> 3   10:128341232 0.09332543 10 128341232
> 4 1:106024283:ID 0.36307805  1 106024283
> 5     3:62707519 0.42657952  3  62707519
> 6     2:80464120 0.89125094  2  80464120
>
>
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Thu, Sep 10, 2015 at 1:46 PM, aldi <aldi at wustl.edu> wrote:
>
>> Hi,
>> I have a data.frame x1, of which a variable A needs to be split by
>> element 1 and element 2 where separator is ":". Sometimes could be three
>> elements in A, but I do not need the third element.
>>
>> Since R does not have a SCAN function as in SAS, C=scan(A,1,":");
>> D=scan(A,2,":");
>> I am using a combination of strsplit and sapply. If I do not use the
>> index [i] then R captures the full vector . Instead I need row by row
>> capturing the first and the second element and from them create two new
>> variables C and D.
>> Right now as is somehow in the loop i C is captured correctly, but D is
>> missing because the variables AA does not have it. Any suggestions?
>> Thank you in advance, Aldi
>>
>> A          B
>> 1:29439275 0.46773514
>> 5:85928892 0.81283052
>> 10:128341232 0.09332543
>> 1:106024283:ID 0.36307805
>> 3:62707519 0.42657952
>> 2:80464120 0.89125094
>>
>> x1<-read.table(file='./test.txt',head=T,sep='\t')
>> x1$A <- as.character(x1$A)
>>
>> for(i in 1:length(x1$A)){
>>
>> x1$AA[i] <- as.numeric(unlist(strsplit(x1$A[i],':')))
>>
>> x1$C[i] <- sapply(x1$AA[i],function(x)x[1])
>> x1$D[i] <- sapply(x1$AA[i],function(x)x[2])
>> }
>>
>> x1
>>
>>
>>
>>  > x1
>>                 A          B AA  C  D
>> 1     1:29439275 0.46773514  1  1 NA
>> 2     5:85928892 0.81283052  5  5 NA
>> 3   10:128341232 0.09332543 10 10 NA
>> 4 1:106024283:ID 0.36307805  1  1 NA
>> 5     3:62707519 0.42657952  3  3 NA
>> 6     2:80464120 0.89125094  2  2 NA
>>
>>
>> --
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From paul.bivand at gmail.com  Thu Sep 10 21:36:09 2015
From: paul.bivand at gmail.com (Paul Bivand)
Date: Thu, 10 Sep 2015 20:36:09 +0100
Subject: [R] ggplot2 will not install after system upgrade
In-Reply-To: <1441317188.3562.15.camel@trefftzs.org>
References: <1441306999.3562.3.camel@trefftzs.org>
	<CA+vqiLETyCwDHs+XGutSgz3QH5KAfKt0PTEYkgSrPZL2UyNGFg@mail.gmail.com>
	<1441317188.3562.15.camel@trefftzs.org>
Message-ID: <CAC=KSNgiYiW4LfZxPfUTMChEJxZMjs7tKrmJiv3hbjYVuRYOXw@mail.gmail.com>

This is most likely the "stringi" dependency, which is new.

Follow the links from the CRAN page for "stringi" and you may find some
guidance.

I initially had the same problem with my Mageia install, but it's sorted
now.

On 3 September 2015 at 22:53, Jeff Trefftzs <jeff at trefftzs.org> wrote:

> On Thu, 2015-09-03 at 16:47 -0400, Ista Zahn wrote:
> > Hi Jeff,
> > Your chances of getting a useful response will increase if you
> > provide
> > some additional information. For example, which version of R? Which
> > version of ggplot2?
>
>
> Sorry.  R was version 3.2.1
> ggplot2 1.0.1
>
> > What sequence of commands produces the error?
>
> install.packages("ggplot2") (or various equivalents while in R-Studio
>
> > What
> > _exactly_ does the error message say?
>
> I was working on my laptop, where I didn't have email enabled, so I was
> unable to cut & paste all the output.  The last bit of the error
> messages boiled down to "unable to find libicui18n.so.50.  No such file
> or directory"
>
> Does
> > update.packages(ask=FALSE, checkBuilt=TRUE)
> > install.packages("ggplot2")
>
> I hadn't tried that.
>
> Follow-up:  On the laptop I downgraded to R-3.1.3 and things worked
> again.  The various error messages I got were confusing.  When I tried
> to install ggplot2 from the first US mirror the https server at
> Berkeley), it told me "gplot2 not available for R 3.2.1".  When I tried
> one of the other servers (e.g., other Berkeley server, or the UCLA
> server) it would download, and come to grief with the libicu message.
>
> But downgrading to R 3.1.3 seems to have cured things.
>
> I'm still baffled, however, since I'm writing this on my desktop
> computer which has R version 3.2.1 and a successful install of ggplot2
> -1.0.1 actually working.
>
> --
> Jeff Trefftzs
> http://www.trefftzs.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mazzola at jimmy.harvard.edu  Thu Sep 10 22:51:29 2015
From: mazzola at jimmy.harvard.edu (Emanuele Mazzola)
Date: Thu, 10 Sep 2015 16:51:29 -0400
Subject: [R] help with svychisq
Message-ID: <7F2A3D42-4A7D-49C3-9AF1-E5CA037BE68F@jimmy.harvard.edu>

Hello,

I?m having a weird issue with the function ?svychisq? in package ?survey", which would be very helpful for me in this case.

I?m tabulating age categories (a factor variable subdivided into 4 categories: [18,25), [25, 45), [45,65), [65, 85) ) with respect to ethnicity/race (another factor variable subdivided into ?hispanic white?, ?non hispanic black?, ?hispanic black?).

I?m perfectly able to get to the ?svytable" object, which looks like this

> svytable(~age+ETN, design=sv1)
         ETN
age       hisp black hispanic white non hisp black
  [18,25)   26.97019      798.87444      183.61834
  [25,45)  145.19650     4783.47678      854.82748
  [45,65)  104.83682     2537.15021      595.04924
  [65,85]    0.00000        0.00000        0.00000

 Since it has last row equal to 0 (which would give me troubles with the corresponding chi-square p-value), I try to get rid of it by using

> svytable(~factor(age)+ETN, design=sv1)
           ETN
factor(age) hisp black hispanic white non hisp black
    [18,25)   26.97019      798.87444      183.61834
    [25,45)  145.19650     4783.47678      854.82748
    [45,65)  104.83682     2537.15021      595.04924

which exactly responds to what I?m looking for and to what I?m expecting.

The design is built by using

sv1 = svydesign(ids=~factor(age)+ETN, weights=~WTFA.n, data=totfor)

Now, if I would like to evaluate the corresponding weighted chi squared test, I use

svychisq(~factor(age)+ETN, design=sv1)

but here?s what I get from R: 

> svychisq(~factor(age)+ETN, design=sv1)
Error in `[.data.frame`(design$variables, , as.character(rows)) : 
  undefined columns selected

Maybe it is a stupid question but I really can?t figure out where the error is.

Could you please help me with this?
Thanks in advance for any information you will provide me with!

Emanuele

***********************************************************************
Emanuele Mazzola, Ph.D.
Department of Biostatistics & Computational Biology
Dana-Farber Cancer Institute
450 Brookline Ave 
Mail Location: LC1056 
Office Location: Longwood Center, Room 1056
Boston, MA 02215
Office phone 617-582-7614
Fax 617-632-2516


	[[alternative HTML version deleted]]


From ajdamico at gmail.com  Thu Sep 10 23:37:47 2015
From: ajdamico at gmail.com (Anthony Damico)
Date: Thu, 10 Sep 2015 17:37:47 -0400
Subject: [R] help with svychisq
In-Reply-To: <7F2A3D42-4A7D-49C3-9AF1-E5CA037BE68F@jimmy.harvard.edu>
References: <7F2A3D42-4A7D-49C3-9AF1-E5CA037BE68F@jimmy.harvard.edu>
Message-ID: <CAOwvMDxiZpZZeiWtcc_XzVdV+bLGG4MhdT2yi75BCdx-LyrNVw@mail.gmail.com>

could you try this, and then not use factor(age) elsewhere?

sv1 <- update( sv1 , age = factor( age ) )

if that doesn't work, is it possible for you to share a reproducible
example? thanks


On Thu, Sep 10, 2015 at 4:51 PM, Emanuele Mazzola <mazzola at jimmy.harvard.edu
> wrote:

> Hello,
>
> I?m having a weird issue with the function ?svychisq? in package ?survey",
> which would be very helpful for me in this case.
>
> I?m tabulating age categories (a factor variable subdivided into 4
> categories: [18,25), [25, 45), [45,65), [65, 85) ) with respect to
> ethnicity/race (another factor variable subdivided into ?hispanic white?,
> ?non hispanic black?, ?hispanic black?).
>
> I?m perfectly able to get to the ?svytable" object, which looks like this
>
> > svytable(~age+ETN, design=sv1)
>          ETN
> age       hisp black hispanic white non hisp black
>   [18,25)   26.97019      798.87444      183.61834
>   [25,45)  145.19650     4783.47678      854.82748
>   [45,65)  104.83682     2537.15021      595.04924
>   [65,85]    0.00000        0.00000        0.00000
>
>  Since it has last row equal to 0 (which would give me troubles with the
> corresponding chi-square p-value), I try to get rid of it by using
>
> > svytable(~factor(age)+ETN, design=sv1)
>            ETN
> factor(age) hisp black hispanic white non hisp black
>     [18,25)   26.97019      798.87444      183.61834
>     [25,45)  145.19650     4783.47678      854.82748
>     [45,65)  104.83682     2537.15021      595.04924
>
> which exactly responds to what I?m looking for and to what I?m expecting.
>
> The design is built by using
>
> sv1 = svydesign(ids=~factor(age)+ETN, weights=~WTFA.n, data=totfor)
>
> Now, if I would like to evaluate the corresponding weighted chi squared
> test, I use
>
> svychisq(~factor(age)+ETN, design=sv1)
>
> but here?s what I get from R:
>
> > svychisq(~factor(age)+ETN, design=sv1)
> Error in `[.data.frame`(design$variables, , as.character(rows)) :
>   undefined columns selected
>
> Maybe it is a stupid question but I really can?t figure out where the
> error is.
>
> Could you please help me with this?
> Thanks in advance for any information you will provide me with!
>
> Emanuele
>
> ***********************************************************************
> Emanuele Mazzola, Ph.D.
> Department of Biostatistics & Computational Biology
> Dana-Farber Cancer Institute
> 450 Brookline Ave
> Mail Location: LC1056
> Office Location: Longwood Center, Room 1056
> Boston, MA 02215
> Office phone 617-582-7614
> Fax 617-632-2516
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From shouro at gmail.com  Fri Sep 11 00:28:19 2015
From: shouro at gmail.com (Shouro Dasgupta)
Date: Fri, 11 Sep 2015 00:28:19 +0200
Subject: [R] Help with Binning Data
Message-ID: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>

Dear all,

I have 3-hourly temperature data from 1970-2010 for 122 cities in the US. I
would like to bin this data by city-year-week. My idea is if the
temperature for a particular city in a given week falls within a given
range (-17.78 & -12.22), (-12.22 & -6.67), ... (37.78 & 43.33), then the
corresponding bin would have a value of 1 and 0 otherwise.

The data looks like this. Basically, I need to generate a dummy variable
for each temperature range. Any help will be greatly appreciated.

tmp2<- dput(head(tmp1,10))
> structure(list(yearday = c(1970001L, 1970001L, 1970001L, 1970001L,
> 1970001L, 1970001L, 1970001L, 1970001L, 1970001L, 1970001L),
>     City = structure(1:10, .Label = c("AKRON", "ALBANY", "ALBUQUERQUE",
>     "ALLENTOWN", "ATLANTA", "AUSTIN", "BALTIMORE", "BATON ROUGE",
>     "BERKELEY", "BIRMINGHAM", "BOISE", "BOSTON", "BRIDGEPORT",
>     "BUFFALO", "CAMBRIDGE", "CAMDEN", "CANTON", "CHARLOTTE",
>     "CHATTANOOGA", "CHICAGO", "CINCINNATI", "CLEVELAND", "COLORADO
> SPRINGS",
>     "COLUMBUS", "CORPUS CHRISTI", "DALLAS", "DAYTON", "DENVER",
>     "DES MOINES", "DETROIT", "DULUTH", "EL PASO", "ELIZABETH",
>     "ERIE", "EVANSVILLE", "FALL RIVER", "FLINT", "FORT WAYNE",
>     "FRESNO", "FT WORTH", "GARY", "GLENDALE", "GRAND RAPIDS",
>     "HARTFORD", "HONOLULU", "HOUSTON", "INDIANAPOLIS", "JACKSONVILLE",
>     "JERSEY CITY", "KANSAS CITY", "KANSAS ITY", "KNOXVILLE",
>     "Lansing ", "LAS VEGAS", "LEXINGTON", "LINCOLN", "LITTLE ROCK",
>     "LONG BEACH", "LOS ANGELES", "LOUISVILLE", "LOWELL", "LYNN",
>     "MADISON", "MEMPHIS", "MIAMI", "MILWAUKEE", "MINNEAPOLIS",
>     "MOBILE", "MONTGOMERY", "NASHVILLE", "NEW BEDFORD", "NEW HAVEN",
>     "NEW ORLEANS", "NEW YORK CITY", "NEWARK", "NORFOLK", "OAKLAND",
>     "OGDEN", "OKLAHOMA CITY", "OMAHA", "PASADENA", "PATERSON",
>     "PEORIA", "PHILADELPHIA", "PHOENIX", "PITTSBURG", "PORTLAND",
>     "PROVIDENCE", "PUEBLO", "READING", "RICHMOND", "ROCHESTER",
>     "ROCKFORD", "SACRAMENTO", "SALT LAKE CITY", "SAN ANTONIO",
>     "SAN CRUZ", "SAN DIEGO", "SAN FRANCISCO", "SAN JOSE", "SAVANNAH",
>     "SCHENECTADY", "SCRANTON", "SEATTLE", "SHREVEPORT", "SOMERVILLE",
>     "SOUTH BEND", "SPOKANE", "SPRINGFIELD", "ST LOUIS", "ST PAUL",
>     "ST PETERSBURG", "SYRACUSE", "TACOMA", "TAMPA", "TOLEDO",
>     "TRENTON", "TUCSON", "TULSA", "UTICA", "WASHINGTON", "WATERBURY",
>     "WICHITA", "WILMINGTON", "WORCESTER", "YONKERS", "YOUNGSTOWN"
>     ), class = "factor"), cell_number = c(17379L, 17027L, 19514L,
>     17745L, 20256L, 21323L, 18104L, 21329L, 18779L, 20254L),
>     longitude = c(-81.519005, -73.756232, -106.609991, -75.490183,
>     -84.387982, -97.743061, -76.612189, -91.14032, -121.635963,
>     -86.80249), latitude = c(41.081445, 42.652579, 35.110703,
>     40.608431, 33.748995, 30.267153, 39.290385, 30.458283, 37.871744,
>     33.520661), State = structure(c(29L, 28L, 27L, 32L, 10L,
>     35L, 19L, 17L, 4L, 1L), .Label = c(" ALA", " ARIZ", " ARK",
>     " CAL", " COLO", " CONN", " DC", " DEL", " FLA", " GA", " HAWAII",
>     " ILL", " IND", " IOWA", " KANS", " KY", " LA", " MASS",
>     " MD", " MICH", " MINN", " MO", " NC", " NEBR", " NEV", " NJ",
>     " NM", " NY", " OHIO", " OKLA", " ORE", " PA", " RI", " TENN",
>     " TEX", " UTAH", " VA", " WASH", " WIS", "CAL", "CONN", "IDAH",
>     "KY", "MASS"), class = "factor"), avsft = c(-7.81, -16.06,
>     -7.71999999999997, -1.88999999999999, 2.90000000000003, 5.12,
>     -5.02999999999997, 9.33000000000004, 15.08, 2.89000000000004
>     ), year = c(1970L, 1970L, 1970L, 1970L, 1970L, 1970L, 1970L,
>     1970L, 1970L, 1970L), day = c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L), hour = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
>     0L), yearweek = c(197001L, 197001L, 197001L, 197001L, 197001L,
>     197001L, 197001L, 197001L, 197001L, 197001L), week = c(1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("yearday",
> "City", "cell_number", "longitude", "latitude", "State", "avsft",
> "year", "day", "hour", "yearweek", "week"), row.names = c(NA,
> 10L), class = "data.frame")


Sincerely,

Shouro

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Sep 11 00:33:54 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Sep 2015 15:33:54 -0700
Subject: [R] Help with Binning Data
In-Reply-To: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
References: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
Message-ID: <CAGxFJbRMJg0Kqxw8QEEmxDV=gJU0MrvYrK5hFOqeWh93+LHQ7w@mail.gmail.com>

1. Posting in HTML largely negated your ability to provide data
through dput(). Folow he posting guide and post in PLAIN TEXT only,
please.

2. See ?cut  . I think this will at least get you started.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Sep 10, 2015 at 3:28 PM, Shouro Dasgupta <shouro at gmail.com> wrote:
> Dear all,
>
> I have 3-hourly temperature data from 1970-2010 for 122 cities in the US. I
> would like to bin this data by city-year-week. My idea is if the
> temperature for a particular city in a given week falls within a given
> range (-17.78 & -12.22), (-12.22 & -6.67), ... (37.78 & 43.33), then the
> corresponding bin would have a value of 1 and 0 otherwise.
>
> The data looks like this. Basically, I need to generate a dummy variable
> for each temperature range. Any help will be greatly appreciated.
>
> tmp2<- dput(head(tmp1,10))
>> structure(list(yearday = c(1970001L, 1970001L, 1970001L, 1970001L,
>> 1970001L, 1970001L, 1970001L, 1970001L, 1970001L, 1970001L),
>>     City = structure(1:10, .Label = c("AKRON", "ALBANY", "ALBUQUERQUE",
>>     "ALLENTOWN", "ATLANTA", "AUSTIN", "BALTIMORE", "BATON ROUGE",
>>     "BERKELEY", "BIRMINGHAM", "BOISE", "BOSTON", "BRIDGEPORT",
>>     "BUFFALO", "CAMBRIDGE", "CAMDEN", "CANTON", "CHARLOTTE",
>>     "CHATTANOOGA", "CHICAGO", "CINCINNATI", "CLEVELAND", "COLORADO
>> SPRINGS",
>>     "COLUMBUS", "CORPUS CHRISTI", "DALLAS", "DAYTON", "DENVER",
>>     "DES MOINES", "DETROIT", "DULUTH", "EL PASO", "ELIZABETH",
>>     "ERIE", "EVANSVILLE", "FALL RIVER", "FLINT", "FORT WAYNE",
>>     "FRESNO", "FT WORTH", "GARY", "GLENDALE", "GRAND RAPIDS",
>>     "HARTFORD", "HONOLULU", "HOUSTON", "INDIANAPOLIS", "JACKSONVILLE",
>>     "JERSEY CITY", "KANSAS CITY", "KANSAS ITY", "KNOXVILLE",
>>     "Lansing ", "LAS VEGAS", "LEXINGTON", "LINCOLN", "LITTLE ROCK",
>>     "LONG BEACH", "LOS ANGELES", "LOUISVILLE", "LOWELL", "LYNN",
>>     "MADISON", "MEMPHIS", "MIAMI", "MILWAUKEE", "MINNEAPOLIS",
>>     "MOBILE", "MONTGOMERY", "NASHVILLE", "NEW BEDFORD", "NEW HAVEN",
>>     "NEW ORLEANS", "NEW YORK CITY", "NEWARK", "NORFOLK", "OAKLAND",
>>     "OGDEN", "OKLAHOMA CITY", "OMAHA", "PASADENA", "PATERSON",
>>     "PEORIA", "PHILADELPHIA", "PHOENIX", "PITTSBURG", "PORTLAND",
>>     "PROVIDENCE", "PUEBLO", "READING", "RICHMOND", "ROCHESTER",
>>     "ROCKFORD", "SACRAMENTO", "SALT LAKE CITY", "SAN ANTONIO",
>>     "SAN CRUZ", "SAN DIEGO", "SAN FRANCISCO", "SAN JOSE", "SAVANNAH",
>>     "SCHENECTADY", "SCRANTON", "SEATTLE", "SHREVEPORT", "SOMERVILLE",
>>     "SOUTH BEND", "SPOKANE", "SPRINGFIELD", "ST LOUIS", "ST PAUL",
>>     "ST PETERSBURG", "SYRACUSE", "TACOMA", "TAMPA", "TOLEDO",
>>     "TRENTON", "TUCSON", "TULSA", "UTICA", "WASHINGTON", "WATERBURY",
>>     "WICHITA", "WILMINGTON", "WORCESTER", "YONKERS", "YOUNGSTOWN"
>>     ), class = "factor"), cell_number = c(17379L, 17027L, 19514L,
>>     17745L, 20256L, 21323L, 18104L, 21329L, 18779L, 20254L),
>>     longitude = c(-81.519005, -73.756232, -106.609991, -75.490183,
>>     -84.387982, -97.743061, -76.612189, -91.14032, -121.635963,
>>     -86.80249), latitude = c(41.081445, 42.652579, 35.110703,
>>     40.608431, 33.748995, 30.267153, 39.290385, 30.458283, 37.871744,
>>     33.520661), State = structure(c(29L, 28L, 27L, 32L, 10L,
>>     35L, 19L, 17L, 4L, 1L), .Label = c(" ALA", " ARIZ", " ARK",
>>     " CAL", " COLO", " CONN", " DC", " DEL", " FLA", " GA", " HAWAII",
>>     " ILL", " IND", " IOWA", " KANS", " KY", " LA", " MASS",
>>     " MD", " MICH", " MINN", " MO", " NC", " NEBR", " NEV", " NJ",
>>     " NM", " NY", " OHIO", " OKLA", " ORE", " PA", " RI", " TENN",
>>     " TEX", " UTAH", " VA", " WASH", " WIS", "CAL", "CONN", "IDAH",
>>     "KY", "MASS"), class = "factor"), avsft = c(-7.81, -16.06,
>>     -7.71999999999997, -1.88999999999999, 2.90000000000003, 5.12,
>>     -5.02999999999997, 9.33000000000004, 15.08, 2.89000000000004
>>     ), year = c(1970L, 1970L, 1970L, 1970L, 1970L, 1970L, 1970L,
>>     1970L, 1970L, 1970L), day = c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
>>     1L, 1L, 1L), hour = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
>>     0L), yearweek = c(197001L, 197001L, 197001L, 197001L, 197001L,
>>     197001L, 197001L, 197001L, 197001L, 197001L), week = c(1L,
>>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("yearday",
>> "City", "cell_number", "longitude", "latitude", "State", "avsft",
>> "year", "day", "hour", "yearweek", "week"), row.names = c(NA,
>> 10L), class = "data.frame")
>
>
> Sincerely,
>
> Shouro
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Sep 11 01:57:37 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 10 Sep 2015 16:57:37 -0700
Subject: [R] Help with Binning Data
In-Reply-To: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
References: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
Message-ID: <413442D8-6BFD-4192-93BE-D16FA17FE749@comcast.net>


On Sep 10, 2015, at 3:28 PM, Shouro Dasgupta wrote:

> Dear all,
> 
> I have 3-hourly temperature data from 1970-2010 for 122 cities in the US. I
> would like to bin this data by city-year-week. My idea is if the
> temperature for a particular city in a given week falls within a given
> range (-17.78 & -12.22), (-12.22 & -6.67), ... (37.78 & 43.33), then the
> corresponding bin would have a value of 1 and 0 otherwise.
> 
> The data looks like this. Basically, I need to generate a dummy variable
> for each temperature range. Any help will be greatly appreciated.

The urge to imitate other statistical package that rely on profusion of dummies should be resisted. R repression functions can handle factor variables and the `cut` function can deliver them along with appropriate use of `seq`:

  tmp2$Tcat <- cut( tmp2$avsft, breaks=seq (-17.78,  43.33, by= 5.55 ) )

> tmp2$Tcat
 [1] (-12.2,-6.68] (-17.8,-12.2] (-12.2,-6.68] (-6.68,-1.13]
 [5] (-1.13,4.42]  (4.42,9.97]   (-6.68,-1.13] (4.42,9.97]  
 [9] (9.97,15.5]   (-1.13,4.42] 
11 Levels: (-17.8,-12.2] (-12.2,-6.68] ... (37.7,43.3]


> tmp2[ , c("City", "Tcat")]
          City          Tcat
1        AKRON (-12.2,-6.68]
2       ALBANY (-17.8,-12.2]
3  ALBUQUERQUE (-12.2,-6.68]
4    ALLENTOWN (-6.68,-1.13]
5      ATLANTA  (-1.13,4.42]
6       AUSTIN   (4.42,9.97]
7    BALTIMORE (-6.68,-1.13]
8  BATON ROUGE   (4.42,9.97]
9     BERKELEY   (9.97,15.5]
10  BIRMINGHAM  (-1.13,4.42]

Must have been a cold snap in the southeast that New Years Day.


There.... isn't that much neater than have a messy bunch of dummies? If you really need to build them then look at `?model.frame`.

-- 
David. 
> 
> tmp2<- dput(head(tmp1,10))
>> structure(list(yearday = c(1970001L, 1970001L, 1970001L, 1970001L,
>> 1970001L, 1970001L, 1970001L, 1970001L, 1970001L, 1970001L),
>>    City = structure(1:10, .Label = c("AKRON", "ALBANY", "ALBUQUERQUE",
>>    "ALLENTOWN", "ATLANTA", "AUSTIN", "BALTIMORE", "BATON ROUGE",
>>    "BERKELEY", "BIRMINGHAM", "BOISE", "BOSTON", "BRIDGEPORT",
>>    "BUFFALO", "CAMBRIDGE", "CAMDEN", "CANTON", "CHARLOTTE",
>>    "CHATTANOOGA", "CHICAGO", "CINCINNATI", "CLEVELAND", "COLORADO
>> SPRINGS",
>>    "COLUMBUS", "CORPUS CHRISTI", "DALLAS", "DAYTON", "DENVER",
>>    "DES MOINES", "DETROIT", "DULUTH", "EL PASO", "ELIZABETH",
>>    "ERIE", "EVANSVILLE", "FALL RIVER", "FLINT", "FORT WAYNE",
>>    "FRESNO", "FT WORTH", "GARY", "GLENDALE", "GRAND RAPIDS",
>>    "HARTFORD", "HONOLULU", "HOUSTON", "INDIANAPOLIS", "JACKSONVILLE",
>>    "JERSEY CITY", "KANSAS CITY", "KANSAS ITY", "KNOXVILLE",
>>    "Lansing ", "LAS VEGAS", "LEXINGTON", "LINCOLN", "LITTLE ROCK",
>>    "LONG BEACH", "LOS ANGELES", "LOUISVILLE", "LOWELL", "LYNN",
>>    "MADISON", "MEMPHIS", "MIAMI", "MILWAUKEE", "MINNEAPOLIS",
>>    "MOBILE", "MONTGOMERY", "NASHVILLE", "NEW BEDFORD", "NEW HAVEN",
>>    "NEW ORLEANS", "NEW YORK CITY", "NEWARK", "NORFOLK", "OAKLAND",
>>    "OGDEN", "OKLAHOMA CITY", "OMAHA", "PASADENA", "PATERSON",
>>    "PEORIA", "PHILADELPHIA", "PHOENIX", "PITTSBURG", "PORTLAND",
>>    "PROVIDENCE", "PUEBLO", "READING", "RICHMOND", "ROCHESTER",
>>    "ROCKFORD", "SACRAMENTO", "SALT LAKE CITY", "SAN ANTONIO",
>>    "SAN CRUZ", "SAN DIEGO", "SAN FRANCISCO", "SAN JOSE", "SAVANNAH",
>>    "SCHENECTADY", "SCRANTON", "SEATTLE", "SHREVEPORT", "SOMERVILLE",
>>    "SOUTH BEND", "SPOKANE", "SPRINGFIELD", "ST LOUIS", "ST PAUL",
>>    "ST PETERSBURG", "SYRACUSE", "TACOMA", "TAMPA", "TOLEDO",
>>    "TRENTON", "TUCSON", "TULSA", "UTICA", "WASHINGTON", "WATERBURY",
>>    "WICHITA", "WILMINGTON", "WORCESTER", "YONKERS", "YOUNGSTOWN"
>>    ), class = "factor"), cell_number = c(17379L, 17027L, 19514L,
>>    17745L, 20256L, 21323L, 18104L, 21329L, 18779L, 20254L),
>>    longitude = c(-81.519005, -73.756232, -106.609991, -75.490183,
>>    -84.387982, -97.743061, -76.612189, -91.14032, -121.635963,
>>    -86.80249), latitude = c(41.081445, 42.652579, 35.110703,
>>    40.608431, 33.748995, 30.267153, 39.290385, 30.458283, 37.871744,
>>    33.520661), State = structure(c(29L, 28L, 27L, 32L, 10L,
>>    35L, 19L, 17L, 4L, 1L), .Label = c(" ALA", " ARIZ", " ARK",
>>    " CAL", " COLO", " CONN", " DC", " DEL", " FLA", " GA", " HAWAII",
>>    " ILL", " IND", " IOWA", " KANS", " KY", " LA", " MASS",
>>    " MD", " MICH", " MINN", " MO", " NC", " NEBR", " NEV", " NJ",
>>    " NM", " NY", " OHIO", " OKLA", " ORE", " PA", " RI", " TENN",
>>    " TEX", " UTAH", " VA", " WASH", " WIS", "CAL", "CONN", "IDAH",
>>    "KY", "MASS"), class = "factor"), avsft = c(-7.81, -16.06,
>>    -7.71999999999997, -1.88999999999999, 2.90000000000003, 5.12,
>>    -5.02999999999997, 9.33000000000004, 15.08, 2.89000000000004
>>    ), year = c(1970L, 1970L, 1970L, 1970L, 1970L, 1970L, 1970L,
>>    1970L, 1970L, 1970L), day = c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
>>    1L, 1L, 1L), hour = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
>>    0L), yearweek = c(197001L, 197001L, 197001L, 197001L, 197001L,
>>    197001L, 197001L, 197001L, 197001L, 197001L), week = c(1L,
>>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("yearday",
>> "City", "cell_number", "longitude", "latitude", "State", "avsft",
>> "year", "day", "hour", "yearweek", "week"), row.names = c(NA,
>> 10L), class = "data.frame")
> 
> 
> Sincerely,
> 
> Shouro
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From r.turner at auckland.ac.nz  Fri Sep 11 02:22:38 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 11 Sep 2015 12:22:38 +1200
Subject: [R] [FORGED] Re:  Help with Binning Data
In-Reply-To: <413442D8-6BFD-4192-93BE-D16FA17FE749@comcast.net>
References: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
	<413442D8-6BFD-4192-93BE-D16FA17FE749@comcast.net>
Message-ID: <55F21ECE.9070306@auckland.ac.nz>

On 11/09/15 11:57, David Winsemius wrote:

<SNIP>

> The urge to imitate other statistical package that rely on profusion
> of dummies should be resisted. R repression functions can handle
> factor variables ....

<SNIP>

Fortune? :-)

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From Achim.Zeileis at uibk.ac.at  Fri Sep 11 02:25:32 2015
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Fri, 11 Sep 2015 02:25:32 +0200 (CEST)
Subject: [R] [FORGED] Re:  Help with Binning Data
In-Reply-To: <55F21ECE.9070306@auckland.ac.nz>
References: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
	<413442D8-6BFD-4192-93BE-D16FA17FE749@comcast.net>
	<55F21ECE.9070306@auckland.ac.nz>
Message-ID: <alpine.DEB.2.11.1509110224590.3106@paninaro.uibk.ac.at>

On Fri, 11 Sep 2015, Rolf Turner wrote:

> On 11/09/15 11:57, David Winsemius wrote:
>
> <SNIP>
>
>> The urge to imitate other statistical package that rely on profusion
>> of dummies should be resisted. R repression functions can handle
>> factor variables ....
>
> <SNIP>
>
> Fortune? :-)

Nice! Should I include the "repression" typo? :-)

Best,
Z

> cheers,
>
> Rolf
>
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>


From r.turner at auckland.ac.nz  Fri Sep 11 04:37:58 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 11 Sep 2015 14:37:58 +1200
Subject: [R] [FORGED] Re:  Help with Binning Data
In-Reply-To: <alpine.DEB.2.11.1509110224590.3106@paninaro.uibk.ac.at>
References: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
	<413442D8-6BFD-4192-93BE-D16FA17FE749@comcast.net>
	<55F21ECE.9070306@auckland.ac.nz>
	<alpine.DEB.2.11.1509110224590.3106@paninaro.uibk.ac.at>
Message-ID: <55F23E86.8050607@auckland.ac.nz>

On 11/09/15 12:25, Achim Zeileis wrote:
> On Fri, 11 Sep 2015, Rolf Turner wrote:
>
>> On 11/09/15 11:57, David Winsemius wrote:
>>
>> <SNIP>
>>
>>> The urge to imitate other statistical package that rely on profusion
>>> of dummies should be resisted. R repression functions can handle
>>> factor variables ....
>>
>> <SNIP>
>>
>> Fortune? :-)
>
> Nice! Should I include the "repression" typo? :-)

Yes!  That's the point, from my point of view! :-)

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Fri Sep 11 05:38:35 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 10 Sep 2015 20:38:35 -0700
Subject: [R] [FORGED]  Help with Binning Data
In-Reply-To: <alpine.DEB.2.11.1509110224590.3106@paninaro.uibk.ac.at>
References: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
	<413442D8-6BFD-4192-93BE-D16FA17FE749@comcast.net>
	<55F21ECE.9070306@auckland.ac.nz>
	<alpine.DEB.2.11.1509110224590.3106@paninaro.uibk.ac.at>
Message-ID: <2970F860-A251-487D-84F3-0CB649C78DB1@comcast.net>


On Sep 10, 2015, at 5:25 PM, Achim Zeileis wrote:

> On Fri, 11 Sep 2015, Rolf Turner wrote:
> 
>> On 11/09/15 11:57, David Winsemius wrote:
>> 
>> <SNIP>
>> 
>>> The urge to imitate other statistical package that rely on profusion
>>> of dummies should be resisted. R repression functions can handle
>>> factor variables ....
>> 
>> <SNIP>
>> 
>> Fortune? :-)
> 
> Nice! Should I include the "repression" typo? :-)
> 

 Er, maybe not. Or the package[s] error.

Whatever;
David.

> Best,
> Z
> 
>> cheers,
>> 
>> Rolf
>> 
>> -- 
>> Technical Editor ANZJS
>> Department of Statistics
>> University of Auckland
>> Phone: +64-9-373-7599 ext. 88276
>> 

David Winsemius
Alameda, CA, USA


From r at catwhisker.org  Fri Sep 11 07:04:29 2015
From: r at catwhisker.org (David Wolfskill)
Date: Thu, 10 Sep 2015 22:04:29 -0700
Subject: [R] Generate a vector of values, given a vector of keys and a table?
Message-ID: <20150911050429.GB1131@albert.catwhisker.org>

I apologize in advance: I must be overlooking something quite simple,
but I'm failing to make progress.

Suppose I have a "lookup table":

Browse[2]> dput(lk_up)
structure(c("1.1", "1.9", "1.4", "1.5", "1.15", "10000", "10000", 
"15000", "20000", "25000"), .Dim = c(5L, 2L), .Dimnames = list(
    NULL, c("key", "val")))
Browse[2]> lk_up
     key    val    
[1,] "1.1"  "10000"
[2,] "1.9"  "10000"
[3,] "1.4"  "15000"
[4,] "1.5"  "20000"
[5,] "1.15" "25000"

and a vector whose elements correspond with the "key" column of the
table:

Browse[2]> dput(x)
c("1.9", "1.9", "1.1", "1.1", "1.4", "1.4", "1.5", "1.5", "1.5", 
"1.5")
Browse[2]> x
 [1] "1.9" "1.9" "1.1" "1.1" "1.4" "1.4" "1.5" "1.5" "1.5" "1.5"
Browse[2]> 

Is there a (relatively) simple (i.e., not explicitly looping) construct
that will yield a vector of the same size and shape as "x", but contain
the "value" entries from the lookup table (preserving the sequence: the
1st entry of the result must correspond to the 1st entry of the list of
keys) -- in the current example:

Browse[2]> dput(y)
c("10000", "10000", "10000", "10000", "15000", "15000", "20000", 
"20000", "20000", "20000")
Browse[2]> y
 [1] "10000" "10000" "10000" "10000" "15000" "15000" "20000" "20000"
"20000" "20000"
Browse[2]> 

I am (unfortunately) presently limited to R-3.0.2.

Thanks....

Peace,
david
-- 
David H. Wolfskill				r at catwhisker.org
Those who would murder in the name of God or prophet are blasphemous cowards.

See http://www.catwhisker.org/~david/publickey.gpg for my public key.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 949 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150910/ca3c75f8/attachment.bin>

From bgunter.4567 at gmail.com  Fri Sep 11 07:30:50 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Sep 2015 22:30:50 -0700
Subject: [R] Generate a vector of values,
	given a vector of keys and a table?
In-Reply-To: <20150911050429.GB1131@albert.catwhisker.org>
References: <20150911050429.GB1131@albert.catwhisker.org>
Message-ID: <CAGxFJbQUmA2CAeDomXOX7GaScn4ZGMKZwpJARhF-f1YOKHXQvg@mail.gmail.com>

?match

as in:

> y <- lk_up[match(x,lk_up[,"key"]),"val"]
> y
 [1] "10000" "10000" "10000" "10000" "15000" "15000" "20000"
 [8] "20000" "20000" "20000"



Bert



Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Sep 10, 2015 at 10:04 PM, David Wolfskill <r at catwhisker.org> wrote:
> I apologize in advance: I must be overlooking something quite simple,
> but I'm failing to make progress.
>
> Suppose I have a "lookup table":
>
> Browse[2]> dput(lk_up)
> structure(c("1.1", "1.9", "1.4", "1.5", "1.15", "10000", "10000",
> "15000", "20000", "25000"), .Dim = c(5L, 2L), .Dimnames = list(
>     NULL, c("key", "val")))
> Browse[2]> lk_up
>      key    val
> [1,] "1.1"  "10000"
> [2,] "1.9"  "10000"
> [3,] "1.4"  "15000"
> [4,] "1.5"  "20000"
> [5,] "1.15" "25000"
>
> and a vector whose elements correspond with the "key" column of the
> table:
>
> Browse[2]> dput(x)
> c("1.9", "1.9", "1.1", "1.1", "1.4", "1.4", "1.5", "1.5", "1.5",
> "1.5")
> Browse[2]> x
>  [1] "1.9" "1.9" "1.1" "1.1" "1.4" "1.4" "1.5" "1.5" "1.5" "1.5"
> Browse[2]>
>
> Is there a (relatively) simple (i.e., not explicitly looping) construct
> that will yield a vector of the same size and shape as "x", but contain
> the "value" entries from the lookup table (preserving the sequence: the
> 1st entry of the result must correspond to the 1st entry of the list of
> keys) -- in the current example:
>
> Browse[2]> dput(y)
> c("10000", "10000", "10000", "10000", "15000", "15000", "20000",
> "20000", "20000", "20000")
> Browse[2]> y
>  [1] "10000" "10000" "10000" "10000" "15000" "15000" "20000" "20000"
> "20000" "20000"
> Browse[2]>
>
> I am (unfortunately) presently limited to R-3.0.2.
>
> Thanks....
>
> Peace,
> david
> --
> David H. Wolfskill                              r at catwhisker.org
> Those who would murder in the name of God or prophet are blasphemous cowards.
>
> See http://www.catwhisker.org/~david/publickey.gpg for my public key.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tanasa at gmail.com  Fri Sep 11 08:14:26 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Fri, 11 Sep 2015 02:14:26 -0400
Subject: [R] scaling loess curves
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E8AE@SRVEXCHMBX.precheza.cz>
References: <CA+JEM03QfeZeA4ResFMS+2bcCJMxR9o0zjRj+m8sFBiqjEEiLg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E8AE@SRVEXCHMBX.precheza.cz>
Message-ID: <CA+JEM00kg_kT3VYs62PaX+5Fp8KGWZoKOoO3NkhtFKGt4+bhAA@mail.gmail.com>

Hi Petr,

thank you for your reply regarding the scaling of loess curves. Our
situation is the following :

we do have 2 experiments, and for each experiment, the set of data is in
the following format : "nodeA (chr, start, end) - node B (chr, start, end)
- interaction intensity (between A and B)".

We are trying to SCALE the LOESS curves ( for the graphs "distance between
node A and node B" vs "intensity") for experiment1 vs experiment2, in order
to make the experiments directly comparable.

I have attached 2 figures with the LOESS curves for experiment1 and
experiment2 to my email. Shall you have any suggestions, please let me
know. Thanks a lot,


-- bogdan

On Mon, Sep 7, 2015 at 7:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
> what about xlim or ylim?
>
> Cheers
> Petr
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bogdan
> > Tanasa
> > Sent: Monday, September 07, 2015 8:00 AM
> > To: r-help
> > Subject: [R] scaling loess curves
> >
> > Dear all,
> >
> > please could you advise about a method to scale 2 plots of LOESS
> > curves.
> > More specifically, we do have 2 sets of 5C data, and the loess plots
> > reflect the relationship between INTENSITY and DISTANCE (please see the
> > R code below).
> >
> > I am looking for a method/formula to scale these 2 LOESS plots and make
> > them directly comparable.
> >
> > many thanks,
> >
> > -- bogdan
> >
> >
> >
> > -------------- the R code ------------------
> >
> >
> >
> > a <- read.delim("a",header=T)
> > qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size =
> > 1,
> > span=0.01)+xlab("distance")+ylab("intensity")
> >
> >
> >
> > b <- read.delim("b",header=T)
> > qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size =
> > 1,
> > span=0.01)+xlab("distance")+ylab("intensity")
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: example1-1.png
Type: image/png
Size: 16282 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150911/3fa5e4ef/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: example2-2.png
Type: image/png
Size: 15036 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150911/3fa5e4ef/attachment-0001.png>

From petr.pikal at precheza.cz  Fri Sep 11 09:06:29 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 11 Sep 2015 07:06:29 +0000
Subject: [R] scaling loess curves
In-Reply-To: <CA+JEM00kg_kT3VYs62PaX+5Fp8KGWZoKOoO3NkhtFKGt4+bhAA@mail.gmail.com>
References: <CA+JEM03QfeZeA4ResFMS+2bcCJMxR9o0zjRj+m8sFBiqjEEiLg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E8AE@SRVEXCHMBX.precheza.cz>
	<CA+JEM00kg_kT3VYs62PaX+5Fp8KGWZoKOoO3NkhtFKGt4+bhAA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F33A@SRVEXCHMBX.precheza.cz>

Hi

based on your data maybe using logarithmic y scale shall give you desired result.

http://stackoverflow.com/questions/4699493/transform-only-one-axis-to-log10-scale-with-ggplot2

Or you can recalculate intensity to scale 100-0 (or any other suitable scale).

?rescale

Cheers
Petr

From: Bogdan Tanasa [mailto:tanasa at gmail.com]
Sent: Friday, September 11, 2015 8:14 AM
To: PIKAL Petr; r-help
Subject: Re: [R] scaling loess curves

Hi Petr,
thank you for your reply regarding the scaling of loess curves. Our situation is the following :

we do have 2 experiments, and for each experiment, the set of data is in the following format : "nodeA (chr, start, end) - node B (chr, start, end) - interaction intensity (between A and B)".

We are trying to SCALE the LOESS curves ( for the graphs "distance between node A and node B" vs "intensity") for experiment1 vs experiment2, in order to make the experiments directly comparable.

I have attached 2 figures with the LOESS curves for experiment1 and experiment2 to my email. Shall you have any suggestions, please let me know. Thanks a lot,



-- bogdan

On Mon, Sep 7, 2015 at 7:34 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

what about xlim or ylim?

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Bogdan
> Tanasa
> Sent: Monday, September 07, 2015 8:00 AM
> To: r-help
> Subject: [R] scaling loess curves
>
> Dear all,
>
> please could you advise about a method to scale 2 plots of LOESS
> curves.
> More specifically, we do have 2 sets of 5C data, and the loess plots
> reflect the relationship between INTENSITY and DISTANCE (please see the
> R code below).
>
> I am looking for a method/formula to scale these 2 LOESS plots and make
> them directly comparable.
>
> many thanks,
>
> -- bogdan
>
>
>
> -------------- the R code ------------------
>
>
>
> a <- read.delim("a",header=T)
> qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size =
> 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
>
>
> b <- read.delim("b",header=T)
> qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size =
> 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From tanasa at gmail.com  Fri Sep 11 10:03:07 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Fri, 11 Sep 2015 04:03:07 -0400
Subject: [R] scaling loess curves
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F33A@SRVEXCHMBX.precheza.cz>
References: <CA+JEM03QfeZeA4ResFMS+2bcCJMxR9o0zjRj+m8sFBiqjEEiLg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E8AE@SRVEXCHMBX.precheza.cz>
	<CA+JEM00kg_kT3VYs62PaX+5Fp8KGWZoKOoO3NkhtFKGt4+bhAA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F33A@SRVEXCHMBX.precheza.cz>
Message-ID: <CA+JEM024k3xLxFrmqQCYc=baTidn311+4L=uD4ji3s0CngWxow@mail.gmail.com>

Dear Petr,

thank you very much, it helped. On a side note, shall I have 2 plots and 2
loess curves (as below), is there any way in ggplot2 to overlay these 2
graphs for "a" and "b" ? much thanks again !

qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size = 1,
span=0.01)+xlab("distance")+ylab("intensity")

qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size = 1,
span=0.01)+xlab("distance")+ylab("intensity")
-- bogdan

On Fri, Sep 11, 2015 at 3:06 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
>
>
> based on your data maybe using logarithmic y scale shall give you desired
> result.
>
>
>
>
> http://stackoverflow.com/questions/4699493/transform-only-one-axis-to-log10-scale-with-ggplot2
>
>
>
> Or you can recalculate intensity to scale 100-0 (or any other suitable
> scale).
>
>
>
> ?rescale
>
>
>
> Cheers
>
> Petr
>
>
>
> *From:* Bogdan Tanasa [mailto:tanasa at gmail.com]
> *Sent:* Friday, September 11, 2015 8:14 AM
> *To:* PIKAL Petr; r-help
> *Subject:* Re: [R] scaling loess curves
>
>
>
> Hi Petr,
>
> thank you for your reply regarding the scaling of loess curves. Our
> situation is the following :
>
> we do have 2 experiments, and for each experiment, the set of data is in
> the following format : "nodeA (chr, start, end) - node B (chr, start, end)
> - interaction intensity (between A and B)".
>
> We are trying to SCALE the LOESS curves ( for the graphs "distance between
> node A and node B" vs "intensity") for experiment1 vs experiment2, in order
> to make the experiments directly comparable.
>
> I have attached 2 figures with the LOESS curves for experiment1 and
> experiment2 to my email. Shall you have any suggestions, please let me
> know. Thanks a lot,
>
>
>
> -- bogdan
>
>
>
> On Mon, Sep 7, 2015 at 7:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>
> Hi
>
> what about xlim or ylim?
>
> Cheers
> Petr
>
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bogdan
> > Tanasa
> > Sent: Monday, September 07, 2015 8:00 AM
> > To: r-help
> > Subject: [R] scaling loess curves
> >
> > Dear all,
> >
> > please could you advise about a method to scale 2 plots of LOESS
> > curves.
> > More specifically, we do have 2 sets of 5C data, and the loess plots
> > reflect the relationship between INTENSITY and DISTANCE (please see the
> > R code below).
> >
> > I am looking for a method/formula to scale these 2 LOESS plots and make
> > them directly comparable.
> >
> > many thanks,
> >
> > -- bogdan
> >
> >
> >
> > -------------- the R code ------------------
> >
> >
> >
> > a <- read.delim("a",header=T)
> > qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size =
> > 1,
> > span=0.01)+xlab("distance")+ylab("intensity")
> >
> >
> >
> > b <- read.delim("b",header=T)
> > qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size =
> > 1,
> > span=0.01)+xlab("distance")+ylab("intensity")
> >
>
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
>
>
> ------------------------------
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From shouro at gmail.com  Fri Sep 11 10:25:30 2015
From: shouro at gmail.com (Shouro Dasgupta)
Date: Fri, 11 Sep 2015 10:25:30 +0200
Subject: [R] Help with Binning Data
In-Reply-To: <CAGxFJbRMJg0Kqxw8QEEmxDV=gJU0MrvYrK5hFOqeWh93+LHQ7w@mail.gmail.com>
References: <CAMx+UYfKJw7=W06v1HibSvqUuL3rmtr-PdoNwVpPBtmrMo5UbA@mail.gmail.com>
	<CAGxFJbRMJg0Kqxw8QEEmxDV=gJU0MrvYrK5hFOqeWh93+LHQ7w@mail.gmail.com>
Message-ID: <CAMx+UYfuvmtx8WxLZfQnxc8wkBwfX1WRkA8D1PJ82OUzrf36EQ@mail.gmail.com>

Apologies for the HTML. It shouldn't have happened. I would like to use the
dummies as independent variables in a regression. I did manage to use count
of observations in a given range using the following code:

for (i in filelist) {
  # i <- filelist[1]
  tmp1 <- as.data.table(read.csv(i, sep=","))
  year<-tmp1$year[1]
  mykey=c("City","year","week")
  output <- as.data.frame(tmp1[,sum(avsft< -0),by=mykey])[,1:length(mykey)]
  output$avsft_1<- as.data.frame(tmp1[,sum(avsft>= -17.78 & avsft< -12.22,
na.rm=T), by=mykey])[,length(mykey)+1]

Where "i" is filenames (each file has data for 1 year). But instead of
count I would like to generate dummy variables for ranges [(-17.78 &
-12.22), (-12.22 & -6.67), ... (37.78 & 43.33)], so if a temperature
observation falls within a given range - the dummy variable for that range
will have a value of 1 for that week. Thanks again!

tmp2<- dput(head(tmp1,10))
structure(list(yearday = c(1970001L, 1970001L, 1970001L, 1970001L,
1970001L, 1970001L, 1970001L, 1970001L, 1970001L, 1970001L),
    City = structure(1:10, .Label = c("AKRON", "ALBANY", "ALBUQUERQUE",
    "ALLENTOWN", "ATLANTA", "AUSTIN", "BALTIMORE", "BATON ROUGE",
    "BERKELEY", "BIRMINGHAM", "BOISE", "BOSTON", "BRIDGEPORT",
    "BUFFALO", "CAMBRIDGE", "CAMDEN", "CANTON", "CHARLOTTE",
    "CHATTANOOGA", "CHICAGO", "CINCINNATI", "CLEVELAND", "COLORADO
SPRINGS",
    "COLUMBUS", "CORPUS CHRISTI", "DALLAS", "DAYTON", "DENVER",
    "DES MOINES", "DETROIT", "DULUTH", "EL PASO", "ELIZABETH",
    "ERIE", "EVANSVILLE", "FALL RIVER", "FLINT", "FORT WAYNE",
    "FRESNO", "FT WORTH", "GARY", "GLENDALE", "GRAND RAPIDS",
    "HARTFORD", "HONOLULU", "HOUSTON", "INDIANAPOLIS", "JACKSONVILLE",
    "JERSEY CITY", "KANSAS CITY", "KANSAS ITY", "KNOXVILLE",
    "Lansing ", "LAS VEGAS", "LEXINGTON", "LINCOLN", "LITTLE ROCK",
    "LONG BEACH", "LOS ANGELES", "LOUISVILLE", "LOWELL", "LYNN",
    "MADISON", "MEMPHIS", "MIAMI", "MILWAUKEE", "MINNEAPOLIS",
    "MOBILE", "MONTGOMERY", "NASHVILLE", "NEW BEDFORD", "NEW HAVEN",
    "NEW ORLEANS", "NEW YORK CITY", "NEWARK", "NORFOLK", "OAKLAND",
    "OGDEN", "OKLAHOMA CITY", "OMAHA", "PASADENA", "PATERSON",
    "PEORIA", "PHILADELPHIA", "PHOENIX", "PITTSBURG", "PORTLAND",
    "PROVIDENCE", "PUEBLO", "READING", "RICHMOND", "ROCHESTER",
    "ROCKFORD", "SACRAMENTO", "SALT LAKE CITY", "SAN ANTONIO",
    "SAN CRUZ", "SAN DIEGO", "SAN FRANCISCO", "SAN JOSE", "SAVANNAH",
    "SCHENECTADY", "SCRANTON", "SEATTLE", "SHREVEPORT", "SOMERVILLE",
    "SOUTH BEND", "SPOKANE", "SPRINGFIELD", "ST LOUIS", "ST PAUL",
    "ST PETERSBURG", "SYRACUSE", "TACOMA", "TAMPA", "TOLEDO",
    "TRENTON", "TUCSON", "TULSA", "UTICA", "WASHINGTON", "WATERBURY",
    "WICHITA", "WILMINGTON", "WORCESTER", "YONKERS", "YOUNGSTOWN"
    ), class = "factor"), cell_number = c(17379L, 17027L, 19514L,
    17745L, 20256L, 21323L, 18104L, 21329L, 18779L, 20254L),
    longitude = c(-81.519005, -73.756232, -106.609991, -75.490183,
    -84.387982, -97.743061, -76.612189, -91.14032, -121.635963,
    -86.80249), latitude = c(41.081445, 42.652579, 35.110703,
    40.608431, 33.748995, 30.267153, 39.290385, 30.458283, 37.871744,
    33.520661), State = structure(c(29L, 28L, 27L, 32L, 10L,
    35L, 19L, 17L, 4L, 1L), .Label = c(" ALA", " ARIZ", " ARK",
    " CAL", " COLO", " CONN", " DC", " DEL", " FLA", " GA", " HAWAII",
    " ILL", " IND", " IOWA", " KANS", " KY", " LA", " MASS",
    " MD", " MICH", " MINN", " MO", " NC", " NEBR", " NEV", " NJ",
    " NM", " NY", " OHIO", " OKLA", " ORE", " PA", " RI", " TENN",
    " TEX", " UTAH", " VA", " WASH", " WIS", "CAL", "CONN", "IDAH",
    "KY", "MASS"), class = "factor"), avsft = c(-7.81, -16.06,
    -7.71999999999997, -1.88999999999999, 2.90000000000003, 5.12,
    -5.02999999999997, 9.33000000000004, 15.08, 2.89000000000004
    ), year = c(1970L, 1970L, 1970L, 1970L, 1970L, 1970L, 1970L,
    1970L, 1970L, 1970L), day = c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L), hour = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
    0L), yearweek = c(197001L, 197001L, 197001L, 197001L, 197001L,
    197001L, 197001L, 197001L, 197001L, 197001L), week = c(1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("yearday",
"City", "cell_number", "longitude", "latitude", "State", "avsft",
"year", "day", "hour", "yearweek", "week"), row.names = c(NA,
10L), class = "data.frame")

Sincerely,

Shouro




On Fri, Sep 11, 2015 at 12:33 AM, Bert Gunter <bgunter.4567 at gmail.com>
wrote:

> 1. Posting in HTML largely negated your ability to provide data
> through dput(). Folow he posting guide and post in PLAIN TEXT only,
> please.
>
> 2. See ?cut  . I think this will at least get you started.
>
> Cheers,
> Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Thu, Sep 10, 2015 at 3:28 PM, Shouro Dasgupta <shouro at gmail.com> wrote:
> > Dear all,
> >
> > I have 3-hourly temperature data from 1970-2010 for 122 cities in the
> US. I
> > would like to bin this data by city-year-week. My idea is if the
> > temperature for a particular city in a given week falls within a given
> > range (-17.78 & -12.22), (-12.22 & -6.67), ... (37.78 & 43.33), then the
> > corresponding bin would have a value of 1 and 0 otherwise.
> >
> > The data looks like this. Basically, I need to generate a dummy variable
> > for each temperature range. Any help will be greatly appreciated.
> >
> > tmp2<- dput(head(tmp1,10))
> >> structure(list(yearday = c(1970001L, 1970001L, 1970001L, 1970001L,
> >> 1970001L, 1970001L, 1970001L, 1970001L, 1970001L, 1970001L),
> >>     City = structure(1:10, .Label = c("AKRON", "ALBANY", "ALBUQUERQUE",
> >>     "ALLENTOWN", "ATLANTA", "AUSTIN", "BALTIMORE", "BATON ROUGE",
> >>     "BERKELEY", "BIRMINGHAM", "BOISE", "BOSTON", "BRIDGEPORT",
> >>     "BUFFALO", "CAMBRIDGE", "CAMDEN", "CANTON", "CHARLOTTE",
> >>     "CHATTANOOGA", "CHICAGO", "CINCINNATI", "CLEVELAND", "COLORADO
> >> SPRINGS",
> >>     "COLUMBUS", "CORPUS CHRISTI", "DALLAS", "DAYTON", "DENVER",
> >>     "DES MOINES", "DETROIT", "DULUTH", "EL PASO", "ELIZABETH",
> >>     "ERIE", "EVANSVILLE", "FALL RIVER", "FLINT", "FORT WAYNE",
> >>     "FRESNO", "FT WORTH", "GARY", "GLENDALE", "GRAND RAPIDS",
> >>     "HARTFORD", "HONOLULU", "HOUSTON", "INDIANAPOLIS", "JACKSONVILLE",
> >>     "JERSEY CITY", "KANSAS CITY", "KANSAS ITY", "KNOXVILLE",
> >>     "Lansing ", "LAS VEGAS", "LEXINGTON", "LINCOLN", "LITTLE ROCK",
> >>     "LONG BEACH", "LOS ANGELES", "LOUISVILLE", "LOWELL", "LYNN",
> >>     "MADISON", "MEMPHIS", "MIAMI", "MILWAUKEE", "MINNEAPOLIS",
> >>     "MOBILE", "MONTGOMERY", "NASHVILLE", "NEW BEDFORD", "NEW HAVEN",
> >>     "NEW ORLEANS", "NEW YORK CITY", "NEWARK", "NORFOLK", "OAKLAND",
> >>     "OGDEN", "OKLAHOMA CITY", "OMAHA", "PASADENA", "PATERSON",
> >>     "PEORIA", "PHILADELPHIA", "PHOENIX", "PITTSBURG", "PORTLAND",
> >>     "PROVIDENCE", "PUEBLO", "READING", "RICHMOND", "ROCHESTER",
> >>     "ROCKFORD", "SACRAMENTO", "SALT LAKE CITY", "SAN ANTONIO",
> >>     "SAN CRUZ", "SAN DIEGO", "SAN FRANCISCO", "SAN JOSE", "SAVANNAH",
> >>     "SCHENECTADY", "SCRANTON", "SEATTLE", "SHREVEPORT", "SOMERVILLE",
> >>     "SOUTH BEND", "SPOKANE", "SPRINGFIELD", "ST LOUIS", "ST PAUL",
> >>     "ST PETERSBURG", "SYRACUSE", "TACOMA", "TAMPA", "TOLEDO",
> >>     "TRENTON", "TUCSON", "TULSA", "UTICA", "WASHINGTON", "WATERBURY",
> >>     "WICHITA", "WILMINGTON", "WORCESTER", "YONKERS", "YOUNGSTOWN"
> >>     ), class = "factor"), cell_number = c(17379L, 17027L, 19514L,
> >>     17745L, 20256L, 21323L, 18104L, 21329L, 18779L, 20254L),
> >>     longitude = c(-81.519005, -73.756232, -106.609991, -75.490183,
> >>     -84.387982, -97.743061, -76.612189, -91.14032, -121.635963,
> >>     -86.80249), latitude = c(41.081445, 42.652579, 35.110703,
> >>     40.608431, 33.748995, 30.267153, 39.290385, 30.458283, 37.871744,
> >>     33.520661), State = structure(c(29L, 28L, 27L, 32L, 10L,
> >>     35L, 19L, 17L, 4L, 1L), .Label = c(" ALA", " ARIZ", " ARK",
> >>     " CAL", " COLO", " CONN", " DC", " DEL", " FLA", " GA", " HAWAII",
> >>     " ILL", " IND", " IOWA", " KANS", " KY", " LA", " MASS",
> >>     " MD", " MICH", " MINN", " MO", " NC", " NEBR", " NEV", " NJ",
> >>     " NM", " NY", " OHIO", " OKLA", " ORE", " PA", " RI", " TENN",
> >>     " TEX", " UTAH", " VA", " WASH", " WIS", "CAL", "CONN", "IDAH",
> >>     "KY", "MASS"), class = "factor"), avsft = c(-7.81, -16.06,
> >>     -7.71999999999997, -1.88999999999999, 2.90000000000003, 5.12,
> >>     -5.02999999999997, 9.33000000000004, 15.08, 2.89000000000004
> >>     ), year = c(1970L, 1970L, 1970L, 1970L, 1970L, 1970L, 1970L,
> >>     1970L, 1970L, 1970L), day = c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
> >>     1L, 1L, 1L), hour = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> >>     0L), yearweek = c(197001L, 197001L, 197001L, 197001L, 197001L,
> >>     197001L, 197001L, 197001L, 197001L, 197001L), week = c(1L,
> >>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L)), .Names = c("yearday",
> >> "City", "cell_number", "longitude", "latitude", "State", "avsft",
> >> "year", "day", "hour", "yearweek", "week"), row.names = c(NA,
> >> 10L), class = "data.frame")
> >
> >
> > Sincerely,
> >
> > Shouro
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Sep 11 10:34:43 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 11 Sep 2015 08:34:43 +0000
Subject: [R] scaling loess curves
In-Reply-To: <CA+JEM024k3xLxFrmqQCYc=baTidn311+4L=uD4ji3s0CngWxow@mail.gmail.com>
References: <CA+JEM03QfeZeA4ResFMS+2bcCJMxR9o0zjRj+m8sFBiqjEEiLg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E8AE@SRVEXCHMBX.precheza.cz>
	<CA+JEM00kg_kT3VYs62PaX+5Fp8KGWZoKOoO3NkhtFKGt4+bhAA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F33A@SRVEXCHMBX.precheza.cz>
	<CA+JEM024k3xLxFrmqQCYc=baTidn311+4L=uD4ji3s0CngWxow@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F3E0@SRVEXCHMBX.precheza.cz>

Hi

you need to merge those two data frames with a column indicating given set.

Without data it is only a guess but

a$set<-?a?
b$set<-?b?

complete <- rbind(a,b)

p <-ggplot(complete, aes(x=distance, y=intensity, colour=set))

p+geom_smooth(method = "loess", size = 1, span=0.01)+xlab("distance")+ylab("intensity")
shall do it.

Cheers
Petr

From: Bogdan Tanasa [mailto:tanasa at gmail.com]
Sent: Friday, September 11, 2015 10:03 AM
To: PIKAL Petr; r-help
Subject: Re: [R] scaling loess curves

Dear Petr,
thank you very much, it helped. On a side note, shall I have 2 plots and 2 loess curves (as below), is there any way in ggplot2 to overlay these 2 graphs for "a" and "b" ? much thanks again !

qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size = 1, span=0.01)+xlab("distance")+ylab("intensity")

qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size = 1, span=0.01)+xlab("distance")+ylab("intensity")
-- bogdan

On Fri, Sep 11, 2015 at 3:06 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

based on your data maybe using logarithmic y scale shall give you desired result.

http://stackoverflow.com/questions/4699493/transform-only-one-axis-to-log10-scale-with-ggplot2

Or you can recalculate intensity to scale 100-0 (or any other suitable scale).

?rescale

Cheers
Petr

From: Bogdan Tanasa [mailto:tanasa at gmail.com<mailto:tanasa at gmail.com>]
Sent: Friday, September 11, 2015 8:14 AM
To: PIKAL Petr; r-help
Subject: Re: [R] scaling loess curves

Hi Petr,
thank you for your reply regarding the scaling of loess curves. Our situation is the following :

we do have 2 experiments, and for each experiment, the set of data is in the following format : "nodeA (chr, start, end) - node B (chr, start, end) - interaction intensity (between A and B)".

We are trying to SCALE the LOESS curves ( for the graphs "distance between node A and node B" vs "intensity") for experiment1 vs experiment2, in order to make the experiments directly comparable.

I have attached 2 figures with the LOESS curves for experiment1 and experiment2 to my email. Shall you have any suggestions, please let me know. Thanks a lot,



-- bogdan

On Mon, Sep 7, 2015 at 7:34 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

what about xlim or ylim?

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Bogdan
> Tanasa
> Sent: Monday, September 07, 2015 8:00 AM
> To: r-help
> Subject: [R] scaling loess curves
>
> Dear all,
>
> please could you advise about a method to scale 2 plots of LOESS
> curves.
> More specifically, we do have 2 sets of 5C data, and the loess plots
> reflect the relationship between INTENSITY and DISTANCE (please see the
> R code below).
>
> I am looking for a method/formula to scale these 2 LOESS plots and make
> them directly comparable.
>
> many thanks,
>
> -- bogdan
>
>
>
> -------------- the R code ------------------
>
>
>
> a <- read.delim("a",header=T)
> qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size =
> 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
>
>
> b <- read.delim("b",header=T)
> qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size =
> 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From tanasa at gmail.com  Fri Sep 11 10:45:47 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Fri, 11 Sep 2015 04:45:47 -0400
Subject: [R] scaling loess curves
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F3E0@SRVEXCHMBX.precheza.cz>
References: <CA+JEM03QfeZeA4ResFMS+2bcCJMxR9o0zjRj+m8sFBiqjEEiLg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3E8AE@SRVEXCHMBX.precheza.cz>
	<CA+JEM00kg_kT3VYs62PaX+5Fp8KGWZoKOoO3NkhtFKGt4+bhAA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F33A@SRVEXCHMBX.precheza.cz>
	<CA+JEM024k3xLxFrmqQCYc=baTidn311+4L=uD4ji3s0CngWxow@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F3E0@SRVEXCHMBX.precheza.cz>
Message-ID: <CA+JEM01TGJt47ny2SZxz7G9erJ-r4uu7UdfMM8aMokh5DCc4yw@mail.gmail.com>

thanks Petr. It shall work ;)

On Fri, Sep 11, 2015 at 4:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
>
>
> you need to merge those two data frames with a column indicating given set.
>
>
>
> Without data it is only a guess but
>
>
>
> a$set<-?a?
>
> b$set<-?b?
>
>
>
> complete <- rbind(a,b)
>
>
>
> p <-ggplot(complete, aes(x=distance, y=intensity, colour=set))
>
> p+geom_smooth(method = "loess", size = 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
> shall do it.
>
>
>
> Cheers
>
> Petr
>
>
>
> *From:* Bogdan Tanasa [mailto:tanasa at gmail.com]
> *Sent:* Friday, September 11, 2015 10:03 AM
>
> *To:* PIKAL Petr; r-help
> *Subject:* Re: [R] scaling loess curves
>
>
>
> Dear Petr,
>
> thank you very much, it helped. On a side note, shall I have 2 plots and 2
> loess curves (as below), is there any way in ggplot2 to overlay these 2
> graphs for "a" and "b" ? much thanks again !
>
> qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size = 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
> qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size = 1,
> span=0.01)+xlab("distance")+ylab("intensity")
>
> -- bogdan
>
>
>
> On Fri, Sep 11, 2015 at 3:06 AM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
>
> Hi
>
>
>
> based on your data maybe using logarithmic y scale shall give you desired
> result.
>
>
>
>
> http://stackoverflow.com/questions/4699493/transform-only-one-axis-to-log10-scale-with-ggplot2
>
>
>
> Or you can recalculate intensity to scale 100-0 (or any other suitable
> scale).
>
>
>
> ?rescale
>
>
>
> Cheers
>
> Petr
>
>
>
> *From:* Bogdan Tanasa [mailto:tanasa at gmail.com]
> *Sent:* Friday, September 11, 2015 8:14 AM
> *To:* PIKAL Petr; r-help
> *Subject:* Re: [R] scaling loess curves
>
>
>
> Hi Petr,
>
> thank you for your reply regarding the scaling of loess curves. Our
> situation is the following :
>
> we do have 2 experiments, and for each experiment, the set of data is in
> the following format : "nodeA (chr, start, end) - node B (chr, start, end)
> - interaction intensity (between A and B)".
>
> We are trying to SCALE the LOESS curves ( for the graphs "distance between
> node A and node B" vs "intensity") for experiment1 vs experiment2, in order
> to make the experiments directly comparable.
>
> I have attached 2 figures with the LOESS curves for experiment1 and
> experiment2 to my email. Shall you have any suggestions, please let me
> know. Thanks a lot,
>
>
>
> -- bogdan
>
>
>
> On Mon, Sep 7, 2015 at 7:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>
> Hi
>
> what about xlim or ylim?
>
> Cheers
> Petr
>
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bogdan
> > Tanasa
> > Sent: Monday, September 07, 2015 8:00 AM
> > To: r-help
> > Subject: [R] scaling loess curves
> >
> > Dear all,
> >
> > please could you advise about a method to scale 2 plots of LOESS
> > curves.
> > More specifically, we do have 2 sets of 5C data, and the loess plots
> > reflect the relationship between INTENSITY and DISTANCE (please see the
> > R code below).
> >
> > I am looking for a method/formula to scale these 2 LOESS plots and make
> > them directly comparable.
> >
> > many thanks,
> >
> > -- bogdan
> >
> >
> >
> > -------------- the R code ------------------
> >
> >
> >
> > a <- read.delim("a",header=T)
> > qplot(data=a,distance,intensity)+geom_smooth(method = "loess", size =
> > 1,
> > span=0.01)+xlab("distance")+ylab("intensity")
> >
> >
> >
> > b <- read.delim("b",header=T)
> > qplot(data=b,distance,intensity)+geom_smooth(method = "loess", size =
> > 1,
> > span=0.01)+xlab("distance")+ylab("intensity")
> >
>
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
>
>
>
> ------------------------------
>
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
>
>
> ------------------------------
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From stefano.sofia at regione.marche.it  Fri Sep 11 11:46:37 2015
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Fri, 11 Sep 2015 09:46:37 +0000
Subject: [R] how to divide a data frame of half-hourly wind observations
 into subset of three hours
Message-ID: <8B435C9568170B469AE31E8891E8CC4F280A9BD7@ESINO.regionemarche.intra>

Dear R-users,
from a data frame of half-hourly wind observations (direction and speed, three years data), I need to evaluate the first main direction for each period of three hours (h03, h06, h09, h12, h15, h18, h21, h24 of each day).

The command "windRose" of the package "openair" crates a data frame that contains the information I need.
Can I ask you which is the most efficient way to run the windRose command for each subset of three hours?

The initial data frame is called wind_df is like

Date Direction Speed
2015-06-30 00:30:00 315 2.9
2015-06-30 01:00:00 338 3.5
2015-06-30 01:30:00 23 4.8
2015-06-30 02:00:00 338 4.1
2015-06-30 02:30:00 135 1.1
2015-06-30 03:00:00 45 2.7
2015-06-30 03:30:00  1.7
2015-06-30 04:00:00 293 1
2015-06-30 04:30:00 45 1.5
2015-06-30 05:00:00 45 2.3
...

The command for the wind rose creation is
output1_df <- windRose(wind_df, ws="Speed", wd="Direction", layout=c(1,1), ws.int=4, angle=22.5, type="default", bias.corr=TRUE, cols="default", grid.line=NULL, width=2, seg=NULL, auto.text=TRUE, breaks=c(0,0.3,1.5,3.4,5.4,7.9,10.7,13.8,17.1,20.7,24.4,28.4,32.6,40), offset=5, normalise=TRUE, max.freq=NULL, paddle=TRUE, key.header=NULL, key.footer="(km/hr)", key.position="bottom", key=TRUE, dig.lab=2, statistic="prop.count", pollutant=NULL, annotate=TRUE, border=NA, par.settings=list(fontsize=list(text=10)))

and I evaluate the first main direction through
output2_df <- output1_df$data
output2_df$wd[output2_df$freqs == max(output2_df$freqs)])


Thank you for your attention and your help
Stefano Sofia


________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Fri Sep 11 11:47:58 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 11 Sep 2015 01:47:58 -0800
Subject: [R] [FORGED] Re: Help with Binning Data
In-Reply-To: <55F21ECE.9070306@auckland.ac.nz>
References: <camx+uyfkjw7=w06v1hibsvquul3rmtr-pdonwvppbtmrmo5uba@mail.gmail.com>
	<413442d8-6bfd-4192-93be-d16fa17fe749@comcast.net>
Message-ID: <A4B052BD183.00000441jrkrideau@inbox.com>

"that rely on profusion of dummies" :)

+1

John Kane
Kingston ON Canada


> -----Original Message-----
> From: r.turner at auckland.ac.nz
> Sent: Fri, 11 Sep 2015 12:22:38 +1200
> To: dwinsemius at comcast.net
> Subject: Re: [R] [FORGED] Re: Help with Binning Data
> 
> On 11/09/15 11:57, David Winsemius wrote:
> 
> <SNIP>
> 
>> The urge to imitate other statistical package that rely on profusion
>> of dummies should be resisted. R repression functions can handle
>> factor variables ....
> 
> <SNIP>
> 
> Fortune? :-)
> 
> cheers,
> 
> Rolf
> 
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From drjimlemon at gmail.com  Fri Sep 11 12:14:05 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 11 Sep 2015 20:14:05 +1000
Subject: [R] [FORGED] Re: Help with Binning Data
In-Reply-To: <A4B052BD183.00000441jrkrideau@inbox.com>
References: <camx+uyfkjw7=w06v1hibsvquul3rmtr-pdonwvppbtmrmo5uba@mail.gmail.com>
	<413442d8-6bfd-4192-93be-d16fa17fe749@comcast.net>
	<55F21ECE.9070306@auckland.ac.nz>
	<A4B052BD183.00000441jrkrideau@inbox.com>
Message-ID: <CA+8X3fU8HgX_R5CP1cYw=fu87G_RiaZYoDNbYJ0YPMWBUtRHMA@mail.gmail.com>

Hi Shouro,
While I have enjoyed the continuing discussion on this particular message
(repression may have been a Galtonian slip), there is a lingering doubt in
my mind. You say that you want to categorize the weekly temperatures for
cities in bins of about 5.6 degrees (centigrade?). In almost all of the
cities you include in your sample data (quite a few of which I have
personal experience) the variation in temperature over a day, not to
mention a week, is more than this. Unless you derive some particular
temperature value, many cities will span more than one bin over a week.
Have you already calculated a weekly average from your 3 hour observations?

Jim


On Fri, Sep 11, 2015 at 7:47 PM, John Kane <jrkrideau at inbox.com> wrote:

> "that rely on profusion of dummies" :)
>
> +1
>
> John Kane
> Kingston ON Canada
>
>
> > -----Original Message-----
> > From: r.turner at auckland.ac.nz
> > Sent: Fri, 11 Sep 2015 12:22:38 +1200
> > To: dwinsemius at comcast.net
> > Subject: Re: [R] [FORGED] Re: Help with Binning Data
> >
> > On 11/09/15 11:57, David Winsemius wrote:
> >
> > <SNIP>
> >
> >> The urge to imitate other statistical package that rely on profusion
> >> of dummies should be resisted. R repression functions can handle
> >> factor variables ....
> >
> > <SNIP>
> >
> > Fortune? :-)
> >
> > cheers,
> >
> > Rolf
> >
> > --
> > Technical Editor ANZJS
> > Department of Statistics
> > University of Auckland
> > Phone: +64-9-373-7599 ext. 88276
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> FREE ONLINE PHOTOSHARING - Share your photos online with your friends and
> family!
> Visit http://www.inbox.com/photosharing to find out more!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Sep 11 13:05:42 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 11 Sep 2015 11:05:42 +0000
Subject: [R] how to divide a data frame of half-hourly wind observations
 into subset of three hours
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F280A9BD7@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F280A9BD7@ESINO.regionemarche.intra>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F49F@SRVEXCHMBX.precheza.cz>

Hi

if you have full set of data, so no observation is missing, you can split your data frame values e.g. by

split(yourdata, trunc(0:nrow(yourdata))/6)

and use lapply or for cycle.

If you need to cut according to dates you can use

?cut

POSIX date.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Stefano
> Sofia
> Sent: Friday, September 11, 2015 11:47 AM
> To: r-help at r-project.org
> Subject: [R] how to divide a data frame of half-hourly wind
> observations into subset of three hours
>
> Dear R-users,
> from a data frame of half-hourly wind observations (direction and
> speed, three years data), I need to evaluate the first main direction
> for each period of three hours (h03, h06, h09, h12, h15, h18, h21, h24
> of each day).
>
> The command "windRose" of the package "openair" crates a data frame
> that contains the information I need.
> Can I ask you which is the most efficient way to run the windRose
> command for each subset of three hours?
>
> The initial data frame is called wind_df is like
>
> Date Direction Speed
> 2015-06-30 00:30:00 315 2.9
> 2015-06-30 01:00:00 338 3.5
> 2015-06-30 01:30:00 23 4.8
> 2015-06-30 02:00:00 338 4.1
> 2015-06-30 02:30:00 135 1.1
> 2015-06-30 03:00:00 45 2.7
> 2015-06-30 03:30:00  1.7
> 2015-06-30 04:00:00 293 1
> 2015-06-30 04:30:00 45 1.5
> 2015-06-30 05:00:00 45 2.3
> ...
>
> The command for the wind rose creation is
> output1_df <- windRose(wind_df, ws="Speed", wd="Direction",
> layout=c(1,1), ws.int=4, angle=22.5, type="default", bias.corr=TRUE,
> cols="default", grid.line=NULL, width=2, seg=NULL, auto.text=TRUE,
> breaks=c(0,0.3,1.5,3.4,5.4,7.9,10.7,13.8,17.1,20.7,24.4,28.4,32.6,40),
> offset=5, normalise=TRUE, max.freq=NULL, paddle=TRUE, key.header=NULL,
> key.footer="(km/hr)", key.position="bottom", key=TRUE, dig.lab=2,
> statistic="prop.count", pollutant=NULL, annotate=TRUE, border=NA,
> par.settings=list(fontsize=list(text=10)))
>
> and I evaluate the first main direction through
> output2_df <- output1_df$data
> output2_df$wd[output2_df$freqs == max(output2_df$freqs)])
>
>
> Thank you for your attention and your help
> Stefano Sofia
>
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate alla ricezione. I messaggi di posta elettronica per i
> client di Regione Marche possono contenere informazioni confidenziali e
> con privilegi legali. Se non si ? il destinatario specificato, non
> leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
> ricevuto questo messaggio per errore, inoltrarlo al mittente ed
> eliminarlo completamente dal sistema del proprio computer. Ai sensi
> dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit?
> ed urgenza, la risposta al presente messaggio di posta elettronica pu?
> essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only
> by persons entitled to receive the confidential information it may
> contain. E-mail messages to clients of Regione Marche may contain
> information that is confidential and legally privileged. Please do not
> read, copy, forward, or store this message unless you are an intended
> recipient of it. If you have received this message in error, please
> forward it to the sender and delete it completely from your computer
> system.
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From shouro at gmail.com  Fri Sep 11 13:06:37 2015
From: shouro at gmail.com (Shouro Dasgupta)
Date: Fri, 11 Sep 2015 13:06:37 +0200
Subject: [R] [FORGED] Re: Help with Binning Data
In-Reply-To: <CA+8X3fU8HgX_R5CP1cYw=fu87G_RiaZYoDNbYJ0YPMWBUtRHMA@mail.gmail.com>
References: <camx+uyfkjw7=w06v1hibsvquul3rmtr-pdonwvppbtmrmo5uba@mail.gmail.com>
	<413442d8-6bfd-4192-93be-d16fa17fe749@comcast.net>
	<55F21ECE.9070306@auckland.ac.nz>
	<A4B052BD183.00000441jrkrideau@inbox.com>
	<CA+8X3fU8HgX_R5CP1cYw=fu87G_RiaZYoDNbYJ0YPMWBUtRHMA@mail.gmail.com>
Message-ID: <CAMx+UYdz4ZWZRo6UvL5AY0Rh2rEhFXxf6=k5NB18HVvjk8V2CQ@mail.gmail.com>

Dear Jim,

Thank you for your reply and pointing this out. I thought about it and then
I forgot. I have computed the weekly average (and max and min). The data is
below. Again I computed the max/min/mean by each year, so each file
contains data for one year. Can I modify the code I used for count? Thanks
again!

Sincerely,

Shouro

structure(list(City = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L), .Label = c("AKRON", "ALBANY", "ALBUQUERQUE", "ALLENTOWN",
"ATLANTA", "AUSTIN", "BALTIMORE", "BATON ROUGE", "BERKELEY",
"BIRMINGHAM", "BOISE", "BOSTON", "BRIDGEPORT", "BUFFALO", "CAMBRIDGE",
"CAMDEN", "CANTON", "CHARLOTTE", "CHATTANOOGA", "CHICAGO", "CINCINNATI",
"CLEVELAND", "COLORADO SPRINGS", "COLUMBUS", "CORPUS CHRISTI",
"DALLAS", "DAYTON", "DENVER", "DES MOINES", "DETROIT", "DULUTH",
"EL PASO", "ELIZABETH", "ERIE", "EVANSVILLE", "FALL RIVER", "FLINT",
"FORT WAYNE", "FRESNO", "FT WORTH", "GARY", "GLENDALE", "GRAND RAPIDS",
"HARTFORD", "HONOLULU", "HOUSTON", "INDIANAPOLIS", "JACKSONVILLE",
"JERSEY CITY", "KANSAS CITY", "KANSAS ITY", "KNOXVILLE", "Lansing ",
"LAS VEGAS", "LEXINGTON", "LINCOLN", "LITTLE ROCK", "LONG BEACH",
"LOS ANGELES", "LOUISVILLE", "LOWELL", "LYNN", "MADISON", "MEMPHIS",
"MIAMI", "MILWAUKEE", "MINNEAPOLIS", "MOBILE", "MONTGOMERY",
"NASHVILLE", "NEW BEDFORD", "NEW HAVEN", "NEW ORLEANS", "NEW YORK CITY",
"NEWARK", "NORFOLK", "OAKLAND", "OGDEN", "OKLAHOMA CITY", "OMAHA",
"PASADENA", "PATERSON", "PEORIA", "PHILADELPHIA", "PHOENIX",
"PITTSBURG", "PORTLAND", "PROVIDENCE", "PUEBLO", "READING", "RICHMOND",
"ROCHESTER", "ROCKFORD", "SACRAMENTO", "SALT LAKE CITY", "SAN ANTONIO",
"SAN CRUZ", "SAN DIEGO", "SAN FRANCISCO", "SAN JOSE", "SAVANNAH",
"SCHENECTADY", "SCRANTON", "SEATTLE", "SHREVEPORT", "SOMERVILLE",
"SOUTH BEND", "SPOKANE", "SPRINGFIELD", "ST LOUIS", "ST PAUL",
"ST PETERSBURG", "SYRACUSE", "TACOMA", "TAMPA", "TOLEDO", "TRENTON",
"TUCSON", "TULSA", "UTICA", "WASHINGTON", "WATERBURY", "WICHITA",
"WILMINGTON", "WORCESTER", "YONKERS", "YOUNGSTOWN"), class = "factor"),
    year = c(1970L, 1970L, 1970L, 1970L, 1970L, 1970L, 1970L,
    1970L, 1970L, 1970L), week = 1:10, tmax = c(-3.94999999999997,
    -6.28714285714283, -3.38285714285712, -4.24571428571427,
    0.188571428571453, -1.3485714285714, -1.40285714285712,
3.66285714285717,
    4.55000000000002, 5.7157142857143), tmin = c(-10.7316666666667,
    -12.2057142857143, -10.7885714285714, -13.3157142857143,
    -6.73999999999998, -8.60999999999998, -8.47999999999997,
    -6.02428571428569, -4.36428571428569, -2.43999999999998),
    tmean = c(-7.36583333333332, -9.77446428571427, -7.11892857142855,
    -9.07499999999999, -3.45946428571426, -4.99214285714284,
    -5.27874999999998, -1.31928571428569, -0.556249999999979,
    1.24714285714287)), .Names = c("City", "year", "week", "tmax",
"tmin", "tmean"), row.names = c(NA, 10L), class = "data.frame")

On Fri, Sep 11, 2015 at 12:14 PM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Shouro,
> While I have enjoyed the continuing discussion on this particular message
> (repression may have been a Galtonian slip), there is a lingering doubt in
> my mind. You say that you want to categorize the weekly temperatures for
> cities in bins of about 5.6 degrees (centigrade?). In almost all of the
> cities you include in your sample data (quite a few of which I have
> personal experience) the variation in temperature over a day, not to
> mention a week, is more than this. Unless you derive some particular
> temperature value, many cities will span more than one bin over a week.
> Have you already calculated a weekly average from your 3 hour observations?
>
> Jim
>
>
> On Fri, Sep 11, 2015 at 7:47 PM, John Kane <jrkrideau at inbox.com> wrote:
>
>> "that rely on profusion of dummies" :)
>>
>> +1
>>
>> John Kane
>> Kingston ON Canada
>>
>>
>> > -----Original Message-----
>> > From: r.turner at auckland.ac.nz
>> > Sent: Fri, 11 Sep 2015 12:22:38 +1200
>> > To: dwinsemius at comcast.net
>> > Subject: Re: [R] [FORGED] Re: Help with Binning Data
>> >
>> > On 11/09/15 11:57, David Winsemius wrote:
>> >
>> > <SNIP>
>> >
>> >> The urge to imitate other statistical package that rely on profusion
>> >> of dummies should be resisted. R repression functions can handle
>> >> factor variables ....
>> >
>> > <SNIP>
>> >
>> > Fortune? :-)
>> >
>> > cheers,
>> >
>> > Rolf
>> >
>> > --
>> > Technical Editor ANZJS
>> > Department of Statistics
>> > University of Auckland
>> > Phone: +64-9-373-7599 ext. 88276
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ____________________________________________________________
>> FREE ONLINE PHOTOSHARING - Share your photos online with your friends and
>> family!
>> Visit http://www.inbox.com/photosharing to find out more!
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 

*Shouro Dasgupta*
PhD Candidate
Science and Management of Climate Change
Department of Economics | Ca' Foscari University of Venice
-------------------------------------------------------------------------------------------------------
Junior Researcher
Fondazione Eni Enrico Mattei (FEEM) | Centro Euro-Mediterraneo per i
Cambiamenti Climatici (CMCC)
Isola di San Giorgio Maggiore, 8
30124 Venezia
Phone: +39 041 2700 436

	[[alternative HTML version deleted]]


From r at catwhisker.org  Fri Sep 11 13:23:23 2015
From: r at catwhisker.org (David Wolfskill)
Date: Fri, 11 Sep 2015 04:23:23 -0700
Subject: [R] Generate a vector of values,
 given a vector of keys and a table?
In-Reply-To: <CAGxFJbQUmA2CAeDomXOX7GaScn4ZGMKZwpJARhF-f1YOKHXQvg@mail.gmail.com>
References: <20150911050429.GB1131@albert.catwhisker.org>
	<CAGxFJbQUmA2CAeDomXOX7GaScn4ZGMKZwpJARhF-f1YOKHXQvg@mail.gmail.com>
Message-ID: <20150911112323.GC1131@albert.catwhisker.org>

On Thu, Sep 10, 2015 at 10:30:50PM -0700, Bert Gunter wrote:
> ?match
> 
> as in:
> 
> > y <- lk_up[match(x,lk_up[,"key"]),"val"]
> > y
>  [1] "10000" "10000" "10000" "10000" "15000" "15000" "20000"
>  [8] "20000" "20000" "20000"
> ...

Aye -- thank you very much!

Peace,
david
-- 
David H. Wolfskill				r at catwhisker.org
Those who would murder in the name of God or prophet are blasphemous cowards.

See http://www.catwhisker.org/~david/publickey.gpg for my public key.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 949 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150911/9ad0700f/attachment.bin>

From ronald.koelpin at gmail.com  Fri Sep 11 14:20:12 2015
From: ronald.koelpin at gmail.com (=?UTF-8?Q?Ronald_K=c3=b6lpin?=)
Date: Fri, 11 Sep 2015 14:20:12 +0200
Subject: [R] maximum-likelihood-estimation with mle()
Message-ID: <55F2C6FC.7080903@gmail.com>

Hi everyone,

I have a problem with maximum-likelihood-estimation in the following
situation:

Assume a functional relation y = f(x) (the specific form of f should be
irrelevant). For my observations I assume (for simplicity) white noise,
such that hat(y_i) = f(x_i) + epsilon_i, with the epsilon_i iid N(0,
sigma^2). Y_i should then be N(f(x_i), sigma^2)-distributed and due to
the iid assumption the density of Y = (Y_1, ..., Y_n) is simply the
product of the individual densities, taking the log gives the the sum of
the log of individual densities.

I tried coding this in R with a simple example: f(x) = a*x + b (simple
linear regression). This way I wanted to compare the results from my
ml-estimation (specifying the log-likelihood manually and estimating
with mle()) with the results from using lm(y~x). In my example however
it doesn't work:

x <- 1:10
y <- 3*x - 1 + rnorm(length(x), mean=0, sd=0.5)

library("stats4")
nLL <- function(a, b, sigma)
{
  -sum(dnorm(y, mean=a*x+b, sd=sigma, log=TRUE))
}

fit <- mle(nLL1, start=list(a=0, b=0, sigma=1), nobs=length(y))

summary(lm(y~x))
summary(fit)

These should be the same but the aren't. I must have made some mistake
specifying the (negative) log-likehood (but I just don't see it). I also
actually don't care much (at the moment) for estimating sigma but I
don't know of a way to specify (and estimate) the (negative)
log-likelihood without estimating sigma.

Thanks a lot
and kind regards

Ronald Koelpin


From pdalgd at gmail.com  Fri Sep 11 15:24:07 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 11 Sep 2015 15:24:07 +0200
Subject: [R] maximum-likelihood-estimation with mle()
In-Reply-To: <55F2C6FC.7080903@gmail.com>
References: <55F2C6FC.7080903@gmail.com>
Message-ID: <72CE6C02-E1AE-4FE8-891B-97C220970726@gmail.com>

You are being over-optimistic with your starting values, and/or with constrains on the parameter space. 
Your fit is diverging in sigma for some reason known only to nonlinear-optimizer gurus...

For me, it works either to put in an explicit constraint or to reparametrize with log(sigma).

E.g.

> fit <- mle(nLL, start=list(a=0, b=0, sigma=1), method="L-BFGS-B", lower=c(-Inf, -Inf, 0.001))
> summary(fit)
Maximum likelihood estimation

Call:
mle(minuslogl = nLL, start = list(a = 0, b = 0, sigma = 1), method = "L-BFGS-B", 
    lower = c(-Inf, -Inf, 0.001))

Coefficients:
        Estimate Std. Error
a      3.0293645 0.01904026
b     -1.1516406 0.11814174
sigma  0.1729418 0.03866673

-2 log L: -6.717779 

or 

> nLL <- function(a, b, lsigma)
+  -sum(dnorm(y, mean=a*x+b, sd=exp(lsigma), log=TRUE))
> fit <- mle(nLL, start=list(a=0, b=0, lsigma=0))
> summary(fit)
Maximum likelihood estimation

Call:
mle(minuslogl = nLL, start = list(a = 0, b = 0, lsigma = 0))

Coefficients:
        Estimate Std. Error
a       3.029365 0.01903975
b      -1.151642 0.11813856
lsigma -1.754827 0.22360675

-2 log L: -6.717779 

both of which reproduce lm() except for DF issues.

To fix sigma, use fixed=list(sigma=0.19) in mle(). This also stabilizes the convergence (which it blinking well should, since it is now a purely quadratic problem).

-pd


On 11 Sep 2015, at 14:20 , Ronald K?lpin <ronald.koelpin at gmail.com> wrote:

> Hi everyone,
> 
> I have a problem with maximum-likelihood-estimation in the following
> situation:
> 
> Assume a functional relation y = f(x) (the specific form of f should be
> irrelevant). For my observations I assume (for simplicity) white noise,
> such that hat(y_i) = f(x_i) + epsilon_i, with the epsilon_i iid N(0,
> sigma^2). Y_i should then be N(f(x_i), sigma^2)-distributed and due to
> the iid assumption the density of Y = (Y_1, ..., Y_n) is simply the
> product of the individual densities, taking the log gives the the sum of
> the log of individual densities.
> 
> I tried coding this in R with a simple example: f(x) = a*x + b (simple
> linear regression). This way I wanted to compare the results from my
> ml-estimation (specifying the log-likelihood manually and estimating
> with mle()) with the results from using lm(y~x). In my example however
> it doesn't work:
> 
> x <- 1:10
> y <- 3*x - 1 + rnorm(length(x), mean=0, sd=0.5)
> 
> library("stats4")
> nLL <- function(a, b, sigma)
> {
>  -sum(dnorm(y, mean=a*x+b, sd=sigma, log=TRUE))
> }
> 
> fit <- mle(nLL1, start=list(a=0, b=0, sigma=1), nobs=length(y))
> 
> summary(lm(y~x))
> summary(fit)
> 
> These should be the same but the aren't. I must have made some mistake
> specifying the (negative) log-likehood (but I just don't see it). I also
> actually don't care much (at the moment) for estimating sigma but I
> don't know of a way to specify (and estimate) the (negative)
> log-likelihood without estimating sigma.
> 
> Thanks a lot
> and kind regards
> 
> Ronald Koelpin
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ghada.f.mm at gmail.com  Fri Sep 11 16:18:16 2015
From: ghada.f.mm at gmail.com (Ghada Almousa)
Date: Fri, 11 Sep 2015 17:18:16 +0300
Subject: [R] Help me in cluster
In-Reply-To: <CADG8gku-zLsdkekcYHZYL-7N-diAYbNKiOS+hFo8GknGFuVKow@mail.gmail.com>
References: <CADG8gku-zLsdkekcYHZYL-7N-diAYbNKiOS+hFo8GknGFuVKow@mail.gmail.com>
Message-ID: <CADG8gkv564J0DiCLHL6VPjJ=L6s1F3EX5+o_OKd8t8pOox-q4w@mail.gmail.com>

On Tuesday, September 8, 2015, Ghada Almousa <ghada.f.mm at gmail.com> wrote:

> I have project to study and analysis clusters algorithm in R
> "K-mean, Hierarchical, Density based and EM"
> I want to calculate
> Cluster instance , number of iteration , sum of squared error SSE and the
> accuracy for each cluster algorithms that i mention above
> And the log likelihood for EM and DBSCAN
>
> I wrote code in r for
> "K-mean, Hierarchical, Density based and EM
> But i can't calculate
> Cluster instance , number of iteration , sum of squared error SSE and the
> accuracy for each cluster algorithms that i mention above
> And the log likelihood for EM and DBSCAN

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Fri Sep 11 16:34:01 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 11 Sep 2015 10:34:01 -0400
Subject: [R] Help me in cluster
In-Reply-To: <CADG8gkv564J0DiCLHL6VPjJ=L6s1F3EX5+o_OKd8t8pOox-q4w@mail.gmail.com>
References: <CADG8gku-zLsdkekcYHZYL-7N-diAYbNKiOS+hFo8GknGFuVKow@mail.gmail.com>
	<CADG8gkv564J0DiCLHL6VPjJ=L6s1F3EX5+o_OKd8t8pOox-q4w@mail.gmail.com>
Message-ID: <CAM_vjumqSdqPk5_F5LYNtFn0phM5k1wXhfp6SfK1HbRYVQ65ZA@mail.gmail.com>

Repeating your post won't help. Writing a good question with sample
data and the code you've tried, as well as describing your *specific*
difficulties will.

Without a reproducible example that includes some sample data provided
using dput() (fake is fine), the code you used, and some clear idea of
what output you expect, it's impossible to figure out how to help you.
Here are some suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

You were already asked whether this was homework: the list has a
no-homework policy. Even if it isn't, nobody here will do your entire
project, which is what you are asking.


On Fri, Sep 11, 2015 at 10:18 AM, Ghada Almousa <ghada.f.mm at gmail.com> wrote:
> On Tuesday, September 8, 2015, Ghada Almousa <ghada.f.mm at gmail.com> wrote:
>
>> I have project to study and analysis clusters algorithm in R
>> "K-mean, Hierarchical, Density based and EM"
>> I want to calculate
>> Cluster instance , number of iteration , sum of squared error SSE and the
>> accuracy for each cluster algorithms that i mention above
>> And the log likelihood for EM and DBSCAN
>>
>> I wrote code in r for
>> "K-mean, Hierarchical, Density based and EM
>> But i can't calculate
>> Cluster instance , number of iteration , sum of squared error SSE and the
>> accuracy for each cluster algorithms that i mention above
>> And the log likelihood for EM and DBSCAN
>
>         [[alternative HTML version deleted]]

Not posting in HTML is another good practice.

> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

So is reading the posting guide.

-- 
Sarah Goslee
http://www.functionaldiversity.org


From pdalgd at gmail.com  Fri Sep 11 16:42:07 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 11 Sep 2015 16:42:07 +0200
Subject: [R] Generate a vector of values,
	given a vector of keys and a table?
In-Reply-To: <CAGxFJbQUmA2CAeDomXOX7GaScn4ZGMKZwpJARhF-f1YOKHXQvg@mail.gmail.com>
References: <20150911050429.GB1131@albert.catwhisker.org>
	<CAGxFJbQUmA2CAeDomXOX7GaScn4ZGMKZwpJARhF-f1YOKHXQvg@mail.gmail.com>
Message-ID: <5496C036-7767-4679-899B-30A79B86FDE3@gmail.com>

Or change the data format slightly and use indexing:

> l
     key    val    
[1,] "1.1"  "10000"
[2,] "1.9"  "10000"
[3,] "1.4"  "15000"
[4,] "1.5"  "20000"
[5,] "1.15" "25000"
> v <- l[,2]
> names(v) <- l[,1]
> x <- c("1.9", "1.9", "1.1", "1.1", "1.4", "1.4", "1.5", "1.5", "1.5", 
+ "1.5")
> v[x]
    1.9     1.9     1.1     1.1     1.4     1.4     1.5     1.5     1.5     1.5 
"10000" "10000" "10000" "10000" "15000" "15000" "20000" "20000" "20000" "20000" 
> 


On 11 Sep 2015, at 07:30 , Bert Gunter <bgunter.4567 at gmail.com> wrote:

> ?match
> 
> as in:
> 
>> y <- lk_up[match(x,lk_up[,"key"]),"val"]
>> y
> [1] "10000" "10000" "10000" "10000" "15000" "15000" "20000"
> [8] "20000" "20000" "20000"
> 
> 
> 
> Bert
> 
> 
> 
> Bert Gunter
> 
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>   -- Clifford Stoll
> 
> 
> On Thu, Sep 10, 2015 at 10:04 PM, David Wolfskill <r at catwhisker.org> wrote:
>> I apologize in advance: I must be overlooking something quite simple,
>> but I'm failing to make progress.
>> 
>> Suppose I have a "lookup table":
>> 
>> Browse[2]> dput(lk_up)
>> structure(c("1.1", "1.9", "1.4", "1.5", "1.15", "10000", "10000",
>> "15000", "20000", "25000"), .Dim = c(5L, 2L), .Dimnames = list(
>>    NULL, c("key", "val")))
>> Browse[2]> lk_up
>>     key    val
>> [1,] "1.1"  "10000"
>> [2,] "1.9"  "10000"
>> [3,] "1.4"  "15000"
>> [4,] "1.5"  "20000"
>> [5,] "1.15" "25000"
>> 
>> and a vector whose elements correspond with the "key" column of the
>> table:
>> 
>> Browse[2]> dput(x)
>> c("1.9", "1.9", "1.1", "1.1", "1.4", "1.4", "1.5", "1.5", "1.5",
>> "1.5")
>> Browse[2]> x
>> [1] "1.9" "1.9" "1.1" "1.1" "1.4" "1.4" "1.5" "1.5" "1.5" "1.5"
>> Browse[2]>
>> 
>> Is there a (relatively) simple (i.e., not explicitly looping) construct
>> that will yield a vector of the same size and shape as "x", but contain
>> the "value" entries from the lookup table (preserving the sequence: the
>> 1st entry of the result must correspond to the 1st entry of the list of
>> keys) -- in the current example:
>> 
>> Browse[2]> dput(y)
>> c("10000", "10000", "10000", "10000", "15000", "15000", "20000",
>> "20000", "20000", "20000")
>> Browse[2]> y
>> [1] "10000" "10000" "10000" "10000" "15000" "15000" "20000" "20000"
>> "20000" "20000"
>> Browse[2]>
>> 
>> I am (unfortunately) presently limited to R-3.0.2.
>> 
>> Thanks....
>> 
>> Peace,
>> david
>> --
>> David H. Wolfskill                              r at catwhisker.org
>> Those who would murder in the name of God or prophet are blasphemous cowards.
>> 
>> See http://www.catwhisker.org/~david/publickey.gpg for my public key.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From lid.zigh at gmail.com  Fri Sep 11 16:45:42 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Fri, 11 Sep 2015 09:45:42 -0500
Subject: [R] question about categorical variables in R
Message-ID: <CAMqbV1AEcFrbWbzCNs1ijmcqdpJJEd+jjK6VXa7h-tfGtBrMzw@mail.gmail.com>

Hi dear experts,
I have a general question in R, about the categorical variable such as
Gender(Male or Female)
If I have this column in my data and wanted to do regression model or feed
the data to  seqmeta packages (singlesnp, skat meta) , would you please let
me know should I code them first ( male=0 and female=1) or R programming do
it for me?
Because when I didn't code them, R still can do the analysis without any
error but I'm not sure it's correct or not?
Thanks

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Sep 11 16:55:19 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 11 Sep 2015 07:55:19 -0700
Subject: [R] question about categorical variables in R
In-Reply-To: <CAMqbV1AEcFrbWbzCNs1ijmcqdpJJEd+jjK6VXa7h-tfGtBrMzw@mail.gmail.com>
References: <CAMqbV1AEcFrbWbzCNs1ijmcqdpJJEd+jjK6VXa7h-tfGtBrMzw@mail.gmail.com>
Message-ID: <CAGxFJbQs_Q+RomqafiEUxBqTNqCiuVF-fgzktys6SFXvPGGEkQ@mail.gmail.com>

It's correct.

You need to spend some time with an R tutorial -- there are many on
the web -- and learn about how R handles factors in modeling.

?factor

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Sep 11, 2015 at 7:45 AM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> Hi dear experts,
> I have a general question in R, about the categorical variable such as
> Gender(Male or Female)
> If I have this column in my data and wanted to do regression model or feed
> the data to  seqmeta packages (singlesnp, skat meta) , would you please let
> me know should I code them first ( male=0 and female=1) or R programming do
> it for me?
> Because when I didn't code them, R still can do the analysis without any
> error but I'm not sure it's correct or not?
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Fri Sep 11 17:00:44 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 11 Sep 2015 08:00:44 -0700
Subject: [R] question about categorical variables in R
In-Reply-To: <CAMqbV1AEcFrbWbzCNs1ijmcqdpJJEd+jjK6VXa7h-tfGtBrMzw@mail.gmail.com>
References: <CAMqbV1AEcFrbWbzCNs1ijmcqdpJJEd+jjK6VXa7h-tfGtBrMzw@mail.gmail.com>
Message-ID: <D5C404D0-B2DF-4DA8-BC08-8C4067EE6DB9@dcn.davis.CA.us>

You need to read the Introduction to R, paying particular attention to the factors data type, which is designed for this problem.

You should also be aware that on this list failure to include a small example of your problem in R, using plain text email (a setting in your email program), often leads to getting no response at all. Conversely, if you do provide an example, the response will often include modifications to your example code that you can study.[1] Also, you really ought to read the Posting Guide given at the bottom of every R-help posting.

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 11, 2015 7:45:42 AM PDT, Lida Zeighami <lid.zigh at gmail.com> wrote:
>Hi dear experts,
>I have a general question in R, about the categorical variable such as
>Gender(Male or Female)
>If I have this column in my data and wanted to do regression model or
>feed
>the data to  seqmeta packages (singlesnp, skat meta) , would you please
>let
>me know should I code them first ( male=0 and female=1) or R
>programming do
>it for me?
>Because when I didn't code them, R still can do the analysis without
>any
>error but I'm not sure it's correct or not?
>Thanks
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From sarah.goslee at gmail.com  Fri Sep 11 17:15:34 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 11 Sep 2015 11:15:34 -0400
Subject: [R] Help me in cluster
In-Reply-To: <CADG8gks1oKHku0tp6Y9ufMH=wJTXreqtO_Tm2g_kSXkOs5xj9A@mail.gmail.com>
References: <CADG8gku-zLsdkekcYHZYL-7N-diAYbNKiOS+hFo8GknGFuVKow@mail.gmail.com>
	<CADG8gkv564J0DiCLHL6VPjJ=L6s1F3EX5+o_OKd8t8pOox-q4w@mail.gmail.com>
	<CAM_vjumqSdqPk5_F5LYNtFn0phM5k1wXhfp6SfK1HbRYVQ65ZA@mail.gmail.com>
	<CADG8gks1oKHku0tp6Y9ufMH=wJTXreqtO_Tm2g_kSXkOs5xj9A@mail.gmail.com>
Message-ID: <CAM_vju=rT=xW6svOjAmM_s9UHOmddnBv0=5aNORPgHF6osufxw@mail.gmail.com>

Please reply to the list, not just me. I've added the list address to
my own reply.

On Fri, Sep 11, 2015 at 11:04 AM, Ghada Almousa <ghada.f.mm at gmail.com> wrote:
> If you don't help me , why put help and subscription
> this is  not a homework it's question I searched  in the web  but there is
> no answer

The participants in this list are happy to help with specific R
questions, as discussed in the posting guide, and ideally specific
questions accompanied by reproducible data and code, as I requested
directly and as also discussed in the posting guide.

As stated, your question is so vague that I would have to write many
pages to explain it adequately, and I'm not doing that for free. (Or
for money either; I have my own work to do.)

It's also obvious that however you were searching, you weren't doing a
very good job. Even the main R help page for, eg, kmeans() discusses
SSE, and that very quantity is returned by the function itself. Ditto
number of iterations, etc.

You need to do some work on your own before we can help you.

Sarah

> 2015-09-11 17:34 GMT+03:00 Sarah Goslee <sarah.goslee at gmail.com>:
>>
>> Repeating your post won't help. Writing a good question with sample
>> data and the code you've tried, as well as describing your *specific*
>> difficulties will.
>>
>> Without a reproducible example that includes some sample data provided
>> using dput() (fake is fine), the code you used, and some clear idea of
>> what output you expect, it's impossible to figure out how to help you.
>> Here are some suggestions for creating a good reproducible example:
>>
>> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>>
>> You were already asked whether this was homework: the list has a
>> no-homework policy. Even if it isn't, nobody here will do your entire
>> project, which is what you are asking.
>>
>>
>> On Fri, Sep 11, 2015 at 10:18 AM, Ghada Almousa <ghada.f.mm at gmail.com>
>> wrote:
>> > On Tuesday, September 8, 2015, Ghada Almousa <ghada.f.mm at gmail.com>
>> > wrote:
>> >
>> >> I have project to study and analysis clusters algorithm in R
>> >> "K-mean, Hierarchical, Density based and EM"
>> >> I want to calculate
>> >> Cluster instance , number of iteration , sum of squared error SSE and
>> >> the
>> >> accuracy for each cluster algorithms that i mention above
>> >> And the log likelihood for EM and DBSCAN
>> >>
>> >> I wrote code in r for
>> >> "K-mean, Hierarchical, Density based and EM
>> >> But i can't calculate
>> >> Cluster instance , number of iteration , sum of squared error SSE and
>> >> the
>> >> accuracy for each cluster algorithms that i mention above
>> >> And the log likelihood for EM and DBSCAN
>> >
>> >         [[alternative HTML version deleted]]
>>
>> Not posting in HTML is another good practice.
>>
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> So is reading the posting guide.
>>
>> --
>> Sarah Goslee
>> http://www.functionaldiversity.org
>
>


From dcarlson at tamu.edu  Fri Sep 11 17:37:19 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Fri, 11 Sep 2015 15:37:19 +0000
Subject: [R] kendall tau distance
In-Reply-To: <DUB125-W94DB648B732F8E71C200C2B3510@phx.gbl>
References: <DUB125-W94DB648B732F8E71C200C2B3510@phx.gbl>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C0914@mb02.ads.tamu.edu>

The Wikipedia article gives a simple formula based on the number of discordant pairs. You can get that from the ConDisPairs() function in package DescTools.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ragia Ibrahim
Sent: Thursday, September 10, 2015 12:40 PM
To: r-help at r-project.org
Subject: [R] kendall tau distance

Dear group
how to calculate ?kendall tau distance according to ?Kendall_tau_distance at wikipedia

?<a href="https&#58;&#47;&#47;en.wikipedia.org&#47;wiki&#47;Kendall_tau_distance" target="_blank" class="newlyinsertedlink">https&#58;&#47;&#47;en.wikipedia.org&#47;wiki&#47;Kendall_tau_distance</a>?


thanks in advance
Ragia 		 	   		  
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From 2hanl2da at naver.com  Fri Sep 11 12:53:09 2015
From: 2hanl2da at naver.com (massmatics)
Date: Fri, 11 Sep 2015 03:53:09 -0700 (PDT)
Subject: [R] I need help with extracting data :(
Message-ID: <1441968789907-4712128.post@n4.nabble.com>

So this is the question that I need help on. :(
4.) ?Warpbreaks? is a built-in dataset in R. Load it using the function
**data(warpbreaks)**. It consists of the number of warp breaks per loom,
where a loom corresponds to a fixed length of yarn. It has three variables
namely, breaks, wool, and tension.
a.)	Write a code (Hint: a logical expression) that extracts the observations
with wool type A and tension level M. Assign it to the object
**AM.warpbreaks**.
b.)	For the ?AM.warpbreaks? dataset, compute for the mean and the standard
deviation of the breaks variable for those observations with **breaks value
not exceeding 30**. 

My code for 4a) (However, it didn't work. Can somebody help me how to solve
this problem?) :(
>warpbreak <- data(warpbreaks("breaks", "wool", "tension"))

>AM.warpbreaks <- c('','type A','level M')

Thanks! :)




--
View this message in context: http://r.789695.n4.nabble.com/I-need-help-with-extracting-data-tp4712128.html
Sent from the R help mailing list archive at Nabble.com.


From allennugent at hotmail.com  Fri Sep 11 14:44:56 2015
From: allennugent at hotmail.com (AltShift)
Date: Fri, 11 Sep 2015 05:44:56 -0700 (PDT)
Subject: [R] Is there a time series resampling function ?
In-Reply-To: <1441707402995-4711987.post@n4.nabble.com>
References: <1441540485695-4711907.post@n4.nabble.com>
	<2F9D3942-C7B6-4497-A62A-AF5CBBE610F4@dcn.davis.CA.us>
	<CAJRuHop-Nki7K7ZAv22dEv2PBNMF4SH4sXGUGdSQY3y0NA40-w@mail.gmail.com>
	<1441707402995-4711987.post@n4.nabble.com>
Message-ID: <1441975496283-4712133.post@n4.nabble.com>

In case you were wondering, I ended up writing a function that calls
spline().  The resample() function wasn't suitable for my requirement.

But, thanks for steering me in the right direction!




--
View this message in context: http://r.789695.n4.nabble.com/Is-there-a-time-series-resampling-function-tp4711907p4712133.html
Sent from the R help mailing list archive at Nabble.com.


From arfelizardo at isa.utl.pt  Fri Sep 11 15:00:32 2015
From: arfelizardo at isa.utl.pt (Ana Raquel Felizardo Rodrigues)
Date: Fri, 11 Sep 2015 14:00:32 +0100
Subject: [R] hist() bar width definition
Message-ID: <936BD41B82474954AB87EE1638556178@DavidPC>

Hi,

I have a realy dumb question about hist(). Is it possible to define classes width independently from breaks? I have breaks=c(0,2,100,max(mydata)), but would like the 3 bars to have the same width and to label x axe with 0, 2 and >100. Can anyone help me?

Thank you all!

Ana Raquel Rodrigues
	[[alternative HTML version deleted]]


From Julianeleuschner at web.de  Fri Sep 11 16:15:06 2015
From: Julianeleuschner at web.de (Juli)
Date: Fri, 11 Sep 2015 07:15:06 -0700 (PDT)
Subject: [R] removing outlier
Message-ID: <1441980906861-4712137.post@n4.nabble.com>

Hey,

i want to remove outliers so I tried do do this: 

# 1 define mean and sd
sd.AT_ZU_SPAET <- sd(AT_ZU_SPAET)
mitt.AT_ZU_SPAET <- mean(AT_ZU_SPAET)
#
sd.Anzahl_BAF <- sd(Anzahl_BAF)
mitt.Anzahl_BAF <- mean(Anzahl_BAF)
#
sd.?nderungsintervall <- sd(?nderungsintervall)
mitt.?nderungsintervall <- mean(?nderungsintervall)
#
# 2 identify outliers 
DA[ abs(AT_ZU_SPAET - mitt.AT_ZU_SPAET) > ( 3 * sd.AT_ZU_SPAET)  , ]
DA[ abs(Anzahl_BAF - mitt.Anzahl_BAF) > ( 3 * sd.Anzahl_BAF)  , ]
DA[ abs(?nderungsintervall - mitt.?nderungsintervall) > ( 3 *
sd.?nderungsintervall)  , ]
#
# 3 remove outliers
AT_ZU_SPAET.clean <- DA[ (abs(AT_ZU_SPAET - mitt.AT_ZU_SPAET) <
(3*sd.AT_ZU_SPAET)), ]
Anzahl_BAF.clean <- DA[ (abs(Anzahl_BAF - mitt.Anzahl_BAF) <
(3*sd.Anzahl_BAF)), ]
?nderungsintervall.clean <- DA[ (abs(?nderungsintervall -
mitt.?nderungsintervall) <
(3*sd.?nderungsintervall)), ]

My problem ist, that I am only able to remove the outliers of one column of
my table, but I want to remove the outliers of every column of the table. 

Could anybody help me?




--
View this message in context: http://r.789695.n4.nabble.com/removing-outlier-tp4712137.html
Sent from the R help mailing list archive at Nabble.com.


From 2hanl2da at naver.com  Fri Sep 11 16:48:33 2015
From: 2hanl2da at naver.com (massmatics)
Date: Fri, 11 Sep 2015 07:48:33 -0700 (PDT)
Subject: [R] how to find the mean and sd :(
Message-ID: <1441982913615-4712141.post@n4.nabble.com>

I need help on 4b please :( 
4.) ?Warpbreaks? is a built-in dataset in R. Load it using the function
data(warpbreaks). It consists of the number of warp breaks per loom, where a
loom corresponds to a fixed length of yarn. It has three variables namely,
breaks, wool, and tension.

a.)	Write a code (Hint: a logical expression) that extracts the observations
with wool type A and tension level M. Assign it to the object AM.warpbreaks
*(This is done)*

b.) For the ?AM.warpbreaks? dataset, compute for the mean and the standard
deviation of the breaks variable for those observations with breaks value
not exceeding 30.

data(warpbreaks)

warpbreaks <- data.frame(warpbreaks)

AM.warpbreaks <- subset(warpbreaks, wool=="A" & tension=="M")

*> mean(AM.warpbreaks<=30)

>sd(AM.warpbreaks<=30)*

This is what I understood this problem and typed the code as in the last two
lines. However, I wasn't able to run the last two lines while the first 3
lines ran successfully. Can anybody tell me what is the error here? Thanks!
:)



--
View this message in context: http://r.789695.n4.nabble.com/how-to-find-the-mean-and-sd-tp4712141.html
Sent from the R help mailing list archive at Nabble.com.


From aldi at dsgmail.wustl.edu  Fri Sep 11 17:11:43 2015
From: aldi at dsgmail.wustl.edu (Aldi)
Date: Fri, 11 Sep 2015 10:11:43 -0500
Subject: [R] how to split row elements [1] and [2] of a string variable
 A via srtsplit and sapply
In-Reply-To: <CAGxFJbRGd-nPpzne7uBJh5R1i04PqnBphnMfp+pNqQiyVwCycA@mail.gmail.com>
References: <55F1C1EE.4090308@wustl.edu>
	<CAAxdm-4Mu70QS1fTowW+EgaRPUGc0rTs63eAuNZCjh+ivfAn5A@mail.gmail.com>
	<CAGxFJbRGd-nPpzne7uBJh5R1i04PqnBphnMfp+pNqQiyVwCycA@mail.gmail.com>
Message-ID: <55F2EF2F.8030903@dsgmail.wustl.edu>

Thank you Jim and Bert for your suggestions.

Following is the final version used:
### Original tiny test data from Aldi Kraja, 9.11.2015.
### Purpose: split A into element 1 and 2, not interested on 3d element 
of A. Assign element one and two to vectors C and D of the same data.frame.
### Do similar work that SAS SCAN function could have done: 
C=SCAN(x,1":") ; D=SCAN(x,2,":") ;
### Jim Holtman suggested

### temp <- strsplit(x$A, ":")
### x$C <- sapply(temp, '[[', 1)
### x$D <- sapply(temp, '[[', 2)

### Bert Gunter suggested:
### do.call(rbind,strsplit(x[[1]],":"))[,-3]

### Start of script: a full R solution:

x <- read.table(text = "A          B
   1:29439275 0.46773514
   5:85928892 0.81283052
   10:128341232 0.09332543
   1:106024283:ID 0.36307805
   3:62707519 0.42657952
   2:80464120 0.89125094", header = TRUE, as.is = TRUE)
  

x$A <- as.character(x$A)
temp <- strsplit(x$A,":")
x$C <- sapply(temp,'[[',1)
x$D <- sapply(temp,'[[',2)
x$C <- as.numeric(x$C)
x$D <- as.numeric(x$D)
### Final results:
x
### end of the script
# A          B  C         D
#1     1:29439275 0.46773514  1  29439275
#2     5:85928892 0.81283052  5  85928892
#3   10:128341232 0.09332543 10 128341232
#4 1:106024283:ID 0.36307805  1 106024283
#5     3:62707519 0.42657952  3  62707519
#6     2:80464120 0.89125094  2  80464120
With best wishes,

Aldi


On 9/10/2015 1:35 PM, Bert Gunter wrote:
> ...
> Alternatively, you can avoid the looping (i.e. sapply) altogether by:
>
> do.call(rbind,strsplit(x[[1]],":"))[,-3]
>
>
>       [,1] [,2]
> [1,] "1"  "29439275"
> [2,] "5"  "85928892"
> [3,] "10" "128341232"
> [4,] "1"  "106024283"
> [5,] "3"  "62707519"
> [6,] "2"  "80464120"
>
> These can then be added to the existing frame, converted to numeric, etc.
>
> Cheers,
> Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>     -- Clifford Stoll
>
>
> On Thu, Sep 10, 2015 at 11:05 AM, jim holtman <jholtman at gmail.com> wrote:
>> try this:
>>
>>
>>> x <- read.table(text = "A          B
>> +  1:29439275 0.46773514
>> +  5:85928892 0.81283052
>> +  10:128341232 0.09332543
>> +  1:106024283:ID 0.36307805
>> +  3:62707519 0.42657952
>> +  2:80464120 0.89125094", header = TRUE, as.is = TRUE)
>>> temp <- strsplit(x$A, ":")
>>> x$C <- sapply(temp, '[[', 1)
>>> x$D <- sapply(temp, '[[', 2)
>>>
>>> x
>>                 A          B  C         D
>> 1     1:29439275 0.46773514  1  29439275
>> 2     5:85928892 0.81283052  5  85928892
>> 3   10:128341232 0.09332543 10 128341232
>> 4 1:106024283:ID 0.36307805  1 106024283
>> 5     3:62707519 0.42657952  3  62707519
>> 6     2:80464120 0.89125094  2  80464120
>>
>>
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Thu, Sep 10, 2015 at 1:46 PM, aldi <aldi at wustl.edu> wrote:
>>
>>> Hi,
>>> I have a data.frame x1, of which a variable A needs to be split by
>>> element 1 and element 2 where separator is ":". Sometimes could be three
>>> elements in A, but I do not need the third element.
>>>
>>> Since R does not have a SCAN function as in SAS, C=scan(A,1,":");
>>> D=scan(A,2,":");
>>> I am using a combination of strsplit and sapply. If I do not use the
>>> index [i] then R captures the full vector . Instead I need row by row
>>> capturing the first and the second element and from them create two new
>>> variables C and D.
>>> Right now as is somehow in the loop i C is captured correctly, but D is
>>> missing because the variables AA does not have it. Any suggestions?
>>> Thank you in advance, Aldi
>>>
>>> A          B
>>> 1:29439275 0.46773514
>>> 5:85928892 0.81283052
>>> 10:128341232 0.09332543
>>> 1:106024283:ID 0.36307805
>>> 3:62707519 0.42657952
>>> 2:80464120 0.89125094
>>>
>>> x1<-read.table(file='./test.txt',head=T,sep='\t')
>>> x1$A <- as.character(x1$A)
>>>
>>> for(i in 1:length(x1$A)){
>>>
>>> x1$AA[i] <- as.numeric(unlist(strsplit(x1$A[i],':')))
>>>
>>> x1$C[i] <- sapply(x1$AA[i],function(x)x[1])
>>> x1$D[i] <- sapply(x1$AA[i],function(x)x[2])
>>> }
>>>
>>> x1
>>>
>>>
>>>
>>>   > x1
>>>                  A          B AA  C  D
>>> 1     1:29439275 0.46773514  1  1 NA
>>> 2     5:85928892 0.81283052  5  5 NA
>>> 3   10:128341232 0.09332543 10 10 NA
>>> 4 1:106024283:ID 0.36307805  1  1 NA
>>> 5     3:62707519 0.42657952  3  3 NA
>>> 6     2:80464120 0.89125094  2  2 NA
>>>
>>>
>>> --
>>>
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


-- 


	[[alternative HTML version deleted]]


From yradi at hotmail.com  Fri Sep 11 17:13:16 2015
From: yradi at hotmail.com (vegas55)
Date: Fri, 11 Sep 2015 08:13:16 -0700 (PDT)
Subject: [R] textmining-Clusting-dendogram : invalid dendrogram input
Message-ID: <1441984396521-4712145.post@n4.nabble.com>

HI All, 

Please I need help with the following. I use the TM package on text mining
purpuse. Everything works fine until the stage of trying to do a dendogram.
R gives this message (See the end of the script) :

Error in graphics:::plotHclust(n, merge, height, order(x$order), hang,  : 
  invalid dendrogram input


# Lire le fichier texte
filePath <- "C:\\BI\\projet supervise\\Sondage satisfaction
PV//PVTextMining.txt"
text <- readLines(filePath)

# Charger les donn?es comme un corpus
docs <- Corpus(VectorSource(text))
inspect(docs)

# Convertir le texte en minuscule
docs =tm_map(docs,tolower)
#docs <- tm_map(docs, content_transformer(tolower))

# Supprimer les nombres
docs <- tm_map(docs, removeNumbers)

# Supprimer les mots vides fran?ais
docs <- tm_map(docs, removeWords, stopwords("french"))

# Supprimer votre propre liste de mots non d?sir?s
docs <- tm_map(docs, removeWords, c("non") 

# Supprimer les ponctuations
docs <- tm_map(docs, removePunctuation)

# Supprimer les espaces vides suppl?mentaires
docs <- tm_map(docs, stripWhitespace)

# Text stemming
docs <- tm_map(docs, stemDocument)

#Enl?ve des caract?res sp?ciaux

docs = tm_map(docs, function(x) gsub("\\W", " ", x))

#Etape 4: Construire la matrice des mots

dtm <- TermDocumentMatrix(docs, control=list(wordLenghts=c(1, Inf)))

##############cluster

#######methode 1

#Hierarchical Clustering

dtm2 <- removeSparseTerms(dtm, sparse=0.95)
matrix2 <- as.matrix(dtm2)

#Now we compute the distance matrix for the hclust() function.

distMatrix <- dist(scale(matrix2))

cluster <- hclust(distMatrix, method="ward")
pp

pp <- dist(matrix2, method="ward")

#plot dendogram euclidean
windows()

plot(cluster, hang=-1, main="Clusters")

Error in graphics:::plotHclust(n, merge, height, order(x$order), hang,  : 
  invalid dendrogram input

Thanks a lot for your help ! 

YR






--
View this message in context: http://r.789695.n4.nabble.com/textmining-Clusting-dendogram-invalid-dendrogram-input-tp4712145.html
Sent from the R help mailing list archive at Nabble.com.


From pdalgd at gmail.com  Fri Sep 11 17:56:10 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 11 Sep 2015 17:56:10 +0200
Subject: [R] hist() bar width definition
In-Reply-To: <936BD41B82474954AB87EE1638556178@DavidPC>
References: <936BD41B82474954AB87EE1638556178@DavidPC>
Message-ID: <542E6696-B25F-4C12-9477-A55FC62B8177@gmail.com>


On 11 Sep 2015, at 15:00 , Ana Raquel Felizardo Rodrigues <arfelizardo at isa.utl.pt> wrote:

> Hi,
> 
> I have a realy dumb question about hist(). Is it possible to define classes width independently from breaks? I have breaks=c(0,2,100,max(mydata)), but would like the 3 bars to have the same width and to label x axe with 0, 2 and >100. Can anyone help me?

It's not a histogram then....

I think it is both practically and conceptually easier to do it as a barplot, as in

f <- cut(x, breaks=c(....))
levels(f) <-  c("0-2", "3-100", ">100") # or whatever...
barplot(table(f))

-pd 


> 
> Thank you all!
> 
> Ana Raquel Rodrigues
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From tom at maladmin.com  Fri Sep 11 18:02:41 2015
From: tom at maladmin.com (Tom Wright)
Date: Fri, 11 Sep 2015 12:02:41 -0400
Subject: [R] how to find the mean and sd :(
In-Reply-To: <1441982913615-4712141.post@n4.nabble.com>
References: <1441982913615-4712141.post@n4.nabble.com>
Message-ID: <1441987361.2128.8.camel@maladmin.com>

On Fri, 2015-09-11 at 07:48 -0700, massmatics wrote:
> AM.warpbreaks<=30

The above command is not returning what you expected, what part of the
AM.warpbreaks dataframe is expected to be <= 30?

Effectively you are using a two stage process.
1) Create a logical vector identifying rows in the dataframe with a
breaks value <= 30
2) use the vector in 1. to extract just the rows you are interested in
and use that to calculate the mean of the breaks column.


From bgunter.4567 at gmail.com  Fri Sep 11 18:17:01 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 11 Sep 2015 09:17:01 -0700
Subject: [R] how to find the mean and sd :(
In-Reply-To: <1441987361.2128.8.camel@maladmin.com>
References: <1441982913615-4712141.post@n4.nabble.com>
	<1441987361.2128.8.camel@maladmin.com>
Message-ID: <CAGxFJbQVc-gyG_uhunCj02Ym8=yJLE_nB3yLfmTnPmNQeP3k6w@mail.gmail.com>

?tapply

or better yet

?ave ## a wrapper for tapply

allows it to be done without extraction.

Cheers,

Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Sep 11, 2015 at 9:02 AM, Tom Wright <tom at maladmin.com> wrote:
> On Fri, 2015-09-11 at 07:48 -0700, massmatics wrote:
>> AM.warpbreaks<=30
>
> The above command is not returning what you expected, what part of the
> AM.warpbreaks dataframe is expected to be <= 30?
>
> Effectively you are using a two stage process.
> 1) Create a logical vector identifying rows in the dataframe with a
> breaks value <= 30
> 2) use the vector in 1. to extract just the rows you are interested in
> and use that to calculate the mean of the breaks column.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cdetermanjr at gmail.com  Fri Sep 11 18:17:08 2015
From: cdetermanjr at gmail.com (Charles Determan)
Date: Fri, 11 Sep 2015 11:17:08 -0500
Subject: [R] how to find the mean and sd :(
In-Reply-To: <1441987361.2128.8.camel@maladmin.com>
References: <1441982913615-4712141.post@n4.nabble.com>
	<1441987361.2128.8.camel@maladmin.com>
Message-ID: <CAKxd1KNOfWu==Y7LHYYOV57YMYK71UV0HyvB6JuF64Co-_PyPA@mail.gmail.com>

massmatics,

You are trying to take the mean/sd of an entire data.frame and therefore
you receive an error.  You must do some form of subset and take the mean of
the 'breaks' column.  This can be done a few ways (as with almost anything
in R).

AM.warpbreaks2 <- subset(AM.warpbreaks, breaks <= 30)
mean(AM.warpbreaks2$breaks)

or

mean(AM.warpbreaks$breaks[Am.warpbreaks$breaks <= 30])

or more concisely

with(AM.warpbreaks, mean(breaks[breaks <= 30]))

Again, the main point here is that you need to specify the column when
working with a data.frame object.

Regards,
Charles


On Fri, Sep 11, 2015 at 11:02 AM, Tom Wright <tom at maladmin.com> wrote:

> On Fri, 2015-09-11 at 07:48 -0700, massmatics wrote:
> > AM.warpbreaks<=30
>
> The above command is not returning what you expected, what part of the
> AM.warpbreaks dataframe is expected to be <= 30?
>
> Effectively you are using a two stage process.
> 1) Create a logical vector identifying rows in the dataframe with a
> breaks value <= 30
> 2) use the vector in 1. to extract just the rows you are interested in
> and use that to calculate the mean of the breaks column.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r at catwhisker.org  Fri Sep 11 18:54:16 2015
From: r at catwhisker.org (David Wolfskill)
Date: Fri, 11 Sep 2015 09:54:16 -0700
Subject: [R] Generate a vector of values,
 given a vector of keys and a table?
In-Reply-To: <5496C036-7767-4679-899B-30A79B86FDE3@gmail.com>
References: <20150911050429.GB1131@albert.catwhisker.org>
	<CAGxFJbQUmA2CAeDomXOX7GaScn4ZGMKZwpJARhF-f1YOKHXQvg@mail.gmail.com>
	<5496C036-7767-4679-899B-30A79B86FDE3@gmail.com>
Message-ID: <20150911165416.GR1131@albert.catwhisker.org>

On Fri, Sep 11, 2015 at 04:42:07PM +0200, peter dalgaard wrote:
> Or change the data format slightly and use indexing:
> 
> > l
>      key    val    
> [1,] "1.1"  "10000"
> [2,] "1.9"  "10000"
> [3,] "1.4"  "15000"
> [4,] "1.5"  "20000"
> [5,] "1.15" "25000"
> > v <- l[,2]
> > names(v) <- l[,1]
> > x <- c("1.9", "1.9", "1.1", "1.1", "1.4", "1.4", "1.5", "1.5", "1.5", 
> + "1.5")
> > v[x]
>     1.9     1.9     1.1     1.1     1.4     1.4     1.5     1.5     1.5     1.5 
> "10000" "10000" "10000" "10000" "15000" "15000" "20000" "20000" "20000" "20000" 
> > 
> ...

Yeah --  *that* is actually what I was trying to accomplish; thank you
for the help!  (Not to detract from Bert's suggestion -- that also
works.  But Peter's is what I had been trying to accomplish before I
gave in & sent my plea.)

[This is actually a part of a function I'm cobbling up to filter
data.frames by values of a given column, where the acceptable bounds for
the values can vary depending on the contents of a different column.
E.g., we might expect certain "performance-related" metrics to have
different ranges of interest for different classes of hardware.]

And the function appears to be working so far (though I'll be poking and
prodding it a bit yet).

Peace,
david
-- 
David H. Wolfskill				r at catwhisker.org
Those who would murder in the name of God or prophet are blasphemous cowards.

See http://www.catwhisker.org/~david/publickey.gpg for my public key.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 949 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150911/30b9e2f8/attachment.bin>

From hannah.hlx at gmail.com  Fri Sep 11 19:30:28 2015
From: hannah.hlx at gmail.com (li li)
Date: Fri, 11 Sep 2015 13:30:28 -0400
Subject: [R] Adding a second Y axis on a dotplot
Message-ID: <CAHLnndYU6zacv6DG3fBW-nmcrDzRJnDcJ+kmYQZTG2p0PxPaCA@mail.gmail.com>

Hi all,
  I plotted a dotplot based on the data below and code below. I would
like to add another yaxis on the right with a different col, different
tickmarks and a different label. Can anyone give some help?Thanks very
much!!
     Hanna


> tmp1
   result  lot  trt trtsymb trtcol
1      98 lot1 trt1       1   blue
2      99 lot2 trt1       1   blue
3      98 lot3 trt1       1   blue
4     100 lot4 trt1       1   blue
5     100 lot5 trt1       1   blue
6     101 lot6 trt1       1   blue
7     101 lot7 trt1       1   blue
8      99 lot8 trt1       1   blue
9     100 lot9 trt1       1   blue
10     94 lot1 trt2      16    red
11    105 lot2 trt2      16    red
12     87 lot3 trt2      16    red
13    119 lot4 trt2      16    red
14     96 lot5 trt2      16    red
15    113 lot6 trt2      16    red
16    106 lot7 trt2      16    red
17     71 lot8 trt2      16    red
18     95 lot9 trt2      16    red


library(lattice)
dotplot(result ~ lot, tmp1, cex=1.1,  ylab = "values", xlab="lot",
jitter.y = F, aspect=1.0,
        pch=tmp1$trtsymb, col=tmp1$trtcol, scales=list(rot=30),
main="", key = list(text = list(labels = c("trt1", "trt2"),cex=c(0.9,0.9)),
points = list(pch =c(1,12), col =c("blue", "red")),
space = "right"))


From Wang.Xue at mayo.edu  Fri Sep 11 21:35:46 2015
From: Wang.Xue at mayo.edu (Wang, Xue, Ph.D.)
Date: Fri, 11 Sep 2015 19:35:46 +0000
Subject: [R] the functionality of outputing junction reads in Rsubread
	package
Message-ID: <c10f8b$1da3dv@ironport10.mayo.edu>

Hi Rsubread developers and R users,

I am using R version 3.1 and the associated "Rsubread" Bioconductor package. The "subjunc" function allows the user to output junction reads in the following format, 

#Chr, StartLeftBlock, EndRightBlock, Junction_Name, nSupport, Strand, StartLeftBlock, EndRightBlock, Color, nBlocks, BlockSizes, BlockStarts
chr10	94009	94602	JUNC00000001	251	+	94009	94602	255,0,0		2	46,47	0,546
chr10 	94822	95396	JUNC00000002	993	-	94822	95396	0,255,255	2	30,49	0,525

My question is, for the second last column (BlockSizes column), is there a way to modify the default maximum block size from 49 to other value (e.g. 20)? I understand this may involve the modification of source C code and re-compilation.


Thanks,


Xue 
    


From jdnewmil at dcn.davis.CA.us  Fri Sep 11 23:12:29 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 11 Sep 2015 14:12:29 -0700
Subject: [R] the functionality of outputing junction reads in
	Rsubread	package
In-Reply-To: <c10f8b$1da3dv@ironport10.mayo.edu>
References: <c10f8b$1da3dv@ironport10.mayo.edu>
Message-ID: <05550E83-32FC-43BB-BC83-7FCC16D0D812@dcn.davis.CA.us>

This belongs in an email to the Rsubread maintainers, e.g. maintainer("Rsubread"), or on the Bioconductor mailing list.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 11, 2015 12:35:46 PM PDT, "Wang, Xue, Ph.D." <Wang.Xue at mayo.edu> wrote:
>Hi Rsubread developers and R users,
>
>I am using R version 3.1 and the associated "Rsubread" Bioconductor
>package. The "subjunc" function allows the user to output junction
>reads in the following format, 
>
>#Chr, StartLeftBlock, EndRightBlock, Junction_Name, nSupport, Strand,
>StartLeftBlock, EndRightBlock, Color, nBlocks, BlockSizes, BlockStarts
>chr10	94009	94602	JUNC00000001	251	+	94009	94602	255,0,0		2	46,47	0,546
>chr10
>	94822	95396	JUNC00000002	993	-	94822	95396	0,255,255	2	30,49	0,525
>
>My question is, for the second last column (BlockSizes column), is
>there a way to modify the default maximum block size from 49 to other
>value (e.g. 20)? I understand this may involve the modification of
>source C code and re-compilation.
>
>
>Thanks,
>
>
>Xue 
>    
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Sat Sep 12 01:10:31 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 12 Sep 2015 11:10:31 +1200
Subject: [R] I need help with extracting data :(
In-Reply-To: <1441968789907-4712128.post@n4.nabble.com>
References: <1441968789907-4712128.post@n4.nabble.com>
Message-ID: <55F35F67.7030804@auckland.ac.nz>


This is pretty clearly a homework question and this list does not do 
people's homework for them.

You need to learn more about the basics of R syntax.  Read "An 
Introduction to R", readily available from the R web site.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

On 11/09/15 22:53, massmatics wrote:
> So this is the question that I need help on. :(
> 4.) ?Warpbreaks? is a built-in dataset in R. Load it using the function
> **data(warpbreaks)**. It consists of the number of warp breaks per loom,
> where a loom corresponds to a fixed length of yarn. It has three variables
> namely, breaks, wool, and tension.
> a.)	Write a code (Hint: a logical expression) that extracts the observations
> with wool type A and tension level M. Assign it to the object
> **AM.warpbreaks**.
> b.)	For the ?AM.warpbreaks? dataset, compute for the mean and the standard
> deviation of the breaks variable for those observations with **breaks value
> not exceeding 30**.
>
> My code for 4a) (However, it didn't work. Can somebody help me how to solve
> this problem?) :(
>> warpbreak <- data(warpbreaks("breaks", "wool", "tension"))
>
>> AM.warpbreaks <- c('','type A','level M')


From yanita at meta.ua  Fri Sep 11 22:22:56 2015
From: yanita at meta.ua (tonina)
Date: Fri, 11 Sep 2015 13:22:56 -0700 (PDT)
Subject: [R] Installation R package Rssa from directory
Message-ID: <1442002976547-4712160.post@n4.nabble.com>

Hello. I'm trying to install R package Rssa from directory
install.packages("d:\\Rssa\\", repos=NULL, type="source")

But I have an error

* installing *binary* package 'Rssa' ...
Warning: running command 'cp -R . "E:/Program Files/R/R-3.1.3/library/Rssa"
|| ( tar cd - .| (cd "E:/Program Files/R/R-3.1.3/library/Rssa" && tar -xf -
))' had status 127
ERROR: installing binary package failed
* removing 'E:/Program Files/R/R-3.1.3/library/Rssa'
Warning in install.packages :
  running command '"E:/PROGRA~1/R/R-31~1.3/bin/x64/R" CMD INSTALL -l 
"E:\Program Files\R\R-3.1.3\library" "d:/Rssa/"' had status 1
Warning in install.packages :
 installation of package ?d:/Rssa/? had non-zero exit status
I installed this packages early from directory without problem. And when I
use this package without installation

library("Rssa", lib.loc="d:\\Rssa\\")

package works now too.

I have been looking for similar errors, but I still can not understand this
error and don't know what to do. Thank you for help.



--
View this message in context: http://r.789695.n4.nabble.com/Installation-R-package-Rssa-from-directory-tp4712160.html
Sent from the R help mailing list archive at Nabble.com.


From drjimlemon at gmail.com  Sat Sep 12 03:27:06 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 12 Sep 2015 11:27:06 +1000
Subject: [R] Adding a second Y axis on a dotplot
In-Reply-To: <CAHLnndYU6zacv6DG3fBW-nmcrDzRJnDcJ+kmYQZTG2p0PxPaCA@mail.gmail.com>
References: <CAHLnndYU6zacv6DG3fBW-nmcrDzRJnDcJ+kmYQZTG2p0PxPaCA@mail.gmail.com>
Message-ID: <CA+8X3fUKvFFaZ+my98eeekYZDSHYeNgyDPPT8zE_oc2u-CTC9w@mail.gmail.com>

Hi Hanna,
Not within lattice, but you could try this:

plot(as.numeric(tmp1$lot[tmp1$trt=="trt1"]),
 tmp1$result[tmp1$trt=="trt1"],xlab="lot",
 ylab="Values",type="p",axes=FALSE,
 col="red",pch=1,ylim=c(68,120))
abline(v=1:9,col="lightgray",lty=2)
box()
library(plotrix)
axis(1,at=1:9,labels=as.character(tmp1$lot[tmp1$trt=="trt1"]))
points(as.numeric(tmp1$lot[tmp1$trt=="trt2"]),
 tmp1$result[tmp1$trt=="trt2"],col="blue",pch=2)
fullaxis(2,at=seq(70,120,by=10),col="red")
fullaxis(4,at=seq(70,120,by=10),labels=7:12,col="blue")
axis.break(2,breakpos=68.5)
axis.break(4,breakpos=68.5)

Jim


On Sat, Sep 12, 2015 at 3:30 AM, li li <hannah.hlx at gmail.com> wrote:

> Hi all,
>   I plotted a dotplot based on the data below and code below. I would
> like to add another yaxis on the right with a different col, different
> tickmarks and a different label. Can anyone give some help?Thanks very
> much!!
>      Hanna
>
>
> > tmp1
>    result  lot  trt trtsymb trtcol
> 1      98 lot1 trt1       1   blue
> 2      99 lot2 trt1       1   blue
> 3      98 lot3 trt1       1   blue
> 4     100 lot4 trt1       1   blue
> 5     100 lot5 trt1       1   blue
> 6     101 lot6 trt1       1   blue
> 7     101 lot7 trt1       1   blue
> 8      99 lot8 trt1       1   blue
> 9     100 lot9 trt1       1   blue
> 10     94 lot1 trt2      16    red
> 11    105 lot2 trt2      16    red
> 12     87 lot3 trt2      16    red
> 13    119 lot4 trt2      16    red
> 14     96 lot5 trt2      16    red
> 15    113 lot6 trt2      16    red
> 16    106 lot7 trt2      16    red
> 17     71 lot8 trt2      16    red
> 18     95 lot9 trt2      16    red
>
>
> library(lattice)
> dotplot(result ~ lot, tmp1, cex=1.1,  ylab = "values", xlab="lot",
> jitter.y = F, aspect=1.0,
>         pch=tmp1$trtsymb, col=tmp1$trtcol, scales=list(rot=30),
> main="", key = list(text = list(labels = c("trt1", "trt2"),cex=c(0.9,0.9)),
> points = list(pch =c(1,12), col =c("blue", "red")),
> space = "right"))
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sat Sep 12 07:12:36 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 12 Sep 2015 15:12:36 +1000
Subject: [R] question about categorical variables in R
In-Reply-To: <CAMqbV1AEcFrbWbzCNs1ijmcqdpJJEd+jjK6VXa7h-tfGtBrMzw@mail.gmail.com>
References: <CAMqbV1AEcFrbWbzCNs1ijmcqdpJJEd+jjK6VXa7h-tfGtBrMzw@mail.gmail.com>
Message-ID: <CA+8X3fUCAwvcJ9AvPeH5H2HngaHrYb=qiEZC_4_4z2bRR4G+gw@mail.gmail.com>

Hi Lida,
Given that this is such a common question and the R FAQ doesn't really
answer it, perhaps a brief explanation will help. In R the factor class is
a sort of combination of the literal representation of the data and a
sequence of numbers beginning at 1 that are alphabetically ordered by
default. For example, suppose you read in what you think are a set of
numbers like this:

x<-read.table(text="1 2 3
+ 4 5 6
+ 7 . 9")
x
 V1 V2 V3
1  1  2  3
2  4  5  6
3  7  .  9

Now look at the classes of the columns:

sapply(x,class)
       V1        V2        V3
"integer"  "factor" "integer"

Somehow that second column has become a factor. This is because "." cannot
be represented as a number and I didn't tell R that it should be regarded
as a missing value (na.strings="."). R has taken the literal values in that
column

levels(x$V2)
[1] "." "2" "5"

and attached numbers to those values their alphabetic order.

as.numeric(x$V2)
[1] 2 3 1

You can get the original numbers back like this:

as.numeric(as.character(x$V2))
[1]  2  5 NA
Warning message:
NAs introduced by coercion

and R helpfully tells you that it couldn't coerce "." to a number.

In your example, the factor is created for you

mf<-factor(c("male","female"))
> mf
[1] male   female
Levels: female male

but as you can see, the default order of the factor may not be what you
think

as.numeric(mf)
[1] 2 1

For a more complete account of factors, see "An Introduction to R" section
4 "Ordered and unordered factors".

Jim

On Sat, Sep 12, 2015 at 12:45 AM, Lida Zeighami <lid.zigh at gmail.com> wrote:

> Hi dear experts,
> I have a general question in R, about the categorical variable such as
> Gender(Male or Female)
> If I have this column in my data and wanted to do regression model or feed
> the data to  seqmeta packages (singlesnp, skat meta) , would you please let
> me know should I code them first ( male=0 and female=1) or R programming do
> it for me?
> Because when I didn't code them, R still can do the analysis without any
> error but I'm not sure it's correct or not?
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sat Sep 12 07:23:33 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 12 Sep 2015 15:23:33 +1000
Subject: [R] removing outlier
In-Reply-To: <1441980906861-4712137.post@n4.nabble.com>
References: <1441980906861-4712137.post@n4.nabble.com>
Message-ID: <CA+8X3fUb90tmCQYG=8i+zTvj1RST7wQ6BkAUEqUG=prd+7B2cw@mail.gmail.com>

Hi Juli,
What you can do is to make your outlier remover into a function like this:

remove_outlier_by_sd<-function(x,nsd=3) {
 meanx<-mean(x,na.rm=TRUE)
 sdx<-sd(x,na.rm=TRUE)
 return(x[abs(x-xmean) < nsd*sdx])
}

Then apply the function to your data frame ("table")

newDA<-sapply(DA,remove_outlier_by_sd)

newDA will be a list, as it is likely that its elements will be of
different lengths. You may be told that you really shouldn't remove
outliers and learn to love them, but I will leave that to others.

Jim


On Sat, Sep 12, 2015 at 12:15 AM, Juli <Julianeleuschner at web.de> wrote:

> Hey,
>
> i want to remove outliers so I tried do do this:
>
> # 1 define mean and sd
> sd.AT_ZU_SPAET <- sd(AT_ZU_SPAET)
> mitt.AT_ZU_SPAET <- mean(AT_ZU_SPAET)
> #
> sd.Anzahl_BAF <- sd(Anzahl_BAF)
> mitt.Anzahl_BAF <- mean(Anzahl_BAF)
> #
> sd.?nderungsintervall <- sd(?nderungsintervall)
> mitt.?nderungsintervall <- mean(?nderungsintervall)
> #
> # 2 identify outliers
> DA[ abs(AT_ZU_SPAET - mitt.AT_ZU_SPAET) > ( 3 * sd.AT_ZU_SPAET)  , ]
> DA[ abs(Anzahl_BAF - mitt.Anzahl_BAF) > ( 3 * sd.Anzahl_BAF)  , ]
> DA[ abs(?nderungsintervall - mitt.?nderungsintervall) > ( 3 *
> sd.?nderungsintervall)  , ]
> #
> # 3 remove outliers
> AT_ZU_SPAET.clean <- DA[ (abs(AT_ZU_SPAET - mitt.AT_ZU_SPAET) <
> (3*sd.AT_ZU_SPAET)), ]
> Anzahl_BAF.clean <- DA[ (abs(Anzahl_BAF - mitt.Anzahl_BAF) <
> (3*sd.Anzahl_BAF)), ]
> ?nderungsintervall.clean <- DA[ (abs(?nderungsintervall -
> mitt.?nderungsintervall) <
> (3*sd.?nderungsintervall)), ]
>
> My problem ist, that I am only able to remove the outliers of one column of
> my table, but I want to remove the outliers of every column of the table.
>
> Could anybody help me?
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/removing-outlier-tp4712137.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ubuntu.diego at gmail.com  Sat Sep 12 07:58:20 2015
From: ubuntu.diego at gmail.com (Diego Ubuntu)
Date: Sat, 12 Sep 2015 01:58:20 -0400
Subject: [R] Wavelets Forecast/Prediction
Message-ID: <CAH03yy=S1KYz0_s7D_dHZkOHSX0vet-OK3HNd2aQBrUeWwLW9A@mail.gmail.com>

Is there any way to forecast/predict a time series using wavelets ? Is
there any package with that functionality included?

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Sat Sep 12 08:42:27 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 11 Sep 2015 23:42:27 -0700
Subject: [R] Installation R package Rssa from directory
In-Reply-To: <1442002976547-4712160.post@n4.nabble.com>
References: <1442002976547-4712160.post@n4.nabble.com>
Message-ID: <10F6FE5F-5823-43C7-BF44-384C756F9DCA@dcn.davis.CA.us>

Looks to me like you are giving the name of a directory where a file name of a tar.gz (package source) would be required. R prefers to be given the package file rather than an extracted directory of files.

Most packages on Windows are used in their binary form ("zip" file), so you would not specify the type="source" option of you install from the zip file.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 11, 2015 1:22:56 PM PDT, tonina <yanita at meta.ua> wrote:
>Hello. I'm trying to install R package Rssa from directory
>install.packages("d:\\Rssa\\", repos=NULL, type="source")
>
>But I have an error
>
>* installing *binary* package 'Rssa' ...
>Warning: running command 'cp -R . "E:/Program
>Files/R/R-3.1.3/library/Rssa"
>|| ( tar cd - .| (cd "E:/Program Files/R/R-3.1.3/library/Rssa" && tar
>-xf -
>))' had status 127
>ERROR: installing binary package failed
>* removing 'E:/Program Files/R/R-3.1.3/library/Rssa'
>Warning in install.packages :
>  running command '"E:/PROGRA~1/R/R-31~1.3/bin/x64/R" CMD INSTALL -l 
>"E:\Program Files\R\R-3.1.3\library" "d:/Rssa/"' had status 1
>Warning in install.packages :
> installation of package ?d:/Rssa/? had non-zero exit status
>I installed this packages early from directory without problem. And
>when I
>use this package without installation
>
>library("Rssa", lib.loc="d:\\Rssa\\")
>
>package works now too.
>
>I have been looking for similar errors, but I still can not understand
>this
>error and don't know what to do. Thank you for help.
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/Installation-R-package-Rssa-from-directory-tp4712160.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ragia11 at hotmail.com  Sat Sep 12 11:20:53 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Sat, 12 Sep 2015 12:20:53 +0300
Subject: [R] kendall tau distance
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C0914@mb02.ads.tamu.edu>
References: <DUB125-W94DB648B732F8E71C200C2B3510@phx.gbl>,
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C0914@mb02.ads.tamu.edu>
Message-ID: <DUB125-W727649217D6675E1D7958B35F0@phx.gbl>

many thanks for replying.
?I have the vectors
?running the code

ConDisPairs( data.frame(c(1,2,3,4,5),c(3,4,1,2,5)) )?

$pi.c
? ? ?[,1] [,2]
[1,] ? 12 ? ?0
[2,] ? ?8 ? ?1
[3,] ? ?7 ? ?3
[4,] ? ?5 ? ?6
[5,] ? ?0 ? 10

$pi.d
? ? ?[,1] [,2]
[1,] ? ?0 ? 14
[2,] ? ?3 ? 12
[3,] ? ?7 ? ?9
[4,] ? ?8 ? ?5
[5,] ? 10 ? ?0

$C
[1] 69

$D
[1] 109


I could not find the result related to the result at wiki page in any way..looking for 4 ? dissimilar pairs?

many thanks

----------------------------------------
> From: dcarlson at tamu.edu
> To: ragia11 at hotmail.com; r-help at r-project.org
> Subject: RE: [R] kendall tau distance
> Date: Fri, 11 Sep 2015 15:37:19 +0000
>
> The Wikipedia article gives a simple formula based on the number of discordant pairs. You can get that from the ConDisPairs() function in package DescTools.
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ragia Ibrahim
> Sent: Thursday, September 10, 2015 12:40 PM
> To: r-help at r-project.org
> Subject: [R] kendall tau distance
>
> Dear group
> how to calculate kendall tau distance according to Kendall_tau_distance at wikipedia
>
> <a href="https&#58;&#47;&#47;en.wikipedia.org&#47;wiki&#47;Kendall_tau_distance" target="_blank" class="newlyinsertedlink">https&#58;&#47;&#47;en.wikipedia.org&#47;wiki&#47;Kendall_tau_distance</a>
>
>
> thanks in advance
> Ragia
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
 		 	   		  

From bgunter.4567 at gmail.com  Sat Sep 12 16:17:31 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 12 Sep 2015 07:17:31 -0700
Subject: [R] Wavelets Forecast/Prediction
In-Reply-To: <CAH03yy=S1KYz0_s7D_dHZkOHSX0vet-OK3HNd2aQBrUeWwLW9A@mail.gmail.com>
References: <CAH03yy=S1KYz0_s7D_dHZkOHSX0vet-OK3HNd2aQBrUeWwLW9A@mail.gmail.com>
Message-ID: <CAGxFJbSDsUbR8awLDSGpy8vC9Ky9=ruv7GN66D9rkyumQm7xPA@mail.gmail.com>

(internet) search!

e.g. on "R package wavelets"

There are several. See also the time series task view on CRAN.

-- Bert




Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Sep 11, 2015 at 10:58 PM, Diego Ubuntu <ubuntu.diego at gmail.com> wrote:
> Is there any way to forecast/predict a time series using wavelets ? Is
> there any package with that functionality included?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Sat Sep 12 16:55:47 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Sun, 13 Sep 2015 00:55:47 +1000
Subject: [R] Adding a second Y axis on a dotplot
In-Reply-To: <CAHLnndYU6zacv6DG3fBW-nmcrDzRJnDcJ+kmYQZTG2p0PxPaCA@mail.gmail.com>
References: <CAHLnndYU6zacv6DG3fBW-nmcrDzRJnDcJ+kmYQZTG2p0PxPaCA@mail.gmail.com>
Message-ID: <000001d0ed6b$1c715f60$55541e20$@bigpond.com>

Hi

see https://stat.ethz.ch/pipermail/r-help/2007-June/134524.html

to get you started. Its toolate or too early here

Regards

Duncan 

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of li li
Sent: Saturday, 12 September 2015 03:30
To: r-help
Subject: [R] Adding a second Y axis on a dotplot

Hi all,
  I plotted a dotplot based on the data below and code below. I would
like to add another yaxis on the right with a different col, different
tickmarks and a different label. Can anyone give some help?Thanks very
much!!
     Hanna


> tmp1
   result  lot  trt trtsymb trtcol
1      98 lot1 trt1       1   blue
2      99 lot2 trt1       1   blue
3      98 lot3 trt1       1   blue
4     100 lot4 trt1       1   blue
5     100 lot5 trt1       1   blue
6     101 lot6 trt1       1   blue
7     101 lot7 trt1       1   blue
8      99 lot8 trt1       1   blue
9     100 lot9 trt1       1   blue
10     94 lot1 trt2      16    red
11    105 lot2 trt2      16    red
12     87 lot3 trt2      16    red
13    119 lot4 trt2      16    red
14     96 lot5 trt2      16    red
15    113 lot6 trt2      16    red
16    106 lot7 trt2      16    red
17     71 lot8 trt2      16    red
18     95 lot9 trt2      16    red


library(lattice)
dotplot(result ~ lot, tmp1, cex=1.1,  ylab = "values", xlab="lot",
jitter.y = F, aspect=1.0,
        pch=tmp1$trtsymb, col=tmp1$trtcol, scales=list(rot=30),
main="", key = list(text = list(labels = c("trt1", "trt2"),cex=c(0.9,0.9)),
points = list(pch =c(1,12), col =c("blue", "red")),
space = "right"))

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Julianeleuschner at web.de  Sat Sep 12 11:32:39 2015
From: Julianeleuschner at web.de (Juli)
Date: Sat, 12 Sep 2015 02:32:39 -0700 (PDT)
Subject: [R] removing outlier
In-Reply-To: <CA+8X3fUb90tmCQYG=8i+zTvj1RST7wQ6BkAUEqUG=prd+7B2cw@mail.gmail.com>
References: <1441980906861-4712137.post@n4.nabble.com>
	<CA+8X3fUb90tmCQYG=8i+zTvj1RST7wQ6BkAUEqUG=prd+7B2cw@mail.gmail.com>
Message-ID: <1442050359278-4712170.post@n4.nabble.com>

Hi Jim, 

thank you for your help. :)

My point is, that there are outlier and I don?t really know how to deal with
that. 

I need the dataframe for a regression and read often that only a few outlier
can change your results very much. In addition, regression diacnostics
didn?t indcate me the best results.
Yes, and I know its not the core of statistics to work in a way you get
results you would like to have ;).

So what is your suggestion?

And if I remove the outliers, my problem ist, that as you said, they differ
in length. I need the data frame for a regression, so can I remove the whole
column or is there a call to exclude the data?

JULI



--
View this message in context: http://r.789695.n4.nabble.com/removing-outlier-tp4712137p4712170.html
Sent from the R help mailing list archive at Nabble.com.


From ghada.f.mm at gmail.com  Sat Sep 12 16:39:33 2015
From: ghada.f.mm at gmail.com (Ghada Almousa)
Date: Sat, 12 Sep 2015 17:39:33 +0300
Subject: [R] help
Message-ID: <CADG8gkspY1mb7Yu6FeX3XnOOZ9kNcvRf54h-gMZmLZ2UskvHTg@mail.gmail.com>

hi

In cluster analysis K-means , hirarical cluster ,DBSCAN and EM how do we
calculate purity

	[[alternative HTML version deleted]]


From yanita at meta.ua  Sat Sep 12 16:46:19 2015
From: yanita at meta.ua (tonina)
Date: Sat, 12 Sep 2015 07:46:19 -0700 (PDT)
Subject: [R] Installation R package Rssa from directory
In-Reply-To: <10F6FE5F-5823-43C7-BF44-384C756F9DCA@dcn.davis.CA.us>
References: <1442002976547-4712160.post@n4.nabble.com>
	<10F6FE5F-5823-43C7-BF44-384C756F9DCA@dcn.davis.CA.us>
Message-ID: <1442069179662-4712174.post@n4.nabble.com>

Many thanks, Jeff!
I install package with
install.packages('d:\\Rssa\\Rssa_0.13.zip', repos=NULL)



--
View this message in context: http://r.789695.n4.nabble.com/Installation-R-package-Rssa-from-directory-tp4712160p4712174.html
Sent from the R help mailing list archive at Nabble.com.


From dwinsemius at comcast.net  Sat Sep 12 18:52:46 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 12 Sep 2015 09:52:46 -0700
Subject: [R] removing outlier
In-Reply-To: <1442050359278-4712170.post@n4.nabble.com>
References: <1441980906861-4712137.post@n4.nabble.com>
	<CA+8X3fUb90tmCQYG=8i+zTvj1RST7wQ6BkAUEqUG=prd+7B2cw@mail.gmail.com>
	<1442050359278-4712170.post@n4.nabble.com>
Message-ID: <BA02E3C3-BD49-424D-9D26-810CB8BF8E6E@comcast.net>


On Sep 12, 2015, at 2:32 AM, Juli wrote:

> Hi Jim, 
> 
> thank you for your help. :)
> 
> My point is, that there are outlier and I don?t really know how to deal with
> that. 
> 
> I need the dataframe for a regression and read often that only a few outlier
> can change your results very much. In addition, regression diacnostics
> didn?t indcate me the best results.
> Yes, and I know its not the core of statistics to work in a way you get
> results you would like to have ;).
> 
> So what is your suggestion?
> 
> And if I remove the outliers, my problem ist, that as you said, they differ
> in length. I need the data frame for a regression, so can I remove the whole
> column or is there a call to exclude the data?

Most regression methods have a 'subset' parameter which would allow you to distort the data to your desired specification. But why not think about examining a different statistical model or using robust methods? That way you can keep all your data. (Sounds like you don't really have a lot.)

-- 
David.
> 
> JULI
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/removing-outlier-tp4712137p4712170.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bbolker at gmail.com  Sun Sep 13 00:46:04 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 12 Sep 2015 22:46:04 +0000
Subject: [R] maximum-likelihood-estimation with mle()
References: <55F2C6FC.7080903@gmail.com>
	<72CE6C02-E1AE-4FE8-891B-97C220970726@gmail.com>
Message-ID: <loom.20150913T004225-14@post.gmane.org>

peter dalgaard <pdalgd <at> gmail.com> writes:

> 
> You are being over-optimistic with your starting values, and/or 
> with constrains on the parameter space. 
> Your fit is diverging in sigma for some reason known
>  only to nonlinear-optimizer gurus...
> 
> For me, it works either to put in an explicit
>  constraint or to reparametrize with log(sigma).

  [snip]

  Another few strategies:

set.seed(101)
x <- 1:10
y <- 3*x - 1 + rnorm(length(x), mean=0, sd=0.5)

library("stats4")
nLL <- function(a, b, sigma) {
  -sum(dnorm(y, mean=a*x+b, sd=sigma, log=TRUE))
}

fit <- mle(nLL, start=list(a=0, b=0, sigma=1), nobs=length(y))


   Nelder-Mead is generally more robust than BFGS, although slower:


fit2 <- mle(nLL, start=list(a=0, b=0, sigma=1),
     method="Nelder-Mead",nobs=length(y))

   bbmle can be used to simplify things slightly (this repeats
Peter Dalgaard's transformation of sigma):

library("bbmle")
fit3 <- mle2(y~dnorm(mean=mu,sd=exp(logsigma)),
   parameters=list(mu~x),
   data=data.frame(x,y),
   start=list(mu=0, logsigma=0))

  You can also profile out the sigma:

## dnorm with sd profiled out
dnorm2 <- function(x,mean,log=FALSE) {
  ssq <- sum((x-mean)^2)
  dnorm(x,mean,sd=sqrt(ssq/length(x)),log=log)
}
fit4 <- mle2(y~dnorm2(mean=mu),
   parameters=list(mu~x),
   data=data.frame(x,y),
   start=list(mu=0))


From dulcalma at bigpond.com  Sun Sep 13 02:26:41 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Sun, 13 Sep 2015 10:26:41 +1000
Subject: [R] Adding a second Y axis on a dotplot
In-Reply-To: <000001d0ed6b$1c715f60$55541e20$@bigpond.com>
References: <CAHLnndYU6zacv6DG3fBW-nmcrDzRJnDcJ+kmYQZTG2p0PxPaCA@mail.gmail.com>
	<000001d0ed6b$1c715f60$55541e20$@bigpond.com>
Message-ID: <000001d0edba$ddad81a0$990884e0$@bigpond.com>


I forgot to put a line about latticeExtra's doubleYScale
library(latticeExtra)
? doubleYScale
Duncan


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Duncan
Mackay
Sent: Sunday, 13 September 2015 00:56
To: R
Subject: Re: [R] Adding a second Y axis on a dotplot

Hi

see https://stat.ethz.ch/pipermail/r-help/2007-June/134524.html

to get you started. Its toolate or too early here

Regards

Duncan 

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of li li
Sent: Saturday, 12 September 2015 03:30
To: r-help
Subject: [R] Adding a second Y axis on a dotplot

Hi all,
  I plotted a dotplot based on the data below and code below. I would
like to add another yaxis on the right with a different col, different
tickmarks and a different label. Can anyone give some help?Thanks very
much!!
     Hanna


> tmp1
   result  lot  trt trtsymb trtcol
1      98 lot1 trt1       1   blue
2      99 lot2 trt1       1   blue
3      98 lot3 trt1       1   blue
4     100 lot4 trt1       1   blue
5     100 lot5 trt1       1   blue
6     101 lot6 trt1       1   blue
7     101 lot7 trt1       1   blue
8      99 lot8 trt1       1   blue
9     100 lot9 trt1       1   blue
10     94 lot1 trt2      16    red
11    105 lot2 trt2      16    red
12     87 lot3 trt2      16    red
13    119 lot4 trt2      16    red
14     96 lot5 trt2      16    red
15    113 lot6 trt2      16    red
16    106 lot7 trt2      16    red
17     71 lot8 trt2      16    red
18     95 lot9 trt2      16    red


library(lattice)
dotplot(result ~ lot, tmp1, cex=1.1,  ylab = "values", xlab="lot",
jitter.y = F, aspect=1.0,
        pch=tmp1$trtsymb, col=tmp1$trtcol, scales=list(rot=30),
main="", key = list(text = list(labels = c("trt1", "trt2"),cex=c(0.9,0.9)),
points = list(pch =c(1,12), col =c("blue", "red")),
space = "right"))

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From suharto_anggono at yahoo.com  Sat Sep 12 18:17:01 2015
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Sat, 12 Sep 2015 09:17:01 -0700
Subject: [R] For logical i and length(i) > length(x),
	x[i] <- value makes length(x) == length(i)
Message-ID: <1442074621.8236.YahooMailBasic@web125102.mail.ne1.yahoo.com>

I'll make my points clearer.

- The case of logical index vector longer than the original vector is not usual case for me. As I said, I encountered it in the code of function 'rank' in R.

- Before, I have read "S-PLUS help" (version 3.4, it seems), http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/helpfiles/Subscript.html. The following is the relevant part. There are missing pieces there that I fill by guessing.

Vector subscripts are generated with when i and x are both vectors. (....) The result of the expression is to extract or replace elements of x corresponding to a vector of positive indices computed according to the value of i.

.... If i is logical the indices are produced by starting at 1 and selecting the numbers for which the corresponding element is T. If is shorter than it is extended by cyclic repetition. It can be longer than as well, with no change in the computation of indices. ....

....

For replacements, x[i] <- value the rule is that the length of will be set to the largest value in the indices, if that is bigger than the current length of x. ....


>From it, I infer that, if i is logical and length(i) >= length(x), x[i] <- value has the same effect to x[which(i)] <- value, where which(i) takes indices where i is T.

- In R, if i is logical and length(i) > length(x), length(x) after x[i] <- value may be different from after x[which(i)] <- value.

- So, I wonder if R inherits the behavior from S or not.

- The behavior is not clearly documented in R. I just find "R Language Definition", "3.4.1 Indexing by vectors", that can be interpreted to imply the behavior.

- However, for a particular case, function 'rank' in R relies on the behavior.

However, it seems that relying on the behavior is not on purpose. Previously, at least until R 3.1.3, the code of function 'rank' has the following before yy <- NA .
        yy <- integer(length(x))
        storage.mode(yy) <- storage.mode(y)

It seems that yy[] <- NA is what is intended.

- However, for me, the behavior is plausible. The assumption is that indices from 1 to length(i) exist.

--------------------------------------

I think this behavior is consistent with typical indexing behaviour in R... I would ask you what result you thought you should get? I, for one, can think of all sorts of uses for numeric indexes that have different lengths than the vector, but am stumped to think of any use for what you are proposing. 
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<[hidden email]>        Basics: ##.#.       ##.#.  Live Go... 
                                      Live:   OO#.. Dead: OO#..  Playing 
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with 
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
---------------------------------------------------------------------------
Sent from my phone. Please excuse my brevity. 

On September 5, 2015 10:02:05 AM PDT, Suharto Anggono Suharto Anggono via R-help <[hidden email]> wrote: 

>I came across this behavior when I followed the code of function 'rank' 
>in R. 
> 
>It seems that subassignment of a vector by a logical index vector that 
>is longer than the original vector always results in expanding the 
>original vector to the length of the index vector. 
> 
>The resulting length may be different from the result of subassignment 
>by the equivalent numeric vector. For subassignment of a vector by a 
>numeric index vector, the original vector is expanded to the maximum 
>index, if it is larger than the length of the original vector. 
> 
>This is an example. 
> 
>> x <- NA 
>> x[c(FALSE,TRUE,FALSE)] <- 1 
>> x 
>[1] NA  1 NA 
> 
>Compare to this. 
> 
>> x <- NA 
>> x[which(c(FALSE,TRUE,FALSE))] <- 1 
>> x 
>[1] NA  1 
> 
>Does S exhibit the same behavior? 
> 
>Currently, if there is NA and na.last = "keep", function 'rank' in R 
>relies on this behavior to give correct result length. 
> 
>In "R Language Definition", "3.4.1 Indexing by vectors" says: "Logical. 
>The indexing i should generally have the same length as x. .... If it 
>is longer, then x is conceptually extended with NAs. ...." The 
>statement can be taught to support the observed behavior. 
> 
>> sessionInfo() 
>R version 3.2.2 (2015-08-14) 
>Platform: i386-w64-mingw32/i386 (32-bit) 
>Running under: Windows XP (build 2600) Service Pack 2 
> 
>locale: 
>[1] LC_COLLATE=English_United States.1252 
>[2] LC_CTYPE=English_United States.1252 
>[3] LC_MONETARY=English_United States.1252 
>[4] LC_NUMERIC=C 
>[5] LC_TIME=English_United States.1252 
> 
>attached base packages: 
>[1] stats     graphics  grDevices utils     datasets  methods   base 
> 
>______________________________________________ 
>[hidden email] mailing list -- To UNSUBSCRIBE and more, see 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code. 

______________________________________________ 
[hidden email] mailing list -- To UNSUBSCRIBE and more, see 
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code. 


From yaohui-zeng at uiowa.edu  Sat Sep 12 21:27:02 2015
From: yaohui-zeng at uiowa.edu (yazeng)
Date: Sat, 12 Sep 2015 12:27:02 -0700 (PDT)
Subject: [R] R CRAN check works great on local Mac OS,
 but failed on Windows (win-builder web). Why?
Message-ID: <1442086022581-4712179.post@n4.nabble.com>

I have my R package checked on local Mac OS via R CMD check --as-cran.
Everything works great: no errors, no warnings, just 1 note about first
submission, all test files passed! However, when I submit my .tar.gz package
file to the http://win-builder.r-project.org to check on Windows. Some
errors occurred when running the examples.

The main issue is related to the object created by Matrix. All errors
indicated 

*invalid class "dgCMatrix" object: Dimnames[1] is not a character vector.
*

Detailed check log is attached below. I checked the examples on local
machine line-by-line, but cannot identify the problem.

Is the error caused by that maybe the Matrix package was written differently
for Mac OS and Windows platforms?

/* checking examples ...
** running examples for arch 'i386' ... ERROR
Running examples in 'grpregOverlap-Ex.R' failed
The error most likely occurred in:

> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: cv.grpregOverlap
> ### Title: Cross-validation for choosing regularization parameter lambda
> ### Aliases: cv.grpregOverlap
> 
> ### ** Examples
> 
> ## linear regression, a simulation demo.
> set.seed(123)
> group <- list(gr1 = c(1, 2, 3),
+               gr2 = c(1, 4),
+               gr3 = c(2, 4, 5),
+               gr4 = c(3, 5),
+               gr5 = c(6))
> beta.latent.T <- c(5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0) # true latent
> coefficients.
> # beta.T <- c(5, 5, 10, 0, 5, 0), true variables: 1, 2, 3, 5; true groups:
> 1, 4.
> X <- matrix(rnorm(n = 6*100), ncol = 6)
> X.latent <- expandX(X, group)
Error in validObject(.Object) : 
  invalid class "dgCMatrix" object: Dimnames[1] is not a character vector
Calls: expandX ... initialize -> callNextMethod -> .nextMethod ->
validObject
Execution halted/


*I also attached source code for the two related functions.*


/expandX <- function(X, group) {
  incidence.mat <- incidenceMatrix(X, group) # group membership incidence
matrix
  over.mat <- Matrix(incidence.mat %*% t(incidence.mat), sparse = TRUE, 
                     dimnames = dimnames(incidence.mat)) # overlap matrix
  grp.vec <- rep(1:nrow(over.mat), times = diag(over.mat)) # group index
vector

  # expand X to X.latent
  X.latent <- NULL
  names <- NULL

  ## the following code will automatically remove variables not included in
'group'
  for(i in 1:nrow(incidence.mat)) {
    idx <- incidence.mat[i,]==1
    X.latent <- cbind(X.latent, X[, idx, drop=FALSE])
    names <- c(names, colnames(incidence.mat)[idx])
  }
  colnames(X.latent) <- paste('grp', grp.vec, '_', names, sep = "")
  X.latent
}

incidenceMatrix <- function(X, group) {
  n <- nrow(X)
  p <- ncol(X)
  if (class(group) != 'list') {
    stop("Argument 'group' must be a list of integer indices or character
names of variables!")
  }
  J <- length(group)
  grp.mat <- Matrix(0, nrow = J, ncol = p, dimnames=list(rep(NA, J),
                                                         rep(NA, p)))    
  if(is.null(colnames(X))) {
    colnames(X) <- paste("V", 1:ncol(X), sep="")    
  }
  if (is.null(names(group))) {
    names(group) <- paste("grp", 1:J, sep="")
  }

  if (class(group[[1]]) == 'numeric') {
    for (i in 1:J) {
      ind <- group[[i]]
      grp.mat[i, ind] <- 1
      colnames(grp.mat)[ind] <- colnames(X)[ind]
    }
  } else { ## character, names of variables
    for (i in 1:J) {
      grp.i <- as.character(group[[i]])
      ind <- colnames(X) %in% grp.i
      grp.mat[i, ] <- 1*ind
      colnames(grp.mat)[ind] <- colnames(X)[ind]
    }
  }
  rownames(grp.mat) <- as.character(names(group))
  # check grp.mat
  if (all(grp.mat == 0)) {
    stop("The names of variables in X don't match with names in group!")
  }

  grp.mat
}
/



--
View this message in context: http://r.789695.n4.nabble.com/R-CRAN-check-works-great-on-local-Mac-OS-but-failed-on-Windows-win-builder-web-Why-tp4712179.html
Sent from the R help mailing list archive at Nabble.com.


From wdunlap at tibco.com  Sun Sep 13 04:22:44 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Sat, 12 Sep 2015 19:22:44 -0700
Subject: [R] For logical i and length(i) > length(x),
 x[i] <- value makes length(x) == length(i)
In-Reply-To: <1442074621.8236.YahooMailBasic@web125102.mail.ne1.yahoo.com>
References: <1442074621.8236.YahooMailBasic@web125102.mail.ne1.yahoo.com>
Message-ID: <CAF8bMcZvCLtsqye44wduFxfr=m+CZbfLh4frS3NOzuj0yykuFw@mail.gmail.com>

Splus 8.2.0 (c. 2010) and Splus6.0 (c. 2001) act like R in this respect.
  x <- c(10.0, 11.0, 12.0)
  i <- c(FALSE,TRUE,FALSE,FALSE,TRUE)
  x[i]
  #[1] 11 NA
  x[i] <- 1:2
  x
  #[1] 10  1 12 NA  2
I no longer have access to a running version of Splus 3.4, but it does not
look like a feature we would have changed.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Sat, Sep 12, 2015 at 9:17 AM, Suharto Anggono Suharto Anggono via
R-help <r-help at r-project.org> wrote:
> I'll make my points clearer.
>
> - The case of logical index vector longer than the original vector is not usual case for me. As I said, I encountered it in the code of function 'rank' in R.
>
> - Before, I have read "S-PLUS help" (version 3.4, it seems), http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/helpfiles/Subscript.html. The following is the relevant part. There are missing pieces there that I fill by guessing.
>
> Vector subscripts are generated with when i and x are both vectors. (....) The result of the expression is to extract or replace elements of x corresponding to a vector of positive indices computed according to the value of i.
>
> .... If i is logical the indices are produced by starting at 1 and selecting the numbers for which the corresponding element is T. If is shorter than it is extended by cyclic repetition. It can be longer than as well, with no change in the computation of indices. ....
>
> ....
>
> For replacements, x[i] <- value the rule is that the length of will be set to the largest value in the indices, if that is bigger than the current length of x. ....
>
>
> >From it, I infer that, if i is logical and length(i) >= length(x), x[i] <- value has the same effect to x[which(i)] <- value, where which(i) takes indices where i is T.
>
> - In R, if i is logical and length(i) > length(x), length(x) after x[i] <- value may be different from after x[which(i)] <- value.
>
> - So, I wonder if R inherits the behavior from S or not.
>
> - The behavior is not clearly documented in R. I just find "R Language Definition", "3.4.1 Indexing by vectors", that can be interpreted to imply the behavior.
>
> - However, for a particular case, function 'rank' in R relies on the behavior.
>
> However, it seems that relying on the behavior is not on purpose. Previously, at least until R 3.1.3, the code of function 'rank' has the following before yy <- NA .
>         yy <- integer(length(x))
>         storage.mode(yy) <- storage.mode(y)
>
> It seems that yy[] <- NA is what is intended.
>
> - However, for me, the behavior is plausible. The assumption is that indices from 1 to length(i) exist.
>
> --------------------------------------
>
> I think this behavior is consistent with typical indexing behaviour in R... I would ask you what result you thought you should get? I, for one, can think of all sorts of uses for numeric indexes that have different lengths than the vector, but am stumped to think of any use for what you are proposing.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<[hidden email]>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 5, 2015 10:02:05 AM PDT, Suharto Anggono Suharto Anggono via R-help <[hidden email]> wrote:
>
>>I came across this behavior when I followed the code of function 'rank'
>>in R.
>>
>>It seems that subassignment of a vector by a logical index vector that
>>is longer than the original vector always results in expanding the
>>original vector to the length of the index vector.
>>
>>The resulting length may be different from the result of subassignment
>>by the equivalent numeric vector. For subassignment of a vector by a
>>numeric index vector, the original vector is expanded to the maximum
>>index, if it is larger than the length of the original vector.
>>
>>This is an example.
>>
>>> x <- NA
>>> x[c(FALSE,TRUE,FALSE)] <- 1
>>> x
>>[1] NA  1 NA
>>
>>Compare to this.
>>
>>> x <- NA
>>> x[which(c(FALSE,TRUE,FALSE))] <- 1
>>> x
>>[1] NA  1
>>
>>Does S exhibit the same behavior?
>>
>>Currently, if there is NA and na.last = "keep", function 'rank' in R
>>relies on this behavior to give correct result length.
>>
>>In "R Language Definition", "3.4.1 Indexing by vectors" says: "Logical.
>>The indexing i should generally have the same length as x. .... If it
>>is longer, then x is conceptually extended with NAs. ...." The
>>statement can be taught to support the observed behavior.
>>
>>> sessionInfo()
>>R version 3.2.2 (2015-08-14)
>>Platform: i386-w64-mingw32/i386 (32-bit)
>>Running under: Windows XP (build 2600) Service Pack 2
>>
>>locale:
>>[1] LC_COLLATE=English_United States.1252
>>[2] LC_CTYPE=English_United States.1252
>>[3] LC_MONETARY=English_United States.1252
>>[4] LC_NUMERIC=C
>>[5] LC_TIME=English_United States.1252
>>
>>attached base packages:
>>[1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>______________________________________________
>>[hidden email] mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> [hidden email] mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j.para.fernandez at hotmail.com  Sun Sep 13 09:04:01 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Sun, 13 Sep 2015 00:04:01 -0700 (PDT)
Subject: [R] Model after random forest
Message-ID: <1442127841457-4712189.post@n4.nabble.com>

Hi there, 

I?m using random Forest package to create a random Forest:

model<-randomForest(A~.,data=mydata)

, and I use the varImpPlot(model) to see which are the most important
variables, so I obtain that C, D and F are the most important ones, but...

How can I see the model, in which levels I must set up this variables C, D
and F?

Thanks  for all



-----

Guided Tours Basque Country 

Guided tours in the three capitals of the Basque Country: Bilbao, Vitoria-Gasteiz and San Sebastian, as well as in their provinces. Available languages.

Travel planners for groups and design of tourist routes across the Basque Country.
--
View this message in context: http://r.789695.n4.nabble.com/Model-after-random-forest-tp4712189.html
Sent from the R help mailing list archive at Nabble.com.


From bgunter.4567 at gmail.com  Sun Sep 13 16:33:53 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 13 Sep 2015 07:33:53 -0700
Subject: [R] removing outlier
In-Reply-To: <BA02E3C3-BD49-424D-9D26-810CB8BF8E6E@comcast.net>
References: <1441980906861-4712137.post@n4.nabble.com>
	<CA+8X3fUb90tmCQYG=8i+zTvj1RST7wQ6BkAUEqUG=prd+7B2cw@mail.gmail.com>
	<1442050359278-4712170.post@n4.nabble.com>
	<BA02E3C3-BD49-424D-9D26-810CB8BF8E6E@comcast.net>
Message-ID: <CAGxFJbSAXmEweyW4ESe+YGCMdnYBrNu2g7KPq+3BoQyiCbeJvw@mail.gmail.com>

... and this, of course, is a nice example of how statistics
contributes to the "irreproducibility crisis" now roiling Science.

Cheers,
Bert

(Quote from a long ago engineering colleague: "Whenever I see an
outlier, I never know whether to throw it away or patent it.")


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Sep 12, 2015 at 9:52 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>
> On Sep 12, 2015, at 2:32 AM, Juli wrote:
>
>> Hi Jim,
>>
>> thank you for your help. :)
>>
>> My point is, that there are outlier and I don?t really know how to deal with
>> that.
>>
>> I need the dataframe for a regression and read often that only a few outlier
>> can change your results very much. In addition, regression diacnostics
>> didn?t indcate me the best results.
>> Yes, and I know its not the core of statistics to work in a way you get
>> results you would like to have ;).
>>
>> So what is your suggestion?
>>
>> And if I remove the outliers, my problem ist, that as you said, they differ
>> in length. I need the data frame for a regression, so can I remove the whole
>> column or is there a call to exclude the data?
>
> Most regression methods have a 'subset' parameter which would allow you to distort the data to your desired specification. But why not think about examining a different statistical model or using robust methods? That way you can keep all your data. (Sounds like you don't really have a lot.)
>
> --
> David.
>>
>> JULI
>>
>>
>>
>> --
>> View this message in context: http://r.789695.n4.nabble.com/removing-outlier-tp4712137p4712170.html
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From zkarimi1985 at yahoo.com  Sun Sep 13 07:12:07 2015
From: zkarimi1985 at yahoo.com (Zahra Karimi)
Date: Sun, 13 Sep 2015 05:12:07 +0000 (UTC)
Subject: [R] a question about data manipulation in R
Message-ID: <530131797.2037273.1442121127042.JavaMail.yahoo@mail.yahoo.com>

Hello,
I have a data like this:1 2331 3331 4551 3452 5432 4332 3442 4003 4443 1113 0003 432I want to change it to this new dataset:c1 ? ?c2 ? ? c3233 ?543 ?444333 ?433 ?111455 ?344 ?000345 ?400 ?432
?How can I do this in R? Please if this not the correct place to ask ?my queston.. then could you please guid me , where should I ask my question?kind regards,Zahra

	[[alternative HTML version deleted]]


From ryanshuell at gmail.com  Sun Sep 13 18:04:19 2015
From: ryanshuell at gmail.com (Ryan Shuell)
Date: Sun, 13 Sep 2015 09:04:19 -0700 (PDT)
Subject: [R] Exporting from R to Excel or .csv
In-Reply-To: <51c06627-6eab-4791-9fee-bb36a9e17338@me.com>
References: <51c06627-6eab-4791-9fee-bb36a9e17338@me.com>
Message-ID: <13add45a-028f-44fa-ab58-4d19952fcbe0@googlegroups.com>

This is my first time posting here....


library(BradleyTerry2)
library(xlsx)

data(flatlizards)
str(flatlizards)

a <- data.frame(x=rnorm(10), y=runif(10))
b <- data.frame(w=rnorm(20), z=runif(20))

ablist <- list(a, b)

write.xlsx(mydata, "c:/Test/mydata.xlsx") 





On Tuesday, June 16, 2015 at 11:49:41 PM UTC-4, Kevin Kowitski wrote:
>
> Hello, 
>
>   Does anyone have some insight on how to; or where I can find better 
> information on how to, export multiple data.frames of different dimensions 
> to the same .csv or excel file?
>
> -Kevin
> ______________________________________________
> R-h... at r-project.org <javascript:> mailing list -- To UNSUBSCRIBE and 
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

From suharto_anggono at yahoo.com  Sun Sep 13 13:44:54 2015
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Sun, 13 Sep 2015 04:44:54 -0700
Subject: [R] For logical i and length(i) > length(x),
	x[i] <- value makes length(x) == length(i)
In-Reply-To: <CAF8bMcZvCLtsqye44wduFxfr=m+CZbfLh4frS3NOzuj0yykuFw@mail.gmail.com>
Message-ID: <1442144694.51422.YahooMailBasic@web125102.mail.ne1.yahoo.com>

Thank you for looking.

If the last element of i is TRUE, the largest value of which(i) is length(i). So, even if x[i] <- value were translated to x[which(i)] <- value, the result would have length(x) == length(i).

Difference between x[i] <- value and x[which(i)] <- value in R would occur when the last element of i is FALSE, as in my original example:
x <- NA
x[c(FALSE,TRUE,FALSE)] <- 1
--------------------------------------------
On Sun, 13/9/15, William Dunlap <wdunlap at tibco.com> wrote:

 Subject: Re: [R] For logical i and length(i) > length(x), x[i] <- value makes length(x) == length(i)

 Cc: R-help at r-project.org
 Date: Sunday, 13 September, 2015, 9:22 AM

 Splus 8.2.0 (c. 2010) and
 Splus6.0 (c. 2001) act like R in this respect.
 ? x <- c(10.0, 11.0, 12.0)
 ? i <- c(FALSE,TRUE,FALSE,FALSE,TRUE)
 ? x[i]
 ? #[1] 11 NA
 ? x[i] <- 1:2
 ? x
 ? #[1] 10? 1 12 NA? 2
 I no
 longer have access to a running version of Splus 3.4, but it
 does not
 look like a feature we would have
 changed.

 Bill Dunlap
 TIBCO Software
 wdunlap
 tibco.com


 On Sat, Sep 12, 2015 at 9:17 AM, Suharto
 Anggono Suharto Anggono via
 R-help <r-help at r-project.org>
 wrote:
 > I'll make my points
 clearer.
 >
 > - The
 case of logical index vector longer than the original vector
 is not usual case for me. As I said, I encountered it in the
 code of function 'rank' in R.
 >
 > - Before, I have read
 "S-PLUS help" (version 3.4, it seems), http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/helpfiles/Subscript.html.
 The following is the relevant part. There are missing
 pieces there that I fill by guessing.
 >
 > Vector subscripts are
 generated with when i and x are both vectors. (....) The
 result of the expression is to extract or replace elements
 of x corresponding to a vector of positive indices computed
 according to the value of i.
 >
 > .... If i is logical the indices are
 produced by starting at 1 and selecting the numbers for
 which the corresponding element is T. If is shorter than it
 is extended by cyclic repetition. It can be longer than as
 well, with no change in the computation of indices. ....
 >
 > ....
 >
 > For replacements,
 x[i] <- value the rule is that the length of will be set
 to the largest value in the indices, if that is bigger than
 the current length of x. ....
 >
 >
 > >From it, I infer
 that, if i is logical and length(i) >= length(x), x[i]
 <- value has the same effect to x[which(i)] <- value,
 where which(i) takes indices where i is T.
 >
 > - In R, if i is
 logical and length(i) > length(x), length(x) after x[i]
 <- value may be different from after x[which(i)] <-
 value.
 >
 > - So, I
 wonder if R inherits the behavior from S or not.
 >
 > - The behavior is not
 clearly documented in R. I just find "R Language
 Definition", "3.4.1 Indexing by vectors",
 that can be interpreted to imply the behavior.
 >
 > - However, for a
 particular case, function 'rank' in R relies on the
 behavior.
 >
 > However,
 it seems that relying on the behavior is not on purpose.
 Previously, at least until R 3.1.3, the code of function
 'rank' has the following before yy <- NA .
 >? ? ? ???yy <-
 integer(length(x))
 >? ? ?
 ???storage.mode(yy) <- storage.mode(y)
 >
 > It seems that yy[]
 <- NA is what is intended.
 >
 > - However, for me, the behavior is
 plausible. The assumption is that indices from 1 to
 length(i) exist.
 >
 >
 --------------------------------------
 >
 > I think this behavior
 is consistent with typical indexing behaviour in R... I
 would ask you what result you thought you should get? I, for
 one, can think of all sorts of uses for numeric indexes that
 have different lengths than the vector, but am stumped to
 think of any use for what you are proposing.
 >
 ---------------------------------------------------------------------------
 > Jeff Newmiller? ? ? ? ? ? ? ? ?
 ? ? ? The? ???.....? ?
 ???.....? Go Live...
 >
 DCN:<[hidden email]>? ? ? ? Basics: ##.#.? ?
 ???##.#.? Live Go...
 >? ?
 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
 ???Live:???OO#.. Dead: OO#..?
 Playing
 > Research Engineer
 (Solar/Batteries? ? ? ? ? ? O.O#.? ?
 ???#.O#.? with
 >
 /Software/Embedded Controllers)? ? ? ? ? ?
 ???.OO#.? ? ???.OO#.? rocks...1k
 >
 ---------------------------------------------------------------------------
 > Sent from my phone. Please excuse my
 brevity.
 >
 > On
 September 5, 2015 10:02:05 AM PDT, Suharto Anggono Suharto
 Anggono via R-help <[hidden email]> wrote:
 >
 >>I came across this
 behavior when I followed the code of function
 'rank'
 >>in R.
 >>
 >>It seems that
 subassignment of a vector by a logical index vector that
 >>is longer than the original vector
 always results in expanding the
 >>original vector to the length of the
 index vector.
 >>
 >>The resulting length may be different
 from the result of subassignment
 >>by
 the equivalent numeric vector. For subassignment of a vector
 by a
 >>numeric index vector, the
 original vector is expanded to the maximum
 >>index, if it is larger than the length
 of the original vector.
 >>
 >>This is an example.
 >>
 >>> x <-
 NA
 >>> x[c(FALSE,TRUE,FALSE)] <-
 1
 >>> x
 >>[1]
 NA? 1 NA
 >>
 >>Compare to this.
 >>
 >>> x <-
 NA
 >>>
 x[which(c(FALSE,TRUE,FALSE))] <- 1
 >>> x
 >>[1] NA?
 1
 >>
 >>Does S
 exhibit the same behavior?
 >>
 >>Currently, if there is NA and na.last =
 "keep", function 'rank' in R
 >>relies on this behavior to give correct
 result length.
 >>
 >>In "R Language Definition",
 "3.4.1 Indexing by vectors" says:
 "Logical.
 >>The indexing i should
 generally have the same length as x. .... If it
 >>is longer, then x is conceptually
 extended with NAs. ...." The
 >>statement can be taught to support the
 observed behavior.
 >>
 >>> sessionInfo()
 >>R version 3.2.2 (2015-08-14)
 >>Platform: i386-w64-mingw32/i386
 (32-bit)
 >>Running under: Windows XP
 (build 2600) Service Pack 2
 >>
 >>locale:
 >>[1]
 LC_COLLATE=English_United States.1252
 >>[2] LC_CTYPE=English_United
 States.1252
 >>[3]
 LC_MONETARY=English_United States.1252
 >>[4] LC_NUMERIC=C
 >>[5] LC_TIME=English_United
 States.1252
 >>
 >>attached base packages:
 >>[1] stats? ???graphics?
 grDevices utils? ???datasets?
 methods???base
 >>
 >>______________________________________________
 >>[hidden email] mailing list -- To
 UNSUBSCRIBE and more, see
 >>https://stat.ethz.ch/mailman/listinfo/r-help
 >>PLEASE do read the posting guide
 >>http://www.R-project.org/posting-guide.html
 >>and provide commented, minimal,
 self-contained, reproducible code.
 >
 >
 ______________________________________________
 > [hidden email] mailing list -- To
 UNSUBSCRIBE and more, see
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal,
 self-contained, reproducible code.
 >
 >
 ______________________________________________
 > R-help at r-project.org
 mailing list -- To UNSUBSCRIBE and more, see
 >
 https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal,
 self-contained, reproducible code.


From mehmetdogan_07 at windowslive.com  Sun Sep 13 14:31:15 2015
From: mehmetdogan_07 at windowslive.com (=?windows-1254?B?TWVobWV0IERv8GFu?=)
Date: Sun, 13 Sep 2015 15:31:15 +0300
Subject: [R] Please read **Urgent** Need help with R
Message-ID: <DUB116-W11C68AB4656AD52939242BFB5E0@phx.gbl>

Dear All,
I can NOT format and create an xts object of my data in order to forecast Volatility. Moreover I can NOT plot the data. basically i cant work with the data i have. I need to finish my project till tomorrow. Could you please help me with those problems?
I also attached my data. the problem could be also with the format of excel.
when i could like to plot data, the following error appears:Error in plot.window(...) : need finite 'ylim' valuesIn addition: Warning messages:1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion2: In min(x) : no non-missing arguments to min; returning Inf3: In max(x) : no non-missing arguments to max; returning -Inf
I have following codes.
I can NOT format the date after these codes ****# function assumes that the data is stored in the subdirectory called "data"SP500<-read.csv("Data_Forecast/EURUSD.csv")

SP500<-read.csv("Data_Forecast/EURUSD.csv",stringsAsFactors=F)****

setwd("D:\\NewThesis")

# install and load needed librariesinstall.packages("fBasics")install.packages("tseries")install.packages("car")install.packages("FinTS")install.packages("fGarch")install.packages("rugarch")
# rugarch# http://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf

library(xts)library(fBasics) # e.g. basicStats()library(tseries)# e.g. jarque.bera.test()library(car) # e.g. durbinWatsonTest()library(FinTS) # e.g. ArchTest()library(fGarch) # e.g. garchFit()library(rugarch) # e.g. ugarchfit()
# !!! Introduction to the rugarch package# http://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf
# lets load additional functions prepared by the lecturerssource("functions/TSA_lab06_functions.R")


#################### Example #1.         # stylized facts      ###################
#1.# To import data we will use a function import_data()# written by lecturers and stored in TSA_lab06_functions.R# (already loaded)
rm(list=ls())
# function assumes that the data is stored in the subdirectory called "data"SP500<-read.csv("Data_Forecast/EURUSD.csv")

SP500<-read.csv("Data_Forecast/EURUSD.csv",stringsAsFactors=F)


#############################################################################3SP500$Date<-as.Date(SP500$Date,format="%Y-%m-%d" , "h:m:s")
                    #format(chron(0, 0), c("d/m/yy", "h:m:s"), sep = " ", enclose = c("", ""))###################################################################################

str(SP500)# Date column is already converted to Date format
#First 6 head(SP500)
#Last 6tail(SP500)
# lets leave just date and close priceSP500<-SP500[,c(1,5)]

# and change the name of the last column into SP500names(SP500)[2]<-"SP500"
SP500.xts<-xts(SP500$SP500,SP500$Date)
#First 6 head(SP500)
#Last 6tail(SP500)
#############################################################################333attach(SP500)x<-Timey<-Closedetach(SP500)plot(x,y)##############################################################################plot(SP500)
plot(SP500$Date,SP500$SP500,type="l",     main="Daily close price of SP500")
# lets add log-returns to the dataSP500$r<-diff.xts(log(SP500$SP500))

# lets limit our data to days since the beginning of 2000SP500<-SP500[SP500$Date>=as.Date("04-09-2015"),]
# lets plot the close priceplot(SP500$Date,SP500$SP500,type="l",     main="Daily close price of SP500")
# ... and it's log-returnsplot(SP500$Date,SP500$r,type="l", col="red",     main="Log-returns of SP500")abline(h=0,col="gray",lty=2) #abline functions adds line to the plot
# lets also plot the ACF function of log-returnsacf(SP500$r,lag.max=36,na.action=na.pass,    ylim=c(-0.1,0.1), # we rescale the vertical axis    col="darkblue",lwd=7,main="ACF of log-returns of SP500")
# this indicates some autoregressive relations among returns# which can be used to build an ARIMA model

# and do the same for the quared log-returnsacf(SP500$r^2,lag.max=36,na.action=na.pass,    ylim=c(0,0.5), # we rescale the vertical axis    col="darkblue",lwd=7,main="ACF of log-returns of SP500")
# this in turn indicates some autoregressive relations among SQUARED returns# (their variance!) which can be used to build a (G)ARCH model

# 2.# Do log-returns come from normal distribution?
basicStats(SP500$r)
hist(SP500$r,prob=T,breaks=40)# lets add a normal density curve for # sample mean and variance # dnorm() generates density function for a specified value# and parameters (mean, sd)
curve(dnorm(x,     # data vector  # dnorm creates a density value for the             mean=mean(SP500$r,na.rm=T), # mean    # normal distribution              sd=sd(SP500$r,na.rm=T)),    # sd      col="darkblue", lwd=2, add=TRUE)

# 3.# Let's also examine:# *) Jarque-Bera statistic
jarque.bera.test(SP500$r)
# the function doesn't accept missing values
# lets omit the missing valuesjarque.bera.test(na.omit(SP500$r)) # na.omit functions omits the missing values
# null about normality strongly rejected
# *) Durbin-Watson statistic# the durbinWatsonTest() function requires # the linear model as an argument# lets simulate a model with just a constant term
durbinWatsonTest(lm(SP500$r~1), # here 1 means a constant                 max.lag=5) # lets check first 5 orders.
# lack of autocorrelation in returns strongly rejected # for lag 1,2 and 5
# *) ARCH effects among log-returns
ArchTest(SP500$r,lags=5)  #(first 5 lags together)
# null stongly rejected

# we can also apply a DW test for squared returnsdurbinWatsonTest(lm(SP500$r^2~1),                 max.lag=5) # lets check first 5 orders# all pvalues 0. It means in all lags wereject autocorrelation # now ALL lags significant !!!


################################################# Example #2                                    # Choosing the proper order of GARCH(p,q) model ################################################

# 1.# ARCH(1)
k.arch1<-garchFit(SP500$r~garch(1,0), # we assume that returns follow an ARCH(1) process                  cond.dist= "norm", # conditional distribution of errors                  trace=FALSE) # if we don't want to see history of iterations
# summary of results and some diagnostic testssummary(k.arch1)
# The intercept in the model above is not significatn so we can drop it.k.arch1<-garchFit(SP500$r~garch(1,0),include.mean=F,                  cond.dist= "norm", # conditional distribution of errors                  trace=FALSE) # if we don't want to see history of iterations
# summary of results and some diagnostic testssummary(k.arch1)
# Let's examine squares of standardized residuals.
# we can automatically plot several parts of results# (lets select 11: ACF of Squared Standardized Residuals, # 10: ACF of Standardized Residuals,# and 11: Conditional SD; to quit press ESC)plot(k.arch1)
# squared residuals still have significant ACF for lags: 2,3,5,7,8,9,10

# 2.# lets try if ARCH(5) is enough to catch these phenomenonk.arch5<-garchFit(SP500$r~garch(5,0),include.mean=F,                  cond.dist= "norm", # conditional distribution of errors                  trace=FALSE) # if we don't want to see history of iterations
summary(k.arch5)
# all parameters significant# Ljung-box test for R^2 shows there is no more autocorrelation # between the current and past standardized squared residuals (variance)
# lets plot the ACF of standardized residuals# and standardized squared residuals (plots 10, 11)# ESC to exit
plot(k.arch5)
# lag nr 10 in ACF for squared residuals seems to be still significant, but# lets ignore it based on previous tests (LB)
# 4.# lets compare it with GARCH(1,1)k.garch11<-garchFit(SP500$r~garch(1,1),include.mean=F,                    cond.dist= "norm", # conditional distribution of errors                    trace=FALSE) # if we don't want to see history of iterations
summary(k.garch11)
# all parameters significant# Ljung-box test for R^2 shows there is no more autocorrelation # between the current and past standardized squared residuals (variance)

# lets plot the ACF of standardized residuals# and standardized squared residuals (plots 10, 11)# ESC to exit
plot(k.garch11)
# 5.# lets check higher order GARCH(1,2)k.garch12<-garchFit(SP500$r~garch(1,2),include.mean=F,                    cond.dist= "norm", # conditional distribution of errors                    trace=F) # if we don't want to see history of iterations
summary(k.garch12)

# lets plot the ACF of standardized residuals# and standardized squared residuals (plots 10, 11)# ESC to exit
plot(k.garch12)
# all parameters significant# Ljung-box test for R^2 shows there is no more autocorrelation # between the current and past standardized squared residuals (variance)

# Which model has most favourable information criteria AIC and SBC?
# function written by the lecturers# requires a list of names of EXISTING # (g)arch model results as an argumentcompare.ICs(c("k.arch1","k.arch5","k.garch11","k.garch12"))
# Does the best model have all parameters significant? 


# 6.# Let's assume that the final model is GARCH(1,2) 
str(k.garch12)
# 7.# Plot of conditional variance estimatespar(mfrow=c(2,1))plot(k.garch12 at data, # @data = original data values     type="l",col="red",ylab="r",main="Log-returns of SP500")plot(k.garch12 at h.t, # @h.t = conditional variance     type="l",col="darkgreen",     ylab="cvar",main="Conditional variance from GARCH(1,2)")par(mfrow=c(1,1))
# 8.# Do standardized residuals come from normal distribution?stdres<-k.garch12 at residuals/sqrt(k.garch12 at h.t)
hist(stdres,breaks=20,prob=T,     main="Histogram of standardized residuals \n from GARCH(1,2) for SP500")# lets add a normal density curve for # sample mean and variance curve(dnorm(x, mean=mean(stdres,na.rm=T),             sd=sd(stdres,na.rm=T)),       col="darkblue", lwd=2, add=TRUE)
# Jarque-Bera testjarque.bera.test(stdres)
# normalit yrejected
# Durbin Watson testdurbinWatsonTest(lm(stdres~1),                 max.lag=5) # lets check first 5 orders
# the first seems to be significant - indicates we can add # an ARIMA component in the mean equation
# ARCH effects among standardized residualsArchTest(stdres,lags=5)

Thanks in advance,
Yours sincerely,

Mehmet Dogan



 		 	   		  

From lists at dewey.myzen.co.uk  Sun Sep 13 19:42:50 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sun, 13 Sep 2015 18:42:50 +0100
Subject: [R] a question about data manipulation in R
In-Reply-To: <530131797.2037273.1442121127042.JavaMail.yahoo@mail.yahoo.com>
References: <530131797.2037273.1442121127042.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55F5B59A.2060401@dewey.myzen.co.uk>

Dear Zahra

Your email is completely unreadable because you posted in HTML. Please 
reset your mailer and try again.

On 13/09/2015 06:12, Zahra Karimi via R-help wrote:
> Hello,
> I have a data like this:1 2331 3331 4551 3452 5432 4332 3442 4003 4443 1113 0003 432I want to change it to this new dataset:c1    c2     c3233  543  444333  433  111455  344  000345  400  432
>   How can I do this in R? Please if this not the correct place to ask  my queston.. then could you please guid me , where should I ask my question?kind regards,Zahra
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From wdunlap at tibco.com  Sun Sep 13 19:50:27 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 13 Sep 2015 10:50:27 -0700
Subject: [R] For logical i and length(i) > length(x),
 x[i] <- value makes length(x) == length(i)
In-Reply-To: <1442074621.8236.YahooMailBasic@web125102.mail.ne1.yahoo.com>
References: <1442074621.8236.YahooMailBasic@web125102.mail.ne1.yahoo.com>
Message-ID: <CAF8bMcY3serk7iUPhUN3RcgKGGRxQ1YSZoVXeP7dzTtNViTG2Q@mail.gmail.com>

>>From it, I infer that, if i is logical and length(i) >= length(x), x[i] <- value has >the same effect to x[which(i)] <- value, where which(i) takes indices where i is >T.

The return value of which(i) gives only the indices of the TRUE values in i.
It does not return the length of i.  Hence
   x[which(i)]
which is equivalent to
   w <- which(i)
   x[w]
cannot return something that depends on length(i).

Your inference is right for length(i)==length(x), which is the usual case.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Sat, Sep 12, 2015 at 9:17 AM, Suharto Anggono Suharto Anggono via
R-help <r-help at r-project.org> wrote:
> I'll make my points clearer.
>
> - The case of logical index vector longer than the original vector is not usual case for me. As I said, I encountered it in the code of function 'rank' in R.
>
> - Before, I have read "S-PLUS help" (version 3.4, it seems), http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/helpfiles/Subscript.html. The following is the relevant part. There are missing pieces there that I fill by guessing.
>
> Vector subscripts are generated with when i and x are both vectors. (....) The result of the expression is to extract or replace elements of x corresponding to a vector of positive indices computed according to the value of i.
>
> .... If i is logical the indices are produced by starting at 1 and selecting the numbers for which the corresponding element is T. If is shorter than it is extended by cyclic repetition. It can be longer than as well, with no change in the computation of indices. ....
>
> ....
>
> For replacements, x[i] <- value the rule is that the length of will be set to the largest value in the indices, if that is bigger than the current length of x. ....
>
>
> >From it, I infer that, if i is logical and length(i) >= length(x), x[i] <- value has the same effect to x[which(i)] <- value, where which(i) takes indices where i is T.
>
> - In R, if i is logical and length(i) > length(x), length(x) after x[i] <- value may be different from after x[which(i)] <- value.
>
> - So, I wonder if R inherits the behavior from S or not.
>
> - The behavior is not clearly documented in R. I just find "R Language Definition", "3.4.1 Indexing by vectors", that can be interpreted to imply the behavior.
>
> - However, for a particular case, function 'rank' in R relies on the behavior.
>
> However, it seems that relying on the behavior is not on purpose. Previously, at least until R 3.1.3, the code of function 'rank' has the following before yy <- NA .
>         yy <- integer(length(x))
>         storage.mode(yy) <- storage.mode(y)
>
> It seems that yy[] <- NA is what is intended.
>
> - However, for me, the behavior is plausible. The assumption is that indices from 1 to length(i) exist.
>
> --------------------------------------
>
> I think this behavior is consistent with typical indexing behaviour in R... I would ask you what result you thought you should get? I, for one, can think of all sorts of uses for numeric indexes that have different lengths than the vector, but am stumped to think of any use for what you are proposing.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<[hidden email]>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 5, 2015 10:02:05 AM PDT, Suharto Anggono Suharto Anggono via R-help <[hidden email]> wrote:
>
>>I came across this behavior when I followed the code of function 'rank'
>>in R.
>>
>>It seems that subassignment of a vector by a logical index vector that
>>is longer than the original vector always results in expanding the
>>original vector to the length of the index vector.
>>
>>The resulting length may be different from the result of subassignment
>>by the equivalent numeric vector. For subassignment of a vector by a
>>numeric index vector, the original vector is expanded to the maximum
>>index, if it is larger than the length of the original vector.
>>
>>This is an example.
>>
>>> x <- NA
>>> x[c(FALSE,TRUE,FALSE)] <- 1
>>> x
>>[1] NA  1 NA
>>
>>Compare to this.
>>
>>> x <- NA
>>> x[which(c(FALSE,TRUE,FALSE))] <- 1
>>> x
>>[1] NA  1
>>
>>Does S exhibit the same behavior?
>>
>>Currently, if there is NA and na.last = "keep", function 'rank' in R
>>relies on this behavior to give correct result length.
>>
>>In "R Language Definition", "3.4.1 Indexing by vectors" says: "Logical.
>>The indexing i should generally have the same length as x. .... If it
>>is longer, then x is conceptually extended with NAs. ...." The
>>statement can be taught to support the observed behavior.
>>
>>> sessionInfo()
>>R version 3.2.2 (2015-08-14)
>>Platform: i386-w64-mingw32/i386 (32-bit)
>>Running under: Windows XP (build 2600) Service Pack 2
>>
>>locale:
>>[1] LC_COLLATE=English_United States.1252
>>[2] LC_CTYPE=English_United States.1252
>>[3] LC_MONETARY=English_United States.1252
>>[4] LC_NUMERIC=C
>>[5] LC_TIME=English_United States.1252
>>
>>attached base packages:
>>[1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>______________________________________________
>>[hidden email] mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> [hidden email] mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Sun Sep 13 20:07:41 2015
From: jrkrideau at inbox.com (John Kane)
Date: Sun, 13 Sep 2015 10:07:41 -0800
Subject: [R] a question about data manipulation in R
In-Reply-To: <530131797.2037273.1442121127042.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <C23295B3CBA.0000028Djrkrideau@inbox.com>

You are in the right place but the question in not very clear. Have a look at the links below for some suggestions on how to ask a question on R-help.

I think we need the data in dput() format. See ?dput or the discussion of dput() in the links.

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html

Did you intend that the last item in the data set be  432I where that last character is an I (??  not a one 1?

John Kane
Kingston ON Canada


> -----Original Message-----
> From: r-help at r-project.org
> Sent: Sun, 13 Sep 2015 05:12:07 +0000 (UTC)
> To: r-help at r-project.org
> Subject: [R] a question about data manipulation in R
> 
> Hello,
> I have a data like this:1 2331 3331 4551 3452 5432 4332 3442 4003 4443
> 1113 0003 432I want to change it to this new dataset:c1 ? ?c2 ? ? c3233
> 543 ?444333 ?433 ?111455 ?344 ?000345 ?400 ?432
> ?How can I do this in R? Please if this not the correct place to ask ?my
> queston.. then could you please guid me , where should I ask my
> question?kind regards,Zahra
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From jrkrideau at inbox.com  Sun Sep 13 20:29:52 2015
From: jrkrideau at inbox.com (John Kane)
Date: Sun, 13 Sep 2015 10:29:52 -0800
Subject: [R] Please read **Urgent** Need help with R
In-Reply-To: <DUB116-W11C68AB4656AD52939242BFB5E0@phx.gbl>
Message-ID: <C26429B7F0A.000002AAjrkrideau@inbox.com>

Hi Mehmet,
Did you send this in HTML?  That's what it looks like.
It is currently almost unreadable. 
You need to reformat it and send it as plain text.

You need to reduce the code to the key parts if possible. It looks like most of the code is unneeded for the problem and is just a distraction.

Your data did not arrive. The R-help list is very fussy about what kind of files it allows, in order to prevent viruses and malware.

Here are a couple of links with suggestions on how to ask a question on R-help. In particular read about supplying data in dput() form. That is the best way to send data to R-help. 

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html


> I also attached my data. the problem could be also with the format of
> excel.
What has Excel got to do with this?

This looks like homework and we have a no-homework policy but get things cleaned up and we may be able to make some suggestions.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: mehmetdogan_07 at windowslive.com
> Sent: Sun, 13 Sep 2015 15:31:15 +0300
> To: r-help at r-project.org
> Subject: [R] Please read **Urgent** Need help with R
> 
> Dear All,
> I can NOT format and create an xts object of my data in order to forecast
> Volatility. Moreover I can NOT plot the data. basically i cant work with
> the data i have. I need to finish my project till tomorrow. Could you
> please help me with those problems?
> I also attached my data. the problem could be also with the format of
> excel.
> when i could like to plot data, the following error appears:Error in
> plot.window(...) : need finite 'ylim' valuesIn addition: Warning
> messages:1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by
> coercion2: In min(x) : no non-missing arguments to min; returning Inf3:
> In max(x) : no non-missing arguments to max; returning -Inf
> I have following codes.
> I can NOT format the date after these codes ****# function assumes that
> the data is stored in the subdirectory called
> "data"SP500<-read.csv("Data_Forecast/EURUSD.csv")
> 
> SP500<-read.csv("Data_Forecast/EURUSD.csv",stringsAsFactors=F)****
> 
> setwd("D:\\NewThesis")
> 
> # install and load needed
> librariesinstall.packages("fBasics")install.packages("tseries")install.packages("car")install.packages("FinTS")install.packages("fGarch")install.packages("rugarch")
> # rugarch#
> http://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf
> 
> library(xts)library(fBasics) # e.g. basicStats()library(tseries)# e.g.
> jarque.bera.test()library(car) # e.g. durbinWatsonTest()library(FinTS) #
> e.g. ArchTest()library(fGarch) # e.g. garchFit()library(rugarch) # e.g.
> ugarchfit()
> # !!! Introduction to the rugarch package#
> http://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf
> # lets load additional functions prepared by the
> lecturerssource("functions/TSA_lab06_functions.R")
> 
> 
> #################### Example #1.         # stylized facts
> ###################
> #1.# To import data we will use a function import_data()# written by
> lecturers and stored in TSA_lab06_functions.R# (already loaded)
> rm(list=ls())
> # function assumes that the data is stored in the subdirectory called
> "data"SP500<-read.csv("Data_Forecast/EURUSD.csv")
> 
> SP500<-read.csv("Data_Forecast/EURUSD.csv",stringsAsFactors=F)
> 
> 
#############################################################################3SP500$Date<-as.Date(SP500$Date,format="%Y-%m-%d"
> , "h:m:s")
>                     #format(chron(0, 0), c("d/m/yy", "h:m:s"), sep = " ",
> enclose = c("",
> ""))###################################################################################
> 
> str(SP500)# Date column is already converted to Date format
> #First 6 head(SP500)
> #Last 6tail(SP500)
> # lets leave just date and close priceSP500<-SP500[,c(1,5)]
> 
> # and change the name of the last column into
> SP500names(SP500)[2]<-"SP500"
> SP500.xts<-xts(SP500$SP500,SP500$Date)
> #First 6 head(SP500)
> #Last 6tail(SP500)
> #############################################################################333attach(SP500)x<-Timey<-Closedetach(SP500)plot(x,y)##############################################################################plot(SP500)
> plot(SP500$Date,SP500$SP500,type="l",     main="Daily close price of
> SP500")
> # lets add log-returns to the dataSP500$r<-diff.xts(log(SP500$SP500))
> 
> # lets limit our data to days since the beginning of
> 2000SP500<-SP500[SP500$Date>=as.Date("04-09-2015"),]
> # lets plot the close priceplot(SP500$Date,SP500$SP500,type="l",
> main="Daily close price of SP500")
> # ... and it's log-returnsplot(SP500$Date,SP500$r,type="l", col="red",
> main="Log-returns of SP500")abline(h=0,col="gray",lty=2) #abline
> functions adds line to the plot
> # lets also plot the ACF function of
> log-returnsacf(SP500$r,lag.max=36,na.action=na.pass,    ylim=c(-0.1,0.1),
> # we rescale the vertical axis    col="darkblue",lwd=7,main="ACF of
> log-returns of SP500")
> # this indicates some autoregressive relations among returns# which can
> be used to build an ARIMA model
> 
> # and do the same for the quared
> log-returnsacf(SP500$r^2,lag.max=36,na.action=na.pass,    ylim=c(0,0.5),
> # we rescale the vertical axis    col="darkblue",lwd=7,main="ACF of
> log-returns of SP500")
> # this in turn indicates some autoregressive relations among SQUARED
> returns# (their variance!) which can be used to build a (G)ARCH model
> 
> # 2.# Do log-returns come from normal distribution?
> basicStats(SP500$r)
> hist(SP500$r,prob=T,breaks=40)# lets add a normal density curve for #
> sample mean and variance # dnorm() generates density function for a
> specified value# and parameters (mean, sd)
> curve(dnorm(x,     # data vector  # dnorm creates a density value for the
> mean=mean(SP500$r,na.rm=T), # mean    # normal distribution
> sd=sd(SP500$r,na.rm=T)),    # sd      col="darkblue", lwd=2, add=TRUE)
> 
> # 3.# Let's also examine:# *) Jarque-Bera statistic
> jarque.bera.test(SP500$r)
> # the function doesn't accept missing values
> # lets omit the missing valuesjarque.bera.test(na.omit(SP500$r)) #
> na.omit functions omits the missing values
> # null about normality strongly rejected
> # *) Durbin-Watson statistic# the durbinWatsonTest() function requires #
> the linear model as an argument# lets simulate a model with just a
> constant term
> durbinWatsonTest(lm(SP500$r~1), # here 1 means a constant
> max.lag=5) # lets check first 5 orders.
> # lack of autocorrelation in returns strongly rejected # for lag 1,2 and
> 5
> # *) ARCH effects among log-returns
> ArchTest(SP500$r,lags=5)  #(first 5 lags together)
> # null stongly rejected
> 
> # we can also apply a DW test for squared
> returnsdurbinWatsonTest(lm(SP500$r^2~1),                 max.lag=5) #
> lets check first 5 orders# all pvalues 0. It means in all lags wereject
> autocorrelation # now ALL lags significant !!!
> 
> 
> ################################################# Example #2
> # Choosing the proper order of GARCH(p,q) model
> ################################################
> 
> # 1.# ARCH(1)
> k.arch1<-garchFit(SP500$r~garch(1,0), # we assume that returns follow an
> ARCH(1) process                  cond.dist= "norm", # conditional
> distribution of errors                  trace=FALSE) # if we don't want
> to see history of iterations
> # summary of results and some diagnostic testssummary(k.arch1)
> # The intercept in the model above is not significatn so we can drop
> it.k.arch1<-garchFit(SP500$r~garch(1,0),include.mean=F,
> cond.dist= "norm", # conditional distribution of errors
> trace=FALSE) # if we don't want to see history of iterations
> # summary of results and some diagnostic testssummary(k.arch1)
> # Let's examine squares of standardized residuals.
> # we can automatically plot several parts of results# (lets select 11:
> ACF of Squared Standardized Residuals, # 10: ACF of Standardized
> Residuals,# and 11: Conditional SD; to quit press ESC)plot(k.arch1)
> # squared residuals still have significant ACF for lags: 2,3,5,7,8,9,10
> 
> # 2.# lets try if ARCH(5) is enough to catch these
> phenomenonk.arch5<-garchFit(SP500$r~garch(5,0),include.mean=F,
> cond.dist= "norm", # conditional distribution of errors
> trace=FALSE) # if we don't want to see history of iterations
> summary(k.arch5)
> # all parameters significant# Ljung-box test for R^2 shows there is no
> more autocorrelation # between the current and past standardized squared
> residuals (variance)
> # lets plot the ACF of standardized residuals# and standardized squared
> residuals (plots 10, 11)# ESC to exit
> plot(k.arch5)
> # lag nr 10 in ACF for squared residuals seems to be still significant,
> but# lets ignore it based on previous tests (LB)
> # 4.# lets compare it with
> GARCH(1,1)k.garch11<-garchFit(SP500$r~garch(1,1),include.mean=F,
> cond.dist= "norm", # conditional distribution of errors
> trace=FALSE) # if we don't want to see history of iterations
> summary(k.garch11)
> # all parameters significant# Ljung-box test for R^2 shows there is no
> more autocorrelation # between the current and past standardized squared
> residuals (variance)
> 
> # lets plot the ACF of standardized residuals# and standardized squared
> residuals (plots 10, 11)# ESC to exit
> plot(k.garch11)
> # 5.# lets check higher order
> GARCH(1,2)k.garch12<-garchFit(SP500$r~garch(1,2),include.mean=F,
> cond.dist= "norm", # conditional distribution of errors
> trace=F) # if we don't want to see history of iterations
> summary(k.garch12)
> 
> # lets plot the ACF of standardized residuals# and standardized squared
> residuals (plots 10, 11)# ESC to exit
> plot(k.garch12)
> # all parameters significant# Ljung-box test for R^2 shows there is no
> more autocorrelation # between the current and past standardized squared
> residuals (variance)
> 
> # Which model has most favourable information criteria AIC and SBC?
> # function written by the lecturers# requires a list of names of EXISTING
> # (g)arch model results as an
> argumentcompare.ICs(c("k.arch1","k.arch5","k.garch11","k.garch12"))
> # Does the best model have all parameters significant?
> 
> 
> # 6.# Let's assume that the final model is GARCH(1,2)
> str(k.garch12)
> # 7.# Plot of conditional variance
> estimatespar(mfrow=c(2,1))plot(k.garch12 at data, # @data = original data
> values     type="l",col="red",ylab="r",main="Log-returns of
> SP500")plot(k.garch12 at h.t, # @h.t = conditional variance
> type="l",col="darkgreen",     ylab="cvar",main="Conditional variance from
> GARCH(1,2)")par(mfrow=c(1,1))
> # 8.# Do standardized residuals come from normal
> distribution?stdres<-k.garch12 at residuals/sqrt(k.garch12 at h.t)
> hist(stdres,breaks=20,prob=T,     main="Histogram of standardized
> residuals \n from GARCH(1,2) for SP500")# lets add a normal density curve
> for # sample mean and variance curve(dnorm(x, mean=mean(stdres,na.rm=T),
> sd=sd(stdres,na.rm=T)),       col="darkblue", lwd=2, add=TRUE)
> # Jarque-Bera testjarque.bera.test(stdres)
> # normalit yrejected
> # Durbin Watson testdurbinWatsonTest(lm(stdres~1),
> max.lag=5) # lets check first 5 orders
> # the first seems to be significant - indicates we can add # an ARIMA
> component in the mean equation
> # ARCH effects among standardized residualsArchTest(stdres,lags=5)
> 
> Thanks in advance,
> Yours sincerely,
> 
> Mehmet Dogan
> 
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From dcarlson at tamu.edu  Sun Sep 13 22:30:52 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Sun, 13 Sep 2015 20:30:52 +0000
Subject: [R] kendall tau distance
In-Reply-To: <DUB125-W727649217D6675E1D7958B35F0@phx.gbl>
References: <DUB125-W94DB648B732F8E71C200C2B3510@phx.gbl>,
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C0914@mb02.ads.tamu.edu>
	<DUB125-W727649217D6675E1D7958B35F0@phx.gbl>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C12B2@mb02.ads.tamu.edu>

The function takes a two way table. Assuming your example involves two ordinal vectors c(1,2,3,4,5) and c(3,4,1,2,5), you need:

> xy <- table(c(1,2,3,4,5),c(3,4,1,2,5))
> xy
   
    1 2 3 4 5
  1 0 0 1 0 0
  2 0 0 0 1 0
  3 1 0 0 0 0
  4 0 1 0 0 0
  5 0 0 0 0 1
> cd <- ConDisPairs(xy)
> cd
$pi.c
     [,1] [,2] [,3] [,4] [,5]
[1,]    3    2    2    1    0
[2,]    2    1    1    2    1
[3,]    2    1    1    2    2
[4,]    1    2    2    3    3
[5,]    0    1    2    3    4

$pi.d
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    1    2    2    3
[2,]    1    2    2    2    2
[3,]    2    2    2    1    1
[4,]    2    2    1    0    0
[5,]    3    2    1    0    0

$C
[1] 6

$D
[1] 4

Where C is the number of concordant pairs and D is the number of discordant pairs. Kendall's tau distance is the number of discordant pairs = 4 and the normalized tau is 4/(5*(5-1)/2) = .4.

$pi.c and $pi.d are used to compute C and D as follows:

> sum((xy * cd$pi.c))/2
[1] 6
> sum((xy * cd$pi.d))/2
[1] 4


David L. Carlson
Department of Anthropology
Texas A&M University


-----Original Message-----
From: Ragia Ibrahim [mailto:ragia11 at hotmail.com] 
Sent: Saturday, September 12, 2015 4:21 AM
To: David L Carlson <dcarlson at tamu.edu>; r-help at r-project.org
Subject: RE: [R] kendall tau distance

many thanks for replying.
?I have the vectors
?running the code

ConDisPairs( data.frame(c(1,2,3,4,5),c(3,4,1,2,5)) )?

$pi.c
? ? ?[,1] [,2]
[1,] ? 12 ? ?0
[2,] ? ?8 ? ?1
[3,] ? ?7 ? ?3
[4,] ? ?5 ? ?6
[5,] ? ?0 ? 10

$pi.d
? ? ?[,1] [,2]
[1,] ? ?0 ? 14
[2,] ? ?3 ? 12
[3,] ? ?7 ? ?9
[4,] ? ?8 ? ?5
[5,] ? 10 ? ?0

$C
[1] 69

$D
[1] 109


I could not find the result related to the result at wiki page in any way..looking for 4 ? dissimilar pairs?

many thanks

----------------------------------------
> From: dcarlson at tamu.edu
> To: ragia11 at hotmail.com; r-help at r-project.org
> Subject: RE: [R] kendall tau distance
> Date: Fri, 11 Sep 2015 15:37:19 +0000
>
> The Wikipedia article gives a simple formula based on the number of discordant pairs. You can get that from the ConDisPairs() function in package DescTools.
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ragia Ibrahim
> Sent: Thursday, September 10, 2015 12:40 PM
> To: r-help at r-project.org
> Subject: [R] kendall tau distance
>
> Dear group
> how to calculate kendall tau distance according to Kendall_tau_distance at wikipedia
>
> <a href="https&#58;&#47;&#47;en.wikipedia.org&#47;wiki&#47;Kendall_tau_distance" target="_blank" class="newlyinsertedlink">https&#58;&#47;&#47;en.wikipedia.org&#47;wiki&#47;Kendall_tau_distance</a>
>
>
> thanks in advance
> Ragia
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
 		 	   		  


From m.dogan at mail.com  Sun Sep 13 23:08:35 2015
From: m.dogan at mail.com (Mehmet Dogan)
Date: Sun, 13 Sep 2015 23:08:35 +0200
Subject: [R] Please read **URGENT** Need help with R
Message-ID: <trinity-1eb60c87-3e6c-4450-ba53-46e3bd0ad841-1442178514358@3capp-mailcom-lxa15>

An embedded and charset-unspecified text was scrubbed...
Name: R_Codes.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150913/227d82d8/attachment.txt>

From anguillaphile at outlook.com  Mon Sep 14 00:11:17 2015
From: anguillaphile at outlook.com (B Jessop)
Date: Sun, 13 Sep 2015 19:11:17 -0300
Subject: [R] Problem solved
Message-ID: <BLU184-W48194BE4F3490C4BAE4B3FB15E0@phx.gbl>

Further to my posting re: extracting case values from ancova adjusted means, I have solved the problem.
 
Regards,
Brian Jessop 
 		 	   		  
	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Mon Sep 14 00:28:52 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 14 Sep 2015 10:28:52 +1200
Subject: [R] Please read **Urgent** Need help with R
In-Reply-To: <DUB116-W11C68AB4656AD52939242BFB5E0@phx.gbl>
References: <DUB116-W11C68AB4656AD52939242BFB5E0@phx.gbl>
Message-ID: <55F5F8A4.5000507@auckland.ac.nz>


On 14/09/15 00:31, Mehmet Do?an wrote:

> Dear All, I can NOT format and create an xts object of my data in
> order to forecast Volatility. Moreover I can NOT plot the data.
> basically i cant work with the data i have. I need to finish my
> project till tomorrow. Could you please help me with those problems?

<SNIP>

See fortune(122).

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From leptostracan at yahoo.com  Mon Sep 14 05:43:18 2015
From: leptostracan at yahoo.com (Christine Lee)
Date: Sun, 13 Sep 2015 20:43:18 -0700
Subject: [R] adding a line across plots in xy plot with panel.abline
Message-ID: <1442202198.46114.YahooMailBasic@web120805.mail.ne1.yahoo.com>

Dear all,

I want to draw a line at DI=1 across all four graphs in the xy plot, I have used panel.abline, but I failed to do so, does any one has an idea of what has went wrong?

structure(list(Date = structure(c(6L, 7L, 2L, 4L, 13L, 17L, 5L, 
12L, 4L, 11L, 14L, 9L, 7L, 2L, 4L, 13L, 10L, 17L, 5L, 12L, 8L, 
4L, 11L, 14L, 9L, 16L, 15L, 3L, 10L, 1L, 17L, 5L, 12L, 8L, 4L, 
11L, 14L, 12L, 8L, 4L, 11L, 14L), .Label = c("1/10", "1/11", 
"11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", "23/9", 
"26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class = "factor"), 
    Year = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
    2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
    1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"), 
    Station = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 
    4L, 4L, 4L), .Label = c("E", "F", "H", "I"), class = "factor"), 
    DI = c(13.4, 7, 12.6, 12.6, 5.2, 2.2, 1, 1, 1.2, 0.8, 2.3, 
    3.8, 5.4, 4.6, 5, 3.2, 3.1, 8.7, 2.1, 2.7, 4.9, 4, 2.2, 5.3, 
    5.6, 4.8, 4, 8.6, 1.9, 2.9, 5.9, 2.6, 8.9, 4, 13.5, 15.3, 
    12.8, 3.4, 4.7, 1.7, 0.9, 1.7), Date1 = structure(c(16697, 
    16710, 16740, 16751, 16768, 16714, 16721, 16737, 16751, 16765, 
    16769, 16698, 16710, 16740, 16751, 16768, 16701, 16714, 16721, 
    16737, 16741, 16751, 16765, 16769, 16698, 16712, 16739, 16750, 
    16701, 16709, 16714, 16721, 16737, 16741, 16751, 16765, 16769, 
    16737, 16741, 16751, 16765, 16769), class = "Date")), .Names = c("Date", 
"Year", "Station", "DI", "Date1"), row.names = c(NA, -42L), class = "data.frame")
Raw$Date1<-as.Date(Raw$Date,"%d/%m")
culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014")
library(lattice)
xyplot(DI~Date1|Station, data=Raw,
        groups = culr,
        par.settings = list(strip.background = list(col = "transparent"),
                            superpose.symbol = list(cex = rep(2, 2),
                                                    col=c("grey","black"),
                                                    pch = rep(16,2))),
        type="p",
        xlab=list("Month",cex=1.5),
        ylab=list("Dispersion index",cex=1.5),  
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1),
        panel=function(Date1,DI){
        panel.labline(h=1,lty=2,lwd=3)
        })

Many thanks.

Regards,
Christine

 


From ryanshuell at gmail.com  Mon Sep 14 04:18:25 2015
From: ryanshuell at gmail.com (Ryan Shuell)
Date: Sun, 13 Sep 2015 19:18:25 -0700 (PDT)
Subject: [R] How to read CSV from web?
In-Reply-To: <1438166277331-4710513.post@n4.nabble.com>
References: <1438140099663-4710502.post@n4.nabble.com>
	<1438166277331-4710513.post@n4.nabble.com>
Message-ID: <ff51806b-1b73-4551-921f-c887932e3f02@googlegroups.com>

How about this?


library(XML)
library(RCurl)

url <- 
"https://raw.githubusercontent.com/sjkiss/Survey/master/mlogit.out.csv"

urldata <- getURL(url)
data <- readHTMLTable(urldata, stringsAsFactors = FALSE)







On Wednesday, July 29, 2015 at 6:41:35 AM UTC-4, jpara3 wrote:
>
> data<-read.csv("
> https://raw.githubusercontent.com/sjkiss/Survey/master/mlogit.out.csv",header=T,sep=",") 
>
>
>
>
> -- 
> View this message in context: 
> http://r.789695.n4.nabble.com/How-to-read-CSV-from-web-tp4710502p4710513.html 
> Sent from the R help mailing list archive at Nabble.com. 
>
> ______________________________________________ 
> R-h... at r-project.org <javascript:> mailing list -- To UNSUBSCRIBE and 
> more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 
>

From ryanshuell at gmail.com  Mon Sep 14 04:19:05 2015
From: ryanshuell at gmail.com (Ryan Shuell)
Date: Sun, 13 Sep 2015 19:19:05 -0700 (PDT)
Subject: [R] How to read CSV from web?
In-Reply-To: <1438166277331-4710513.post@n4.nabble.com>
References: <1438140099663-4710502.post@n4.nabble.com>
	<1438166277331-4710513.post@n4.nabble.com>
Message-ID: <e95ea21c-2029-4515-83e0-cc09aeb8b138@googlegroups.com>

Also, you should bookmark this link.

http://www.r-bloggers.com/this-r-data-import-tutorial-is-everything-you-need/




On Wednesday, July 29, 2015 at 6:41:35 AM UTC-4, jpara3 wrote:
>
> data<-read.csv("
> https://raw.githubusercontent.com/sjkiss/Survey/master/mlogit.out.csv",header=T,sep=",") 
>
>
>
>
> -- 
> View this message in context: 
> http://r.789695.n4.nabble.com/How-to-read-CSV-from-web-tp4710502p4710513.html 
> Sent from the R help mailing list archive at Nabble.com. 
>
> ______________________________________________ 
> R-h... at r-project.org <javascript:> mailing list -- To UNSUBSCRIBE and 
> more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 
>

From dwinsemius at comcast.net  Mon Sep 14 07:29:30 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 13 Sep 2015 22:29:30 -0700 (PDT)
Subject: [R] removing outlier
In-Reply-To: <CAGxFJbSAXmEweyW4ESe+YGCMdnYBrNu2g7KPq+3BoQyiCbeJvw@mail.gmail.com>
References: <1441980906861-4712137.post@n4.nabble.com>
	<CA+8X3fUb90tmCQYG=8i+zTvj1RST7wQ6BkAUEqUG=prd+7B2cw@mail.gmail.com>
	<1442050359278-4712170.post@n4.nabble.com>
	<BA02E3C3-BD49-424D-9D26-810CB8BF8E6E@comcast.net>
	<CAGxFJbSAXmEweyW4ESe+YGCMdnYBrNu2g7KPq+3BoQyiCbeJvw@mail.gmail.com>
Message-ID: <1442208570416-4712208.post@n4.nabble.com>



If this mailing list accepted formatted submissions I would have used the
tr?sModernSarcastic font for my first sentence. Failing the availability of
that mode of communication I am (top) posting through Nabble (perhaps)  in
"Comic Sans".<br />

On Sat, Sep 12, 2015 at 9:52 AM, David Winsemius &lt;dwinsemius@&gt; wrote:
>
> On Sep 12, 2015, at 2:32 AM, Juli wrote:

>> And if I remove the outliers, my problem ist, that as you said, they
>> differ
>> in length. I need the data frame for a regression, so can I remove the
>> whole
>> column or is there a call to exclude the data?
>
*> Most regression methods have a 'subset' parameter which would allow you
to distort the data to your desired specification.*


Bert Gunter-2 wrote
> 
/
> ... and this, of course, is a nice example of how statistics
> contributes to the "irreproducibility crisis" now roiling Science.
/
> 
> Cheers,
> Bert
> 
> (Quote from a long ago engineering colleague: "Whenever I see an
> outlier, I never know whether to throw it away or patent it.")
> 
> 
> Bert Gunter
> 
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
> 
> 
> On Sat, Sep 12, 2015 at 9:52 AM, David Winsemius &lt;

> dwinsemius@

> &gt; wrote:
>>
>> On Sep 12, 2015, at 2:32 AM, Juli wrote:
>>
>>> Hi Jim,
>>>
>>> thank you for your help. :)
>>>
>>> My point is, that there are outlier and I don?t really know how to deal
>>> with
>>> that.
>>>
>>> I need the dataframe for a regression and read often that only a few
>>> outlier
>>> can change your results very much. In addition, regression diacnostics
>>> didn?t indcate me the best results.
>>> Yes, and I know its not the core of statistics to work in a way you get
>>> results you would like to have ;).
>>>
>>> So what is your suggestion?
>>>
>>> And if I remove the outliers, my problem ist, that as you said, they
>>> differ
>>> in length. I need the data frame for a regression, so can I remove the
>>> whole
>>> column or is there a call to exclude the data?
>>
>> Most regression methods have a 'subset' parameter which would allow you
>> to distort the data to your desired specification. But why not think
>> about examining a different statistical model or using robust methods?
>> That way you can keep all your data. (Sounds like you don't really have a
>> lot.)
>>
>> --
>> David.
>>>
>>> JULI
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://r.789695.n4.nabble.com/removing-outlier-tp4712137p4712170.html
>>> Sent from the R help mailing list archive at Nabble.com.
>>>
>>> ______________________________________________
>>> 

> R-help@

>  mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> ______________________________________________
>> 

> R-help@

>  mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________

> R-help@

>  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.





--
View this message in context: http://r.789695.n4.nabble.com/removing-outlier-tp4712137p4712208.html
Sent from the R help mailing list archive at Nabble.com.


From bgunter.4567 at gmail.com  Mon Sep 14 08:00:24 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 13 Sep 2015 23:00:24 -0700
Subject: [R] adding a line across plots in xy plot with panel.abline
In-Reply-To: <1442202198.46114.YahooMailBasic@web120805.mail.ne1.yahoo.com>
References: <1442202198.46114.YahooMailBasic@web120805.mail.ne1.yahoo.com>
Message-ID: <CAGxFJbS1BEsah2MWnHoCY8dTbMHwito53rZHWAeXPG53fd=A+w@mail.gmail.com>

You misspelled panel.abline as panel.labline in your code you included
here. If that's the real code, that's why.

-- Bert



Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sun, Sep 13, 2015 at 8:43 PM, Christine Lee via R-help
<r-help at r-project.org> wrote:
> Dear all,
>
> I want to draw a line at DI=1 across all four graphs in the xy plot, I have used panel.abline, but I failed to do so, does any one has an idea of what has went wrong?
>
> structure(list(Date = structure(c(6L, 7L, 2L, 4L, 13L, 17L, 5L,
> 12L, 4L, 11L, 14L, 9L, 7L, 2L, 4L, 13L, 10L, 17L, 5L, 12L, 8L,
> 4L, 11L, 14L, 9L, 16L, 15L, 3L, 10L, 1L, 17L, 5L, 12L, 8L, 4L,
> 11L, 14L, 12L, 8L, 4L, 11L, 14L), .Label = c("1/10", "1/11",
> "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", "23/9",
> "26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class = "factor"),
>     Year = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
>     2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,
>     1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"),
>     Station = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L,
>     4L, 4L, 4L), .Label = c("E", "F", "H", "I"), class = "factor"),
>     DI = c(13.4, 7, 12.6, 12.6, 5.2, 2.2, 1, 1, 1.2, 0.8, 2.3,
>     3.8, 5.4, 4.6, 5, 3.2, 3.1, 8.7, 2.1, 2.7, 4.9, 4, 2.2, 5.3,
>     5.6, 4.8, 4, 8.6, 1.9, 2.9, 5.9, 2.6, 8.9, 4, 13.5, 15.3,
>     12.8, 3.4, 4.7, 1.7, 0.9, 1.7), Date1 = structure(c(16697,
>     16710, 16740, 16751, 16768, 16714, 16721, 16737, 16751, 16765,
>     16769, 16698, 16710, 16740, 16751, 16768, 16701, 16714, 16721,
>     16737, 16741, 16751, 16765, 16769, 16698, 16712, 16739, 16750,
>     16701, 16709, 16714, 16721, 16737, 16741, 16751, 16765, 16769,
>     16737, 16741, 16751, 16765, 16769), class = "Date")), .Names = c("Date",
> "Year", "Station", "DI", "Date1"), row.names = c(NA, -42L), class = "data.frame")
> Raw$Date1<-as.Date(Raw$Date,"%d/%m")
> culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014")
> library(lattice)
> xyplot(DI~Date1|Station, data=Raw,
>         groups = culr,
>         par.settings = list(strip.background = list(col = "transparent"),
>                             superpose.symbol = list(cex = rep(2, 2),
>                                                     col=c("grey","black"),
>                                                     pch = rep(16,2))),
>         type="p",
>         xlab=list("Month",cex=1.5),
>         ylab=list("Dispersion index",cex=1.5),
>         index.cond=list(c(1,2,3,4)),
>         auto.key = T,
>         layout=c(4,1),
>         panel=function(Date1,DI){
>         panel.labline(h=1,lty=2,lwd=3)
>         })
>
> Many thanks.
>
> Regards,
> Christine
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j.para.fernandez at hotmail.com  Mon Sep 14 09:19:02 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Mon, 14 Sep 2015 00:19:02 -0700 (PDT)
Subject: [R] Model after random forest
In-Reply-To: <1442127841457-4712189.post@n4.nabble.com>
References: <1442127841457-4712189.post@n4.nabble.com>
Message-ID: <1442215142807-4712210.post@n4.nabble.com>

Is using a tree model a good idea?

Thanks




-----

Guided Tours Basque Country 

Guided tours in the three capitals of the Basque Country: Bilbao, Vitoria-Gasteiz and San Sebastian, as well as in their provinces. Available languages.

Travel planners for groups and design of tourist routes across the Basque Country.
--
View this message in context: http://r.789695.n4.nabble.com/Model-after-random-forest-tp4712189p4712210.html
Sent from the R help mailing list archive at Nabble.com.


From dulcalma at bigpond.com  Mon Sep 14 09:56:22 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Mon, 14 Sep 2015 17:56:22 +1000
Subject: [R] adding a line across plots in xy plot with panel.abline
In-Reply-To: <1442202198.46114.YahooMailBasic@web120805.mail.ne1.yahoo.com>
References: <1442202198.46114.YahooMailBasic@web120805.mail.ne1.yahoo.com>
Message-ID: <000001d0eec2$d9f14c00$8dd3e400$@bigpond.com>

Hi 

As Bert has intimated there was a misspelling in panel.abline

To get points as well you need the appropriate arguments for a panel
function otherwise you would only get a line.
You only had a function for  a line in panel function so adding panel.xyplot
will give you points as well.


xyplot(DI~Date1|Station, data=Raw,
        groups = culr,
        par.settings = list(strip.background = list(col = "transparent"),
                            superpose.symbol = list(cex = rep(2, 2),
                                                    col=c("grey","black"),
                                                    pch = rep(16,2))),
        type="p",
        xlab=list("Month",cex=1.5),
        ylab=list("Dispersion index",cex=1.5),
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1),
        panel = function(x, y, ...){
        
          panel.xyplot(x,y, ...)
          panel.abline(h=1,lty=2,lwd=3)
        }

 )

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Christine
Lee via R-help
Sent: Monday, 14 September 2015 13:43
To: r-help at r-project.org
Subject: [R] adding a line across plots in xy plot with panel.abline

Dear all,

I want to draw a line at DI=1 across all four graphs in the xy plot, I have
used panel.abline, but I failed to do so, does any one has an idea of what
has went wrong?

structure(list(Date = structure(c(6L, 7L, 2L, 4L, 13L, 17L, 5L, 
12L, 4L, 11L, 14L, 9L, 7L, 2L, 4L, 13L, 10L, 17L, 5L, 12L, 8L, 
4L, 11L, 14L, 9L, 16L, 15L, 3L, 10L, 1L, 17L, 5L, 12L, 8L, 4L, 
11L, 14L, 12L, 8L, 4L, 11L, 14L), .Label = c("1/10", "1/11", 
"11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", "23/9", 
"26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class =
"factor"), 
    Year = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
    2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
    1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"), 
    Station = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 
    4L, 4L, 4L), .Label = c("E", "F", "H", "I"), class = "factor"), 
    DI = c(13.4, 7, 12.6, 12.6, 5.2, 2.2, 1, 1, 1.2, 0.8, 2.3, 
    3.8, 5.4, 4.6, 5, 3.2, 3.1, 8.7, 2.1, 2.7, 4.9, 4, 2.2, 5.3, 
    5.6, 4.8, 4, 8.6, 1.9, 2.9, 5.9, 2.6, 8.9, 4, 13.5, 15.3, 
    12.8, 3.4, 4.7, 1.7, 0.9, 1.7), Date1 = structure(c(16697, 
    16710, 16740, 16751, 16768, 16714, 16721, 16737, 16751, 16765, 
    16769, 16698, 16710, 16740, 16751, 16768, 16701, 16714, 16721, 
    16737, 16741, 16751, 16765, 16769, 16698, 16712, 16739, 16750, 
    16701, 16709, 16714, 16721, 16737, 16741, 16751, 16765, 16769, 
    16737, 16741, 16751, 16765, 16769), class = "Date")), .Names = c("Date",

"Year", "Station", "DI", "Date1"), row.names = c(NA, -42L), class =
"data.frame")
Raw$Date1<-as.Date(Raw$Date,"%d/%m")
culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014")
library(lattice)
xyplot(DI~Date1|Station, data=Raw,
        groups = culr,
        par.settings = list(strip.background = list(col = "transparent"),
                            superpose.symbol = list(cex = rep(2, 2),
                                                    col=c("grey","black"),
                                                    pch = rep(16,2))),
        type="p",
        xlab=list("Month",cex=1.5),
        ylab=list("Dispersion index",cex=1.5),  
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1),
        panel=function(Date1,DI){
        panel.labline(h=1,lty=2,lwd=3)
        })

Many thanks.

Regards,
Christine

 

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From cryan at binghamton.edu  Mon Sep 14 00:35:08 2015
From: cryan at binghamton.edu (cryan at binghamton.edu)
Date: Sun, 13 Sep 2015 18:35:08 -0400
Subject: [R] Please read **URGENT** Need help with R
In-Reply-To: <trinity-1eb60c87-3e6c-4450-ba53-46e3bd0ad841-1442178514358@3capp-mailcom-lxa15>
References: <trinity-1eb60c87-3e6c-4450-ba53-46e3bd0ad841-1442178514358@3capp-mailcom-lxa15>
Message-ID: <ee411999-ce2f-456b-987f-d4770fce7cbc.maildroid@localhost>

After your 4th line of code (your second read.csv statement) what is the output of

str(EURUSD)

--Chris Ryan

Sent from my android device.

-----Original Message-----
From: Mehmet Dogan <m.dogan at mail.com>
To: r-help at r-project.org
Sent: Sun, 13 Sep 2015 17:57
Subject: [R] Please read **URGENT** Need help with R

library(xts)
library(zoo)

EURUSD<-read.csv("Data_Forecast/EURUSD.csv")   # to load the data

EURUSD<-read.csv("Data_Forecast/EURUSD.csv",stringsAsFactors=F)   # to load as a factor

#??????????
EURUSD$Date<-as.Date(EURUSD$Date,format="%Y-%m-%d" , "h:m:s")   # here the code is not working to format the "Date"


# lets leave just date and close price
EURUSD<-EURUSD[,c(1,5)]


# and change the name of the last column into EURUSD
names(EURUSD)[2]<-"EURUSD"

#?????????????
EURUSD.xts<-xts(EURUSD$EURUSD,SP500$Date)  # here also i create xts object for these time series
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From leptostracan at yahoo.com  Mon Sep 14 11:30:02 2015
From: leptostracan at yahoo.com (Christine Lee)
Date: Mon, 14 Sep 2015 09:30:02 +0000 (UTC)
Subject: [R] =?utf-8?b?5Zue6KaG77iwICBhZGRpbmcgYSBsaW5lIGFjcm9zcyBwbG90?=
 =?utf-8?q?s_in_xy_plot_with_panel=2Eabline?=
In-Reply-To: <000001d0eec2$d9f14c00$8dd3e400$@bigpond.com>
References: <000001d0eec2$d9f14c00$8dd3e400$@bigpond.com>
Message-ID: <590582702.2206506.1442223002152.JavaMail.yahoo@mail.yahoo.com>

Thanks, Duncan,?I am confused by the function(x,y, ...) and panel.xyplot(x,y,...).? Is this a must to put in "..."??I knew this may be a stupid question.? I am sorry.?Christine 


     Duncan Mackay <dulcalma at bigpond.com> ? 2015?09?14? (??) 3:56 PM ???
   

 Hi 

As Bert has intimated there was a misspelling in panel.abline

To get points as well you need the appropriate arguments for a panel
function otherwise you would only get a line.
You only had a function for? a line in panel function so adding panel.xyplot
will give you points as well.


xyplot(DI~Date1|Station, data=Raw,
? ? ? ? groups = culr,
? ? ? ? par.settings = list(strip.background = list(col = "transparent"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? superpose.symbol = list(cex = rep(2, 2),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? col=c("grey","black"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? pch = rep(16,2))),
? ? ? ? type="p",
? ? ? ? xlab=list("Month",cex=1.5),
? ? ? ? ylab=list("Dispersion index",cex=1.5),
? ? ? ? index.cond=list(c(1,2,3,4)),
? ? ? ? auto.key = T,
? ? ? ? layout=c(4,1),
? ? ? ? panel = function(x, y, ...){
? ? ? ? 
? ? ? ? ? panel.xyplot(x,y, ...)
? ? ? ? ? panel.abline(h=1,lty=2,lwd=3)
? ? ? ? }

 )

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Christine
Lee via R-help
Sent: Monday, 14 September 2015 13:43
To: r-help at r-project.org
Subject: [R] adding a line across plots in xy plot with panel.abline

Dear all,

I want to draw a line at DI=1 across all four graphs in the xy plot, I have
used panel.abline, but I failed to do so, does any one has an idea of what
has went wrong?

structure(list(Date = structure(c(6L, 7L, 2L, 4L, 13L, 17L, 5L, 
12L, 4L, 11L, 14L, 9L, 7L, 2L, 4L, 13L, 10L, 17L, 5L, 12L, 8L, 
4L, 11L, 14L, 9L, 16L, 15L, 3L, 10L, 1L, 17L, 5L, 12L, 8L, 4L, 
11L, 14L, 12L, 8L, 4L, 11L, 14L), .Label = c("1/10", "1/11", 
"11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", "23/9", 
"26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class =
"factor"), 
? ? Year = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
? ? 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
? ? 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
? ? 2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"), 
? ? Station = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
? ? 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
? ? 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 
? ? 4L, 4L, 4L), .Label = c("E", "F", "H", "I"), class = "factor"), 
? ? DI = c(13.4, 7, 12.6, 12.6, 5.2, 2.2, 1, 1, 1.2, 0.8, 2.3, 
? ? 3.8, 5.4, 4.6, 5, 3.2, 3.1, 8.7, 2.1, 2.7, 4.9, 4, 2.2, 5.3, 
? ? 5.6, 4.8, 4, 8.6, 1.9, 2.9, 5.9, 2.6, 8.9, 4, 13.5, 15.3, 
? ? 12.8, 3.4, 4.7, 1.7, 0.9, 1.7), Date1 = structure(c(16697, 
? ? 16710, 16740, 16751, 16768, 16714, 16721, 16737, 16751, 16765, 
? ? 16769, 16698, 16710, 16740, 16751, 16768, 16701, 16714, 16721, 
? ? 16737, 16741, 16751, 16765, 16769, 16698, 16712, 16739, 16750, 
? ? 16701, 16709, 16714, 16721, 16737, 16741, 16751, 16765, 16769, 
? ? 16737, 16741, 16751, 16765, 16769), class = "Date")), .Names = c("Date",

"Year", "Station", "DI", "Date1"), row.names = c(NA, -42L), class =
"data.frame")
Raw$Date1<-as.Date(Raw$Date,"%d/%m")
culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014")
library(lattice)
xyplot(DI~Date1|Station, data=Raw,
? ? ? ? groups = culr,
? ? ? ? par.settings = list(strip.background = list(col = "transparent"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? superpose.symbol = list(cex = rep(2, 2),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? col=c("grey","black"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? pch = rep(16,2))),
? ? ? ? type="p",
? ? ? ? xlab=list("Month",cex=1.5),
? ? ? ? ylab=list("Dispersion index",cex=1.5),? 
? ? ? ? index.cond=list(c(1,2,3,4)),
? ? ? ? auto.key = T,
? ? ? ? layout=c(4,1),
? ? ? ? panel=function(Date1,DI){
? ? ? ? panel.labline(h=1,lty=2,lwd=3)
? ? ? ? })

Many thanks.

Regards,
Christine

 

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



  
	[[alternative HTML version deleted]]


From marongiu.luigi at gmail.com  Mon Sep 14 13:07:54 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Mon, 14 Sep 2015 12:07:54 +0100
Subject: [R] convert list int data frame
Message-ID: <CAMk+s2SnjReMFkks6wdw5_PSzGN1Vogg6QSfOfYNHeXe=GtevQ@mail.gmail.com>

Dear all,
i would like to convert a list onto a data frame object. Is there an
easy way of doing it? I know I should use unlist() but I am not sure
about the implementation for the addition of the row numbers; or I
could extract one member at the time with something like x <-
my.list[[i]] but then how to append the results to an object?
Thank you.
Best regards
Luigi

>>>
my.list <- replicate(5, matrix(1:(6*27), ncol = 6, byrow = TRUE), FALSE)


From jl.iccp at gmail.com  Mon Sep 14 13:10:25 2015
From: jl.iccp at gmail.com (Jue Lin-Ye)
Date: Mon, 14 Sep 2015 13:10:25 +0200
Subject: [R] Boolean expression
Message-ID: <CAAMkPW-TrQq+mS7eYLGU3+hwydcn=KDkGCqDp+zq1160eS9bNg@mail.gmail.com>

Dear fellow R users,

I would like to discuss something. First of all, I would like to make clear
that I think that the current R software is great.

I have noticed that, for example, in the boolean expression "if(a<-1)", it
is not clear if what is asked is "if 'a' is 1" or "if 'a' is lesser than
-1". Should we just enclose "-1" within parentheses or should we have a
more unequivocal syntax, here, like "a<$", "a<%"?

Best regards,

?Jue

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Mon Sep 14 13:19:02 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 14 Sep 2015 21:19:02 +1000
Subject: [R] Boolean expression
In-Reply-To: <CAAMkPW-TrQq+mS7eYLGU3+hwydcn=KDkGCqDp+zq1160eS9bNg@mail.gmail.com>
References: <CAAMkPW-TrQq+mS7eYLGU3+hwydcn=KDkGCqDp+zq1160eS9bNg@mail.gmail.com>
Message-ID: <CA+8X3fXfyf33d8BT4hUW43hP9x+CchgqXRAhinQK_nw6c7S9Nw@mail.gmail.com>

Hi Jue,
The character sequence "<-" is preferentially interpreted as "assign the
value on the right to the name on the left". You can use parentheses to
force the unary minus to be interpreted:

if(a<(-1))

or you can just break the sequence with a space:

if(a< -1)

One less character, but perhaps easier to misread.

Jim


On Mon, Sep 14, 2015 at 9:10 PM, Jue Lin-Ye <jl.iccp at gmail.com> wrote:

> Dear fellow R users,
>
> I would like to discuss something. First of all, I would like to make clear
> that I think that the current R software is great.
>
> I have noticed that, for example, in the boolean expression "if(a<-1)", it
> is not clear if what is asked is "if 'a' is 1" or "if 'a' is lesser than
> -1". Should we just enclose "-1" within parentheses or should we have a
> more unequivocal syntax, here, like "a<$", "a<%"?
>
> Best regards,
>
> ?Jue
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ggrothendieck at gmail.com  Mon Sep 14 13:56:17 2015
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 14 Sep 2015 07:56:17 -0400
Subject: [R] How to read CSV from web?
In-Reply-To: <1438166277331-4710513.post@n4.nabble.com>
References: <1438140099663-4710502.post@n4.nabble.com>
	<1438166277331-4710513.post@n4.nabble.com>
Message-ID: <CAP01uRmL_K=2PgzDQ3skoUzgHuPTSuXmgC3EY0d-gMbeTzZbGA@mail.gmail.com>

Your read.csv call works for me under Windows on "R version 3.2.2 Patched
(2015-08-25 r69180)"  but not on "R version 3.1.3 Patched (2015-03-16
r68169)".

Suggest you upgrade your R installation and try again.

If you are on Windowsw and don't want to upgrade right now an alternative
is to issue this command first: setInternet2()

Also note that header = TRUE and sep = "," are the defaults for read.csv so
those arguments can be optionally omitted.



On Wed, Jul 29, 2015 at 6:37 AM, jpara3 <j.para.fernandez at hotmail.com>
wrote:

> data<-read.csv("
> https://raw.githubusercontent.com/sjkiss/Survey/master/mlogit.out.csv
> ",header=T,sep=",")
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/How-to-read-CSV-from-web-tp4710502p4710513.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Mon Sep 14 14:50:30 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 14 Sep 2015 08:50:30 -0400
Subject: [R] convert list int data frame
In-Reply-To: <CAMk+s2SnjReMFkks6wdw5_PSzGN1Vogg6QSfOfYNHeXe=GtevQ@mail.gmail.com>
References: <CAMk+s2SnjReMFkks6wdw5_PSzGN1Vogg6QSfOfYNHeXe=GtevQ@mail.gmail.com>
Message-ID: <CAM_vjumjxWhgQmL-RXYwFZc2Cp9+U9HcW45x8zXP40CdNN_T0g@mail.gmail.com>

You don't say how you want the new data frame (which incidentally is
also a list) to be constructed, so here are two options:
> my.list <- replicate(5, matrix(1:(6*27), ncol = 6, byrow = TRUE), FALSE)
> dim(my.list[[1]])
[1] 27  6
> dim(do.call(rbind.data.frame, my.list))
[1] 135   6
> dim(do.call(cbind.data.frame, my.list))
[1] 27 30

Sarah

On Mon, Sep 14, 2015 at 7:07 AM, Luigi Marongiu
<marongiu.luigi at gmail.com> wrote:
> Dear all,
> i would like to convert a list onto a data frame object. Is there an
> easy way of doing it? I know I should use unlist() but I am not sure
> about the implementation for the addition of the row numbers; or I
> could extract one member at the time with something like x <-
> my.list[[i]] but then how to append the results to an object?
> Thank you.
> Best regards
> Luigi
>
>>>>
> my.list <- replicate(5, matrix(1:(6*27), ncol = 6, byrow = TRUE), FALSE)
>


-- 
Sarah Goslee
http://www.functionaldiversity.org


From ivan.calandra at univ-reims.fr  Mon Sep 14 14:58:10 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Mon, 14 Sep 2015 14:58:10 +0200
Subject: [R] convert list int data frame
In-Reply-To: <CAMk+s2SnjReMFkks6wdw5_PSzGN1Vogg6QSfOfYNHeXe=GtevQ@mail.gmail.com>
References: <CAMk+s2SnjReMFkks6wdw5_PSzGN1Vogg6QSfOfYNHeXe=GtevQ@mail.gmail.com>
Message-ID: <55F6C462.7000801@univ-reims.fr>

Hi Luigi,

It depends on how you want to put the different elements of my.list into 
a data.frame. Basically, data.frames are lists, so doing
data.frame(my.list)
is straigthforward.

HTH,
Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 14/09/15 13:07, Luigi Marongiu a ?crit :
> Dear all,
> i would like to convert a list onto a data frame object. Is there an
> easy way of doing it? I know I should use unlist() but I am not sure
> about the implementation for the addition of the row numbers; or I
> could extract one member at the time with something like x <-
> my.list[[i]] but then how to append the results to an object?
> Thank you.
> Best regards
> Luigi
>
> my.list <- replicate(5, matrix(1:(6*27), ncol = 6, byrow = TRUE), FALSE)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jl.iccp at gmail.com  Mon Sep 14 15:05:24 2015
From: jl.iccp at gmail.com (Jue Lin-Ye)
Date: Mon, 14 Sep 2015 15:05:24 +0200
Subject: [R] Boolean expression
In-Reply-To: <CA+8X3fXfyf33d8BT4hUW43hP9x+CchgqXRAhinQK_nw6c7S9Nw@mail.gmail.com>
References: <CAAMkPW-TrQq+mS7eYLGU3+hwydcn=KDkGCqDp+zq1160eS9bNg@mail.gmail.com>
	<CA+8X3fXfyf33d8BT4hUW43hP9x+CchgqXRAhinQK_nw6c7S9Nw@mail.gmail.com>
Message-ID: <CAAMkPW-kydgyHUWZFynUYAR4cZ2HdwLkR-YD00fPkyEcc8jodg@mail.gmail.com>

?Dear Dr. Lemon,

I am amazed, I did not know that a space could solve this issue. So you
mean that, besides being sensible to letter capitalization, R is also
sensible to spaces?

Many thanks!

Jue?

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Mon Sep 14 15:27:05 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Mon, 14 Sep 2015 14:27:05 +0100
Subject: [R] =?utf-8?b?5Zue6KaG77iwIGFkZGluZyBhIGxpbmUgYWNyb3NzIHBsb3Rz?=
 =?utf-8?q?_in_xy_plot_with_panel=2Eabline?=
In-Reply-To: <590582702.2206506.1442223002152.JavaMail.yahoo@mail.yahoo.com>
References: <000001d0eec2$d9f14c00$8dd3e400$@bigpond.com>
	<590582702.2206506.1442223002152.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55F6CB29.3060605@dewey.myzen.co.uk>

Dear Christine

The point of the ellipsis ... is to pass into the interior function any 
other parameters which were supplied to the exterior function. If you 
know for sure that there will never, never, never be such additional 
parameters then you can leave it out but that is a bit like not using a 
seat belt on the basis that you do not plan to have an accident.

On 14/09/2015 10:30, Christine Lee via R-help wrote:
> Thanks, Duncan, I am confused by the function(x,y, ...) and panel.xyplot(x,y,...).  Is this a must to put in "..."? I knew this may be a stupid question.  I am sorry. Christine
>
>
>       Duncan Mackay <dulcalma at bigpond.com> ? 2015?09?14? (??) 3:56 PM ???
>
>
>   Hi
>
> As Bert has intimated there was a misspelling in panel.abline
>
> To get points as well you need the appropriate arguments for a panel
> function otherwise you would only get a line.
> You only had a function for  a line in panel function so adding panel.xyplot
> will give you points as well.
>
>
> xyplot(DI~Date1|Station, data=Raw,
>          groups = culr,
>          par.settings = list(strip.background = list(col = "transparent"),
>                              superpose.symbol = list(cex = rep(2, 2),
>                                                      col=c("grey","black"),
>                                                      pch = rep(16,2))),
>          type="p",
>          xlab=list("Month",cex=1.5),
>          ylab=list("Dispersion index",cex=1.5),
>          index.cond=list(c(1,2,3,4)),
>          auto.key = T,
>          layout=c(4,1),
>          panel = function(x, y, ...){
>
>            panel.xyplot(x,y, ...)
>            panel.abline(h=1,lty=2,lwd=3)
>          }
>
>   )
>
> Regards
>
> Duncan
>
> Duncan Mackay
> Department of Agronomy and Soil Science
> University of New England
> Armidale NSW 2351
> Email: home: mackay at northnet.com.au
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Christine
> Lee via R-help
> Sent: Monday, 14 September 2015 13:43
> To: r-help at r-project.org
> Subject: [R] adding a line across plots in xy plot with panel.abline
>
> Dear all,
>
> I want to draw a line at DI=1 across all four graphs in the xy plot, I have
> used panel.abline, but I failed to do so, does any one has an idea of what
> has went wrong?
>
> structure(list(Date = structure(c(6L, 7L, 2L, 4L, 13L, 17L, 5L,
> 12L, 4L, 11L, 14L, 9L, 7L, 2L, 4L, 13L, 10L, 17L, 5L, 12L, 8L,
> 4L, 11L, 14L, 9L, 16L, 15L, 3L, 10L, 1L, 17L, 5L, 12L, 8L, 4L,
> 11L, 14L, 12L, 8L, 4L, 11L, 14L), .Label = c("1/10", "1/11",
> "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", "23/9",
> "26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class =
> "factor"),
>      Year = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
>      2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,
>      1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>      2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"),
>      Station = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>      1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>      3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L,
>      4L, 4L, 4L), .Label = c("E", "F", "H", "I"), class = "factor"),
>      DI = c(13.4, 7, 12.6, 12.6, 5.2, 2.2, 1, 1, 1.2, 0.8, 2.3,
>      3.8, 5.4, 4.6, 5, 3.2, 3.1, 8.7, 2.1, 2.7, 4.9, 4, 2.2, 5.3,
>      5.6, 4.8, 4, 8.6, 1.9, 2.9, 5.9, 2.6, 8.9, 4, 13.5, 15.3,
>      12.8, 3.4, 4.7, 1.7, 0.9, 1.7), Date1 = structure(c(16697,
>      16710, 16740, 16751, 16768, 16714, 16721, 16737, 16751, 16765,
>      16769, 16698, 16710, 16740, 16751, 16768, 16701, 16714, 16721,
>      16737, 16741, 16751, 16765, 16769, 16698, 16712, 16739, 16750,
>      16701, 16709, 16714, 16721, 16737, 16741, 16751, 16765, 16769,
>      16737, 16741, 16751, 16765, 16769), class = "Date")), .Names = c("Date",
>
> "Year", "Station", "DI", "Date1"), row.names = c(NA, -42L), class =
> "data.frame")
> Raw$Date1<-as.Date(Raw$Date,"%d/%m")
> culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014")
> library(lattice)
> xyplot(DI~Date1|Station, data=Raw,
>          groups = culr,
>          par.settings = list(strip.background = list(col = "transparent"),
>                              superpose.symbol = list(cex = rep(2, 2),
>                                                      col=c("grey","black"),
>                                                      pch = rep(16,2))),
>          type="p",
>          xlab=list("Month",cex=1.5),
>          ylab=list("Dispersion index",cex=1.5),
>          index.cond=list(c(1,2,3,4)),
>          auto.key = T,
>          layout=c(4,1),
>          panel=function(Date1,DI){
>          panel.labline(h=1,lty=2,lwd=3)
>          })
>
> Many thanks.
>
> Regards,
> Christine
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From petr.pikal at precheza.cz  Mon Sep 14 15:32:21 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 14 Sep 2015 13:32:21 +0000
Subject: [R] Boolean expression
In-Reply-To: <CAAMkPW-kydgyHUWZFynUYAR4cZ2HdwLkR-YD00fPkyEcc8jodg@mail.gmail.com>
References: <CAAMkPW-TrQq+mS7eYLGU3+hwydcn=KDkGCqDp+zq1160eS9bNg@mail.gmail.com>
	<CA+8X3fXfyf33d8BT4hUW43hP9x+CchgqXRAhinQK_nw6c7S9Nw@mail.gmail.com>
	<CAAMkPW-kydgyHUWZFynUYAR4cZ2HdwLkR-YD00fPkyEcc8jodg@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3F870@SRVEXCHMBX.precheza.cz>

Hi

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jue
> Lin-Ye
> Sent: Monday, September 14, 2015 3:05 PM
> To: Jim Lemon
> Cc: r-help mailing list
> Subject: Re: [R] Boolean expression
>
> ?Dear Dr. Lemon,
>
> I am amazed, I did not know that a space could solve this issue. So you
> mean that, besides being sensible to letter capitalization, R is also
> sensible to spaces?

Yes and no. It depends where you put spaces.

> head(iris[10 : 20, 2 : 3])
   Sepal.Width Petal.Length
10         3.1          1.5
11         3.7          1.5
12         3.4          1.6
13         3.0          1.4
14         3.0          1.1
15         4.0          1.2

> head(iris[1 0 : 2 0, 2 : 3])
Error: unexpected numeric constant in "head(iris[1 0"

With space you may separate syntactically correct expressions and nothing happens. However if space makes syntactically incorrect expression R complains. Mostly R is quite clever in evaluating what user really intends. However in some occasions it can be wrong.

Cheers
Petr

>
> Many thanks!
>
> Jue?
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ch_koch at gmx.de  Mon Sep 14 12:11:34 2015
From: ch_koch at gmx.de (apeshifter)
Date: Mon, 14 Sep 2015 03:11:34 -0700 (PDT)
Subject: [R] Trees (and Forests) with packages 'party' vs. 'partykit':
 Different results
Message-ID: <1442225494911-4712214.post@n4.nabble.com>

Dear all, 

I'm currently exploring a dataset with the help of conditional inference
trees (still very much a beginner with this technique & log. reg. methods as
a whole t.b.h.), since they explained more variation in my dataset than a
binary logistic regression with /glm/. I started out with the /party
/package, but after I while I ran into the 'updated' /partykit /package and
tried this out, too. Now, the strange thing is that both trees look quite
different - actually even the very first split is different. So I did some
research and came across the 'forest' concept. However, it seems that the
/varImp /function does not yet work in the /partykit /implementation, which
raises the question for me how I should evaluate the /partykit /forest - how
can I find out whether the variables are important in the forest as in my
/partykit /tree? Is there some way to do this or some other solution for
this problem? I'd prefer to continue the /partykit /implementation of ctree,
since it allows more settings for the final plot, which I'd need to get the
final (large) plot into a readable form.

Related to this project, I'd also like to give statistics for the overall
model, e.g. overall significance, Nagelkerke's R?, a C-value. After a
'regular' binary log. reg., I would use the lrm function to get these
values, but I am unsure whether it would be correct to also apply this
method to my tree data.

Any help would be greatly appreciated! 

-- Christopher



--
View this message in context: http://r.789695.n4.nabble.com/Trees-and-Forests-with-packages-party-vs-partykit-Different-results-tp4712214.html
Sent from the R help mailing list archive at Nabble.com.


From laura.fernandezp at edu.uah.es  Mon Sep 14 14:11:27 2015
From: laura.fernandezp at edu.uah.es (laurafdez56)
Date: Mon, 14 Sep 2015 05:11:27 -0700 (PDT)
Subject: [R] R lines type in a glm/predict model
Message-ID: <1442232687664-4712219.post@n4.nabble.com>

Hi everyone!
I want to change the line type into a square/circle/triangle in a plot, and
I can not find the way, could someone help me please?? I used the
glm/predict function to predict the curves. 

Here is my code:

glm1<-glm(Supervivencia~Temperatura*Especie,pinos,family=binomial)

range(pinos$Temperatura)
xv<-5:-30
sp1<-rep("P.halepensis:-11.6(-10.6,-12)",length(xv))
sp2<-rep("P.nigra:-23.6(-22.9,-23.9)",length(xv))
sp3<-rep("P.pinaster:-19.2(-18.7,-19.4)",length(xv))
sp4<-rep("P.pinea:-12.4(-11.8,-12.7)",length(xv))
sp5<-rep("P.sylvestris:-21.4(-20.9,-21.6)",length(xv))
sp6<-rep("P.uncinata:-21.3(-20.7,-21.6)",length(xv))

yv1<-predict(glm1,list(Temperatura=xv,Especie=sp1),type="response")
yv2<-predict(glm1,list(Temperatura=xv,Especie=sp2),type="response")
yv3<-predict(glm1,list(Temperatura=xv,Especie=sp3),type="response")
yv4<-predict(glm1,list(Temperatura=xv,Especie=sp4),type="response")
yv5<-predict(glm1,list(Temperatura=xv,Especie=sp5),type="response")
yv6<-predict(glm1,list(Temperatura=xv,Especie=sp6),type="response")


plot(Supervivencia~Temperatura,pinos, xlab=(expression(Temperature *
degree~C)), ylab="Survival",type="n")
lines(xv,yv1,lwd=2,lty=5, col="black")###halepensis
lines(xv,yv2,lwd=2,lty=6, col="black")###nigra
lines(xv,yv3,lwd=2,lty=1, col="black")###pinaster
lines(xv,yv4,lwd=2,lty=4, col="black")###pinea
lines(xv,yv5,lwd=2,lty=3, col="black")###sylvestris
lines(xv,yv6,lwd=2,lty=2, col="black")###uncinta
abline(0.5,0, col="black", lty=7)
legend(legend=levels(pinos$Especie),"topleft",bty="n",lty=c(lty=5,lty=6,lty=1,lty=4,lty=3,lty=2))





--
View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219.html
Sent from the R help mailing list archive at Nabble.com.


From bgunter.4567 at gmail.com  Mon Sep 14 16:16:11 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 14 Sep 2015 07:16:11 -0700
Subject: [R] =?utf-8?b?5Zue6KaG77iwIGFkZGluZyBhIGxpbmUgYWNyb3NzIHBsb3Rz?=
	=?utf-8?q?_in_xy_plot_with_panel=2Eabline?=
In-Reply-To: <55F6CB29.3060605@dewey.myzen.co.uk>
References: <000001d0eec2$d9f14c00$8dd3e400$@bigpond.com>
	<590582702.2206506.1442223002152.JavaMail.yahoo@mail.yahoo.com>
	<55F6CB29.3060605@dewey.myzen.co.uk>
Message-ID: <CAGxFJbSr7V+F08eUVwq+79OX6owU+vWE=4HcZxLWLs8PJADuOA@mail.gmail.com>

Christine:

You need to spend some time with an R tutorial ("An Intro to R" ships
with R, but there are many on the web) where function arguments are
explained more fully. That should go a long way to clearing up your
"stupidity."

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Sep 14, 2015 at 6:27 AM, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> Dear Christine
>
> The point of the ellipsis ... is to pass into the interior function any
> other parameters which were supplied to the exterior function. If you know
> for sure that there will never, never, never be such additional parameters
> then you can leave it out but that is a bit like not using a seat belt on
> the basis that you do not plan to have an accident.
>
>
> On 14/09/2015 10:30, Christine Lee via R-help wrote:
>>
>> Thanks, Duncan, I am confused by the function(x,y, ...) and
>> panel.xyplot(x,y,...).  Is this a must to put in "..."? I knew this may be a
>> stupid question.  I am sorry. Christine
>>
>>
>>       Duncan Mackay <dulcalma at bigpond.com> ? 2015?09?14? (??) 3:56 PM ???
>>
>>
>>   Hi
>>
>> As Bert has intimated there was a misspelling in panel.abline
>>
>> To get points as well you need the appropriate arguments for a panel
>> function otherwise you would only get a line.
>> You only had a function for  a line in panel function so adding
>> panel.xyplot
>> will give you points as well.
>>
>>
>> xyplot(DI~Date1|Station, data=Raw,
>>          groups = culr,
>>          par.settings = list(strip.background = list(col = "transparent"),
>>                              superpose.symbol = list(cex = rep(2, 2),
>>
>> col=c("grey","black"),
>>                                                      pch = rep(16,2))),
>>          type="p",
>>          xlab=list("Month",cex=1.5),
>>          ylab=list("Dispersion index",cex=1.5),
>>          index.cond=list(c(1,2,3,4)),
>>          auto.key = T,
>>          layout=c(4,1),
>>          panel = function(x, y, ...){
>>
>>            panel.xyplot(x,y, ...)
>>            panel.abline(h=1,lty=2,lwd=3)
>>          }
>>
>>   )
>>
>> Regards
>>
>> Duncan
>>
>> Duncan Mackay
>> Department of Agronomy and Soil Science
>> University of New England
>> Armidale NSW 2351
>> Email: home: mackay at northnet.com.au
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Christine
>> Lee via R-help
>> Sent: Monday, 14 September 2015 13:43
>> To: r-help at r-project.org
>> Subject: [R] adding a line across plots in xy plot with panel.abline
>>
>> Dear all,
>>
>> I want to draw a line at DI=1 across all four graphs in the xy plot, I
>> have
>> used panel.abline, but I failed to do so, does any one has an idea of what
>> has went wrong?
>>
>> structure(list(Date = structure(c(6L, 7L, 2L, 4L, 13L, 17L, 5L,
>> 12L, 4L, 11L, 14L, 9L, 7L, 2L, 4L, 13L, 10L, 17L, 5L, 12L, 8L,
>> 4L, 11L, 14L, 9L, 16L, 15L, 3L, 10L, 1L, 17L, 5L, 12L, 8L, 4L,
>> 11L, 14L, 12L, 8L, 4L, 11L, 14L), .Label = c("1/10", "1/11",
>> "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", "23/9",
>> "26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class =
>> "factor"),
>>      Year = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
>>      2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,
>>      1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>      2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"),
>>      Station = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>>      1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>      3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L,
>>      4L, 4L, 4L), .Label = c("E", "F", "H", "I"), class = "factor"),
>>      DI = c(13.4, 7, 12.6, 12.6, 5.2, 2.2, 1, 1, 1.2, 0.8, 2.3,
>>      3.8, 5.4, 4.6, 5, 3.2, 3.1, 8.7, 2.1, 2.7, 4.9, 4, 2.2, 5.3,
>>      5.6, 4.8, 4, 8.6, 1.9, 2.9, 5.9, 2.6, 8.9, 4, 13.5, 15.3,
>>      12.8, 3.4, 4.7, 1.7, 0.9, 1.7), Date1 = structure(c(16697,
>>      16710, 16740, 16751, 16768, 16714, 16721, 16737, 16751, 16765,
>>      16769, 16698, 16710, 16740, 16751, 16768, 16701, 16714, 16721,
>>      16737, 16741, 16751, 16765, 16769, 16698, 16712, 16739, 16750,
>>      16701, 16709, 16714, 16721, 16737, 16741, 16751, 16765, 16769,
>>      16737, 16741, 16751, 16765, 16769), class = "Date")), .Names =
>> c("Date",
>>
>> "Year", "Station", "DI", "Date1"), row.names = c(NA, -42L), class =
>> "data.frame")
>> Raw$Date1<-as.Date(Raw$Date,"%d/%m")
>> culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014")
>> library(lattice)
>> xyplot(DI~Date1|Station, data=Raw,
>>          groups = culr,
>>          par.settings = list(strip.background = list(col = "transparent"),
>>                              superpose.symbol = list(cex = rep(2, 2),
>>
>> col=c("grey","black"),
>>                                                      pch = rep(16,2))),
>>          type="p",
>>          xlab=list("Month",cex=1.5),
>>          ylab=list("Dispersion index",cex=1.5),
>>          index.cond=list(c(1,2,3,4)),
>>          auto.key = T,
>>          layout=c(4,1),
>>          panel=function(Date1,DI){
>>          panel.labline(h=1,lty=2,lwd=3)
>>          })
>>
>> Many thanks.
>>
>> Regards,
>> Christine
>>
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Achim.Zeileis at uibk.ac.at  Mon Sep 14 16:52:24 2015
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Mon, 14 Sep 2015 16:52:24 +0200 (CEST)
Subject: [R] Trees (and Forests) with packages 'party' vs. 'partykit':
 Different results
In-Reply-To: <1442225494911-4712214.post@n4.nabble.com>
References: <1442225494911-4712214.post@n4.nabble.com>
Message-ID: <alpine.DEB.2.11.1509141636120.21785@paninaro.uibk.ac.at>

Christopher,

thanks for you interest.

> I'm currently exploring a dataset with the help of conditional inference 
> trees (still very much a beginner with this technique & log. reg. 
> methods as a whole t.b.h.), since they explained more variation in my 
> dataset than a binary logistic regression with /glm/. I started out with 
> the /party /package, but after I while I ran into the 'updated' 
> /partykit /package and tried this out, too.

If you want to use individual trees (as opposed to forests), then the 
"partykit" package is recommended because it contains much improved 
re-implementations of ctree() and mob() as well as the mob() convenience 
interfaces lmtree() and glmtree(). For forests see below.

> Now, the strange thing is that both trees look quite different - 
> actually even the very first split is different.

This might be due to several partitioning variables being associated with 
tiny p-values in the root node. The re-implementation in partykit 
internally computes with log-p-values and hence should be numerically more 
stable. In the old implementation it could happen that from several highly 
significant variables, always the first is chosen because the p-values 
were essentially indistinguishable for the computer.

If you think that this is not the problem, then please contact the package 
maintainer with a reproducible example.

Except for bug fixes like the one above, the trees grown by 
partykit::ctree and party::ctree should be the same.

> So I did some research and came across the 'forest' concept. However, it 
> seems that the /varImp /function does not yet work in the /partykit 
> /implementation,

Correct. While the ctree() implementation in partykit is better than that 
in party, the same is _not_ true for cforest(). The new partykit::cforest 
is currently still a basic implementation which doesn't offer as many 
features as the party::cforest implementation. More work is needed 
especially for variable importance measures and different kinds of 
predictions.

> which raises the question for me how I should evaluate the /partykit 
> /forest - how can I find out whether the variables are important in the 
> forest as in my /partykit /tree? Is there some way to do this or some 
> other solution for this problem? I'd prefer to continue the /partykit 
> /implementation of ctree, since it allows more settings for the final 
> plot, which I'd need to get the final (large) plot into a readable form.
>
> Related to this project, I'd also like to give statistics for the overall
> model, e.g. overall significance, Nagelkerke's R?, a C-value. After a
> 'regular' binary log. reg., I would use the lrm function to get these
> values, but I am unsure whether it would be correct to also apply this
> method to my tree data.

Overall significance is difficult because you have done model selection 
when growing the tree. As for pseudo R-squared or information criteria 
etc., it is relatively easy to compute these "by hand" based on the 
observed and fitted responses. An example for this is provided at:
http://stackoverflow.com/questions/29524670/how-to-find-the-the-deviance-of-an-as-party-object-converted-from-rpart-tree-in/29693223#29693223

> Any help would be greatly appreciated! 
>
> -- Christopher
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Trees-and-Forests-with-packages-party-vs-partykit-Different-results-tp4712214.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From sarah.goslee at gmail.com  Mon Sep 14 16:53:37 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 14 Sep 2015 10:53:37 -0400
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <1442232687664-4712219.post@n4.nabble.com>
References: <1442232687664-4712219.post@n4.nabble.com>
Message-ID: <CAM_vjukjdjh=XmRFh9ph4aJuQ6FG6pyT-B0TNfjGrc=hCZmB5g@mail.gmail.com>

It's not clear to me quite what you want, but you might look at the
type="b"
argument to lines, or potentially
?points

Sarah

On Mon, Sep 14, 2015 at 8:11 AM, laurafdez56
<laura.fernandezp at edu.uah.es> wrote:
> Hi everyone!
> I want to change the line type into a square/circle/triangle in a plot, and
> I can not find the way, could someone help me please?? I used the
> glm/predict function to predict the curves.
>
> Here is my code:
>
> glm1<-glm(Supervivencia~Temperatura*Especie,pinos,family=binomial)
>
> range(pinos$Temperatura)
> xv<-5:-30
> sp1<-rep("P.halepensis:-11.6(-10.6,-12)",length(xv))
> sp2<-rep("P.nigra:-23.6(-22.9,-23.9)",length(xv))
> sp3<-rep("P.pinaster:-19.2(-18.7,-19.4)",length(xv))
> sp4<-rep("P.pinea:-12.4(-11.8,-12.7)",length(xv))
> sp5<-rep("P.sylvestris:-21.4(-20.9,-21.6)",length(xv))
> sp6<-rep("P.uncinata:-21.3(-20.7,-21.6)",length(xv))
>
> yv1<-predict(glm1,list(Temperatura=xv,Especie=sp1),type="response")
> yv2<-predict(glm1,list(Temperatura=xv,Especie=sp2),type="response")
> yv3<-predict(glm1,list(Temperatura=xv,Especie=sp3),type="response")
> yv4<-predict(glm1,list(Temperatura=xv,Especie=sp4),type="response")
> yv5<-predict(glm1,list(Temperatura=xv,Especie=sp5),type="response")
> yv6<-predict(glm1,list(Temperatura=xv,Especie=sp6),type="response")
>
>
> plot(Supervivencia~Temperatura,pinos, xlab=(expression(Temperature *
> degree~C)), ylab="Survival",type="n")
> lines(xv,yv1,lwd=2,lty=5, col="black")###halepensis
> lines(xv,yv2,lwd=2,lty=6, col="black")###nigra
> lines(xv,yv3,lwd=2,lty=1, col="black")###pinaster
> lines(xv,yv4,lwd=2,lty=4, col="black")###pinea
> lines(xv,yv5,lwd=2,lty=3, col="black")###sylvestris
> lines(xv,yv6,lwd=2,lty=2, col="black")###uncinta
> abline(0.5,0, col="black", lty=7)
> legend(legend=levels(pinos$Especie),"topleft",bty="n",lty=c(lty=5,lty=6,lty=1,lty=4,lty=3,lty=2))
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219.html
> Sent from the R help mailing list archive at Nabble.com.
>
-- 
Sarah Goslee
http://www.functionaldiversity.org


From loris.bennett at fu-berlin.de  Mon Sep 14 17:05:25 2015
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Mon, 14 Sep 2015 17:05:25 +0200
Subject: [R] igraph tree: increase vertex separation within tier
	(self-contained example)
In-Reply-To: <87twre78k8.fsf@hornfels.zedat.fu-berlin.de> (Loris Bennett's
	message of "Tue, 1 Sep 2015 12:11:51 +0200")
References: <87twre78k8.fsf@hornfels.zedat.fu-berlin.de>
Message-ID: <874mixqbwq.fsf@hornfels.zedat.fu-berlin.de>

Loris Bennett <loris.bennett at fu-berlin.de> writes:

> Hi,
>
> I am collecting data about network errors and would like to visualise
> the results in some sort of graph which reflects the hierarchy of the
> components in the network (i.e. core switches are connected to leaf
> switches and nodes are connected to leaf switches).
>
> The errors are in file which looks something like this:
>
> --------
> 2015-07-15;coreswitch01;1;0
> 2015-07-15;coreswitch01;2;0
> 2015-07-15;leafswitch01;1;0
> 2015-07-15;leafswitch01;2;0
> 2015-07-15;leafswitch01;3;0
> 2015-07-15;leafswitch01;4;0
> 2015-07-15;leafswitch01;5;0
> 2015-07-15;leafswitch02;1;0
> 2015-07-15;leafswitch02;2;0
> 2015-07-15;leafswitch02;3;0
> 2015-07-15;leafswitch02;4;0
> 2015-07-15;leafswitch02;5;0
> 2015-07-15;leafswitch02;2;561
> 2015-07-15;node001;1;0
> 2015-07-15;node002;1;0
> 2015-07-15;node003;1;0
> 2015-07-15;node004;1;3
> 2015-07-15;node005;1;10529
> 2015-07-15;node007;1;0
> 2015-07-15;node008;1;2081
> --------
>
> and the topology file, which relates ports to ports and ports to
> multiport components, looks something like this:
>
> --------
> coreswitch01;coreswitch01_01
> coreswitch01;coreswitch01_02
> leafswitch01;leafswitch01_01
> leafswitch01;leafswitch01_02
> leafswitch01;leafswitch01_03
> leafswitch01;leafswitch01_04
> leafswitch01;leafswitch01_05
> leafswitch02;leafswitch02_01
> leafswitch02;leafswitch02_02
> leafswitch02;leafswitch02_03
> leafswitch02;leafswitch02_04
> leafswitch02;leafswitch02_05
> coreswitch01_01;leafswitch01_01
> coreswitch01_02;leafswitch02_01
> leafswitch01_02;node001_01
> leafswitch01_03;node002_01
> leafswitch01_04;node003_01
> leafswitch01_05;node004_01
> leafswitch02_02;node005_01
> leafswitch02_03;node006_01
> leafswitch02_04;node007_01
> leafswitch02_05;node008_01
> --------
>
>
> I'm plotting the data with the following:
>
> --------
> library("igraph")
>
> error_data <- read.csv(file="errors.csv",head=FALSE,sep=";")
> colnames(error_data) <- c("datetime","name","portnumber","generic_error");
>
> topo_data <- read.csv(file="topology.csv",head=FALSE,sep=";")
> G <-graph.data.frame(topo_data, directed=F)
>
> error_counter <- 'generic_error'
> error_counter_max <- max(error_data[,error_counter])
>
> vcolours <- 100 - round(99*error_data[,error_counter]/error_counter_max)
> hc100 <- heat.colors(100)
> vcolour_values <- hc100[vcolours]
>
> l <- layout_(G,as_tree(root=c(1),rootlevel=c(1)))
>
> plot(G, layout = l, vertex.shape = "rectangle", vertex.color=vcolour_values)
> --------
>
> This produces roughly what I want.  However, even with this subset of
> the full network, there is quite a lot of overlapping of vertex labels
> within the lowest tier.  The full data set has over 100 vertices on the
> lowest tier, which causes the labels to become illegible.  I can adjust
> the aspect ratio of the plot and/or the canvas, but this doesn't affect
> the separation between the vertices.
>
> Does anyone have any advice about how to display such information?
> (Completely different approaches also welcome.)
>
> Cheers,
>
> Loris

Here's a self-contained version of the above:

========

library("igraph")

error_data <- structure(list(datetime = structure(c(1L, 1L, 1L, 1L, 1L, 1L,1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = "2015-07-15", class = "factor"),name = structure(c(1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,3L, 3L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L), .Label = c("coreswitch01","leafswitch01", "leafswitch02", "node001", "node002", "node003","node004", "node005", "node007", "node008"), class = "factor"),portnumber = c(1L, 2L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L,5L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), generic_error = c(0L,0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 561L, 0L, 0L,0L, 3L, 10529L, 0L, 2081L)), .Names = c("datetime", "name","portnumber", "generic_error"), class = "data.frame", row.names = c(NA,-20L))

colnames(error_data) <- c("datetime","name","portnumber","generic_error");

#topo_data <- read.csv(file="topology.csv",head=FALSE,sep=";")
topo_data <- structure(list(V1 = structure(c(1L, 1L, 4L, 4L, 4L, 4L, 4L, 9L, 9L, 9L, 9L, 9L, 2L, 3L, 5L, 6L, 7L, 8L, 10L, 11L, 12L, 13L), .Label = c("coreswitch01", "coreswitch01_01", "coreswitch01_02", "leafswitch01", "leafswitch01_02", "leafswitch01_03", "leafswitch01_04", "leafswitch01_05", "leafswitch02", "leafswitch02_02", "leafswitch02_03", "leafswitch02_04", "leafswitch02_05"), class = "factor"), V2 = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 3L, 8L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L), .Label = c("coreswitch01_01", "coreswitch01_02", "leafswitch01_01", "leafswitch01_02", "leafswitch01_03", "leafswitch01_04", "leafswitch01_05", "leafswitch02_01", "leafswitch02_02", "leafswitch02_03", "leafswitch02_04", "leafswitch02_05", "node001_01", "node002_01", "node003_01", "node004_01", "node005_01", "node006_01", "node007_01", "node008_01"), class = "factor")), .Names = c("V1", "V2"), class = "data.frame", row.names = c(NA, -22L))

G <-graph.data.frame(topo_data, directed=F)

error_counter <- 'generic_error'
error_counter_max <- max(error_data[,error_counter])

vcolours <- 100 - round(99*error_data[,error_counter]/error_counter_max)
hc100 <- heat.colors(100)
vcolour_values <- hc100[vcolours]

l <- layout_(G,as_tree(root=c(1),rootlevel=c(1)))

plot(G, layout = l, vertex.shape = "rectangle", vertex.color=vcolour_values)

========

The question is how the aspect ratio can be changed to prevent the node
labels overlapping.  An ideal solution would accommodate around 100
nodes on the lowest tier of the diagram.

Cheers,

Loris

-- 
This signature is currently under construction.


From lists at dewey.myzen.co.uk  Mon Sep 14 17:34:57 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Mon, 14 Sep 2015 16:34:57 +0100
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <1442232687664-4712219.post@n4.nabble.com>
References: <1442232687664-4712219.post@n4.nabble.com>
Message-ID: <55F6E921.2060700@dewey.myzen.co.uk>

Dear Laura

The line type is usually something like: solid, dotted, dashed, ...
The shapes you suggest are usually point symbols.

When you repost perhaps it would be a good idea to
1 - give us a small dataset
2 - just supply code for two species
3 - tell us what you see, and what you really wanted to see

On 14/09/2015 13:11, laurafdez56 wrote:
> Hi everyone!
> I want to change the line type into a square/circle/triangle in a plot, and
> I can not find the way, could someone help me please?? I used the
> glm/predict function to predict the curves.
>
> Here is my code:
>
> glm1<-glm(Supervivencia~Temperatura*Especie,pinos,family=binomial)
>
> range(pinos$Temperatura)
> xv<-5:-30
> sp1<-rep("P.halepensis:-11.6(-10.6,-12)",length(xv))
> sp2<-rep("P.nigra:-23.6(-22.9,-23.9)",length(xv))
> sp3<-rep("P.pinaster:-19.2(-18.7,-19.4)",length(xv))
> sp4<-rep("P.pinea:-12.4(-11.8,-12.7)",length(xv))
> sp5<-rep("P.sylvestris:-21.4(-20.9,-21.6)",length(xv))
> sp6<-rep("P.uncinata:-21.3(-20.7,-21.6)",length(xv))
>
> yv1<-predict(glm1,list(Temperatura=xv,Especie=sp1),type="response")
> yv2<-predict(glm1,list(Temperatura=xv,Especie=sp2),type="response")
> yv3<-predict(glm1,list(Temperatura=xv,Especie=sp3),type="response")
> yv4<-predict(glm1,list(Temperatura=xv,Especie=sp4),type="response")
> yv5<-predict(glm1,list(Temperatura=xv,Especie=sp5),type="response")
> yv6<-predict(glm1,list(Temperatura=xv,Especie=sp6),type="response")
>
>
> plot(Supervivencia~Temperatura,pinos, xlab=(expression(Temperature *
> degree~C)), ylab="Survival",type="n")
> lines(xv,yv1,lwd=2,lty=5, col="black")###halepensis
> lines(xv,yv2,lwd=2,lty=6, col="black")###nigra
> lines(xv,yv3,lwd=2,lty=1, col="black")###pinaster
> lines(xv,yv4,lwd=2,lty=4, col="black")###pinea
> lines(xv,yv5,lwd=2,lty=3, col="black")###sylvestris
> lines(xv,yv6,lwd=2,lty=2, col="black")###uncinta
> abline(0.5,0, col="black", lty=7)
> legend(legend=levels(pinos$Especie),"topleft",bty="n",lty=c(lty=5,lty=6,lty=1,lty=4,lty=3,lty=2))
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dulcalma at bigpond.com  Mon Sep 14 17:48:38 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Tue, 15 Sep 2015 01:48:38 +1000
Subject: [R] =?utf-8?b?5Zue6KaG77iwICBhZGRpbmcgYSBsaW5lIGFjcm9zcyBwbG90?=
	=?utf-8?q?s_in_xy_plot_with_panel=2Eabline?=
References: <000001d0eec2$d9f14c00$8dd3e400$@bigpond.com>
	<590582702.2206506.1442223002152.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <000901d0ef04$d327e860$7977b920$@bigpond.com>

forgot to send to list

 

From: Duncan Mackay [mailto:dulcalma at bigpond.com] 
Sent: Tuesday, 15 September 2015 01:47
To: 'Christine Lee'
Subject: RE: ??? [R] adding a line across plots in xy plot with panel.abline

 

Hi

 

If your panel function was like this (just adding to plot the points with the argument panel.xyplot  

 

        panel= function(Date1,DI){

                 panel.xyplot(Date1,DI)

                 panel.abline(h=1,lty=2,lwd=3)

        }

 

and this would cause an error see ? panel.xyplot

you can test it by replacing the lines  and see what happens

 

Now with the panel function like this 

        panel = function(x, y, ...){

        

          panel.xyplot(x,y, ...)

          panel.abline(h=1,lty=2,lwd=3)

        }

 

it will ?carry? the parameters in par.settings to the functions within the panel function.

One advantage of this is that you do not have to finish panel.xplot with what you want eg

 

panel.xyplot(x,y,  pch = 16, col = c(1,2), cex = 2)

 

In some contexts you can get away with just

   panel = function(...){

        

but in this case there is an error message in each panel saying y cannot be found

 

I cannot think of a good example at the moment run

 

demo(lattice::intervals )

and look at the code output

 

see also ? panel.superpose

 

 

Regards

 

Duncan

 

 

 

From: Christine Lee [mailto:leptostracan at yahoo.com] 
Sent: Monday, 14 September 2015 19:30
To: Duncan Mackay; R
Subject: ??? [R] adding a line across plots in xy plot with panel.abline

 

Thanks, Duncan,

 

I am confused by the function(x,y, ...) and panel.xyplot(x,y,...).  Is this a must to put in "..."? I knew this may be a stupid question.  I am sorry.

 

Christine

 

 

Duncan Mackay <dulcalma at bigpond.com> ? 2015?09?14? (??) 3:56 PM ???

 

Hi 

As Bert has intimated there was a misspelling in panel.abline

To get points as well you need the appropriate arguments for a panel
function otherwise you would only get a line.
You only had a function for  a line in panel function so adding panel.xyplot
will give you points as well.


xyplot(DI~Date1|Station, data=Raw,
        groups = culr,
        par.settings = list(strip.background = list(col = "transparent"),
                            superpose.symbol = list(cex = rep(2, 2),
                                                    col=c("grey","black"),
                                                    pch = rep(16,2))),
        type="p",
        xlab=list("Month",cex=1.5),
        ylab=list("Dispersion index",cex=1.5),
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1),
        panel = function(x, y, ...){
        
          panel.xyplot(x,y, ...)
          panel.abline(h=1,lty=2,lwd=3)
        }

)

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Christine
Lee via R-help
Sent: Monday, 14 September 2015 13:43
To: r-help at r-project.org
Subject: [R] adding a line across plots in xy plot with panel.abline

Dear all,

I want to draw a line at DI=1 across all four graphs in the xy plot, I have
used panel.abline, but I failed to do so, does any one has an idea of what
has went wrong?

structure(list(Date = structure(c(6L, 7L, 2L, 4L, 13L, 17L, 5L, 
12L, 4L, 11L, 14L, 9L, 7L, 2L, 4L, 13L, 10L, 17L, 5L, 12L, 8L, 
4L, 11L, 14L, 9L, 16L, 15L, 3L, 10L, 1L, 17L, 5L, 12L, 8L, 4L, 
11L, 14L, 12L, 8L, 4L, 11L, 14L), .Label = c("1/10", "1/11", 
"11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", "23/9", 
"26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class =
"factor"), 
    Year = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
    2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
    1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"), 
    Station = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 
    4L, 4L, 4L), .Label = c("E", "F", "H", "I"), class = "factor"), 
    DI = c(13.4, 7, 12.6, 12.6, 5.2, 2.2, 1, 1, 1.2, 0.8, 2.3, 
    3.8, 5.4, 4.6, 5, 3.2, 3.1, 8.7, 2.1, 2.7, 4.9, 4, 2.2, 5.3, 
    5.6, 4.8, 4, 8.6, 1.9, 2.9, 5.9, 2.6, 8.9, 4, 13.5, 15.3, 
    12.8, 3.4, 4.7, 1.7, 0.9, 1.7), Date1 = structure(c(16697, 
    16710, 16740, 16751, 16768, 16714, 16721, 16737, 16751, 16765, 
    16769, 16698, 16710, 16740, 16751, 16768, 16701, 16714, 16721, 
    16737, 16741, 16751, 16765, 16769, 16698, 16712, 16739, 16750, 
    16701, 16709, 16714, 16721, 16737, 16741, 16751, 16765, 16769, 
    16737, 16741, 16751, 16765, 16769), class = "Date")), .Names = c("Date",

"Year", "Station", "DI", "Date1"), row.names = c(NA, -42L), class =
"data.frame")
Raw$Date1<-as.Date(Raw$Date,"%d/%m")
culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014")
library(lattice)
xyplot(DI~Date1|Station, data=Raw,
        groups = culr,
        par.settings = list(strip.background = list(col = "transparent"),
                            superpose.symbol = list(cex = rep(2, 2),
                                                    col=c("grey","black"),
                                                    pch = rep(16,2))),
        type="p",
        xlab=list("Month",cex=1.5),
        ylab=list("Dispersion index",cex=1.5),  
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1),
        panel=function(Date1,DI){
        panel.labline(h=1,lty=2,lwd=3)
        })

Many thanks.

Regards,
Christine



______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




	[[alternative HTML version deleted]]


From karraspito at yahoo.es  Mon Sep 14 18:29:15 2015
From: karraspito at yahoo.es (Iker Vaquero Alba)
Date: Mon, 14 Sep 2015 16:29:15 +0000 (UTC)
Subject: [R] Multinomial data
Message-ID: <1602844348.3241404.1442248155655.JavaMail.yahoo@mail.yahoo.com>


?? Hello everyone,
?? I have some data I would like to analyse where my response variables are categorical data: several participants were asked to give answers in the form of 1 to 5 (for the degree of importance given to certain items). So, what I have now is response variables that can just take 5 possible values: 1, 2, 3, 4 or 5. 
?? After a bit of research, I have learned this might be a case of multinomial categorical data, although I am not sure whether ordered multinomial or nominal multinomial. I would like to apply generalized linear mixed models to my response variables, but I am a bit lost at the moment. Should I just use lmer and change the link function? Is it better to try MCMCglmm package and start from there?

?? Any slight guidance or light shed on this would be greatly appreciated. 

?? Thanks a lot!
?__________________________________________________________________

?? Dr. Iker Vaquero-Alba
?? Visiting Postdoctoral Research Associate
?? Laboratory of Evolutionary Ecology of Adaptations 
?? School of Life Sciences
?? Joseph Banks Laboratories
?? University of Lincoln?? Brayford Campus, Lincoln
?? LN6 7DL
?? United Kingdom

?? https://eric.exeter.ac.uk/repository/handle/10036/3381


	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Mon Sep 14 18:49:28 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Mon, 14 Sep 2015 09:49:28 -0700 (PDT)
Subject: [R] Multinomial data
In-Reply-To: <1602844348.3241404.1442248155655.JavaMail.yahoo@mail.yahoo.com>
References: <1602844348.3241404.1442248155655.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <alpine.LNX.2.11.1509140945490.32105@localhost>

On Mon, 14 Sep 2015, Iker Vaquero Alba wrote:

> ?? I have some data I would like to analyse where my response variables
> are categorical data: several participants were asked to give answers in
> the form of 1 to 5 (for the degree of importance given to certain items).
> So, what I have now is response variables that can just take 5 possible
> values: 1, 2, 3, 4 or 5.

Iker,

   This situation calls for the application of Thomas Saaty's Analytical
Hierarchy Process (AHP). A good summary is on wikipedia:
<https://en.wikipedia.org/wiki/Analytic_hierarchy_process>, and web searches
will augment that information.

   It's easy to implement with octave as well as with python code. Perhaps
there is a package within R but I've not found one.

Rich


From 2hanl2da at naver.com  Mon Sep 14 16:45:18 2015
From: 2hanl2da at naver.com (massmatics)
Date: Mon, 14 Sep 2015 07:45:18 -0700 (PDT)
Subject: [R] I have trouble loading histogram :(
Message-ID: <1442241918762-4712231.post@n4.nabble.com>

Make a histogram and boxplot of the data sets: *south.csv*.

south.csv (the following is how it is displayed in the excel file)
x -> Acolumn, 1st row
12
10
10
13
12
12
14
7
16
18
8
29
12
14
33
10
6
18
11
25
8
16
14
11
10
20
14
11
12
13 ->  Acolumn, 31st row

This is the code I used to run but failed :(
My solution:
*south <- read.csv("C:/Users/Win/Desktop/south.csv")
attach(south)
hist(south)* --> When I ran this, it didn't work. Isn't this supposed to be
correct? :(

Thanks!




--
View this message in context: http://r.789695.n4.nabble.com/I-have-trouble-loading-histogram-tp4712231.html
Sent from the R help mailing list archive at Nabble.com.


From jtkulas at stcloudstate.edu  Mon Sep 14 17:44:33 2015
From: jtkulas at stcloudstate.edu (Kulas, John T.)
Date: Mon, 14 Sep 2015 15:44:33 +0000
Subject: [R] SSD vs RAM upgrade
Message-ID: <6934DE30FA68C04E9227F3AFAB89D12F0148A99F53@SCSU83A.campus.stcloudstate.edu>

I have several lab computers that frequently lag/crash with R apparently due to low RAM (they're all 8 GB)

I put in a request to up the RAM to 32 GB on a few, but my tech support is suggesting an SSD harddrive upgrade instead of the increased RAM.

Any suggestions on the better approach (SSD harddrive vs increased RAM to help R chug along)?

Thanks much - John

	[[alternative HTML version deleted]]


From kittlein at mdp.edu.ar  Mon Sep 14 15:46:29 2015
From: kittlein at mdp.edu.ar (Marcelo Kittlein)
Date: Mon, 14 Sep 2015 13:46:29 +0000
Subject: [R] Error in principal component loadings calculation
Message-ID: <55F6CFB5.7080107@mdp.edu.ar>

Hi all

I have been using "princomp" to obtain the principal components of some 
data and find that the loadings returned by the function appear to have 
some error.

in a simple example if a calculate de pc for a random matrix I get that 
all loadings for the different components have the same proportion of 
variance

data <- matrix(runif(100), 20, 5)
pc <- princomp(data, cor=TRUE)
loadings(pc)

Loadings:
      Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
[1,] -0.280  0.510  0.674 -0.217 -0.400
[2,]  0.529 -0.353        -0.694 -0.330
[3,] -0.111  0.563 -0.713 -0.336 -0.222
[4,] -0.530 -0.502 -0.178  0.140 -0.645
[5,] -0.590 -0.215        -0.582  0.516

                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
SS loadings       1.0    1.0    1.0    1.0    1.0
Proportion Var    0.2    0.2    0.2    0.2    0.2
Cumulative Var    0.2    0.4    0.6    0.8    1.0

This keep returning the same proportion of variance for each component 
regardless of the data used.

my R version is

 > R.Version()
$platform
[1] "x86_64-unknown-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] ""

$major
[1] "3"

$minor
[1] "2.1"

$year
[1] "2015"

$month
[1] "06"

$day
[1] "18"

$`svn rev`
[1] "68531"

$language
[1] "R"

$version.string
[1] "R version 3.2.1 (2015-06-18)"

$nickname
[1] "World-Famous Astronaut"

some hint would be much appreciated.

Best regards

Marcelo Kittlein

	[[alternative HTML version deleted]]


From j_colaco at utad.pt  Mon Sep 14 17:11:57 2015
From: j_colaco at utad.pt (JORGE COLACO)
Date: Mon, 14 Sep 2015 16:11:57 +0100
Subject: [R] R WRONG CALCULATIONS - Please Help
Message-ID: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>

I would greatly appreciate if you could let me know why the R does not make
the right computations in the case below.
Waiting for your reply
Jorge Cola?o






R version 3.2.2 (2015-08-14) -- "Fire Safety"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: i386-w64-mingw32/i386 (32-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.
> X<-matrix(c(-1,0,1,-1,1,0,
+ 0,0,1,1,1,-1,
+ -1,0,-1,1,0,1,
+ 1,1,-1,-1,0,0,
+ 0,0,1,1,-1,1),nrow=5,ncol=6,byrow=T)

>
mean<-c(mean(X[,1]),mean(X[,2]),mean(X[,3]),mean(X[,4]),mean(X[,5]),mean(X[,6]))
> mean
[1] -0.2  0.2  0.2  0.2  0.2  0.2
> X-mean
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] -0.8 -0.2  0.8 -1.2  0.8 -0.2
[2,] -0.2  0.2  0.8  0.8  0.8 -1.2
[3,] -1.2 -0.2 -0.8  0.8 -0.2  0.8
[4,]  0.8  0.8 -1.2 -0.8 -0.2 -0.2
[5,] -0.2 -0.2  0.8  0.8 -0.8  0.8
>

Right Result Should Be:

ans =
  -0.80000  -0.20000   0.80000  -1.20000   0.80000  -0.20000
   0.20000  -0.20000   0.80000   0.80000   0.80000  -1.20000
  -0.80000  -0.20000  -1.20000   0.80000  -0.20000   0.80000
   1.20000   0.80000  -1.20000  -1.20000  -0.20000  -0.20000
   0.20000  -0.20000   0.80000   0.80000  -1.20000   0.80000

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Mon Sep 14 19:04:23 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Mon, 14 Sep 2015 18:04:23 +0100
Subject: [R] R WRONG CALCULATIONS - Please Help
In-Reply-To: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
References: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
Message-ID: <55F6FE17.1010903@sapo.pt>

Hello,

R subtracts the _column_ vector 'mean' to X (column by column).
Apparently you want the transpose, or think 'mean' is a row vector. Try 
instead

t(t(X) - mean)

And use another name for 'mean', it already is the name of a function.

Hope it helps,

Rui Barradas

Em 14-09-2015 16:11, JORGE COLACO escreveu:
> I would greatly appreciate if you could let me know why the R does not make
> the right computations in the case below.
> Waiting for your reply
> Jorge Cola?o
>
>
>
>
>
>
> R version 3.2.2 (2015-08-14) -- "Fire Safety"
> Copyright (C) 2015 The R Foundation for Statistical Computing
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
>
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
>> X<-matrix(c(-1,0,1,-1,1,0,
> + 0,0,1,1,1,-1,
> + -1,0,-1,1,0,1,
> + 1,1,-1,-1,0,0,
> + 0,0,1,1,-1,1),nrow=5,ncol=6,byrow=T)
>
>>
> mean<-c(mean(X[,1]),mean(X[,2]),mean(X[,3]),mean(X[,4]),mean(X[,5]),mean(X[,6]))
>> mean
> [1] -0.2  0.2  0.2  0.2  0.2  0.2
>> X-mean
>       [,1] [,2] [,3] [,4] [,5] [,6]
> [1,] -0.8 -0.2  0.8 -1.2  0.8 -0.2
> [2,] -0.2  0.2  0.8  0.8  0.8 -1.2
> [3,] -1.2 -0.2 -0.8  0.8 -0.2  0.8
> [4,]  0.8  0.8 -1.2 -0.8 -0.2 -0.2
> [5,] -0.2 -0.2  0.8  0.8 -0.8  0.8
>>
>
> Right Result Should Be:
>
> ans =
>    -0.80000  -0.20000   0.80000  -1.20000   0.80000  -0.20000
>     0.20000  -0.20000   0.80000   0.80000   0.80000  -1.20000
>    -0.80000  -0.20000  -1.20000   0.80000  -0.20000   0.80000
>     1.20000   0.80000  -1.20000  -1.20000  -0.20000  -0.20000
>     0.20000  -0.20000   0.80000   0.80000  -1.20000   0.80000
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sarah.goslee at gmail.com  Mon Sep 14 19:06:42 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 14 Sep 2015 13:06:42 -0400
Subject: [R] R WRONG CALCULATIONS - Please Help
In-Reply-To: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
References: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
Message-ID: <CAM_vjunb6Uuv31_ZYMc6Hwx79kHPErfc1kpN23+XW9THcNDaxA@mail.gmail.com>

On Mon, Sep 14, 2015 at 11:11 AM, JORGE COLACO <j_colaco at utad.pt> wrote:
> I would greatly appreciate if you could let me know why the R does not make
> the right computations in the case below.
> Waiting for your reply
> Jorge Cola?o

R made the correct computations: it did exactly what you told it. It
isn't R's fault that what you told it isn't what you meant. You want
to subtract the column means from each column; what you actual told R
was to subtract Xmean from X element by element column-wise, recycling
Xmean as necessary.

Here's what you meant:

X<-matrix(c(-1,0,1,-1,1,0,
 0,0,1,1,1,-1,
 -1,0,-1,1,0,1,
 1,1,-1,-1,0,0,
 0,0,1,1,-1,1),nrow=5,ncol=6,byrow=T)

Xmean <- colMeans(X)

sweep(X, 2, Xmean, "-")

Thank you for providing a simple reproducible example and clear idea
of what you intended. It made answering your question very
straightforward.


>
> R version 3.2.2 (2015-08-14) -- "Fire Safety"
> Copyright (C) 2015 The R Foundation for Statistical Computing
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
>
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
>> X<-matrix(c(-1,0,1,-1,1,0,
> + 0,0,1,1,1,-1,
> + -1,0,-1,1,0,1,
> + 1,1,-1,-1,0,0,
> + 0,0,1,1,-1,1),nrow=5,ncol=6,byrow=T)
>
>>
> mean<-c(mean(X[,1]),mean(X[,2]),mean(X[,3]),mean(X[,4]),mean(X[,5]),mean(X[,6]))
>> mean
> [1] -0.2  0.2  0.2  0.2  0.2  0.2
>> X-mean
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,] -0.8 -0.2  0.8 -1.2  0.8 -0.2
> [2,] -0.2  0.2  0.8  0.8  0.8 -1.2
> [3,] -1.2 -0.2 -0.8  0.8 -0.2  0.8
> [4,]  0.8  0.8 -1.2 -0.8 -0.2 -0.2
> [5,] -0.2 -0.2  0.8  0.8 -0.8  0.8
>>
>
> Right Result Should Be:
>
> ans =
>   -0.80000  -0.20000   0.80000  -1.20000   0.80000  -0.20000
>    0.20000  -0.20000   0.80000   0.80000   0.80000  -1.20000
>   -0.80000  -0.20000  -1.20000   0.80000  -0.20000   0.80000
>    1.20000   0.80000  -1.20000  -1.20000  -0.20000  -0.20000
>    0.20000  -0.20000   0.80000   0.80000  -1.20000   0.80000
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee
http://www.functionaldiversity.org


From sarah.goslee at gmail.com  Mon Sep 14 19:08:10 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 14 Sep 2015 13:08:10 -0400
Subject: [R] I have trouble loading histogram :(
In-Reply-To: <1442241918762-4712231.post@n4.nabble.com>
References: <1442241918762-4712231.post@n4.nabble.com>
Message-ID: <CAM_vju=rYdrbJfo7WKC5nW2pPC3HY8B+c45JvL2Yg-EX8vnB7g@mail.gmail.com>

Is this homework? We don't do homework here.

Two hints, though:
use str() to look at your data after you read it into R.
don't use attach().

Sarah

On Mon, Sep 14, 2015 at 10:45 AM, massmatics <2hanl2da at naver.com> wrote:
> Make a histogram and boxplot of the data sets: *south.csv*.
>
> south.csv (the following is how it is displayed in the excel file)
> x -> Acolumn, 1st row
> 12
> 10
> 10
> 13
> 12
> 12
> 14
> 7
> 16
> 18
> 8
> 29
> 12
> 14
> 33
> 10
> 6
> 18
> 11
> 25
> 8
> 16
> 14
> 11
> 10
> 20
> 14
> 11
> 12
> 13 ->  Acolumn, 31st row
>
> This is the code I used to run but failed :(
> My solution:
> *south <- read.csv("C:/Users/Win/Desktop/south.csv")
> attach(south)
> hist(south)* --> When I ran this, it didn't work. Isn't this supposed to be
> correct? :(
>
> Thanks!
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/I-have-trouble-loading-histogram-tp4712231.html
> Sent from the R help mailing list archive at Nabble.com.
>
-- 
Sarah Goslee
http://www.functionaldiversity.org


From msharp at txbiomed.org  Mon Sep 14 19:14:03 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Mon, 14 Sep 2015 17:14:03 +0000
Subject: [R] SSD vs RAM upgrade
In-Reply-To: <6934DE30FA68C04E9227F3AFAB89D12F0148A99F53@SCSU83A.campus.stcloudstate.edu>
References: <6934DE30FA68C04E9227F3AFAB89D12F0148A99F53@SCSU83A.campus.stcloudstate.edu>
Message-ID: <63A22D38-602D-4F5D-8608-FD2867734A1E@txbiomed.org>

John,

Unless you are doing something very unusual (such as using a database to keep intermediate results) SSD hardware will have no affect on R being memory bound. According to the behavior you described, you need RAM. 

R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org




> On Sep 14, 2015, at 10:44 AM, Kulas, John T. <jtkulas at stcloudstate.edu> wrote:
> 
> I have several lab computers that frequently lag/crash with R apparently due to low RAM (they're all 8 GB)
> 
> I put in a request to up the RAM to 32 GB on a few, but my tech support is suggesting an SSD harddrive upgrade instead of the increased RAM.
> 
> Any suggestions on the better approach (SSD harddrive vs increased RAM to help R chug along)?
> 
> Thanks much - John
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From NordlDJ at dshs.wa.gov  Mon Sep 14 19:21:23 2015
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Mon, 14 Sep 2015 17:21:23 +0000
Subject: [R] R WRONG CALCULATIONS - Please Help
In-Reply-To: <CAM_vjunb6Uuv31_ZYMc6Hwx79kHPErfc1kpN23+XW9THcNDaxA@mail.gmail.com>
References: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
	<CAM_vjunb6Uuv31_ZYMc6Hwx79kHPErfc1kpN23+XW9THcNDaxA@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27662ED8BB43@WAXMXOLYMB025.WAX.wa.lcl>

Another option is

apply(X,2,function(x) x-mean(x))


Hope this is helpful,

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sarah Goslee
Sent: Monday, September 14, 2015 10:07 AM
To: JORGE COLACO
Cc: r-help
Subject: Re: [R] R WRONG CALCULATIONS - Please Help

On Mon, Sep 14, 2015 at 11:11 AM, JORGE COLACO <j_colaco at utad.pt> wrote:
> I would greatly appreciate if you could let me know why the R does not 
> make the right computations in the case below.
> Waiting for your reply
> Jorge Cola?o

R made the correct computations: it did exactly what you told it. It isn't R's fault that what you told it isn't what you meant. You want to subtract the column means from each column; what you actual told R was to subtract Xmean from X element by element column-wise, recycling Xmean as necessary.

Here's what you meant:

X<-matrix(c(-1,0,1,-1,1,0,
 0,0,1,1,1,-1,
 -1,0,-1,1,0,1,
 1,1,-1,-1,0,0,
 0,0,1,1,-1,1),nrow=5,ncol=6,byrow=T)

Xmean <- colMeans(X)

sweep(X, 2, Xmean, "-")

Thank you for providing a simple reproducible example and clear idea of what you intended. It made answering your question very straightforward.


>
> R version 3.2.2 (2015-08-14) -- "Fire Safety"
> Copyright (C) 2015 The R Foundation for Statistical Computing
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
>
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and 'citation()' on how to 
> cite R or R packages in publications.
>
> Type 'demo()' for some demos, 'help()' for on-line help, or 
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
>> X<-matrix(c(-1,0,1,-1,1,0,
> + 0,0,1,1,1,-1,
> + -1,0,-1,1,0,1,
> + 1,1,-1,-1,0,0,
> + 0,0,1,1,-1,1),nrow=5,ncol=6,byrow=T)
>
>>
> mean<-c(mean(X[,1]),mean(X[,2]),mean(X[,3]),mean(X[,4]),mean(X[,5]),me
> an(X[,6]))
>> mean
> [1] -0.2  0.2  0.2  0.2  0.2  0.2
>> X-mean
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,] -0.8 -0.2  0.8 -1.2  0.8 -0.2
> [2,] -0.2  0.2  0.8  0.8  0.8 -1.2
> [3,] -1.2 -0.2 -0.8  0.8 -0.2  0.8
> [4,]  0.8  0.8 -1.2 -0.8 -0.2 -0.2
> [5,] -0.2 -0.2  0.8  0.8 -0.8  0.8
>>
>
> Right Result Should Be:
>
> ans =
>   -0.80000  -0.20000   0.80000  -1.20000   0.80000  -0.20000
>    0.20000  -0.20000   0.80000   0.80000   0.80000  -1.20000
>   -0.80000  -0.20000  -1.20000   0.80000  -0.20000   0.80000
>    1.20000   0.80000  -1.20000  -1.20000  -0.20000  -0.20000
>    0.20000  -0.20000   0.80000   0.80000  -1.20000   0.80000
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Sarah Goslee
http://www.functionaldiversity.org

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From miaojpm at gmail.com  Mon Sep 14 19:35:38 2015
From: miaojpm at gmail.com (jpm miao)
Date: Mon, 14 Sep 2015 10:35:38 -0700
Subject: [R] Is there any concise way to write a one-to-one mapping?
Message-ID: <CABcx46CwbZ-Pm_Z2zr8XJ5NbpatyaTEnGTyRH=w7M4p=iheZtg@mail.gmail.com>

My code is:



if(type=="none")

  type2<-"nc"

if(type=="drift")

  type2<-"c"

if(type=="trend")

  type2<-"ct"


I am wondering if there's a concise way to write a mapping from type to
type2, especially when the number of categories is high. Thanks!

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Mon Sep 14 19:41:45 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 14 Sep 2015 10:41:45 -0700
Subject: [R] Is there any concise way to write a one-to-one mapping?
In-Reply-To: <CABcx46CwbZ-Pm_Z2zr8XJ5NbpatyaTEnGTyRH=w7M4p=iheZtg@mail.gmail.com>
References: <CABcx46CwbZ-Pm_Z2zr8XJ5NbpatyaTEnGTyRH=w7M4p=iheZtg@mail.gmail.com>
Message-ID: <5E4B1262-24EF-4D34-BA44-A77FF2EC3F05@dcn.davis.CA.us>

Assuming your data are in a data frame, make anorher data frame with the type and type2 columns and merge it with your original data frame.

?merge
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 14, 2015 10:35:38 AM PDT, jpm miao <miaojpm at gmail.com> wrote:
>My code is:
>
>
>
>if(type=="none")
>
>  type2<-"nc"
>
>if(type=="drift")
>
>  type2<-"c"
>
>if(type=="trend")
>
>  type2<-"ct"
>
>
>I am wondering if there's a concise way to write a mapping from type to
>type2, especially when the number of categories is high. Thanks!
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Mon Sep 14 20:15:59 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 14 Sep 2015 14:15:59 -0400
Subject: [R] Is there any concise way to write a one-to-one mapping?
In-Reply-To: <CABcx46CwbZ-Pm_Z2zr8XJ5NbpatyaTEnGTyRH=w7M4p=iheZtg@mail.gmail.com>
References: <CABcx46CwbZ-Pm_Z2zr8XJ5NbpatyaTEnGTyRH=w7M4p=iheZtg@mail.gmail.com>
Message-ID: <55F70EDF.9080802@gmail.com>

On 14/09/2015 1:35 PM, jpm miao wrote:
> My code is:
>
>
>
> if(type=="none")
>
>    type2<-"nc"
>
> if(type=="drift")
>
>    type2<-"c"
>
> if(type=="trend")
>
>    type2<-"ct"

These are concise:

type2 <- c(none = "nc", drift = "c", trend = "ct")[type]

type2 <- switch(type, none = "nc", drift = "c", trend = "ct")

Duncan Murdoch

>
>
> I am wondering if there's a concise way to write a mapping from type to
> type2, especially when the number of categories is high. Thanks!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From schwidom at gmx.net  Mon Sep 14 20:45:16 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Mon, 14 Sep 2015 20:45:16 +0200
Subject: [R] R WRONG CALCULATIONS - Please Help
In-Reply-To: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
References: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
Message-ID: <20150914184516.GA3585@debian64>

On Mon, Sep 14, 2015 at 04:11:57PM +0100, JORGE COLACO wrote:

> > X-mean
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,] -0.8 -0.2  0.8 -1.2  0.8 -0.2
> [2,] -0.2  0.2  0.8  0.8  0.8 -1.2
> [3,] -1.2 -0.2 -0.8  0.8 -0.2  0.8
> [4,]  0.8  0.8 -1.2 -0.8 -0.2 -0.2
> [5,] -0.2 -0.2  0.8  0.8 -0.8  0.8
> >

try this:

X - mean[ col( X)]
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] -0.8 -0.2  0.8 -1.2  0.8 -0.2
[2,]  0.2 -0.2  0.8  0.8  0.8 -1.2
[3,] -0.8 -0.2 -1.2  0.8 -0.2  0.8
[4,]  1.2  0.8 -1.2 -1.2 -0.2 -0.2
[5,]  0.2 -0.2  0.8  0.8 -1.2  0.8

Regards.

> Right Result Should Be:
> 
> ans =
>   -0.80000  -0.20000   0.80000  -1.20000   0.80000  -0.20000
>    0.20000  -0.20000   0.80000   0.80000   0.80000  -1.20000
>   -0.80000  -0.20000  -1.20000   0.80000  -0.20000   0.80000
>    1.20000   0.80000  -1.20000  -1.20000  -0.20000  -0.20000
>    0.20000  -0.20000   0.80000   0.80000  -1.20000   0.80000
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mtilley3 at gatech.edu  Mon Sep 14 20:49:19 2015
From: mtilley3 at gatech.edu (mtilley3)
Date: Mon, 14 Sep 2015 11:49:19 -0700 (PDT)
Subject: [R] Plotting Numeric Values with Non-Numeric Values
Message-ID: <1442256559528-4712253.post@n4.nabble.com>

I have looked for quite a long time and can't quite find the answer for what
I'm looking for. I want to graph bmi vs gender, with male/female on the
x-axis and the bmi numerical value on the y-axis. Can anyone explain how to
do this? Thanks!



--
View this message in context: http://r.789695.n4.nabble.com/Plotting-Numeric-Values-with-Non-Numeric-Values-tp4712253.html
Sent from the R help mailing list archive at Nabble.com.


From msharp at txbiomed.org  Mon Sep 14 22:20:25 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Mon, 14 Sep 2015 20:20:25 +0000
Subject: [R] Plotting Numeric Values with Non-Numeric Values
In-Reply-To: <1442256559528-4712253.post@n4.nabble.com>
References: <1442256559528-4712253.post@n4.nabble.com>
Message-ID: <75C72356-8525-4E83-9CFD-55140747274B@txbiomed.org>

Try
?boxplot

Mark
R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org







> On Sep 14, 2015, at 1:49 PM, mtilley3 <mtilley3 at gatech.edu> wrote:
> 
> I have looked for quite a long time and can't quite find the answer for what
> I'm looking for. I want to graph bmi vs gender, with male/female on the
> x-axis and the bmi numerical value on the y-axis. Can anyone explain how to
> do this? Thanks!
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Plotting-Numeric-Values-with-Non-Numeric-Values-tp4712253.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Mon Sep 14 22:29:59 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 14 Sep 2015 13:29:59 -0700
Subject: [R] Plotting Numeric Values with Non-Numeric Values
In-Reply-To: <1442256559528-4712253.post@n4.nabble.com>
References: <1442256559528-4712253.post@n4.nabble.com>
Message-ID: <CAF8bMca6sSmxX5hAMQbfXqtxUvA+z-jS_caJTRWt2YBn1=mqDA@mail.gmail.com>

d <- data.frame( # data to make a self-contained example
    gender=rep(c("Female","Male"),c(4,6)),
    bmi=c(seq(24,25,len=4), seq(25,27,len=6)),
    age=c("Young","Old")[c(1,1,2,2,1,1,1,1,2,2)])
boxplot(bmi ~ gender, data=d)
lattice::bwplot(bmi ~ gender, data=d)
lattice::bwplot(bmi ~ gender | age, data=d) # separate panel for each age group
lattice::stripplot(bmi ~ gender, data=d)
lattice::dotplot(bmi ~ gender|age, data=d)

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Sep 14, 2015 at 11:49 AM, mtilley3 <mtilley3 at gatech.edu> wrote:
> I have looked for quite a long time and can't quite find the answer for what
> I'm looking for. I want to graph bmi vs gender, with male/female on the
> x-axis and the bmi numerical value on the y-axis. Can anyone explain how to
> do this? Thanks!
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Plotting-Numeric-Values-with-Non-Numeric-Values-tp4712253.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Mon Sep 14 23:07:31 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 14 Sep 2015 21:07:31 +0000
Subject: [R] Error in principal component loadings calculation
In-Reply-To: <55F6CFB5.7080107@mdp.edu.ar>
References: <55F6CFB5.7080107@mdp.edu.ar>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C1ACB@mb02.ads.tamu.edu>

The sum of the squared loadings will always sum to 1 because they are standardized by dividing them by the standard deviation of each component. The terminology for principal components is not as consistent as we could hope. What princomp() calls loadings is really the structure matrix (the correlation between each variable and the component). The pattern matrix (often called the loadings) are the regression coefficients for computing the principal component scores. You are probably looking for the pattern matrix which is easy to obtain by multiplying by the standard deviations:

> set.seed(42)
> data <- matrix(runif(100), 20, 5)
> pc <- princomp(data, cor=TRUE)
> loadings(pc)

Loadings:
     Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
[1,]  0.638  0.249        -0.260 -0.679
[2,]        -0.714  0.449  0.298 -0.444
[3,]  0.585 -0.152  0.522 -0.231  0.555
[4,]        -0.617 -0.543 -0.564       
[5,] -0.496  0.154  0.479 -0.687 -0.172

               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
SS loadings       1.0    1.0    1.0    1.0    1.0
Proportion Var    0.2    0.2    0.2    0.2    0.2
Cumulative Var    0.2    0.4    0.6    0.8    1.0
> rowSums(pc$loadings^2)
[1] 1 1 1 1 1
> # Notice that the column sums of the squared loadings all equal 0
> # Now multiply each loading by its standard deviation
> sweep(pc$loadings, 2, pc$sdev, "*")

Loadings:
     Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
[1,]  0.765  0.275        -0.237 -0.531
[2,]        -0.787  0.427  0.271 -0.347
[3,]  0.701 -0.167  0.497 -0.211  0.434
[4,]        -0.680 -0.518 -0.515       
[5,] -0.594  0.169  0.456 -0.627 -0.134

               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
SS loadings     1.436  1.215  0.907  0.832  0.611
Proportion Var  0.287  0.243  0.181  0.166  0.122
Cumulative Var  0.287  0.530  0.712  0.878  1.000
> pc$sdev^2
   Comp.1    Comp.2    Comp.3    Comp.4    Comp.5 
1.4362072 1.2145055 0.9068555 0.8315685 0.6108632 
> # Now the sum of the squared loadings equals the 
> # squared standard deviation (aka the eigenvalues)

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Marcelo Kittlein
Sent: Monday, September 14, 2015 8:46 AM
To: r-help at r-project.org
Subject: [R] Error in principal component loadings calculation

Hi all

I have been using "princomp" to obtain the principal components of some 
data and find that the loadings returned by the function appear to have 
some error.

in a simple example if a calculate de pc for a random matrix I get that 
all loadings for the different components have the same proportion of 
variance

data <- matrix(runif(100), 20, 5)
pc <- princomp(data, cor=TRUE)
loadings(pc)

Loadings:
      Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
[1,] -0.280  0.510  0.674 -0.217 -0.400
[2,]  0.529 -0.353        -0.694 -0.330
[3,] -0.111  0.563 -0.713 -0.336 -0.222
[4,] -0.530 -0.502 -0.178  0.140 -0.645
[5,] -0.590 -0.215        -0.582  0.516

                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
SS loadings       1.0    1.0    1.0    1.0    1.0
Proportion Var    0.2    0.2    0.2    0.2    0.2
Cumulative Var    0.2    0.4    0.6    0.8    1.0

This keep returning the same proportion of variance for each component 
regardless of the data used.

my R version is

 > R.Version()
$platform
[1] "x86_64-unknown-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] ""

$major
[1] "3"

$minor
[1] "2.1"

$year
[1] "2015"

$month
[1] "06"

$day
[1] "18"

$`svn rev`
[1] "68531"

$language
[1] "R"

$version.string
[1] "R version 3.2.1 (2015-06-18)"

$nickname
[1] "World-Famous Astronaut"

some hint would be much appreciated.

Best regards

Marcelo Kittlein

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From laura.fernandezp at edu.uah.es  Mon Sep 14 22:18:37 2015
From: laura.fernandezp at edu.uah.es (laurafdez56)
Date: Mon, 14 Sep 2015 13:18:37 -0700 (PDT)
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <55F6E921.2060700@dewey.myzen.co.uk>
References: <1442232687664-4712219.post@n4.nabble.com>
	<55F6E921.2060700@dewey.myzen.co.uk>
Message-ID: <1442261917039-4712255.post@n4.nabble.com>

Dear Michael! 
Here is my database:  Survival_help.csv
<http://r.789695.n4.nabble.com/file/n4712255/Survival_help.csv>  
That is just what I thought about the point symbols, but is there possible
to put square, circle or whatever instead lines solid, dotted, etc?? 

I draw the graphics with dotted lines, but I want to put squares. 

Thank you very much for you help!!!
Laura



--
View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712255.html
Sent from the R help mailing list archive at Nabble.com.


From laura.fernandezp at edu.uah.es  Mon Sep 14 22:24:27 2015
From: laura.fernandezp at edu.uah.es (laurafdez56)
Date: Mon, 14 Sep 2015 13:24:27 -0700 (PDT)
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <CAM_vjukjdjh=XmRFh9ph4aJuQ6FG6pyT-B0TNfjGrc=hCZmB5g@mail.gmail.com>
References: <1442232687664-4712219.post@n4.nabble.com>
	<CAM_vjukjdjh=XmRFh9ph4aJuQ6FG6pyT-B0TNfjGrc=hCZmB5g@mail.gmail.com>
Message-ID: <1442262267711-4712256.post@n4.nabble.com>

Dear Sara!!
Thank you for your advice, but is very weird the graphic type="b.
I only want to draw square line instead dotted line, do I explain myself?

Thank you!!
Laura



--
View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712256.html
Sent from the R help mailing list archive at Nabble.com.


From dwinsemius at comcast.net  Mon Sep 14 23:56:43 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 14 Sep 2015 14:56:43 -0700
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <1442261917039-4712255.post@n4.nabble.com>
References: <1442232687664-4712219.post@n4.nabble.com>
	<55F6E921.2060700@dewey.myzen.co.uk>
	<1442261917039-4712255.post@n4.nabble.com>
Message-ID: <8FAD4A6E-B880-423F-ABE2-873104DF58A6@comcast.net>


On Sep 14, 2015, at 1:18 PM, laurafdez56 wrote:

> Dear Michael! 
> Here is my database:  Survival_help.csv
> <http://r.789695.n4.nabble.com/file/n4712255/Survival_help.csv>  
> That is just what I thought about the point symbols, but is there possible
> to put square, circle or whatever instead lines solid, dotted, etc?? 
> 
> I draw the graphics with dotted lines, but I want to put squares. 

If you do not want point symbols, then you have not explained yourself well. Can you offer a link to an example in your domain of investigation for "putting squares" to signify value that would otherwise be represented with a line? Are you hoping to represent a value by varying the dimensions of a square with relative proportions of the sides or areas? (This is not generally considered an unambiguous strategy because of the manner in which humans perceive areas.)


-- 
David.

> 
> Thank you very much for you help!!!
> Laura
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712255.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dcarlson at tamu.edu  Tue Sep 15 00:36:34 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 14 Sep 2015 22:36:34 +0000
Subject: [R] Error in principal component loadings calculation
In-Reply-To: <55F711B9.10101@mdp.edu.ar>
References: <55F6CFB5.7080107@mdp.edu.ar>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C1ACB@mb02.ads.tamu.edu>
	<55F711B9.10101@mdp.edu.ar>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C1C15@mb02.ads.tamu.edu>

The quickest way to get that is

> summary(pc)
Importance of components:
                          Comp.1    Comp.2    Comp.3    Comp.4    Comp.5
Standard deviation     1.1984186 1.1020461 0.9522896 0.9119038 0.7815774
Proportion of Variance 0.2872414 0.2429011 0.1813711 0.1663137 0.1221726
Cumulative Proportion  0.2872414 0.5301425 0.7115137 0.8778274 1.0000000

David

-----Original Message-----
From: Marcelo Kittlein [mailto:kittlein at mdp.edu.ar] 
Sent: Monday, September 14, 2015 1:28 PM
To: David L Carlson <dcarlson at tamu.edu>
Subject: Re: [R] Error in principal component loadings calculation

Thanks David

I thought that "Proportion var" was the proportion of the variance of 
successive component scores. The one you get with "summary" of the 
princomp object.



Proportion Var    0.2    0.2    0.2    0.2    0.2

On 14/09/15 21:07, David L Carlson wrote:
> The sum of the squared loadings will always sum to 1 because they are standardized by dividing them by the standard deviation of each component. The terminology for principal components is not as consistent as we could hope. What princomp() calls loadings is really the structure matrix (the correlation between each variable and the component). The pattern matrix (often called the loadings) are the regression coefficients for computing the principal component scores. You are probably looking for the pattern matrix which is easy to obtain by multiplying by the standard deviations:
>
>> set.seed(42)
>> data <- matrix(runif(100), 20, 5)
>> pc <- princomp(data, cor=TRUE)
>> loadings(pc)
> Loadings:
>       Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
> [1,]  0.638  0.249        -0.260 -0.679
> [2,]        -0.714  0.449  0.298 -0.444
> [3,]  0.585 -0.152  0.522 -0.231  0.555
> [4,]        -0.617 -0.543 -0.564
> [5,] -0.496  0.154  0.479 -0.687 -0.172
>
>                 Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
> SS loadings       1.0    1.0    1.0    1.0    1.0
> Proportion Var    0.2    0.2    0.2    0.2    0.2
> Cumulative Var    0.2    0.4    0.6    0.8    1.0
>> rowSums(pc$loadings^2)
> [1] 1 1 1 1 1
>> # Notice that the column sums of the squared loadings all equal 0
>> # Now multiply each loading by its standard deviation
>> sweep(pc$loadings, 2, pc$sdev, "*")
> Loadings:
>       Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
> [1,]  0.765  0.275        -0.237 -0.531
> [2,]        -0.787  0.427  0.271 -0.347
> [3,]  0.701 -0.167  0.497 -0.211  0.434
> [4,]        -0.680 -0.518 -0.515
> [5,] -0.594  0.169  0.456 -0.627 -0.134
>
>                 Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
> SS loadings     1.436  1.215  0.907  0.832  0.611
> Proportion Var  0.287  0.243  0.181  0.166  0.122
> Cumulative Var  0.287  0.530  0.712  0.878  1.000
>> pc$sdev^2
>     Comp.1    Comp.2    Comp.3    Comp.4    Comp.5
> 1.4362072 1.2145055 0.9068555 0.8315685 0.6108632
>> # Now the sum of the squared loadings equals the
>> # squared standard deviation (aka the eigenvalues)
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Marcelo Kittlein
> Sent: Monday, September 14, 2015 8:46 AM
> To: r-help at r-project.org
> Subject: [R] Error in principal component loadings calculation
>
> Hi all
>
> I have been using "princomp" to obtain the principal components of some
> data and find that the loadings returned by the function appear to have
> some error.
>
> in a simple example if a calculate de pc for a random matrix I get that
> all loadings for the different components have the same proportion of
> variance
>
> data <- matrix(runif(100), 20, 5)
> pc <- princomp(data, cor=TRUE)
> loadings(pc)
>
> Loadings:
>        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
> [1,] -0.280  0.510  0.674 -0.217 -0.400
> [2,]  0.529 -0.353        -0.694 -0.330
> [3,] -0.111  0.563 -0.713 -0.336 -0.222
> [4,] -0.530 -0.502 -0.178  0.140 -0.645
> [5,] -0.590 -0.215        -0.582  0.516
>
>                  Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
> SS loadings       1.0    1.0    1.0    1.0    1.0
> Proportion Var    0.2    0.2    0.2    0.2    0.2
> Cumulative Var    0.2    0.4    0.6    0.8    1.0
>
> This keep returning the same proportion of variance for each component
> regardless of the data used.
>
> my R version is
>
>   > R.Version()
> $platform
> [1] "x86_64-unknown-linux-gnu"
>
> $arch
> [1] "x86_64"
>
> $os
> [1] "linux-gnu"
>
> $system
> [1] "x86_64, linux-gnu"
>
> $status
> [1] ""
>
> $major
> [1] "3"
>
> $minor
> [1] "2.1"
>
> $year
> [1] "2015"
>
> $month
> [1] "06"
>
> $day
> [1] "18"
>
> $`svn rev`
> [1] "68531"
>
> $language
> [1] "R"
>
> $version.string
> [1] "R version 3.2.1 (2015-06-18)"
>
> $nickname
> [1] "World-Famous Astronaut"
>
> some hint would be much appreciated.
>
> Best regards
>
> Marcelo Kittlein
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.turner at auckland.ac.nz  Tue Sep 15 01:02:34 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 15 Sep 2015 11:02:34 +1200
Subject: [R] [FORGED]  R WRONG CALCULATIONS - Please Help
In-Reply-To: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
References: <CAOfUjf88uzzKiV6sPAofnr164RPAoNkzcf3kTGmaMSb9vZrbmA@mail.gmail.com>
Message-ID: <55F7520A.9090303@auckland.ac.nz>

On 15/09/15 03:11, JORGE COLACO wrote:
> I would greatly appreciate if you could let me know why the R does not make
> the right computations in the case below.
> Waiting for your reply.

It is very irritating, at least to me, when list correspondents submit 
postings asserting that "R gets it wrong".  It is very rare for (base) R 
to get it wrong; there are very few bugs in base R.  There are more in 
contributed packages, but even these are quite rare.

Almost always if you think that "R gets it wrong" there is something 
wrong with *your* thinking or with your understanding of how R works and 
you need to study some more about the issue in question.

Almost always the fault lies not in R but in yourself.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jrkrideau at inbox.com  Tue Sep 15 01:43:09 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 14 Sep 2015 15:43:09 -0800
Subject: [R] I have trouble loading histogram :(
In-Reply-To: <1442241918762-4712231.post@n4.nabble.com>
Message-ID: <D1B3127F1E8.00000A15jrkrideau@inbox.com>


This looks a lot like homework and we have a no homework policy.

However hist(south) is definitely not correct.  Hint south is a data,frame not the variable.
John Kane
Kingston ON Canada


> -----Original Message-----
> From: 2hanl2da at naver.com
> Sent: Mon, 14 Sep 2015 07:45:18 -0700 (PDT)
> To: r-help at r-project.org
> Subject: [R] I have trouble loading histogram :(
> 
> Make a histogram and boxplot of the data sets: *south.csv*.
> 
> south.csv (the following is how it is displayed in the excel file)
> x -> Acolumn, 1st row
> 12
> 10
> 10
> 13
> 12
> 12
> 14
> 7
> 16
> 18
> 8
> 29
> 12
> 14
> 33
> 10
> 6
> 18
> 11
> 25
> 8
> 16
> 14
> 11
> 10
> 20
> 14
> 11
> 12
> 13 ->  Acolumn, 31st row
> 
> This is the code I used to run but failed :(
> My solution:
> *south <- read.csv("C:/Users/Win/Desktop/south.csv")
> attach(south)
> hist(south)* --> When I ran this, it didn't work. Isn't this supposed to
> be
> correct? :(
> 
> Thanks!
> 
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/I-have-trouble-loading-histogram-tp4712231.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
GET FREE 5GB EMAIL - Check out spam free email with many cool features!
Visit http://www.inbox.com/email to find out more!


From hannah.hlx at gmail.com  Tue Sep 15 03:23:50 2015
From: hannah.hlx at gmail.com (li li)
Date: Mon, 14 Sep 2015 21:23:50 -0400
Subject: [R] Order of boxplots
Message-ID: <CAHLnndbAB5FD5_geRcOu_1CWn3fjPXNz1Za9uQH-Yo709P5Lpw@mail.gmail.com>

Hi all,
  I have the following data "tmp" and I want to plot  boxplots for
each level of the factor "type" and the order the factor should be c,
b ,a. In other words, the boxplot corresponding to the level "c"
should be the first and so on.
Any suggestions?
   Li

> tmp
   result type
1     101    a
2     101    a
3     101    a
4     101    a
5     101    a
6     101    a
7     100    a
8     106    b
9      91    b
10     78    b
11     95    b
12    111    b
13     92    b
14     98    b
15    108    c
16    112    c
17     98    c
18    102    c
19     88    c
20     86    c
21     81    c


From jrkrideau at inbox.com  Tue Sep 15 03:26:42 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 14 Sep 2015 17:26:42 -0800
Subject: [R] a question about data manipulation in R
In-Reply-To: <782581298.3245775.1442252264124.JavaMail.yahoo@mail.yahoo.com>
References: <c23295b3cba.0000028djrkrideau@inbox.com>
Message-ID: <D29A810FCBF.00000B02jrkrideau@inbox.com>

Please do not send email in HTML. The R-help mailer removes any HTML code and the result is a mess. Your results were readable but still a terrible mess.

Also please reply to the R-help list not just to me. Your chances of getting an answer are much better if you reply to the list.

Anyway here are your two data.frames. It should be very easy to do what you want but I am completely blank for the moment. We'll let someone else have a go at this

input <- structure(list(V1 = c(1L, 1L, 1L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 4L,

4L), V2 = c(33313L, 45534L, 34533L, 54321L,43322L, 34462L, 40023L, 44412L, 11115L, 26L,

2236L, 3L, 12343L, 12334L, 94845L)), .Names = c("V1", "V2"), class ="data.frame",

row.names = c(NA, -15L))

output <- structure(list(V1 = c(33313L, 45534L, 34533L, NA, NA), V2 =c(54321L, 43322L,

34462L, 40023L, NA), V3 = c(44412L, 11115L, 26L, NA, NA), V4 = c(2236L, 3L, 12343L,

12334L, 94845L)), .Names =c("V1", "V2", "V3", "V4"), class ="data.frame", row.names =

c(NA, -5L)) 

John Kane

Kingston ON Canada


John Kane
Kingston ON Canada

-----Original Message-----
From: zkarimi1985 at yahoo.com
Sent: Mon, 14 Sep 2015 17:37:44 +0000 (UTC)
To: jrkrideau at inbox.com
Subject: Re: [R] a question about data manipulation in R

Dear John,

Thank you for your response.

my last item was 432. I Put my input and output file in duput() and this is the result:

> dput(input)

structure(list(V1 = c(1L, 1L, 1L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 

4L, 4L, 4L, 4L, 4L), V2 = c(33313L, 45534L, 34533L, 54321L, 43322L, 

34462L, 40023L, 44412L, 11115L, 26L, 2236L, 3L, 12343L, 12334L, 

94845L)), .Names = c("V1", "V2"), class = "data.frame", row.names = c(NA, 

-15L))

> dput(output)

structure(list(V1 = c(33313L, 45534L, 34533L, NA, NA), V2 = c(54321L, 

43322L, 34462L, 40023L, NA), V3 = c(44412L, 11115L, 26L, NA, 

NA), V4 = c(2236L, 3L, 12343L, 12334L, 94845L)), .Names = c("V1", 

"V2", "V3", "V4"), class = "data.frame", row.names = c(NA, -5L

))

my problem is that I do not know how to write a code in R to reach from the described input to the output that I need. I am beginner in R and I try to learn. I should mention that My real input data is very large and this is only small example similar to the real data. can you please show me how to write a script in R to reach to the output I need?

kind regards,

Zahra

  On Sunday, 13 September 2015, 14:07, John Kane <jrkrideau at inbox.com> wrote:

You are in the right place but the question in not very clear. Have a look at the links below for some suggestions on how to ask a question on R-help.

I think we need the data in dput() format. See ?dput or the discussion of dput() in the links.

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example [http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example]and http://adv-r.had.co.nz/Reproducibility.html [http://adv-r.had.co.nz/Reproducibility.html]

Did you intend that the last item in the data set be? 432I where that last character is an I (??? not a one 1?

John Kane
Kingston ON Canada

> -----Original Message-----
> From: r-help at r-project.org
> Sent: Sun, 13 Sep 2015 05:12:07 +0000 (UTC)
> To: r-help at r-project.org
> Subject: [R] a question about data manipulation in R
> 
> Hello,
> I have a data like this:1 2331 3331 4551 3452 5432 4332 3442 4003 4443
> 1113 0003 432I want to change it to this new dataset:c1 ? ?c2 ? ? c3233
> 543 ?444333 ?433 ?111455 ?344 ?000345 ?400 ?432
> ?How can I do this in R? Please if this not the correct place to ask ?my
> queston.. then could you please guid me , where should I ask my
> question?kind regards,Zahra

> 
> ??? [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help [https://stat.ethz.ch/mailman/listinfo/r-help]
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html [http://www.r-project.org/posting-guide.html]
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.

[http://mysecurelogon.com/password-manager]

____________________________________________________________
Send your photos by email in seconds...
TRY FREE IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if3
Works in all emails, instant messengers, blogs, forums and social networks.


From jdnewmil at dcn.davis.CA.us  Tue Sep 15 03:44:41 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 14 Sep 2015 18:44:41 -0700
Subject: [R] Order of boxplots
In-Reply-To: <CAHLnndbAB5FD5_geRcOu_1CWn3fjPXNz1Za9uQH-Yo709P5Lpw@mail.gmail.com>
References: <CAHLnndbAB5FD5_geRcOu_1CWn3fjPXNz1Za9uQH-Yo709P5Lpw@mail.gmail.com>
Message-ID: <C8E4529E-14C2-4E38-9AFA-476E7DDCA9C5@dcn.davis.CA.us>

Make your factor variable deliberately. That is, specify the levels parameter with the values in order when you create the factor.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 14, 2015 6:23:50 PM PDT, li li <hannah.hlx at gmail.com> wrote:
>Hi all,
>  I have the following data "tmp" and I want to plot  boxplots for
>each level of the factor "type" and the order the factor should be c,
>b ,a. In other words, the boxplot corresponding to the level "c"
>should be the first and so on.
>Any suggestions?
>   Li
>
>> tmp
>   result type
>1     101    a
>2     101    a
>3     101    a
>4     101    a
>5     101    a
>6     101    a
>7     100    a
>8     106    b
>9      91    b
>10     78    b
>11     95    b
>12    111    b
>13     92    b
>14     98    b
>15    108    c
>16    112    c
>17     98    c
>18    102    c
>19     88    c
>20     86    c
>21     81    c
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From hannah.hlx at gmail.com  Tue Sep 15 04:31:16 2015
From: hannah.hlx at gmail.com (li li)
Date: Mon, 14 Sep 2015 22:31:16 -0400
Subject: [R] Order of boxplots
In-Reply-To: <C8E4529E-14C2-4E38-9AFA-476E7DDCA9C5@dcn.davis.CA.us>
References: <CAHLnndbAB5FD5_geRcOu_1CWn3fjPXNz1Za9uQH-Yo709P5Lpw@mail.gmail.com>
	<C8E4529E-14C2-4E38-9AFA-476E7DDCA9C5@dcn.davis.CA.us>
Message-ID: <CAHLnndZ27a2ccK+WDuW8FxKjZDhHrYgH8UKagaCx8M0-xfgzDw@mail.gmail.com>

Hi Jeff,
  Thanks for replying. I actually tried "ordered(tmp$type, levels=c("c",
"b", "a")."
But I think only the order of the letters on x axis changed but the order
of the boxplot did not. So there is some problem there. I also tried
as.factor(tmp$type); levels(tmp$type)=c("c", "b", "a") and got the same
thing.
    Thanks.
      Li

2015-09-14 21:44 GMT-04:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:

> Make your factor variable deliberately. That is, specify the levels
> parameter with the values in order when you create the factor.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 14, 2015 6:23:50 PM PDT, li li <hannah.hlx at gmail.com> wrote:
> >Hi all,
> >  I have the following data "tmp" and I want to plot  boxplots for
> >each level of the factor "type" and the order the factor should be c,
> >b ,a. In other words, the boxplot corresponding to the level "c"
> >should be the first and so on.
> >Any suggestions?
> >   Li
> >
> >> tmp
> >   result type
> >1     101    a
> >2     101    a
> >3     101    a
> >4     101    a
> >5     101    a
> >6     101    a
> >7     100    a
> >8     106    b
> >9      91    b
> >10     78    b
> >11     95    b
> >12    111    b
> >13     92    b
> >14     98    b
> >15    108    c
> >16    112    c
> >17     98    c
> >18    102    c
> >19     88    c
> >20     86    c
> >21     81    c
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From kmwolf at ucdavis.edu  Tue Sep 15 05:26:00 2015
From: kmwolf at ucdavis.edu (Kristina Wolf)
Date: Mon, 14 Sep 2015 20:26:00 -0700
Subject: [R] Order of boxplots
In-Reply-To: <CAHLnndZ27a2ccK+WDuW8FxKjZDhHrYgH8UKagaCx8M0-xfgzDw@mail.gmail.com>
References: <CAHLnndbAB5FD5_geRcOu_1CWn3fjPXNz1Za9uQH-Yo709P5Lpw@mail.gmail.com>
	<C8E4529E-14C2-4E38-9AFA-476E7DDCA9C5@dcn.davis.CA.us>
	<CAHLnndZ27a2ccK+WDuW8FxKjZDhHrYgH8UKagaCx8M0-xfgzDw@mail.gmail.com>
Message-ID: <CAHq6VpfpOvQ_q5Ra4txp1tSGMwNV8wafyNvqPFA9z=giB7EqEA@mail.gmail.com>

Have you tried:

tmp$type= factor(tmp$type, levels = c("c", "b", "a"))

Then try your boxplots.


*?~ Kristina*

??
Kristina Wolf
?
?
Ph.D. Candidate, Graduate Group in Ecology
M.S. Soil Science
?,
?
B.S. Animal Science?
?
KristinaMWolf.com
Restoration Ecology Lab
?
Department of Plant Sciences
?
University of California, Davis?
?
(530) 750-9771

"We have to remember that what we observe is not nature herself, but nature
exposed to our method of questioning." ~ Werner Heisenberg


On Mon, Sep 14, 2015 at 7:31 PM, li li <hannah.hlx at gmail.com> wrote:

> Hi Jeff,
>   Thanks for replying. I actually tried "ordered(tmp$type, levels=c("c",
> "b", "a")."
> But I think only the order of the letters on x axis changed but the order
> of the boxplot did not. So there is some problem there. I also tried
> as.factor(tmp$type); levels(tmp$type)=c("c", "b", "a") and got the same
> thing.
>     Thanks.
>       Li
>
> 2015-09-14 21:44 GMT-04:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>
> > Make your factor variable deliberately. That is, specify the levels
> > parameter with the values in order when you create the factor.
> >
> ---------------------------------------------------------------------------
> > Jeff Newmiller                        The     .....       .....  Go
> Live...
> > DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> > Go...
> >                                       Live:   OO#.. Dead: OO#..  Playing
> > Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> > /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> >
> ---------------------------------------------------------------------------
> > Sent from my phone. Please excuse my brevity.
> >
> > On September 14, 2015 6:23:50 PM PDT, li li <hannah.hlx at gmail.com>
> wrote:
> > >Hi all,
> > >  I have the following data "tmp" and I want to plot  boxplots for
> > >each level of the factor "type" and the order the factor should be c,
> > >b ,a. In other words, the boxplot corresponding to the level "c"
> > >should be the first and so on.
> > >Any suggestions?
> > >   Li
> > >
> > >> tmp
> > >   result type
> > >1     101    a
> > >2     101    a
> > >3     101    a
> > >4     101    a
> > >5     101    a
> > >6     101    a
> > >7     100    a
> > >8     106    b
> > >9      91    b
> > >10     78    b
> > >11     95    b
> > >12    111    b
> > >13     92    b
> > >14     98    b
> > >15    108    c
> > >16    112    c
> > >17     98    c
> > >18    102    c
> > >19     88    c
> > >20     86    c
> > >21     81    c
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Tue Sep 15 08:50:14 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 15 Sep 2015 08:50:14 +0200
Subject: [R] Order of boxplots
In-Reply-To: <CAHLnndZ27a2ccK+WDuW8FxKjZDhHrYgH8UKagaCx8M0-xfgzDw@mail.gmail.com>
References: <CAHLnndbAB5FD5_geRcOu_1CWn3fjPXNz1Za9uQH-Yo709P5Lpw@mail.gmail.com>
	<C8E4529E-14C2-4E38-9AFA-476E7DDCA9C5@dcn.davis.CA.us>
	<CAHLnndZ27a2ccK+WDuW8FxKjZDhHrYgH8UKagaCx8M0-xfgzDw@mail.gmail.com>
Message-ID: <9AC3661A-07BC-4C8B-BBA7-8F3F68C1459C@gmail.com>


> On 15 Sep 2015, at 04:31 , li li <hannah.hlx at gmail.com> wrote:
> 
> Hi Jeff,
>  Thanks for replying. I actually tried "ordered(tmp$type, levels=c("c",
> "b", "a")."
> But I think only the order of the letters on x axis changed but the order

You _think_ ??? Documentation, please...

The boxplots certainly move if I do

plot(result ~ type, tmp)
plot(result ~ factor(type, levels=c("c","b","a")), tmp)



> of the boxplot did not. So there is some problem there. I also tried
> as.factor(tmp$type); levels(tmp$type)=c("c", "b", "a") and got the same

That changes the level _names_: 1st group name becomes "c" instead of "a"; you want the 3rd group to become the 1st but still be called "c".

-pd


> thing.
>    Thanks.
>      Li
> 
> 2015-09-14 21:44 GMT-04:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
> 
>> Make your factor variable deliberately. That is, specify the levels
>> parameter with the values in order when you create the factor.
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                      Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> ---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>> 
>> On September 14, 2015 6:23:50 PM PDT, li li <hannah.hlx at gmail.com> wrote:
>>> Hi all,
>>> I have the following data "tmp" and I want to plot  boxplots for
>>> each level of the factor "type" and the order the factor should be c,
>>> b ,a. In other words, the boxplot corresponding to the level "c"
>>> should be the first and so on.
>>> Any suggestions?
>>>  Li
>>> 
>>>> tmp
>>>  result type
>>> 1     101    a
>>> 2     101    a
>>> 3     101    a
>>> 4     101    a
>>> 5     101    a
>>> 6     101    a
>>> 7     100    a
>>> 8     106    b
>>> 9      91    b
>>> 10     78    b
>>> 11     95    b
>>> 12    111    b
>>> 13     92    b
>>> 14     98    b
>>> 15    108    c
>>> 16    112    c
>>> 17     98    c
>>> 18    102    c
>>> 19     88    c
>>> 20     86    c
>>> 21     81    c
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From laura.fernandezp at edu.uah.es  Tue Sep 15 10:21:19 2015
From: laura.fernandezp at edu.uah.es (laurafdez56)
Date: Tue, 15 Sep 2015 01:21:19 -0700 (PDT)
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <8FAD4A6E-B880-423F-ABE2-873104DF58A6@comcast.net>
References: <1442232687664-4712219.post@n4.nabble.com>
	<55F6E921.2060700@dewey.myzen.co.uk>
	<1442261917039-4712255.post@n4.nabble.com>
	<8FAD4A6E-B880-423F-ABE2-873104DF58A6@comcast.net>
Message-ID: <1442305279327-4712271.post@n4.nabble.com>

Hey David! 
This is my graphic.  sup.png
<http://r.789695.n4.nabble.com/file/n4712271/sup.png>  
I want only to change the line type (square line instead dotted line) in
order to distinguish the species. 

Thank you very much! 
Laura
 



--
View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712271.html
Sent from the R help mailing list archive at Nabble.com.


From maechler at stat.math.ethz.ch  Tue Sep 15 10:37:18 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 15 Sep 2015 10:37:18 +0200
Subject: [R] removing outlier --> use robust regression !
In-Reply-To: <1442050359278-4712170.post@n4.nabble.com>
References: <1441980906861-4712137.post@n4.nabble.com>
	<CA+8X3fUb90tmCQYG=8i+zTvj1RST7wQ6BkAUEqUG=prd+7B2cw@mail.gmail.com>
	<1442050359278-4712170.post@n4.nabble.com>
Message-ID: <22007.55486.840780.840328@stat.math.ethz.ch>

>>>>> Juli  <Julianeleuschner at web.de>
>>>>>     on Sat, 12 Sep 2015 02:32:39 -0700 writes:

     > Hi Jim, thank you for your help. :)

     > My point is, that there are outlier and I don?t really
     > know how to deal with that.

     > I need the dataframe for a regression and read often that
     > only a few outlier can change your results very much. In
     > addition, regression diacnostics didn?t indcate me the
     > best results.  Yes, and I know its not the core of
     > statistics to work in a way you get results you would
     > like to have ;).

     > So what is your suggestion?

Use robust regression, e.g.
    MASS::rlm()  {part of every R installation},
or a somewhat better and more sophisticated version.
    lmrob()  from package 'robustbase' {yes, shameless promotion}.

Further: 

1) Removing outliers is not at all the best way to deal with such
  problems (intuitively, because it is a *dis*continuous method).
  Rather they should be downweighted (continuously, as it
  happens with methods used in  rlm() or lmrob() see above)

2) Removing outliers in *multivariate* setting, if you want to do
   it in spite of 1)  by using univariate treatment {each column
   separately as you do here} is often completely insufficient.  E.g.
   the bivariate outlier  in
      xy <- cbind(x= c(2,1:9), y=c(8,1:9));  plot(xy)
   cannot be found by looking at 'x' and 'y' separately.
   
3) If, in spite of 1) and 2) you are considering univariate
   treatment, using mean() and sd() for detecting univariate outliers
   has been proven to be insufficient more than 50 years ago (*1), and
   if one looks closer into the literature (say "L_1") even
   considerably longer ago. 
   Using  median() and mad() instead, is one possibility (*2) of
   what you should do. Hampel's rule (*3)
   proposes declaring outliers for the observations outside
   the interval   median(x) +/- 3.5*mad(x)


*1 Tukey, J. W. (1960) A survey of sampling from contaminated distributions. 
   In Contributions to Probability and Statistics, 
   eds I. Olkin, S. Ghurye, W. Hoeffding, W. Madow and H. Mann,
   pp. 448?485. Stanford: Stanford University Press.

*2 Another (less robust, but still infinitely better than mean/sd) approach
   uses  median() and IQR() which is
   basically/approximately what boxplots do to identify outliers.


*3 Frank R. Hampel (1985)
   The Breakdown Points of the Mean Combined With Some Rejection Rules,
   Technometrics, 27:2, 95-107
	  [ http://dx.doi.org/10.1080/00401706.1985.10488027 ]

   See also section 
       "1.4b. How Well Are Objective and Subjective Metbods for
       the ReJection of Outliers Doing in the Context of Robust
       Estimation?",
    page 62 ff  od
  of
    Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw and Werner A. Stahel
    (1986) Robust Statistics: The Approach Based on Influence Functions.
    John Wiley & Sons, Inc.


From paumarc at gmail.com  Tue Sep 15 11:54:20 2015
From: paumarc at gmail.com (=?UTF-8?Q?Pau_Marc_Mu=C3=B1oz_Torres?=)
Date: Tue, 15 Sep 2015 11:54:20 +0200
Subject: [R] help with old libraries
Message-ID: <CADFuJLjr5jUv7v=0_4OTsvTuWHCjFJBXVtgwsLEd8F5pgCTmCA@mail.gmail.com>

Hello everybody,

 I want to use Rapidr package, it is an old package that uses the
package requires GenomicRanges version 1.14.4. The current version of the
package is GenomicRanges 1.20.6. There is some way of having both the
actual and the previous packages installed? I tried to install the package
locally, but I had problems with dependences, how can I deal with it? Would
be possible have both versions installed and choose which one to use?


Thanks in advance

Pau Marc Mu?oz Torres
skype: pau_marc
http://www.linkedin.com/in/paumarc
http://www.researchgate.net/profile/Pau_Marc_Torres3/info/

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Tue Sep 15 12:41:10 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 15 Sep 2015 22:41:10 +1200
Subject: [R] [FORGED]  help with old libraries
In-Reply-To: <CADFuJLjr5jUv7v=0_4OTsvTuWHCjFJBXVtgwsLEd8F5pgCTmCA@mail.gmail.com>
References: <CADFuJLjr5jUv7v=0_4OTsvTuWHCjFJBXVtgwsLEd8F5pgCTmCA@mail.gmail.com>
Message-ID: <55F7F5C6.7000209@auckland.ac.nz>

On 15/09/15 21:54, Pau Marc Mu?oz Torres wrote:
> Hello everybody,
>
>   I want to use Rapidr package, it is an old package that uses the
> package requires GenomicRanges version 1.14.4. The current version of the
> package is GenomicRanges 1.20.6. There is some way of having both the
> actual and the previous packages installed? I tried to install the package
> locally, but I had problems with dependences, how can I deal with it? Would
> be possible have both versions installed and choose which one to use?

To some extent you can do this by having different versions of packages 
installed in different libraries (directories), say "Lib1" and "Lib2", 
and then issuing commands of the form

    library(foo,lib.loc="Lib1")

or

    library(foo,lib.loc="Lib2")

However having different dependencies might mess up this strategy.  I 
dunno.  Perhaps younger and wiser heads will contribute some more 
definitive advice.

Also my experience with this strategy is under a Linux OS.  If you are 
running Windoze (as you probably are --- you didn't say) who knows what 
will go wrong?

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Tue Sep 15 12:45:21 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 15 Sep 2015 22:45:21 +1200
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <1442305279327-4712271.post@n4.nabble.com>
References: <1442232687664-4712219.post@n4.nabble.com>
	<55F6E921.2060700@dewey.myzen.co.uk>
	<1442261917039-4712255.post@n4.nabble.com>
	<8FAD4A6E-B880-423F-ABE2-873104DF58A6@comcast.net>
	<1442305279327-4712271.post@n4.nabble.com>
Message-ID: <55F7F6C1.5090402@auckland.ac.nz>

On 15/09/15 20:21, laurafdez56 wrote:
> Hey David!
> This is my graphic.  sup.png
> <http://r.789695.n4.nabble.com/file/n4712271/sup.png>
> I want only to change the line type (square line instead dotted line) in
> order to distinguish the species.

What *on earth* do you mean by a "square line"?

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From chienpang.c at gmail.com  Tue Sep 15 12:58:25 2015
From: chienpang.c at gmail.com (Chien-Pang Chin)
Date: Tue, 15 Sep 2015 18:58:25 +0800
Subject: [R] Beta distribution approximate to Normal distribution
Message-ID: <1E00A389-8F4C-4E0C-9FBC-791067B93B8A@gmail.com>

Hi,

I need to generate 1000 numbers from N(u, a^2), however I don't want to include 0 and negative values. How can I use beta distribution approximate to N(u, a^2) in R.

Thx for help


From Martin.Morgan at roswellpark.org  Tue Sep 15 13:52:41 2015
From: Martin.Morgan at roswellpark.org (Morgan, Martin)
Date: Tue, 15 Sep 2015 11:52:41 +0000
Subject: [R] help with old libraries
In-Reply-To: <CADFuJLjr5jUv7v=0_4OTsvTuWHCjFJBXVtgwsLEd8F5pgCTmCA@mail.gmail.com>
References: <CADFuJLjr5jUv7v=0_4OTsvTuWHCjFJBXVtgwsLEd8F5pgCTmCA@mail.gmail.com>
Message-ID: <DF23DAC5A53912408040FF04D8B780AADE894E@EXMB3RSC.roswellpark.org>

GenomicRanges is a Bioconductor package.

Bioconductor packages are associated with R versions.

GenomicRanges 1.14.4 is from Bioconductor 2.13 / R version 3.0. This is from 2013, which is Quite A Long Time Ago. 

The 'landing page' for this version of GenomicRanges is at
	
  http://bioconductor.org/packages/2.13/bioc/html/GenomicRanges.html

Install R 3.0, then use biocLite() as described on the landing page; it will install the correct version of GenomicRanges in the R-3.0 directory.

Install RapidR with R-3.0.

The R-3.0 installation will not interfere with a current R installation.

Martin Morgan

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Pau Marc Mu?oz Torres
Sent: Tuesday, September 15, 2015 5:54 AM
To: r-help at r-project.org
Subject: [R] help with old libraries

Hello everybody,

 I want to use Rapidr package, it is an old package that uses the package requires GenomicRanges version 1.14.4. The current version of the package is GenomicRanges 1.20.6. There is some way of having both the actual and the previous packages installed? I tried to install the package locally, but I had problems with dependences, how can I deal with it? Would be possible have both versions installed and choose which one to use?


Thanks in advance

Pau Marc Mu?oz Torres
skype: pau_marc
http://www.linkedin.com/in/paumarc
http://www.researchgate.net/profile/Pau_Marc_Torres3/info/

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


This email message may contain legally privileged and/or confidential information.  If you are not the intended recipient(s), or the employee or agent responsible for the delivery of this message to the intended recipient(s), you are hereby notified that any disclosure, copying, distribution, or use of this email message is prohibited.  If you have received this message in error, please notify the sender immediately by e-mail and delete this email message from your computer. Thank you.

From paumarc at gmail.com  Tue Sep 15 13:57:54 2015
From: paumarc at gmail.com (=?UTF-8?Q?Pau_Marc_Mu=C3=B1oz_Torres?=)
Date: Tue, 15 Sep 2015 13:57:54 +0200
Subject: [R] help with old libraries
In-Reply-To: <DF23DAC5A53912408040FF04D8B780AADE894E@EXMB3RSC.roswellpark.org>
References: <CADFuJLjr5jUv7v=0_4OTsvTuWHCjFJBXVtgwsLEd8F5pgCTmCA@mail.gmail.com>
	<DF23DAC5A53912408040FF04D8B780AADE894E@EXMB3RSC.roswellpark.org>
Message-ID: <CADFuJLi70MdP=w3cQNDeeBjEbqENLiikFELCJ2sCTxsqLxEWLQ@mail.gmail.com>

Great martin,

 i just was going to ask to bioconductor guys!

thanks!

Pau Marc Mu?oz Torres
skype: pau_marc
http://www.linkedin.com/in/paumarc
http://www.researchgate.net/profile/Pau_Marc_Torres3/info/


2015-09-15 13:52 GMT+02:00 Morgan, Martin <Martin.Morgan at roswellpark.org>:

> GenomicRanges is a Bioconductor package.
>
> Bioconductor packages are associated with R versions.
>
> GenomicRanges 1.14.4 is from Bioconductor 2.13 / R version 3.0. This is
> from 2013, which is Quite A Long Time Ago.
>
> The 'landing page' for this version of GenomicRanges is at
>
>   http://bioconductor.org/packages/2.13/bioc/html/GenomicRanges.html
>
> Install R 3.0, then use biocLite() as described on the landing page; it
> will install the correct version of GenomicRanges in the R-3.0 directory.
>
> Install RapidR with R-3.0.
>
> The R-3.0 installation will not interfere with a current R installation.
>
> Martin Morgan
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Pau Marc
> Mu?oz Torres
> Sent: Tuesday, September 15, 2015 5:54 AM
> To: r-help at r-project.org
> Subject: [R] help with old libraries
>
> Hello everybody,
>
>  I want to use Rapidr package, it is an old package that uses the package
> requires GenomicRanges version 1.14.4. The current version of the package
> is GenomicRanges 1.20.6. There is some way of having both the actual and
> the previous packages installed? I tried to install the package locally,
> but I had problems with dependences, how can I deal with it? Would be
> possible have both versions installed and choose which one to use?
>
>
> Thanks in advance
>
> Pau Marc Mu?oz Torres
> skype: pau_marc
> http://www.linkedin.com/in/paumarc
> http://www.researchgate.net/profile/Pau_Marc_Torres3/info/
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> This email message may contain legally privileged and/or confidential
> information.  If you are not the intended recipient(s), or the employee or
> agent responsible for the delivery of this message to the intended
> recipient(s), you are hereby notified that any disclosure, copying,
> distribution, or use of this email message is prohibited.  If you have
> received this message in error, please notify the sender immediately by
> e-mail and delete this email message from your computer. Thank you.

	[[alternative HTML version deleted]]


From JLucke at ria.buffalo.edu  Tue Sep 15 15:26:44 2015
From: JLucke at ria.buffalo.edu (JLucke at ria.buffalo.edu)
Date: Tue, 15 Sep 2015 09:26:44 -0400
Subject: [R] Beta distribution approximate to Normal distribution
In-Reply-To: <1E00A389-8F4C-4E0C-9FBC-791067B93B8A@gmail.com>
References: <1E00A389-8F4C-4E0C-9FBC-791067B93B8A@gmail.com>
Message-ID: <OF9A9429CA.A4DC186D-ON85257EC1.00491942-85257EC1.0049DB30@ria.buffalo.edu>

Your question makes no sense as stated.  However, guessing at what you 
want, you should  perhaps consider the non-central chi-square density with 
1 df and ncp = u/a, i.e,

rchisq(100, df=1, ncp=u/a)

Joe



Joseph F. Lucke, PhD
Senior Statistician
Research Institute on Addictions
University at Buffalo
State University of New York
1021 Main Street
Buffalo, NY  14203-1016






Chien-Pang Chin <chienpang.c at gmail.com> 
Sent by: "R-help" <r-help-bounces at r-project.org>
09/15/2015 06:58 AM

To
"r-help at r-project.org" <r-help at r-project.org>, 
cc

Subject
[R] Beta distribution approximate to Normal distribution






Hi,

I need to generate 1000 numbers from N(u, a^2), however I don't want to 
include 0 and negative values. How can I use beta distribution approximate 
to N(u, a^2) in R.

Thx for help

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Tue Sep 15 15:50:52 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 15 Sep 2015 15:50:52 +0200
Subject: [R] Beta distribution approximate to Normal distribution
In-Reply-To: <OF9A9429CA.A4DC186D-ON85257EC1.00491942-85257EC1.0049DB30@ria.buffalo.edu>
References: <1E00A389-8F4C-4E0C-9FBC-791067B93B8A@gmail.com>
	<OF9A9429CA.A4DC186D-ON85257EC1.00491942-85257EC1.0049DB30@ria.buffalo.edu>
Message-ID: <F743CE82-1A3C-472E-9AB4-7569C09DA8AE@gmail.com>


On 15 Sep 2015, at 15:26 , JLucke at ria.buffalo.edu wrote:

> Your question makes no sense as stated.  However, guessing at what you 
> want, you should  perhaps consider the non-central chi-square density with 
> 1 df and ncp = u/a, i.e,
> 
> rchisq(100, df=1, ncp=u/a)

Something's not right with that. Noncentral chisquare has mean df+ncp and variance 2*df + 4*ncp, which doesn't work out as anything like u and a^2. 

Other possibilities include gamma and lognormal distributions.

-pd

> 
> Joe
> 
> 
> 
> Joseph F. Lucke, PhD
> Senior Statistician
> Research Institute on Addictions
> University at Buffalo
> State University of New York
> 1021 Main Street
> Buffalo, NY  14203-1016
> 
> 
> 
> 
> 
> 
> Chien-Pang Chin <chienpang.c at gmail.com> 
> Sent by: "R-help" <r-help-bounces at r-project.org>
> 09/15/2015 06:58 AM
> 
> To
> "r-help at r-project.org" <r-help at r-project.org>, 
> cc
> 
> Subject
> [R] Beta distribution approximate to Normal distribution
> 
> 
> 
> 
> 
> 
> Hi,
> 
> I need to generate 1000 numbers from N(u, a^2), however I don't want to 
> include 0 and negative values. How can I use beta distribution approximate 
> to N(u, a^2) in R.
> 
> Thx for help
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jrkrideau at inbox.com  Tue Sep 15 16:32:26 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 15 Sep 2015 06:32:26 -0800
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <55F7F6C1.5090402@auckland.ac.nz>
References: <1442261917039-4712255.post@n4.nabble.com>
	<55f6e921.2060700@dewey.myzen.co.uk>
	<8fad4a6e-b880-423f-abe2-873104df58a6@comcast.net>
	<1442232687664-4712219.post@n4.nabble.com>
	<1442305279327-4712271.post@n4.nabble.com>
Message-ID: <D976C060776.0000012Fjrkrideau@inbox.com>

Hi Rolf,

I think Laura wants something like this for one of the curves, although I'm guessing.

aa  <-  1:100
plot(aa, pch = 22)

John Kane
Kingston ON Canada


> -----Original Message-----
> From: r.turner at auckland.ac.nz
> Sent: Tue, 15 Sep 2015 22:45:21 +1200
> To: laura.fernandezp at edu.uah.es, r-help at r-project.org
> Subject: Re: [R] R lines type in a glm/predict model
> 
> On 15/09/15 20:21, laurafdez56 wrote:
>> Hey David!
>> This is my graphic.  sup.png
>> <http://r.789695.n4.nabble.com/file/n4712271/sup.png>
>> I want only to change the line type (square line instead dotted line) in
>> order to distinguish the species.
> 
> What *on earth* do you mean by a "square line"?
> 
> cheers,
> 
> Rolf Turner
> 
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From Ted.Harding at wlandres.net  Tue Sep 15 17:12:39 2015
From: Ted.Harding at wlandres.net ( (Ted Harding))
Date: Tue, 15 Sep 2015 16:12:39 +0100 (BST)
Subject: [R] Beta distribution approximate to Normal distribution
In-Reply-To: <OF9A9429CA.A4DC186D-ON85257EC1.00491942-85257EC1.0049DB30@ria.buffalo.edu>
Message-ID: <XFMail.20150915161239.Ted.Harding@wlandres.net>

Using non-central chi-squared (especially with df=1) is unlikely
to generate random numbers anywhere near a Normal distribution
(see below).

And "rchisq(100, df=1, ncp=u/a)" won't work anyway with u<0,
since ncp must be >= 0 (if < 0 then all are NA).

Better to shoot straight for the target (truncated Normal), though
several shots are likely to be required! For example (code which
spells it out), taking u=3 and a=2:

  n <- 100
  u <- 3 ; a <- 2
  x <- NULL
  N <- length(x)
  while(N < n){
    x <- c(x,rnorm(n,mean=u,sd=a))
    x <- x[x>0]
    N <- length(x)
  }
  x <- x[1:n]

Comparison with non-central chi-squared:

  y <- rchisq(100, df=1, ncp=u/a)
  hist(x)
  hist(y)



On 15-Sep-2015 13:26:44 JLucke at ria.buffalo.edu wrote:
> Your question makes no sense as stated.  However, guessing at what you 
> want, you should  perhaps consider the non-central chi-square density with 
> 1 df and ncp = u/a, i.e,
> 
> rchisq(100, df=1, ncp=u/a)
> 
> Joe
> Joseph F. Lucke, PhD
> Senior Statistician
> Research Institute on Addictions
> University at Buffalo
> State University of New York
> 1021 Main Street
> Buffalo, NY  14203-1016
> 
> Chien-Pang Chin <chienpang.c at gmail.com> 
> Sent by: "R-help" <r-help-bounces at r-project.org>
> 09/15/2015 06:58 AM
> 
> To
> "r-help at r-project.org" <r-help at r-project.org>, 
> 
> Subject
> [R] Beta distribution approximate to Normal distribution
> 
> Hi,
> I need to generate 1000 numbers from N(u, a^2), however I don't
> want to include 0 and negative values. How can I use beta distribution
> approximate to N(u, a^2) in R.
> 
> Thx for help

-------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
Date: 15-Sep-2015  Time: 16:12:35
This message was sent by XFMail


From JLucke at ria.buffalo.edu  Tue Sep 15 18:15:23 2015
From: JLucke at ria.buffalo.edu (JLucke at ria.buffalo.edu)
Date: Tue, 15 Sep 2015 12:15:23 -0400
Subject: [R] Beta distribution approximate to Normal distribution
In-Reply-To: <XFMail.20150915161239.Ted.Harding@wlandres.net>
References: <OF9A9429CA.A4DC186D-ON85257EC1.00491942-85257EC1.0049DB30@ria.buffalo.edu>
	<XFMail.20150915161239.Ted.Harding@wlandres.net>
Message-ID: <OF7FF1A210.6F7BC641-ON85257EC1.005872F0-85257EC1.00594C02@ria.buffalo.edu>

Scratch the  rchisq  (it should have been sqrt(rchisq), but that doesn't 
help.). 

Use the truncated normal

u <- 3; a <- 2;
N <- 100 
x <- numeric(N)
for (i in 1:N){
  repeat{
    if( (x[i] <- rnorm(1, u, a)) >= 0 ) break
  }
}
 
or the folded normal

abs(rnorm(N, u, a)),

They give similar results. 
The  code for the truncated normal allows you to set any truncation point.

Joe



(Ted Harding) <Ted.Harding at wlandres.net> 
Sent by: "R-help" <r-help-bounces at r-project.org>
09/15/2015 11:12 AM
Please respond to
Ted.Harding at wlandres.net


To
"r-help at r-project.org" <r-help at r-project.org>, 
cc
Chien-Pang Chin <chienpang.c at gmail.com>
Subject
Re: [R] Beta distribution approximate to Normal distribution






Using non-central chi-squared (especially with df=1) is unlikely
to generate random numbers anywhere near a Normal distribution
(see below).

And "rchisq(100, df=1, ncp=u/a)" won't work anyway with u<0,
since ncp must be >= 0 (if < 0 then all are NA).

Better to shoot straight for the target (truncated Normal), though
several shots are likely to be required! For example (code which
spells it out), taking u=3 and a=2:

  n <- 100
  u <- 3 ; a <- 2
  x <- NULL
  N <- length(x)
  while(N < n){
    x <- c(x,rnorm(n,mean=u,sd=a))
    x <- x[x>0]
    N <- length(x)
  }
  x <- x[1:n]

Comparison with non-central chi-squared:

  y <- rchisq(100, df=1, ncp=u/a)
  hist(x)
  hist(y)



On 15-Sep-2015 13:26:44 JLucke at ria.buffalo.edu wrote:
> Your question makes no sense as stated.  However, guessing at what you 
> want, you should  perhaps consider the non-central chi-square density 
with 
> 1 df and ncp = u/a, i.e,
> 
> rchisq(100, df=1, ncp=u/a)
> 
> Joe
> Joseph F. Lucke, PhD
> Senior Statistician
> Research Institute on Addictions
> University at Buffalo
> State University of New York
> 1021 Main Street
> Buffalo, NY  14203-1016
> 
> Chien-Pang Chin <chienpang.c at gmail.com> 
> Sent by: "R-help" <r-help-bounces at r-project.org>
> 09/15/2015 06:58 AM
> 
> To
> "r-help at r-project.org" <r-help at r-project.org>, 
> 
> Subject
> [R] Beta distribution approximate to Normal distribution
> 
> Hi,
> I need to generate 1000 numbers from N(u, a^2), however I don't
> want to include 0 and negative values. How can I use beta distribution
> approximate to N(u, a^2) in R.
> 
> Thx for help

-------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
Date: 15-Sep-2015  Time: 16:12:35
This message was sent by XFMail

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Tue Sep 15 19:24:21 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 15 Sep 2015 18:24:21 +0100
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <1442305279327-4712271.post@n4.nabble.com>
References: <1442232687664-4712219.post@n4.nabble.com>
	<55F6E921.2060700@dewey.myzen.co.uk>
	<1442261917039-4712255.post@n4.nabble.com>
	<8FAD4A6E-B880-423F-ABE2-873104DF58A6@comcast.net>
	<1442305279327-4712271.post@n4.nabble.com>
Message-ID: <55F85445.2080405@dewey.myzen.co.uk>

Dear Laura

As you can see you have us all very puzzled. I think something is 
getting lost in translation between the language of Cervantes and the 
language of Shakespeare.

Perhaps
1 - send a picture of a square line
2 - tell us what square line is in Spanish
3 - find the Spanish language R help list and try there

Obviously do not try all three at once

On 15/09/2015 09:21, laurafdez56 wrote:
> Hey David!
> This is my graphic.  sup.png
> <http://r.789695.n4.nabble.com/file/n4712271/sup.png>
> I want only to change the line type (square line instead dotted line) in
> order to distinguish the species.
>
> Thank you very much!
> Laura
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712271.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From oluola2011 at yahoo.com  Tue Sep 15 19:34:14 2015
From: oluola2011 at yahoo.com (Olu Ola)
Date: Tue, 15 Sep 2015 10:34:14 -0700
Subject: [R] Drop in a Loop
Message-ID: <1442338454.1159.YahooMailBasic@web161602.mail.bf1.yahoo.com>

Hello,
I am doing some estimation using optimx and after each round of estimation, I store the coefficient. However, I need to drop the set of coefficients for which the convergence code in optimx is GREATER than Zero. How do I go about this?

A way forward will be highly appreciated.

Thank you


From ruipbarradas at sapo.pt  Tue Sep 15 19:40:38 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Tue, 15 Sep 2015 18:40:38 +0100
Subject: [R] Beta distribution approximate to Normal distribution
In-Reply-To: <XFMail.20150915161239.Ted.Harding@wlandres.net>
References: <XFMail.20150915161239.Ted.Harding@wlandres.net>
Message-ID: <55F85816.3020207@sapo.pt>

Hello,

If you want a truncated something rng, you can use the following 
function. Note that 'distr' is the name of an R distribution function 
without the dpqr prefix.

rtrunc <- function(n, distr, lower = -Inf, upper = Inf, ...){
	makefun <- function(prefix, FUN, ...){
		txt <- paste(prefix, FUN, "(x, ...)", sep = "")
		function(x, ...) eval(parse(text = txt))
	}
	if(length(n) > 1) n <- length(n)
	pfun <- makefun("p", distr, ...)
	qfun <- makefun("q", distr, ...)
	lo <- pfun(lower, ...)
	up <- pfun(upper, ...)
	u <- runif(n, lo, up)
	qfun(u, ...)
}

u <- 2; a <- 3
x <- rtrunc(1000, "norm", lower = 0, mean = u, sd = a)
hist(x)


Hope this helps,

Rui Barradas

Em 15-09-2015 16:12, (Ted Harding) escreveu:
> Using non-central chi-squared (especially with df=1) is unlikely
> to generate random numbers anywhere near a Normal distribution
> (see below).
>
> And "rchisq(100, df=1, ncp=u/a)" won't work anyway with u<0,
> since ncp must be >= 0 (if < 0 then all are NA).
>
> Better to shoot straight for the target (truncated Normal), though
> several shots are likely to be required! For example (code which
> spells it out), taking u=3 and a=2:
>
>    n <- 100
>    u <- 3 ; a <- 2
>    x <- NULL
>    N <- length(x)
>    while(N < n){
>      x <- c(x,rnorm(n,mean=u,sd=a))
>      x <- x[x>0]
>      N <- length(x)
>    }
>    x <- x[1:n]
>
> Comparison with non-central chi-squared:
>
>    y <- rchisq(100, df=1, ncp=u/a)
>    hist(x)
>    hist(y)
>
>
>
> On 15-Sep-2015 13:26:44 JLucke at ria.buffalo.edu wrote:
>> Your question makes no sense as stated.  However, guessing at what you
>> want, you should  perhaps consider the non-central chi-square density with
>> 1 df and ncp = u/a, i.e,
>>
>> rchisq(100, df=1, ncp=u/a)
>>
>> Joe
>> Joseph F. Lucke, PhD
>> Senior Statistician
>> Research Institute on Addictions
>> University at Buffalo
>> State University of New York
>> 1021 Main Street
>> Buffalo, NY  14203-1016
>>
>> Chien-Pang Chin <chienpang.c at gmail.com>
>> Sent by: "R-help" <r-help-bounces at r-project.org>
>> 09/15/2015 06:58 AM
>>
>> To
>> "r-help at r-project.org" <r-help at r-project.org>,
>>
>> Subject
>> [R] Beta distribution approximate to Normal distribution
>>
>> Hi,
>> I need to generate 1000 numbers from N(u, a^2), however I don't
>> want to include 0 and negative values. How can I use beta distribution
>> approximate to N(u, a^2) in R.
>>
>> Thx for help
>
> -------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
> Date: 15-Sep-2015  Time: 16:12:35
> This message was sent by XFMail
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dcarlson at tamu.edu  Tue Sep 15 20:12:37 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 15 Sep 2015 18:12:37 +0000
Subject: [R] Beta distribution approximate to Normal distribution
In-Reply-To: <55F85816.3020207@sapo.pt>
References: <XFMail.20150915161239.Ted.Harding@wlandres.net>
	<55F85816.3020207@sapo.pt>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C212F@mb02.ads.tamu.edu>

There are also truncated normal distributions in packages truncnorm, crch, and msm. Also a multivariate truncated normal distribution in package tmvtnorm.

e.g.

library(truncnorm)
x <- rtruncnorm(n, a=0, mean=u, sd=a)
hist(x)

library(msm)
x <- rtnorm(n, mean=u, sd=a, lower=0)
hist(x)

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Rui Barradas
Sent: Tuesday, September 15, 2015 12:41 PM
To: Ted.Harding at wlandres.net; r-help at r-project.org
Cc: Chien-Pang Chin
Subject: Re: [R] Beta distribution approximate to Normal distribution

Hello,

If you want a truncated something rng, you can use the following 
function. Note that 'distr' is the name of an R distribution function 
without the dpqr prefix.

rtrunc <- function(n, distr, lower = -Inf, upper = Inf, ...){
	makefun <- function(prefix, FUN, ...){
		txt <- paste(prefix, FUN, "(x, ...)", sep = "")
		function(x, ...) eval(parse(text = txt))
	}
	if(length(n) > 1) n <- length(n)
	pfun <- makefun("p", distr, ...)
	qfun <- makefun("q", distr, ...)
	lo <- pfun(lower, ...)
	up <- pfun(upper, ...)
	u <- runif(n, lo, up)
	qfun(u, ...)
}

u <- 2; a <- 3
x <- rtrunc(1000, "norm", lower = 0, mean = u, sd = a)
hist(x)


Hope this helps,

Rui Barradas

Em 15-09-2015 16:12, (Ted Harding) escreveu:
> Using non-central chi-squared (especially with df=1) is unlikely
> to generate random numbers anywhere near a Normal distribution
> (see below).
>
> And "rchisq(100, df=1, ncp=u/a)" won't work anyway with u<0,
> since ncp must be >= 0 (if < 0 then all are NA).
>
> Better to shoot straight for the target (truncated Normal), though
> several shots are likely to be required! For example (code which
> spells it out), taking u=3 and a=2:
>
>    n <- 100
>    u <- 3 ; a <- 2
>    x <- NULL
>    N <- length(x)
>    while(N < n){
>      x <- c(x,rnorm(n,mean=u,sd=a))
>      x <- x[x>0]
>      N <- length(x)
>    }
>    x <- x[1:n]
>
> Comparison with non-central chi-squared:
>
>    y <- rchisq(100, df=1, ncp=u/a)
>    hist(x)
>    hist(y)
>
>
>
> On 15-Sep-2015 13:26:44 JLucke at ria.buffalo.edu wrote:
>> Your question makes no sense as stated.  However, guessing at what you
>> want, you should  perhaps consider the non-central chi-square density with
>> 1 df and ncp = u/a, i.e,
>>
>> rchisq(100, df=1, ncp=u/a)
>>
>> Joe
>> Joseph F. Lucke, PhD
>> Senior Statistician
>> Research Institute on Addictions
>> University at Buffalo
>> State University of New York
>> 1021 Main Street
>> Buffalo, NY  14203-1016
>>
>> Chien-Pang Chin <chienpang.c at gmail.com>
>> Sent by: "R-help" <r-help-bounces at r-project.org>
>> 09/15/2015 06:58 AM
>>
>> To
>> "r-help at r-project.org" <r-help at r-project.org>,
>>
>> Subject
>> [R] Beta distribution approximate to Normal distribution
>>
>> Hi,
>> I need to generate 1000 numbers from N(u, a^2), however I don't
>> want to include 0 and negative values. How can I use beta distribution
>> approximate to N(u, a^2) in R.
>>
>> Thx for help
>
> -------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
> Date: 15-Sep-2015  Time: 16:12:35
> This message was sent by XFMail
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wjhopper510 at gmail.com  Tue Sep 15 20:30:27 2015
From: wjhopper510 at gmail.com (Will Hopper)
Date: Tue, 15 Sep 2015 14:30:27 -0400
Subject: [R] Drop in a Loop
In-Reply-To: <1442338454.1159.YahooMailBasic@web161602.mail.bf1.yahoo.com>
References: <1442338454.1159.YahooMailBasic@web161602.mail.bf1.yahoo.com>
Message-ID: <CAGTXQPuVeaJxXXZvF_d+aOYiUPaYUOpXb+VFFDicPTxe2aqt3A@mail.gmail.com>

I think you ought to show a small example of how the code you're using. Are
you saving results at every iteration? In a list, data frame, etc? People
likely need that to help answer your question.

 Also probably have a look the control list argument and the save.failures
option, that might be something you're interested in.

- Will

On Tue, Sep 15, 2015 at 1:34 PM, Olu Ola via R-help <r-help at r-project.org>
wrote:

> Hello,
> I am doing some estimation using optimx and after each round of
> estimation, I store the coefficient. However, I need to drop the set of
> coefficients for which the convergence code in optimx is GREATER than Zero.
> How do I go about this?
>
> A way forward will be highly appreciated.
>
> Thank you
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From A.Martinovici at uvt.nl  Tue Sep 15 14:33:31 2015
From: A.Martinovici at uvt.nl (A. Martinovici)
Date: Tue, 15 Sep 2015 12:33:31 +0000
Subject: [R]  Difference MNP-package and rmnpGibbs from bayesm-package
Message-ID: <97E18D44BD245B4D9B00CBB2AC8E120E036A4475@exmbx001.campus.uvt.nl>

Hi,

The first thing to check is if both functions use the same 'base' option - they usually don't.
On a more general note, the two approaches use a different data augmentation procedure, but this should not have a very large impact on the results provided enough draws and similar priors are used. If you want to go more into detail on what are the exact differences, the Imai and van Dyk (2005) paper is very useful.

Best regards,
Ana

	[[alternative HTML version deleted]]


From mlathouri at yahoo.gr  Tue Sep 15 12:56:37 2015
From: mlathouri at yahoo.gr (Maria Lathouri)
Date: Tue, 15 Sep 2015 10:56:37 +0000 (UTC)
Subject: [R] Multiple if function
Message-ID: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>

Dear all, 

I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
ASBclass??? Flow1????????????? 11.51?????????????? 9.2
2????????????? 10.5
3?????????????? 6.7??...????????????? ...
I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.

If (ASBclass=1) { deviation<-Flow*0.1}
If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2}
I am not sure whether I should add the else function and how can I combine these separate functions.

Can anyone help me on that?
Thank you very much. 

Kind regardsMaria

	[[alternative HTML version deleted]]


From oluola2011 at yahoo.com  Tue Sep 15 21:12:13 2015
From: oluola2011 at yahoo.com (Olu Ola)
Date: Tue, 15 Sep 2015 12:12:13 -0700
Subject: [R] Drop in a Loop
In-Reply-To: <CAGTXQPuVeaJxXXZvF_d+aOYiUPaYUOpXb+VFFDicPTxe2aqt3A@mail.gmail.com>
Message-ID: <1442344333.30741.YahooMailBasic@web161606.mail.bf1.yahoo.com>

Thanks Will.
 Below is the flow of my code

Yhat is the fitted value
Errhat is the difference between the dependent variable and the yhat
gmmdata is the data name
 
N <- nrow(gmmdata)
B <- 1000
store <- matrix(0,B,11)
for (j in 1:B) {
  index = sample(1:N, N, replace=T)
  errnew = errhat[index]
  yt = yhat + errnew
objective function subroutine
gradient function subroutine  
gmmiv =Optimx() 
  store[j,] = coef(gmmiv)
}

What I want to do is that if the convergence code from optimx for a particular iteration is Not zero, then it should not be stored in store[j,].

Any help will be appreciated

Thank you






--------------------------------------------
On Tue, 9/15/15, Will Hopper <wjhopper510 at gmail.com> wrote:

 Subject: Re: [R] Drop in a Loop

 Cc: r-help at r-project.org
 Date: Tuesday, September 15, 2015, 2:30 PM

 I
 think you ought to show a small example of how the code
 you're using. Are you saving results at every iteration?
 In a list, data frame, etc? People likely need that to help
 answer your question.

 ?Also probably have a look the control list
 argument and the save.failures option, that might be
 something you're interested in. 

 - Will 

 On Tue, Sep 15, 2015 at
 1:34 PM, Olu Ola via R-help <r-help at r-project.org>
 wrote:
 Hello,

 I am doing some estimation using optimx and after each round
 of estimation, I store the coefficient. However, I need to
 drop the set of coefficients for which the convergence code
 in optimx is GREATER than Zero. How do I go about this?



 A way forward will be highly appreciated.



 Thank you



 ______________________________________________

 R-help at r-project.org
 mailing list -- To UNSUBSCRIBE and more, see

 https://stat.ethz.ch/mailman/listinfo/r-help

 PLEASE do read the posting guide http://www.R-project.org/posting-guide.html

 and provide commented, minimal, self-contained, reproducible
 code.


From bgunter.4567 at gmail.com  Tue Sep 15 21:16:26 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 15 Sep 2015 12:16:26 -0700
Subject: [R] Multiple if function
In-Reply-To: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAGxFJbQL4W+=Q_w4jvZY6MxUzJQY62KQ+L7jOORhFSbTtTQRZg@mail.gmail.com>

Maria:

Have you read An Intro to R or other R tutorial? There are many on the
web and this is a basic idea that they would explain (with examples).

?ifelse  ##a vectorized kind of if conditional

is one way to do this (though it can get a little convoluted). There
are others (e.g. splitting and recombining -- see the plyr package),
which others may suggest. Nevertheless, time spent with a tutorial
would be useful.

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 15, 2015 at 3:56 AM, Maria Lathouri <mlathouri at yahoo.gr> wrote:
> Dear all,
>
> I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
> ASBclass    Flow1              11.51               9.2
> 2              10.5
> 3               6.7  ...              ...
> I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.
>
> If (ASBclass=1) { deviation<-Flow*0.1}
> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2}
> I am not sure whether I should add the else function and how can I combine these separate functions.
>
> Can anyone help me on that?
> Thank you very much.
>
> Kind regardsMaria
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Peter.Alspach at plantandfood.co.nz  Tue Sep 15 21:23:02 2015
From: Peter.Alspach at plantandfood.co.nz (Peter Alspach)
Date: Wed, 16 Sep 2015 07:23:02 +1200
Subject: [R] Multiple if function
In-Reply-To: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>

Tena koe Maria

It seems you need to multiply Flow by 0.05+ASBClass/20 (i.e., no if calls are necessary)

HTH ....

Peter Alspach

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Maria Lathouri
Sent: Tuesday, 15 September 2015 10:57 p.m.
To: r-help at r-project.org
Subject: [R] Multiple if function

Dear all, 

I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
ASBclass??? Flow1????????????? 11.51?????????????? 9.2
2????????????? 10.5
3?????????????? 6.7??...????????????? ...
I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.

If (ASBclass=1) { deviation<-Flow*0.1}
If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2} I am not sure whether I should add the else function and how can I combine these separate functions.

Can anyone help me on that?
Thank you very much. 

Kind regardsMaria

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

The contents of this e-mail are confidential and may be subject to legal privilege.
 If you are not the intended recipient you must not use, disseminate, distribute or
 reproduce all or any part of this e-mail or attachments.  If you have received this
 e-mail in error, please notify the sender and delete all material pertaining to this
 e-mail.  Any opinion or views expressed in this e-mail are those of the individual
 sender and may not represent those of The New Zealand Institute for Plant and
 Food Research Limited.

From hannah.hlx at gmail.com  Tue Sep 15 22:04:55 2015
From: hannah.hlx at gmail.com (li li)
Date: Tue, 15 Sep 2015 16:04:55 -0400
Subject: [R] Order of boxplots
In-Reply-To: <9AC3661A-07BC-4C8B-BBA7-8F3F68C1459C@gmail.com>
References: <CAHLnndbAB5FD5_geRcOu_1CWn3fjPXNz1Za9uQH-Yo709P5Lpw@mail.gmail.com>
	<C8E4529E-14C2-4E38-9AFA-476E7DDCA9C5@dcn.davis.CA.us>
	<CAHLnndZ27a2ccK+WDuW8FxKjZDhHrYgH8UKagaCx8M0-xfgzDw@mail.gmail.com>
	<9AC3661A-07BC-4C8B-BBA7-8F3F68C1459C@gmail.com>
Message-ID: <CAHLnndYYTmrkx3wNbRX5AEUQ-P9ydja58-m5A2KTWXk-X+6tVQ@mail.gmail.com>

Thank you! That worked.


2015-09-15 2:50 GMT-04:00 peter dalgaard <pdalgd at gmail.com>:

>
> > On 15 Sep 2015, at 04:31 , li li <hannah.hlx at gmail.com> wrote:
> >
> > Hi Jeff,
> >  Thanks for replying. I actually tried "ordered(tmp$type, levels=c("c",
> > "b", "a")."
> > But I think only the order of the letters on x axis changed but the order
>
> You _think_ ??? Documentation, please...
>
> The boxplots certainly move if I do
>
> plot(result ~ type, tmp)
> plot(result ~ factor(type, levels=c("c","b","a")), tmp)
>
>
>
> > of the boxplot did not. So there is some problem there. I also tried
> > as.factor(tmp$type); levels(tmp$type)=c("c", "b", "a") and got the same
>
> That changes the level _names_: 1st group name becomes "c" instead of "a";
> you want the 3rd group to become the 1st but still be called "c".
>
> -pd
>
>
> > thing.
> >    Thanks.
> >      Li
> >
> > 2015-09-14 21:44 GMT-04:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
> >
> >> Make your factor variable deliberately. That is, specify the levels
> >> parameter with the values in order when you create the factor.
> >>
> ---------------------------------------------------------------------------
> >> Jeff Newmiller                        The     .....       .....  Go
> Live...
> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> >> Go...
> >>                                      Live:   OO#.. Dead: OO#..  Playing
> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >> /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> >>
> ---------------------------------------------------------------------------
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On September 14, 2015 6:23:50 PM PDT, li li <hannah.hlx at gmail.com>
> wrote:
> >>> Hi all,
> >>> I have the following data "tmp" and I want to plot  boxplots for
> >>> each level of the factor "type" and the order the factor should be c,
> >>> b ,a. In other words, the boxplot corresponding to the level "c"
> >>> should be the first and so on.
> >>> Any suggestions?
> >>>  Li
> >>>
> >>>> tmp
> >>>  result type
> >>> 1     101    a
> >>> 2     101    a
> >>> 3     101    a
> >>> 4     101    a
> >>> 5     101    a
> >>> 6     101    a
> >>> 7     100    a
> >>> 8     106    b
> >>> 9      91    b
> >>> 10     78    b
> >>> 11     95    b
> >>> 12    111    b
> >>> 13     92    b
> >>> 14     98    b
> >>> 15    108    c
> >>> 16    112    c
> >>> 17     98    c
> >>> 18    102    c
> >>> 19     88    c
> >>> 20     86    c
> >>> 21     81    c
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From laura.fernandezp at edu.uah.es  Tue Sep 15 22:40:21 2015
From: laura.fernandezp at edu.uah.es (laurafdez56)
Date: Tue, 15 Sep 2015 13:40:21 -0700 (PDT)
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <55F7F6C1.5090402@auckland.ac.nz>
References: <1442232687664-4712219.post@n4.nabble.com>
	<55F6E921.2060700@dewey.myzen.co.uk>
	<1442261917039-4712255.post@n4.nabble.com>
	<8FAD4A6E-B880-423F-ABE2-873104DF58A6@comcast.net>
	<1442305279327-4712271.post@n4.nabble.com>
	<55F7F6C1.5090402@auckland.ac.nz>
Message-ID: <1442349621245-4712296.post@n4.nabble.com>

Jajaja!
Well, may be I didn't express myself properly, but I did it!!!! See my
new-beautiful graphic and I really want to say square-line :). Finally, it
was very easy, but I had to think in a big group with you guys!
I speak/read/write english very well, but perhaps my thoughts were not very
clear, you don't need to be mean to me Rolf and Michael! ;)
sup.sup <http://r.789695.n4.nabble.com/file/n4712296/sup.sup>  

Thank you very much for your help! 
Un abrazo muy fuerte para todos!!

Laura




--
View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712296.html
Sent from the R help mailing list archive at Nabble.com.


From bgunter.4567 at gmail.com  Tue Sep 15 22:51:24 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 15 Sep 2015 13:51:24 -0700
Subject: [R] Multiple if function
In-Reply-To: <E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
Message-ID: <CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>

... but this only works if ASBclass is numeric. What if it is a factor
(or even character)?

One can always finesse factors in simple cases like this, but for 2
reasons, I don't think it's a good idea.

1. One should make use of a factor's API, rather than its internal
integer representation(which, I grant, ain't likely to change);

2. For more complicated alternatives (e.g. entirely different
functions depending on the factor value) it won't work anyway.

For simple cases, ifelse() seems reasonable; but for more alternatives
-- say 10 or 50 -- this becomes too cumbersome (imho). I think the
split and recombine approach then becomes the best option, but maybe
there is some easier, shorter, approach that I am overlooking. Please
correct me if this is the case.

Best,
Bert






Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 15, 2015 at 12:23 PM, Peter Alspach
<Peter.Alspach at plantandfood.co.nz> wrote:
> Tena koe Maria
>
> It seems you need to multiply Flow by 0.05+ASBClass/20 (i.e., no if calls are necessary)
>
> HTH ....
>
> Peter Alspach
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Maria Lathouri
> Sent: Tuesday, 15 September 2015 10:57 p.m.
> To: r-help at r-project.org
> Subject: [R] Multiple if function
>
> Dear all,
>
> I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
> ASBclass    Flow1              11.51               9.2
> 2              10.5
> 3               6.7  ...              ...
> I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.
>
> If (ASBclass=1) { deviation<-Flow*0.1}
> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2} I am not sure whether I should add the else function and how can I combine these separate functions.
>
> Can anyone help me on that?
> Thank you very much.
>
> Kind regardsMaria
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> The contents of this e-mail are confidential and may be subject to legal privilege.
>  If you are not the intended recipient you must not use, disseminate, distribute or
>  reproduce all or any part of this e-mail or attachments.  If you have received this
>  e-mail in error, please notify the sender and delete all material pertaining to this
>  e-mail.  Any opinion or views expressed in this e-mail are those of the individual
>  sender and may not represent those of The New Zealand Institute for Plant and
>  Food Research Limited.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From laura.fernandezp at edu.uah.es  Tue Sep 15 22:49:52 2015
From: laura.fernandezp at edu.uah.es (laurafdez56)
Date: Tue, 15 Sep 2015 13:49:52 -0700 (PDT)
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <55F85445.2080405@dewey.myzen.co.uk>
References: <1442232687664-4712219.post@n4.nabble.com>
	<55F6E921.2060700@dewey.myzen.co.uk>
	<1442261917039-4712255.post@n4.nabble.com>
	<8FAD4A6E-B880-423F-ABE2-873104DF58A6@comcast.net>
	<1442305279327-4712271.post@n4.nabble.com>
	<55F85445.2080405@dewey.myzen.co.uk>
Message-ID: <VI1PR03MB1040A83E637B865CE73262B5BA5C0@VI1PR03MB1040.eurprd03.prod.outlook.com>

Dear Michael!!

Thank you very much for your help!!

Finally I did it, as you can see:


[cid:82cbea8b-7d9d-4416-88b3-3ed499b2bf87]

I didn`t explain myself properly, sorry!!!

Thank you very much for your help!!


Un abrazo fuerte!!


Laura Fern?ndez P?rez
Doctorado en Ecolog?a, Conservaci?n y Restauraci?n de Ecosistemas
Departamento de Ciencias de la Vida
Universidad de Alcal?
Alcal? de Henares, Madrid, Espa?a.


________________________________
De: Michael Dewey-3 [via R] <ml-node+s789695n4712285h41 at n4.nabble.com>
Enviado: martes, 15 de septiembre de 2015 19:20
Para: Fern?ndez P?rez Laura
Asunto: Re: R lines type in a glm/predict model

Dear Laura

As you can see you have us all very puzzled. I think something is
getting lost in translation between the language of Cervantes and the
language of Shakespeare.

Perhaps
1 - send a picture of a square line
2 - tell us what square line is in Spanish
3 - find the Spanish language R help list and try there

Obviously do not try all three at once

On 15/09/2015 09:21, laurafdez56 wrote:

> Hey David!
> This is my graphic.  sup.png
> <http://r.789695.n4.nabble.com/file/n4712271/sup.png>
> I want only to change the line type (square line instead dotted line) in
> order to distinguish the species.
>
> Thank you very much!
> Laura
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712271.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> [hidden email]</user/SendEmail.jtp?type=node&node=4712285&i=0> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

--
Michael
http://www.dewey.myzen.co.uk/home.html

______________________________________________
[hidden email]</user/SendEmail.jtp?type=node&node=4712285&i=1> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


________________________________
If you reply to this email, your message will be added to the discussion below:
http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712285.html
To unsubscribe from R lines type in a glm/predict model, click here<http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4712219&code=bGF1cmEuZmVybmFuZGV6cEBlZHUudWFoLmVzfDQ3MTIyMTl8OTkyMjg0Mzg2>.
NAML<http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


pastedImage.png (32K) <http://r.789695.n4.nabble.com/attachment/4712298/0/pastedImage.png>




--
View this message in context: http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712298.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From john.posner at MJBIOSTAT.COM  Tue Sep 15 22:59:59 2015
From: john.posner at MJBIOSTAT.COM (John Posner)
Date: Tue, 15 Sep 2015 20:59:59 +0000
Subject: [R] a question about data manipulation in R
In-Reply-To: <D29A810FCBF.00000B02jrkrideau@inbox.com>
References: <c23295b3cba.0000028djrkrideau@inbox.com>
	<D29A810FCBF.00000B02jrkrideau@inbox.com>
Message-ID: <9E73F88F04AA25408DBB58FB730BA65329BBF2DB@AUSP01DAG0503.collaborationhost.net>

Given your "input: data frame, with variables "V1" and "V2", here's a solution. This might not be the most "R-like" solution, since I'm still more of a Python refugee than a native R coder.

-John


# analyze input, using run-length encoding
runs_table = rle(input$V1)
number_of_runs = length(runs_table$values)  # number of columns in answer matrix
lengths_of_runs = runs_table$lengths
max_run = max(lengths_of_runs)              # number of rows in answer matrix

# set up answer matrix, with all NA values
answer = matrix(rep(NA, number_of_runs * max_run),
                           nrow=max_run, ncol=number_of_runs)

# find the locations in the input$V1 column where a new value begins
indexes = c(0, cumsum(lengths_of_runs)) + 1

# column-by-column: copy values from input$V2 to the answer matrix, overwriting NA values
for (col in 1:number_of_runs) {
  answer[1:lengths_of_runs[col], col] = input[ (indexes[col]):(indexes[col+1]-1) , 2]
}


From dcarlson at tamu.edu  Tue Sep 15 23:11:39 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 15 Sep 2015 21:11:39 +0000
Subject: [R] Multiple if function
In-Reply-To: <CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>

You could use match() and avoid ifelse():

> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5), stringsAsFactors=FALSE)
> cat <- LETTERS[1:3]
> mult <- c(.1, .15, .2)
> dat$Flow * mult[match(dat$ASB, cat)]
[1] 1.151 1.380 2.100

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
Sent: Tuesday, September 15, 2015 3:51 PM
To: Peter Alspach
Cc: r-help at r-project.org
Subject: Re: [R] Multiple if function

... but this only works if ASBclass is numeric. What if it is a factor
(or even character)?

One can always finesse factors in simple cases like this, but for 2
reasons, I don't think it's a good idea.

1. One should make use of a factor's API, rather than its internal
integer representation(which, I grant, ain't likely to change);

2. For more complicated alternatives (e.g. entirely different
functions depending on the factor value) it won't work anyway.

For simple cases, ifelse() seems reasonable; but for more alternatives
-- say 10 or 50 -- this becomes too cumbersome (imho). I think the
split and recombine approach then becomes the best option, but maybe
there is some easier, shorter, approach that I am overlooking. Please
correct me if this is the case.

Best,
Bert






Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 15, 2015 at 12:23 PM, Peter Alspach
<Peter.Alspach at plantandfood.co.nz> wrote:
> Tena koe Maria
>
> It seems you need to multiply Flow by 0.05+ASBClass/20 (i.e., no if calls are necessary)
>
> HTH ....
>
> Peter Alspach
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Maria Lathouri
> Sent: Tuesday, 15 September 2015 10:57 p.m.
> To: r-help at r-project.org
> Subject: [R] Multiple if function
>
> Dear all,
>
> I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
> ASBclass    Flow1              11.51               9.2
> 2              10.5
> 3               6.7  ...              ...
> I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.
>
> If (ASBclass=1) { deviation<-Flow*0.1}
> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2} I am not sure whether I should add the else function and how can I combine these separate functions.
>
> Can anyone help me on that?
> Thank you very much.
>
> Kind regardsMaria
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> The contents of this e-mail are confidential and may be subject to legal privilege.
>  If you are not the intended recipient you must not use, disseminate, distribute or
>  reproduce all or any part of this e-mail or attachments.  If you have received this
>  e-mail in error, please notify the sender and delete all material pertaining to this
>  e-mail.  Any opinion or views expressed in this e-mail are those of the individual
>  sender and may not represent those of The New Zealand Institute for Plant and
>  Food Research Limited.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Tue Sep 15 23:42:42 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 15 Sep 2015 14:42:42 -0700
Subject: [R] Multiple if function
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
Message-ID: <CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>

Thanks, David.

I would say, not quite.

What if the alternatives are:

If class = "a" multiply y by 2;
If class = "b"  add 5 to y;
If class = "c" take sqrt(y)
 (where y is numeric, say)
?

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 15, 2015 at 2:11 PM, David L Carlson <dcarlson at tamu.edu> wrote:
> You could use match() and avoid ifelse():
>
>> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5), stringsAsFactors=FALSE)
>> cat <- LETTERS[1:3]
>> mult <- c(.1, .15, .2)
>> dat$Flow * mult[match(dat$ASB, cat)]
> [1] 1.151 1.380 2.100
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
> Sent: Tuesday, September 15, 2015 3:51 PM
> To: Peter Alspach
> Cc: r-help at r-project.org
> Subject: Re: [R] Multiple if function
>
> ... but this only works if ASBclass is numeric. What if it is a factor
> (or even character)?
>
> One can always finesse factors in simple cases like this, but for 2
> reasons, I don't think it's a good idea.
>
> 1. One should make use of a factor's API, rather than its internal
> integer representation(which, I grant, ain't likely to change);
>
> 2. For more complicated alternatives (e.g. entirely different
> functions depending on the factor value) it won't work anyway.
>
> For simple cases, ifelse() seems reasonable; but for more alternatives
> -- say 10 or 50 -- this becomes too cumbersome (imho). I think the
> split and recombine approach then becomes the best option, but maybe
> there is some easier, shorter, approach that I am overlooking. Please
> correct me if this is the case.
>
> Best,
> Bert
>
>
>
>
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Tue, Sep 15, 2015 at 12:23 PM, Peter Alspach
> <Peter.Alspach at plantandfood.co.nz> wrote:
>> Tena koe Maria
>>
>> It seems you need to multiply Flow by 0.05+ASBClass/20 (i.e., no if calls are necessary)
>>
>> HTH ....
>>
>> Peter Alspach
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Maria Lathouri
>> Sent: Tuesday, 15 September 2015 10:57 p.m.
>> To: r-help at r-project.org
>> Subject: [R] Multiple if function
>>
>> Dear all,
>>
>> I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
>> ASBclass    Flow1              11.51               9.2
>> 2              10.5
>> 3               6.7  ...              ...
>> I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.
>>
>> If (ASBclass=1) { deviation<-Flow*0.1}
>> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2} I am not sure whether I should add the else function and how can I combine these separate functions.
>>
>> Can anyone help me on that?
>> Thank you very much.
>>
>> Kind regardsMaria
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> The contents of this e-mail are confidential and may be subject to legal privilege.
>>  If you are not the intended recipient you must not use, disseminate, distribute or
>>  reproduce all or any part of this e-mail or attachments.  If you have received this
>>  e-mail in error, please notify the sender and delete all material pertaining to this
>>  e-mail.  Any opinion or views expressed in this e-mail are those of the individual
>>  sender and may not represent those of The New Zealand Institute for Plant and
>>  Food Research Limited.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Wed Sep 16 00:12:03 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 15 Sep 2015 15:12:03 -0700
Subject: [R] Multiple if function
In-Reply-To: <CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
Message-ID: <EDFA4C59-8ED9-46B7-8F3B-B7C689E06128@dcn.davis.CA.us>

If dat is large and you want to be efficient about this, then compute only the answers you need and put them right where they belong as you go:

dat$result <- NA
idx <- "A" == dat$ASB
dat$result[ idx ] <- 2 * dat$Flow[ idx ]
idx <- "B" == dat$ASB
dat$result[ idx ] <- 5 + dat$Flow[ idx ]
idx <- "C" == dat$ASB
dat$result[ idx ] <- sqrt( dat$Flow[ idx ] )

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 15, 2015 2:42:42 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>Thanks, David.
>
>I would say, not quite.
>
>What if the alternatives are:
>
>If class = "a" multiply y by 2;
>If class = "b"  add 5 to y;
>If class = "c" take sqrt(y)
> (where y is numeric, say)
>?
>
>Cheers,
>Bert
>Bert Gunter
>
>"Data is not information. Information is not knowledge. And knowledge
>is certainly not wisdom."
>   -- Clifford Stoll
>
>
>On Tue, Sep 15, 2015 at 2:11 PM, David L Carlson <dcarlson at tamu.edu>
>wrote:
>> You could use match() and avoid ifelse():
>>
>>> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5),
>stringsAsFactors=FALSE)
>>> cat <- LETTERS[1:3]
>>> mult <- c(.1, .15, .2)
>>> dat$Flow * mult[match(dat$ASB, cat)]
>> [1] 1.151 1.380 2.100
>>
>> -------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77840-4352
>>
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert
>Gunter
>> Sent: Tuesday, September 15, 2015 3:51 PM
>> To: Peter Alspach
>> Cc: r-help at r-project.org
>> Subject: Re: [R] Multiple if function
>>
>> ... but this only works if ASBclass is numeric. What if it is a
>factor
>> (or even character)?
>>
>> One can always finesse factors in simple cases like this, but for 2
>> reasons, I don't think it's a good idea.
>>
>> 1. One should make use of a factor's API, rather than its internal
>> integer representation(which, I grant, ain't likely to change);
>>
>> 2. For more complicated alternatives (e.g. entirely different
>> functions depending on the factor value) it won't work anyway.
>>
>> For simple cases, ifelse() seems reasonable; but for more
>alternatives
>> -- say 10 or 50 -- this becomes too cumbersome (imho). I think the
>> split and recombine approach then becomes the best option, but maybe
>> there is some easier, shorter, approach that I am overlooking. Please
>> correct me if this is the case.
>>
>> Best,
>> Bert
>>
>>
>>
>>
>>
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>    -- Clifford Stoll
>>
>>
>> On Tue, Sep 15, 2015 at 12:23 PM, Peter Alspach
>> <Peter.Alspach at plantandfood.co.nz> wrote:
>>> Tena koe Maria
>>>
>>> It seems you need to multiply Flow by 0.05+ASBClass/20 (i.e., no if
>calls are necessary)
>>>
>>> HTH ....
>>>
>>> Peter Alspach
>>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>Maria Lathouri
>>> Sent: Tuesday, 15 September 2015 10:57 p.m.
>>> To: r-help at r-project.org
>>> Subject: [R] Multiple if function
>>>
>>> Dear all,
>>>
>>> I am writing as I would like your help. I have a dataframe with two
>columns, ASB and Flow, where the the first one has values 1, 2 or 3 and
>the second flow data. Something like that:
>>> ASBclass    Flow1              11.51               9.2
>>> 2              10.5
>>> 3               6.7  ...              ...
>>> I would like to produce a third column named eg. deviation where it
>would get me values based on if ASBclass is 1, multiply Flow by 0.1; if
>ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then
>multiply by 0.2.
>>>
>>> If (ASBclass=1) { deviation<-Flow*0.1}
>>> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) {
>deviation<-Flow*0.2} I am not sure whether I should add the else
>function and how can I combine these separate functions.
>>>
>>> Can anyone help me on that?
>>> Thank you very much.
>>>
>>> Kind regardsMaria
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> The contents of this e-mail are confidential and may be subject to
>legal privilege.
>>>  If you are not the intended recipient you must not use,
>disseminate, distribute or
>>>  reproduce all or any part of this e-mail or attachments.  If you
>have received this
>>>  e-mail in error, please notify the sender and delete all material
>pertaining to this
>>>  e-mail.  Any opinion or views expressed in this e-mail are those of
>the individual
>>>  sender and may not represent those of The New Zealand Institute for
>Plant and
>>>  Food Research Limited.
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Wed Sep 16 01:46:32 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 15 Sep 2015 23:46:32 +0000
Subject: [R] Multiple if function
In-Reply-To: <CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>

I realize that you can break this approach as well with a suitably complex expression, but I took it as a challenge:
 
> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5), stringsAsFactors=FALSE)
> cat <- LETTERS[1:3]
> mult <- c("'*'(2," , "'+'(5,", "sqrt(")
> sapply(parse(text=paste0(mult[match(dat$ASB, cat)], dat$Flow, ")")), eval)
[1] 23.02000 14.20000  3.24037

David

-----Original Message-----
From: Bert Gunter [mailto:bgunter.4567 at gmail.com] 
Sent: Tuesday, September 15, 2015 4:43 PM
To: David L Carlson <dcarlson at tamu.edu>
Cc: Peter Alspach <Peter.Alspach at plantandfood.co.nz>; r-help at r-project.org
Subject: Re: [R] Multiple if function

Thanks, David.

I would say, not quite.

What if the alternatives are:

If class = "a" multiply y by 2;
If class = "b"  add 5 to y;
If class = "c" take sqrt(y)
 (where y is numeric, say)
?

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 15, 2015 at 2:11 PM, David L Carlson <dcarlson at tamu.edu> wrote:
> You could use match() and avoid ifelse():
>
>> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5), stringsAsFactors=FALSE)
>> cat <- LETTERS[1:3]
>> mult <- c(.1, .15, .2)
>> dat$Flow * mult[match(dat$ASB, cat)]
> [1] 1.151 1.380 2.100
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
> Sent: Tuesday, September 15, 2015 3:51 PM
> To: Peter Alspach
> Cc: r-help at r-project.org
> Subject: Re: [R] Multiple if function
>
> ... but this only works if ASBclass is numeric. What if it is a factor
> (or even character)?
>
> One can always finesse factors in simple cases like this, but for 2
> reasons, I don't think it's a good idea.
>
> 1. One should make use of a factor's API, rather than its internal
> integer representation(which, I grant, ain't likely to change);
>
> 2. For more complicated alternatives (e.g. entirely different
> functions depending on the factor value) it won't work anyway.
>
> For simple cases, ifelse() seems reasonable; but for more alternatives
> -- say 10 or 50 -- this becomes too cumbersome (imho). I think the
> split and recombine approach then becomes the best option, but maybe
> there is some easier, shorter, approach that I am overlooking. Please
> correct me if this is the case.
>
> Best,
> Bert
>
>
>
>
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Tue, Sep 15, 2015 at 12:23 PM, Peter Alspach
> <Peter.Alspach at plantandfood.co.nz> wrote:
>> Tena koe Maria
>>
>> It seems you need to multiply Flow by 0.05+ASBClass/20 (i.e., no if calls are necessary)
>>
>> HTH ....
>>
>> Peter Alspach
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Maria Lathouri
>> Sent: Tuesday, 15 September 2015 10:57 p.m.
>> To: r-help at r-project.org
>> Subject: [R] Multiple if function
>>
>> Dear all,
>>
>> I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
>> ASBclass    Flow1              11.51               9.2
>> 2              10.5
>> 3               6.7  ...              ...
>> I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.
>>
>> If (ASBclass=1) { deviation<-Flow*0.1}
>> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2} I am not sure whether I should add the else function and how can I combine these separate functions.
>>
>> Can anyone help me on that?
>> Thank you very much.
>>
>> Kind regardsMaria
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> The contents of this e-mail are confidential and may be subject to legal privilege.
>>  If you are not the intended recipient you must not use, disseminate, distribute or
>>  reproduce all or any part of this e-mail or attachments.  If you have received this
>>  e-mail in error, please notify the sender and delete all material pertaining to this
>>  e-mail.  Any opinion or views expressed in this e-mail are those of the individual
>>  sender and may not represent those of The New Zealand Institute for Plant and
>>  Food Research Limited.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From dwinsemius at comcast.net  Wed Sep 16 02:00:02 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 15 Sep 2015 17:00:02 -0700
Subject: [R] Multiple if function
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
Message-ID: <D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>


On Sep 15, 2015, at 4:46 PM, David L Carlson wrote:

> I realize that you can break this approach as well with a suitably complex expression, but I took it as a challenge:
> 
>> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5), stringsAsFactors=FALSE)
>> cat <- LETTERS[1:3]
>> mult <- c("'*'(2," , "'+'(5,", "sqrt(")
>> sapply(parse(text=paste0(mult[match(dat$ASB, cat)], dat$Flow, ")")), eval)
> [1] 23.02000 14.20000  3.24037

This seems a bit less tortured:

> mapply(  function(x,y) switch(x, A=y*2, B=y+3, C=sqrt(y) ), dat$ASB, dat$Flow )
       A        B        C 
23.02000 12.20000  3.24037 

Best;
The Other David

> 
> David
> 
> -----Original Message-----
> From: Bert Gunter [mailto:bgunter.4567 at gmail.com] 
> Sent: Tuesday, September 15, 2015 4:43 PM
> To: David L Carlson <dcarlson at tamu.edu>
> Cc: Peter Alspach <Peter.Alspach at plantandfood.co.nz>; r-help at r-project.org
> Subject: Re: [R] Multiple if function
> 
> Thanks, David.
> 
> I would say, not quite.
> 
> What if the alternatives are:
> 
> If class = "a" multiply y by 2;
> If class = "b"  add 5 to y;
> If class = "c" take sqrt(y)
> (where y is numeric, say)
> ?
> 
> Cheers,
> Bert
> Bert Gunter
> 
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>   -- Clifford Stoll
> 
> 
> On Tue, Sep 15, 2015 at 2:11 PM, David L Carlson <dcarlson at tamu.edu> wrote:
>> You could use match() and avoid ifelse():
>> 
>>> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5), stringsAsFactors=FALSE)
>>> cat <- LETTERS[1:3]
>>> mult <- c(.1, .15, .2)
>>> dat$Flow * mult[match(dat$ASB, cat)]
>> [1] 1.151 1.380 2.100
>> 
>> -------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77840-4352
>> 
>> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
>> Sent: Tuesday, September 15, 2015 3:51 PM
>> To: Peter Alspach
>> Cc: r-help at r-project.org
>> Subject: Re: [R] Multiple if function
>> 
>> ... but this only works if ASBclass is numeric. What if it is a factor
>> (or even character)?
>> 
>> One can always finesse factors in simple cases like this, but for 2
>> reasons, I don't think it's a good idea.
>> 
>> 1. One should make use of a factor's API, rather than its internal
>> integer representation(which, I grant, ain't likely to change);
>> 
>> 2. For more complicated alternatives (e.g. entirely different
>> functions depending on the factor value) it won't work anyway.
>> 
>> For simple cases, ifelse() seems reasonable; but for more alternatives
>> -- say 10 or 50 -- this becomes too cumbersome (imho). I think the
>> split and recombine approach then becomes the best option, but maybe
>> there is some easier, shorter, approach that I am overlooking. Please
>> correct me if this is the case.
>> 
>> Best,
>> Bert
>> 
>> 
>> 
>> 
>> 
>> 
>> Bert Gunter
>> 
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>   -- Clifford Stoll
>> 
>> 
>> On Tue, Sep 15, 2015 at 12:23 PM, Peter Alspach
>> <Peter.Alspach at plantandfood.co.nz> wrote:
>>> Tena koe Maria
>>> 
>>> It seems you need to multiply Flow by 0.05+ASBClass/20 (i.e., no if calls are necessary)
>>> 
>>> HTH ....
>>> 
>>> Peter Alspach
>>> 
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Maria Lathouri
>>> Sent: Tuesday, 15 September 2015 10:57 p.m.
>>> To: r-help at r-project.org
>>> Subject: [R] Multiple if function
>>> 
>>> Dear all,
>>> 
>>> I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
>>> ASBclass    Flow1              11.51               9.2
>>> 2              10.5
>>> 3               6.7  ...              ...
>>> I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.
>>> 
>>> If (ASBclass=1) { deviation<-Flow*0.1}
>>> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2} I am not sure whether I should add the else function and how can I combine these separate functions.
>>> 
>>> Can anyone help me on that?
>>> Thank you very much.
>>> 
>>> Kind regardsMaria
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> The contents of this e-mail are confidential and may be subject to legal privilege.
>>> If you are not the intended recipient you must not use, disseminate, distribute or
>>> reproduce all or any part of this e-mail or attachments.  If you have received this
>>> e-mail in error, please notify the sender and delete all material pertaining to this
>>> e-mail.  Any opinion or views expressed in this e-mail are those of the individual
>>> sender and may not represent those of The New Zealand Institute for Plant and
>>> Food Research Limited.
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From Matthieu.Huot at dfo-mpo.gc.ca  Tue Sep 15 21:09:19 2015
From: Matthieu.Huot at dfo-mpo.gc.ca (Huot, Matthieu)
Date: Tue, 15 Sep 2015 19:09:19 +0000
Subject: [R] Looking for Post-hoc tests (a la TukeyHSD) or
 interaction-level independent contrasts for survival analysis.
Message-ID: <D5673654417F1D40A0F59F27C64F7CFC8D44E6@SVONKENMBX03.ENT.dfo-mpo.ca>

Hi Tom

I know the post is over 7-8 years old but I am having the same question. How to do a post-hoc test like TukeyHSD on coxph type output.

Have you received any info in this matter?
Thanks
Matthieu

Looking for Post-hoc tests (a la TukeyHSD) or interaction-level independent contrasts for survival analysis.
Thomas Oliver toliver at stanford.edu <mailto:r-help%40r-project.org?Subject=Re%3A%20%5BR%5D%20Looking%20for%20Post-hoc%20tests%20%28a%20la%20TukeyHSD%29%20or%20interaction-level%0A%20independent%20contrasts%20for%20survival%20analysis.&In-Reply-To=%3C6.2.5.6.2.20080429131826.04deaeb0%40stanford.edu%3E>
Tue Apr 29 23:12:33 CEST 2008

  *   Previous message: [R] AIC extract and comparison <https://stat.ethz.ch/pipermail/r-help/2008-April/160916.html>
  *   Next message: [R] for loop in nls function <https://stat.ethz.ch/pipermail/r-help/2008-April/160886.html>
  *   Messages sorted by: [ date ]<https://stat.ethz.ch/pipermail/r-help/2008-April/date.html#160885> [ thread ]<https://stat.ethz.ch/pipermail/r-help/2008-April/thread.html#160885> [ subject ]<https://stat.ethz.ch/pipermail/r-help/2008-April/subject.html#160885> [ author ]<https://stat.ethz.ch/pipermail/r-help/2008-April/author.html#160885>

________________________________

Hello all R-helpers,



    I've performed an experiment to test for differential effects of

elevated temperatures on three different groups of corals.  I'm

currently performing a cox proportional hazards regression with

censoring on the survivorship (days to mortality) of each individual

in the experiment with two factors: Temperature Treatment (2 levels:

ambient and elevated) and experimental group (3 levels: say 1,2,3).



In my experiment, all three groups survived equally well in the

ambient control treatment, but  two of three of the groups succumbed

to heat stress in the elevated temperature treatment.  I can see that

the third group had a small degree of mortality, but it appears to be

significantly less than the other two and may be not significantly

different from the ambient controls.



I would like to ask three questions:  1) Is group 3 different from

controls? 2) Is group 3 different from group 1 and/or group 2 in the

elevated treatment? and 3) are groups 1 and 2 different from each

other in the elevated treatment?



   Because I'm testing for differential effects among the elevated

temperature treatment group, and "I've seen the data" by now,  the

analysis would be easiest for me if I performed a responsible

multiple comparisons test like TukeyHSD to see how each of the six

Treatment:Group subgroups compared to each other.  TukeyHSD does not

appear to be defined for outputs from the function coxph -- (see

survival library).



cph <- coxph(Surv(DayToMort, Censor) ~ Treatment*Group, data=subb)



--> Does anyone know of an implementation of TukeyHSD that would

work, or of another post-hoc multiple comparison test?



I believe that another responsible tack would be to clearly define

the contrasts I'd like to make within the interaction term. However

this has yet to work as fully as I'd like it.



  I've successfully set the contrasts matrix for the three-level

factor "Group" following Crawley's The R Book.



cmat<-cbind(c(-1,1,0),c(0,-1,1))

contrasts(subb$Group)<-cmat

contrasts(subb$Group)



By setting these contrasts and then looking at the interaction terms

in the coxph model, this allows me to compare groups _within_ each

separate treatment, and confirms both that #2) that groups 1 and 2

are not sig. different in the elevated treatment, and #3) the group3

corals survived significantly better than the other groups in the

elevated treatment. BUT it does not allow me to say if the group3

survival is or is not different from its own control.(#1 above).



To make this comparison, I've tried setting the contrast matrix for

the Treatment:Group interaction term, with no success.  Whenever I

attempt to do so, I run the code below:



cmat<-cbind(c(-1,0,0,1,0,0),c(0,-1,0,0,1,0),c(0,0,-1,0,0,1),c(0,0,0,-1,1,0),c(0,0,0,0,-1,1))

#Build a matrix

rownames(cmat)<-rownames(contrasts(subb$Treatment:subb$Group))  #give

cmat the correct rownames

colnames(cmat)<-colnames(contrasts(subb$Treatment:subb$Group))

#give cmat the correct colnames

contrasts(subb$Treatment:subb$Group)<-cmat           #try to assign cmat



and I get this error message:

Error in contrasts(subb$Treatment:subb$Group, ) <- cmat :

   could not find function ":<-"



Alternatively I could run:

options(contrasts=c("contr.sum","contr.poly"))

where:

  contrasts(subb$Treatment:subb$Group)



             [,1] [,2] [,3] [,4] [,5]

Con:1   1    0    0    0    0

Con:2    0    1    0    0    0

Con:3    0    0    1    0    0

Exp:1    0    0    0    1    0

Exp:2    0    0    0    0    1

Exp:3   -1   -1   -1   -1   -1



But even that doesn't appear to affect the output of :



cph <- coxph(Surv(DayToMort, Censor) ~ Treatment*Group, data=subb)





--> Is what I'm trying to do statistically invalid and R is trying to

quietly save me from statistical destruction, or it is just being a

pain?  Is there a way around it?



--> Any other suggestions?



Many Thanks in Advance,



Tom Oliver




	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Sep 16 02:41:40 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 15 Sep 2015 17:41:40 -0700
Subject: [R] Multiple if function
In-Reply-To: <D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
Message-ID: <CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>

Thanks to both Davids.

I realize that these things are often a matter of aesthetics -- and
hence have little rational justification -- but I agree with The Other
David: eval(parse) seems to me to violate R's soul( it makes R a macro
language instead of a functional one).

However, mapply(... switch) effectively loops through the frame row by
row. Aesthetically, I like it; but it seems inefficient. If there are
e.g. 1e6 rows in say 10 categories, I think Jeff's approach should do
much better.  I'll try to generate some actual data to see unless
someone else beats me to it.

However, maybe plyr or dplyr or data.table can do better, and I would
appreciate any comments from cogniscenti with these packages.  Maybe
I've been foolish by making the criteria too general, in which case my
knuckles need to be rapped.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 15, 2015 at 5:00 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
> On Sep 15, 2015, at 4:46 PM, David L Carlson wrote:
>
>> I realize that you can break this approach as well with a suitably complex expression, but I took it as a challenge:
>>
>>> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5), stringsAsFactors=FALSE)
>>> cat <- LETTERS[1:3]
>>> mult <- c("'*'(2," , "'+'(5,", "sqrt(")
>>> sapply(parse(text=paste0(mult[match(dat$ASB, cat)], dat$Flow, ")")), eval)
>> [1] 23.02000 14.20000  3.24037
>
> This seems a bit less tortured:
>
>> mapply(  function(x,y) switch(x, A=y*2, B=y+3, C=sqrt(y) ), dat$ASB, dat$Flow )
>        A        B        C
> 23.02000 12.20000  3.24037
>
> Best;
> The Other David
>
>>
>> David
>>
>> -----Original Message-----
>> From: Bert Gunter [mailto:bgunter.4567 at gmail.com]
>> Sent: Tuesday, September 15, 2015 4:43 PM
>> To: David L Carlson <dcarlson at tamu.edu>
>> Cc: Peter Alspach <Peter.Alspach at plantandfood.co.nz>; r-help at r-project.org
>> Subject: Re: [R] Multiple if function
>>
>> Thanks, David.
>>
>> I would say, not quite.
>>
>> What if the alternatives are:
>>
>> If class = "a" multiply y by 2;
>> If class = "b"  add 5 to y;
>> If class = "c" take sqrt(y)
>> (where y is numeric, say)
>> ?
>>
>> Cheers,
>> Bert
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>   -- Clifford Stoll
>>
>>
>> On Tue, Sep 15, 2015 at 2:11 PM, David L Carlson <dcarlson at tamu.edu> wrote:
>>> You could use match() and avoid ifelse():
>>>
>>>> dat <- data.frame(ASB = c(LETTERS[1:3]), Flow=c(11.51, 9.2, 10.5), stringsAsFactors=FALSE)
>>>> cat <- LETTERS[1:3]
>>>> mult <- c(.1, .15, .2)
>>>> dat$Flow * mult[match(dat$ASB, cat)]
>>> [1] 1.151 1.380 2.100
>>>
>>> -------------------------------------
>>> David L Carlson
>>> Department of Anthropology
>>> Texas A&M University
>>> College Station, TX 77840-4352
>>>
>>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
>>> Sent: Tuesday, September 15, 2015 3:51 PM
>>> To: Peter Alspach
>>> Cc: r-help at r-project.org
>>> Subject: Re: [R] Multiple if function
>>>
>>> ... but this only works if ASBclass is numeric. What if it is a factor
>>> (or even character)?
>>>
>>> One can always finesse factors in simple cases like this, but for 2
>>> reasons, I don't think it's a good idea.
>>>
>>> 1. One should make use of a factor's API, rather than its internal
>>> integer representation(which, I grant, ain't likely to change);
>>>
>>> 2. For more complicated alternatives (e.g. entirely different
>>> functions depending on the factor value) it won't work anyway.
>>>
>>> For simple cases, ifelse() seems reasonable; but for more alternatives
>>> -- say 10 or 50 -- this becomes too cumbersome (imho). I think the
>>> split and recombine approach then becomes the best option, but maybe
>>> there is some easier, shorter, approach that I am overlooking. Please
>>> correct me if this is the case.
>>>
>>> Best,
>>> Bert
>>>
>>>
>>>
>>>
>>>
>>>
>>> Bert Gunter
>>>
>>> "Data is not information. Information is not knowledge. And knowledge
>>> is certainly not wisdom."
>>>   -- Clifford Stoll
>>>
>>>
>>> On Tue, Sep 15, 2015 at 12:23 PM, Peter Alspach
>>> <Peter.Alspach at plantandfood.co.nz> wrote:
>>>> Tena koe Maria
>>>>
>>>> It seems you need to multiply Flow by 0.05+ASBClass/20 (i.e., no if calls are necessary)
>>>>
>>>> HTH ....
>>>>
>>>> Peter Alspach
>>>>
>>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Maria Lathouri
>>>> Sent: Tuesday, 15 September 2015 10:57 p.m.
>>>> To: r-help at r-project.org
>>>> Subject: [R] Multiple if function
>>>>
>>>> Dear all,
>>>>
>>>> I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
>>>> ASBclass    Flow1              11.51               9.2
>>>> 2              10.5
>>>> 3               6.7  ...              ...
>>>> I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.
>>>>
>>>> If (ASBclass=1) { deviation<-Flow*0.1}
>>>> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2} I am not sure whether I should add the else function and how can I combine these separate functions.
>>>>
>>>> Can anyone help me on that?
>>>> Thank you very much.
>>>>
>>>> Kind regardsMaria
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> The contents of this e-mail are confidential and may be subject to legal privilege.
>>>> If you are not the intended recipient you must not use, disseminate, distribute or
>>>> reproduce all or any part of this e-mail or attachments.  If you have received this
>>>> e-mail in error, please notify the sender and delete all material pertaining to this
>>>> e-mail.  Any opinion or views expressed in this e-mail are those of the individual
>>>> sender and may not represent those of The New Zealand Institute for Plant and
>>>> Food Research Limited.
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>


From jrkrideau at inbox.com  Wed Sep 16 03:17:01 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 15 Sep 2015 17:17:01 -0800
Subject: [R] R lines type in a glm/predict model
In-Reply-To: <1442349621245-4712296.post@n4.nabble.com>
References: <1442261917039-4712255.post@n4.nabble.com>
	<55f7f6c1.5090402@auckland.ac.nz>
	<1442305279327-4712271.post@n4.nabble.com>
	<8fad4a6e-b880-423f-abe2-873104df58a6@comcast.net>
	<55f6e921.2060700@dewey.myzen.co.uk>
	<1442232687664-4712219.post@n4.nabble.com>
Message-ID: <DF17843A112.00000865jrkrideau@inbox.com>

I thought that was was what you wanted. Congratulations.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: laura.fernandezp at edu.uah.es
> Sent: Tue, 15 Sep 2015 13:40:21 -0700 (PDT)
> To: r-help at r-project.org
> Subject: Re: [R] R lines type in a glm/predict model
> 
> Jajaja!
> Well, may be I didn't express myself properly, but I did it!!!! See my
> new-beautiful graphic and I really want to say square-line :). Finally,
> it
> was very easy, but I had to think in a big group with you guys!
> I speak/read/write english very well, but perhaps my thoughts were not
> very
> clear, you don't need to be mean to me Rolf and Michael! ;)
> sup.sup <http://r.789695.n4.nabble.com/file/n4712296/sup.sup>
> 
> Thank you very much for your help!
> Un abrazo muy fuerte para todos!!
> 
> Laura
> 
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/R-lines-type-in-a-glm-predict-model-tp4712219p4712296.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From jrkrideau at inbox.com  Wed Sep 16 03:20:16 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 15 Sep 2015 17:20:16 -0800
Subject: [R] a question about data manipulation in R
In-Reply-To: <9E73F88F04AA25408DBB58FB730BA65329BBF2DB@AUSP01DAG0503.collaborationhost.net>
References: <d29a810fcbf.00000b02jrkrideau@inbox.com>
	<c23295b3cba.0000028djrkrideau@inbox.com>
Message-ID: <DF1ECBC10DC.00000870jrkrideau@inbox.com>

Refugees are welcome. Just register at the desk over there. :)

Thanks, I have been drawing a complete blank without attacking it by brute force and advanced stupidity.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: john.posner at mjbiostat.com
> Sent: Tue, 15 Sep 2015 20:59:59 +0000
> To: zkarimi1985 at yahoo.com
> Subject: Re: [R] a question about data manipulation in R
> 
> Given your "input: data frame, with variables "V1" and "V2", here's a
> solution. This might not be the most "R-like" solution, since I'm still
> more of a Python refugee than a native R coder.
> 
> -John
> 
> 
> # analyze input, using run-length encoding
> runs_table = rle(input$V1)
> number_of_runs = length(runs_table$values)  # number of columns in answer
> matrix
> lengths_of_runs = runs_table$lengths
> max_run = max(lengths_of_runs)              # number of rows in answer
> matrix
> 
> # set up answer matrix, with all NA values
> answer = matrix(rep(NA, number_of_runs * max_run),
>                            nrow=max_run, ncol=number_of_runs)
> 
> # find the locations in the input$V1 column where a new value begins
> indexes = c(0, cumsum(lengths_of_runs)) + 1
> 
> # column-by-column: copy values from input$V2 to the answer matrix,
> overwriting NA values
> for (col in 1:number_of_runs) {
>   answer[1:lengths_of_runs[col], col] = input[
> (indexes[col]):(indexes[col+1]-1) , 2]
> }
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Share photos & screenshots in seconds...
TRY FREE IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if1
Works in all emails, instant messengers, blogs, forums and social networks.


From ccberry at ucsd.edu  Wed Sep 16 04:20:49 2015
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Tue, 15 Sep 2015 19:20:49 -0700
Subject: [R] Multiple if function
In-Reply-To: <CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
Message-ID: <alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>

On Tue, 15 Sep 2015, Bert Gunter wrote:

> Thanks to both Davids.
>
> I realize that these things are often a matter of aesthetics -- and
> hence have little rational justification -- but I agree with The Other
> David: eval(parse) seems to me to violate R's soul( it makes R a macro
> language instead of a functional one).
>
> However, mapply(... switch) effectively loops through the frame row by
> row. Aesthetically, I like it; but it seems inefficient. If there are
> e.g. 1e6 rows in say 10 categories, I think Jeff's approach should do
> much better.  I'll try to generate some actual data to see unless
> someone else beats me to it.

Use mapply like this on large problems:

unsplit(
     mapply(
         function(x,z) eval( x, list( y=z )),
         expression( A=y*2, B=y+3, C=sqrt(y) ),
         split( dat$Flow, dat$ASB ),
         SIMPLIFY=FALSE),
     dat$ASB)

Chuck


From dstr7320 at uni.sydney.edu.au  Wed Sep 16 04:00:12 2015
From: dstr7320 at uni.sydney.edu.au (Dario Strbenac)
Date: Wed, 16 Sep 2015 02:00:12 +0000
Subject: [R] by Function Result Factor Levels
Message-ID: <SN1PR0101MB14399AA4324097CAF3A8F9EBCD5B0@SN1PR0101MB1439.prod.exchangelabs.com>

Good day,

How is it possible to get a data.frame of factor levels used for obtaining each element of the result of the by function ? For example,

result <- by(warpbreaks[, 1],   warpbreaks[, -1], summary)
> result
wool: A
tension: L
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  25.00   26.00   51.00   44.56   54.00   70.00 
 ...

I'd like to obtain a data.frame of the two columns, wool and tension, specifying the level of each factor that corresponds to each element of result.

--------------------------------------
Dario Strbenac
PhD Student
University of Sydney
Camperdown NSW 2050
Australia


From wdunlap at tibco.com  Wed Sep 16 05:52:40 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 15 Sep 2015 20:52:40 -0700
Subject: [R] by Function Result Factor Levels
In-Reply-To: <SN1PR0101MB14399AA4324097CAF3A8F9EBCD5B0@SN1PR0101MB1439.prod.exchangelabs.com>
References: <SN1PR0101MB14399AA4324097CAF3A8F9EBCD5B0@SN1PR0101MB1439.prod.exchangelabs.com>
Message-ID: <CAF8bMcbbdTTOM-NyMtk_KY7xtGBvYj63uhnKCwncUmstTkZPog@mail.gmail.com>

Do you want something like the following?

> library(dplyr, quietly=TRUE, warn.conflicts=FALSE)
> warpbreaks %>% group_by(wool, tension) %>% summarize(Min=min(breaks), Median=median(breaks), Max=max(breaks))
Source: local data frame [6 x 5]
Groups: wool

  wool tension Min Median Max
1    A       L  25     51  70
2    A       M  12     21  36
3    A       H  10     24  43
4    B       L  14     29  44
5    B       M  16     28  42
6    B       H  13     17  28

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, Sep 15, 2015 at 7:00 PM, Dario Strbenac
<dstr7320 at uni.sydney.edu.au> wrote:
> Good day,
>
> How is it possible to get a data.frame of factor levels used for obtaining each element of the result of the by function ? For example,
>
> result <- by(warpbreaks[, 1],   warpbreaks[, -1], summary)
>> result
> wool: A
> tension: L
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   25.00   26.00   51.00   44.56   54.00   70.00
>  ...
>
> I'd like to obtain a data.frame of the two columns, wool and tension, specifying the level of each factor that corresponds to each element of result.
>
> --------------------------------------
> Dario Strbenac
> PhD Student
> University of Sydney
> Camperdown NSW 2050
> Australia
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From peter.anthoni at kit.edu  Wed Sep 16 07:40:03 2015
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Wed, 16 Sep 2015 05:40:03 +0000
Subject: [R] Multiple if function
In-Reply-To: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CB32FC05-3A49-4A59-8211-3257EE907EBD@kit.edu>

Hi Maria, 

Why not exploit some simple boolean facts (FALSE=0, TRUE=1) in your calculation, so

ASBclass = c(1,2,2,3,2,1)
Flow = c(1,1,1,1,1,1)

factor = ((ASBclass==1) * 0.1 + (ASBclass==2) * 0.15 + (ASBclass==3) * 0.2)
deviation = factor * Flow

cheers
Peter



> On 15 Sep 2015, at 12:56, Maria Lathouri <mlathouri at yahoo.gr> wrote:
> 
> Dear all, 
> 
> I am writing as I would like your help. I have a dataframe with two columns, ASB and Flow, where the the first one has values 1, 2 or 3 and the second flow data. Something like that:
> ASBclass    Flow1              11.51               9.2
> 2              10.5
> 3               6.7  ...              ...
> I would like to produce a third column named eg. deviation where it would get me values based on if ASBclass is 1, multiply Flow by 0.1; if ASBclass is 2 then multiply Flow by 0.15 and if ASBclass is 3 then multiply by 0.2.
> 
> If (ASBclass=1) { deviation<-Flow*0.1}
> If (ASBclass=2) { deviation<-Flow*0.15}If (ASBclass=1) { deviation<-Flow*0.2}
> I am not sure whether I should add the else function and how can I combine these separate functions.
> 
> Can anyone help me on that?
> Thank you very much. 
> 
> Kind regardsMaria
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From peter.anthoni at kit.edu  Wed Sep 16 07:56:38 2015
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Wed, 16 Sep 2015 05:56:38 +0000
Subject: [R] Multiple if function
In-Reply-To: <alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
Message-ID: <865084AF-2702-4453-94D3-AE4A0EC45DB5@kit.edu>

Hi,

I guess this might work too and might be quite speedy:

ASBclass = factor(c(1,2,2,3,2,1))
Flow = c(1,1,1,1,1,1)

mult = ((ASBclass==1) * 0.1 + (ASBclass==2) * 0.15 + (ASBclass==3) * 0.2)
deviation = mult * Flow

or with the more complex arithmetic:

deviation = ((ASBclass==1) * (Flow*2) + (ASBclass==2) * (Flow+3) + (ASBclass==3) * sqrt(Flow))

cheers
Peter



> On 16 Sep 2015, at 04:20, Charles C. Berry <ccberry at ucsd.edu> wrote:
> 
> On Tue, 15 Sep 2015, Bert Gunter wrote:
> 
>> Thanks to both Davids.
>> 
>> I realize that these things are often a matter of aesthetics -- and
>> hence have little rational justification -- but I agree with The Other
>> David: eval(parse) seems to me to violate R's soul( it makes R a macro
>> language instead of a functional one).
>> 
>> However, mapply(... switch) effectively loops through the frame row by
>> row. Aesthetically, I like it; but it seems inefficient. If there are
>> e.g. 1e6 rows in say 10 categories, I think Jeff's approach should do
>> much better.  I'll try to generate some actual data to see unless
>> someone else beats me to it.
> 
> Use mapply like this on large problems:
> 
> unsplit(
>    mapply(
>        function(x,z) eval( x, list( y=z )),
>        expression( A=y*2, B=y+3, C=sqrt(y) ),
>        split( dat$Flow, dat$ASB ),
>        SIMPLIFY=FALSE),
>    dat$ASB)
> 
> Chuck
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Wed Sep 16 09:40:59 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 16 Sep 2015 09:40:59 +0200
Subject: [R] Multiple if function
In-Reply-To: <865084AF-2702-4453-94D3-AE4A0EC45DB5@kit.edu>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
	<865084AF-2702-4453-94D3-AE4A0EC45DB5@kit.edu>
Message-ID: <9803D422-0B5F-4BA7-BBF8-77AF502A1EB2@gmail.com>

I think a more common idiom for the simpler case would be to use indexing

vals <- c(0.1, 0.15, 0.2) 
mult <- vals[ASBclass]

(However, some people are on the move to enforce 

mult <- vals[as.numeric(ASBclass)]

because people who confuse factors and character variables get even more confused about factor indexing being different from character indexing.)

For the more complex cases, I think Chuck's split/unsplit principle is the ticket. For one thing, you avoid silliness like (X >= 0) * sqrt(X) + (X < 0) * -sqrt(-X) coming out with warnings from calculating the non-selected alternative.

-pd

> On 16 Sep 2015, at 07:56 , Anthoni, Peter (IMK) <peter.anthoni at kit.edu> wrote:
> 
> Hi,
> 
> I guess this might work too and might be quite speedy:
> 
> ASBclass = factor(c(1,2,2,3,2,1))
> Flow = c(1,1,1,1,1,1)
> 
> mult = ((ASBclass==1) * 0.1 + (ASBclass==2) * 0.15 + (ASBclass==3) * 0.2)
> deviation = mult * Flow
> 
> or with the more complex arithmetic:
> 
> deviation = ((ASBclass==1) * (Flow*2) + (ASBclass==2) * (Flow+3) + (ASBclass==3) * sqrt(Flow))
> 
> cheers
> Peter
> 
> 
> 
>> On 16 Sep 2015, at 04:20, Charles C. Berry <ccberry at ucsd.edu> wrote:
>> 
>> On Tue, 15 Sep 2015, Bert Gunter wrote:
>> 
>>> Thanks to both Davids.
>>> 
>>> I realize that these things are often a matter of aesthetics -- and
>>> hence have little rational justification -- but I agree with The Other
>>> David: eval(parse) seems to me to violate R's soul( it makes R a macro
>>> language instead of a functional one).
>>> 
>>> However, mapply(... switch) effectively loops through the frame row by
>>> row. Aesthetically, I like it; but it seems inefficient. If there are
>>> e.g. 1e6 rows in say 10 categories, I think Jeff's approach should do
>>> much better.  I'll try to generate some actual data to see unless
>>> someone else beats me to it.
>> 
>> Use mapply like this on large problems:
>> 
>> unsplit(
>>   mapply(
>>       function(x,z) eval( x, list( y=z )),
>>       expression( A=y*2, B=y+3, C=sqrt(y) ),
>>       split( dat$Flow, dat$ASB ),
>>       SIMPLIFY=FALSE),
>>   dat$ASB)
>> 
>> Chuck
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ligges at statistik.tu-dortmund.de  Wed Sep 16 11:15:28 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 16 Sep 2015 11:15:28 +0200
Subject: [R] help with old libraries
In-Reply-To: <CADFuJLjr5jUv7v=0_4OTsvTuWHCjFJBXVtgwsLEd8F5pgCTmCA@mail.gmail.com>
References: <CADFuJLjr5jUv7v=0_4OTsvTuWHCjFJBXVtgwsLEd8F5pgCTmCA@mail.gmail.com>
Message-ID: <55F93330.3040207@statistik.tu-dortmund.de>



On 15.09.2015 11:54, Pau Marc Mu?oz Torres wrote:
> Hello everybody,
>
>   I want to use Rapidr package, it is an old package that uses the
> package requires GenomicRanges version 1.14.4. The current version of the
> package is GenomicRanges 1.20.6. There is some way of having both the
> actual and the previous packages installed? I tried to install the package
> locally, but I had problems with dependences, how can I deal with it? Would
> be possible have both versions installed and choose which one to use?


Install the otehr version in a separate library and specify in your call 
to require() or library() which library to use.

Best,
Uwe Ligges


>
> Thanks in advance
>
> Pau Marc Mu?oz Torres
> skype: pau_marc
> http://www.linkedin.com/in/paumarc
> http://www.researchgate.net/profile/Pau_Marc_Torres3/info/
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wjhopper510 at gmail.com  Wed Sep 16 16:20:28 2015
From: wjhopper510 at gmail.com (Will Hopper)
Date: Wed, 16 Sep 2015 10:20:28 -0400
Subject: [R] Drop in a Loop
In-Reply-To: <1442344333.30741.YahooMailBasic@web161606.mail.bf1.yahoo.com>
References: <CAGTXQPuVeaJxXXZvF_d+aOYiUPaYUOpXb+VFFDicPTxe2aqt3A@mail.gmail.com>
	<1442344333.30741.YahooMailBasic@web161606.mail.bf1.yahoo.com>
Message-ID: <CAGTXQPu=Z2oYTHk=oj-nhc3hRtuF+CQvJCKP6Twz5yF+Sg+DPg@mail.gmail.com>

The data structure returned by optimx (that you're storing as gmmiv) is
like a data frame, and stores the conversion code in the field  convcode.
So do something like this:

gmmiv =Optimx()
if (gmmiv$convcode ==0) {
  store[j,] = coef(gmmiv)
}

On Tue, Sep 15, 2015 at 3:12 PM, Olu Ola <oluola2011 at yahoo.com> wrote:

> Thanks Will.
>  Below is the flow of my code
>
> Yhat is the fitted value
> Errhat is the difference between the dependent variable and the yhat
> gmmdata is the data name
>
> N <- nrow(gmmdata)
> B <- 1000
> store <- matrix(0,B,11)
> for (j in 1:B) {
>   index = sample(1:N, N, replace=T)
>   errnew = errhat[index]
>   yt = yhat + errnew
> objective function subroutine
> gradient function subroutine
> gmmiv =Optimx()
>   store[j,] = coef(gmmiv)
> }
>
> What I want to do is that if the convergence code from optimx for a
> particular iteration is Not zero, then it should not be stored in store[j,].
>
> Any help will be appreciated
>
> Thank you
>
>
>
>
>
>
> --------------------------------------------
> On Tue, 9/15/15, Will Hopper <wjhopper510 at gmail.com> wrote:
>
>  Subject: Re: [R] Drop in a Loop
>  To: "Olu Ola" <oluola2011 at yahoo.com>
>  Cc: r-help at r-project.org
>  Date: Tuesday, September 15, 2015, 2:30 PM
>
>  I
>  think you ought to show a small example of how the code
>  you're using. Are you saving results at every iteration?
>  In a list, data frame, etc? People likely need that to help
>  answer your question.
>
>   Also probably have a look the control list
>  argument and the save.failures option, that might be
>  something you're interested in.
>
>  - Will
>
>  On Tue, Sep 15, 2015 at
>  1:34 PM, Olu Ola via R-help <r-help at r-project.org>
>  wrote:
>  Hello,
>
>  I am doing some estimation using optimx and after each round
>  of estimation, I store the coefficient. However, I need to
>  drop the set of coefficients for which the convergence code
>  in optimx is GREATER than Zero. How do I go about this?
>
>
>
>  A way forward will be highly appreciated.
>
>
>
>  Thank you
>
>
>
>  ______________________________________________
>
>  R-help at r-project.org
>  mailing list -- To UNSUBSCRIBE and more, see
>
>  https://stat.ethz.ch/mailman/listinfo/r-help
>
>  PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>
>  and provide commented, minimal, self-contained, reproducible
>  code.
>
>
>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Sep 16 16:41:31 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Sep 2015 07:41:31 -0700
Subject: [R] Multiple if function
In-Reply-To: <9803D422-0B5F-4BA7-BBF8-77AF502A1EB2@gmail.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
	<865084AF-2702-4453-94D3-AE4A0EC45DB5@kit.edu>
	<9803D422-0B5F-4BA7-BBF8-77AF502A1EB2@gmail.com>
Message-ID: <CAGxFJbReNe0E2=2QYM6iRaksZjZwaXPL1wuUjmsGbiY4XMqkmw@mail.gmail.com>

Yes! Chuck's use of mapply is exactly the split/combine strategy I was
looking for. In retrospect, exactly how one should think about it.
Many thanks to all for a constructive discussion .

-- Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 16, 2015 at 12:40 AM, peter dalgaard <pdalgd at gmail.com> wrote:
> I think a more common idiom for the simpler case would be to use indexing
>
> vals <- c(0.1, 0.15, 0.2)
> mult <- vals[ASBclass]
>
> (However, some people are on the move to enforce
>
> mult <- vals[as.numeric(ASBclass)]
>
> because people who confuse factors and character variables get even more confused about factor indexing being different from character indexing.)
>
> For the more complex cases, I think Chuck's split/unsplit principle is the ticket. For one thing, you avoid silliness like (X >= 0) * sqrt(X) + (X < 0) * -sqrt(-X) coming out with warnings from calculating the non-selected alternative.
>
> -pd
>
>> On 16 Sep 2015, at 07:56 , Anthoni, Peter (IMK) <peter.anthoni at kit.edu> wrote:
>>
>> Hi,
>>
>> I guess this might work too and might be quite speedy:
>>
>> ASBclass = factor(c(1,2,2,3,2,1))
>> Flow = c(1,1,1,1,1,1)
>>
>> mult = ((ASBclass==1) * 0.1 + (ASBclass==2) * 0.15 + (ASBclass==3) * 0.2)
>> deviation = mult * Flow
>>
>> or with the more complex arithmetic:
>>
>> deviation = ((ASBclass==1) * (Flow*2) + (ASBclass==2) * (Flow+3) + (ASBclass==3) * sqrt(Flow))
>>
>> cheers
>> Peter
>>
>>
>>
>>> On 16 Sep 2015, at 04:20, Charles C. Berry <ccberry at ucsd.edu> wrote:
>>>
>>> On Tue, 15 Sep 2015, Bert Gunter wrote:
>>>
>>>> Thanks to both Davids.
>>>>
>>>> I realize that these things are often a matter of aesthetics -- and
>>>> hence have little rational justification -- but I agree with The Other
>>>> David: eval(parse) seems to me to violate R's soul( it makes R a macro
>>>> language instead of a functional one).
>>>>
>>>> However, mapply(... switch) effectively loops through the frame row by
>>>> row. Aesthetically, I like it; but it seems inefficient. If there are
>>>> e.g. 1e6 rows in say 10 categories, I think Jeff's approach should do
>>>> much better.  I'll try to generate some actual data to see unless
>>>> someone else beats me to it.
>>>
>>> Use mapply like this on large problems:
>>>
>>> unsplit(
>>>   mapply(
>>>       function(x,z) eval( x, list( y=z )),
>>>       expression( A=y*2, B=y+3, C=sqrt(y) ),
>>>       split( dat$Flow, dat$ASB ),
>>>       SIMPLIFY=FALSE),
>>>   dat$ASB)
>>>
>>> Chuck
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jon.skoien at jrc.ec.europa.eu  Wed Sep 16 16:45:21 2015
From: jon.skoien at jrc.ec.europa.eu (Jon Skoien)
Date: Wed, 16 Sep 2015 16:45:21 +0200
Subject: [R] setTxtProgressBar clearing console (Windows RGui)
Message-ID: <55F98081.1020401@jrc.ec.europa.eu>

I have frequently noticed a strange effect when using the progress bar 
in a loop, that the console will sometimes be cleared. Whatever was 
printed before will then be inaccessible, which can be annoying when I 
want to check the progress and possible issues in a long script. It 
seems to be an intermittent problem, so it could also be related to my 
computer or other software running. I have had this issue in several R 
versions, and managed to provoke it in 3.2.2 with the following:

testit <- function(x = 100, ...)
{
     pb <- txtProgressBar(0, x, ...)
     for(i in 0:x) {Sys.sleep(0.1); setTxtProgressBar(pb, i)}
     Sys.sleep(4)
     close(pb)
     print(1:x)
     print(1:x)
     print(1:x)
     print(length(x))
}

for (i in 1:1000) {
   testit(style = 3)
}


The printing is just to fill up the console quicker, as the clearing 
seems to occur more frequent then. However, the loop above can sometimes 
call testit 5-30 times before the console is cleared again. In my real 
script the console is usually cleared more frequent.
With x = 10 above, it seems not to clear the console, at least not for a 
long time.

Does anyone have a clue why this happens, or if something can be done to 
prevent it? Is it reproducible, or something that only bothers me?

Thanks,
Jon

BTW:
 > sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base







-- 
Jon Olav Sk?ien
Joint Research Centre - European Commission
Institute for Environment and Sustainability (IES)
Climate Risk Management Unit

Via Fermi 2749, TP 100-01,  I-21027 Ispra (VA), ITALY

jon.skoien at jrc.ec.europa.eu
Tel:  +39 0332 789205

Disclaimer: Views expressed in this email are those of the individual 
and do not necessarily represent official views of the European Commission.


From dstr7320 at uni.sydney.edu.au  Wed Sep 16 08:00:25 2015
From: dstr7320 at uni.sydney.edu.au (Dario Strbenac)
Date: Wed, 16 Sep 2015 06:00:25 +0000
Subject: [R] by Function Result Factor Levels
In-Reply-To: <CAF8bMcbbdTTOM-NyMtk_KY7xtGBvYj63uhnKCwncUmstTkZPog@mail.gmail.com>
References: <SN1PR0101MB14399AA4324097CAF3A8F9EBCD5B0@SN1PR0101MB1439.prod.exchangelabs.com>,
	<CAF8bMcbbdTTOM-NyMtk_KY7xtGBvYj63uhnKCwncUmstTkZPog@mail.gmail.com>
Message-ID: <SN1PR0101MB1439075EF58977AFAA788DF5CD5B0@SN1PR0101MB1439.prod.exchangelabs.com>

Good day,

Yes, exactly. I found that aggregate is another alternative which doesn't require a package dependency, although the column formatting is less suitable, always prepending x.

aggregate(warpbreaks[, 1], warpbreaks[, 2:3], function(breaks) c(Min = min(breaks), Med = median(breaks), Max = max(breaks)))
  wool tension x.Min x.Med x.Max
1    A       L    25    51    70
2    B       L    14    29    44
3    A       M    12    21    36
4    B       M    16    28    42
5    A       H    10    24    43
6    B       H    13    17    28

--------------------------------------
Dario Strbenac
PhD Student
University of Sydney
Camperdown NSW 2050
Australia

From hnorpois at gmail.com  Wed Sep 16 12:00:02 2015
From: hnorpois at gmail.com (Hermann Norpois)
Date: Wed, 16 Sep 2015 12:00:02 +0200
Subject: [R] mtext in the top left of margin
Message-ID: <CAKyZeBtNGXnj7WQJQR7+PET3Y7pmAC2bKNDqgd0gij4c_oL8_g@mail.gmail.com>

Hello,

for a multiple figures plot I am looking for the syntax to put text in the
top left of the margin (of the plot). I want my testfunction plot.figure to
place mtext in the top left of the red margin (created by box("figure",
col="red")).

Can anybody help?

Thanks
Hermann

plot.figure <- function ()
    {


        par (mfrow=c(3,1))
        par (mar=c(3,3,1,0.5))
        par (oma=c(1,1,1,1))
        par (mar=c(3,5,3,2))

        plot (dnorm, from=-4, to=4, main="Test")
        box ("plot", col="grey")
        box ("figure", col="red")
        box ("outer", col="blue")
        mtext ("A", side=3, adj=0, line=1.5)

        plot (dnorm, from=-4, to=4)
        box ("plot", col="grey")
        box ("figure", col="red")
        box ("outer", col="blue")
 mtext ("B", side=3, adj=0, line=1)
        plot (dnorm, from=-4, to=4)
        box ("plot", col="grey")
        box ("figure", col="red")
        box ("outer", col="blue")
mtext ("C", side=3, adj=0)
    }

	[[alternative HTML version deleted]]


From michel.kodobetti at ca-cib.com  Wed Sep 16 18:07:23 2015
From: michel.kodobetti at ca-cib.com (KODO BETTI, Michel (CA-CIB))
Date: Wed, 16 Sep 2015 16:07:23 +0000
Subject: [R] [Help] rugarch loading
Message-ID: <971785F0A418AE4095938EA3E98AC0FBC1D3C7@AMMXNY004.AMERICAS.cib>

Good afternoon to all,

I have installed rugarch and all the packages related to it, particularly truncnorm and all the installations went just fine, but when I try to load it I get this error:
Error in FUN(X[[i]], ...) :
  no such symbol do_dtruncnorm in package C:/Users/ut23g7/Documents/R/Libraries/truncnorm/libs/x64/truncnorm.dll
Error: package or namespace load failed for 'rugarch'

Does anyone have an idea of how I can resolve this issue?
I work on Windows 7 64-bit and I use R-3.2.2.

Regards,

Michel Kodo Betti.

This message and any attachments are intended for the sole use of its addressee.
If you are not the addressee, please immediately notify the sender and then destroy the message.
As this message and/or any attachments may have been altered without our knowledge, its content is not legally binding on Cr?dit Agricole Corporate and Investment Bank.
All rights reserved.

 
Ce message et ses pi?ces jointes sont destin?s ? l'usage exclusif de leur destinataire.
Si vous recevez ce message par erreur, merci d'en aviser imm?diatement l'exp?diteur et de le d?truire ensuite.
Le pr?sent message pouvant ?tre alt?r? ? notre insu, Cr?dit Agricole Corporate and Investment Bank ne peut pas ?tre engag? par son contenu.
Tous droits r?serv?s.

"Dodd-Frank Disclosure for U.S. Persons pursuant to CFTC Regulation 23.431:

In addition to the terms and conditions and such other information related to the Transaction(s) described in this communication, Credit Agricole Corporate and Investment Bank ("CA-CIB") provides
"Additional Materials and Disclosures" by the means set forth below.  If you are an entity other than a swap dealer, a major swap participant, a securities based swap dealer and/or a major
securities based swap participant, your execution of a Transaction described in this communication with CA-CIB shall be an acknowledgement and agreement by you of the following:

1. The adequacy of the means by which the "Additional Materials and Disclosures" are or were delivered;
2. that such "Additional Materials and Disclosures" are or were made available to you on the basis that you are a U.S. person to whom such content is provided in respect of CFTC Rule 23.431 (a), or
such analogous SEC rule; and
3. you have read and understood the relevant "Additional Materials and Disclosures", and you confirm that they along with other information related to the Transaction(s) included herein are
adequate for purposes of your evaluation of such Transaction(s).

The "Additional Materials and Disclosures" may be found at our website at http://www.ca-cib.com/group-overview/dodd-frank-otc-derivatives.htm


	[[alternative HTML version deleted]]


From thanoon.younis80 at gmail.com  Wed Sep 16 19:11:01 2015
From: thanoon.younis80 at gmail.com (thanoon younis)
Date: Wed, 16 Sep 2015 20:11:01 +0300
Subject: [R] generate ordered categorical variable in R
Message-ID: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>

Dear R- users

I want to generate ordered categorical variable vector with 200x1 dimension
and from 1 to 4 categories and i tried with this code

Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
decimals like 1.244, 2.342,4,321 and so on ... My question how can i
generate a vector and also a matrix with orered categorical variables and
without decimals just 1,2,3 ,4 ,1,2,3,4, ....

Many thanks in advance





-- 
Thanoon Y. Thanoon
PhD Candidate
Department of Mathematical Sciences
Faculty of Science
University Technology Malaysia, UTM
E.Mail: Thanoon.younis80 at gmail.com
E.Mail: dawn_prayer80 at yahoo.com
Facebook:Thanoon Younis AL-Shakerchy
Twitter: Thanoon Alshakerchy
H.P:00601127550205

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Wed Sep 16 19:26:27 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 16 Sep 2015 18:26:27 +0100
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
Message-ID: <55F9A643.7010607@dewey.myzen.co.uk>

If I understand correctly

?sample


On 16/09/2015 18:11, thanoon younis wrote:
> Dear R- users
>
> I want to generate ordered categorical variable vector with 200x1 dimension
> and from 1 to 4 categories and i tried with this code
>
> Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
> decimals like 1.244, 2.342,4,321 and so on ... My question how can i
> generate a vector and also a matrix with orered categorical variables and
> without decimals just 1,2,3 ,4 ,1,2,3,4, ....
>
> Many thanks in advance
>
>
>
>
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dcarlson at tamu.edu  Wed Sep 16 19:29:35 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 16 Sep 2015 17:29:35 +0000
Subject: [R] by Function Result Factor Levels
In-Reply-To: <SN1PR0101MB1439075EF58977AFAA788DF5CD5B0@SN1PR0101MB1439.prod.exchangelabs.com>
References: <SN1PR0101MB14399AA4324097CAF3A8F9EBCD5B0@SN1PR0101MB1439.prod.exchangelabs.com>,
	<CAF8bMcbbdTTOM-NyMtk_KY7xtGBvYj63uhnKCwncUmstTkZPog@mail.gmail.com>
	<SN1PR0101MB1439075EF58977AFAA788DF5CD5B0@SN1PR0101MB1439.prod.exchangelabs.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C297B@mb02.ads.tamu.edu>

Actually x is the variable name since your function returned a vector of three values: 

> tbl <- aggregate(warpbreaks[, 1], warpbreaks[, 2:3], function(breaks) c(Min = min(breaks),
+  Med = median(breaks), Max = max(breaks)))
> str(tbl)
'data.frame':   6 obs. of  3 variables:
 $ wool   : Factor w/ 2 levels "A","B": 1 2 1 2 1 2
 $ tension: Factor w/ 3 levels "L","M","H": 1 1 2 2 3 3
 $ x      : num [1:6, 1:3] 25 14 12 16 10 13 51 29 21 28 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : NULL
  .. ..$ : chr  "Min" "Med" "Max"

You have two options. One is to convert the matrix to three separate columns:

> tbl2 <- data.frame(tbl[, 1:2], tbl$x)
> str(tbl2)
'data.frame':   6 obs. of  5 variables:
 $ wool   : Factor w/ 2 levels "A","B": 1 2 1 2 1 2
 $ tension: Factor w/ 3 levels "L","M","H": 1 1 2 2 3 3
 $ Min    : num  25 14 12 16 10 13
 $ Med    : num  51 29 21 28 24 17
 $ Max    : num  70 44 36 42 43 28
> tbl2
  wool tension Min Med Max
1    A       L  25  51  70
2    B       L  14  29  44
3    A       M  12  21  36
4    B       M  16  28  42
5    A       H  10  24  43
6    B       H  13  17  28

The other is to change the name of x to something more informative:

> names(tbl)[3] <- "breaks"
> str(tbl)
'data.frame':   6 obs. of  3 variables:
 $ wool   : Factor w/ 2 levels "A","B": 1 2 1 2 1 2
 $ tension: Factor w/ 3 levels "L","M","H": 1 1 2 2 3 3
 $ breaks : num [1:6, 1:3] 25 14 12 16 10 13 51 29 21 28 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : NULL
  .. ..$ : chr  "Min" "Med" "Max"
> tbl
  wool tension breaks.Min breaks.Med breaks.Max
1    A       L         25         51         70
2    B       L         14         29         44
3    A       M         12         21         36
4    B       M         16         28         42
5    A       H         10         24         43
6    B       H         13         17         28

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dario Strbenac
Sent: Wednesday, September 16, 2015 1:00 AM
To: William Dunlap
Cc: r-help at R-project.org
Subject: Re: [R] by Function Result Factor Levels

Good day,

Yes, exactly. I found that aggregate is another alternative which doesn't require a package dependency, although the column formatting is less suitable, always prepending x.

aggregate(warpbreaks[, 1], warpbreaks[, 2:3], function(breaks) c(Min = min(breaks), Med = median(breaks), Max = max(breaks)))
  wool tension x.Min x.Med x.Max
1    A       L    25    51    70
2    B       L    14    29    44
3    A       M    12    21    36
4    B       M    16    28    42
5    A       H    10    24    43
6    B       H    13    17    28

--------------------------------------
Dario Strbenac
PhD Student
University of Sydney
Camperdown NSW 2050
Australia
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Wed Sep 16 19:30:05 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Wed, 16 Sep 2015 18:30:05 +0100
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
Message-ID: <55F9A71D.8010103@sapo.pt>

Hello,

Try ?sample.

Hope this helps,

Rui Barradas

Em 16-09-2015 18:11, thanoon younis escreveu:
> Dear R- users
>
> I want to generate ordered categorical variable vector with 200x1 dimension
> and from 1 to 4 categories and i tried with this code
>
> Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
> decimals like 1.244, 2.342,4,321 and so on ... My question how can i
> generate a vector and also a matrix with orered categorical variables and
> without decimals just 1,2,3 ,4 ,1,2,3,4, ....
>
> Many thanks in advance
>
>
>
>
>


From dcarlson at tamu.edu  Wed Sep 16 19:33:22 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 16 Sep 2015 17:33:22 +0000
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C29AE@mb02.ads.tamu.edu>

There is a version of sample especially for integers:

> Q1 <- matrix(sample.int(4, 200, replace=TRUE), 200)
> str(Q1)
 int [1:200, 1] 1 4 4 2 3 3 4 4 2 3 ...

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of thanoon younis
Sent: Wednesday, September 16, 2015 12:11 PM
To: r-help at r-project.org
Subject: [R] generate ordered categorical variable in R

Dear R- users

I want to generate ordered categorical variable vector with 200x1 dimension
and from 1 to 4 categories and i tried with this code

Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
decimals like 1.244, 2.342,4,321 and so on ... My question how can i
generate a vector and also a matrix with orered categorical variables and
without decimals just 1,2,3 ,4 ,1,2,3,4, ....

Many thanks in advance





-- 
Thanoon Y. Thanoon
PhD Candidate
Department of Mathematical Sciences
Faculty of Science
University Technology Malaysia, UTM
E.Mail: Thanoon.younis80 at gmail.com
E.Mail: dawn_prayer80 at yahoo.com
Facebook:Thanoon Younis AL-Shakerchy
Twitter: Thanoon Alshakerchy
H.P:00601127550205

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Wed Sep 16 19:33:31 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 16 Sep 2015 12:33:31 -0500
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
Message-ID: <2000F935-86ED-4196-BB6B-07CB79EBA199@me.com>


> On Sep 16, 2015, at 12:11 PM, thanoon younis <thanoon.younis80 at gmail.com> wrote:
> 
> Dear R- users
> 
> I want to generate ordered categorical variable vector with 200x1 dimension
> and from 1 to 4 categories and i tried with this code
> 
> Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
> decimals like 1.244, 2.342,4,321 and so on ... My question how can i
> generate a vector and also a matrix with orered categorical variables and
> without decimals just 1,2,3 ,4 ,1,2,3,4, ....
> 
> Many thanks in advance


You are sampling from a uniform distribution on the continuous interval from 1 to 4.

See ?sample

set.seed(1)

> sample(4, 200, replace = TRUE)
  [1] 2 2 3 4 1 4 4 3 3 1 1 1 3 2 4 2 3 4 2 4 4 1 3 1 2 2 1 2 4 2 2 3 2
 [34] 1 4 3 4 1 3 2 4 3 4 3 3 4 1 2 3 3 2 4 2 1 1 1 2 3 3 2 4 2 2 2 3 2
 [67] 2 4 1 4 2 4 2 2 2 4 4 2 4 4 2 3 2 2 4 1 3 1 1 1 1 1 3 4 4 4 2 2 4
[100] 3 3 2 2 4 3 1 1 2 4 3 4 3 2 2 1 1 3 1 2 3 4 2 2 1 4 2 3 1 1 3 3 1
[133] 1 3 4 3 3 3 4 3 3 3 1 2 3 2 1 3 1 4 3 3 2 2 3 1 3 1 2 1 2 4 2 4 4
[166] 2 1 2 3 2 3 4 4 2 2 4 3 3 3 4 2 1 4 3 4 1 4 3 4 3 3 2 1 4 2 3 1 4
[199] 2 4

Here, the result is a vector of integers.


set.seed(1)

> sample(factor(1:4), 200, replace = TRUE)
  [1] 2 2 3 4 1 4 4 3 3 1 1 1 3 2 4 2 3 4 2 4 4 1 3 1 2 2 1 2 4 2 2 3 2
 [34] 1 4 3 4 1 3 2 4 3 4 3 3 4 1 2 3 3 2 4 2 1 1 1 2 3 3 2 4 2 2 2 3 2
 [67] 2 4 1 4 2 4 2 2 2 4 4 2 4 4 2 3 2 2 4 1 3 1 1 1 1 1 3 4 4 4 2 2 4
[100] 3 3 2 2 4 3 1 1 2 4 3 4 3 2 2 1 1 3 1 2 3 4 2 2 1 4 2 3 1 1 3 3 1
[133] 1 3 4 3 3 3 4 3 3 3 1 2 3 2 1 3 1 4 3 3 2 2 3 1 3 1 2 1 2 4 2 4 4
[166] 2 1 2 3 2 3 4 4 2 2 4 3 3 3 4 2 1 4 3 4 1 4 3 4 3 3 2 1 4 2 3 1 4
[199] 2 4
Levels: 1 2 3 4

Here, the result is a factor.


set.seed(1)

> sample(factor(c("A", "B", "C", "D")), 200, replace = TRUE)
  [1] B B C D A D D C C A A A C B D B C D B D D A C A B B A B D B B C B
 [34] A D C D A C B D C D C C D A B C C B D B A A A B C C B D B B B C B
 [67] B D A D B D B B B D D B D D B C B B D A C A A A A A C D D D B B D
[100] C C B B D C A A B D C D C B B A A C A B C D B B A D B C A A C C A
[133] A C D C C C D C C C A B C B A C A D C C B B C A C A B A B D B D D
[166] B A B C B C D D B B D C C C D B A D C D A D C D C C B A D B C A D
[199] B D
Levels: A B C D

Here, the result is a factor.


Just depends upon what you want for the categorial variable.

Regards,

Marc Schwartz


From jvadams at usgs.gov  Wed Sep 16 20:04:15 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Wed, 16 Sep 2015 13:04:15 -0500
Subject: [R] mtext in the top left of margin
In-Reply-To: <CAKyZeBtNGXnj7WQJQR7+PET3Y7pmAC2bKNDqgd0gij4c_oL8_g@mail.gmail.com>
References: <CAKyZeBtNGXnj7WQJQR7+PET3Y7pmAC2bKNDqgd0gij4c_oL8_g@mail.gmail.com>
Message-ID: <CAN5YmCGLFHqiceebiTqdfZjfztSb8SRMrLvnxe80qMJ5zFEcfw@mail.gmail.com>

You can use the coordinates of the plot region as fractions of the figure
region, par("plt"), to define the adj= argument of mtext().  And you can
use the number of lines of the plot margin to define the line= argument of
mtext().  For example:

plot.figure <- function() {
  par(mfrow=c(3, 1), mar=c(3, 5, 3, 2), oma=c(1, 1, 1, 1))
  # use the coords of the plot region as fractions of the figure region
  # to define the adj= argument of mtext()
  pplt <- par("plt")
  adjx <- (0 - pplt[1]) / (pplt[2] - pplt[1])
  # use the number of lines of margin to define the line= argument of
mtext()
  liney <- par("mar")[3] - 1.5
  plot(dnorm, from=-4, to=4, main="Test")
    box("plot", col="grey")
    box("figure", col="red")
    mtext("A", side=3, adj=adjx, line=liney)
  plot(dnorm, from=-4, to=4)
    box("plot", col="grey")
    box("figure", col="red")
    mtext("B", side=3, adj=adjx, line=liney)
  plot(dnorm, from=-4, to=4)
    box("plot", col="grey")
    box("figure", col="red")
    mtext("C", side=3, adj=adjx, line=liney)
  box("outer", col="blue")
}
plot.figure()

Jean

On Wed, Sep 16, 2015 at 5:00 AM, Hermann Norpois <hnorpois at gmail.com> wrote:

> Hello,
>
> for a multiple figures plot I am looking for the syntax to put text in the
> top left of the margin (of the plot). I want my testfunction plot.figure to
> place mtext in the top left of the red margin (created by box("figure",
> col="red")).
>
> Can anybody help?
>
> Thanks
> Hermann
>
> plot.figure <- function ()
>     {
>
>
>         par (mfrow=c(3,1))
>         par (mar=c(3,3,1,0.5))
>         par (oma=c(1,1,1,1))
>         par (mar=c(3,5,3,2))
>
>         plot (dnorm, from=-4, to=4, main="Test")
>         box ("plot", col="grey")
>         box ("figure", col="red")
>         box ("outer", col="blue")
>         mtext ("A", side=3, adj=0, line=1.5)
>
>         plot (dnorm, from=-4, to=4)
>         box ("plot", col="grey")
>         box ("figure", col="red")
>         box ("outer", col="blue")
>  mtext ("B", side=3, adj=0, line=1)
>         plot (dnorm, from=-4, to=4)
>         box ("plot", col="grey")
>         box ("figure", col="red")
>         box ("outer", col="blue")
> mtext ("C", side=3, adj=0)
>     }
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Sep 16 20:06:26 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Sep 2015 11:06:26 -0700
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
Message-ID: <CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>

Yikes! The uniform distribution is a **continuous** distribution over
an interval. You seem to want to sample over a discrete distribution.
See ?sample for that, as in:

sample(1:4,100,rep=TRUE)

## or for this special case and faster

sample.int(4,size=100,rep=TRUE)

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 16, 2015 at 10:11 AM, thanoon younis
<thanoon.younis80 at gmail.com> wrote:
> Dear R- users
>
> I want to generate ordered categorical variable vector with 200x1 dimension
> and from 1 to 4 categories and i tried with this code
>
> Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
> decimals like 1.244, 2.342,4,321 and so on ... My question how can i
> generate a vector and also a matrix with orered categorical variables and
> without decimals just 1,2,3 ,4 ,1,2,3,4, ....
>
> Many thanks in advance
>
>
>
>
>
> --
> Thanoon Y. Thanoon
> PhD Candidate
> Department of Mathematical Sciences
> Faculty of Science
> University Technology Malaysia, UTM
> E.Mail: Thanoon.younis80 at gmail.com
> E.Mail: dawn_prayer80 at yahoo.com
> Facebook:Thanoon Younis AL-Shakerchy
> Twitter: Thanoon Alshakerchy
> H.P:00601127550205
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Wed Sep 16 22:10:30 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Sep 2015 13:10:30 -0700
Subject: [R] Looking for Post-hoc tests (a la TukeyHSD) or
	interaction-level independent contrasts for survival analysis.
In-Reply-To: <D5673654417F1D40A0F59F27C64F7CFC8D44E6@SVONKENMBX03.ENT.dfo-mpo.ca>
References: <D5673654417F1D40A0F59F27C64F7CFC8D44E6@SVONKENMBX03.ENT.dfo-mpo.ca>
Message-ID: <994BD7AE-1E28-4E50-A7DC-8EEBD66893DA@comcast.net>


On Sep 15, 2015, at 12:09 PM, Huot, Matthieu wrote:

> Hi Tom
> 
> I know the post is over 7-8 years old but I am having the same question. How to do a post-hoc test like TukeyHSD on coxph type output.

Create a new variable using the `interaction`-function, apply you contrasts to that object, and that should let you side-step all the errors of the person trying to follow Crawley's book.

-- 
David.

> 
> Have you received any info in this matter?
> Thanks
> Matthieu
> 
> Looking for Post-hoc tests (a la TukeyHSD) or interaction-level independent contrasts for survival analysis.
> Thomas Oliver toliver at stanford.edu <mailto:r-help%40r-project.org?Subject=Re%3A%20%5BR%5D%20Looking%20for%20Post-hoc%20tests%20%28a%20la%20TukeyHSD%29%20or%20interaction-level%0A%20independent%20contrasts%20for%20survival%20analysis.&In-Reply-To=%3C6.2.5.6.2.20080429131826.04deaeb0%40stanford.edu%3E>
> Tue Apr 29 23:12:33 CEST 2008
> 
>  *   Previous message: [R] AIC extract and comparison <https://stat.ethz.ch/pipermail/r-help/2008-April/160916.html>
>  *   Next message: [R] for loop in nls function <https://stat.ethz.ch/pipermail/r-help/2008-April/160886.html>
>  *   Messages sorted by: [ date ]<https://stat.ethz.ch/pipermail/r-help/2008-April/date.html#160885> [ thread ]<https://stat.ethz.ch/pipermail/r-help/2008-April/thread.html#160885> [ subject ]<https://stat.ethz.ch/pipermail/r-help/2008-April/subject.html#160885> [ author ]<https://stat.ethz.ch/pipermail/r-help/2008-April/author.html#160885>
> 
> ________________________________
> 
> Hello all R-helpers,
> 
> 
> 
>    I've performed an experiment to test for differential effects of
> 
> elevated temperatures on three different groups of corals.  I'm
> 
> currently performing a cox proportional hazards regression with
> 
> censoring on the survivorship (days to mortality) of each individual
> 
> in the experiment with two factors: Temperature Treatment (2 levels:
> 
> ambient and elevated) and experimental group (3 levels: say 1,2,3).
> 
> 
> 
> In my experiment, all three groups survived equally well in the
> 
> ambient control treatment, but  two of three of the groups succumbed
> 
> to heat stress in the elevated temperature treatment.  I can see that
> 
> the third group had a small degree of mortality, but it appears to be
> 
> significantly less than the other two and may be not significantly
> 
> different from the ambient controls.
> 
> 
> 
> I would like to ask three questions:  1) Is group 3 different from
> 
> controls? 2) Is group 3 different from group 1 and/or group 2 in the
> 
> elevated treatment? and 3) are groups 1 and 2 different from each
> 
> other in the elevated treatment?
> 
> 
> 
>   Because I'm testing for differential effects among the elevated
> 
> temperature treatment group, and "I've seen the data" by now,  the
> 
> analysis would be easiest for me if I performed a responsible
> 
> multiple comparisons test like TukeyHSD to see how each of the six
> 
> Treatment:Group subgroups compared to each other.  TukeyHSD does not
> 
> appear to be defined for outputs from the function coxph -- (see
> 
> survival library).
> 
> 
> 
> cph <- coxph(Surv(DayToMort, Censor) ~ Treatment*Group, data=subb)
> 
> 
> 
> --> Does anyone know of an implementation of TukeyHSD that would
> 
> work, or of another post-hoc multiple comparison test?
> 
> 
> 
> I believe that another responsible tack would be to clearly define
> 
> the contrasts I'd like to make within the interaction term. However
> 
> this has yet to work as fully as I'd like it.
> 
> 
> 
>  I've successfully set the contrasts matrix for the three-level
> 
> factor "Group" following Crawley's The R Book.
> 
> 
> 
> cmat<-cbind(c(-1,1,0),c(0,-1,1))
> 
> contrasts(subb$Group)<-cmat
> 
> contrasts(subb$Group)
> 
> 
> 
> By setting these contrasts and then looking at the interaction terms
> 
> in the coxph model, this allows me to compare groups _within_ each
> 
> separate treatment, and confirms both that #2) that groups 1 and 2
> 
> are not sig. different in the elevated treatment, and #3) the group3
> 
> corals survived significantly better than the other groups in the
> 
> elevated treatment. BUT it does not allow me to say if the group3
> 
> survival is or is not different from its own control.(#1 above).
> 
> 
> 
> To make this comparison, I've tried setting the contrast matrix for
> 
> the Treatment:Group interaction term, with no success.  Whenever I
> 
> attempt to do so, I run the code below:
> 
> 
> 
> cmat<-cbind(c(-1,0,0,1,0,0),c(0,-1,0,0,1,0),c(0,0,-1,0,0,1),c(0,0,0,-1,1,0),c(0,0,0,0,-1,1))
> 
> #Build a matrix
> 
> rownames(cmat)<-rownames(contrasts(subb$Treatment:subb$Group))  #give
> 
> cmat the correct rownames
> 
> colnames(cmat)<-colnames(contrasts(subb$Treatment:subb$Group))
> 
> #give cmat the correct colnames
> 
> contrasts(subb$Treatment:subb$Group)<-cmat           #try to assign cmat
> 
> 
> 
> and I get this error message:
> 
> Error in contrasts(subb$Treatment:subb$Group, ) <- cmat :
> 
>   could not find function ":<-"
> 
> 
> 
> Alternatively I could run:
> 
> options(contrasts=c("contr.sum","contr.poly"))
> 
> where:
> 
>  contrasts(subb$Treatment:subb$Group)
> 
> 
> 
>             [,1] [,2] [,3] [,4] [,5]
> 
> Con:1   1    0    0    0    0
> 
> Con:2    0    1    0    0    0
> 
> Con:3    0    0    1    0    0
> 
> Exp:1    0    0    0    1    0
> 
> Exp:2    0    0    0    0    1
> 
> Exp:3   -1   -1   -1   -1   -1
> 
> 
> 
> But even that doesn't appear to affect the output of :
> 
> 
> 
> cph <- coxph(Surv(DayToMort, Censor) ~ Treatment*Group, data=subb)
> 
> 
> 
> 
> 
> --> Is what I'm trying to do statistically invalid and R is trying to
> 
> quietly save me from statistical destruction, or it is just being a
> 
> pain?  Is there a way around it?
> 
> 
> 
> --> Any other suggestions?
> 
> 
> 
> Many Thanks in Advance,
> 
> 
> 
> Tom Oliver
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From marc_schwartz at me.com  Wed Sep 16 22:28:34 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 16 Sep 2015 15:28:34 -0500
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
	<CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
Message-ID: <C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>


> On Sep 16, 2015, at 1:06 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> Yikes! The uniform distribution is a **continuous** distribution over
> an interval. You seem to want to sample over a discrete distribution.
> See ?sample for that, as in:
> 
> sample(1:4,100,rep=TRUE)
> 
> ## or for this special case and faster
> 
> sample.int(4,size=100,rep=TRUE)


Bert,

I am not sure that it is really faster, since internally, sample() calls sample.int():

> sample
function (x, size, replace = FALSE, prob = NULL) 
{
    if (length(x) == 1L && is.numeric(x) && x >= 1) {
        if (missing(size)) 
            size <- x
        sample.int(x, size, replace, prob)
    }
    else {
        if (missing(size)) 
            size <- length(x)
        x[sample.int(length(x), size, replace, prob)]
    }
}


set.seed(1)

> system.time(x1 <- sample(1e10, 1e8, replace = TRUE))
   user  system elapsed 
  2.755   0.170   2.925 


set.seed(1)
> system.time(x2 <- sample.int(1e10, 1e8, replace = TRUE))
   user  system elapsed 
  2.767   0.183   2.951 


> all(x1 == x2)
[1] TRUE


Regards,

Marc


> 
> Cheers,
> Bert
> 
> Bert Gunter
> 
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>   -- Clifford Stoll
> 
> 
> On Wed, Sep 16, 2015 at 10:11 AM, thanoon younis
> <thanoon.younis80 at gmail.com> wrote:
>> Dear R- users
>> 
>> I want to generate ordered categorical variable vector with 200x1 dimension
>> and from 1 to 4 categories and i tried with this code
>> 
>> Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
>> decimals like 1.244, 2.342,4,321 and so on ... My question how can i
>> generate a vector and also a matrix with orered categorical variables and
>> without decimals just 1,2,3 ,4 ,1,2,3,4, ....
>> 
>> Many thanks in advance


From bgunter.4567 at gmail.com  Wed Sep 16 22:34:13 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Sep 2015 13:34:13 -0700
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
	<CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
	<C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>
Message-ID: <CAGxFJbRDJ8-Qq-Qq5LBKVZ7+xOYW6ovd4-9o+ud3hj-Kvm-yUQ@mail.gmail.com>

Yes. Thanks Marc. I stand corrected.

-- Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 16, 2015 at 1:28 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
>
>> On Sep 16, 2015, at 1:06 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> Yikes! The uniform distribution is a **continuous** distribution over
>> an interval. You seem to want to sample over a discrete distribution.
>> See ?sample for that, as in:
>>
>> sample(1:4,100,rep=TRUE)
>>
>> ## or for this special case and faster
>>
>> sample.int(4,size=100,rep=TRUE)
>
>
> Bert,
>
> I am not sure that it is really faster, since internally, sample() calls sample.int():
>
>> sample
> function (x, size, replace = FALSE, prob = NULL)
> {
>     if (length(x) == 1L && is.numeric(x) && x >= 1) {
>         if (missing(size))
>             size <- x
>         sample.int(x, size, replace, prob)
>     }
>     else {
>         if (missing(size))
>             size <- length(x)
>         x[sample.int(length(x), size, replace, prob)]
>     }
> }
>
>
> set.seed(1)
>
>> system.time(x1 <- sample(1e10, 1e8, replace = TRUE))
>    user  system elapsed
>   2.755   0.170   2.925
>
>
> set.seed(1)
>> system.time(x2 <- sample.int(1e10, 1e8, replace = TRUE))
>    user  system elapsed
>   2.767   0.183   2.951
>
>
>> all(x1 == x2)
> [1] TRUE
>
>
> Regards,
>
> Marc
>
>
>>
>> Cheers,
>> Bert
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>   -- Clifford Stoll
>>
>>
>> On Wed, Sep 16, 2015 at 10:11 AM, thanoon younis
>> <thanoon.younis80 at gmail.com> wrote:
>>> Dear R- users
>>>
>>> I want to generate ordered categorical variable vector with 200x1 dimension
>>> and from 1 to 4 categories and i tried with this code
>>>
>>> Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
>>> decimals like 1.244, 2.342,4,321 and so on ... My question how can i
>>> generate a vector and also a matrix with orered categorical variables and
>>> without decimals just 1,2,3 ,4 ,1,2,3,4, ....
>>>
>>> Many thanks in advance
>


From bgunter.4567 at gmail.com  Wed Sep 16 22:40:34 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Sep 2015 13:40:34 -0700
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <CAGxFJbRDJ8-Qq-Qq5LBKVZ7+xOYW6ovd4-9o+ud3hj-Kvm-yUQ@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
	<CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
	<C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>
	<CAGxFJbRDJ8-Qq-Qq5LBKVZ7+xOYW6ovd4-9o+ud3hj-Kvm-yUQ@mail.gmail.com>
Message-ID: <CAGxFJbRXcbdK2crkepYv0Bp641LShJq6T3OYMV9R13Kf=aMCew@mail.gmail.com>

Nope. Take it back. I stand uncorrected.

> system.time(z <-sample(1:10,1e6, rep=TRUE))
   user  system elapsed
  0.045   0.001   0.047

> system.time(z <-sample.int(10,1e6,rep=TRUE))
   user  system elapsed
  0.012   0.000   0.013


sample() has to do subscripting in the general case; sample.int doesn't.

But I would agree that the difference is likely almost always unnoticeable.


-- Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 16, 2015 at 1:34 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Yes. Thanks Marc. I stand corrected.
>
> -- Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Wed, Sep 16, 2015 at 1:28 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
>>
>>> On Sep 16, 2015, at 1:06 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>
>>> Yikes! The uniform distribution is a **continuous** distribution over
>>> an interval. You seem to want to sample over a discrete distribution.
>>> See ?sample for that, as in:
>>>
>>> sample(1:4,100,rep=TRUE)
>>>
>>> ## or for this special case and faster
>>>
>>> sample.int(4,size=100,rep=TRUE)
>>
>>
>> Bert,
>>
>> I am not sure that it is really faster, since internally, sample() calls sample.int():
>>
>>> sample
>> function (x, size, replace = FALSE, prob = NULL)
>> {
>>     if (length(x) == 1L && is.numeric(x) && x >= 1) {
>>         if (missing(size))
>>             size <- x
>>         sample.int(x, size, replace, prob)
>>     }
>>     else {
>>         if (missing(size))
>>             size <- length(x)
>>         x[sample.int(length(x), size, replace, prob)]
>>     }
>> }
>>
>>
>> set.seed(1)
>>
>>> system.time(x1 <- sample(1e10, 1e8, replace = TRUE))
>>    user  system elapsed
>>   2.755   0.170   2.925
>>
>>
>> set.seed(1)
>>> system.time(x2 <- sample.int(1e10, 1e8, replace = TRUE))
>>    user  system elapsed
>>   2.767   0.183   2.951
>>
>>
>>> all(x1 == x2)
>> [1] TRUE
>>
>>
>> Regards,
>>
>> Marc
>>
>>
>>>
>>> Cheers,
>>> Bert
>>>
>>> Bert Gunter
>>>
>>> "Data is not information. Information is not knowledge. And knowledge
>>> is certainly not wisdom."
>>>   -- Clifford Stoll
>>>
>>>
>>> On Wed, Sep 16, 2015 at 10:11 AM, thanoon younis
>>> <thanoon.younis80 at gmail.com> wrote:
>>>> Dear R- users
>>>>
>>>> I want to generate ordered categorical variable vector with 200x1 dimension
>>>> and from 1 to 4 categories and i tried with this code
>>>>
>>>> Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
>>>> decimals like 1.244, 2.342,4,321 and so on ... My question how can i
>>>> generate a vector and also a matrix with orered categorical variables and
>>>> without decimals just 1,2,3 ,4 ,1,2,3,4, ....
>>>>
>>>> Many thanks in advance
>>


From govokai at gmail.com  Wed Sep 16 22:43:16 2015
From: govokai at gmail.com (Kai Mx)
Date: Wed, 16 Sep 2015 22:43:16 +0200
Subject: [R] aggregate counting variable factors
Message-ID: <CAHOWgNWsu++ShKUrLg+ftMs+81AErNBoyg4xnz4-5sRG-H7YwA@mail.gmail.com>

Hi everybody,

>From a questionnaire, I have a dataset  like this one with some 40 items:

df1 <- data.frame(subject=c('user1','user2', 'user3', 'user4'),
item1=c(0,1,2,5), item2=c(1,2,1,2), item3=c(2,3,4,0), item4=c(0,3,3,2),
item5=c(5,5,5,5))

Users can choose an answer from 0 to 5 for each item.

Now I want to reshape the dataset to have the items in rows and the count
of each of the result factors in columns:

result <- data.frame (item=c("item1", "item2", "item3", "item4", "item5"),
result0=c(1,0,1,1,0), result1=c(1,2,0,0,0), result2=c(1,2,1,1,0),
result3=c(0,0,1,2,0), result4=c(0,0,1,0,0), result5=c(1,0,0,0,4))

I have been fiddling around with melt/plyr, but haven't been able to figure
it out. What's the most elegant way to do this (preferably without typing
in all the item names).

Thanks so much!

Best,

Kai

	[[alternative HTML version deleted]]


From 1sfkan+1c29qq0sj70co at sharklasers.com  Wed Sep 16 20:23:10 2015
From: 1sfkan+1c29qq0sj70co at sharklasers.com (holograph)
Date: Wed, 16 Sep 2015 11:23:10 -0700 (PDT)
Subject: [R] Get R-realisations for a 'custom' distribution
Message-ID: <1442427790049-4712338.post@n4.nabble.com>

Hello all,

I've been busy figuring out how to get realisations for a non-standard
distribution in R. Define \theta=o (not 0). Consider  f_x(x)=o*x for o in
\sqrt{2/o}.

Yet, I can't seem to find a way to get realisations for custom
distributions. Can anyone help me out here? 



--
View this message in context: http://r.789695.n4.nabble.com/Get-R-realisations-for-a-custom-distribution-tp4712338.html
Sent from the R help mailing list archive at Nabble.com.


From ahaywood3 at gmail.com  Wed Sep 16 19:57:24 2015
From: ahaywood3 at gmail.com (andrew haywood)
Date: Thu, 17 Sep 2015 01:57:24 +0800
Subject: [R] Error Message when using VarSelection in YaiImpute package with
	the randomForest
Message-ID: <CAEDDhTA_p0kjQWUmJNrppZ4s30=-hMY24UcpxGCvnexdVcmqDg@mail.gmail.com>

Dear All,

when using the following code

x <- iris[,1:2]  # Sepal.Length Sepal.Width
y <- iris[,3:4]  # Petal.Length Petal.Width
vsel <-
varSelection(x=x,y=y,nboot=5,yaiMethod="randomForest",useParallel=FALSE)


I get the following error code

Error in yai(x = xa, y = y, method = yaiMethod, bootstrap = bootstrap,  :
  object 'xcvRefs' not found


If anybody could tell me what I am doing wrong.

Cheers
Andrew

	[[alternative HTML version deleted]]


From rosita21 at gmail.com  Wed Sep 16 18:45:09 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Wed, 16 Sep 2015 17:45:09 +0100
Subject: [R] HELP IN GRAPHS - slip screen
Message-ID: <F53D3AF7-922F-4352-B554-1E7F5244AB75@gmail.com>

Dear all,

I?m trying to do a graph, 

3 rows, 5 columns, with the design:
#   3   4   5   6 
#                    2
#   7   8   9   10 

I had a code for 3 rows, 3 columns, with the design::
#   3   4   
#            2
#   7   8    
 and I tried to modify it, but I had no success :(

I suppose the problem is in the slip.screen code (red part of the code).

I attach my code, can anyone please help me?


Best,
RO


setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")

library(ggplot2)
library(reshape)
library(lattice)


# read in what looks like half of the data

bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
SE.alpha1<-read.csv("graphs_SE_alpha1.csv")



quartz(width=10,height=6)
# do the first split, to get the rightmost screen for the legend
split.screen(figs=matrix(c(0,0.8,0,1,0.8,1,0,1),nrow=2,byrow=TRUE))
# now split the first screen to get your six screens for the plots



split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
                           0.5,1,0.5,1,#primeira linha segunda coluna
                           0,0.5,0,0.5,#segunda linha primeira coluna
                           0.5,1,0,0.5),#segunda linha segunda coluna
                         ncol=4,byrow=TRUE),screen=1)


# this produces seven screens numbered like this:
#   3   4   5   6 
#                    2
#   7   8   9   10 
# select the upper left screen



screen(3)
par(mar=c(0,3.5,3,0))
# now the second set
n250<-bias.alpha1$nsample==250
matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1, .6),main="nsample=250",ylab="", cex.main=1)
abline(h = 0, col = "gray60")
mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2, cex.main=1)

screen(4)
par(mar=c(0,0,3,0))
# now the second set
n1000<-bias.alpha1$nsample==1000
matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1, .6),main="nsample=1000",ylab="")
abline(h = 0, col = "gray60")



screen(5)
par(mar=c(0,3.5,3,0))
# now the second set
par(mar=c(3,3.5,0,0))
# now the second set
n250<-bias.alpha2$nsample==250
matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
abline(h = 0, col = "gray60")
mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2, cex.main=1.5)

screen(6)
par(mar=c(3,0,0,0))
# now the second set
n1000<-bias.alpha2$nsample==1000
matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
abline(h = 0, col = "gray60")




screen(7)
par(mar=c(0,3.5,3,0))
# now the second set
n250<-SE.alpha1$nsample==250
matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0, 1.1),main="nsample=250",ylab="", cex.main=1)
abline(h = -1, col = "gray60")
mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2, cex.main=1)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)


screen(8)
par(mar=c(0,0,3,0))
# now the second set
n1000<-SE.alpha1$nsample==1000
matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0, 1.1),main="nsample=1000",ylab="")
abline(h = -1, col = "gray60")




screen(9)
par(mar=c(3,3.5,0,0))
# now the second set
n250<-SE.alpha2$nsample==250
matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
abline(h = -.5, col = "gray60")
mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2, cex.main=1.5)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)


screen(10)
par(mar=c(3,0,0,0))
# now the second set
n1000<-SE.alpha2$nsample==1000
matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
abline(h = -.5, col = "gray60")
mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)



screen(2)
par(mar=c(0,0,0,0))
# plot an empty plot to get the coordinates
plot(0:1,0:1,type="n",axes=FALSE)
legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n", lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)


close.screen(all=TRUE)




Best,
RO


Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates


From marc_schwartz at me.com  Wed Sep 16 23:07:00 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 16 Sep 2015 16:07:00 -0500
Subject: [R] generate ordered categorical variable in R
In-Reply-To: <CAGxFJbRXcbdK2crkepYv0Bp641LShJq6T3OYMV9R13Kf=aMCew@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
	<CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
	<C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>
	<CAGxFJbRDJ8-Qq-Qq5LBKVZ7+xOYW6ovd4-9o+ud3hj-Kvm-yUQ@mail.gmail.com>
	<CAGxFJbRXcbdK2crkepYv0Bp641LShJq6T3OYMV9R13Kf=aMCew@mail.gmail.com>
Message-ID: <7B443CFC-4E47-44D9-9668-5C71207BD054@me.com>


> On Sep 16, 2015, at 3:40 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> Nope. Take it back. I stand uncorrected.
> 
>> system.time(z <-sample(1:10,1e6, rep=TRUE))
>   user  system elapsed
>  0.045   0.001   0.047
> 
>> system.time(z <-sample.int(10,1e6,rep=TRUE))
>   user  system elapsed
>  0.012   0.000   0.013
> 
> 
> sample() has to do subscripting in the general case; sample.int doesn't.
> 
> But I would agree that the difference is likely almost always unnoticeable.


Well, in your defense Bert, given the nuance of the example you provided, it actually gets worse the larger the initial sample space is, if defined as a vector rather than a scalar.

On my MacBook Pro, with 16 Gb of RAM and a 2.5 Ghz i7, running R version 3.2.2 (2015-08-14):

> system.time(x1 <- sample(1:1e10, 1e8, replace = TRUE))
Killed: 9

That ran for a couple of minutes and eventually crashed R.

However, as below:

> system.time(x1 <- sample(1e10, 1e8, replace = TRUE))
   user  system elapsed 
  2.943   0.238   3.191 

> system.time(x1 <- sample.int(1e10, 1e8, replace = TRUE))
   user  system elapsed 
  3.135   0.198   3.336 


Here is another example that works, showing a larger time difference with the sample space as a vector:

> system.time(x1 <- sample(1:1e9, 1e8, replace = TRUE))
   user  system elapsed 
  7.069   1.317   8.399 

> system.time(x1 <- sample(1e9, 1e8, replace = TRUE))
   user  system elapsed 
  1.324   0.111   1.438 

> system.time(x1 <- sample.int(1e9, 1e8, replace = TRUE))
   user  system elapsed 
  1.328   0.116   1.450 


If one is running Monte Carlo simulations, repeating the above a very large number of times, it can become a meaningful difference.

Thus, there is an incentive for one to specify the sample space as a scalar and perhaps consider the resultant vector, if needed, as indices (1:x) into the actual sample space desired.

Interesting...

Regards,

Marc


> 
> 
> -- Bert
> Bert Gunter
> 
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>   -- Clifford Stoll
> 
> 
> On Wed, Sep 16, 2015 at 1:34 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> Yes. Thanks Marc. I stand corrected.
>> 
>> -- Bert
>> Bert Gunter
>> 
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>   -- Clifford Stoll
>> 
>> 
>> On Wed, Sep 16, 2015 at 1:28 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
>>> 
>>>> On Sep 16, 2015, at 1:06 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>> 
>>>> Yikes! The uniform distribution is a **continuous** distribution over
>>>> an interval. You seem to want to sample over a discrete distribution.
>>>> See ?sample for that, as in:
>>>> 
>>>> sample(1:4,100,rep=TRUE)
>>>> 
>>>> ## or for this special case and faster
>>>> 
>>>> sample.int(4,size=100,rep=TRUE)
>>> 
>>> 
>>> Bert,
>>> 
>>> I am not sure that it is really faster, since internally, sample() calls sample.int():
>>> 
>>>> sample
>>> function (x, size, replace = FALSE, prob = NULL)
>>> {
>>>    if (length(x) == 1L && is.numeric(x) && x >= 1) {
>>>        if (missing(size))
>>>            size <- x
>>>        sample.int(x, size, replace, prob)
>>>    }
>>>    else {
>>>        if (missing(size))
>>>            size <- length(x)
>>>        x[sample.int(length(x), size, replace, prob)]
>>>    }
>>> }
>>> 
>>> 
>>> set.seed(1)
>>> 
>>>> system.time(x1 <- sample(1e10, 1e8, replace = TRUE))
>>>   user  system elapsed
>>>  2.755   0.170   2.925
>>> 
>>> 
>>> set.seed(1)
>>>> system.time(x2 <- sample.int(1e10, 1e8, replace = TRUE))
>>>   user  system elapsed
>>>  2.767   0.183   2.951
>>> 
>>> 
>>>> all(x1 == x2)
>>> [1] TRUE
>>> 
>>> 
>>> Regards,
>>> 
>>> Marc
>>> 
>>> 
>>>> 
>>>> Cheers,
>>>> Bert
>>>> 
>>>> Bert Gunter
>>>> 
>>>> "Data is not information. Information is not knowledge. And knowledge
>>>> is certainly not wisdom."
>>>>  -- Clifford Stoll
>>>> 
>>>> 
>>>> On Wed, Sep 16, 2015 at 10:11 AM, thanoon younis
>>>> <thanoon.younis80 at gmail.com> wrote:
>>>>> Dear R- users
>>>>> 
>>>>> I want to generate ordered categorical variable vector with 200x1 dimension
>>>>> and from 1 to 4 categories and i tried with this code
>>>>> 
>>>>> Q1=runif(200,1,4) the results are not just 1 ,2 3,4, but the results with
>>>>> decimals like 1.244, 2.342,4,321 and so on ... My question how can i
>>>>> generate a vector and also a matrix with orered categorical variables and
>>>>> without decimals just 1,2,3 ,4 ,1,2,3,4, ....
>>>>> 
>>>>> Many thanks in advance
>>> 


From dwinsemius at comcast.net  Wed Sep 16 23:10:24 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Sep 2015 14:10:24 -0700
Subject: [R] Multiple if function
In-Reply-To: <alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
Message-ID: <54485518-B795-4779-BF34-EEF553FCD53B@comcast.net>


On Sep 15, 2015, at 7:20 PM, Charles C. Berry wrote:

> On Tue, 15 Sep 2015, Bert Gunter wrote:
> 
>> Thanks to both Davids.
>> 
>> I realize that these things are often a matter of aesthetics -- and
>> hence have little rational justification -- but I agree with The Other
>> David: eval(parse) seems to me to violate R's soul( it makes R a macro
>> language instead of a functional one).
>> 
>> However, mapply(... switch) effectively loops through the frame row by
>> row. Aesthetically, I like it; but it seems inefficient. If there are
>> e.g. 1e6 rows in say 10 categories, I think Jeff's approach should do
>> much better.  I'll try to generate some actual data to see unless
>> someone else beats me to it.
> 
> Use mapply like this on large problems:
> 
> unsplit(
>    mapply(
>        function(x,z) eval( x, list( y=z )),
>        expression( A=y*2, B=y+3, C=sqrt(y) ),
>        split( dat$Flow, dat$ASB ),
>        SIMPLIFY=FALSE),
>    dat$ASB)
> 

Seems unnecessarily complex, but definitely elegant. Was there a reason it was not just:

mapply(
       function(x,z) eval( x, list( y=z )),
       expression(A= y*2, B=y+3, C=sqrt(y) ),
       split( dat$Flow, dat$ASB )
     )

Also readers should note that the names in that expression vector are quite arbitrary at the moment. The only association is via the order. I don't suppose someone wants to take on the challenge of matching the names of the expression vector with the names of returned split components?


> Chuck

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Wed Sep 16 23:17:43 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Sep 2015 14:17:43 -0700
Subject: [R] Multiple if function
In-Reply-To: <54485518-B795-4779-BF34-EEF553FCD53B@comcast.net>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
	<54485518-B795-4779-BF34-EEF553FCD53B@comcast.net>
Message-ID: <CAGxFJbQriY1mbSuAcuknOUdgd_iZgZd4xk03uHM9wYHVnfwt6w@mail.gmail.com>

I assume it's to return the vectors in their original order.

-- Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 16, 2015 at 2:10 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
> On Sep 15, 2015, at 7:20 PM, Charles C. Berry wrote:
>
>> On Tue, 15 Sep 2015, Bert Gunter wrote:
>>
>>> Thanks to both Davids.
>>>
>>> I realize that these things are often a matter of aesthetics -- and
>>> hence have little rational justification -- but I agree with The Other
>>> David: eval(parse) seems to me to violate R's soul( it makes R a macro
>>> language instead of a functional one).
>>>
>>> However, mapply(... switch) effectively loops through the frame row by
>>> row. Aesthetically, I like it; but it seems inefficient. If there are
>>> e.g. 1e6 rows in say 10 categories, I think Jeff's approach should do
>>> much better.  I'll try to generate some actual data to see unless
>>> someone else beats me to it.
>>
>> Use mapply like this on large problems:
>>
>> unsplit(
>>    mapply(
>>        function(x,z) eval( x, list( y=z )),
>>        expression( A=y*2, B=y+3, C=sqrt(y) ),
>>        split( dat$Flow, dat$ASB ),
>>        SIMPLIFY=FALSE),
>>    dat$ASB)
>>
>
> Seems unnecessarily complex, but definitely elegant. Was there a reason it was not just:
>
> mapply(
>        function(x,z) eval( x, list( y=z )),
>        expression(A= y*2, B=y+3, C=sqrt(y) ),
>        split( dat$Flow, dat$ASB )
>      )
>
> Also readers should note that the names in that expression vector are quite arbitrary at the moment. The only association is via the order. I don't suppose someone wants to take on the challenge of matching the names of the expression vector with the names of returned split components?
>
>
>> Chuck
>
> David Winsemius
> Alameda, CA, USA
>


From Dikra.Khedhaouiria at ete.inrs.ca  Wed Sep 16 23:50:31 2015
From: Dikra.Khedhaouiria at ete.inrs.ca (Khedhaouiria Dikra)
Date: Wed, 16 Sep 2015 17:50:31 -0400
Subject: [R] HELP ERROR WHEN USING GAMMA2 IN VGLM TO SET MU AS AN INTERCEPT
Message-ID: <0DA854E5D5F4C8428ABC1701AE2621CD032DB023AD36@INRSEXCH02.AD.INRS.CA>

Hi everyone,


I'm trying to fit several models with the VGAM packages using the gamma2 function with an identity function link.
I have two vectors, x_m and y_m and I try to work on the model y_m~x_m.

My code is below, but I want to explain my problem first:

First, I took the default model proposed in vgml function (fit1 below), i.e. shape as an intercept --> no problem
Then, I tried the model with zero=NULL (fit3 below), meaning that all parameters are modelled as a function of the explanatory variables (x_m) --> again no problem
Finally, I tried to model mu parameter as an intercept only , i.e. setting zero=1, here my problems begin. In fact, I have the following error message:
VGLM    linear loop  1 :  loglikelihood = NaN
Error in if (take.half.step) { : missing value where TRUE/FALSE needed
In addition: Warning message:
In dgamma(x = y, shape = c(shapemat), scale = c(mymu/shapemat),  :
  production de NaN


I tried to change the initial value (inject estimated parameters of fit2  in fit3) , the number of iteration, but nothing changed, always the same problem.
I tried other link functions (log), it worked but I really want to make it works with the identity link.

Thank you so much!

Dikra

Here is my code:

# Empty the environnement
rm (list=ls())

#--- needed environnment

library(VGAM)


#------------ set the working directorty
pathWork  <- "C:/mydata/"

setwd(pathWork)

#--------- load my data
dataXmodel <- read.table("x_m.txt")
dataYmodel <- read.table("y_m.txt")

# ------ set the data frame
xmodel <- dataXmodel$x
ymodel <- dataYmodel$x
gdata  = data.frame(xmodel , ymodel)

#------------ fit 1
fit1 <- vglm(ymodel ~ xmodel, family = gamma2(lmu ="identitylink", lshape="identitylink"),
             trace = TRUE  , data = gdata, na.action =  "na.omit")

                # identify potential initial values for next steps
CoefFit1 <- coef(fit1)

valinitial2 = c(CoefFit1, 0.01)
valinitial3 = c(CoefFit1[1], CoefFit1[2], 0.02)

# --------- fit 2

fit2 <- vglm(ymodel ~ xmodel, family = gamma2(lmu ="identitylink", lshape="identitylink", zero = NULL),
            trace = TRUE  , data = gdata, na.action =  "na.omit", coefstart =valinitial2 )

# --------- fit 3

fit3 <- vglm(ymodel ~ xmodel, family = gamma2(lmu ="identitylink", lshape="identitylink", zero = 1),
             trace = TRUE  , data = gdata, na.action =  "na.omit" , coefstart = valinitial3 , maxit=60)



-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: x_m.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150916/a073ab3d/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: y_m.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150916/a073ab3d/attachment-0001.txt>

From toth.denes at ttk.mta.hu  Thu Sep 17 01:42:37 2015
From: toth.denes at ttk.mta.hu (=?ISO-8859-1?Q?D=E9nes_T=F3th?=)
Date: Thu, 17 Sep 2015 01:42:37 +0200
Subject: [R] Multiple if function
In-Reply-To: <CAGxFJbReNe0E2=2QYM6iRaksZjZwaXPL1wuUjmsGbiY4XMqkmw@mail.gmail.com>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>	<865084AF-2702-4453-94D3-AE4A0EC45DB5@kit.edu>	<9803D422-0B5F-4BA7-BBF8-77AF502A1EB2@gmail.com>
	<CAGxFJbReNe0E2=2QYM6iRaksZjZwaXPL1wuUjmsGbiY4XMqkmw@mail.gmail.com>
Message-ID: <55F9FE6D.4080404@ttk.mta.hu>



On 09/16/2015 04:41 PM, Bert Gunter wrote:
> Yes! Chuck's use of mapply is exactly the split/combine strategy I was
> looking for. In retrospect, exactly how one should think about it.
> Many thanks to all for a constructive discussion .
>
> -- Bert
>
>
> Bert Gunter
>
>>>>
>>>> Use mapply like this on large problems:
>>>>
>>>> unsplit(
>>>>    mapply(
>>>>        function(x,z) eval( x, list( y=z )),
>>>>        expression( A=y*2, B=y+3, C=sqrt(y) ),
>>>>        split( dat$Flow, dat$ASB ),
>>>>        SIMPLIFY=FALSE),
>>>>    dat$ASB)
>>>>
>>>> Chuck
>>>>


Is there any reason not to use data.table for this purpose, especially 
if efficiency is of concern?

---

# load data.table and microbenchmark
library(data.table)
library(microbenchmark)
#
# prepare data
DF <- data.frame(
     ASB = rep_len(factor(LETTERS[1:3]), 3e5),
     Flow = rnorm(3e5)^2)
DT <- as.data.table(DF)
DT[, ASB := as.character(ASB)]
#
# define functions
#
# Chuck's version
fnSplit <- function(dat) {
     unsplit(
         mapply(
             function(x,z) eval( x, list( y=z )),
             expression( A=y*2, B=y+3, C=sqrt(y) ),
             split( dat$Flow, dat$ASB ),
             SIMPLIFY=FALSE),
         dat$ASB)
}
#
# data.table-way (IMHO, much easier to read)
fnDataTable <- function(dat) {
     dat[,
         result :=
             if (.BY == "A") {
                 2 * Flow
             } else if (.BY == "B") {
                 3 + Flow
             } else if (.BY == "C") {
                 sqrt(Flow)
             },
         by = ASB]
}
#
# benchmark
#
microbenchmark(fnSplit(DF), fnDataTable(DT))
identical(fnSplit(DF), fnDataTable(DT)[, result])

---

Actually, in Chuck's version the unsplit() part is slow. If the order is 
not of concern (e.g., DF is reordered before calling fnSplit), fnSplit 
is comparable to the DT-version.


Denes


From r.turner at auckland.ac.nz  Thu Sep 17 03:33:13 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 17 Sep 2015 13:33:13 +1200
Subject: [R] HELP ERROR WHEN USING GAMMA2 IN VGLM TO SET MU AS AN
 INTERCEPT
In-Reply-To: <0DA854E5D5F4C8428ABC1701AE2621CD032DB023AD36@INRSEXCH02.AD.INRS.CA>
References: <0DA854E5D5F4C8428ABC1701AE2621CD032DB023AD36@INRSEXCH02.AD.INRS.CA>
Message-ID: <55FA1859.7020602@auckland.ac.nz>

On 17/09/15 09:50, Khedhaouiria Dikra wrote:
> Hi everyone,
>
>
> I'm trying to fit several models with the VGAM packages using the
> gamma2 function with an identity function link. ......

<SNIP>

For questions concerning a particular package it is usually best to 
contact the package maintainer (see maintainer("VGAM")) in the first 
instance.  If you do not get a response from the maintainer, then 
address the R-help list.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From bgunter.4567 at gmail.com  Thu Sep 17 03:53:22 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Sep 2015 18:53:22 -0700
Subject: [R] Multiple if function
In-Reply-To: <55F9FE6D.4080404@ttk.mta.hu>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
	<865084AF-2702-4453-94D3-AE4A0EC45DB5@kit.edu>
	<9803D422-0B5F-4BA7-BBF8-77AF502A1EB2@gmail.com>
	<CAGxFJbReNe0E2=2QYM6iRaksZjZwaXPL1wuUjmsGbiY4XMqkmw@mail.gmail.com>
	<55F9FE6D.4080404@ttk.mta.hu>
Message-ID: <CAGxFJbQ2zHDnG2BDGqT1du7t4RbnP8xKrrvcZWMKUfa7PR_B+g@mail.gmail.com>

D?nes:

A fair point! The only reason I have is ignorance -- I have not used
data.table. I am not surprised that it and perhaps other packages
(dplyr maybe?) can do things in a reasonable way very efficiently. The
only problem is that it requires us to learn yet another
package/paradigm.  There may also be issues with ts flexibility
compared to base R data structures, but, again, I must plead ignorance
here.

It is interesting that, mod the unsplit reconstruction of the original
vectors, Chuck's base R solution is as efficient as data.table's.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Sep 16, 2015 at 4:42 PM, D?nes T?th <toth.denes at ttk.mta.hu> wrote:
>
>
> On 09/16/2015 04:41 PM, Bert Gunter wrote:
>>
>> Yes! Chuck's use of mapply is exactly the split/combine strategy I was
>> looking for. In retrospect, exactly how one should think about it.
>> Many thanks to all for a constructive discussion .
>>
>> -- Bert
>>
>>
>> Bert Gunter
>>
>>>>>
>>>>> Use mapply like this on large problems:
>>>>>
>>>>> unsplit(
>>>>>    mapply(
>>>>>        function(x,z) eval( x, list( y=z )),
>>>>>        expression( A=y*2, B=y+3, C=sqrt(y) ),
>>>>>        split( dat$Flow, dat$ASB ),
>>>>>        SIMPLIFY=FALSE),
>>>>>    dat$ASB)
>>>>>
>>>>> Chuck
>>>>>
>
>
> Is there any reason not to use data.table for this purpose, especially if
> efficiency is of concern?
>
> ---
>
> # load data.table and microbenchmark
> library(data.table)
> library(microbenchmark)
> #
> # prepare data
> DF <- data.frame(
>     ASB = rep_len(factor(LETTERS[1:3]), 3e5),
>     Flow = rnorm(3e5)^2)
> DT <- as.data.table(DF)
> DT[, ASB := as.character(ASB)]
> #
> # define functions
> #
> # Chuck's version
> fnSplit <- function(dat) {
>     unsplit(
>         mapply(
>             function(x,z) eval( x, list( y=z )),
>             expression( A=y*2, B=y+3, C=sqrt(y) ),
>             split( dat$Flow, dat$ASB ),
>             SIMPLIFY=FALSE),
>         dat$ASB)
> }
> #
> # data.table-way (IMHO, much easier to read)
> fnDataTable <- function(dat) {
>     dat[,
>         result :=
>             if (.BY == "A") {
>                 2 * Flow
>             } else if (.BY == "B") {
>                 3 + Flow
>             } else if (.BY == "C") {
>                 sqrt(Flow)
>             },
>         by = ASB]
> }
> #
> # benchmark
> #
> microbenchmark(fnSplit(DF), fnDataTable(DT))
> identical(fnSplit(DF), fnDataTable(DT)[, result])
>
> ---
>
> Actually, in Chuck's version the unsplit() part is slow. If the order is not
> of concern (e.g., DF is reordered before calling fnSplit), fnSplit is
> comparable to the DT-version.
>
>
> Denes


From JSorkin at grecc.umaryland.edu  Thu Sep 17 04:43:04 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 16 Sep 2015 22:43:04 -0400
Subject: [R] finding those elements of listA that are found in listB
Message-ID: <55F9F078020000CB001392E3@smtp.medicine.umaryland.edu>

I have two structures. I think they are lists, but I am not sure. Both structures contain integers. I am trying to find those members of list b that are found in list a. I have tried to perform the search using grep, but I get an error. Please see code below. I would appreciate knowing how to search listB for any element in listA
Thanks
John


> str(listA)
 int [1:42] 13083 13705 14123 14168 14382 14652 14654 14678 14817 14822 ...
> str(listB)
 int [1:633] 13083 13083 13083 13083 13083 13083 13705 13705 13705 13705 ...
> grep(listA,listB)
[1] 1 2 3 4 5 6
Warning message:
In grep(listA, listB) :
  argument 'pattern' has length > 1 and only the first element will be used







 

John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 


Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From rmh at temple.edu  Thu Sep 17 05:33:22 2015
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 16 Sep 2015 23:33:22 -0400
Subject: [R] finding those elements of listA that are found in listB
In-Reply-To: <55F9F078020000CB001392E3@smtp.medicine.umaryland.edu>
References: <55F9F078020000CB001392E3@smtp.medicine.umaryland.edu>
Message-ID: <CAGx1TMCETYzB+n6fFckG3=z3Jgxqd9fgW_k=+LDnqboOUvKugg@mail.gmail.com>

I think you are looking for match or %in% (which is a based on match)

> a <- sample(12)
> b <- c(1, 3, 5, 11, 17)
> a
 [1] 10  8  1  4  7  3  6 11  2 12  5  9
> b
[1]  1  3  5 11 17
> [1] 1
> match(a, b)
 [1] NA NA  1 NA NA  2 NA  4 NA NA  3 NA
> match(a, b, 0)
 [1] 0 0 1 0 0 2 0 4 0 0 3 0
> match(b, a)
[1]  3  6 11  8 NA
> match(b, a, 0)
[1]  3  6 11  8  0
> a %in% b
 [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE
> b %in% a
[1]  TRUE  TRUE  TRUE  TRUE FALSE
>

Rich

On Wed, Sep 16, 2015 at 10:43 PM, John Sorkin <JSorkin at grecc.umaryland.edu>
wrote:

> I have two structures. I think they are lists, but I am not sure. Both
> structures contain integers. I am trying to find those members of list b
> that are found in list a. I have tried to perform the search using grep,
> but I get an error. Please see code below. I would appreciate knowing how
> to search listB for any element in listA
> Thanks
> John
>
>
> > str(listA)
>  int [1:42] 13083 13705 14123 14168 14382 14652 14654 14678 14817 14822 ...
> > str(listB)
>  int [1:633] 13083 13083 13083 13083 13083 13083 13705 13705 13705 13705
> ...
> > grep(listA,listB)
> [1] 1 2 3 4 5 6
> Warning message:
> In grep(listA, listB) :
>   argument 'pattern' has length > 1 and only the first element will be used
>
>
>
>
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:16}}


From bhh at xs4all.nl  Thu Sep 17 10:17:11 2015
From: bhh at xs4all.nl (Berend Hasselman)
Date: Thu, 17 Sep 2015 10:17:11 +0200
Subject: [R] Multiple if function
In-Reply-To: <55F9FE6D.4080404@ttk.mta.hu>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
	<865084AF-2702-4453-94D3-AE4A0EC45DB5@kit.edu>
	<9803D422-0B5F-4BA7-BBF8-77AF502A1EB2@gmail.com>
	<CAGxFJbReNe0E2=2QYM6iRaksZjZwaXPL1wuUjmsGbiY4XMqkmw@mail.gmail.com>
	<55F9FE6D.4080404@ttk.mta.hu>
Message-ID: <BADC6A14-1C55-461C-8E73-22C0EDA768BC@xs4all.nl>


> On 17 Sep 2015, at 01:42, D?nes T?th <toth.denes at ttk.mta.hu> wrote:
> 
> 
> 
> On 09/16/2015 04:41 PM, Bert Gunter wrote:
>> Yes! Chuck's use of mapply is exactly the split/combine strategy I was
>> looking for. In retrospect, exactly how one should think about it.
>> Many thanks to all for a constructive discussion .
>> 
>> -- Bert
>> 
>> 
>> Bert Gunter
>> 
>>>>> 
>>>>> Use mapply like this on large problems:
>>>>> 
>>>>> unsplit(
>>>>>   mapply(
>>>>>       function(x,z) eval( x, list( y=z )),
>>>>>       expression( A=y*2, B=y+3, C=sqrt(y) ),
>>>>>       split( dat$Flow, dat$ASB ),
>>>>>       SIMPLIFY=FALSE),
>>>>>   dat$ASB)
>>>>> 
>>>>> Chuck
>>>>> 
> 
> 
> Is there any reason not to use data.table for this purpose, especially if efficiency is of concern?
> 
> ---
> 
> # load data.table and microbenchmark
> library(data.table)
> library(microbenchmark)
> #
> # prepare data
> DF <- data.frame(
>    ASB = rep_len(factor(LETTERS[1:3]), 3e5),
>    Flow = rnorm(3e5)^2)
> DT <- as.data.table(DF)
> DT[, ASB := as.character(ASB)]
> #
> # define functions
> #
> # Chuck's version
> fnSplit <- function(dat) {
>    unsplit(
>        mapply(
>            function(x,z) eval( x, list( y=z )),
>            expression( A=y*2, B=y+3, C=sqrt(y) ),
>            split( dat$Flow, dat$ASB ),
>            SIMPLIFY=FALSE),
>        dat$ASB)
> }
> #
> # data.table-way (IMHO, much easier to read)
> fnDataTable <- function(dat) {
>    dat[,
>        result :=
>            if (.BY == "A") {
>                2 * Flow
>            } else if (.BY == "B") {
>                3 + Flow
>            } else if (.BY == "C") {
>                sqrt(Flow)
>            },
>        by = ASB]
> }
> #
> # benchmark
> #
> microbenchmark(fnSplit(DF), fnDataTable(DT))
> identical(fnSplit(DF), fnDataTable(DT)[, result])
> 
> ---
> 
> Actually, in Chuck's version the unsplit() part is slow. If the order is not of concern (e.g., DF is reordered before calling fnSplit), fnSplit is comparable to the DT-version.
> 

But David?s version is faster than Chuck?s fnSplit. I modified David?s solution slightly to get a result that is identical to fnSplit.

# David's version
# my modification to return a vector just like fnSplit
fnDavid <- function(dat) {
    z <- mapply(
          function(x,z) eval( x, list( y=z )),
          expression(A= y*2, B=y+3, C=sqrt(y) ),
          split( dat$Flow, dat$ASB ),
          USE.NAMES=FALSE, SIMPLIFY=TRUE
        )
    as.vector(t(z))
}

Added this to D?nes's code.
Benchmarking  with R package rbenchmark and testing result like this

library(rbenchmark)
benchmark(fnSplit(DF), fnDataTable(DT),fnDavid(DF))
identical(fnSplit(DF), fnDataTable(DT)[, result])
identical(fnSplit(DF), fnDavid(DF))

gave this:

             test replications elapsed relative user.self sys.self user.child
2 fnDataTable(DT)          100   0.829    1.000     0.762    0.066          0
3     fnDavid(DF)          100   1.615    1.948     1.515    0.098          0
1     fnSplit(DF)          100   2.878    3.472     2.685    0.190          0
  sys.child
2         0
3         0
1         0

> identical(fnSplit(DF), fnDataTable(DT)[, result])
[1] TRUE
> identical(fnSplit(DF), fnDavid(DF))
[1] TRUE
 

Berend


From petr.pikal at precheza.cz  Thu Sep 17 10:22:10 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 17 Sep 2015 08:22:10 +0000
Subject: [R] aggregate counting variable factors
In-Reply-To: <CAHOWgNWsu++ShKUrLg+ftMs+81AErNBoyg4xnz4-5sRG-H7YwA@mail.gmail.com>
References: <CAHOWgNWsu++ShKUrLg+ftMs+81AErNBoyg4xnz4-5sRG-H7YwA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C40124@SRVEXCHMBX.precheza.cz>

Hi

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Kai Mx
> Sent: Wednesday, September 16, 2015 10:43 PM
> To: r-help mailing list
> Subject: [R] aggregate counting variable factors
>
> Hi everybody,
>
> >From a questionnaire, I have a dataset  like this one with some 40
> items:
>
> df1 <- data.frame(subject=c('user1','user2', 'user3', 'user4'),
> item1=c(0,1,2,5), item2=c(1,2,1,2), item3=c(2,3,4,0), item4=c(0,3,3,2),
> item5=c(5,5,5,5))
>
> Users can choose an answer from 0 to 5 for each item.
>
> Now I want to reshape the dataset to have the items in rows and the
> count
> of each of the result factors in columns:
>
> result <- data.frame (item=c("item1", "item2", "item3", "item4",
> "item5"),
> result0=c(1,0,1,1,0), result1=c(1,2,0,0,0), result2=c(1,2,1,1,0),
> result3=c(0,0,1,2,0), result4=c(0,0,1,0,0), result5=c(1,0,0,0,4))
>
> I have been fiddling around with melt/plyr, but haven't been able to
> figure
> it out. What's the most elegant way to do this (preferably without
> typing
> in all the item names).

Perhaps,

m<-melt(df1)
m$value<-paste("res",m$value, sep="")
dcast(m, variable~value)
Aggregation function missing: defaulting to length
  variable res0 res1 res2 res3 res4 res5
1    item1    1    1    1    0    0    1
2    item2    0    2    2    0    0    0
3    item3    1    0    1    1    1    0
4    item4    1    0    1    2    0    0
5    item5    0    0    0    0    0    4

Cheers
Petr


>
> Thanks so much!
>
> Best,
>
> Kai
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From jsorkin at grecc.umaryland.edu  Thu Sep 17 13:06:54 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 17 Sep 2015 07:06:54 -0400
Subject: [R] getting means by group within time point for data on multiple
 lines (long rather than wide file)
Message-ID: <55FA6696020000CB00139325@smtp.medicine.umaryland.edu>

I have a long (rather than wide file), i.e. the data for each subject is on multiple lines rather than one line. Each line has the following layout:
subject group time value
I have two groups, multiple subjects, each subject can be seen up to three times a time 0, and at most once at times 4 and 8.
An example of the data follows:

1 control 0 100
1 control 0 NA
1 control 0 55
1 control 4 100
1 control 8 100

2 exp 0 99
2 exp 0 67
2 exp 0 66
2 exp 4 110
2 exp 8 200

I need to get means by group (control vs. exp) within time (0,4,8). The means should include only those subjects who have at least one observation at each time point (0, 4, 8). I also need to determine the number of subjects who contribute data at each time-point by group. Any suggestion on how to get them means would be appreciated. Sad to say I worked on this for four hours last night without coming to any understanding how this can be done. UGG!  

Thank you,
John




> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From drjimlemon at gmail.com  Thu Sep 17 13:18:22 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 17 Sep 2015 21:18:22 +1000
Subject: [R] HELP IN GRAPHS - slip screen
In-Reply-To: <F53D3AF7-922F-4352-B554-1E7F5244AB75@gmail.com>
References: <F53D3AF7-922F-4352-B554-1E7F5244AB75@gmail.com>
Message-ID: <CA+8X3fUpj3vfn=V8rrUNnRSMpu2R9D2DB1gfNQnMQ_tYC96iuQ@mail.gmail.com>

Hi Rosa,
Try this:

# do the first split, to get the rightmost screen for the legend
split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
# now split the first screen to get your eight screens (numbered 3 to 10)
for the plots
split.screen(figs=matrix(c(0,0.25,0.5,1,
                           0.25,0.5,0.5,1,
                           0.5,0.75,0.5,1,
                           0.75,1,0.5,1,
                           0,0.25,0,0.5,
                           0.25,0.5,0,0.5,
                           0.5,0.75,0,0.5,
                           0.75,1,0,0.5),
                         ncol=4,byrow=TRUE),screen=1)

Jim


On Thu, Sep 17, 2015 at 2:45 AM, Rosa Oliveira <rosita21 at gmail.com> wrote:

> Dear all,
>
> I?m trying to do a graph,
>
> 3 rows, 5 columns, with the design:
> #   3   4   5   6
> #                    2
> #   7   8   9   10
>
> I had a code for 3 rows, 3 columns, with the design::
> #   3   4
> #            2
> #   7   8
>  and I tried to modify it, but I had no success :(
>
> I suppose the problem is in the slip.screen code (red part of the code).
>
> I attach my code, can anyone please help me?
>
>
> Best,
> RO
>
>
> setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
>
> library(ggplot2)
> library(reshape)
> library(lattice)
>
>
> # read in what looks like half of the data
>
> bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
> SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
> bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
> SE.alpha1<-read.csv("graphs_SE_alpha1.csv")
>
>
>
> quartz(width=10,height=6)
> # do the first split, to get the rightmost screen for the legend
> split.screen(figs=matrix(c(0,0.8,0,1,0.8,1,0,1),nrow=2,byrow=TRUE))
> # now split the first screen to get your six screens for the plots
>
>
>
> split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
>                            0.5,1,0.5,1,#primeira linha segunda coluna
>                            0,0.5,0,0.5,#segunda linha primeira coluna
>                            0.5,1,0,0.5),#segunda linha segunda coluna
>                          ncol=4,byrow=TRUE),screen=1)
>
>
> # this produces seven screens numbered like this:
> #   3   4   5   6
> #                    2
> #   7   8   9   10
> # select the upper left screen
>
>
>
> screen(3)
> par(mar=c(0,3.5,3,0))
> # now the second set
> n250<-bias.alpha1$nsample==250
> matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1,
> .6),main="nsample=250",ylab="", cex.main=1)
> abline(h = 0, col = "gray60")
> mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2,
> cex.main=1)
>
> screen(4)
> par(mar=c(0,0,3,0))
> # now the second set
> n1000<-bias.alpha1$nsample==1000
> matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1,
> .6),main="nsample=1000",ylab="")
> abline(h = 0, col = "gray60")
>
>
>
> screen(5)
> par(mar=c(0,3.5,3,0))
> # now the second set
> par(mar=c(3,3.5,0,0))
> # now the second set
> n250<-bias.alpha2$nsample==250
> matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
> abline(h = 0, col = "gray60")
> mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2,
> cex.main=1.5)
>
> screen(6)
> par(mar=c(3,0,0,0))
> # now the second set
> n1000<-bias.alpha2$nsample==1000
> matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
> abline(h = 0, col = "gray60")
>
>
>
>
> screen(7)
> par(mar=c(0,3.5,3,0))
> # now the second set
> n250<-SE.alpha1$nsample==250
> matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0,
> 1.1),main="nsample=250",ylab="", cex.main=1)
> abline(h = -1, col = "gray60")
> mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2, cex.main=1)
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>
>
> screen(8)
> par(mar=c(0,0,3,0))
> # now the second set
> n1000<-SE.alpha1$nsample==1000
> matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0,
> 1.1),main="nsample=1000",ylab="")
> abline(h = -1, col = "gray60")
>
>
>
>
> screen(9)
> par(mar=c(3,3.5,0,0))
> # now the second set
> n250<-SE.alpha2$nsample==250
> matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
> abline(h = -.5, col = "gray60")
> mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2,
> cex.main=1.5)
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>
>
> screen(10)
> par(mar=c(3,0,0,0))
> # now the second set
> n1000<-SE.alpha2$nsample==1000
> matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
> abline(h = -.5, col = "gray60")
> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
>
>
>
> screen(2)
> par(mar=c(0,0,0,0))
> # plot an empty plot to get the coordinates
> plot(0:1,0:1,type="n",axes=FALSE)
> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n",
> lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
>
>
> close.screen(all=TRUE)
>
>
>
>
> Best,
> RO
>
>
> Atenciosamente,
> Rosa Oliveira
>
> --
>
> ____________________________________________________________________________
>
>
> Rosa Celeste dos Santos Oliveira,
>
> E-mail: rosita21 at gmail.com
> Tlm: +351 939355143
> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>
> ____________________________________________________________________________
> "Many admire, few know"
> Hippocrates
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Thu Sep 17 13:36:16 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 17 Sep 2015 07:36:16 -0400
Subject: [R] getting means by group within time point for data on
 multiple lines (long rather than wide file)
In-Reply-To: <55FA6696020000CB00139325@smtp.medicine.umaryland.edu>
References: <55FA6696020000CB00139325@smtp.medicine.umaryland.edu>
Message-ID: <55FAA5B0.7090309@gmail.com>

On 17/09/2015 7:06 AM, John Sorkin wrote:
> I have a long (rather than wide file), i.e. the data for each subject is on multiple lines rather than one line. Each line has the following layout:
> subject group time value
> I have two groups, multiple subjects, each subject can be seen up to three times a time 0, and at most once at times 4 and 8.
> An example of the data follows:
> 
> 1 control 0 100
> 1 control 0 NA
> 1 control 0 55
> 1 control 4 100
> 1 control 8 100
> 
> 2 exp 0 99
> 2 exp 0 67
> 2 exp 0 66
> 2 exp 4 110
> 2 exp 8 200
> 
> I need to get means by group (control vs. exp) within time (0,4,8). The means should include only those subjects who have at least one observation at each time point (0, 4, 8). I also need to determine the number of subjects who contribute data at each time-point by group. Any suggestion on how to get them means would be appreciated. Sad to say I worked on this for four hours last night without coming to any understanding how this can be done. UGG!  

Do it in two stages.  First, group the data by subject id, and delete
any subjects that don't have sufficient observations.  Then group by
treatment and time and take means.

The tapply() or by() functions will be useful for both of these steps.
For example,

do.call(rbind,
  by(x, x$subjectid,
     function(sub)
       if (length(unique(sub$times)) == 3) sub
       else NULL))

will remove subjects with other than 3 observed times.  (It doesn't take
NA into account; if you need to do that, you'll need to make that
function(sub) more complicated.  "sub" will be a dataframe containing
data for just one subject.)

The "do.call(rbind" puts the list output from by() back together as a
single dataframe.

Duncan Murdoch


From ivan.calandra at univ-reims.fr  Thu Sep 17 13:44:26 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Thu, 17 Sep 2015 13:44:26 +0200
Subject: [R] getting means by group within time point for data on
 multiple lines (long rather than wide file)
In-Reply-To: <55FA6696020000CB00139325@smtp.medicine.umaryland.edu>
References: <55FA6696020000CB00139325@smtp.medicine.umaryland.edu>
Message-ID: <55FAA79A.40200@univ-reims.fr>

Hi John,

This will not be the complete answer, but it can probably help you in 
the right direction.

First, I would subset your data.frame to include only subjects with one 
observation at each time point (and I'm not sure how to do that easily).

But then, the aggregate() function is what you need. Let's say your 
subset data.frame is called df:
aggregate(value~group+time, data=df, FUN=function(x) c(length(x),mean(x)))

By defining your own function in aggregate() you can compute both the 
length(), i.e. the number of subjects that were used in the computation, 
and the mean() per group and per time-point.

HTH,
Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 17/09/15 13:06, John Sorkin a ?crit :
> I have a long (rather than wide file), i.e. the data for each subject is on multiple lines rather than one line. Each line has the following layout:
> subject group time value
> I have two groups, multiple subjects, each subject can be seen up to three times a time 0, and at most once at times 4 and 8.
> An example of the data follows:
>
> 1 control 0 100
> 1 control 0 NA
> 1 control 0 55
> 1 control 4 100
> 1 control 8 100
>
> 2 exp 0 99
> 2 exp 0 67
> 2 exp 0 66
> 2 exp 4 110
> 2 exp 8 200
>
> I need to get means by group (control vs. exp) within time (0,4,8). The means should include only those subjects who have at least one observation at each time point (0, 4, 8). I also need to determine the number of subjects who contribute data at each time-point by group. Any suggestion on how to get them means would be appreciated. Sad to say I worked on this for four hours last night without coming to any understanding how this can be done. UGG!
>
> Thank you,
> John
>
>
>
>
>> John David Sorkin M.D., Ph.D.
>> Professor of Medicine
>> Chief, Biostatistics and Informatics
>> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
>> Baltimore VA Medical Center
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>> (Phone) 410-605-7119
>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:8}}


From PabloEmilio.Verde at uni-duesseldorf.de  Thu Sep 17 15:00:29 2015
From: PabloEmilio.Verde at uni-duesseldorf.de (Pablo Verde)
Date: Thu, 17 Sep 2015 15:00:29 +0200
Subject: [R] short course: Bayesian Data Analysis with R and WinBUGS
Message-ID: <17259d5832a48f4237ac5289b6a739a2@uni-duesseldorf.de>

Dear list members,

Apologies for cross-posting. Please, find below the information of a 
course: "Bayesian Data Analysis with R and WinBUGS".

The course takes place in a nice area nearby the city of Essen. In the
course?s price is included teaching material (The BUGS Book and my 
slides),
an excursion to the Christmas' market of Essen and another excursion to 
one historical industrial area.

If you have any question doesn?t hesitate to contact me.

Best regards,

Pablo

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
*Three days course in: Bayesian Data Analysis with R and WinBUGS
*Where: Linuxhotel, Essen-Horst, Germany
*When: 17.12.2015 to 19.12.2015

*Instructor:
Dr. Pablo E. Verde
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
*Target audience:
This course is for data analyst who are familiar with classical 
statistics
and they want to get a working knowledge in Bayesian analysis. This is a
3 days intensive training course with 8 hours per day including 
lecturing and
exercises. The course presentation is practical with many worked 
examples. To
attend the course you do NOT need experience with R or with WinBUGS.

  Lectures are given in English. Discussions can be in English, German or 
Spanish!
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
*Course content:
Day 1
*Lecture 1: Introduction to Bayesian Inference
*Lecture 2: Bayesian analysis for single parameter models
*Lecture 3: Prior distributions: univariate
Day 2
*Lecture 4: Bayesian analysis for multiple parameter models
*Lecture 5: An introduction to WinBUGS
*Lecture 6: Multivariate models with WinBUGS
Day 3
*Lecture 7: An introduction to MCMC computations
*Lecture 8: Bayesian regression with WinBUGS
*Lecture 9: Introduction to Hierarchical Statistical modeling

*Prices:
Public sector and commercial: 3 days, 998.00 ? + 19% vat = 1,187.62 ?
Student:  course without accommodation 225 ? per day or with
accommodation 290 ? per day.
For more information, please contact:  info at linuxhotel.de
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


From thanoon.younis80 at gmail.com  Thu Sep 17 15:40:34 2015
From: thanoon.younis80 at gmail.com (thanoon younis)
Date: Thu, 17 Sep 2015 16:40:34 +0300
Subject: [R] Fwd:  generate ordered categorical variable in R
In-Reply-To: <CABLo8nGorhH=VMZBfXDK5K6GUcEc_OaUB=82rf8bmQ+sbLeTHA@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
	<CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
	<C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>
	<CAGxFJbRDJ8-Qq-Qq5LBKVZ7+xOYW6ovd4-9o+ud3hj-Kvm-yUQ@mail.gmail.com>
	<CAGxFJbRXcbdK2crkepYv0Bp641LShJq6T3OYMV9R13Kf=aMCew@mail.gmail.com>
	<7B443CFC-4E47-44D9-9668-5C71207BD054@me.com>
	<CABLo8nFEOJoqbOzmfK0epfQsA8Y5ENTDZdAzJgufEpkh3Y_ROQ@mail.gmail.com>
	<CABLo8nGorhH=VMZBfXDK5K6GUcEc_OaUB=82rf8bmQ+sbLeTHA@mail.gmail.com>
Message-ID: <CABLo8nF0KAwzx6pTET6gSE=VMUQf=PgD2rNmKz9WDTTZqk5r1A@mail.gmail.com>

Dear all users

I want to write a vector with one column and just NA values and nrow=200
when i write X=numeric(NA) is not correct  how can i do this please?


Regards

	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Thu Sep 17 16:07:08 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 17 Sep 2015 10:07:08 -0400
Subject: [R] Fwd:  generate ordered categorical variable in R
In-Reply-To: <CABLo8nF0KAwzx6pTET6gSE=VMUQf=PgD2rNmKz9WDTTZqk5r1A@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
	<CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
	<C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>
	<CAGxFJbRDJ8-Qq-Qq5LBKVZ7+xOYW6ovd4-9o+ud3hj-Kvm-yUQ@mail.gmail.com>
	<CAGxFJbRXcbdK2crkepYv0Bp641LShJq6T3OYMV9R13Kf=aMCew@mail.gmail.com>
	<7B443CFC-4E47-44D9-9668-5C71207BD054@me.com>
	<CABLo8nFEOJoqbOzmfK0epfQsA8Y5ENTDZdAzJgufEpkh3Y_ROQ@mail.gmail.com>
	<CABLo8nGorhH=VMZBfXDK5K6GUcEc_OaUB=82rf8bmQ+sbLeTHA@mail.gmail.com>
	<CABLo8nF0KAwzx6pTET6gSE=VMUQf=PgD2rNmKz9WDTTZqk5r1A@mail.gmail.com>
Message-ID: <AF50D9BA-C55E-4E8D-A369-07AE1B895A3D@utoronto.ca>

x <- rep(NA, 200)

For all cases I can think of, that is enough. If you MUST have a matrix with one column and two hundred rows, set:

dim(x) <- c(200,1)


B.

On Sep 17, 2015, at 9:40 AM, thanoon younis <thanoon.younis80 at gmail.com> wrote:

> Dear all users
> 
> I want to write a vector with one column and just NA values and nrow=200
> when i write X=numeric(NA) is not correct  how can i do this please?
> 
> 
> Regards
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From boris.steipe at utoronto.ca  Thu Sep 17 16:09:57 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 17 Sep 2015 10:09:57 -0400
Subject: [R] Fwd:  generate ordered categorical variable in R
In-Reply-To: <AF50D9BA-C55E-4E8D-A369-07AE1B895A3D@utoronto.ca>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
	<CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
	<C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>
	<CAGxFJbRDJ8-Qq-Qq5LBKVZ7+xOYW6ovd4-9o+ud3hj-Kvm-yUQ@mail.gmail.com>
	<CAGxFJbRXcbdK2crkepYv0Bp641LShJq6T3OYMV9R13Kf=aMCew@mail.gmail.com>
	<7B443CFC-4E47-44D9-9668-5C71207BD054@me.com>
	<CABLo8nFEOJoqbOzmfK0epfQsA8Y5ENTDZdAzJgufEpkh3Y_ROQ@mail.gmail.com>
	<CABLo8nGorhH=VMZBfXDK5K6GUcEc_OaUB=82rf8bmQ+sbLeTHA@mail.gmail.com>
	<CABLo8nF0KAwzx6pTET6gSE=VMUQf=PgD2rNmKz9WDTTZqk5r1A@mail.gmail.com>
	<AF50D9BA-C55E-4E8D-A369-07AE1B895A3D@utoronto.ca>
Message-ID: <773C5FF3-0CA2-471A-9BB3-E7EA7ACD6C3D@utoronto.ca>

... and, less explicitly, but more compactly:
y <- array(dim=c(200,1))


B.


On Sep 17, 2015, at 10:07 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote:

> x <- rep(NA, 200)
> 
> For all cases I can think of, that is enough. If you MUST have a matrix with one column and two hundred rows, set:
> 
> dim(x) <- c(200,1)
> 
> 
> B.
> 
> On Sep 17, 2015, at 9:40 AM, thanoon younis <thanoon.younis80 at gmail.com> wrote:
> 
>> Dear all users
>> 
>> I want to write a vector with one column and just NA values and nrow=200
>> when i write X=numeric(NA) is not correct  how can i do this please?
>> 
>> 
>> Regards
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Thu Sep 17 16:19:12 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 17 Sep 2015 07:19:12 -0700
Subject: [R] Fwd:  generate ordered categorical variable in R
In-Reply-To: <CABLo8nF0KAwzx6pTET6gSE=VMUQf=PgD2rNmKz9WDTTZqk5r1A@mail.gmail.com>
References: <CABLo8nHwvA+pW-JGkua=SCZiAow3_oOpNVEsKpaLHbEMd=kTNQ@mail.gmail.com>
	<CAGxFJbSPEwFTakaQ6sydnNTJgMXYv_jDt6FoD=JeKqOUso=SvQ@mail.gmail.com>
	<C216AA40-5708-4E8B-B4AD-FEAEC10C081E@me.com>
	<CAGxFJbRDJ8-Qq-Qq5LBKVZ7+xOYW6ovd4-9o+ud3hj-Kvm-yUQ@mail.gmail.com>
	<CAGxFJbRXcbdK2crkepYv0Bp641LShJq6T3OYMV9R13Kf=aMCew@mail.gmail.com>
	<7B443CFC-4E47-44D9-9668-5C71207BD054@me.com>
	<CABLo8nFEOJoqbOzmfK0epfQsA8Y5ENTDZdAzJgufEpkh3Y_ROQ@mail.gmail.com>
	<CABLo8nGorhH=VMZBfXDK5K6GUcEc_OaUB=82rf8bmQ+sbLeTHA@mail.gmail.com>
	<CABLo8nF0KAwzx6pTET6gSE=VMUQf=PgD2rNmKz9WDTTZqk5r1A@mail.gmail.com>
Message-ID: <AC02741D-56FD-4824-8F28-740C00CBDF97@dcn.davis.CA.us>

Vectors have no columns or rows.

rep(  NA, 200 )

If you need a matrix, you have to turn it into one:

matrix( rep(  NA, 200 ), ncol=1 )
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 17, 2015 6:40:34 AM PDT, thanoon younis <thanoon.younis80 at gmail.com> wrote:
>Dear all users
>
>I want to write a vector with one column and just NA values and
>nrow=200
>when i write X=numeric(NA) is not correct  how can i do this please?
>
>
>Regards
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From thanoon.younis80 at gmail.com  Thu Sep 17 16:37:32 2015
From: thanoon.younis80 at gmail.com (thanoon younis)
Date: Thu, 17 Sep 2015 17:37:32 +0300
Subject: [R] (no subject)
Message-ID: <CABLo8nF02o06-JOr-gkhabAi+cohMJSzb1h11icVhFnY5E70AA@mail.gmail.com>

Dear R - Users
I have a small problem when i generated two column with 200 rows and as
follows

Q1[i,1]=sample(4, 200, replace = TRUE); Q1[i,2]=rbinom(200,1,0.7)

the first vector is ordered categorical variable and the second vector is
dichotomous variable but when i run this code i found this error

Error in Q1[i, 2] = rbinom(200, 1, 0.7) :
  number of items to replace is not a multiple of replacement length


Any help would be appreciated.



Regards


-- 
Thanoon Y. Thanoon
PhD Candidate
Department of Mathematical Sciences
Faculty of Science
University Technology Malaysia, UTM
E.Mail: Thanoon.younis80 at gmail.com
E.Mail: dawn_prayer80 at yahoo.com
Facebook:Thanoon Younis AL-Shakerchy
Twitter: Thanoon Alshakerchy
H.P:00601127550205

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Thu Sep 17 16:43:56 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 17 Sep 2015 15:43:56 +0100
Subject: [R] (no subject)
In-Reply-To: <CABLo8nF02o06-JOr-gkhabAi+cohMJSzb1h11icVhFnY5E70AA@mail.gmail.com>
References: <CABLo8nF02o06-JOr-gkhabAi+cohMJSzb1h11icVhFnY5E70AA@mail.gmail.com>
Message-ID: <55FAD1AC.1070701@sapo.pt>

Hello,

Try th following.

Q1 <- matrix(c(sample(4, 200, replace = TRUE), rbinom(200,1,0.7)), ncol = 2)
Q1


Hope this helps,

Rui Barradas

Em 17-09-2015 15:37, thanoon younis escreveu:
> Dear R - Users
> I have a small problem when i generated two column with 200 rows and as
> follows
>
> Q1[i,1]=sample(4, 200, replace = TRUE); Q1[i,2]=rbinom(200,1,0.7)
>
> the first vector is ordered categorical variable and the second vector is
> dichotomous variable but when i run this code i found this error
>
> Error in Q1[i, 2] = rbinom(200, 1, 0.7) :
>    number of items to replace is not a multiple of replacement length
>
>
> Any help would be appreciated.
>
>
>
> Regards
>
>


From karraspito at yahoo.es  Thu Sep 17 14:41:27 2015
From: karraspito at yahoo.es (Iker Vaquero Alba)
Date: Thu, 17 Sep 2015 12:41:27 +0000 (UTC)
Subject: [R] Factor response and explanatory variables.
Message-ID: <1072170684.1295823.1442493687421.JavaMail.yahoo@mail.yahoo.com>


?? Hello everyone,
?? I am going to ask this certainly tricky question here not (yet) with the intention of getting a definitive answer, as I need to deepen my questions much more, but just to have an approximate idea of which direction taking next. 
?? I have a dataset where the potential response variables are categorical multinomial (ordered, I think): several people were asked to give a value from 1 to 5 to several attributes in a potential partner, both for a short term commitment or for a longer relationship. The age, religion, sexual orientation, sexual identity (gender), self-perceived sexual attractivess and minimum attractiveness demanded in a potential partner (this ones also with values 1 to 5) were recorded for each participant.?? The idea is using the values given to attributes in potential partners as the response variable, and see to what extent these are influenced by the person's age, gender, religion and so on. 
?? The problem is that all my variables are factors, I have no numeric ones. Also, as the values given to the same attributes for a short term commitment of for a longer relationship are expected to be correlated, I was considering using multiple response variables, which adds even more difficulty to the model.
?? I have been reading about MCMCglmm, the course notes, which are not easy to understand. In any case, at some point I have read that if I don't want to fit random effects (as I think it's my case), I'd better use the pscl package instead.

?? Can you give me any advice at this point? Should I try and use pscl, or is it better to try with MCMCglmm given the difficulty of the model? Any little help will be highly appreciated. 

?? Thank you very much
?? Iker.

__________________________________________________________________

?? Dr. Iker Vaquero-Alba
?? Visiting Postdoctoral Research Associate
?? Laboratory of Evolutionary Ecology of Adaptations 
?? School of Life Sciences
?? Joseph Banks Laboratories
?? University of Lincoln?? Brayford Campus, Lincoln
?? LN6 7DL
?? United Kingdom

?? https://eric.exeter.ac.uk/repository/handle/10036/3381


	[[alternative HTML version deleted]]


From rosita21 at gmail.com  Thu Sep 17 17:16:47 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Thu, 17 Sep 2015 16:16:47 +0100
Subject: [R] HELP IN GRAPHS - slip screen
In-Reply-To: <CA+8X3fUpj3vfn=V8rrUNnRSMpu2R9D2DB1gfNQnMQ_tYC96iuQ@mail.gmail.com>
References: <F53D3AF7-922F-4352-B554-1E7F5244AB75@gmail.com>
	<CA+8X3fUpj3vfn=V8rrUNnRSMpu2R9D2DB1gfNQnMQ_tYC96iuQ@mail.gmail.com>
Message-ID: <CAB630FFBi7xFeZ1GtfZNzcmbSGpgiMB8s_xL8iNEFOgGud1f6Q@mail.gmail.com>

Dear Jim,

It works, nonetheless, it doesn't slip the screen correctly :(

Do you have any idea?


I used the code:


#setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
setwd("~/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")


library(ggplot2)
library(reshape)
library(lattice)


# read in what looks like half of the data

bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
SE.alpha1<-read.csv("graphs_SE_alpha1.csv")



quartz(width=10,height=6)

# do the first split, to get the rightmost screen for the legend
split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
# now split the first screen to get your eight screens (numbered 3 to 10)
for the plots
split.screen(figs=matrix(c(0,0.25,0.5,1,
                           0.25,0.5,0.5,1,
                           0.5,0.75,0.5,1,
                           0.75,1,0.5,1,
                           0,0.25,0,0.5,
                           0.25,0.5,0,0.5,
                           0.5,0.75,0,0.5,
                           0.75,1,0,0.5),
                         ncol=4,byrow=TRUE),screen=1)



#split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
#                           0.5,1,0.5,1,#primeira linha segunda coluna
#                           0,0.5,0,0.5,#segunda linha primeira coluna
#                           0.5,1,0,0.5),#segunda linha segunda coluna
#                         ncol=4,byrow=TRUE),screen=1)


# this produces seven screens numbered like this:
#   3   4   5   6
#                    2
#   7   8   9   10
# select the upper left screen



screen(3)
par(mar=c(0,3.5,3,0))
# now the second set
n250<-bias.alpha1$nsample==250
matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1,
.6),main="nsample=250",ylab="", cex.main=1)
abline(h = 0, col = "gray60")
mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2,
cex.main=1)

screen(4)
par(mar=c(0,0,3,0))
# now the second set
n1000<-bias.alpha1$nsample==1000
matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1,
.6),main="nsample=1000",ylab="")
abline(h = 0, col = "gray60")



screen(5)
par(mar=c(0,3.5,3,0))
# now the second set
par(mar=c(3,3.5,0,0))
# now the second set
n250<-bias.alpha2$nsample==250
matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
abline(h = 0, col = "gray60")
mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2,
cex.main=1.5)

screen(6)
par(mar=c(3,0,0,0))
# now the second set
n1000<-bias.alpha2$nsample==1000
matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
abline(h = 0, col = "gray60")




screen(7)
par(mar=c(0,3.5,3,0))
# now the second set
n250<-SE.alpha1$nsample==250
matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0,
1.1),main="nsample=250",ylab="", cex.main=1)
abline(h = -1, col = "gray60")
mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2, cex.main=1)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)


screen(8)
par(mar=c(0,0,3,0))
# now the second set
n1000<-SE.alpha1$nsample==1000
matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0,
1.1),main="nsample=1000",ylab="")
abline(h = -1, col = "gray60")




screen(9)
par(mar=c(3,3.5,0,0))
# now the second set
n250<-SE.alpha2$nsample==250
matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
abline(h = -.5, col = "gray60")
mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2,
cex.main=1.5)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)


screen(10)
par(mar=c(3,0,0,0))
# now the second set
n1000<-SE.alpha2$nsample==1000
matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
abline(h = -.5, col = "gray60")
mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)



screen(2)
par(mar=c(0,0,0,0))
# plot an empty plot to get the coordinates
plot(0:1,0:1,type="n",axes=FALSE)
legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n",
lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)


close.screen(all=TRUE)


and I attach the output graph.



Best,
RO

Atenciosamente,
Rosa Oliveira

_________________________________

Antes de imprimir este e-mail pense bem se tem mesmo que o fazer.
H? cada vez menos ?rvores.
N?o imprima, pense na sua responsabilidade e compromisso com o MEIO
AMBIENTE!
<http://pt.dreamstime.com/cora-ccedil-atildeo-criado-das-folhas-de-aacutervores-diferentes-thumb12275776.jpg>
<http://pt.dreamstime.com/cora-ccedil-atildeo-criado-das-folhas-de-aacutervores-diferentes-thumb12275776.jpg>

2015-09-17 12:18 GMT+01:00 Jim Lemon <drjimlemon at gmail.com>:

> Hi Rosa,
> Try this:
>
> # do the first split, to get the rightmost screen for the legend
> split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
> # now split the first screen to get your eight screens (numbered 3 to 10)
> for the plots
> split.screen(figs=matrix(c(0,0.25,0.5,1,
>                            0.25,0.5,0.5,1,
>                            0.5,0.75,0.5,1,
>                            0.75,1,0.5,1,
>                            0,0.25,0,0.5,
>                            0.25,0.5,0,0.5,
>                            0.5,0.75,0,0.5,
>                            0.75,1,0,0.5),
>                          ncol=4,byrow=TRUE),screen=1)
>
> Jim
>
>
> On Thu, Sep 17, 2015 at 2:45 AM, Rosa Oliveira <rosita21 at gmail.com> wrote:
>
>> Dear all,
>>
>> I?m trying to do a graph,
>>
>> 3 rows, 5 columns, with the design:
>> #   3   4   5   6
>> #                    2
>> #   7   8   9   10
>>
>> I had a code for 3 rows, 3 columns, with the design::
>> #   3   4
>> #            2
>> #   7   8
>>  and I tried to modify it, but I had no success :(
>>
>> I suppose the problem is in the slip.screen code (red part of the code).
>>
>> I attach my code, can anyone please help me?
>>
>>
>> Best,
>> RO
>>
>>
>> setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
>>
>> library(ggplot2)
>> library(reshape)
>> library(lattice)
>>
>>
>> # read in what looks like half of the data
>>
>> bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
>> SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
>> bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
>> SE.alpha1<-read.csv("graphs_SE_alpha1.csv")
>>
>>
>>
>> quartz(width=10,height=6)
>> # do the first split, to get the rightmost screen for the legend
>> split.screen(figs=matrix(c(0,0.8,0,1,0.8,1,0,1),nrow=2,byrow=TRUE))
>> # now split the first screen to get your six screens for the plots
>>
>>
>>
>> split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
>>                            0.5,1,0.5,1,#primeira linha segunda coluna
>>                            0,0.5,0,0.5,#segunda linha primeira coluna
>>                            0.5,1,0,0.5),#segunda linha segunda coluna
>>                          ncol=4,byrow=TRUE),screen=1)
>>
>>
>> # this produces seven screens numbered like this:
>> #   3   4   5   6
>> #                    2
>> #   7   8   9   10
>> # select the upper left screen
>>
>>
>>
>> screen(3)
>> par(mar=c(0,3.5,3,0))
>> # now the second set
>> n250<-bias.alpha1$nsample==250
>> matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1,
>> .6),main="nsample=250",ylab="", cex.main=1)
>> abline(h = 0, col = "gray60")
>> mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2,
>> cex.main=1)
>>
>> screen(4)
>> par(mar=c(0,0,3,0))
>> # now the second set
>> n1000<-bias.alpha1$nsample==1000
>> matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1,
>> .6),main="nsample=1000",ylab="")
>> abline(h = 0, col = "gray60")
>>
>>
>>
>> screen(5)
>> par(mar=c(0,3.5,3,0))
>> # now the second set
>> par(mar=c(3,3.5,0,0))
>> # now the second set
>> n250<-bias.alpha2$nsample==250
>> matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
>> abline(h = 0, col = "gray60")
>> mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2,
>> cex.main=1.5)
>>
>> screen(6)
>> par(mar=c(3,0,0,0))
>> # now the second set
>> n1000<-bias.alpha2$nsample==1000
>> matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
>> abline(h = 0, col = "gray60")
>>
>>
>>
>>
>> screen(7)
>> par(mar=c(0,3.5,3,0))
>> # now the second set
>> n250<-SE.alpha1$nsample==250
>> matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0,
>> 1.1),main="nsample=250",ylab="", cex.main=1)
>> abline(h = -1, col = "gray60")
>> mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2,
>> cex.main=1)
>> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>>
>>
>> screen(8)
>> par(mar=c(0,0,3,0))
>> # now the second set
>> n1000<-SE.alpha1$nsample==1000
>> matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0,
>> 1.1),main="nsample=1000",ylab="")
>> abline(h = -1, col = "gray60")
>>
>>
>>
>>
>> screen(9)
>> par(mar=c(3,3.5,0,0))
>> # now the second set
>> n250<-SE.alpha2$nsample==250
>> matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
>> abline(h = -.5, col = "gray60")
>> mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2,
>> cex.main=1.5)
>> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>>
>>
>> screen(10)
>> par(mar=c(3,0,0,0))
>> # now the second set
>> n1000<-SE.alpha2$nsample==1000
>> matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
>> abline(h = -.5, col = "gray60")
>> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
>>
>>
>>
>> screen(2)
>> par(mar=c(0,0,0,0))
>> # plot an empty plot to get the coordinates
>> plot(0:1,0:1,type="n",axes=FALSE)
>> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n",
>> lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
>>
>>
>> close.screen(all=TRUE)
>>
>>
>>
>>
>> Best,
>> RO
>>
>>
>> Atenciosamente,
>> Rosa Oliveira
>>
>> --
>>
>> ____________________________________________________________________________
>>
>>
>> Rosa Celeste dos Santos Oliveira,
>>
>> E-mail: rosita21 at gmail.com
>> Tlm: +351 939355143
>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>>
>> ____________________________________________________________________________
>> "Many admire, few know"
>> Hippocrates
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: compare_8.pdf
Type: application/pdf
Size: 396551 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150917/1b3594ae/attachment.pdf>

From sheasley at aegonusa.com  Thu Sep 17 17:20:33 2015
From: sheasley at aegonusa.com (smheas)
Date: Thu, 17 Sep 2015 08:20:33 -0700 (PDT)
Subject: [R] Need data labels to jitter with datapoints in boxplot
Message-ID: <1442503233094-4712380.post@n4.nabble.com>

Hello, I have created a boxplot with the data points overlayed on top using
the below code. I am happy with the way the datapoints are jittered, however
I cannot figure out how to get the labels to jitter along with the
datapoints. The labels remain in the center and are unreadable. I have tried
a lot of different ways to get them to jitter but can't seem to make it
work. I have also tried making the label font smaller, but they still
overlap too much. Note I cannot use the method of assigning the lcoation of
each label - there are too many and this graph will be updated frequently
with new data. I greatly appreciate your help. Thanks.

After reading in data...

#load packages
library(ggplot2)
#change letter to factor
df$Letter<-as.factor(df$Letter)
#create boxplot
plot<-qplot(Letter,Number,data=df,geom=c("boxplot","jitter"),fill=Letter) 
#Add formatting
plot<-plot + geom_text(aes(label=Identifier),size=3)
ggsave(filename="Spreads Plot.pdf",plot=plot,width=11,height=8,units="in")

Result:
<http://r.789695.n4.nabble.com/file/n4712380/PostPic.png> 

Sample Data:
Identifier	Letter	Number
AB	AA-	46.74
BC	A	59.62
CD	A	61.63
DE	A	69.49
EF	A+	69.73
FG	A+	74.57
GH	A	75.01
HI	A	77.52
IJ	A	77.52
JK	A	80.12
KL	A	80.14
LM	A-	80.35
MN	A	81.98
NO	A-	82.72
OP	A+	83.56
PQ	A	85.29
QR	A-	85.46
RS	A-	85.92
ST	A	86.11
TU	A-	86.55
UV	A	86.57
VW	A	88.32
WX	A	89.4
XY	A-	96.81
YZ	A+	97.6
BA	A-	101.86
CB	A	102.37
DC	A	104.29
ED	A	104.92
FE	A	106.29
GF	A-	111.84
HG	A+	121.91
IH	A-	123.64




--
View this message in context: http://r.789695.n4.nabble.com/Need-data-labels-to-jitter-with-datapoints-in-boxplot-tp4712380.html
Sent from the R help mailing list archive at Nabble.com.


From schwidom at gmx.net  Thu Sep 17 18:01:06 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Thu, 17 Sep 2015 18:01:06 +0200
Subject: [R] aggregate counting variable factors
In-Reply-To: <CAHOWgNWsu++ShKUrLg+ftMs+81AErNBoyg4xnz4-5sRG-H7YwA@mail.gmail.com>
References: <CAHOWgNWsu++ShKUrLg+ftMs+81AErNBoyg4xnz4-5sRG-H7YwA@mail.gmail.com>
Message-ID: <20150917160106.GA2770@debian64>

Hi

res <- sapply( df1[ , -1], function( x) table(x)[as.character( 0:5)])
rownames( res) <- paste( sep='', 'result', 0:5)
res[ is.na( res)] <- 0

res
        item1 item2 item3 item4 item5
result0     1     0     1     1     0
result1     1     2     0     0     0
result2     1     2     1     1     0
result3     0     0     1     2     0
result4     0     0     1     0     0
result5     1     0     0     0     4


t( res)
      result0 result1 result2 result3 result4 result5
item1       1       1       1       0       0       1
item2       0       2       2       0       0       0
item3       1       0       1       1       1       0
item4       1       0       1       2       0       0
item5       0       0       0       0       0       4


Regards


On Wed, Sep 16, 2015 at 10:43:16PM +0200, Kai Mx wrote:
> Hi everybody,
> 
> >From a questionnaire, I have a dataset  like this one with some 40 items:
> 
> df1 <- data.frame(subject=c('user1','user2', 'user3', 'user4'),
> item1=c(0,1,2,5), item2=c(1,2,1,2), item3=c(2,3,4,0), item4=c(0,3,3,2),
> item5=c(5,5,5,5))
> 
> Users can choose an answer from 0 to 5 for each item.
> 
> Now I want to reshape the dataset to have the items in rows and the count
> of each of the result factors in columns:
> 
> result <- data.frame (item=c("item1", "item2", "item3", "item4", "item5"),
> result0=c(1,0,1,1,0), result1=c(1,2,0,0,0), result2=c(1,2,1,1,0),
> result3=c(0,0,1,2,0), result4=c(0,0,1,0,0), result5=c(1,0,0,0,4))
> 
> I have been fiddling around with melt/plyr, but haven't been able to figure
> it out. What's the most elegant way to do this (preferably without typing
> in all the item names).
> 
> Thanks so much!
> 
> Best,
> 
> Kai
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From schwidom at gmx.net  Thu Sep 17 18:03:28 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Thu, 17 Sep 2015 18:03:28 +0200
Subject: [R] aggregate counting variable factors
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C40124@SRVEXCHMBX.precheza.cz>
References: <CAHOWgNWsu++ShKUrLg+ftMs+81AErNBoyg4xnz4-5sRG-H7YwA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C40124@SRVEXCHMBX.precheza.cz>
Message-ID: <20150917160328.GB2770@debian64>

Hi

where can i find 'melt' and 'dcast' ?

Regards


On Thu, Sep 17, 2015 at 08:22:10AM +0000, PIKAL Petr wrote:
> Hi
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Kai Mx
> > Sent: Wednesday, September 16, 2015 10:43 PM
> > To: r-help mailing list
> > Subject: [R] aggregate counting variable factors
> >
> > Hi everybody,
> >
> > >From a questionnaire, I have a dataset  like this one with some 40
> > items:
> >
> > df1 <- data.frame(subject=c('user1','user2', 'user3', 'user4'),
> > item1=c(0,1,2,5), item2=c(1,2,1,2), item3=c(2,3,4,0), item4=c(0,3,3,2),
> > item5=c(5,5,5,5))
> >
> > Users can choose an answer from 0 to 5 for each item.
> >
> > Now I want to reshape the dataset to have the items in rows and the
> > count
> > of each of the result factors in columns:
> >
> > result <- data.frame (item=c("item1", "item2", "item3", "item4",
> > "item5"),
> > result0=c(1,0,1,1,0), result1=c(1,2,0,0,0), result2=c(1,2,1,1,0),
> > result3=c(0,0,1,2,0), result4=c(0,0,1,0,0), result5=c(1,0,0,0,4))
> >
> > I have been fiddling around with melt/plyr, but haven't been able to
> > figure
> > it out. What's the most elegant way to do this (preferably without
> > typing
> > in all the item names).
> 
> Perhaps,
> 
> m<-melt(df1)
> m$value<-paste("res",m$value, sep="")
> dcast(m, variable~value)
> Aggregation function missing: defaulting to length
>   variable res0 res1 res2 res3 res4 res5
> 1    item1    1    1    1    0    0    1
> 2    item2    0    2    2    0    0    0
> 3    item3    1    0    1    1    1    0
> 4    item4    1    0    1    2    0    0
> 5    item5    0    0    0    0    0    4
> 
> Cheers
> Petr
> 
> 
> >
> > Thanks so much!
> >
> > Best,
> >
> > Kai
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> 
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> 
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ccberry at ucsd.edu  Thu Sep 17 18:29:38 2015
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Thu, 17 Sep 2015 09:29:38 -0700
Subject: [R] Multiple if function
In-Reply-To: <BADC6A14-1C55-461C-8E73-22C0EDA768BC@xs4all.nl>
References: <185029592.262853.1442314597939.JavaMail.yahoo@mail.yahoo.com>
	<E41B375B7520DE4A8C60781AC60B75450676994E28@AKLEXM01.PFR.CO.NZ>
	<CAGxFJbR-713f4OLPUVc70D5tFXg4n4ik0UreAXmYSxv05w8xfw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2267@mb02.ads.tamu.edu>
	<CAGxFJbQaqy+Xm8VNhBTX0CMSNS5C2D1f2gGNVadMH6zXD9e7Qw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C2385@mb02.ads.tamu.edu>
	<D7D70A25-A919-4E20-8BEC-3D4D14253B67@comcast.net>
	<CAGxFJbTWLFM86CdBRS4NTrAaeB0-cnYniOMRpXf84B-OURUzBQ@mail.gmail.com>
	<alpine.OSX.2.20.1509151914220.1091@charles-berrys-macbook.local>
	<865084AF-2702-4453-94D3-AE4A0EC45DB5@kit.edu>
	<9803D422-0B5F-4BA7-BBF8-77AF502A1EB2@gmail.com>
	<CAGxFJbReNe0E2=2QYM6iRaksZjZwaXPL1wuUjmsGbiY4XMqkmw@mail.gmail.com>
	<55F9FE6D.4080404@ttk.mta.hu>
	<BADC6A14-1C55-461C-8E73-22C0EDA768BC@xs4all.nl>
Message-ID: <alpine.OSX.2.20.1509170917580.470@charles-berrys-macbook.local>

On Thu, 17 Sep 2015, Berend Hasselman wrote:

>
>> On 17 Sep 2015, at 01:42, D?nes T?th <toth.denes at ttk.mta.hu> wrote:
>>
>>
>>
>> On 09/16/2015 04:41 PM, Bert Gunter wrote:
>>> Yes! Chuck's use of mapply is exactly the split/combine strategy I was
>>> looking for. In retrospect, exactly how one should think about it.
>>> Many thanks to all for a constructive discussion .
>>>
>>> -- Bert
>>>
>>>
>>> Bert Gunter
>>>
>>>>>>
>>>>>> Use mapply like this on large problems:
>>>>>>
>>>>>> unsplit(
>>>>>>   mapply(
>>>>>>       function(x,z) eval( x, list( y=z )),
>>>>>>       expression( A=y*2, B=y+3, C=sqrt(y) ),
>>>>>>       split( dat$Flow, dat$ASB ),
>>>>>>       SIMPLIFY=FALSE),
>>>>>>   dat$ASB)
>>>>>>
>>>>>> Chuck
>>>>>>
>>
>>
>> Is there any reason not to use data.table for this purpose, especially if efficiency is of concern?
>>
>> ---
>>
>> # load data.table and microbenchmark
>> library(data.table)
>> library(microbenchmark)
>> #
>> # prepare data
>> DF <- data.frame(
>>    ASB = rep_len(factor(LETTERS[1:3]), 3e5),
>>    Flow = rnorm(3e5)^2)
>> DT <- as.data.table(DF)
>> DT[, ASB := as.character(ASB)]
>> #
>> # define functions
>> #
>> # Chuck's version
>> fnSplit <- function(dat) {
>>    unsplit(
>>        mapply(
>>            function(x,z) eval( x, list( y=z )),
>>            expression( A=y*2, B=y+3, C=sqrt(y) ),
>>            split( dat$Flow, dat$ASB ),
>>            SIMPLIFY=FALSE),
>>        dat$ASB)
>> }
>> #
>> # data.table-way (IMHO, much easier to read)
>> fnDataTable <- function(dat) {
>>    dat[,
>>        result :=
>>            if (.BY == "A") {
>>                2 * Flow
>>            } else if (.BY == "B") {
>>                3 + Flow
>>            } else if (.BY == "C") {
>>                sqrt(Flow)
>>            },
>>        by = ASB]
>> }
>> #
>> # benchmark
>> #
>> microbenchmark(fnSplit(DF), fnDataTable(DT))
>> identical(fnSplit(DF), fnDataTable(DT)[, result])
>>
>> ---
>>
>> Actually, in Chuck's version the unsplit() part is slow. If the order is not of concern (e.g., DF is reordered before calling fnSplit), fnSplit is comparable to the DT-version.
>>
>
> But David?s version is faster than Chuck?s fnSplit. I modified David?s solution slightly to get a result that is identical to fnSplit.
>
> # David's version
> # my modification to return a vector just like fnSplit
> fnDavid <- function(dat) {
>    z <- mapply(
>          function(x,z) eval( x, list( y=z )),
>          expression(A= y*2, B=y+3, C=sqrt(y) ),
>          split( dat$Flow, dat$ASB ),
>          USE.NAMES=FALSE, SIMPLIFY=TRUE
>        )
>    as.vector(t(z))
> }
>
> Added this to D?nes's code.
> Benchmarking  with R package rbenchmark and testing result like this
>
> library(rbenchmark)
> benchmark(fnSplit(DF), fnDataTable(DT),fnDavid(DF))
> identical(fnSplit(DF), fnDataTable(DT)[, result])
> identical(fnSplit(DF), fnDavid(DF))
>
> gave this:
>
>             test replications elapsed relative user.self sys.self user.child
> 2 fnDataTable(DT)          100   0.829    1.000     0.762    0.066          0
> 3     fnDavid(DF)          100   1.615    1.948     1.515    0.098          0
> 1     fnSplit(DF)          100   2.878    3.472     2.685    0.190          0
>  sys.child
> 2         0
> 3         0
> 1         0
>
>> identical(fnSplit(DF), fnDataTable(DT)[, result])
> [1] TRUE
>> identical(fnSplit(DF), fnDavid(DF))
> [1] TRUE

The above `TRUE' depends on the structure of ASB here. identical(...) is 
often FALSE in the general case. A permutation of ASB is enough to show 
this:

> DF$ASB <- sample(DF$ASB)
> identical(fnSplit(DF), fnDavid(DF))
[1] FALSE
>

unsplit() is the price you pay to cope with general orderings.

Chuck

From bgunter.4567 at gmail.com  Thu Sep 17 18:36:47 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 17 Sep 2015 09:36:47 -0700
Subject: [R] Factor response and explanatory variables.
In-Reply-To: <1072170684.1295823.1442493687421.JavaMail.yahoo@mail.yahoo.com>
References: <1072170684.1295823.1442493687421.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAGxFJbQ=jkDXLLX_M0dKjyFRJCuX1AvrJuN-iKegX_7CD-FA0g@mail.gmail.com>

Best advice I can give: Find a local statistical expert to work with.
You appear to be asking for help understanding statistical methodology
when you do not have the necessary background to do so.

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Sep 17, 2015 at 5:41 AM, Iker Vaquero Alba <karraspito at yahoo.es> wrote:
>
>    Hello everyone,
>    I am going to ask this certainly tricky question here not (yet) with the intention of getting a definitive answer, as I need to deepen my questions much more, but just to have an approximate idea of which direction taking next.
>    I have a dataset where the potential response variables are categorical multinomial (ordered, I think): several people were asked to give a value from 1 to 5 to several attributes in a potential partner, both for a short term commitment or for a longer relationship. The age, religion, sexual orientation, sexual identity (gender), self-perceived sexual attractivess and minimum attractiveness demanded in a potential partner (this ones also with values 1 to 5) were recorded for each participant.   The idea is using the values given to attributes in potential partners as the response variable, and see to what extent these are influenced by the person's age, gender, religion and so on.
>    The problem is that all my variables are factors, I have no numeric ones. Also, as the values given to the same attributes for a short term commitment of for a longer relationship are expected to be correlated, I was considering using multiple response variables, which adds even more difficulty to the model.
>    I have been reading about MCMCglmm, the course notes, which are not easy to understand. In any case, at some point I have read that if I don't want to fit random effects (as I think it's my case), I'd better use the pscl package instead.
>
>    Can you give me any advice at this point? Should I try and use pscl, or is it better to try with MCMCglmm given the difficulty of the model? Any little help will be highly appreciated.
>
>    Thank you very much
>    Iker.
>
> __________________________________________________________________
>
>    Dr. Iker Vaquero-Alba
>    Visiting Postdoctoral Research Associate
>    Laboratory of Evolutionary Ecology of Adaptations
>    School of Life Sciences
>    Joseph Banks Laboratories
>    University of Lincoln   Brayford Campus, Lincoln
>    LN6 7DL
>    United Kingdom
>
>    https://eric.exeter.ac.uk/repository/handle/10036/3381
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Thu Sep 17 18:57:43 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 17 Sep 2015 17:57:43 +0100
Subject: [R] aggregate counting variable factors
In-Reply-To: <20150917160328.GB2770@debian64>
References: <CAHOWgNWsu++ShKUrLg+ftMs+81AErNBoyg4xnz4-5sRG-H7YwA@mail.gmail.com>	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C40124@SRVEXCHMBX.precheza.cz>
	<20150917160328.GB2770@debian64>
Message-ID: <55FAF107.1060806@sapo.pt>

In package reshape2

Hope this helps,

Rui Barradas

Em 17-09-2015 17:03, Frank Schwidom escreveu:
> Hi
>
> where can i find 'melt' and 'dcast' ?
>
> Regards
>
>
> On Thu, Sep 17, 2015 at 08:22:10AM +0000, PIKAL Petr wrote:
>> Hi
>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Kai Mx
>>> Sent: Wednesday, September 16, 2015 10:43 PM
>>> To: r-help mailing list
>>> Subject: [R] aggregate counting variable factors
>>>
>>> Hi everybody,
>>>
>>> >From a questionnaire, I have a dataset  like this one with some 40
>>> items:
>>>
>>> df1 <- data.frame(subject=c('user1','user2', 'user3', 'user4'),
>>> item1=c(0,1,2,5), item2=c(1,2,1,2), item3=c(2,3,4,0), item4=c(0,3,3,2),
>>> item5=c(5,5,5,5))
>>>
>>> Users can choose an answer from 0 to 5 for each item.
>>>
>>> Now I want to reshape the dataset to have the items in rows and the
>>> count
>>> of each of the result factors in columns:
>>>
>>> result <- data.frame (item=c("item1", "item2", "item3", "item4",
>>> "item5"),
>>> result0=c(1,0,1,1,0), result1=c(1,2,0,0,0), result2=c(1,2,1,1,0),
>>> result3=c(0,0,1,2,0), result4=c(0,0,1,0,0), result5=c(1,0,0,0,4))
>>>
>>> I have been fiddling around with melt/plyr, but haven't been able to
>>> figure
>>> it out. What's the most elegant way to do this (preferably without
>>> typing
>>> in all the item names).
>>
>> Perhaps,
>>
>> m<-melt(df1)
>> m$value<-paste("res",m$value, sep="")
>> dcast(m, variable~value)
>> Aggregation function missing: defaulting to length
>>    variable res0 res1 res2 res3 res4 res5
>> 1    item1    1    1    1    0    0    1
>> 2    item2    0    2    2    0    0    0
>> 3    item3    1    0    1    1    1    0
>> 4    item4    1    0    1    2    0    0
>> 5    item5    0    0    0    0    0    4
>>
>> Cheers
>> Petr
>>
>>
>>>
>>> Thanks so much!
>>>
>>> Best,
>>>
>>> Kai
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
>> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
>> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jvadams at usgs.gov  Thu Sep 17 19:39:41 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Thu, 17 Sep 2015 12:39:41 -0500
Subject: [R] finding those elements of listA that are found in listB
In-Reply-To: <55F9F078020000CB001392E3@smtp.medicine.umaryland.edu>
References: <55F9F078020000CB001392E3@smtp.medicine.umaryland.edu>
Message-ID: <CAN5YmCE6O-n8NxfsioqO2cOZG7-2qfZ_+D9UFe=t9EpvVojXjg@mail.gmail.com>

John,

The intersect() function may help you.  For example:

listA <- sort(sample(10, 5))
listB <- sort(sample(10, 5))
both <- intersect(listA, listB)

> listA
[1] 2 4 7 8 9
> listB
[1]  1  2  3  8 10
> both
[1] 2 8

Jean

On Wed, Sep 16, 2015 at 9:43 PM, John Sorkin <JSorkin at grecc.umaryland.edu>
wrote:

> I have two structures. I think they are lists, but I am not sure. Both
> structures contain integers. I am trying to find those members of list b
> that are found in list a. I have tried to perform the search using grep,
> but I get an error. Please see code below. I would appreciate knowing how
> to search listB for any element in listA
> Thanks
> John
>
>
> > str(listA)
>  int [1:42] 13083 13705 14123 14168 14382 14652 14654 14678 14817 14822 ...
> > str(listB)
>  int [1:633] 13083 13083 13083 13083 13083 13083 13705 13705 13705 13705
> ...
> > grep(listA,listB)
> [1] 1 2 3 4 5 6
> Warning message:
> In grep(listA, listB) :
>   argument 'pattern' has length > 1 and only the first element will be used
>
>
>
>
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:16}}


From ddalthorp at usgs.gov  Thu Sep 17 20:19:05 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Thu, 17 Sep 2015 11:19:05 -0700 (PDT)
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
Message-ID: <1442513945858-4712390.post@n4.nabble.com>

Can anyone think of a slick way to create an array that looks like c(1:n,
1:(n-1), 1:(n-2), ... , 1)?

The following works, but it's inefficient and a little hard to follow:
n<-5
junk<-array(1:n,dim=c(n,n))
junk[((lower.tri(t(junk),diag=T)))[n:1,]]

Any help would be greatly appreciated!

-Dan



--
View this message in context: http://r.789695.n4.nabble.com/c-1-n-1-n-1-1-n-2-1-tp4712390.html
Sent from the R help mailing list archive at Nabble.com.


From peter.langfelder at gmail.com  Thu Sep 17 20:35:09 2015
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Thu, 17 Sep 2015 11:35:09 -0700
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <1442513945858-4712390.post@n4.nabble.com>
References: <1442513945858-4712390.post@n4.nabble.com>
Message-ID: <CA+hbrhUVZ6LEJc_6XiiffgYuRo_4RRjUE2US=pc0diicKcnNjA@mail.gmail.com>

Not sure if this is slicker or easier to follow than your solution,
but it is shorter :)

do.call(c, lapply(n:1, function(n1) 1:n1))

Peter

On Thu, Sep 17, 2015 at 11:19 AM, Dan D <ddalthorp at usgs.gov> wrote:
> Can anyone think of a slick way to create an array that looks like c(1:n,
> 1:(n-1), 1:(n-2), ... , 1)?
>
> The following works, but it's inefficient and a little hard to follow:
> n<-5
> junk<-array(1:n,dim=c(n,n))
> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
>
> Any help would be greatly appreciated!
>
> -Dan
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/c-1-n-1-n-1-1-n-2-1-tp4712390.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From schwidom at gmx.net  Thu Sep 17 20:37:51 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Thu, 17 Sep 2015 20:37:51 +0200
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <1442513945858-4712390.post@n4.nabble.com>
References: <1442513945858-4712390.post@n4.nabble.com>
Message-ID: <20150917183751.GB4704@debian64>


sequence( 5:1)

Regards.

On Thu, Sep 17, 2015 at 11:19:05AM -0700, Dan D wrote:
> Can anyone think of a slick way to create an array that looks like c(1:n,
> 1:(n-1), 1:(n-2), ... , 1)?
> 
> The following works, but it's inefficient and a little hard to follow:
> n<-5
> junk<-array(1:n,dim=c(n,n))
> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
> 
> Any help would be greatly appreciated!
> 
> -Dan
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/c-1-n-1-n-1-1-n-2-1-tp4712390.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Achim.Zeileis at uibk.ac.at  Thu Sep 17 20:40:49 2015
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Thu, 17 Sep 2015 20:40:49 +0200 (CEST)
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <CA+hbrhUVZ6LEJc_6XiiffgYuRo_4RRjUE2US=pc0diicKcnNjA@mail.gmail.com>
References: <1442513945858-4712390.post@n4.nabble.com>
	<CA+hbrhUVZ6LEJc_6XiiffgYuRo_4RRjUE2US=pc0diicKcnNjA@mail.gmail.com>
Message-ID: <alpine.DEB.2.11.1509172039510.15300@paninaro.uibk.ac.at>

On Thu, 17 Sep 2015, Peter Langfelder wrote:

> Not sure if this is slicker or easier to follow than your solution,
> but it is shorter :)
>
> do.call(c, lapply(n:1, function(n1) 1:n1))

Also not sure about efficiency but somewhat shorter...
unlist(lapply(5:1, seq))

> Peter
>
> On Thu, Sep 17, 2015 at 11:19 AM, Dan D <ddalthorp at usgs.gov> wrote:
>> Can anyone think of a slick way to create an array that looks like c(1:n,
>> 1:(n-1), 1:(n-2), ... , 1)?
>>
>> The following works, but it's inefficient and a little hard to follow:
>> n<-5
>> junk<-array(1:n,dim=c(n,n))
>> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
>>
>> Any help would be greatly appreciated!
>>
>> -Dan
>>
>>
>>
>> --
>> View this message in context: http://r.789695.n4.nabble.com/c-1-n-1-n-1-1-n-2-1-tp4712390.html
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Thu Sep 17 20:52:27 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 17 Sep 2015 11:52:27 -0700
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <alpine.DEB.2.11.1509172039510.15300@paninaro.uibk.ac.at>
References: <1442513945858-4712390.post@n4.nabble.com>
	<CA+hbrhUVZ6LEJc_6XiiffgYuRo_4RRjUE2US=pc0diicKcnNjA@mail.gmail.com>
	<alpine.DEB.2.11.1509172039510.15300@paninaro.uibk.ac.at>
Message-ID: <EF76ED63-CF8A-486E-AC2C-1C61C637F753@comcast.net>

You can add this to the list of options to be tested, although my bet would be placed on `sequence(5:1)`:

> Reduce( function(x,y){c( 1:y, x)}, 1:5)
 [1] 1 2 3 4 5 1 2 3 4 1 2 3 1 2 1


On Sep 17, 2015, at 11:40 AM, Achim Zeileis wrote:

> On Thu, 17 Sep 2015, Peter Langfelder wrote:
> 
>> Not sure if this is slicker or easier to follow than your solution,
>> but it is shorter :)
>> 
>> do.call(c, lapply(n:1, function(n1) 1:n1))
> 
> Also not sure about efficiency but somewhat shorter...
> unlist(lapply(5:1, seq))
> 
>> Peter
>> 
>> On Thu, Sep 17, 2015 at 11:19 AM, Dan D <ddalthorp at usgs.gov> wrote:
>>> Can anyone think of a slick way to create an array that looks like c(1:n,
>>> 1:(n-1), 1:(n-2), ... , 1)?
>>> 
>>> The following works, but it's inefficient and a little hard to follow:
>>> n<-5
>>> junk<-array(1:n,dim=c(n,n))
>>> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
>>> 
>>> Any help would be greatly appreciated!
>>> 
>>> -Dan
>>> 
>>> 

David Winsemius
Alameda, CA, USA


From john.archie.mckown at gmail.com  Thu Sep 17 20:57:59 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Thu, 17 Sep 2015 13:57:59 -0500
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <1442513945858-4712390.post@n4.nabble.com>
References: <1442513945858-4712390.post@n4.nabble.com>
Message-ID: <CAAJSdjg+6UB-UcmmXvcUX1jBfw6f78_qZLKdTW=0Udh5Z+TsCw@mail.gmail.com>

I'm not too sure this is any better:

n<-5
c<-0; # establish result as numeric
for(i in seq(n,1,-1)){ c<-c(c,seq(1,i)); str(c); }; #generate array
c<-c[2:length(c)]; #remove the leading 0

If you're a fan of recursive programming:

> mklist <- function(x) { if (x==1) return(1) else return(
c(seq(1,x),mklist(x-1)) ) ; }
> mklist(5);
 [1] 1 2 3 4 5 1 2 3 4 1 2 3 1 2 1
>

Of course, I've not done any error checking in my function definition. And,
for large values, it can nest too deeply and get

Error: evaluation nested too deeply: infinite recursion /
options(expressions=)?


On Thu, Sep 17, 2015 at 1:19 PM, Dan D <ddalthorp at usgs.gov> wrote:

> Can anyone think of a slick way to create an array that looks like c(1:n,
> 1:(n-1), 1:(n-2), ... , 1)?
>
> The following works, but it's inefficient and a little hard to follow:
> n<-5
> junk<-array(1:n,dim=c(n,n))
> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
>
> Any help would be greatly appreciated!
>
> -Dan
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/c-1-n-1-n-1-1-n-2-1-tp4712390.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From schwidom at gmx.net  Thu Sep 17 21:06:30 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Thu, 17 Sep 2015 21:06:30 +0200
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <EF76ED63-CF8A-486E-AC2C-1C61C637F753@comcast.net>
References: <1442513945858-4712390.post@n4.nabble.com>
	<CA+hbrhUVZ6LEJc_6XiiffgYuRo_4RRjUE2US=pc0diicKcnNjA@mail.gmail.com>
	<alpine.DEB.2.11.1509172039510.15300@paninaro.uibk.ac.at>
	<EF76ED63-CF8A-486E-AC2C-1C61C637F753@comcast.net>
Message-ID: <20150917190630.GA5556@debian64>


how abount a more complicated one?

outer( 1:5, 1:5, '-')[ outer( 1:5, 1:5, '>')]
 [1] 1 2 3 4 1 2 3 1 2 1


On Thu, Sep 17, 2015 at 11:52:27AM -0700, David Winsemius wrote:
> You can add this to the list of options to be tested, although my bet would be placed on `sequence(5:1)`:
> 
> > Reduce( function(x,y){c( 1:y, x)}, 1:5)
>  [1] 1 2 3 4 5 1 2 3 4 1 2 3 1 2 1
> 
> 
> On Sep 17, 2015, at 11:40 AM, Achim Zeileis wrote:
> 
> > On Thu, 17 Sep 2015, Peter Langfelder wrote:
> > 
> >> Not sure if this is slicker or easier to follow than your solution,
> >> but it is shorter :)
> >> 
> >> do.call(c, lapply(n:1, function(n1) 1:n1))
> > 
> > Also not sure about efficiency but somewhat shorter...
> > unlist(lapply(5:1, seq))
> > 
> >> Peter
> >> 
> >> On Thu, Sep 17, 2015 at 11:19 AM, Dan D <ddalthorp at usgs.gov> wrote:
> >>> Can anyone think of a slick way to create an array that looks like c(1:n,
> >>> 1:(n-1), 1:(n-2), ... , 1)?
> >>> 
> >>> The following works, but it's inefficient and a little hard to follow:
> >>> n<-5
> >>> junk<-array(1:n,dim=c(n,n))
> >>> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
> >>> 
> >>> Any help would be greatly appreciated!
> >>> 
> >>> -Dan
> >>> 
> >>> 
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From patzelt at g.harvard.edu  Thu Sep 17 19:55:52 2015
From: patzelt at g.harvard.edu (Patzelt, Edward)
Date: Thu, 17 Sep 2015 13:55:52 -0400
Subject: [R] Optimization Grid Search Slow
Message-ID: <CAB9UfhSaWK2RkMRv_b0=rMt57SOp_nKMvVO42q+hU4gq7koT9w@mail.gmail.com>

R Help -

I am trying to use a grid search for a 2 free parameter reinforcement
learning model and the grid search is incredibly slow. I've used optimx but
can't seem to get reasonable answers. Is there a way to speed up this grid
search dramatically?


dat <- structure(list(choice = c(0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
                                 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
0, 1, 0, 1, 0, 1, 0,
                                 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
0, 0, 1, 0, 0, 1, 1,
                                 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
0, 1, 0, 0, 0, 0, 1,
                                 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
1, 0, 0, 0, 0, 0, 0,
                                 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
1, 0, 0, 0, 0, 0, 1,
                                 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
1, 0, 0, 1, 1, 0, 0,
                                 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
0, 0, 1, 0, 0, 0, 0,
                                 1, 0, 1, 1, 1, 0), reward = c(0L, 0L, 0L,
0L, 1L, 1L, 0L, 0L,
                                                               1L, 0L, 0L,
0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 1L,
                                                               1L, 0L, 1L,
0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L,
                                                               1L, 0L, 1L,
0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L,
                                                               0L, 0L, 1L,
1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L,
                                                               1L, 1L, 0L,
0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
                                                               0L, 0L, 0L,
0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L,
                                                               1L, 0L, 0L,
1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L,
                                                               0L, 1L, 0L,
0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
                                                               0L, 1L, 0L,
1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L,
                                                               0L, 0L, 1L,
0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), RepNum = c(1L,

                                                   1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,

                                                   1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,

                                                   1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,

                                                   1L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,

                                                   2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,

                                                   2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,

                                                   2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,

                                                   2L, 2L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,

                                                   3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,

                                                   3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,

                                                   3L, 3L, 3L, 3L)), .Names
= c("choice", "reward", "RepNum"), row.names = c(NA,


                                                 165L), class =
"data.frame")


CNTRACSID <- 0; subjectFit <- 0;
pLlist <- 0; pRlist <- 0; logLikelihood <- 0; trialProb <- 0;

hmmFunc <- function(delta, temperature){

  pLlist = 1
  pRlist = 1
  block = 0
  for (i in 1:length(dat$choice))
  {
    if (dat$RepNum[i] != block)
    {
      pL = 0.5
      pR = 0.5
      block = dat$RepNum[i]
    }
    # Markov Transitions
    pL <- pL*(1-delta) + pR*delta
    pR <- 1-pL
    # Apply feedback
    #denom <- p(F|L,C) * p(L) + p(F|R,C) * p(R)

    pflc <- ifelse(dat$choice[i] == dat$reward[i], .8, .2)
    pfrc <- 1 - pflc
    denom <- pflc * pL + pfrc * pR

    # What's the new belief given observation
    posteriorL <- pflc * pL/denom
    posteriorR <- 1-posteriorL

    pL <- posteriorL
    pR <- posteriorR

    pL <- (1/(1 + exp(-temperature * (pL-.5))))
    pR <- (1/(1 + exp(-temperature * (pR-.5))))

    pLlist[i] = pL
    pRlist[i] = pR

    if(i > 1){
      if(dat$choice[i] == 1){
        trialProb[i] <- pLlist[i-1]
      } else
      {
        trialProb[i] <- 1-pLlist[i-1]
      }
    }
    else {
      trialProb[1] <- .5
    }

  }
  trialProb2 <- sum(log(trialProb))
  subFit <- exp(trialProb2/length(dat$choice))
  hmmOutput <- list("logLikelihood" = trialProb2, "subjectFit" = subFit,
"probabilities" = pLlist)
  # print(hmmOutput$logLikelihood)
  return(hmmOutput)
}


subjectFits <- 0; subLogLike <- 0; bestTemp <- 0; bestDelta= 0;

min = 0.001; max = .5; inc = 0.001;
deltaList = seq(min, max, inc)
mina = 0; maxa = 5; inca = .01
amList = seq(mina, maxa, inca)
    maxLogValue <- -1000
    for(delta in deltaList){
      for(temp in amList){
        probabilities <- hmmFunc(delta, temp)
        if(probabilities$logLikelihood > maxLogValue){
          pList <- probabilities$probabilities
          maxLogValue <- probabilities$logLikelihood
          subLogLike <- probabilities$logLikelihood
          subjectFits <- probabilities$subjectFit
          bestTemp <- temp
          bestDelta <- delta

        }
      }
    }




-- 
Edward H Patzelt | Clinical Science PhD Student
Psychology | Harvard University
Systems Neuroscience of Psychopathology Laboratory

	[[alternative HTML version deleted]]


From profjcnash at gmail.com  Thu Sep 17 21:31:12 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Thu, 17 Sep 2015 15:31:12 -0400
Subject: [R] Optimization Grid Search Slow
In-Reply-To: <CAB9UfhSaWK2RkMRv_b0=rMt57SOp_nKMvVO42q+hU4gq7koT9w@mail.gmail.com>
References: <CAB9UfhSaWK2RkMRv_b0=rMt57SOp_nKMvVO42q+hU4gq7koT9w@mail.gmail.com>
Message-ID: <55FB1500.5080105@gmail.com>

optimx does nothing to speed up optim or the other component optimizers. 
In fact, it does a lot of checking and extra work to improve reliability 
and add KKT tests that actually slow things down. The purpose of optimx 
is to allow comparison of methods and discovery of improved approaches 
to a problem. Is your function computing correctly?

Assuming you've got a correct function, then spending some time to speed 
up the function (I've found FORTRAN speediest) is likely your best hope.

JN



On 15-09-17 01:55 PM, Patzelt, Edward wrote:
> R Help -
>
> I am trying to use a grid search for a 2 free parameter reinforcement
> learning model and the grid search is incredibly slow. I've used optimx but
> can't seem to get reasonable answers. Is there a way to speed up this grid
> search dramatically?
>
>
> dat <- structure(list(choice = c(0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
>                                   1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
> 0, 1, 0, 1, 0, 1, 0,
>                                   0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
> 0, 0, 1, 0, 0, 1, 1,
>                                   1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
> 0, 1, 0, 0, 0, 0, 1,
>                                   1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
> 1, 0, 0, 0, 0, 0, 0,
>                                   1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
> 1, 0, 0, 0, 0, 0, 1,
>                                   1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
> 1, 0, 0, 1, 1, 0, 0,
>                                   0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
> 0, 0, 1, 0, 0, 0, 0,
>                                   1, 0, 1, 1, 1, 0), reward = c(0L, 0L, 0L,
> 0L, 1L, 1L, 0L, 0L,
>                                                                 1L, 0L, 0L,
> 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 1L,
>                                                                 1L, 0L, 1L,
> 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L,
>                                                                 1L, 0L, 1L,
> 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L,
>                                                                 0L, 0L, 1L,
> 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L,
>                                                                 1L, 1L, 0L,
> 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
>                                                                 0L, 0L, 0L,
> 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L,
>                                                                 1L, 0L, 0L,
> 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L,
>                                                                 0L, 1L, 0L,
> 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
>                                                                 0L, 1L, 0L,
> 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L,
>                                                                 0L, 0L, 1L,
> 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), RepNum = c(1L,
>
>                                                     1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>
>                                                     1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>
>                                                     1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>
>                                                     1L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>
>                                                     2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>
>                                                     2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>
>                                                     2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>
>                                                     2L, 2L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>
>                                                     3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>
>                                                     3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>
>                                                     3L, 3L, 3L, 3L)), .Names
> = c("choice", "reward", "RepNum"), row.names = c(NA,
>
>
>                                                   165L), class =
> "data.frame")
>
>
> CNTRACSID <- 0; subjectFit <- 0;
> pLlist <- 0; pRlist <- 0; logLikelihood <- 0; trialProb <- 0;
>
> hmmFunc <- function(delta, temperature){
>
>    pLlist = 1
>    pRlist = 1
>    block = 0
>    for (i in 1:length(dat$choice))
>    {
>      if (dat$RepNum[i] != block)
>      {
>        pL = 0.5
>        pR = 0.5
>        block = dat$RepNum[i]
>      }
>      # Markov Transitions
>      pL <- pL*(1-delta) + pR*delta
>      pR <- 1-pL
>      # Apply feedback
>      #denom <- p(F|L,C) * p(L) + p(F|R,C) * p(R)
>
>      pflc <- ifelse(dat$choice[i] == dat$reward[i], .8, .2)
>      pfrc <- 1 - pflc
>      denom <- pflc * pL + pfrc * pR
>
>      # What's the new belief given observation
>      posteriorL <- pflc * pL/denom
>      posteriorR <- 1-posteriorL
>
>      pL <- posteriorL
>      pR <- posteriorR
>
>      pL <- (1/(1 + exp(-temperature * (pL-.5))))
>      pR <- (1/(1 + exp(-temperature * (pR-.5))))
>
>      pLlist[i] = pL
>      pRlist[i] = pR
>
>      if(i > 1){
>        if(dat$choice[i] == 1){
>          trialProb[i] <- pLlist[i-1]
>        } else
>        {
>          trialProb[i] <- 1-pLlist[i-1]
>        }
>      }
>      else {
>        trialProb[1] <- .5
>      }
>
>    }
>    trialProb2 <- sum(log(trialProb))
>    subFit <- exp(trialProb2/length(dat$choice))
>    hmmOutput <- list("logLikelihood" = trialProb2, "subjectFit" = subFit,
> "probabilities" = pLlist)
>    # print(hmmOutput$logLikelihood)
>    return(hmmOutput)
> }
>
>
> subjectFits <- 0; subLogLike <- 0; bestTemp <- 0; bestDelta= 0;
>
> min = 0.001; max = .5; inc = 0.001;
> deltaList = seq(min, max, inc)
> mina = 0; maxa = 5; inca = .01
> amList = seq(mina, maxa, inca)
>      maxLogValue <- -1000
>      for(delta in deltaList){
>        for(temp in amList){
>          probabilities <- hmmFunc(delta, temp)
>          if(probabilities$logLikelihood > maxLogValue){
>            pList <- probabilities$probabilities
>            maxLogValue <- probabilities$logLikelihood
>            subLogLike <- probabilities$logLikelihood
>            subjectFits <- probabilities$subjectFit
>            bestTemp <- temp
>            bestDelta <- delta
>
>          }
>        }
>      }
>
>
>
>


From ddalthorp at usgs.gov  Thu Sep 17 21:53:35 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Thu, 17 Sep 2015 12:53:35 -0700 (PDT)
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <1442513945858-4712390.post@n4.nabble.com>
References: <1442513945858-4712390.post@n4.nabble.com>
Message-ID: <1442519615470-4712399.post@n4.nabble.com>

Very nice variety of solutions to create c(1:n, 1:(n-1), 1:(n-2), ... , 1)

#Testing the methods with n=1000 (microbenchmark)
n<-1000

# by far the nicest-looking, easiest to follow, and fastest is Frank
Schwidom's:
# it also requires the minimum amount of memory (as do several of the
others)
# 2.73 milliseconds (1x)
sequence(n:1)  

# not nearly as nice-looking but almost as fast:
# 2.82 milliseconds (1.03x)
do.call(c, lapply(n:1, function(n1) 1:n1)) 

# an improvement on look but 5x slower than do.call is:
# 13.3 milliseconds  (4.9x)
unlist(lapply(n:1, seq))

## the others are uglier and way slower [uses a full (n+1) x (n+1) matrix]
# 60.8 milliseconds (22.3x)
outer( 1:(n+1), 1:(n+1), '-')[ outer( 1:n, 1:(n+1), '>')]

# 71.8 milliseconds (26.3x) [uses a full (n x n) matrix]
junk<-array(1:n,dim=c(n,n))
junk[((lower.tri(t(junk),diag=T)))[n:1,]]

# 421.3 milliseconds (154x)
Reduce( function(x,y){c( 1:y, x)}, 1:n)

# 3200 milliseconds (1170x)
cc<-0; # establish result as numeric
for(i in seq(n,1,-1)){ cc<-c(cc,seq(1,i)); str(cc); }; #generate array
    cc<-cc[2:length(cc)]; #remove the leading 0
} # 

# crashes:
mklist <- function(n) { 
     if (n==1) return(1) else return( c(seq(1,n),mklist(n-1)) ) 
}



--
View this message in context: http://r.789695.n4.nabble.com/c-1-n-1-n-1-1-n-2-1-tp4712390p4712399.html
Sent from the R help mailing list archive at Nabble.com.


From drjimlemon at gmail.com  Thu Sep 17 22:14:42 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 18 Sep 2015 06:14:42 +1000
Subject: [R] HELP IN GRAPHS - slip screen
In-Reply-To: <CAB630FFBi7xFeZ1GtfZNzcmbSGpgiMB8s_xL8iNEFOgGud1f6Q@mail.gmail.com>
References: <F53D3AF7-922F-4352-B554-1E7F5244AB75@gmail.com>
	<CA+8X3fUpj3vfn=V8rrUNnRSMpu2R9D2DB1gfNQnMQ_tYC96iuQ@mail.gmail.com>
	<CAB630FFBi7xFeZ1GtfZNzcmbSGpgiMB8s_xL8iNEFOgGud1f6Q@mail.gmail.com>
Message-ID: <CA+8X3fXF0t8BmuEq6MVeEYnsNjLRid-k+xFFO+AKzWmoy_dDqg@mail.gmail.com>

Hi Rosa,
I don't think the problem is with the split.screen command, for you are
getting the eight plots and the screen at the right as you requested. It
looks like your margins for each plot need adjusting, and I also think you
should have about a 2.2 to 1 width to height ratio in the graphics device.
I can't analyze the rest of the code at the moment, but perhaps tomorrow if
you can't work it out I can provide some suggestions.

Jim


On Fri, Sep 18, 2015 at 1:16 AM, Rosa Oliveira <rosita21 at gmail.com> wrote:

> Dear Jim,
>
> It works, nonetheless, it doesn't slip the screen correctly :(
>
> Do you have any idea?
>
>
> I used the code:
>
>
> #setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
> setwd("~/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
>
>
> library(ggplot2)
> library(reshape)
> library(lattice)
>
>
> # read in what looks like half of the data
>
> bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
> SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
> bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
> SE.alpha1<-read.csv("graphs_SE_alpha1.csv")
>
>
>
> quartz(width=10,height=6)
>
> # do the first split, to get the rightmost screen for the legend
> split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
> # now split the first screen to get your eight screens (numbered 3 to 10)
> for the plots
> split.screen(figs=matrix(c(0,0.25,0.5,1,
>                            0.25,0.5,0.5,1,
>                            0.5,0.75,0.5,1,
>                            0.75,1,0.5,1,
>                            0,0.25,0,0.5,
>                            0.25,0.5,0,0.5,
>                            0.5,0.75,0,0.5,
>                            0.75,1,0,0.5),
>                          ncol=4,byrow=TRUE),screen=1)
>
>
>
> #split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
> #                           0.5,1,0.5,1,#primeira linha segunda coluna
> #                           0,0.5,0,0.5,#segunda linha primeira coluna
> #                           0.5,1,0,0.5),#segunda linha segunda coluna
> #                         ncol=4,byrow=TRUE),screen=1)
>
>
> # this produces seven screens numbered like this:
> #   3   4   5   6
> #                    2
> #   7   8   9   10
> # select the upper left screen
>
>
>
> screen(3)
> par(mar=c(0,3.5,3,0))
> # now the second set
> n250<-bias.alpha1$nsample==250
> matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1,
> .6),main="nsample=250",ylab="", cex.main=1)
> abline(h = 0, col = "gray60")
> mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2,
> cex.main=1)
>
> screen(4)
> par(mar=c(0,0,3,0))
> # now the second set
> n1000<-bias.alpha1$nsample==1000
> matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1,
> .6),main="nsample=1000",ylab="")
> abline(h = 0, col = "gray60")
>
>
>
> screen(5)
> par(mar=c(0,3.5,3,0))
> # now the second set
> par(mar=c(3,3.5,0,0))
> # now the second set
> n250<-bias.alpha2$nsample==250
> matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
> abline(h = 0, col = "gray60")
> mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2,
> cex.main=1.5)
>
> screen(6)
> par(mar=c(3,0,0,0))
> # now the second set
> n1000<-bias.alpha2$nsample==1000
> matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
> abline(h = 0, col = "gray60")
>
>
>
>
> screen(7)
> par(mar=c(0,3.5,3,0))
> # now the second set
> n250<-SE.alpha1$nsample==250
> matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0,
> 1.1),main="nsample=250",ylab="", cex.main=1)
> abline(h = -1, col = "gray60")
> mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2, cex.main=1)
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>
>
> screen(8)
> par(mar=c(0,0,3,0))
> # now the second set
> n1000<-SE.alpha1$nsample==1000
> matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0,
> 1.1),main="nsample=1000",ylab="")
> abline(h = -1, col = "gray60")
>
>
>
>
> screen(9)
> par(mar=c(3,3.5,0,0))
> # now the second set
> n250<-SE.alpha2$nsample==250
> matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
> abline(h = -.5, col = "gray60")
> mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2,
> cex.main=1.5)
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>
>
> screen(10)
> par(mar=c(3,0,0,0))
> # now the second set
> n1000<-SE.alpha2$nsample==1000
> matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
> abline(h = -.5, col = "gray60")
> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
>
>
>
> screen(2)
> par(mar=c(0,0,0,0))
> # plot an empty plot to get the coordinates
> plot(0:1,0:1,type="n",axes=FALSE)
> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n",
> lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
>
>
> close.screen(all=TRUE)
>
>
> and I attach the output graph.
>
>
>
> Best,
> RO
>
> Atenciosamente,
> Rosa Oliveira
>
> _________________________________
>
> Antes de imprimir este e-mail pense bem se tem mesmo que o fazer.
> H? cada vez menos ?rvores.
> N?o imprima, pense na sua responsabilidade e compromisso com o MEIO
> AMBIENTE!
>
> <http://pt.dreamstime.com/cora-ccedil-atildeo-criado-das-folhas-de-aacutervores-diferentes-thumb12275776.jpg>
>
> <http://pt.dreamstime.com/cora-ccedil-atildeo-criado-das-folhas-de-aacutervores-diferentes-thumb12275776.jpg>
>
> 2015-09-17 12:18 GMT+01:00 Jim Lemon <drjimlemon at gmail.com>:
>
>> Hi Rosa,
>> Try this:
>>
>> # do the first split, to get the rightmost screen for the legend
>> split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
>> # now split the first screen to get your eight screens (numbered 3 to 10)
>> for the plots
>> split.screen(figs=matrix(c(0,0.25,0.5,1,
>>                            0.25,0.5,0.5,1,
>>                            0.5,0.75,0.5,1,
>>                            0.75,1,0.5,1,
>>                            0,0.25,0,0.5,
>>                            0.25,0.5,0,0.5,
>>                            0.5,0.75,0,0.5,
>>                            0.75,1,0,0.5),
>>                          ncol=4,byrow=TRUE),screen=1)
>>
>> Jim
>>
>>
>> On Thu, Sep 17, 2015 at 2:45 AM, Rosa Oliveira <rosita21 at gmail.com>
>> wrote:
>>
>>> Dear all,
>>>
>>> I?m trying to do a graph,
>>>
>>> 3 rows, 5 columns, with the design:
>>> #   3   4   5   6
>>> #                    2
>>> #   7   8   9   10
>>>
>>> I had a code for 3 rows, 3 columns, with the design::
>>> #   3   4
>>> #            2
>>> #   7   8
>>>  and I tried to modify it, but I had no success :(
>>>
>>> I suppose the problem is in the slip.screen code (red part of the code).
>>>
>>> I attach my code, can anyone please help me?
>>>
>>>
>>> Best,
>>> RO
>>>
>>>
>>> setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
>>>
>>> library(ggplot2)
>>> library(reshape)
>>> library(lattice)
>>>
>>>
>>> # read in what looks like half of the data
>>>
>>> bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
>>> SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
>>> bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
>>> SE.alpha1<-read.csv("graphs_SE_alpha1.csv")
>>>
>>>
>>>
>>> quartz(width=10,height=6)
>>> # do the first split, to get the rightmost screen for the legend
>>> split.screen(figs=matrix(c(0,0.8,0,1,0.8,1,0,1),nrow=2,byrow=TRUE))
>>> # now split the first screen to get your six screens for the plots
>>>
>>>
>>>
>>> split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
>>>                            0.5,1,0.5,1,#primeira linha segunda coluna
>>>                            0,0.5,0,0.5,#segunda linha primeira coluna
>>>                            0.5,1,0,0.5),#segunda linha segunda coluna
>>>                          ncol=4,byrow=TRUE),screen=1)
>>>
>>>
>>> # this produces seven screens numbered like this:
>>> #   3   4   5   6
>>> #                    2
>>> #   7   8   9   10
>>> # select the upper left screen
>>>
>>>
>>>
>>> screen(3)
>>> par(mar=c(0,3.5,3,0))
>>> # now the second set
>>> n250<-bias.alpha1$nsample==250
>>> matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
>>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1,
>>> .6),main="nsample=250",ylab="", cex.main=1)
>>> abline(h = 0, col = "gray60")
>>> mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2,
>>> cex.main=1)
>>>
>>> screen(4)
>>> par(mar=c(0,0,3,0))
>>> # now the second set
>>> n1000<-bias.alpha1$nsample==1000
>>> matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
>>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1,
>>> .6),main="nsample=1000",ylab="")
>>> abline(h = 0, col = "gray60")
>>>
>>>
>>>
>>> screen(5)
>>> par(mar=c(0,3.5,3,0))
>>> # now the second set
>>> par(mar=c(3,3.5,0,0))
>>> # now the second set
>>> n250<-bias.alpha2$nsample==250
>>> matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
>>>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
>>> abline(h = 0, col = "gray60")
>>> mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2,
>>> cex.main=1.5)
>>>
>>> screen(6)
>>> par(mar=c(3,0,0,0))
>>> # now the second set
>>> n1000<-bias.alpha2$nsample==1000
>>> matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
>>>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
>>> abline(h = 0, col = "gray60")
>>>
>>>
>>>
>>>
>>> screen(7)
>>> par(mar=c(0,3.5,3,0))
>>> # now the second set
>>> n250<-SE.alpha1$nsample==250
>>> matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
>>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0,
>>> 1.1),main="nsample=250",ylab="", cex.main=1)
>>> abline(h = -1, col = "gray60")
>>> mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2,
>>> cex.main=1)
>>> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>>>
>>>
>>> screen(8)
>>> par(mar=c(0,0,3,0))
>>> # now the second set
>>> n1000<-SE.alpha1$nsample==1000
>>> matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
>>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0,
>>> 1.1),main="nsample=1000",ylab="")
>>> abline(h = -1, col = "gray60")
>>>
>>>
>>>
>>>
>>> screen(9)
>>> par(mar=c(3,3.5,0,0))
>>> # now the second set
>>> n250<-SE.alpha2$nsample==250
>>> matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
>>>         type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
>>> abline(h = -.5, col = "gray60")
>>> mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2,
>>> cex.main=1.5)
>>> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>>>
>>>
>>> screen(10)
>>> par(mar=c(3,0,0,0))
>>> # now the second set
>>> n1000<-SE.alpha2$nsample==1000
>>> matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
>>>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
>>> abline(h = -.5, col = "gray60")
>>> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
>>>
>>>
>>>
>>> screen(2)
>>> par(mar=c(0,0,0,0))
>>> # plot an empty plot to get the coordinates
>>> plot(0:1,0:1,type="n",axes=FALSE)
>>> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n",
>>> lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
>>>
>>>
>>> close.screen(all=TRUE)
>>>
>>>
>>>
>>>
>>> Best,
>>> RO
>>>
>>>
>>> Atenciosamente,
>>> Rosa Oliveira
>>>
>>> --
>>>
>>> ____________________________________________________________________________
>>>
>>>
>>> Rosa Celeste dos Santos Oliveira,
>>>
>>> E-mail: rosita21 at gmail.com
>>> Tlm: +351 939355143
>>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>>>
>>> ____________________________________________________________________________
>>> "Many admire, few know"
>>> Hippocrates
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Sep 17 22:15:08 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 17 Sep 2015 13:15:08 -0700
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <1442519615470-4712399.post@n4.nabble.com>
References: <1442513945858-4712390.post@n4.nabble.com>
	<1442519615470-4712399.post@n4.nabble.com>
Message-ID: <CAF8bMcYZi_gEzLHzLw3GRkOi3AbNHdEYVpsH3s+w6P4pjqL+5Q@mail.gmail.com>

If you are interested in speed for long input vectors try the following,
which should give the same result as sequence().
mySequence <-function (nvec)
{
    nvec <- as.integer(nvec)
    seq_len(sum(nvec)) - rep(cumsum(c(0L, nvec[-length(nvec)])),
        nvec)
}

E.g.,
> n <- rpois(1e6, 3)
> system.time(mySequence(n))
   user  system elapsed
   0.07    0.00    0.07
> system.time(sequence(n))
   user  system elapsed
   0.88    0.00    0.87
> identical(mySequence(n), sequence(n))
[1] TRUE


Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Sep 17, 2015 at 12:53 PM, Dan D <ddalthorp at usgs.gov> wrote:
> Very nice variety of solutions to create c(1:n, 1:(n-1), 1:(n-2), ... , 1)
>
> #Testing the methods with n=1000 (microbenchmark)
> n<-1000
>
> # by far the nicest-looking, easiest to follow, and fastest is Frank
> Schwidom's:
> # it also requires the minimum amount of memory (as do several of the
> others)
> # 2.73 milliseconds (1x)
> sequence(n:1)
>
> # not nearly as nice-looking but almost as fast:
> # 2.82 milliseconds (1.03x)
> do.call(c, lapply(n:1, function(n1) 1:n1))
>
> # an improvement on look but 5x slower than do.call is:
> # 13.3 milliseconds  (4.9x)
> unlist(lapply(n:1, seq))
>
> ## the others are uglier and way slower [uses a full (n+1) x (n+1) matrix]
> # 60.8 milliseconds (22.3x)
> outer( 1:(n+1), 1:(n+1), '-')[ outer( 1:n, 1:(n+1), '>')]
>
> # 71.8 milliseconds (26.3x) [uses a full (n x n) matrix]
> junk<-array(1:n,dim=c(n,n))
> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
>
> # 421.3 milliseconds (154x)
> Reduce( function(x,y){c( 1:y, x)}, 1:n)
>
> # 3200 milliseconds (1170x)
> cc<-0; # establish result as numeric
> for(i in seq(n,1,-1)){ cc<-c(cc,seq(1,i)); str(cc); }; #generate array
>     cc<-cc[2:length(cc)]; #remove the leading 0
> } #
>
> # crashes:
> mklist <- function(n) {
>      if (n==1) return(1) else return( c(seq(1,n),mklist(n-1)) )
> }
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/c-1-n-1-n-1-1-n-2-1-tp4712390p4712399.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Thu Sep 17 22:19:01 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 17 Sep 2015 13:19:01 -0700
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <EF76ED63-CF8A-486E-AC2C-1C61C637F753@comcast.net>
References: <1442513945858-4712390.post@n4.nabble.com>
	<CA+hbrhUVZ6LEJc_6XiiffgYuRo_4RRjUE2US=pc0diicKcnNjA@mail.gmail.com>
	<alpine.DEB.2.11.1509172039510.15300@paninaro.uibk.ac.at>
	<EF76ED63-CF8A-486E-AC2C-1C61C637F753@comcast.net>
Message-ID: <CAGxFJbRysapBn_PDV6Mtz0_H+riXBgsv+KhA_0YfgCaMh6yqtQ@mail.gmail.com>

Note that:


>> Also not sure about efficiency but somewhat shorter...
>> unlist(lapply(5:1, seq))
>>
>>> Peter
>>>

is almost exactly sequence(5:1)

which is

unlist(lapply(5:1,seq_len))

which is "must preferred". See ?seq for details

Cheers,
Bert



>>> On Thu, Sep 17, 2015 at 11:19 AM, Dan D <ddalthorp at usgs.gov> wrote:
>>>> Can anyone think of a slick way to create an array that looks like c(1:n,
>>>> 1:(n-1), 1:(n-2), ... , 1)?
>>>>
>>>> The following works, but it's inefficient and a little hard to follow:
>>>> n<-5
>>>> junk<-array(1:n,dim=c(n,n))
>>>> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
>>>>
>>>> Any help would be greatly appreciated!
>>>>
>>>> -Dan
>>>>
>>>>
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mckellercran at gmail.com  Thu Sep 17 22:36:50 2015
From: mckellercran at gmail.com (Matthew Keller)
Date: Thu, 17 Sep 2015 14:36:50 -0600
Subject: [R] fast way to create composite matrix based on mixed indices?
Message-ID: <CAB7vCMSZx=Vp1A+UHvOsrOb9DfHkwDTRq0qdSOoBSidnxPKvPQ@mail.gmail.com>

HI all,

Sorry for the title here but I find this difficult to describe succinctly.
Here's the problem.

I want to create a new matrix where each row is a composite of an old
matrix, but where the row & column indexes of the old matrix change for
different parts of the new matrix. For example, the second row of new
matrix (which has , e.g., 10 columns) might be columns 1 to 3 of row 2 of
old matrix, columns 4 to 8 of row 1 of old matrix, and columns 9 to 10 of
row 3 of old matrix.

Here's an example in code:

#The old matrix
(old.mat <- matrix(1:30,nrow=3,byrow=TRUE))

#matrix of indices to create the new matrix from the old one.
#The 1st column gives the row number of the new matrix
#the 2nd gives the row of the old matrix that we're going to copy into the
new matrix
#the 3rd gives the starting column of the old matrix for the row in col 2
#the 4th gives the end column of the old matrix for the row in col 2
index <- matrix(c(1,1,1,4,
                  1,3,5,10,
                  2,2,1,3,
                  2,1,4,8,
                  2,3,9,10),
                nrow=5,byrow=TRUE,

dimnames=list(NULL,c('new.mat.row','old.mat.row','old.mat.col.start','old.mat.col.end')))

I will be given old.mat and index and want to create new.mat from them.

I want to create a new.matrix of two rows that looks like this:
new.mat <- matrix(c(1:4,25:30,11:13,4:8,29:30),byrow=TRUE,nrow=2)

So here, the first row of new.mat is columns 1 to 4 of row 1 of the old.mat
and columns 5 to 10 of row 3 of old.mat.

new.mat and old.mat will always have the same number of columns but the
number of rows could differ.

I could accomplish this in a loop, but the real problem is quite large
(new.mat might have 1e8 elements), and so a for loop would be prohibitively
slow.

I may resort to unix tools and use a shell script, but wanted to first see
if this is doable in R in a fast way.

Thanks in advance!

Matt


-- 
Matthew C Keller
Asst. Professor of Psychology
University of Colorado at Boulder
www.matthewckeller.com

	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Thu Sep 17 23:11:19 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Thu, 17 Sep 2015 17:11:19 -0400
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
Message-ID: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>

(x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1", "q13_11"))

# Which strings start with "q10" or "q12? - WORKS
x[grep("^q10|q12", x)]

# Which strings end with "1"? - WORKS
x[grep("1$", x)]

# Which strings end with "_1"? - WORKS
x[grep("\\_1$", x)]

# Which strings start with "q10" AND contain a "1"? - WORKS
x[grep("^q10.+1", x)]

# Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK
x[grep("^q10.+\\_1$", x)]

# Which strings start with "q10" or "q12 AND end with "_1"? - WORKS INCORRECTLY
x[grep("^q10|q12.+\\_1$", x)]

Thank you!
Dimitri Liakhovitski


From toth.denes at ttk.mta.hu  Thu Sep 17 23:22:56 2015
From: toth.denes at ttk.mta.hu (=?ISO-8859-1?Q?D=E9nes_T=F3th?=)
Date: Thu, 17 Sep 2015 23:22:56 +0200
Subject: [R] fast way to create composite matrix based on mixed indices?
In-Reply-To: <CAB7vCMSZx=Vp1A+UHvOsrOb9DfHkwDTRq0qdSOoBSidnxPKvPQ@mail.gmail.com>
References: <CAB7vCMSZx=Vp1A+UHvOsrOb9DfHkwDTRq0qdSOoBSidnxPKvPQ@mail.gmail.com>
Message-ID: <55FB2F30.7030107@ttk.mta.hu>

Hi Matt,

you could use matrix indexing. Here is a possible solution, which could 
be optimized further (probably).

# The old matrix
(old.mat <- matrix(1:30,nrow=3,byrow=TRUE))
# matrix of indices
index <- matrix(c(1,1,1,4,
                   1,3,5,10,
                   2,2,1,3,
                   2,1,4,8,
                   2,3,9,10),
                 nrow=5,byrow=TRUE,
                 dimnames=list(NULL,
                               c('new.mat.row','old.mat.row',
                                 'old.mat.col.start','old.mat.col.end')))
# expected result
new.mat <- matrix(c(1:4,25:30,11:13,4:8,29:30),
                   byrow=TRUE, nrow=2)
#
# column indices
ind <- mapply(seq, index[, 3], index[,4],
               SIMPLIFY = FALSE, USE.NAMES = FALSE)
ind_len <- vapply(ind, length, integer(1))
ind <- unlist(ind)

#
# old indices
old.ind <- cbind(rep(index[,2], ind_len), ind)
#
# new indices
new.ind <- cbind(rep(index[,1], ind_len), ind)
#
# create the new matrix
result <- matrix(NA_integer_, max(index[,1]), max(index[,4]))
#
# fill the new matrix
result[new.ind] <- old.mat[old.ind]
#
# check the results
identical(result, new.mat)


HTH,
   Denes




On 09/17/2015 10:36 PM, Matthew Keller wrote:
> HI all,
>
> Sorry for the title here but I find this difficult to describe succinctly.
> Here's the problem.
>
> I want to create a new matrix where each row is a composite of an old
> matrix, but where the row & column indexes of the old matrix change for
> different parts of the new matrix. For example, the second row of new
> matrix (which has , e.g., 10 columns) might be columns 1 to 3 of row 2 of
> old matrix, columns 4 to 8 of row 1 of old matrix, and columns 9 to 10 of
> row 3 of old matrix.
>
> Here's an example in code:
>
> #The old matrix
> (old.mat <- matrix(1:30,nrow=3,byrow=TRUE))
>
> #matrix of indices to create the new matrix from the old one.
> #The 1st column gives the row number of the new matrix
> #the 2nd gives the row of the old matrix that we're going to copy into the
> new matrix
> #the 3rd gives the starting column of the old matrix for the row in col 2
> #the 4th gives the end column of the old matrix for the row in col 2
> index <- matrix(c(1,1,1,4,
>                    1,3,5,10,
>                    2,2,1,3,
>                    2,1,4,8,
>                    2,3,9,10),
>                  nrow=5,byrow=TRUE,
>
> dimnames=list(NULL,c('new.mat.row','old.mat.row','old.mat.col.start','old.mat.col.end')))
>
> I will be given old.mat and index and want to create new.mat from them.
>
> I want to create a new.matrix of two rows that looks like this:
> new.mat <- matrix(c(1:4,25:30,11:13,4:8,29:30),byrow=TRUE,nrow=2)
>
> So here, the first row of new.mat is columns 1 to 4 of row 1 of the old.mat
> and columns 5 to 10 of row 3 of old.mat.
>
> new.mat and old.mat will always have the same number of columns but the
> number of rows could differ.
>
> I could accomplish this in a loop, but the real problem is quite large
> (new.mat might have 1e8 elements), and so a for loop would be prohibitively
> slow.
>
> I may resort to unix tools and use a shell script, but wanted to first see
> if this is doable in R in a fast way.
>
> Thanks in advance!
>
> Matt
>
>


From alfadiallo at mac.com  Thu Sep 17 22:41:46 2015
From: alfadiallo at mac.com (Alfa Diallo)
Date: Thu, 17 Sep 2015 16:41:46 -0400
Subject: [R] best data storage format?
Message-ID: <E4422A2C-0480-4485-A256-F0C8BDE0C4AB@mac.com>

Hello -

I?m working on dataset that will eventually be used in an xyz-plot.

I?m having trouble figuring out the best way to store the data (see an attached .csv sheet exported from Excel). Some information on the data:

- Columns B - F are labels that describe the z data points
- Rows above x and y data pairs show the corresponding labels for the data point.

I want to use R to visualize the data and appreciate any feedback on the best mechanism to store/organize the info. I?m new to R and want to start the data assembly on the right foot - thanks for the help and advice!

Alfa


From govokai at gmail.com  Thu Sep 17 23:26:39 2015
From: govokai at gmail.com (Kai Mx)
Date: Thu, 17 Sep 2015 23:26:39 +0200
Subject: [R] aggregate counting variable factors
In-Reply-To: <55FAF107.1060806@sapo.pt>
References: <CAHOWgNWsu++ShKUrLg+ftMs+81AErNBoyg4xnz4-5sRG-H7YwA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C40124@SRVEXCHMBX.precheza.cz>
	<20150917160328.GB2770@debian64> <55FAF107.1060806@sapo.pt>
Message-ID: <CAHOWgNWk2bBC5L-JEivhSo7vhiHN60xx+HvqZ=qYANRLwG56wA@mail.gmail.com>

Thanks everybody!

On Thu, Sep 17, 2015 at 6:57 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> In package reshape2
>
> Hope this helps,
>
> Rui Barradas
>
>
> Em 17-09-2015 17:03, Frank Schwidom escreveu:
>
>> Hi
>>
>> where can i find 'melt' and 'dcast' ?
>>
>> Regards
>>
>>
>> On Thu, Sep 17, 2015 at 08:22:10AM +0000, PIKAL Petr wrote:
>>
>>> Hi
>>>
>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Kai Mx
>>>> Sent: Wednesday, September 16, 2015 10:43 PM
>>>> To: r-help mailing list
>>>> Subject: [R] aggregate counting variable factors
>>>>
>>>> Hi everybody,
>>>>
>>>> >From a questionnaire, I have a dataset  like this one with some 40
>>>> items:
>>>>
>>>> df1 <- data.frame(subject=c('user1','user2', 'user3', 'user4'),
>>>> item1=c(0,1,2,5), item2=c(1,2,1,2), item3=c(2,3,4,0), item4=c(0,3,3,2),
>>>> item5=c(5,5,5,5))
>>>>
>>>> Users can choose an answer from 0 to 5 for each item.
>>>>
>>>> Now I want to reshape the dataset to have the items in rows and the
>>>> count
>>>> of each of the result factors in columns:
>>>>
>>>> result <- data.frame (item=c("item1", "item2", "item3", "item4",
>>>> "item5"),
>>>> result0=c(1,0,1,1,0), result1=c(1,2,0,0,0), result2=c(1,2,1,1,0),
>>>> result3=c(0,0,1,2,0), result4=c(0,0,1,0,0), result5=c(1,0,0,0,4))
>>>>
>>>> I have been fiddling around with melt/plyr, but haven't been able to
>>>> figure
>>>> it out. What's the most elegant way to do this (preferably without
>>>> typing
>>>> in all the item names).
>>>>
>>>
>>> Perhaps,
>>>
>>> m<-melt(df1)
>>> m$value<-paste("res",m$value, sep="")
>>> dcast(m, variable~value)
>>> Aggregation function missing: defaulting to length
>>>    variable res0 res1 res2 res3 res4 res5
>>> 1    item1    1    1    1    0    0    1
>>> 2    item2    0    2    2    0    0    0
>>> 3    item3    1    0    1    1    1    0
>>> 4    item4    1    0    1    2    0    0
>>> 5    item5    0    0    0    0    0    4
>>>
>>> Cheers
>>> Petr
>>>
>>>
>>>
>>>> Thanks so much!
>>>>
>>>> Best,
>>>>
>>>> Kai
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>> guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> ________________________________
>>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>>> ur?eny pouze jeho adres?t?m.
>>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
>>> vyma?te ze sv?ho syst?mu.
>>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
>>> modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>>>
>>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
>>> strany p??jemce s dodatkem ?i odchylkou.
>>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
>>> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
>>> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
>>> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>>
>>> This e-mail and any documents attached to it may be confidential and are
>>> intended only for its intended recipients.
>>> If you received this e-mail by mistake, please immediately inform its
>>> sender. Delete the contents of this e-mail with all attachments and its
>>> copies from your system.
>>> If you are not the intended recipient of this e-mail, you are not
>>> authorized to use, disseminate, copy or disclose this e-mail in any manner.
>>> The sender of this e-mail shall not be liable for any possible damage
>>> caused by modifications of the e-mail or by delay with transfer of the
>>> email.
>>>
>>> In case that this e-mail forms part of business dealings:
>>> - the sender reserves the right to end negotiations about entering into
>>> a contract in any time, for any reason, and without stating any reasoning.
>>> - if the e-mail contains an offer, the recipient is entitled to
>>> immediately accept such offer; The sender of this e-mail (offer) excludes
>>> any acceptance of the offer on the part of the recipient containing any
>>> amendment or variation.
>>> - the sender insists on that the respective contract is concluded only
>>> upon an express mutual agreement on all its aspects.
>>> - the sender of this e-mail informs that he/she is not authorized to
>>> enter into any contracts on behalf of the company except for cases in which
>>> he/she is expressly authorized to do so in writing, and such authorization
>>> or power of attorney is submitted to the recipient or the person
>>> represented by the recipient, or the existence of such authorization is
>>> known to the recipient of the person represented by the recipient.
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Thu Sep 17 23:42:24 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 17 Sep 2015 17:42:24 -0400
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
In-Reply-To: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
References: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
Message-ID: <55FB33C0.5060001@gmail.com>

On 17/09/2015 5:11 PM, Dimitri Liakhovitski wrote:
> (x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1", "q13_11"))
> 
> # Which strings start with "q10" or "q12? - WORKS
> x[grep("^q10|q12", x)]
> 
> # Which strings end with "1"? - WORKS
> x[grep("1$", x)]
> 
> # Which strings end with "_1"? - WORKS
> x[grep("\\_1$", x)]
> 
> # Which strings start with "q10" AND contain a "1"? - WORKS
> x[grep("^q10.+1", x)]
> 
> # Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK
> x[grep("^q10.+\\_1$", x)]

Your verbal description doesn't match your regexp, and you didn't show
us your output, so how can we tell whether this is you not understanding
regular expressions, or an actual problem?

When I try this example, I get

character(0)

which is the correct outcome, given the input string.

Duncan Murdoch

> 
> # Which strings start with "q10" or "q12 AND end with "_1"? - WORKS INCORRECTLY
> x[grep("^q10|q12.+\\_1$", x)]
> 
> Thank you!
> Dimitri Liakhovitski
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From john.vanaarde at gmail.com  Thu Sep 17 23:24:42 2015
From: john.vanaarde at gmail.com (John van Aarde)
Date: Fri, 18 Sep 2015 07:24:42 +1000
Subject: [R] Rcpp RNG - runif() override
Message-ID: <DD41738F-89AD-4701-B959-828D23636F30@gmail.com>

Dear R users,

I'm attempting to override the base function runif() with a function, custom_runif(), that I've written and tested in Rcpp - with the aim of using my own RNG in DEoptim.

I've attempted setting the name in a namespace changing locking to no avail, e.g.

assignInNamespace(runif, custom_runif, ns='DEoptim") 

and would appreciate any guidance on the best approach to this.

Many thanks,
John


From dimitri.liakhovitski at gmail.com  Thu Sep 17 23:46:12 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Thu, 17 Sep 2015 17:46:12 -0400
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
In-Reply-To: <55FB33C0.5060001@gmail.com>
References: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
	<55FB33C0.5060001@gmail.com>
Message-ID: <CAN2xGJaSWityt20Ogg4zjmdZBrHdO7fYDEheFUW5Bo2-r4z4Og@mail.gmail.com>

Duncan,
Of course my verbal descriptions and my code don't match my regexp -
otherwise I wouldn't be asking the question, would I?
Please assume my verbal descriptions are correctly describing what I want.
Thank you!

On Thu, Sep 17, 2015 at 5:42 PM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 17/09/2015 5:11 PM, Dimitri Liakhovitski wrote:
>> (x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1", "q13_11"))
>>
>> # Which strings start with "q10" or "q12? - WORKS
>> x[grep("^q10|q12", x)]
>>
>> # Which strings end with "1"? - WORKS
>> x[grep("1$", x)]
>>
>> # Which strings end with "_1"? - WORKS
>> x[grep("\\_1$", x)]
>>
>> # Which strings start with "q10" AND contain a "1"? - WORKS
>> x[grep("^q10.+1", x)]
>>
>> # Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK
>> x[grep("^q10.+\\_1$", x)]
>
> Your verbal description doesn't match your regexp, and you didn't show
> us your output, so how can we tell whether this is you not understanding
> regular expressions, or an actual problem?
>
> When I try this example, I get
>
> character(0)
>
> which is the correct outcome, given the input string.
>
> Duncan Murdoch
>
>>
>> # Which strings start with "q10" or "q12 AND end with "_1"? - WORKS INCORRECTLY
>> x[grep("^q10|q12.+\\_1$", x)]
>>
>> Thank you!
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>



-- 
Dimitri Liakhovitski


From dimitri.liakhovitski at gmail.com  Thu Sep 17 23:49:53 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Thu, 17 Sep 2015 17:49:53 -0400
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
In-Reply-To: <CAN2xGJaSWityt20Ogg4zjmdZBrHdO7fYDEheFUW5Bo2-r4z4Og@mail.gmail.com>
References: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
	<55FB33C0.5060001@gmail.com>
	<CAN2xGJaSWityt20Ogg4zjmdZBrHdO7fYDEheFUW5Bo2-r4z4Og@mail.gmail.com>
Message-ID: <CAN2xGJaJzfSPpvU+FeOBcmhDKU-beM3vHpy24o8LN0WhhoU0gg@mail.gmail.com>

For the last one, looks like this one works:
x[grep("^(q10|q12).*\\_1$", x)]

On Thu, Sep 17, 2015 at 5:46 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Duncan,
> Of course my verbal descriptions and my code don't match my regexp -
> otherwise I wouldn't be asking the question, would I?
> Please assume my verbal descriptions are correctly describing what I want.
> Thank you!
>
> On Thu, Sep 17, 2015 at 5:42 PM, Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
>> On 17/09/2015 5:11 PM, Dimitri Liakhovitski wrote:
>>> (x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1", "q13_11"))
>>>
>>> # Which strings start with "q10" or "q12? - WORKS
>>> x[grep("^q10|q12", x)]
>>>
>>> # Which strings end with "1"? - WORKS
>>> x[grep("1$", x)]
>>>
>>> # Which strings end with "_1"? - WORKS
>>> x[grep("\\_1$", x)]
>>>
>>> # Which strings start with "q10" AND contain a "1"? - WORKS
>>> x[grep("^q10.+1", x)]
>>>
>>> # Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK
>>> x[grep("^q10.+\\_1$", x)]
>>
>> Your verbal description doesn't match your regexp, and you didn't show
>> us your output, so how can we tell whether this is you not understanding
>> regular expressions, or an actual problem?
>>
>> When I try this example, I get
>>
>> character(0)
>>
>> which is the correct outcome, given the input string.
>>
>> Duncan Murdoch
>>
>>>
>>> # Which strings start with "q10" or "q12 AND end with "_1"? - WORKS INCORRECTLY
>>> x[grep("^q10|q12.+\\_1$", x)]
>>>
>>> Thank you!
>>> Dimitri Liakhovitski
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
>
>
> --
> Dimitri Liakhovitski



-- 
Dimitri Liakhovitski


From dwinsemius at comcast.net  Fri Sep 18 00:08:59 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 17 Sep 2015 15:08:59 -0700
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
In-Reply-To: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
References: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
Message-ID: <63DACE52-F513-4725-B7A4-FD17751057D2@comcast.net>


On Sep 17, 2015, at 2:11 PM, Dimitri Liakhovitski wrote:

> (x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1", "q13_11"))
> 
> # Which strings start with "q10" or "q12? - WORKS
> x[grep("^q10|q12", x)]
> 
> # Which strings end with "1"? - WORKS
> x[grep("1$", x)]
> 
> # Which strings end with "_1"? - WORKS
> x[grep("\\_1$", x)]
> 
> # Which strings start with "q10" AND contain a "1"? - WORKS
> x[grep("^q10.+1", x)]
> 
> # Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK
> x[grep("^q10.+\\_1$", x)]
> 
> # Which strings start with "q10" or "q12 AND end with "_1"? - WORKS INCORRECTLY
> x[grep("^q10|q12.+\\_1$", x)]
> 

You solved the last one with grouping around the OR-operator. The penultimate error needs only a very slight change to find a single instance in x:

x[grep("^q10.*_1$", x)]
[1] "q10_1"

Using "+" requires at leas one character between the two end, whereas you use the "*" operator to allow for the possibility of no intervening characters.

-- 
David Winsemius
Alameda, CA, USA


From farnoosh_81 at yahoo.com  Fri Sep 18 00:36:05 2015
From: farnoosh_81 at yahoo.com (Farnoosh Sheikhi)
Date: Thu, 17 Sep 2015 22:36:05 +0000 (UTC)
Subject: [R] Distance in miles btw Zipcodes
Message-ID: <686300151.1256817.1442529365055.JavaMail.yahoo@mail.yahoo.com>

?Hello,
I'm trying to get the distances between two Zipcode variables, but for some reason I get this error:
"matching was not perfect, returning what was found.Error: no such index at level 1"
Here is my code:

library(ggmap)mapdist(data$Zip.A, data$Zip.B, mode = "driving")
The Zip codes are all in 5 digits format.I really appreciate any help or suggestion.Thanks.

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Fri Sep 18 01:27:50 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 17 Sep 2015 19:27:50 -0400
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
In-Reply-To: <CAN2xGJaSWityt20Ogg4zjmdZBrHdO7fYDEheFUW5Bo2-r4z4Og@mail.gmail.com>
References: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
	<55FB33C0.5060001@gmail.com>
	<CAN2xGJaSWityt20Ogg4zjmdZBrHdO7fYDEheFUW5Bo2-r4z4Og@mail.gmail.com>
Message-ID: <55FB4C76.8040401@gmail.com>

On 17/09/2015 5:46 PM, Dimitri Liakhovitski wrote:
> Duncan,
> Of course my verbal descriptions and my code don't match my regexp -
> otherwise I wouldn't be asking the question, would I?
> Please assume my verbal descriptions are correctly describing what I want.

Sorry, I interpreted "works" and "does not work" to be saying that R's
regexp matching was working or not.

I think you've got your answers from others now...

Duncan Murdoch

> Thank you!
> 
> On Thu, Sep 17, 2015 at 5:42 PM, Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
>> On 17/09/2015 5:11 PM, Dimitri Liakhovitski wrote:
>>> (x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1", "q13_11"))
>>>
>>> # Which strings start with "q10" or "q12? - WORKS
>>> x[grep("^q10|q12", x)]
>>>
>>> # Which strings end with "1"? - WORKS
>>> x[grep("1$", x)]
>>>
>>> # Which strings end with "_1"? - WORKS
>>> x[grep("\\_1$", x)]
>>>
>>> # Which strings start with "q10" AND contain a "1"? - WORKS
>>> x[grep("^q10.+1", x)]
>>>
>>> # Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK
>>> x[grep("^q10.+\\_1$", x)]
>>
>> Your verbal description doesn't match your regexp, and you didn't show
>> us your output, so how can we tell whether this is you not understanding
>> regular expressions, or an actual problem?
>>
>> When I try this example, I get
>>
>> character(0)
>>
>> which is the correct outcome, given the input string.
>>
>> Duncan Murdoch
>>
>>>
>>> # Which strings start with "q10" or "q12 AND end with "_1"? - WORKS INCORRECTLY
>>> x[grep("^q10|q12.+\\_1$", x)]
>>>
>>> Thank you!
>>> Dimitri Liakhovitski
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
> 
> 
>


From jrkrideau at inbox.com  Fri Sep 18 01:41:43 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 17 Sep 2015 15:41:43 -0800
Subject: [R] best data storage format?
In-Reply-To: <E4422A2C-0480-4485-A256-F0C8BDE0C4AB@mac.com>
Message-ID: <F767CCEF5EA.00000058jrkrideau@inbox.com>

No data. See dput() (?dput) as the preferred way to send data

John Kane
Kingston ON Canada


> -----Original Message-----
> From: alfadiallo at mac.com
> Sent: Thu, 17 Sep 2015 16:41:46 -0400
> To: r-help at r-project.org
> Subject: [R] best data storage format?
> 
> Hello -
> 
> I?m working on dataset that will eventually be used in an xyz-plot.
> 
> I?m having trouble figuring out the best way to store the data (see an
> attached .csv sheet exported from Excel). Some information on the data:
> 
> - Columns B - F are labels that describe the z data points
> - Rows above x and y data pairs show the corresponding labels for the
> data point.
> 
> I want to use R to visualize the data and appreciate any feedback on the
> best mechanism to store/organize the info. I?m new to R and want to start
> the data assembly on the right foot - thanks for the help and advice!
> 
> Alfa
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From dwinsemius at comcast.net  Fri Sep 18 02:08:20 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 17 Sep 2015 17:08:20 -0700
Subject: [R] Distance in miles btw Zipcodes
In-Reply-To: <686300151.1256817.1442529365055.JavaMail.yahoo@mail.yahoo.com>
References: <686300151.1256817.1442529365055.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1582FB47-FF9F-4E37-AD36-1F307F614B5B@comcast.net>


On Sep 17, 2015, at 3:36 PM, Farnoosh Sheikhi via R-help wrote:

>  Hello,
> I'm trying to get the distances between two Zipcode variables, but for some reason I get this error:
> "matching was not perfect, returning what was found.Error: no such index at level 1"
> Here is my code:
> 
> library(ggmap)mapdist(data$Zip.A, data$Zip.B, mode = "driving")

Yes, for "some reason". But how should we be able to guess the reason when no data is available?


> The Zip codes are all in 5 digits format.I really appreciate any help or suggestion.Thanks.
> 
> 	[[alternative HTML version deleted]]
> 

The reason you code is messy is that you posted in HTML to a plain-text mailing list.


> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From oma.gonzales at gmail.com  Fri Sep 18 05:04:56 2015
From: oma.gonzales at gmail.com (=?UTF-8?B?T21hciBBbmRyw6kgR29uesOhbGVzIETDrWF6?=)
Date: Thu, 17 Sep 2015 22:04:56 -0500
Subject: [R] Clustering of clients (retail) - Free data sets?
Message-ID: <CAM-xyZgWeD4fcqhYOMcBnKDhJXPkr7HhxwP0Fftv5098VYA0+A@mail.gmail.com>

Hi all,

I'm learning about how to do clusters of clients. ?

I've founde this nice presentation on the subject, but the data is not
avaliable to use. I've contacted the autor, hope he'll answer soon.

https://ds4ci.files.wordpress.com/2013/09/user08_jimp_custseg_revnov08.pdf

Someone knows similar tutorials? or good books on the subject?

Thanks very much!


From bhh at xs4all.nl  Fri Sep 18 07:30:25 2015
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 18 Sep 2015 07:30:25 +0200
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
In-Reply-To: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
References: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
Message-ID: <913BE192-5834-4597-B20C-F5C6CD4E2935@xs4all.nl>


> On 17 Sep 2015, at 23:11, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
> 
> (x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1", "q13_11"))
> 
> # Which strings start with "q10" or "q12? - WORKS
> x[grep("^q10|q12", x)]
> 
> # Which strings end with "1"? - WORKS
> x[grep("1$", x)]
> 
> # Which strings end with "_1"? - WORKS
> x[grep("\\_1$", x)]
> 
> # Which strings start with "q10" AND contain a "1"? - WORKS
> x[grep("^q10.+1", x)]
> 

For these last to this should ?work?

> # Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK
> x[grep("^q10.+\\_1$", x)]
> 

x[grep("^q10.*\\_1$", x)]

> # Which strings start with "q10" or "q12 AND end with "_1"? - WORKS INCORRECTLY
> x[grep("^q10|q12.+\\_1$", x)]
> 

x[grep("^q10|q12.*\\_1$", x)]

It?s the .+ that?s the problem.

Berend

> Thank you!
> Dimitri Liakhovitski
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lists at dewey.myzen.co.uk  Fri Sep 18 10:05:55 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 18 Sep 2015 09:05:55 +0100
Subject: [R] best data storage format?
In-Reply-To: <F767CCEF5EA.00000058jrkrideau@inbox.com>
References: <F767CCEF5EA.00000058jrkrideau@inbox.com>
Message-ID: <55FBC5E3.4090205@dewey.myzen.co.uk>

Dear Alfa

Although John's advice is excellent if you already have the dataset in R 
in your case it seems that is not the case. Since R-help strips off most 
attachments you may need to put your .csv file somewhere like Dropbox or 
fool R-help into thinking it is a plain text file.

On 18/09/2015 00:41, John Kane wrote:
> No data. See dput() (?dput) as the preferred way to send data
>
> John Kane
> Kingston ON Canada
>
>
>> -----Original Message-----
>> From: alfadiallo at mac.com
>> Sent: Thu, 17 Sep 2015 16:41:46 -0400
>> To: r-help at r-project.org
>> Subject: [R] best data storage format?
>>
>> Hello -
>>
>> I?m working on dataset that will eventually be used in an xyz-plot.
>>
>> I?m having trouble figuring out the best way to store the data (see an
>> attached .csv sheet exported from Excel). Some information on the data:
>>
>> - Columns B - F are labels that describe the z data points
>> - Rows above x and y data pairs show the corresponding labels for the
>> data point.
>>
>> I want to use R to visualize the data and appreciate any feedback on the
>> best mechanism to store/organize the info. I?m new to R and want to start
>> the data assembly on the right foot - thanks for the help and advice!
>>
>> Alfa
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> Can't remember your password? Do you need a strong and secure password?
> Use Password manager! It stores your passwords & protects your account.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From drjimlemon at gmail.com  Fri Sep 18 11:38:16 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 18 Sep 2015 19:38:16 +1000
Subject: [R] HELP IN GRAPHS - slip screen
In-Reply-To: <CA+8X3fXF0t8BmuEq6MVeEYnsNjLRid-k+xFFO+AKzWmoy_dDqg@mail.gmail.com>
References: <F53D3AF7-922F-4352-B554-1E7F5244AB75@gmail.com>
	<CA+8X3fUpj3vfn=V8rrUNnRSMpu2R9D2DB1gfNQnMQ_tYC96iuQ@mail.gmail.com>
	<CAB630FFBi7xFeZ1GtfZNzcmbSGpgiMB8s_xL8iNEFOgGud1f6Q@mail.gmail.com>
	<CA+8X3fXF0t8BmuEq6MVeEYnsNjLRid-k+xFFO+AKzWmoy_dDqg@mail.gmail.com>
Message-ID: <CA+8X3fUVNzSg9JNsP-cVG8gU7jscoE6FXwT324nbgitiHuCG3A@mail.gmail.com>

Hi Rosa,
I have had a moment to look at your code. First I think you should start
your device as:

quartz(width=12,height=5)

The split.screen code that I sent seems to work for me, giving the

3    4    5    6
                       2
7    8    9    10

layout of screens. To get the aspect ratio of the plots more similar, try
this:

# do the first split, to get the rightmost screen for the legend
split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
# now split the first screen to get your eight screens (numbered 3 to 10)
for the plots
split.screen(figs=matrix(c(0,0.31,0.5,1,
                           0.31,0.54,0.5,1,
                           0.54,0.77,0.5,1,
                           0.77,1,0.5,1,
                           0,0.31,0,0.5,
                           0.31,0.54,0,0.5,
                           0.54,0.77,0,0.5,
                           0.77,1,0,0.5),
                         ncol=4,byrow=TRUE),screen=1)

 I'm not sure of which plots should go on the top line and which on the
bottom, but I think you want margins like this:

screen(3)
par(mar=c(0,3.5,3,0))
screen(4)
par(mar=c(0,0,3,0))
screen(5)
par(mar=c(0,0,3,0))
screen(6)
par(mar=c(0,0,3,0))
screen(7)
par(mar=c(3,3.5,0,0))
screen(8)
par(mar=c(3,0,3,0))
screen(9)
par(mar=c(3,0,3,0))
screen(10)
par(mar=c(3,0,3,0))

Perhaps this will help.

Jim


On Fri, Sep 18, 2015 at 6:14 AM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Rosa,
> I don't think the problem is with the split.screen command, for you are
> getting the eight plots and the screen at the right as you requested. It
> looks like your margins for each plot need adjusting, and I also think you
> should have about a 2.2 to 1 width to height ratio in the graphics device.
> I can't analyze the rest of the code at the moment, but perhaps tomorrow if
> you can't work it out I can provide some suggestions.
>
> Jim
>
>
> On Fri, Sep 18, 2015 at 1:16 AM, Rosa Oliveira <rosita21 at gmail.com> wrote:
>
>> Dear Jim,
>>
>> It works, nonetheless, it doesn't slip the screen correctly :(
>>
>> Do you have any idea?
>>
>>
>> I used the code:
>>
>>
>> #setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
>> setwd("~/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
>>
>>
>> library(ggplot2)
>> library(reshape)
>> library(lattice)
>>
>>
>> # read in what looks like half of the data
>>
>> bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
>> SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
>> bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
>> SE.alpha1<-read.csv("graphs_SE_alpha1.csv")
>>
>>
>>
>> quartz(width=10,height=6)
>>
>> # do the first split, to get the rightmost screen for the legend
>> split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
>> # now split the first screen to get your eight screens (numbered 3 to 10)
>> for the plots
>> split.screen(figs=matrix(c(0,0.25,0.5,1,
>>                            0.25,0.5,0.5,1,
>>                            0.5,0.75,0.5,1,
>>                            0.75,1,0.5,1,
>>                            0,0.25,0,0.5,
>>                            0.25,0.5,0,0.5,
>>                            0.5,0.75,0,0.5,
>>                            0.75,1,0,0.5),
>>                          ncol=4,byrow=TRUE),screen=1)
>>
>>
>>
>> #split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
>> #                           0.5,1,0.5,1,#primeira linha segunda coluna
>> #                           0,0.5,0,0.5,#segunda linha primeira coluna
>> #                           0.5,1,0,0.5),#segunda linha segunda coluna
>> #                         ncol=4,byrow=TRUE),screen=1)
>>
>>
>> # this produces seven screens numbered like this:
>> #   3   4   5   6
>> #                    2
>> #   7   8   9   10
>> # select the upper left screen
>>
>>
>>
>> screen(3)
>> par(mar=c(0,3.5,3,0))
>> # now the second set
>> n250<-bias.alpha1$nsample==250
>> matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1,
>> .6),main="nsample=250",ylab="", cex.main=1)
>> abline(h = 0, col = "gray60")
>> mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2,
>> cex.main=1)
>>
>> screen(4)
>> par(mar=c(0,0,3,0))
>> # now the second set
>> n1000<-bias.alpha1$nsample==1000
>> matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1,
>> .6),main="nsample=1000",ylab="")
>> abline(h = 0, col = "gray60")
>>
>>
>>
>> screen(5)
>> par(mar=c(0,3.5,3,0))
>> # now the second set
>> par(mar=c(3,3.5,0,0))
>> # now the second set
>> n250<-bias.alpha2$nsample==250
>> matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
>> abline(h = 0, col = "gray60")
>> mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2,
>> cex.main=1.5)
>>
>> screen(6)
>> par(mar=c(3,0,0,0))
>> # now the second set
>> n1000<-bias.alpha2$nsample==1000
>> matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
>> abline(h = 0, col = "gray60")
>>
>>
>>
>>
>> screen(7)
>> par(mar=c(0,3.5,3,0))
>> # now the second set
>> n250<-SE.alpha1$nsample==250
>> matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0,
>> 1.1),main="nsample=250",ylab="", cex.main=1)
>> abline(h = -1, col = "gray60")
>> mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2,
>> cex.main=1)
>> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>>
>>
>> screen(8)
>> par(mar=c(0,0,3,0))
>> # now the second set
>> n1000<-SE.alpha1$nsample==1000
>> matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0,
>> 1.1),main="nsample=1000",ylab="")
>> abline(h = -1, col = "gray60")
>>
>>
>>
>>
>> screen(9)
>> par(mar=c(3,3.5,0,0))
>> # now the second set
>> n250<-SE.alpha2$nsample==250
>> matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
>> abline(h = -.5, col = "gray60")
>> mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2,
>> cex.main=1.5)
>> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>>
>>
>> screen(10)
>> par(mar=c(3,0,0,0))
>> # now the second set
>> n1000<-SE.alpha2$nsample==1000
>> matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
>>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
>> abline(h = -.5, col = "gray60")
>> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
>>
>>
>>
>> screen(2)
>> par(mar=c(0,0,0,0))
>> # plot an empty plot to get the coordinates
>> plot(0:1,0:1,type="n",axes=FALSE)
>> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n",
>> lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
>>
>>
>> close.screen(all=TRUE)
>>
>>
>> and I attach the output graph.
>>
>>
>>
>> Best,
>> RO
>>
>> Atenciosamente,
>> Rosa Oliveira
>>
>> _________________________________
>>
>> Antes de imprimir este e-mail pense bem se tem mesmo que o fazer.
>> H? cada vez menos ?rvores.
>> N?o imprima, pense na sua responsabilidade e compromisso com o MEIO
>> AMBIENTE!
>>
>> <http://pt.dreamstime.com/cora-ccedil-atildeo-criado-das-folhas-de-aacutervores-diferentes-thumb12275776.jpg>
>>
>> <http://pt.dreamstime.com/cora-ccedil-atildeo-criado-das-folhas-de-aacutervores-diferentes-thumb12275776.jpg>
>>
>> 2015-09-17 12:18 GMT+01:00 Jim Lemon <drjimlemon at gmail.com>:
>>
>>> Hi Rosa,
>>> Try this:
>>>
>>> # do the first split, to get the rightmost screen for the legend
>>> split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
>>> # now split the first screen to get your eight screens (numbered 3 to
>>> 10) for the plots
>>> split.screen(figs=matrix(c(0,0.25,0.5,1,
>>>                            0.25,0.5,0.5,1,
>>>                            0.5,0.75,0.5,1,
>>>                            0.75,1,0.5,1,
>>>                            0,0.25,0,0.5,
>>>                            0.25,0.5,0,0.5,
>>>                            0.5,0.75,0,0.5,
>>>                            0.75,1,0,0.5),
>>>                          ncol=4,byrow=TRUE),screen=1)
>>>
>>> Jim
>>>
>>>
>>> On Thu, Sep 17, 2015 at 2:45 AM, Rosa Oliveira <rosita21 at gmail.com>
>>> wrote:
>>>
>>>> Dear all,
>>>>
>>>> I?m trying to do a graph,
>>>>
>>>> 3 rows, 5 columns, with the design:
>>>> #   3   4   5   6
>>>> #                    2
>>>> #   7   8   9   10
>>>>
>>>> I had a code for 3 rows, 3 columns, with the design::
>>>> #   3   4
>>>> #            2
>>>> #   7   8
>>>>  and I tried to modify it, but I had no success :(
>>>>
>>>> I suppose the problem is in the slip.screen code (red part of the code).
>>>>
>>>> I attach my code, can anyone please help me?
>>>>
>>>>
>>>> Best,
>>>> RO
>>>>
>>>>
>>>> setwd("/Users/RO/Dropbox/LMER -
>>>> 3rdproblem/R/latest_version/graphs/data")
>>>>
>>>> library(ggplot2)
>>>> library(reshape)
>>>> library(lattice)
>>>>
>>>>
>>>> # read in what looks like half of the data
>>>>
>>>> bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
>>>> SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
>>>> bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
>>>> SE.alpha1<-read.csv("graphs_SE_alpha1.csv")
>>>>
>>>>
>>>>
>>>> quartz(width=10,height=6)
>>>> # do the first split, to get the rightmost screen for the legend
>>>> split.screen(figs=matrix(c(0,0.8,0,1,0.8,1,0,1),nrow=2,byrow=TRUE))
>>>> # now split the first screen to get your six screens for the plots
>>>>
>>>>
>>>>
>>>> split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
>>>>                            0.5,1,0.5,1,#primeira linha segunda coluna
>>>>                            0,0.5,0,0.5,#segunda linha primeira coluna
>>>>                            0.5,1,0,0.5),#segunda linha segunda coluna
>>>>                          ncol=4,byrow=TRUE),screen=1)
>>>>
>>>>
>>>> # this produces seven screens numbered like this:
>>>> #   3   4   5   6
>>>> #                    2
>>>> #   7   8   9   10
>>>> # select the upper left screen
>>>>
>>>>
>>>>
>>>> screen(3)
>>>> par(mar=c(0,3.5,3,0))
>>>> # now the second set
>>>> n250<-bias.alpha1$nsample==250
>>>> matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
>>>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1,
>>>> .6),main="nsample=250",ylab="", cex.main=1)
>>>> abline(h = 0, col = "gray60")
>>>> mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2,
>>>> cex.main=1)
>>>>
>>>> screen(4)
>>>> par(mar=c(0,0,3,0))
>>>> # now the second set
>>>> n1000<-bias.alpha1$nsample==1000
>>>> matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
>>>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1,
>>>> .6),main="nsample=1000",ylab="")
>>>> abline(h = 0, col = "gray60")
>>>>
>>>>
>>>>
>>>> screen(5)
>>>> par(mar=c(0,3.5,3,0))
>>>> # now the second set
>>>> par(mar=c(3,3.5,0,0))
>>>> # now the second set
>>>> n250<-bias.alpha2$nsample==250
>>>> matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
>>>>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
>>>> abline(h = 0, col = "gray60")
>>>> mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2,
>>>> cex.main=1.5)
>>>>
>>>> screen(6)
>>>> par(mar=c(3,0,0,0))
>>>> # now the second set
>>>> n1000<-bias.alpha2$nsample==1000
>>>> matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
>>>>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
>>>> abline(h = 0, col = "gray60")
>>>>
>>>>
>>>>
>>>>
>>>> screen(7)
>>>> par(mar=c(0,3.5,3,0))
>>>> # now the second set
>>>> n250<-SE.alpha1$nsample==250
>>>> matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
>>>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0,
>>>> 1.1),main="nsample=250",ylab="", cex.main=1)
>>>> abline(h = -1, col = "gray60")
>>>> mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2,
>>>> cex.main=1)
>>>> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>>>>
>>>>
>>>> screen(8)
>>>> par(mar=c(0,0,3,0))
>>>> # now the second set
>>>> n1000<-SE.alpha1$nsample==1000
>>>> matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
>>>>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0,
>>>> 1.1),main="nsample=1000",ylab="")
>>>> abline(h = -1, col = "gray60")
>>>>
>>>>
>>>>
>>>>
>>>> screen(9)
>>>> par(mar=c(3,3.5,0,0))
>>>> # now the second set
>>>> n250<-SE.alpha2$nsample==250
>>>> matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
>>>>         type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
>>>> abline(h = -.5, col = "gray60")
>>>> mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2,
>>>> cex.main=1.5)
>>>> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
>>>>
>>>>
>>>> screen(10)
>>>> par(mar=c(3,0,0,0))
>>>> # now the second set
>>>> n1000<-SE.alpha2$nsample==1000
>>>> matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
>>>>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
>>>> abline(h = -.5, col = "gray60")
>>>> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
>>>>
>>>>
>>>>
>>>> screen(2)
>>>> par(mar=c(0,0,0,0))
>>>> # plot an empty plot to get the coordinates
>>>> plot(0:1,0:1,type="n",axes=FALSE)
>>>> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n",
>>>> lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
>>>>
>>>>
>>>> close.screen(all=TRUE)
>>>>
>>>>
>>>>
>>>>
>>>> Best,
>>>> RO
>>>>
>>>>
>>>> Atenciosamente,
>>>> Rosa Oliveira
>>>>
>>>> --
>>>>
>>>> ____________________________________________________________________________
>>>>
>>>>
>>>> Rosa Celeste dos Santos Oliveira,
>>>>
>>>> E-mail: rosita21 at gmail.com
>>>> Tlm: +351 939355143
>>>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>>>>
>>>> ____________________________________________________________________________
>>>> "Many admire, few know"
>>>> Hippocrates
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From thanoon.younis80 at gmail.com  Fri Sep 18 12:04:01 2015
From: thanoon.younis80 at gmail.com (thanoon younis)
Date: Fri, 18 Sep 2015 13:04:01 +0300
Subject: [R] help
Message-ID: <CABLo8nFwt7iUqL+A9Y8c4sUuLfGcLoUg9OeGQ83kdNo5-APMmQ@mail.gmail.com>

Hi,


I need you help to correct the code below please
I have this error
Error in if (v[j] >= 0) yo[i, j] <- 1 else yo[i, j] <- 0 :
  missing value where TRUE/FALSE needed





library(mvtnorm)   #Load mvtnorm package
N<-200;P<-9
W=matrix(NA, nrow=N, ncol=2)
xi=matrix(NA, nrow=N, ncol=2)
Eta=numeric(N)
phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
yo<-matrix(data=NA,nrow=N1,ncol=P); p<-numeric(P); v<- numeric(P)
for (t in 1:25) {
  for (i in 1:N) {
    xi1[i,]=rmvnorm(1, c(0,0), phi)
    delta1<-rnorm(1,0,sqrt(0.3))
eps<-rnorm(3,0,1)

Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta
    v[1]<-1.0+xi[i,1]; v1[2]<-1.0+0.8*xi1[i,1]; v1[3]<- 1.0+0.8*xi1[i,1]
    v[4]<-1.0+xi[i,2]; v1[5]<-1.0+0.8*xi1[i,2]; v1[6]<- 1.0+0.8*xi1[i,2]
    v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
    v[9]<-1.0+0.8*Eta[i]+eps[3]

        for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }

}#end




Any help would be appreciated.


Regards


-- 
Thanoon Y. Thanoon
PhD Candidate
Department of Mathematical Sciences
Faculty of Science
University Technology Malaysia, UTM
E.Mail: Thanoon.younis80 at gmail.com
E.Mail: dawn_prayer80 at yahoo.com
Facebook:Thanoon Younis AL-Shakerchy
Twitter: Thanoon Alshakerchy
H.P:00601127550205

	[[alternative HTML version deleted]]


From hasan.diwan at gmail.com  Fri Sep 18 12:42:20 2015
From: hasan.diwan at gmail.com (Hasan Diwan)
Date: Fri, 18 Sep 2015 03:42:20 -0700
Subject: [R] Distance in miles btw Zipcodes
In-Reply-To: <686300151.1256817.1442529365055.JavaMail.yahoo@mail.yahoo.com>
References: <686300151.1256817.1442529365055.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAP+bYWDH8gjxiPgFR2ztNjU=erUPjjTmDcAF0YOuAV=VdwA0PA@mail.gmail.com>

Farnoosh,
Please add your data by doing a dput(sample(data)) and we'll be able to
help you further. -- H

On 17 September 2015 at 15:36, Farnoosh Sheikhi via R-help <
r-help at r-project.org> wrote:

>  Hello,
> I'm trying to get the distances between two Zipcode variables, but for
> some reason I get this error:
> "matching was not perfect, returning what was found.Error: no such index
> at level 1"
> Here is my code:
>
> library(ggmap)mapdist(data$Zip.A, data$Zip.B, mode = "driving")
> The Zip codes are all in 5 digits format.I really appreciate any help or
> suggestion.Thanks.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




-- 
OpenPGP: https://hasan.d8u.us/gpg.key
Sent from my mobile device
Envoy? de mon portable

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Sep 18 13:06:25 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 18 Sep 2015 11:06:25 +0000
Subject: [R] Need data labels to jitter with datapoints in boxplot
In-Reply-To: <1442503233094-4712380.post@n4.nabble.com>
References: <1442503233094-4712380.post@n4.nabble.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C4053E@SRVEXCHMBX.precheza.cz>

Hi

What about to get rid of points jittering and let text labels jitter instead.

plot<-qplot(Letter,Number,data=test,geom=c("boxplot"),fill=Letter)
plot + geom_text(aes(label=Identifier),size=3, position = position_jitter(w = 0.3))

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of smheas
> Sent: Thursday, September 17, 2015 5:21 PM
> To: r-help at r-project.org
> Subject: [R] Need data labels to jitter with datapoints in boxplot
>
> Hello, I have created a boxplot with the data points overlayed on top
> using the below code. I am happy with the way the datapoints are
> jittered, however I cannot figure out how to get the labels to jitter
> along with the datapoints. The labels remain in the center and are
> unreadable. I have tried a lot of different ways to get them to jitter
> but can't seem to make it work. I have also tried making the label font
> smaller, but they still overlap too much. Note I cannot use the method
> of assigning the lcoation of each label - there are too many and this
> graph will be updated frequently with new data. I greatly appreciate
> your help. Thanks.
>
> After reading in data...
>
> #load packages
> library(ggplot2)
> #change letter to factor
> df$Letter<-as.factor(df$Letter)
> #create boxplot
> plot<-
> qplot(Letter,Number,data=df,geom=c("boxplot","jitter"),fill=Letter)
> #Add formatting
> plot<-plot + geom_text(aes(label=Identifier),size=3)
> ggsave(filename="Spreads
> Plot.pdf",plot=plot,width=11,height=8,units="in")
>
> Result:
> <http://r.789695.n4.nabble.com/file/n4712380/PostPic.png>
>
> Sample Data:
> Identifier    Letter  Number
> AB    AA-     46.74
> BC    A       59.62
> CD    A       61.63
> DE    A       69.49
> EF    A+      69.73
> FG    A+      74.57
> GH    A       75.01
> HI    A       77.52
> IJ    A       77.52
> JK    A       80.12
> KL    A       80.14
> LM    A-      80.35
> MN    A       81.98
> NO    A-      82.72
> OP    A+      83.56
> PQ    A       85.29
> QR    A-      85.46
> RS    A-      85.92
> ST    A       86.11
> TU    A-      86.55
> UV    A       86.57
> VW    A       88.32
> WX    A       89.4
> XY    A-      96.81
> YZ    A+      97.6
> BA    A-      101.86
> CB    A       102.37
> DC    A       104.29
> ED    A       104.92
> FE    A       106.29
> GF    A-      111.84
> HG    A+      121.91
> IH    A-      123.64
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Need-data-
> labels-to-jitter-with-datapoints-in-boxplot-tp4712380.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From lists at dewey.myzen.co.uk  Fri Sep 18 13:25:35 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 18 Sep 2015 12:25:35 +0100
Subject: [R] help
In-Reply-To: <CABLo8nFwt7iUqL+A9Y8c4sUuLfGcLoUg9OeGQ83kdNo5-APMmQ@mail.gmail.com>
References: <CABLo8nFwt7iUqL+A9Y8c4sUuLfGcLoUg9OeGQ83kdNo5-APMmQ@mail.gmail.com>
Message-ID: <55FBF4AF.6080507@dewey.myzen.co.uk>

Comment in-line

On 18/09/2015 11:04, thanoon younis wrote:
> Hi,
>
>
> I need you help to correct the code below please
> I have this error
> Error in if (v[j] >= 0) yo[i, j] <- 1 else yo[i, j] <- 0 :
>    missing value where TRUE/FALSE needed
>
>
>
>
>
> library(mvtnorm)   #Load mvtnorm package
> N<-200;P<-9
> W=matrix(NA, nrow=N, ncol=2)
> xi=matrix(NA, nrow=N, ncol=2)
> Eta=numeric(N)
> phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
> yo<-matrix(data=NA,nrow=N1,ncol=P); p<-numeric(P); v<- numeric(P)

When I try your code it fails here because it does not know what N1 is. 
Please post code which is self-contained.

> for (t in 1:25) {
>    for (i in 1:N) {
>      xi1[i,]=rmvnorm(1, c(0,0), phi)
>      delta1<-rnorm(1,0,sqrt(0.3))
> eps<-rnorm(3,0,1)
>
> Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta
>      v[1]<-1.0+xi[i,1]; v1[2]<-1.0+0.8*xi1[i,1]; v1[3]<- 1.0+0.8*xi1[i,1]
>      v[4]<-1.0+xi[i,2]; v1[5]<-1.0+0.8*xi1[i,2]; v1[6]<- 1.0+0.8*xi1[i,2]
>      v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
>      v[9]<-1.0+0.8*Eta[i]+eps[3]
>
>          for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }
>
> }#end
>
>
>
>
> Any help would be appreciated.
>
>
> Regards
>
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From 2hanl2da at naver.com  Fri Sep 18 13:18:30 2015
From: 2hanl2da at naver.com (massmatics)
Date: Fri, 18 Sep 2015 04:18:30 -0700 (PDT)
Subject: [R] =?utf-8?q?How_do_I_use_the_proper_r_code_to_find_the_value_of?=
 =?utf-8?q?_intersection_=28What_horsepower=28hp=29_does_the_=E2=80=9CVali?=
 =?utf-8?b?YW504oCdIGhhdmU/KQ==?=
Message-ID: <1442575110121-4712428.post@n4.nabble.com>

I am studying for a quiz and I have some problems while reviewing. I used the
command data(mtcars) and I used *intersect(mtcarsValiant,mtcarshp)* but the
value I received was *numeric(0) *What is the proper r code I must use to
get the correct value? (The value I should get is 105) Thanks!



--
View this message in context: http://r.789695.n4.nabble.com/How-do-I-use-the-proper-r-code-to-find-the-value-of-intersection-What-horsepower-hp-does-the-Valiant-tp4712428.html
Sent from the R help mailing list archive at Nabble.com.


From jrkrideau at inbox.com  Fri Sep 18 13:45:00 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 18 Sep 2015 03:45:00 -0800
Subject: [R] best data storage format?
In-Reply-To: <55FBC5E3.4090205@dewey.myzen.co.uk>
References: <f767ccef5ea.00000058jrkrideau@inbox.com>
Message-ID: <FDB876585D2.000004A2jrkrideau@inbox.com>

Thanks Michael, I stupidly assumed that the data was in R and just being sent in .csv form

John Kane
Kingston ON Canada


> -----Original Message-----
> From: lists at dewey.myzen.co.uk
> Sent: Fri, 18 Sep 2015 09:05:55 +0100
> To: jrkrideau at inbox.com, alfadiallo at mac.com, r-help at r-project.org
> Subject: Re: [R] best data storage format?
> 
> Dear Alfa
> 
> Although John's advice is excellent if you already have the dataset in R
> in your case it seems that is not the case. Since R-help strips off most
> attachments you may need to put your .csv file somewhere like Dropbox or
> fool R-help into thinking it is a plain text file.
> 
> On 18/09/2015 00:41, John Kane wrote:
>> No data. See dput() (?dput) as the preferred way to send data
>> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: alfadiallo at mac.com
>>> Sent: Thu, 17 Sep 2015 16:41:46 -0400
>>> To: r-help at r-project.org
>>> Subject: [R] best data storage format?
>>> 
>>> Hello -
>>> 
>>> I?m working on dataset that will eventually be used in an xyz-plot.
>>> 
>>> I?m having trouble figuring out the best way to store the data (see an
>>> attached .csv sheet exported from Excel). Some information on the data:
>>> 
>>> - Columns B - F are labels that describe the z data points
>>> - Rows above x and y data pairs show the corresponding labels for the
>>> data point.
>>> 
>>> I want to use R to visualize the data and appreciate any feedback on
>>> the
>>> best mechanism to store/organize the info. I?m new to R and want to
>>> start
>>> the data assembly on the right foot - thanks for the help and advice!
>>> 
>>> Alfa
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ____________________________________________________________
>> Can't remember your password? Do you need a strong and secure password?
>> Use Password manager! It stores your passwords & protects your account.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From thanoon.younis80 at gmail.com  Fri Sep 18 13:47:58 2015
From: thanoon.younis80 at gmail.com (thanoon younis)
Date: Fri, 18 Sep 2015 14:47:58 +0300
Subject: [R] help
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C4051F@SRVEXCHMBX.precheza.cz>
References: <CABLo8nFwt7iUqL+A9Y8c4sUuLfGcLoUg9OeGQ83kdNo5-APMmQ@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C4051F@SRVEXCHMBX.precheza.cz>
Message-ID: <CABLo8nGLy8KrHqL+vm69hittQLUYLyRv8V-5ESQsNV_VoAAQHw@mail.gmail.com>

Sorry this is the code

library(mvtnorm)   #Load mvtnorm package
N<-200;P<-9
W=matrix(NA, nrow=N, ncol=2)
xi=matrix(NA, nrow=N, ncol=2)
Eta=numeric(N)
phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
yo<-matrix(data=NA,nrow=N1,ncol=P); p<-numeric(P); v<- numeric(P)
for (t in 1:25) {
  for (i in 1:N) {
    xi1[i,]=rmvnorm(1, c(0,0), phi)
    delta<-rnorm(1,0,sqrt(0.3))
eps<-rnorm(3,0,1)

Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta
    v[1]<-1.0+xi[i,1]; v1[2]<-1.0+0.8*xi1[i,1]; v1[3]<- 1.0+0.8*xi1[i,1]
    v[4]<-1.0+xi[i,2]; v1[5]<-1.0+0.8*xi1[i,2]; v1[6]<- 1.0+0.8*xi1[i,2]
    v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
    v[9]<-1.0+0.8*Eta[i]+eps[3]

        for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }

}#end

On 18 September 2015 at 13:54, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
> You do not say anything what the code shall do and after getting third
> error of similar type, I gave up further investigation of your code.
>
> Error: object 'delta' not found
>
> Your error can have something to do with v[j] beeing NA
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of thanoon
> > younis
> > Sent: Friday, September 18, 2015 12:04 PM
> > To: r-help at r-project.org
> > Subject: [R] help
> >
> > Hi,
> >
> >
> > I need you help to correct the code below please
> > I have this error
> > Error in if (v[j] >= 0) yo[i, j] <- 1 else yo[i, j] <- 0 :
> >   missing value where TRUE/FALSE needed
> >
> >
> >
> >
> >
> > library(mvtnorm)   #Load mvtnorm package
> > N<-200;P<-9
> > W=matrix(NA, nrow=N, ncol=2)
> > xi=matrix(NA, nrow=N, ncol=2)
> > Eta=numeric(N)
> > phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
> > yo<-matrix(data=NA,nrow=N1,ncol=P); p<-numeric(P); v<- numeric(P)
> > for (t in 1:25) {
> >   for (i in 1:N) {
> >     xi1[i,]=rmvnorm(1, c(0,0), phi)
> >     delta1<-rnorm(1,0,sqrt(0.3))
> > eps<-rnorm(3,0,1)
> >
> > Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i
> > ,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta
> >     v[1]<-1.0+xi[i,1]; v1[2]<-1.0+0.8*xi1[i,1]; v1[3]<-
> > 1.0+0.8*xi1[i,1]
> >     v[4]<-1.0+xi[i,2]; v1[5]<-1.0+0.8*xi1[i,2]; v1[6]<-
> > 1.0+0.8*xi1[i,2]
> >     v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
> >     v[9]<-1.0+0.8*Eta[i]+eps[3]
> >
> >         for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }
> >
> > }#end
> >
> >
> >
> >
> > Any help would be appreciated.
> >
> >
> > Regards
> >
> >
> > --
> > Thanoon Y. Thanoon
> > PhD Candidate
> > Department of Mathematical Sciences
> > Faculty of Science
> > University Technology Malaysia, UTM
> > E.Mail: Thanoon.younis80 at gmail.com
> > E.Mail: dawn_prayer80 at yahoo.com
> > Facebook:Thanoon Younis AL-Shakerchy
> > Twitter: Thanoon Alshakerchy
> > H.P:00601127550205
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>



-- 
Thanoon Y. Thanoon
PhD Candidate
Department of Mathematical Sciences
Faculty of Science
University Technology Malaysia, UTM
E.Mail: Thanoon.younis80 at gmail.com
E.Mail: dawn_prayer80 at yahoo.com
Facebook:Thanoon Younis AL-Shakerchy
Twitter: Thanoon Alshakerchy
H.P:00601127550205

	[[alternative HTML version deleted]]


From thanoon.younis80 at gmail.com  Fri Sep 18 14:15:08 2015
From: thanoon.younis80 at gmail.com (thanoon younis)
Date: Fri, 18 Sep 2015 15:15:08 +0300
Subject: [R] help
In-Reply-To: <CABLo8nGLy8KrHqL+vm69hittQLUYLyRv8V-5ESQsNV_VoAAQHw@mail.gmail.com>
References: <CABLo8nFwt7iUqL+A9Y8c4sUuLfGcLoUg9OeGQ83kdNo5-APMmQ@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C4051F@SRVEXCHMBX.precheza.cz>
	<CABLo8nGLy8KrHqL+vm69hittQLUYLyRv8V-5ESQsNV_VoAAQHw@mail.gmail.com>
Message-ID: <CABLo8nHRRNbVmZgBK+AWBsKSV2xq3xLY_G=Y7gu59dXer7nF3g@mail.gmail.com>

This is the final code and i am so sorry for this mistake.
library(mvtnorm)   #Load mvtnorm package
N<-200;P<-9
W=matrix(NA, nrow=N, ncol=2)
xi=matrix(NA, nrow=N, ncol=2)
Eta=numeric(N)
phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
yo<-matrix(data=NA,nrow=N,ncol=P); p<-numeric(P); v<- numeric(P)
for (t in 1:25) {
  for (i in 1:N) {
    xi[i,]=rmvnorm(1, c(0,0), phi)
    delta<-rnorm(1,0,sqrt(0.3))
eps<-rnorm(3,0,1)

Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta

    v[1]<-1.0+xi[i,1]; v[2]<-1.0+0.8*xi[i,1]; v[3]<- 1.0+0.8*xi[i,1]
    v[4]<-1.0+xi[i,2]; v[5]<-1.0+0.8*xi[i,2]; v[6]<- 1.0+0.8*xi[i,2]
    v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
    v[9]<-1.0+0.8*Eta[i]+eps[3]

        for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }

}#end


On 18 September 2015 at 14:47, thanoon younis <thanoon.younis80 at gmail.com>
wrote:

> Sorry this is the code
>
> library(mvtnorm)   #Load mvtnorm package
> N<-200;P<-9
> W=matrix(NA, nrow=N, ncol=2)
> xi=matrix(NA, nrow=N, ncol=2)
> Eta=numeric(N)
> phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
> yo<-matrix(data=NA,nrow=N1,ncol=P); p<-numeric(P); v<- numeric(P)
> for (t in 1:25) {
>   for (i in 1:N) {
>     xi1[i,]=rmvnorm(1, c(0,0), phi)
>     delta<-rnorm(1,0,sqrt(0.3))
> eps<-rnorm(3,0,1)
>
> Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta
>     v[1]<-1.0+xi[i,1]; v1[2]<-1.0+0.8*xi1[i,1]; v1[3]<- 1.0+0.8*xi1[i,1]
>     v[4]<-1.0+xi[i,2]; v1[5]<-1.0+0.8*xi1[i,2]; v1[6]<- 1.0+0.8*xi1[i,2]
>     v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
>     v[9]<-1.0+0.8*Eta[i]+eps[3]
>
>         for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }
>
> }#end
>
> On 18 September 2015 at 13:54, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>
>> Hi
>>
>> You do not say anything what the code shall do and after getting third
>> error of similar type, I gave up further investigation of your code.
>>
>> Error: object 'delta' not found
>>
>> Your error can have something to do with v[j] beeing NA
>>
>> Cheers
>> Petr
>>
>> > -----Original Message-----
>> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of thanoon
>> > younis
>> > Sent: Friday, September 18, 2015 12:04 PM
>> > To: r-help at r-project.org
>> > Subject: [R] help
>> >
>> > Hi,
>> >
>> >
>> > I need you help to correct the code below please
>> > I have this error
>> > Error in if (v[j] >= 0) yo[i, j] <- 1 else yo[i, j] <- 0 :
>> >   missing value where TRUE/FALSE needed
>> >
>> >
>> >
>> >
>> >
>> > library(mvtnorm)   #Load mvtnorm package
>> > N<-200;P<-9
>> > W=matrix(NA, nrow=N, ncol=2)
>> > xi=matrix(NA, nrow=N, ncol=2)
>> > Eta=numeric(N)
>> > phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
>> > yo<-matrix(data=NA,nrow=N1,ncol=P); p<-numeric(P); v<- numeric(P)
>> > for (t in 1:25) {
>> >   for (i in 1:N) {
>> >     xi1[i,]=rmvnorm(1, c(0,0), phi)
>> >     delta1<-rnorm(1,0,sqrt(0.3))
>> > eps<-rnorm(3,0,1)
>> >
>> > Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i
>> > ,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta
>> >     v[1]<-1.0+xi[i,1]; v1[2]<-1.0+0.8*xi1[i,1]; v1[3]<-
>> > 1.0+0.8*xi1[i,1]
>> >     v[4]<-1.0+xi[i,2]; v1[5]<-1.0+0.8*xi1[i,2]; v1[6]<-
>> > 1.0+0.8*xi1[i,2]
>> >     v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
>> >     v[9]<-1.0+0.8*Eta[i]+eps[3]
>> >
>> >         for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }
>> >
>> > }#end
>> >
>> >
>> >
>> >
>> > Any help would be appreciated.
>> >
>> >
>> > Regards
>> >
>> >
>> > --
>> > Thanoon Y. Thanoon
>> > PhD Candidate
>> > Department of Mathematical Sciences
>> > Faculty of Science
>> > University Technology Malaysia, UTM
>> > E.Mail: Thanoon.younis80 at gmail.com
>> > E.Mail: dawn_prayer80 at yahoo.com
>> > Facebook:Thanoon Younis AL-Shakerchy
>> > Twitter: Thanoon Alshakerchy
>> > H.P:00601127550205
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-
>> > guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>> ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
>> vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
>> ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
>> strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
>> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
>> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
>> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are
>> intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its
>> sender. Delete the contents of this e-mail with all attachments and its
>> copies from your system.
>> If you are not the intended recipient of this e-mail, you are not
>> authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage
>> caused by modifications of the e-mail or by delay with transfer of the
>> email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a
>> contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to
>> immediately accept such offer; The sender of this e-mail (offer) excludes
>> any acceptance of the offer on the part of the recipient containing any
>> amendment or variation.
>> - the sender insists on that the respective contract is concluded only
>> upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to
>> enter into any contracts on behalf of the company except for cases in which
>> he/she is expressly authorized to do so in writing, and such authorization
>> or power of attorney is submitted to the recipient or the person
>> represented by the recipient, or the existence of such authorization is
>> known to the recipient of the person represented by the recipient.
>>
>
>
>
> --
> Thanoon Y. Thanoon
> PhD Candidate
> Department of Mathematical Sciences
> Faculty of Science
> University Technology Malaysia, UTM
> E.Mail: Thanoon.younis80 at gmail.com
> E.Mail: dawn_prayer80 at yahoo.com
> Facebook:Thanoon Younis AL-Shakerchy
> Twitter: Thanoon Alshakerchy
> H.P:00601127550205
>



-- 
Thanoon Y. Thanoon
PhD Candidate
Department of Mathematical Sciences
Faculty of Science
University Technology Malaysia, UTM
E.Mail: Thanoon.younis80 at gmail.com
E.Mail: dawn_prayer80 at yahoo.com
Facebook:Thanoon Younis AL-Shakerchy
Twitter: Thanoon Alshakerchy
H.P:00601127550205

	[[alternative HTML version deleted]]


From francois.morneau at ign.fr  Fri Sep 18 14:17:43 2015
From: francois.morneau at ign.fr (=?UTF-8?Q?Fran=c3=a7ois_Morneau?=)
Date: Fri, 18 Sep 2015 14:17:43 +0200
Subject: [R]
 =?utf-8?q?How_do_I_use_the_proper_r_code_to_find_the_value_of?=
 =?utf-8?q?_intersection_=28What_horsepower=28hp=29_does_the_=E2=80=9CVali?=
 =?utf-8?b?YW504oCdIGhhdmU/KQ==?=
In-Reply-To: <1442575110121-4712428.post@n4.nabble.com>
References: <1442575110121-4712428.post@n4.nabble.com>
Message-ID: <55FC00E7.1060104@ign.fr>

Hello (Who?),

Perhaps should you try to do your homework, look at "An Introduction to
R" for example and read the help pages :

> ?intersect # is clearly not what your are looking for
> ?'[' # seems better
and you would have find that :
> mtcars["Valiant", "hp"] # is one way to get what you are looking for.

You should also have a look at :

> library(fortunes)
> fortune(122)

Regards,

Fran?ois

Le 18/09/2015 13:18, massmatics a ?crit :
> I am studying for a quiz and I have some problems while reviewing. I used the
> command data(mtcars) and I used *intersect(mtcarsValiant,mtcarshp)* but the
> value I received was *numeric(0) *What is the proper r code I must use to
> get the correct value? (The value I should get is 105) Thanks!
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/How-do-I-use-the-proper-r-code-to-find-the-value-of-intersection-What-horsepower-hp-does-the-Valiant-tp4712428.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Fri Sep 18 14:27:46 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 18 Sep 2015 12:27:46 +0000
Subject: [R] help
In-Reply-To: <CABLo8nHRRNbVmZgBK+AWBsKSV2xq3xLY_G=Y7gu59dXer7nF3g@mail.gmail.com>
References: <CABLo8nFwt7iUqL+A9Y8c4sUuLfGcLoUg9OeGQ83kdNo5-APMmQ@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C4051F@SRVEXCHMBX.precheza.cz>
	<CABLo8nGLy8KrHqL+vm69hittQLUYLyRv8V-5ESQsNV_VoAAQHw@mail.gmail.com>
	<CABLo8nHRRNbVmZgBK+AWBsKSV2xq3xLY_G=Y7gu59dXer7nF3g@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C405D5@SRVEXCHMBX.precheza.cz>

Hi

as I said v has some NA values

> i
[1] 1
> j
[1] 7
> v
[1]  1.5194356  1.4155485  1.4155485 -0.6085982 -0.2868786 -0.2868786         NA
[8]         NA         NA
> Eta[i]
[1] NA
> Eta
  [1] NA  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
[26]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
...
Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta

> W
       [,1] [,2]
  [1,]   NA   NA
  [2,]   NA   NA

Unless you change W to numeric value you wil always get Eta[i] NA and v[7] NA

So this is why you get error. Without disclosing what you want to do, you probably do not get better answer.

Cheers
Petr

From: thanoon younis [mailto:thanoon.younis80 at gmail.com]
Sent: Friday, September 18, 2015 2:15 PM
To: PIKAL Petr; r-help at r-project.org
Subject: Re: [R] help

This is the final code and i am so sorry for this mistake.
library(mvtnorm)   #Load mvtnorm package
N<-200;P<-9
W=matrix(NA, nrow=N, ncol=2)
xi=matrix(NA, nrow=N, ncol=2)
Eta=numeric(N)
phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
yo<-matrix(data=NA,nrow=N,ncol=P); p<-numeric(P); v<- numeric(P)
for (t in 1:25) {
  for (i in 1:N) {
    xi[i,]=rmvnorm(1, c(0,0), phi)
    delta<-rnorm(1,0,sqrt(0.3))
eps<-rnorm(3,0,1)
    Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta

    v[1]<-1.0+xi[i,1]; v[2]<-1.0+0.8*xi[i,1]; v[3]<- 1.0+0.8*xi[i,1]
    v[4]<-1.0+xi[i,2]; v[5]<-1.0+0.8*xi[i,2]; v[6]<- 1.0+0.8*xi[i,2]
    v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
    v[9]<-1.0+0.8*Eta[i]+eps[3]

        for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }

}#end


On 18 September 2015 at 14:47, thanoon younis <thanoon.younis80 at gmail.com<mailto:thanoon.younis80 at gmail.com>> wrote:
Sorry this is the code

library(mvtnorm)   #Load mvtnorm package
N<-200;P<-9
W=matrix(NA, nrow=N, ncol=2)
xi=matrix(NA, nrow=N, ncol=2)
Eta=numeric(N)
phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
yo<-matrix(data=NA,nrow=N1,ncol=P); p<-numeric(P); v<- numeric(P)
for (t in 1:25) {
  for (i in 1:N) {
    xi1[i,]=rmvnorm(1, c(0,0), phi)
    delta<-rnorm(1,0,sqrt(0.3))
eps<-rnorm(3,0,1)
    Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta
    v[1]<-1.0+xi[i,1]; v1[2]<-1.0+0.8*xi1[i,1]; v1[3]<- 1.0+0.8*xi1[i,1]
    v[4]<-1.0+xi[i,2]; v1[5]<-1.0+0.8*xi1[i,2]; v1[6]<- 1.0+0.8*xi1[i,2]
    v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
    v[9]<-1.0+0.8*Eta[i]+eps[3]

        for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }

}#end

On 18 September 2015 at 13:54, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

You do not say anything what the code shall do and after getting third error of similar type, I gave up further investigation of your code.

Error: object 'delta' not found

Your error can have something to do with v[j] beeing NA

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of thanoon
> younis
> Sent: Friday, September 18, 2015 12:04 PM
> To: r-help at r-project.org<mailto:r-help at r-project.org>
> Subject: [R] help
>
> Hi,
>
>
> I need you help to correct the code below please
> I have this error
> Error in if (v[j] >= 0) yo[i, j] <- 1 else yo[i, j] <- 0 :
>   missing value where TRUE/FALSE needed
>
>
>
>
>
> library(mvtnorm)   #Load mvtnorm package
> N<-200;P<-9
> W=matrix(NA, nrow=N, ncol=2)
> xi=matrix(NA, nrow=N, ncol=2)
> Eta=numeric(N)
> phi<-matrix(data=c(1.0,0.3,0.3,1.0),ncol=2)
> yo<-matrix(data=NA,nrow=N1,ncol=P); p<-numeric(P); v<- numeric(P)
> for (t in 1:25) {
>   for (i in 1:N) {
>     xi1[i,]=rmvnorm(1, c(0,0), phi)
>     delta1<-rnorm(1,0,sqrt(0.3))
> eps<-rnorm(3,0,1)
>
> Eta[i]=0.6*W[i,1]+0.6*W[i,1]*xi[i,1]+0.6*W[i,2]*xi[i,2]+0.6*W[i,2]*xi[i
> ,2]*xi[i,2]+0.6*xi[i,1]+0.6*xi[i,2]+0.6*xi[i,1]*xi[i,1]+delta
>     v[1]<-1.0+xi[i,1]; v1[2]<-1.0+0.8*xi1[i,1]; v1[3]<-
> 1.0+0.8*xi1[i,1]
>     v[4]<-1.0+xi[i,2]; v1[5]<-1.0+0.8*xi1[i,2]; v1[6]<-
> 1.0+0.8*xi1[i,2]
>     v[7]<-1.0+Eta[i]+eps[1]; v[8]<-1.0+0.8*Eta[i]+eps[2];
>     v[9]<-1.0+0.8*Eta[i]+eps[3]
>
>         for (j in 1:9) { if (v[j]>=0) yo[i,j]<-1 else yo[i,j]<-0 } }
>
> }#end
>
>
>
>
> Any help would be appreciated.
>
>
> Regards
>
>
> --
> Thanoon Y. Thanoon
> PhD Candidate
> Department of Mathematical Sciences
> Faculty of Science
> University Technology Malaysia, UTM
> E.Mail: Thanoon.younis80 at gmail.com<mailto:Thanoon.younis80 at gmail.com>
> E.Mail: dawn_prayer80 at yahoo.com<mailto:dawn_prayer80 at yahoo.com>
> Facebook:Thanoon Younis AL-Shakerchy
> Twitter: Thanoon Alshakerchy
> H.P:00601127550205<tel:00601127550205>
>

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From scupton at nps.edu  Fri Sep 18 14:48:57 2015
From: scupton at nps.edu (Upton, Stephen (Steve) (CIV))
Date: Fri, 18 Sep 2015 12:48:57 +0000
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
In-Reply-To: <913BE192-5834-4597-B20C-F5C6CD4E2935@xs4all.nl>
References: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
	<913BE192-5834-4597-B20C-F5C6CD4E2935@xs4all.nl>
Message-ID: <C750A9148B73B84EB6293187A5F7DA75B816C528@GROWLER.ern.nps.edu>

And unless I'm mistaken, escaping the underscore is superfluous (I'd be curious to know if it's a function of locale).

x[grep("_", x)]

x[grep("^q10.*_1$", x)]

both work.

steve

Stephen C. Upton
SEED (Simulation Experiments & Efficient Designs) Center
Operations Research Department
Naval Postgraduate School
SEED Center web site: http://harvest.nps.edu

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Berend Hasselman
Sent: Friday, September 18, 2015 1:30 AM
To: Dimitri Liakhovitski
Cc: r-help
Subject: Re: [R] Hep with regex! - combination of ^, |, \\, _ and $


> On 17 Sep 2015, at 23:11, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
> 
> (x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1", 
> "q13_11"))
> 
> # Which strings start with "q10" or "q12? - WORKS x[grep("^q10|q12", 
> x)]
> 
> # Which strings end with "1"? - WORKS
> x[grep("1$", x)]
> 
> # Which strings end with "_1"? - WORKS x[grep("\\_1$", x)]
> 
> # Which strings start with "q10" AND contain a "1"? - WORKS 
> x[grep("^q10.+1", x)]
> 

For these last to this should ?work?

> # Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK 
> x[grep("^q10.+\\_1$", x)]
> 

x[grep("^q10.*\\_1$", x)]

> # Which strings start with "q10" or "q12 AND end with "_1"? - WORKS 
> INCORRECTLY x[grep("^q10|q12.+\\_1$", x)]
> 

x[grep("^q10|q12.*\\_1$", x)]

It?s the .+ that?s the problem.

Berend

> Thank you!
> Dimitri Liakhovitski
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From dimitri.liakhovitski at gmail.com  Fri Sep 18 15:06:43 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Fri, 18 Sep 2015 09:06:43 -0400
Subject: [R] Hep with regex! - combination of ^, |, \\, _ and $
In-Reply-To: <C750A9148B73B84EB6293187A5F7DA75B816C528@GROWLER.ern.nps.edu>
References: <CAN2xGJbnCKcxiW7Hhk=UL1ON8yikROsbk=i_yo9YNYx0xvTNzw@mail.gmail.com>
	<913BE192-5834-4597-B20C-F5C6CD4E2935@xs4all.nl>
	<C750A9148B73B84EB6293187A5F7DA75B816C528@GROWLER.ern.nps.edu>
Message-ID: <CAN2xGJada8xz7jcQBMX60WVu2mRYkHLC_4hK9iF3__DnnQQRmg@mail.gmail.com>

Thank you very much, guys!
As always - great learnings for me! Esp. + vs. *

On Fri, Sep 18, 2015 at 8:48 AM, Upton, Stephen (Steve) (CIV)
<scupton at nps.edu> wrote:
> And unless I'm mistaken, escaping the underscore is superfluous (I'd be curious to know if it's a function of locale).
>
> x[grep("_", x)]
>
> x[grep("^q10.*_1$", x)]
>
> both work.
>
> steve
>
> Stephen C. Upton
> SEED (Simulation Experiments & Efficient Designs) Center
> Operations Research Department
> Naval Postgraduate School
> SEED Center web site: http://harvest.nps.edu
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Berend Hasselman
> Sent: Friday, September 18, 2015 1:30 AM
> To: Dimitri Liakhovitski
> Cc: r-help
> Subject: Re: [R] Hep with regex! - combination of ^, |, \\, _ and $
>
>
>> On 17 Sep 2015, at 23:11, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
>>
>> (x <- c("q10_1", "q10_2", "q10_11", "q12_1", "q12_2", "q13_1",
>> "q13_11"))
>>
>> # Which strings start with "q10" or "q12? - WORKS x[grep("^q10|q12",
>> x)]
>>
>> # Which strings end with "1"? - WORKS
>> x[grep("1$", x)]
>>
>> # Which strings end with "_1"? - WORKS x[grep("\\_1$", x)]
>>
>> # Which strings start with "q10" AND contain a "1"? - WORKS
>> x[grep("^q10.+1", x)]
>>
>
> For these last to this should ?work?
>
>> # Which strings start with "q10" AND end with a "_1"? - DOES NOT WORK
>> x[grep("^q10.+\\_1$", x)]
>>
>
> x[grep("^q10.*\\_1$", x)]
>
>> # Which strings start with "q10" or "q12 AND end with "_1"? - WORKS
>> INCORRECTLY x[grep("^q10|q12.+\\_1$", x)]
>>
>
> x[grep("^q10|q12.*\\_1$", x)]
>
> It?s the .+ that?s the problem.
>
> Berend
>
>> Thank you!
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Dimitri Liakhovitski


From jrkrideau at inbox.com  Fri Sep 18 15:39:57 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 18 Sep 2015 05:39:57 -0800
Subject: [R] Spreadsheet math problem (exponentiation)
Message-ID: <FEB96270AED.000005E7jrkrideau@inbox.com>

It appears that at least three major spreadsheets, Excel, Apache OpenOffice Cal and gnumeric have a problem with the correct order of operations when dealing with exponents. The gnumeric result is very strange. 

This problem has probably been reported before but just in case it has not, it would appear to be one more serious problem with spreadsheets. It might be useful in warning people away from using a spreadsheet for serious analysis.

Excel 

-2^2 = 4

2^2^3 = 64

Apache OpenOffice

-2^2 = 4

2^2^3 = 64

gnumeric # note one correct, one error!

-2^2 = 4

2^2^3 = 256

John Kane
Kingston ON Canada

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From 2hanl2da at naver.com  Fri Sep 18 15:47:31 2015
From: 2hanl2da at naver.com (massmatics)
Date: Fri, 18 Sep 2015 06:47:31 -0700 (PDT)
Subject: [R]
 =?utf-8?q?How_do_I_use_the_proper_r_code_to_find_the_value_of?=
 =?utf-8?q?_intersection_=28What_horsepower=28hp=29_does_the_=E2=80=9CVali?=
 =?utf-8?b?YW504oCdIGhhdmU/KQ==?=
In-Reply-To: <55FC00E7.1060104@ign.fr>
References: <1442575110121-4712428.post@n4.nabble.com>
	<55FC00E7.1060104@ign.fr>
Message-ID: <1442584051545-4712438.post@n4.nabble.com>

Wow thank you so much!



--
View this message in context: http://r.789695.n4.nabble.com/How-do-I-use-the-proper-r-code-to-find-the-value-of-intersection-What-horsepower-hp-does-the-Valiant-tp4712428p4712438.html
Sent from the R help mailing list archive at Nabble.com.


From alfadiallo at mac.com  Fri Sep 18 16:25:57 2015
From: alfadiallo at mac.com (Alfa Diallo)
Date: Fri, 18 Sep 2015 10:25:57 -0400
Subject: [R] best data storage format?
In-Reply-To: <FDB876585D2.000004A2jrkrideau@inbox.com>
References: <f767ccef5ea.00000058jrkrideau@inbox.com>
	<FDB876585D2.000004A2jrkrideau@inbox.com>
Message-ID: <91054466-5199-4FE0-B653-2469D1205BA8@mac.com>

Hello Michael and Co.

I uploaded the files:
http://www.alfadiallo.com/r/r_sample.csv <http://www.alfadiallo.com/r/r_sample.csv> 
http://www.alfadiallo.com/r/r_sample.xlsx <http://www.alfadiallo.com/r/r_sample.csv>

Thanks!

Alfa



> On Sep 18, 2015, at 7:45 AM, John Kane <jrkrideau at inbox.com> wrote:
> 
> Thanks Michael, I stupidly assumed that the data was in R and just being sent in .csv form
> 
> John Kane
> Kingston ON Canada
> 
> 
>> -----Original Message-----
>> From: lists at dewey.myzen.co.uk
>> Sent: Fri, 18 Sep 2015 09:05:55 +0100
>> To: jrkrideau at inbox.com, alfadiallo at mac.com, r-help at r-project.org
>> Subject: Re: [R] best data storage format?
>> 
>> Dear Alfa
>> 
>> Although John's advice is excellent if you already have the dataset in R
>> in your case it seems that is not the case. Since R-help strips off most
>> attachments you may need to put your .csv file somewhere like Dropbox or
>> fool R-help into thinking it is a plain text file.
>> 
>> On 18/09/2015 00:41, John Kane wrote:
>>> No data. See dput() (?dput) as the preferred way to send data
>>> 
>>> John Kane
>>> Kingston ON Canada
>>> 
>>> 
>>>> -----Original Message-----
>>>> From: alfadiallo at mac.com
>>>> Sent: Thu, 17 Sep 2015 16:41:46 -0400
>>>> To: r-help at r-project.org
>>>> Subject: [R] best data storage format?
>>>> 
>>>> Hello -
>>>> 
>>>> I?m working on dataset that will eventually be used in an xyz-plot.
>>>> 
>>>> I?m having trouble figuring out the best way to store the data (see an
>>>> attached .csv sheet exported from Excel). Some information on the data:
>>>> 
>>>> - Columns B - F are labels that describe the z data points
>>>> - Rows above x and y data pairs show the corresponding labels for the
>>>> data point.
>>>> 
>>>> I want to use R to visualize the data and appreciate any feedback on
>>>> the
>>>> best mechanism to store/organize the info. I?m new to R and want to
>>>> start
>>>> the data assembly on the right foot - thanks for the help and advice!
>>>> 
>>>> Alfa
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ____________________________________________________________
>>> Can't remember your password? Do you need a strong and secure password?
>>> Use Password manager! It stores your passwords & protects your account.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
> 
> ____________________________________________________________
> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
> Check it out at http://www.inbox.com/earth
> 
> 


	[[alternative HTML version deleted]]


From john.archie.mckown at gmail.com  Fri Sep 18 16:31:02 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Fri, 18 Sep 2015 09:31:02 -0500
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <FEB96270AED.000005E7jrkrideau@inbox.com>
References: <FEB96270AED.000005E7jrkrideau@inbox.com>
Message-ID: <CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>

On Fri, Sep 18, 2015 at 8:39 AM, John Kane <jrkrideau at inbox.com> wrote:

> It appears that at least three major spreadsheets, Excel, Apache
> OpenOffice Cal and gnumeric have a problem with the correct order of
> operations when dealing with exponents. The gnumeric result is very strange.
>
> This problem has probably been reported before but just in case it has
> not, it would appear to be one more serious problem with spreadsheets. It
> might be useful in warning people away from using a spreadsheet for serious
> analysis.
>
> Excel
>
> -2^2 = 4
>
> 2^2^3 = 64
>
> Apache OpenOffice
>
> -2^2 = 4
>
> 2^2^3 = 64
>

My opinion: One correct, one error!? R agrees with me on this:
> 2^2
[1] 4
> 2^2^3
[1] 256
> 2^(2^3)
[1] 256
> -2^2
[1] -4
> (-2)^2
[1] 4
>




>
> gnumeric # note one correct, one error!
>

?My opinion: two correct!?



>
> -2^2 = 4
>
> 2^2^3 = 256
>
> John Kane
> Kingston ON Canada
>
>
?Seems to be a bit off-topic. Unless your point to is to use R for
important work instead of some spreadsheet. A point with which I completely
agree!?


?MS-Excel, and Apache OpenOffice, appear to implement the above as
(2^2)^3==64. ?Whereas gnumeric implements appears to implement this as:
2^(2^3)==256. Which is "correct"? Depends on whom you ask.

ref: https://en.wikipedia.org/wiki/Order_of_operations
<quote>

If exponentiation is indicated by stacked symbols, the usual rule is to
work from the top down, thus:
[image: a^{b^c} = a^{(b^c)}],

which typically is not equal to [image: (a^b)^c]. However, some computer
systems may resolve the ambiguous expression differently. For example,
Microsoft
Office Excel <https://en.wikipedia.org/wiki/Microsoft_Office_Excel>
 evaluates *a*^*b*^*c* as (*a*^*b*)^*c*, which is opposite of normally
accepted convention of top-down order of execution for exponentiation. If
a=4, p=3, and q=2, [image: a^{p^q}] is evaluated to 4096 in Microsoft Excel
2013, the same as [image: (a^p)^q]. The expression [image: a^{(p^q)}], on
the other hand, results in 262144 using the same program.
</quote>

?Gnumeric abides by the above definition. FWIW. BTW - MS-Excel also has
1900 as a friggin' leap year (due to Lotus 1-2-3 apparently), so I don't
consider MS-Excel (or anything else from MS for that matter) to be a
definitive source of correctness.? Personal opinion. FSF associate member.
Penguinista.

-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From Muck at econ.uni-frankfurt.de  Fri Sep 18 07:43:28 2015
From: Muck at econ.uni-frankfurt.de (Johannes Muck)
Date: Fri, 18 Sep 2015 07:43:28 +0200
Subject: [R] Estimating Endogenous Selection Model
Message-ID: <003001d0f1d4$f1fa7c30$d5ef7490$@econ.uni-frankfurt.de>

Dear all,

I want to estimate a model in which individuals self-select into two
different actions (e.g. invest or not invest). Moreover, the factors that
influence the selection decision also affect the ultimate outcome variable
(e.g. return on investment). That is, I want to estimate a model with
endogenous selection.

My question is: Which R package supports this kind of estimation? 
To me it seems like the package sampleSelection
(https://cran.r-project.org/web/packages/sampleSelection/sampleSelection.pdf
) can only be used in case of a SAMPLE selection, i.e. in the example above:
only individuals that invest are observed. Yet in my case, both actions are
observed but the decision which action to choose is endogenous. Or does this
package also support self-selection models?

Thank you very much in advance.

Best,

Johannes


From ps629 at medschl.cam.ac.uk  Fri Sep 18 14:08:11 2015
From: ps629 at medschl.cam.ac.uk (Praveen Surendran)
Date: Fri, 18 Sep 2015 12:08:11 +0000
Subject: [R] Issue with results from 'summary' function in R
Message-ID: <C35F8007418891489CF1ED67E9DE3874BA4F3150@me-mbx4.medschl.cam.ac.uk>

Hi all,

Attached table (that contains summary for a genetic association study) was read using the command:

test <- read.table('testDat.txt',header=FALSE,stringsAsFactors=FALSE)

Results from the summary of the attached table is provided below:

> summary(test$V5)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  22070   22070   22070   22070   22070   22070

As we can see column 5 of this table contains only one value - 22072
I am confused as to why I am getting a value 22070 in the summary of this column.

I tested this using versions of R including - R version 3.2.1 (2015-06-18) -- "World-Famous Astronaut"

Thank you for looking at this issue.
Kind Regards,

Praveen.



-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: testDat.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150918/08e46309/attachment.txt>

From monika.novac at gmail.com  Fri Sep 18 15:46:10 2015
From: monika.novac at gmail.com (monika nov)
Date: Fri, 18 Sep 2015 14:46:10 +0100
Subject: [R] Function stslshac {sphet}: heteroskedasticity and
 autocorrelation consistent (HAC) estimator
Message-ID: <CAHsnuZM7Bjg9UKVaR5w0nhs+mGE-twwvrmHr0pwHnUequdLJLQ@mail.gmail.com>

Dear R-users,

I have quite basic question for econometricians, however I would like to be
sure in this.

If I use a HAC estimator of the variance-covariance (VC) matrix for a
spatial econometric model, do I still need to test the residuals for
spatial autocorrelation and heteroscedasticity? (in particular I am using
function stslshac available in package sphet. The estimator is based on
Kelejian, H.H. and Prucha, I.R. (2007) HAC estimation in a spatial
framework, Journal of Econometrics, 140, pages 131?154).

What if the residuals from model estimated by stslshac are spatially
autocorrelated and (or) heteroscedastic? Can I still use this estimator
with HAC estimate of VC matrix or shall I go for different estimator or
specification? Do the estimates have required properties (are they
unbiased, consistent, efficient)?

I would be grateful for any reaction.

Monika

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Fri Sep 18 16:51:16 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 18 Sep 2015 16:51:16 +0200
Subject: [R] Issue with results from 'summary' function in R
In-Reply-To: <C35F8007418891489CF1ED67E9DE3874BA4F3150@me-mbx4.medschl.cam.ac.uk>
References: <C35F8007418891489CF1ED67E9DE3874BA4F3150@me-mbx4.medschl.cam.ac.uk>
Message-ID: <CAJuCY5xs6CNGVVjrSYPY7d=a2krFKEpEBCXpyN5CNEkvk1CKEQ@mail.gmail.com>

This is described in ?summary

> x <- 22072
> getOption("digits")
[1] 7
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  22070   22070   22070   22070   22070   22070
> options(digits = 10)
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  22072   22072   22072   22072   22072   22072
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no
more than asking him to perform a post-mortem examination: he may be
able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does
not ensure that a reasonable answer can be extracted from a given body
of data. ~ John Tukey


2015-09-18 14:08 GMT+02:00 Praveen Surendran <ps629 at medschl.cam.ac.uk>:
> Hi all,
>
> Attached table (that contains summary for a genetic association study) was read using the command:
>
> test <- read.table('testDat.txt',header=FALSE,stringsAsFactors=FALSE)
>
> Results from the summary of the attached table is provided below:
>
>> summary(test$V5)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   22070   22070   22070   22070   22070   22070
>
> As we can see column 5 of this table contains only one value - 22072
> I am confused as to why I am getting a value 22070 in the summary of this column.
>
> I tested this using versions of R including - R version 3.2.1 (2015-06-18) -- "World-Famous Astronaut"
>
> Thank you for looking at this issue.
> Kind Regards,
>
> Praveen.
>
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Fri Sep 18 17:01:34 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 18 Sep 2015 15:01:34 +0000
Subject: [R] Need data labels to jitter with datapoints in boxplot
In-Reply-To: <1442503233094-4712380.post@n4.nabble.com>
References: <1442503233094-4712380.post@n4.nabble.com>
Message-ID: <D22174F4.138FDF%macqueen1@llnl.gov>

How about jittering outside the plot?

That is, insert
  df$jNum <- jitter(df$Number)
somewhere before the plot commands, then use jNum in your plots instead of
Number.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 9/17/15, 8:20 AM, "R-help on behalf of smheas"
<r-help-bounces at r-project.org on behalf of sheasley at aegonusa.com> wrote:

>Hello, I have created a boxplot with the data points overlayed on top
>using
>the below code. I am happy with the way the datapoints are jittered,
>however
>I cannot figure out how to get the labels to jitter along with the
>datapoints. The labels remain in the center and are unreadable. I have
>tried
>a lot of different ways to get them to jitter but can't seem to make it
>work. I have also tried making the label font smaller, but they still
>overlap too much. Note I cannot use the method of assigning the lcoation
>of
>each label - there are too many and this graph will be updated frequently
>with new data. I greatly appreciate your help. Thanks.
>
>After reading in data...
>
>#load packages
>library(ggplot2)
>#change letter to factor
>df$Letter<-as.factor(df$Letter)
>#create boxplot
>plot<-qplot(Letter,Number,data=df,geom=c("boxplot","jitter"),fill=Letter)
>#Add formatting
>plot<-plot + geom_text(aes(label=Identifier),size=3)
>ggsave(filename="Spreads Plot.pdf",plot=plot,width=11,height=8,units="in")
>
>Result:
><http://r.789695.n4.nabble.com/file/n4712380/PostPic.png>
>
>Sample Data:
>Identifier	Letter	Number
>AB	AA-	46.74
>BC	A	59.62
>CD	A	61.63
>DE	A	69.49
>EF	A+	69.73
>FG	A+	74.57
>GH	A	75.01
>HI	A	77.52
>IJ	A	77.52
>JK	A	80.12
>KL	A	80.14
>LM	A-	80.35
>MN	A	81.98
>NO	A-	82.72
>OP	A+	83.56
>PQ	A	85.29
>QR	A-	85.46
>RS	A-	85.92
>ST	A	86.11
>TU	A-	86.55
>UV	A	86.57
>VW	A	88.32
>WX	A	89.4
>XY	A-	96.81
>YZ	A+	97.6
>BA	A-	101.86
>CB	A	102.37
>DC	A	104.29
>ED	A	104.92
>FE	A	106.29
>GF	A-	111.84
>HG	A+	121.91
>IH	A-	123.64
>
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/Need-data-labels-to-jitter-with-datapoints-i
>n-boxplot-tp4712380.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Fri Sep 18 17:04:33 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Fri, 18 Sep 2015 15:04:33 +0000
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
References: <FEB96270AED.000005E7jrkrideau@inbox.com>
	<CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C3741@mb02.ads.tamu.edu>

Unfortunately the order of operations is not universal in computing. The real question is whether a program performs the way it is documented. Excel documents that unary operations take precedence over exponentiation and that within groups, the order is left to right. LibreOffice Calc behaves as Excel, but does not document the order of operations except to say */ before +-, left to right. I couldn't find any statement about the order of operations in the documentation for Gnumeric.

R documents that unary operations come after exponentiation and, within exponentiation, the order is right to left. Fortran puts unary operations with addition and subtraction after exponentiation with exponentiation right to left. C does not have an exponentiation operator, but unary operations come before multiplication and division.

When in doubt, use parentheses to make sure you get what you want.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John McKown
Sent: Friday, September 18, 2015 9:31 AM
To: John Kane
Cc: r-help
Subject: Re: [R] Spreadsheet math problem (exponentiation)

On Fri, Sep 18, 2015 at 8:39 AM, John Kane <jrkrideau at inbox.com> wrote:

> It appears that at least three major spreadsheets, Excel, Apache
> OpenOffice Cal and gnumeric have a problem with the correct order of
> operations when dealing with exponents. The gnumeric result is very strange.
>
> This problem has probably been reported before but just in case it has
> not, it would appear to be one more serious problem with spreadsheets. It
> might be useful in warning people away from using a spreadsheet for serious
> analysis.
>
> Excel
>
> -2^2 = 4
>
> 2^2^3 = 64
>
> Apache OpenOffice
>
> -2^2 = 4
>
> 2^2^3 = 64
>

My opinion: One correct, one error!? R agrees with me on this:
> 2^2
[1] 4
> 2^2^3
[1] 256
> 2^(2^3)
[1] 256
> -2^2
[1] -4
> (-2)^2
[1] 4
>




>
> gnumeric # note one correct, one error!
>

?My opinion: two correct!?



>
> -2^2 = 4
>
> 2^2^3 = 256
>
> John Kane
> Kingston ON Canada
>
>
?Seems to be a bit off-topic. Unless your point to is to use R for
important work instead of some spreadsheet. A point with which I completely
agree!?


?MS-Excel, and Apache OpenOffice, appear to implement the above as
(2^2)^3==64. ?Whereas gnumeric implements appears to implement this as:
2^(2^3)==256. Which is "correct"? Depends on whom you ask.

ref: https://en.wikipedia.org/wiki/Order_of_operations
<quote>

If exponentiation is indicated by stacked symbols, the usual rule is to
work from the top down, thus:
[image: a^{b^c} = a^{(b^c)}],

which typically is not equal to [image: (a^b)^c]. However, some computer
systems may resolve the ambiguous expression differently. For example,
Microsoft
Office Excel <https://en.wikipedia.org/wiki/Microsoft_Office_Excel>
 evaluates *a*^*b*^*c* as (*a*^*b*)^*c*, which is opposite of normally
accepted convention of top-down order of execution for exponentiation. If
a=4, p=3, and q=2, [image: a^{p^q}] is evaluated to 4096 in Microsoft Excel
2013, the same as [image: (a^p)^q]. The expression [image: a^{(p^q)}], on
the other hand, results in 262144 using the same program.
</quote>

?Gnumeric abides by the above definition. FWIW. BTW - MS-Excel also has
1900 as a friggin' leap year (due to Lotus 1-2-3 apparently), so I don't
consider MS-Excel (or anything else from MS for that matter) to be a
definitive source of correctness.? Personal opinion. FSF associate member.
Penguinista.

-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From bhh at xs4all.nl  Fri Sep 18 17:05:45 2015
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 18 Sep 2015 17:05:45 +0200
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
References: <FEB96270AED.000005E7jrkrideau@inbox.com>
	<CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
Message-ID: <9DE3057E-368D-4E02-8E34-EB8EE2ADF674@xs4all.nl>


> On 18 Sep 2015, at 16:31, John McKown <john.archie.mckown at gmail.com> wrote:
> 
> On Fri, Sep 18, 2015 at 8:39 AM, John Kane <jrkrideau at inbox.com> wrote:
> 
>> It appears that at least three major spreadsheets, Excel, Apache
>> OpenOffice Cal and gnumeric have a problem with the correct order of
>> operations when dealing with exponents. The gnumeric result is very strange.
>> 
>> This problem has probably been reported before but just in case it has
>> not, it would appear to be one more serious problem with spreadsheets. It
>> might be useful in warning people away from using a spreadsheet for serious
>> analysis.
>> 
>> Excel
>> 
>> -2^2 = 4
>> 
>> 2^2^3 = 64
>> 
>> Apache OpenOffice
>> 
>> -2^2 = 4
>> 
>> 2^2^3 = 64
>> 
> 
> My opinion: One correct, one error!? R agrees with me on this:
>> 2^2
> [1] 4
>> 2^2^3
> [1] 256
>> 2^(2^3)
> [1] 256
>> -2^2
> [1] -4
>> (-2)^2
> [1] 4
>> 
> 
> 
> 
> 
>> 
>> gnumeric # note one correct, one error!
>> 
> 
> ?My opinion: two correct!?
> 
> 

I don?t agree.
All are wrong according to standard math rules except Gnumeric with the exponentiation.

R is correct.

See https://en.wikipedia.org/wiki/Order_of_operations

Lesson: always use parentheses to make absolutely clear what you mean.

Berend

> 
>> -2^2 = 4
>> 
>> 2^2^3 = 256
>> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
> ?Seems to be a bit off-topic. Unless your point to is to use R for
> important work instead of some spreadsheet. A point with which I completely
> agree!?
> 
> 
> ?MS-Excel, and Apache OpenOffice, appear to implement the above as
> (2^2)^3==64. ?Whereas gnumeric implements appears to implement this as:
> 2^(2^3)==256. Which is "correct"? Depends on whom you ask.
> 
> ref: https://en.wikipedia.org/wiki/Order_of_operations
> <quote>
> 
> If exponentiation is indicated by stacked symbols, the usual rule is to
> work from the top down, thus:
> [image: a^{b^c} = a^{(b^c)}],
> 
> which typically is not equal to [image: (a^b)^c]. However, some computer
> systems may resolve the ambiguous expression differently. For example,
> Microsoft
> Office Excel <https://en.wikipedia.org/wiki/Microsoft_Office_Excel>
> evaluates *a*^*b*^*c* as (*a*^*b*)^*c*, which is opposite of normally
> accepted convention of top-down order of execution for exponentiation. If
> a=4, p=3, and q=2, [image: a^{p^q}] is evaluated to 4096 in Microsoft Excel
> 2013, the same as [image: (a^p)^q]. The expression [image: a^{(p^q)}], on
> the other hand, results in 262144 using the same program.
> </quote>
> 
> ?Gnumeric abides by the above definition. FWIW. BTW - MS-Excel also has
> 1900 as a friggin' leap year (due to Lotus 1-2-3 apparently), so I don't
> consider MS-Excel (or anything else from MS for that matter) to be a
> definitive source of correctness.? Personal opinion. FSF associate member.
> Penguinista.
> 
> -- 
> 
> Schrodinger's backup: The condition of any backup is unknown until a
> restore is attempted.
> 
> Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.
> 
> He's about as useful as a wax frying pan.
> 
> 10 to the 12th power microphones = 1 Megaphone
> 
> Maranatha! <><
> John McKown
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mckellercran at gmail.com  Fri Sep 18 17:11:19 2015
From: mckellercran at gmail.com (Matthew Keller)
Date: Fri, 18 Sep 2015 09:11:19 -0600
Subject: [R] fast way to create composite matrix based on mixed indices?
In-Reply-To: <55FB2F30.7030107@ttk.mta.hu>
References: <CAB7vCMSZx=Vp1A+UHvOsrOb9DfHkwDTRq0qdSOoBSidnxPKvPQ@mail.gmail.com>
	<55FB2F30.7030107@ttk.mta.hu>
Message-ID: <CAB7vCMR9CGaq6WD446KwUNJheAxHW52t-xE7ptVYq2QszAMYzQ@mail.gmail.com>

Brilliant Denes. Thank you for your help. This worked and is obviously much
faster than a loop...

On Thu, Sep 17, 2015 at 3:22 PM, D?nes T?th <toth.denes at ttk.mta.hu> wrote:

> Hi Matt,
>
> you could use matrix indexing. Here is a possible solution, which could be
> optimized further (probably).
>
> # The old matrix
> (old.mat <- matrix(1:30,nrow=3,byrow=TRUE))
> # matrix of indices
> index <- matrix(c(1,1,1,4,
>                   1,3,5,10,
>                   2,2,1,3,
>                   2,1,4,8,
>                   2,3,9,10),
>                 nrow=5,byrow=TRUE,
>                 dimnames=list(NULL,
>                               c('new.mat.row','old.mat.row',
>                                 'old.mat.col.start','old.mat.col.end')))
> # expected result
> new.mat <- matrix(c(1:4,25:30,11:13,4:8,29:30),
>                   byrow=TRUE, nrow=2)
> #
> # column indices
> ind <- mapply(seq, index[, 3], index[,4],
>               SIMPLIFY = FALSE, USE.NAMES = FALSE)
> ind_len <- vapply(ind, length, integer(1))
> ind <- unlist(ind)
>
> #
> # old indices
> old.ind <- cbind(rep(index[,2], ind_len), ind)
> #
> # new indices
> new.ind <- cbind(rep(index[,1], ind_len), ind)
> #
> # create the new matrix
> result <- matrix(NA_integer_, max(index[,1]), max(index[,4]))
> #
> # fill the new matrix
> result[new.ind] <- old.mat[old.ind]
> #
> # check the results
> identical(result, new.mat)
>
>
> HTH,
>   Denes
>
>
>
>
>
> On 09/17/2015 10:36 PM, Matthew Keller wrote:
>
>> HI all,
>>
>> Sorry for the title here but I find this difficult to describe succinctly.
>> Here's the problem.
>>
>> I want to create a new matrix where each row is a composite of an old
>> matrix, but where the row & column indexes of the old matrix change for
>> different parts of the new matrix. For example, the second row of new
>> matrix (which has , e.g., 10 columns) might be columns 1 to 3 of row 2 of
>> old matrix, columns 4 to 8 of row 1 of old matrix, and columns 9 to 10 of
>> row 3 of old matrix.
>>
>> Here's an example in code:
>>
>> #The old matrix
>> (old.mat <- matrix(1:30,nrow=3,byrow=TRUE))
>>
>> #matrix of indices to create the new matrix from the old one.
>> #The 1st column gives the row number of the new matrix
>> #the 2nd gives the row of the old matrix that we're going to copy into the
>> new matrix
>> #the 3rd gives the starting column of the old matrix for the row in col 2
>> #the 4th gives the end column of the old matrix for the row in col 2
>> index <- matrix(c(1,1,1,4,
>>                    1,3,5,10,
>>                    2,2,1,3,
>>                    2,1,4,8,
>>                    2,3,9,10),
>>                  nrow=5,byrow=TRUE,
>>
>>
>> dimnames=list(NULL,c('new.mat.row','old.mat.row','old.mat.col.start','old.mat.col.end')))
>>
>> I will be given old.mat and index and want to create new.mat from them.
>>
>> I want to create a new.matrix of two rows that looks like this:
>> new.mat <- matrix(c(1:4,25:30,11:13,4:8,29:30),byrow=TRUE,nrow=2)
>>
>> So here, the first row of new.mat is columns 1 to 4 of row 1 of the
>> old.mat
>> and columns 5 to 10 of row 3 of old.mat.
>>
>> new.mat and old.mat will always have the same number of columns but the
>> number of rows could differ.
>>
>> I could accomplish this in a loop, but the real problem is quite large
>> (new.mat might have 1e8 elements), and so a for loop would be
>> prohibitively
>> slow.
>>
>> I may resort to unix tools and use a shell script, but wanted to first see
>> if this is doable in R in a fast way.
>>
>> Thanks in advance!
>>
>> Matt
>>
>>
>>


-- 
Matthew C Keller
Asst. Professor of Psychology
University of Colorado at Boulder
www.matthewckeller.com

	[[alternative HTML version deleted]]


From erich.neuwirth at univie.ac.at  Fri Sep 18 17:13:44 2015
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Fri, 18 Sep 2015 17:13:44 +0200
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
References: <FEB96270AED.000005E7jrkrideau@inbox.com>
	<CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
Message-ID: <47A1882E-7549-4AE6-B4F6-139AF7078F81@univie.ac.at>

Let me add a little bit here:

When using math formulas, one should know about the parsing rules form complex expression
which do not have all the necessary parenthesis.

Different systems do have different parings rules.
In the case of a^b^c, the expression is ambiguus because
(as mentioned in a previous mail) in general
(a^b)^c != a^(b^c)
To avoid unintended consequences, just us parentheses and you will get
the right result.
in the case of -a^b
The question is the order of precedence of unary - and binary ^.

In Excel, -2^2=4, but 0-2^2=-4

Reason: For Excel, unary - is stronger than the power operator, but binary minus is weaker.

My feeling is that too many people are bashing spreadsheets for the wrong reason.
Spreadsheets ca do things R cannot do: Automatic recalculation when input changes,
and visual point and click modelling of dependencies.
The calculation engine of Excel admittedly has some weak points.

That is the reason why I wrote RExcel which gives you all the advantages of the spreadsheet interface
and allows you to use the R calculation within this interface whenever needed.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 670 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150918/dbe37d89/attachment.bin>

From john.archie.mckown at gmail.com  Fri Sep 18 17:14:36 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Fri, 18 Sep 2015 10:14:36 -0500
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C3741@mb02.ads.tamu.edu>
References: <FEB96270AED.000005E7jrkrideau@inbox.com>
	<CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C3741@mb02.ads.tamu.edu>
Message-ID: <CAAJSdjiNyExXPy0Z75uuwvRvL-_yj-nsk5-kVsYwtKYF9Gi22w@mail.gmail.com>

On Fri, Sep 18, 2015 at 10:04 AM, David L Carlson <dcarlson at tamu.edu> wrote:

> Unfortunately the order of operations is not universal in computing. The
> real question is whether a program performs the way it is documented. Excel
> documents that unary operations take precedence over exponentiation and
> that within groups, the order is left to right. LibreOffice Calc behaves as
> Excel, but does not document the order of operations except to say */
> before +-, left to right. I couldn't find any statement about the order of
> operations in the documentation for Gnumeric.
>
> R documents that unary operations come after exponentiation and, within
> exponentiation, the order is right to left. Fortran puts unary operations
> with addition and subtraction after exponentiation with exponentiation
> right to left. C does not have an exponentiation operator, but unary
> operations come before multiplication and division.
>
> When in doubt, use parentheses to make sure you get what you want.
>
>
?Very true. I do that if there is almost any chance that I, or another,
might not actually know which is first. I especially adopted this when I
learned a language called APL. It has _no_ precedence of operations. And it
does them from right to left. E.g. A=B*C+D is interpreted as A=B*(C+D). I
did programming in it for a couple of months. Then went back to normal
programming. Wrote completely _wrong_ code. When I asked a friend why the
calculation didn't get the "right" answer (2*4+3 == 14 was example I gave),
he looked at me like I was insane <grin/>. ?


-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From Keith.Jewell at campdenbri.co.uk  Fri Sep 18 16:56:16 2015
From: Keith.Jewell at campdenbri.co.uk (Keith Jewell)
Date: Fri, 18 Sep 2015 15:56:16 +0100
Subject: [R] Issue with results from 'summary' function in R
In-Reply-To: <C35F8007418891489CF1ED67E9DE3874BA4F3150@me-mbx4.medschl.cam.ac.uk>
References: <C35F8007418891489CF1ED67E9DE3874BA4F3150@me-mbx4.medschl.cam.ac.uk>
Message-ID: <55FC2610.1010706@campdenbri.co.uk>

On 18/09/2015 13:08, Praveen Surendran wrote:
> Hi all,
>
> Attached table (that contains summary for a genetic association study) was read using the command:
>
> test <- read.table('testDat.txt',header=FALSE,stringsAsFactors=FALSE)
>
> Results from the summary of the attached table is provided below:
>
>> summary(test$V5)
>     Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>    22070   22070   22070   22070   22070   22070
>
> As we can see column 5 of this table contains only one value - 22072
> I am confused as to why I am getting a value 22070 in the summary of this column.
>
> I tested this using versions of R including - R version 3.2.1 (2015-06-18) -- "World-Famous Astronaut"
>
> Thank you for looking at this issue.
> Kind Regards,
>
> Praveen.

 > summary(22072, digits=5)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   22072   22072   22072   22072   22072   22072


From lists at dewey.myzen.co.uk  Fri Sep 18 17:50:00 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 18 Sep 2015 16:50:00 +0100
Subject: [R] best data storage format?
In-Reply-To: <91054466-5199-4FE0-B653-2469D1205BA8@mac.com>
References: <f767ccef5ea.00000058jrkrideau@inbox.com>
	<FDB876585D2.000004A2jrkrideau@inbox.com>
	<91054466-5199-4FE0-B653-2469D1205BA8@mac.com>
Message-ID: <55FC32A8.70100@dewey.myzen.co.uk>

I am afraid I cannot see any quick way to do this although I am sure 
some of our data structure experts will be able to. As I see it your 
lines come in triples. The first line mostly consists of character data, 
the second is predominantly numeric and is values of x, the third is 
predominantly numeric and values of y.

I am not sure what #REF! means.

On 18/09/2015 15:25, Alfa Diallo wrote:
> Hello Michael and Co.
>
> I uploaded the files:
> http://www.alfadiallo.com/r/r_sample.csv
> http://www.alfadiallo.com/r/r_sample.xlsx
> <http://www.alfadiallo.com/r/r_sample.csv>
>
> Thanks!
>
> Alfa
>
>
>
>> On Sep 18, 2015, at 7:45 AM, John Kane <jrkrideau at inbox.com
>> <mailto:jrkrideau at inbox.com>> wrote:
>>
>> Thanks Michael, I stupidly assumed that the data was in R and just
>> being sent in .csv form
>>
>> John Kane
>> Kingston ON Canada
>>
>>
>>> -----Original Message-----
>>> From: lists at dewey.myzen.co.uk <mailto:lists at dewey.myzen.co.uk>
>>> Sent: Fri, 18 Sep 2015 09:05:55 +0100
>>> To: jrkrideau at inbox.com <mailto:jrkrideau at inbox.com>,
>>> alfadiallo at mac.com <mailto:alfadiallo at mac.com>, r-help at r-project.org
>>> <mailto:r-help at r-project.org>
>>> Subject: Re: [R] best data storage format?
>>>
>>> Dear Alfa
>>>
>>> Although John's advice is excellent if you already have the dataset in R
>>> in your case it seems that is not the case. Since R-help strips off most
>>> attachments you may need to put your .csv file somewhere like Dropbox or
>>> fool R-help into thinking it is a plain text file.
>>>
>>> On 18/09/2015 00:41, John Kane wrote:
>>>> No data. See dput() (?dput) as the preferred way to send data
>>>>
>>>> John Kane
>>>> Kingston ON Canada
>>>>
>>>>
>>>>> -----Original Message-----
>>>>> From: alfadiallo at mac.com <mailto:alfadiallo at mac.com>
>>>>> Sent: Thu, 17 Sep 2015 16:41:46 -0400
>>>>> To: r-help at r-project.org <mailto:r-help at r-project.org>
>>>>> Subject: [R] best data storage format?
>>>>>
>>>>> Hello -
>>>>>
>>>>> I?m working on dataset that will eventually be used in an xyz-plot.
>>>>>
>>>>> I?m having trouble figuring out the best way to store the data (see an
>>>>> attached .csv sheet exported from Excel). Some information on the data:
>>>>>
>>>>> - Columns B - F are labels that describe the z data points
>>>>> - Rows above x and y data pairs show the corresponding labels for the
>>>>> data point.
>>>>>
>>>>> I want to use R to visualize the data and appreciate any feedback on
>>>>> the
>>>>> best mechanism to store/organize the info. I?m new to R and want to
>>>>> start
>>>>> the data assembly on the right foot - thanks for the help and advice!
>>>>>
>>>>> Alfa
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>>>> To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> ____________________________________________________________
>>>> Can't remember your password? Do you need a strong and secure password?
>>>> Use Password manager! It stores your passwords & protects your account.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>>> To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> --
>>> Michael
>>> http://www.dewey.myzen.co.uk/home.html
>>
>> ____________________________________________________________
>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>> Check it out at http://www.inbox.com/earth
>>
>>
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From erich.neuwirth at univie.ac.at  Fri Sep 18 18:06:23 2015
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Fri, 18 Sep 2015 18:06:23 +0200
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <FFF393B90B7.000007C8jrkrideau@inbox.com>
References: <caajsdjjq3tm+dw4_=njrgwottaem4q3sqprwszxve1v755lx+q@mail.gmail.com>
	<feb96270aed.000005e7jrkrideau@inbox.com>
	<FFF393B90B7.000007C8jrkrideau@inbox.com>
Message-ID: <F14C0AF8-053F-4E0D-8058-5DA54DFD0AC1@univie.ac.at>

Methinks that any math teaching should make learners aware of the fact that
math conventions are not laws of nature, and that ambiguous expressions may
produce different values in different systems.

I think -2^2=4 is perfectly reasonable.

In my experience, most people after high school math do not know that
binary - und unary - are very different operations.
And that is the fault of the current way of teaching math!



> On Sep 18, 2015, at 18:00, John Kane <jrkrideau at inbox.com> wrote:
> 
> A very good point re RExcel   for sophisticated users.  The majority of spreadsheet users, will never have heard of RExcel (or R for that matter) and very likely will no  have the sophistication of knowing that they need to use brackets.  Plus RExcel  is not available in  flavours for Apache OpenOffice or gnumeric as far as I am aware.
> 
> If one actually learned the formal mathematical order of operations and still remembers  them the expectation is that -2^2 will return -2.  In a  20 sheet spreadsheet the error is likely to go completely undetected and may or may not have significant effect on final results.
> 
> I, recently, was reading an education blog where the author was bemoaning the fact that shiny new math teachers were teaching that -2^2 = 4. Presumably they are putting their faith in Excel, etc., rather than the actual math conventions.
> 
> John Kane
> Kingston ON Canada
> 
> 
>> -----Original Message-----
>> From: erich.neuwirth at univie.ac.at
>> Sent: Fri, 18 Sep 2015 17:13:44 +0200
>> To: r-help at r-project.org
>> Subject: Re: [R] Spreadsheet math problem (exponentiation)
>> 
>> Let me add a little bit here:
>> 
>> When using math formulas, one should know about the parsing rules form
>> complex expression
>> which do not have all the necessary parenthesis.
>> 
>> Different systems do have different parings rules.
>> In the case of a^b^c, the expression is ambiguus because
>> (as mentioned in a previous mail) in general
>> (a^b)^c != a^(b^c)
>> To avoid unintended consequences, just us parentheses and you will get
>> the right result.
>> in the case of -a^b
>> The question is the order of precedence of unary - and binary ^.
>> 
>> In Excel, -2^2=4, but 0-2^2=-4
>> 
>> Reason: For Excel, unary - is stronger than the power operator, but
>> binary minus is weaker.
>> 
>> My feeling is that too many people are bashing spreadsheets for the wrong
>> reason.
>> Spreadsheets ca do things R cannot do: Automatic recalculation when input
>> changes,
>> and visual point and click modelling of dependencies.
>> The calculation engine of Excel admittedly has some weak points.
>> 
>> That is the reason why I wrote RExcel which gives you all the advantages
>> of the spreadsheet interface
>> and allows you to use the R calculation within this interface whenever
>> needed.
> 
> ____________________________________________________________
> Send any screenshot to your friends in seconds...
> Works in all emails, instant messengers, blogs, forums and social networks.
> TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if2 for FREE
> 
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 670 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150918/fa5f219d/attachment.bin>

From alfadiallo at mac.com  Fri Sep 18 18:10:38 2015
From: alfadiallo at mac.com (Alfa Diallo)
Date: Fri, 18 Sep 2015 12:10:38 -0400
Subject: [R] best data storage format?
In-Reply-To: <55FC32A8.70100@dewey.myzen.co.uk>
References: <f767ccef5ea.00000058jrkrideau@inbox.com>
	<FDB876585D2.000004A2jrkrideau@inbox.com>
	<91054466-5199-4FE0-B653-2469D1205BA8@mac.com>
	<55FC32A8.70100@dewey.myzen.co.uk>
Message-ID: <12771698-3DDD-4F9B-9DD1-2BF91EFD9D4C@mac.com>

Thanks for your quick reply. 

I had a feeling that would be the case. 

Rookie question: if I proceed in this fashion, will it be a problem to use R to create an xyz plot with labels for the unique datapoints?

Sent from my iPhone

> On Sep 18, 2015, at 11:50 AM, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> 
> I am afraid I cannot see any quick way to do this although I am sure some of our data structure experts will be able to. As I see it your lines come in triples. The first line mostly consists of character data, the second is predominantly numeric and is values of x, the third is predominantly numeric and values of y.
> 
> I am not sure what #REF! means.
> 
>> On 18/09/2015 15:25, Alfa Diallo wrote:
>> Hello Michael and Co.
>> 
>> I uploaded the files:
>> http://www.alfadiallo.com/r/r_sample.csv
>> http://www.alfadiallo.com/r/r_sample.xlsx
>> <http://www.alfadiallo.com/r/r_sample.csv>
>> 
>> Thanks!
>> 
>> Alfa
>> 
>> 
>> 
>>> On Sep 18, 2015, at 7:45 AM, John Kane <jrkrideau at inbox.com
>>> <mailto:jrkrideau at inbox.com>> wrote:
>>> 
>>> Thanks Michael, I stupidly assumed that the data was in R and just
>>> being sent in .csv form
>>> 
>>> John Kane
>>> Kingston ON Canada
>>> 
>>> 
>>>> -----Original Message-----
>>>> From: lists at dewey.myzen.co.uk <mailto:lists at dewey.myzen.co.uk>
>>>> Sent: Fri, 18 Sep 2015 09:05:55 +0100
>>>> To: jrkrideau at inbox.com <mailto:jrkrideau at inbox.com>,
>>>> alfadiallo at mac.com <mailto:alfadiallo at mac.com>, r-help at r-project.org
>>>> <mailto:r-help at r-project.org>
>>>> Subject: Re: [R] best data storage format?
>>>> 
>>>> Dear Alfa
>>>> 
>>>> Although John's advice is excellent if you already have the dataset in R
>>>> in your case it seems that is not the case. Since R-help strips off most
>>>> attachments you may need to put your .csv file somewhere like Dropbox or
>>>> fool R-help into thinking it is a plain text file.
>>>> 
>>>>> On 18/09/2015 00:41, John Kane wrote:
>>>>> No data. See dput() (?dput) as the preferred way to send data
>>>>> 
>>>>> John Kane
>>>>> Kingston ON Canada
>>>>> 
>>>>> 
>>>>>> -----Original Message-----
>>>>>> From: alfadiallo at mac.com <mailto:alfadiallo at mac.com>
>>>>>> Sent: Thu, 17 Sep 2015 16:41:46 -0400
>>>>>> To: r-help at r-project.org <mailto:r-help at r-project.org>
>>>>>> Subject: [R] best data storage format?
>>>>>> 
>>>>>> Hello -
>>>>>> 
>>>>>> I?m working on dataset that will eventually be used in an xyz-plot.
>>>>>> 
>>>>>> I?m having trouble figuring out the best way to store the data (see an
>>>>>> attached .csv sheet exported from Excel). Some information on the data:
>>>>>> 
>>>>>> - Columns B - F are labels that describe the z data points
>>>>>> - Rows above x and y data pairs show the corresponding labels for the
>>>>>> data point.
>>>>>> 
>>>>>> I want to use R to visualize the data and appreciate any feedback on
>>>>>> the
>>>>>> best mechanism to store/organize the info. I?m new to R and want to
>>>>>> start
>>>>>> the data assembly on the right foot - thanks for the help and advice!
>>>>>> 
>>>>>> Alfa
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>>>>> To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> ____________________________________________________________
>>>>> Can't remember your password? Do you need a strong and secure password?
>>>>> Use Password manager! It stores your passwords & protects your account.
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>>>> To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> --
>>>> Michael
>>>> http://www.dewey.myzen.co.uk/home.html
>>> 
>>> ____________________________________________________________
>>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>>> Check it out at http://www.inbox.com/earth
> 
> -- 
> Michael
> http://www.dewey.myzen.co.uk/home.html


From es at enricoschumann.net  Fri Sep 18 18:20:42 2015
From: es at enricoschumann.net (Enrico Schumann)
Date: Fri, 18 Sep 2015 18:20:42 +0200
Subject: [R] Optimization Grid Search Slow
In-Reply-To: <CAB9UfhSaWK2RkMRv_b0=rMt57SOp_nKMvVO42q+hU4gq7koT9w@mail.gmail.com>
	(Edward Patzelt's message of "Thu, 17 Sep 2015 13:55:52 -0400")
References: <CAB9UfhSaWK2RkMRv_b0=rMt57SOp_nKMvVO42q+hU4gq7koT9w@mail.gmail.com>
Message-ID: <87d1xfr95x.fsf@enricoschumann.net>

On Thu, 17 Sep 2015, "Patzelt, Edward" <patzelt at g.harvard.edu> writes:

> R Help -
>
> I am trying to use a grid search for a 2 free parameter reinforcement
> learning model and the grid search is incredibly slow. I've used optimx but
> can't seem to get reasonable answers. Is there a way to speed up this grid
> search dramatically?
>
>
> dat <- structure(list(choice = c(0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
>                                  1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
> 0, 1, 0, 1, 0, 1, 0,
>                                  0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
> 0, 0, 1, 0, 0, 1, 1,
>                                  1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
> 0, 1, 0, 0, 0, 0, 1,
>                                  1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
> 1, 0, 0, 0, 0, 0, 0,
>                                  1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
> 1, 0, 0, 0, 0, 0, 1,
>                                  1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
> 1, 0, 0, 1, 1, 0, 0,
>                                  0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
> 0, 0, 1, 0, 0, 0, 0,
>                                  1, 0, 1, 1, 1, 0), reward = c(0L, 0L, 0L,
> 0L, 1L, 1L, 0L, 0L,
>                                                                1L, 0L, 0L,
> 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 1L,
>                                                                1L, 0L, 1L,
> 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L,
>                                                                1L, 0L, 1L,
> 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L,
>                                                                0L, 0L, 1L,
> 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L,
>                                                                1L, 1L, 0L,
> 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
>                                                                0L, 0L, 0L,
> 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L,
>                                                                1L, 0L, 0L,
> 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L,
>                                                                0L, 1L, 0L,
> 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
>                                                                0L, 1L, 0L,
> 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L,
>                                                                0L, 0L, 1L,
> 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), RepNum = c(1L,
>
>                                                    1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>
>                                                    1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>
>                                                    1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>
>                                                    1L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>
>                                                    2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>
>                                                    2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>
>                                                    2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>
>                                                    2L, 2L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>
>                                                    3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>
>                                                    3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>
>                                                    3L, 3L, 3L, 3L)), .Names
> = c("choice", "reward", "RepNum"), row.names = c(NA,
>
>
>                                                  165L), class =
> "data.frame")
>
>
> CNTRACSID <- 0; subjectFit <- 0;
> pLlist <- 0; pRlist <- 0; logLikelihood <- 0; trialProb <- 0;
>
> hmmFunc <- function(delta, temperature){
>
>   pLlist = 1
>   pRlist = 1
>   block = 0
>   for (i in 1:length(dat$choice))
>   {
>     if (dat$RepNum[i] != block)
>     {
>       pL = 0.5
>       pR = 0.5
>       block = dat$RepNum[i]
>     }
>     # Markov Transitions
>     pL <- pL*(1-delta) + pR*delta
>     pR <- 1-pL
>     # Apply feedback
>     #denom <- p(F|L,C) * p(L) + p(F|R,C) * p(R)
>
>     pflc <- ifelse(dat$choice[i] == dat$reward[i], .8, .2)
>     pfrc <- 1 - pflc
>     denom <- pflc * pL + pfrc * pR
>
>     # What's the new belief given observation
>     posteriorL <- pflc * pL/denom
>     posteriorR <- 1-posteriorL
>
>     pL <- posteriorL
>     pR <- posteriorR
>
>     pL <- (1/(1 + exp(-temperature * (pL-.5))))
>     pR <- (1/(1 + exp(-temperature * (pR-.5))))
>
>     pLlist[i] = pL
>     pRlist[i] = pR
>
>     if(i > 1){
>       if(dat$choice[i] == 1){
>         trialProb[i] <- pLlist[i-1]
>       } else
>       {
>         trialProb[i] <- 1-pLlist[i-1]
>       }
>     }
>     else {
>       trialProb[1] <- .5
>     }
>
>   }
>   trialProb2 <- sum(log(trialProb))
>   subFit <- exp(trialProb2/length(dat$choice))
>   hmmOutput <- list("logLikelihood" = trialProb2, "subjectFit" = subFit,
> "probabilities" = pLlist)
>   # print(hmmOutput$logLikelihood)
>   return(hmmOutput)
> }
>
>
> subjectFits <- 0; subLogLike <- 0; bestTemp <- 0; bestDelta= 0;
>
> min = 0.001; max = .5; inc = 0.001;
> deltaList = seq(min, max, inc)
> mina = 0; maxa = 5; inca = .01
> amList = seq(mina, maxa, inca)
>     maxLogValue <- -1000
>     for(delta in deltaList){
>       for(temp in amList){
>         probabilities <- hmmFunc(delta, temp)
>         if(probabilities$logLikelihood > maxLogValue){
>           pList <- probabilities$probabilities
>           maxLogValue <- probabilities$logLikelihood
>           subLogLike <- probabilities$logLikelihood
>           subjectFits <- probabilities$subjectFit
>           bestTemp <- temp
>           bestDelta <- delta
>
>         }
>       }
>     }


Another option, perhaps: there is a function 'gridSearch' in package
NMOF that allows you to distribute (i.e. run in parallel) the
computations.

(Disclosure: I am the maintainer of NMOF.)

-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From john.archie.mckown at gmail.com  Fri Sep 18 18:46:34 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Fri, 18 Sep 2015 11:46:34 -0500
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <F14C0AF8-053F-4E0D-8058-5DA54DFD0AC1@univie.ac.at>
References: <caajsdjjq3tm+dw4_=njrgwottaem4q3sqprwszxve1v755lx+q@mail.gmail.com>
	<feb96270aed.000005e7jrkrideau@inbox.com>
	<FFF393B90B7.000007C8jrkrideau@inbox.com>
	<F14C0AF8-053F-4E0D-8058-5DA54DFD0AC1@univie.ac.at>
Message-ID: <CAAJSdjiGxMsFfDTwBNJM9okAY9+--X-WyRcSBNW7-+Z-CrD2MQ@mail.gmail.com>

On Fri, Sep 18, 2015 at 11:06 AM, Erich Neuwirth <
erich.neuwirth at univie.ac.at> wrote:

> Methinks that any math teaching should make learners aware of the fact that
> math conventions are not laws of nature, and that ambiguous expressions may
> produce different values in different systems.
>
> I think -2^2=4 is perfectly reasonable.
>

?I agree. That is what APL would do. Of course, the number "minus 2" in APL
is not encoded as "-2". It is encoded (hope this works) ?2 ?This removes
the ambiguity.?



>
> In my experience, most people after high school math do not know that
> binary - und unary - are very different operations.
> And that is the fault of the current way of teaching math!
>

?Agree. Perhaps we should just "bite the bullet" and go with Reverse Polish
Notation for equations. Much easier to parse. No parentheses and no
ambiguities. Of course, this would raise more howls that converting from
Imperial measures to Metric did. ?We still haven't done this in the U.S. I
really want to! I sound much taller and thinner in metric. No, I won't say
what the number are. Bad enough that I'm old.


-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Fri Sep 18 19:23:09 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 18 Sep 2015 09:23:09 -0800
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <F14C0AF8-053F-4E0D-8058-5DA54DFD0AC1@univie.ac.at>
References: <caajsdjjq3tm+dw4_=njrgwottaem4q3sqprwszxve1v755lx+q@mail.gmail.com>
	<fff393b90b7.000007c8jrkrideau@inbox.com>
	<feb96270aed.000005e7jrkrideau@inbox.com>
Message-ID: <00AC4890AD9.000008E9jrkrideau@inbox.com>

I  have no problem with -2^2 = 4 if we have consistency.
R  -2^2 = -4
Spreadsheets -2^2 = 4

Which is "TRUE"? (For some nebulous value of "TRUE")

For a relatively unsophisticated user this does not bode well if he or see is transferring work from one application to another.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: erich.neuwirth at univie.ac.at
> Sent: Fri, 18 Sep 2015 18:06:23 +0200
> To: jrkrideau at inbox.com
> Subject: Re: [R] Spreadsheet math problem (exponentiation)
> 
> Methinks that any math teaching should make learners aware of the fact
> that
> math conventions are not laws of nature, and that ambiguous expressions
> may
> produce different values in different systems.
> 
> I think -2^2=4 is perfectly reasonable.
> 
> In my experience, most people after high school math do not know that
> binary - und unary - are very different operations.
> And that is the fault of the current way of teaching math!
> 
> 
> 
>> On Sep 18, 2015, at 18:00, John Kane <jrkrideau at inbox.com> wrote:
>> 
>> A very good point re RExcel   for sophisticated users.  The majority of
>> spreadsheet users, will never have heard of RExcel (or R for that
>> matter) and very likely will no  have the sophistication of knowing that
>> they need to use brackets.  Plus RExcel  is not available in  flavours
>> for Apache OpenOffice or gnumeric as far as I am aware.
>> 
>> If one actually learned the formal mathematical order of operations and
>> still remembers  them the expectation is that -2^2 will return -2.  In a
>> 20 sheet spreadsheet the error is likely to go completely undetected and
>> may or may not have significant effect on final results.
>> 
>> I, recently, was reading an education blog where the author was
>> bemoaning the fact that shiny new math teachers were teaching that -2^2
>> = 4. Presumably they are putting their faith in Excel, etc., rather than
>> the actual math conventions.
>> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: erich.neuwirth at univie.ac.at
>>> Sent: Fri, 18 Sep 2015 17:13:44 +0200
>>> To: r-help at r-project.org
>>> Subject: Re: [R] Spreadsheet math problem (exponentiation)
>>> 
>>> Let me add a little bit here:
>>> 
>>> When using math formulas, one should know about the parsing rules form
>>> complex expression
>>> which do not have all the necessary parenthesis.
>>> 
>>> Different systems do have different parings rules.
>>> In the case of a^b^c, the expression is ambiguus because
>>> (as mentioned in a previous mail) in general
>>> (a^b)^c != a^(b^c)
>>> To avoid unintended consequences, just us parentheses and you will get
>>> the right result.
>>> in the case of -a^b
>>> The question is the order of precedence of unary - and binary ^.
>>> 
>>> In Excel, -2^2=4, but 0-2^2=-4
>>> 
>>> Reason: For Excel, unary - is stronger than the power operator, but
>>> binary minus is weaker.
>>> 
>>> My feeling is that too many people are bashing spreadsheets for the
>>> wrong
>>> reason.
>>> Spreadsheets ca do things R cannot do: Automatic recalculation when
>>> input
>>> changes,
>>> and visual point and click modelling of dependencies.
>>> The calculation engine of Excel admittedly has some weak points.
>>> 
>>> That is the reason why I wrote RExcel which gives you all the
>>> advantages
>>> of the spreadsheet interface
>>> and allows you to use the R calculation within this interface whenever
>>> needed.
>> 
>> ____________________________________________________________
>> Send any screenshot to your friends in seconds...
>> Works in all emails, instant messengers, blogs, forums and social
>> networks.
>> TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if2 for
>> FREE
>> 
>>

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From dwinsemius at comcast.net  Fri Sep 18 19:30:11 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 18 Sep 2015 10:30:11 -0700
Subject: [R] best data storage format?
In-Reply-To: <E4422A2C-0480-4485-A256-F0C8BDE0C4AB@mac.com>
References: <E4422A2C-0480-4485-A256-F0C8BDE0C4AB@mac.com>
Message-ID: <43B4D0DB-F931-495A-9D0A-C8D46B35AFBA@comcast.net>


On Sep 17, 2015, at 1:41 PM, Alfa Diallo wrote:

> Hello -
> 
> I?m working on dataset that will eventually be used in an xyz-plot.
> 
> I?m having trouble figuring out the best way to store the data (see an attached .csv sheet exported from Excel). Some information on the data:
> 
> - Columns B - F are labels that describe the z data points
> - Rows above x and y data pairs show the corresponding labels for the data point.
> 
> I want to use R to visualize the data and appreciate any feedback on the best mechanism to store/organize the info. I?m new to R and want to start the data assembly on the right foot - thanks for the help and advice!
> 
> Alfa


You later provided a link to a csv version of that dataset. I loaded it into an OpenOffice spreadsheet (because my purchased copy of Excel was no longer functional after a harddisk crash and restore from backup to a different disk and MS now claims to not have any record of it so refuses to let me re-register it to the new disk despite having all the original boxes and fancy security tags.) 

At any rate, I am attaching a png copy of the data in its original version and in a transposed version what I would suggest is closer to how one might store your data. Presenting pictures of data is almost always a bad idea on Rhelp, but this seems the best way to illustrate the current arangement.  At the moment you have a mixture of columnar and row-wise storage. You should convert to all columnar. This will require copying the "z" values of interest to new columns after the transpose. You can probably do that in Excel easier than doing it in R. Then when you have an all-columnar arrangement, just use read.csv.




David Winsemius
Alameda, CA, USA


From marianar at ime.unicamp.br  Fri Sep 18 17:43:53 2015
From: marianar at ime.unicamp.br (marianar)
Date: Fri, 18 Sep 2015 08:43:53 -0700 (PDT)
Subject: [R] R2WinBUGS error
In-Reply-To: <1428493384235-4705613.post@n4.nabble.com>
References: <1428493384235-4705613.post@n4.nabble.com>
Message-ID: <1442591033128-4712456.post@n4.nabble.com>

I am facing the same problem. I have been able to run my code in another
computer with Windows 8 but not in my computer at work, which also runs
Windowns 8. 

I am looking forward for solving this problem.

Mariana.



--
View this message in context: http://r.789695.n4.nabble.com/R2WinBUGS-error-tp4705613p4712456.html
Sent from the R help mailing list archive at Nabble.com.


From georgialclack92 at outlook.com  Fri Sep 18 18:03:35 2015
From: georgialclack92 at outlook.com (icelandic1992)
Date: Fri, 18 Sep 2015 09:03:35 -0700 (PDT)
Subject: [R] Need help with GLM on R
Message-ID: <1442592215416-4712459.post@n4.nabble.com>

I am using some data for a population count 
This what my data is set out as 
Area   Count    Year      DOY      Rain     Wind  

all continuous effects except wind and rain and area are categorical
variables with 1 for rain and wind and 0 for no rain or wind and areas 1-27

I am trying to do a GLM analysis of certain treatment effects on Loge(total
count+1) to find out which factors significantly affect the count. I used
the formula glm.nb(Count~factor(Area)+Year+Rain+Wind+DOY)
However it does not work, and when I looked at it with the formula I was
given by someone else which was 
glm.nb(Count~factor(Area)+Year+DOY+Year*DOY) it came up with intercept as
the first area and then areas 1-27 listed and then year and DOY. 

I am unsure of what this means as i think it is comparing all the areas to
the first area. I really want it to come up with a table that looks like
this

Effect              Coefficient        SE      F-value
Area number 
Year
Wind
Rain


Any help will be greatly appreciated as I have tried to understand how to
get this table but I am struggling to understand





--
View this message in context: http://r.789695.n4.nabble.com/Need-help-with-GLM-on-R-tp4712459.html
Sent from the R help mailing list archive at Nabble.com.


From dwinsemius at comcast.net  Fri Sep 18 20:06:06 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 18 Sep 2015 11:06:06 -0700
Subject: [R] best data storage format?
In-Reply-To: <43B4D0DB-F931-495A-9D0A-C8D46B35AFBA@comcast.net>
References: <E4422A2C-0480-4485-A256-F0C8BDE0C4AB@mac.com>
	<43B4D0DB-F931-495A-9D0A-C8D46B35AFBA@comcast.net>
Message-ID: <A278C306-E7D9-43D5-A493-CE3E01D37C99@comcast.net>


On Sep 18, 2015, at 10:30 AM, David Winsemius wrote:

> 
> On Sep 17, 2015, at 1:41 PM, Alfa Diallo wrote:
> 
>> Hello -
>> 
>> I?m working on dataset that will eventually be used in an xyz-plot.
>> 
>> I?m having trouble figuring out the best way to store the data (see an attached .csv sheet exported from Excel). Some information on the data:
>> 
>> - Columns B - F are labels that describe the z data points
>> - Rows above x and y data pairs show the corresponding labels for the data point.
>> 
>> I want to use R to visualize the data and appreciate any feedback on the best mechanism to store/organize the info. I?m new to R and want to start the data assembly on the right foot - thanks for the help and advice!
>> 
>> Alfa
> 
> 
> You later provided a link to a csv version of that dataset. I loaded it into an OpenOffice spreadsheet (because my purchased copy of Excel was no longer functional after a harddisk crash and restore from backup to a different disk and MS now claims to not have any record of it so refuses to let me re-register it to the new disk despite having all the original boxes and fancy security tags.) 
> 
> At any rate, I am attaching a png copy of the data in its original version and in a transposed version what I would suggest is closer to how one might store your data. Presenting pictures of data is almost always a bad idea on Rhelp, but this seems the best way to illustrate the current arrangement.

In the past I had success attaching .png formatted files but it appears to have failed in this instance. The mailing did get shunted to the moderation queue because of the size of the image file, but then got stripped despite being "accepted".  C'est la vie.


>  At the moment you have a mixture of columnar and row-wise storage. You should convert to all columnar. This will require copying the "z" values of interest to new columns after the transpose. You can probably do that in Excel easier than doing it in R. Then when you have an all-columnar arrangement, just use read.csv.


David Winsemius
Alameda, CA, USA


From murdoch.duncan at gmail.com  Fri Sep 18 20:13:20 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 18 Sep 2015 14:13:20 -0400
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <47A1882E-7549-4AE6-B4F6-139AF7078F81@univie.ac.at>
References: <FEB96270AED.000005E7jrkrideau@inbox.com>
	<CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
	<47A1882E-7549-4AE6-B4F6-139AF7078F81@univie.ac.at>
Message-ID: <55FC5440.8010804@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On 18/09/2015 11:13 AM, Erich Neuwirth wrote:
> Let me add a little bit here:
> 
> When using math formulas, one should know about the parsing rules
> form complex expression which do not have all the necessary
> parenthesis.
> 
> Different systems do have different parings rules. In the case of
> a^b^c, the expression is ambiguus because (as mentioned in a
> previous mail) in general (a^b)^c != a^(b^c) To avoid unintended
> consequences, just us parentheses and you will get the right
> result. in the case of -a^b The question is the order of precedence
> of unary - and binary ^.
> 
> In Excel, -2^2=4, but 0-2^2=-4
> 
> Reason: For Excel, unary - is stronger than the power operator, but
> binary minus is weaker.
> 
> My feeling is that too many people are bashing spreadsheets for the
> wrong reason. Spreadsheets ca do things R cannot do: Automatic
> recalculation when input changes,

Shiny is implemented in R and it has this.

> and visual point and click modelling of dependencies.

I'm not exactly sure what you mean here, but I think you're probably
right on this one, in that it is a user-interface issue.  People can
implement front-ends to R that do just about anything, but those
aren't part of R.

Duncan Murdoch

> The calculation engine of Excel admittedly has some weak points.
> 
> That is the reason why I wrote RExcel which gives you all the
> advantages of the spreadsheet interface and allows you to use the R
> calculation within this interface whenever needed.
> 
> 
> 
> 
> ______________________________________________ R-help at r-project.org
> mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and
> provide commented, minimal, self-contained, reproducible code.
> 

-----BEGIN PGP SIGNATURE-----
Version: GnuPG/MacGPG2 v2
Comment: GPGTools - https://gpgtools.org

iQEcBAEBCgAGBQJV/FRAAAoJEHE2Kz23YMZygz0H/2TZEQk2JOS8GQc5jvSXTk5+
qrgROnpgTTZwCX7rh0vPCDTGXSjAxgJikACYxGVWo/jkQKpoIkqcyrk7KnTXkPXG
fmvQ6gkJ8nq7UJ7la4alThtmrBdNhkz2xgvVn3aZIItRwRhgRT2P7f8whi/Ia5IN
CEpJyEg0ymrlVNMG8ncmAgAyebayeJDesJNPma7x7Hw0g00COQ9ZvD7U8joHgIGn
4Tic2nzlcK/ZItz3WQBcOdkqrVMCX5XcsZC6XOPMZyGPk+jIUA1qq7pQovMLWKeq
7JzH6xm6YeQ41QN4AbZyxmIZ8LvonT65MhQpS89hgGbROCZ49nUFquMcZn/Exj4=
=e1J7
-----END PGP SIGNATURE-----


From Charles.Burchill at umanitoba.ca  Fri Sep 18 21:00:59 2015
From: Charles.Burchill at umanitoba.ca (Charles Burchill)
Date: Fri, 18 Sep 2015 19:00:59 +0000
Subject: [R] Connecting R to SAS SPDS through ODBC - no results.
Message-ID: <DDE3476F4B10F042AE9A95AA2350890D06BEB3FF@UMCE3EXMD02.ad.umanitoba.ca>

We have recently setup our SAS SPDS data server for use with ODBC connections.

Unfortunately when sending a query to the data server I am getting no data returned.  It is not clear if this is an issue with SPDS or RODBC and I am hoping someone has explored this already.



I know the connection works since I can query data through MS Access/Excel, or STATA and have data returned.



odbcConnect(ch)  - works fine.

odbcGetInfo(ch) - returns the server information



sqlTables()  returns no table information

sqlQuery()  returns the file structure but no records.

sqlFetch()  doesn't find the table (although an sqlQuery obviously does since it returns structure information)



I am hoping that someone can send me an example of accessing SPDS through ODBC using R.



Thanks!





--
Charles Burchill, Associate Director Data Access and Use,
Manitoba Centre for Health Policy
College of Medicine, Faculty of Health Sciences
408-727 McDermot Ave, winnipeg, Manitoba, R3E 3P5
Tel: 204 789 3429  Fax: 204 789 3910
Charles.Burchill at umanitoba.ca<mailto:Charles.Burchill at med.umanitoba.ca>

	[[alternative HTML version deleted]]


From sakko.saul at gmx.de  Fri Sep 18 20:55:38 2015
From: sakko.saul at gmx.de (sakko)
Date: Fri, 18 Sep 2015 11:55:38 -0700 (PDT)
Subject: [R] How to replace strings in data frames by condition
Message-ID: <1442602538651-4712469.post@n4.nabble.com>

Hi,
this is my first post in this forum and I am new to Ra. I am desperately
seeking for help.

There is a data frame given in which I want to replace some values like
following:

Existing data frame:    
                   q_1_SQ009
1                              4
2                           <NA>
3                              3
4                              2
5   5: Stimme voll und ganz zu
6                              4
7                              4
8                              2
9   1: Stimme ?berhaupt nicht zu

Wanted data frame:
                       q_1_SQ009
1                              4
2                              0
3                              3
4                              2
5                              5
6                              4
7                              4
8                              2
9                              1

This means that every "<NA>", "5: Stimme voll und ganz zu" and "1: Stimme
?berhaupt nicht zu" shall be replaced by 0, 5, 1


I beg you all for any help.... please

Many thanks in advance



Robert





--
View this message in context: http://r.789695.n4.nabble.com/How-to-replace-strings-in-data-frames-by-condition-tp4712469.html
Sent from the R help mailing list archive at Nabble.com.


From emptican at gmail.com  Fri Sep 18 21:27:08 2015
From: emptican at gmail.com (SH)
Date: Fri, 18 Sep 2015 15:27:08 -0400
Subject: [R] R code help!
Message-ID: <CALSKosCcLNPX6jtSaJeJx+cxVt5PG9WxLui181uhjYPZDN89bA@mail.gmail.com>

Dear R users,

I am trying to simulate surveys and the survey result will be used to
determine the population to be "accepted" or "rejected".  With the results,
I would like to calculate cumulative means and plot them to see if a
converged value is as expected.  Below is R-code I generated.  I need a
help to repeat this simulation code as many times (e.g., 100) and keep the
results as list format if possible.  Could you give me any insight?

Thanks  a lot in advance,

Steve

sim.f <- function(p.s, N, sample.size, n.sim) {
pop = sampled.pop = decision = decisionB = cum.mn = as.list(NULL)
for(i in 1:n.sim) {
   p <- c(rep(1, p.s*N), pop2 <- rep(0, N*(1-p.s))) # Generate sample space
   pop[[i]] <- sample(p) # Randomization sample space
   sampled.pop[[i]] <- sample(pop[[i]], sample.size)# Random sampling
   decision[i] <- ifelse(sum(sampled.pop[[i]])>=1, 'Reject','Pass') #
Decision for each group of n.sim
   decisionB <- ifelse(decision == 'Reject', 1, 0) # Convert to binary
   cum.mn <- cumsum(decisionB) / seq_along(decisionB) # Cummulative mean of
n.sim group decisions
   }
result = list(population=pop,
  pop_sub = sampled.pop,
  decision = decision,
  decisionB = decisionB,
  cum.mn = cum.mn)
}
sim.out <- sim.f(p.s=.05, N=1000, sample.size=69, n.sim=500)
# I want to repeat this simulation function for example 100 times or and
also #keep the data so that I can explore later.  If it is not possible to
keep all #outputs, at least I would like to have cum.mn outputs.

summary(sim.out)
sim.out$population
sim.out$pop_sub
sim.out$decision
sim.out$decisionB
y1 <- sim.out$cum.mn
#plot(y1, type='l')
lines(y2, type='l')
...
lines(y100, type='l')
abline(h=.95, col='red')

	[[alternative HTML version deleted]]


From djnordlund at frontier.com  Fri Sep 18 21:42:21 2015
From: djnordlund at frontier.com (Daniel Nordlund)
Date: Fri, 18 Sep 2015 12:42:21 -0700
Subject: [R] Connecting R to SAS SPDS through ODBC - no results.
In-Reply-To: <DDE3476F4B10F042AE9A95AA2350890D06BEB3FF@UMCE3EXMD02.ad.umanitoba.ca>
References: <DDE3476F4B10F042AE9A95AA2350890D06BEB3FF@UMCE3EXMD02.ad.umanitoba.ca>
Message-ID: <55FC691D.9050001@frontier.com>

On 9/18/2015 12:00 PM, Charles Burchill wrote:
> We have recently setup our SAS SPDS data server for use with ODBC connections.
>
> Unfortunately when sending a query to the data server I am getting no data returned.  It is not clear if this is an issue with SPDS or RODBC and I am hoping someone has explored this already.
>
>
>
> I know the connection works since I can query data through MS Access/Excel, or STATA and have data returned.
>
>
>
> odbcConnect(ch)  - works fine.
>
> odbcGetInfo(ch) - returns the server information
>
>
>
> sqlTables()  returns no table information
>
> sqlQuery()  returns the file structure but no records.
>
> sqlFetch()  doesn't find the table (although an sqlQuery obviously does since it returns structure information)
>
>
>
> I am hoping that someone can send me an example of accessing SPDS through ODBC using R.
>
>
>
> Thanks!
>
>
>
>
>
> --
> Charles Burchill, Associate Director Data Access and Use,
> Manitoba Centre for Health Policy
> College of Medicine, Faculty of Health Sciences
> 408-727 McDermot Ave, winnipeg, Manitoba, R3E 3P5
> Tel: 204 789 3429  Fax: 204 789 3910
> Charles.Burchill at umanitoba.ca<mailto:Charles.Burchill at med.umanitoba.ca>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
I haven't worked with SPDS data sources, but I found that when 
connecting to "regular" SAS datasets I had to set believeNRows and 
colQuote to get the connection to work properly. For example I set up a 
tmp DSN and connected like this

ch <- odbcConnect('tmp', believeNRows=FALSE, colQuote=NULL)


Hope this is helpful,

Dan

-- 
Daniel Nordlund
Bothell, WA  USA


From catalinroibu at gmail.com  Fri Sep 18 22:13:51 2015
From: catalinroibu at gmail.com (catalin roibu)
Date: Fri, 18 Sep 2015 23:13:51 +0300
Subject: [R] idl code in R
Message-ID: <CAEW+BD+iK8rJ1TjJzLVPURLToJo=c_nqh3AP3hAUHtnL8uwrPQ@mail.gmail.com>

Dear all,

Is there a possibility to compile an IDL code in R? If yes please tell me
how to do that.

Thank you very much!

best regards!


CR

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Fri Sep 18 22:17:56 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 18 Sep 2015 22:17:56 +0200
Subject: [R] Spreadsheet math problem (exponentiation)
In-Reply-To: <CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
References: <FEB96270AED.000005E7jrkrideau@inbox.com>
	<CAAJSdjjQ3tm+Dw4_=njrgWOttAEM4Q3SQprWsZxvE1V755LX+Q@mail.gmail.com>
Message-ID: <35326791-0C69-4B0F-A4D0-1781EE1776B0@gmail.com>


> On 18 Sep 2015, at 16:31 , John McKown <john.archie.mckown at gmail.com> wrote:
> 
> ref: https://en.wikipedia.org/wiki/Order_of_operations
> <quote>
> 
> If exponentiation is indicated by stacked symbols, the usual rule is to
> work from the top down, thus:
> [image: a^{b^c} = a^{(b^c)}],
[snip]

...and it might be added that this is (in "paper math") because (a^b)^c==a^{bc} and there would be no point in writing a^b^c if you might as well have written a^{bc}. (The curly braces only being there to indicate that the exponent is bc.)

Of course this sort of consideration hasn't always had effect on programmers, so you do need to check.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From macqueen1 at llnl.gov  Sat Sep 19 00:11:46 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 18 Sep 2015 22:11:46 +0000
Subject: [R] idl code in R
In-Reply-To: <CAEW+BD+iK8rJ1TjJzLVPURLToJo=c_nqh3AP3hAUHtnL8uwrPQ@mail.gmail.com>
References: <CAEW+BD+iK8rJ1TjJzLVPURLToJo=c_nqh3AP3hAUHtnL8uwrPQ@mail.gmail.com>
Message-ID: <D221D42F.13904D%macqueen1@llnl.gov>

Compile?  No. R is not a compiled language.

Can an IDL script be translated into an R script? Very likely, depending
on what the IDL script is supposed to do.

Can I tell you how? No. I don't know IDL. You need to find someone who
knows both languages.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 9/18/15, 1:13 PM, "R-help on behalf of catalin roibu"
<r-help-bounces at r-project.org on behalf of catalinroibu at gmail.com> wrote:

>Dear all,
>
>Is there a possibility to compile an IDL code in R? If yes please tell me
>how to do that.
>
>Thank you very much!
>
>best regards!
>
>
>CR
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From alfadiallo at mac.com  Sat Sep 19 00:29:24 2015
From: alfadiallo at mac.com (Alfa Diallo)
Date: Fri, 18 Sep 2015 18:29:24 -0400
Subject: [R] best data storage format?
In-Reply-To: <43B4D0DB-F931-495A-9D0A-C8D46B35AFBA@comcast.net>
References: <E4422A2C-0480-4485-A256-F0C8BDE0C4AB@mac.com>
	<43B4D0DB-F931-495A-9D0A-C8D46B35AFBA@comcast.net>
Message-ID: <A6386A39-BE43-40F8-9C86-B413009A60B9@mac.com>

Hello David and Co. -

I got the screen shot!

I also downloaded OpenOffice and amended the data based on your recs - here it is:
http://www.alfadiallo.com/r/r_sample_20150918.ods

Would this be the best course of action? 
Will R be able to simultaneously plot the shown data despite variability in the number of rows for each xyz grouping?

Thanks!

Alfa





> On Sep 18, 2015, at 1:30 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
> On Sep 17, 2015, at 1:41 PM, Alfa Diallo wrote:
> 
>> Hello -
>> 
>> I?m working on dataset that will eventually be used in an xyz-plot.
>> 
>> I?m having trouble figuring out the best way to store the data (see an attached .csv sheet exported from Excel). Some information on the data:
>> 
>> - Columns B - F are labels that describe the z data points
>> - Rows above x and y data pairs show the corresponding labels for the data point.
>> 
>> I want to use R to visualize the data and appreciate any feedback on the best mechanism to store/organize the info. I?m new to R and want to start the data assembly on the right foot - thanks for the help and advice!
>> 
>> Alfa
> 
> 
> You later provided a link to a csv version of that dataset. I loaded it into an OpenOffice spreadsheet (because my purchased copy of Excel was no longer functional after a harddisk crash and restore from backup to a different disk and MS now claims to not have any record of it so refuses to let me re-register it to the new disk despite having all the original boxes and fancy security tags.) 
> 
> At any rate, I am attaching a png copy of the data in its original version and in a transposed version what I would suggest is closer to how one might store your data. Presenting pictures of data is almost always a bad idea on Rhelp, but this seems the best way to illustrate the current arangement.  At the moment you have a mixture of columnar and row-wise storage. You should convert to all columnar. This will require copying the "z" values of interest to new columns after the transpose. You can probably do that in Excel easier than doing it in R. Then when you have an all-columnar arrangement, just use read.csv.
> 
> <sprdsht.transpose.png>
> 
> 
> David Winsemius
> Alameda, CA, USA
> 


From drjimlemon at gmail.com  Sat Sep 19 00:52:37 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 19 Sep 2015 08:52:37 +1000
Subject: [R] Need help with GLM on R
In-Reply-To: <1442592215416-4712459.post@n4.nabble.com>
References: <1442592215416-4712459.post@n4.nabble.com>
Message-ID: <CA+8X3fXh5rsuz3+GT4NMYHAViC+xZ=4Q3cckifxxe2wUYASexA@mail.gmail.com>

Hi icelandic1992,
If I am correct, the default model is comparing the first area with the
rest. I assume that "area" does not refer to the physical area of the
locations, but is a nominal order variable. If I am wrong, and there are
actual "areas", that is probably what you want to have in the model, as one
would expect that larger areas would have larger populations.

Jim


On Sat, Sep 19, 2015 at 2:03 AM, icelandic1992 <georgialclack92 at outlook.com>
wrote:

> I am using some data for a population count
> This what my data is set out as
> Area   Count    Year      DOY      Rain     Wind
>
> all continuous effects except wind and rain and area are categorical
> variables with 1 for rain and wind and 0 for no rain or wind and areas 1-27
>
> I am trying to do a GLM analysis of certain treatment effects on Loge(total
> count+1) to find out which factors significantly affect the count. I used
> the formula glm.nb(Count~factor(Area)+Year+Rain+Wind+DOY)
> However it does not work, and when I looked at it with the formula I was
> given by someone else which was
> glm.nb(Count~factor(Area)+Year+DOY+Year*DOY) it came up with intercept as
> the first area and then areas 1-27 listed and then year and DOY.
>
> I am unsure of what this means as i think it is comparing all the areas to
> the first area. I really want it to come up with a table that looks like
> this
>
> Effect              Coefficient        SE      F-value
> Area number
> Year
> Wind
> Rain
>
>
> Any help will be greatly appreciated as I have tried to understand how to
> get this table but I am struggling to understand
>
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Need-help-with-GLM-on-R-tp4712459.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sat Sep 19 01:34:49 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 19 Sep 2015 09:34:49 +1000
Subject: [R] How to replace strings in data frames by condition
In-Reply-To: <1442602538651-4712469.post@n4.nabble.com>
References: <1442602538651-4712469.post@n4.nabble.com>
Message-ID: <CA+8X3fVctDr485tcE7yThn-nGrGK+SmoLW9-bBKtay658PzY2Q@mail.gmail.com>

Hi Robert,
In your example, all of the numeric values are single digit. If this is the
case in the real data, I think this will do:

# assume edf is the name of the data frame
levels(edf$q_1_SQ009)<-c(levels(edf$q_1_SQ009),"0")
edf$q_1_SQ009[is.na(edf$q_1_SQ009)]<-"0"
get_first_char<-function(x) return(unlist(strsplit(as.character(x),""))[1])
edf$q_1_SQ009<-sapply(sapply(edf$q_1_SQ009,get_first_char),as.numeric)

Jim


On Sat, Sep 19, 2015 at 4:55 AM, sakko <sakko.saul at gmx.de> wrote:

> Hi,
> this is my first post in this forum and I am new to Ra. I am desperately
> seeking for help.
>
> There is a data frame given in which I want to replace some values like
> following:
>
> Existing data frame:
>                    q_1_SQ009
> 1                              4
> 2                           <NA>
> 3                              3
> 4                              2
> 5   5: Stimme voll und ganz zu
> 6                              4
> 7                              4
> 8                              2
> 9   1: Stimme ?berhaupt nicht zu
>
> Wanted data frame:
>                        q_1_SQ009
> 1                              4
> 2                              0
> 3                              3
> 4                              2
> 5                              5
> 6                              4
> 7                              4
> 8                              2
> 9                              1
>
> This means that every "<NA>", "5: Stimme voll und ganz zu" and "1: Stimme
> ?berhaupt nicht zu" shall be replaced by 0, 5, 1
>
>
> I beg you all for any help.... please
>
> Many thanks in advance
>
>
>
> Robert
>
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/How-to-replace-strings-in-data-frames-by-condition-tp4712469.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From mashranga at yahoo.com  Sat Sep 19 04:44:24 2015
From: mashranga at yahoo.com (Mohammad Tanvir Ahamed)
Date: Sat, 19 Sep 2015 02:44:24 +0000 (UTC)
Subject: [R] Help with Package creation
Message-ID: <1889360163.58293.1442630664341.JavaMail.yahoo@mail.yahoo.com>

Hello ,
I am new in R package built.?
I want to install one package (randomForest) that will install automatically while my package(myPack) will install.?I am using R studio to built.?
My Description file is :?
Package: myPackType: PackageTitle: What the Package Does (Title Case)Version: 0.1Date: 2015-09-19Author: Who wrote itMaintainer: Who to complain to <yourfault at somewhere.net>Description: More about what it does (maybe more than one line)License: What license is it under?Depends: RImports: randomForestLazyData: TRUE?------------------------------------------------------------------------------------But it shows me a error ...
Updating myPack documentationLoading myPackError in (function (dep_name, dep_ver = NA, dep_compare = NA) ?:?? Dependency package randomForest not available.Calls: suppressPackageStartupMessages ... <Anonymous> -> load_all -> load_imports -> mapply -> <Anonymous>Execution halted
Exited with status 1.------------------------------------------------------------------------------------
Any help regarding this problem will appreciated .?Thank you .?
?Tanvir Ahamed 
   G?teborg, Sweden     |?
	[[alternative HTML version deleted]]


From fanjianling at gmail.com  Sat Sep 19 06:54:54 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Sat, 19 Sep 2015 12:54:54 +0800
Subject: [R] How to coerce a parameter in nls?
Message-ID: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>

Hello, everyone,

I am using a nls regression with 6 groups data. I am trying to coerce
a parameter to 1 by using a upper and lower statement. but I always
get an error like below:

Error in ifelse(internalPars < upper, 1, -1) :
  (list) object cannot be coerced to type 'double'

does anyone know how to fix it?

thanks in advance!

My code is below:



> dproot
   depth       den ref
1     20 0.5730000   1
2     40 0.7800000   1
3     60 0.9470000   1
4     80 0.9900000   1
5    100 1.0000000   1
6     10 0.6000000   2
7     20 0.8200000   2
8     30 0.9300000   2
9     40 1.0000000   2
10    20 0.4800000   3
11    40 0.7340000   3
12    60 0.9610000   3
13    80 0.9980000   3
14   100 1.0000000   3
15    20 3.2083491   4
16    40 4.9683383   4
17    60 6.2381133   4
18    80 6.5322348   4
19   100 6.5780660   4
20   120 6.6032064   4
21    20 0.6140000   5
22    40 0.8270000   5
23    60 0.9500000   5
24    80 0.9950000   5
25   100 1.0000000   5
26    20 0.4345774   6
27    40 0.6654726   6
28    60 0.8480684   6
29    80 0.9268951   6
30   100 0.9723207   6
31   120 0.9939966   6
32   140 0.9992400   6

> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
+ start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
> summary(fitdp)

Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)

Parameters:
    Estimate Std. Error t value Pr(>|t|)
Rm1  1.12560    0.07156   15.73 3.84e-14 ***
Rm2  1.57643    0.11722   13.45 1.14e-12 ***
Rm3  1.10697    0.07130   15.53 5.11e-14 ***
Rm4  7.23925    0.20788   34.83  < 2e-16 ***
Rm5  1.14516    0.07184   15.94 2.87e-14 ***
Rm6  1.03658    0.05664   18.30 1.33e-15 ***
d50 22.69426    1.03855   21.85  < 2e-16 ***
c   -1.59796    0.15589  -10.25 3.02e-10 ***
---
Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1

Residual standard error: 0.1094 on 24 degrees of freedom

Number of iterations to convergence: 8
Achieved convergence tolerance: 9.374e-06

> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot, algorithm="port",
+ start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
+ lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
+ upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))

Error in ifelse(internalPars < upper, 1, -1) :
  (list) object cannot be coerced to type 'double'


From arne.henningsen at gmail.com  Sat Sep 19 07:06:47 2015
From: arne.henningsen at gmail.com (Arne Henningsen)
Date: Sat, 19 Sep 2015 07:06:47 +0200
Subject: [R] Estimating Endogenous Selection Model
In-Reply-To: <003001d0f1d4$f1fa7c30$d5ef7490$@econ.uni-frankfurt.de>
References: <003001d0f1d4$f1fa7c30$d5ef7490$@econ.uni-frankfurt.de>
Message-ID: <CAMTWbJi+DM5BsPnrTnbnyKttYsc9a3dnckS4oV71nHTn-o6k+g@mail.gmail.com>

Dear Johannes

You can use function selection() of the sampleSelection package and
estimate an endogenous switching regression ("tobit-5") model, where
you have the selection equation to model investmentment vs.
non-investment and you have two outcome equations, one for firms that
invest and the other for firms that do not invest.

Best regards,
Arne



On 18 September 2015 at 07:43, Johannes Muck <Muck at econ.uni-frankfurt.de> wrote:
> Dear all,
>
> I want to estimate a model in which individuals self-select into two
> different actions (e.g. invest or not invest). Moreover, the factors that
> influence the selection decision also affect the ultimate outcome variable
> (e.g. return on investment). That is, I want to estimate a model with
> endogenous selection.
>
> My question is: Which R package supports this kind of estimation?
> To me it seems like the package sampleSelection
> (https://cran.r-project.org/web/packages/sampleSelection/sampleSelection.pdf
> ) can only be used in case of a SAMPLE selection, i.e. in the example above:
> only individuals that invest are observed. Yet in my case, both actions are
> observed but the decision which action to choose is endogenous. Or does this
> package also support self-selection models?
>
> Thank you very much in advance.
>
> Best,
>
> Johannes
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Arne Henningsen
http://www.arne-henningsen.name


From jdnewmil at dcn.davis.CA.us  Sat Sep 19 07:33:18 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 18 Sep 2015 22:33:18 -0700
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
Message-ID: <7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>

Why not rewrite the function so that value is not a parameter?
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 18, 2015 9:54:54 PM PDT, Jianling Fan <fanjianling at gmail.com> wrote:
>Hello, everyone,
>
>I am using a nls regression with 6 groups data. I am trying to coerce
>a parameter to 1 by using a upper and lower statement. but I always
>get an error like below:
>
>Error in ifelse(internalPars < upper, 1, -1) :
>  (list) object cannot be coerced to type 'double'
>
>does anyone know how to fix it?
>
>thanks in advance!
>
>My code is below:
>
>
>
>> dproot
>   depth       den ref
>1     20 0.5730000   1
>2     40 0.7800000   1
>3     60 0.9470000   1
>4     80 0.9900000   1
>5    100 1.0000000   1
>6     10 0.6000000   2
>7     20 0.8200000   2
>8     30 0.9300000   2
>9     40 1.0000000   2
>10    20 0.4800000   3
>11    40 0.7340000   3
>12    60 0.9610000   3
>13    80 0.9980000   3
>14   100 1.0000000   3
>15    20 3.2083491   4
>16    40 4.9683383   4
>17    60 6.2381133   4
>18    80 6.5322348   4
>19   100 6.5780660   4
>20   120 6.6032064   4
>21    20 0.6140000   5
>22    40 0.8270000   5
>23    60 0.9500000   5
>24    80 0.9950000   5
>25   100 1.0000000   5
>26    20 0.4345774   6
>27    40 0.6654726   6
>28    60 0.8480684   6
>29    80 0.9268951   6
>30   100 0.9723207   6
>31   120 0.9939966   6
>32   140 0.9992400   6
>
>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>+ start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>> summary(fitdp)
>
>Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>
>Parameters:
>    Estimate Std. Error t value Pr(>|t|)
>Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>d50 22.69426    1.03855   21.85  < 2e-16 ***
>c   -1.59796    0.15589  -10.25 3.02e-10 ***
>---
>Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>
>Residual standard error: 0.1094 on 24 degrees of freedom
>
>Number of iterations to convergence: 8
>Achieved convergence tolerance: 9.374e-06
>
>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>algorithm="port",
>+ start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>+ lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>+ upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>
>Error in ifelse(internalPars < upper, 1, -1) :
>  (list) object cannot be coerced to type 'double'
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Sat Sep 19 07:39:28 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 18 Sep 2015 22:39:28 -0700
Subject: [R] Help with Package creation
In-Reply-To: <1889360163.58293.1442630664341.JavaMail.yahoo@mail.yahoo.com>
References: <1889360163.58293.1442630664341.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <C5A70A72-18A0-4171-A3D8-B3220520355D@dcn.davis.CA.us>

Your HTML formatted email is almost unreadable, but it looks like you need to read some description files of other packages to get an idea how it is supposed to be filled out. In particular pay attention to the Depends field. Also, there is a very detailed "Writing R Extensions" manual that comes with R. There is also a mailing list dedicated to helping with making packages (R-package-devel).


---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 18, 2015 7:44:24 PM PDT, Mohammad Tanvir Ahamed via R-help <r-help at r-project.org> wrote:
>Hello ,
>I am new in R package built.?
>I want to install one package (randomForest) that will install
>automatically while my package(myPack) will install.?I am using R
>studio to built.?
>My Description file is :?
>Package: myPackType: PackageTitle: What the Package Does (Title
>Case)Version: 0.1Date: 2015-09-19Author: Who wrote itMaintainer: Who to
>complain to <yourfault at somewhere.net>Description: More about what it
>does (maybe more than one line)License: What license is it
>under?Depends: RImports: randomForestLazyData:
>TRUE?------------------------------------------------------------------------------------But
>it shows me a error ...
>Updating myPack documentationLoading myPackError in (function
>(dep_name, dep_ver = NA, dep_compare = NA) ?:?? Dependency package
>randomForest not available.Calls: suppressPackageStartupMessages ...
><Anonymous> -> load_all -> load_imports -> mapply ->
><Anonymous>Execution halted
>Exited with status
>1.------------------------------------------------------------------------------------
>Any help regarding this problem will appreciated .?Thank you .?
>?Tanvir Ahamed 
>   G?teborg, Sweden     |?
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From patrick-breheny at uiowa.edu  Fri Sep 18 22:43:21 2015
From: patrick-breheny at uiowa.edu (Patrick Breheny)
Date: Fri, 18 Sep 2015 15:43:21 -0500
Subject: [R] ASA Statistical Computing & Graphics Student Paper Competition
	2016
Message-ID: <55FC7769.3000001@uiowa.edu>

Statistical Computing and Statistical Graphics Sections
American Statistical Association

Student Paper Competition 2016

The Statistical Computing and Statistical Graphics Sections of the ASA
are co-sponsoring a student paper competition on the topics of
Statistical Computing and Statistical Graphics.  Students are
encouraged to submit a paper in one of these areas, which might be
original methodological research, some novel computing or graphical
application in statistics, or any other suitable contribution (for
example, a software-related project).  The selected winners will
present their papers in a topic-contributed session at the 2016 Joint
Statistical Meetings and will receive a cash prize of $1,000.

Anyone who is a student (graduate or undergraduate) on or after
September 1, 2015 is eligible to participate.  An entry must include
an abstract, a six page manuscript (including figures, tables and
references), blinded versions of the abstract and manuscript (with no
authors and no references that easily lead to identifying the
authors), a C.V., and a letter from a faculty member familiar with the
student's work.  The applicant must be the first author of the paper.
The faculty letter must include a verification of the applicant's
student status and, in the case of joint authorship, should indicate
what fraction of the contribution is attributable to the applicant.
We prefer that electronic submissions of papers be in PDF.  All
materials must be in English.

Students may submit papers to no more than two sections and may accept
only one section's award. Students must inform both sections applied
to when he or she wins and accepts an award, thereby removing the
student from the award competition for the second section.

All application materials must be received by 5:00 PM EST, Tuesday,
December 15, 2015 and should be sent to the e-mail address below.  They
will be reviewed by the Student Paper Competition Award committee of the 
Statistical Computing and Graphics Sections.  The selection criteria 
used by the committee will include innovation and significance of the 
contribution as well as the professional quality of the manuscript. 
Award announcements will be made by January 15th, 2016.

Additional information on the competition can be accessed at the ASA's 
Student Paper Competition/Travel Awards page:
http://www.amstat.org/sections/studentpaperawards.cfm
as well as the website of the Statistical Computing Section:
http://stat-computing.org/awards/
Inquiries and application materials should be emailed to:

Student Paper Competition
Patrick Breheny
patrick-breheny at uiowa.edu

-- 
Patrick Breheny
Assistant Professor
Department of Biostatistics
University of Iowa
N336 College of Public Health Building
319-384-1584


From ps629 at medschl.cam.ac.uk  Sat Sep 19 02:47:42 2015
From: ps629 at medschl.cam.ac.uk (Praveen Surendran)
Date: Sat, 19 Sep 2015 00:47:42 +0000
Subject: [R] Issue with results from 'summary' function in R
In-Reply-To: <CAJuCY5xs6CNGVVjrSYPY7d=a2krFKEpEBCXpyN5CNEkvk1CKEQ@mail.gmail.com>
References: <C35F8007418891489CF1ED67E9DE3874BA4F3150@me-mbx4.medschl.cam.ac.uk>
	<CAJuCY5xs6CNGVVjrSYPY7d=a2krFKEpEBCXpyN5CNEkvk1CKEQ@mail.gmail.com>
Message-ID: <D2226F09.13238%ps629@medschl.cam.ac.uk>

Hi Thierry,

Thank you for the response. I should have looked at the help page.

Kind Regards,
Praveen.

On 18/09/2015 15:51, "Thierry Onkelinx" <thierry.onkelinx at inbo.be> wrote:

>This is described in ?summary
>
>> x <- 22072
>> getOption("digits")
>[1] 7
>> summary(x)
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  22070   22070   22070   22070   22070   22070
>> options(digits = 10)
>> summary(x)
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  22072   22072   22072   22072   22072   22072
>ir. Thierry Onkelinx
>Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>and Forest
>team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>Kliniekstraat 25
>1070 Anderlecht
>Belgium
>
>To call in the statistician after the experiment is done may be no
>more than asking him to perform a post-mortem examination: he may be
>able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>The plural of anecdote is not data. ~ Roger Brinner
>The combination of some data and an aching desire for an answer does
>not ensure that a reasonable answer can be extracted from a given body
>of data. ~ John Tukey
>
>
>2015-09-18 14:08 GMT+02:00 Praveen Surendran <ps629 at medschl.cam.ac.uk>:
>> Hi all,
>>
>> Attached table (that contains summary for a genetic association study)
>>was read using the command:
>>
>> test <- read.table('testDat.txt',header=FALSE,stringsAsFactors=FALSE)
>>
>> Results from the summary of the attached table is provided below:
>>
>>> summary(test$V5)
>>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>   22070   22070   22070   22070   22070   22070
>>
>> As we can see column 5 of this table contains only one value - 22072
>> I am confused as to why I am getting a value 22070 in the summary of
>>this column.
>>
>> I tested this using versions of R including - R version 3.2.1
>>(2015-06-18) -- "World-Famous Astronaut"
>>
>> Thank you for looking at this issue.
>> Kind Regards,
>>
>> Praveen.
>>
>>
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From sarah.goslee at gmail.com  Sat Sep 19 11:10:02 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Sat, 19 Sep 2015 05:10:02 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
Message-ID: <CAM_vjukEhHWMVY-24scPpuvBNQ-z960fRZPG7=s93yHE1B4gGA@mail.gmail.com>

You need to read the help more closely. start should be a list, as you've
done, but upper and lower should be vectors instead. Which is exactly what
your error message is telling you.

lower, upper

vectors of lower and upper bounds, replicated to be as long as start. If
unspecified, all parameters are assumed to be unconstrained. Bounds can
only be used with the "port" algorithm. They are ignored, with a warning,
if given for other algorithms.
Sarah

On Saturday, September 19, 2015, Jianling Fan <fanjianling at gmail.com> wrote:

> Hello, everyone,
>
> I am using a nls regression with 6 groups data. I am trying to coerce
> a parameter to 1 by using a upper and lower statement. but I always
> get an error like below:
>
> Error in ifelse(internalPars < upper, 1, -1) :
>   (list) object cannot be coerced to type 'double'
>
> does anyone know how to fix it?
>
> thanks in advance!
>
> My code is below:
>
>
>
> > dproot
>    depth       den ref
> 1     20 0.5730000   1
> 2     40 0.7800000   1
> 3     60 0.9470000   1
> 4     80 0.9900000   1
> 5    100 1.0000000   1
> 6     10 0.6000000   2
> 7     20 0.8200000   2
> 8     30 0.9300000   2
> 9     40 1.0000000   2
> 10    20 0.4800000   3
> 11    40 0.7340000   3
> 12    60 0.9610000   3
> 13    80 0.9980000   3
> 14   100 1.0000000   3
> 15    20 3.2083491   4
> 16    40 4.9683383   4
> 17    60 6.2381133   4
> 18    80 6.5322348   4
> 19   100 6.5780660   4
> 20   120 6.6032064   4
> 21    20 0.6140000   5
> 22    40 0.8270000   5
> 23    60 0.9500000   5
> 24    80 0.9950000   5
> 25   100 1.0000000   5
> 26    20 0.4345774   6
> 27    40 0.6654726   6
> 28    60 0.8480684   6
> 29    80 0.9268951   6
> 30   100 0.9723207   6
> 31   120 0.9939966   6
> 32   140 0.9992400   6
>
> > fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
> > summary(fitdp)
>
> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>
> Parameters:
>     Estimate Std. Error t value Pr(>|t|)
> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
> d50 22.69426    1.03855   21.85  < 2e-16 ***
> c   -1.59796    0.15589  -10.25 3.02e-10 ***
> ---
> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>
> Residual standard error: 0.1094 on 24 degrees of freedom
>
> Number of iterations to convergence: 8
> Achieved convergence tolerance: 9.374e-06
>
> > fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot, algorithm="port",
> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>
> Error in ifelse(internalPars < upper, 1, -1) :
>   (list) object cannot be coerced to type 'double'
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Sarah Goslee
http://www.stringpage.com
http://www.sarahgoslee.com
http://www.functionaldiversity.org

	[[alternative HTML version deleted]]


From profjcnash at gmail.com  Sat Sep 19 11:25:39 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Sat, 19 Sep 2015 05:25:39 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAM_vjukEhHWMVY-24scPpuvBNQ-z960fRZPG7=s93yHE1B4gGA@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<CAM_vjukEhHWMVY-24scPpuvBNQ-z960fRZPG7=s93yHE1B4gGA@mail.gmail.com>
Message-ID: <55FD2A12.5070005@gmail.com>

Besides this, using bounds to fix (also called "mask") parameters is 
generally a very bad idea. Some optimization methods allow this 
explicitly. For nonlinear least squares nlmrt package has it, but I'm 
not sure I fully documented the process. For optimization, Rvmmin and 
Rcgmin both allow masks, but again the documentation is not fully 
developed. There is a section in my 2014 book Nonlinear Parameter 
Optimization in R. I'm on vacation, and don't have the page refs. at 
this moment, however.

I know there are some other packages that include the possibility of 
fixed parameters, and perhaps others could give pointers, as I don't 
have the references with me.

JN

On 15-09-19 05:10 AM, Sarah Goslee wrote:
> You need to read the help more closely. start should be a list, as you've
> done, but upper and lower should be vectors instead. Which is exactly what
> your error message is telling you.
>
> lower, upper
>
> vectors of lower and upper bounds, replicated to be as long as start. If
> unspecified, all parameters are assumed to be unconstrained. Bounds can
> only be used with the "port" algorithm. They are ignored, with a warning,
> if given for other algorithms.
> Sarah
>
> On Saturday, September 19, 2015, Jianling Fan <fanjianling at gmail.com> wrote:
>
>> Hello, everyone,
>>
>> I am using a nls regression with 6 groups data. I am trying to coerce
>> a parameter to 1 by using a upper and lower statement. but I always
>> get an error like below:
>>
>> Error in ifelse(internalPars < upper, 1, -1) :
>>    (list) object cannot be coerced to type 'double'
>>
>> does anyone know how to fix it?
>>
>> thanks in advance!
>>
>> My code is below:
>>
>>
>>
>>> dproot
>>     depth       den ref
>> 1     20 0.5730000   1
>> 2     40 0.7800000   1
>> 3     60 0.9470000   1
>> 4     80 0.9900000   1
>> 5    100 1.0000000   1
>> 6     10 0.6000000   2
>> 7     20 0.8200000   2
>> 8     30 0.9300000   2
>> 9     40 1.0000000   2
>> 10    20 0.4800000   3
>> 11    40 0.7340000   3
>> 12    60 0.9610000   3
>> 13    80 0.9980000   3
>> 14   100 1.0000000   3
>> 15    20 3.2083491   4
>> 16    40 4.9683383   4
>> 17    60 6.2381133   4
>> 18    80 6.5322348   4
>> 19   100 6.5780660   4
>> 20   120 6.6032064   4
>> 21    20 0.6140000   5
>> 22    40 0.8270000   5
>> 23    60 0.9500000   5
>> 24    80 0.9950000   5
>> 25   100 1.0000000   5
>> 26    20 0.4345774   6
>> 27    40 0.6654726   6
>> 28    60 0.8480684   6
>> 29    80 0.9268951   6
>> 30   100 0.9723207   6
>> 31   120 0.9939966   6
>> 32   140 0.9992400   6
>>
>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>>> summary(fitdp)
>>
>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>>
>> Parameters:
>>      Estimate Std. Error t value Pr(>|t|)
>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>> ---
>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>>
>> Residual standard error: 0.1094 on 24 degrees of freedom
>>
>> Number of iterations to convergence: 8
>> Achieved convergence tolerance: 9.374e-06
>>
>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot, algorithm="port",
>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>>
>> Error in ifelse(internalPars < upper, 1, -1) :
>>    (list) object cannot be coerced to type 'double'
>>
>> ______________________________________________
>> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
>> more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


From jvadams at usgs.gov  Sat Sep 19 19:02:26 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Sat, 19 Sep 2015 12:02:26 -0500
Subject: [R] R code help!
In-Reply-To: <CALSKosCcLNPX6jtSaJeJx+cxVt5PG9WxLui181uhjYPZDN89bA@mail.gmail.com>
References: <CALSKosCcLNPX6jtSaJeJx+cxVt5PG9WxLui181uhjYPZDN89bA@mail.gmail.com>
Message-ID: <CAN5YmCH_D0w1-+sCWDPahqgEUCOLQTXq+rNGfw9CuoYgvk71-A@mail.gmail.com>

Here's one way to save your results, using a list of lists and a for() loop.

nsim <- 100
outputs <- vector("list", nsim)
for(i in 1:nsim) {
  outputs[[i]] <- sim.f(p.s=.05, N=1000, sample.size=69, n.sim=500)
}

Jean

On Fri, Sep 18, 2015 at 2:27 PM, SH <emptican at gmail.com> wrote:

> Dear R users,
>
> I am trying to simulate surveys and the survey result will be used to
> determine the population to be "accepted" or "rejected".  With the results,
> I would like to calculate cumulative means and plot them to see if a
> converged value is as expected.  Below is R-code I generated.  I need a
> help to repeat this simulation code as many times (e.g., 100) and keep the
> results as list format if possible.  Could you give me any insight?
>
> Thanks  a lot in advance,
>
> Steve
>
> sim.f <- function(p.s, N, sample.size, n.sim) {
> pop = sampled.pop = decision = decisionB = cum.mn = as.list(NULL)
> for(i in 1:n.sim) {
>    p <- c(rep(1, p.s*N), pop2 <- rep(0, N*(1-p.s))) # Generate sample space
>    pop[[i]] <- sample(p) # Randomization sample space
>    sampled.pop[[i]] <- sample(pop[[i]], sample.size)# Random sampling
>    decision[i] <- ifelse(sum(sampled.pop[[i]])>=1, 'Reject','Pass') #
> Decision for each group of n.sim
>    decisionB <- ifelse(decision == 'Reject', 1, 0) # Convert to binary
>    cum.mn <- cumsum(decisionB) / seq_along(decisionB) # Cummulative mean
> of
> n.sim group decisions
>    }
> result = list(population=pop,
>   pop_sub = sampled.pop,
>   decision = decision,
>   decisionB = decisionB,
>   cum.mn = cum.mn)
> }
> sim.out <- sim.f(p.s=.05, N=1000, sample.size=69, n.sim=500)
> # I want to repeat this simulation function for example 100 times or and
> also #keep the data so that I can explore later.  If it is not possible to
> keep all #outputs, at least I would like to have cum.mn outputs.
>
> summary(sim.out)
> sim.out$population
> sim.out$pop_sub
> sim.out$decision
> sim.out$decisionB
> y1 <- sim.out$cum.mn
> #plot(y1, type='l')
> lines(y2, type='l')
> ...
> lines(y100, type='l')
> abline(h=.95, col='red')
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From roger.bivand at nhh.no  Sat Sep 19 19:48:36 2015
From: roger.bivand at nhh.no (Roger Bivand)
Date: Sat, 19 Sep 2015 17:48:36 +0000
Subject: [R]
	=?utf-8?q?Function_stslshac_=7Bsphet=7D=3A_heteroskedasticity?=
	=?utf-8?q?_and_autocorrelation_consistent_=28HAC=29_estimator?=
References: <CAHsnuZM7Bjg9UKVaR5w0nhs+mGE-twwvrmHr0pwHnUequdLJLQ@mail.gmail.com>
Message-ID: <loom.20150919T194439-473@post.gmane.org>

monika nov <monika.novac <at> gmail.com> writes:

> 
> Dear R-users,
> 
> I have quite basic question for econometricians, however I would like to be
> sure in this.
> 
> If I use a HAC estimator of the variance-covariance (VC) matrix for a
> spatial econometric model, do I still need to test the residuals for
> spatial autocorrelation and heteroscedasticity? (in particular I am using
> function stslshac available in package sphet. The estimator is based on
> Kelejian, H.H. and Prucha, I.R. (2007) HAC estimation in a spatial
> framework, Journal of Econometrics, 140, pages 131?154).
> 

Please consider posting on R-sig-geo, since your question concerns spatial
regression. Roughly, you might mean that if you use a model with
semiparametric fitting of HAC, how might you check that it actually worked,
but your meaning isn't obvious. If you include an example using a built-in
data set, then your intentions would be clearer.

...
> I would be grateful for any reaction.
> 
> Monika
> 
PS. Please post plain text, not HTML

Roger Bivand

From Muck at econ.uni-frankfurt.de  Sat Sep 19 07:52:30 2015
From: Muck at econ.uni-frankfurt.de (Johannes Muck)
Date: Sat, 19 Sep 2015 07:52:30 +0200
Subject: [R] Estimating Endogenous Selection Model
In-Reply-To: <CAMTWbJi+DM5BsPnrTnbnyKttYsc9a3dnckS4oV71nHTn-o6k+g@mail.gmail.com>
References: <003001d0f1d4$f1fa7c30$d5ef7490$@econ.uni-frankfurt.de>
	<CAMTWbJi+DM5BsPnrTnbnyKttYsc9a3dnckS4oV71nHTn-o6k+g@mail.gmail.com>
Message-ID: <000a01d0f29f$5f7661a0$1e6324e0$@econ.uni-frankfurt.de>

Dear Arne,

thank you very much!

Best,

Johannes

-----Urspr?ngliche Nachricht-----
Von: Arne Henningsen [mailto:arne.henningsen at gmail.com] 
Gesendet: Samstag, 19. September 2015 07:07
An: Johannes Muck <Muck at econ.uni-frankfurt.de>
Cc: r-help at r-project.org
Betreff: Re: [R] Estimating Endogenous Selection Model

Dear Johannes

You can use function selection() of the sampleSelection package and estimate an endogenous switching regression ("tobit-5") model, where you have the selection equation to model investmentment vs.
non-investment and you have two outcome equations, one for firms that invest and the other for firms that do not invest.

Best regards,
Arne



On 18 September 2015 at 07:43, Johannes Muck <Muck at econ.uni-frankfurt.de> wrote:
> Dear all,
>
> I want to estimate a model in which individuals self-select into two 
> different actions (e.g. invest or not invest). Moreover, the factors 
> that influence the selection decision also affect the ultimate outcome 
> variable (e.g. return on investment). That is, I want to estimate a 
> model with endogenous selection.
>
> My question is: Which R package supports this kind of estimation?
> To me it seems like the package sampleSelection 
> (https://cran.r-project.org/web/packages/sampleSelection/sampleSelecti
> on.pdf
> ) can only be used in case of a SAMPLE selection, i.e. in the example above:
> only individuals that invest are observed. Yet in my case, both 
> actions are observed but the decision which action to choose is 
> endogenous. Or does this package also support self-selection models?
>
> Thank you very much in advance.
>
> Best,
>
> Johannes
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Arne Henningsen
http://www.arne-henningsen.name


From sakko.saul at gmx.de  Sat Sep 19 11:41:47 2015
From: sakko.saul at gmx.de (sakko)
Date: Sat, 19 Sep 2015 02:41:47 -0700 (PDT)
Subject: [R] How to replace strings in data frames by condition
In-Reply-To: <CA+8X3fVctDr485tcE7yThn-nGrGK+SmoLW9-bBKtay658PzY2Q@mail.gmail.com>
References: <1442602538651-4712469.post@n4.nabble.com>
	<CA+8X3fVctDr485tcE7yThn-nGrGK+SmoLW9-bBKtay658PzY2Q@mail.gmail.com>
Message-ID: <1442655707537-4712491.post@n4.nabble.com>

Hi Jim,

I just tried the code and it worked perfectly! You brought me a huge step
nearer to my bachelors degree ;) thank you very much!

Robert



--
View this message in context: http://r.789695.n4.nabble.com/How-to-replace-strings-in-data-frames-by-condition-tp4712469p4712491.html
Sent from the R help mailing list archive at Nabble.com.


From bgnumis at gmail.com  Sat Sep 19 16:42:47 2015
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Sat, 19 Sep 2015 16:42:47 +0200
Subject: [R] Plot with R un html5
Message-ID: <CAN25tHQ0ZMoP0XrrQxSBG3W1P+N7mWY_rZsrScH-LYmjL=vpzA@mail.gmail.com>

Hi al,

I want to put a graph in a html5 webpage  plotted with R (I want to get dar
from Google finance). Is it posible?

F.e NYSE:C

Does everybody know how to plot (with code) un html5 to plot trough R o
directly  plot the sale plot with monthly data?

I Hope you can help me explosing an example.

	[[alternative HTML version deleted]]


From msjitlal at hotmail.com  Sat Sep 19 18:17:09 2015
From: msjitlal at hotmail.com (msjitlal)
Date: Sat, 19 Sep 2015 09:17:09 -0700 (PDT)
Subject: [R] R2WinBUGS error
In-Reply-To: <1442591033128-4712456.post@n4.nabble.com>
References: <1428493384235-4705613.post@n4.nabble.com>
	<1442591033128-4712456.post@n4.nabble.com>
Message-ID: <1442679429765-4712494.post@n4.nabble.com>

Hi Mariana,

I was not able to solve this issue entirely satisfactorily, but after
speaking to IT personnel, they advised that I try running R from my desktop
by right clicking on the R icon and selecting "Run as Administrator", rather
than opening in the normal manner. Remarkably this avoided the problem that
I previously encountered. I hope this works for you.

Good luck,
Mark



--
View this message in context: http://r.789695.n4.nabble.com/R2WinBUGS-error-tp4705613p4712494.html
Sent from the R help mailing list archive at Nabble.com.


From msjitlal at hotmail.com  Sat Sep 19 18:25:42 2015
From: msjitlal at hotmail.com (msjitlal)
Date: Sat, 19 Sep 2015 09:25:42 -0700 (PDT)
Subject: [R] R2WinBUGS error
In-Reply-To: <1442591033128-4712456.post@n4.nabble.com>
References: <1428493384235-4705613.post@n4.nabble.com>
	<1442591033128-4712456.post@n4.nabble.com>
Message-ID: <DUB111-W507493935F58EC99039A81C7580@phx.gbl>

Hi Mariana,

Not sure if my post was accepted. Try running R from your desktop but right clicking on the icon and selecting "Run as Administrator", rather than opening R in the normal manner.

Good luck,
Mark

Date: Fri, 18 Sep 2015 08:43:53 -0700
From: ml-node+s789695n4712456h91 at n4.nabble.com
To: msjitlal at hotmail.com
Subject: Re: R2WinBUGS error



	I am facing the same problem. I have been able to run my code in another computer with Windows 8 but not in my computer at work, which also runs Windowns 8. 


I am looking forward for solving this problem.


Mariana.

	
	
	
	

	

	
	
		If you reply to this email, your message will be added to the discussion below:
		http://r.789695.n4.nabble.com/R2WinBUGS-error-tp4705613p4712456.html
	
	
		
		To unsubscribe from R2WinBUGS error, click here.

		NAML
	 		 	   		  



--
View this message in context: http://r.789695.n4.nabble.com/R2WinBUGS-error-tp4705613p4712495.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sat Sep 19 22:33:15 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 20 Sep 2015 06:33:15 +1000
Subject: [R] Need help with GLM on R
In-Reply-To: <DUB408-EAS188585B2CF33C58CC4E26F1AD580@phx.gbl>
References: <1442592215416-4712459.post@n4.nabble.com>
	<CA+8X3fXh5rsuz3+GT4NMYHAViC+xZ=4Q3cckifxxe2wUYASexA@mail.gmail.com>
	<DUB408-EAS188585B2CF33C58CC4E26F1AD580@phx.gbl>
Message-ID: <CA+8X3fUi2fk4gDXMP9mY9M2zot=puc9xqHoa2BHV0z4QcB=Eug@mail.gmail.com>

Hi Georgia,
In that case you are conducting an exploratory analysis, sometimes called a
fishing expedition, on the effect of location. The usual procedure is to
plot the populations by the areas and put the Mark I human eyeball to work.
If you see some pattern that you can plausibly defend, such as southern
areas have higher populations than northern ones, you can code your areas
accordingly and try it out. It is understandably difficult to get an
analysis like this accepted unless some causal factor just pops out in your
plot. Remember that you are now testing the effect of, say, latitude, not
"area numbers" which are just arbitrary names. Good luck.

Jim


On Sat, Sep 19, 2015 at 11:41 PM, Georgia Clack <georgialclack92 at outlook.com
> wrote:

> Hello
> The area number does refer to the area location, but I want to see if area
> number has an effect. The problem is I'm not sure how to look at this
> because whenever I run the glm it makes area 1 the intercept and I don't
> want it to
> I just wanted to know what code to use for it to produce the table I want
> Thanks
> Georgia
>
> On 18 Sep 2015, at 22:52, Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi icelandic1992,
> If I am correct, the default model is comparing the first area with the
> rest. I assume that "area" does not refer to the physical area of the
> locations, but is a nominal order variable. If I am wrong, and there are
> actual "areas", that is probably what you want to have in the model, as one
> would expect that larger areas would have larger populations.
>
> Jim
>
>
> On Sat, Sep 19, 2015 at 2:03 AM, icelandic1992 <
> georgialclack92 at outlook.com> wrote:
>
>> I am using some data for a population count
>> This what my data is set out as
>> Area   Count    Year      DOY      Rain     Wind
>>
>> all continuous effects except wind and rain and area are categorical
>> variables with 1 for rain and wind and 0 for no rain or wind and areas
>> 1-27
>>
>> I am trying to do a GLM analysis of certain treatment effects on
>> Loge(total
>> count+1) to find out which factors significantly affect the count. I used
>> the formula glm.nb(Count~factor(Area)+Year+Rain+Wind+DOY)
>> However it does not work, and when I looked at it with the formula I was
>> given by someone else which was
>> glm.nb(Count~factor(Area)+Year+DOY+Year*DOY) it came up with intercept as
>> the first area and then areas 1-27 listed and then year and DOY.
>>
>> I am unsure of what this means as i think it is comparing all the areas to
>> the first area. I really want it to come up with a table that looks like
>> this
>>
>> Effect              Coefficient        SE      F-value
>> Area number
>> Year
>> Wind
>> Rain
>>
>>
>> Any help will be greatly appreciated as I have tried to understand how to
>> get this table but I am struggling to understand
>>
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://r.789695.n4.nabble.com/Need-help-with-GLM-on-R-tp4712459.html
>> Sent from the R help mailing list archive at Nabble.com
>> <http://nabble.com>.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From schwidom at gmx.net  Sat Sep 19 22:46:18 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Sat, 19 Sep 2015 22:46:18 +0200
Subject: [R] Plot with R un html5
In-Reply-To: <CAN25tHQ0ZMoP0XrrQxSBG3W1P+N7mWY_rZsrScH-LYmjL=vpzA@mail.gmail.com>
References: <CAN25tHQ0ZMoP0XrrQxSBG3W1P+N7mWY_rZsrScH-LYmjL=vpzA@mail.gmail.com>
Message-ID: <20150919204618.GA9408@debian64>

Hi,

when you can plot this graph using the rgl-package,
then you can use "rgl::writeWebGL" to create an 3D-View
in the Browser.

Regards

On Sat, Sep 19, 2015 at 04:42:47PM +0200, bgnumis bgnum wrote:
> Hi al,
> 
> I want to put a graph in a html5 webpage  plotted with R (I want to get dar
> from Google finance). Is it posible?
> 
> F.e NYSE:C
> 
> Does everybody know how to plot (with code) un html5 to plot trough R o
> directly  plot the sale plot with monthly data?
> 
> I Hope you can help me explosing an example.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From oma.gonzales at gmail.com  Sat Sep 19 22:56:02 2015
From: oma.gonzales at gmail.com (=?UTF-8?B?T21hciBBbmRyw6kgR29uesOhbGVzIETDrWF6?=)
Date: Sat, 19 Sep 2015 15:56:02 -0500
Subject: [R] Plot with R un html5
In-Reply-To: <20150919204618.GA9408@debian64>
References: <CAN25tHQ0ZMoP0XrrQxSBG3W1P+N7mWY_rZsrScH-LYmjL=vpzA@mail.gmail.com>
	<20150919204618.GA9408@debian64>
Message-ID: <CAM-xyZgU+DBu4UdtUkWd0pZW2=yhgvKFQ_AgADE9Wnumz6+Cdw@mail.gmail.com>

You could use Rmarkdown, and use as output: html.

http://rmarkdown.rstudio.com/html_document_format.html

2015-09-19 15:46 GMT-05:00 Frank Schwidom <schwidom at gmx.net>:
> Hi,
>
> when you can plot this graph using the rgl-package,
> then you can use "rgl::writeWebGL" to create an 3D-View
> in the Browser.
>
> Regards
>
> On Sat, Sep 19, 2015 at 04:42:47PM +0200, bgnumis bgnum wrote:
>> Hi al,
>>
>> I want to put a graph in a html5 webpage  plotted with R (I want to get dar
>> from Google finance). Is it posible?
>>
>> F.e NYSE:C
>>
>> Does everybody know how to plot (with code) un html5 to plot trough R o
>> directly  plot the sale plot with monthly data?
>>
>> I Hope you can help me explosing an example.
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jacksonmrodrigues at gmail.com  Sun Sep 20 00:25:33 2015
From: jacksonmrodrigues at gmail.com (Jackson Rodrigues)
Date: Sun, 20 Sep 2015 00:25:33 +0200
Subject: [R] To plot and to extract info from a plot
Message-ID: <CAPL76w93xm_JWu0bben=2mSKj+qZa7OZefN32ki64SQ6q3pcqg@mail.gmail.com>

Hi everybody,

I need help to plot (question 1) and to extract information from another
plot (question 2). Could anyone help me? I will be very grateful.
I do not know if I could enclose a figure here, so I saved it in my dropbox
at
https://www.dropbox.com/s/97ud54886cn6u8i/figure%20for%20r%20group.jpg?dl=0

My questions are based on this figure.

Question 1:
The first figure (Figure 1) is a compilation of time series of some
variables that I have to plot in parallel. However, this figure was made in
adobe Illustrator because I could not plot these time series in parallel
directly from R. Using R, I always get one box and one y axis for each
plot. So, How to make a figure like this (several curves in paralel with 1
y axis, and no x axis)  in R? I am asking because i have to plot about 50
curves in parallel and doing it in R would save a long time.

Question 2:
The second plot (Figure 2) is a smoothed curve through time in which each
peak indicates an independent event. I would like to know when exactly on
time (x axis) each one of these peaks ocurred. So, how to intercept the x
axis at the maximum value of a peak.

I appreciate any help.

Jackson Rodrigues

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Sun Sep 20 01:08:51 2015
From: jrkrideau at inbox.com (John Kane)
Date: Sat, 19 Sep 2015 15:08:51 -0800
Subject: [R] To plot and to extract info from a plot
In-Reply-To: <CAPL76w93xm_JWu0bben=2mSKj+qZa7OZefN32ki64SQ6q3pcqg@mail.gmail.com>
Message-ID: <1043A7595CD.000006B0jrkrideau@inbox.com>

I don't think we can do a lot with Q1 without some data. Data would probably help with Q2 as well.  Have a look at the following links especially on how to use dput() to supply sample data.  

John Kane
Kingston ON Canada


> -----Original Message-----
> From: jacksonmrodrigues at gmail.com
> Sent: Sun, 20 Sep 2015 00:25:33 +0200
> To: r-help at r-project.org
> Subject: [R] To plot and to extract info from a plot
> 
> Hi everybody,
> 
> I need help to plot (question 1) and to extract information from another
> plot (question 2). Could anyone help me? I will be very grateful.
> I do not know if I could enclose a figure here, so I saved it in my
> dropbox
> at
> https://www.dropbox.com/s/97ud54886cn6u8i/figure%20for%20r%20group.jpg?dl=0
> 
> My questions are based on this figure.
> 
> Question 1:
> The first figure (Figure 1) is a compilation of time series of some
> variables that I have to plot in parallel. However, this figure was made
> in
> adobe Illustrator because I could not plot these time series in parallel
> directly from R. Using R, I always get one box and one y axis for each
> plot. So, How to make a figure like this (several curves in paralel with
> 1
> y axis, and no x axis)  in R? I am asking because i have to plot about 50
> curves in parallel and doing it in R would save a long time.
> 
> Question 2:
> The second plot (Figure 2) is a smoothed curve through time in which each
> peak indicates an independent event. I would like to know when exactly on
> time (x axis) each one of these peaks ocurred. So, how to intercept the x
> axis at the maximum value of a peak.
> 
> I appreciate any help.
> 
> Jackson Rodrigues
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From dwinsemius at comcast.net  Sun Sep 20 02:05:08 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 19 Sep 2015 17:05:08 -0700
Subject: [R] To plot and to extract info from a plot
In-Reply-To: <CAPL76w93xm_JWu0bben=2mSKj+qZa7OZefN32ki64SQ6q3pcqg@mail.gmail.com>
References: <CAPL76w93xm_JWu0bben=2mSKj+qZa7OZefN32ki64SQ6q3pcqg@mail.gmail.com>
Message-ID: <C7CF5D85-789E-4388-B6DC-BA0D63E530DF@comcast.net>


On Sep 19, 2015, at 3:25 PM, Jackson Rodrigues wrote:

> Hi everybody,
> 
> I need help to plot (question 1) and to extract information from another
> plot (question 2). Could anyone help me? I will be very grateful.
> I do not know if I could enclose a figure here, so I saved it in my dropbox
> at
> https://www.dropbox.com/s/97ud54886cn6u8i/figure%20for%20r%20group.jpg?dl=0
> 
> My questions are based on this figure.
> 
> Question 1:
> The first figure (Figure 1) is a compilation of time series of some
> variables that I have to plot in parallel. However, this figure was made in
> adobe Illustrator because I could not plot these time series in parallel
> directly from R. Using R, I always get one box and one y axis for each
> plot. So, How to make a figure like this (several curves in paralel with 1
> y axis, and no x axis)  in R? I am asking because i have to plot about 50
> curves in parallel and doing it in R would save a long time.
> 

Inase graphics you would plot one series with `plot()` then add additonal series with `lines()`
> Question 2:
> The second plot (Figure 2) is a smoothed curve through time in which each
> peak indicates an independent event. I would like to know when exactly on
> time (x axis) each one of these peaks ocurred. So, how to intercept the x
> axis at the maximum value of a peak.

This looks like a densityplot. There are methods in all three R 2-D graphics paradigms.


If you need worked examples you need to provide data. Learn to use dput() for that purpose.

-- 
> 
> I appreciate any help.
> 
> Jackson Rodrigues
> 
> 	[[alternative HTML version deleted]]

And do read the Posting guide. This is a plain text mailing list.

> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From monika.novac at gmail.com  Sat Sep 19 22:46:10 2015
From: monika.novac at gmail.com (monika nov)
Date: Sat, 19 Sep 2015 21:46:10 +0100
Subject: [R] Function stslshac {sphet}: heteroskedasticity and
 autocorrelation consistent (HAC) estimator
In-Reply-To: <loom.20150919T194439-473@post.gmane.org>
References: <CAHsnuZM7Bjg9UKVaR5w0nhs+mGE-twwvrmHr0pwHnUequdLJLQ@mail.gmail.com>
	<loom.20150919T194439-473@post.gmane.org>
Message-ID: <CAHsnuZODP8EH6JA7jhMxkBSwY72YuNTcmJdyk5UJq7hHGsJMJA@mail.gmail.com>

Dear Roger,

Thank you for your email.

I am sorry for HTML containing email. I am not very good in IT so I
didn't know that my email client is using HTML by default. Now I have
tried to turn it off and I would be very grateful if you could let me
know if it is all right now.

Regarding my question. 'if you use a model with semiparametric fitting
of HAC, how might you check that it actually worked'

is not exactly what I wanted to ask. Although answer to this question
would be also helpful.


What I wanted to ask is a bit more specific. I wanted to know, whether
it is a good practice to apply tests for spatial autocorrelation (for
example one of them can be run in R by function moran.test, package
sphet) to residuals from a model obtained by function stslshac
(package spdep). Further I would like to know whether significance of
this test (rejecting the null hypothesis) means, that I should not use
the estimate produced by stslshac function because model is
missspecified. This is maybe more an econometric theory question
rather than R programming question and I am very sorry if I am not
supposed to ask these questions in this forum.

This is quite specific and technical question so I think that the
question may not be understandable for people without econometric
background, even if asked clearly.

I do not think that including an example of code would help to
understand my question, however I am including one bellow.


Thank you for your suggestion about r-sig-geo mailing list, I have
already posted it there.

Best wishes

Monika

####################################################################################################################################

library(sphet)
library(spdep)
data(auckland)

# In the following part objects needed as arguments for stslshac
function are created:

auckland.listw<-nb2listw(auckland.nb, style="W")
coord1 <- cbind(seq(1, nrow(auckland)), auckland$Easting, auckland$Norting)
id1 <- seq(1, nrow(auckland))
tmp <- distance(auckland, region.id = id1, output = TRUE,
                type = "NN", nn = 10, shape.name = "shapefile",
region.id.name = "id1",
                  firstline = TRUE, file.name = "auckland_nn_10.GWT")
coldist <- read.gwt2dist(file = "auckland_nn_10.GWT", region.id = id1,skip = 1)


# Now follows the estimation and printing of summary of results:

AucklandHAC<-(stslshac(Deaths.1977.85~Deaths.1977.85+Easting+Northing,
                         listw=auckland.listw, type=c("Triangular"),
                         distance=coldist,data=auckland))

summary(AucklandHAC)

# Now follows an example of the tests I am asking about. I am not sure
whether it is econometrically correct to apply this test for residuals
estiated by stslshac function

moran.test(AucklandHAC$residuals,listw=auckland.listw, alternative="two.sided")

######################################################################################################################

Here are the results I get after running the Moran.test. The test is
significant and the null hypothesis is rejected. I am wondering, can I
still use the stslshac function and its estimate?



    Moran's I test under randomisation

data:  AucklandHAC$residuals
weights: auckland.listw

Moran I statistic standard deviate = -4.302, p-value = 1.693e-05
alternative hypothesis: two.sided
sample estimates:
Moran I statistic       Expectation          Variance
     -0.230081673      -0.006024096       0.002712539






On 19 September 2015 at 18:48, Roger Bivand <roger.bivand at nhh.no> wrote:
> monika nov <monika.novac <at> gmail.com> writes:
>
>>
>> Dear R-users,
>>
>> I have quite basic question for econometricians, however I would like to be
>> sure in this.
>>
>> If I use a HAC estimator of the variance-covariance (VC) matrix for a
>> spatial econometric model, do I still need to test the residuals for
>> spatial autocorrelation and heteroscedasticity? (in particular I am using
>> function stslshac available in package sphet. The estimator is based on
>> Kelejian, H.H. and Prucha, I.R. (2007) HAC estimation in a spatial
>> framework, Journal of Econometrics, 140, pages 131?154).
>>
>
> Please consider posting on R-sig-geo, since your question concerns spatial
> regression. Roughly, you might mean that if you use a model with
> semiparametric fitting of HAC, how might you check that it actually worked,
> but your meaning isn't obvious. If you include an example using a built-in
> data set, then your intentions would be clearer.
>
> ...
>> I would be grateful for any reaction.
>>
>> Monika
>>
> PS. Please post plain text, not HTML
>
> Roger Bivand
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From raviteja2504 at gmail.com  Sat Sep 19 23:09:45 2015
From: raviteja2504 at gmail.com (Ravi Teja)
Date: Sun, 20 Sep 2015 02:39:45 +0530
Subject: [R] what is the effective method to apply the below logic for ~1.2
 million records in R
Message-ID: <CADeUTLKj_LHd2thGnpH15a-3YafLosZjtRBEtsXYH14Jq5ivMw@mail.gmail.com>

Hi,

I am trying to apply the below logic to generate flag_1 column on a data
set consisting of ~1.2 million records in R.

Code :

for(i in 1: nrows)
  {
              if(A$customer[i]==A$customer[i+1])
                {

                  if(is.na(A$Time_Diff[i]))
                     A$flag_1[i] <- 1
                     else if (A$Time_Diff[i] > 12)
                     A$flag_1[i] <- 1
                     else
                     A$flag_1[i] <- A$flag_1[i-1]+1

               }

            else
            {

              if(is.na(A$Time_Diff[i]))
                     A$flag_1[i] <- 1
                     else if (A$Time_Diff[i] > 12)
                     A$flag_1[i] <- 1
                     else
                     A$flag_1[i] <- A$flag_1[i-1]+1

               }
}


Resultant dataset should look like

Customer   Time_diff    flag_1
1                   NA           1
1                   10             2
1                    8              3
1                    15            1
1                    9               2
1                    10              3
2                     NA            1
2                      2               2
2                      5               3

The above logic will take approximately 60 hours to generate the flag_1
column on a dataset consisting of ~1.2 million records. Is there any
effective way in R to implement this logic in R ?

Appreciate your help.

Thanks,
Ravi

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Sep 20 04:25:12 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 19 Sep 2015 19:25:12 -0700
Subject: [R] what is the effective method to apply the below logic for
	~1.2 million records in R
In-Reply-To: <CADeUTLKj_LHd2thGnpH15a-3YafLosZjtRBEtsXYH14Jq5ivMw@mail.gmail.com>
References: <CADeUTLKj_LHd2thGnpH15a-3YafLosZjtRBEtsXYH14Jq5ivMw@mail.gmail.com>
Message-ID: <19CDDDA9-67E0-4DF8-9BB6-A68CE3D34D35@comcast.net>


On Sep 19, 2015, at 2:09 PM, Ravi Teja wrote:

> Hi,
> 
> I am trying to apply the below logic to generate flag_1 column on a data
> set consisting of ~1.2 million records in R.
> 
> Code :
> 
> for(i in 1: nrows)
>  {
>              if(A$customer[i]==A$customer[i+1])
>                {
> 
>                  if(is.na(A$Time_Diff[i]))
>                     A$flag_1[i] <- 1
>                     else if (A$Time_Diff[i] > 12)
>                     A$flag_1[i] <- 1
>                     else
>                     A$flag_1[i] <- A$flag_1[i-1]+1
> 
>               }
> 
>            else
>            {
> 
>              if(is.na(A$Time_Diff[i]))
>                     A$flag_1[i] <- 1
>                     else if (A$Time_Diff[i] > 12)
>                     A$flag_1[i] <- 1
>                     else
>                     A$flag_1[i] <- A$flag_1[i-1]+1
> 
>               }
> }

The inner logic of the consequent and alternative appear identical.  Vectorized approaches would surely be faster. You should post some code that matches the data. In R customer is not the same as Customer, and Time_diff is not Time_Diff,  and my patience for this code review has expired.

Post the output from and do include code to create `nrows`:

 dput( head (A, 20) )


> 
> Resultant dataset should look like
> 
> Customer   Time_diff    flag_1
> 1                   NA           1
> 1                   10             2
> 1                    8              3
> 1                    15            1
> 1                    9               2
> 1                    10              3
> 2                     NA            1
> 2                      2               2
> 2                      5               3
> 
> The above logic will take approximately 60 hours to generate the flag_1
> column on a dataset consisting of ~1.2 million records. Is there any
> effective way in R to implement this logic in R ?
> 
> Appreciate your help.
> 
> Thanks,
> Ravi
> 
> 	[[alternative HTML version deleted]]

AND R-help is a plain text only mailing list.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From istazahn at gmail.com  Sun Sep 20 04:48:43 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Sat, 19 Sep 2015 22:48:43 -0400
Subject: [R] what is the effective method to apply the below logic for
 ~1.2 million records in R
In-Reply-To: <CADeUTLKj_LHd2thGnpH15a-3YafLosZjtRBEtsXYH14Jq5ivMw@mail.gmail.com>
References: <CADeUTLKj_LHd2thGnpH15a-3YafLosZjtRBEtsXYH14Jq5ivMw@mail.gmail.com>
Message-ID: <CA+vqiLES0U=MKsJOfXvkOiUj7g-v71V3g-RMb30yy1Eyb2efXQ@mail.gmail.com>

This assumes that the data are sorted by customer, and that only the
first value of Time_Diff is missing for each customer (and that the
first value is always missing for each customer). If those assumptions
hold you can do something like

A <- read.table(text = "customer   Time_Diff    flag_1
1                   NA           1
1                   10           2
1                    8           3
1                   15           1
1                    9           2
1                   10           3
2                   NA           1
2                    2           2
2                    5           3",
header = TRUE)

A$flag_1 <- NULL

library(data.table)

A <- as.data.table(A)
A[ , g15 := cumsum(c(0, ifelse(is.na(diff(Time_Diff > 12)), 0,
diff(Time_Diff > 12) > 0)))]
## I'm not proud of the previous line, probably there is a cleaner way
A[ , flag_1 := 1:.N, by = c("customer", "g15")]
A[ , g15 := NULL]

Best,
Ista

On Sat, Sep 19, 2015 at 5:09 PM, Ravi Teja <raviteja2504 at gmail.com> wrote:
> Hi,
>
> I am trying to apply the below logic to generate flag_1 column on a data
> set consisting of ~1.2 million records in R.
>
> Code :
>
> for(i in 1: nrows)
>   {
>               if(A$customer[i]==A$customer[i+1])
>                 {
>
>                   if(is.na(A$Time_Diff[i]))
>                      A$flag_1[i] <- 1
>                      else if (A$Time_Diff[i] > 12)
>                      A$flag_1[i] <- 1
>                      else
>                      A$flag_1[i] <- A$flag_1[i-1]+1
>
>                }
>
>             else
>             {
>
>               if(is.na(A$Time_Diff[i]))
>                      A$flag_1[i] <- 1
>                      else if (A$Time_Diff[i] > 12)
>                      A$flag_1[i] <- 1
>                      else
>                      A$flag_1[i] <- A$flag_1[i-1]+1
>
>                }
> }
>
>
> Resultant dataset should look like
>
> Customer   Time_diff    flag_1
> 1                   NA           1
> 1                   10             2
> 1                    8              3
> 1                    15            1
> 1                    9               2
> 1                    10              3
> 2                     NA            1
> 2                      2               2
> 2                      5               3
>
> The above logic will take approximately 60 hours to generate the flag_1
> column on a dataset consisting of ~1.2 million records. Is there any
> effective way in R to implement this logic in R ?
>
> Appreciate your help.
>
> Thanks,
> Ravi
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Sun Sep 20 05:31:25 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 20 Sep 2015 13:31:25 +1000
Subject: [R] what is the effective method to apply the below logic for
 ~1.2 million records in R
In-Reply-To: <19CDDDA9-67E0-4DF8-9BB6-A68CE3D34D35@comcast.net>
References: <CADeUTLKj_LHd2thGnpH15a-3YafLosZjtRBEtsXYH14Jq5ivMw@mail.gmail.com>
	<19CDDDA9-67E0-4DF8-9BB6-A68CE3D34D35@comcast.net>
Message-ID: <CA+8X3fW707PjGSrugw_rLe9M1mup3UFtWokntUspNXttLQJH_Q@mail.gmail.com>

Hi Ravi,
Try this:

current_customer<-0
for(row in 1:dim(A)[1]) {
 if(current_customer == A$Customer[row]) {
  if(A$Time_Diff[row] > 12) A$flag_1[row]<-1
  else A$flag_1[row]<-A$flag_1[row-1]+1
 }
 else {
  current_customer<-A$Customer[row]
  A$flag_1[row]<-1
 }
}

Jim

On Sun, Sep 20, 2015 at 12:25 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> On Sep 19, 2015, at 2:09 PM, Ravi Teja wrote:
>
> > Hi,
> >
> > I am trying to apply the below logic to generate flag_1 column on a data
> > set consisting of ~1.2 million records in R.
> >
> > Code :
> >
> > for(i in 1: nrows)
> >  {
> >              if(A$customer[i]==A$customer[i+1])
> >                {
> >
> >                  if(is.na(A$Time_Diff[i]))
> >                     A$flag_1[i] <- 1
> >                     else if (A$Time_Diff[i] > 12)
> >                     A$flag_1[i] <- 1
> >                     else
> >                     A$flag_1[i] <- A$flag_1[i-1]+1
> >
> >               }
> >
> >            else
> >            {
> >
> >              if(is.na(A$Time_Diff[i]))
> >                     A$flag_1[i] <- 1
> >                     else if (A$Time_Diff[i] > 12)
> >                     A$flag_1[i] <- 1
> >                     else
> >                     A$flag_1[i] <- A$flag_1[i-1]+1
> >
> >               }
> > }
>
> The inner logic of the consequent and alternative appear identical.
> Vectorized approaches would surely be faster. You should post some code
> that matches the data. In R customer is not the same as Customer, and
> Time_diff is not Time_Diff,  and my patience for this code review has
> expired.
>
> Post the output from and do include code to create `nrows`:
>
>  dput( head (A, 20) )
>
>
> >
> > Resultant dataset should look like
> >
> > Customer   Time_diff    flag_1
> > 1                   NA           1
> > 1                   10             2
> > 1                    8              3
> > 1                    15            1
> > 1                    9               2
> > 1                    10              3
> > 2                     NA            1
> > 2                      2               2
> > 2                      5               3
> >
> > The above logic will take approximately 60 hours to generate the flag_1
> > column on a dataset consisting of ~1.2 million records. Is there any
> > effective way in R to implement this logic in R ?
> >
> > Appreciate your help.
> >
> > Thanks,
> > Ravi
> >
> >       [[alternative HTML version deleted]]
>
> AND R-help is a plain text only mailing list.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sun Sep 20 05:44:03 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 20 Sep 2015 13:44:03 +1000
Subject: [R] To plot and to extract info from a plot
In-Reply-To: <C7CF5D85-789E-4388-B6DC-BA0D63E530DF@comcast.net>
References: <CAPL76w93xm_JWu0bben=2mSKj+qZa7OZefN32ki64SQ6q3pcqg@mail.gmail.com>
	<C7CF5D85-789E-4388-B6DC-BA0D63E530DF@comcast.net>
Message-ID: <CA+8X3fWyDm=+q050sZOXxS472ynB+1L76u_Ji5vrhKZozhYw1w@mail.gmail.com>

HI Jackson,
This might get you started:

# plot a few squiggles
plot(runif(100)+1,seq(10000,0,length.out=100),xlim=c(1,6),
 type="l",main="Squiggles by Years",
 xlab="Squiggles",ylab="Years",xaxt="n")
lines(runif(100)+2,seq(10000,0,length.out=100))
lines(runif(100)+3,seq(10000,0,length.out=100))
lines(runif(100)+4,seq(10000,0,length.out=100))
lines(runif(100)+5,seq(10000,0,length.out=100))
axis(1,at=1:5+0.5,labels=1:5)

Jim


On Sun, Sep 20, 2015 at 10:05 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> On Sep 19, 2015, at 3:25 PM, Jackson Rodrigues wrote:
>
> > Hi everybody,
> >
> > I need help to plot (question 1) and to extract information from another
> > plot (question 2). Could anyone help me? I will be very grateful.
> > I do not know if I could enclose a figure here, so I saved it in my
> dropbox
> > at
> >
> https://www.dropbox.com/s/97ud54886cn6u8i/figure%20for%20r%20group.jpg?dl=0
> >
> > My questions are based on this figure.
> >
> > Question 1:
> > The first figure (Figure 1) is a compilation of time series of some
> > variables that I have to plot in parallel. However, this figure was made
> in
> > adobe Illustrator because I could not plot these time series in parallel
> > directly from R. Using R, I always get one box and one y axis for each
> > plot. So, How to make a figure like this (several curves in paralel with
> 1
> > y axis, and no x axis)  in R? I am asking because i have to plot about 50
> > curves in parallel and doing it in R would save a long time.
> >
>
> Inase graphics you would plot one series with `plot()` then add additonal
> series with `lines()`
> > Question 2:
> > The second plot (Figure 2) is a smoothed curve through time in which each
> > peak indicates an independent event. I would like to know when exactly on
> > time (x axis) each one of these peaks ocurred. So, how to intercept the x
> > axis at the maximum value of a peak.
>
> This looks like a densityplot. There are methods in all three R 2-D
> graphics paradigms.
>
>
> If you need worked examples you need to provide data. Learn to use dput()
> for that purpose.
>
> --
> >
> > I appreciate any help.
> >
> > Jackson Rodrigues
> >
> >       [[alternative HTML version deleted]]
>
> And do read the Posting guide. This is a plain text mailing list.
>
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From islandelephants at gmail.com  Sun Sep 20 09:03:54 2015
From: islandelephants at gmail.com (=?UTF-8?Q?Alexander_Mo=C3=9Fbrucker?=)
Date: Sun, 20 Sep 2015 14:03:54 +0700
Subject: [R] Fwd: ERROR - non-conformable arguments
In-Reply-To: <mailman.4158.1442732527.3797.r-help@r-project.org>
References: <mailman.4158.1442732527.3797.r-help@r-project.org>
Message-ID: <CAM4izHsC51WMj_xq0SFNVA=yPg-StYtSkgdyBaJ62uN+WzmV9w@mail.gmail.com>

Hi,

I am using adehabitatHS for habitat selection analysis using widesII(), when
running the script I get the error message: Error in as.vector(X) %*%
t(as.vector(Y)) : non-conformable arguments

Here the script I am using including the data

> #dataset (read from tab deliminated text file)
>
> available
  Riparian Conifer MtShrub1 Aspen RockOutcrop Bitterbrush Windblown MtShrub2
PresBurn
1     0.06    0.13     0.16  0.15        0.06        0.17      0.12     0.04
0.09
  Clearcut
1     0.02
> used
     Riparian Conifer MtShrub1 Aspen RockOutcrop Bitterbrush Windblown
MtShrub2 PresBurn
Ani1        0       0        0     2           0          16         5
14       28
Ani2        0       2        1     2           2           5        10
10       35
Ani3        0       1        2     1           0          14         9
8       40
Ani4        0       1        3     7           5           3         6
9       31
Ani5        0       0        2     2           5          18        10
6       25
Ani6        0       2        1     4           2           7         6
15       19
     Clearcut
Ani1        8
Ani2        9
Ani3        4
Ani4        9
Ani5        0
Ani6       19
>
> #analysis
>
> widesII(used, available, avknown = TRUE, alpha = 0.05)
Error in as.vector(X) %*% t(as.vector(Y)) : non-conformable arguments

Note: I am using an example dataset to test out a script I want to use to
analyse my own dataset, for this I exported the example dataset to excel,
converted it to tab deliminated text file, then imported it.

All worked well when using the example dataset provided by adehabitatHS,
thus I suspect that somhow my test dataset is not correctly read in or
something similar.

I am quite new to R, thus apologize for any shortcomings in reporting etc..

Many thanks, best, Alex



--
View this message in context:
http://r.789695.n4.nabble.com/ERROR-non-conformable-arguments-tp4712516.html
Sent from the R help mailing list archive at Nabble.com.

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Sun Sep 20 14:42:44 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Sun, 20 Sep 2015 08:42:44 -0400
Subject: [R] what is the effective method to apply the below logic for
 ~1.2 million records in R
In-Reply-To: <CADeUTLLwu5rP8fTg5GaKR+c2hVL5aQAPnzPK2BipnM6KwBKprA@mail.gmail.com>
References: <CADeUTLKj_LHd2thGnpH15a-3YafLosZjtRBEtsXYH14Jq5ivMw@mail.gmail.com>
	<CA+vqiLES0U=MKsJOfXvkOiUj7g-v71V3g-RMb30yy1Eyb2efXQ@mail.gmail.com>
	<CADeUTLLwu5rP8fTg5GaKR+c2hVL5aQAPnzPK2BipnM6KwBKprA@mail.gmail.com>
Message-ID: <CA+vqiLHvXjvJuTYy2=0PTx0E0xvGXc-haeO_jnJD1peSczYMbA@mail.gmail.com>

Hi Ravi,

Did you try fixing the problem? What did you try and what went wrong?

The answer is probably

A <- as.data.table(A)
A[ , g15 := cumsum(ifelse(is.na(Time_Diff > 12), 0, Time_Diff > 12))]
A[ , flag_1 := 1:.N, by = c("customer", "g15")]
A[ , g15 := NULL]

but you would have learned more if you had at least tried getting
there yourself.

Best,
Ista

On Sun, Sep 20, 2015 at 6:19 AM, Ravi Teja <raviteja2504 at gmail.com> wrote:
> Hi Ista.
>
> Thanks a ton for the response and your assumptions were right.
>
> f the Time_Diff is missing then flag_1 value should be 1
> if the Time_Diff is > 12 then flag_1 value should be 1
> if the Time_Diff is < 12 the flag_1 value should be (if the current row is i
> then flag_1 value should be (flag_1[i-1] + 1) )
>
> When I tried to apply the logic you had shared, the results are deviating
> from the expected results.
>
> I think the logic you had shared will not function if there are two
> successive rows with Time_Diff values > 12
>
> I have attached a sample of my original data set and the expected flag_1
> column to this mail.
>
> Please help in tweaking your code to generate the attached result.
>
> Awaiting for your reply
>
> Thanks,
> Ravi
>
> On Sun, Sep 20, 2015 at 8:18 AM, Ista Zahn <istazahn at gmail.com> wrote:
>>
>> This assumes that the data are sorted by customer, and that only the
>> first value of Time_Diff is missing for each customer (and that the
>> first value is always missing for each customer). If those assumptions
>> hold you can do something like
>>
>> A <- read.table(text = "customer   Time_Diff    flag_1
>> 1                   NA           1
>> 1                   10           2
>> 1                    8           3
>> 1                   15           1
>> 1                    9           2
>> 1                   10           3
>> 2                   NA           1
>> 2                    2           2
>> 2                    5           3",
>> header = TRUE)
>>
>> A$flag_1 <- NULL
>>
>> library(data.table)
>>
>> A <- as.data.table(A)
>> A[ , g15 := cumsum(c(0, ifelse(is.na(diff(Time_Diff > 12)), 0,
>> diff(Time_Diff > 12) > 0)))]
>> ## I'm not proud of the previous line, probably there is a cleaner way
>> A[ , flag_1 := 1:.N, by = c("customer", "g15")]
>> A[ , g15 := NULL]
>>
>> Best,
>> Ista
>>
>> On Sat, Sep 19, 2015 at 5:09 PM, Ravi Teja <raviteja2504 at gmail.com> wrote:
>> > Hi,
>> >
>> > I am trying to apply the below logic to generate flag_1 column on a data
>> > set consisting of ~1.2 million records in R.
>> >
>> > Code :
>> >
>> > for(i in 1: nrows)
>> >   {
>> >               if(A$customer[i]==A$customer[i+1])
>> >                 {
>> >
>> >                   if(is.na(A$Time_Diff[i]))
>> >                      A$flag_1[i] <- 1
>> >                      else if (A$Time_Diff[i] > 12)
>> >                      A$flag_1[i] <- 1
>> >                      else
>> >                      A$flag_1[i] <- A$flag_1[i-1]+1
>> >
>> >                }
>> >
>> >             else
>> >             {
>> >
>> >               if(is.na(A$Time_Diff[i]))
>> >                      A$flag_1[i] <- 1
>> >                      else if (A$Time_Diff[i] > 12)
>> >                      A$flag_1[i] <- 1
>> >                      else
>> >                      A$flag_1[i] <- A$flag_1[i-1]+1
>> >
>> >                }
>> > }
>> >
>> >
>> > Resultant dataset should look like
>> >
>> > Customer   Time_diff    flag_1
>> > 1                   NA           1
>> > 1                   10             2
>> > 1                    8              3
>> > 1                    15            1
>> > 1                    9               2
>> > 1                    10              3
>> > 2                     NA            1
>> > 2                      2               2
>> > 2                      5               3
>> >
>> > The above logic will take approximately 60 hours to generate the flag_1
>> > column on a dataset consisting of ~1.2 million records. Is there any
>> > effective way in R to implement this logic in R ?
>> >
>> > Appreciate your help.
>> >
>> > Thanks,
>> > Ravi
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> raviteja


From jdnewmil at dcn.davis.CA.us  Sun Sep 20 19:13:57 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sun, 20 Sep 2015 10:13:57 -0700
Subject: [R] Fwd: ERROR - non-conformable arguments
In-Reply-To: <CAM4izHsC51WMj_xq0SFNVA=yPg-StYtSkgdyBaJ62uN+WzmV9w@mail.gmail.com>
References: <mailman.4158.1442732527.3797.r-help@r-project.org>
	<CAM4izHsC51WMj_xq0SFNVA=yPg-StYtSkgdyBaJ62uN+WzmV9w@mail.gmail.com>
Message-ID: <09AFEFF3-7B4B-435A-8EAD-634181234592@dcn.davis.CA.us>

To debug this kind of problem you really need to use dput() to provide the data in your email. [1]

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 20, 2015 12:03:54 AM PDT, "Alexander Mo?brucker" <islandelephants at gmail.com> wrote:
>Hi,
>
>I am using adehabitatHS for habitat selection analysis using widesII(),
>when
>running the script I get the error message: Error in as.vector(X) %*%
>t(as.vector(Y)) : non-conformable arguments
>
>Here the script I am using including the data
>
>> #dataset (read from tab deliminated text file)
>>
>> available
>Riparian Conifer MtShrub1 Aspen RockOutcrop Bitterbrush Windblown
>MtShrub2
>PresBurn
>1     0.06    0.13     0.16  0.15        0.06        0.17      0.12    
>0.04
>0.09
>  Clearcut
>1     0.02
>> used
>     Riparian Conifer MtShrub1 Aspen RockOutcrop Bitterbrush Windblown
>MtShrub2 PresBurn
>Ani1        0       0        0     2           0          16         5
>14       28
>Ani2        0       2        1     2           2           5        10
>10       35
>Ani3        0       1        2     1           0          14         9
>8       40
>Ani4        0       1        3     7           5           3         6
>9       31
>Ani5        0       0        2     2           5          18        10
>6       25
>Ani6        0       2        1     4           2           7         6
>15       19
>     Clearcut
>Ani1        8
>Ani2        9
>Ani3        4
>Ani4        9
>Ani5        0
>Ani6       19
>>
>> #analysis
>>
>> widesII(used, available, avknown = TRUE, alpha = 0.05)
>Error in as.vector(X) %*% t(as.vector(Y)) : non-conformable arguments
>
>Note: I am using an example dataset to test out a script I want to use
>to
>analyse my own dataset, for this I exported the example dataset to
>excel,
>converted it to tab deliminated text file, then imported it.
>
>All worked well when using the example dataset provided by
>adehabitatHS,
>thus I suspect that somhow my test dataset is not correctly read in or
>something similar.
>
>I am quite new to R, thus apologize for any shortcomings in reporting
>etc..
>
>Many thanks, best, Alex
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/ERROR-non-conformable-arguments-tp4712516.html
>Sent from the R help mailing list archive at Nabble.com.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From fanjianling at gmail.com  Sun Sep 20 19:19:46 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Mon, 21 Sep 2015 01:19:46 +0800
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
Message-ID: <CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>

no, I am doing a regression with 6 group data with 2 shared parameters
and 1 different parameter for each group data. the parameter I want to
coerce is for one group. I don't know how to do it. Any suggestion?

Thanks!

On 19 September 2015 at 13:33, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> Why not rewrite the function so that value is not a parameter?
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 18, 2015 9:54:54 PM PDT, Jianling Fan <fanjianling at gmail.com> wrote:
>>Hello, everyone,
>>
>>I am using a nls regression with 6 groups data. I am trying to coerce
>>a parameter to 1 by using a upper and lower statement. but I always
>>get an error like below:
>>
>>Error in ifelse(internalPars < upper, 1, -1) :
>>  (list) object cannot be coerced to type 'double'
>>
>>does anyone know how to fix it?
>>
>>thanks in advance!
>>
>>My code is below:
>>
>>
>>
>>> dproot
>>   depth       den ref
>>1     20 0.5730000   1
>>2     40 0.7800000   1
>>3     60 0.9470000   1
>>4     80 0.9900000   1
>>5    100 1.0000000   1
>>6     10 0.6000000   2
>>7     20 0.8200000   2
>>8     30 0.9300000   2
>>9     40 1.0000000   2
>>10    20 0.4800000   3
>>11    40 0.7340000   3
>>12    60 0.9610000   3
>>13    80 0.9980000   3
>>14   100 1.0000000   3
>>15    20 3.2083491   4
>>16    40 4.9683383   4
>>17    60 6.2381133   4
>>18    80 6.5322348   4
>>19   100 6.5780660   4
>>20   120 6.6032064   4
>>21    20 0.6140000   5
>>22    40 0.8270000   5
>>23    60 0.9500000   5
>>24    80 0.9950000   5
>>25   100 1.0000000   5
>>26    20 0.4345774   6
>>27    40 0.6654726   6
>>28    60 0.8480684   6
>>29    80 0.9268951   6
>>30   100 0.9723207   6
>>31   120 0.9939966   6
>>32   140 0.9992400   6
>>
>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>+ start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>>> summary(fitdp)
>>
>>Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>>
>>Parameters:
>>    Estimate Std. Error t value Pr(>|t|)
>>Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>>Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>>Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>>Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>>Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>>Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>>d50 22.69426    1.03855   21.85  < 2e-16 ***
>>c   -1.59796    0.15589  -10.25 3.02e-10 ***
>>---
>>Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>>
>>Residual standard error: 0.1094 on 24 degrees of freedom
>>
>>Number of iterations to convergence: 8
>>Achieved convergence tolerance: 9.374e-06
>>
>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>algorithm="port",
>>+ start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>+ lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>+ upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>>
>>Error in ifelse(internalPars < upper, 1, -1) :
>>  (list) object cannot be coerced to type 'double'
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>



-- 
Jianling Fan
???


From zenlines at gmail.com  Sun Sep 20 17:49:00 2015
From: zenlines at gmail.com (Brian)
Date: Sun, 20 Sep 2015 17:49:00 +0200
Subject: [R] Unexpected/undocumented behavior of 'within': dropping variable
 names that start with '.'
Message-ID: <55FED56C.10204@gmail.com>

Dear List,

Somewhere I missed something, and now I'm really missing something!
 
> d.f <- data.frame(.id = c(TRUE, FALSE, TRUE), dummy = c(1, 2, 3), a =
c(1, 2, 3), b = c(1, 2, 3) + 1)
 > within(d.f, {d = a + b})
   dummy a b d
 1     1 1 2 3
 2     2 2 3 5
 3     3 3 4 7
 > d.f <- data.frame(.id = c(TRUE, FALSE, TRUE), .dummy = c(1, 2, 3), a
= c(1, 2, 3), b = c(1, 2, 3) + 1)
 > within(d.f, {d = a + b})
   a b d
 1 1 2 3
 2 2 3 5
 3 3 4 7

Could somebody please explain to me why this does this? I think could be
considered a feature (for lots of calculations within a data frame you
don't have to remove all extra variables at the end).  I just wish it
was documented.

Cheers,
Brian


sessionInfo()
 R version 3.1.0 (2014-04-10)
 Platform: x86_64-pc-linux-gnu (64-bit)

 locale:
  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
  [9] LC_ADDRESS=C               LC_TELEPHONE=C
 [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

 attached base packages:
 [1] splines   grid      stats     graphics  grDevices utils     datasets
 [8] methods   base

 other attached packages:
  [1] scales_0.2.4       plyr_1.8.3         reshape2_1.4      
ccchDataProc_0.7
  [5] ccchTools_0.6      xtable_1.7-4       tables_0.7.79      Hmisc_3.14-5
  [9] Formula_1.1-2      survival_2.37-7    ggplot2_1.0.1     
IDPmisc_1.1.17
 [13] lattice_0.20-29    myRplots_1.1       myRtools_1.2       meteoconv_0.1
 [17] pixmap_0.4-11      RColorBrewer_1.0-5 maptools_0.8-30    sp_1.1-1
 [21] mapdata_2.2-3      mapproj_1.2-2      maps_2.3-9         chron_2.3-45
 [25] MASS_7.3-35

 loaded via a namespace (and not attached):
  [1] acepack_1.3-3.3     cluster_1.15.2      colorspace_1.2-4
  [4] compiler_3.1.0      data.table_1.9.4    digest_0.6.4
  [7] foreign_0.8-61      gtable_0.1.2        labeling_0.3
 [10] latticeExtra_0.6-26 munsell_0.4.2       nnet_7.3-8
 [13] proto_0.3-10        Rcpp_0.12.0         rpart_4.1-8
 [16] stringr_0.6.2       tools_3.1.0
 > within
 function (data, expr, ...)
 UseMethod("within")
 <bytecode: 0x26d32c8>
 <environment: namespace:base>


From rosita21 at gmail.com  Sun Sep 20 18:14:20 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Sun, 20 Sep 2015 17:14:20 +0100
Subject: [R] HELP IN GRAPHS - slip screen (URGENT)
In-Reply-To: <CA+8X3fUVNzSg9JNsP-cVG8gU7jscoE6FXwT324nbgitiHuCG3A@mail.gmail.com>
References: <F53D3AF7-922F-4352-B554-1E7F5244AB75@gmail.com>
	<CA+8X3fUpj3vfn=V8rrUNnRSMpu2R9D2DB1gfNQnMQ_tYC96iuQ@mail.gmail.com>
	<CAB630FFBi7xFeZ1GtfZNzcmbSGpgiMB8s_xL8iNEFOgGud1f6Q@mail.gmail.com>
	<CA+8X3fXF0t8BmuEq6MVeEYnsNjLRid-k+xFFO+AKzWmoy_dDqg@mail.gmail.com>
	<CA+8X3fUVNzSg9JNsP-cVG8gU7jscoE6FXwT324nbgitiHuCG3A@mail.gmail.com>
Message-ID: <4FCCB6B8-BCCF-42F5-8C85-D9DF43328A5E@gmail.com>

Dear Jim,


I?ve tried till today, but I could not solve the problems.

1. despite the scales are the same (equal: lambda ={0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.98}), the "matrix" is not equal. If you see, for n = 250 the column is narrower than for n = 1000, and "lambda" has the same values.

2. the first 2 columns relate to alpha 1 and the second two columns alpha2. Is it possible to place a title above nsample that concerns the first 2 columns and other over the last 2?

something like:

                    alpha1                                                         alpha2
nsample=250      nsample=1000                   nsample=250      nsample=1000


3. Notice that have 4 separate drawings and must place the 4 "groups" together.

Can you help?

It's really important.

Best,
RO





library(ggplot2)
library(reshape)
library(lattice)


# read in what looks like half of the data

bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
SE.alpha1<-read.csv("graphs_SE_alpha1.csv")



quartz(width=10,height=6)

# do the first split, to get the rightmost screen for the legend
split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
# now split the first screen to get your eight screens (numbered 3 to 10) for the plots
split.screen(figs=matrix(c(0,0.25,0.5,1,
                           0.25,0.5,0.5,1,
                           0.5,0.75,0.5,1,
                           0.75,1,0.5,1,
                           0,0.25,0,0.5,
                           0.25,0.5,0,0.5,
                           0.5,0.75,0,0.5,
                           0.75,1,0,0.5),
                         ncol=4,byrow=TRUE),screen=1)



#split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
#                           0.5,1,0.5,1,#primeira linha segunda coluna
#                           0,0.5,0,0.5,#segunda linha primeira coluna
#                           0.5,1,0,0.5),#segunda linha segunda coluna
#                         ncol=4,byrow=TRUE),screen=1)


# this produces seven screens numbered like this:
#   3   4   5   6 
#                    2
#   7   8   9   10 
# select the upper left screen



screen(3)
par(mar=c(0,3.5,3,0))
# now the second set
n250<-bias.alpha1$nsample==250
matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1, .6),main="nsample=250",ylab="", cex.main=1)
abline(h = 0, col = "gray60")
mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2, cex.main=1)

screen(4)
par(mar=c(0,0,3,0))
# now the second set
n1000<-bias.alpha1$nsample==1000
matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1, .6),main="nsample=1000",ylab="")
abline(h = 0, col = "gray60")



screen(5)
par(mar=c(0,3.5,3,0))
# now the second set
par(mar=c(3,3.5,0,0))
# now the second set
n250<-bias.alpha2$nsample==250
matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1, .6),main="nsample=250",ylab="")
abline(h = 0, col = "gray60")


screen(6)
par(mar=c(3,0,0,0))
# now the second set
n1000<-bias.alpha2$nsample==1000
matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1, .6),main="nsample=1000",ylab="")
abline(h = 0, col = "gray60")




screen(7)
par(mar=c(0,3.5,3,0))
# now the second set
n250<-SE.alpha1$nsample==250
matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),main="nsample=250",ylab="", cex.main=1)
abline(h = -1, col = "gray60")
mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2, cex.main=1)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)


screen(8)
par(mar=c(0,0,3,0))
# now the second set
n1000<-SE.alpha1$nsample==1000
matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1),main="nsample=1000",ylab="")
abline(h = -1, col = "gray60")




screen(9)
par(mar=c(3,3.5,0,0))
# now the second set
n250<-SE.alpha2$nsample==250
matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
        type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1),ylab="")
abline(h = -.5, col = "gray60")
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)


screen(10)
par(mar=c(3,0,0,0))
# now the second set
n1000<-SE.alpha2$nsample==1000
matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
        type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1),ylab="")
abline(h = -1, col = "gray60")
mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)



screen(2)
par(mar=c(0,0,0,0))
# plot an empty plot to get the coordinates
plot(0:1,0:1,type="n",axes=FALSE)
legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n", lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)


close.screen(all=TRUE)



Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates

> On 18 Sep 2015, at 10:38, Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Hi Rosa,
> I have had a moment to look at your code. First I think you should start your device as:
> 
> quartz(width=12,height=5)
> 
> The split.screen code that I sent seems to work for me, giving the 
> 
> 3    4    5    6
>                        2
> 7    8    9    10
> 
> layout of screens. To get the aspect ratio of the plots more similar, try this:
> 
> # do the first split, to get the rightmost screen for the legend
> split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
> # now split the first screen to get your eight screens (numbered 3 to 10) for the plots
> split.screen(figs=matrix(c(0,0.31,0.5,1,
>                            0.31,0.54,0.5,1,
>                            0.54,0.77,0.5,1,
>                            0.77,1,0.5,1,
>                            0,0.31,0,0.5,
>                            0.31,0.54,0,0.5,
>                            0.54,0.77,0,0.5,
>                            0.77,1,0,0.5),
>                          ncol=4,byrow=TRUE),screen=1)
> 
>  I'm not sure of which plots should go on the top line and which on the bottom, but I think you want margins like this:
> 
> screen(3)
> par(mar=c(0,3.5,3,0))
> screen(4)
> par(mar=c(0,0,3,0))
> screen(5)
> par(mar=c(0,0,3,0))
> screen(6)
> par(mar=c(0,0,3,0))
> screen(7)
> par(mar=c(3,3.5,0,0))
> screen(8)
> par(mar=c(3,0,3,0))
> screen(9)
> par(mar=c(3,0,3,0))
> screen(10)
> par(mar=c(3,0,3,0))
> 
> Perhaps this will help.
> 
> Jim
> 
> 
> On Fri, Sep 18, 2015 at 6:14 AM, Jim Lemon <drjimlemon at gmail.com <mailto:drjimlemon at gmail.com>> wrote:
> Hi Rosa,
> I don't think the problem is with the split.screen command, for you are getting the eight plots and the screen at the right as you requested. It looks like your margins for each plot need adjusting, and I also think you should have about a 2.2 to 1 width to height ratio in the graphics device. I can't analyze the rest of the code at the moment, but perhaps tomorrow if you can't work it out I can provide some suggestions.
> 
> Jim
> 
> 
> On Fri, Sep 18, 2015 at 1:16 AM, Rosa Oliveira <rosita21 at gmail.com <mailto:rosita21 at gmail.com>> wrote:
> Dear Jim, 
> 
> It works, nonetheless, it doesn't slip the screen correctly :(
> 
> Do you have any idea?
> 
> 
> I used the code:
> 
> 
> #setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
> setwd("~/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
> 
> 
> library(ggplot2)
> library(reshape)
> library(lattice)
> 
> 
> # read in what looks like half of the data
> 
> bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
> SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
> bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
> SE.alpha1<-read.csv("graphs_SE_alpha1.csv")
> 
> 
> 
> quartz(width=10,height=6)
> 
> # do the first split, to get the rightmost screen for the legend
> split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
> # now split the first screen to get your eight screens (numbered 3 to 10) for the plots
> split.screen(figs=matrix(c(0,0.25,0.5,1,
>                            0.25,0.5,0.5,1,
>                            0.5,0.75,0.5,1,
>                            0.75,1,0.5,1,
>                            0,0.25,0,0.5,
>                            0.25,0.5,0,0.5,
>                            0.5,0.75,0,0.5,
>                            0.75,1,0,0.5),
>                          ncol=4,byrow=TRUE),screen=1)
> 
> 
> 
> #split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
> #                           0.5,1,0.5,1,#primeira linha segunda coluna
> #                           0,0.5,0,0.5,#segunda linha primeira coluna
> #                           0.5,1,0,0.5),#segunda linha segunda coluna
> #                         ncol=4,byrow=TRUE),screen=1)
> 
> 
> # this produces seven screens numbered like this:
> #   3   4   5   6 
> #                    2
> #   7   8   9   10 
> # select the upper left screen
> 
> 
> 
> screen(3)
> par(mar=c(0,3.5,3,0))
> # now the second set
> n250<-bias.alpha1$nsample==250
> matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1, .6),main="nsample=250",ylab="", cex.main=1)
> abline(h = 0, col = "gray60")
> mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2, cex.main=1)
> 
> screen(4)
> par(mar=c(0,0,3,0))
> # now the second set
> n1000<-bias.alpha1$nsample==1000
> matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1, .6),main="nsample=1000",ylab="")
> abline(h = 0, col = "gray60")
> 
> 
> 
> screen(5)
> par(mar=c(0,3.5,3,0))
> # now the second set
> par(mar=c(3,3.5,0,0))
> # now the second set
> n250<-bias.alpha2$nsample==250
> matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
> abline(h = 0, col = "gray60")
> mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2, cex.main=1.5)
> 
> screen(6)
> par(mar=c(3,0,0,0))
> # now the second set
> n1000<-bias.alpha2$nsample==1000
> matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
> abline(h = 0, col = "gray60")
> 
> 
> 
> 
> screen(7)
> par(mar=c(0,3.5,3,0))
> # now the second set
> n250<-SE.alpha1$nsample==250
> matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0, 1.1),main="nsample=250",ylab="", cex.main=1)
> abline(h = -1, col = "gray60")
> mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2, cex.main=1)
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
> 
> 
> screen(8)
> par(mar=c(0,0,3,0))
> # now the second set
> n1000<-SE.alpha1$nsample==1000
> matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0, 1.1),main="nsample=1000",ylab="")
> abline(h = -1, col = "gray60")
> 
> 
> 
> 
> screen(9)
> par(mar=c(3,3.5,0,0))
> # now the second set
> n250<-SE.alpha2$nsample==250
> matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
> abline(h = -.5, col = "gray60")
> mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2, cex.main=1.5)
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
> 
> 
> screen(10)
> par(mar=c(3,0,0,0))
> # now the second set
> n1000<-SE.alpha2$nsample==1000
> matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
> abline(h = -.5, col = "gray60")
> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
> 
> 
> 
> screen(2)
> par(mar=c(0,0,0,0))
> # plot an empty plot to get the coordinates
> plot(0:1,0:1,type="n",axes=FALSE)
> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n", lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
> 
> 
> close.screen(all=TRUE)
> 
> 
> and I attach the output graph.
> 
> 
> 
> Best,
> RO
> 
> Atenciosamente,
> Rosa Oliveira
> 
> _________________________________
> 
> 
> Antes de imprimir este e-mail pense bem se tem mesmo que o fazer. 
> H? cada vez menos ?rvores.
> N?o imprima, pense na sua responsabilidade e compromisso com o MEIO AMBIENTE!
>  <http://pt.dreamstime.com/cora-ccedil-atildeo-criado-das-folhas-de-aacutervores-diferentes-thumb12275776.jpg>
>  <http://pt.dreamstime.com/cora-ccedil-atildeo-criado-das-folhas-de-aacutervores-diferentes-thumb12275776.jpg>
> 
> 2015-09-17 12:18 GMT+01:00 Jim Lemon <drjimlemon at gmail.com <mailto:drjimlemon at gmail.com>>:
> Hi Rosa,
> Try this:
> 
> # do the first split, to get the rightmost screen for the legend
> split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
> # now split the first screen to get your eight screens (numbered 3 to 10) for the plots
> split.screen(figs=matrix(c(0,0.25,0.5,1,
>                            0.25,0.5,0.5,1,
>                            0.5,0.75,0.5,1,
>                            0.75,1,0.5,1,
>                            0,0.25,0,0.5,
>                            0.25,0.5,0,0.5,
>                            0.5,0.75,0,0.5,
>                            0.75,1,0,0.5),
>                          ncol=4,byrow=TRUE),screen=1)
> 
> Jim
> 
> 
> On Thu, Sep 17, 2015 at 2:45 AM, Rosa Oliveira <rosita21 at gmail.com <mailto:rosita21 at gmail.com>> wrote:
> Dear all,
> 
> I?m trying to do a graph,
> 
> 3 rows, 5 columns, with the design:
> #   3   4   5   6
> #                    2
> #   7   8   9   10
> 
> I had a code for 3 rows, 3 columns, with the design::
> #   3   4
> #            2
> #   7   8
>  and I tried to modify it, but I had no success :(
> 
> I suppose the problem is in the slip.screen code (red part of the code).
> 
> I attach my code, can anyone please help me?
> 
> 
> Best,
> RO
> 
> 
> setwd("/Users/RO/Dropbox/LMER - 3rdproblem/R/latest_version/graphs/data")
> 
> library(ggplot2)
> library(reshape)
> library(lattice)
> 
> 
> # read in what looks like half of the data
> 
> bias.alpha2<-read.csv("graphs_bias_alpha2.csv")
> SE.alpha2<-read.csv("graphs_SE_alpha2.csv")
> bias.alpha1<-read.csv("graphs_bias_alpha1.csv")
> SE.alpha1<-read.csv("graphs_SE_alpha1.csv")
> 
> 
> 
> quartz(width=10,height=6)
> # do the first split, to get the rightmost screen for the legend
> split.screen(figs=matrix(c(0,0.8,0,1,0.8,1,0,1),nrow=2,byrow=TRUE))
> # now split the first screen to get your six screens for the plots
> 
> 
> 
> split.screen(figs=matrix(c(0,0.5,0.5,1,#primeira linha primeira coluna
>                            0.5,1,0.5,1,#primeira linha segunda coluna
>                            0,0.5,0,0.5,#segunda linha primeira coluna
>                            0.5,1,0,0.5),#segunda linha segunda coluna
>                          ncol=4,byrow=TRUE),screen=1)
> 
> 
> # this produces seven screens numbered like this:
> #   3   4   5   6
> #                    2
> #   7   8   9   10
> # select the upper left screen
> 
> 
> 
> screen(3)
> par(mar=c(0,3.5,3,0))
> # now the second set
> n250<-bias.alpha1$nsample==250
> matplot(x=bias.alpha1$lambda[n250],y=bias.alpha1[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(-.1, .6),main="nsample=250",ylab="", cex.main=1)
> abline(h = 0, col = "gray60")
> mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2, cex.main=1)
> 
> screen(4)
> par(mar=c(0,0,3,0))
> # now the second set
> n1000<-bias.alpha1$nsample==1000
> matplot(x=bias.alpha1$lambda[n1000],y=bias.alpha1[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(-.1, .6),main="nsample=1000",ylab="")
> abline(h = 0, col = "gray60")
> 
> 
> 
> screen(5)
> par(mar=c(0,3.5,3,0))
> # now the second set
> par(mar=c(3,3.5,0,0))
> # now the second set
> n250<-bias.alpha2$nsample==250
> matplot(x=bias.alpha2$lambda[n250],y=bias.alpha2[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(-.1, .6),ylab="")
> abline(h = 0, col = "gray60")
> mtext(expression(paste("Bias av. for  ",alpha[2])),side=2,line=2, cex.main=1.5)
> 
> screen(6)
> par(mar=c(3,0,0,0))
> # now the second set
> n1000<-bias.alpha2$nsample==1000
> matplot(x=bias.alpha2$lambda[n1000],y=bias.alpha2[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(-.1, .6))
> abline(h = 0, col = "gray60")
> 
> 
> 
> 
> screen(7)
> par(mar=c(0,3.5,3,0))
> # now the second set
> n250<-SE.alpha1$nsample==250
> matplot(x=SE.alpha1$lambda[n250],y=SE.alpha1[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",ylim=c(0, 1.1),main="nsample=250",ylab="", cex.main=1)
> abline(h = -1, col = "gray60")
> mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2, cex.main=1)
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
> 
> 
> screen(8)
> par(mar=c(0,0,3,0))
> # now the second set
> n1000<-SE.alpha1$nsample==1000
> matplot(x=SE.alpha1$lambda[n1000],y=SE.alpha1[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),xaxt="n",yaxt="n",ylim=c(0, 1.1),main="nsample=1000",ylab="")
> abline(h = -1, col = "gray60")
> 
> 
> 
> 
> screen(9)
> par(mar=c(3,3.5,0,0))
> # now the second set
> n250<-SE.alpha2$nsample==250
> matplot(x=SE.alpha2$lambda[n250],y=SE.alpha2[n250,3:5],
>         type="l",pch=1:3,col=c(4,2,3),ylim=c(0, 1.1),ylab="")
> abline(h = -.5, col = "gray60")
> mtext(expression(paste("SE av. for  ",alpha[2])),side=2,line=2, cex.main=1.5)
> mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)
> 
> 
> screen(10)
> par(mar=c(3,0,0,0))
> # now the second set
> n1000<-SE.alpha2$nsample==1000
> matplot(x=SE.alpha2$lambda[n1000],y=SE.alpha2[n1000,3:5],
>         type="l",pch=1:3,col=c(4,2,3),yaxt="n",ylim=c(0, 1.1))
> abline(h = -.5, col = "gray60")
> mtext(expression(paste(lambda)),side=1,line=2, , cex.main=1.5)
> 
> 
> 
> screen(2)
> par(mar=c(0,0,0,0))
> # plot an empty plot to get the coordinates
> plot(0:1,0:1,type="n",axes=FALSE)
> legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n", lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)
> 
> 
> close.screen(all=TRUE)
> 
> 
> 
> 
> Best,
> RO
> 
> 
> Atenciosamente,
> Rosa Oliveira
> 
> --
> ____________________________________________________________________________
> 
> 
> Rosa Celeste dos Santos Oliveira,
> 
> E-mail: rosita21 at gmail.com <mailto:rosita21 at gmail.com>
> Tlm: +351 939355143 <tel:%2B351%20939355143>
> Linkedin: https://pt.linkedin.com/in/rosacsoliveira <https://pt.linkedin.com/in/rosacsoliveira>
> ____________________________________________________________________________
> "Many admire, few know"
> Hippocrates
> 
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 


From h.wickham at gmail.com  Sun Sep 20 20:23:43 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Sun, 20 Sep 2015 14:23:43 -0400
Subject: [R] Unexpected/undocumented behavior of 'within': dropping
 variable names that start with '.'
In-Reply-To: <55FED56C.10204@gmail.com>
References: <55FED56C.10204@gmail.com>
Message-ID: <CABdHhvECWtEmDAx6ZKsjJDjfJ8NPAmb+SqdwkVu3aC=nT6z3_g@mail.gmail.com>

The problem is that within.data.frame calls as.list.environment with
the default value of all.names = FALSE. I doubt this is a deliberate
feature, and is more likely to be a minor oversight.

Hadley

On Sun, Sep 20, 2015 at 11:49 AM, Brian <zenlines at gmail.com> wrote:
> Dear List,
>
> Somewhere I missed something, and now I'm really missing something!
>
>> d.f <- data.frame(.id = c(TRUE, FALSE, TRUE), dummy = c(1, 2, 3), a =
> c(1, 2, 3), b = c(1, 2, 3) + 1)
>  > within(d.f, {d = a + b})
>    dummy a b d
>  1     1 1 2 3
>  2     2 2 3 5
>  3     3 3 4 7
>  > d.f <- data.frame(.id = c(TRUE, FALSE, TRUE), .dummy = c(1, 2, 3), a
> = c(1, 2, 3), b = c(1, 2, 3) + 1)
>  > within(d.f, {d = a + b})
>    a b d
>  1 1 2 3
>  2 2 3 5
>  3 3 4 7
>
> Could somebody please explain to me why this does this? I think could be
> considered a feature (for lots of calculations within a data frame you
> don't have to remove all extra variables at the end).  I just wish it
> was documented.
>
> Cheers,
> Brian
>
>
> sessionInfo()
>  R version 3.1.0 (2014-04-10)
>  Platform: x86_64-pc-linux-gnu (64-bit)
>
>  locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
>  [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
>  attached base packages:
>  [1] splines   grid      stats     graphics  grDevices utils     datasets
>  [8] methods   base
>
>  other attached packages:
>   [1] scales_0.2.4       plyr_1.8.3         reshape2_1.4
> ccchDataProc_0.7
>   [5] ccchTools_0.6      xtable_1.7-4       tables_0.7.79      Hmisc_3.14-5
>   [9] Formula_1.1-2      survival_2.37-7    ggplot2_1.0.1
> IDPmisc_1.1.17
>  [13] lattice_0.20-29    myRplots_1.1       myRtools_1.2       meteoconv_0.1
>  [17] pixmap_0.4-11      RColorBrewer_1.0-5 maptools_0.8-30    sp_1.1-1
>  [21] mapdata_2.2-3      mapproj_1.2-2      maps_2.3-9         chron_2.3-45
>  [25] MASS_7.3-35
>
>  loaded via a namespace (and not attached):
>   [1] acepack_1.3-3.3     cluster_1.15.2      colorspace_1.2-4
>   [4] compiler_3.1.0      data.table_1.9.4    digest_0.6.4
>   [7] foreign_0.8-61      gtable_0.1.2        labeling_0.3
>  [10] latticeExtra_0.6-26 munsell_0.4.2       nnet_7.3-8
>  [13] proto_0.3-10        Rcpp_0.12.0         rpart_4.1-8
>  [16] stringr_0.6.2       tools_3.1.0
>  > within
>  function (data, expr, ...)
>  UseMethod("within")
>  <bytecode: 0x26d32c8>
>  <environment: namespace:base>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://had.co.nz/


From islandelephants at gmail.com  Mon Sep 21 08:57:39 2015
From: islandelephants at gmail.com (=?UTF-8?Q?Alexander_Mo=C3=9Fbrucker?=)
Date: Mon, 21 Sep 2015 13:57:39 +0700
Subject: [R] ERROR - non-conformable arguments
In-Reply-To: <CAM4izHsC51WMj_xq0SFNVA=yPg-StYtSkgdyBaJ62uN+WzmV9w@mail.gmail.com>
References: <mailman.4158.1442732527.3797.r-help@r-project.org>
	<CAM4izHsC51WMj_xq0SFNVA=yPg-StYtSkgdyBaJ62uN+WzmV9w@mail.gmail.com>
Message-ID: <CAM4izHsjGByY1f60NTDzO1MKs-PGOiYd-9Ey8BHWbQGfkKyE_Q@mail.gmail.com>

Hi all,

I am sorry I forgot to attach the excel file and txt files with the test
data I used for this example.....

I am almost sure the problem was caused by the way R is reading this data,

Hope to get some help, best, Alex

2015-09-20 14:03 GMT+07:00 Alexander Mo?brucker <islandelephants at gmail.com>:

>
> Hi,
>
> I am using adehabitatHS for habitat selection analysis using widesII(),
> when
> running the script I get the error message: Error in as.vector(X) %*%
> t(as.vector(Y)) : non-conformable arguments
>
> Here the script I am using including the data
>
> > #dataset (read from tab deliminated text file)
> >
> > available
>   Riparian Conifer MtShrub1 Aspen RockOutcrop Bitterbrush Windblown
> MtShrub2
> PresBurn
> 1     0.06    0.13     0.16  0.15        0.06        0.17      0.12
>  0.04
> 0.09
>   Clearcut
> 1     0.02
> > used
>      Riparian Conifer MtShrub1 Aspen RockOutcrop Bitterbrush Windblown
> MtShrub2 PresBurn
> Ani1        0       0        0     2           0          16         5
> 14       28
> Ani2        0       2        1     2           2           5        10
> 10       35
> Ani3        0       1        2     1           0          14         9
> 8       40
> Ani4        0       1        3     7           5           3         6
> 9       31
> Ani5        0       0        2     2           5          18        10
> 6       25
> Ani6        0       2        1     4           2           7         6
> 15       19
>      Clearcut
> Ani1        8
> Ani2        9
> Ani3        4
> Ani4        9
> Ani5        0
> Ani6       19
> >
> > #analysis
> >
> > widesII(used, available, avknown = TRUE, alpha = 0.05)
> Error in as.vector(X) %*% t(as.vector(Y)) : non-conformable arguments
>
> Note: I am using an example dataset to test out a script I want to use to
> analyse my own dataset, for this I exported the example dataset to excel,
> converted it to tab deliminated text file, then imported it.
>
> All worked well when using the example dataset provided by adehabitatHS,
> thus I suspect that somhow my test dataset is not correctly read in or
> something similar.
>
> I am quite new to R, thus apologize for any shortcomings in reporting etc..
>
> Many thanks, best, Alex
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/ERROR-non-conformable-arguments-tp4712516.html
> Sent from the R help mailing list archive at Nabble.com.
>
>
>
>
-------------- next part --------------
Riparian	Conifer	MtShrub1	Aspen	RockOutcrop	Bitterbrush	Windblown	MtShrub2	PresBurn	Clearcut
0.06	0.13	0.16	0.15	0.06	0.17	0.12	0.04	0.09	0.02
-------------- next part --------------
	Riparian	Conifer	MtShrub1	Aspen	RockOutcrop	Bitterbrush	Windblown	MtShrub2	PresBurn	Clearcut
Ani1	0	0	0	2	0	16	5	14	28	8
Ani2	0	2	1	2	2	5	10	10	35	9
Ani3	0	1	2	1	0	14	9	8	40	4
Ani4	0	1	3	7	5	3	6	9	31	9
Ani5	0	0	2	2	5	18	10	6	25	0
Ani6	0	2	1	4	2	7	6	15	19	19

From aolinto.lst at gmail.com  Sun Sep 20 16:52:07 2015
From: aolinto.lst at gmail.com (Antonio Silva)
Date: Sun, 20 Sep 2015 11:52:07 -0300
Subject: [R] Fwd: gdal_translate and gdalwarp question
In-Reply-To: <CAE8g1gPWbDwhHOuxdg+_oCNwwdHK7b+9w5oHW0UEymgqZOWWUA@mail.gmail.com>
References: <CAE8g1gPWbDwhHOuxdg+_oCNwwdHK7b+9w5oHW0UEymgqZOWWUA@mail.gmail.com>
Message-ID: <CAE8g1gP+7eQn=BBDH=5hs_aFxx=jebAOO6vj4sxKh+e1CU-UOw@mail.gmail.com>

Hello,

I have some doubts on the usage of some gdal tools.

After converting a hdf file to tif I want to reproject to SIRGAS2000 and
clip between lats of 22 to 29 S and lats of 40 to 50 W.

HDF file can be downloaded at
https://app.box.com/s/16cf7qv6af6gsz1v66staori2mtneu0r

Basically I'm following
https://scottishsnow.wordpress.com/2014/08/24/many-rastered-beast/

Well to convert HDF file I'm using:

gdal_translate("A20080012008031.L3m_MO_SST_4","georef.tif",sd_index=1,a_ullr=c(0,4320,8640,0),
a_srs="+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +a=6371007
+b=6371007 +units=m +no_defs")

Without a_ullr and a_srs options I was getting an error message when using
gdalwarp: "ERROR 1: Unable to compute a transformation between pixel/line
and georeferenced coordinates"

a_ullr and a_srs values I got with GDALinfo("georef.tif"). I also tryed
a_ullr=c(-180,90,180,-90).

map <- raster("georef.tif")
plot(map)

My problem now is reproject to SIRGAS2000 and clip the image georef.tif:

gdalwarp("georef.tif", "georef2.tif",
s_srs="+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +a=6371007
+b=6371007 +units=m +no_defs",
t_srs="+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs")

map2 <- raster("georef2.tif")
plot(map2)

Map2 is not in SIRGAS2000 projection and clipping option
te=c(-50,-29,-40,-22) does not work.

Where is my mistake? I hope someone can tell me.

Thanks for any help.

Antonio Olinto



















-- 
Ant?nio Olinto ?vila da Silva
Bi?logo / Ocean?grafo
Instituto de Pesca (Fisheries Institute)
S?o Paulo, Brasil



-- 
Ant?nio Olinto ?vila da Silva
Bi?logo / Ocean?grafo
Instituto de Pesca (Fisheries Institute)
S?o Paulo, Brasil

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Sep 21 12:38:27 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 21 Sep 2015 10:38:27 +0000
Subject: [R] ERROR - non-conformable arguments
In-Reply-To: <CAM4izHsjGByY1f60NTDzO1MKs-PGOiYd-9Ey8BHWbQGfkKyE_Q@mail.gmail.com>
References: <mailman.4158.1442732527.3797.r-help@r-project.org>
	<CAM4izHsC51WMj_xq0SFNVA=yPg-StYtSkgdyBaJ62uN+WzmV9w@mail.gmail.com>
	<CAM4izHsjGByY1f60NTDzO1MKs-PGOiYd-9Ey8BHWbQGfkKyE_Q@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C40957@SRVEXCHMBX.precheza.cz>

Hi

Do not use attachment, copy result of dput(object) to your mail instead. This enables us to see how your data really look like.

I get different error.
> widesII(used, available, avknown = TRUE, alpha = 0.05)
Error in widesII(used, available, avknown = TRUE, alpha = 0.05) :
  used and available matrices should have the same number of habitats

The error is due to fact that row names were read as column of data frame. After correcting this I got the same error as you.

> widesII(used, available, avknown = TRUE, alpha = 0.05)
Error in as.vector(X) %*% t(as.vector(Y)) : non-conformable arguments

I checked the help page and I noticed that

a
for widesI and widesII, a vector with named elements describing the sample or the proportion of available resource units. For widesIII a matrix or a data frame giving the number or the proportion of available resource units for each animal (in rows) in each resource category (in columns)

but your "available" is a data frame.


When I change it to named vector (as required)

> dput(avail)
structure(c(0.06, 0.13, 0.16, 0.15, 0.06, 0.17, 0.12, 0.04, 0.09,
0.02), .Names = c("Riparian", "Conifer", "MtShrub1", "Aspen",
"RockOutcrop", "Bitterbrush", "Windblown", "MtShrub2", "PresBurn",
"Clearcut"))

widesII came with result without complain.

> widesII(used, avail, avknown = TRUE, alpha = 0.05)


************** Manly's Selection ratios for design II ********

1. Test of identical use of habitat by all animals
   (Classical Khi-2 performed on the used matrix):
      Khi2L1           df       pvalue

So the lesson is:

1. Read the help page.
2. Look to your data by ?str
3. When your data are as expected by the function, check if there is no syntax problem.
4. Try to simplify the example and send it to help list preferably by ?dput.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Alexander Mo?brucker
> Sent: Monday, September 21, 2015 8:58 AM
> To: r-help at r-project.org; jdnewmil at dcn.davis.ca.us
> Subject: Re: [R] ERROR - non-conformable arguments
>
> Hi all,
>
> I am sorry I forgot to attach the excel file and txt files with the
> test data I used for this example.....
>
> I am almost sure the problem was caused by the way R is reading this
> data,
>
> Hope to get some help, best, Alex
>
> 2015-09-20 14:03 GMT+07:00 Alexander Mo?brucker
> <islandelephants at gmail.com>:
>
> >
> > Hi,
> >
> > I am using adehabitatHS for habitat selection analysis using
> > widesII(), when running the script I get the error message: Error in
> > as.vector(X) %*%
> > t(as.vector(Y)) : non-conformable arguments
> >
> > Here the script I am using including the data
> >
> > > #dataset (read from tab deliminated text file)
> > >
> > > available
> >   Riparian Conifer MtShrub1 Aspen RockOutcrop Bitterbrush Windblown
> > MtShrub2
> > PresBurn
> > 1     0.06    0.13     0.16  0.15        0.06        0.17      0.12
> >  0.04
> > 0.09
> >   Clearcut
> > 1     0.02
> > > used
> >      Riparian Conifer MtShrub1 Aspen RockOutcrop Bitterbrush
> Windblown
> > MtShrub2 PresBurn
> > Ani1        0       0        0     2           0          16
> 5
> > 14       28
> > Ani2        0       2        1     2           2           5
> 10
> > 10       35
> > Ani3        0       1        2     1           0          14
> 9
> > 8       40
> > Ani4        0       1        3     7           5           3
> 6
> > 9       31
> > Ani5        0       0        2     2           5          18
> 10
> > 6       25
> > Ani6        0       2        1     4           2           7
> 6
> > 15       19
> >      Clearcut
> > Ani1        8
> > Ani2        9
> > Ani3        4
> > Ani4        9
> > Ani5        0
> > Ani6       19
> > >
> > > #analysis
> > >
> > > widesII(used, available, avknown = TRUE, alpha = 0.05)
> > Error in as.vector(X) %*% t(as.vector(Y)) : non-conformable arguments
> >
> > Note: I am using an example dataset to test out a script I want to
> use
> > to analyse my own dataset, for this I exported the example dataset to
> > excel, converted it to tab deliminated text file, then imported it.
> >
> > All worked well when using the example dataset provided by
> > adehabitatHS, thus I suspect that somhow my test dataset is not
> > correctly read in or something similar.
> >
> > I am quite new to R, thus apologize for any shortcomings in reporting
> etc..
> >
> > Many thanks, best, Alex
> >
> >
> >
> > --
> > View this message in context:
> > http://r.789695.n4.nabble.com/ERROR-non-conformable-arguments-
> tp471251
> > 6.html Sent from the R help mailing list archive at Nabble.com.
> >
> >
> >
> >

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From profjcnash at gmail.com  Sun Sep 20 20:56:44 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Sun, 20 Sep 2015 14:56:44 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
Message-ID: <55FF016C.7000900@gmail.com>

I posted a suggestion to use nlmrt package (function nlxb to be 
precise), which has masked (fixed) parameters. Examples in my 2014 book 
on Nonlinear parameter optimization with R tools. However, I'm 
travelling just now, or would consider giving this a try.

JN

On 15-09-20 01:19 PM, Jianling Fan wrote:
> no, I am doing a regression with 6 group data with 2 shared parameters
> and 1 different parameter for each group data. the parameter I want to
> coerce is for one group. I don't know how to do it. Any suggestion?
>
> Thanks!
>
> On 19 September 2015 at 13:33, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>> Why not rewrite the function so that value is not a parameter?
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>>                                        Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> ---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>>
>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan <fanjianling at gmail.com> wrote:
>>> Hello, everyone,
>>>
>>> I am using a nls regression with 6 groups data. I am trying to coerce
>>> a parameter to 1 by using a upper and lower statement. but I always
>>> get an error like below:
>>>
>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>   (list) object cannot be coerced to type 'double'
>>>
>>> does anyone know how to fix it?
>>>
>>> thanks in advance!
>>>
>>> My code is below:
>>>
>>>
>>>
>>>> dproot
>>>    depth       den ref
>>> 1     20 0.5730000   1
>>> 2     40 0.7800000   1
>>> 3     60 0.9470000   1
>>> 4     80 0.9900000   1
>>> 5    100 1.0000000   1
>>> 6     10 0.6000000   2
>>> 7     20 0.8200000   2
>>> 8     30 0.9300000   2
>>> 9     40 1.0000000   2
>>> 10    20 0.4800000   3
>>> 11    40 0.7340000   3
>>> 12    60 0.9610000   3
>>> 13    80 0.9980000   3
>>> 14   100 1.0000000   3
>>> 15    20 3.2083491   4
>>> 16    40 4.9683383   4
>>> 17    60 6.2381133   4
>>> 18    80 6.5322348   4
>>> 19   100 6.5780660   4
>>> 20   120 6.6032064   4
>>> 21    20 0.6140000   5
>>> 22    40 0.8270000   5
>>> 23    60 0.9500000   5
>>> 24    80 0.9950000   5
>>> 25   100 1.0000000   5
>>> 26    20 0.4345774   6
>>> 27    40 0.6654726   6
>>> 28    60 0.8480684   6
>>> 29    80 0.9268951   6
>>> 30   100 0.9723207   6
>>> 31   120 0.9939966   6
>>> 32   140 0.9992400   6
>>>
>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>>>> summary(fitdp)
>>>
>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>>>
>>> Parameters:
>>>     Estimate Std. Error t value Pr(>|t|)
>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>>> ---
>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>>>
>>> Residual standard error: 0.1094 on 24 degrees of freedom
>>>
>>> Number of iterations to convergence: 8
>>> Achieved convergence tolerance: 9.374e-06
>>>
>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>> algorithm="port",
>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>>>
>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>   (list) object cannot be coerced to type 'double'
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>


From drjimlemon at gmail.com  Mon Sep 21 12:52:15 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 21 Sep 2015 20:52:15 +1000
Subject: [R] HELP IN GRAPHS - slip screen (URGENT)
In-Reply-To: <4FCCB6B8-BCCF-42F5-8C85-D9DF43328A5E@gmail.com>
References: <F53D3AF7-922F-4352-B554-1E7F5244AB75@gmail.com>
	<CA+8X3fUpj3vfn=V8rrUNnRSMpu2R9D2DB1gfNQnMQ_tYC96iuQ@mail.gmail.com>
	<CAB630FFBi7xFeZ1GtfZNzcmbSGpgiMB8s_xL8iNEFOgGud1f6Q@mail.gmail.com>
	<CA+8X3fXF0t8BmuEq6MVeEYnsNjLRid-k+xFFO+AKzWmoy_dDqg@mail.gmail.com>
	<CA+8X3fUVNzSg9JNsP-cVG8gU7jscoE6FXwT324nbgitiHuCG3A@mail.gmail.com>
	<4FCCB6B8-BCCF-42F5-8C85-D9DF43328A5E@gmail.com>
Message-ID: <CA+8X3fVDPW7jJar9amTB3W7yzor8O9yKNqN1HH-tyMiCsOHsyw@mail.gmail.com>

Hi Rosa,
I think I understand more or less what you want, but the example below uses
made up data as I do not have access to your files. The following code
should produce a block of eight plots with the y axes on the leftmost plots
only and the x axes on the bottom row only. I think I have what you wanted
for the titles. You can get a gap between the two rows of plots if you add
a 0.5 bottom margin to the top row and a 0.5 top margin to the bottom row.
All you have to do is use your data instead of the made up data. I used a
device with width=12 and height=5.

# do the first split, to get the rightmost screen for the legend
split.screen(figs=matrix(c(0,0.84,0,1,0.84,1,0,1),nrow=2,byrow=TRUE))
# now split the first screen to get your eight screens
# (numbered 3 to 10) for the plots
split.screen(figs=matrix(c(0,0.31,0.5,1,
                           0.31,0.54,0.5,1,
                           0.54,0.77,0.5,1,
                           0.77,1,0.5,1,
                           0,0.31,0,0.5,
                           0.31,0.54,0,0.5,
                           0.54,0.77,0,0.5,
                           0.77,1,0,0.5),
                         ncol=4,byrow=TRUE),screen=1)

# now make up some data so that I can try out the plot
lambda<-seq(0.7,0.95,by=0.05)
ols_bias_alpha1_250<-c(0.6,0.5,0.4,0.3,0.2,0.1)
ols_bias_alpha1_1000<-c(0.62,0.53,0.44,0.35,0.26,0.17)
gls_bias_alpha1_250<-c(-0.05,0,0.01,0.02,0,-0.01)
gls_bias_alpha1_1000<-c(0.09,0.08,0.07,0.05,0.04,0.045)

screen(3)
par(mar=c(0,4,3,0))
# plot bias for alpha1 250
matplot(lambda,matrix(c(ols_bias_alpha1_250,gls_bias_alpha1_250),ncol=2),
 type="l",col=c(4,2),xaxt="n",ylim=c(-.1, .6),
 ylab="", cex.main=1)
abline(h = 0, col = "gray60")
mtext("nsample=250",side=3,line=0.6)
mtext(expression(paste("Bias av. for  ",alpha[1])),side=2,line=2,
cex.main=1)

screen(4)
par(mar=c(0,0,3,0))
# plot bias for alpha1 1000
matplot(lambda,matrix(c(ols_bias_alpha1_1000,gls_bias_alpha1_1000),ncol=2),
 type="l",col=c(4,2),xaxt="n",yaxt="n",ylim=c(-.1, .6),
 ylab="", cex.main=1)
abline(h = 0, col = "gray60")
mtext("nsample=1000",side=3,line=0.6)
# now the centered title
par(xpd=TRUE)
mtext("alpha1",side=3,line=1.7,at=0.69)
par(xpd=FALSE)

# make up more data
ols_bias_alpha2_250<-c(-0.04,-0.05,-0.06,-0.05,-0.04,-0.03)
ols_bias_alpha2_1000<-c(-0.04,-0.05,-0.06,-0.05,-0.04,-0.035)
gls_bias_alpha2_250<-c(0.1,0.08,0.06,0.04,0.03,0.01)
gls_bias_alpha2_1000<-c(0.1,0.08,0.06,0.04,0.03,0.01)

screen(5)
par(mar=c(0,0,3,0))
# plot bias for alpha2 250
matplot(lambda,matrix(c(ols_bias_alpha2_250,gls_bias_alpha2_250),ncol=2),
 type="l",col=c(4,2),xaxt="n",yaxt="n",ylim=c(-.1, .6),
 ylab="", cex.main=1)
abline(h = 0, col = "gray60")
mtext("nsample=250",side=3,line=0.6)

screen(6)
par(mar=c(0,0,3,0))
# plot bias for alpha2 1000
matplot(lambda,matrix(c(ols_bias_alpha2_1000,gls_bias_alpha2_1000),ncol=2),
 type="l",col=c(4,2),xaxt="n",yaxt="n",ylim=c(-.1, .6),
 ylab="", cex.main=1)
abline(h = 0, col = "gray60")
mtext("nsample=1000",side=3,line=0.6)
# now the centered title
par(xpd=TRUE)
mtext("alpha2",side=3,line=1.7,at=0.69)
par(xpd=FALSE)

# more data!
ols_se_alpha1_250<-c(0.45,0.46,0.47,0.48,0.49,0.51)
gls_se_alpha1_250<-c(0.7,0.62,0.58,0.56,0.54,0.52)
ols_se_alpha1_1000<-c(0.2,0.21,0.22,0.23,0.24,0.251)
gls_se_alpha1_1000<-c(0.3,0.29,0.28,0.27,0.26,0.252)

screen(7)
par(mar=c(3,4,0,0))
# plot SE for alpha1 250
matplot(lambda,matrix(c(ols_se_alpha1_250,gls_se_alpha1_250),ncol=2),
 type="l",col=c(4,2),ylim=c(0,1.1),
 ylab="", cex.main=1)
mtext(expression(paste("SE av. for  ",alpha[1])),side=2,line=2, cex.main=1)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)

screen(8)
par(mar=c(3,0,0,0))
# plot SE for alpha1 1000
matplot(lambda,matrix(c(ols_se_alpha1_1000,gls_se_alpha1_1000),ncol=2),
 type="l",col=c(4,2),ylim=c(0,1.1),yaxt="n",
 ylab="", cex.main=1)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)

# even more data!
ols_se_alpha2_250<-c(0.12,0.13,0.14,0.16,0.18,0.2)
gls_se_alpha2_250<-c(0.3,0.26,0.24,0.22,0.21,0.2)
ols_se_alpha2_1000<-c(0.1,0.11,0.12,0.13,0.14,0.15)
gls_se_alpha2_1000<-c(0.2,0.19,0.18,0.17,0.16,0.15)

screen(9)
par(mar=c(3,0,0,0))
# plot SE for alpha1 250
matplot(lambda,matrix(c(ols_se_alpha2_250,gls_se_alpha2_250),ncol=2),
 type="l",col=c(4,2),ylim=c(0,1.1),
 yaxt="n",ylab="", cex.main=1)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)

screen(10)
par(mar=c(3,0,0,0))
# plot SE for alpha1 1000
matplot(lambda,matrix(c(ols_se_alpha2_1000,gls_se_alpha2_1000),ncol=2),
 type="l",col=c(4,2),ylim=c(0,1.1),yaxt="n",
 ylab="", cex.main=1)
mtext(expression(paste(lambda)),side=1,line=2, cex.main=1.5)

# finally the legend
screen(2)
par(mar=c(0,0,0,0))
# plot an empty plot to get the coordinates
plot(0:1,0:1,type="n",axes=FALSE)
legend(0,0.6,c("OLS", "GLS", "Reg. Cal.", "0"),bty = "n",
lty=1:3,col=c(4,2,3,"gray60"),xpd=TRUE)

close.screen(all=TRUE)

Jim


On Mon, Sep 21, 2015 at 2:14 AM, Rosa Oliveira <rosita21 at gmail.com> wrote:

> Dear Jim,
>
>
> I?ve tried till today, but I could not solve the problems.
>
> 1. despite the scales are the same (equal: lambda ={0.70, 0.75, 0.80,
> 0.85, 0.90, 0.95, 0.98}), the "matrix" is not equal. If you see, for n =
> 250 the column is narrower than for n = 1000, and "lambda" has the same
> values.
>
> 2. the first 2 columns relate to alpha 1 and the second two columns
> alpha2. Is it possible to place a title above nsample that concerns the
> first 2 columns and other over the last 2?
>
> something like:
>
>                     *alpha1*
>             *alpha2*
> nsample=250      nsample=1000                   nsample=250
>  nsample=1000
>
>
> 3. Notice that have 4 separate drawings and must place the 4 "groups"
> together.
> ...
>

	[[alternative HTML version deleted]]


From klebyn at yahoo.com.br  Mon Sep 21 04:03:35 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sun, 20 Sep 2015 23:03:35 -0300
Subject: [R] how to add 1 + 1 with the interface between R and C
In-Reply-To: <55FF5546.4000201@yahoo.com.br>
References: <55FF5546.4000201@yahoo.com.br>
Message-ID: <55FF6577.9050400@yahoo.com.br>

Dear useRs,

I would like some help on how to make the sum of 1 + 1
but using the interface between A and C.

the function call does .call lock and close the R.

Thank you for any help it.

Cleber
#######################
############  R code
text_code <-"
#include <stdlib.h>
#include <R.h>
#include <Rdefines.h>
void testfun( double *k, double *res );
void testfun( double *k, double *res )
{
     res = k + 1;
}
"
sink('test.c')
cat( text_code )
sink()
system("R CMD SHLIB test.c")
dyn.load( "test.dll" )
is.loaded( 'testfun' )
k=1
.C('testfun', k=as.double(k), res=as.double(0) )

####################################
###  Output

 > text_code <-"
+ #include <stdlib.h>
+ #include <R.h>
+ #include <Rdefines.h>
+ void testfun( double *k, double *res );
+ void testfun( double *k, double *res )
+ {
+ res = k + 1;
+ }
+ "
 > sink('test.c')
 > cat( text_code )
 > sink()
 > system("R CMD SHLIB test.c")
cygwin warning:
   MS-DOS style path detected: C:/R/etc/x64/Makeconf
   Preferred POSIX equivalent is: /cygdrive/c/R/etc/x64/Makeconf
   CYGWIN environment variable option "nodosfilewarning" turns off this 
warning.
   Consult the user's guide for more details about POSIX paths:
     http://cygwin.com/cygwin-ug-net/using.html#using-pathnames
gcc -m64 -I"C:/R/include" -DNDEBUG 
-I"d:/RCompile/r-compiling/local/local320/include"     -O2 -Wall  
-std=gnu99 -mtune=core2 -c test.c -o test.o
gcc -m64 -shared -s -static-libgcc -o test.dll tmp.def test.o 
-Ld:/RCompile/r-compiling/local/local320/lib/x64 
-Ld:/RCompile/r-compiling/local/local320/lib -LC:/R/bin/x64 -lR
 >
 > dyn.load( "test.dll" )
 >
 > is.loaded( 'testfun' )
[1] TRUE
 > k=1
 > .C('testfun', k=as.double(k), res=as.double(0) )
$k
[1] 1

$res
[1] 0












---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From pdalgd at gmail.com  Mon Sep 21 16:33:34 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 21 Sep 2015 16:33:34 +0200
Subject: [R] how to add 1 + 1 with the interface between R and C
In-Reply-To: <55FF6577.9050400@yahoo.com.br>
References: <55FF5546.4000201@yahoo.com.br> <55FF6577.9050400@yahoo.com.br>
Message-ID: <D97A25A9-E9E5-4550-8E3E-8CCE5514A155@gmail.com>


> On 21 Sep 2015, at 04:03 , Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> 
> Dear useRs,
> 
> I would like some help on how to make the sum of 1 + 1
> but using the interface between A and C.
> 
> the function call does .call lock and close the R.
> 
> Thank you for any help it.

Either you need a basic course in C, or you need a refresher....:


> 
> void testfun( double *k, double *res )
> {
>    res = k + 1;
> }

C is call by value and k and res are pointers. You need a dereferencing step or nothing with happen. Try

*res = *k + 1;


-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From emptican at gmail.com  Mon Sep 21 17:06:54 2015
From: emptican at gmail.com (SH)
Date: Mon, 21 Sep 2015 11:06:54 -0400
Subject: [R] R code help!
In-Reply-To: <CAN5YmCH_D0w1-+sCWDPahqgEUCOLQTXq+rNGfw9CuoYgvk71-A@mail.gmail.com>
References: <CALSKosCcLNPX6jtSaJeJx+cxVt5PG9WxLui181uhjYPZDN89bA@mail.gmail.com>
	<CAN5YmCH_D0w1-+sCWDPahqgEUCOLQTXq+rNGfw9CuoYgvk71-A@mail.gmail.com>
Message-ID: <CALSKosDxchTTJZNjRQNVPeG5CJQPV7=cEyAw9bjHr9DE+3U-gg@mail.gmail.com>

Hi Jean,

Thank you so much!

Steve

On Sat, Sep 19, 2015 at 1:02 PM, Adams, Jean <jvadams at usgs.gov> wrote:

> Here's one way to save your results, using a list of lists and a for()
> loop.
>
> nsim <- 100
> outputs <- vector("list", nsim)
> for(i in 1:nsim) {
>   outputs[[i]] <- sim.f(p.s=.05, N=1000, sample.size=69, n.sim=500)
> }
>
> Jean
>
> On Fri, Sep 18, 2015 at 2:27 PM, SH <emptican at gmail.com> wrote:
>
>> Dear R users,
>>
>> I am trying to simulate surveys and the survey result will be used to
>> determine the population to be "accepted" or "rejected".  With the
>> results,
>> I would like to calculate cumulative means and plot them to see if a
>> converged value is as expected.  Below is R-code I generated.  I need a
>> help to repeat this simulation code as many times (e.g., 100) and keep the
>> results as list format if possible.  Could you give me any insight?
>>
>> Thanks  a lot in advance,
>>
>> Steve
>>
>> sim.f <- function(p.s, N, sample.size, n.sim) {
>> pop = sampled.pop = decision = decisionB = cum.mn = as.list(NULL)
>> for(i in 1:n.sim) {
>>    p <- c(rep(1, p.s*N), pop2 <- rep(0, N*(1-p.s))) # Generate sample
>> space
>>    pop[[i]] <- sample(p) # Randomization sample space
>>    sampled.pop[[i]] <- sample(pop[[i]], sample.size)# Random sampling
>>    decision[i] <- ifelse(sum(sampled.pop[[i]])>=1, 'Reject','Pass') #
>> Decision for each group of n.sim
>>    decisionB <- ifelse(decision == 'Reject', 1, 0) # Convert to binary
>>    cum.mn <- cumsum(decisionB) / seq_along(decisionB) # Cummulative mean
>> of
>> n.sim group decisions
>>    }
>> result = list(population=pop,
>>   pop_sub = sampled.pop,
>>   decision = decision,
>>   decisionB = decisionB,
>>   cum.mn = cum.mn)
>> }
>> sim.out <- sim.f(p.s=.05, N=1000, sample.size=69, n.sim=500)
>> # I want to repeat this simulation function for example 100 times or and
>> also #keep the data so that I can explore later.  If it is not possible to
>> keep all #outputs, at least I would like to have cum.mn outputs.
>>
>> summary(sim.out)
>> sim.out$population
>> sim.out$pop_sub
>> sim.out$decision
>> sim.out$decisionB
>> y1 <- sim.out$cum.mn
>> #plot(y1, type='l')
>> lines(y2, type='l')
>> ...
>> lines(y100, type='l')
>> abline(h=.95, col='red')
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From fanjianling at gmail.com  Mon Sep 21 18:22:53 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Mon, 21 Sep 2015 10:22:53 -0600
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <55FF016C.7000900@gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
Message-ID: <CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>

Thanks Prof. Nash,

Sorry for late reply. I am learning and trying to use your nlmrt
package since I got your email. It works good to mask a parameter in
regression but seems does work for my equation. I think the problem is
that the parameter I want to mask is a group-specific parameter and I
have a "[]" syntax in my equation. However, I don't have your 2014
book on hand and couldn't find it in our library. So I am wondering if
nlxb works for group data?
Thanks a lot!

following is my code and I got a error form it.

> fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
                + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
Rm5=1.01, Rm6=1, d50=20, c=-1),
                + masked=c("Rm6"))

Error in deriv.default(parse(text = resexp), names(start)) :
  Function '`[`' is not in the derivatives table


Best regards,

Jianling


On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
> I posted a suggestion to use nlmrt package (function nlxb to be precise),
> which has masked (fixed) parameters. Examples in my 2014 book on Nonlinear
> parameter optimization with R tools. However, I'm travelling just now, or
> would consider giving this a try.
>
> JN
>
>
> On 15-09-20 01:19 PM, Jianling Fan wrote:
>>
>> no, I am doing a regression with 6 group data with 2 shared parameters
>> and 1 different parameter for each group data. the parameter I want to
>> coerce is for one group. I don't know how to do it. Any suggestion?
>>
>> Thanks!
>>
>> On 19 September 2015 at 13:33, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>> wrote:
>>>
>>> Why not rewrite the function so that value is not a parameter?
>>>
>>> ---------------------------------------------------------------------------
>>> Jeff Newmiller                        The     .....       .....  Go
>>> Live...
>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>> Go...
>>>                                        Live:   OO#.. Dead: OO#..  Playing
>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>> rocks...1k
>>>
>>> ---------------------------------------------------------------------------
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>>> <fanjianling at gmail.com> wrote:
>>>>
>>>> Hello, everyone,
>>>>
>>>> I am using a nls regression with 6 groups data. I am trying to coerce
>>>> a parameter to 1 by using a upper and lower statement. but I always
>>>> get an error like below:
>>>>
>>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>>   (list) object cannot be coerced to type 'double'
>>>>
>>>> does anyone know how to fix it?
>>>>
>>>> thanks in advance!
>>>>
>>>> My code is below:
>>>>
>>>>
>>>>
>>>>> dproot
>>>>
>>>>    depth       den ref
>>>> 1     20 0.5730000   1
>>>> 2     40 0.7800000   1
>>>> 3     60 0.9470000   1
>>>> 4     80 0.9900000   1
>>>> 5    100 1.0000000   1
>>>> 6     10 0.6000000   2
>>>> 7     20 0.8200000   2
>>>> 8     30 0.9300000   2
>>>> 9     40 1.0000000   2
>>>> 10    20 0.4800000   3
>>>> 11    40 0.7340000   3
>>>> 12    60 0.9610000   3
>>>> 13    80 0.9980000   3
>>>> 14   100 1.0000000   3
>>>> 15    20 3.2083491   4
>>>> 16    40 4.9683383   4
>>>> 17    60 6.2381133   4
>>>> 18    80 6.5322348   4
>>>> 19   100 6.5780660   4
>>>> 20   120 6.6032064   4
>>>> 21    20 0.6140000   5
>>>> 22    40 0.8270000   5
>>>> 23    60 0.9500000   5
>>>> 24    80 0.9950000   5
>>>> 25   100 1.0000000   5
>>>> 26    20 0.4345774   6
>>>> 27    40 0.6654726   6
>>>> 28    60 0.8480684   6
>>>> 29    80 0.9268951   6
>>>> 30   100 0.9723207   6
>>>> 31   120 0.9939966   6
>>>> 32   140 0.9992400   6
>>>>
>>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>>
>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>>>>>
>>>>> summary(fitdp)
>>>>
>>>>
>>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>>>>
>>>> Parameters:
>>>>     Estimate Std. Error t value Pr(>|t|)
>>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>>>> ---
>>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>>>>
>>>> Residual standard error: 0.1094 on 24 degrees of freedom
>>>>
>>>> Number of iterations to convergence: 8
>>>> Achieved convergence tolerance: 9.374e-06
>>>>
>>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>>
>>>> algorithm="port",
>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>>>>
>>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>>   (list) object cannot be coerced to type 'double'
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Jianling Fan
???


From ggrothendieck at gmail.com  Mon Sep 21 18:43:42 2015
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Sep 2015 12:43:42 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
Message-ID: <CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>

Express the formula in terms of simple operations like this:

# add 0/1 columns ref.1, ref.2, ..., ref.6
dproot2 <- do.call(data.frame, transform(dproot, ref = outer(dproot$ref,
seq(6), "==") + 0))

# now express the formula in terms of the new columns
library(nlmrt)
fitdp1<-nlxb(den ~ (Rm1 * ref.1 + Rm2 * ref.2 + Rm3 * ref.3 + Rm4 * ref.4 +
Rm5 * ref.5 + Rm6 * ref.6)/(1+(depth/d50)^c),
         data = dproot2,
         start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01, Rm6=1,
d50=20, c=-1),
         masked = "Rm6")

where we used this input:

Lines <- "   depth       den ref
1     20 0.5730000   1
2     40 0.7800000   1
3     60 0.9470000   1
4     80 0.9900000   1
5    100 1.0000000   1
6     10 0.6000000   2
7     20 0.8200000   2
8     30 0.9300000   2
9     40 1.0000000   2
10    20 0.4800000   3
11    40 0.7340000   3
12    60 0.9610000   3
13    80 0.9980000   3
14   100 1.0000000   3
15    20 3.2083491   4
16    40 4.9683383   4
17    60 6.2381133   4
18    80 6.5322348   4
19   100 6.5780660   4
20   120 6.6032064   4
21    20 0.6140000   5
22    40 0.8270000   5
23    60 0.9500000   5
24    80 0.9950000   5
25   100 1.0000000   5
26    20 0.4345774   6
27    40 0.6654726   6
28    60 0.8480684   6
29    80 0.9268951   6
30   100 0.9723207   6
31   120 0.9939966   6
32   140 0.9992400   6"

dproot <- read.table(text = Lines, header = TRUE)



On Mon, Sep 21, 2015 at 12:22 PM, Jianling Fan <fanjianling at gmail.com>
wrote:

> Thanks Prof. Nash,
>
> Sorry for late reply. I am learning and trying to use your nlmrt
> package since I got your email. It works good to mask a parameter in
> regression but seems does work for my equation. I think the problem is
> that the parameter I want to mask is a group-specific parameter and I
> have a "[]" syntax in my equation. However, I don't have your 2014
> book on hand and couldn't find it in our library. So I am wondering if
> nlxb works for group data?
> Thanks a lot!
>
> following is my code and I got a error form it.
>
> > fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>                 + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
> Rm5=1.01, Rm6=1, d50=20, c=-1),
>                 + masked=c("Rm6"))
>
> Error in deriv.default(parse(text = resexp), names(start)) :
>   Function '`[`' is not in the derivatives table
>
>
> Best regards,
>
> Jianling
>
>
> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
> > I posted a suggestion to use nlmrt package (function nlxb to be precise),
> > which has masked (fixed) parameters. Examples in my 2014 book on
> Nonlinear
> > parameter optimization with R tools. However, I'm travelling just now, or
> > would consider giving this a try.
> >
> > JN
> >
> >
> > On 15-09-20 01:19 PM, Jianling Fan wrote:
> >>
> >> no, I am doing a regression with 6 group data with 2 shared parameters
> >> and 1 different parameter for each group data. the parameter I want to
> >> coerce is for one group. I don't know how to do it. Any suggestion?
> >>
> >> Thanks!
> >>
> >> On 19 September 2015 at 13:33, Jeff Newmiller <jdnewmil at dcn.davis.ca.us
> >
> >> wrote:
> >>>
> >>> Why not rewrite the function so that value is not a parameter?
> >>>
> >>>
> ---------------------------------------------------------------------------
> >>> Jeff Newmiller                        The     .....       .....  Go
> >>> Live...
> >>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> >>> Go...
> >>>                                        Live:   OO#.. Dead: OO#..
> Playing
> >>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >>> /Software/Embedded Controllers)               .OO#.       .OO#.
> >>> rocks...1k
> >>>
> >>>
> ---------------------------------------------------------------------------
> >>> Sent from my phone. Please excuse my brevity.
> >>>
> >>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
> >>> <fanjianling at gmail.com> wrote:
> >>>>
> >>>> Hello, everyone,
> >>>>
> >>>> I am using a nls regression with 6 groups data. I am trying to coerce
> >>>> a parameter to 1 by using a upper and lower statement. but I always
> >>>> get an error like below:
> >>>>
> >>>> Error in ifelse(internalPars < upper, 1, -1) :
> >>>>   (list) object cannot be coerced to type 'double'
> >>>>
> >>>> does anyone know how to fix it?
> >>>>
> >>>> thanks in advance!
> >>>>
> >>>> My code is below:
> >>>>
> >>>>
> >>>>
> >>>>> dproot
> >>>>
> >>>>    depth       den ref
> >>>> 1     20 0.5730000   1
> >>>> 2     40 0.7800000   1
> >>>> 3     60 0.9470000   1
> >>>> 4     80 0.9900000   1
> >>>> 5    100 1.0000000   1
> >>>> 6     10 0.6000000   2
> >>>> 7     20 0.8200000   2
> >>>> 8     30 0.9300000   2
> >>>> 9     40 1.0000000   2
> >>>> 10    20 0.4800000   3
> >>>> 11    40 0.7340000   3
> >>>> 12    60 0.9610000   3
> >>>> 13    80 0.9980000   3
> >>>> 14   100 1.0000000   3
> >>>> 15    20 3.2083491   4
> >>>> 16    40 4.9683383   4
> >>>> 17    60 6.2381133   4
> >>>> 18    80 6.5322348   4
> >>>> 19   100 6.5780660   4
> >>>> 20   120 6.6032064   4
> >>>> 21    20 0.6140000   5
> >>>> 22    40 0.8270000   5
> >>>> 23    60 0.9500000   5
> >>>> 24    80 0.9950000   5
> >>>> 25   100 1.0000000   5
> >>>> 26    20 0.4345774   6
> >>>> 27    40 0.6654726   6
> >>>> 28    60 0.8480684   6
> >>>> 29    80 0.9268951   6
> >>>> 30   100 0.9723207   6
> >>>> 31   120 0.9939966   6
> >>>> 32   140 0.9992400   6
> >>>>
> >>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> >>>>
> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
> >>>>>
> >>>>> summary(fitdp)
> >>>>
> >>>>
> >>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
> >>>>
> >>>> Parameters:
> >>>>     Estimate Std. Error t value Pr(>|t|)
> >>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
> >>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
> >>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
> >>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
> >>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
> >>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
> >>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
> >>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
> >>>> ---
> >>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
> >>>>
> >>>> Residual standard error: 0.1094 on 24 degrees of freedom
> >>>>
> >>>> Number of iterations to convergence: 8
> >>>> Achieved convergence tolerance: 9.374e-06
> >>>>
> >>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> >>>>
> >>>> algorithm="port",
> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
> >>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
> >>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
> >>>>
> >>>> Error in ifelse(internalPars < upper, 1, -1) :
> >>>>   (list) object cannot be coerced to type 'double'
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >>
> >>
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Jianling Fan
> ???
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From fanjianling at gmail.com  Mon Sep 21 19:03:11 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Mon, 21 Sep 2015 11:03:11 -0600
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>
Message-ID: <CAJ7mryKyL+4ZVz8YNmi-9_jP7vT6wvibAWBmtv5nbnEt_3wnSw@mail.gmail.com>

Thanks Gabor,

 That works good to rewrite the express the formula!

Thanks a lot!

Regards,

Jianling

On 21 September 2015 at 10:43, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> Express the formula in terms of simple operations like this:
>
> # add 0/1 columns ref.1, ref.2, ..., ref.6
> dproot2 <- do.call(data.frame, transform(dproot, ref = outer(dproot$ref,
> seq(6), "==") + 0))
>
> # now express the formula in terms of the new columns
> library(nlmrt)
> fitdp1<-nlxb(den ~ (Rm1 * ref.1 + Rm2 * ref.2 + Rm3 * ref.3 + Rm4 * ref.4 +
> Rm5 * ref.5 + Rm6 * ref.6)/(1+(depth/d50)^c),
>          data = dproot2,
>          start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01, Rm6=1,
> d50=20, c=-1),
>          masked = "Rm6")
>
> where we used this input:
>
> Lines <- "   depth       den ref
> 1     20 0.5730000   1
> 2     40 0.7800000   1
> 3     60 0.9470000   1
> 4     80 0.9900000   1
> 5    100 1.0000000   1
> 6     10 0.6000000   2
> 7     20 0.8200000   2
> 8     30 0.9300000   2
> 9     40 1.0000000   2
> 10    20 0.4800000   3
> 11    40 0.7340000   3
> 12    60 0.9610000   3
> 13    80 0.9980000   3
> 14   100 1.0000000   3
> 15    20 3.2083491   4
> 16    40 4.9683383   4
> 17    60 6.2381133   4
> 18    80 6.5322348   4
> 19   100 6.5780660   4
> 20   120 6.6032064   4
> 21    20 0.6140000   5
> 22    40 0.8270000   5
> 23    60 0.9500000   5
> 24    80 0.9950000   5
> 25   100 1.0000000   5
> 26    20 0.4345774   6
> 27    40 0.6654726   6
> 28    60 0.8480684   6
> 29    80 0.9268951   6
> 30   100 0.9723207   6
> 31   120 0.9939966   6
> 32   140 0.9992400   6"
>
> dproot <- read.table(text = Lines, header = TRUE)
>
>
>
> On Mon, Sep 21, 2015 at 12:22 PM, Jianling Fan <fanjianling at gmail.com>
> wrote:
>>
>> Thanks Prof. Nash,
>>
>> Sorry for late reply. I am learning and trying to use your nlmrt
>> package since I got your email. It works good to mask a parameter in
>> regression but seems does work for my equation. I think the problem is
>> that the parameter I want to mask is a group-specific parameter and I
>> have a "[]" syntax in my equation. However, I don't have your 2014
>> book on hand and couldn't find it in our library. So I am wondering if
>> nlxb works for group data?
>> Thanks a lot!
>>
>> following is my code and I got a error form it.
>>
>> > fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>                 + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>> Rm5=1.01, Rm6=1, d50=20, c=-1),
>>                 + masked=c("Rm6"))
>>
>> Error in deriv.default(parse(text = resexp), names(start)) :
>>   Function '`[`' is not in the derivatives table
>>
>>
>> Best regards,
>>
>> Jianling
>>
>>
>> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
>> > I posted a suggestion to use nlmrt package (function nlxb to be
>> > precise),
>> > which has masked (fixed) parameters. Examples in my 2014 book on
>> > Nonlinear
>> > parameter optimization with R tools. However, I'm travelling just now,
>> > or
>> > would consider giving this a try.
>> >
>> > JN
>> >
>> >
>> > On 15-09-20 01:19 PM, Jianling Fan wrote:
>> >>
>> >> no, I am doing a regression with 6 group data with 2 shared parameters
>> >> and 1 different parameter for each group data. the parameter I want to
>> >> coerce is for one group. I don't know how to do it. Any suggestion?
>> >>
>> >> Thanks!
>> >>
>> >> On 19 September 2015 at 13:33, Jeff Newmiller
>> >> <jdnewmil at dcn.davis.ca.us>
>> >> wrote:
>> >>>
>> >>> Why not rewrite the function so that value is not a parameter?
>> >>>
>> >>>
>> >>> ---------------------------------------------------------------------------
>> >>> Jeff Newmiller                        The     .....       .....  Go
>> >>> Live...
>> >>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> >>> Go...
>> >>>                                        Live:   OO#.. Dead: OO#..
>> >>> Playing
>> >>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> >>> /Software/Embedded Controllers)               .OO#.       .OO#.
>> >>> rocks...1k
>> >>>
>> >>>
>> >>> ---------------------------------------------------------------------------
>> >>> Sent from my phone. Please excuse my brevity.
>> >>>
>> >>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>> >>> <fanjianling at gmail.com> wrote:
>> >>>>
>> >>>> Hello, everyone,
>> >>>>
>> >>>> I am using a nls regression with 6 groups data. I am trying to coerce
>> >>>> a parameter to 1 by using a upper and lower statement. but I always
>> >>>> get an error like below:
>> >>>>
>> >>>> Error in ifelse(internalPars < upper, 1, -1) :
>> >>>>   (list) object cannot be coerced to type 'double'
>> >>>>
>> >>>> does anyone know how to fix it?
>> >>>>
>> >>>> thanks in advance!
>> >>>>
>> >>>> My code is below:
>> >>>>
>> >>>>
>> >>>>
>> >>>>> dproot
>> >>>>
>> >>>>    depth       den ref
>> >>>> 1     20 0.5730000   1
>> >>>> 2     40 0.7800000   1
>> >>>> 3     60 0.9470000   1
>> >>>> 4     80 0.9900000   1
>> >>>> 5    100 1.0000000   1
>> >>>> 6     10 0.6000000   2
>> >>>> 7     20 0.8200000   2
>> >>>> 8     30 0.9300000   2
>> >>>> 9     40 1.0000000   2
>> >>>> 10    20 0.4800000   3
>> >>>> 11    40 0.7340000   3
>> >>>> 12    60 0.9610000   3
>> >>>> 13    80 0.9980000   3
>> >>>> 14   100 1.0000000   3
>> >>>> 15    20 3.2083491   4
>> >>>> 16    40 4.9683383   4
>> >>>> 17    60 6.2381133   4
>> >>>> 18    80 6.5322348   4
>> >>>> 19   100 6.5780660   4
>> >>>> 20   120 6.6032064   4
>> >>>> 21    20 0.6140000   5
>> >>>> 22    40 0.8270000   5
>> >>>> 23    60 0.9500000   5
>> >>>> 24    80 0.9950000   5
>> >>>> 25   100 1.0000000   5
>> >>>> 26    20 0.4345774   6
>> >>>> 27    40 0.6654726   6
>> >>>> 28    60 0.8480684   6
>> >>>> 29    80 0.9268951   6
>> >>>> 30   100 0.9723207   6
>> >>>> 31   120 0.9939966   6
>> >>>> 32   140 0.9992400   6
>> >>>>
>> >>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >>>>
>> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>> >>>>>
>> >>>>> summary(fitdp)
>> >>>>
>> >>>>
>> >>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>> >>>>
>> >>>> Parameters:
>> >>>>     Estimate Std. Error t value Pr(>|t|)
>> >>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>> >>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>> >>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>> >>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>> >>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>> >>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>> >>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>> >>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>> >>>> ---
>> >>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>> >>>>
>> >>>> Residual standard error: 0.1094 on 24 degrees of freedom
>> >>>>
>> >>>> Number of iterations to convergence: 8
>> >>>> Achieved convergence tolerance: 9.374e-06
>> >>>>
>> >>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >>>>
>> >>>> algorithm="port",
>> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>> >>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>> >>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>> >>>>
>> >>>> Error in ifelse(internalPars < upper, 1, -1) :
>> >>>>   (list) object cannot be coerced to type 'double'
>> >>>>
>> >>>> ______________________________________________
>> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>> PLEASE do read the posting guide
>> >>>> http://www.R-project.org/posting-guide.html
>> >>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>>
>> >>
>> >>
>> >>
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Jianling Fan
>> ???
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Jianling Fan
???


From ch_koch at gmx.de  Mon Sep 21 15:27:28 2015
From: ch_koch at gmx.de (apeshifter)
Date: Mon, 21 Sep 2015 06:27:28 -0700 (PDT)
Subject: [R] Trees (and Forests) with packages 'party' vs. 'partykit':
 Different results
In-Reply-To: <alpine.DEB.2.11.1509141636120.21785@paninaro.uibk.ac.at>
References: <1442225494911-4712214.post@n4.nabble.com>
	<alpine.DEB.2.11.1509141636120.21785@paninaro.uibk.ac.at>
Message-ID: <1442842048398-4712539.post@n4.nabble.com>

Achim, 

thank you very much for your help, this really cleared up a number of
issues.

As for the differences in results between the party and partykit
implementations of ctree, I guess that the situation is indeed as you
assumed. Four out of five variables have p-values <2.2e-16. (However, it is
not the first of these variables that is selected but the one in the second
column.) I will just continue using the newer implementation. 

-- Christopher



--
View this message in context: http://r.789695.n4.nabble.com/Trees-and-Forests-with-packages-party-vs-partykit-Different-results-tp4712214p4712539.html
Sent from the R help mailing list archive at Nabble.com.


From le4864 at mweb.co.za  Mon Sep 21 16:14:41 2015
From: le4864 at mweb.co.za (le4864 at mweb.co.za)
Date: Mon, 21 Sep 2015 16:14:41 +0200
Subject: [R] R-help, please
Message-ID: <MWSJHBMC05bb4d0e6585834f87b8ec6823b115ab65@MWSJHBMC05>

Good day,
My name is Kim Nguyen and please I need your help with: How to calculate the PASS rate of the data in the table below, with PASS in a single subject if value>=50 and PASS will be given if PASS 3 out of 4 subjectsWhich package will I need to use in this analysis




ID

Literacy

Maths

Physics

Chemistry


A

65

70

79

80


B

65

45

38

50


C

50

62

48

49


D

70

85

82

84


E

45

69

65

62

I appreciate your consultant very much
With kind regardsKim Nguyen
Sent from MWEB Message Centre - CONNECT AND YOU CAN

	[[alternative HTML version deleted]]


From tim_kingston at hotmail.com  Mon Sep 21 16:30:32 2015
From: tim_kingston at hotmail.com (Tim Kingston)
Date: 21 Sep 2015 07:30:32 -0700
Subject: [R] =?utf-8?q?Download_showing_as_exploit?=
Message-ID: <SNT404-EAS585679549CDE488A1BDD31ED460@phx.gbl>


Hi , 

I work for the NHS, and our IT service has been unable to download as its anti-virus software says it contains an exploit.

Is this normal? Is there a way around this?

Kind regards,

Tim Kingston

Sent from my HTC



	[[alternative HTML version deleted]]


From oyomoare at gmail.com  Mon Sep 21 16:35:32 2015
From: oyomoare at gmail.com (Oyomoare Osazuwa-Peters)
Date: Mon, 21 Sep 2015 09:35:32 -0500
Subject: [R] Accounting for correlated random effects in coxme with matrix
 from a phylogeny rather than pedigree
Message-ID: <CAA87cYVgXxL76HY1T08CG6xZZycfMUF1g+ryo_mXjCU6f7vAkA@mail.gmail.com>

Hello All,

I have a problem with running the mixed effects Cox regression model using
a distance matrix from a phylogeny rather than a pedigree. I searched
previous posts and didn't find any directly relevant previous posts.

I am interested in using a mixed effects Cox regression model to determine
the best predictors of time to recruitment in 80 different reintroduced
plant populations representing a total of 31 species. I will like to
account for correlated random effects that result from phylogenetic
relationships amongst species. Dr. Therneau's 2015 article on Mixed Effects
Cox Models provide a very helpful template for me to do this with the coxme
function in R. In this article, the correlation structure due to genetic
relationships amongst individuals was defined using a kinship matrix
derived from a pedigree. Instead of a pedigree, I have a phylogeny for
these 31 species. Hence, I used the inverseA function in the MCMCglmm
package to generate an inverse additive genetic relatedness matrix from the
phylogeny for these 31 species. And then fed it in as input to the varlist
argument in my mixed effects cox regression model (using function coxme). I
got an error message (please see below). Based on the error, one thought I
had was to convert the inverseA matrix from a ?dgCMatrix? to ?bdsmatrix?
but this was not successful either. I have also unsuccessfully tried to use
a pairwise phylogenetic distance matrix.

Is there a better way to do this? I basically just want to account for the
correlated random effects due to phylogenetic relatedness amongst the 31
species represented in the dataset for the Cox regression model.  Please
see my code below and I welcome suggestions on how best to make this work.

Thank you.

#Load packages
library(MCMCglmm)
library(asremlPLUS)
library(ape)

source("read.newick.R")


mytree <- read.newick(file="Phylo_2015Sept15.txt")

mytree6 <- makeNodeLabel(mytree, method="number", prefix = "node")#Make
sure each node is uniquely labeled

IA <- inverseA(mytree6, scale=TRUE) #generate inverse of the additive
genetic relatedness matrix (A) from phylogeny

#Use IA as input in correlated random effects model. Doesn't work.

fit2 <- coxme(Surv(surv.time, recruitment) ~ pred1 + pred2 + sixcatD1 +
sixcatD2 + sixcatD3 + (1|species), data = traitcox, varlist=coxmeMlist(IA,
rescale=F))

Error in as(x, "bdsmatrix") :
  no method or default for coercing ?dgCMatrix? to ?bdsmatrix?

	[[alternative HTML version deleted]]


From edd at debian.org  Mon Sep 21 17:36:35 2015
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 21 Sep 2015 15:36:35 +0000
Subject: [R] how to add 1 + 1 with the interface between R and C
References: <55FF5546.4000201@yahoo.com.br> <55FF6577.9050400@yahoo.com.br>
	<D97A25A9-E9E5-4550-8E3E-8CCE5514A155@gmail.com>
Message-ID: <loom.20150921T173244-741@post.gmane.org>

peter dalgaard <pdalgd <at> gmail.com> writes:
> C is call by value and k and res are pointers. You need a dereferencing 
step or nothing with happen. Try
> 
> *res = *k + 1;

Or you use Rcpp which writes the glue code. Save the following into a file:


#include <Rcpp.h>

// [[Rcpp::export]]
int adder(int x, int y) {
  return x + y;
}

/*** R
adder(40, 2)
*/




Running this is as simple as sourcing it:

  R> Rcpp::sourceCpp("/tmp/adder.cpp")

  R> adder(40, 2)
  [1] 42
  R> 

and it even runs the R snippet at the bottom.

Dirk


From nico.gutierrezo at gmail.com  Mon Sep 21 16:48:45 2015
From: nico.gutierrezo at gmail.com (Nico Gutierrez)
Date: Mon, 21 Sep 2015 16:48:45 +0200
Subject: [R] extract from data.frame (indexing)
Message-ID: <CAJ+rinNs8MUeNpBtxP7HDn9J0btXqaarZgdMeEeNAFi1J5uP_Q@mail.gmail.com>

Hi All,

I need to do the following operation:


  Year Amount Amount.1
1 2001    150      150
2 2002    120      120
3 2003    175      175
4 2004    160      160
5 2005    120      120
6 2006    105      105
7 2007    135      135

	[[alternative HTML version deleted]]


From nico.gutierrezo at gmail.com  Mon Sep 21 16:52:46 2015
From: nico.gutierrezo at gmail.com (Nico Gutierrez)
Date: Mon, 21 Sep 2015 16:52:46 +0200
Subject: [R] Extract from data.frame
Message-ID: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>

Hi All,

I need to do the following operation from data.frame:

df <- data.frame(Year = c("2001", "2002", "2003", "2004", "2005", "2006",
"2007"), Amount = c(150, 120, 175, 160, 120, 105, 135))
df[which.max(df$Amount),]  #to extract row with max Amount.

Now I need to do 3 years average around the max Amount value (ie:
mean(120,175,160))

Thanks!
N

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Sep 21 19:37:36 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 21 Sep 2015 13:37:36 -0400
Subject: [R] Download showing as exploit
In-Reply-To: <SNT404-EAS585679549CDE488A1BDD31ED460@phx.gbl>
References: <SNT404-EAS585679549CDE488A1BDD31ED460@phx.gbl>
Message-ID: <56004060.7000902@gmail.com>

On 21/09/2015 10:30 AM, Tim Kingston wrote:
> 
> Hi , 
> 
> I work for the NHS, and our IT service has been unable to download as its anti-virus software says it contains an exploit.
> 
> Is this normal? Is there a way around this?
> 
> Kind regards,
> 
> Tim Kingston
> 

You don't say what you're trying to download, or from where.

In the past there have been false positive reports from various
anti-virus packages about R.  I don't recall any cases of real malware
being distributed, but eventually it will probably happen.

What you should do is ask your IT service for details of what the
problem is, and ask them to confirm it's real.  If so, please let us
know the details.  If not, then I can't see how we can help.

Duncan Murdoch


From msharp at txbiomed.org  Mon Sep 21 19:51:16 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Mon, 21 Sep 2015 17:51:16 +0000
Subject: [R] Extract from data.frame
In-Reply-To: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
References: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
Message-ID: <B0A385A3-6F15-4175-91C1-0FC910019A57@txbiomed.org>

Nico, 

I expect there are many better ways to do this, but this does work:
max_row <- (1:nrow(df))[which.max(df$Amount)]
mean(df$Amount[max_row + c(-1, 0, 1)])

> max_row <- (1:nrow(df))[which.max(df$Amount)]
> mean(df$Amount[max_row + c(-1, 0, 1)])
[1] 151.6667

R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org




> On Sep 21, 2015, at 9:52 AM, Nico Gutierrez <nico.gutierrezo at gmail.com> wrote:
> 
> Hi All,
> 
> I need to do the following operation from data.frame:
> 
> df <- data.frame(Year = c("2001", "2002", "2003", "2004", "2005", "2006",
> "2007"), Amount = c(150, 120, 175, 160, 120, 105, 135))
> df[which.max(df$Amount),]  #to extract row with max Amount.
> 
> Now I need to do 3 years average around the max Amount value (ie:
> mean(120,175,160))
> 
> Thanks!
> N
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From klebyn at yahoo.com.br  Mon Sep 21 19:56:32 2015
From: klebyn at yahoo.com.br (Cleber Borges)
Date: Mon, 21 Sep 2015 14:56:32 -0300
Subject: [R] how to add 1 + 1 with the interface between R and C
In-Reply-To: <loom.20150921T173244-741@post.gmane.org>
References: <55FF5546.4000201@yahoo.com.br> <55FF6577.9050400@yahoo.com.br>
	<D97A25A9-E9E5-4550-8E3E-8CCE5514A155@gmail.com>
	<loom.20150921T173244-741@post.gmane.org>
Message-ID: <560044D0.7010302@yahoo.com.br>

I think is time to learning Rcpp! :-)

thank you   (Peter Dalgaard and Dirk Eddelbuettel )  by the examples!

the more simples are often more informatives...

cleber

Em 21/09/2015 12:36, Dirk Eddelbuettel escreveu:
> peter dalgaard <pdalgd <at> gmail.com> writes:
>> C is call by value and k and res are pointers. You need a dereferencing
> step or nothing with happen. Try
>> *res = *k + 1;
> Or you use Rcpp which writes the glue code. Save the following into a file:
>
>
> #include <Rcpp.h>
>
> // [[Rcpp::export]]
> int adder(int x, int y) {
>    return x + y;
> }
>
> /*** R
> adder(40, 2)
> */
>
>
>
>
> Running this is as simple as sourcing it:
>
>    R> Rcpp::sourceCpp("/tmp/adder.cpp")
>
>    R> adder(40, 2)
>    [1] 42
>    R>
>
> and it even runs the R snippet at the bottom.
>
> Dirk
>
> ______________________________________________


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From john.archie.mckown at gmail.com  Mon Sep 21 19:58:07 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Mon, 21 Sep 2015 12:58:07 -0500
Subject: [R] Extract from data.frame
In-Reply-To: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
References: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
Message-ID: <CAAJSdjgvdbPzQjPnzgYkU77RWTTRnYAC42RbFY4as7U2QcaVmA@mail.gmail.com>

On Mon, Sep 21, 2015 at 9:52 AM, Nico Gutierrez <nico.gutierrezo at gmail.com>
wrote:

> Hi All,
>
> I need to do the following operation from data.frame:
>
> df <- data.frame(Year = c("2001", "2002", "2003", "2004", "2005", "2006",
> "2007"), Amount = c(150, 120, 175, 160, 120, 105, 135))
> df[which.max(df$Amount),]  #to extract row with max Amount.
>
> Now I need to do 3 years average around the max Amount value (ie:
> mean(120,175,160))
>
> Thanks!
> N
>
>
?The simplistic answer is something like:

df <- structure(list(Year = structure(1:7, .Label = c("2001", "2002",
"2003", "2004", "2005", "2006", "2007"), class = "factor"), Amount = c(150,
120, 175, 160, 120, 105, 135)), .Names = c("Year", "Amount"), row.names =
c(NA,
-7L), class = "data.frame");
wdf <- which.max(df$Amount);
adf3 <- mean(df$Amount[adf-1:adr+1]);

But that ignores the boundry condition where the maximum is at either end.
What do you want to do in that case??


-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From jeff at trefftzs.org  Mon Sep 21 20:00:22 2015
From: jeff at trefftzs.org (Jeff Trefftzs)
Date: Mon, 21 Sep 2015 11:00:22 -0700
Subject: [R] ggplot2 will not install after system upgrade
In-Reply-To: <CA+vqiLETyCwDHs+XGutSgz3QH5KAfKt0PTEYkgSrPZL2UyNGFg@mail.gmail.com>
References: <1441306999.3562.3.camel@trefftzs.org>
	<CA+vqiLETyCwDHs+XGutSgz3QH5KAfKt0PTEYkgSrPZL2UyNGFg@mail.gmail.com>
Message-ID: <1442858422.14110.1.camel@trefftzs.org>

On Thu, 2015-09-03 at 16:47 -0400, Ista Zahn wrote:
> Hi Jeff,
> Your chances of getting a useful response will increase if you
> provide
> some additional information. For example, which version of R? Which
> version of ggplot2? What sequence of commands produces the error?
> What
> _exactly_ does the error message say?
> 
> Does
> 
> update.packages(ask=FALSE, checkBuilt=TRUE)
> install.packages("ggplot2")
> 
> help?

Thank you, Ista!  This did, indeed, fix the problem.  ggplot2 now works
fine on both the laptop and the desktop computers.

-- 
Jeff Trefftzs
http://www.trefftzs.org


From bgunter.4567 at gmail.com  Mon Sep 21 20:03:56 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 21 Sep 2015 11:03:56 -0700
Subject: [R] Extract from data.frame
In-Reply-To: <B0A385A3-6F15-4175-91C1-0FC910019A57@txbiomed.org>
References: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
	<B0A385A3-6F15-4175-91C1-0FC910019A57@txbiomed.org>
Message-ID: <CAGxFJbQDUgMBdjxv0r=BF6eMi1fD=d3z7=psEYrDSvS+5xUgvw@mail.gmail.com>

Note the following problems:

1. " max_row <- (1:nrow(df))[which.max(df$Amount)]"

This is a bit silly.

max_row <- which.max(df$Amount)


will do. See ?which.max


2. What happens if the max is the first or last row? e.g.

> dat <- data.frame(a=runif(5),b=1:5)
> max_row<- which.max(dat$b)
> mean(dat[max_row+c(-1,0,1),"b"]) ## 2-d indexing
[1] NA


Cheers,
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Sep 21, 2015 at 10:51 AM, Mark Sharp <msharp at txbiomed.org> wrote:
> Nico,
>
> I expect there are many better ways to do this, but this does work:
> max_row <- (1:nrow(df))[which.max(df$Amount)]
> mean(df$Amount[max_row + c(-1, 0, 1)])
>
>> max_row <- (1:nrow(df))[which.max(df$Amount)]
>> mean(df$Amount[max_row + c(-1, 0, 1)])
> [1] 151.6667
>
> R. Mark Sharp, Ph.D.
> Director of Primate Records Database
> Southwest National Primate Research Center
> Texas Biomedical Research Institute
> P.O. Box 760549
> San Antonio, TX 78245-0549
> Telephone: (210)258-9476
> e-mail: msharp at TxBiomed.org
>
>
>
>
>> On Sep 21, 2015, at 9:52 AM, Nico Gutierrez <nico.gutierrezo at gmail.com> wrote:
>>
>> Hi All,
>>
>> I need to do the following operation from data.frame:
>>
>> df <- data.frame(Year = c("2001", "2002", "2003", "2004", "2005", "2006",
>> "2007"), Amount = c(150, 120, 175, 160, 120, 105, 135))
>> df[which.max(df$Amount),]  #to extract row with max Amount.
>>
>> Now I need to do 3 years average around the max Amount value (ie:
>> mean(120,175,160))
>>
>> Thanks!
>> N
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From glennmschultz at me.com  Mon Sep 21 20:01:36 2015
From: glennmschultz at me.com (Glenn Schultz)
Date: Mon, 21 Sep 2015 18:01:36 +0000 (GMT)
Subject: [R] extracting a value from XML
Message-ID: <a4ac0cf8-906d-441c-b945-ba92ea5f399f@me.com>

Hi All,

I have been trying to extract a value from XML. ?I have been at it for several days off and on and I can't seem to get my head around the problem. ? I basically understand the examples in R help but I cannot replicate success with the below?

I tried to use xmlValue(doc, "//[[value]]") but no success. ?All I need is the 0.46 value. ?Any suggestions to help me along are greatly appreciated.

Glenn

<?xml version="1.0" encoding="utf-8"?>
<observations realtime_start="2015-09-21" realtime_end="2015-09-21" observation_start="2015-09-01" observation_end="2015-09-01" units="lin" output_type="1" file_type="xml" order_by="observation_date" sort_order="asc" count="1" offset="0" limit="100000">
? <observation realtime_start="2015-09-21" realtime_end="2015-09-21" date="2015-09-01" value="0.46"/>
</observations>
?

From bob at rudis.net  Mon Sep 21 20:55:43 2015
From: bob at rudis.net (boB Rudis)
Date: Mon, 21 Sep 2015 14:55:43 -0400
Subject: [R] extracting a value from XML
In-Reply-To: <a4ac0cf8-906d-441c-b945-ba92ea5f399f@me.com>
References: <a4ac0cf8-906d-441c-b945-ba92ea5f399f@me.com>
Message-ID: <CAJ4QxaPGxWJSSmjNMDMwwK_kx4OjbU0wY2pOkiC_mjzZQ5jMcQ@mail.gmail.com>

This is how (one way) in both the xml2 package and XML package:

library(xml2)
library(XML)

txt <- '<?xml version="1.0" encoding="utf-8"?>
<observations realtime_start="2015-09-21" realtime_end="2015-09-21"
observation_start="2015-09-01" observation_end="2015-09-01"
units="lin" output_type="1" file_type="xml"
order_by="observation_date" sort_order="asc" count="1" offset="0"
limit="100000">
  <observation realtime_start="2015-09-21" realtime_end="2015-09-21"
date="2015-09-01" value="0.46"/>
</observations>'

doc <- read_xml(txt)
xml_attr(xml_find_all(doc, "//observation"), "value")

doc1 <- xmlParse(txt)
xpathSApply(doc1, "//observation", xmlGetAttr, "value")



On Mon, Sep 21, 2015 at 2:01 PM, Glenn Schultz <glennmschultz at me.com> wrote:
> <?xml version="1.0" encoding="utf-8"?>
> <observations realtime_start="2015-09-21" realtime_end="2015-09-21"
> observation_start="2015-09-01" observation_end="2015-09-01" units="lin"
> output_type="1" file_type="xml" order_by="observation_date" sort_order="asc"
> count="1" offset="0" limit="100000">
>   <observation realtime_start="2015-09-21" realtime_end="2015-09-21"
> date="2015-09-01" value="0.46"/>
> </observations>


From profjcnash at gmail.com  Mon Sep 21 21:38:48 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Mon, 21 Sep 2015 15:38:48 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
Message-ID: <56005CC8.5040102@gmail.com>

I've not used it for group data, and suspect that the code to generate 
derivatives cannot cope with the bracket syntax. If you can rewrite the 
equation without the brackets, you could get the derivatives and solve 
that way. This will probably mean having a "translation" routine to glue 
things together.

JN

On 15-09-21 12:22 PM, Jianling Fan wrote:
> Thanks Prof. Nash,
>
> Sorry for late reply. I am learning and trying to use your nlmrt
> package since I got your email. It works good to mask a parameter in
> regression but seems does work for my equation. I think the problem is
> that the parameter I want to mask is a group-specific parameter and I
> have a "[]" syntax in my equation. However, I don't have your 2014
> book on hand and couldn't find it in our library. So I am wondering if
> nlxb works for group data?
> Thanks a lot!
>
> following is my code and I got a error form it.
>
>> fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>                  + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
> Rm5=1.01, Rm6=1, d50=20, c=-1),
>                  + masked=c("Rm6"))
>
> Error in deriv.default(parse(text = resexp), names(start)) :
>    Function '`[`' is not in the derivatives table
>
>
> Best regards,
>
> Jianling
>
>
> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
>> I posted a suggestion to use nlmrt package (function nlxb to be precise),
>> which has masked (fixed) parameters. Examples in my 2014 book on Nonlinear
>> parameter optimization with R tools. However, I'm travelling just now, or
>> would consider giving this a try.
>>
>> JN
>>
>>
>> On 15-09-20 01:19 PM, Jianling Fan wrote:
>>>
>>> no, I am doing a regression with 6 group data with 2 shared parameters
>>> and 1 different parameter for each group data. the parameter I want to
>>> coerce is for one group. I don't know how to do it. Any suggestion?
>>>
>>> Thanks!
>>>
>>> On 19 September 2015 at 13:33, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>>> wrote:
>>>>
>>>> Why not rewrite the function so that value is not a parameter?
>>>>
>>>> ---------------------------------------------------------------------------
>>>> Jeff Newmiller                        The     .....       .....  Go
>>>> Live...
>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>> Go...
>>>>                                         Live:   OO#.. Dead: OO#..  Playing
>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>>> rocks...1k
>>>>
>>>> ---------------------------------------------------------------------------
>>>> Sent from my phone. Please excuse my brevity.
>>>>
>>>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>>>> <fanjianling at gmail.com> wrote:
>>>>>
>>>>> Hello, everyone,
>>>>>
>>>>> I am using a nls regression with 6 groups data. I am trying to coerce
>>>>> a parameter to 1 by using a upper and lower statement. but I always
>>>>> get an error like below:
>>>>>
>>>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>>>    (list) object cannot be coerced to type 'double'
>>>>>
>>>>> does anyone know how to fix it?
>>>>>
>>>>> thanks in advance!
>>>>>
>>>>> My code is below:
>>>>>
>>>>>
>>>>>
>>>>>> dproot
>>>>>
>>>>>     depth       den ref
>>>>> 1     20 0.5730000   1
>>>>> 2     40 0.7800000   1
>>>>> 3     60 0.9470000   1
>>>>> 4     80 0.9900000   1
>>>>> 5    100 1.0000000   1
>>>>> 6     10 0.6000000   2
>>>>> 7     20 0.8200000   2
>>>>> 8     30 0.9300000   2
>>>>> 9     40 1.0000000   2
>>>>> 10    20 0.4800000   3
>>>>> 11    40 0.7340000   3
>>>>> 12    60 0.9610000   3
>>>>> 13    80 0.9980000   3
>>>>> 14   100 1.0000000   3
>>>>> 15    20 3.2083491   4
>>>>> 16    40 4.9683383   4
>>>>> 17    60 6.2381133   4
>>>>> 18    80 6.5322348   4
>>>>> 19   100 6.5780660   4
>>>>> 20   120 6.6032064   4
>>>>> 21    20 0.6140000   5
>>>>> 22    40 0.8270000   5
>>>>> 23    60 0.9500000   5
>>>>> 24    80 0.9950000   5
>>>>> 25   100 1.0000000   5
>>>>> 26    20 0.4345774   6
>>>>> 27    40 0.6654726   6
>>>>> 28    60 0.8480684   6
>>>>> 29    80 0.9268951   6
>>>>> 30   100 0.9723207   6
>>>>> 31   120 0.9939966   6
>>>>> 32   140 0.9992400   6
>>>>>
>>>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>>>
>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>>>>>>
>>>>>> summary(fitdp)
>>>>>
>>>>>
>>>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>>>>>
>>>>> Parameters:
>>>>>      Estimate Std. Error t value Pr(>|t|)
>>>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>>>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>>>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>>>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>>>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>>>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>>>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>>>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>>>>> ---
>>>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>>>>>
>>>>> Residual standard error: 0.1094 on 24 degrees of freedom
>>>>>
>>>>> Number of iterations to convergence: 8
>>>>> Achieved convergence tolerance: 9.374e-06
>>>>>
>>>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>>>
>>>>> algorithm="port",
>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>>>>>
>>>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>>>    (list) object cannot be coerced to type 'double'
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>
>>>
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From profjcnash at gmail.com  Mon Sep 21 21:40:00 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Mon, 21 Sep 2015 15:40:00 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAJ7mryKyL+4ZVz8YNmi-9_jP7vT6wvibAWBmtv5nbnEt_3wnSw@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>
	<CAJ7mryKyL+4ZVz8YNmi-9_jP7vT6wvibAWBmtv5nbnEt_3wnSw@mail.gmail.com>
Message-ID: <56005D10.50107@gmail.com>

Apologies for replying and overlapping Gabor's contribution, which 
actually did the work!

Best, JN

On 15-09-21 01:03 PM, Jianling Fan wrote:
> Thanks Gabor,
>
>   That works good to rewrite the express the formula!
>
> Thanks a lot!
>
> Regards,
>
> Jianling
>
> On 21 September 2015 at 10:43, Gabor Grothendieck
> <ggrothendieck at gmail.com> wrote:
>> Express the formula in terms of simple operations like this:
>>
>> # add 0/1 columns ref.1, ref.2, ..., ref.6
>> dproot2 <- do.call(data.frame, transform(dproot, ref = outer(dproot$ref,
>> seq(6), "==") + 0))
>>
>> # now express the formula in terms of the new columns
>> library(nlmrt)
>> fitdp1<-nlxb(den ~ (Rm1 * ref.1 + Rm2 * ref.2 + Rm3 * ref.3 + Rm4 * ref.4 +
>> Rm5 * ref.5 + Rm6 * ref.6)/(1+(depth/d50)^c),
>>           data = dproot2,
>>           start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01, Rm6=1,
>> d50=20, c=-1),
>>           masked = "Rm6")
>>
>> where we used this input:
>>
>> Lines <- "   depth       den ref
>> 1     20 0.5730000   1
>> 2     40 0.7800000   1
>> 3     60 0.9470000   1
>> 4     80 0.9900000   1
>> 5    100 1.0000000   1
>> 6     10 0.6000000   2
>> 7     20 0.8200000   2
>> 8     30 0.9300000   2
>> 9     40 1.0000000   2
>> 10    20 0.4800000   3
>> 11    40 0.7340000   3
>> 12    60 0.9610000   3
>> 13    80 0.9980000   3
>> 14   100 1.0000000   3
>> 15    20 3.2083491   4
>> 16    40 4.9683383   4
>> 17    60 6.2381133   4
>> 18    80 6.5322348   4
>> 19   100 6.5780660   4
>> 20   120 6.6032064   4
>> 21    20 0.6140000   5
>> 22    40 0.8270000   5
>> 23    60 0.9500000   5
>> 24    80 0.9950000   5
>> 25   100 1.0000000   5
>> 26    20 0.4345774   6
>> 27    40 0.6654726   6
>> 28    60 0.8480684   6
>> 29    80 0.9268951   6
>> 30   100 0.9723207   6
>> 31   120 0.9939966   6
>> 32   140 0.9992400   6"
>>
>> dproot <- read.table(text = Lines, header = TRUE)
>>
>>
>>
>> On Mon, Sep 21, 2015 at 12:22 PM, Jianling Fan <fanjianling at gmail.com>
>> wrote:
>>>
>>> Thanks Prof. Nash,
>>>
>>> Sorry for late reply. I am learning and trying to use your nlmrt
>>> package since I got your email. It works good to mask a parameter in
>>> regression but seems does work for my equation. I think the problem is
>>> that the parameter I want to mask is a group-specific parameter and I
>>> have a "[]" syntax in my equation. However, I don't have your 2014
>>> book on hand and couldn't find it in our library. So I am wondering if
>>> nlxb works for group data?
>>> Thanks a lot!
>>>
>>> following is my code and I got a error form it.
>>>
>>>> fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>                  + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>>> Rm5=1.01, Rm6=1, d50=20, c=-1),
>>>                  + masked=c("Rm6"))
>>>
>>> Error in deriv.default(parse(text = resexp), names(start)) :
>>>    Function '`[`' is not in the derivatives table
>>>
>>>
>>> Best regards,
>>>
>>> Jianling
>>>
>>>
>>> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
>>>> I posted a suggestion to use nlmrt package (function nlxb to be
>>>> precise),
>>>> which has masked (fixed) parameters. Examples in my 2014 book on
>>>> Nonlinear
>>>> parameter optimization with R tools. However, I'm travelling just now,
>>>> or
>>>> would consider giving this a try.
>>>>
>>>> JN
>>>>
>>>>
>>>> On 15-09-20 01:19 PM, Jianling Fan wrote:
>>>>>
>>>>> no, I am doing a regression with 6 group data with 2 shared parameters
>>>>> and 1 different parameter for each group data. the parameter I want to
>>>>> coerce is for one group. I don't know how to do it. Any suggestion?
>>>>>
>>>>> Thanks!
>>>>>
>>>>> On 19 September 2015 at 13:33, Jeff Newmiller
>>>>> <jdnewmil at dcn.davis.ca.us>
>>>>> wrote:
>>>>>>
>>>>>> Why not rewrite the function so that value is not a parameter?
>>>>>>
>>>>>>
>>>>>> ---------------------------------------------------------------------------
>>>>>> Jeff Newmiller                        The     .....       .....  Go
>>>>>> Live...
>>>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>>>> Go...
>>>>>>                                         Live:   OO#.. Dead: OO#..
>>>>>> Playing
>>>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>>>>> rocks...1k
>>>>>>
>>>>>>
>>>>>> ---------------------------------------------------------------------------
>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>
>>>>>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>>>>>> <fanjianling at gmail.com> wrote:
>>>>>>>
>>>>>>> Hello, everyone,
>>>>>>>
>>>>>>> I am using a nls regression with 6 groups data. I am trying to coerce
>>>>>>> a parameter to 1 by using a upper and lower statement. but I always
>>>>>>> get an error like below:
>>>>>>>
>>>>>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>>>>>    (list) object cannot be coerced to type 'double'
>>>>>>>
>>>>>>> does anyone know how to fix it?
>>>>>>>
>>>>>>> thanks in advance!
>>>>>>>
>>>>>>> My code is below:
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>> dproot
>>>>>>>
>>>>>>>     depth       den ref
>>>>>>> 1     20 0.5730000   1
>>>>>>> 2     40 0.7800000   1
>>>>>>> 3     60 0.9470000   1
>>>>>>> 4     80 0.9900000   1
>>>>>>> 5    100 1.0000000   1
>>>>>>> 6     10 0.6000000   2
>>>>>>> 7     20 0.8200000   2
>>>>>>> 8     30 0.9300000   2
>>>>>>> 9     40 1.0000000   2
>>>>>>> 10    20 0.4800000   3
>>>>>>> 11    40 0.7340000   3
>>>>>>> 12    60 0.9610000   3
>>>>>>> 13    80 0.9980000   3
>>>>>>> 14   100 1.0000000   3
>>>>>>> 15    20 3.2083491   4
>>>>>>> 16    40 4.9683383   4
>>>>>>> 17    60 6.2381133   4
>>>>>>> 18    80 6.5322348   4
>>>>>>> 19   100 6.5780660   4
>>>>>>> 20   120 6.6032064   4
>>>>>>> 21    20 0.6140000   5
>>>>>>> 22    40 0.8270000   5
>>>>>>> 23    60 0.9500000   5
>>>>>>> 24    80 0.9950000   5
>>>>>>> 25   100 1.0000000   5
>>>>>>> 26    20 0.4345774   6
>>>>>>> 27    40 0.6654726   6
>>>>>>> 28    60 0.8480684   6
>>>>>>> 29    80 0.9268951   6
>>>>>>> 30   100 0.9723207   6
>>>>>>> 31   120 0.9939966   6
>>>>>>> 32   140 0.9992400   6
>>>>>>>
>>>>>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>>>>>
>>>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>>>>>>>>
>>>>>>>> summary(fitdp)
>>>>>>>
>>>>>>>
>>>>>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>>>>>>>
>>>>>>> Parameters:
>>>>>>>      Estimate Std. Error t value Pr(>|t|)
>>>>>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>>>>>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>>>>>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>>>>>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>>>>>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>>>>>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>>>>>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>>>>>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>>>>>>> ---
>>>>>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>>>>>>>
>>>>>>> Residual standard error: 0.1094 on 24 degrees of freedom
>>>>>>>
>>>>>>> Number of iterations to convergence: 8
>>>>>>> Achieved convergence tolerance: 9.374e-06
>>>>>>>
>>>>>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>>>>>
>>>>>>> algorithm="port",
>>>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>>>>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>>>>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>>>>>>>
>>>>>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>>>>>    (list) object cannot be coerced to type 'double'
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>> --
>>> Jianling Fan
>>> ???
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>> --
>> Statistics & Software Consulting
>> GKX Group, GKX Associates Inc.
>> tel: 1-877-GKX-GROUP
>> email: ggrothendieck at gmail.com
>
>
>


From bob at rudis.net  Mon Sep 21 21:46:38 2015
From: bob at rudis.net (boB Rudis)
Date: Mon, 21 Sep 2015 15:46:38 -0400
Subject: [R] extracting a value from XML
In-Reply-To: <97c80aaa-623c-46d5-981b-1a0a764ed516@me.com>
References: <97c80aaa-623c-46d5-981b-1a0a764ed516@me.com>
Message-ID: <CAJ4QxaNbbdBq5VEhTadqW_rCENzKO9wUPZmh46ByOrR5ODo7AA@mail.gmail.com>

The "<observation ..." format of the tag tells us it's a [child] node
in the document and the "//observation" XPath expression targets _all_
"observation" nodes (sometimes that's acceptable, sometimes you need
to be more specific in the XPath expression you use). The 'value="...'
part of the tag is an attribute of the tag, hence using `xml_attr()`
or `xmlGetAttr` to retrieve it from the extracted node(s)

On Mon, Sep 21, 2015 at 3:41 PM, Glenn Schultz <glennmschultz at me.com> wrote:
> Hi Bob,
>
> Thanks, can you help me undestand why it starts with //observation and how
> you know that it is a node?
>
> Glenn
>
> On Sep 21, 2015, at 01:56 PM, boB Rudis <bob at rudis.net> wrote:
>
> This is how (one way) in both the xml2 package and XML package:
>
> library(xml2)
> library(XML)
>
> txt <- '<?xml version="1.0" encoding="utf-8"?>
> <observations realtime_start="2015-09-21" realtime_end="2015-09-21"
> observation_start="2015-09-01" observation_end="2015-09-01"
> units="lin" output_type="1" file_type="xml"
> order_by="observation_date" sort_order="asc" count="1" offset="0"
> limit="100000">
> <observation realtime_start="2015-09-21" realtime_end="2015-09-21"
> date="2015-09-01" value="0.46"/>
> </observations>'
>
> doc <- read_xml(txt)
> xml_attr(xml_find_all(doc, "//observation"), "value")
>
> doc1 <- xmlParse(txt)
> xpathSApply(doc1, "//observation", xmlGetAttr, "value")
>
>
>
> On Mon, Sep 21, 2015 at 2:01 PM, Glenn Schultz <glennmschultz at me.com> wrote:
>
> <?xml version="1.0" encoding="utf-8"?>
>
> <observations realtime_start="2015-09-21" realtime_end="2015-09-21"
>
> observation_start="2015-09-01" observation_end="2015-09-01" units="lin"
>
> output_type="1" file_type="xml" order_by="observation_date" sort_order="asc"
>
> count="1" offset="0" limit="100000">
>
> <observation realtime_start="2015-09-21" realtime_end="2015-09-21"
>
> date="2015-09-01" value="0.46"/>
>
> </observations>


From lordpreetam at gmail.com  Mon Sep 21 21:49:38 2015
From: lordpreetam at gmail.com (Preetam Pal)
Date: Tue, 22 Sep 2015 01:19:38 +0530
Subject: [R] Running GCV Optimization under Ridge Regression
Message-ID: <CAHVFrXEd_S8p96Cwz38FUHhUdd=4ys9iHNLP4PcLfjsfvgAjXQ@mail.gmail.com>

Hi guys,

I am running Ridge regression on a dataset (predicted variable = y; GDP,
HPA and FX are regressors). I found that lm.ridge() can perform the ridge
regression given any value of lambda (i.e. the ridge parameter). However,
in order to choose the best results, I need to select the model output
corresponding to that lambda which optimizes some logically defined GCV
criteria. I thought there would be some in-built funcion in R Studio for
this, but could not find one. Also, I am not being able to write the
required code for this. Any help here will be appreciated. I have attached
the data in case it is required.

Thanks,
Preetam

From bgunter.4567 at gmail.com  Mon Sep 21 22:00:07 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 21 Sep 2015 13:00:07 -0700
Subject: [R] Extract from data.frame
In-Reply-To: <CAAJSdjgvdbPzQjPnzgYkU77RWTTRnYAC42RbFY4as7U2QcaVmA@mail.gmail.com>
References: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
	<CAAJSdjgvdbPzQjPnzgYkU77RWTTRnYAC42RbFY4as7U2QcaVmA@mail.gmail.com>
Message-ID: <CAGxFJbTSmfxj_TRCno6Np2W_R4HSihKdzg9bJjoYjmm=b64u-w@mail.gmail.com>

No.


On Mon, Sep 21, 2015 at 10:58 AM, John McKown
<john.archie.mckown at gmail.com> wrote:
> On Mon, Sep 21, 2015 at 9:52 AM, Nico Gutierrez <nico.gutierrezo at gmail.com>
> wrote:
>
>> Hi All,
>>
>> I need to do the following operation from data.frame:
>>
>> df <- data.frame(Year = c("2001", "2002", "2003", "2004", "2005", "2006",
>> "2007"), Amount = c(150, 120, 175, 160, 120, 105, 135))
>> df[which.max(df$Amount),]  #to extract row with max Amount.
>>
>> Now I need to do 3 years average around the max Amount value (ie:
>> mean(120,175,160))
>>
>> Thanks!
>> N
>>
>>
> The simplistic answer is something like:
>
> df <- structure(list(Year = structure(1:7, .Label = c("2001", "2002",
> "2003", "2004", "2005", "2006", "2007"), class = "factor"), Amount = c(150,
> 120, 175, 160, 120, 105, 135)), .Names = c("Year", "Amount"), row.names =
> c(NA,
> -7L), class = "data.frame");
> wdf <- which.max(df$Amount);
> adf3 <- mean(df$Amount[adf-1:adr+1]);

Typos?!
But it won't work anyway. See ?Syntax for operator precedence and

Example:

> a <- 1:5
> mid <- 3
> a[mid-1:mid+1]
[1] 3 2 1
> a[(mid-1):(mid+1)]
[1] 2 3 4

Cheers,
Bert


>
> But that ignores the boundry condition where the maximum is at either end.
> What do you want to do in that case?
>
>
> --
>
> Schrodinger's backup: The condition of any backup is unknown until a
> restore is attempted.
>
> Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.
>
> He's about as useful as a wax frying pan.
>
> 10 to the 12th power microphones = 1 Megaphone
>
> Maranatha! <><
> John McKown
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From schwidom at gmx.net  Mon Sep 21 22:49:34 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Mon, 21 Sep 2015 22:49:34 +0200
Subject: [R] Extract from data.frame
In-Reply-To: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
References: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
Message-ID: <20150921204934.GA2793@debian64>


year <- df$Year[ which.max( df$Amount)]
df[ df$Year %in% (as.numeric( as.character( year)) + -1:1), ]
  Year Amount
2 2002    120
3 2003    175
4 2004    160


On Mon, Sep 21, 2015 at 04:52:46PM +0200, Nico Gutierrez wrote:
> Hi All,
> 
> I need to do the following operation from data.frame:
> 
> df <- data.frame(Year = c("2001", "2002", "2003", "2004", "2005", "2006",
> "2007"), Amount = c(150, 120, 175, 160, 120, 105, 135))
> df[which.max(df$Amount),]  #to extract row with max Amount.
> 
> Now I need to do 3 years average around the max Amount value (ie:
> mean(120,175,160))
> 
> Thanks!
> N
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From glennmschultz at me.com  Mon Sep 21 21:41:47 2015
From: glennmschultz at me.com (Glenn Schultz)
Date: Mon, 21 Sep 2015 19:41:47 +0000 (GMT)
Subject: [R] extracting a value from XML
Message-ID: <97c80aaa-623c-46d5-981b-1a0a764ed516@me.com>

Hi Bob,

Thanks, can you help me undestand why it starts with //observation and how you know that it is a node?

Glenn

On Sep 21, 2015, at 01:56 PM, boB Rudis <bob at rudis.net> wrote:

This is how (one way) in both the xml2 package and XML package:

library(xml2)
library(XML)

txt <- '<?xml version="1.0" encoding="utf-8"?>
<observations realtime_start="2015-09-21" realtime_end="2015-09-21"
observation_start="2015-09-01" observation_end="2015-09-01"
units="lin" output_type="1" file_type="xml"
order_by="observation_date" sort_order="asc" count="1" offset="0"
limit="100000">
<observation realtime_start="2015-09-21" realtime_end="2015-09-21"
date="2015-09-01" value="0.46"/>
</observations>'

doc <- read_xml(txt)
xml_attr(xml_find_all(doc, "//observation"), "value")

doc1 <- xmlParse(txt)
xpathSApply(doc1, "//observation", xmlGetAttr, "value")



On Mon, Sep 21, 2015 at 2:01 PM, Glenn Schultz <glennmschultz at me.com> wrote:
<?xml version="1.0" encoding="utf-8"?>
<observations realtime_start="2015-09-21" realtime_end="2015-09-21"
observation_start="2015-09-01" observation_end="2015-09-01" units="lin"
output_type="1" file_type="xml" order_by="observation_date" sort_order="asc"
count="1" offset="0" limit="100000">
<observation realtime_start="2015-09-21" realtime_end="2015-09-21"
date="2015-09-01" value="0.46"/>
</observations>

From schwidom at gmx.net  Mon Sep 21 22:58:08 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Mon, 21 Sep 2015 22:58:08 +0200
Subject: [R] Extract from data.frame
In-Reply-To: <20150921204934.GA2793@debian64>
References: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
	<20150921204934.GA2793@debian64>
Message-ID: <20150921205808.GA4956@debian64>


better ( if year is an vector of more than 1 element):

df[ df$Year %in% outer(as.numeric( as.character( year)), -1:1, FUN='+'), ]
  Year Amount
2 2002    120
3 2003    175
4 2004    160


On Mon, Sep 21, 2015 at 10:49:34PM +0200, Frank Schwidom wrote:
> 
> year <- df$Year[ which.max( df$Amount)]
> df[ df$Year %in% (as.numeric( as.character( year)) + -1:1), ]
>   Year Amount
> 2 2002    120
> 3 2003    175
> 4 2004    160
> 
> 
> On Mon, Sep 21, 2015 at 04:52:46PM +0200, Nico Gutierrez wrote:
> > Hi All,
> > 
> > I need to do the following operation from data.frame:
> > 
> > df <- data.frame(Year = c("2001", "2002", "2003", "2004", "2005", "2006",
> > "2007"), Amount = c(150, 120, 175, 160, 120, 105, 135))
> > df[which.max(df$Amount),]  #to extract row with max Amount.
> > 
> > Now I need to do 3 years average around the max Amount value (ie:
> > mean(120,175,160))
> > 
> > Thanks!
> > N
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ddalthorp at usgs.gov  Mon Sep 21 23:17:40 2015
From: ddalthorp at usgs.gov (Dan D)
Date: Mon, 21 Sep 2015 14:17:40 -0700 (PDT)
Subject: [R] vector manipulations -- differences
Message-ID: <1442870260243-4712575.post@n4.nabble.com>

I need an efficient way to build a new n x (n-1)/2 vector from an n-vector x
as:

c(x[-1]-x[1], x[-(1:2)]-x[2], ... , x[-(1:(n-1)] - x[n-1])

x is increasing with x[1] = 0. 

The following works but is not the greatest:
junk<-outer(x, x, '-')
junk[junk>0]

e.g., 
given
x<-c(0, 3, 7, 20)
junk<-outer(x, x, '-')
junk[junk>0] # yields: c(3, 7, 20, 4, 17, 13) as needed, but it has to go
through 
junk
#     [,1] [,2] [,3] [,4]
#[1,]    0   -3   -7  -20
#[2,]    3    0   -4  -17
#[3,]    7    4    0  -13
#[4,]   20   17   13    0

Anyone have a better idea?

-Dan



--
View this message in context: http://r.789695.n4.nabble.com/vector-manipulations-differences-tp4712575.html
Sent from the R help mailing list archive at Nabble.com.


From r.turner at auckland.ac.nz  Mon Sep 21 23:45:15 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 22 Sep 2015 09:45:15 +1200
Subject: [R] R-help, please
In-Reply-To: <MWSJHBMC05bb4d0e6585834f87b8ec6823b115ab65@MWSJHBMC05>
References: <MWSJHBMC05bb4d0e6585834f87b8ec6823b115ab65@MWSJHBMC05>
Message-ID: <56007A6B.5080809@auckland.ac.nz>



Your question looks very much like homework and this list does not do 
homework for people.  Talk to your instructor.

cheers,

Rolf Turner

On 22/09/15 02:14, le4864 at mweb.co.za wrote:
> Good day,
> My name is Kim Nguyen and please I need your help with: How to calculate the PASS rate of the data in the table below, with PASS in a single subject if value>=50 and PASS will be given if PASS 3 out of 4 subjectsWhich package will I need to use in this analysis
>
>
>
>
> ID
>
> Literacy
>
> Maths
>
> Physics
>
> Chemistry
>
>
> A
>
> 65
>
> 70
>
> 79
>
> 80
>
>
> B
>
> 65
>
> 45
>
> 38
>
> 50
>
>
> C
>
> 50
>
> 62
>
> 48
>
> 49
>
>
> D
>
> 70
>
> 85
>
> 82
>
> 84
>
>
> E
>
> 45
>
> 69
>
> 65
>
> 62
>
> I appreciate your consultant very much
> With kind regardsKim Nguyen


From bgunter.4567 at gmail.com  Tue Sep 22 01:14:03 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 21 Sep 2015 16:14:03 -0700
Subject: [R] vector manipulations -- differences
In-Reply-To: <1442870260243-4712575.post@n4.nabble.com>
References: <1442870260243-4712575.post@n4.nabble.com>
Message-ID: <CAGxFJbR-dutC1MBZpQdHO1OUwC7JNhBFTWz1kT-YE6q0CUxBTQ@mail.gmail.com>

Use ?mappy and ?rep.int

> x[unlist(mapply(":",2:4,4))] - x[rep.int(1:3,3:1)]
[1]  3  7 20  4 17 13

Cheers,

Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Sep 21, 2015 at 2:17 PM, Dan D <ddalthorp at usgs.gov> wrote:
> I need an efficient way to build a new n x (n-1)/2 vector from an n-vector x
> as:
>
> c(x[-1]-x[1], x[-(1:2)]-x[2], ... , x[-(1:(n-1)] - x[n-1])
>
> x is increasing with x[1] = 0.
>
> The following works but is not the greatest:
> junk<-outer(x, x, '-')
> junk[junk>0]
>
> e.g.,
> given
> x<-c(0, 3, 7, 20)
> junk<-outer(x, x, '-')
> junk[junk>0] # yields: c(3, 7, 20, 4, 17, 13) as needed, but it has to go
> through
> junk
> #     [,1] [,2] [,3] [,4]
> #[1,]    0   -3   -7  -20
> #[2,]    3    0   -4  -17
> #[3,]    7    4    0  -13
> #[4,]   20   17   13    0
>
> Anyone have a better idea?
>
> -Dan
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/vector-manipulations-differences-tp4712575.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Tue Sep 22 02:24:37 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 21 Sep 2015 16:24:37 -0800
Subject: [R] R-help, please
In-Reply-To: <MWSJHBMC05bb4d0e6585834f87b8ec6823b115ab65@MWSJHBMC05>
Message-ID: <2A124F5BF77.00000B36jrkrideau@inbox.com>

It looks like you sent the e-mail in HTML. 

It is unreadable.

You must sent e-mail to R-help in plain text not HTML

John Kane
Kingston ON Canada


> -----Original Message-----
> From: le4864 at mweb.co.za
> Sent: Mon, 21 Sep 2015 16:14:41 +0200
> To: r-help at r-project.org
> Subject: Re: [R] R-help, please
> 
> Good day,
> My name is Kim Nguyen and please I need your help with: How to calculate
> the PASS rate of the data in the table below, with PASS in a single
> subject if value>=50 and PASS will be given if PASS 3 out of 4
> subjectsWhich package will I need to use in this analysis
> 
> 
> 
> 
> ID
> 
> Literacy
> 
> Maths
> 
> Physics
> 
> Chemistry
> 
> 
> A
> 
> 65
> 
> 70
> 
> 79
> 
> 80
> 
> 
> B
> 
> 65
> 
> 45
> 
> 38
> 
> 50
> 
> 
> C
> 
> 50
> 
> 62
> 
> 48
> 
> 49
> 
> 
> D
> 
> 70
> 
> 85
> 
> 82
> 
> 84
> 
> 
> E
> 
> 45
> 
> 69
> 
> 65
> 
> 62
> 
> I appreciate your consultant very much
> With kind regardsKim Nguyen
> Sent from MWEB Message Centre - CONNECT AND YOU CAN
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From jrkrideau at inbox.com  Tue Sep 22 02:27:44 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 21 Sep 2015 16:27:44 -0800
Subject: [R] extract from data.frame (indexing)
In-Reply-To: <CAJ+rinNs8MUeNpBtxP7HDn9J0btXqaarZgdMeEeNAFi1J5uP_Q@mail.gmail.com>
Message-ID: <2A19472B3A0.00000B43jrkrideau@inbox.com>

And the action is?

John Kane
Kingston ON Canada


> -----Original Message-----
> From: nico.gutierrezo at gmail.com
> Sent: Mon, 21 Sep 2015 16:48:45 +0200
> To: r-help at r-project.org
> Subject: [R] extract from data.frame (indexing)
> 
> Hi All,
> 
> I need to do the following operation:
> 
> 
>   Year Amount Amount.1
> 1 2001    150      150
> 2 2002    120      120
> 3 2003    175      175
> 4 2004    160      160
> 5 2005    120      120
> 6 2006    105      105
> 7 2007    135      135
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From jrkrideau at inbox.com  Tue Sep 22 02:39:21 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 21 Sep 2015 16:39:21 -0800
Subject: [R] Running GCV Optimization under Ridge Regression
In-Reply-To: <CAHVFrXEd_S8p96Cwz38FUHhUdd=4ys9iHNLP4PcLfjsfvgAjXQ@mail.gmail.com>
Message-ID: <2A333C625C1.00000B67jrkrideau@inbox.com>


No data. 

Please have a look at http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html
John Kane
Kingston ON Canada


> -----Original Message-----
> From: lordpreetam at gmail.com
> Sent: Tue, 22 Sep 2015 01:19:38 +0530
> To: r-help at r-project.org
> Subject: [R] Running GCV Optimization under Ridge Regression
> 
> Hi guys,
> 
> I am running Ridge regression on a dataset (predicted variable = y; GDP,
> HPA and FX are regressors). I found that lm.ridge() can perform the ridge
> regression given any value of lambda (i.e. the ridge parameter). However,
> in order to choose the best results, I need to select the model output
> corresponding to that lambda which optimizes some logically defined GCV
> criteria. I thought there would be some in-built funcion in R Studio for
> this, but could not find one. Also, I am not being able to write the
> required code for this. Any help here will be appreciated. I have
> attached
> the data in case it is required.
> 
> Thanks,
> Preetam
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From rmcgu at doh.health.nsw.gov.au  Tue Sep 22 01:06:02 2015
From: rmcgu at doh.health.nsw.gov.au (MCGUIRE, Rhydwyn)
Date: Mon, 21 Sep 2015 23:06:02 +0000
Subject: [R] R-help, please
In-Reply-To: <MWSJHBMC05bb4d0e6585834f87b8ec6823b115ab65@MWSJHBMC05>
References: <MWSJHBMC05bb4d0e6585834f87b8ec6823b115ab65@MWSJHBMC05>
Message-ID: <AF36C32BE015CB48883C4A9F73CC8C32015D04D99D@DOHNSMXDB02.doh.health.nsw.gov.au>

Hi Kim, this sounds like a homework question which is not meant for this list, I don't believe you need a package for this, but you may find the ifelse function along with the sum()function useful. You can get more information by typing ?ifelse and ?sum into R.

Regards,
Rhydwyn


Rhydwyn McGuire
Senior Biostatistician | Health Statistics NSW
Level 7, 73 Miller St, North Sydney 2060
Tel 02 9391 9781 | rmcgu at doh.health.nsw.gov.au
www.health.nsw.gov.au




-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of le4864 at mweb.co.za
Sent: Tuesday, 22 September 2015 12:15 AM
To: r-help at R-project.org
Subject: Re: [R] R-help, please

Good day,
My name is Kim Nguyen and please I need your help with: How to calculate the PASS rate of the data in the table below, with PASS in a single subject if value>=50 and PASS will be given if PASS 3 out of 4 subjectsWhich package will I need to use in this analysis




ID

Literacy

Maths

Physics

Chemistry


A

65

70

79

80


B

65

45

38

50


C

50

62

48

49


D

70

85

82

84


E

45

69

65

62

I appreciate your consultant very much
With kind regardsKim Nguyen
Sent from MWEB Message Centre - CONNECT AND YOU CAN

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
__________________________________________________________________________________________________________
This email has been scanned for the NSW Ministry of Health by the Websense Hosted Email Security System.
Emails and attachments are monitored to ensure compliance with the NSW Ministry of health's Electronic Messaging Policy.
__________________________________________________________________________________________________________
_______________________________________________________________________________________________________
Disclaimer: This message is intended for the addressee named and may contain confidential information.
If you are not the intended recipient, please delete it and notify the sender.
Views expressed in this message are those of the individual sender, and are not necessarily the views of the NSW Ministry of Health.
_______________________________________________________________________________________________________
This email has been scanned for the NSW Ministry of Health by the Websense Hosted Email Security System.
Emails and attachments are monitored to ensure compliance with the NSW Ministry of Health's Electronic Messaging Policy.


From fanjianling at gmail.com  Tue Sep 22 04:26:02 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Tue, 22 Sep 2015 10:26:02 +0800
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>
Message-ID: <CAJ7mryKogK=Tx=S3Q9ojSHmbZiNAbcN+FvbP+Otez-a5=bRRtA@mail.gmail.com>

Hello, Gabor,

Thanks again for your suggestion. And now I am trying to improve the
code by adding a function to replace the express "Rm1 * ref.1 + Rm2 *
ref.2 + Rm3 * ref.3 + Rm4 * ref.4 + Rm5 * ref.5 + Rm6 * ref.6" because
I have some other dataset need to fitted to the same model but with
more groups (>20).

I tried to add the function as:

denfun<-function(i){
               for(i in 1:6){
                 Rm<-sum(Rm[i]*ref.i)
                 return(Rm)}
}

but I got another error when I incorporate this function into my regression:

>fitdp1<-nlxb(den ~ denfun(6)/(1+(depth/d50)^c),
                   data = dproot2,
                 start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
Rm5=1.01, Rm6=1, d50=20, c=-1),
                masked = "Rm6")

Error in deriv.default(parse(text = resexp), names(start)) :
  Function 'denfun' is not in the derivatives table

I think there must be something wrong with my function. I tried some
times but am not sure how to improve it because I am quite new to R.

Could anyone please give me some suggestion.

Thanks a lot!


Jianling


On 22 September 2015 at 00:43, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> Express the formula in terms of simple operations like this:
>
> # add 0/1 columns ref.1, ref.2, ..., ref.6
> dproot2 <- do.call(data.frame, transform(dproot, ref = outer(dproot$ref,
> seq(6), "==") + 0))
>
> # now express the formula in terms of the new columns
> library(nlmrt)
> fitdp1<-nlxb(den ~ (Rm1 * ref.1 + Rm2 * ref.2 + Rm3 * ref.3 + Rm4 * ref.4 +
> Rm5 * ref.5 + Rm6 * ref.6)/(1+(depth/d50)^c),
>          data = dproot2,
>          start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01, Rm6=1,
> d50=20, c=-1),
>          masked = "Rm6")
>
> where we used this input:
>
> Lines <- "   depth       den ref
> 1     20 0.5730000   1
> 2     40 0.7800000   1
> 3     60 0.9470000   1
> 4     80 0.9900000   1
> 5    100 1.0000000   1
> 6     10 0.6000000   2
> 7     20 0.8200000   2
> 8     30 0.9300000   2
> 9     40 1.0000000   2
> 10    20 0.4800000   3
> 11    40 0.7340000   3
> 12    60 0.9610000   3
> 13    80 0.9980000   3
> 14   100 1.0000000   3
> 15    20 3.2083491   4
> 16    40 4.9683383   4
> 17    60 6.2381133   4
> 18    80 6.5322348   4
> 19   100 6.5780660   4
> 20   120 6.6032064   4
> 21    20 0.6140000   5
> 22    40 0.8270000   5
> 23    60 0.9500000   5
> 24    80 0.9950000   5
> 25   100 1.0000000   5
> 26    20 0.4345774   6
> 27    40 0.6654726   6
> 28    60 0.8480684   6
> 29    80 0.9268951   6
> 30   100 0.9723207   6
> 31   120 0.9939966   6
> 32   140 0.9992400   6"
>
> dproot <- read.table(text = Lines, header = TRUE)
>
>
>
> On Mon, Sep 21, 2015 at 12:22 PM, Jianling Fan <fanjianling at gmail.com>
> wrote:
>>
>> Thanks Prof. Nash,
>>
>> Sorry for late reply. I am learning and trying to use your nlmrt
>> package since I got your email. It works good to mask a parameter in
>> regression but seems does work for my equation. I think the problem is
>> that the parameter I want to mask is a group-specific parameter and I
>> have a "[]" syntax in my equation. However, I don't have your 2014
>> book on hand and couldn't find it in our library. So I am wondering if
>> nlxb works for group data?
>> Thanks a lot!
>>
>> following is my code and I got a error form it.
>>
>> > fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>                 + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>> Rm5=1.01, Rm6=1, d50=20, c=-1),
>>                 + masked=c("Rm6"))
>>
>> Error in deriv.default(parse(text = resexp), names(start)) :
>>   Function '`[`' is not in the derivatives table
>>
>>
>> Best regards,
>>
>> Jianling
>>
>>
>> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
>> > I posted a suggestion to use nlmrt package (function nlxb to be
>> > precise),
>> > which has masked (fixed) parameters. Examples in my 2014 book on
>> > Nonlinear
>> > parameter optimization with R tools. However, I'm travelling just now,
>> > or
>> > would consider giving this a try.
>> >
>> > JN
>> >
>> >
>> > On 15-09-20 01:19 PM, Jianling Fan wrote:
>> >>
>> >> no, I am doing a regression with 6 group data with 2 shared parameters
>> >> and 1 different parameter for each group data. the parameter I want to
>> >> coerce is for one group. I don't know how to do it. Any suggestion?
>> >>
>> >> Thanks!
>> >>
>> >> On 19 September 2015 at 13:33, Jeff Newmiller
>> >> <jdnewmil at dcn.davis.ca.us>
>> >> wrote:
>> >>>
>> >>> Why not rewrite the function so that value is not a parameter?
>> >>>
>> >>>
>> >>> ---------------------------------------------------------------------------
>> >>> Jeff Newmiller                        The     .....       .....  Go
>> >>> Live...
>> >>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> >>> Go...
>> >>>                                        Live:   OO#.. Dead: OO#..
>> >>> Playing
>> >>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> >>> /Software/Embedded Controllers)               .OO#.       .OO#.
>> >>> rocks...1k
>> >>>
>> >>>
>> >>> ---------------------------------------------------------------------------
>> >>> Sent from my phone. Please excuse my brevity.
>> >>>
>> >>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>> >>> <fanjianling at gmail.com> wrote:
>> >>>>
>> >>>> Hello, everyone,
>> >>>>
>> >>>> I am using a nls regression with 6 groups data. I am trying to coerce
>> >>>> a parameter to 1 by using a upper and lower statement. but I always
>> >>>> get an error like below:
>> >>>>
>> >>>> Error in ifelse(internalPars < upper, 1, -1) :
>> >>>>   (list) object cannot be coerced to type 'double'
>> >>>>
>> >>>> does anyone know how to fix it?
>> >>>>
>> >>>> thanks in advance!
>> >>>>
>> >>>> My code is below:
>> >>>>
>> >>>>
>> >>>>
>> >>>>> dproot
>> >>>>
>> >>>>    depth       den ref
>> >>>> 1     20 0.5730000   1
>> >>>> 2     40 0.7800000   1
>> >>>> 3     60 0.9470000   1
>> >>>> 4     80 0.9900000   1
>> >>>> 5    100 1.0000000   1
>> >>>> 6     10 0.6000000   2
>> >>>> 7     20 0.8200000   2
>> >>>> 8     30 0.9300000   2
>> >>>> 9     40 1.0000000   2
>> >>>> 10    20 0.4800000   3
>> >>>> 11    40 0.7340000   3
>> >>>> 12    60 0.9610000   3
>> >>>> 13    80 0.9980000   3
>> >>>> 14   100 1.0000000   3
>> >>>> 15    20 3.2083491   4
>> >>>> 16    40 4.9683383   4
>> >>>> 17    60 6.2381133   4
>> >>>> 18    80 6.5322348   4
>> >>>> 19   100 6.5780660   4
>> >>>> 20   120 6.6032064   4
>> >>>> 21    20 0.6140000   5
>> >>>> 22    40 0.8270000   5
>> >>>> 23    60 0.9500000   5
>> >>>> 24    80 0.9950000   5
>> >>>> 25   100 1.0000000   5
>> >>>> 26    20 0.4345774   6
>> >>>> 27    40 0.6654726   6
>> >>>> 28    60 0.8480684   6
>> >>>> 29    80 0.9268951   6
>> >>>> 30   100 0.9723207   6
>> >>>> 31   120 0.9939966   6
>> >>>> 32   140 0.9992400   6
>> >>>>
>> >>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >>>>
>> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>> >>>>>
>> >>>>> summary(fitdp)
>> >>>>
>> >>>>
>> >>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>> >>>>
>> >>>> Parameters:
>> >>>>     Estimate Std. Error t value Pr(>|t|)
>> >>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>> >>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>> >>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>> >>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>> >>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>> >>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>> >>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>> >>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>> >>>> ---
>> >>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>> >>>>
>> >>>> Residual standard error: 0.1094 on 24 degrees of freedom
>> >>>>
>> >>>> Number of iterations to convergence: 8
>> >>>> Achieved convergence tolerance: 9.374e-06
>> >>>>
>> >>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >>>>
>> >>>> algorithm="port",
>> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>> >>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>> >>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>> >>>>
>> >>>> Error in ifelse(internalPars < upper, 1, -1) :
>> >>>>   (list) object cannot be coerced to type 'double'
>> >>>>
>> >>>> ______________________________________________
>> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>> PLEASE do read the posting guide
>> >>>> http://www.R-project.org/posting-guide.html
>> >>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>>
>> >>
>> >>
>> >>
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Jianling Fan
>> ???
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Jianling Fan
???


From lordpreetam at gmail.com  Tue Sep 22 06:49:11 2015
From: lordpreetam at gmail.com (Preetam Pal)
Date: Tue, 22 Sep 2015 10:19:11 +0530
Subject: [R] Running GCV Optimization under Ridge Regression
In-Reply-To: <2A333C625C1.00000B67jrkrideau@inbox.com>
References: <CAHVFrXEd_S8p96Cwz38FUHhUdd=4ys9iHNLP4PcLfjsfvgAjXQ@mail.gmail.com>
	<2A333C625C1.00000B67jrkrideau@inbox.com>
Message-ID: <CAHVFrXGQuUpxtysZPe9O7a67FrT1r50Rt1k4=EBiy_VB3kv=1w@mail.gmail.com>

Hi John/R-users,

   - I have attached the data set in the mail in .txt format, can be read
   using read.table(). Kindly let me know please if this is not sufficient.
   - Also, to specify the modeling scheme I am stuck at:


   1. Have numerical regressors GDP, HPA and FX to predict the variable Y
   -- all these are quarterly time series.
   2. Am looking to implement *Weighted Ridge regression* where the
   observation weights (in SSE computation) are decreasing into the past at 5%
   rate each quarter
   3. Need to optimize wrt GCV criterion (leave K out scheme, K = 10% of
   data size) to get the best lambda (Ridge parameter)
   4. Then, for this optimum lambda, compute beta over the whole data as
   [X'W'WX + Lambda * I]^-1 * X'W'WY (W'W is a diagonal matrix with entries
   decreasing at 5% from the last entry to the first, and preferably summing
   upto 1 ]

Please let me know if anything is unclear, would be happy to elaborate. The
problem is I am very new to coding and although I know of some functions
that may be relevant (like lm.ridge), I am not being able to implement the
entire code myself.Appreciate your help.

Regards,
Preetam


On Tue, Sep 22, 2015 at 6:09 AM, John Kane <jrkrideau at inbox.com> wrote:

>
> No data.
>
> Please have a look at
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> and http://adv-r.had.co.nz/Reproducibility.html
> John Kane
> Kingston ON Canada
>
>
> > -----Original Message-----
> > From: lordpreetam at gmail.com
> > Sent: Tue, 22 Sep 2015 01:19:38 +0530
> > To: r-help at r-project.org
> > Subject: [R] Running GCV Optimization under Ridge Regression
> >
> > Hi guys,
> >
> > I am running Ridge regression on a dataset (predicted variable = y; GDP,
> > HPA and FX are regressors). I found that lm.ridge() can perform the ridge
> > regression given any value of lambda (i.e. the ridge parameter). However,
> > in order to choose the best results, I need to select the model output
> > corresponding to that lambda which optimizes some logically defined GCV
> > criteria. I thought there would be some in-built funcion in R Studio for
> > this, but could not find one. Also, I am not being able to write the
> > required code for this. Any help here will be appreciated. I have
> > attached
> > the data in case it is required.
> >
> > Thanks,
> > Preetam
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on
> your desktop!
> Check it out at http://www.inbox.com/marineaquarium
>
>
>


-- 
Preetam Pal
(+91)-9432212774
M-Stat 2nd Year,                                             Room No. N-114
Statistics Division,                                           C.V.Raman
Hall
Indian Statistical Institute,                                 B.H.O.S.
Kolkata.
-------------- next part --------------
T	GDP Rate	HPA	FX	Y
1	0.806660537	2.177803167	1.14980573	2.733594304
2	0.997724655	1.585686087	0.814496976	3.193948056
3	0.99032353	0.569843997	0.464488882	3.065751781
4	0.606121306	3.037648988	0.565322084	4.537399052
5	0.858131141	4.816423605	1.924534222	7.871730873
6	0.052909178	2.048591352	1.470221953	2.580646078
7	0.081400487	1.152495559	1.128828557	7.200336313
8	0.840972911	3.848225962	1.004272646	1.211124673
9	0.965868218	1.039679934	0.231408747	7.566968
10	0.952626722	4.455565591	0.483541015	9.412639513
11	0.067691757	0.038417569	0.69744243	8.055369029
12	0.985658841	1.143481763	1.65850909	6.962599601
13	0.177186946	3.762691635	0.44379572	9.904367023
14	0.490066697	0.655629739	1.281478696	1.796422139
15	0.223740666	1.393201062	1.235291827	5.237943945
16	0.782873809	1.485727273	0.224511215	6.399036418
17	0.947492758	0.318485005	1.158911495	8.183470692
18	0.49692711	2.169601457	1.777618832	8.830805294
19	0.956704273	1.546827505	0.241838792	7.554654431
20	0.404624372	3.041530693	1.66039172	6.709330773
21	0.98557461	2.45656369	1.695179666	8.638707974
22	0.494102398	4.527230971	0.993352283	7.958872374
23	0.893182943	3.429112971	0.675541115	5.665249801
24	0.669680459	0.459919029	1.011872328	8.883120607
25	0.017296599	2.184045646	1.575891106	2.585709635

From nico.gutierrezo at gmail.com  Tue Sep 22 09:52:44 2015
From: nico.gutierrezo at gmail.com (Nico Gutierrez)
Date: Tue, 22 Sep 2015 09:52:44 +0200
Subject: [R] Extract from data.frame
In-Reply-To: <CAGxFJbTSmfxj_TRCno6Np2W_R4HSihKdzg9bJjoYjmm=b64u-w@mail.gmail.com>
References: <CAJ+rinMkVi2S5y7U5GEnR1Aey4Kao86CqwkccgDqLtai2d_YRA@mail.gmail.com>
	<CAAJSdjgvdbPzQjPnzgYkU77RWTTRnYAC42RbFY4as7U2QcaVmA@mail.gmail.com>
	<CAGxFJbTSmfxj_TRCno6Np2W_R4HSihKdzg9bJjoYjmm=b64u-w@mail.gmail.com>
Message-ID: <CAJ+rinOc4=17dwZbchdbz_2Ld4yp0=6Uk51ZNUs0Nf-UyP6q6Q@mail.gmail.com>

Thank you all!
n

On Mon, Sep 21, 2015 at 10:00 PM, Bert Gunter <bgunter.4567 at gmail.com>
wrote:

> No.
>
>
> On Mon, Sep 21, 2015 at 10:58 AM, John McKown
> <john.archie.mckown at gmail.com> wrote:
> > On Mon, Sep 21, 2015 at 9:52 AM, Nico Gutierrez <
> nico.gutierrezo at gmail.com>
> > wrote:
> >
> >> Hi All,
> >>
> >> I need to do the following operation from data.frame:
> >>
> >> df <- data.frame(Year = c("2001", "2002", "2003", "2004", "2005",
> "2006",
> >> "2007"), Amount = c(150, 120, 175, 160, 120, 105, 135))
> >> df[which.max(df$Amount),]  #to extract row with max Amount.
> >>
> >> Now I need to do 3 years average around the max Amount value (ie:
> >> mean(120,175,160))
> >>
> >> Thanks!
> >> N
> >>
> >>
> > The simplistic answer is something like:
> >
> > df <- structure(list(Year = structure(1:7, .Label = c("2001", "2002",
> > "2003", "2004", "2005", "2006", "2007"), class = "factor"), Amount =
> c(150,
> > 120, 175, 160, 120, 105, 135)), .Names = c("Year", "Amount"), row.names =
> > c(NA,
> > -7L), class = "data.frame");
> > wdf <- which.max(df$Amount);
> > adf3 <- mean(df$Amount[adf-1:adr+1]);
>
> Typos?!
> But it won't work anyway. See ?Syntax for operator precedence and
>
> Example:
>
> > a <- 1:5
> > mid <- 3
> > a[mid-1:mid+1]
> [1] 3 2 1
> > a[(mid-1):(mid+1)]
> [1] 2 3 4
>
> Cheers,
> Bert
>
>
> >
> > But that ignores the boundry condition where the maximum is at either
> end.
> > What do you want to do in that case?
> >
> >
> > --
> >
> > Schrodinger's backup: The condition of any backup is unknown until a
> > restore is attempted.
> >
> > Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will
> be.
> >
> > He's about as useful as a wax frying pan.
> >
> > 10 to the 12th power microphones = 1 Megaphone
> >
> > Maranatha! <><
> > John McKown
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From nico.gutierrezo at gmail.com  Tue Sep 22 11:36:09 2015
From: nico.gutierrezo at gmail.com (Nico Gutierrez)
Date: Tue, 22 Sep 2015 11:36:09 +0200
Subject: [R] store results in loop
Message-ID: <CAJ+rinOK4Cu69Gu6AtiUWrGwu2pCyWWVrE2nV_MLVFgZAyvh8g@mail.gmail.com>

Hi All,

very rusty in R.. my results get overwritten when try to store within the
loop. This my code:
ListS=unique(data$Spec)
Stat<- numeric(0)

for(i in 5){

SS=subset(data,data$Spec==ListS[i])
maxC<- which.max(SS$Cc)
smoothC=mean(SS$Cc[maxC + c(-2:2)])
currC=tail(SS,1)$Cc
Index=currC/smoothC

Stat[i]=c(Stat, Index[i])
}
Stat

This is what I get:

[1] NA NA NA NA NA


I am obviously not indexing well here.


Thanks!!!

N

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Tue Sep 22 11:46:35 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 22 Sep 2015 19:46:35 +1000
Subject: [R] store results in loop
In-Reply-To: <CAJ+rinOK4Cu69Gu6AtiUWrGwu2pCyWWVrE2nV_MLVFgZAyvh8g@mail.gmail.com>
References: <CAJ+rinOK4Cu69Gu6AtiUWrGwu2pCyWWVrE2nV_MLVFgZAyvh8g@mail.gmail.com>
Message-ID: <CA+8X3fUyGXXdG1HK+tqZq1a9rgxLvdro07VjYbgpWEtSnZ8jXA@mail.gmail.com>

Hi Nico,
A bit difficult to see what is happening without the data, but two
suggestions:

smoothC=mean(SS$Cc[maxC + c(-2:2)],na.rm=TRUE)
...
Stat[i]<-Index

Jim


On Tue, Sep 22, 2015 at 7:36 PM, Nico Gutierrez <nico.gutierrezo at gmail.com>
wrote:

> Hi All,
>
> very rusty in R.. my results get overwritten when try to store within the
> loop. This my code:
> ListS=unique(data$Spec)
> Stat<- numeric(0)
>
> for(i in 5){
>
> SS=subset(data,data$Spec==ListS[i])
> maxC<- which.max(SS$Cc)
> smoothC=mean(SS$Cc[maxC + c(-2:2)])
> currC=tail(SS,1)$Cc
> Index=currC/smoothC
>
> Stat[i]=c(Stat, Index[i])
> }
> Stat
>
> This is what I get:
>
> [1] NA NA NA NA NA
>
>
> I am obviously not indexing well here.
>
>
> Thanks!!!
>
> N
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Tue Sep 22 12:51:22 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 22 Sep 2015 11:51:22 +0100
Subject: [R] store results in loop
In-Reply-To: <CAJ+rinOK4Cu69Gu6AtiUWrGwu2pCyWWVrE2nV_MLVFgZAyvh8g@mail.gmail.com>
References: <CAJ+rinOK4Cu69Gu6AtiUWrGwu2pCyWWVrE2nV_MLVFgZAyvh8g@mail.gmail.com>
Message-ID: <560132AA.9000600@dewey.myzen.co.uk>

Dear Nico

Comment inline

On 22/09/2015 10:36, Nico Gutierrez wrote:
> Hi All,
>
> very rusty in R.. my results get overwritten when try to store within the
> loop. This my code:
> ListS=unique(data$Spec)
> Stat<- numeric(0)
>
> for(i in 5){
>

Is that what you meant? I would have expected something like 1:5

> SS=subset(data,data$Spec==ListS[i])
> maxC<- which.max(SS$Cc)
> smoothC=mean(SS$Cc[maxC + c(-2:2)])
> currC=tail(SS,1)$Cc
> Index=currC/smoothC
>
> Stat[i]=c(Stat, Index[i])
> }
> Stat
>
> This is what I get:
>
> [1] NA NA NA NA NA
>
>
> I am obviously not indexing well here.
>
>
> Thanks!!!
>
> N
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From ggrothendieck at gmail.com  Tue Sep 22 13:04:39 2015
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Sep 2015 07:04:39 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAJ7mryKogK=Tx=S3Q9ojSHmbZiNAbcN+FvbP+Otez-a5=bRRtA@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>
	<CAJ7mryKogK=Tx=S3Q9ojSHmbZiNAbcN+FvbP+Otez-a5=bRRtA@mail.gmail.com>
Message-ID: <CAP01uRkBEfopg4UfMOx7eeVsemW1axSFXML+d=dTmZO62-x0FQ@mail.gmail.com>

Just write out the 20 terms.

On Mon, Sep 21, 2015 at 10:26 PM, Jianling Fan <fanjianling at gmail.com>
wrote:

> Hello, Gabor,
>
> Thanks again for your suggestion. And now I am trying to improve the
> code by adding a function to replace the express "Rm1 * ref.1 + Rm2 *
> ref.2 + Rm3 * ref.3 + Rm4 * ref.4 + Rm5 * ref.5 + Rm6 * ref.6" because
> I have some other dataset need to fitted to the same model but with
> more groups (>20).
>
> I tried to add the function as:
>
> denfun<-function(i){
>                for(i in 1:6){
>                  Rm<-sum(Rm[i]*ref.i)
>                  return(Rm)}
> }
>
> but I got another error when I incorporate this function into my
> regression:
>
> >fitdp1<-nlxb(den ~ denfun(6)/(1+(depth/d50)^c),
>                    data = dproot2,
>                  start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
> Rm5=1.01, Rm6=1, d50=20, c=-1),
>                 masked = "Rm6")
>
> Error in deriv.default(parse(text = resexp), names(start)) :
>   Function 'denfun' is not in the derivatives table
>
> I think there must be something wrong with my function. I tried some
> times but am not sure how to improve it because I am quite new to R.
>
> Could anyone please give me some suggestion.
>
> Thanks a lot!
>
>
> Jianling
>
>
> On 22 September 2015 at 00:43, Gabor Grothendieck
> <ggrothendieck at gmail.com> wrote:
> > Express the formula in terms of simple operations like this:
> >
> > # add 0/1 columns ref.1, ref.2, ..., ref.6
> > dproot2 <- do.call(data.frame, transform(dproot, ref = outer(dproot$ref,
> > seq(6), "==") + 0))
> >
> > # now express the formula in terms of the new columns
> > library(nlmrt)
> > fitdp1<-nlxb(den ~ (Rm1 * ref.1 + Rm2 * ref.2 + Rm3 * ref.3 + Rm4 *
> ref.4 +
> > Rm5 * ref.5 + Rm6 * ref.6)/(1+(depth/d50)^c),
> >          data = dproot2,
> >          start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01,
> Rm6=1,
> > d50=20, c=-1),
> >          masked = "Rm6")
> >
> > where we used this input:
> >
> > Lines <- "   depth       den ref
> > 1     20 0.5730000   1
> > 2     40 0.7800000   1
> > 3     60 0.9470000   1
> > 4     80 0.9900000   1
> > 5    100 1.0000000   1
> > 6     10 0.6000000   2
> > 7     20 0.8200000   2
> > 8     30 0.9300000   2
> > 9     40 1.0000000   2
> > 10    20 0.4800000   3
> > 11    40 0.7340000   3
> > 12    60 0.9610000   3
> > 13    80 0.9980000   3
> > 14   100 1.0000000   3
> > 15    20 3.2083491   4
> > 16    40 4.9683383   4
> > 17    60 6.2381133   4
> > 18    80 6.5322348   4
> > 19   100 6.5780660   4
> > 20   120 6.6032064   4
> > 21    20 0.6140000   5
> > 22    40 0.8270000   5
> > 23    60 0.9500000   5
> > 24    80 0.9950000   5
> > 25   100 1.0000000   5
> > 26    20 0.4345774   6
> > 27    40 0.6654726   6
> > 28    60 0.8480684   6
> > 29    80 0.9268951   6
> > 30   100 0.9723207   6
> > 31   120 0.9939966   6
> > 32   140 0.9992400   6"
> >
> > dproot <- read.table(text = Lines, header = TRUE)
> >
> >
> >
> > On Mon, Sep 21, 2015 at 12:22 PM, Jianling Fan <fanjianling at gmail.com>
> > wrote:
> >>
> >> Thanks Prof. Nash,
> >>
> >> Sorry for late reply. I am learning and trying to use your nlmrt
> >> package since I got your email. It works good to mask a parameter in
> >> regression but seems does work for my equation. I think the problem is
> >> that the parameter I want to mask is a group-specific parameter and I
> >> have a "[]" syntax in my equation. However, I don't have your 2014
> >> book on hand and couldn't find it in our library. So I am wondering if
> >> nlxb works for group data?
> >> Thanks a lot!
> >>
> >> following is my code and I got a error form it.
> >>
> >> > fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> >>                 + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
> >> Rm5=1.01, Rm6=1, d50=20, c=-1),
> >>                 + masked=c("Rm6"))
> >>
> >> Error in deriv.default(parse(text = resexp), names(start)) :
> >>   Function '`[`' is not in the derivatives table
> >>
> >>
> >> Best regards,
> >>
> >> Jianling
> >>
> >>
> >> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
> >> > I posted a suggestion to use nlmrt package (function nlxb to be
> >> > precise),
> >> > which has masked (fixed) parameters. Examples in my 2014 book on
> >> > Nonlinear
> >> > parameter optimization with R tools. However, I'm travelling just now,
> >> > or
> >> > would consider giving this a try.
> >> >
> >> > JN
> >> >
> >> >
> >> > On 15-09-20 01:19 PM, Jianling Fan wrote:
> >> >>
> >> >> no, I am doing a regression with 6 group data with 2 shared
> parameters
> >> >> and 1 different parameter for each group data. the parameter I want
> to
> >> >> coerce is for one group. I don't know how to do it. Any suggestion?
> >> >>
> >> >> Thanks!
> >> >>
> >> >> On 19 September 2015 at 13:33, Jeff Newmiller
> >> >> <jdnewmil at dcn.davis.ca.us>
> >> >> wrote:
> >> >>>
> >> >>> Why not rewrite the function so that value is not a parameter?
> >> >>>
> >> >>>
> >> >>>
> ---------------------------------------------------------------------------
> >> >>> Jeff Newmiller                        The     .....       .....  Go
> >> >>> Live...
> >> >>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
> Live
> >> >>> Go...
> >> >>>                                        Live:   OO#.. Dead: OO#..
> >> >>> Playing
> >> >>> Research Engineer (Solar/Batteries            O.O#.       #.O#.
> with
> >> >>> /Software/Embedded Controllers)               .OO#.       .OO#.
> >> >>> rocks...1k
> >> >>>
> >> >>>
> >> >>>
> ---------------------------------------------------------------------------
> >> >>> Sent from my phone. Please excuse my brevity.
> >> >>>
> >> >>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
> >> >>> <fanjianling at gmail.com> wrote:
> >> >>>>
> >> >>>> Hello, everyone,
> >> >>>>
> >> >>>> I am using a nls regression with 6 groups data. I am trying to
> coerce
> >> >>>> a parameter to 1 by using a upper and lower statement. but I always
> >> >>>> get an error like below:
> >> >>>>
> >> >>>> Error in ifelse(internalPars < upper, 1, -1) :
> >> >>>>   (list) object cannot be coerced to type 'double'
> >> >>>>
> >> >>>> does anyone know how to fix it?
> >> >>>>
> >> >>>> thanks in advance!
> >> >>>>
> >> >>>> My code is below:
> >> >>>>
> >> >>>>
> >> >>>>
> >> >>>>> dproot
> >> >>>>
> >> >>>>    depth       den ref
> >> >>>> 1     20 0.5730000   1
> >> >>>> 2     40 0.7800000   1
> >> >>>> 3     60 0.9470000   1
> >> >>>> 4     80 0.9900000   1
> >> >>>> 5    100 1.0000000   1
> >> >>>> 6     10 0.6000000   2
> >> >>>> 7     20 0.8200000   2
> >> >>>> 8     30 0.9300000   2
> >> >>>> 9     40 1.0000000   2
> >> >>>> 10    20 0.4800000   3
> >> >>>> 11    40 0.7340000   3
> >> >>>> 12    60 0.9610000   3
> >> >>>> 13    80 0.9980000   3
> >> >>>> 14   100 1.0000000   3
> >> >>>> 15    20 3.2083491   4
> >> >>>> 16    40 4.9683383   4
> >> >>>> 17    60 6.2381133   4
> >> >>>> 18    80 6.5322348   4
> >> >>>> 19   100 6.5780660   4
> >> >>>> 20   120 6.6032064   4
> >> >>>> 21    20 0.6140000   5
> >> >>>> 22    40 0.8270000   5
> >> >>>> 23    60 0.9500000   5
> >> >>>> 24    80 0.9950000   5
> >> >>>> 25   100 1.0000000   5
> >> >>>> 26    20 0.4345774   6
> >> >>>> 27    40 0.6654726   6
> >> >>>> 28    60 0.8480684   6
> >> >>>> 29    80 0.9268951   6
> >> >>>> 30   100 0.9723207   6
> >> >>>> 31   120 0.9939966   6
> >> >>>> 32   140 0.9992400   6
> >> >>>>
> >> >>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> >> >>>>
> >> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
> >> >>>>>
> >> >>>>> summary(fitdp)
> >> >>>>
> >> >>>>
> >> >>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
> >> >>>>
> >> >>>> Parameters:
> >> >>>>     Estimate Std. Error t value Pr(>|t|)
> >> >>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
> >> >>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
> >> >>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
> >> >>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
> >> >>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
> >> >>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
> >> >>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
> >> >>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
> >> >>>> ---
> >> >>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
> >> >>>>
> >> >>>> Residual standard error: 0.1094 on 24 degrees of freedom
> >> >>>>
> >> >>>> Number of iterations to convergence: 8
> >> >>>> Achieved convergence tolerance: 9.374e-06
> >> >>>>
> >> >>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> >> >>>>
> >> >>>> algorithm="port",
> >> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20,
> c=-1),
> >> >>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20,
> c=-1),
> >> >>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
> >> >>>>
> >> >>>> Error in ifelse(internalPars < upper, 1, -1) :
> >> >>>>   (list) object cannot be coerced to type 'double'
> >> >>>>
> >> >>>> ______________________________________________
> >> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>>> PLEASE do read the posting guide
> >> >>>> http://www.R-project.org/posting-guide.html
> >> >>>> and provide commented, minimal, self-contained, reproducible code.
> >> >>>
> >> >>>
> >> >>
> >> >>
> >> >>
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> > http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >> --
> >> Jianling Fan
> >> ???
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> > --
> > Statistics & Software Consulting
> > GKX Group, GKX Associates Inc.
> > tel: 1-877-GKX-GROUP
> > email: ggrothendieck at gmail.com
>
>
>
> --
> Jianling Fan
> ???
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From ggrothendieck at gmail.com  Tue Sep 22 14:45:10 2015
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Sep 2015 08:45:10 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAP01uRkBEfopg4UfMOx7eeVsemW1axSFXML+d=dTmZO62-x0FQ@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>
	<CAJ7mryKogK=Tx=S3Q9ojSHmbZiNAbcN+FvbP+Otez-a5=bRRtA@mail.gmail.com>
	<CAP01uRkBEfopg4UfMOx7eeVsemW1axSFXML+d=dTmZO62-x0FQ@mail.gmail.com>
Message-ID: <CAP01uR=+AuHiBUpBTZN7uxY25Ab5Syoof5UEdw_zAgV87-6brg@mail.gmail.com>

Or if you really can't bear to write out 20 terms have R do it for you:

# number of terms is the number of unique values in ref column
nterms <- length(unique(dproot$ref))

dproot2 <- do.call(data.frame, transform(dproot, ref =
outer(dproot$ref, seq(nterms),
"==") + 0))

# construct the formula as a string
terms <- paste( sprintf("Rm%d*ref.%d", 1:nterms, 1:nterms), collapse = "+")
fo <- sprintf("den ~ (%s)/(1+(depth/d50)^c)", terms)

library(nlmrt)
fm <- nlxb(fo, data = dproot2, masked = "Rm6",
         start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01, Rm6=1,
d50=20, c=-1))


On Tue, Sep 22, 2015 at 7:04 AM, Gabor Grothendieck <ggrothendieck at gmail.com
> wrote:

> Just write out the 20 terms.
>
> On Mon, Sep 21, 2015 at 10:26 PM, Jianling Fan <fanjianling at gmail.com>
> wrote:
>
>> Hello, Gabor,
>>
>> Thanks again for your suggestion. And now I am trying to improve the
>> code by adding a function to replace the express "Rm1 * ref.1 + Rm2 *
>> ref.2 + Rm3 * ref.3 + Rm4 * ref.4 + Rm5 * ref.5 + Rm6 * ref.6" because
>> I have some other dataset need to fitted to the same model but with
>> more groups (>20).
>>
>> I tried to add the function as:
>>
>> denfun<-function(i){
>>                for(i in 1:6){
>>                  Rm<-sum(Rm[i]*ref.i)
>>                  return(Rm)}
>> }
>>
>> but I got another error when I incorporate this function into my
>> regression:
>>
>> >fitdp1<-nlxb(den ~ denfun(6)/(1+(depth/d50)^c),
>>                    data = dproot2,
>>                  start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>> Rm5=1.01, Rm6=1, d50=20, c=-1),
>>                 masked = "Rm6")
>>
>> Error in deriv.default(parse(text = resexp), names(start)) :
>>   Function 'denfun' is not in the derivatives table
>>
>> I think there must be something wrong with my function. I tried some
>> times but am not sure how to improve it because I am quite new to R.
>>
>> Could anyone please give me some suggestion.
>>
>> Thanks a lot!
>>
>>
>> Jianling
>>
>>
>> On 22 September 2015 at 00:43, Gabor Grothendieck
>> <ggrothendieck at gmail.com> wrote:
>> > Express the formula in terms of simple operations like this:
>> >
>> > # add 0/1 columns ref.1, ref.2, ..., ref.6
>> > dproot2 <- do.call(data.frame, transform(dproot, ref = outer(dproot$ref,
>> > seq(6), "==") + 0))
>> >
>> > # now express the formula in terms of the new columns
>> > library(nlmrt)
>> > fitdp1<-nlxb(den ~ (Rm1 * ref.1 + Rm2 * ref.2 + Rm3 * ref.3 + Rm4 *
>> ref.4 +
>> > Rm5 * ref.5 + Rm6 * ref.6)/(1+(depth/d50)^c),
>> >          data = dproot2,
>> >          start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01,
>> Rm6=1,
>> > d50=20, c=-1),
>> >          masked = "Rm6")
>> >
>> > where we used this input:
>> >
>> > Lines <- "   depth       den ref
>> > 1     20 0.5730000   1
>> > 2     40 0.7800000   1
>> > 3     60 0.9470000   1
>> > 4     80 0.9900000   1
>> > 5    100 1.0000000   1
>> > 6     10 0.6000000   2
>> > 7     20 0.8200000   2
>> > 8     30 0.9300000   2
>> > 9     40 1.0000000   2
>> > 10    20 0.4800000   3
>> > 11    40 0.7340000   3
>> > 12    60 0.9610000   3
>> > 13    80 0.9980000   3
>> > 14   100 1.0000000   3
>> > 15    20 3.2083491   4
>> > 16    40 4.9683383   4
>> > 17    60 6.2381133   4
>> > 18    80 6.5322348   4
>> > 19   100 6.5780660   4
>> > 20   120 6.6032064   4
>> > 21    20 0.6140000   5
>> > 22    40 0.8270000   5
>> > 23    60 0.9500000   5
>> > 24    80 0.9950000   5
>> > 25   100 1.0000000   5
>> > 26    20 0.4345774   6
>> > 27    40 0.6654726   6
>> > 28    60 0.8480684   6
>> > 29    80 0.9268951   6
>> > 30   100 0.9723207   6
>> > 31   120 0.9939966   6
>> > 32   140 0.9992400   6"
>> >
>> > dproot <- read.table(text = Lines, header = TRUE)
>> >
>> >
>> >
>> > On Mon, Sep 21, 2015 at 12:22 PM, Jianling Fan <fanjianling at gmail.com>
>> > wrote:
>> >>
>> >> Thanks Prof. Nash,
>> >>
>> >> Sorry for late reply. I am learning and trying to use your nlmrt
>> >> package since I got your email. It works good to mask a parameter in
>> >> regression but seems does work for my equation. I think the problem is
>> >> that the parameter I want to mask is a group-specific parameter and I
>> >> have a "[]" syntax in my equation. However, I don't have your 2014
>> >> book on hand and couldn't find it in our library. So I am wondering if
>> >> nlxb works for group data?
>> >> Thanks a lot!
>> >>
>> >> following is my code and I got a error form it.
>> >>
>> >> > fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >>                 + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>> >> Rm5=1.01, Rm6=1, d50=20, c=-1),
>> >>                 + masked=c("Rm6"))
>> >>
>> >> Error in deriv.default(parse(text = resexp), names(start)) :
>> >>   Function '`[`' is not in the derivatives table
>> >>
>> >>
>> >> Best regards,
>> >>
>> >> Jianling
>> >>
>> >>
>> >> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com>
>> wrote:
>> >> > I posted a suggestion to use nlmrt package (function nlxb to be
>> >> > precise),
>> >> > which has masked (fixed) parameters. Examples in my 2014 book on
>> >> > Nonlinear
>> >> > parameter optimization with R tools. However, I'm travelling just
>> now,
>> >> > or
>> >> > would consider giving this a try.
>> >> >
>> >> > JN
>> >> >
>> >> >
>> >> > On 15-09-20 01:19 PM, Jianling Fan wrote:
>> >> >>
>> >> >> no, I am doing a regression with 6 group data with 2 shared
>> parameters
>> >> >> and 1 different parameter for each group data. the parameter I want
>> to
>> >> >> coerce is for one group. I don't know how to do it. Any suggestion?
>> >> >>
>> >> >> Thanks!
>> >> >>
>> >> >> On 19 September 2015 at 13:33, Jeff Newmiller
>> >> >> <jdnewmil at dcn.davis.ca.us>
>> >> >> wrote:
>> >> >>>
>> >> >>> Why not rewrite the function so that value is not a parameter?
>> >> >>>
>> >> >>>
>> >> >>>
>> ---------------------------------------------------------------------------
>> >> >>> Jeff Newmiller                        The     .....       .....  Go
>> >> >>> Live...
>> >> >>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
>> Live
>> >> >>> Go...
>> >> >>>                                        Live:   OO#.. Dead: OO#..
>> >> >>> Playing
>> >> >>> Research Engineer (Solar/Batteries            O.O#.       #.O#.
>> with
>> >> >>> /Software/Embedded Controllers)               .OO#.       .OO#.
>> >> >>> rocks...1k
>> >> >>>
>> >> >>>
>> >> >>>
>> ---------------------------------------------------------------------------
>> >> >>> Sent from my phone. Please excuse my brevity.
>> >> >>>
>> >> >>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>> >> >>> <fanjianling at gmail.com> wrote:
>> >> >>>>
>> >> >>>> Hello, everyone,
>> >> >>>>
>> >> >>>> I am using a nls regression with 6 groups data. I am trying to
>> coerce
>> >> >>>> a parameter to 1 by using a upper and lower statement. but I
>> always
>> >> >>>> get an error like below:
>> >> >>>>
>> >> >>>> Error in ifelse(internalPars < upper, 1, -1) :
>> >> >>>>   (list) object cannot be coerced to type 'double'
>> >> >>>>
>> >> >>>> does anyone know how to fix it?
>> >> >>>>
>> >> >>>> thanks in advance!
>> >> >>>>
>> >> >>>> My code is below:
>> >> >>>>
>> >> >>>>
>> >> >>>>
>> >> >>>>> dproot
>> >> >>>>
>> >> >>>>    depth       den ref
>> >> >>>> 1     20 0.5730000   1
>> >> >>>> 2     40 0.7800000   1
>> >> >>>> 3     60 0.9470000   1
>> >> >>>> 4     80 0.9900000   1
>> >> >>>> 5    100 1.0000000   1
>> >> >>>> 6     10 0.6000000   2
>> >> >>>> 7     20 0.8200000   2
>> >> >>>> 8     30 0.9300000   2
>> >> >>>> 9     40 1.0000000   2
>> >> >>>> 10    20 0.4800000   3
>> >> >>>> 11    40 0.7340000   3
>> >> >>>> 12    60 0.9610000   3
>> >> >>>> 13    80 0.9980000   3
>> >> >>>> 14   100 1.0000000   3
>> >> >>>> 15    20 3.2083491   4
>> >> >>>> 16    40 4.9683383   4
>> >> >>>> 17    60 6.2381133   4
>> >> >>>> 18    80 6.5322348   4
>> >> >>>> 19   100 6.5780660   4
>> >> >>>> 20   120 6.6032064   4
>> >> >>>> 21    20 0.6140000   5
>> >> >>>> 22    40 0.8270000   5
>> >> >>>> 23    60 0.9500000   5
>> >> >>>> 24    80 0.9950000   5
>> >> >>>> 25   100 1.0000000   5
>> >> >>>> 26    20 0.4345774   6
>> >> >>>> 27    40 0.6654726   6
>> >> >>>> 28    60 0.8480684   6
>> >> >>>> 29    80 0.9268951   6
>> >> >>>> 30   100 0.9723207   6
>> >> >>>> 31   120 0.9939966   6
>> >> >>>> 32   140 0.9992400   6
>> >> >>>>
>> >> >>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >> >>>>
>> >> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>> >> >>>>>
>> >> >>>>> summary(fitdp)
>> >> >>>>
>> >> >>>>
>> >> >>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>> >> >>>>
>> >> >>>> Parameters:
>> >> >>>>     Estimate Std. Error t value Pr(>|t|)
>> >> >>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>> >> >>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>> >> >>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>> >> >>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>> >> >>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>> >> >>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>> >> >>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>> >> >>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>> >> >>>> ---
>> >> >>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>> >> >>>>
>> >> >>>> Residual standard error: 0.1094 on 24 degrees of freedom
>> >> >>>>
>> >> >>>> Number of iterations to convergence: 8
>> >> >>>> Achieved convergence tolerance: 9.374e-06
>> >> >>>>
>> >> >>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >> >>>>
>> >> >>>> algorithm="port",
>> >> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20,
>> c=-1),
>> >> >>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20,
>> c=-1),
>> >> >>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>> >> >>>>
>> >> >>>> Error in ifelse(internalPars < upper, 1, -1) :
>> >> >>>>   (list) object cannot be coerced to type 'double'
>> >> >>>>
>> >> >>>> ______________________________________________
>> >> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> >>>> PLEASE do read the posting guide
>> >> >>>> http://www.R-project.org/posting-guide.html
>> >> >>>> and provide commented, minimal, self-contained, reproducible code.
>> >> >>>
>> >> >>>
>> >> >>
>> >> >>
>> >> >>
>> >> >
>> >> > ______________________________________________
>> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > PLEASE do read the posting guide
>> >> > http://www.R-project.org/posting-guide.html
>> >> > and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >>
>> >>
>> >> --
>> >> Jianling Fan
>> >> ???
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>> >
>> >
>> > --
>> > Statistics & Software Consulting
>> > GKX Group, GKX Associates Inc.
>> > tel: 1-877-GKX-GROUP
>> > email: ggrothendieck at gmail.com
>>
>>
>>
>> --
>> Jianling Fan
>> ???
>>
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From fathi.stat at gmail.com  Tue Sep 22 14:59:07 2015
From: fathi.stat at gmail.com (arsalan fathi)
Date: Tue, 22 Sep 2015 16:29:07 +0330
Subject: [R] (no subject)
Message-ID: <CALfbmxGMDm7ni4AX6btKf4c6Ot=tdHs6WstcLDTBcdtN+pi1TQ@mail.gmail.com>



	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Tue Sep 22 15:38:53 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 22 Sep 2015 05:38:53 -0800
Subject: [R] (no subject)
In-Reply-To: <CALfbmxGMDm7ni4AX6btKf4c6Ot=tdHs6WstcLDTBcdtN+pi1TQ@mail.gmail.com>
Message-ID: <3101A16396F.00000065jrkrideau@inbox.com>


You seem to have sent a blank message.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: fathi.stat at gmail.com
> Sent: Tue, 22 Sep 2015 16:29:07 +0330
> To: r-help at r-project.org
> Subject: [R] (no subject)
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From fanjianling at gmail.com  Tue Sep 22 17:12:13 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Tue, 22 Sep 2015 09:12:13 -0600
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAP01uR=+AuHiBUpBTZN7uxY25Ab5Syoof5UEdw_zAgV87-6brg@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<CAP01uRmWrhmquN++BywiEeyj4nfLm9Yy420wofSzzVE+MfTsgA@mail.gmail.com>
	<CAJ7mryKogK=Tx=S3Q9ojSHmbZiNAbcN+FvbP+Otez-a5=bRRtA@mail.gmail.com>
	<CAP01uRkBEfopg4UfMOx7eeVsemW1axSFXML+d=dTmZO62-x0FQ@mail.gmail.com>
	<CAP01uR=+AuHiBUpBTZN7uxY25Ab5Syoof5UEdw_zAgV87-6brg@mail.gmail.com>
Message-ID: <CAJ7mryLzRDVZbKrOHncqG72aRybZgVmr+tDb=+1LfWhTNhm4UA@mail.gmail.com>

 Hello Gabor,

It is very kind of you to reply and give suggestion so rapid. I will
try to learn and use it.

Thanks very much for your help!

Best regards,

Jianling

On 22 September 2015 at 06:45, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> Or if you really can't bear to write out 20 terms have R do it for you:
>
> # number of terms is the number of unique values in ref column
> nterms <- length(unique(dproot$ref))
>
> dproot2 <- do.call(data.frame, transform(dproot, ref = outer(dproot$ref,
> seq(nterms), "==") + 0))
>
> # construct the formula as a string
> terms <- paste( sprintf("Rm%d*ref.%d", 1:nterms, 1:nterms), collapse = "+")
> fo <- sprintf("den ~ (%s)/(1+(depth/d50)^c)", terms)
>
> library(nlmrt)
> fm <- nlxb(fo, data = dproot2, masked = "Rm6",
>          start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01, Rm6=1,
> d50=20, c=-1))
>
>
> On Tue, Sep 22, 2015 at 7:04 AM, Gabor Grothendieck
> <ggrothendieck at gmail.com> wrote:
>>
>> Just write out the 20 terms.
>>
>> On Mon, Sep 21, 2015 at 10:26 PM, Jianling Fan <fanjianling at gmail.com>
>> wrote:
>>>
>>> Hello, Gabor,
>>>
>>> Thanks again for your suggestion. And now I am trying to improve the
>>> code by adding a function to replace the express "Rm1 * ref.1 + Rm2 *
>>> ref.2 + Rm3 * ref.3 + Rm4 * ref.4 + Rm5 * ref.5 + Rm6 * ref.6" because
>>> I have some other dataset need to fitted to the same model but with
>>> more groups (>20).
>>>
>>> I tried to add the function as:
>>>
>>> denfun<-function(i){
>>>                for(i in 1:6){
>>>                  Rm<-sum(Rm[i]*ref.i)
>>>                  return(Rm)}
>>> }
>>>
>>> but I got another error when I incorporate this function into my
>>> regression:
>>>
>>> >fitdp1<-nlxb(den ~ denfun(6)/(1+(depth/d50)^c),
>>>                    data = dproot2,
>>>                  start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>>> Rm5=1.01, Rm6=1, d50=20, c=-1),
>>>                 masked = "Rm6")
>>>
>>> Error in deriv.default(parse(text = resexp), names(start)) :
>>>   Function 'denfun' is not in the derivatives table
>>>
>>> I think there must be something wrong with my function. I tried some
>>> times but am not sure how to improve it because I am quite new to R.
>>>
>>> Could anyone please give me some suggestion.
>>>
>>> Thanks a lot!
>>>
>>>
>>> Jianling
>>>
>>>
>>> On 22 September 2015 at 00:43, Gabor Grothendieck
>>> <ggrothendieck at gmail.com> wrote:
>>> > Express the formula in terms of simple operations like this:
>>> >
>>> > # add 0/1 columns ref.1, ref.2, ..., ref.6
>>> > dproot2 <- do.call(data.frame, transform(dproot, ref =
>>> > outer(dproot$ref,
>>> > seq(6), "==") + 0))
>>> >
>>> > # now express the formula in terms of the new columns
>>> > library(nlmrt)
>>> > fitdp1<-nlxb(den ~ (Rm1 * ref.1 + Rm2 * ref.2 + Rm3 * ref.3 + Rm4 *
>>> > ref.4 +
>>> > Rm5 * ref.5 + Rm6 * ref.6)/(1+(depth/d50)^c),
>>> >          data = dproot2,
>>> >          start = c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01,
>>> > Rm6=1,
>>> > d50=20, c=-1),
>>> >          masked = "Rm6")
>>> >
>>> > where we used this input:
>>> >
>>> > Lines <- "   depth       den ref
>>> > 1     20 0.5730000   1
>>> > 2     40 0.7800000   1
>>> > 3     60 0.9470000   1
>>> > 4     80 0.9900000   1
>>> > 5    100 1.0000000   1
>>> > 6     10 0.6000000   2
>>> > 7     20 0.8200000   2
>>> > 8     30 0.9300000   2
>>> > 9     40 1.0000000   2
>>> > 10    20 0.4800000   3
>>> > 11    40 0.7340000   3
>>> > 12    60 0.9610000   3
>>> > 13    80 0.9980000   3
>>> > 14   100 1.0000000   3
>>> > 15    20 3.2083491   4
>>> > 16    40 4.9683383   4
>>> > 17    60 6.2381133   4
>>> > 18    80 6.5322348   4
>>> > 19   100 6.5780660   4
>>> > 20   120 6.6032064   4
>>> > 21    20 0.6140000   5
>>> > 22    40 0.8270000   5
>>> > 23    60 0.9500000   5
>>> > 24    80 0.9950000   5
>>> > 25   100 1.0000000   5
>>> > 26    20 0.4345774   6
>>> > 27    40 0.6654726   6
>>> > 28    60 0.8480684   6
>>> > 29    80 0.9268951   6
>>> > 30   100 0.9723207   6
>>> > 31   120 0.9939966   6
>>> > 32   140 0.9992400   6"
>>> >
>>> > dproot <- read.table(text = Lines, header = TRUE)
>>> >
>>> >
>>> >
>>> > On Mon, Sep 21, 2015 at 12:22 PM, Jianling Fan <fanjianling at gmail.com>
>>> > wrote:
>>> >>
>>> >> Thanks Prof. Nash,
>>> >>
>>> >> Sorry for late reply. I am learning and trying to use your nlmrt
>>> >> package since I got your email. It works good to mask a parameter in
>>> >> regression but seems does work for my equation. I think the problem is
>>> >> that the parameter I want to mask is a group-specific parameter and I
>>> >> have a "[]" syntax in my equation. However, I don't have your 2014
>>> >> book on hand and couldn't find it in our library. So I am wondering if
>>> >> nlxb works for group data?
>>> >> Thanks a lot!
>>> >>
>>> >> following is my code and I got a error form it.
>>> >>
>>> >> > fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>> >>                 + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>>> >> Rm5=1.01, Rm6=1, d50=20, c=-1),
>>> >>                 + masked=c("Rm6"))
>>> >>
>>> >> Error in deriv.default(parse(text = resexp), names(start)) :
>>> >>   Function '`[`' is not in the derivatives table
>>> >>
>>> >>
>>> >> Best regards,
>>> >>
>>> >> Jianling
>>> >>
>>> >>
>>> >> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com>
>>> >> wrote:
>>> >> > I posted a suggestion to use nlmrt package (function nlxb to be
>>> >> > precise),
>>> >> > which has masked (fixed) parameters. Examples in my 2014 book on
>>> >> > Nonlinear
>>> >> > parameter optimization with R tools. However, I'm travelling just
>>> >> > now,
>>> >> > or
>>> >> > would consider giving this a try.
>>> >> >
>>> >> > JN
>>> >> >
>>> >> >
>>> >> > On 15-09-20 01:19 PM, Jianling Fan wrote:
>>> >> >>
>>> >> >> no, I am doing a regression with 6 group data with 2 shared
>>> >> >> parameters
>>> >> >> and 1 different parameter for each group data. the parameter I want
>>> >> >> to
>>> >> >> coerce is for one group. I don't know how to do it. Any suggestion?
>>> >> >>
>>> >> >> Thanks!
>>> >> >>
>>> >> >> On 19 September 2015 at 13:33, Jeff Newmiller
>>> >> >> <jdnewmil at dcn.davis.ca.us>
>>> >> >> wrote:
>>> >> >>>
>>> >> >>> Why not rewrite the function so that value is not a parameter?
>>> >> >>>
>>> >> >>>
>>> >> >>>
>>> >> >>> ---------------------------------------------------------------------------
>>> >> >>> Jeff Newmiller                        The     .....       .....
>>> >> >>> Go
>>> >> >>> Live...
>>> >> >>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
>>> >> >>> Live
>>> >> >>> Go...
>>> >> >>>                                        Live:   OO#.. Dead: OO#..
>>> >> >>> Playing
>>> >> >>> Research Engineer (Solar/Batteries            O.O#.       #.O#.
>>> >> >>> with
>>> >> >>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>> >> >>> rocks...1k
>>> >> >>>
>>> >> >>>
>>> >> >>>
>>> >> >>> ---------------------------------------------------------------------------
>>> >> >>> Sent from my phone. Please excuse my brevity.
>>> >> >>>
>>> >> >>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>>> >> >>> <fanjianling at gmail.com> wrote:
>>> >> >>>>
>>> >> >>>> Hello, everyone,
>>> >> >>>>
>>> >> >>>> I am using a nls regression with 6 groups data. I am trying to
>>> >> >>>> coerce
>>> >> >>>> a parameter to 1 by using a upper and lower statement. but I
>>> >> >>>> always
>>> >> >>>> get an error like below:
>>> >> >>>>
>>> >> >>>> Error in ifelse(internalPars < upper, 1, -1) :
>>> >> >>>>   (list) object cannot be coerced to type 'double'
>>> >> >>>>
>>> >> >>>> does anyone know how to fix it?
>>> >> >>>>
>>> >> >>>> thanks in advance!
>>> >> >>>>
>>> >> >>>> My code is below:
>>> >> >>>>
>>> >> >>>>
>>> >> >>>>
>>> >> >>>>> dproot
>>> >> >>>>
>>> >> >>>>    depth       den ref
>>> >> >>>> 1     20 0.5730000   1
>>> >> >>>> 2     40 0.7800000   1
>>> >> >>>> 3     60 0.9470000   1
>>> >> >>>> 4     80 0.9900000   1
>>> >> >>>> 5    100 1.0000000   1
>>> >> >>>> 6     10 0.6000000   2
>>> >> >>>> 7     20 0.8200000   2
>>> >> >>>> 8     30 0.9300000   2
>>> >> >>>> 9     40 1.0000000   2
>>> >> >>>> 10    20 0.4800000   3
>>> >> >>>> 11    40 0.7340000   3
>>> >> >>>> 12    60 0.9610000   3
>>> >> >>>> 13    80 0.9980000   3
>>> >> >>>> 14   100 1.0000000   3
>>> >> >>>> 15    20 3.2083491   4
>>> >> >>>> 16    40 4.9683383   4
>>> >> >>>> 17    60 6.2381133   4
>>> >> >>>> 18    80 6.5322348   4
>>> >> >>>> 19   100 6.5780660   4
>>> >> >>>> 20   120 6.6032064   4
>>> >> >>>> 21    20 0.6140000   5
>>> >> >>>> 22    40 0.8270000   5
>>> >> >>>> 23    60 0.9500000   5
>>> >> >>>> 24    80 0.9950000   5
>>> >> >>>> 25   100 1.0000000   5
>>> >> >>>> 26    20 0.4345774   6
>>> >> >>>> 27    40 0.6654726   6
>>> >> >>>> 28    60 0.8480684   6
>>> >> >>>> 29    80 0.9268951   6
>>> >> >>>> 30   100 0.9723207   6
>>> >> >>>> 31   120 0.9939966   6
>>> >> >>>> 32   140 0.9992400   6
>>> >> >>>>
>>> >> >>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>> >> >>>>
>>> >> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20,
>>> >> >>>> c=-1))
>>> >> >>>>>
>>> >> >>>>> summary(fitdp)
>>> >> >>>>
>>> >> >>>>
>>> >> >>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>>> >> >>>>
>>> >> >>>> Parameters:
>>> >> >>>>     Estimate Std. Error t value Pr(>|t|)
>>> >> >>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>>> >> >>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>>> >> >>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>>> >> >>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>>> >> >>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>>> >> >>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>>> >> >>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>>> >> >>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>>> >> >>>> ---
>>> >> >>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>>> >> >>>>
>>> >> >>>> Residual standard error: 0.1094 on 24 degrees of freedom
>>> >> >>>>
>>> >> >>>> Number of iterations to convergence: 8
>>> >> >>>> Achieved convergence tolerance: 9.374e-06
>>> >> >>>>
>>> >> >>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>> >> >>>>
>>> >> >>>> algorithm="port",
>>> >> >>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20,
>>> >> >>>> c=-1),
>>> >> >>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20,
>>> >> >>>> c=-1),
>>> >> >>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>>> >> >>>>
>>> >> >>>> Error in ifelse(internalPars < upper, 1, -1) :
>>> >> >>>>   (list) object cannot be coerced to type 'double'
>>> >> >>>>
>>> >> >>>> ______________________________________________
>>> >> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> >>>> PLEASE do read the posting guide
>>> >> >>>> http://www.R-project.org/posting-guide.html
>>> >> >>>> and provide commented, minimal, self-contained, reproducible
>>> >> >>>> code.
>>> >> >>>
>>> >> >>>
>>> >> >>
>>> >> >>
>>> >> >>
>>> >> >
>>> >> > ______________________________________________
>>> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> > PLEASE do read the posting guide
>>> >> > http://www.R-project.org/posting-guide.html
>>> >> > and provide commented, minimal, self-contained, reproducible code.
>>> >>
>>> >>
>>> >>
>>> >> --
>>> >> Jianling Fan
>>> >> ???
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide
>>> >> http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.
>>> >
>>> >
>>> >
>>> >
>>> > --
>>> > Statistics & Software Consulting
>>> > GKX Group, GKX Associates Inc.
>>> > tel: 1-877-GKX-GROUP
>>> > email: ggrothendieck at gmail.com
>>>
>>>
>>>
>>> --
>>> Jianling Fan
>>> ???
>>
>>
>>
>>
>> --
>> Statistics & Software Consulting
>> GKX Group, GKX Associates Inc.
>> tel: 1-877-GKX-GROUP
>> email: ggrothendieck at gmail.com
>
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Jianling Fan
???


From alaasindi at gmail.com  Tue Sep 22 17:33:20 2015
From: alaasindi at gmail.com (Alaa Sindi)
Date: Tue, 22 Sep 2015 11:33:20 -0400
Subject: [R] error in mlogit.optim
Message-ID: <2E2512D4-9670-4BDB-B9C9-29BA39153881@gmail.com>

Hi all

I hope you are doing well.

I am trying to install and use mlogit.optim and getting this error. 

Error: could not find function ?mlogit.optim"

Warning in install.packages :
  unable to access index for repository https://cran.rstudio.com/src/contrib <https://cran.rstudio.com/src/contrib>
Warning in install.packages :
  unable to access index for repository https://cran.rstudio.com/src/contrib <https://cran.rstudio.com/src/contrib>
Warning in install.packages :
  package ?mlogit.optim? is not available (for R version 3.2.2)
Warning in install.packages :
  unable to access index for repository https://cran.rstudio.com/bin/macosx/mavericks/contrib/3.2 <https://cran.rstudio.com/bin/macosx/mavericks/contrib/3.2>


Thanks


	[[alternative HTML version deleted]]


From roryrwilson at yahoo.ca  Tue Sep 22 15:18:16 2015
From: roryrwilson at yahoo.ca (Rory Wilson)
Date: Tue, 22 Sep 2015 13:18:16 +0000 (UTC)
Subject: [R] Error from lme4: "Error: (p <- ncol(X)) == ncol(Y) is not TRUE"
In-Reply-To: <mailman.5.1442916002.27623.r-help@r-project.org>
References: <mailman.5.1442916002.27623.r-help@r-project.org>
Message-ID: <396393592.1539223.1442927896027.JavaMail.yahoo@mail.yahoo.com>

Hello all,?I am trying to run a random intercept model using lme4. The random effect is a factor of 29 possibilities, making a model with one random effect (one level). It is just a linear model. There are 713 observations. However, when trying to run the model I receive the error?"Error: (p <- ncol(X)) == ncol(Y) is not TRUE",
a search for which reveals somewhat surprisingly little. Has anyone seen this before? Note that if I simply change the random effect into a fixed effect and use lm, the model works perfectly.Thank you!Rory
  
	[[alternative HTML version deleted]]


From sheasley at aegonusa.com  Tue Sep 22 15:21:12 2015
From: sheasley at aegonusa.com (smheas)
Date: Tue, 22 Sep 2015 06:21:12 -0700 (PDT)
Subject: [R] Need data labels to jitter with datapoints in boxplot
In-Reply-To: <1442503233094-4712380.post@n4.nabble.com>
References: <1442503233094-4712380.post@n4.nabble.com>
Message-ID: <1442928072112-4712605.post@n4.nabble.com>

Thank you both for your responses! I ended up going with PIKAL Petr's
suggestion.



--
View this message in context: http://r.789695.n4.nabble.com/Need-data-labels-to-jitter-with-datapoints-in-boxplot-tp4712380p4712605.html
Sent from the R help mailing list archive at Nabble.com.


From JonyGreen at outlook.com  Tue Sep 22 13:29:08 2015
From: JonyGreen at outlook.com (JonyGreen)
Date: Tue, 22 Sep 2015 04:29:08 -0700 (PDT)
Subject: [R] Millisecond TimeStamps
In-Reply-To: <000b01cbea46$d5baf1c0$8130d540$@com>
References: <000b01cbea46$d5baf1c0$8130d540$@com>
Message-ID: <1442921348331-4712598.post@n4.nabble.com>

you can try this free  online unix timestamp creator
<http://www.online-code.net/unix-timestamp.html>  , you can get the current
time stamp in milliseconds.



--
View this message in context: http://r.789695.n4.nabble.com/Millisecond-TimeStamps-tp3403594p4712598.html
Sent from the R help mailing list archive at Nabble.com.


From JonyGreen at outlook.com  Tue Sep 22 14:01:08 2015
From: JonyGreen at outlook.com (JonyGreen)
Date: Tue, 22 Sep 2015 05:01:08 -0700 (PDT)
Subject: [R] unixtime conversion
In-Reply-To: <87fy3rlivl.fsf@pdrechsler.de>
References: <87fy3rlivl.fsf@pdrechsler.de>
Message-ID: <1442923268674-4712599.post@n4.nabble.com>

you can try this  free online timestamp converter
<http://www.online-code.net/unix-timestamp.html>  to convert timestamp to
readable date.




--
View this message in context: http://r.789695.n4.nabble.com/unixtime-conversion-tp829898p4712599.html
Sent from the R help mailing list archive at Nabble.com.


From niels at ohlsen-web.de  Tue Sep 22 16:05:18 2015
From: niels at ohlsen-web.de (SEMson)
Date: Tue, 22 Sep 2015 07:05:18 -0700 (PDT)
Subject: [R] [FORGED] Re:  Weighted skewness and curtosis
In-Reply-To: <55A838C4.5010807@auckland.ac.nz>
References: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
	<2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>
	<CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>
	<CAGxFJbQHSx+7j3ssErbrkmANyg4Kb33OnHW1OH=U+aWeWqpTiQ@mail.gmail.com>
	<55A838C4.5010807@auckland.ac.nz>
Message-ID: <1442930718777-4712612.post@n4.nabble.com>

I?m also looking for an answer on this question right now. 

You can?t use a weight in the moments package, but i found a
weighted.moments()-function in the acid-package ( weighted.moments-function
<https://github.com/cran/acid/blob/master/R/weighted.moments.R>  ). If your
data has NA, you can do the following:

#-----------------------------------------------------------------
skew <- function(x,weight){
  weight<-weight[!is.na(x)]  #delete weight for cases with NA
  x<-x[!is.na(x)]                 # delete NA
  acid::weighted.moments(x, w8=weight) #calulate moments
}
skew(mydata$var,weight)
#-----------------------------------------------------------------


I also tried to write a weighted-skew-function by myself:
The result is different from the acid-package: i get a skew of 0.7692313.
Perhaps, because x and length(x) aren?t weighted here. The unweighted skew
was 0.58 btw.
#-----------------------------------------------------------------
skew.wtd <- function(x,weight){
  weight<-weight[!is.na(x)]
  x<-x[!is.na(x)]
  sum.w <- sum(weight)
  sum.w2 <- sum(weight^2)
  mean.w <- sum(x * weight) / sum(weight)
  x.sd.w<-sqrt((sum.w / (sum.w^2 - sum.w2)) * sum(weight * (x - mean.w)^2))
  ((sum(((x - mean.w)/ x.sd.w)^3))/(length(x) - 1))
}
skew.wtd(mydata$var,weight)
#-----------------------------------------------------------------


Because the acid-package doesn?t give a weighted kurtosis, i tried the
following:
#-----------------------------------------------------------------
kurt <- function(x,weight){
  weight<-weight[!is.na(x)]
  x<-x[!is.na(x)]
  mean.w <- sum(x * weight) / sum(weight)
  sum.w <- sum(weight)
  sum.w2 <- sum(weight^2)
  x.sd.w<-sqrt((sum.w / (sum.w^2 - sum.w2)) * sum(weight * (x - mean.w)^2))
 #((sum(((x - mean.w)/(sd(x)))^4))/(length(x) - 1))         #formula A
 (((sum(((x - mean(x))/(sd(x)))^4))/(length(x) - 1)) - 3)   #formula B
}
kurt(mydata$var,weight)
# weighted Kurtosis is -0.7127631

#-----------------------------------------------------------------
kurtosis<-function(x,weight) {
  weight<-weight[!is.na(x)]
  x<-x[!is.na(x)]
  mean.w <- sum(x * weight) / sum(weight)
  sum.w <- sum(weight)
  sum.w2 <- sum(weight^2)
  x.sd.w<-sqrt((sum.w / (sum.w^2 - sum.w2)) * sum(weight * (x - mean.w)^2))
  m4<-mean((x - mean.w)^4)                                         #formula
C
  kurt<-m4/(x.sd.w^4)-3 
  kurt}
kurtosis(mydata$var,weight)

# weighted Kurtosis is -0.5076363
# unweighted Kurtosis was -0.72
#-----------------------------------------------------------------




--
View this message in context: http://r.789695.n4.nabble.com/Weighted-skewness-and-curtosis-tp4709956p4712612.html
Sent from the R help mailing list archive at Nabble.com.


From oliver.barrett at skema.edu  Tue Sep 22 16:27:03 2015
From: oliver.barrett at skema.edu (BARRETT, Oliver)
Date: Tue, 22 Sep 2015 14:27:03 +0000
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
Message-ID: <HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>


Dear 'R' community support,


I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.


I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:

lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
    Fed.t.4., data = OLS_CAR, x = TRUE)

Residuals:
      Min        1Q    Median        3Q       Max
-0.154587 -0.015961  0.001429  0.017196  0.110907

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.001630   0.001763  -0.925   0.3559
Fed         -0.121595   0.165359  -0.735   0.4627
Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
Fed.t.2.     0.026529   0.143648   0.185   0.8536
Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.0293 on 304 degrees of freedom
  (20 observations deleted due to missingness)
Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05

I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.

I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.

I would like to thank you in advance,

Kind regards,

Oliver Barrett
(+44) 7341 834 217

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Sep 22 18:01:15 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 22 Sep 2015 09:01:15 -0700
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
Message-ID: <CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>

1. It is highly unlikely that we could be of help (unless someone else
has experienced this and knows what happened). You will have to
contact the Urkund people and ask them why their algorithms raised the
flags.

2. But of course, the regression methodology is not "your own" -- it's
just a standard tool that you used in your work, which is entirely
legitimate of course.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 22, 2015 at 7:27 AM, BARRETT, Oliver
<oliver.barrett at skema.edu> wrote:
>
> Dear 'R' community support,
>
>
> I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.
>
>
> I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:
>
> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>     Fed.t.4., data = OLS_CAR, x = TRUE)
>
> Residuals:
>       Min        1Q    Median        3Q       Max
> -0.154587 -0.015961  0.001429  0.017196  0.110907
>
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept) -0.001630   0.001763  -0.925   0.3559
> Fed         -0.121595   0.165359  -0.735   0.4627
> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
> Fed.t.2.     0.026529   0.143648   0.185   0.8536
> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Residual standard error: 0.0293 on 304 degrees of freedom
>   (20 observations deleted due to missingness)
> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
>
> I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.
>
> I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.
>
> I would like to thank you in advance,
>
> Kind regards,
>
> Oliver Barrett
> (+44) 7341 834 217
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Tue Sep 22 18:24:13 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 22 Sep 2015 11:24:13 -0500
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
	<CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
Message-ID: <4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>

Hi,

With the usual caveat that I Am Not A Lawyer....and that I am not speaking on behalf of any organization...

My guess is that they are claiming that the output of R, simply being copied and pasted verbatim into your thesis constitutes the use of copyrighted output from the software.

It is not clear to me that R's output is copyrighted by the R Foundation (or by other parties for CRAN packages), albeit, the source code underlying R is, along with other copyright owner's as apropos. There is some caselaw to support the notion that the output alone is not protected in a similar manner, but that may be country specific.

Did you provide any credit to R (see the output of citation() ) in your thesis and indicate that your analyses were performed using R?

If R is uncredited, I could see them raising the issue.

You might check with your institution's legal/policy folks to see if there is any guidance provided for students regarding the crediting of software used in this manner, especially if that guidance is at no cost to you.

Regards,

Marc Schwartz


> On Sep 22, 2015, at 11:01 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> 1. It is highly unlikely that we could be of help (unless someone else
> has experienced this and knows what happened). You will have to
> contact the Urkund people and ask them why their algorithms raised the
> flags.
> 
> 2. But of course, the regression methodology is not "your own" -- it's
> just a standard tool that you used in your work, which is entirely
> legitimate of course.
> 
> Cheers,
> Bert
> 
> 
> Bert Gunter
> 
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>   -- Clifford Stoll
> 
> 
> On Tue, Sep 22, 2015 at 7:27 AM, BARRETT, Oliver
> <oliver.barrett at skema.edu> wrote:
>> 
>> Dear 'R' community support,
>> 
>> 
>> I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.
>> 
>> 
>> I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:
>> 
>> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>>    Fed.t.4., data = OLS_CAR, x = TRUE)
>> 
>> Residuals:
>>      Min        1Q    Median        3Q       Max
>> -0.154587 -0.015961  0.001429  0.017196  0.110907
>> 
>> Coefficients:
>>             Estimate Std. Error t value Pr(>|t|)
>> (Intercept) -0.001630   0.001763  -0.925   0.3559
>> Fed         -0.121595   0.165359  -0.735   0.4627
>> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
>> Fed.t.2.     0.026529   0.143648   0.185   0.8536
>> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
>> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>> 
>> Residual standard error: 0.0293 on 304 degrees of freedom
>>  (20 observations deleted due to missingness)
>> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
>> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
>> 
>> I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.
>> 
>> I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.
>> 
>> I would like to thank you in advance,
>> 
>> Kind regards,
>> 
>> Oliver Barrett
>> (+44) 7341 834 217
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fanjianling at gmail.com  Tue Sep 22 18:46:13 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Tue, 22 Sep 2015 10:46:13 -0600
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <56005CC8.5040102@gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<56005CC8.5040102@gmail.com>
Message-ID: <CAJ7mryJq8GfaC3FAG_H4kGB5=_vizsPjSZ4hVsj1EzdsO1iEcQ@mail.gmail.com>

Hello Prof. Nash,

My regression works good now. But I found another problem when I using
nlxb. In the output, the SE, t-stat, and p-value are not available.
Furthermore, I can't extract AIC from the output. The output looks
like below:

Do you have any suggestion for this?

Thanks a lot!

Regards,

nlmrt class object: x
residual sumsquares =  0.29371  on  33 observations
    after  9    Jacobian and  10 function evaluations
  name            coeff          SE       tstat      pval
gradient    JSingval
Rm1               1.1162            NA         NA         NA
-3.059e-13       2.745
Rm2              1.56072            NA         NA         NA
1.417e-13        1.76
Rm3              1.09775            NA         NA         NA
-3.179e-13       1.748
Rm4              7.18377            NA         NA         NA
-2.941e-12       1.748
Rm5              1.13562            NA         NA         NA
-3.305e-13       1.076
Rm6                    1  M         NA         NA         NA
0       0.603
d50              22.4803            NA         NA         NA
4.975e-13       0.117
c               -1.64075            NA         NA         NA
4.12e-12   1.908e-17



On 21 September 2015 at 13:38, ProfJCNash <profjcnash at gmail.com> wrote:
> I've not used it for group data, and suspect that the code to generate
> derivatives cannot cope with the bracket syntax. If you can rewrite the
> equation without the brackets, you could get the derivatives and solve that
> way. This will probably mean having a "translation" routine to glue things
> together.
>
> JN
>
>
> On 15-09-21 12:22 PM, Jianling Fan wrote:
>>
>> Thanks Prof. Nash,
>>
>> Sorry for late reply. I am learning and trying to use your nlmrt
>> package since I got your email. It works good to mask a parameter in
>> regression but seems does work for my equation. I think the problem is
>> that the parameter I want to mask is a group-specific parameter and I
>> have a "[]" syntax in my equation. However, I don't have your 2014
>> book on hand and couldn't find it in our library. So I am wondering if
>> nlxb works for group data?
>> Thanks a lot!
>>
>> following is my code and I got a error form it.
>>
>>> fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>
>>                  + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>> Rm5=1.01, Rm6=1, d50=20, c=-1),
>>                  + masked=c("Rm6"))
>>
>> Error in deriv.default(parse(text = resexp), names(start)) :
>>    Function '`[`' is not in the derivatives table
>>
>>
>> Best regards,
>>
>> Jianling
>>
>>
>> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
>>>
>>> I posted a suggestion to use nlmrt package (function nlxb to be precise),
>>> which has masked (fixed) parameters. Examples in my 2014 book on
>>> Nonlinear
>>> parameter optimization with R tools. However, I'm travelling just now, or
>>> would consider giving this a try.
>>>
>>> JN
>>>
>>>
>>> On 15-09-20 01:19 PM, Jianling Fan wrote:
>>>>
>>>>
>>>> no, I am doing a regression with 6 group data with 2 shared parameters
>>>> and 1 different parameter for each group data. the parameter I want to
>>>> coerce is for one group. I don't know how to do it. Any suggestion?
>>>>
>>>> Thanks!
>>>>
>>>> On 19 September 2015 at 13:33, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>>>> wrote:
>>>>>
>>>>>
>>>>> Why not rewrite the function so that value is not a parameter?
>>>>>
>>>>>
>>>>> ---------------------------------------------------------------------------
>>>>> Jeff Newmiller                        The     .....       .....  Go
>>>>> Live...
>>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>>> Go...
>>>>>                                         Live:   OO#.. Dead: OO#..
>>>>> Playing
>>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>>>> rocks...1k
>>>>>
>>>>>
>>>>> ---------------------------------------------------------------------------
>>>>> Sent from my phone. Please excuse my brevity.
>>>>>
>>>>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>>>>> <fanjianling at gmail.com> wrote:
>>>>>>
>>>>>>
>>>>>> Hello, everyone,
>>>>>>
>>>>>> I am using a nls regression with 6 groups data. I am trying to coerce
>>>>>> a parameter to 1 by using a upper and lower statement. but I always
>>>>>> get an error like below:
>>>>>>
>>>>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>>>>    (list) object cannot be coerced to type 'double'
>>>>>>
>>>>>> does anyone know how to fix it?
>>>>>>
>>>>>> thanks in advance!
>>>>>>
>>>>>> My code is below:
>>>>>>
>>>>>>
>>>>>>
>>>>>>> dproot
>>>>>>
>>>>>>
>>>>>>     depth       den ref
>>>>>> 1     20 0.5730000   1
>>>>>> 2     40 0.7800000   1
>>>>>> 3     60 0.9470000   1
>>>>>> 4     80 0.9900000   1
>>>>>> 5    100 1.0000000   1
>>>>>> 6     10 0.6000000   2
>>>>>> 7     20 0.8200000   2
>>>>>> 8     30 0.9300000   2
>>>>>> 9     40 1.0000000   2
>>>>>> 10    20 0.4800000   3
>>>>>> 11    40 0.7340000   3
>>>>>> 12    60 0.9610000   3
>>>>>> 13    80 0.9980000   3
>>>>>> 14   100 1.0000000   3
>>>>>> 15    20 3.2083491   4
>>>>>> 16    40 4.9683383   4
>>>>>> 17    60 6.2381133   4
>>>>>> 18    80 6.5322348   4
>>>>>> 19   100 6.5780660   4
>>>>>> 20   120 6.6032064   4
>>>>>> 21    20 0.6140000   5
>>>>>> 22    40 0.8270000   5
>>>>>> 23    60 0.9500000   5
>>>>>> 24    80 0.9950000   5
>>>>>> 25   100 1.0000000   5
>>>>>> 26    20 0.4345774   6
>>>>>> 27    40 0.6654726   6
>>>>>> 28    60 0.8480684   6
>>>>>> 29    80 0.9268951   6
>>>>>> 30   100 0.9723207   6
>>>>>> 31   120 0.9939966   6
>>>>>> 32   140 0.9992400   6
>>>>>>
>>>>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>>>>
>>>>>>
>>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>>>>>>>
>>>>>>>
>>>>>>> summary(fitdp)
>>>>>>
>>>>>>
>>>>>>
>>>>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>>>>>>
>>>>>> Parameters:
>>>>>>      Estimate Std. Error t value Pr(>|t|)
>>>>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>>>>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>>>>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>>>>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>>>>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>>>>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>>>>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>>>>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>>>>>> ---
>>>>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>>>>>>
>>>>>> Residual standard error: 0.1094 on 24 degrees of freedom
>>>>>>
>>>>>> Number of iterations to convergence: 8
>>>>>> Achieved convergence tolerance: 9.374e-06
>>>>>>
>>>>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>>>>>>
>>>>>>
>>>>>> algorithm="port",
>>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>>>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
>>>>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>>>>>>
>>>>>> Error in ifelse(internalPars < upper, 1, -1) :
>>>>>>    (list) object cannot be coerced to type 'double'
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>



-- 
Jianling Fan
???


From jrkrideau at inbox.com  Tue Sep 22 18:50:08 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 22 Sep 2015 08:50:08 -0800
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
References: <vi1pr01mb081311ac5a3874e3f1fe121292450@vi1pr01mb0813.eurprd01.prod.exchangelabs.com>
Message-ID: <32AD17379EA.00000356jrkrideau@inbox.com>

This is just guessing but the reason is probably that the regression output (not including the specific numbers, and your variable names) is standard R output as already noted.

It probably appears in many other theses and dissertations , in books on R, and possibly in appendices in published books and papers reporting research findings. 

It, or parts of it, may occur thousands of times on R-help and in R oriented blogs and other documents on the Web.  It quite likely shows up on Stack Overflow.

Here is one  example it took me about 2 minutes to find.
http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf. And here's another http://www.princeton.edu/~otorres/Regression101R.pdf. 

Have a look at Julian Faraway's pdf "Practical Regression and Anova using R" book in the Contributed section of the R home site at pp -23-24. There it is again.

I think you probably should do a bit of on-line searching and a sweep of some of the Manuals and Contributed materials on the R site and point out to the powers that be that it is not plagiarism, it's just standard R reporting.of regression results.


John Kane
Kingston ON Canada


> -----Original Message-----
> From: oliver.barrett at skema.edu
> Sent: Tue, 22 Sep 2015 14:27:03 +0000
> To: r-help at r-project.org
> Subject: [R] 'R' Software Output Plagiarism
> 
> 
> Dear 'R' community support,
> 
> 
> I am a student at Skema business school and I have recently submitted my
> MSc thesis/dissertation. This has been passed on to an external
> plagiarism service provider, Urkund, who have scanned my document and
> returned a plagiarism report to my professor having detected 32%
> plagiarism.
> 
> 
> I have contacted Urkund regarding this issue having committed no such
> plagiarism and they have told me that all the plagiarism detected in my
> document comes from the last 25% which consists only of 'R' regressions
> like the one I have pasted below:
> 
> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>     Fed.t.4., data = OLS_CAR, x = TRUE)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max
> -0.154587 -0.015961  0.001429  0.017196  0.110907
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept) -0.001630   0.001763  -0.925   0.3559
> Fed         -0.121595   0.165359  -0.735   0.4627
> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
> Fed.t.2.     0.026529   0.143648   0.185   0.8536
> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Residual standard error: 0.0293 on 304 degrees of freedom
>   (20 observations deleted due to missingness)
> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
> 
> I have produced all of these regressions myself and pasted them directly
> from the 'R' software package. My regression methodology is entirely my
> own along with the sourcing and preperation of the data used to produce
> these statistics.
> 
> I would be very grateful if you could provide my with some clarity as to
> why this output from 'R' is reading as plagiarism.
> 
> I would like to thank you in advance,
> 
> Kind regards,
> 
> Oliver Barrett
> (+44) 7341 834 217
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From jrkrideau at inbox.com  Tue Sep 22 18:52:32 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 22 Sep 2015 08:52:32 -0800
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
References: <he1pr01mb08098500d1aeacb205ecd48292450@he1pr01mb0809.eurprd01.prod.exchangelabs.com>
	<vi1pr01mb081311ac5a3874e3f1fe121292450@vi1pr01mb0813.eurprd01.prod.exchangelabs.com>
	<cagxfjbqx0vtbe9jm_y-eza=q4bvm5sdeajgsgf-1oyrqtao+ca@mail.gmail.com>
Message-ID: <32B2740B2E2.0000035Ejrkrideau@inbox.com>

Very good point about the referencing. 

I wonder if this is happening to users of Stata or SAS as well?

John Kane
Kingston ON Canada


> -----Original Message-----
> From: marc_schwartz at me.com
> Sent: Tue, 22 Sep 2015 11:24:13 -0500
> To: bgunter.4567 at gmail.com
> Subject: Re: [R] 'R' Software Output Plagiarism
> 
> Hi,
> 
> With the usual caveat that I Am Not A Lawyer....and that I am not
> speaking on behalf of any organization...
> 
> My guess is that they are claiming that the output of R, simply being
> copied and pasted verbatim into your thesis constitutes the use of
> copyrighted output from the software.
> 
> It is not clear to me that R's output is copyrighted by the R Foundation
> (or by other parties for CRAN packages), albeit, the source code
> underlying R is, along with other copyright owner's as apropos. There is
> some caselaw to support the notion that the output alone is not protected
> in a similar manner, but that may be country specific.
> 
> Did you provide any credit to R (see the output of citation() ) in your
> thesis and indicate that your analyses were performed using R?
> 
> If R is uncredited, I could see them raising the issue.
> 
> You might check with your institution's legal/policy folks to see if
> there is any guidance provided for students regarding the crediting of
> software used in this manner, especially if that guidance is at no cost
> to you.
> 
> Regards,
> 
> Marc Schwartz
> 
> 
>> On Sep 22, 2015, at 11:01 AM, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>> 
>> 1. It is highly unlikely that we could be of help (unless someone else
>> has experienced this and knows what happened). You will have to
>> contact the Urkund people and ask them why their algorithms raised the
>> flags.
>> 
>> 2. But of course, the regression methodology is not "your own" -- it's
>> just a standard tool that you used in your work, which is entirely
>> legitimate of course.
>> 
>> Cheers,
>> Bert
>> 
>> 
>> Bert Gunter
>> 
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>   -- Clifford Stoll
>> 
>> 
>> On Tue, Sep 22, 2015 at 7:27 AM, BARRETT, Oliver
>> <oliver.barrett at skema.edu> wrote:
>>> 
>>> Dear 'R' community support,
>>> 
>>> 
>>> I am a student at Skema business school and I have recently submitted
>>> my MSc thesis/dissertation. This has been passed on to an external
>>> plagiarism service provider, Urkund, who have scanned my document and
>>> returned a plagiarism report to my professor having detected 32%
>>> plagiarism.
>>> 
>>> 
>>> I have contacted Urkund regarding this issue having committed no such
>>> plagiarism and they have told me that all the plagiarism detected in my
>>> document comes from the last 25% which consists only of 'R' regressions
>>> like the one I have pasted below:
>>> 
>>> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>>>    Fed.t.4., data = OLS_CAR, x = TRUE)
>>> 
>>> Residuals:
>>>      Min        1Q    Median        3Q       Max
>>> -0.154587 -0.015961  0.001429  0.017196  0.110907
>>> 
>>> Coefficients:
>>>             Estimate Std. Error t value Pr(>|t|)
>>> (Intercept) -0.001630   0.001763  -0.925   0.3559
>>> Fed         -0.121595   0.165359  -0.735   0.4627
>>> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
>>> Fed.t.2.     0.026529   0.143648   0.185   0.8536
>>> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
>>> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
>>> ---
>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>> 
>>> Residual standard error: 0.0293 on 304 degrees of freedom
>>>  (20 observations deleted due to missingness)
>>> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
>>> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
>>> 
>>> I have produced all of these regressions myself and pasted them
>>> directly from the 'R' software package. My regression methodology is
>>> entirely my own along with the sourcing and preperation of the data
>>> used to produce these statistics.
>>> 
>>> I would be very grateful if you could provide my with some clarity as
>>> to why this output from 'R' is reading as plagiarism.
>>> 
>>> I would like to thank you in advance,
>>> 
>>> Kind regards,
>>> 
>>> Oliver Barrett
>>> (+44) 7341 834 217
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From jvadams at usgs.gov  Tue Sep 22 19:46:22 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Tue, 22 Sep 2015 12:46:22 -0500
Subject: [R] Error from lme4: "Error: (p <- ncol(X)) == ncol(Y) is not
	TRUE"
In-Reply-To: <396393592.1539223.1442927896027.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.5.1442916002.27623.r-help@r-project.org>
	<396393592.1539223.1442927896027.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAN5YmCGGsrWGKJ1TNKgByLY3bxd-=T6eeFjCXMxytF2BrhONdg@mail.gmail.com>

Rory,

When I searched online, I found an issue with lme4 on GitHub that suggests
this error is "due to NA values in non-factor variables".
https://github.com/lme4/lme4/issues/246

Hope this helps.

Jean

On Tue, Sep 22, 2015 at 8:18 AM, Rory Wilson <roryrwilson at yahoo.ca> wrote:

> Hello all, I am trying to run a random intercept model using lme4. The
> random effect is a factor of 29 possibilities, making a model with one
> random effect (one level). It is just a linear model. There are 713
> observations. However, when trying to run the model I receive the
> error "Error: (p <- ncol(X)) == ncol(Y) is not TRUE",
> a search for which reveals somewhat surprisingly little. Has anyone seen
> this before? Note that if I simply change the random effect into a fixed
> effect and use lm, the model works perfectly.Thank you!Rory
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From therneau at mayo.edu  Tue Sep 22 20:09:33 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Tue, 22 Sep 2015 13:09:33 -0500
Subject: [R] Accounting for correlated random effects in coxme
In-Reply-To: <mailman.5.1442916002.27623.r-help@r-project.org>
References: <mailman.5.1442916002.27623.r-help@r-project.org>
Message-ID: <c10f8b$1g6l15@ironport10.mayo.edu>

I've been away for a couple weeks and am now catching up on email.

The issue is that the coxme code does not have conversions built-in for all of the 
possible types of sparse matrix.  Since it assumes that the variance matrix must be 
symmetric, the non-neccarily-symmetric dgCMatrix class is not one that I had considered. 
You should transform it to dsCMatrix first, which is a symmetric
class.  Or if it is small enough, to a simple matrix.

Terry T.


On 09/22/2015 05:00 AM, r-help-request at r-project.org wrote:
> I have a problem with running the mixed effects Cox regression model using
> a distance matrix from a phylogeny rather than a pedigree. I searched
> previous posts and didn't find any directly relevant previous posts.
>
> I am interested in using a mixed effects Cox regression model to determine
> the best predictors of time to recruitment in 80 different reintroduced
> plant populations representing a total of 31 species. I will like to
> account for correlated random effects that result from phylogenetic
> relationships amongst species. Dr. Therneau's 2015 article on Mixed Effects
> Cox Models provide a very helpful template for me to do this with the coxme
> function in R. In this article, the correlation structure due to genetic
> relationships amongst individuals was defined using a kinship matrix
> derived from a pedigree. Instead of a pedigree, I have a phylogeny for
> these 31 species. Hence, I used the inverseA function in the MCMCglmm
> package to generate an inverse additive genetic relatedness matrix from the
> phylogeny for these 31 species. And then fed it in as input to the varlist
> argument in my mixed effects cox regression model (using function coxme). I
> got an error message (please see below). Based on the error, one thought I
> had was to convert the inverseA matrix from a ?dgCMatrix? to ?bdsmatrix?
> but this was not successful either. I have also unsuccessfully tried to use
> a pairwise phylogenetic distance matrix.
>
> Is there a better way to do this? I basically just want to account for the
> correlated random effects due to phylogenetic relatedness amongst the 31
> species represented in the dataset for the Cox regression model.  Please
> see my code below and I welcome suggestions on how best to make this work.


From ggrothendieck at gmail.com  Tue Sep 22 20:07:43 2015
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Sep 2015 14:07:43 -0400
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAJ7mryJq8GfaC3FAG_H4kGB5=_vizsPjSZ4hVsj1EzdsO1iEcQ@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<56005CC8.5040102@gmail.com>
	<CAJ7mryJq8GfaC3FAG_H4kGB5=_vizsPjSZ4hVsj1EzdsO1iEcQ@mail.gmail.com>
Message-ID: <CAP01uRkV1ZmdhOjHhntYiNGWiftSBA28fQEht4hTj2Np32OP3Q@mail.gmail.com>

You may have to do without masking and switch back to nls.  dproot2 and fo
are from prior post.

# to mask Rm6 omit it from start and set it explicitly
st <- c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01, d50=20, c=-1)
Rm6 <- 1

fm.nls <- nls(fo, dproot2, start = st)

AIC(fm.nls)
summary(fm.nls)


On Tue, Sep 22, 2015 at 12:46 PM, Jianling Fan <fanjianling at gmail.com>
wrote:

> Hello Prof. Nash,
>
> My regression works good now. But I found another problem when I using
> nlxb. In the output, the SE, t-stat, and p-value are not available.
> Furthermore, I can't extract AIC from the output. The output looks
> like below:
>
> Do you have any suggestion for this?
>
> Thanks a lot!
>
> Regards,
>
> nlmrt class object: x
> residual sumsquares =  0.29371  on  33 observations
>     after  9    Jacobian and  10 function evaluations
>   name            coeff          SE       tstat      pval
> gradient    JSingval
> Rm1               1.1162            NA         NA         NA
> -3.059e-13       2.745
> Rm2              1.56072            NA         NA         NA
> 1.417e-13        1.76
> Rm3              1.09775            NA         NA         NA
> -3.179e-13       1.748
> Rm4              7.18377            NA         NA         NA
> -2.941e-12       1.748
> Rm5              1.13562            NA         NA         NA
> -3.305e-13       1.076
> Rm6                    1  M         NA         NA         NA
> 0       0.603
> d50              22.4803            NA         NA         NA
> 4.975e-13       0.117
> c               -1.64075            NA         NA         NA
> 4.12e-12   1.908e-17
>
>
>
> On 21 September 2015 at 13:38, ProfJCNash <profjcnash at gmail.com> wrote:
> > I've not used it for group data, and suspect that the code to generate
> > derivatives cannot cope with the bracket syntax. If you can rewrite the
> > equation without the brackets, you could get the derivatives and solve
> that
> > way. This will probably mean having a "translation" routine to glue
> things
> > together.
> >
> > JN
> >
> >
> > On 15-09-21 12:22 PM, Jianling Fan wrote:
> >>
> >> Thanks Prof. Nash,
> >>
> >> Sorry for late reply. I am learning and trying to use your nlmrt
> >> package since I got your email. It works good to mask a parameter in
> >> regression but seems does work for my equation. I think the problem is
> >> that the parameter I want to mask is a group-specific parameter and I
> >> have a "[]" syntax in my equation. However, I don't have your 2014
> >> book on hand and couldn't find it in our library. So I am wondering if
> >> nlxb works for group data?
> >> Thanks a lot!
> >>
> >> following is my code and I got a error form it.
> >>
> >>> fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> >>
> >>                  + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
> >> Rm5=1.01, Rm6=1, d50=20, c=-1),
> >>                  + masked=c("Rm6"))
> >>
> >> Error in deriv.default(parse(text = resexp), names(start)) :
> >>    Function '`[`' is not in the derivatives table
> >>
> >>
> >> Best regards,
> >>
> >> Jianling
> >>
> >>
> >> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
> >>>
> >>> I posted a suggestion to use nlmrt package (function nlxb to be
> precise),
> >>> which has masked (fixed) parameters. Examples in my 2014 book on
> >>> Nonlinear
> >>> parameter optimization with R tools. However, I'm travelling just now,
> or
> >>> would consider giving this a try.
> >>>
> >>> JN
> >>>
> >>>
> >>> On 15-09-20 01:19 PM, Jianling Fan wrote:
> >>>>
> >>>>
> >>>> no, I am doing a regression with 6 group data with 2 shared parameters
> >>>> and 1 different parameter for each group data. the parameter I want to
> >>>> coerce is for one group. I don't know how to do it. Any suggestion?
> >>>>
> >>>> Thanks!
> >>>>
> >>>> On 19 September 2015 at 13:33, Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us>
> >>>> wrote:
> >>>>>
> >>>>>
> >>>>> Why not rewrite the function so that value is not a parameter?
> >>>>>
> >>>>>
> >>>>>
> ---------------------------------------------------------------------------
> >>>>> Jeff Newmiller                        The     .....       .....  Go
> >>>>> Live...
> >>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
> Live
> >>>>> Go...
> >>>>>                                         Live:   OO#.. Dead: OO#..
> >>>>> Playing
> >>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
> >>>>> rocks...1k
> >>>>>
> >>>>>
> >>>>>
> ---------------------------------------------------------------------------
> >>>>> Sent from my phone. Please excuse my brevity.
> >>>>>
> >>>>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
> >>>>> <fanjianling at gmail.com> wrote:
> >>>>>>
> >>>>>>
> >>>>>> Hello, everyone,
> >>>>>>
> >>>>>> I am using a nls regression with 6 groups data. I am trying to
> coerce
> >>>>>> a parameter to 1 by using a upper and lower statement. but I always
> >>>>>> get an error like below:
> >>>>>>
> >>>>>> Error in ifelse(internalPars < upper, 1, -1) :
> >>>>>>    (list) object cannot be coerced to type 'double'
> >>>>>>
> >>>>>> does anyone know how to fix it?
> >>>>>>
> >>>>>> thanks in advance!
> >>>>>>
> >>>>>> My code is below:
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>> dproot
> >>>>>>
> >>>>>>
> >>>>>>     depth       den ref
> >>>>>> 1     20 0.5730000   1
> >>>>>> 2     40 0.7800000   1
> >>>>>> 3     60 0.9470000   1
> >>>>>> 4     80 0.9900000   1
> >>>>>> 5    100 1.0000000   1
> >>>>>> 6     10 0.6000000   2
> >>>>>> 7     20 0.8200000   2
> >>>>>> 8     30 0.9300000   2
> >>>>>> 9     40 1.0000000   2
> >>>>>> 10    20 0.4800000   3
> >>>>>> 11    40 0.7340000   3
> >>>>>> 12    60 0.9610000   3
> >>>>>> 13    80 0.9980000   3
> >>>>>> 14   100 1.0000000   3
> >>>>>> 15    20 3.2083491   4
> >>>>>> 16    40 4.9683383   4
> >>>>>> 17    60 6.2381133   4
> >>>>>> 18    80 6.5322348   4
> >>>>>> 19   100 6.5780660   4
> >>>>>> 20   120 6.6032064   4
> >>>>>> 21    20 0.6140000   5
> >>>>>> 22    40 0.8270000   5
> >>>>>> 23    60 0.9500000   5
> >>>>>> 24    80 0.9950000   5
> >>>>>> 25   100 1.0000000   5
> >>>>>> 26    20 0.4345774   6
> >>>>>> 27    40 0.6654726   6
> >>>>>> 28    60 0.8480684   6
> >>>>>> 29    80 0.9268951   6
> >>>>>> 30   100 0.9723207   6
> >>>>>> 31   120 0.9939966   6
> >>>>>> 32   140 0.9992400   6
> >>>>>>
> >>>>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> >>>>>>
> >>>>>>
> >>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
> >>>>>>>
> >>>>>>>
> >>>>>>> summary(fitdp)
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
> >>>>>>
> >>>>>> Parameters:
> >>>>>>      Estimate Std. Error t value Pr(>|t|)
> >>>>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
> >>>>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
> >>>>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
> >>>>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
> >>>>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
> >>>>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
> >>>>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
> >>>>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
> >>>>>>
> >>>>>> Residual standard error: 0.1094 on 24 degrees of freedom
> >>>>>>
> >>>>>> Number of iterations to convergence: 8
> >>>>>> Achieved convergence tolerance: 9.374e-06
> >>>>>>
> >>>>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
> >>>>>>
> >>>>>>
> >>>>>> algorithm="port",
> >>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
> >>>>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20, c=-1),
> >>>>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
> >>>>>>
> >>>>>> Error in ifelse(internalPars < upper, 1, -1) :
> >>>>>>    (list) object cannot be coerced to type 'double'
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>>
> >>>>>
> >>>>
> >>>>
> >>>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >>
> >
>
>
>
> --
> Jianling Fan
> ???
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From fanjianling at gmail.com  Tue Sep 22 20:55:24 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Tue, 22 Sep 2015 12:55:24 -0600
Subject: [R] How to coerce a parameter in nls?
In-Reply-To: <CAP01uRkV1ZmdhOjHhntYiNGWiftSBA28fQEht4hTj2Np32OP3Q@mail.gmail.com>
References: <CAJ7mryJdAjhc_v7FmVDwe4EAtqUq53dWNptCtJhCE8RFeV4VmQ@mail.gmail.com>
	<7229E3D3-9070-4C3B-89EB-F3CF4FEB56AF@dcn.davis.CA.us>
	<CAJ7mryL9ZCYBzSF4k6snfxRFwoTVf6iJ-Gx1kopP9cHOKSbQrw@mail.gmail.com>
	<55FF016C.7000900@gmail.com>
	<CAJ7mry+V_7TP3sAZN3u8KbbS3FeRPH7Z5XP9bW-AtjYMw=+QbQ@mail.gmail.com>
	<56005CC8.5040102@gmail.com>
	<CAJ7mryJq8GfaC3FAG_H4kGB5=_vizsPjSZ4hVsj1EzdsO1iEcQ@mail.gmail.com>
	<CAP01uRkV1ZmdhOjHhntYiNGWiftSBA28fQEht4hTj2Np32OP3Q@mail.gmail.com>
Message-ID: <CAJ7mryLX-sZc4Wjde9-kPaMcRG=OWLn2bGOwU2fF5b7=hyWTAg@mail.gmail.com>

great,   Thanks a lot!

On 22 September 2015 at 12:07, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> You may have to do without masking and switch back to nls.  dproot2 and fo
> are from prior post.
>
> # to mask Rm6 omit it from start and set it explicitly
> st <- c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65, Rm5=1.01, d50=20, c=-1)
> Rm6 <- 1
>
> fm.nls <- nls(fo, dproot2, start = st)
>
> AIC(fm.nls)
> summary(fm.nls)
>
>
> On Tue, Sep 22, 2015 at 12:46 PM, Jianling Fan <fanjianling at gmail.com>
> wrote:
>>
>> Hello Prof. Nash,
>>
>> My regression works good now. But I found another problem when I using
>> nlxb. In the output, the SE, t-stat, and p-value are not available.
>> Furthermore, I can't extract AIC from the output. The output looks
>> like below:
>>
>> Do you have any suggestion for this?
>>
>> Thanks a lot!
>>
>> Regards,
>>
>> nlmrt class object: x
>> residual sumsquares =  0.29371  on  33 observations
>>     after  9    Jacobian and  10 function evaluations
>>   name            coeff          SE       tstat      pval
>> gradient    JSingval
>> Rm1               1.1162            NA         NA         NA
>> -3.059e-13       2.745
>> Rm2              1.56072            NA         NA         NA
>> 1.417e-13        1.76
>> Rm3              1.09775            NA         NA         NA
>> -3.179e-13       1.748
>> Rm4              7.18377            NA         NA         NA
>> -2.941e-12       1.748
>> Rm5              1.13562            NA         NA         NA
>> -3.305e-13       1.076
>> Rm6                    1  M         NA         NA         NA
>> 0       0.603
>> d50              22.4803            NA         NA         NA
>> 4.975e-13       0.117
>> c               -1.64075            NA         NA         NA
>> 4.12e-12   1.908e-17
>>
>>
>>
>> On 21 September 2015 at 13:38, ProfJCNash <profjcnash at gmail.com> wrote:
>> > I've not used it for group data, and suspect that the code to generate
>> > derivatives cannot cope with the bracket syntax. If you can rewrite the
>> > equation without the brackets, you could get the derivatives and solve
>> > that
>> > way. This will probably mean having a "translation" routine to glue
>> > things
>> > together.
>> >
>> > JN
>> >
>> >
>> > On 15-09-21 12:22 PM, Jianling Fan wrote:
>> >>
>> >> Thanks Prof. Nash,
>> >>
>> >> Sorry for late reply. I am learning and trying to use your nlmrt
>> >> package since I got your email. It works good to mask a parameter in
>> >> regression but seems does work for my equation. I think the problem is
>> >> that the parameter I want to mask is a group-specific parameter and I
>> >> have a "[]" syntax in my equation. However, I don't have your 2014
>> >> book on hand and couldn't find it in our library. So I am wondering if
>> >> nlxb works for group data?
>> >> Thanks a lot!
>> >>
>> >> following is my code and I got a error form it.
>> >>
>> >>> fitdp1<-nlxb(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >>
>> >>                  + start =c(Rm1=1.01, Rm2=1.01, Rm3=1.01, Rm4=6.65,
>> >> Rm5=1.01, Rm6=1, d50=20, c=-1),
>> >>                  + masked=c("Rm6"))
>> >>
>> >> Error in deriv.default(parse(text = resexp), names(start)) :
>> >>    Function '`[`' is not in the derivatives table
>> >>
>> >>
>> >> Best regards,
>> >>
>> >> Jianling
>> >>
>> >>
>> >> On 20 September 2015 at 12:56, ProfJCNash <profjcnash at gmail.com> wrote:
>> >>>
>> >>> I posted a suggestion to use nlmrt package (function nlxb to be
>> >>> precise),
>> >>> which has masked (fixed) parameters. Examples in my 2014 book on
>> >>> Nonlinear
>> >>> parameter optimization with R tools. However, I'm travelling just now,
>> >>> or
>> >>> would consider giving this a try.
>> >>>
>> >>> JN
>> >>>
>> >>>
>> >>> On 15-09-20 01:19 PM, Jianling Fan wrote:
>> >>>>
>> >>>>
>> >>>> no, I am doing a regression with 6 group data with 2 shared
>> >>>> parameters
>> >>>> and 1 different parameter for each group data. the parameter I want
>> >>>> to
>> >>>> coerce is for one group. I don't know how to do it. Any suggestion?
>> >>>>
>> >>>> Thanks!
>> >>>>
>> >>>> On 19 September 2015 at 13:33, Jeff Newmiller
>> >>>> <jdnewmil at dcn.davis.ca.us>
>> >>>> wrote:
>> >>>>>
>> >>>>>
>> >>>>> Why not rewrite the function so that value is not a parameter?
>> >>>>>
>> >>>>>
>> >>>>>
>> >>>>> ---------------------------------------------------------------------------
>> >>>>> Jeff Newmiller                        The     .....       .....  Go
>> >>>>> Live...
>> >>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
>> >>>>> Live
>> >>>>> Go...
>> >>>>>                                         Live:   OO#.. Dead: OO#..
>> >>>>> Playing
>> >>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.
>> >>>>> with
>> >>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>> >>>>> rocks...1k
>> >>>>>
>> >>>>>
>> >>>>>
>> >>>>> ---------------------------------------------------------------------------
>> >>>>> Sent from my phone. Please excuse my brevity.
>> >>>>>
>> >>>>> On September 18, 2015 9:54:54 PM PDT, Jianling Fan
>> >>>>> <fanjianling at gmail.com> wrote:
>> >>>>>>
>> >>>>>>
>> >>>>>> Hello, everyone,
>> >>>>>>
>> >>>>>> I am using a nls regression with 6 groups data. I am trying to
>> >>>>>> coerce
>> >>>>>> a parameter to 1 by using a upper and lower statement. but I always
>> >>>>>> get an error like below:
>> >>>>>>
>> >>>>>> Error in ifelse(internalPars < upper, 1, -1) :
>> >>>>>>    (list) object cannot be coerced to type 'double'
>> >>>>>>
>> >>>>>> does anyone know how to fix it?
>> >>>>>>
>> >>>>>> thanks in advance!
>> >>>>>>
>> >>>>>> My code is below:
>> >>>>>>
>> >>>>>>
>> >>>>>>
>> >>>>>>> dproot
>> >>>>>>
>> >>>>>>
>> >>>>>>     depth       den ref
>> >>>>>> 1     20 0.5730000   1
>> >>>>>> 2     40 0.7800000   1
>> >>>>>> 3     60 0.9470000   1
>> >>>>>> 4     80 0.9900000   1
>> >>>>>> 5    100 1.0000000   1
>> >>>>>> 6     10 0.6000000   2
>> >>>>>> 7     20 0.8200000   2
>> >>>>>> 8     30 0.9300000   2
>> >>>>>> 9     40 1.0000000   2
>> >>>>>> 10    20 0.4800000   3
>> >>>>>> 11    40 0.7340000   3
>> >>>>>> 12    60 0.9610000   3
>> >>>>>> 13    80 0.9980000   3
>> >>>>>> 14   100 1.0000000   3
>> >>>>>> 15    20 3.2083491   4
>> >>>>>> 16    40 4.9683383   4
>> >>>>>> 17    60 6.2381133   4
>> >>>>>> 18    80 6.5322348   4
>> >>>>>> 19   100 6.5780660   4
>> >>>>>> 20   120 6.6032064   4
>> >>>>>> 21    20 0.6140000   5
>> >>>>>> 22    40 0.8270000   5
>> >>>>>> 23    60 0.9500000   5
>> >>>>>> 24    80 0.9950000   5
>> >>>>>> 25   100 1.0000000   5
>> >>>>>> 26    20 0.4345774   6
>> >>>>>> 27    40 0.6654726   6
>> >>>>>> 28    60 0.8480684   6
>> >>>>>> 29    80 0.9268951   6
>> >>>>>> 30   100 0.9723207   6
>> >>>>>> 31   120 0.9939966   6
>> >>>>>> 32   140 0.9992400   6
>> >>>>>>
>> >>>>>>> fitdp<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >>>>>>
>> >>>>>>
>> >>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65,1.01,1), d50=20, c=-1))
>> >>>>>>>
>> >>>>>>>
>> >>>>>>> summary(fitdp)
>> >>>>>>
>> >>>>>>
>> >>>>>>
>> >>>>>> Formula: den ~ Rm[ref]/(1 + (depth/d50)^c)
>> >>>>>>
>> >>>>>> Parameters:
>> >>>>>>      Estimate Std. Error t value Pr(>|t|)
>> >>>>>> Rm1  1.12560    0.07156   15.73 3.84e-14 ***
>> >>>>>> Rm2  1.57643    0.11722   13.45 1.14e-12 ***
>> >>>>>> Rm3  1.10697    0.07130   15.53 5.11e-14 ***
>> >>>>>> Rm4  7.23925    0.20788   34.83  < 2e-16 ***
>> >>>>>> Rm5  1.14516    0.07184   15.94 2.87e-14 ***
>> >>>>>> Rm6  1.03658    0.05664   18.30 1.33e-15 ***
>> >>>>>> d50 22.69426    1.03855   21.85  < 2e-16 ***
>> >>>>>> c   -1.59796    0.15589  -10.25 3.02e-10 ***
>> >>>>>> ---
>> >>>>>> Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1
>> >>>>>>
>> >>>>>> Residual standard error: 0.1094 on 24 degrees of freedom
>> >>>>>>
>> >>>>>> Number of iterations to convergence: 8
>> >>>>>> Achieved convergence tolerance: 9.374e-06
>> >>>>>>
>> >>>>>>> fitdp1<-nls(den~Rm[ref]/(1+(depth/d50)^c),data=dproot,
>> >>>>>>
>> >>>>>>
>> >>>>>> algorithm="port",
>> >>>>>> + start = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20,
>> >>>>>> c=-1),
>> >>>>>> + lower = list(Rm=c(1.01, 1.01, 1.01, 6.65, 1.01, 1), d50=20,
>> >>>>>> c=-1),
>> >>>>>> + upper = list(Rm=c(2.1, 2.2, 2.12, 12.5, 2.3, 1), d50=50, c=1))
>> >>>>>>
>> >>>>>> Error in ifelse(internalPars < upper, 1, -1) :
>> >>>>>>    (list) object cannot be coerced to type 'double'
>> >>>>>>
>> >>>>>> ______________________________________________
>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>> PLEASE do read the posting guide
>> >>>>>> http://www.R-project.org/posting-guide.html
>> >>>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>>>
>> >>>>>
>> >>>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>
>> >>> ______________________________________________
>> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> PLEASE do read the posting guide
>> >>> http://www.R-project.org/posting-guide.html
>> >>> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >>
>> >>
>> >>
>> >
>>
>>
>>
>> --
>> Jianling Fan
>> ???
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Jianling Fan
???


From alaasindi at gmail.com  Tue Sep 22 21:07:22 2015
From: alaasindi at gmail.com (Alaa Sindi)
Date: Tue, 22 Sep 2015 15:07:22 -0400
Subject: [R] Error in eval(expr, envir, enclos) : could not find function
Message-ID: <4FBBE564-6BDD-46FD-8292-96AE6ECE5796@gmail.com>

hi all

I am getting this error "Error in eval(expr, envir, enclos) : could not find function ?

do you have an idea what might cause this problem. 

thanks

From jholtman at gmail.com  Tue Sep 22 21:22:40 2015
From: jholtman at gmail.com (jim holtman)
Date: Tue, 22 Sep 2015 15:22:40 -0400
Subject: [R] unixtime conversion
In-Reply-To: <1442923268674-4712599.post@n4.nabble.com>
References: <87fy3rlivl.fsf@pdrechsler.de>
	<1442923268674-4712599.post@n4.nabble.com>
Message-ID: <CAAxdm-5fCG9HvZ8Q0qSsHyEXXxSq_519baFNbNqQUZkXGVd0LA@mail.gmail.com>

you can also do:

> structure(1183377301, class = c("POSIXct", "POSIXt"))
[1] "2007-07-02 07:55:01 EDT"
>




Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Tue, Sep 22, 2015 at 8:01 AM, JonyGreen <JonyGreen at outlook.com> wrote:

> you can try this  free online timestamp converter
> <http://www.online-code.net/unix-timestamp.html>  to convert timestamp to
> readable date.
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/unixtime-conversion-tp829898p4712599.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From msharp at txbiomed.org  Tue Sep 22 21:41:13 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Tue, 22 Sep 2015 19:41:13 +0000
Subject: [R] Error in eval(expr, envir, enclos) : could not find function
In-Reply-To: <4FBBE564-6BDD-46FD-8292-96AE6ECE5796@gmail.com>
References: <4FBBE564-6BDD-46FD-8292-96AE6ECE5796@gmail.com>
Message-ID: <26CB81C5-7876-4146-900A-6EFEB185B713@txbiomed.org>

Please provide a context for your question. See the posting guide referenced below for instructions on providing commented, minimal, self-contained, reproducible code. If you can show how to produce the error, someone can almost certainly show you how to avoid it.

Mark
R. Mark Sharp, Ph.D.
msharp at TxBiomed.org





> On Sep 22, 2015, at 2:07 PM, Alaa Sindi <alaasindi at gmail.com> wrote:
> 
> hi all
> 
> I am getting this error "Error in eval(expr, envir, enclos) : could not find function ?
> 
> do you have an idea what might cause this problem. 
> 
> thanks
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Tue Sep 22 22:06:43 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 22 Sep 2015 22:06:43 +0200
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
	<CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
	<4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
Message-ID: <5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>

Marc,

I don't think Copyright/Intellectual property issues factor into this. Urkund and similar tools are to my knowledge entirely about plagiarism. So the issue would seem to be that the R output is considered identical or nearly indentical to R output in other published orotherwise  submitted material.

What puzzles me (except for how a document can be deemed 32% plagiarized in 25% of the text) is whether this includes the numbers and variable names. If those are somehow factored out, then any R regression could be pretty much identical to any other R regression. However, two analyses with similar variable names could happen if they are based on the same cookbook recipe and analyses with similar numerical output come from analyzing the same standard data. Such situations would not necessarily be considered plagiarism (I mean: If you claim that you are analyzing data from experiments that you yourself have performed, and your numbers are exactly identical to something that has been previously published, then it would be suspect. If you analyze something from public sources, someone else might well have done the same thing.). 

Similarly to John Kane, I think it is necessary to know exactly what sources the text is claimed to be plagiarized from and/or what parts of the text that are being matched by Urkund. If it turns out that Urkund is generating false positives, then this needs to be pointed out to them and to the people basing decisions on it.

-pd

> On 22 Sep 2015, at 18:24 , Marc Schwartz <marc_schwartz at me.com> wrote:
> 
> Hi,
> 
> With the usual caveat that I Am Not A Lawyer....and that I am not speaking on behalf of any organization...
> 
> My guess is that they are claiming that the output of R, simply being copied and pasted verbatim into your thesis constitutes the use of copyrighted output from the software.
> 
> It is not clear to me that R's output is copyrighted by the R Foundation (or by other parties for CRAN packages), albeit, the source code underlying R is, along with other copyright owner's as apropos. There is some caselaw to support the notion that the output alone is not protected in a similar manner, but that may be country specific.
> 
> Did you provide any credit to R (see the output of citation() ) in your thesis and indicate that your analyses were performed using R?
> 
> If R is uncredited, I could see them raising the issue.
> 
> You might check with your institution's legal/policy folks to see if there is any guidance provided for students regarding the crediting of software used in this manner, especially if that guidance is at no cost to you.
> 
> Regards,
> 
> Marc Schwartz
> 
> 
>> On Sep 22, 2015, at 11:01 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> 
>> 1. It is highly unlikely that we could be of help (unless someone else
>> has experienced this and knows what happened). You will have to
>> contact the Urkund people and ask them why their algorithms raised the
>> flags.
>> 
>> 2. But of course, the regression methodology is not "your own" -- it's
>> just a standard tool that you used in your work, which is entirely
>> legitimate of course.
>> 
>> Cheers,
>> Bert
>> 
>> 
>> Bert Gunter
>> 
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>  -- Clifford Stoll
>> 
>> 
>> On Tue, Sep 22, 2015 at 7:27 AM, BARRETT, Oliver
>> <oliver.barrett at skema.edu> wrote:
>>> 
>>> Dear 'R' community support,
>>> 
>>> 
>>> I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.
>>> 
>>> 
>>> I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:
>>> 
>>> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>>>   Fed.t.4., data = OLS_CAR, x = TRUE)
>>> 
>>> Residuals:
>>>     Min        1Q    Median        3Q       Max
>>> -0.154587 -0.015961  0.001429  0.017196  0.110907
>>> 
>>> Coefficients:
>>>            Estimate Std. Error t value Pr(>|t|)
>>> (Intercept) -0.001630   0.001763  -0.925   0.3559
>>> Fed         -0.121595   0.165359  -0.735   0.4627
>>> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
>>> Fed.t.2.     0.026529   0.143648   0.185   0.8536
>>> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
>>> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
>>> ---
>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>> 
>>> Residual standard error: 0.0293 on 304 degrees of freedom
>>> (20 observations deleted due to missingness)
>>> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
>>> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
>>> 
>>> I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.
>>> 
>>> I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.
>>> 
>>> I would like to thank you in advance,
>>> 
>>> Kind regards,
>>> 
>>> Oliver Barrett
>>> (+44) 7341 834 217
>>> 
>>>       [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From wdunlap at tibco.com  Tue Sep 22 22:09:02 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 22 Sep 2015 13:09:02 -0700
Subject: [R] Error in eval(expr, envir, enclos) : could not find function
In-Reply-To: <4FBBE564-6BDD-46FD-8292-96AE6ECE5796@gmail.com>
References: <4FBBE564-6BDD-46FD-8292-96AE6ECE5796@gmail.com>
Message-ID: <CAF8bMcaQO9xBYv0ecQDMBuU1qG5UBS-Yi0_4gkjLP9-9HU9Syw@mail.gmail.com>

You left out the rest of the error message (the name of the function
it is looking for is key):
    > lm(Y ~ nosuchfuncion(X), data=data.frame(Y=1:10,X=log(1:10)))
    Error in eval(expr, envir, enclos) :
       could not find function "nosuchfuncion"
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, Sep 22, 2015 at 12:07 PM, Alaa Sindi <alaasindi at gmail.com> wrote:
> hi all
>
> I am getting this error "Error in eval(expr, envir, enclos) : could not find function ?
>
> do you have an idea what might cause this problem.
>
> thanks
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mmalten at gmail.com  Tue Sep 22 22:18:43 2015
From: mmalten at gmail.com (Mitchell Maltenfort)
Date: Tue, 22 Sep 2015 16:18:43 -0400
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
	<CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
	<4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
	<5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>
Message-ID: <CANOgrHbEzD0eHsOWcNGCn-PLpTdp+YD5VKVEsovxdsVb6k6r3w@mail.gmail.com>

Isn't plagiarism detection based on overlaps with sentence structure?
That way, it would catch plagiarism if someone simply did a
find-and-replace. But that would also catch regressions with the same
output format.

How long was the original thesis?  If 25% of it was all regression
output, sounds like a lot of regressions.



On Tue, Sep 22, 2015 at 4:06 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> Marc,
>
> I don't think Copyright/Intellectual property issues factor into this. Urkund and similar tools are to my knowledge entirely about plagiarism. So the issue would seem to be that the R output is considered identical or nearly indentical to R output in other published orotherwise  submitted material.
>
> What puzzles me (except for how a document can be deemed 32% plagiarized in 25% of the text) is whether this includes the numbers and variable names. If those are somehow factored out, then any R regression could be pretty much identical to any other R regression. However, two analyses with similar variable names could happen if they are based on the same cookbook recipe and analyses with similar numerical output come from analyzing the same standard data. Such situations would not necessarily be considered plagiarism (I mean: If you claim that you are analyzing data from experiments that you yourself have performed, and your numbers are exactly identical to something that has been previously published, then it would be suspect. If you analyze something from public sources, someone else might well have done the same thing.).
>
> Similarly to John Kane, I think it is necessary to know exactly what sources the text is claimed to be plagiarized from and/or what parts of the text that are being matched by Urkund. If it turns out that Urkund is generating false positives, then this needs to be pointed out to them and to the people basing decisions on it.
>
> -pd
>
>> On 22 Sep 2015, at 18:24 , Marc Schwartz <marc_schwartz at me.com> wrote:
>>
>> Hi,
>>
>> With the usual caveat that I Am Not A Lawyer....and that I am not speaking on behalf of any organization...
>>
>> My guess is that they are claiming that the output of R, simply being copied and pasted verbatim into your thesis constitutes the use of copyrighted output from the software.
>>
>> It is not clear to me that R's output is copyrighted by the R Foundation (or by other parties for CRAN packages), albeit, the source code underlying R is, along with other copyright owner's as apropos. There is some caselaw to support the notion that the output alone is not protected in a similar manner, but that may be country specific.
>>
>> Did you provide any credit to R (see the output of citation() ) in your thesis and indicate that your analyses were performed using R?
>>
>> If R is uncredited, I could see them raising the issue.
>>
>> You might check with your institution's legal/policy folks to see if there is any guidance provided for students regarding the crediting of software used in this manner, especially if that guidance is at no cost to you.
>>
>> Regards,
>>
>> Marc Schwartz
>>
>>
>>> On Sep 22, 2015, at 11:01 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>
>>> 1. It is highly unlikely that we could be of help (unless someone else
>>> has experienced this and knows what happened). You will have to
>>> contact the Urkund people and ask them why their algorithms raised the
>>> flags.
>>>
>>> 2. But of course, the regression methodology is not "your own" -- it's
>>> just a standard tool that you used in your work, which is entirely
>>> legitimate of course.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> Bert Gunter
>>>
>>> "Data is not information. Information is not knowledge. And knowledge
>>> is certainly not wisdom."
>>>  -- Clifford Stoll
>>>
>>>
>>> On Tue, Sep 22, 2015 at 7:27 AM, BARRETT, Oliver
>>> <oliver.barrett at skema.edu> wrote:
>>>>
>>>> Dear 'R' community support,
>>>>
>>>>
>>>> I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.
>>>>
>>>>
>>>> I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:
>>>>
>>>> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>>>>   Fed.t.4., data = OLS_CAR, x = TRUE)
>>>>
>>>> Residuals:
>>>>     Min        1Q    Median        3Q       Max
>>>> -0.154587 -0.015961  0.001429  0.017196  0.110907
>>>>
>>>> Coefficients:
>>>>            Estimate Std. Error t value Pr(>|t|)
>>>> (Intercept) -0.001630   0.001763  -0.925   0.3559
>>>> Fed         -0.121595   0.165359  -0.735   0.4627
>>>> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
>>>> Fed.t.2.     0.026529   0.143648   0.185   0.8536
>>>> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
>>>> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
>>>> ---
>>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>>
>>>> Residual standard error: 0.0293 on 304 degrees of freedom
>>>> (20 observations deleted due to missingness)
>>>> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
>>>> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
>>>>
>>>> I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.
>>>>
>>>> I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.
>>>>
>>>> I would like to thank you in advance,
>>>>
>>>> Kind regards,
>>>>
>>>> Oliver Barrett
>>>> (+44) 7341 834 217
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From JSorkin at grecc.umaryland.edu  Tue Sep 22 22:21:36 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 22 Sep 2015 16:21:36 -0400
Subject: [R] Compare two normal to one normal
Message-ID: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>

I have data that may be the mixture of two normal distributions (one contained within the other) vs. a single normal. 
I used normalmixEM to get estimates of parameters assuming two normals:


GLUT <- scale(na.omit(data[,"FCW_glut"]))
GLUT
mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
summary(mixmdl)
plot(mixmdl,which=2)
lines(density(data[,"GLUT"]), lty=2, lwd=2)





summary of normalmixEM object:
           comp 1   comp 2
lambda  0.7035179 0.296482
mu     -0.0592302 0.140545
sigma   1.1271620 0.536076
loglik at estimate:  -110.8037 



I would like to see if the two normal distributions are a better fit that one normal. I have two problems 
(1) normalmixEM does not seem to what to fit a single normal (even if I address the error message produced):


> mixmdl = normalmixEM(GLUT,k=1)
Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k = k,  : 
  arbmean and arbvar cannot both be FALSE
> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k = k,  : 
  arbmean and arbvar cannot both be FALSE



(2) Even if I had the loglik from a single normal, I am not sure how many DFs to use when computing the -2LL ratio test. 


Any suggestions for comparing the two-normal vs. one normal distribution would be appreciated.


Thanks
John









John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 


Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From giorgio.garziano at ericsson.com  Tue Sep 22 22:24:25 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 22 Sep 2015 20:24:25 +0000
Subject: [R] Randomness tests
Message-ID: <248E6FA047A8C746BA491485764190F522091D35@ESESSMB207.ericsson.se>

Hi,

to test randomness of time series whose values can only be +1 and -1, are all following
randomness tests applicable or only a part of ?

cox.stuart.test
difference.sign.test
bartels.rank.test
rank.test
runs.test

Tests provided by the randtests R package.

Thanks.

Giorgio Garziano



	[[alternative HTML version deleted]]


From lordpreetam at gmail.com  Tue Sep 22 22:25:39 2015
From: lordpreetam at gmail.com (Preetam Pal)
Date: Wed, 23 Sep 2015 01:55:39 +0530
Subject: [R] Weighted Ridge Regression with GCV Optimization
Message-ID: <CAHVFrXGJk+urqfFP-9ZTxGK=KmFRRXbc5N2qDmc0mG+7naaqdw@mail.gmail.com>

Hi R-users,

I am having problems while implementing the following model:

   1. I have numerical regressors (GDP, HPA and FX observed quarterly) and
   need to predict the numerical variable Y.
   2. I have to run *weighted Ridge Regression* where the weights of the
   squared residuals are decreasing at 5% with every quarter into the past.
   3. Before estimating beta, I need select the *optimal Ridge parameter*
   (lambda) wrt the GCV criterion:
                                                  a> For any lambda, divide
   the data into say, blocks B1, B2, B3, B4 and B5 of size k = 20% of data
   size. For each i, remove  B_i, estimate the beta vector           over the
   remaining data set and find the unweighted SSE (or any other deviation
   metric ) using this beta vector on the block B_i. Iterate over all
                 five B_i''s  ( i =1,2,3,4) and get the average of the 4 sse
   values.
                                                                    b> Allow
   lambda to vary between 0 to 1 in steps of size 0.01 and choose that lambda
   which minimizes the average sse computed in step a>
   4. With this choice of lambda, my final beta estimate would be [X'W'WX +
   lambda * Identity Matrix]^(-1)  * X'W'WY.
   5. Here W'W is a diagonal matrix whose diagonals are decreasing from the
   last entry upwards at 5% decay rate and trace(W'W) = 1 (i.e. sum of weights
   = 1)

I know lm.ridge() can do Ridge Regression, but I dont know how to write the
code with these weights, GCV criterion etc.

Can you please help me with this? I have attached the exact data in .txt
format (should be readable with read.table() ).Please let me know in case I
need to provide any more clarifications.

Thanks,
Preetam
-------------- next part --------------
T	GDP Rate	HPA	FX	Y
1	0.806660537	2.177803167	1.14980573	2.733594304
2	0.997724655	1.585686087	0.814496976	3.193948056
3	0.99032353	0.569843997	0.464488882	3.065751781
4	0.606121306	3.037648988	0.565322084	4.537399052
5	0.858131141	4.816423605	1.924534222	7.871730873
6	0.052909178	2.048591352	1.470221953	2.580646078
7	0.081400487	1.152495559	1.128828557	7.200336313
8	0.840972911	3.848225962	1.004272646	1.211124673
9	0.965868218	1.039679934	0.231408747	7.566968
10	0.952626722	4.455565591	0.483541015	9.412639513
11	0.067691757	0.038417569	0.69744243	8.055369029
12	0.985658841	1.143481763	1.65850909	6.962599601
13	0.177186946	3.762691635	0.44379572	9.904367023
14	0.490066697	0.655629739	1.281478696	1.796422139
15	0.223740666	1.393201062	1.235291827	5.237943945
16	0.782873809	1.485727273	0.224511215	6.399036418
17	0.947492758	0.318485005	1.158911495	8.183470692
18	0.49692711	2.169601457	1.777618832	8.830805294
19	0.956704273	1.546827505	0.241838792	7.554654431
20	0.404624372	3.041530693	1.66039172	6.709330773
21	0.98557461	2.45656369	1.695179666	8.638707974
22	0.494102398	4.527230971	0.993352283	7.958872374
23	0.893182943	3.429112971	0.675541115	5.665249801
24	0.669680459	0.459919029	1.011872328	8.883120607
25	0.017296599	2.184045646	1.575891106	2.585709635

From marc_schwartz at me.com  Tue Sep 22 22:27:24 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 22 Sep 2015 15:27:24 -0500
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
	<CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
	<4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
	<5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>
Message-ID: <DD9E4BC0-4C92-4F23-9E2D-97AA173507A4@me.com>

Peter,

Great distinction. 

I was leaning in the direction that the "look and feel" of the output (standard wording, table structure, column headings, significance stars and so forth in the output) is similar to whatever Urkund is using as the basis for the comparison and less so on an exact replication (covariates, coefficients, etc.), or nearly so, of prior work.

Thanks,

Marc


> On Sep 22, 2015, at 3:06 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> Marc,
> 
> I don't think Copyright/Intellectual property issues factor into this. Urkund and similar tools are to my knowledge entirely about plagiarism. So the issue would seem to be that the R output is considered identical or nearly indentical to R output in other published orotherwise  submitted material.
> 
> What puzzles me (except for how a document can be deemed 32% plagiarized in 25% of the text) is whether this includes the numbers and variable names. If those are somehow factored out, then any R regression could be pretty much identical to any other R regression. However, two analyses with similar variable names could happen if they are based on the same cookbook recipe and analyses with similar numerical output come from analyzing the same standard data. Such situations would not necessarily be considered plagiarism (I mean: If you claim that you are analyzing data from experiments that you yourself have performed, and your numbers are exactly identical to something that has been previously published, then it would be suspect. If you analyze something from public sources, someone else might well have done the same thing.). 
> 
> Similarly to John Kane, I think it is necessary to know exactly what sources the text is claimed to be plagiarized from and/or what parts of the text that are being matched by Urkund. If it turns out that Urkund is generating false positives, then this needs to be pointed out to them and to the people basing decisions on it.
> 
> -pd
> 
>> On 22 Sep 2015, at 18:24 , Marc Schwartz <marc_schwartz at me.com> wrote:
>> 
>> Hi,
>> 
>> With the usual caveat that I Am Not A Lawyer....and that I am not speaking on behalf of any organization...
>> 
>> My guess is that they are claiming that the output of R, simply being copied and pasted verbatim into your thesis constitutes the use of copyrighted output from the software.
>> 
>> It is not clear to me that R's output is copyrighted by the R Foundation (or by other parties for CRAN packages), albeit, the source code underlying R is, along with other copyright owner's as apropos. There is some caselaw to support the notion that the output alone is not protected in a similar manner, but that may be country specific.
>> 
>> Did you provide any credit to R (see the output of citation() ) in your thesis and indicate that your analyses were performed using R?
>> 
>> If R is uncredited, I could see them raising the issue.
>> 
>> You might check with your institution's legal/policy folks to see if there is any guidance provided for students regarding the crediting of software used in this manner, especially if that guidance is at no cost to you.
>> 
>> Regards,
>> 
>> Marc Schwartz
>> 
>> 
>>> On Sep 22, 2015, at 11:01 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>> 
>>> 1. It is highly unlikely that we could be of help (unless someone else
>>> has experienced this and knows what happened). You will have to
>>> contact the Urkund people and ask them why their algorithms raised the
>>> flags.
>>> 
>>> 2. But of course, the regression methodology is not "your own" -- it's
>>> just a standard tool that you used in your work, which is entirely
>>> legitimate of course.
>>> 
>>> Cheers,
>>> Bert
>>> 
>>> 
>>> Bert Gunter
>>> 
>>> "Data is not information. Information is not knowledge. And knowledge
>>> is certainly not wisdom."
>>> -- Clifford Stoll
>>> 
>>> 
>>> On Tue, Sep 22, 2015 at 7:27 AM, BARRETT, Oliver
>>> <oliver.barrett at skema.edu> wrote:
>>>> 
>>>> Dear 'R' community support,
>>>> 
>>>> 
>>>> I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.
>>>> 
>>>> 
>>>> I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:
>>>> 
>>>> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>>>>  Fed.t.4., data = OLS_CAR, x = TRUE)
>>>> 
>>>> Residuals:
>>>>    Min        1Q    Median        3Q       Max
>>>> -0.154587 -0.015961  0.001429  0.017196  0.110907
>>>> 
>>>> Coefficients:
>>>>           Estimate Std. Error t value Pr(>|t|)
>>>> (Intercept) -0.001630   0.001763  -0.925   0.3559
>>>> Fed         -0.121595   0.165359  -0.735   0.4627
>>>> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
>>>> Fed.t.2.     0.026529   0.143648   0.185   0.8536
>>>> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
>>>> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
>>>> ---
>>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>> 
>>>> Residual standard error: 0.0293 on 304 degrees of freedom
>>>> (20 observations deleted due to missingness)
>>>> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
>>>> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
>>>> 
>>>> I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.
>>>> 
>>>> I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.
>>>> 
>>>> I would like to thank you in advance,
>>>> 
>>>> Kind regards,
>>>> 
>>>> Oliver Barrett
>>>> (+44) 7341 834 217


From bgunter.4567 at gmail.com  Tue Sep 22 22:30:34 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 22 Sep 2015 13:30:34 -0700
Subject: [R] Compare two normal to one normal
In-Reply-To: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
Message-ID: <CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>

Two normals will **always** be a better fit than one, as the latter
must be a subset of the former (with identical parameters for both
normals).

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 22, 2015 at 1:21 PM, John Sorkin
<JSorkin at grecc.umaryland.edu> wrote:
> I have data that may be the mixture of two normal distributions (one contained within the other) vs. a single normal.
> I used normalmixEM to get estimates of parameters assuming two normals:
>
>
> GLUT <- scale(na.omit(data[,"FCW_glut"]))
> GLUT
> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
> summary(mixmdl)
> plot(mixmdl,which=2)
> lines(density(data[,"GLUT"]), lty=2, lwd=2)
>
>
>
>
>
> summary of normalmixEM object:
>            comp 1   comp 2
> lambda  0.7035179 0.296482
> mu     -0.0592302 0.140545
> sigma   1.1271620 0.536076
> loglik at estimate:  -110.8037
>
>
>
> I would like to see if the two normal distributions are a better fit that one normal. I have two problems
> (1) normalmixEM does not seem to what to fit a single normal (even if I address the error message produced):
>
>
>> mixmdl = normalmixEM(GLUT,k=1)
> Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k = k,  :
>   arbmean and arbvar cannot both be FALSE
>> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
> Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k = k,  :
>   arbmean and arbvar cannot both be FALSE
>
>
>
> (2) Even if I had the loglik from a single normal, I am not sure how many DFs to use when computing the -2LL ratio test.
>
>
> Any suggestions for comparing the two-normal vs. one normal distribution would be appreciated.
>
>
> Thanks
> John
>
>
>
>
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:12}}


From schwidom at gmx.net  Tue Sep 22 22:34:53 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Tue, 22 Sep 2015 22:34:53 +0200
Subject: [R] c(1:n, 1:(n-1), 1:(n-2), ... , 1)
In-Reply-To: <20150917190630.GA5556@debian64>
References: <1442513945858-4712390.post@n4.nabble.com>
	<CA+hbrhUVZ6LEJc_6XiiffgYuRo_4RRjUE2US=pc0diicKcnNjA@mail.gmail.com>
	<alpine.DEB.2.11.1509172039510.15300@paninaro.uibk.ac.at>
	<EF76ED63-CF8A-486E-AC2C-1C61C637F753@comcast.net>
	<20150917190630.GA5556@debian64>
Message-ID: <20150922203453.GA3769@debian64>

Hi

I have to correct myself, this last solution is not universally valid

here a better one:

tmp1 <- ( 1 - outer( max( x):1, x, FUN='-'))
tmp1[ tmp1 > 0]


On 2015-09-17 21:06:30, Frank Schwidom wrote:
> 
> how abount a more complicated one?
> 
> outer( 1:5, 1:5, '-')[ outer( 1:5, 1:5, '>')]
>  [1] 1 2 3 4 1 2 3 1 2 1
> 
> 
> On Thu, Sep 17, 2015 at 11:52:27AM -0700, David Winsemius wrote:
> > You can add this to the list of options to be tested, although my bet would be placed on `sequence(5:1)`:
> > 
> > > Reduce( function(x,y){c( 1:y, x)}, 1:5)
> >  [1] 1 2 3 4 5 1 2 3 4 1 2 3 1 2 1
> > 
> > 
> > On Sep 17, 2015, at 11:40 AM, Achim Zeileis wrote:
> > 
> > > On Thu, 17 Sep 2015, Peter Langfelder wrote:
> > > 
> > >> Not sure if this is slicker or easier to follow than your solution,
> > >> but it is shorter :)
> > >> 
> > >> do.call(c, lapply(n:1, function(n1) 1:n1))
> > > 
> > > Also not sure about efficiency but somewhat shorter...
> > > unlist(lapply(5:1, seq))
> > > 
> > >> Peter
> > >> 
> > >> On Thu, Sep 17, 2015 at 11:19 AM, Dan D <ddalthorp at usgs.gov> wrote:
> > >>> Can anyone think of a slick way to create an array that looks like c(1:n,
> > >>> 1:(n-1), 1:(n-2), ... , 1)?
> > >>> 
> > >>> The following works, but it's inefficient and a little hard to follow:
> > >>> n<-5
> > >>> junk<-array(1:n,dim=c(n,n))
> > >>> junk[((lower.tri(t(junk),diag=T)))[n:1,]]
> > >>> 
> > >>> Any help would be greatly appreciated!
> > >>> 
> > >>> -Dan
> > >>> 
> > >>> 
> > 
> > David Winsemius
> > Alameda, CA, USA
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From JSorkin at grecc.umaryland.edu  Tue Sep 22 22:34:11 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 22 Sep 2015 16:34:11 -0400
Subject: [R] Compare two normal to one normal
In-Reply-To: <CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
Message-ID: <56018303020000CB00139BB9@smtp.medicine.umaryland.edu>

Bert,Better, perhaps, but will something like the LR test be significant? Adding an extra parameter to a linear regression almost always improves the R2, the if one compares models, the model with the extra parameter is not always significantly better.
John
P.S. Please forgive the appeal to "significantly better" . . .


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

>>> Bert Gunter <bgunter.4567 at gmail.com> 09/22/15 4:30 PM >>>
Two normals will **always** be a better fit than one, as the latter
must be a subset of the former (with identical parameters for both
normals).

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 22, 2015 at 1:21 PM, John Sorkin
<JSorkin at grecc.umaryland.edu> wrote:
> I have data that may be the mixture of two normal distributions (one contained within the other) vs. a single normal.
> I used normalmixEM to get estimates of parameters assuming two normals:
>
>
> GLUT <- scale(na.omit(data[,"FCW_glut"]))
> GLUT
> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
> summary(mixmdl)
> plot(mixmdl,which=2)
> lines(density(data[,"GLUT"]), lty=2, lwd=2)
>
>
>
>
>
> summary of normalmixEM object:
>            comp 1   comp 2
> lambda  0.7035179 0.296482
> mu     -0.0592302 0.140545
> sigma   1.1271620 0.536076
> loglik at estimate:  -110.8037
>
>
>
> I would like to see if the two normal distributions are a better fit that one normal. I have two problems
> (1) normalmixEM does not seem to what to fit a single normal (even if I address the error message produced):
>
>
>> mixmdl = normalmixEM(GLUT,k=1)
> Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k = k,  :
>   arbmean and arbvar cannot both be FALSE
>> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
> Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k = k,  :
>   arbmean and arbvar cannot both be FALSE
>
>
>
> (2) Even if I had the loglik from a single normal, I am not sure how many DFs to use when computing the -2LL ratio test.
>
>
> Any suggestions for comparing the two-normal vs. one normal distribution would be appreciated.
>
>
> Thanks
> John
>
>
>
>
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> Confidentiality Statement:
> This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Call
Send SMS
Call from mobile
Add to Skype
You'll need Skype CreditFree via Skype


Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From markleeds2 at gmail.com  Tue Sep 22 22:35:56 2015
From: markleeds2 at gmail.com (Mark Leeds)
Date: Tue, 22 Sep 2015 16:35:56 -0400
Subject: [R] Compare two normal to one normal
In-Reply-To: <CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
Message-ID: <CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>

That's true but if he uses some AIC or BIC criterion that penalizes the
number of parameters,
then he might see something else ? This ( comparing mixtures to not
mixtures ) is not something I deal with so I'm just throwing it out there.




On Tue, Sep 22, 2015 at 4:30 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Two normals will **always** be a better fit than one, as the latter
> must be a subset of the former (with identical parameters for both
> normals).
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Tue, Sep 22, 2015 at 1:21 PM, John Sorkin
> <JSorkin at grecc.umaryland.edu> wrote:
> > I have data that may be the mixture of two normal distributions (one
> contained within the other) vs. a single normal.
> > I used normalmixEM to get estimates of parameters assuming two normals:
> >
> >
> > GLUT <- scale(na.omit(data[,"FCW_glut"]))
> > GLUT
> > mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
> > summary(mixmdl)
> > plot(mixmdl,which=2)
> > lines(density(data[,"GLUT"]), lty=2, lwd=2)
> >
> >
> >
> >
> >
> > summary of normalmixEM object:
> >            comp 1   comp 2
> > lambda  0.7035179 0.296482
> > mu     -0.0592302 0.140545
> > sigma   1.1271620 0.536076
> > loglik at estimate:  -110.8037
> >
> >
> >
> > I would like to see if the two normal distributions are a better fit
> that one normal. I have two problems
> > (1) normalmixEM does not seem to what to fit a single normal (even if I
> address the error message produced):
> >
> >
> >> mixmdl = normalmixEM(GLUT,k=1)
> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k =
> k,  :
> >   arbmean and arbvar cannot both be FALSE
> >> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k =
> k,  :
> >   arbmean and arbvar cannot both be FALSE
> >
> >
> >
> > (2) Even if I had the loglik from a single normal, I am not sure how
> many DFs to use when computing the -2LL ratio test.
> >
> >
> > Any suggestions for comparing the two-normal vs. one normal distribution
> would be appreciated.
> >
> >
> > Thanks
> > John
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> >
> >
> > Confidentiality Statement:
> > This email message, including any attachments, is for ...{{dropped:12}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From JSorkin at grecc.umaryland.edu  Tue Sep 22 22:45:16 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 22 Sep 2015 16:45:16 -0400
Subject: [R] Compare two normal to one normal
In-Reply-To: <CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
Message-ID: <5601859C020000CB00139BCB@smtp.medicine.umaryland.edu>

I am not sure AIC or BIC would be needed as the two normal distribution has at least two additional parameters to estimate; mean 1, var1, mean 2, var 2 where as the one normal has to estimate only var1 and var2.In any event, I don't know how to fit the single normal and get values for the loglik let alone AIC or BIC
John



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

>>> Mark Leeds <markleeds2 at gmail.com> 09/22/15 4:36 PM >>>
That's true but if he uses some AIC or BIC criterion that penalizes the number of parameters,

then he might see something else ? This ( comparing mixtures to not mixtures ) is not something I deal with so I'm just throwing it out there.






On Tue, Sep 22, 2015 at 4:30 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
Two normals will **always** be a better fit than one, as the latter
 must be a subset of the former (with identical parameters for both
 normals).
 
 Cheers,
 Bert
 
 
 Bert Gunter
 
 "Data is not information. Information is not knowledge. And knowledge
 is certainly not wisdom."
    -- Clifford Stoll
 
 
 On Tue, Sep 22, 2015 at 1:21 PM, John Sorkin
 <JSorkin at grecc.umaryland.edu> wrote:
 > I have data that may be the mixture of two normal distributions (one contained within the other) vs. a single normal.
 > I used normalmixEM to get estimates of parameters assuming two normals:
 >
 >
 > GLUT <- scale(na.omit(data[,"FCW_glut"]))
 > GLUT
 > mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
 > summary(mixmdl)
 > plot(mixmdl,which=2)
 > lines(density(data[,"GLUT"]), lty=2, lwd=2)
 >
 >
 >
 >
 >
 > summary of normalmixEM object:
 >            comp 1   comp 2
 > lambda  0.7035179 0.296482
 > mu     -0.0592302 0.140545
 > sigma   1.1271620 0.536076
 > loglik at estimate:  -110.8037
 >
 >
 >
 > I would like to see if the two normal distributions are a better fit that one normal. I have two problems
 > (1) normalmixEM does not seem to what to fit a single normal (even if I address the error message produced):
 >
 >
 >> mixmdl = normalmixEM(GLUT,k=1)
 > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k = k,  :
 >   arbmean and arbvar cannot both be FALSE
 >> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
 > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k = k,  :
 >   arbmean and arbvar cannot both be FALSE
 >
 >
 >
 > (2) Even if I had the loglik from a single normal, I am not sure how many DFs to use when computing the -2LL ratio test.
 >
 >
 > Any suggestions for comparing the two-normal vs. one normal distribution would be appreciated.
 >
 >
 > Thanks
 > John
 >
 >
 >
 >
 >
 >
 >
 >
 >
 > John David Sorkin M.D., Ph.D.
 > Professor of Medicine
 > Chief, Biostatistics and Informatics
 > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
 > Baltimore VA Medical Center
 > 10 North Greene Street
 > GRECC (BT/18/GR)
 > Baltimore, MD 21201-1524
 > (Phone) 410-605-7119410-605-7119
 > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
 >
 >
 > Confidentiality Statement:
 

> This email message, including any attachments, is for ...{{dropped:12}}
 
 ______________________________________________
 R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
 https://stat.ethz.ch/mailman/listinfo/r-help
 PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 and provide commented, minimal, self-contained, reproducible code.






Call
Send SMS
Call from mobile
Add to Skype
You'll need Skype CreditFree via Skype



Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From bgunter.4567 at gmail.com  Tue Sep 22 22:48:45 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 22 Sep 2015 13:48:45 -0700
Subject: [R] Compare two normal to one normal
In-Reply-To: <CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
Message-ID: <CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>

I'll be brief in my reply to you both, as this is off topic.

So what?  All this statistical stuff is irrelevant baloney(and of
questionable accuracy, since based on asymptotics and strong
assumptions, anyway) . The question of interest is whether a mixture
fit better suits the context, which only the OP knows and which none
of us can answer.

I know that many will disagree with this -- maybe a few might agree --
but please send all replies, insults, praise, and learned discourse to
me privately,  as I have already occupied more space on the list than
I should.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 22, 2015 at 1:35 PM, Mark Leeds <markleeds2 at gmail.com> wrote:
> That's true but if he uses some AIC or BIC criterion that penalizes the
> number of parameters,
> then he might see something else ? This ( comparing mixtures to not mixtures
> ) is not something I deal with so I'm just throwing it out there.
>
>
>
>
> On Tue, Sep 22, 2015 at 4:30 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> Two normals will **always** be a better fit than one, as the latter
>> must be a subset of the former (with identical parameters for both
>> normals).
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>    -- Clifford Stoll
>>
>>
>> On Tue, Sep 22, 2015 at 1:21 PM, John Sorkin
>> <JSorkin at grecc.umaryland.edu> wrote:
>> > I have data that may be the mixture of two normal distributions (one
>> > contained within the other) vs. a single normal.
>> > I used normalmixEM to get estimates of parameters assuming two normals:
>> >
>> >
>> > GLUT <- scale(na.omit(data[,"FCW_glut"]))
>> > GLUT
>> > mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
>> > summary(mixmdl)
>> > plot(mixmdl,which=2)
>> > lines(density(data[,"GLUT"]), lty=2, lwd=2)
>> >
>> >
>> >
>> >
>> >
>> > summary of normalmixEM object:
>> >            comp 1   comp 2
>> > lambda  0.7035179 0.296482
>> > mu     -0.0592302 0.140545
>> > sigma   1.1271620 0.536076
>> > loglik at estimate:  -110.8037
>> >
>> >
>> >
>> > I would like to see if the two normal distributions are a better fit
>> > that one normal. I have two problems
>> > (1) normalmixEM does not seem to what to fit a single normal (even if I
>> > address the error message produced):
>> >
>> >
>> >> mixmdl = normalmixEM(GLUT,k=1)
>> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k =
>> > k,  :
>> >   arbmean and arbvar cannot both be FALSE
>> >> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
>> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k =
>> > k,  :
>> >   arbmean and arbvar cannot both be FALSE
>> >
>> >
>> >
>> > (2) Even if I had the loglik from a single normal, I am not sure how
>> > many DFs to use when computing the -2LL ratio test.
>> >
>> >
>> > Any suggestions for comparing the two-normal vs. one normal distribution
>> > would be appreciated.
>> >
>> >
>> > Thanks
>> > John
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > John David Sorkin M.D., Ph.D.
>> > Professor of Medicine
>> > Chief, Biostatistics and Informatics
>> > University of Maryland School of Medicine Division of Gerontology and
>> > Geriatric Medicine
>> > Baltimore VA Medical Center
>> > 10 North Greene Street
>> > GRECC (BT/18/GR)
>> > Baltimore, MD 21201-1524
>> > (Phone) 410-605-7119
>> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>> >
>> >
>> > Confidentiality Statement:
>> > This email message, including any attachments, is for ...{{dropped:12}}
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From r.turner at auckland.ac.nz  Tue Sep 22 23:26:04 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 23 Sep 2015 09:26:04 +1200
Subject: [R] [FORGED] Error from lme4: "Error: (p <- ncol(X)) == ncol(Y)
 is not TRUE"
In-Reply-To: <396393592.1539223.1442927896027.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.5.1442916002.27623.r-help@r-project.org>
	<396393592.1539223.1442927896027.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5601C76C.4000107@auckland.ac.nz>


On 23/09/15 01:18, Rory Wilson wrote:

> Hello all, I am trying to run a random intercept model using lme4.
> The random effect is a factor of 29 possibilities, making a model
> with one random effect (one level). It is just a linear model. There
> are 713 observations. However, when trying to run the model I receive
> the error "Error: (p <- ncol(X)) == ncol(Y) is not TRUE", a search
> for which reveals somewhat surprisingly little. Has anyone seen this
> before? Note that if I simply change the random effect into a fixed
> effect and use lm, the model works perfectly.Thank you!

[Caveat:  I really find the syntax of lmer() incomprehensible, so my 
example below could be a load of dingos' kidneys.]

I think a reproducible example (as specified by the posting guide) is 
needed here.  When I do:

set.seed(42)
f <- factor(sample(1:29,713,TRUE))
x <- seq(0,1,length=713)
y <- rnorm(713)
require(lme4)
fit <- lmer(y ~ x + (1|f))

I get a reasonable (???) looking result and no error messages.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Tue Sep 22 23:31:45 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 23 Sep 2015 09:31:45 +1200
Subject: [R] [FORGED]  error in mlogit.optim
In-Reply-To: <2E2512D4-9670-4BDB-B9C9-29BA39153881@gmail.com>
References: <2E2512D4-9670-4BDB-B9C9-29BA39153881@gmail.com>
Message-ID: <5601C8C1.9070409@auckland.ac.nz>

On 23/09/15 03:33, Alaa Sindi wrote:
> Hi all
>
> I hope you are doing well.
>
> I am trying to install and use mlogit.optim and getting this error.
>
> Error: could not find function ?mlogit.optim"
>
> Warning in install.packages : unable to access index for repository
> https://cran.rstudio.com/src/contrib
> <https://cran.rstudio.com/src/contrib> Warning in install.packages :
> unable to access index for repository
> https://cran.rstudio.com/src/contrib
> <https://cran.rstudio.com/src/contrib> Warning in install.packages :
> package ?mlogit.optim? is not available (for R version 3.2.2) Warning
> in install.packages : unable to access index for repository
> https://cran.rstudio.com/bin/macosx/mavericks/contrib/3.2
> <https://cran.rstudio.com/bin/macosx/mavericks/contrib/3.2>

I see that the out-patients are out in force tonight.

You need to install (and then load) the ***mlogit*** package.  The 
function mlogit.optim() is a function in this package.  Learn to use 
search tools.  Better still, learn to think.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From schwidom at gmx.net  Tue Sep 22 23:43:10 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Tue, 22 Sep 2015 23:43:10 +0200
Subject: [R] vector manipulations -- differences
In-Reply-To: <1442870260243-4712575.post@n4.nabble.com>
References: <1442870260243-4712575.post@n4.nabble.com>
Message-ID: <20150922214310.GB3769@debian64>

Hi,

xr <- rev( x)
vec <- 1:(length( x) - 1)
rev( xr[ sequence( vec)] - rep.int( xr[ -1], vec))


On 2015-09-21 14:17:40, Dan D wrote:
> I need an efficient way to build a new n x (n-1)/2 vector from an n-vector x
> as:
> 
> c(x[-1]-x[1], x[-(1:2)]-x[2], ... , x[-(1:(n-1)] - x[n-1])
> 
> x is increasing with x[1] = 0. 
> 
> The following works but is not the greatest:
> junk<-outer(x, x, '-')
> junk[junk>0]
> 
> e.g., 
> given
> x<-c(0, 3, 7, 20)
> junk<-outer(x, x, '-')
> junk[junk>0] # yields: c(3, 7, 20, 4, 17, 13) as needed, but it has to go
> through 
> junk
> #     [,1] [,2] [,3] [,4]
> #[1,]    0   -3   -7  -20
> #[2,]    3    0   -4  -17
> #[3,]    7    4    0  -13
> #[4,]   20   17   13    0
> 
> Anyone have a better idea?
> 
> -Dan
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/vector-manipulations-differences-tp4712575.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From schwidom at gmx.net  Tue Sep 22 23:50:39 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Tue, 22 Sep 2015 23:50:39 +0200
Subject: [R] vector manipulations -- differences
In-Reply-To: <20150922214310.GB3769@debian64>
References: <1442870260243-4712575.post@n4.nabble.com>
	<20150922214310.GB3769@debian64>
Message-ID: <20150922215039.GC3769@debian64>


And if we want to use the approach of William Dunlap for sequence.optimization
, then we can write:

rev( xr[ seq_len(sum(vec)) - rep.int(cumsum(c(0L, vec[-length(vec)])), vec)] - rep.int( xr[ -1], vec))

Regards.

On 2015-09-22 23:43:10, Frank Schwidom wrote:
> Hi,
> 
> xr <- rev( x)
> vec <- 1:(length( x) - 1)
> rev( xr[ sequence( vec)] - rep.int( xr[ -1], vec))
> 
> 
> On 2015-09-21 14:17:40, Dan D wrote:
> > I need an efficient way to build a new n x (n-1)/2 vector from an n-vector x
> > as:
> > 
> > c(x[-1]-x[1], x[-(1:2)]-x[2], ... , x[-(1:(n-1)] - x[n-1])
> > 
> > x is increasing with x[1] = 0. 
> > 
> > The following works but is not the greatest:
> > junk<-outer(x, x, '-')
> > junk[junk>0]
> > 
> > e.g., 
> > given
> > x<-c(0, 3, 7, 20)
> > junk<-outer(x, x, '-')
> > junk[junk>0] # yields: c(3, 7, 20, 4, 17, 13) as needed, but it has to go
> > through 
> > junk
> > #     [,1] [,2] [,3] [,4]
> > #[1,]    0   -3   -7  -20
> > #[2,]    3    0   -4  -17
> > #[3,]    7    4    0  -13
> > #[4,]   20   17   13    0
> > 
> > Anyone have a better idea?
> > 
> > -Dan
> > 
> > 
> > 
> > --
> > View this message in context: http://r.789695.n4.nabble.com/vector-manipulations-differences-tp4712575.html
> > Sent from the R help mailing list archive at Nabble.com.
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From therneau at mayo.edu  Wed Sep 23 00:00:03 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Tue, 22 Sep 2015 17:00:03 -0500
Subject: [R] retaining characters in a csv file
Message-ID: <c10f8b$1g9f15@ironport10.mayo.edu>

I have a csv file from an automatic process (so this will happen thousands of times), for 
which the first row is a vector of variable names and the second row often starts 
something like this:

5724550,"000202075214",2005.02.17,2005.02.17,"F", .....

Notice the second variable which is
       a character string (note the quotation marks)
       a sequence of numeric digits
       leading zeros are significant

The read.csv function insists on turning this into a numeric.  Is there any simple set of 
options that
will turn this behavior off?  I'm looking for a way to tell it to "obey the bloody quotes" 
-- I still want the first, third, etc columns to become numeric.  There can be more than 
one variable like this, and not always in the second position.

This happens deep inside the httr library; there is an easy way for me to add more options 
to the read.csv call but it is not so easy to replace it with something else.

Terry T


From h.wickham at gmail.com  Tue Sep 22 23:58:40 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Tue, 22 Sep 2015 16:58:40 -0500
Subject: [R] retaining characters in a csv file
In-Reply-To: <c10f8b$1g9f14@ironport10.mayo.edu>
References: <c10f8b$1g9f14@ironport10.mayo.edu>
Message-ID: <CABdHhvFSS=aUK-EcoQ94F+GuRSeJK3ePAZRF4sQa9V12zcoHSw@mail.gmail.com>

The problem is that quotes in csv files are commonly held to me
meaningless (i.e. they don't automatically force components to be
strings).

Earlier this morning I committed a fix to readr so that numbers
starting with a sequence of zeros are read as character strings. You
may want to try out the dev version: https://github.com/hadley/readr.

Hadley

On Tue, Sep 22, 2015 at 5:00 PM, Therneau, Terry M., Ph.D.
<therneau at mayo.edu> wrote:
> I have a csv file from an automatic process (so this will happen thousands
> of times), for which the first row is a vector of variable names and the
> second row often starts something like this:
>
> 5724550,"000202075214",2005.02.17,2005.02.17,"F", .....
>
> Notice the second variable which is
>       a character string (note the quotation marks)
>       a sequence of numeric digits
>       leading zeros are significant
>
> The read.csv function insists on turning this into a numeric.  Is there any
> simple set of options that
> will turn this behavior off?  I'm looking for a way to tell it to "obey the
> bloody quotes" -- I still want the first, third, etc columns to become
> numeric.  There can be more than one variable like this, and not always in
> the second position.
>
> This happens deep inside the httr library; there is an easy way for me to
> add more options to the read.csv call but it is not so easy to replace it
> with something else.
>
> Terry T



-- 
http://had.co.nz/


From JSorkin at grecc.umaryland.edu  Wed Sep 23 00:01:11 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 22 Sep 2015 18:01:11 -0400
Subject: [R] Compare two normal to one normal
In-Reply-To: <CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
Message-ID: <56019767020000CB00139BF6@smtp.medicine.umaryland.edu>

Bert
I am surprised by your response. Statistics serves two purposes: estimation and hypothesis testing. Sometimes we are fortunate and theory, physiology, physics, or something else tell us what is the correct, or perhaps I should same most adequate model. Sometimes theory fails us and we wish to choose between two competing models. This is my case.  The cell sizes may come from one normal distribution (theory 1) or two (theory 2). Choosing between the models will help us postulate about physiology. I want to use statistics to help me decide between the two competing models, and thus inform my understanding of physiology. It is true that statistics can't tell me which model is the "correct" or "true" model, but it should be able to help me select the more "adequate" or "appropriate" or "closer to he truth" model.


In any event, I still don't know how to fit a single normal distribution and get a measure of fit e.g. log likelihood.


John


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

>>> Bert Gunter <bgunter.4567 at gmail.com> 09/22/15 4:48 PM >>>
I'll be brief in my reply to you both, as this is off topic.

So what?  All this statistical stuff is irrelevant baloney(and of
questionable accuracy, since based on asymptotics and strong
assumptions, anyway) . The question of interest is whether a mixture
fit better suits the context, which only the OP knows and which none
of us can answer.

I know that many will disagree with this -- maybe a few might agree --
but please send all replies, insults, praise, and learned discourse to
me privately,  as I have already occupied more space on the list than
I should.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Sep 22, 2015 at 1:35 PM, Mark Leeds <markleeds2 at gmail.com> wrote:
> That's true but if he uses some AIC or BIC criterion that penalizes the
> number of parameters,
> then he might see something else ? This ( comparing mixtures to not mixtures
> ) is not something I deal with so I'm just throwing it out there.
>
>
>
>
> On Tue, Sep 22, 2015 at 4:30 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> Two normals will **always** be a better fit than one, as the latter
>> must be a subset of the former (with identical parameters for both
>> normals).
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>    -- Clifford Stoll
>>
>>
>> On Tue, Sep 22, 2015 at 1:21 PM, John Sorkin
>> <JSorkin at grecc.umaryland.edu> wrote:
>> > I have data that may be the mixture of two normal distributions (one
>> > contained within the other) vs. a single normal.
>> > I used normalmixEM to get estimates of parameters assuming two normals:
>> >
>> >
>> > GLUT <- scale(na.omit(data[,"FCW_glut"]))
>> > GLUT
>> > mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
>> > summary(mixmdl)
>> > plot(mixmdl,which=2)
>> > lines(density(data[,"GLUT"]), lty=2, lwd=2)
>> >
>> >
>> >
>> >
>> >
>> > summary of normalmixEM object:
>> >            comp 1   comp 2
>> > lambda  0.7035179 0.296482
>> > mu     -0.0592302 0.140545
>> > sigma   1.1271620 0.536076
>> > loglik at estimate:  -110.8037
>> >
>> >
>> >
>> > I would like to see if the two normal distributions are a better fit
>> > that one normal. I have two problems
>> > (1) normalmixEM does not seem to what to fit a single normal (even if I
>> > address the error message produced):
>> >
>> >
>> >> mixmdl = normalmixEM(GLUT,k=1)
>> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k =
>> > k,  :
>> >   arbmean and arbvar cannot both be FALSE
>> >> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
>> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k =
>> > k,  :
>> >   arbmean and arbvar cannot both be FALSE
>> >
>> >
>> >
>> > (2) Even if I had the loglik from a single normal, I am not sure how
>> > many DFs to use when computing the -2LL ratio test.
>> >
>> >
>> > Any suggestions for comparing the two-normal vs. one normal distribution
>> > would be appreciated.
>> >
>> >
>> > Thanks
>> > John
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > John David Sorkin M.D., Ph.D.
>> > Professor of Medicine
>> > Chief, Biostatistics and Informatics
>> > University of Maryland School of Medicine Division of Gerontology and
>> > Geriatric Medicine
>> > Baltimore VA Medical Center
>> > 10 North Greene Street
>> > GRECC (BT/18/GR)
>> > Baltimore, MD 21201-1524
>> > (Phone) 410-605-7119410-605-7119
>> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>> >
>> >
>> > Confidentiality Statement:
>> > This email message, including any attachments, is for ...{{dropped:12}}
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


Call
Send SMS
Call from mobile
Add to Skype
You'll need Skype CreditFree via Skype


Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From ccberry at ucsd.edu  Wed Sep 23 00:23:37 2015
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Tue, 22 Sep 2015 15:23:37 -0700
Subject: [R] Compare two normal to one normal
In-Reply-To: <56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
	<56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
Message-ID: <alpine.OSX.2.20.1509221521500.1538@charles-berrys-macbook.local>

On Tue, 22 Sep 2015, John Sorkin wrote:

>
> In any event, I still don't know how to fit a single normal distribution 
> and get a measure of fit e.g. log likelihood.
>

Gotta love R:

> y <- rnorm(10)
> logLik(glm(y~1))
'log Lik.' -17.36071 (df=2)

HTH,

Chuck


From r.turner at auckland.ac.nz  Wed Sep 23 00:33:13 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 23 Sep 2015 10:33:13 +1200
Subject: [R] retaining characters in a csv file
In-Reply-To: <c10f8b$1g9f15@ironport10.mayo.edu>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
Message-ID: <5601D729.3060502@auckland.ac.nz>

On 23/09/15 10:00, Therneau, Terry M., Ph.D. wrote:
> I have a csv file from an automatic process (so this will happen
> thousands of times), for which the first row is a vector of variable
> names and the second row often starts something like this:
>
> 5724550,"000202075214",2005.02.17,2005.02.17,"F", .....
>
> Notice the second variable which is
>        a character string (note the quotation marks)
>        a sequence of numeric digits
>        leading zeros are significant
>
> The read.csv function insists on turning this into a numeric.  Is there
> any simple set of options that
> will turn this behavior off?  I'm looking for a way to tell it to "obey
> the bloody quotes" -- I still want the first, third, etc columns to
> become numeric.  There can be more than one variable like this, and not
> always in the second position.
>
> This happens deep inside the httr library; there is an easy way for me
> to add more options to the read.csv call but it is not so easy to
> replace it with something else.

IMHO this is a bug in read.csv().

A possible workaround:

ccc <- c("integer","character",rep(NA,k))
X   <- read.csv("melvin.csv",colClasses=ccc)

where "melvin.csv" is the file from which you are attempting to read and
where k+2 = the number of columns in that file.

Kludgey, but it might work.

Another workaround is to specify quote="", but this has the side effect
of making the 5th column character rather than logical.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Wed Sep 23 00:58:08 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 22 Sep 2015 15:58:08 -0700
Subject: [R] retaining characters in a csv file
In-Reply-To: <c10f8b$1g9f15@ironport10.mayo.edu>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
Message-ID: <6BDA4FEF-D438-4B8C-94F9-36BD9940FC05@comcast.net>


On Sep 22, 2015, at 3:00 PM, Therneau, Terry M., Ph.D. wrote:

> I have a csv file from an automatic process (so this will happen thousands of times), for which the first row is a vector of variable names and the second row often starts something like this:
> 
> 5724550,"000202075214",2005.02.17,2005.02.17,"F", .....
> 
> Notice the second variable which is
>      a character string (note the quotation marks)
>      a sequence of numeric digits
>      leading zeros are significant
> 
> The read.csv function insists on turning this into a numeric.  Is there any simple set of options that
> will turn this behavior off?  I'm looking for a way to tell it to "obey the bloody quotes" -- I still want the first, third, etc columns to become numeric.  There can be more than one variable like this, and not always in the second position.

The last part about not knowing which col might be an issue might require inputting everything with character class, but if there is a way to pass in a colClasses argument this might help:

> read.csv(text='5724550,"000202075214",2005.02.17,2005.02.17,"F"', stringsAsFactors=FALSE, header=FALSE, colClasses=c("numeric", rep("character", 4)))
       V1           V2         V3         V4 V5
1 5724550 000202075214 2005.02.17 2005.02.17  F

Or you can create a class with an As method:

> setClass('myChar')
> setAs('character', 'myChar', def=function(from, to ) to <- I(from))
> read.csv(text='5724550,"000202075214",2005.02.17,2005.02.17,"F"', stringsAsFactors=FALSE, header=FALSE, colClasses=c("numeric", rep('myChar',4)) )
       V1           V2         V3         V4 V5
1 5724550 000202075214 2005.02.17 2005.02.17  F

(Neither of the third or fourth columns makes sense as a numeric, so now illustrating coercion to Date.)

> setClass('dotDate')
> setAs('character', 'dotDate', def=function(from, to ) to <- as.Date(from, "%Y.%m.%d")  )

> read.csv(text='5724550,"000202075214",2005.02.17,2005.02.17,"F"', stringsAsFactors=FALSE, header=FALSE, colClasses=c("numeric", "character", rep('dotDate',2), "character") )
       V1           V2         V3         V4 V5
1 5724550 000202075214 2005-02-17 2005-02-17  F


> 
> This happens deep inside the httr library; there is an easy way for me to add more options to the read.csv call but it is not so easy to replace it with something else.
> 
> Terry T
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From aragorn168b at gmail.com  Wed Sep 23 01:11:02 2015
From: aragorn168b at gmail.com (Arunkumar Srinivasan)
Date: Wed, 23 Sep 2015 01:11:02 +0200
Subject: [R] retaining characters in a csv file
In-Reply-To: <c10f8b$1g9f15@ironport10.mayo.edu>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
Message-ID: <CAAf756No46_m-qvvKUNRW3tKoBfEC8yrnaj_S9V5yMVz7c2t+w@mail.gmail.com>

data.table's fread reads this as expected. Quoted strings aren't coerced.

sapply(fread('5724550,"000202075214",2005.02.17,2005.02.17,"F"\n'), class)
#          V1          V2          V3          V4          V5
#   "integer" "character" "character" "character" "character"

Best,
Arun.

On Wed, Sep 23, 2015 at 12:00 AM, Therneau, Terry M., Ph.D.
<therneau at mayo.edu> wrote:
> I have a csv file from an automatic process (so this will happen thousands
> of times), for which the first row is a vector of variable names and the
> second row often starts something like this:
>
> 5724550,"000202075214",2005.02.17,2005.02.17,"F", .....
>
> Notice the second variable which is
>       a character string (note the quotation marks)
>       a sequence of numeric digits
>       leading zeros are significant
>
> The read.csv function insists on turning this into a numeric.  Is there any
> simple set of options that
> will turn this behavior off?  I'm looking for a way to tell it to "obey the
> bloody quotes" -- I still want the first, third, etc columns to become
> numeric.  There can be more than one variable like this, and not always in
> the second position.
>
> This happens deep inside the httr library; there is an easy way for me to
> add more options to the read.csv call but it is not so easy to replace it
> with something else.
>
> Terry T
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Wed Sep 23 01:19:48 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 23 Sep 2015 01:19:48 +0200
Subject: [R] retaining characters in a csv file
In-Reply-To: <5601D729.3060502@auckland.ac.nz>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
	<5601D729.3060502@auckland.ac.nz>
Message-ID: <0BBB21D0-70FF-4978-B1C6-DD4A17F2234D@gmail.com>


> On 23 Sep 2015, at 00:33 , Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 

[read.csv() doesn't distinguish "123.4" from 123.4]

> IMHO this is a bug in read.csv().
> 

Dunno about that:

pd$ cat ~/tmp/junk.csv 
"1";1
2;"2"
pd$ open !$
open ~/tmp/junk.csv

And lo and behold, Excel opens with 

1 1
2 2

and all cells numeric.

I don't think the CSV standard (if there is one...) specifies that quoted strings are necessarily text.

I think we have been here before, and found that even if we decide that it is a bug (or misfeature), it would be hard to change, because the modus operandi of read.* is to first read everything as character and _then_ see (in type.convert()) which entries can be converted to numeric, logical, etc.

-pd

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From murdoch.duncan at gmail.com  Wed Sep 23 02:33:49 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 22 Sep 2015 20:33:49 -0400
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
	<CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
	<4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
	<5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>
Message-ID: <5601F36D.5060906@gmail.com>

On 22/09/2015 4:06 PM, peter dalgaard wrote:
> Marc,
> 
> I don't think Copyright/Intellectual property issues factor into this. Urkund and similar tools are to my knowledge entirely about plagiarism. So the issue would seem to be that the R output is considered identical or nearly indentical to R output in other published orotherwise  submitted material.
> 
> What puzzles me (except for how a document can be deemed 32% plagiarized in 25% of the text) is whether this includes the numbers and variable names. If those are somehow factored out, then any R regression could be pretty much identical to any other R regression. However, two analyses with similar variable names could happen if they are based on the same cookbook recipe and analyses with similar numerical output come from analyzing the same standard data. Such situations would not necessarily be considered plagiarism (I mean: If you claim that you are analyzing data from experiments that you yourself have performed, and your numbers are exactly identical to something that has been previously published, then it would be suspect. If you analyze something from public sources, someone else might well have done the same thing.).

I don't see why this puzzles you.  A simple explanation is that Urkund
is incompetent.

Many companies that sell software to university administrations are
incompetent, because the buyers have been promoted so far beyond their
competence that they'll buy anything if it is expensive enough.

This isn't uncommon.

Duncan Murdoch

> 
> Similarly to John Kane, I think it is necessary to know exactly what sources the text is claimed to be plagiarized from and/or what parts of the text that are being matched by Urkund. If it turns out that Urkund is generating false positives, then this needs to be pointed out to them and to the people basing decisions on it.
> 
> -pd
> 
>> On 22 Sep 2015, at 18:24 , Marc Schwartz <marc_schwartz at me.com> wrote:
>>
>> Hi,
>>
>> With the usual caveat that I Am Not A Lawyer....and that I am not speaking on behalf of any organization...
>>
>> My guess is that they are claiming that the output of R, simply being copied and pasted verbatim into your thesis constitutes the use of copyrighted output from the software.
>>
>> It is not clear to me that R's output is copyrighted by the R Foundation (or by other parties for CRAN packages), albeit, the source code underlying R is, along with other copyright owner's as apropos. There is some caselaw to support the notion that the output alone is not protected in a similar manner, but that may be country specific.
>>
>> Did you provide any credit to R (see the output of citation() ) in your thesis and indicate that your analyses were performed using R?
>>
>> If R is uncredited, I could see them raising the issue.
>>
>> You might check with your institution's legal/policy folks to see if there is any guidance provided for students regarding the crediting of software used in this manner, especially if that guidance is at no cost to you.
>>
>> Regards,
>>
>> Marc Schwartz
>>
>>
>>> On Sep 22, 2015, at 11:01 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>
>>> 1. It is highly unlikely that we could be of help (unless someone else
>>> has experienced this and knows what happened). You will have to
>>> contact the Urkund people and ask them why their algorithms raised the
>>> flags.
>>>
>>> 2. But of course, the regression methodology is not "your own" -- it's
>>> just a standard tool that you used in your work, which is entirely
>>> legitimate of course.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> Bert Gunter
>>>
>>> "Data is not information. Information is not knowledge. And knowledge
>>> is certainly not wisdom."
>>>  -- Clifford Stoll
>>>
>>>
>>> On Tue, Sep 22, 2015 at 7:27 AM, BARRETT, Oliver
>>> <oliver.barrett at skema.edu> wrote:
>>>>
>>>> Dear 'R' community support,
>>>>
>>>>
>>>> I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.
>>>>
>>>>
>>>> I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:
>>>>
>>>> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>>>>   Fed.t.4., data = OLS_CAR, x = TRUE)
>>>>
>>>> Residuals:
>>>>     Min        1Q    Median        3Q       Max
>>>> -0.154587 -0.015961  0.001429  0.017196  0.110907
>>>>
>>>> Coefficients:
>>>>            Estimate Std. Error t value Pr(>|t|)
>>>> (Intercept) -0.001630   0.001763  -0.925   0.3559
>>>> Fed         -0.121595   0.165359  -0.735   0.4627
>>>> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
>>>> Fed.t.2.     0.026529   0.143648   0.185   0.8536
>>>> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
>>>> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
>>>> ---
>>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>>
>>>> Residual standard error: 0.0293 on 304 degrees of freedom
>>>> (20 observations deleted due to missingness)
>>>> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
>>>> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
>>>>
>>>> I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.
>>>>
>>>> I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.
>>>>
>>>> I would like to thank you in advance,
>>>>
>>>> Kind regards,
>>>>
>>>> Oliver Barrett
>>>> (+44) 7341 834 217
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch.duncan at gmail.com  Wed Sep 23 02:36:58 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 22 Sep 2015 20:36:58 -0400
Subject: [R] retaining characters in a csv file
In-Reply-To: <c10f8b$1g9f15@ironport10.mayo.edu>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
Message-ID: <5601F42A.5090701@gmail.com>

On 22/09/2015 6:00 PM, Therneau, Terry M., Ph.D. wrote:
> I have a csv file from an automatic process (so this will happen thousands of times), for 
> which the first row is a vector of variable names and the second row often starts 
> something like this:
> 
> 5724550,"000202075214",2005.02.17,2005.02.17,"F", .....
> 
> Notice the second variable which is
>        a character string (note the quotation marks)
>        a sequence of numeric digits
>        leading zeros are significant
> 
> The read.csv function insists on turning this into a numeric. 

No it doesn't.  All you need to do is specify colClasses and it will
follow your instructions.


 Is there any simple set of
> options that
> will turn this behavior off?  I'm looking for a way to tell it to "obey the bloody quotes" 
> -- I still want the first, third, etc columns to become numeric.  There can be more than 
> one variable like this, and not always in the second position.

No, because the bloody quotes are part of the "csv standard".  They
aren't meaningful.

If you don't know what the data is, that's your fault.  You shouldn't be
analyzing data when you are so ignorant.

Duncan Murdoch

> This happens deep inside the httr library; there is an easy way for me to add more options 
> to the read.csv call but it is not so easy to replace it with something else.
> 
> Terry T
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch.duncan at gmail.com  Wed Sep 23 02:40:03 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 22 Sep 2015 20:40:03 -0400
Subject: [R] retaining characters in a csv file
In-Reply-To: <0BBB21D0-70FF-4978-B1C6-DD4A17F2234D@gmail.com>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
	<5601D729.3060502@auckland.ac.nz>
	<0BBB21D0-70FF-4978-B1C6-DD4A17F2234D@gmail.com>
Message-ID: <5601F4E3.6050201@gmail.com>

On 22/09/2015 7:19 PM, peter dalgaard wrote:
> 
>> On 23 Sep 2015, at 00:33 , Rolf Turner <r.turner at auckland.ac.nz> wrote:
>>
> 
> [read.csv() doesn't distinguish "123.4" from 123.4]
> 
>> IMHO this is a bug in read.csv().
>>
> 
> Dunno about that:
> 
> pd$ cat ~/tmp/junk.csv 
> "1";1
> 2;"2"
> pd$ open !$
> open ~/tmp/junk.csv
> 
> And lo and behold, Excel opens with 
> 
> 1 1
> 2 2
> 
> and all cells numeric.
> 
> I don't think the CSV standard (if there is one...) specifies that quoted strings are necessarily text.

It specifically does not.  Quotes allow commas and spaces to be ignored
as column separators.  That's all.  They say nothing about the type of data.

Duncan Murdoch


> 
> I think we have been here before, and found that even if we decide that it is a bug (or misfeature), it would be hard to change, because the modus operandi of read.* is to first read everything as character and _then_ see (in type.convert()) which entries can be converted to numeric, logical, etc.


From murdoch.duncan at gmail.com  Wed Sep 23 02:48:42 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 22 Sep 2015 20:48:42 -0400
Subject: [R] retaining characters in a csv file
In-Reply-To: <5601D729.3060502@auckland.ac.nz>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
	<5601D729.3060502@auckland.ac.nz>
Message-ID: <5601F6EA.4050200@gmail.com>

On 22/09/2015 6:33 PM, Rolf Turner wrote:
> On 23/09/15 10:00, Therneau, Terry M., Ph.D. wrote:
>> I have a csv file from an automatic process (so this will happen
>> thousands of times), for which the first row is a vector of variable
>> names and the second row often starts something like this:
>>
>> 5724550,"000202075214",2005.02.17,2005.02.17,"F", .....
>>
>> Notice the second variable which is
>>        a character string (note the quotation marks)
>>        a sequence of numeric digits
>>        leading zeros are significant
>>
>> The read.csv function insists on turning this into a numeric.  Is there
>> any simple set of options that
>> will turn this behavior off?  I'm looking for a way to tell it to "obey
>> the bloody quotes" -- I still want the first, third, etc columns to
>> become numeric.  There can be more than one variable like this, and not
>> always in the second position.
>>
>> This happens deep inside the httr library; there is an easy way for me
>> to add more options to the read.csv call but it is not so easy to
>> replace it with something else.
> 
> IMHO this is a bug in read.csv().

No, it's a bug in "Rolf Turner", who believes in fairies at the end of
his garden, rather than in documentation for file formats.

Duncan Murdoch

> 
> A possible workaround:
> 
> ccc <- c("integer","character",rep(NA,k))
> X   <- read.csv("melvin.csv",colClasses=ccc)
> 
> where "melvin.csv" is the file from which you are attempting to read and
> where k+2 = the number of columns in that file.
> 
> Kludgey, but it might work.
> 
> Another workaround is to specify quote="", but this has the side effect
> of making the 5th column character rather than logical.
> 
> cheers,
> 
> Rolf
>


From r.turner at auckland.ac.nz  Wed Sep 23 02:54:34 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 23 Sep 2015 12:54:34 +1200
Subject: [R] [FORGED] Re:  'R' Software Output Plagiarism
In-Reply-To: <5601F36D.5060906@gmail.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
	<CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
	<4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
	<5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>
	<5601F36D.5060906@gmail.com>
Message-ID: <5601F84A.6030102@auckland.ac.nz>


RIGHT ON!!!  I concur most heartily with the sentiments expressed by Duncan.

cheers,

Rolf

On 23/09/15 12:33, Duncan Murdoch wrote:

> On 22/09/2015 4:06 PM, peter dalgaard wrote:
>> Marc,
>>
>> I don't think Copyright/Intellectual property issues factor into
>> this. Urkund and similar tools are to my knowledge entirely about
>> plagiarism. So the issue would seem to be that the R output is
>> considered identical or nearly indentical to R output in other
>> published orotherwise  submitted material.
>>
>> What puzzles me (except for how a document can be deemed 32%
>> plagiarized in 25% of the text) is whether this includes the
>> numbers and variable names. If those are somehow factored out, then
>> any R regression could be pretty much identical to any other R
>> regression. However, two analyses with similar variable names could
>> happen if they are based on the same cookbook recipe and analyses
>> with similar numerical output come from analyzing the same standard
>> data. Such situations would not necessarily be considered
>> plagiarism (I mean: If you claim that you are analyzing data from
>> experiments that you yourself have performed, and your numbers are
>> exactly identical to something that has been previously published,
>> then it would be suspect. If you analyze something from public
>> sources, someone else might well have done the same thing.).
>
> I don't see why this puzzles you.  A simple explanation is that Urkund
> is incompetent.
>
> Many companies that sell software to university administrations are
> incompetent, because the buyers have been promoted so far beyond their
> competence that they'll buy anything if it is expensive enough.
>
> This isn't uncommon.

<SNIP>


From r.turner at auckland.ac.nz  Wed Sep 23 03:19:24 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 23 Sep 2015 13:19:24 +1200
Subject: [R] retaining characters in a csv file
In-Reply-To: <5601F6EA.4050200@gmail.com>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
	<5601D729.3060502@auckland.ac.nz> <5601F6EA.4050200@gmail.com>
Message-ID: <5601FE1C.2000400@auckland.ac.nz>


On 23/09/15 12:48, Duncan Murdoch wrote:

> On 22/09/2015 6:33 PM, Rolf Turner wrote:

<SNIP>

>> IMHO this is a bug in read.csv().
>
> No, it's a bug in "Rolf Turner", who believes in fairies at the end of
> his garden, rather than in documentation for file formats.

Naturally, I beg to differ.

The documentation for read.csv() refers to a quote character.  Nowhere 
does it mention that quotes only serve to keep commas and white space 
from being interpreted as delimiters.  The usual meaning of quotes in R 
is to enclose character strings and so it is a reasonable assumption 
that this would be their function in this instance.

Before you fly off into some idiotic rant about how one "should never 
make assumptions" consider the fact that if one made no assumptions at 
all one could not get out of bed in the morning.  One has to assume that 
the documentation is reasonably consistent and that any serious 
inconsistencies are drawn to the user's attention.  If one had to read 
the (entire) documentation for each system called upon by a given piece 
of software (apply recursively!) then one would spend one's entire life 
reading documentation and never get any work done.

Although I most definitely do not believe in fairies at the bottom of my 
garden, I am the first to admit that I am not all that bright and could 
have erringly missed something.  HOWEVER Terry Therneau was flummoxed by 
the quirky and counter-intuitive nature of quotes in read.csv(), and Dr. 
Therneau is very bright indeed.

So the fault is not in the user/reader but in the function and its 
documentation.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Wed Sep 23 03:26:58 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 23 Sep 2015 13:26:58 +1200
Subject: [R] retaining characters in a csv file
In-Reply-To: <0BBB21D0-70FF-4978-B1C6-DD4A17F2234D@gmail.com>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
	<5601D729.3060502@auckland.ac.nz>
	<0BBB21D0-70FF-4978-B1C6-DD4A17F2234D@gmail.com>
Message-ID: <5601FFE2.1060207@auckland.ac.nz>

On 23/09/15 11:19, peter dalgaard wrote:
>
>> On 23 Sep 2015, at 00:33 , Rolf Turner <r.turner at auckland.ac.nz> wrote:
>>
>
> [read.csv() doesn't distinguish "123.4" from 123.4]
>
>> IMHO this is a bug in read.csv().
>>
>
> Dunno about that:
>
> pd$ cat ~/tmp/junk.csv
> "1";1
> 2;"2"
> pd$ open !$
> open ~/tmp/junk.csv
>
> And lo and behold, Excel opens with
>
> 1 1
> 2 2
>
> and all cells numeric.

I would say that this phenomenon ("Excel does it") is *overwhelming* 
evidence that it is bad practice!!! :-)

> I don't think the CSV standard (if there is one...) specifies that
> quoted strings are necessarily text.

Duncan Murdoch has pointed out that this is definitely *not* the case.

> I think we have been here before, and found that even if we decide
> that it is a bug (or misfeature), it would be hard to change, because
> the modus operandi of read.* is to first read everything as character
> and _then_ see (in type.convert()) which entries can be converted to
> numeric, logical, etc.

As Arunkumar Srinivasan has pointed out, fread() from the data.table 
package can handle this, so it is *not impossible*.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From JSorkin at grecc.umaryland.edu  Wed Sep 23 03:39:43 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 22 Sep 2015 21:39:43 -0400
Subject: [R] Compare two normal to one normal
In-Reply-To: <alpine.OSX.2.20.1509221521500.1538@charles-berrys-macbook.local>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
	<56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
	<alpine.OSX.2.20.1509221521500.1538@charles-berrys-macbook.local>
Message-ID: <5601CA9F020000CB00139C43@smtp.medicine.umaryland.edu>

Charles,
I am not sure the answer to me question, given a dataset, how can one compare the fit of a model of the fits the data to a mixture of two normal distributions to the fit of a model that uses a single normal distribution, can be based on the glm model you suggest. 


I have used normalmixEM to fit the data to a mixture of two normal curves. The model estimates four (perhaps five) parameters: mu1, sd^2 1, mu2, sd^2, (and perhaps lambda, the mixing proportion. The mixing proportion may not need to be estimated, it may be determined once once specifies mu1, sd^2 1, mu2, and sd^2.) Your model fits the data to a model that contains only the mean, and estimates 2 parameters mu0 and sd0^2.  I am not sure that your model and mine can be considered to be nested. If I am correct I can't compare the log likelihood values from the two models. I  may be wrong. If I am, I should be able to perform a log likelihood test with 2 (or 3, I am not sure which) DFs. Are you suggesting the models are nested? If so, should I use 3 or 2 DFs?


May thanks,
John





John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

>>> "Charles C. Berry" <ccberry at ucsd.edu> 09/22/15 6:23 PM >>>
On Tue, 22 Sep 2015, John Sorkin wrote:

>
> In any event, I still don't know how to fit a single normal distribution 
> and get a measure of fit e.g. log likelihood.
>

Gotta love R:

> y <- rnorm(10)
> logLik(glm(y~1))
'log Lik.' -17.36071 (df=2)

HTH,

Chuck





Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From r.turner at auckland.ac.nz  Wed Sep 23 04:45:47 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 23 Sep 2015 14:45:47 +1200
Subject: [R] Compare two normal to one normal
In-Reply-To: <5601CA9F020000CB00139C43@smtp.medicine.umaryland.edu>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
	<56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
	<alpine.OSX.2.20.1509221521500.1538@charles-berrys-macbook.local>
	<5601CA9F020000CB00139C43@smtp.medicine.umaryland.edu>
Message-ID: <5602125B.1040402@auckland.ac.nz>


On 23/09/15 13:39, John Sorkin wrote:

> Charles, I am not sure the answer to me question, given a dataset,
> how can one compare the fit of a model of the fits the data to a
> mixture of two normal distributions to the fit of a model that uses a
> single normal distribution, can be based on the glm model you
> suggest.
>
> I have used normalmixEM to fit the data to a mixture of two normal
> curves. The model estimates four (perhaps five) parameters: mu1, sd^2
> 1, mu2, sd^2, (and perhaps lambda, the mixing proportion. The mixing
> proportion may not need to be estimated, it may be determined once
> once specifies mu1, sd^2 1, mu2, and sd^2.) Your model fits the data
> to a model that contains only the mean, and estimates 2 parameters
> mu0 and sd0^2.  I am not sure that your model and mine can be
> considered to be nested. If I am correct I can't compare the log
> likelihood values from the two models. I  may be wrong. If I am, I
> should be able to perform a log likelihood test with 2 (or 3, I am
> not sure which) DFs. Are you suggesting the models are nested? If so,
> should I use 3 or 2 DFs?

You are quite correct; there are subtleties involved here.

The one-component model *is* nested in the two-component model, but is 
nested "ambiguously".

(1) The null (single component) model for a mixture distribution is 
ill-defined.  Note that a single component could be achieved either by 
setting the mixing probabilities equal to (1,0) or (0,1) or by setting
mu_1 = mu_2 and sigma_1 = sigma_2.


(2) However you slice it, the parameter values corresponding to the null 
model fall on the *boundary* of the parameter space.

(3) Consequently the asymptotics go to hell in a handcart and the 
likelihood ratio statistic, however you specify the null model, does not 
have an asymptotic chi-squared distribution.

(4) I have a vague idea that there are ways of obtaining a valid 
asymptotic null distribution for the LRT but I am not sufficiently 
knowledgeable to provide any guidance here.

(5) You might be able to gain some insight from delving into the 
literature --- a reasonable place to start would be with "Finite Mixture 
Models" by McLachlan and Peel:

@book{mclachlan2000finite,
   title={Finite Mixture Models, Wiley Series in
          Probability and Statistics},
   author={McLachlan, G and Peel, D},
   year={2000},
   publisher={John Wiley \& Sons, New York}
}

(6) My own approach would be to do "parametric bootstrapping":

* fit (to the real data) the null model and calculate
   the log-likelihood L1, any way you like
* fit the full model and determine the log-likelihood L2
* form the test statistic LRT = 2*(L2 - L1)
* simulate data sets from the fitted parameters for the null model
* for each such simulate data set calculate a test statistic in the
   foregoing manner, obtaining LRT^*_1, ..., LRT^*_N
* the p-value for your test is then

   p = (m+1)/(N+1)

   where m = the number of LRT^*_i values that greater than LRT

The factor of 2 is of course completely unnecessary.  I just put it in 
"by analogy" with the "real", usual, likelihood ratio statistic.

Note that this p-value is *exact* (not an approximation!) --- for any 
value of N --- when interpreted with respect to the "total observation
procedure" of observing both the real and simulated data.  (But see 
below.) That is, the probability, under the null hypothesis, of 
observing a test statistic "as extreme as" what you actually observed is 
*exactly* (m+1)/(N+1).  See e.g.:

@article{Barnard1963,
author = {G. A. Barnard},
title  = {Discussion of ``{T}he spectral analysis of point processes'' 
by {M}. {S}. {B}artlett},
journal = {J. Royal Statist. Soc.},
series  = {B},
volume  = {25},
year = {1963},
pages = {294}
}

or

@article{Hope1968,
author =  {A.C.A. Hope},
title =  {A simplified {M}onte {C}arlo significance test procedure},
journal =  {Journal of the Royal Statistical Society, series {B}},
year =  1968,
volume = 30,
pages = {582--598}
}

Taking N=99 (or 999) is arithmetically convenient.

However I exaggerate when I say that the p-value is exact.  It would be 
exact if you *knew* the parameters of the null model.  Since you have to 
estimate these parameters the test is (a bit?) conservative.  Note that 
the conservatism would be present even if you eschewed the "exact" test 
and an "approximate" test using a (very) large value of N.

Generally conservatism (in this context! :-) ) is deemed to be no bad thing.

cheers,

Rolf Turner

P. S.  I think that the mixing parameter must *always* be estimated. 
I.e. even if you knew mu_1, mu_2, sigma_1 and sigma_2 you would still 
have to estimate "lambda".  So you have 5 parameters in your full model. 
  Not that this is particularly relevant.

R. T.

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From markleeds2 at gmail.com  Wed Sep 23 06:33:00 2015
From: markleeds2 at gmail.com (Mark Leeds)
Date: Wed, 23 Sep 2015 00:33:00 -0400
Subject: [R] Compare two normal to one normal
In-Reply-To: <56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
	<56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
Message-ID: <CAHz+bWaiYLVoTaf9jgofyvZAtM=1Hoo4AKWRTvZ51o0L9egS3Q@mail.gmail.com>

Hi John:  For the log likelihood in the single case, you can just calculate
it directly
using the normal density, so the sum from i = 1 to n of f(x_i, uhat,
sigmahat)
where f(x_i, uhat, sigma hat)  is the density of the normal with that mean
and variance.
so you can use dnorm with log = TRUE.  Of course you need to estimate the
parameters uhat and sigma hat first but for the single normal case, they
are of course just the sample mean and sample variance

Note though: If you going to calculate a log likelihood ratio, make sure
you compare
apples and apples and not apples and oranges in the sense that the
loglikelihood
that comes out of the mixture case may include constants such
1/radical(2pi) etc.
So you need to know EXACTLY how the mixture algorithm is calculating it's
log likelihood.

In fact, it may be better and safer to just calculate the loglikelihood for
the mixture yourself also so sum  from i = 1 to n of [ lambda*f(x_i,
mu1hat, sigma1hat) + (1-lambda)*f(x_i, mu2hat, sigma2hat) By calculating it
yourself and being consistent, you then know that you will be calculating
apples and applies.

As I said earlier, another way is by comparing AICs. in that case, you
calculate it
in both cases and see which AIC is lower. Lower wins and it penalizes for
number of parameters. There are asymptotics required in both the LRT
approach and the AIC
approach so you can pick your poison !!! :).



























On Tue, Sep 22, 2015 at 6:01 PM, John Sorkin <JSorkin at grecc.umaryland.edu>
wrote:

> Bert
> I am surprised by your response. Statistics serves two purposes:
> estimation and hypothesis testing. Sometimes we are fortunate and theory,
> physiology, physics, or something else tell us what is the correct, or
> perhaps I should same most adequate model. Sometimes theory fails us and we
> wish to choose between two competing models. This is my case.  The cell
> sizes may come from one normal distribution (theory 1) or two (theory 2).
> Choosing between the models will help us postulate about physiology. I want
> to use statistics to help me decide between the two competing models, and
> thus inform my understanding of physiology. It is true that statistics
> can't tell me which model is the "correct" or "true" model, but it should
> be able to help me select the more "adequate" or "appropriate" or "closer
> to he truth" model.
>
> In any event, I still don't know how to fit a single normal distribution
> and get a measure of fit e.g. log likelihood.
>
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> >>> Bert Gunter <bgunter.4567 at gmail.com> 09/22/15 4:48 PM >>>
> I'll be brief in my reply to you both, as this is off topic.
>
> So what? All this statistical stuff is irrelevant baloney(and of
> questionable accuracy, since based on asymptotics and strong
> assumptions, anyway) . The question of interest is whether a mixture
> fit better suits the context, which only the OP knows and which none
> of us can answer.
>
> I know that many will disagree with this -- maybe a few might agree --
> but please send all replies, insults, praise, and learned discourse to
> me privately, as I have already occupied more space on the list than
> I should.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
> -- Clifford Stoll
>
>
> On Tue, Sep 22, 2015 at 1:35 PM, Mark Leeds <markleeds2 at gmail.com> wrote:
> > That's true but if he uses some AIC or BIC criterion that penalizes the
> > number of parameters,
> > then he might see something else ? This ( comparing mixtures to not
> mixtures
> > ) is not something I deal with so I'm just throwing it out there.
> >
> >
> >
> >
> > On Tue, Sep 22, 2015 at 4:30 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >>
> >> Two normals will **always** be a better fit than one, as the latter
> >> must be a subset of the former (with identical parameters for both
> >> normals).
> >>
> >> Cheers,
> >> Bert
> >>
> >>
> >> Bert Gunter
> >>
> >> "Data is not information. Information is not knowledge. And knowledge
> >> is certainly not wisdom."
> >> -- Clifford Stoll
> >>
> >>
> >> On Tue, Sep 22, 2015 at 1:21 PM, John Sorkin
> >> <JSorkin at grecc.umaryland.edu> wrote:
> >> > I have data that may be the mixture of two normal distributions (one
> >> > contained within the other) vs. a single normal.
> >> > I used normalmixEM to get estimates of parameters assuming two
> normals:
> >> >
> >> >
> >> > GLUT <- scale(na.omit(data[,"FCW_glut"]))
> >> > GLUT
> >> > mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
> >> > summary(mixmdl)
> >> > plot(mixmdl,which=2)
> >> > lines(density(data[,"GLUT"]), lty=2, lwd=2)
> >> >
> >> >
> >> >
> >> >
> >> >
> >> > summary of normalmixEM object:
> >> > comp 1 comp 2
> >> > lambda 0.7035179 0.296482
> >> > mu -0.0592302 0.140545
> >> > sigma 1.1271620 0.536076
> >> > loglik at estimate: -110.8037
> >> >
> >> >
> >> >
> >> > I would like to see if the two normal distributions are a better fit
> >> > that one normal. I have two problems
> >> > (1) normalmixEM does not seem to what to fit a single normal (even if
> I
> >> > address the error message produced):
> >> >
> >> >
> >> >> mixmdl = normalmixEM(GLUT,k=1)
> >> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k
> =
> >> > k, :
> >> > arbmean and arbvar cannot both be FALSE
> >> >> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
> >> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma, k
> =
> >> > k, :
> >> > arbmean and arbvar cannot both be FALSE
> >> >
> >> >
> >> >
> >> > (2) Even if I had the loglik from a single normal, I am not sure how
> >> > many DFs to use when computing the -2LL ratio test.
> >> >
> >> >
> >> > Any suggestions for comparing the two-normal vs. one normal
> distribution
> >> > would be appreciated.
> >> >
> >> >
> >> > Thanks
> >> > John
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> > John David Sorkin M.D., Ph.D.
> >> > Professor of Medicine
> >> > Chief, Biostatistics and Informatics
> >> > University of Maryland School of Medicine Division of Gerontology and
> >> > Geriatric Medicine
> >> > Baltimore VA Medical Center
> >> > 10 North Greene Street
> >> > GRECC (BT/18/GR)
> >> > Baltimore, MD 21201-1524
> >> > (Phone) 410-605-7119410-605-7119
> >> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> >> >
> >> >
> >> > Confidentiality Statement:
> >> > This email message, including any attachments, is for
> ...{{dropped:12}}
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
> Call
> Send SMS
> Call from mobile
> Add to Skype
> You'll need Skype CreditFree via Skype
>
> *Confidentiality Statement:*
>
> This email message, including any attachments, is for ...{{dropped:10}}


From markleeds2 at gmail.com  Wed Sep 23 06:38:27 2015
From: markleeds2 at gmail.com (Mark Leeds)
Date: Wed, 23 Sep 2015 00:38:27 -0400
Subject: [R] Compare two normal to one normal
In-Reply-To: <CAHz+bWaiYLVoTaf9jgofyvZAtM=1Hoo4AKWRTvZ51o0L9egS3Q@mail.gmail.com>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
	<56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
	<CAHz+bWaiYLVoTaf9jgofyvZAtM=1Hoo4AKWRTvZ51o0L9egS3Q@mail.gmail.com>
Message-ID: <CAHz+bWb0f=A3vXeWMs9EDy63d7irbfvW-EFc25HTPHGLzsUSwg@mail.gmail.com>

John: After I sent what I wrote, I read Rolf's intelligent response. I
didn't realize that
there are boundary issues so yes, he's correct and  my approach is EL
WRONGO. I feel very not good that I just sent that email being that it's
totally wrong. My apologies for noise
and thanks Rolf for the correct response.

Oh,  thing that does still hold in my response is  the AIC approach unless
Rolf
tells us that it's not valid also. I don't see why it wouldn't be though
because you're
not doing a hypothesis test when you go the AIC route.






On Wed, Sep 23, 2015 at 12:33 AM, Mark Leeds <markleeds2 at gmail.com> wrote:

> Hi John:  For the log likelihood in the single case, you can just
> calculate it directly
> using the normal density, so the sum from i = 1 to n of f(x_i, uhat,
> sigmahat)
> where f(x_i, uhat, sigma hat)  is the density of the normal with that mean
> and variance.
> so you can use dnorm with log = TRUE.  Of course you need to estimate the
> parameters uhat and sigma hat first but for the single normal case, they
> are of course just the sample mean and sample variance
>
> Note though: If you going to calculate a log likelihood ratio, make sure
> you compare
> apples and apples and not apples and oranges in the sense that the
> loglikelihood
> that comes out of the mixture case may include constants such
> 1/radical(2pi) etc.
> So you need to know EXACTLY how the mixture algorithm is calculating it's
> log likelihood.
>
> In fact, it may be better and safer to just calculate the loglikelihood
> for the mixture yourself also so sum  from i = 1 to n of [ lambda*f(x_i,
> mu1hat, sigma1hat) + (1-lambda)*f(x_i, mu2hat, sigma2hat) By calculating it
> yourself and being consistent, you then know that you will be calculating
> apples and applies.
>
> As I said earlier, another way is by comparing AICs. in that case, you
> calculate it
> in both cases and see which AIC is lower. Lower wins and it penalizes for
> number of parameters. There are asymptotics required in both the LRT
> approach and the AIC
> approach so you can pick your poison !!! :).
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> On Tue, Sep 22, 2015 at 6:01 PM, John Sorkin <JSorkin at grecc.umaryland.edu>
> wrote:
>
>> Bert
>> I am surprised by your response. Statistics serves two purposes:
>> estimation and hypothesis testing. Sometimes we are fortunate and theory,
>> physiology, physics, or something else tell us what is the correct, or
>> perhaps I should same most adequate model. Sometimes theory fails us and we
>> wish to choose between two competing models. This is my case.  The cell
>> sizes may come from one normal distribution (theory 1) or two (theory 2).
>> Choosing between the models will help us postulate about physiology. I want
>> to use statistics to help me decide between the two competing models, and
>> thus inform my understanding of physiology. It is true that statistics
>> can't tell me which model is the "correct" or "true" model, but it should
>> be able to help me select the more "adequate" or "appropriate" or "closer
>> to he truth" model.
>>
>> In any event, I still don't know how to fit a single normal distribution
>> and get a measure of fit e.g. log likelihood.
>>
>> John
>>
>>
>> John David Sorkin M.D., Ph.D.
>> Professor of Medicine
>> Chief, Biostatistics and Informatics
>> University of Maryland School of Medicine Division of Gerontology and
>> Geriatric Medicine
>> Baltimore VA Medical Center
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>> (Phone) 410-605-7119
>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>
>> >>> Bert Gunter <bgunter.4567 at gmail.com> 09/22/15 4:48 PM >>>
>> I'll be brief in my reply to you both, as this is off topic.
>>
>> So what? All this statistical stuff is irrelevant baloney(and of
>> questionable accuracy, since based on asymptotics and strong
>> assumptions, anyway) . The question of interest is whether a mixture
>> fit better suits the context, which only the OP knows and which none
>> of us can answer.
>>
>> I know that many will disagree with this -- maybe a few might agree --
>> but please send all replies, insults, praise, and learned discourse to
>> me privately, as I have already occupied more space on the list than
>> I should.
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>> -- Clifford Stoll
>>
>>
>> On Tue, Sep 22, 2015 at 1:35 PM, Mark Leeds <markleeds2 at gmail.com> wrote:
>> > That's true but if he uses some AIC or BIC criterion that penalizes the
>> > number of parameters,
>> > then he might see something else ? This ( comparing mixtures to not
>> mixtures
>> > ) is not something I deal with so I'm just throwing it out there.
>> >
>> >
>> >
>> >
>> > On Tue, Sep 22, 2015 at 4:30 PM, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>> >>
>> >> Two normals will **always** be a better fit than one, as the latter
>> >> must be a subset of the former (with identical parameters for both
>> >> normals).
>> >>
>> >> Cheers,
>> >> Bert
>> >>
>> >>
>> >> Bert Gunter
>> >>
>> >> "Data is not information. Information is not knowledge. And knowledge
>> >> is certainly not wisdom."
>> >> -- Clifford Stoll
>> >>
>> >>
>> >> On Tue, Sep 22, 2015 at 1:21 PM, John Sorkin
>> >> <JSorkin at grecc.umaryland.edu> wrote:
>> >> > I have data that may be the mixture of two normal distributions (one
>> >> > contained within the other) vs. a single normal.
>> >> > I used normalmixEM to get estimates of parameters assuming two
>> normals:
>> >> >
>> >> >
>> >> > GLUT <- scale(na.omit(data[,"FCW_glut"]))
>> >> > GLUT
>> >> > mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
>> >> > summary(mixmdl)
>> >> > plot(mixmdl,which=2)
>> >> > lines(density(data[,"GLUT"]), lty=2, lwd=2)
>> >> >
>> >> >
>> >> >
>> >> >
>> >> >
>> >> > summary of normalmixEM object:
>> >> > comp 1 comp 2
>> >> > lambda 0.7035179 0.296482
>> >> > mu -0.0592302 0.140545
>> >> > sigma 1.1271620 0.536076
>> >> > loglik at estimate: -110.8037
>> >> >
>> >> >
>> >> >
>> >> > I would like to see if the two normal distributions are a better fit
>> >> > that one normal. I have two problems
>> >> > (1) normalmixEM does not seem to what to fit a single normal (even
>> if I
>> >> > address the error message produced):
>> >> >
>> >> >
>> >> >> mixmdl = normalmixEM(GLUT,k=1)
>> >> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma,
>> k =
>> >> > k, :
>> >> > arbmean and arbvar cannot both be FALSE
>> >> >> mixmdl = normalmixEM(GLUT,k=1,arbmean=TRUE)
>> >> > Error in normalmix.init(x = x, lambda = lambda, mu = mu, s = sigma,
>> k =
>> >> > k, :
>> >> > arbmean and arbvar cannot both be FALSE
>> >> >
>> >> >
>> >> >
>> >> > (2) Even if I had the loglik from a single normal, I am not sure how
>> >> > many DFs to use when computing the -2LL ratio test.
>> >> >
>> >> >
>> >> > Any suggestions for comparing the two-normal vs. one normal
>> distribution
>> >> > would be appreciated.
>> >> >
>> >> >
>> >> > Thanks
>> >> > John
>> >> >
>> >> >
>> >> >
>> >> >
>> >> >
>> >> >
>> >> >
>> >> >
>> >> >
>> >> > John David Sorkin M.D., Ph.D.
>> >> > Professor of Medicine
>> >> > Chief, Biostatistics and Informatics
>> >> > University of Maryland School of Medicine Division of Gerontology and
>> >> > Geriatric Medicine
>> >> > Baltimore VA Medical Center
>> >> > 10 North Greene Street
>> >> > GRECC (BT/18/GR)
>> >> > Baltimore, MD 21201-1524
>> >> > (Phone) 410-605-7119410-605-7119
>> >> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>> >> >
>> >> >
>> >> > Confidentiality Statement:
>> >> > This email message, including any attachments, is for
>> ...{{dropped:12}}
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>>
>> Call
>> Send SMS
>> Call from mobile
>> Add to Skype
>> You'll need Skype CreditFree via Skype
>>
>> *Confidentiality Statement:*
>>
>> This email message, including any attachments, is for the sole use of the
>> intended recipient(s) and may contain confidential and privileged
>> information. Any unauthorized use, disclosure or distribution is
>> prohibited. If you are not the intended recipient, please contact the
>> sender by reply email and destroy all copies of the original message.
>>
>
>

	[[alternative HTML version deleted]]


From bgnumis at gmail.com  Tue Sep 22 21:55:31 2015
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Tue, 22 Sep 2015 21:55:31 +0200
Subject: [R] Running R on a hosting o server
Message-ID: <CAN25tHQtqzBwvFxyTth+P8t3zVKHTajZ+y4QdeWu4OH+LMoL+w@mail.gmail.com>

Hi all,

Hope I can Explain:

I want to "run" online some function (code written by me) so that this code
"saves" an output file on my server and my html webpage read the file R
plots and save (really manually I would run R function, open Filezilla, and
pass the output png o jpg file).

Is it possible to do this "authomatically" telling R, something like each
15 minutes run this "pru.txt" file, and take save this plot.png and execute
filezilla with this inputs and save the plot in this folder?

Hope you can understand me.

In rmarkdown it is true that output html file but my intention is tu run my
own function and the need is to run my function in R, and run and open
filezilla and deposit the file in the right place.

	[[alternative HTML version deleted]]


From bgnumis at gmail.com  Tue Sep 22 22:10:19 2015
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Tue, 22 Sep 2015 22:10:19 +0200
Subject: [R] Loading data chartseries
Message-ID: <CAN25tHRjK_PsESSGtDF2r2NqVYHPxrJxpQhme=PQ3vOs-a61+A@mail.gmail.com>

Hi all,

I want to plot this data on file.txt that has this format

01/01/2000;970,1877
02/01/2000;970,2224
03/01/2000;969,0336
04/01/2000;958,3023
05/01/2000;952,8527

I?m trying to plot with quantmode with this code but it is not working


X<-read.table("file.txt", col.names=c("Date","LAST"), sep=";",dec=",")




chartSeries(
X,theme="white",
  TA = c(addBBands(200,2))


)

But it says on error

Error in try.xts(x, error = "chartSeries requires an xtsible object") :
  chartSeries requires an xtsible object


How can I run chartseries with my own data?

	[[alternative HTML version deleted]]


From rosita21 at gmail.com  Tue Sep 22 23:17:22 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Tue, 22 Sep 2015 22:17:22 +0100
Subject: [R] doubt with Odds ratio - URGENT HELP NEEDED
Message-ID: <00BC0247-0C23-46CF-8E7A-509186C10FCC@gmail.com>

Dear all,


I?m trying to compute Odds ratio and OR confidence interval.

I?m really naive, sorry for that.


I attach my data and my code.

I?m having lots of errors:

1. Error in data.frame(tas1 = tas.data$tas_d2, tas2 = tas.data$tas_d3, tas3 = tas.data$tas_d4,  : 
  arguments imply differing number of rows: 90, 0

2. Error in data.frame(tas = c(unlist(tas.data[, -8:-6])), time = rep(c(0:4),  : 
  arguments imply differing number of rows: 630, 450, 0

3. Error: object 'tas.data.long' not found

4. Error in data.frame(media = c(mean.dead, mean.alive), standarderror = c(se.dead,  : 
  arguments imply differing number of rows: 14, 10

5. Error in ggplot(summarytas, aes(x = c(c(1:5), c(1:5)), y = mean, colour = discharge)) : 
  object 'summarytas' not found

6. Error in summary(glm(tas.data[, 6] ~ tas.data[, 4], family = binomial(link = probit))) : 
  error in evaluating the argument 'object' in selecting a method for function 'summary': Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1

7. Error in wilcox.test.default(pred[obs == 1], pred[obs == 0], alternative = "great") : 
  not enough (finite) 'x' observations
In addition: Warning message:
In is.finite(x) & apply(pred, 1, f) :
  longer object length is not a multiple of shorter object length


and off course I?m not getting OR.

Nonetheless all this errors, I think I have not been able to compute de code to get OR and OR confidence interval.


Can anyone help me please. It?s really urgent.

PLEASE

THE CODE:

the hospital outcome is discharge.

require(gdata)
library(foreign)
library(nlme)
library(lme4)
library(boot) 
library(MASS)
library(Hmisc)
library(plotrix)
library(verification)
library(mvtnorm)
library(statmod) 
library(epiR)

#########################################################################################
# Data preparation                                                                      #
#########################################################################################

setwd("/Users/RO/Desktop")

	casedata <-read.spss("tas_05112008.sav")
	tas.data<-data.frame(casedata)

	#Delete patients that were not discharged
		tas.data                     <- tas.data[ tas.data$hosp!="si ",]
		tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)

  	tas.data$tas_d2      <- log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA, tas.data$tas_d2))
		tas.data$tas_d3      <- log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA, tas.data$tas_d3))
		tas.data$tas_d4      <- log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA, tas.data$tas_d4))
		tas.data$tas_d5      <- log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA, tas.data$tas_d5))
		tas.data$tas_d6      <- log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA, tas.data$tas_d6))

    tas.data$age      <- ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)


    tas.data                     <-   data.frame(tas1 = tas.data$tas_d2, tas2 = tas.data$tas_d3, 
                                             tas3 = tas.data$tas_d4, tas4 = tas.data$tas_d5, 
                                             tas5 = tas.data$tas_d6, age = tas.data$age, 
                                             discharge = tas.data$resultado.hosp, id.pat=tas.data$ID)

#    tas.data$discharge              <- factor(   tas.data$discharge , levels=c(0,1), labels = c("dead", "alive"))

  #select only cases that have more than 3 tas
    tas.data                      <- tas.data[apply(tas.data[,-8:-6], 1, function(x) sum(!is.na(x)))>2,]


    
    nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients with more than 2 tas measurements

    tas.data.long                 <- data.frame( tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs), age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
                                       id=rep(c(1:n.obs), 5))
    tas.data.long                 <- tas.data.long  [order(tas.data.long$id),]

    age=tas.data$age

##################################################################################################
#PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
##################################################################################################
  mean.alive                      <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
  mean.dead                       <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T) 
  stderr                          <- function(x) sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
  se.alive                        <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
  se.dead                         <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
  summarytas                      <- data.frame (media = c(mean.dead, mean.alive), 
                                      standarderror = c( se.dead, se.alive), discharge = c(rep("dead",5), rep("alive", 5)))


ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) + 
    geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2* standarderror), width=.1) +
  scale_color_manual(values=c("blue", "red")) +
   theme(legend.text=element_text(size=20), axis.text=element_text(size=16), axis.title=element_text(size=20,face="bold")) +
   scale_x_continuous(name="Days") +
  scale_y_continuous(name="log tas") +
  geom_line() +
    geom_point()


library(verification)
prev <- summary(glm(tas.data[,6]~tas.data[,4],family=binomial(link=probit)))
answer = c(prev$coefficients[,1:2])


roc.plot(tas.data[,6], prev, show.thres = FALSE, legen=F )


	modelo<-function (dataainit) 
	
	{

 		#dataa<-tas.data
 		dataa<-dataainit

	 	dataa$ident<-seq(1:90)
  		tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3, 
  		dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
  		time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5), ident=rep(dataa$ident,5))
  
		tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),]) 
 		tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA

 		#mixed model for the longitudinal tas
 		lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days, na.action=na.exclude )
 		
 		#Random intercept and slopes
 		pred.lme<-predict(lme.1)
 		lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1] 
 		lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2] 	
 		selector<-as.numeric(names(lme.intercept)) #to select not NA rows. Apply to the vector in the dataset

 test(dataa$intercept[resultado.hosp==1], dataa$intercept[resultado.hosp==0])
 	
 	print(summary(model.surv1))
 	return(model.surv1$coef)
 	
 }
 	
 	
 	

Best,
RO
 	
  


From victor.maia at fjp.mg.gov.br  Tue Sep 22 21:58:18 2015
From: victor.maia at fjp.mg.gov.br (VictorDelgado)
Date: Tue, 22 Sep 2015 12:58:18 -0700 (PDT)
Subject: [R] Fixing Gale-Shapley Algorithm for R
In-Reply-To: <1329418003915-4395067.post@n4.nabble.com>
References: <1325100810684-4240809.post@n4.nabble.com>
	<1329418003915-4395067.post@n4.nabble.com>
Message-ID: <1442951898355-4712636.post@n4.nabble.com>

Hello R Developers, I have made a new code for this algorithm in R. In the
end I present an very small example with system.time computing.

Gale-Shapley Many-to-One (Note that many are always in Rows):

#############################################
#############################################

gsa.many <- function(m, n, preference.row, preference.col, expand)
{

# m = row number 
# n = col number 
# Remember, rows propose first in this code
# expand = seats per 'school' or column classes
# Note that m > n is needed to algorithm to run
# Comments in Portuguese

loop <- 1 # marca??o do primeiro loop
result <- matrix(0,nrow=m, ncol=n) # Matriz zerada
pos <- NULL # Para ver a posi??o do n?mero mais escolhido
surplus <- 1 # S? para servir de condi??o inicial.

# Core of the Function:

while(any(surplus > 0)){ # Testa a consi??o se o n?mero de alunos ? maior
que o n?mero de vagas

# Obten??o das propostas:

for(i in 1:m){
pos[i] <- which.min(preference.row[i,])
result[i,pos[i]] <- 1}

# Vamos obter quantos alunos requisitam as vagas:

demand <- apply(result, 2, sum)
surplus <- demand - expand # quantos alunos excedentes

# Qual(is) escola(s) ter?(?o) de tirar alunos:

escolas <- which(surplus > 0) 

rejected <- list(NULL) # Vai ser usado p/ descobrir os alunos que precisam
ser retirados:
surplus <- surplus[surplus > 0] # Quantos alunos est?o sobrando

# Vamos criar uma lista auxiliar para o FOR abaixo:

if(length(surplus) > 0){
aux <- list(NULL)

for(i in 1:length(escolas)){
aux[[i]] <- escolas[i]} # ESSA LISTA Coloca a escolas na ordem

for(i in 1:length(escolas)){
proponents <- which(result[,aux[[i]]] == 1) 
decreasing <- sort(preference.col[proponents,aux[[i]]], decreasing = TRUE)
rejected <- decreasing[1:surplus[i]]

retirar <- NULL

for(k in 1:length(rejected)){
retirar[k] <- which(preference.col[,aux[[i]]]==rejected[k])
retirar <- sort(retirar)}

preference.row[retirar,aux[[i]]] <- 2*m
result[retirar,aux[[i]]] <- 0} # FIM DOS DOIS FOR DA ESCOLA!!
} # FIM DO IF

cat("intera??es =",loop,'\n')
flush.console()
loop <- loop+1} # FIM DO WHILE!

# Cospe RESULT

result

} # FIM DA FUN??O! END OF FUNCTION!

#####################################

Comparing Time of previous function with new one:

#####################################

# Setting the Example:

set.seed(51)

m <- 1
n <- 20
S <- NULL

while(m <= 100){
S <- append(S,sample(1:n,n))
m <- m + 1}

m <- m - 1
Pi <- matrix(S, nrow = m, byrow = TRUE)

R <- NULL
n <- 1

while(n <= 20){
R <- append(R,sample(1:m,m))
n <- n + 1}

n <- n - 1
Ps <- matrix(R, nrow=m)

vac <- c(rep(10,5),rep(5,5),rep(4,5),rep(1,5))

######################################


# PREVIOUS CODE

system.time(gsa.many2(m = m, n = n, preference.row = Pi, preference.col =
Ps, first = 1, expand = vac)) # In fact this functions have small changes to
apply a school Vector, please e-mail me for details.

   user  system elapsed 
   0.09    0.05    0.15 

# NEW CODE

system.time(gsa.many(m = m, n = n, preference.row = Pi, preference.col = Ps,
expand = vac))

   user  system elapsed 
   0.03    0.02    0.04 

R Version:

Rx64 3.0.1

My Machine:

i7 3770 CPU @ 3.40 GHz 16GB RAM




-----
Victor Delgado
cedeplar.ufmg.br P.H.D. student
UFOP assistant professor
--
View this message in context: http://r.789695.n4.nabble.com/Gale-Shapley-Algorithm-for-R-tp4240809p4712636.html
Sent from the R help mailing list archive at Nabble.com.


From victor.maia at fjp.mg.gov.br  Tue Sep 22 22:40:17 2015
From: victor.maia at fjp.mg.gov.br (VictorDelgado)
Date: Tue, 22 Sep 2015 13:40:17 -0700 (PDT)
Subject: [R] Top Trading Cycles (TTC) Algorithm in R
Message-ID: <1442954417925-4712649.post@n4.nabble.com>

Hello R users I'm posting here my recent implementation of Top Trading Cycles
Algorithm in R. For more details, look for Shapley and Scarf "On Cores and
Indivisibility" Journal of Mathematical Economics, 1, 23-37.

ttc.many <- function(m, n, preference.row, preference.col,expand)
{

# m = row number 
# n = col number 
# Remember, rows propose first in this code 
# expand = counter of seats per 'school' or column classes 
# Note that m > n is needed to algorithm to run 
# Comments in Portuguese 

##############################################################

students <- 1:m

# Condi??o dos alunos:
# H? alunos na lista?

loop <- 1
result <- matrix(0,nrow=m, ncol=2) # E gerar um resultado

repeat{
ciclo <- NULL
pos <- NULL
s.point <- students[1]

# E vamos armazenar o ciclo em um objeto:

ciclo <- c(ciclo, s.point)

while(all(duplicated(ciclo)==FALSE)){
i.point <- which.min(preference.row[s.point,]) # Para onde o primeiro aluno
da lista aponta:
s.point <- which.min(preference.col[,i.point]) # Para quem essa escola
aponta?
ciclo <- c(ciclo, s.point) # Para quem essa escola aponta formando o ciclo.
					} # FIM DO PEQUENO WHILE!

# Quem ? o duplicado?

dup <- ciclo[which(duplicated(ciclo)==TRUE)]
start <- min(which(ciclo==dup))

# Ciclo apenas com os participantes e sem o repetido ao final:

ciclo <- ciclo[start:(length(ciclo)-1)]

for(i in ciclo){
escola <- which.min(preference.row[i,])
result[i,] <- c(i,escola)
preference.col[i,1:n] <- 2*m

if(expand[escola]>1){
expand[escola] <- expand[escola] - 1}else{
expand[escola] <- expand[escola] - 1
preference.row[,escola] <- 2*m}}

for(k in 1:length(ciclo)){
pos[k] <- which(students==ciclo[k])}
students <- students[-pos]

cat("intera??es =",loop,'\n')
flush.console()
loop <- loop+1
if(length(students) == 0){
break
}
} # FIM DO REPEAT!

result.matrix <- matrix(0, nrow=m, ncol=n)
for(j in result[,1]){
result.matrix[j,result[j,2]] <- 1}
result.matrix

} # FIM DA FUN??O! END OF FUNCTION!

#####################################################

Simple test:

m1 <- c(2,1,3,4)
m2 <- c(1,2,3,4)
m3 <- c(3,2,1,4)
m4 <- c(3,4,1,2)
m5 <- c(1,4,2,3)
m6 <- c(2,3,4,1)
m7 <- c(1,2,3,4)
m8 <- c(1,2,4,3)

n1 <- c(1,2,3,4,5,6,7,8)
n2 <- c(7,6,1,3,2,8,5,4)
n3 <- c(3,5,2,8,1,7,4,6) 
n4 <- c(8,5,6,4,7,1,3,2)

preference.row <- matrix(c(m1,m2,m3,m4,m5,m6,m7,m8), nrow=8, byrow=TRUE)
preference.col <- matrix(c(n1, n2, n3, n4), ncol=4)
exp <- c(2,2,3,3) # Vector of Seats

gsa.many(m=8, n=4, preference.row=preference.row,
preference.col=preference.col, expand=exp))

####### SOME REFERENCES:

A. Abdulkadiroglu, T. Sonmez School Choice: A Mechanism Design Approach.
American Economic Review, 93(3):729?743, 2003.

L. S. Shapley, H. Scarf "On Cores and Indivisibility" Journal of
Mathematical Economics, 1, 23-37.

Klein, T. (2015). matchingMarkets: Structural Estimator and Algorithms for
the Analysis of Stable
Matchings. R package version 0.1-5.

https://cran.r-project.org/web/packages/matchingMarkets/index.html





-----
Victor Delgado
Professor in department of Economics,
UFOP - Univ. Federal de Ouro Preto, Brazil
--
View this message in context: http://r.789695.n4.nabble.com/Top-Trading-Cycles-TTC-Algorithm-in-R-tp4712649.html
Sent from the R help mailing list archive at Nabble.com.


From oliver.barrett at skema.edu  Tue Sep 22 22:51:18 2015
From: oliver.barrett at skema.edu (BARRETT, Oliver)
Date: Tue, 22 Sep 2015 20:51:18 +0000
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <DD9E4BC0-4C92-4F23-9E2D-97AA173507A4@me.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
	<CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
	<4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
	<5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>,
	<DD9E4BC0-4C92-4F23-9E2D-97AA173507A4@me.com>
Message-ID: <VI1PR01MB0813B853413DF081F7C0805892450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>

Hi all,

Thank you so much for your input.

Just to clarify, of the 32% plagiarism detected only 27-8% has come from the regressions but this is expected as the appendix where the regressions are contained is much more dense with text and numbers than the rest of the document.

The other 5% will be my quotations and references but that's normal,

Thanks again, I will be sharing your thoughts with my thesis supervisor.

Cheers,

Oliver

________________________________________
From: Marc Schwartz <marc_schwartz at me.com>
Sent: 22 September 2015 22:27
To: peter dalgaard
Cc: Bert Gunter; BARRETT, Oliver; R-help
Subject: Re: [R] 'R' Software Output Plagiarism

Peter,

Great distinction.

I was leaning in the direction that the "look and feel" of the output (standard wording, table structure, column headings, significance stars and so forth in the output) is similar to whatever Urkund is using as the basis for the comparison and less so on an exact replication (covariates, coefficients, etc.), or nearly so, of prior work.

Thanks,

Marc


> On Sep 22, 2015, at 3:06 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>
> Marc,
>
> I don't think Copyright/Intellectual property issues factor into this. Urkund and similar tools are to my knowledge entirely about plagiarism. So the issue would seem to be that the R output is considered identical or nearly indentical to R output in other published orotherwise  submitted material.
>
> What puzzles me (except for how a document can be deemed 32% plagiarized in 25% of the text) is whether this includes the numbers and variable names. If those are somehow factored out, then any R regression could be pretty much identical to any other R regression. However, two analyses with similar variable names could happen if they are based on the same cookbook recipe and analyses with similar numerical output come from analyzing the same standard data. Such situations would not necessarily be considered plagiarism (I mean: If you claim that you are analyzing data from experiments that you yourself have performed, and your numbers are exactly identical to something that has been previously published, then it would be suspect. If you analyze something from public sources, someone else might well have done the same thing.).
>
> Similarly to John Kane, I think it is necessary to know exactly what sources the text is claimed to be plagiarized from and/or what parts of the text that are being matched by Urkund. If it turns out that Urkund is generating false positives, then this needs to be pointed out to them and to the people basing decisions on it.
>
> -pd
>
>> On 22 Sep 2015, at 18:24 , Marc Schwartz <marc_schwartz at me.com> wrote:
>>
>> Hi,
>>
>> With the usual caveat that I Am Not A Lawyer....and that I am not speaking on behalf of any organization...
>>
>> My guess is that they are claiming that the output of R, simply being copied and pasted verbatim into your thesis constitutes the use of copyrighted output from the software.
>>
>> It is not clear to me that R's output is copyrighted by the R Foundation (or by other parties for CRAN packages), albeit, the source code underlying R is, along with other copyright owner's as apropos. There is some caselaw to support the notion that the output alone is not protected in a similar manner, but that may be country specific.
>>
>> Did you provide any credit to R (see the output of citation() ) in your thesis and indicate that your analyses were performed using R?
>>
>> If R is uncredited, I could see them raising the issue.
>>
>> You might check with your institution's legal/policy folks to see if there is any guidance provided for students regarding the crediting of software used in this manner, especially if that guidance is at no cost to you.
>>
>> Regards,
>>
>> Marc Schwartz
>>
>>
>>> On Sep 22, 2015, at 11:01 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>
>>> 1. It is highly unlikely that we could be of help (unless someone else
>>> has experienced this and knows what happened). You will have to
>>> contact the Urkund people and ask them why their algorithms raised the
>>> flags.
>>>
>>> 2. But of course, the regression methodology is not "your own" -- it's
>>> just a standard tool that you used in your work, which is entirely
>>> legitimate of course.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> Bert Gunter
>>>
>>> "Data is not information. Information is not knowledge. And knowledge
>>> is certainly not wisdom."
>>> -- Clifford Stoll
>>>
>>>
>>> On Tue, Sep 22, 2015 at 7:27 AM, BARRETT, Oliver
>>> <oliver.barrett at skema.edu> wrote:
>>>>
>>>> Dear 'R' community support,
>>>>
>>>>
>>>> I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.
>>>>
>>>>
>>>> I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:
>>>>
>>>> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>>>>  Fed.t.4., data = OLS_CAR, x = TRUE)
>>>>
>>>> Residuals:
>>>>    Min        1Q    Median        3Q       Max
>>>> -0.154587 -0.015961  0.001429  0.017196  0.110907
>>>>
>>>> Coefficients:
>>>>           Estimate Std. Error t value Pr(>|t|)
>>>> (Intercept) -0.001630   0.001763  -0.925   0.3559
>>>> Fed         -0.121595   0.165359  -0.735   0.4627
>>>> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
>>>> Fed.t.2.     0.026529   0.143648   0.185   0.8536
>>>> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
>>>> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
>>>> ---
>>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>>
>>>> Residual standard error: 0.0293 on 304 degrees of freedom
>>>> (20 observations deleted due to missingness)
>>>> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
>>>> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
>>>>
>>>> I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.
>>>>
>>>> I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.
>>>>
>>>> I would like to thank you in advance,
>>>>
>>>> Kind regards,
>>>>
>>>> Oliver Barrett
>>>> (+44) 7341 834 217


From r.turner at auckland.ac.nz  Wed Sep 23 08:22:08 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 23 Sep 2015 18:22:08 +1200
Subject: [R] [FORGED] Re:  Compare two normal to one normal
In-Reply-To: <CAHz+bWb0f=A3vXeWMs9EDy63d7irbfvW-EFc25HTPHGLzsUSwg@mail.gmail.com>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
	<56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
	<CAHz+bWaiYLVoTaf9jgofyvZAtM=1Hoo4AKWRTvZ51o0L9egS3Q@mail.gmail.com>
	<CAHz+bWb0f=A3vXeWMs9EDy63d7irbfvW-EFc25HTPHGLzsUSwg@mail.gmail.com>
Message-ID: <56024510.90801@auckland.ac.nz>

On 23/09/15 16:38, Mark Leeds wrote:
> John: After I sent what I wrote, I read Rolf's intelligent response. I
> didn't realize that
> there are boundary issues so yes, he's correct and  my approach is EL
> WRONGO. I feel very not good that I just sent that email being that it's
> totally wrong. My apologies for noise
> and thanks Rolf for the correct response.
>
> Oh,  thing that does still hold in my response is  the AIC approach unless
> Rolf
> tells us that it's not valid also. I don't see why it wouldn't be though
> because you're
> not doing a hypothesis test when you go the AIC route.

<SNIP>

I am no expert on this, but I would be uneasy applying AIC to such 
problems without having a very close look at the literature on the 
subject.  I'm pretty sure that there *are* regularity conditions that 
must be satisfied in order that AIC should give you a "valid" basis for 
comparison of models.

AIC has most appeal, and is mostly used (in my understanding) in 
settings where there is a multiplicity of models, whereby the multiple 
comparisons problem causes hypothesis testing to lose its appeal. 
Correspondingly AIC has little appeal in a setting in which a single 
hypothesis test is applicable.

I could be wrong about this; as I said, I am no expert.  Perhaps younger 
and wiser heads will chime in and correct me.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From roryrwilson at yahoo.ca  Wed Sep 23 08:37:38 2015
From: roryrwilson at yahoo.ca (Rory Wilson)
Date: Wed, 23 Sep 2015 06:37:38 +0000 (UTC)
Subject: [R] [FORGED] Error from lme4: "Error: (p <- ncol(X)) == ncol(Y)
 is not TRUE"
In-Reply-To: <5601C76C.4000107@auckland.ac.nz>
References: <5601C76C.4000107@auckland.ac.nz>
Message-ID: <1481817059.92651.1442990258068.JavaMail.yahoo@mail.yahoo.com>

Hi Rolf,Yes, a reprodicuble example would be good - but it appears to be trouble with something within the data, and I cannot upload all the data here. Jean indicated the issue could be with NAs in the data, and I am inclined to agree. I will look more into that angle and see if something becomes apparent.Thanks for your help!Rory
      From: Rolf Turner <r.turner at auckland.ac.nz>
 To: Rory Wilson <roryrwilson at yahoo.ca>; "r-help at r-project.org" <r-help at r-project.org> 
 Sent: Tuesday, September 22, 2015 11:26 PM
 Subject: Re: [FORGED] [R] Error from lme4: "Error: (p <- ncol(X)) == ncol(Y) is not TRUE"
   



On 23/09/15 01:18, Rory Wilson wrote:

> Hello all, I am trying to run a random intercept model using lme4.
> The random effect is a factor of 29 possibilities, making a model
> with one random effect (one level). It is just a linear model. There
> are 713 observations. However, when trying to run the model I receive
> the error "Error: (p <- ncol(X)) == ncol(Y) is not TRUE", a search
> for which reveals somewhat surprisingly little. Has anyone seen this
> before? Note that if I simply change the random effect into a fixed
> effect and use lm, the model works perfectly.Thank you!

[Caveat:? I really find the syntax of lmer() incomprehensible, so my 
example below could be a load of dingos' kidneys.]

I think a reproducible example (as specified by the posting guide) is 
needed here.? When I do:

set.seed(42)
f <- factor(sample(1:29,713,TRUE))
x <- seq(0,1,length=713)
y <- rnorm(713)
require(lme4)
fit <- lmer(y ~ x + (1|f))

I get a reasonable (???) looking result and no error messages.

cheers,

Rolf Turner


  
	[[alternative HTML version deleted]]


From josh.m.ulrich at gmail.com  Wed Sep 23 10:00:10 2015
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 23 Sep 2015 03:00:10 -0500
Subject: [R] Loading data chartseries
In-Reply-To: <CAN25tHRjK_PsESSGtDF2r2NqVYHPxrJxpQhme=PQ3vOs-a61+A@mail.gmail.com>
References: <CAN25tHRjK_PsESSGtDF2r2NqVYHPxrJxpQhme=PQ3vOs-a61+A@mail.gmail.com>
Message-ID: <CAPPM_gRxb2P3YDv+P5k0_iF3Wdgo-jhqf6zTA6pE_bRoFMFZ1A@mail.gmail.com>

On Tue, Sep 22, 2015 at 3:10 PM, bgnumis bgnum <bgnumis at gmail.com> wrote:
> Hi all,
>
> I want to plot this data on file.txt that has this format
>
> 01/01/2000;970,1877
> 02/01/2000;970,2224
> 03/01/2000;969,0336
> 04/01/2000;958,3023
> 05/01/2000;952,8527
>
> I?m trying to plot with quantmode with this code but it is not working
>
>
> X<-read.table("file.txt", col.names=c("Date","LAST"), sep=";",dec=",")
>
>
>
>
> chartSeries(
> X,theme="white",
>   TA = c(addBBands(200,2))
>
>
> )
>
> But it says on error
>
> Error in try.xts(x, error = "chartSeries requires an xtsible object") :
>   chartSeries requires an xtsible object
>
>
> How can I run chartseries with my own data?
>
Your data need to be in an xts object, or something that can be
converted to an xts object via as.xts().  I recommend the former.

Lines <-
"01/01/2000;970,1877
02/01/2000;970,2224
03/01/2000;969,0336
04/01/2000;958,3023
05/01/2000;952,8527"

library(quantmod)
X <- as.xts(read.zoo(text=Lines, sep=";", dec=",", format="%m/%d/%Y"))
chartSeries(X, theme="white", TA="addBBands(200,2)")

>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From maechler at stat.math.ethz.ch  Wed Sep 23 12:32:06 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 23 Sep 2015 12:32:06 +0200
Subject: [R] Unexpected/undocumented behavior of 'within': dropping
 variable names that start with '.'
In-Reply-To: <CABdHhvECWtEmDAx6ZKsjJDjfJ8NPAmb+SqdwkVu3aC=nT6z3_g@mail.gmail.com>
References: <55FED56C.10204@gmail.com>
	<CABdHhvECWtEmDAx6ZKsjJDjfJ8NPAmb+SqdwkVu3aC=nT6z3_g@mail.gmail.com>
Message-ID: <22018.32678.392611.953655@stat.math.ethz.ch>

>>>>> Hadley Wickham <h.wickham at gmail.com>
>>>>>     on Sun, 20 Sep 2015 14:23:43 -0400 writes:

    > The problem is that within.data.frame calls as.list.environment with
    > the default value of all.names = FALSE. I doubt this is a deliberate
    > feature, and is more likely to be a minor oversight.

Indeed;
Thank you, Hadley (and Brian)!

It is fixed now in R-devel  .... and will be ported to R-patched
probably tomorrow.

Martin

    > Hadley

    > On Sun, Sep 20, 2015 at 11:49 AM, Brian <zenlines at gmail.com> wrote:
    >> Dear List,
    >> 
    >> Somewhere I missed something, and now I'm really missing something!
    >> 
    >>> d.f <- data.frame(.id = c(TRUE, FALSE, TRUE), dummy = c(1, 2, 3), a =
    >> c(1, 2, 3), b = c(1, 2, 3) + 1)
    >> > within(d.f, {d = a + b})
    >> dummy a b d
    >> 1     1 1 2 3
    >> 2     2 2 3 5
    >> 3     3 3 4 7
    >> > d.f <- data.frame(.id = c(TRUE, FALSE, TRUE), .dummy = c(1, 2, 3), a
    >> = c(1, 2, 3), b = c(1, 2, 3) + 1)
    >> > within(d.f, {d = a + b})
    >> a b d
    >> 1 1 2 3
    >> 2 2 3 5
    >> 3 3 4 7
    >> 
    >> Could somebody please explain to me why this does this? I think could be
    >> considered a feature (for lots of calculations within a data frame you
    >> don't have to remove all extra variables at the end).  I just wish it
    >> was documented.
    >> 
    >> Cheers,
    >> Brian
    >> 
    >> 
    >> sessionInfo()
    >> R version 3.1.0 (2014-04-10)
    >> Platform: x86_64-pc-linux-gnu (64-bit)
    >> 
    >> locale:
    >> [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
    >> [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
    >> [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
    >> [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
    >> [9] LC_ADDRESS=C               LC_TELEPHONE=C
    >> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
    >> 
    >> attached base packages:
    >> [1] splines   grid      stats     graphics  grDevices utils     datasets
    >> [8] methods   base
    >> 
    >> other attached packages:
    >> [1] scales_0.2.4       plyr_1.8.3         reshape2_1.4
    >> ccchDataProc_0.7
    >> [5] ccchTools_0.6      xtable_1.7-4       tables_0.7.79      Hmisc_3.14-5
    >> [9] Formula_1.1-2      survival_2.37-7    ggplot2_1.0.1
    >> IDPmisc_1.1.17
    >> [13] lattice_0.20-29    myRplots_1.1       myRtools_1.2       meteoconv_0.1
    >> [17] pixmap_0.4-11      RColorBrewer_1.0-5 maptools_0.8-30    sp_1.1-1
    >> [21] mapdata_2.2-3      mapproj_1.2-2      maps_2.3-9         chron_2.3-45
    >> [25] MASS_7.3-35
    >> 
    >> loaded via a namespace (and not attached):
    >> [1] acepack_1.3-3.3     cluster_1.15.2      colorspace_1.2-4
    >> [4] compiler_3.1.0      data.table_1.9.4    digest_0.6.4
    >> [7] foreign_0.8-61      gtable_0.1.2        labeling_0.3
    >> [10] latticeExtra_0.6-26 munsell_0.4.2       nnet_7.3-8
    >> [13] proto_0.3-10        Rcpp_0.12.0         rpart_4.1-8
    >> [16] stringr_0.6.2       tools_3.1.0
    >> > within
    >> function (data, expr, ...)
    >> UseMethod("within")
    >> <bytecode: 0x26d32c8>
    >> <environment: namespace:base>
    >> 
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.



    > -- 
    > http://had.co.nz/

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From chunjing.liu at chetuobang.com  Wed Sep 23 09:14:18 2015
From: chunjing.liu at chetuobang.com (chunjing.liu at chetuobang.com)
Date: Wed, 23 Sep 2015 15:14:18 +0800
Subject: [R] help:parLapply error
References: <mailman.5.1442916002.27623.r-help@r-project.org>
Message-ID: <2015092315141841145214@chetuobang.com>

  Dear all:         
          I use R(version 3.2.2)'s parallel processing  code as follows:
    cat("parall cal rtic:",nListLink, ",params:",length(arimaPreParams),"start...","\n")
       cl <- makeCluster(getOption("cl.cores", 12));
       system.time({
                   res <- parLapply(cl,1:nListLink,linkProcess)
         });
      stopCluster(cl);
The following error was reported:
    Error in checkForRemoteErrors(val) : 
    one node produced an error: root finding code failed
   Calls: pre_channel ... clusterApply -> staticClusterApply -> checkForRemoteErrors
I want to know what a mistake it is,and What caused it.I looked up the old mail, did not find the answer.
Please help answer.
   
     Best regards,

chunjing.liu at chetuobang.com
    
   
       
    


	[[alternative HTML version deleted]]


From marionrkeast at gmail.com  Wed Sep 23 11:52:45 2015
From: marionrkeast at gmail.com (Marion Keast)
Date: Wed, 23 Sep 2015 02:52:45 -0700
Subject: [R] Issues fitting Gompertz-Makeham model to mortality data
Message-ID: <CACE+i20e0aJoDB4Cp+V20Sr=HVvmT2c0GGXGQSkfNPtdf2dJVw@mail.gmail.com>

Good evening,

I have a very basic knowledge of how to use R and am struggling with my
code to fit a Gompertz-Makeham model to data. The code and error message is
as follows:

> data <- hmd.mx("AUS","mkeast93 at hotmail.com","1Mrkqazwsx", "country")
Warning message:
In hmd.mx("AUS", "mkeast93 at hotmail.com", "1Mrkqazwsx", "country") :
  NAs introduced by coercion
> summary(data)
Mortality data for country
    Series: female male total
    Years: 1921 - 2011
    Ages:  0 - 110
> m.qx <- data$rate$male[1:111,1:90]
> f.qx <-data$rate$female[1:111,1:90]
> age<-data$age[21:90]
>
> n=111
> m=90
> beta1=(1:m)*0
> beta2=(1:m)*0
> beta3=(1:m)*0
> qhat.mx=array(0,dim=c(n,m))
> qhat.fx=array(0,dim=c(n,m))
>
> #j is ages 0 - 110
> #i is years 1921 - 2011
>
> #MALES
> for (i in 1:m)
+ {
+     model1 <-fitGM(,m.qx[1:n,i])
+     m1 <- model1[1]
+     m2 <- model1[2]
+     m3 <- model1[3]
+     beta1[i]=m1
+     beta2[i]=m2
+     beta3[i]=m3
+     qhat.mx[,i]=GompertzMakeham(beta1[i],beta2[i],beta3[i],age)
+ }
Error in qhat.mx[, i] = GompertzMakeham(beta1[i], beta2[i], beta3[i],  :
  number of items to replace is not a multiple of replacement length
>
> MAPE.m=sum(abs(qhat.mx-m.qx)/m.qx)/(n*m)
> MaPE.m=sum((qhat.mx-m.qx)/m.qx)/(n*m)

Please advise in regards to fixing errors.
Furthermore, if I would like to fit MAPE to each individual age instead of
the population, is there a loop I can use to achieve this.

Thank you very much for your time and assistance.

Marion

	[[alternative HTML version deleted]]


From paolo.canal at iusspavia.it  Wed Sep 23 12:46:46 2015
From: paolo.canal at iusspavia.it (Paolo Canal)
Date: Wed, 23 Sep 2015 12:46:46 +0200
Subject: [R] Appropriate specification of random effects structure for
 EEG/ERP data: including Channels or not?
Message-ID: <56028316.2050004@iusspavia.it>

Dear r-help list,

I work with EEG/ERP data and this is the first time I am using LMM to 
analyze my data (using lme4).
The experimental design is a 2X2: one manipulated factor is agreement, 
the other is noun (agreement being within subjects and items, and noun 
being within subjects and between items).

The data matrix is 31 subjects * 160 items * 33 channels. In ERP 
research, the distribution of the EEG amplitude differences (in a time 
window of interest) are important, and we care about knowing whether a 
negative difference is occurring in Parietal or Frontal electrodes. At 
the same time information from single channel is often too noisy and 
channels are organized in topographic factors for evaluating differences 
in distribution. In the present case I have assigned each channel to one 
of three levels of two factors, i.e., Longitude (Anterior, Central, 
Parietal) and Medial (Left, Midline, Right): for instance, one channel 
is Anterior and Left. With traditional ANOVAs channels from the same 
level of topographic factors are averaged before variance is evaluated 
and this also has the benefit of reducing the noise picked up by the 
electrodes.

I have troubles in deciding the random structure of my model. Very few 
examples on LMM on ERP data exist (e.g., Newman, Tremblay, Nichols, 
Neville & Ullman, 2012) and little detail is provided about the 
treatment of channel. I feel it is a tricky term but very important to 
optimize fit. Newman et al say "data from each electrode within an ROI 
were treated as repeated measures of that ROI". In Newman et al, the 
ROIs are the 9 regions deriving from Longitude X Medial (Anterior-Left, 
Anterior-Midline, Anterior-Right, Central-Left ... and so on), so in a 
way they treated each ROI separately and not according to the relevant 
dimensions of Longitude and Medial.

We used the following specifications in lmer:

[fixed effects specification: ?V ~ Agreement * Noun * Longitude * Medial 
* (cov1 + cov2 + cov3 + cov4)] (the terms within brackets are a series 
of individual covariates, most of which are continuous variables)

[random effects specification: (1+Agreement*Type of Noun | subject) + 
(1+Agreement | item) + (1|longitude:medial:channel)]

What I care the most about is the last term 
(1|longitude:medial:channel). I chose this specification because I 
thought that allowing each channel to have different intercepts in the 
random structure would affect the estimation of the topographic fixed 
effects (Longitude and Medial) in which channel is nested. Unfortunately 
a reviewer commented that since "channel is not included in the fixed 
effects I would probably leave that out".

But each channel is a repeated measure of the eeg amplitude inside the 
two topographic factors, and random terms do not have to be in the fixed 
structure, otherwise we would also include subjects and items in the 
fixed effects structure. So I kind of feel that including channels as 
random effect is correct, and having them nested in longitude:medial 
allows to relax the assumption that the effect in the EEG has always the 
same longitude:medial distribution. But I might be wrong.

I thus tested differences in fit (ML) with anova() between 
(1|longitude:medial:channel) and the same model without the term, and a 
third model with the model with a simpler (1|longitude:medial).

Fullmod vs Nochannel:

Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq)
modnoch 119 969479 970653 -484621 969241
fullmod 120 968972 970156 -484366 968732 508.73 1 < 2.2e-16 ***

Differences in fit is remarkable (no variance components with estimates 
close to zero; no correlation parameters with values close to ?1).

Fullmod vs SimplerMod:

   Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq)

fullmod 120 968972 970156 -484366 968732
simplermod 120 969481 970665 -484621 969241 0 0 1

Here the number of parameters to estimate in fullmod and simplermod is 
the same but the increase in fit is very consistent (-509 BIC). So I 
guess although the chisquare is not significant we do have a string 
increase in fit. As I understand this, a model with better fit will find 
more accurate estimates, and I would be inclined to keep the fullmod 
random structure.

But perhaps I am missing something or I am doing something wrong. Which 
is the correct random structure to use?

Feedbacks are very much appreciated. I often find answers in the list, 
and this is the first time I post a question.
Thanks,
Paolo










	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Wed Sep 23 13:02:04 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 23 Sep 2015 12:02:04 +0100
Subject: [R] doubt with Odds ratio - URGENT HELP NEEDED
In-Reply-To: <00BC0247-0C23-46CF-8E7A-509186C10FCC@gmail.com>
References: <00BC0247-0C23-46CF-8E7A-509186C10FCC@gmail.com>
Message-ID: <560286AC.4000808@dewey.myzen.co.uk>

Dear Rosa

It would help if you posted the error messages where they occur so that 
we can see which of your commands caused which error. However see 
comment inline below.

On 22/09/2015 22:17, Rosa Oliveira wrote:
> Dear all,
>
>
> I?m trying to compute Odds ratio and OR confidence interval.
>
> I?m really naive, sorry for that.
>
>
> I attach my data and my code.
>
> I?m having lots of errors:
>
> 1. Error in data.frame(tas1 = tas.data$tas_d2, tas2 = tas.data$tas_d3, tas3 = tas.data$tas_d4,  :
>    arguments imply differing number of rows: 90, 0
>

At least one of tas_d2, tas_d3, tas_d4 does not exist

I suggest fixing that one and hoping the rest go away.

> 2. Error in data.frame(tas = c(unlist(tas.data[, -8:-6])), time = rep(c(0:4),  :
>    arguments imply differing number of rows: 630, 450, 0
>
> 3. Error: object 'tas.data.long' not found
>
> 4. Error in data.frame(media = c(mean.dead, mean.alive), standarderror = c(se.dead,  :
>    arguments imply differing number of rows: 14, 10
>
> 5. Error in ggplot(summarytas, aes(x = c(c(1:5), c(1:5)), y = mean, colour = discharge)) :
>    object 'summarytas' not found
>
> 6. Error in summary(glm(tas.data[, 6] ~ tas.data[, 4], family = binomial(link = probit))) :
>    error in evaluating the argument 'object' in selecting a method for function 'summary': Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1
>
> 7. Error in wilcox.test.default(pred[obs == 1], pred[obs == 0], alternative = "great") :
>    not enough (finite) 'x' observations
> In addition: Warning message:
> In is.finite(x) & apply(pred, 1, f) :
>    longer object length is not a multiple of shorter object length
>
>
> and off course I?m not getting OR.
>
> Nonetheless all this errors, I think I have not been able to compute de code to get OR and OR confidence interval.
>
>
> Can anyone help me please. It?s really urgent.
>
> PLEASE
>
> THE CODE:
>
> the hospital outcome is discharge.
>
> require(gdata)
> library(foreign)
> library(nlme)
> library(lme4)
> library(boot)
> library(MASS)
> library(Hmisc)
> library(plotrix)
> library(verification)
> library(mvtnorm)
> library(statmod)
> library(epiR)
>
> #########################################################################################
> # Data preparation                                                                      #
> #########################################################################################
>
> setwd("/Users/RO/Desktop")
>
> 	casedata <-read.spss("tas_05112008.sav")
> 	tas.data<-data.frame(casedata)
>
> 	#Delete patients that were not discharged
> 		tas.data                     <- tas.data[ tas.data$hosp!="si ",]
> 		tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>
>    	tas.data$tas_d2      <- log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA, tas.data$tas_d2))
> 		tas.data$tas_d3      <- log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA, tas.data$tas_d3))
> 		tas.data$tas_d4      <- log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA, tas.data$tas_d4))
> 		tas.data$tas_d5      <- log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA, tas.data$tas_d5))
> 		tas.data$tas_d6      <- log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA, tas.data$tas_d6))
>
>      tas.data$age      <- ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>
>
>      tas.data                     <-   data.frame(tas1 = tas.data$tas_d2, tas2 = tas.data$tas_d3,
>                                               tas3 = tas.data$tas_d4, tas4 = tas.data$tas_d5,
>                                               tas5 = tas.data$tas_d6, age = tas.data$age,
>                                               discharge = tas.data$resultado.hosp, id.pat=tas.data$ID)
>
> #    tas.data$discharge              <- factor(   tas.data$discharge , levels=c(0,1), labels = c("dead", "alive"))
>
>    #select only cases that have more than 3 tas
>      tas.data                      <- tas.data[apply(tas.data[,-8:-6], 1, function(x) sum(!is.na(x)))>2,]
>
>
>
>      nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients with more than 2 tas measurements
>
>      tas.data.long                 <- data.frame( tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs), age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>                                         id=rep(c(1:n.obs), 5))
>      tas.data.long                 <- tas.data.long  [order(tas.data.long$id),]
>
>      age=tas.data$age
>
> ##################################################################################################
> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
> ##################################################################################################
>    mean.alive                      <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>    mean.dead                       <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>    stderr                          <- function(x) sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>    se.alive                        <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>    se.dead                         <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>    summarytas                      <- data.frame (media = c(mean.dead, mean.alive),
>                                        standarderror = c( se.dead, se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>
>
> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) +
>      geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2* standarderror), width=.1) +
>    scale_color_manual(values=c("blue", "red")) +
>     theme(legend.text=element_text(size=20), axis.text=element_text(size=16), axis.title=element_text(size=20,face="bold")) +
>     scale_x_continuous(name="Days") +
>    scale_y_continuous(name="log tas") +
>    geom_line() +
>      geom_point()
>
>
> library(verification)
> prev <- summary(glm(tas.data[,6]~tas.data[,4],family=binomial(link=probit)))
> answer = c(prev$coefficients[,1:2])
>
>
> roc.plot(tas.data[,6], prev, show.thres = FALSE, legen=F )
>
>
> 	modelo<-function (dataainit)
> 	
> 	{
>
>   		#dataa<-tas.data
>   		dataa<-dataainit
>
> 	 	dataa$ident<-seq(1:90)
>    		tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>    		dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>    		time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5), ident=rep(dataa$ident,5))
>
> 		tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>   		tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>
>   		#mixed model for the longitudinal tas
>   		lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days, na.action=na.exclude )
>   		
>   		#Random intercept and slopes
>   		pred.lme<-predict(lme.1)
>   		lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>   		lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2] 	
>   		selector<-as.numeric(names(lme.intercept)) #to select not NA rows. Apply to the vector in the dataset
>
>   test(dataa$intercept[resultado.hosp==1], dataa$intercept[resultado.hosp==0])
>   	
>   	print(summary(model.surv1))
>   	return(model.surv1$coef)
>   	
>   }
>   	
>   	
>   	
>
> Best,
> RO
>   	
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From ryszard at czerminski.net  Wed Sep 23 13:13:11 2015
From: ryszard at czerminski.net (=?UTF-8?Q?Ryszard_Czermi=C5=84ski?=)
Date: Wed, 23 Sep 2015 07:13:11 -0400
Subject: [R] Error: pandoc version 1.12.3 or higher is required and was not
	found
Message-ID: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>

I am trying to use R Markdown, but call to render() gives me an error:
Error: pandoc version 1.12.3 or higher is required and was not found.

As I understand [http://yihui.name/knitr/demo/pandoc/] pandoc is a function
defined in knitr, which I have installed and it has pandoc() function
defined.

Looks like some version incompatibility issue, but I do not really know how
to resolve it.
Do I need to go to older R version to use it?

I would appreciate your help.

Best regards,
Ryszard

> sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.10.3 (Yosemite)
other attached packages: knitr_1.11
[...]

Ryszard Czerminski
508-358-6328
ryszard at czerminski.net
LinkedIn.com/in/Ryszard.Czerminski

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Wed Sep 23 13:44:24 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 23 Sep 2015 06:44:24 -0500
Subject: [R] Error: pandoc version 1.12.3 or higher is required and was
 not found
In-Reply-To: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>
References: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>
Message-ID: <15EDEDA6-924C-4D62-81EC-6C3322B6633B@me.com>


> On Sep 23, 2015, at 6:13 AM, Ryszard Czermi?ski <ryszard at czerminski.net> wrote:
> 
> I am trying to use R Markdown, but call to render() gives me an error:
> Error: pandoc version 1.12.3 or higher is required and was not found.
> 
> As I understand [http://yihui.name/knitr/demo/pandoc/] pandoc is a function
> defined in knitr, which I have installed and it has pandoc() function
> defined.
> 
> Looks like some version incompatibility issue, but I do not really know how
> to resolve it.
> Do I need to go to older R version to use it?
> 
> I would appreciate your help.
> 
> Best regards,
> Ryszard
> 
>> sessionInfo()
> R version 3.2.2 (2015-08-14)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.10.3 (Yosemite)
> other attached packages: knitr_1.11
> [...]
> 
> Ryszard Czerminski
> 508-358-6328
> ryszard at czerminski.net
> LinkedIn.com/in/Ryszard.Czerminski


Did you actually install pandoc?

As per the page that you link to above:

"Please follow the instructions on the Pandoc website to install it."

The link in the above sentence is:

  http://johnmacfarlane.net/pandoc/

Regards,

Marc Schwartz


From john.archie.mckown at gmail.com  Wed Sep 23 14:12:52 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Wed, 23 Sep 2015 07:12:52 -0500
Subject: [R] Running R on a hosting o server
In-Reply-To: <CAN25tHQtqzBwvFxyTth+P8t3zVKHTajZ+y4QdeWu4OH+LMoL+w@mail.gmail.com>
References: <CAN25tHQtqzBwvFxyTth+P8t3zVKHTajZ+y4QdeWu4OH+LMoL+w@mail.gmail.com>
Message-ID: <CAAJSdjjnyuMGFs314mPokwMe8DwbZEzKUUFNG-0TuK+a5vVkXQ@mail.gmail.com>

On Tue, Sep 22, 2015 at 2:55 PM, bgnumis bgnum <bgnumis at gmail.com> wrote:

> Hi all,
>
> Hope I can Explain:
>
> I want to "run" online some function (code written by me) so that this code
> "saves" an output file on my server and my html webpage read the file R
> plots and save (really manually I would run R function, open Filezilla, and
> pass the output png o jpg file).
>
> Is it possible to do this "authomatically" telling R, something like each
> 15 minutes run this "pru.txt" file, and take save this plot.png and execute
> filezilla with this inputs and save the plot in this folder?
>

?In Linux (or any UNIX like system), you can schedule tasks to be run by
using "cron" (do a "man crontab" for some information, if you need to). So,
if you can make a "shell script" which does all of your work without user
input, then you can use "cron" to schedule it periodically, every "n"
minutes, daily, weekly on ???day, monthly, etc. I don't know Filezilla, so
I don't know what you are really doing with it.

Windows has a similar scheduler to run "bat" files or "Power Shell"
scripts.? I don't _DO_ Windows! <Start> -> <Administrative Tools> -> <Task
Scheduler>

Mac OSX - not a fanboy, try somebody else. <grin/>



>
> Hope you can understand me.
>
> In rmarkdown it is true that output html file but my intention is tu run my
> own function and the need is to run my function in R, and run and open
> filezilla and deposit the file in the right place.
>
>
?If you are using Filezilla to copy a file, where is it being copied to? In
UNIX, I'd see if I could use just the plain "cp" command (for local, NFS,
or CIFS attached places) or either the "ftp" or "scp" command to copy to a
different server. In Windows the "copy" command could be used to copy the
file to a different folder locally or on a "share" (Windows "share" is,
more or less, the same as UNIX CIFS, if you're interested)?. Again, if this
needs to go to some other server, then a scripted "ftp" should work. One of
my co-workers does this.

-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From therneau at mayo.edu  Wed Sep 23 14:57:53 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Wed, 23 Sep 2015 07:57:53 -0500
Subject: [R] retaining characters in a csv file
In-Reply-To: <5601D729.3060502@auckland.ac.nz>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
	<5601D729.3060502@auckland.ac.nz>
Message-ID: <c10f8b$1gbn9d@ironport10.mayo.edu>

Thanks for all for the comments, I hadn't intended to start a war.

My summary:
   1. Most important: I wasn't missing something obvious.  This is always my first 
suspicion when I submit something to R-help, and it's true more often than not.

   2. Obviously (at least it is now), the CSV standard does not specify that quotes should 
force a character result.  R is not "wrong".  Wrt to using what Excel does as litmus test, 
I consider that to be totally uninformative about standards: neither pro (like Duncan) or 
anti (like Rolf), but simply irrelevant.  (Like many MS choices.)

   3. I'll have to code in my own solution, either pre-scan the first few lines to create 
a colClasses, or use read_csv from the readr library (if there are leading zeros it keeps 
the string as character, which may suffice for my needs), or something else.

   4. The source of the data is a "text/csv" field coming from an http POST request.  This 
is an internal service on an internal Mayo server and coded by our own IT department; this 
will not be the first case where I have found that their definition of "csv" is not quite 
standard.

Terry T.



> On 23/09/15 10:00, Therneau, Terry M., Ph.D. wrote:
>> I have a csv file from an automatic process (so this will happen
>> thousands of times), for which the first row is a vector of variable
>> names and the second row often starts something like this:
>>
>> 5724550,"000202075214",2005.02.17,2005.02.17,"F", .....
>>
>> Notice the second variable which is
>>        a character string (note the quotation marks)
>>        a sequence of numeric digits
>>        leading zeros are significant
>>
>> The read.csv function insists on turning this into a numeric.  Is there
>> any simple set of options that
>> will turn this behavior off?  I'm looking for a way to tell it to "obey
>> the bloody quotes" -- I still want the first, third, etc columns to
>> become numeric.  There can be more than one variable like this, and not
>> always in the second position.
>>
>> This happens deep inside the httr library; there is an easy way for me
>> to add more options to the read.csv call but it is not so easy to
>> replace it with something else.


From murdoch.duncan at gmail.com  Wed Sep 23 16:11:50 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 23 Sep 2015 10:11:50 -0400
Subject: [R] Error: pandoc version 1.12.3 or higher is required and was
 not found
In-Reply-To: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>
References: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>
Message-ID: <5602B326.8050308@gmail.com>

On 23/09/2015 7:13 AM, Ryszard Czermi?ski wrote:
> I am trying to use R Markdown, but call to render() gives me an error:
> Error: pandoc version 1.12.3 or higher is required and was not found.
> 
> As I understand [http://yihui.name/knitr/demo/pandoc/] pandoc is a function
> defined in knitr, which I have installed and it has pandoc() function
> defined.

The error is talking about the non-R software package called pandoc,
which the pandoc() function makes use of.  Pandoc the package does the
heavy lifting, converting the Markdown into HTML, for example.

Duncan Murdoch

> 
> Looks like some version incompatibility issue, but I do not really know how
> to resolve it.
> Do I need to go to older R version to use it?
> 
> I would appreciate your help.
> 
> Best regards,
> Ryszard
> 
>> sessionInfo()
> R version 3.2.2 (2015-08-14)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.10.3 (Yosemite)
> other attached packages: knitr_1.11
> [...]
> 
> Ryszard Czerminski
> 508-358-6328
> ryszard at czerminski.net
> LinkedIn.com/in/Ryszard.Czerminski
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lists at dewey.myzen.co.uk  Wed Sep 23 17:29:37 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 23 Sep 2015 16:29:37 +0100
Subject: [R] doubt with Odds ratio - URGENT HELP NEEDED
In-Reply-To: <EAF5F6E7-6905-4F02-99B7-8E50FE07B446@gmail.com>
References: <00BC0247-0C23-46CF-8E7A-509186C10FCC@gmail.com>
	<560286AC.4000808@dewey.myzen.co.uk>
	<EAF5F6E7-6905-4F02-99B7-8E50FE07B446@gmail.com>
Message-ID: <5602C561.7060207@dewey.myzen.co.uk>

Dear Rosa

Can you remove all the code which is not relevant to calculating the 
odds ratio so we can see what is going on?

On 23/09/2015 16:06, Rosa Oliveira wrote:
> Dear Michael,
>
>
> I found some of the errors, but others I wasn?t able to.
>
> And my huge huge problem concerns OR and OR confidence interval :(
>
>
> *New Corrected code:*
>
>
> casedata <-read.spss("tas_05112008.sav")
> tas.data<-data.frame(casedata)
>
> #Delete patients that were not discharged
> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>
> tas.data$tas_d2      <-
> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
> tas.data$tas_d2))
> tas.data$tas_d3      <-
> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
> tas.data$tas_d3))
> tas.data$tas_d4      <-
> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
> tas.data$tas_d4))
> tas.data$tas_d5      <-
> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
> tas.data$tas_d5))
> tas.data$tas_d6      <-
> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
> tas.data$tas_d6))
>
>      tas.data$age      <-
> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>
>
>      tas.data                     <-   data.frame(tas1 =
> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>                                               tas3 = tas.data$tas_d4,
> tas4 = tas.data$tas_d5,
>                                               tas5 = tas.data$tas_d6,
> age = tas.data$age,
>                                               discharge =
> tas.data$resultado.hosp, id.pat=tas.data$id)
>
> #    tas.data$discharge              <- factor(   tas.data$discharge ,
> levels=c(0,1), labels = c("dead", "alive"))
>
>    #select only cases that have more than 3 tas
>      tas.data                      <- tas.data[apply(tas.data[,-8:-6],
> 1, function(x) sum(!is.na(x)))>2,]
>
>
>      nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients
> with more than 2 tas measurements
>
>      tas.data.long                 <- data.frame(
> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>                                         id=rep(c(1:n.obs), 5))
>      tas.data.long                 <- tas.data.long
>   [order(tas.data.long$id),]
>
>      age=tas.data$age
>
> ##################################################################################################
> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
> ##################################################################################################
>    mean.alive                      <-
> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>    mean.dead                       <-
> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>    stderr                          <- function(x)
> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>    se.alive                        <-
> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>    se.dead                         <-
> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>    summarytas                      <- data.frame (media = c(mean.dead,
> mean.alive),
>                                        standarderror = c( se.dead,
> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>
>
> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) +
>      geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
> standarderror), width=.1) +
>    scale_color_manual(values=c("blue", "red")) +
>     theme(legend.text=element_text(size=20),
> axis.text=element_text(size=16),
> axis.title=element_text(size=20,face="bold")) +
>     scale_x_continuous(name="Days") +
>    scale_y_continuous(name="log tas") +
>    geom_line() +
>      geom_point()
>
>
> library(verification)
> prev <- summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
> answer            = c(prev$coefficients[,1:2])
>
>
> roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
>
>
>
> modelo<-function (dataainit)
> {
>
> #dataa<-tas.data
> dataa<-dataainit
>
> dataa$ident<-seq(1:90)
> tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
> dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
> time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
> ident=rep(dataa$ident,5))
> tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
> tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
> #mixed model for the longitudinal tas
> lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
> na.action=na.exclude )
> #Random intercept and slopes
> pred.lme<-predict(lme.1)
> lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
> lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
> selector<-as.numeric(names(lme.intercept)) #to select not NA rows. Apply
> to the vector in the dataset
>   test(dataa$intercept[resultado.hosp==1],
> dataa$intercept[resultado.hosp==0])
> print(summary(model.surv1))
> return(model.surv1$coef)
>   }
>
>
> *CONSOLE RESULT: (errors in red)*
>
>  > casedata <-read.spss("tas_05112008.sav")
> Warning message:
> In read.spss("tas_05112008.sav") :
>    tas_05112008.sav: Unrecognized record type 7, subtype 18 encountered
> in system file
>  > tas.data<-data.frame(casedata)
>  >
>  > #Delete patients that were not discharged
>  > tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>  > tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>  >
>  > tas.data$tas_d2      <-
> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
> tas.data$tas_d2))
>  > tas.data$tas_d3      <-
> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
> tas.data$tas_d3))
>  > tas.data$tas_d4      <-
> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
> tas.data$tas_d4))
>  > tas.data$tas_d5      <-
> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
> tas.data$tas_d5))
>  > tas.data$tas_d6      <-
> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
> tas.data$tas_d6))
>  >
>  >     tas.data$age      <-
> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>  >
>  >
>  >     tas.data                     <-   data.frame(tas1 =
> tas.data$tas_d2, tas2 = tas.data$tas_d3,
> +                                              tas3 = tas.data$tas_d4,
> tas4 = tas.data$tas_d5,
> +                                              tas5 = tas.data$tas_d6,
> age = tas.data$age,
> +                                              discharge =
> tas.data$resultado.hosp, id.pat=tas.data$id)
>  >
>  > #    tas.data$discharge              <- factor(   tas.data$discharge
> , levels=c(0,1), labels = c("dead", "alive"))
>  >
>  >   #select only cases that have more than 3 tas
>  >     tas.data                      <- tas.data[apply(tas.data[,-8:-6],
> 1, function(x) sum(!is.na(x)))>2,]
>  >
>  >
>  >
>  >     nsample <- n.obs              <- dim(tas.data)[1]  #nr of
> patients with more than 2 tas measurements
>  >
>  >     tas.data.long                 <- data.frame(
> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
> +                                        id=rep(c(1:n.obs), 5))
>  >     tas.data.long                 <- tas.data.long
>   [order(tas.data.long$id),]
>  >
>  >     age=tas.data$age
>  >
>  >
> ##################################################################################################
>  > #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>  >
> ##################################################################################################
>  >   mean.alive                      <-
> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>  >   mean.dead                       <-
> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>  >   stderr                          <- function(x)
> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>  >   se.alive                        <-
> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>  >   se.dead                         <-
> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>  >   summarytas                      <- data.frame (media = c(mean.dead,
> mean.alive),
> +                                       standarderror = c( se.dead,
> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>  >
>  >
>  > ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) +
> +     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
> standarderror), width=.1) +
> +   scale_color_manual(values=c("blue", "red")) +
> +    theme(legend.text=element_text(size=20),
> axis.text=element_text(size=16),
> axis.title=element_text(size=20,face="bold")) +
> +    scale_x_continuous(name="Days") +
> +   scale_y_continuous(name="log tas") +
> +   geom_line() +
> +     geom_point()
> Error in mean - 2 * standarderror :
>    non-numeric argument to binary operator
>  >
>  >
>  > library(verification)
>  > prev <-
> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>  > answer            = c(prev$coefficients[,1:2])
>  >
>  >
>  > roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
> Error in is.finite(x) : default method not implemented for type 'list'
>  >
>  >
>  >
>  > modelo<-function (dataainit)
> +
> + {
> +
> + #dataa<-tas.data
> + dataa<-dataainit
> +
> + dataa$ident<-seq(1:90)
> + tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
> + dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
> + time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
> ident=rep(dataa$ident,5))
> +
> + tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
> + tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
> +
> + #mixed model for the longitudinal tas
> + lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
> na.action=na.exclude )
> +
> + #Random intercept and slopes
> + pred.lme<-predict(lme.1)
> + lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
> + lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
> + selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
> Apply to the vector in the dataset
> +
> +  test(dataa$intercept[resultado.hosp==1],
> dataa$intercept[resultado.hosp==0])
> +
> + print(summary(model.surv1))
> + return(model.surv1$coef)
> +
> +  }
>  >
>
> I can?t get the OR and OR CI :(
>
>
> *DATA:*
>
>
>
>
>
>
> Best,
>
> RO
>
>
>
>
> Atenciosamente,
> Rosa Oliveira
>
> --
> ____________________________________________________________________________
>
>
>
>
> Rosa Celeste dos Santos Oliveira,
>
> E-mail:rosita21 at gmail.com <mailto:rosita21 at gmail.com>
> Tlm: +351 939355143
> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
> ____________________________________________________________________________
> "Many admire, few know"
> Hippocrates
>
>> On 23 Sep 2015, at 12:02, Michael Dewey <lists at dewey.myzen.co.uk
>> <mailto:lists at dewey.myzen.co.uk>> wrote:
>>
>> Dear Rosa
>>
>> It would help if you posted the error messages where they occur so
>> that we can see which of your commands caused which error. However see
>> comment inline below.
>>
>> On 22/09/2015 22:17, Rosa Oliveira wrote:
>>> Dear all,
>>>
>>>
>>> I?m trying to compute Odds ratio and OR confidence interval.
>>>
>>> I?m really naive, sorry for that.
>>>
>>>
>>> I attach my data and my code.
>>>
>>> I?m having lots of errors:
>>>
>>> 1. Error in data.frame(tas1 = tas.data$tas_d2, tas2 =
>>> tas.data$tas_d3, tas3 = tas.data$tas_d4,  :
>>>   arguments imply differing number of rows: 90, 0
>>>
>>
>> At least one of tas_d2, tas_d3, tas_d4 does not exist
>>
>> I suggest fixing that one and hoping the rest go away.
>>
>>> 2. Error in data.frame(tas = c(unlist(tas.data[, -8:-6])), time =
>>> rep(c(0:4),  :
>>>   arguments imply differing number of rows: 630, 450, 0
>>>
>>> 3. Error: object 'tas.data.long' not found
>>>
>>> 4. Error in data.frame(media = c(mean.dead, mean.alive),
>>> standarderror = c(se.dead,  :
>>>   arguments imply differing number of rows: 14, 10
>>>
>>> 5. Error in ggplot(summarytas, aes(x = c(c(1:5), c(1:5)), y = mean,
>>> colour = discharge)) :
>>>   object 'summarytas' not found
>>>
>>> 6. Error in summary(glm(tas.data[, 6] ~ tas.data[, 4], family =
>>> binomial(link = probit))) :
>>>   error in evaluating the argument 'object' in selecting a method for
>>> function 'summary': Error in eval(expr, envir, enclos) : y values
>>> must be 0 <= y <= 1
>>>
>>> 7. Error in wilcox.test.default(pred[obs == 1], pred[obs == 0],
>>> alternative = "great") :
>>>   not enough (finite) 'x' observations
>>> In addition: Warning message:
>>> In is.finite(x) & apply(pred, 1, f) :
>>>   longer object length is not a multiple of shorter object length
>>>
>>>
>>> and off course I?m not getting OR.
>>>
>>> Nonetheless all this errors, I think I have not been able to compute
>>> de code to get OR and OR confidence interval.
>>>
>>>
>>> Can anyone help me please. It?s really urgent.
>>>
>>> PLEASE
>>>
>>> THE CODE:
>>>
>>> the hospital outcome is discharge.
>>>
>>> require(gdata)
>>> library(foreign)
>>> library(nlme)
>>> library(lme4)
>>> library(boot)
>>> library(MASS)
>>> library(Hmisc)
>>> library(plotrix)
>>> library(verification)
>>> library(mvtnorm)
>>> library(statmod)
>>> library(epiR)
>>>
>>> #########################################################################################
>>> # Data preparation
>>>                                                                      #
>>> #########################################################################################
>>>
>>> setwd("/Users/RO/Desktop")
>>>
>>> casedata <-read.spss("tas_05112008.sav")
>>> tas.data<-data.frame(casedata)
>>>
>>> #Delete patients that were not discharged
>>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>
>>> tas.data$tas_d2      <-
>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>> tas.data$tas_d2))
>>> tas.data$tas_d3      <-
>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>> tas.data$tas_d3))
>>> tas.data$tas_d4      <-
>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>> tas.data$tas_d4))
>>> tas.data$tas_d5      <-
>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>> tas.data$tas_d5))
>>> tas.data$tas_d6      <-
>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>> tas.data$tas_d6))
>>>
>>>     tas.data$age      <-
>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>
>>>
>>>     tas.data                     <-   data.frame(tas1 =
>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>                                              tas3 = tas.data$tas_d4,
>>> tas4 = tas.data$tas_d5,
>>>                                              tas5 = tas.data$tas_d6,
>>> age = tas.data$age,
>>>                                              discharge =
>>> tas.data$resultado.hosp, id.pat=tas.data$ID)
>>>
>>> #    tas.data$discharge              <- factor(   tas.data$discharge
>>> , levels=c(0,1), labels = c("dead", "alive"))
>>>
>>>   #select only cases that have more than 3 tas
>>>     tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>> 1, function(x) sum(!is.na(x)))>2,]
>>>
>>>
>>>
>>>     nsample <- n.obs              <- dim(tas.data)[1]  #nr of
>>> patients with more than 2 tas measurements
>>>
>>>     tas.data.long                 <- data.frame(
>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>                                        id=rep(c(1:n.obs), 5))
>>>     tas.data.long                 <- tas.data.long
>>>  [order(tas.data.long$id),]
>>>
>>>     age=tas.data$age
>>>
>>> ##################################################################################################
>>> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>> ##################################################################################################
>>>   mean.alive                      <-
>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>   mean.dead                       <-
>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>   stderr                          <- function(x)
>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>   se.alive                        <-
>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>   se.dead                         <-
>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>   summarytas                      <- data.frame (media = c(mean.dead,
>>> mean.alive),
>>>                                       standarderror = c( se.dead,
>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>
>>>
>>> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) +
>>>     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>> standarderror), width=.1) +
>>>   scale_color_manual(values=c("blue", "red")) +
>>>    theme(legend.text=element_text(size=20),
>>> axis.text=element_text(size=16),
>>> axis.title=element_text(size=20,face="bold")) +
>>>    scale_x_continuous(name="Days") +
>>>   scale_y_continuous(name="log tas") +
>>>   geom_line() +
>>>     geom_point()
>>>
>>>
>>> library(verification)
>>> prev <-
>>> summary(glm(tas.data[,6]~tas.data[,4],family=binomial(link=probit)))
>>> answer = c(prev$coefficients[,1:2])
>>>
>>>
>>> roc.plot(tas.data[,6], prev, show.thres = FALSE, legen=F )
>>>
>>>
>>> modelo<-function (dataainit)
>>>
>>> {
>>>
>>> #dataa<-tas.data
>>> dataa<-dataainit
>>>
>>> dataa$ident<-seq(1:90)
>>> tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>> dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>> time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>> ident=rep(dataa$ident,5))
>>>
>>> tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>> tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>>
>>> #mixed model for the longitudinal tas
>>> lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>> na.action=na.exclude )
>>>
>>> #Random intercept and slopes
>>> pred.lme<-predict(lme.1)
>>> lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>> lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>> selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
>>> Apply to the vector in the dataset
>>>
>>>  test(dataa$intercept[resultado.hosp==1],
>>> dataa$intercept[resultado.hosp==0])
>>>
>>> print(summary(model.surv1))
>>> return(model.surv1$coef)
>>>
>>>  }
>>>
>>>
>>>
>>>
>>> Best,
>>> RO
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To
>>> UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From klebyn at yahoo.com.br  Wed Sep 23 18:45:29 2015
From: klebyn at yahoo.com.br (Cleber Borges)
Date: Wed, 23 Sep 2015 13:45:29 -0300
Subject: [R] How to config the RStudio
Message-ID: <5602D729.4060406@yahoo.com.br>

Dear useRs,

1) how to configure the plot command into windows device by default in 
RStudio?
2) in layout panels, how to config for one panel to start in "minimized 
mode" ?
(i would like to see only the "Environment panel" opened and only the 
headers of the other panel) when start RStudio.

TIA!

  :-)  cleber

---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From markleeds2 at gmail.com  Wed Sep 23 18:50:37 2015
From: markleeds2 at gmail.com (Mark Leeds)
Date: Wed, 23 Sep 2015 12:50:37 -0400
Subject: [R] [FORGED] Re:  Compare two normal to one normal
In-Reply-To: <56024510.90801@auckland.ac.nz>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
	<56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
	<CAHz+bWaiYLVoTaf9jgofyvZAtM=1Hoo4AKWRTvZ51o0L9egS3Q@mail.gmail.com>
	<CAHz+bWb0f=A3vXeWMs9EDy63d7irbfvW-EFc25HTPHGLzsUSwg@mail.gmail.com>
	<56024510.90801@auckland.ac.nz>
Message-ID: <CAHz+bWa6g2Q9gmRGn3ObT8Q1ZdedEey3y7HVBtuprp22HrZoZw@mail.gmail.com>

Hi Rolf: I have  read  a decent amount about  the AIC  but that was a long,
long time ago. I too am no expert on it and John should read some of the
AIC literature John: There's one whole supposedly great text just on AIC
but I don't have it.  Link is here. Of course, it's
absurdly expensive but does get pretty good reviews.

http://www.amazon.com/Model-Selection-Multimodel-Inference-Information-Theoretic/dp/0387953647/ref=sr_1_1?ie=UTF8&qid=1443026829&sr=8-1&keywords=model+selection+aic

Note that if  you end up using the AIC approach, you'll still need the log
likelihoods in both models. I would calculate them yourself and all the
constants like 1/radical 2pi don't need to be included of course since
they'll just be scaling factors.











On Wed, Sep 23, 2015 at 2:22 AM, Rolf Turner <r.turner at auckland.ac.nz>
wrote:

> On 23/09/15 16:38, Mark Leeds wrote:
>
>> John: After I sent what I wrote, I read Rolf's intelligent response. I
>> didn't realize that
>> there are boundary issues so yes, he's correct and  my approach is EL
>> WRONGO. I feel very not good that I just sent that email being that it's
>> totally wrong. My apologies for noise
>> and thanks Rolf for the correct response.
>>
>> Oh,  thing that does still hold in my response is  the AIC approach unless
>> Rolf
>> tells us that it's not valid also. I don't see why it wouldn't be though
>> because you're
>> not doing a hypothesis test when you go the AIC route.
>>
>
> <SNIP>
>
> I am no expert on this, but I would be uneasy applying AIC to such
> problems without having a very close look at the literature on the
> subject.  I'm pretty sure that there *are* regularity conditions that must
> be satisfied in order that AIC should give you a "valid" basis for
> comparison of models.
>
> AIC has most appeal, and is mostly used (in my understanding) in settings
> where there is a multiplicity of models, whereby the multiple comparisons
> problem causes hypothesis testing to lose its appeal. Correspondingly AIC
> has little appeal in a setting in which a single hypothesis test is
> applicable.
>
> I could be wrong about this; as I said, I am no expert.  Perhaps younger
> and wiser heads will chime in and correct me.
>
> cheers,
>
> Rolf
>
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Wed Sep 23 20:22:17 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 23 Sep 2015 10:22:17 -0800
Subject: [R] retaining characters in a csv file
In-Reply-To: <5601FFE2.1060207@auckland.ac.nz>
References: <5601d729.3060502@auckland.ac.nz>
	<0bbb21d0-70ff-4978-b1c6-dd4a17f2234d@gmail.com>
	<c10f8b$1g9f15@ironport10.mayo.edu>
Message-ID: <400DBC12C1C.00000047jrkrideau@inbox.com>





> -----Original Message-----
> From: r.turner at auckland.ac.nz
> Sent: Wed, 23 Sep 2015 13:26:58 +1200
> To: pdalgd at gmail.com
   ..........
> I would say that this phenomenon ("Excel does it") is *overwhelming*
> evidence that it is bad practice!!! :-)

Fortune?

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From jars3n at gmail.com  Wed Sep 23 18:21:42 2015
From: jars3n at gmail.com (Jon Arsenault)
Date: Wed, 23 Sep 2015 12:21:42 -0400
Subject: [R] reproducing Graphpad IC50 in R with drc package
Message-ID: <CAE34pwBZCoz3toKcrHon1ZgMTHjumoSKs5NBYaJKVQ0k-Tca4A@mail.gmail.com>

I've been banging my head against the wall a bit with this and would be
ecstatic if someone could help.

Initial data:

     Response Dose
1  285.17      0.125
2  377.65      0.250
3  438.99      0.500
4  338.46      1.000
5  227.87      2.000
6  165.96      0.010
7  302.92      0.125
8  418.50      0.250
9  464.69      0.500
10 301.36      1.000
11 213.12      2.000
12 160.34      0.010
13 306.18      0.125
14 435.37      0.250
15 451.34      0.500
16 319.50      1.000
17 219.83      2.000
18 172.52      0.010
19 306.56      0.125
20 439.01      0.250
21 469.74      0.500
22 318.05      1.000
23 223.09      2.000

The graphpad template that that this normally would be placed in then
transforms the taking the log(Dose,base=10) and normalizes the Response.
I've confirmed that I was able to recreate that here:

   NormResponse  LogDose
1        41.011 -0.90309
2        72.909 -0.60206
3        94.067 -0.30103
4        59.392  0.00000
5        21.246  0.30103
6        -0.108 -2.00000
7        47.133 -0.90309
8        87.000 -0.60206
9       102.932 -0.30103
10       46.595  0.00000
11       16.159  0.30103
12       -2.047 -2.00000
13       48.258 -0.90309
14       92.819 -0.60206
15       98.327 -0.30103
16       52.852  0.00000
17       18.473  0.30103
18        2.155 -2.00000
19       48.389 -0.90309
20       94.074 -0.60206
21      104.674 -0.30103
22       52.352  0.00000
23       19.598  0.30103




Now graphpad used 'log(inhibitor) vs. normalized response -- Variable
slope' which it states to be a 4parameter sigmodial but it gives me very
different results.


graphpad gives and IC50 of 0.1560 and drm's LL.4 gives 0.1149 and it wont
even take the transformed data,just throws an error.

	[[alternative HTML version deleted]]


From lordpdc at hotmail.com  Wed Sep 23 19:09:12 2015
From: lordpdc at hotmail.com (raoman ramirez)
Date: Wed, 23 Sep 2015 12:09:12 -0500
Subject: [R] get lm by file .cvs
Message-ID: <SNT150-W876A4664BF47B342DF9266AD440@phx.gbl>

well i have info<-read.csv(file, header=T)
and i want try  thatmodel<-lm(info[1]~info[2])
is for metod backward selection but i don't know how
thanks for help 		 	   		  
	[[alternative HTML version deleted]]


From marco.prado.bs at gmail.com  Wed Sep 23 15:37:27 2015
From: marco.prado.bs at gmail.com (Marco)
Date: Wed, 23 Sep 2015 10:37:27 -0300
Subject: [R]  Running R on a hosting o server
In-Reply-To: <mailman.5.1443002401.29276.r-help@r-project.org>
References: <mailman.5.1443002401.29276.r-help@r-project.org>
Message-ID: <1443015179-sup-2972@marco-HP-2000-Notebook-PC>

> Hi all,
> 
> Hope I can Explain:
> 
> I want to "run" online some function (code written by me) so that this code
> "saves" an output file on my server and my html webpage read the file R
> plots and save (really manually I would run R function, open Filezilla, and
> pass the output png o jpg file).
> 
> Is it possible to do this "authomatically" telling R, something like each
> 15 minutes run this "pru.txt" file, and take save this plot.png and execute
> filezilla with this inputs and save the plot in this folder?
> 
> Hope you can understand me.
> 
> In rmarkdown it is true that output html file but my intention is tu run my
> own function and the need is to run my function in R, and run and open
> filezilla and deposit the file in the right place.

You should use cron, (man cron):
cron (8)             - daemon to execute scheduled commands (Vixie Cron)

So you schedule your task.

-- 
Marco Arthur @ (M)arco Creatives


From roryrwilson at yahoo.ca  Wed Sep 23 15:57:32 2015
From: roryrwilson at yahoo.ca (Rory Wilson)
Date: Wed, 23 Sep 2015 13:57:32 +0000 (UTC)
Subject: [R] [FORGED] Error from lme4: "Error: (p <- ncol(X)) == ncol(Y)
 is not TRUE"
In-Reply-To: <5601C76C.4000107@auckland.ac.nz>
References: <5601C76C.4000107@auckland.ac.nz>
Message-ID: <424436034.246678.1443016652708.JavaMail.yahoo@mail.yahoo.com>

In reply to Rolf Turner and Jean Adams who have been helping me:
This does appear to be an issue with NA values in the non-factor variables. In the (non-reproducible) example below, we can see that removing the NAs solves the problem. However, from what I can see to this point, there does not seem be be rhyme nor reason to why the issue is taking place. A slight modification to Rolf Turner's code (introducing some NAs) shows that, in general, NAs are not a problem for lmer (indeed, it just runs na.omit as default).
Examining which factors are affected by the removal of the NAs shows no discernible pattern - no factors disappeared, none became "1" or anything of this nature.I will be able to proceed just by performing the na.omit beforehand, but it is curious.?
Thanks for your help everyone (especially Rolf Turner and Jean Adams)!

mod1<-lmer(beta~expData+techVar$RIN+techVar$sample_storage_time+(1|techVar$p_amplification))#Error: (p <- ncol(X)) == ncol(Y) is not TRUE
mod1<-lm(beta~expData+techVar$RIN+techVar$sample_storage_time+techVar$p_amplification)#No error given.
? #trial with eliminating NAs? elimSamps<-which(is.na(beta))? length(elimSamps) #[1] 4
? #eliminate the NAs from all vectors? betaVals<-beta[-elimSamps]? expD<-expData[-elimSamps])? techRIN<-techVar$RIN[-elimSamps]? techTime<-techVar$sample_storage_time[-elimSamps]? techPlate<-factor(techVar$p_amplification[-elimSamps])

mod1<-lmer(betaVals~expD+techRIN+techTime+(1|techPlate))
summary(mod1)#Linear mixed model fit by REML ['lmerMod']#Formula: betaVals ~ expD + techRIN + techTime + (1 | techPlate)##REML criterion at convergence: -1701.7##Scaled residuals:# ? ?Min ? ? ?1Q ?Median ? ? ?3Q ? ? Max#-2.3582 -0.6996 -0.1085 ?0.6079 ?4.6743##Random effects:# Groups ? ?Name ? ? ? ?Variance ?Std.Dev.# techPlate (Intercept) 1.645e-05 0.004056# Residual ? ? ? ? ? ? ?4.991e-03 0.070644#Number of obs: 709, groups: ?techPlate, 29##Fixed effects:# ? ? ? ? ? ? Estimate Std. Error t value#(Intercept) 2.159e-01 ?9.871e-02 ? 2.188#expD ? ? ? ?2.330e-03 ?1.498e-02 ? 0.156#techRIN ? ? 5.096e-03 ?4.185e-03 ? 1.218#techTime ? ?1.399e-06 ?1.565e-05 ? 0.089##Correlation of Fixed Effects:# ? ? ? ? (Intr) expD ? tchRIN#expD ? ? -0.919#techRIN ?-0.272 -0.103#techTime -0.206 ?0.063 ?0.021

summary(techPlate)# plate01 ?plate02 ?plate03 ?plate04 ?plate05 ?plate06 ?plate07 ?plate09# ? ? ? 1 ? ? ? 22 ? ? ? 34 ? ? ? 28 ? ? ? 31 ? ? ? 28 ? ? ? 32 ? ? ? 10#plate09a ?plate10 ?plate11 ?plate13 ?plate14 ?plate15 ?plate16 ?plate17# ? ? ?16 ? ? ? 17 ? ? ? 15 ? ? ? ?4 ? ? ? 52 ? ? ? 55 ? ? ? 41 ? ? ? 33# plate18 ?plate19 ?plate20 ?plate21 ?plate22 ?plate23 ?plate24 ?plate25# ? ? ?50 ? ? ? 42 ? ? ? 21 ? ? ? 50 ? ? ? 13 ? ? ? 22 ? ? ? 17 ? ? ? ?7# plate26 ?plate27 ?plate28 ?plate30 ?plate32# ? ? ?25 ? ? ? 21 ? ? ? ?5 ? ? ? 13 ? ? ? ?4
summary(techVar$p_amplification)# plate01 ?plate02 ?plate03 ?plate04 ?plate05 ?plate06 ?plate07 ?plate09# ? ? ? 1 ? ? ? 22 ? ? ? 34 ? ? ? 28 ? ? ? 31 ? ? ? 28 ? ? ? 32 ? ? ? 10#plate09a ?plate10 ?plate11 ?plate13 ?plate14 ?plate15 ?plate16 ?plate17# ? ? ?17 ? ? ? 17 ? ? ? 15 ? ? ? ?4 ? ? ? 53 ? ? ? 55 ? ? ? 41 ? ? ? 33# plate18 ?plate19 ?plate20 ?plate21 ?plate22 ?plate23 ?plate24 ?plate25# ? ? ?50 ? ? ? 42 ? ? ? 21 ? ? ? 50 ? ? ? 13 ? ? ? 22 ? ? ? 17 ? ? ? ?8# plate26 ?plate27 ?plate28 ?plate30 ?plate32# ? ? ?25 ? ? ? 21 ? ? ? ?5 ? ? ? 14 ? ? ? ?4

#Counter-example, where it functions fine#Example from Rolf Turner
set.seed(42)f <- factor(sample(1:29,713,TRUE))x <- seq(0,1,length=713)y <- rnorm(713)require(lme4)
x[sample(1:713,4,replace=F)]<-NA
fit <- lmer(y ~ x + (1|f))#No error message given
  
	[[alternative HTML version deleted]]


From rosita21 at gmail.com  Wed Sep 23 17:06:22 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Wed, 23 Sep 2015 16:06:22 +0100
Subject: [R] doubt with Odds ratio - URGENT HELP NEEDED
In-Reply-To: <560286AC.4000808@dewey.myzen.co.uk>
References: <00BC0247-0C23-46CF-8E7A-509186C10FCC@gmail.com>
	<560286AC.4000808@dewey.myzen.co.uk>
Message-ID: <EAF5F6E7-6905-4F02-99B7-8E50FE07B446@gmail.com>

Dear Michael,


I found some of the errors, but others I wasn?t able to.

And my huge huge problem concerns OR and OR confidence interval :(


New Corrected code:


	casedata <-read.spss("tas_05112008.sav")
	tas.data<-data.frame(casedata)

	#Delete patients that were not discharged
		tas.data                     <- tas.data[ tas.data$hosp!="si ",]
		tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)

  	tas.data$tas_d2      <- log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA, tas.data$tas_d2))
		tas.data$tas_d3      <- log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA, tas.data$tas_d3))
		tas.data$tas_d4      <- log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA, tas.data$tas_d4))
		tas.data$tas_d5      <- log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA, tas.data$tas_d5))
		tas.data$tas_d6      <- log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA, tas.data$tas_d6))

    tas.data$age      <- ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)


    tas.data                     <-   data.frame(tas1 = tas.data$tas_d2, tas2 = tas.data$tas_d3, 
                                             tas3 = tas.data$tas_d4, tas4 = tas.data$tas_d5, 
                                             tas5 = tas.data$tas_d6, age = tas.data$age, 
                                             discharge = tas.data$resultado.hosp, id.pat=tas.data$id)

#    tas.data$discharge              <- factor(   tas.data$discharge , levels=c(0,1), labels = c("dead", "alive"))

  #select only cases that have more than 3 tas
    tas.data                      <- tas.data[apply(tas.data[,-8:-6], 1, function(x) sum(!is.na(x)))>2,]


    
    nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients with more than 2 tas measurements

    tas.data.long                 <- data.frame( tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs), age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
                                       id=rep(c(1:n.obs), 5))
    tas.data.long                 <- tas.data.long  [order(tas.data.long$id),]

    age=tas.data$age

##################################################################################################
#PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
##################################################################################################
  mean.alive                      <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
  mean.dead                       <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T) 
  stderr                          <- function(x) sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
  se.alive                        <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
  se.dead                         <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
  summarytas                      <- data.frame (media = c(mean.dead, mean.alive), 
                                      standarderror = c( se.dead, se.alive), discharge = c(rep("dead",5), rep("alive", 5)))


ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) + 
    geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2* standarderror), width=.1) +
  scale_color_manual(values=c("blue", "red")) +
   theme(legend.text=element_text(size=20), axis.text=element_text(size=16), axis.title=element_text(size=20,face="bold")) +
   scale_x_continuous(name="Days") +
  scale_y_continuous(name="log tas") +
  geom_line() +
    geom_point()


library(verification)
prev <- summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
answer            = c(prev$coefficients[,1:2])


roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )



	modelo<-function (dataainit) 
	
	{

 		#dataa<-tas.data
 		dataa<-dataainit

	 	dataa$ident<-seq(1:90)
  		tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3, 
  		dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
  		time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5), ident=rep(dataa$ident,5))
  
		tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),]) 
 		tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA

 		#mixed model for the longitudinal tas
 		lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days, na.action=na.exclude )
 		
 		#Random intercept and slopes
 		pred.lme<-predict(lme.1)
 		lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1] 
 		lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2] 	
 		selector<-as.numeric(names(lme.intercept)) #to select not NA rows. Apply to the vector in the dataset

 test(dataa$intercept[resultado.hosp==1], dataa$intercept[resultado.hosp==0])
 	
 	print(summary(model.surv1))
 	return(model.surv1$coef)
 	
 }


CONSOLE RESULT: (errors in red)

> 	casedata <-read.spss("tas_05112008.sav")
Warning message:
In read.spss("tas_05112008.sav") :
  tas_05112008.sav: Unrecognized record type 7, subtype 18 encountered in system file
> 	tas.data<-data.frame(casedata)
> 
> 	#Delete patients that were not discharged
> 		tas.data                     <- tas.data[ tas.data$hosp!="si ",]
> 		tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
> 
>   	tas.data$tas_d2      <- log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA, tas.data$tas_d2))
> 		tas.data$tas_d3      <- log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA, tas.data$tas_d3))
> 		tas.data$tas_d4      <- log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA, tas.data$tas_d4))
> 		tas.data$tas_d5      <- log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA, tas.data$tas_d5))
> 		tas.data$tas_d6      <- log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA, tas.data$tas_d6))
> 
>     tas.data$age      <- ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
> 
> 
>     tas.data                     <-   data.frame(tas1 = tas.data$tas_d2, tas2 = tas.data$tas_d3, 
+                                              tas3 = tas.data$tas_d4, tas4 = tas.data$tas_d5, 
+                                              tas5 = tas.data$tas_d6, age = tas.data$age, 
+                                              discharge = tas.data$resultado.hosp, id.pat=tas.data$id)
> 
> #    tas.data$discharge              <- factor(   tas.data$discharge , levels=c(0,1), labels = c("dead", "alive"))
> 
>   #select only cases that have more than 3 tas
>     tas.data                      <- tas.data[apply(tas.data[,-8:-6], 1, function(x) sum(!is.na(x)))>2,]
> 
> 
>     
>     nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients with more than 2 tas measurements
> 
>     tas.data.long                 <- data.frame( tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs), age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
+                                        id=rep(c(1:n.obs), 5))
>     tas.data.long                 <- tas.data.long  [order(tas.data.long$id),]
> 
>     age=tas.data$age
> 
> ##################################################################################################
> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
> ##################################################################################################
>   mean.alive                      <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>   mean.dead                       <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T) 
>   stderr                          <- function(x) sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>   se.alive                        <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>   se.dead                         <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>   summarytas                      <- data.frame (media = c(mean.dead, mean.alive), 
+                                       standarderror = c( se.dead, se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
> 
> 
> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) + 
+     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2* standarderror), width=.1) +
+   scale_color_manual(values=c("blue", "red")) +
+    theme(legend.text=element_text(size=20), axis.text=element_text(size=16), axis.title=element_text(size=20,face="bold")) +
+    scale_x_continuous(name="Days") +
+   scale_y_continuous(name="log tas") +
+   geom_line() +
+     geom_point()
Error in mean - 2 * standarderror : 
  non-numeric argument to binary operator
> 
> 
> library(verification)
> prev <- summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
> answer            = c(prev$coefficients[,1:2])
> 
> 
> roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
Error in is.finite(x) : default method not implemented for type 'list'
> 
> 
> 
> 	modelo<-function (dataainit) 
+ 	
+ 	{
+ 
+  		#dataa<-tas.data
+  		dataa<-dataainit
+ 
+ 	 	dataa$ident<-seq(1:90)
+   		tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3, 
+   		dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
+   		time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5), ident=rep(dataa$ident,5))
+   
+ 		tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),]) 
+  		tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
+  
+  		#mixed model for the longitudinal tas
+  		lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days, na.action=na.exclude )
+  		
+  		#Random intercept and slopes
+  		pred.lme<-predict(lme.1)
+  		lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1] 
+  		lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2] 	
+  		selector<-as.numeric(names(lme.intercept)) #to select not NA rows. Apply to the vector in the dataset
+  
+  test(dataa$intercept[resultado.hosp==1], dataa$intercept[resultado.hosp==0])
+  	
+  	print(summary(model.surv1))
+  	return(model.surv1$coef)
+  	
+  }
> 

I can?t get the OR and OR CI :(


DATA:





Best,

RO




Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates

> On 23 Sep 2015, at 12:02, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> 
> Dear Rosa
> 
> It would help if you posted the error messages where they occur so that we can see which of your commands caused which error. However see comment inline below.
> 
> On 22/09/2015 22:17, Rosa Oliveira wrote:
>> Dear all,
>> 
>> 
>> I?m trying to compute Odds ratio and OR confidence interval.
>> 
>> I?m really naive, sorry for that.
>> 
>> 
>> I attach my data and my code.
>> 
>> I?m having lots of errors:
>> 
>> 1. Error in data.frame(tas1 = tas.data$tas_d2, tas2 = tas.data$tas_d3, tas3 = tas.data$tas_d4,  :
>>   arguments imply differing number of rows: 90, 0
>> 
> 
> At least one of tas_d2, tas_d3, tas_d4 does not exist
> 
> I suggest fixing that one and hoping the rest go away.
> 
>> 2. Error in data.frame(tas = c(unlist(tas.data[, -8:-6])), time = rep(c(0:4),  :
>>   arguments imply differing number of rows: 630, 450, 0
>> 
>> 3. Error: object 'tas.data.long' not found
>> 
>> 4. Error in data.frame(media = c(mean.dead, mean.alive), standarderror = c(se.dead,  :
>>   arguments imply differing number of rows: 14, 10
>> 
>> 5. Error in ggplot(summarytas, aes(x = c(c(1:5), c(1:5)), y = mean, colour = discharge)) :
>>   object 'summarytas' not found
>> 
>> 6. Error in summary(glm(tas.data[, 6] ~ tas.data[, 4], family = binomial(link = probit))) :
>>   error in evaluating the argument 'object' in selecting a method for function 'summary': Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1
>> 
>> 7. Error in wilcox.test.default(pred[obs == 1], pred[obs == 0], alternative = "great") :
>>   not enough (finite) 'x' observations
>> In addition: Warning message:
>> In is.finite(x) & apply(pred, 1, f) :
>>   longer object length is not a multiple of shorter object length
>> 
>> 
>> and off course I?m not getting OR.
>> 
>> Nonetheless all this errors, I think I have not been able to compute de code to get OR and OR confidence interval.
>> 
>> 
>> Can anyone help me please. It?s really urgent.
>> 
>> PLEASE
>> 
>> THE CODE:
>> 
>> the hospital outcome is discharge.
>> 
>> require(gdata)
>> library(foreign)
>> library(nlme)
>> library(lme4)
>> library(boot)
>> library(MASS)
>> library(Hmisc)
>> library(plotrix)
>> library(verification)
>> library(mvtnorm)
>> library(statmod)
>> library(epiR)
>> 
>> #########################################################################################
>> # Data preparation                                                                      #
>> #########################################################################################
>> 
>> setwd("/Users/RO/Desktop")
>> 
>> 	casedata <-read.spss("tas_05112008.sav")
>> 	tas.data<-data.frame(casedata)
>> 
>> 	#Delete patients that were not discharged
>> 		tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>> 		tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>> 
>>   	tas.data$tas_d2      <- log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA, tas.data$tas_d2))
>> 		tas.data$tas_d3      <- log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA, tas.data$tas_d3))
>> 		tas.data$tas_d4      <- log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA, tas.data$tas_d4))
>> 		tas.data$tas_d5      <- log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA, tas.data$tas_d5))
>> 		tas.data$tas_d6      <- log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA, tas.data$tas_d6))
>> 
>>     tas.data$age      <- ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>> 
>> 
>>     tas.data                     <-   data.frame(tas1 = tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>                                              tas3 = tas.data$tas_d4, tas4 = tas.data$tas_d5,
>>                                              tas5 = tas.data$tas_d6, age = tas.data$age,
>>                                              discharge = tas.data$resultado.hosp, id.pat=tas.data$ID)
>> 
>> #    tas.data$discharge              <- factor(   tas.data$discharge , levels=c(0,1), labels = c("dead", "alive"))
>> 
>>   #select only cases that have more than 3 tas
>>     tas.data                      <- tas.data[apply(tas.data[,-8:-6], 1, function(x) sum(!is.na(x)))>2,]
>> 
>> 
>> 
>>     nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients with more than 2 tas measurements
>> 
>>     tas.data.long                 <- data.frame( tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs), age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>                                        id=rep(c(1:n.obs), 5))
>>     tas.data.long                 <- tas.data.long  [order(tas.data.long$id),]
>> 
>>     age=tas.data$age
>> 
>> ##################################################################################################
>> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>> ##################################################################################################
>>   mean.alive                      <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>   mean.dead                       <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>   stderr                          <- function(x) sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>   se.alive                        <- apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>   se.dead                         <- apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>   summarytas                      <- data.frame (media = c(mean.dead, mean.alive),
>>                                       standarderror = c( se.dead, se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>> 
>> 
>> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) +
>>     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2* standarderror), width=.1) +
>>   scale_color_manual(values=c("blue", "red")) +
>>    theme(legend.text=element_text(size=20), axis.text=element_text(size=16), axis.title=element_text(size=20,face="bold")) +
>>    scale_x_continuous(name="Days") +
>>   scale_y_continuous(name="log tas") +
>>   geom_line() +
>>     geom_point()
>> 
>> 
>> library(verification)
>> prev <- summary(glm(tas.data[,6]~tas.data[,4],family=binomial(link=probit)))
>> answer = c(prev$coefficients[,1:2])
>> 
>> 
>> roc.plot(tas.data[,6], prev, show.thres = FALSE, legen=F )
>> 
>> 
>> 	modelo<-function (dataainit)
>> 	
>> 	{
>> 
>>  		#dataa<-tas.data
>>  		dataa<-dataainit
>> 
>> 	 	dataa$ident<-seq(1:90)
>>   		tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>   		dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>   		time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5), ident=rep(dataa$ident,5))
>> 
>> 		tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>  		tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>> 
>>  		#mixed model for the longitudinal tas
>>  		lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days, na.action=na.exclude )
>>  		
>>  		#Random intercept and slopes
>>  		pred.lme<-predict(lme.1)
>>  		lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>  		lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2] 	
>>  		selector<-as.numeric(names(lme.intercept)) #to select not NA rows. Apply to the vector in the dataset
>> 
>>  test(dataa$intercept[resultado.hosp==1], dataa$intercept[resultado.hosp==0])
>>  	
>>  	print(summary(model.surv1))
>>  	return(model.surv1$coef)
>>  	
>>  }
>>  	
>>  	
>>  	
>> 
>> Best,
>> RO
>>  	
>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> -- 
> Michael
> http://www.dewey.myzen.co.uk/home.html


From mashranga at yahoo.com  Wed Sep 23 20:51:12 2015
From: mashranga at yahoo.com (Mohammad Tanvir Ahamed)
Date: Wed, 23 Sep 2015 18:51:12 +0000 (UTC)
Subject: [R] Progress bar in random forest model
Message-ID: <1209796162.419088.1443034273606.JavaMail.yahoo@mail.yahoo.com>

Hi , 

I am using randomForest model in R .

For large number of tree my program takes long time to complete . 

In "randomForest" function i can use "do.trace=TRUE" to see the real time progress .  Sample out put in real time on R console is as follows

ntree      OOB      1      2      3      4      5      6      7      8      9 
100:   2.31%  7.14%  2.08%  0.00%  2.25% 10.81%  0.90%  0.00%  0.00%  1.72% 
200:   1.95%  7.14%  2.08%  0.00%  2.25%  8.11%  0.00%  0.00%  0.00%  1.72% 
300:   1.78%  7.14%  2.08%  0.00%  1.69%  8.11%  0.00%  0.00%  0.00%  1.72% 
400:   1.95%  7.14%  2.08%  0.00%  1.69%  8.11%  0.00%  0.00%  0.00%  3.45% 
500:   1.78%  7.14%  2.08%  0.00%  1.69%  8.11%  0.00%  0.00%  0.00%  1.72% 
600:   1.78%  7.14%  2.08%  0.00%  1.69%  8.11%  0.00%  0.00%  0.00%  1.72% 
700:   1.78%  7.14%  2.08%  0.00%  1.69%  8.11%  0.00%  0.00%  0.00%  1.72% 
800:   1.78%  7.14%  2.08%  0.00%  1.69%  8.11%  0.00%  0.00%  0.00%  1.72% 
900:   1.78%  7.14%  2.08%  0.00%  1.69%  8.11%  0.00%  0.00%  0.00%  1.72% 
1000:   1.78%  7.14%  2.08%  0.00%  1.69%  8.11%  0.00%  0.00%  0.00%  1.72%

The first row (100: 2.31% ....) comes first. After 1 second it comes 2nd row and so on. 
Now i like to modify this out put . 

like, when 1st row will come , i like to grab only 100 form the whole line and show only 100 on R console instead of showing whole line. The same this will happen for rest of the row.

[ i tried sink(). but it will not work as sink write the complete output to out file ]

[I searched for do.trace option in randomForest function. but i lost myself  as i feel it call come C program. not sure but could not figured it out]


In general, i like to grab the real time output on R console.

I will be very grateful if any one please help me with . 

Thank you    

 
Tanvir Ahamed 
G?teborg, Sweden  |  mashranga at yahoo.com


From djnordlund at frontier.com  Wed Sep 23 21:05:33 2015
From: djnordlund at frontier.com (Daniel Nordlund)
Date: Wed, 23 Sep 2015 12:05:33 -0700
Subject: [R] retaining characters in a csv file
In-Reply-To: <c10f8b$1gbn9d@ironport10.mayo.edu>
References: <c10f8b$1g9f15@ironport10.mayo.edu>
	<5601D729.3060502@auckland.ac.nz> <c10f8b$1gbn9d@ironport10.mayo.edu>
Message-ID: <5602F7FD.1010009@frontier.com>

On 9/23/2015 5:57 AM, Therneau, Terry M., Ph.D. wrote:
> Thanks for all for the comments, I hadn't intended to start a war.
>
> My summary:
>   1. Most important: I wasn't missing something obvious.  This is 
> always my first suspicion when I submit something to R-help, and it's 
> true more often than not.
>
>   2. Obviously (at least it is now), the CSV standard does not specify 
> that quotes should force a character result.  R is not "wrong".  Wrt 
> to using what Excel does as litmus test, I consider that to be totally 
> uninformative about standards: neither pro (like Duncan) or anti (like 
> Rolf), but simply irrelevant.  (Like many MS choices.)
>
>   3. I'll have to code in my own solution, either pre-scan the first 
> few lines to create a colClasses, or use read_csv from the readr 
> library (if there are leading zeros it keeps the string as character, 
> which may suffice for my needs), or something else.
>
>   4. The source of the data is a "text/csv" field coming from an http 
> POST request.  This is an internal service on an internal Mayo server 
> and coded by our own IT department; this will not be the first case 
> where I have found that their definition of "csv" is not quite standard.
>
> Terry T.
>
>
>
>> On 23/09/15 10:00, Therneau, Terry M., Ph.D. wrote:
>>> I have a csv file from an automatic process (so this will happen
>>> thousands of times), for which the first row is a vector of variable
>>> names and the second row often starts something like this:
>>>
>>> 5724550,"000202075214",2005.02.17,2005.02.17,"F", .....
>>>
>>> Notice the second variable which is
>>>        a character string (note the quotation marks)
>>>        a sequence of numeric digits
>>>        leading zeros are significant
>>>
>>> The read.csv function insists on turning this into a numeric. Is there
>>> any simple set of options that
>>> will turn this behavior off?  I'm looking for a way to tell it to "obey
>>> the bloody quotes" -- I still want the first, third, etc columns to
>>> become numeric.  There can be more than one variable like this, and not
>>> always in the second position.
>>>
>>> This happens deep inside the httr library; there is an easy way for me
>>> to add more options to the read.csv call but it is not so easy to
>>> replace it with something else.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

A fairly simple workaround is to add two lines of code to the process, 
and then add the colClasses parameter as you suggested in item 2 above.

want <- read.csv('yourfile', quote='', stringsAsFactors= FALSE, nrows=1)
classes <- sapply(want, class)
want <- read.csv('yourfile', stringsAsFactors= FALSE, colClasses=classes)

I don't know if you want your final file to convert strings to factors, 
so you can modify as needed.  In addition, if your files aren't as 
regular as I inferred, you can increase the number of rows to read in 
the first line to ensure getting the classes right.


Hope this is helpful,

Dan

-- 
Daniel Nordlund
Bothell, WA  USA


From lorenzo.isella at gmail.com  Wed Sep 23 21:15:37 2015
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Wed, 23 Sep 2015 21:15:37 +0200
Subject: [R] Sampling the Distance Matrix
Message-ID: <20150923191537.GA2548@localhost.localdomain>

Dear All,
Suppose you have a distance matrix stored like a dist object, for
instance

x<-rnorm(20)
y<-rnorm(20)

mm<-as.matrix(cbind(x,y))

dst<-(dist(mm))

Now, my problem is the following: I would like to get the rows of mm
corresponding to points whose distance is always larger of, let's say,
0.9.
In other words, if I were to compute the distance matrix on those
selected rows of mm, apart from the diagonal, I would get all entries
larger than 0.9.
Any idea about how I can efficiently code that?
Regards

Lorenzo


From uclatutor18 at yahoo.com  Wed Sep 23 21:05:57 2015
From: uclatutor18 at yahoo.com (uc la)
Date: Wed, 23 Sep 2015 19:05:57 +0000 (UTC)
Subject: [R] Error of matrix and dimnames when using SVM in R
Message-ID: <2063685162.451435.1443035157014.JavaMail.yahoo@mail.yahoo.com>

Hi all!
I am having problems with using SVM in R

I have a data frame `trainData` which contains 198 rows and? looks like 

Matchup Win HomeID AwayID A_TWPCT A_WST6 A_SEED B_TWPCT B_WST6 B_SEED2010_1115_1457?? 1?? 1115?? 1457?? 0.531????? 5???? 16?? 0.567????? 4???? 16
2010_1124_1358?? 1?? 1124?? 1358?? 0.774????? 5????? 3??? 0.75????? 5???? 14
??? ...

The testData is similar. 

In order to use SVM, I have to change the response variable Win to a factor. I tried the below:

??? trainDataSVM <- trainData
??? trainDataSVM$Win <- factor(trainDataSVM$Win)
??? svmfit =svm (Win ~ A_WST6 + A_SEED + B_WST6 + B_SEED , data = trainDataSVM , kernel ="linear", cost =10,scale =FALSE, probability=TRUE )
??? testDataSVM<-testData
??? testDataSVM$Win <-factor(testDataSVM$Win)
??? predictions_SVM <- predict(bestmod, testDataSVM, type = "response",probability=TRUE)

However, I get the message 

??? Error in matrix(ret$prob, nrow = nrow(newdata), byrow = TRUE, dimnames = list(rowns,? : 
? length of 'dimnames' [2] not equal to array extent

If I re-run the code except not changing trainDataSVM$Win and testDataSVM$Win to factors, if I print out predictions_SVM, I get the message named numeric(0)

How do I fix this?
Thanks!

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Wed Sep 23 22:23:16 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 23 Sep 2015 13:23:16 -0700
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <20150923191537.GA2548@localhost.localdomain>
References: <20150923191537.GA2548@localhost.localdomain>
Message-ID: <CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>

> mm <- cbind(1/(1:5), sqrt(1:5))
> d <- dist(mm)
> d
          1         2         3         4
2 0.6492864
3 0.9901226 0.3588848
4 1.2500000 0.6369033 0.2806086
5 1.4723668 0.8748970 0.5213550 0.2413050
> which(as.matrix(d)>0.9, arr.ind=TRUE)
  row col
3   3   1
4   4   1
5   5   1
1   1   3
1   1   4
1   1   5
I.e., the distances between mm's rows 3 & 1, 4 & 1, and 5,1 are more than 0.9

The as.matrix(d) is needed because dist returns the lower triangle of
the distance
matrix and an object of class "dist" and as.matrix.dist converts that
into a matrix.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Sep 23, 2015 at 12:15 PM, Lorenzo Isella
<lorenzo.isella at gmail.com> wrote:
> Dear All,
> Suppose you have a distance matrix stored like a dist object, for
> instance
>
> x<-rnorm(20)
> y<-rnorm(20)
>
> mm<-as.matrix(cbind(x,y))
>
> dst<-(dist(mm))
>
> Now, my problem is the following: I would like to get the rows of mm
> corresponding to points whose distance is always larger of, let's say,
> 0.9.
> In other words, if I were to compute the distance matrix on those
> selected rows of mm, apart from the diagonal, I would get all entries
> larger than 0.9.
> Any idea about how I can efficiently code that?
> Regards
>
> Lorenzo
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ccberry at ucsd.edu  Wed Sep 23 22:27:51 2015
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Wed, 23 Sep 2015 13:27:51 -0700
Subject: [R] Compare two normal to one normal
In-Reply-To: <5601CA9F020000CB00139C43@smtp.medicine.umaryland.edu>
References: <56018010020000CB00139BA4@smtp.medicine.umaryland.edu>
	<CAGxFJbSRe3wE9X2oM_cNiFkpSrCnFo60a9+tUZcB3NWm68fzSQ@mail.gmail.com>
	<CAHz+bWaJrvhq7aCNX-jLTOuQpkZntasm_JOVp4hWGE3=qM3xWg@mail.gmail.com>
	<CAGxFJbR56aQsfPxrHic5qG_1=NcQkrJQ4J-HUVLNeduNQ8SEZQ@mail.gmail.com>
	<56019767020000CB00139BF6@smtp.medicine.umaryland.edu>
	<alpine.OSX.2.20.1509221521500.1538@charles-berrys-macbook.local>
	<5601CA9F020000CB00139C43@smtp.medicine.umaryland.edu>
Message-ID: <alpine.OSX.2.20.1509231304080.4433@charles-berrys-macbook.local>

On Tue, 22 Sep 2015, John Sorkin wrote:

> Charles,

> I am not sure the answer to me question, given a dataset, how can one 
> compare the fit of a model of the fits the data to a mixture of two 
> normal distributions to the fit of a model that uses a single normal 
> distribution, can be based on the glm model you suggest.

Well you *did* ask how to calculate the log-likelihood of a fitted normal 
density, didn't you? That is what I responded to. You can check that 
result longhand as sum( dnorm( y, y.mean, y.std , log=TRUE ) ) and get the 
same result (as long as you used ML estimates of the mean and standard 
deviation).

>
>
> I have used normalmixEM to fit the data to a mixture of two normal 
> curves. The model estimates four (perhaps five) parameters: mu1, sd^2 1, 
> mu2, sd^2, (and perhaps lambda, the mixing proportion. The mixing 
> proportion may not need to be estimated, it may be determined once once 
> specifies mu1, sd^2 1, mu2, and sd^2.) Your model fits the data to a 
> model that contains only the mean, and estimates 2 parameters mu0 and 
> sd0^2.  I am not sure that your model and mine can be considered to be 
> nested. If I am correct I can't compare the log likelihood values from 
> the two models. I may be wrong. If I am, I should be able to perform a 
> log likelihood test with 2 (or 3, I am not sure which) DFs. Are you 
> suggesting the models are nested? If so, should I use 3 or 2 DFs?

As Rolf points out there is a literature on such tests (and Googling 'test 
finite mixture' covers much of it).

Do you really want a test? If you merely want to pick a winner from two 
candidate models there are other procedures. k-fold crossvalidation 
of the loglikelihood ratio statistic seems like an easy, natural approach.

HTH,

Chuck


From dcarlson at tamu.edu  Wed Sep 23 23:23:04 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 23 Sep 2015 21:23:04 +0000
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>

I think the OP wanted rows where all values were greater than .9.
If so, this works:

> set.seed(42)
> dst <- dist(cbind(rnorm(20), rnorm(20)))
> dst2 <- as.matrix(dst)
> diag(dst2) <- NA
> idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))
> idx
13 18 19 
13 18 19 
> dst2[idx, idx]
         13       18       19
13       NA 2.272407 3.606054
18 2.272407       NA 1.578150
19 3.606054 1.578150       NA

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of William Dunlap
Sent: Wednesday, September 23, 2015 3:23 PM
To: Lorenzo Isella
Cc: r-help at r-project.org
Subject: Re: [R] Sampling the Distance Matrix

> mm <- cbind(1/(1:5), sqrt(1:5))
> d <- dist(mm)
> d
          1         2         3         4
2 0.6492864
3 0.9901226 0.3588848
4 1.2500000 0.6369033 0.2806086
5 1.4723668 0.8748970 0.5213550 0.2413050
> which(as.matrix(d)>0.9, arr.ind=TRUE)
  row col
3   3   1
4   4   1
5   5   1
1   1   3
1   1   4
1   1   5
I.e., the distances between mm's rows 3 & 1, 4 & 1, and 5,1 are more than 0.9

The as.matrix(d) is needed because dist returns the lower triangle of
the distance
matrix and an object of class "dist" and as.matrix.dist converts that
into a matrix.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Sep 23, 2015 at 12:15 PM, Lorenzo Isella
<lorenzo.isella at gmail.com> wrote:
> Dear All,
> Suppose you have a distance matrix stored like a dist object, for
> instance
>
> x<-rnorm(20)
> y<-rnorm(20)
>
> mm<-as.matrix(cbind(x,y))
>
> dst<-(dist(mm))
>
> Now, my problem is the following: I would like to get the rows of mm
> corresponding to points whose distance is always larger of, let's say,
> 0.9.
> In other words, if I were to compute the distance matrix on those
> selected rows of mm, apart from the diagonal, I would get all entries
> larger than 0.9.
> Any idea about how I can efficiently code that?
> Regards
>
> Lorenzo
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Thu Sep 24 00:00:50 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 24 Sep 2015 10:00:50 +1200
Subject: [R] [FORGED] Error from lme4: "Error: (p <- ncol(X)) == ncol(Y)
 is not TRUE"
In-Reply-To: <424436034.246678.1443016652708.JavaMail.yahoo@mail.yahoo.com>
References: <5601C76C.4000107@auckland.ac.nz>
	<424436034.246678.1443016652708.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <56032112.8000702@auckland.ac.nz>

On 24/09/15 01:57, Rory Wilson wrote:
> In reply to Rolf Turner and Jean Adams who have been helping me:
>
> This does appear to be an issue with NA values in the non-factor
> variables. In the (non-reproducible) example below, we can see that
> removing the NAs solves the problem. However, from what I can see to
> this point, there does not seem be be rhyme nor reason to why the issue
> is taking place. A slight modification to Rolf Turner's code
> (introducing some NAs) shows that, in general, NAs are not a problem for
> lmer (indeed, it just runs na.omit as default).
>
> Examining which factors are affected by the removal of the NAs shows no
> discernible pattern - no factors disappeared, none became "1" or
> anything of this nature.
> I will be able to proceed just by performing the na.omit beforehand, but
> it is curious.
>
> Thanks for your help everyone (especially Rolf Turner and Jean Adams)!
>
> mod1<-lmer(beta~expData+techVar$RIN+techVar$sample_storage_time+(1|techVar$p_amplification))
> #Error: (p <- ncol(X)) == ncol(Y) is not TRUE

First a pedantic quibble.  The foregoing call to lmer() would be better 
rendered as:

mod1 <- lmer(beta ~ expData + RIN + sample_storage_time
                             + (1|p_amplification), data=techVar)

I.e. Use the "data" argument (!!!) and put *spaces* in your code!

Second, can you not extract a relatively small subset of your data set 
which demonstrates the problem and make that cut-down data set available?

<SNIP>

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Thu Sep 24 00:10:23 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 23 Sep 2015 15:10:23 -0700
Subject: [R] get lm by file .cvs
In-Reply-To: <SNT150-W876A4664BF47B342DF9266AD440@phx.gbl>
References: <SNT150-W876A4664BF47B342DF9266AD440@phx.gbl>
Message-ID: <84CE94C7-103B-4CD6-9516-D161A71F05BD@comcast.net>


On Sep 23, 2015, at 10:09 AM, raoman ramirez wrote:

> well i have info<-read.csv(file, header=T)
> and i want try  thatmodel<-lm(info[1]~info[2])
> is for metod backward selection but i don't know how

Can you explain why you want to apply backward selection to a model with a single predictor vector?

-- 
David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Thu Sep 24 00:21:51 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 23 Sep 2015 15:21:51 -0700
Subject: [R] Error: pandoc version 1.12.3 or higher is required and was
	not found
In-Reply-To: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>
References: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>
Message-ID: <DBF810B2-9766-43AD-A9C6-EE10BF15ECC9@comcast.net>


On Sep 23, 2015, at 4:13 AM, Ryszard Czermi?ski wrote:

> I am trying to use R Markdown, but call to render() gives me an error:
> Error: pandoc version 1.12.3 or higher is required and was not found.
> 
> As I understand [http://yihui.name/knitr/demo/pandoc/] pandoc is a function
> defined in knitr, which I have installed and it has pandoc() function
> defined.

Pandoc ( to be distinguished from `pandoc()`) is an external-to-R software hammer capable of creating items of special beauty when supplied with dross text-materia.  `knitr` will be relying upon its availability when it casts its magical spells. `knitr` on CRAN is currently at version 1.11.

The incantations needed to acquire your very own pandoc hammer are in the link on that page entitled "Pandoc website".

> 
> Looks like some version incompatibility issue, but I do not really know how
> to resolve it.
> Do I need to go to older R version to use it?
> 
> I would appreciate your help.
> 
> Best regards,
> Ryszard
> 
>> sessionInfo()
> R version 3.2.2 (2015-08-14)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.10.3 (Yosemite)
> other attached packages: knitr_1.11
> [...]
> 
> Ryszard Czerminski
> 508-358-6328
> ryszard at czerminski.net
> LinkedIn.com/in/Ryszard.Czerminski
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Thu Sep 24 00:38:24 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 23 Sep 2015 15:38:24 -0700
Subject: [R] reproducing Graphpad IC50 in R with drc package
In-Reply-To: <CAE34pwBZCoz3toKcrHon1ZgMTHjumoSKs5NBYaJKVQ0k-Tca4A@mail.gmail.com>
References: <CAE34pwBZCoz3toKcrHon1ZgMTHjumoSKs5NBYaJKVQ0k-Tca4A@mail.gmail.com>
Message-ID: <AD6EE782-977A-4C54-95F4-57D2273BD453@comcast.net>


On Sep 23, 2015, at 9:21 AM, Jon Arsenault wrote:

> I've been banging my head against the wall a bit with this and would be
> ecstatic if someone could help.
> 
> Initial data:
> 
>     Response Dose
> 1  285.17      0.125
> 2  377.65      0.250
> 3  438.99      0.500
> 4  338.46      1.000
> 5  227.87      2.000
> 6  165.96      0.010
> 7  302.92      0.125
> 8  418.50      0.250
> 9  464.69      0.500
> 10 301.36      1.000
> 11 213.12      2.000
> 12 160.34      0.010
> 13 306.18      0.125
> 14 435.37      0.250
> 15 451.34      0.500
> 16 319.50      1.000
> 17 219.83      2.000
> 18 172.52      0.010
> 19 306.56      0.125
> 20 439.01      0.250
> 21 469.74      0.500
> 22 318.05      1.000
> 23 223.09      2.000
> 
> The graphpad template that that this normally would be placed in then
> transforms the taking the log(Dose,base=10) and normalizes the Response.
> I've confirmed that I was able to recreate that here:
> 
>   NormResponse  LogDose
> 1        41.011 -0.90309
> 2        72.909 -0.60206
> 3        94.067 -0.30103
> 4        59.392  0.00000
> 5        21.246  0.30103
> 6        -0.108 -2.00000
> 7        47.133 -0.90309
> 8        87.000 -0.60206
> 9       102.932 -0.30103
> 10       46.595  0.00000
> 11       16.159  0.30103
> 12       -2.047 -2.00000
> 13       48.258 -0.90309
> 14       92.819 -0.60206
> 15       98.327 -0.30103
> 16       52.852  0.00000
> 17       18.473  0.30103
> 18        2.155 -2.00000
> 19       48.389 -0.90309
> 20       94.074 -0.60206
> 21      104.674 -0.30103
> 22       52.352  0.00000
> 23       19.598  0.30103
> 
> 
> 
> 
> Now graphpad used 'log(inhibitor) vs. normalized response -- Variable
> slope' which it states to be a 4parameter sigmodial but it gives me very
> different results.

"It" gives you very different results, but what is "it", what does it "give" and what are you comparing it to? You should start by identifying the package you are using, and then present the code. (We cannot read you mind.)

> graphpad gives and IC50 of 0.1560 and drm's LL.4 gives 0.1149 and it wont
> even take the transformed data,just throws an error.
> 
> 	[[alternative HTML version deleted]]

But we can tell see that you are not reading the Posting Guide.

> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From xie at yihui.name  Thu Sep 24 00:42:35 2015
From: xie at yihui.name (Yihui Xie)
Date: Wed, 23 Sep 2015 17:42:35 -0500
Subject: [R] Error: pandoc version 1.12.3 or higher is required and was
 not found
In-Reply-To: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>
References: <CANS_STh09xL2x0wxxdd1QZBH3PgSn27E-X6NsS5YLPduf8k5eQ@mail.gmail.com>
Message-ID: <CANROs4e6NE3w9gB2PntCfejTd8vsw2BCaeGmZHbWD+YNKiaReg@mail.gmail.com>

I guess the confusion here is the relationship between knitr::pandoc()
and rmarkdown::render(). The error message you saw was from
rmarkdown::render(), which requires Pandoc 1.12.3. The easiest way to
go to use rmarkdown (I mean the R package rmarkdown) is to use
RStudio, and you don't even need to install Pandoc separately. There
are a number of possible reasons for the failure you saw: 1) You
didn't install Pandoc; 2) You installed but didn't put it on PATH; 3)
You installed a lower version of Pandoc. I don't mean you should not
figure out the exact reason, but it just saves so much time not having
to take care of such technical details by yourself.

knitr::pandoc() is almost a completely different story. If you are
familiar with Pandoc command-line arguments, please feel free to use
it. If you don't want to waste time on remembering those arguments, go
for rmarkdown::render() instead, which has a much better interface to
Pandoc than knitr::pandoc(). As the author of knitr::pandoc(), I can
tell you this function was about two afternoon's work, and rmarkdown
has been under active development for almost two years now. Hopefully
that makes it clear enough for you to choose between knitr::pandoc()
and rmarkdown::render() :-)

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Wed, Sep 23, 2015 at 6:13 AM, Ryszard Czermi?ski
<ryszard at czerminski.net> wrote:
> I am trying to use R Markdown, but call to render() gives me an error:
> Error: pandoc version 1.12.3 or higher is required and was not found.
>
> As I understand [http://yihui.name/knitr/demo/pandoc/] pandoc is a function
> defined in knitr, which I have installed and it has pandoc() function
> defined.
>
> Looks like some version incompatibility issue, but I do not really know how
> to resolve it.
> Do I need to go to older R version to use it?
>
> I would appreciate your help.
>
> Best regards,
> Ryszard
>
>> sessionInfo()
> R version 3.2.2 (2015-08-14)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.10.3 (Yosemite)
> other attached packages: knitr_1.11
> [...]
>
> Ryszard Czerminski
> 508-358-6328
> ryszard at czerminski.net
> LinkedIn.com/in/Ryszard.Czerminski


From johannes at huesing.name  Thu Sep 24 08:04:40 2015
From: johannes at huesing.name (Johannes Huesing)
Date: Thu, 24 Sep 2015 08:04:40 +0200
Subject: [R] Randomness tests
In-Reply-To: <248E6FA047A8C746BA491485764190F522091D35@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F522091D35@ESESSMB207.ericsson.se>
Message-ID: <20150924060440.GA12519@huesing.name>



Giorgio Garziano <giorgio.garziano at ericsson.com> [Tue, Sep 22, 2015 at 10:24:25PM CEST]:
>Hi,
>
>to test randomness of time series whose values can only be +1 and -1, are all following
>randomness tests applicable or only a part of ?

I don't know the details of all those tests but the general problem is
that the alternative hypothesis of "non-randomness" is of infinite
dimension and thus not testable. It depends on which deviations from
randomness you are interested in. You may also want to look into 
scan statistics but I presume that difference.sign.test() will be 
sensitive in a similar direction.

-- 
Johannes H?sing               
http://derwisch.wikidot.com


From Julias_89 at gmx.net  Thu Sep 24 09:27:24 2015
From: Julias_89 at gmx.net (Julia89)
Date: Thu, 24 Sep 2015 00:27:24 -0700 (PDT)
Subject: [R] boxplot overlap beeswarm
Message-ID: <1443079644936-4712727.post@n4.nabble.com>

Hi everybody,
I'm new and i need help very fast.
I will make a transparent boxplot overlap a beeswarm.
I don't want to use ggplot2, i will use ggplot.
Here is my own script but it doesn't worked.
Maybe you can help me.
Thanks in advance
Julia

require(beeswarm)#rohdaten boxplots

rm(list=ls())
setwd("C:/Dokumente und Einstellungen/jbellsta/Desktop/Paper")

#read data
data =
read.csv(file="HypocotylCellLength.csv",stringsAsFactors=FALSE,header=TRUE,
as.is=TRUE, sep=",")
str(data)
attach(data)


#schreibe gr????ten wert in Variable L.max
L.max <- max(LengthMM)

boxplot(LengthMM ~ Label,
boxwex=.3, 		
outline=F,
add=T, 	
ylim=c(0, L.max))		

beeswarm(LengthMM ~ Label,  # Datenpunkte LengthMM~Label  
pch = 20, 	
cex= .6, 		
spacing = .3,	
method =c("swarm"), 	  				
col= c('grey')	
)                              






--
View this message in context: http://r.789695.n4.nabble.com/boxplot-overlap-beeswarm-tp4712727.html
Sent from the R help mailing list archive at Nabble.com.


From lists at dewey.myzen.co.uk  Thu Sep 24 09:51:53 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Thu, 24 Sep 2015 08:51:53 +0100
Subject: [R] doubt with Odds ratio - URGENT HELP NEEDED
In-Reply-To: <2EE44B10-737B-4F07-A110-2A8C15707FC3@gmail.com>
References: <00BC0247-0C23-46CF-8E7A-509186C10FCC@gmail.com>
	<560286AC.4000808@dewey.myzen.co.uk>
	<EAF5F6E7-6905-4F02-99B7-8E50FE07B446@gmail.com>
	<5602C561.7060207@dewey.myzen.co.uk>
	<2EE44B10-737B-4F07-A110-2A8C15707FC3@gmail.com>
Message-ID: <5603AB99.7050108@dewey.myzen.co.uk>

Dear Rosa

Please keep the list on the recipients as others may be able to help.

See inline

On 23/09/2015 19:19, Rosa Oliveira wrote:
> Dear Michael,
>
> *New cleaned code :)    (I think :))*
>
> casedata <-read.spss("tas_05112008.sav")
> tas.data<-data.frame(casedata)
>
> #Delete patients that were not discharged
> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>
> tas.data$tas_d2      <-
> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
> tas.data$tas_d2))
> tas.data$tas_d3      <-
> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
> tas.data$tas_d3))
> tas.data$tas_d4      <-
> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
> tas.data$tas_d4))
> tas.data$tas_d5      <-
> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
> tas.data$tas_d5))
> tas.data$tas_d6      <-
> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
> tas.data$tas_d6))
>
>      tas.data$age      <-
> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>
>
>      tas.data                     <-   data.frame(tas1 =
> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>                                               tas3 = tas.data$tas_d4,
> tas4 = tas.data$tas_d5,
>                                               tas5 = tas.data$tas_d6,
> age = tas.data$age,
>                                               discharge =
> tas.data$resultado.hosp, id.pat=tas.data$id)
>
> #    tas.data$discharge              <- factor(   tas.data$discharge ,
> levels=c(0,1), labels = c("dead", "alive"))
>
>    #select only cases that have more than 3 tas
>      tas.data                      <- tas.data[apply(tas.data[,-8:-6],
> 1, function(x) sum(!is.na(x)))>2,]
>
>
>      nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients
> with more than 2 tas measurements
>
>      tas.data.long                 <- data.frame(
> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>                                         id=rep(c(1:n.obs), 5))
>      tas.data.long                 <- tas.data.long
>   [order(tas.data.long$id),]
>
>      age=tas.data$age
>
>
>
> library(verification)

What does that do?

> prevOR1 <-
> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
> ORmodel1 <- exp(prevOR1$coeff[,1])#####computes OR?
> ORmodel1
>
> prevOR2 <-
> summary(glm(tas.data[,7]~tas.data[,4]+tas.data[,6],family=binomial(link=probit)))
> ORmodel2 <- exp(prevOR2$coeff[,1])#####computes OR?
> ORmodel2
>

So are you happy that those are odds ratios but you need the confidence 
intervals now?

?confint

may help you
>
> Nonetheless I can?t get OR confidence intervals :( and i?m not sure if I
> have it right :(
>
> Best,
> RO
>
>
>
> Atenciosamente,
> Rosa Oliveira
>
> --
> ____________________________________________________________________________
>
>
> Rosa Celeste dos Santos Oliveira,
>
> E-mail:rosita21 at gmail.com <mailto:rosita21 at gmail.com>
> Tlm: +351 939355143
> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
> ____________________________________________________________________________
> "Many admire, few know"
> Hippocrates
>
>> On 23 Sep 2015, at 16:29, Michael Dewey <lists at dewey.myzen.co.uk
>> <mailto:lists at dewey.myzen.co.uk>> wrote:
>>
>> Dear Rosa
>>
>> Can you remove all the code which is not relevant to calculating the
>> odds ratio so we can see what is going on?
>>
>> On 23/09/2015 16:06, Rosa Oliveira wrote:
>>> Dear Michael,
>>>
>>>
>>> I found some of the errors, but others I wasn?t able to.
>>>
>>> And my huge huge problem concerns OR and OR confidence interval :(
>>>
>>>
>>> *New Corrected code:*
>>>
>>>
>>> casedata <-read.spss("tas_05112008.sav")
>>> tas.data<-data.frame(casedata)
>>>
>>> #Delete patients that were not discharged
>>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>
>>> tas.data$tas_d2      <-
>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>> tas.data$tas_d2))
>>> tas.data$tas_d3      <-
>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>> tas.data$tas_d3))
>>> tas.data$tas_d4      <-
>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>> tas.data$tas_d4))
>>> tas.data$tas_d5      <-
>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>> tas.data$tas_d5))
>>> tas.data$tas_d6      <-
>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>> tas.data$tas_d6))
>>>
>>>     tas.data$age      <-
>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>
>>>
>>>     tas.data                     <-   data.frame(tas1 =
>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>                                              tas3 = tas.data$tas_d4,
>>> tas4 = tas.data$tas_d5,
>>>                                              tas5 = tas.data$tas_d6,
>>> age = tas.data$age,
>>>                                              discharge =
>>> tas.data$resultado.hosp, id.pat=tas.data$id)
>>>
>>> #    tas.data$discharge              <- factor(   tas.data$discharge ,
>>> levels=c(0,1), labels = c("dead", "alive"))
>>>
>>>   #select only cases that have more than 3 tas
>>>     tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>> 1, function(x) sum(!is.na(x)))>2,]
>>>
>>>
>>>     nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients
>>> with more than 2 tas measurements
>>>
>>>     tas.data.long                 <- data.frame(
>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>                                        id=rep(c(1:n.obs), 5))
>>>     tas.data.long                 <- tas.data.long
>>>  [order(tas.data.long$id),]
>>>
>>>     age=tas.data$age
>>>
>>> ##################################################################################################
>>> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>> ##################################################################################################
>>>   mean.alive                      <-
>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>   mean.dead                       <-
>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>   stderr                          <- function(x)
>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>   se.alive                        <-
>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>   se.dead                         <-
>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>   summarytas                      <- data.frame (media = c(mean.dead,
>>> mean.alive),
>>>                                       standarderror = c( se.dead,
>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>
>>>
>>> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) +
>>>     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>> standarderror), width=.1) +
>>>   scale_color_manual(values=c("blue", "red")) +
>>>    theme(legend.text=element_text(size=20),
>>> axis.text=element_text(size=16),
>>> axis.title=element_text(size=20,face="bold")) +
>>>    scale_x_continuous(name="Days") +
>>>   scale_y_continuous(name="log tas") +
>>>   geom_line() +
>>>     geom_point()
>>>
>>>
>>> library(verification)
>>> prev <-
>>> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>>> answer            = c(prev$coefficients[,1:2])
>>>
>>>
>>> roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
>>>
>>>
>>>
>>> modelo<-function (dataainit)
>>> {
>>>
>>> #dataa<-tas.data
>>> dataa<-dataainit
>>>
>>> dataa$ident<-seq(1:90)
>>> tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>> dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>> time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>> ident=rep(dataa$ident,5))
>>> tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>> tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>> #mixed model for the longitudinal tas
>>> lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>> na.action=na.exclude )
>>> #Random intercept and slopes
>>> pred.lme<-predict(lme.1)
>>> lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>> lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>> selector<-as.numeric(names(lme.intercept)) #to select not NA rows. Apply
>>> to the vector in the dataset
>>>  test(dataa$intercept[resultado.hosp==1],
>>> dataa$intercept[resultado.hosp==0])
>>> print(summary(model.surv1))
>>> return(model.surv1$coef)
>>>  }
>>>
>>>
>>> *CONSOLE RESULT: (errors in red)*
>>>
>>> > casedata <-read.spss("tas_05112008.sav")
>>> Warning message:
>>> In read.spss("tas_05112008.sav") :
>>>   tas_05112008.sav: Unrecognized record type 7, subtype 18 encountered
>>> in system file
>>> > tas.data<-data.frame(casedata)
>>> >
>>> > #Delete patients that were not discharged
>>> > tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>> > tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>> >
>>> > tas.data$tas_d2      <-
>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>> tas.data$tas_d2))
>>> > tas.data$tas_d3      <-
>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>> tas.data$tas_d3))
>>> > tas.data$tas_d4      <-
>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>> tas.data$tas_d4))
>>> > tas.data$tas_d5      <-
>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>> tas.data$tas_d5))
>>> > tas.data$tas_d6      <-
>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>> tas.data$tas_d6))
>>> >
>>> >     tas.data$age      <-
>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>> >
>>> >
>>> >     tas.data                     <-   data.frame(tas1 =
>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>> +                                              tas3 = tas.data$tas_d4,
>>> tas4 = tas.data$tas_d5,
>>> +                                              tas5 = tas.data$tas_d6,
>>> age = tas.data$age,
>>> +                                              discharge =
>>> tas.data$resultado.hosp, id.pat=tas.data$id)
>>> >
>>> > #    tas.data$discharge              <- factor(   tas.data$discharge
>>> , levels=c(0,1), labels = c("dead", "alive"))
>>> >
>>> >   #select only cases that have more than 3 tas
>>> >     tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>> 1, function(x) sum(!is.na(x)))>2,]
>>> >
>>> >
>>> >
>>> >     nsample <- n.obs              <- dim(tas.data)[1]  #nr of
>>> patients with more than 2 tas measurements
>>> >
>>> >     tas.data.long                 <- data.frame(
>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>> +                                        id=rep(c(1:n.obs), 5))
>>> >     tas.data.long                 <- tas.data.long
>>>  [order(tas.data.long$id),]
>>> >
>>> >     age=tas.data$age
>>> >
>>> >
>>> ##################################################################################################
>>> > #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>> >
>>> ##################################################################################################
>>> >   mean.alive                      <-
>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>> >   mean.dead                       <-
>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>> >   stderr                          <- function(x)
>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>> >   se.alive                        <-
>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>> >   se.dead                         <-
>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>> >   summarytas                      <- data.frame (media = c(mean.dead,
>>> mean.alive),
>>> +                                       standarderror = c( se.dead,
>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>> >
>>> >
>>> > ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean,
>>> colour=discharge)) +
>>> +     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>> standarderror), width=.1) +
>>> +   scale_color_manual(values=c("blue", "red")) +
>>> +    theme(legend.text=element_text(size=20),
>>> axis.text=element_text(size=16),
>>> axis.title=element_text(size=20,face="bold")) +
>>> +    scale_x_continuous(name="Days") +
>>> +   scale_y_continuous(name="log tas") +
>>> +   geom_line() +
>>> +     geom_point()
>>> Error in mean - 2 * standarderror :
>>>   non-numeric argument to binary operator
>>> >
>>> >
>>> > library(verification)
>>> > prev <-
>>> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>>> > answer            = c(prev$coefficients[,1:2])
>>> >
>>> >
>>> > roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
>>> Error in is.finite(x) : default method not implemented for type 'list'
>>> >
>>> >
>>> >
>>> > modelo<-function (dataainit)
>>> +
>>> + {
>>> +
>>> + #dataa<-tas.data
>>> + dataa<-dataainit
>>> +
>>> + dataa$ident<-seq(1:90)
>>> + tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>> + dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>> + time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>> ident=rep(dataa$ident,5))
>>> +
>>> + tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>> + tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>> +
>>> + #mixed model for the longitudinal tas
>>> + lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>> na.action=na.exclude )
>>> +
>>> + #Random intercept and slopes
>>> + pred.lme<-predict(lme.1)
>>> + lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>> + lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>> + selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
>>> Apply to the vector in the dataset
>>> +
>>> +  test(dataa$intercept[resultado.hosp==1],
>>> dataa$intercept[resultado.hosp==0])
>>> +
>>> + print(summary(model.surv1))
>>> + return(model.surv1$coef)
>>> +
>>> +  }
>>> >
>>>
>>> I can?t get the OR and OR CI :(
>>>
>>>
>>> *DATA:*
>>>
>>>
>>>
>>>
>>>
>>>
>>> Best,
>>>
>>> RO
>>>
>>>
>>>
>>>
>>> Atenciosamente,
>>> Rosa Oliveira
>>>
>>> --
>>> ____________________________________________________________________________
>>>
>>>
>>>
>>>
>>> Rosa Celeste dos Santos Oliveira,
>>>
>>> E-mail:rosita21 at gmail.com <http://gmail.com> <mailto:rosita21 at gmail.com>
>>> Tlm: +351 939355143
>>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>>> ____________________________________________________________________________
>>> "Many admire, few know"
>>> Hippocrates
>>>
>>>> On 23 Sep 2015, at 12:02, Michael Dewey <lists at dewey.myzen.co.uk
>>>> <mailto:lists at dewey.myzen.co.uk>
>>>> <mailto:lists at dewey.myzen.co.uk>> wrote:
>>>>
>>>> Dear Rosa
>>>>
>>>> It would help if you posted the error messages where they occur so
>>>> that we can see which of your commands caused which error. However see
>>>> comment inline below.
>>>>
>>>> On 22/09/2015 22:17, Rosa Oliveira wrote:
>>>>> Dear all,
>>>>>
>>>>>
>>>>> I?m trying to compute Odds ratio and OR confidence interval.
>>>>>
>>>>> I?m really naive, sorry for that.
>>>>>
>>>>>
>>>>> I attach my data and my code.
>>>>>
>>>>> I?m having lots of errors:
>>>>>
>>>>> 1. Error in data.frame(tas1 = tas.data$tas_d2, tas2 =
>>>>> tas.data$tas_d3, tas3 = tas.data$tas_d4,  :
>>>>>  arguments imply differing number of rows: 90, 0
>>>>>
>>>>
>>>> At least one of tas_d2, tas_d3, tas_d4 does not exist
>>>>
>>>> I suggest fixing that one and hoping the rest go away.
>>>>
>>>>> 2. Error in data.frame(tas = c(unlist(tas.data[, -8:-6])), time =
>>>>> rep(c(0:4),  :
>>>>>  arguments imply differing number of rows: 630, 450, 0
>>>>>
>>>>> 3. Error: object 'tas.data.long' not found
>>>>>
>>>>> 4. Error in data.frame(media = c(mean.dead, mean.alive),
>>>>> standarderror = c(se.dead,  :
>>>>>  arguments imply differing number of rows: 14, 10
>>>>>
>>>>> 5. Error in ggplot(summarytas, aes(x = c(c(1:5), c(1:5)), y = mean,
>>>>> colour = discharge)) :
>>>>>  object 'summarytas' not found
>>>>>
>>>>> 6. Error in summary(glm(tas.data[, 6] ~ tas.data[, 4], family =
>>>>> binomial(link = probit))) :
>>>>>  error in evaluating the argument 'object' in selecting a method for
>>>>> function 'summary': Error in eval(expr, envir, enclos) : y values
>>>>> must be 0 <= y <= 1
>>>>>
>>>>> 7. Error in wilcox.test.default(pred[obs == 1], pred[obs == 0],
>>>>> alternative = "great") :
>>>>>  not enough (finite) 'x' observations
>>>>> In addition: Warning message:
>>>>> In is.finite(x) & apply(pred, 1, f) :
>>>>>  longer object length is not a multiple of shorter object length
>>>>>
>>>>>
>>>>> and off course I?m not getting OR.
>>>>>
>>>>> Nonetheless all this errors, I think I have not been able to compute
>>>>> de code to get OR and OR confidence interval.
>>>>>
>>>>>
>>>>> Can anyone help me please. It?s really urgent.
>>>>>
>>>>> PLEASE
>>>>>
>>>>> THE CODE:
>>>>>
>>>>> the hospital outcome is discharge.
>>>>>
>>>>> require(gdata)
>>>>> library(foreign)
>>>>> library(nlme)
>>>>> library(lme4)
>>>>> library(boot)
>>>>> library(MASS)
>>>>> library(Hmisc)
>>>>> library(plotrix)
>>>>> library(verification)
>>>>> library(mvtnorm)
>>>>> library(statmod)
>>>>> library(epiR)
>>>>>
>>>>> #########################################################################################
>>>>> # Data preparation
>>>>>                                                                     #
>>>>> #########################################################################################
>>>>>
>>>>> setwd("/Users/RO/Desktop")
>>>>>
>>>>> casedata <-read.spss("tas_05112008.sav")
>>>>> tas.data<-data.frame(casedata)
>>>>>
>>>>> #Delete patients that were not discharged
>>>>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>>>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>>>
>>>>> tas.data$tas_d2      <-
>>>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>>>> tas.data$tas_d2))
>>>>> tas.data$tas_d3      <-
>>>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>>>> tas.data$tas_d3))
>>>>> tas.data$tas_d4      <-
>>>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>>>> tas.data$tas_d4))
>>>>> tas.data$tas_d5      <-
>>>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>>>> tas.data$tas_d5))
>>>>> tas.data$tas_d6      <-
>>>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>>>> tas.data$tas_d6))
>>>>>
>>>>>    tas.data$age      <-
>>>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>>>
>>>>>
>>>>>    tas.data                     <-   data.frame(tas1 =
>>>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>>>                                             tas3 = tas.data$tas_d4,
>>>>> tas4 = tas.data$tas_d5,
>>>>>                                             tas5 = tas.data$tas_d6,
>>>>> age = tas.data$age,
>>>>>                                             discharge =
>>>>> tas.data$resultado.hosp, id.pat=tas.data$ID)
>>>>>
>>>>> #    tas.data$discharge              <- factor(   tas.data$discharge
>>>>> , levels=c(0,1), labels = c("dead", "alive"))
>>>>>
>>>>>  #select only cases that have more than 3 tas
>>>>>    tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>>>> 1, function(x) sum(!is.na(x)))>2,]
>>>>>
>>>>>
>>>>>
>>>>>    nsample <- n.obs              <- dim(tas.data)[1]  #nr of
>>>>> patients with more than 2 tas measurements
>>>>>
>>>>>    tas.data.long                 <- data.frame(
>>>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>>>                                       id=rep(c(1:n.obs), 5))
>>>>>    tas.data.long                 <- tas.data.long
>>>>> [order(tas.data.long$id),]
>>>>>
>>>>>    age=tas.data$age
>>>>>
>>>>> ##################################################################################################
>>>>> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>>>> ##################################################################################################
>>>>>  mean.alive                      <-
>>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>>>  mean.dead                       <-
>>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>>>  stderr                          <- function(x)
>>>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>>>  se.alive                        <-
>>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>>>  se.dead                         <-
>>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>>>  summarytas                      <- data.frame (media = c(mean.dead,
>>>>> mean.alive),
>>>>>                                      standarderror = c( se.dead,
>>>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>>>
>>>>>
>>>>> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean,
>>>>> colour=discharge)) +
>>>>>    geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>>>> standarderror), width=.1) +
>>>>>  scale_color_manual(values=c("blue", "red")) +
>>>>>   theme(legend.text=element_text(size=20),
>>>>> axis.text=element_text(size=16),
>>>>> axis.title=element_text(size=20,face="bold")) +
>>>>>   scale_x_continuous(name="Days") +
>>>>>  scale_y_continuous(name="log tas") +
>>>>>  geom_line() +
>>>>>    geom_point()
>>>>>
>>>>>
>>>>> library(verification)
>>>>> prev <-
>>>>> summary(glm(tas.data[,6]~tas.data[,4],family=binomial(link=probit)))
>>>>> answer = c(prev$coefficients[,1:2])
>>>>>
>>>>>
>>>>> roc.plot(tas.data[,6], prev, show.thres = FALSE, legen=F )
>>>>>
>>>>>
>>>>> modelo<-function (dataainit)
>>>>>
>>>>> {
>>>>>
>>>>> #dataa<-tas.data
>>>>> dataa<-dataainit
>>>>>
>>>>> dataa$ident<-seq(1:90)
>>>>> tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>>>> dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>>>> time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>>>> ident=rep(dataa$ident,5))
>>>>>
>>>>> tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>>>> tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>>>>
>>>>> #mixed model for the longitudinal tas
>>>>> lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>>>> na.action=na.exclude )
>>>>>
>>>>> #Random intercept and slopes
>>>>> pred.lme<-predict(lme.1)
>>>>> lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>>>> lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>>>> selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
>>>>> Apply to the vector in the dataset
>>>>>
>>>>> test(dataa$intercept[resultado.hosp==1],
>>>>> dataa$intercept[resultado.hosp==0])
>>>>>
>>>>> print(summary(model.surv1))
>>>>> return(model.surv1$coef)
>>>>>
>>>>> }
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> Best,
>>>>> RO
>>>>>
>>>>>
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org <mailto:R-help at r-project.org>
>>>>> <mailto:R-help at r-project.org> mailing list -- To
>>>>> UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>> --
>>>> Michael
>>>> http://www.dewey.myzen.co.uk/home.html
>>>
>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From pdalgd at gmail.com  Thu Sep 24 11:41:19 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 24 Sep 2015 11:41:19 +0200
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <5601F36D.5060906@gmail.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
	<CAGxFJbQX0vTBe9jm_Y-eZA=Q4bvM5sdeAJGSGF-1OyrqtaO+cA@mail.gmail.com>
	<4CC4EADB-60C6-4992-BE6E-D692C0B98DA6@me.com>
	<5EAEC047-AA21-4FE5-B67B-88C4E02870A3@gmail.com>
	<5601F36D.5060906@gmail.com>
Message-ID: <6AEB6B05-8B0F-4FEF-A5CD-36DB7B970C36@gmail.com>


On 23 Sep 2015, at 02:33 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:

> I don't see why this puzzles you.  A simple explanation is that Urkund
> is incompetent.

That much I figured. What I was puzzled about was _how_ it was being incompetent. Also how it could be so in a way that wouldn't be obvious to the professor in question. 

I haven't used Urkund, but people are pushing it here at CBS too. If they start automating it on electronic submissions in maths and stats, some interesting things could happen. I have used iThenticate as a journal editor, and that one will tell you word for word which sections of text are identical to which sections of text in which other documents. 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From p_connolly at slingshot.co.nz  Thu Sep 24 11:52:04 2015
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Thu, 24 Sep 2015 21:52:04 +1200
Subject: [R] Special characters in regular expressions
Message-ID: <20150924095204.GC5022@slingshot.co.nz>

I need to change a vector dd that looks like this:
c("LBAM 5|A|15C|3h", "LBAM 5|K|15C|2h") 

into this:
c("LBAM 5|15C|3h", "LBAM 5|15C|2h")

It's not very imaginative, but I could use a complicated nesting of
gsub() as so:

gsub("-", "\\|", gsub("K-", "", gsub("A-", "", gsub("\\|", "-", dd))))

Or I could make it a bit more readable by using interim objects, 

But I'd prefer to use a single regular expression that can detect "A|"
*and* "K|" without collateral damage from the impact of special
characters and regular characters.

TIA

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From thierry.onkelinx at inbo.be  Thu Sep 24 12:05:45 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 24 Sep 2015 12:05:45 +0200
Subject: [R] Special characters in regular expressions
In-Reply-To: <20150924095204.GC5022@slingshot.co.nz>
References: <20150924095204.GC5022@slingshot.co.nz>
Message-ID: <CAJuCY5wU4uWBs6m2m1Rjnqj_K4=ofr0C4wSDYKF2aQWd5Y2wjw@mail.gmail.com>

gsub("[A|K]\\|", "", x)

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-09-24 11:52 GMT+02:00 Patrick Connolly <p_connolly at slingshot.co.nz>:

> I need to change a vector dd that looks like this:
> c("LBAM 5|A|15C|3h", "LBAM 5|K|15C|2h")
>
> into this:
> c("LBAM 5|15C|3h", "LBAM 5|15C|2h")
>
> It's not very imaginative, but I could use a complicated nesting of
> gsub() as so:
>
> gsub("-", "\\|", gsub("K-", "", gsub("A-", "", gsub("\\|", "-", dd))))
>
> Or I could make it a bit more readable by using interim objects,
>
> But I'd prefer to use a single regular expression that can detect "A|"
> *and* "K|" without collateral damage from the impact of special
> characters and regular characters.
>
> TIA
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>    ___    Patrick Connolly
>  {~._.~}                   Great minds discuss ideas
>  _( Y )_                 Average minds discuss events
> (:_~*~_:)                  Small minds discuss people
>  (_)-(_)                              ..... Eleanor Roosevelt
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Thu Sep 24 12:38:28 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 24 Sep 2015 12:38:28 +0200
Subject: [R] Special characters in regular expressions
In-Reply-To: <CAJuCY5wU4uWBs6m2m1Rjnqj_K4=ofr0C4wSDYKF2aQWd5Y2wjw@mail.gmail.com>
References: <20150924095204.GC5022@slingshot.co.nz>
	<CAJuCY5wU4uWBs6m2m1Rjnqj_K4=ofr0C4wSDYKF2aQWd5Y2wjw@mail.gmail.com>
Message-ID: <2C4D9A7C-5261-4DE9-B947-5A4011107965@gmail.com>


On 24 Sep 2015, at 12:05 , Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:

> gsub("[A|K]\\|", "", x)

That'll probably do it, but what was the point of the | in [A|K] ?? I don't think it does what I think you think it does...

Somewhat safer, maybe:

gsub("\\|[AK]\\|","\\|", x)

(avoids surprises from, say, "LBAM 5|A|15A|3h")

-pd

> [snip]
> 2015-09-24 11:52 GMT+02:00 Patrick Connolly <p_connolly at slingshot.co.nz>:
> 
>> I need to change a vector dd that looks like this:
>> c("LBAM 5|A|15C|3h", "LBAM 5|K|15C|2h")
>> 
>> into this:
>> c("LBAM 5|15C|3h", "LBAM 5|15C|2h")
>> 
>> It's not very imaginative, but I could use a complicated nesting of
>> gsub() as so:
>> 
>> gsub("-", "\\|", gsub("K-", "", gsub("A-", "", gsub("\\|", "-", dd))))
>> 
>> Or I could make it a bit more readable by using interim objects,
>> 
>> But I'd prefer to use a single regular expression that can detect "A|"
>> *and* "K|" without collateral damage from the impact of special
>> characters and regular characters.
>> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From rosita21 at gmail.com  Thu Sep 24 12:34:15 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Thu, 24 Sep 2015 11:34:15 +0100
Subject: [R] doubt with Odds ratio - URGENT HELP NEEDED
In-Reply-To: <5603AB99.7050108@dewey.myzen.co.uk>
References: <00BC0247-0C23-46CF-8E7A-509186C10FCC@gmail.com>
	<560286AC.4000808@dewey.myzen.co.uk>
	<EAF5F6E7-6905-4F02-99B7-8E50FE07B446@gmail.com>
	<5602C561.7060207@dewey.myzen.co.uk>
	<2EE44B10-737B-4F07-A110-2A8C15707FC3@gmail.com>
	<5603AB99.7050108@dewey.myzen.co.uk>
Message-ID: <B6C08B91-13C2-449C-B531-803B73DCCC09@gmail.com>

Dear Michael (and all :))

Thank you very much.

I fixed my problem, I think ;)

Best,
RO


Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates

> On 24 Sep 2015, at 08:51, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> 
> Dear Rosa
> 
> Please keep the list on the recipients as others may be able to help.
> 
> See inline
> 
> On 23/09/2015 19:19, Rosa Oliveira wrote:
>> Dear Michael,
>> 
>> *New cleaned code :)    (I think :))*
>> 
>> casedata <-read.spss("tas_05112008.sav")
>> tas.data<-data.frame(casedata)
>> 
>> #Delete patients that were not discharged
>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>> 
>> tas.data$tas_d2      <-
>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>> tas.data$tas_d2))
>> tas.data$tas_d3      <-
>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>> tas.data$tas_d3))
>> tas.data$tas_d4      <-
>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>> tas.data$tas_d4))
>> tas.data$tas_d5      <-
>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>> tas.data$tas_d5))
>> tas.data$tas_d6      <-
>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>> tas.data$tas_d6))
>> 
>>     tas.data$age      <-
>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>> 
>> 
>>     tas.data                     <-   data.frame(tas1 =
>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>                                              tas3 = tas.data$tas_d4,
>> tas4 = tas.data$tas_d5,
>>                                              tas5 = tas.data$tas_d6,
>> age = tas.data$age,
>>                                              discharge =
>> tas.data$resultado.hosp, id.pat=tas.data$id)
>> 
>> #    tas.data$discharge              <- factor(   tas.data$discharge ,
>> levels=c(0,1), labels = c("dead", "alive"))
>> 
>>   #select only cases that have more than 3 tas
>>     tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>> 1, function(x) sum(!is.na(x)))>2,]
>> 
>> 
>>     nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients
>> with more than 2 tas measurements
>> 
>>     tas.data.long                 <- data.frame(
>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>                                        id=rep(c(1:n.obs), 5))
>>     tas.data.long                 <- tas.data.long
>>  [order(tas.data.long$id),]
>> 
>>     age=tas.data$age
>> 
>> 
>> 
>> library(verification)
> 
> What does that do?
> 
>> prevOR1 <-
>> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>> ORmodel1 <- exp(prevOR1$coeff[,1])#####computes OR?
>> ORmodel1
>> 
>> prevOR2 <-
>> summary(glm(tas.data[,7]~tas.data[,4]+tas.data[,6],family=binomial(link=probit)))
>> ORmodel2 <- exp(prevOR2$coeff[,1])#####computes OR?
>> ORmodel2
>> 
> 
> So are you happy that those are odds ratios but you need the confidence intervals now?
> 
> ?confint
> 
> may help you
>> 
>> Nonetheless I can?t get OR confidence intervals :( and i?m not sure if I
>> have it right :(
>> 
>> Best,
>> RO
>> 
>> 
>> 
>> Atenciosamente,
>> Rosa Oliveira
>> 
>> --
>> ____________________________________________________________________________
>> 
>> 
>> Rosa Celeste dos Santos Oliveira,
>> 
>> E-mail:rosita21 at gmail.com <mailto:rosita21 at gmail.com>
>> Tlm: +351 939355143
>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>> ____________________________________________________________________________
>> "Many admire, few know"
>> Hippocrates
>> 
>>> On 23 Sep 2015, at 16:29, Michael Dewey <lists at dewey.myzen.co.uk
>>> <mailto:lists at dewey.myzen.co.uk>> wrote:
>>> 
>>> Dear Rosa
>>> 
>>> Can you remove all the code which is not relevant to calculating the
>>> odds ratio so we can see what is going on?
>>> 
>>> On 23/09/2015 16:06, Rosa Oliveira wrote:
>>>> Dear Michael,
>>>> 
>>>> 
>>>> I found some of the errors, but others I wasn?t able to.
>>>> 
>>>> And my huge huge problem concerns OR and OR confidence interval :(
>>>> 
>>>> 
>>>> *New Corrected code:*
>>>> 
>>>> 
>>>> casedata <-read.spss("tas_05112008.sav")
>>>> tas.data<-data.frame(casedata)
>>>> 
>>>> #Delete patients that were not discharged
>>>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>> 
>>>> tas.data$tas_d2      <-
>>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>>> tas.data$tas_d2))
>>>> tas.data$tas_d3      <-
>>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>>> tas.data$tas_d3))
>>>> tas.data$tas_d4      <-
>>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>>> tas.data$tas_d4))
>>>> tas.data$tas_d5      <-
>>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>>> tas.data$tas_d5))
>>>> tas.data$tas_d6      <-
>>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>>> tas.data$tas_d6))
>>>> 
>>>>    tas.data$age      <-
>>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>> 
>>>> 
>>>>    tas.data                     <-   data.frame(tas1 =
>>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>>                                             tas3 = tas.data$tas_d4,
>>>> tas4 = tas.data$tas_d5,
>>>>                                             tas5 = tas.data$tas_d6,
>>>> age = tas.data$age,
>>>>                                             discharge =
>>>> tas.data$resultado.hosp, id.pat=tas.data$id)
>>>> 
>>>> #    tas.data$discharge              <- factor(   tas.data$discharge ,
>>>> levels=c(0,1), labels = c("dead", "alive"))
>>>> 
>>>>  #select only cases that have more than 3 tas
>>>>    tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>>> 1, function(x) sum(!is.na(x)))>2,]
>>>> 
>>>> 
>>>>    nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients
>>>> with more than 2 tas measurements
>>>> 
>>>>    tas.data.long                 <- data.frame(
>>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>>                                       id=rep(c(1:n.obs), 5))
>>>>    tas.data.long                 <- tas.data.long
>>>> [order(tas.data.long$id),]
>>>> 
>>>>    age=tas.data$age
>>>> 
>>>> ##################################################################################################
>>>> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>>> ##################################################################################################
>>>>  mean.alive                      <-
>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>>  mean.dead                       <-
>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>>  stderr                          <- function(x)
>>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>>  se.alive                        <-
>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>>  se.dead                         <-
>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>>  summarytas                      <- data.frame (media = c(mean.dead,
>>>> mean.alive),
>>>>                                      standarderror = c( se.dead,
>>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>> 
>>>> 
>>>> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean, colour=discharge)) +
>>>>    geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>>> standarderror), width=.1) +
>>>>  scale_color_manual(values=c("blue", "red")) +
>>>>   theme(legend.text=element_text(size=20),
>>>> axis.text=element_text(size=16),
>>>> axis.title=element_text(size=20,face="bold")) +
>>>>   scale_x_continuous(name="Days") +
>>>>  scale_y_continuous(name="log tas") +
>>>>  geom_line() +
>>>>    geom_point()
>>>> 
>>>> 
>>>> library(verification)
>>>> prev <-
>>>> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>>>> answer            = c(prev$coefficients[,1:2])
>>>> 
>>>> 
>>>> roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
>>>> 
>>>> 
>>>> 
>>>> modelo<-function (dataainit)
>>>> {
>>>> 
>>>> #dataa<-tas.data
>>>> dataa<-dataainit
>>>> 
>>>> dataa$ident<-seq(1:90)
>>>> tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>>> dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>>> time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>>> ident=rep(dataa$ident,5))
>>>> tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>>> tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>>> #mixed model for the longitudinal tas
>>>> lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>>> na.action=na.exclude )
>>>> #Random intercept and slopes
>>>> pred.lme<-predict(lme.1)
>>>> lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>>> lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>>> selector<-as.numeric(names(lme.intercept)) #to select not NA rows. Apply
>>>> to the vector in the dataset
>>>> test(dataa$intercept[resultado.hosp==1],
>>>> dataa$intercept[resultado.hosp==0])
>>>> print(summary(model.surv1))
>>>> return(model.surv1$coef)
>>>> }
>>>> 
>>>> 
>>>> *CONSOLE RESULT: (errors in red)*
>>>> 
>>>> > casedata <-read.spss("tas_05112008.sav")
>>>> Warning message:
>>>> In read.spss("tas_05112008.sav") :
>>>>  tas_05112008.sav: Unrecognized record type 7, subtype 18 encountered
>>>> in system file
>>>> > tas.data<-data.frame(casedata)
>>>> >
>>>> > #Delete patients that were not discharged
>>>> > tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>>> > tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>> >
>>>> > tas.data$tas_d2      <-
>>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>>> tas.data$tas_d2))
>>>> > tas.data$tas_d3      <-
>>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>>> tas.data$tas_d3))
>>>> > tas.data$tas_d4      <-
>>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>>> tas.data$tas_d4))
>>>> > tas.data$tas_d5      <-
>>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>>> tas.data$tas_d5))
>>>> > tas.data$tas_d6      <-
>>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>>> tas.data$tas_d6))
>>>> >
>>>> >     tas.data$age      <-
>>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>> >
>>>> >
>>>> >     tas.data                     <-   data.frame(tas1 =
>>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>> +                                              tas3 = tas.data$tas_d4,
>>>> tas4 = tas.data$tas_d5,
>>>> +                                              tas5 = tas.data$tas_d6,
>>>> age = tas.data$age,
>>>> +                                              discharge =
>>>> tas.data$resultado.hosp, id.pat=tas.data$id)
>>>> >
>>>> > #    tas.data$discharge              <- factor(   tas.data$discharge
>>>> , levels=c(0,1), labels = c("dead", "alive"))
>>>> >
>>>> >   #select only cases that have more than 3 tas
>>>> >     tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>>> 1, function(x) sum(!is.na(x)))>2,]
>>>> >
>>>> >
>>>> >
>>>> >     nsample <- n.obs              <- dim(tas.data)[1]  #nr of
>>>> patients with more than 2 tas measurements
>>>> >
>>>> >     tas.data.long                 <- data.frame(
>>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>> +                                        id=rep(c(1:n.obs), 5))
>>>> >     tas.data.long                 <- tas.data.long
>>>> [order(tas.data.long$id),]
>>>> >
>>>> >     age=tas.data$age
>>>> >
>>>> >
>>>> ##################################################################################################
>>>> > #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>>> >
>>>> ##################################################################################################
>>>> >   mean.alive                      <-
>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>> >   mean.dead                       <-
>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>> >   stderr                          <- function(x)
>>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>> >   se.alive                        <-
>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>> >   se.dead                         <-
>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>> >   summarytas                      <- data.frame (media = c(mean.dead,
>>>> mean.alive),
>>>> +                                       standarderror = c( se.dead,
>>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>> >
>>>> >
>>>> > ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean,
>>>> colour=discharge)) +
>>>> +     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>>> standarderror), width=.1) +
>>>> +   scale_color_manual(values=c("blue", "red")) +
>>>> +    theme(legend.text=element_text(size=20),
>>>> axis.text=element_text(size=16),
>>>> axis.title=element_text(size=20,face="bold")) +
>>>> +    scale_x_continuous(name="Days") +
>>>> +   scale_y_continuous(name="log tas") +
>>>> +   geom_line() +
>>>> +     geom_point()
>>>> Error in mean - 2 * standarderror :
>>>>  non-numeric argument to binary operator
>>>> >
>>>> >
>>>> > library(verification)
>>>> > prev <-
>>>> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>>>> > answer            = c(prev$coefficients[,1:2])
>>>> >
>>>> >
>>>> > roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
>>>> Error in is.finite(x) : default method not implemented for type 'list'
>>>> >
>>>> >
>>>> >
>>>> > modelo<-function (dataainit)
>>>> +
>>>> + {
>>>> +
>>>> + #dataa<-tas.data
>>>> + dataa<-dataainit
>>>> +
>>>> + dataa$ident<-seq(1:90)
>>>> + tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>>> + dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>>> + time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>>> ident=rep(dataa$ident,5))
>>>> +
>>>> + tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>>> + tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>>> +
>>>> + #mixed model for the longitudinal tas
>>>> + lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>>> na.action=na.exclude )
>>>> +
>>>> + #Random intercept and slopes
>>>> + pred.lme<-predict(lme.1)
>>>> + lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>>> + lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>>> + selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
>>>> Apply to the vector in the dataset
>>>> +
>>>> +  test(dataa$intercept[resultado.hosp==1],
>>>> dataa$intercept[resultado.hosp==0])
>>>> +
>>>> + print(summary(model.surv1))
>>>> + return(model.surv1$coef)
>>>> +
>>>> +  }
>>>> >
>>>> 
>>>> I can?t get the OR and OR CI :(
>>>> 
>>>> 
>>>> *DATA:*
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> Best,
>>>> 
>>>> RO
>>>> 
>>>> 
>>>> 
>>>> 
>>>> Atenciosamente,
>>>> Rosa Oliveira
>>>> 
>>>> --
>>>> ____________________________________________________________________________
>>>> 
>>>> 
>>>> 
>>>> 
>>>> Rosa Celeste dos Santos Oliveira,
>>>> 
>>>> E-mail:rosita21 at gmail.com <http://gmail.com> <mailto:rosita21 at gmail.com>
>>>> Tlm: +351 939355143
>>>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>>>> ____________________________________________________________________________
>>>> "Many admire, few know"
>>>> Hippocrates
>>>> 
>>>>> On 23 Sep 2015, at 12:02, Michael Dewey <lists at dewey.myzen.co.uk
>>>>> <mailto:lists at dewey.myzen.co.uk>
>>>>> <mailto:lists at dewey.myzen.co.uk>> wrote:
>>>>> 
>>>>> Dear Rosa
>>>>> 
>>>>> It would help if you posted the error messages where they occur so
>>>>> that we can see which of your commands caused which error. However see
>>>>> comment inline below.
>>>>> 
>>>>> On 22/09/2015 22:17, Rosa Oliveira wrote:
>>>>>> Dear all,
>>>>>> 
>>>>>> 
>>>>>> I?m trying to compute Odds ratio and OR confidence interval.
>>>>>> 
>>>>>> I?m really naive, sorry for that.
>>>>>> 
>>>>>> 
>>>>>> I attach my data and my code.
>>>>>> 
>>>>>> I?m having lots of errors:
>>>>>> 
>>>>>> 1. Error in data.frame(tas1 = tas.data$tas_d2, tas2 =
>>>>>> tas.data$tas_d3, tas3 = tas.data$tas_d4,  :
>>>>>> arguments imply differing number of rows: 90, 0
>>>>>> 
>>>>> 
>>>>> At least one of tas_d2, tas_d3, tas_d4 does not exist
>>>>> 
>>>>> I suggest fixing that one and hoping the rest go away.
>>>>> 
>>>>>> 2. Error in data.frame(tas = c(unlist(tas.data[, -8:-6])), time =
>>>>>> rep(c(0:4),  :
>>>>>> arguments imply differing number of rows: 630, 450, 0
>>>>>> 
>>>>>> 3. Error: object 'tas.data.long' not found
>>>>>> 
>>>>>> 4. Error in data.frame(media = c(mean.dead, mean.alive),
>>>>>> standarderror = c(se.dead,  :
>>>>>> arguments imply differing number of rows: 14, 10
>>>>>> 
>>>>>> 5. Error in ggplot(summarytas, aes(x = c(c(1:5), c(1:5)), y = mean,
>>>>>> colour = discharge)) :
>>>>>> object 'summarytas' not found
>>>>>> 
>>>>>> 6. Error in summary(glm(tas.data[, 6] ~ tas.data[, 4], family =
>>>>>> binomial(link = probit))) :
>>>>>> error in evaluating the argument 'object' in selecting a method for
>>>>>> function 'summary': Error in eval(expr, envir, enclos) : y values
>>>>>> must be 0 <= y <= 1
>>>>>> 
>>>>>> 7. Error in wilcox.test.default(pred[obs == 1], pred[obs == 0],
>>>>>> alternative = "great") :
>>>>>> not enough (finite) 'x' observations
>>>>>> In addition: Warning message:
>>>>>> In is.finite(x) & apply(pred, 1, f) :
>>>>>> longer object length is not a multiple of shorter object length
>>>>>> 
>>>>>> 
>>>>>> and off course I?m not getting OR.
>>>>>> 
>>>>>> Nonetheless all this errors, I think I have not been able to compute
>>>>>> de code to get OR and OR confidence interval.
>>>>>> 
>>>>>> 
>>>>>> Can anyone help me please. It?s really urgent.
>>>>>> 
>>>>>> PLEASE
>>>>>> 
>>>>>> THE CODE:
>>>>>> 
>>>>>> the hospital outcome is discharge.
>>>>>> 
>>>>>> require(gdata)
>>>>>> library(foreign)
>>>>>> library(nlme)
>>>>>> library(lme4)
>>>>>> library(boot)
>>>>>> library(MASS)
>>>>>> library(Hmisc)
>>>>>> library(plotrix)
>>>>>> library(verification)
>>>>>> library(mvtnorm)
>>>>>> library(statmod)
>>>>>> library(epiR)
>>>>>> 
>>>>>> #########################################################################################
>>>>>> # Data preparation
>>>>>>                                                                    #
>>>>>> #########################################################################################
>>>>>> 
>>>>>> setwd("/Users/RO/Desktop")
>>>>>> 
>>>>>> casedata <-read.spss("tas_05112008.sav")
>>>>>> tas.data<-data.frame(casedata)
>>>>>> 
>>>>>> #Delete patients that were not discharged
>>>>>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>>>>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>>>> 
>>>>>> tas.data$tas_d2      <-
>>>>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>>>>> tas.data$tas_d2))
>>>>>> tas.data$tas_d3      <-
>>>>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>>>>> tas.data$tas_d3))
>>>>>> tas.data$tas_d4      <-
>>>>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>>>>> tas.data$tas_d4))
>>>>>> tas.data$tas_d5      <-
>>>>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>>>>> tas.data$tas_d5))
>>>>>> tas.data$tas_d6      <-
>>>>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>>>>> tas.data$tas_d6))
>>>>>> 
>>>>>>   tas.data$age      <-
>>>>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>>>> 
>>>>>> 
>>>>>>   tas.data                     <-   data.frame(tas1 =
>>>>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>>>>                                            tas3 = tas.data$tas_d4,
>>>>>> tas4 = tas.data$tas_d5,
>>>>>>                                            tas5 = tas.data$tas_d6,
>>>>>> age = tas.data$age,
>>>>>>                                            discharge =
>>>>>> tas.data$resultado.hosp, id.pat=tas.data$ID)
>>>>>> 
>>>>>> #    tas.data$discharge              <- factor(   tas.data$discharge
>>>>>> , levels=c(0,1), labels = c("dead", "alive"))
>>>>>> 
>>>>>> #select only cases that have more than 3 tas
>>>>>>   tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>>>>> 1, function(x) sum(!is.na(x)))>2,]
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>>   nsample <- n.obs              <- dim(tas.data)[1]  #nr of
>>>>>> patients with more than 2 tas measurements
>>>>>> 
>>>>>>   tas.data.long                 <- data.frame(
>>>>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>>>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>>>>                                      id=rep(c(1:n.obs), 5))
>>>>>>   tas.data.long                 <- tas.data.long
>>>>>> [order(tas.data.long$id),]
>>>>>> 
>>>>>>   age=tas.data$age
>>>>>> 
>>>>>> ##################################################################################################
>>>>>> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>>>>> ##################################################################################################
>>>>>> mean.alive                      <-
>>>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>>>> mean.dead                       <-
>>>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>>>> stderr                          <- function(x)
>>>>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>>>> se.alive                        <-
>>>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>>>> se.dead                         <-
>>>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>>>> summarytas                      <- data.frame (media = c(mean.dead,
>>>>>> mean.alive),
>>>>>>                                     standarderror = c( se.dead,
>>>>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>>>> 
>>>>>> 
>>>>>> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean,
>>>>>> colour=discharge)) +
>>>>>>   geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>>>>> standarderror), width=.1) +
>>>>>> scale_color_manual(values=c("blue", "red")) +
>>>>>>  theme(legend.text=element_text(size=20),
>>>>>> axis.text=element_text(size=16),
>>>>>> axis.title=element_text(size=20,face="bold")) +
>>>>>>  scale_x_continuous(name="Days") +
>>>>>> scale_y_continuous(name="log tas") +
>>>>>> geom_line() +
>>>>>>   geom_point()
>>>>>> 
>>>>>> 
>>>>>> library(verification)
>>>>>> prev <-
>>>>>> summary(glm(tas.data[,6]~tas.data[,4],family=binomial(link=probit)))
>>>>>> answer = c(prev$coefficients[,1:2])
>>>>>> 
>>>>>> 
>>>>>> roc.plot(tas.data[,6], prev, show.thres = FALSE, legen=F )
>>>>>> 
>>>>>> 
>>>>>> modelo<-function (dataainit)
>>>>>> 
>>>>>> {
>>>>>> 
>>>>>> #dataa<-tas.data
>>>>>> dataa<-dataainit
>>>>>> 
>>>>>> dataa$ident<-seq(1:90)
>>>>>> tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>>>>> dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>>>>> time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>>>>> ident=rep(dataa$ident,5))
>>>>>> 
>>>>>> tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>>>>> tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>>>>> 
>>>>>> #mixed model for the longitudinal tas
>>>>>> lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>>>>> na.action=na.exclude )
>>>>>> 
>>>>>> #Random intercept and slopes
>>>>>> pred.lme<-predict(lme.1)
>>>>>> lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>>>>> lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>>>>> selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
>>>>>> Apply to the vector in the dataset
>>>>>> 
>>>>>> test(dataa$intercept[resultado.hosp==1],
>>>>>> dataa$intercept[resultado.hosp==0])
>>>>>> 
>>>>>> print(summary(model.surv1))
>>>>>> return(model.surv1$coef)
>>>>>> 
>>>>>> }
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> Best,
>>>>>> RO
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org <mailto:R-help at r-project.org>
>>>>>> <mailto:R-help at r-project.org> mailing list -- To
>>>>>> UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>> 
>>>>> --
>>>>> Michael
>>>>> http://www.dewey.myzen.co.uk/home.html
>>>> 
>>> 
>>> --
>>> Michael
>>> http://www.dewey.myzen.co.uk/home.html
>> 
> 
> -- 
> Michael
> http://www.dewey.myzen.co.uk/home.html


From scupton at nps.edu  Thu Sep 24 13:52:18 2015
From: scupton at nps.edu (Upton, Stephen (Steve) (CIV))
Date: Thu, 24 Sep 2015 11:52:18 +0000
Subject: [R] Special characters in regular expressions
In-Reply-To: <2C4D9A7C-5261-4DE9-B947-5A4011107965@gmail.com>
References: <20150924095204.GC5022@slingshot.co.nz>
	<CAJuCY5wU4uWBs6m2m1Rjnqj_K4=ofr0C4wSDYKF2aQWd5Y2wjw@mail.gmail.com>
	<2C4D9A7C-5261-4DE9-B947-5A4011107965@gmail.com>
Message-ID: <C750A9148B73B84EB6293187A5F7DA75B81709E7@GROWLER.ern.nps.edu>

and a somewhat convoluted solution, if A or K are always in the second
"position"

x <-  c("LBAM 5|A|15C|3h", "LBAM 5|K|15C|2h")

unlist(lapply(strsplit(x,"\\|"),function(y)
paste(y[c(1,3,4)],collapse="|")))


Stephen C. Upton
SEED (Simulation Experiments & Efficient Designs) Center
Operations Research Department
Naval Postgraduate School
SEED Center web site: http://harvest.nps.edu

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of peter
dalgaard
Sent: Thursday, September 24, 2015 6:38 AM
To: Thierry Onkelinx
Cc: R-help
Subject: Re: [R] Special characters in regular expressions


On 24 Sep 2015, at 12:05 , Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> gsub("[A|K]\\|", "", x)

That'll probably do it, but what was the point of the | in [A|K] ?? I don't
think it does what I think you think it does...

Somewhat safer, maybe:

gsub("\\|[AK]\\|","\\|", x)

(avoids surprises from, say, "LBAM 5|A|15A|3h")

-pd

> [snip]
> 2015-09-24 11:52 GMT+02:00 Patrick Connolly <p_connolly at slingshot.co.nz>:
> 
>> I need to change a vector dd that looks like this:
>> c("LBAM 5|A|15C|3h", "LBAM 5|K|15C|2h")
>> 
>> into this:
>> c("LBAM 5|15C|3h", "LBAM 5|15C|2h")
>> 
>> It's not very imaginative, but I could use a complicated nesting of
>> gsub() as so:
>> 
>> gsub("-", "\\|", gsub("K-", "", gsub("A-", "", gsub("\\|", "-", 
>> dd))))
>> 
>> Or I could make it a bit more readable by using interim objects,
>> 
>> But I'd prefer to use a single regular expression that can detect "A|"
>> *and* "K|" without collateral damage from the impact of special 
>> characters and regular characters.
>> 

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000
Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From lpfgarcia at gmail.com  Thu Sep 24 14:31:37 2015
From: lpfgarcia at gmail.com (=?UTF-8?Q?Lu=C3=ADs_Paulo_F=2E_Garcia?=)
Date: Thu, 24 Sep 2015 09:31:37 -0300
Subject: [R] RWeka and multicore package
In-Reply-To: <CAPK6mFoR=ErtuexUhVcw2hO1ezVBdNrUVfwJCM1LjudmYdYg+Q@mail.gmail.com>
References: <CAPK6mFru7FFPXLnivwzpDxi_GMqk_yVNpDvNTXz2nUTyWgLeMQ@mail.gmail.com>
	<20131022091119.GB4253@slingshot.co.nz>
	<CAPK6mFoR=ErtuexUhVcw2hO1ezVBdNrUVfwJCM1LjudmYdYg+Q@mail.gmail.com>
Message-ID: <CAPK6mFpCUM8BtEuN8ODBVarJAA9ypqxKDQHa-pMLoubD2=T+jg@mail.gmail.com>

A better solution provide by Adriano Rivolli:

library(parallel)
models <- mclapply(1:10, function(i) {
    model <- RWeka::J48(Species ~., iris[sample(nrow(iris))[1:100],])
    rJava::.jcache(model$classifier)
    model
});

2013-10-24 10:16 GMT-02:00 Lu?s Paulo F. Garcia <lpfgarcia at gmail.com>:

> Dear, Patrick.
>
> I'm using a workaround to work with RWeka and multicore package. The most
> important point is not loading the RWeka package and use the namespace in a
> encapsulated function. Take a look on this code:
>
>
> # C4.5 classifier. Return the prediction for a test dataset.
> cl.c45 = function(tran, test) {
>
>     model = RWeka::J48(Species ~ ., tran);
>     pred = predict(model, test[,-ncol(test)], type="class");
>     names(pred) = row.names(test);
>     return(pred);
> }
>
>
> library(multicore)
> aux = mclapply(1:1000, function(i) {
>     cl.c45(iris, iris)
> })
>
> Regards,
> Luis
>
>
>
>
>
>
>
>
> On Tue, Oct 22, 2013 at 7:11 AM, Patrick Connolly <
> p_connolly at slingshot.co.nz> wrote:
>
>> On Thu, 17-Oct-2013 at 02:21PM -0300, Lu?s Paulo F. Garcia wrote:
>>
>> |> I work very mutch with the packages RWeka and multicore. If you try to
>> run
>> |> J48 or any tree of RWeka with multicore we hava some errors.
>> |>
>> |> Example I:
>> |>
>> |> library(RWeka);
>> |> library(multicore);
>> |>
>> |> mclapply(1:100, function(i) {
>> |>     J48(Species ~., iris);
>> |> });
>> |>
>> |>
>> |> Output:  "Error in .jcall(o, \"Ljava/lang/Class;\", \"getClass\") : \n
>> |> java.lang.ClassFormatError: Incompatible magic value 1347093252 in
>> class
>> |> file java/lang/ProcessEnvironment$StringEnvironment\n"
>> |>
>> |>
>> |> Example II:
>> |>
>> |> library(multicore);
>> |>
>> |> mclapply(1:100, function(i) {
>> |>     RWeka::J48(Species ~., iris);
>> |> });
>> |>
>> |> Output: Erro em .jcall(x$classifier, "S", "toString") :
>> |>   RcallMethod: attempt to call a method of a NULL object.
>> |>
>> |>
>> |> Do you know some way to work with parallel processing and RWeka? I
>> tried
>> |> MPI and SNOW without success.
>>
>> Not much help, but I too have not been able to get parallelling RWeka
>> to work.  OTOH, what RWeka can do is very fast compared with, say, gbm
>> (which does work well with mclapply).
>>
>> I suspect that it has something to do with how Java is set up, but I
>> know nothing about setting up Java.
>>
>>
>>
>>
>> |>
>> |> R version 3.0.2 (2013-09-25) -- "Frisbee Sailing"
>> |> Ubuntu 12.04 x64
>> |>
>> |>
>> |> --
>> |> Lu?s Paulo Faina Garcia
>> |> Engenheiro de Computa??o - Universidade de S?o Paulo
>> |> S?o Carlos - SP - Brasil
>> |>
>> |>      [[alternative HTML version deleted]]
>> |>
>>
>> |> ______________________________________________
>> |> R-help at r-project.org mailing list
>> |> https://stat.ethz.ch/mailman/listinfo/r-help
>> |> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> |> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> --
>> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>>    ___    Patrick Connolly
>>  {~._.~}                   Great minds discuss ideas
>>  _( Y )_                 Average minds discuss events
>> (:_~*~_:)                  Small minds discuss people
>>  (_)-(_)                              ..... Eleanor Roosevelt
>>
>> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>>
>
>
>
> --
> Lu?s Paulo Faina Garcia
> Engenheiro de Computa??o - Universidade de S?o Paulo
> S?o Carlos - SP - Brasil
>



-- 
Lu?s Paulo Faina Garcia
Engenheiro de Computa??o - Universidade de S?o Paulo
S?o Carlos - SP - Brasil

	[[alternative HTML version deleted]]


From aloboaleu at gmail.com  Thu Sep 24 15:07:36 2015
From: aloboaleu at gmail.com (Agustin Lobo)
Date: Thu, 24 Sep 2015 15:07:36 +0200
Subject: [R] Friedman test
Message-ID: <CALPC6DMNgj5y6qaJO3q-NL8EpyT2CDrwUaPAwWTBGWmNN=WJnA@mail.gmail.com>

I would like to make sure that I'm using friedman.test() correctly,
because I have to reformat my data set to fulfil the requirement
"exactly one observation in y for each combination of levels of groups
and blocks" mentioned
in the help page of function friedman.test().

Assuming data from
http://www.inside-r.org/packages/cran/muStat/docs/mu.kruskal.test

treatments <- factor(rep(c("Trt1", "Trt2", "Trt3"), each=4))
people <- factor(rep(c("Subj1", "Subj2", "Subj3", "Subj4"), 3))
y <- c(0.73,0.76,0.46,0.85,0.48,0.78,0.87,0.22,0.51,0.03,0.39,0.44)
midata <- data.frame(treatments,people,y)
midata[1:10,]

I would do as follows:

Test among treatments
friedman.test(y ~ treatments|people,data=midata)
Test among subjects
friedman.test(y ~ people|treatments,data=midata)

Is this correct?

Thanks

-- 
Agustin Lobo
aloboaleu at gmail.com


From E.Vettorazzi at uke.de  Thu Sep 24 15:16:50 2015
From: E.Vettorazzi at uke.de (Eik Vettorazzi)
Date: Thu, 24 Sep 2015 15:16:50 +0200
Subject: [R] doubt with Odds ratio - URGENT HELP NEEDED
In-Reply-To: <5603AB99.7050108@dewey.myzen.co.uk>
References: <00BC0247-0C23-46CF-8E7A-509186C10FCC@gmail.com>
	<560286AC.4000808@dewey.myzen.co.uk>
	<EAF5F6E7-6905-4F02-99B7-8E50FE07B446@gmail.com>
	<5602C561.7060207@dewey.myzen.co.uk>
	<2EE44B10-737B-4F07-A110-2A8C15707FC3@gmail.com>
	<5603AB99.7050108@dewey.myzen.co.uk>
Message-ID: <5603F7C2.7050505@uke.de>

Dear Rosa,
coefficents of a probit-regression do not have a odds-ratio
interpretation, you should use a logit link for that.

cheers.

Am 24.09.2015 um 09:51 schrieb Michael Dewey:
> Dear Rosa
> 
> Please keep the list on the recipients as others may be able to help.
> 
> See inline
> 
> On 23/09/2015 19:19, Rosa Oliveira wrote:
>> Dear Michael,
>>
>> *New cleaned code :)    (I think :))*
>>
>> casedata <-read.spss("tas_05112008.sav")
>> tas.data<-data.frame(casedata)
>>
>> #Delete patients that were not discharged
>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>
>> tas.data$tas_d2      <-
>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>> tas.data$tas_d2))
>> tas.data$tas_d3      <-
>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>> tas.data$tas_d3))
>> tas.data$tas_d4      <-
>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>> tas.data$tas_d4))
>> tas.data$tas_d5      <-
>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>> tas.data$tas_d5))
>> tas.data$tas_d6      <-
>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>> tas.data$tas_d6))
>>
>>      tas.data$age      <-
>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>
>>
>>      tas.data                     <-   data.frame(tas1 =
>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>                                               tas3 = tas.data$tas_d4,
>> tas4 = tas.data$tas_d5,
>>                                               tas5 = tas.data$tas_d6,
>> age = tas.data$age,
>>                                               discharge =
>> tas.data$resultado.hosp, id.pat=tas.data$id)
>>
>> #    tas.data$discharge              <- factor(   tas.data$discharge ,
>> levels=c(0,1), labels = c("dead", "alive"))
>>
>>    #select only cases that have more than 3 tas
>>      tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>> 1, function(x) sum(!is.na(x)))>2,]
>>
>>
>>      nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients
>> with more than 2 tas measurements
>>
>>      tas.data.long                 <- data.frame(
>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>                                         id=rep(c(1:n.obs), 5))
>>      tas.data.long                 <- tas.data.long
>>   [order(tas.data.long$id),]
>>
>>      age=tas.data$age
>>
>>
>>
>> library(verification)
> 
> What does that do?
> 
>> prevOR1 <-
>> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>> ORmodel1 <- exp(prevOR1$coeff[,1])#####computes OR?
>> ORmodel1
>>
>> prevOR2 <-
>> summary(glm(tas.data[,7]~tas.data[,4]+tas.data[,6],family=binomial(link=probit)))
>>
>> ORmodel2 <- exp(prevOR2$coeff[,1])#####computes OR?
>> ORmodel2
>>
> 
> So are you happy that those are odds ratios but you need the confidence
> intervals now?
> 
> ?confint
> 
> may help you
>>
>> Nonetheless I can?t get OR confidence intervals :( and i?m not sure if I
>> have it right :(
>>
>> Best,
>> RO
>>
>>
>>
>> Atenciosamente,
>> Rosa Oliveira
>>
>> -- 
>> ____________________________________________________________________________
>>
>>
>>
>> Rosa Celeste dos Santos Oliveira,
>>
>> E-mail:rosita21 at gmail.com <mailto:rosita21 at gmail.com>
>> Tlm: +351 939355143
>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>> ____________________________________________________________________________
>>
>> "Many admire, few know"
>> Hippocrates
>>
>>> On 23 Sep 2015, at 16:29, Michael Dewey <lists at dewey.myzen.co.uk
>>> <mailto:lists at dewey.myzen.co.uk>> wrote:
>>>
>>> Dear Rosa
>>>
>>> Can you remove all the code which is not relevant to calculating the
>>> odds ratio so we can see what is going on?
>>>
>>> On 23/09/2015 16:06, Rosa Oliveira wrote:
>>>> Dear Michael,
>>>>
>>>>
>>>> I found some of the errors, but others I wasn?t able to.
>>>>
>>>> And my huge huge problem concerns OR and OR confidence interval :(
>>>>
>>>>
>>>> *New Corrected code:*
>>>>
>>>>
>>>> casedata <-read.spss("tas_05112008.sav")
>>>> tas.data<-data.frame(casedata)
>>>>
>>>> #Delete patients that were not discharged
>>>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>>
>>>> tas.data$tas_d2      <-
>>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>>> tas.data$tas_d2))
>>>> tas.data$tas_d3      <-
>>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>>> tas.data$tas_d3))
>>>> tas.data$tas_d4      <-
>>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>>> tas.data$tas_d4))
>>>> tas.data$tas_d5      <-
>>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>>> tas.data$tas_d5))
>>>> tas.data$tas_d6      <-
>>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>>> tas.data$tas_d6))
>>>>
>>>>     tas.data$age      <-
>>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>>
>>>>
>>>>     tas.data                     <-   data.frame(tas1 =
>>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>>                                              tas3 = tas.data$tas_d4,
>>>> tas4 = tas.data$tas_d5,
>>>>                                              tas5 = tas.data$tas_d6,
>>>> age = tas.data$age,
>>>>                                              discharge =
>>>> tas.data$resultado.hosp, id.pat=tas.data$id)
>>>>
>>>> #    tas.data$discharge              <- factor(   tas.data$discharge ,
>>>> levels=c(0,1), labels = c("dead", "alive"))
>>>>
>>>>   #select only cases that have more than 3 tas
>>>>     tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>>> 1, function(x) sum(!is.na(x)))>2,]
>>>>
>>>>
>>>>     nsample <- n.obs              <- dim(tas.data)[1]  #nr of patients
>>>> with more than 2 tas measurements
>>>>
>>>>     tas.data.long                 <- data.frame(
>>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>>                                        id=rep(c(1:n.obs), 5))
>>>>     tas.data.long                 <- tas.data.long
>>>>  [order(tas.data.long$id),]
>>>>
>>>>     age=tas.data$age
>>>>
>>>> ##################################################################################################
>>>>
>>>> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>>> ##################################################################################################
>>>>
>>>>   mean.alive                      <-
>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>>   mean.dead                       <-
>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>>   stderr                          <- function(x)
>>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>>   se.alive                        <-
>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>>   se.dead                         <-
>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>>   summarytas                      <- data.frame (media = c(mean.dead,
>>>> mean.alive),
>>>>                                       standarderror = c( se.dead,
>>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>>
>>>>
>>>> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean,
>>>> colour=discharge)) +
>>>>     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>>> standarderror), width=.1) +
>>>>   scale_color_manual(values=c("blue", "red")) +
>>>>    theme(legend.text=element_text(size=20),
>>>> axis.text=element_text(size=16),
>>>> axis.title=element_text(size=20,face="bold")) +
>>>>    scale_x_continuous(name="Days") +
>>>>   scale_y_continuous(name="log tas") +
>>>>   geom_line() +
>>>>     geom_point()
>>>>
>>>>
>>>> library(verification)
>>>> prev <-
>>>> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>>>> answer            = c(prev$coefficients[,1:2])
>>>>
>>>>
>>>> roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
>>>>
>>>>
>>>>
>>>> modelo<-function (dataainit)
>>>> {
>>>>
>>>> #dataa<-tas.data
>>>> dataa<-dataainit
>>>>
>>>> dataa$ident<-seq(1:90)
>>>> tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>>> dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>>> time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>>> ident=rep(dataa$ident,5))
>>>> tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>>> tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>>> #mixed model for the longitudinal tas
>>>> lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>>> na.action=na.exclude )
>>>> #Random intercept and slopes
>>>> pred.lme<-predict(lme.1)
>>>> lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>>> lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>>> selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
>>>> Apply
>>>> to the vector in the dataset
>>>>  test(dataa$intercept[resultado.hosp==1],
>>>> dataa$intercept[resultado.hosp==0])
>>>> print(summary(model.surv1))
>>>> return(model.surv1$coef)
>>>>  }
>>>>
>>>>
>>>> *CONSOLE RESULT: (errors in red)*
>>>>
>>>> > casedata <-read.spss("tas_05112008.sav")
>>>> Warning message:
>>>> In read.spss("tas_05112008.sav") :
>>>>   tas_05112008.sav: Unrecognized record type 7, subtype 18 encountered
>>>> in system file
>>>> > tas.data<-data.frame(casedata)
>>>> >
>>>> > #Delete patients that were not discharged
>>>> > tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>>> > tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>> >
>>>> > tas.data$tas_d2      <-
>>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>>> tas.data$tas_d2))
>>>> > tas.data$tas_d3      <-
>>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>>> tas.data$tas_d3))
>>>> > tas.data$tas_d4      <-
>>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>>> tas.data$tas_d4))
>>>> > tas.data$tas_d5      <-
>>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>>> tas.data$tas_d5))
>>>> > tas.data$tas_d6      <-
>>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>>> tas.data$tas_d6))
>>>> >
>>>> >     tas.data$age      <-
>>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>> >
>>>> >
>>>> >     tas.data                     <-   data.frame(tas1 =
>>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>> +                                              tas3 = tas.data$tas_d4,
>>>> tas4 = tas.data$tas_d5,
>>>> +                                              tas5 = tas.data$tas_d6,
>>>> age = tas.data$age,
>>>> +                                              discharge =
>>>> tas.data$resultado.hosp, id.pat=tas.data$id)
>>>> >
>>>> > #    tas.data$discharge              <- factor(   tas.data$discharge
>>>> , levels=c(0,1), labels = c("dead", "alive"))
>>>> >
>>>> >   #select only cases that have more than 3 tas
>>>> >     tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>>> 1, function(x) sum(!is.na(x)))>2,]
>>>> >
>>>> >
>>>> >
>>>> >     nsample <- n.obs              <- dim(tas.data)[1]  #nr of
>>>> patients with more than 2 tas measurements
>>>> >
>>>> >     tas.data.long                 <- data.frame(
>>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>> +                                        id=rep(c(1:n.obs), 5))
>>>> >     tas.data.long                 <- tas.data.long
>>>>  [order(tas.data.long$id),]
>>>> >
>>>> >     age=tas.data$age
>>>> >
>>>> >
>>>> ##################################################################################################
>>>>
>>>> > #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>>> >
>>>> ##################################################################################################
>>>>
>>>> >   mean.alive                      <-
>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>> >   mean.dead                       <-
>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>> >   stderr                          <- function(x)
>>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>> >   se.alive                        <-
>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>> >   se.dead                         <-
>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>> >   summarytas                      <- data.frame (media = c(mean.dead,
>>>> mean.alive),
>>>> +                                       standarderror = c( se.dead,
>>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>> >
>>>> >
>>>> > ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean,
>>>> colour=discharge)) +
>>>> +     geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>>> standarderror), width=.1) +
>>>> +   scale_color_manual(values=c("blue", "red")) +
>>>> +    theme(legend.text=element_text(size=20),
>>>> axis.text=element_text(size=16),
>>>> axis.title=element_text(size=20,face="bold")) +
>>>> +    scale_x_continuous(name="Days") +
>>>> +   scale_y_continuous(name="log tas") +
>>>> +   geom_line() +
>>>> +     geom_point()
>>>> Error in mean - 2 * standarderror :
>>>>   non-numeric argument to binary operator
>>>> >
>>>> >
>>>> > library(verification)
>>>> > prev <-
>>>> summary(glm(tas.data[,7]~tas.data[,4],family=binomial(link=probit)))
>>>> > answer            = c(prev$coefficients[,1:2])
>>>> >
>>>> >
>>>> > roc.plot(tas.data[,7], prev, show.thres = FALSE, legen=F )
>>>> Error in is.finite(x) : default method not implemented for type 'list'
>>>> >
>>>> >
>>>> >
>>>> > modelo<-function (dataainit)
>>>> +
>>>> + {
>>>> +
>>>> + #dataa<-tas.data
>>>> + dataa<-dataainit
>>>> +
>>>> + dataa$ident<-seq(1:90)
>>>> + tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>>> + dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>>> + time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>>> ident=rep(dataa$ident,5))
>>>> +
>>>> + tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>>> + tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>>> +
>>>> + #mixed model for the longitudinal tas
>>>> + lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>>> na.action=na.exclude )
>>>> +
>>>> + #Random intercept and slopes
>>>> + pred.lme<-predict(lme.1)
>>>> + lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>>> + lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>>> + selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
>>>> Apply to the vector in the dataset
>>>> +
>>>> +  test(dataa$intercept[resultado.hosp==1],
>>>> dataa$intercept[resultado.hosp==0])
>>>> +
>>>> + print(summary(model.surv1))
>>>> + return(model.surv1$coef)
>>>> +
>>>> +  }
>>>> >
>>>>
>>>> I can?t get the OR and OR CI :(
>>>>
>>>>
>>>> *DATA:*
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Best,
>>>>
>>>> RO
>>>>
>>>>
>>>>
>>>>
>>>> Atenciosamente,
>>>> Rosa Oliveira
>>>>
>>>> -- 
>>>> ____________________________________________________________________________
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Rosa Celeste dos Santos Oliveira,
>>>>
>>>> E-mail:rosita21 at gmail.com <http://gmail.com>
>>>> <mailto:rosita21 at gmail.com>
>>>> Tlm: +351 939355143
>>>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>>>> ____________________________________________________________________________
>>>>
>>>> "Many admire, few know"
>>>> Hippocrates
>>>>
>>>>> On 23 Sep 2015, at 12:02, Michael Dewey <lists at dewey.myzen.co.uk
>>>>> <mailto:lists at dewey.myzen.co.uk>
>>>>> <mailto:lists at dewey.myzen.co.uk>> wrote:
>>>>>
>>>>> Dear Rosa
>>>>>
>>>>> It would help if you posted the error messages where they occur so
>>>>> that we can see which of your commands caused which error. However see
>>>>> comment inline below.
>>>>>
>>>>> On 22/09/2015 22:17, Rosa Oliveira wrote:
>>>>>> Dear all,
>>>>>>
>>>>>>
>>>>>> I?m trying to compute Odds ratio and OR confidence interval.
>>>>>>
>>>>>> I?m really naive, sorry for that.
>>>>>>
>>>>>>
>>>>>> I attach my data and my code.
>>>>>>
>>>>>> I?m having lots of errors:
>>>>>>
>>>>>> 1. Error in data.frame(tas1 = tas.data$tas_d2, tas2 =
>>>>>> tas.data$tas_d3, tas3 = tas.data$tas_d4,  :
>>>>>>  arguments imply differing number of rows: 90, 0
>>>>>>
>>>>>
>>>>> At least one of tas_d2, tas_d3, tas_d4 does not exist
>>>>>
>>>>> I suggest fixing that one and hoping the rest go away.
>>>>>
>>>>>> 2. Error in data.frame(tas = c(unlist(tas.data[, -8:-6])), time =
>>>>>> rep(c(0:4),  :
>>>>>>  arguments imply differing number of rows: 630, 450, 0
>>>>>>
>>>>>> 3. Error: object 'tas.data.long' not found
>>>>>>
>>>>>> 4. Error in data.frame(media = c(mean.dead, mean.alive),
>>>>>> standarderror = c(se.dead,  :
>>>>>>  arguments imply differing number of rows: 14, 10
>>>>>>
>>>>>> 5. Error in ggplot(summarytas, aes(x = c(c(1:5), c(1:5)), y = mean,
>>>>>> colour = discharge)) :
>>>>>>  object 'summarytas' not found
>>>>>>
>>>>>> 6. Error in summary(glm(tas.data[, 6] ~ tas.data[, 4], family =
>>>>>> binomial(link = probit))) :
>>>>>>  error in evaluating the argument 'object' in selecting a method for
>>>>>> function 'summary': Error in eval(expr, envir, enclos) : y values
>>>>>> must be 0 <= y <= 1
>>>>>>
>>>>>> 7. Error in wilcox.test.default(pred[obs == 1], pred[obs == 0],
>>>>>> alternative = "great") :
>>>>>>  not enough (finite) 'x' observations
>>>>>> In addition: Warning message:
>>>>>> In is.finite(x) & apply(pred, 1, f) :
>>>>>>  longer object length is not a multiple of shorter object length
>>>>>>
>>>>>>
>>>>>> and off course I?m not getting OR.
>>>>>>
>>>>>> Nonetheless all this errors, I think I have not been able to compute
>>>>>> de code to get OR and OR confidence interval.
>>>>>>
>>>>>>
>>>>>> Can anyone help me please. It?s really urgent.
>>>>>>
>>>>>> PLEASE
>>>>>>
>>>>>> THE CODE:
>>>>>>
>>>>>> the hospital outcome is discharge.
>>>>>>
>>>>>> require(gdata)
>>>>>> library(foreign)
>>>>>> library(nlme)
>>>>>> library(lme4)
>>>>>> library(boot)
>>>>>> library(MASS)
>>>>>> library(Hmisc)
>>>>>> library(plotrix)
>>>>>> library(verification)
>>>>>> library(mvtnorm)
>>>>>> library(statmod)
>>>>>> library(epiR)
>>>>>>
>>>>>> #########################################################################################
>>>>>>
>>>>>> # Data preparation
>>>>>>                                                                     #
>>>>>> #########################################################################################
>>>>>>
>>>>>>
>>>>>> setwd("/Users/RO/Desktop")
>>>>>>
>>>>>> casedata <-read.spss("tas_05112008.sav")
>>>>>> tas.data<-data.frame(casedata)
>>>>>>
>>>>>> #Delete patients that were not discharged
>>>>>> tas.data                     <- tas.data[ tas.data$hosp!="si ",]
>>>>>> tas.data$resultado.hosp      <- ifelse(tas.data$hosp=="l", 0, 1)
>>>>>>
>>>>>> tas.data$tas_d2      <-
>>>>>> log(ifelse(tas.data$tas_d2==88888|tas.data$tas_d2==99999, NA,
>>>>>> tas.data$tas_d2))
>>>>>> tas.data$tas_d3      <-
>>>>>> log(ifelse(tas.data$tas_d3==88888|tas.data$tas_d3==99999, NA,
>>>>>> tas.data$tas_d3))
>>>>>> tas.data$tas_d4      <-
>>>>>> log(ifelse(tas.data$tas_d4==88888|tas.data$tas_d4==99999, NA,
>>>>>> tas.data$tas_d4))
>>>>>> tas.data$tas_d5      <-
>>>>>> log(ifelse(tas.data$tas_d5==88888|tas.data$tas_d5==99999, NA,
>>>>>> tas.data$tas_d5))
>>>>>> tas.data$tas_d6      <-
>>>>>> log(ifelse(tas.data$tas_d6==88888|tas.data$tas_d6==99999, NA,
>>>>>> tas.data$tas_d6))
>>>>>>
>>>>>>    tas.data$age      <-
>>>>>> ifelse(tas.data$age==88888|tas.data$age==99999, NA, tas.data$age)
>>>>>>
>>>>>>
>>>>>>    tas.data                     <-   data.frame(tas1 =
>>>>>> tas.data$tas_d2, tas2 = tas.data$tas_d3,
>>>>>>                                             tas3 = tas.data$tas_d4,
>>>>>> tas4 = tas.data$tas_d5,
>>>>>>                                             tas5 = tas.data$tas_d6,
>>>>>> age = tas.data$age,
>>>>>>                                             discharge =
>>>>>> tas.data$resultado.hosp, id.pat=tas.data$ID)
>>>>>>
>>>>>> #    tas.data$discharge              <- factor(   tas.data$discharge
>>>>>> , levels=c(0,1), labels = c("dead", "alive"))
>>>>>>
>>>>>>  #select only cases that have more than 3 tas
>>>>>>    tas.data                      <- tas.data[apply(tas.data[,-8:-6],
>>>>>> 1, function(x) sum(!is.na(x)))>2,]
>>>>>>
>>>>>>
>>>>>>
>>>>>>    nsample <- n.obs              <- dim(tas.data)[1]  #nr of
>>>>>> patients with more than 2 tas measurements
>>>>>>
>>>>>>    tas.data.long                 <- data.frame(
>>>>>> tas=c(unlist(tas.data[,-8:-6])), time=rep(c(0:4), each=n.obs),
>>>>>> age=rep(tas.data$age, 5), discharge=rep(tas.data$discharge, 5),
>>>>>>                                       id=rep(c(1:n.obs), 5))
>>>>>>    tas.data.long                 <- tas.data.long
>>>>>> [order(tas.data.long$id),]
>>>>>>
>>>>>>    age=tas.data$age
>>>>>>
>>>>>> ##################################################################################################
>>>>>>
>>>>>> #PLOT EMPIRICAL MEANS OF CRP FOR ALIVE  DEATh
>>>>>> ##################################################################################################
>>>>>>
>>>>>>  mean.alive                      <-
>>>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, mean, na.rm=T)
>>>>>>  mean.dead                       <-
>>>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, mean, na.rm=T)
>>>>>>  stderr                          <- function(x)
>>>>>> sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
>>>>>>  se.alive                        <-
>>>>>> apply(tas.data[tas.data$discharge==0, -8:-6], 2, stderr)
>>>>>>  se.dead                         <-
>>>>>> apply(tas.data[tas.data$discharge==1, -8:-6], 2, stderr)
>>>>>>  summarytas                      <- data.frame (media = c(mean.dead,
>>>>>> mean.alive),
>>>>>>                                      standarderror = c( se.dead,
>>>>>> se.alive), discharge = c(rep("dead",5), rep("alive", 5)))
>>>>>>
>>>>>>
>>>>>> ggplot(summarytas, aes(x=c(c(1:5), c(1:5)), y=mean,
>>>>>> colour=discharge)) +
>>>>>>    geom_errorbar(aes(ymin=mean-2* standarderror, ymax=media+2*
>>>>>> standarderror), width=.1) +
>>>>>>  scale_color_manual(values=c("blue", "red")) +
>>>>>>   theme(legend.text=element_text(size=20),
>>>>>> axis.text=element_text(size=16),
>>>>>> axis.title=element_text(size=20,face="bold")) +
>>>>>>   scale_x_continuous(name="Days") +
>>>>>>  scale_y_continuous(name="log tas") +
>>>>>>  geom_line() +
>>>>>>    geom_point()
>>>>>>
>>>>>>
>>>>>> library(verification)
>>>>>> prev <-
>>>>>> summary(glm(tas.data[,6]~tas.data[,4],family=binomial(link=probit)))
>>>>>> answer = c(prev$coefficients[,1:2])
>>>>>>
>>>>>>
>>>>>> roc.plot(tas.data[,6], prev, show.thres = FALSE, legen=F )
>>>>>>
>>>>>>
>>>>>> modelo<-function (dataainit)
>>>>>>
>>>>>> {
>>>>>>
>>>>>> #dataa<-tas.data
>>>>>> dataa<-dataainit
>>>>>>
>>>>>> dataa$ident<-seq(1:90)
>>>>>> tas.6days<-cbind(id=rep(dataa$id,5),tas=c(dataa$tas_d2, dataa$tas_d3,
>>>>>> dataa$tas_d4, dataa$tas_d5, dataa$tas_d6),
>>>>>> time=rep(c(2:6)-2, each=90), out.come=rep(dataa$hosp,5),
>>>>>> ident=rep(dataa$ident,5))
>>>>>>
>>>>>> tas.6days<-data.frame(tas.6days[order(tas.6days[,1]),])
>>>>>> tas.6days$tas[tas.6days$tas==88888|tas.6days$tas==99999 ]<-NA
>>>>>>
>>>>>> #mixed model for the longitudinal tas
>>>>>> lme.1 <- lme(tas~ time+1, random = ~ time+1 |ident, data=tas.6days,
>>>>>> na.action=na.exclude )
>>>>>>
>>>>>> #Random intercept and slopes
>>>>>> pred.lme<-predict(lme.1)
>>>>>> lme.intercept<-lme.1$coef$random$ident[,1]+lme.1$coef$fixed[1]
>>>>>> lme.slope<- lme.1$coef$random$ident[,2]+lme.1$coef$fixed[2]
>>>>>> selector<-as.numeric(names(lme.intercept)) #to select not NA rows.
>>>>>> Apply to the vector in the dataset
>>>>>>
>>>>>> test(dataa$intercept[resultado.hosp==1],
>>>>>> dataa$intercept[resultado.hosp==0])
>>>>>>
>>>>>> print(summary(model.surv1))
>>>>>> return(model.surv1$coef)
>>>>>>
>>>>>> }
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> Best,
>>>>>> RO
>>>>>>
>>>>>>
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org <mailto:R-help at r-project.org>
>>>>>> <mailto:R-help at r-project.org> mailing list -- To
>>>>>> UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>
>>>>> -- 
>>>>> Michael
>>>>> http://www.dewey.myzen.co.uk/home.html
>>>>
>>>
>>> -- 
>>> Michael
>>> http://www.dewey.myzen.co.uk/home.html
>>
> 

-- 
Eik Vettorazzi

Department of Medical Biometry and Epidemiology
University Medical Center Hamburg-Eppendorf

Martinistr. 52
20246 Hamburg

T ++49/40/7410-58243
F ++49/40/7410-57790
--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Rainer Schoppik
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From pdalgd at gmail.com  Thu Sep 24 16:44:21 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 24 Sep 2015 16:44:21 +0200
Subject: [R] Friedman test
In-Reply-To: <CALPC6DMNgj5y6qaJO3q-NL8EpyT2CDrwUaPAwWTBGWmNN=WJnA@mail.gmail.com>
References: <CALPC6DMNgj5y6qaJO3q-NL8EpyT2CDrwUaPAwWTBGWmNN=WJnA@mail.gmail.com>
Message-ID: <F73EDB05-E3E7-4A75-88B2-30B473E6F39A@gmail.com>


On 24 Sep 2015, at 15:07 , Agustin Lobo <aloboaleu at gmail.com> wrote:

> I would like to make sure that I'm using friedman.test() correctly,
> because I have to reformat my data set to fulfil the requirement
> "exactly one observation in y for each combination of levels of groups
> and blocks" mentioned
> in the help page of function friedman.test().
> 
> Assuming data from
> http://www.inside-r.org/packages/cran/muStat/docs/mu.kruskal.test
> 
> treatments <- factor(rep(c("Trt1", "Trt2", "Trt3"), each=4))
> people <- factor(rep(c("Subj1", "Subj2", "Subj3", "Subj4"), 3))
> y <- c(0.73,0.76,0.46,0.85,0.48,0.78,0.87,0.22,0.51,0.03,0.39,0.44)
> midata <- data.frame(treatments,people,y)
> midata[1:10,]
> 
> I would do as follows:
> 
> Test among treatments
> friedman.test(y ~ treatments|people,data=midata)
> Test among subjects
> friedman.test(y ~ people|treatments,data=midata)
> 
> Is this correct?

Seems so. The tricky bit is to get the factors to replicate in the right way (notice also the gl() function, by the way).

A way to check is

> matrix(c(0.73,0.76,0.46,0.85,0.48,0.78,0.87,0.22,0.51,
+         0.03,0.39,0.44), ncol=3)
     [,1] [,2] [,3]
[1,] 0.73 0.48 0.51
[2,] 0.76 0.78 0.03
[3,] 0.46 0.87 0.39
[4,] 0.85 0.22 0.44
> with(midata,tapply(y,list(people, treatments), mean))
      Trt1 Trt2 Trt3
Subj1 0.73 0.48 0.51
Subj2 0.76 0.78 0.03
Subj3 0.46 0.87 0.39
Subj4 0.85 0.22 0.44

You could also have used this construction:

> m <- matrix(c(0.73,0.76,0.46,0.85,0.48,0.78,0.87,0.22,0.51,
+         0.03,0.39,0.44), ncol=3)
> as.data.frame(as.table(m))
   Var1 Var2 Freq
1     A    A 0.73
2     B    A 0.76
3     C    A 0.46
4     D    A 0.85
5     A    B 0.48
6     B    B 0.78
7     C    B 0.87
8     D    B 0.22
9     A    C 0.51
10    B    C 0.03
11    C    C 0.39
12    D    C 0.44

It can be elaborated: Adding (named) dimnames to the matrix/table will get names and levels right for the two factors, and you might also want responseName="y".

-pd


-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Phillip.Alday at unisa.edu.au  Thu Sep 24 15:12:45 2015
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Thu, 24 Sep 2015 13:12:45 +0000
Subject: [R] Appropriate specification of random effects structure for
 EEG/ERP data: including Channels or not?
In-Reply-To: <mailman.7.1443088802.25661.r-help@r-project.org>
References: <mailman.7.1443088802.25661.r-help@r-project.org>
Message-ID: <1443100371.4117.28.camel@loki>

There is actually a fair amount of ERP literature using mixed-effects
modelling, though you may have to branch out from the traditional
psycholinguistics journals a bit (even just more "neurolinguistics" or
language studies published in "psychology" would get you more!). But
just in the traditional psycholinguistics journals, there is a wealth of
literature, see for example the 2008 special issue on mixed models of
the Journal of Memory and Language.

I would NOT encode the channels/ROIs/other topographic measures as
random effects (grouping variables). If you think about the traditional
ANOVA analysis of ERPs, you'll recall that ROI or some other topographic
measure (laterality, saggitality) are included in the main effects and
interactions. As a rule of thumb, this corresponds to a fixed effect in
random effects models. More specifically, you generally care about
whether the particular levels of the topographic measure (i.e. you care
if an ERP component is located left-anterior or what not) and this is
what fixed effects test. Random effects are more useful when you only
care about the variance introduced by a particular term but not the
specific levels (e.g. participants or items -- we don't care about a
particular participant, but we do care about how much variance there is
between participants, i.e. how the population of participants looks). 

Or, another thought: You may have seen ANOVA by-subjects and by-items,
but I bet you've never seen an ANOVA by-channels. ANOVA "implicitly"
collapses the channels within ROIs and you can do the same with mixed
models. (That's an awkward statement technically, but it should help
with the intuition.)

There is an another, related important point -- "nuisance parameters"
aren't necessarily random effects. So even if you're not interested in
the per-electrode distribution of the ERP component, that doesn't mean
those should automatically be random effects. It *might* make sense to
add a channel (as in per-electrode) random effect, if you care to model
the variation within a given ROI (as you have done), but I haven't seen
that yet. It is somewhat rare to include a per-channel fixed effect,
just because you lose a lot of information that way and introduce more
parameters into the model, but you could include a more fine-grained
notion of saggital / lateral location based on e.g. the 10-20 system and
make that into an ordered factor. (Or you could be extreme and even use
the spherical coordinates that the 10-20 is based on and have continuous
measures of electrode placement!) The big problem with including
"channel" as a random-effect grouping variable is that the channels
would have a very complicated covariance structure (because adjacent
electrodes are very highly correlated with each other) and I'm not sure
how to model this in a straightforward way with lme4.

More generally, in considering your random effects structure, you should
look at Barr et al (2013, "Random effects structure for confirmatory
hypothesis testing: Keep it maximal") and the recent reply by Bates et
al (arXiv, "Parsimonious Mixed Models"). You should read up on the GLMM
FAQ on testing random effects -- there are different opinions on this
and not all think that testing them via likelihood-ratio tests makes
sense.

That wasn't my most coherent response, but maybe it's still useful. And
for questions like this on mixed models, do check out the R Special
Interest Group on Mixed Models. :-)

Best,
Phillip

On Thu, 2015-09-24 at 12:00 +0200, r-help-request at r-project.org wrote:
> Message: 4
> Date: Wed, 23 Sep 2015 12:46:46 +0200
> From: Paolo Canal <paolo.canal at iusspavia.it>
> To: r-help at r-project.org
> Subject: [R] Appropriate specification of random effects structure for
>         EEG/ERP data: including Channels or not?
> Message-ID: <56028316.2050004 at iusspavia.it>
> Content-Type: text/plain; charset="UTF-8"
> 
> Dear r-help list,
> 
> I work with EEG/ERP data and this is the first time I am using LMM to 
> analyze my data (using lme4).
> The experimental design is a 2X2: one manipulated factor is
> agreement, 
> the other is noun (agreement being within subjects and items, and
> noun 
> being within subjects and between items).
> 
> The data matrix is 31 subjects * 160 items * 33 channels. In ERP 
> research, the distribution of the EEG amplitude differences (in a
> time 
> window of interest) are important, and we care about knowing whether
> a 
> negative difference is occurring in Parietal or Frontal electrodes.
> At 
> the same time information from single channel is often too noisy and 
> channels are organized in topographic factors for evaluating
> differences 
> in distribution. In the present case I have assigned each channel to
> one 
> of three levels of two factors, i.e., Longitude (Anterior, Central, 
> Parietal) and Medial (Left, Midline, Right): for instance, one
> channel 
> is Anterior and Left. With traditional ANOVAs channels from the same 
> level of topographic factors are averaged before variance is
> evaluated 
> and this also has the benefit of reducing the noise picked up by the 
> electrodes.
> 
> I have troubles in deciding the random structure of my model. Very
> few 
> examples on LMM on ERP data exist (e.g., Newman, Tremblay, Nichols, 
> Neville & Ullman, 2012) and little detail is provided about the 
> treatment of channel. I feel it is a tricky term but very important
> to 
> optimize fit. Newman et al say "data from each electrode within an
> ROI 
> were treated as repeated measures of that ROI". In Newman et al, the 
> ROIs are the 9 regions deriving from Longitude X Medial
> (Anterior-Left, 
> Anterior-Midline, Anterior-Right, Central-Left ... and so on), so in
> a 
> way they treated each ROI separately and not according to the
> relevant 
> dimensions of Longitude and Medial.
> 
> We used the following specifications in lmer:
> 
> [fixed effects specification: ?V ~ Agreement * Noun * Longitude *
> Medial 
> * (cov1 + cov2 + cov3 + cov4)] (the terms within brackets are a
> series 
> of individual covariates, most of which are continuous variables)
> 
> [random effects specification: (1+Agreement*Type of Noun | subject) + 
> (1+Agreement | item) + (1|longitude:medial:channel)]
> 
> What I care the most about is the last term 
> (1|longitude:medial:channel). I chose this specification because I 
> thought that allowing each channel to have different intercepts in
> the 
> random structure would affect the estimation of the topographic fixed 
> effects (Longitude and Medial) in which channel is nested.
> Unfortunately 
> a reviewer commented that since "channel is not included in the fixed 
> effects I would probably leave that out".
> 
> But each channel is a repeated measure of the eeg amplitude inside
> the 
> two topographic factors, and random terms do not have to be in the
> fixed 
> structure, otherwise we would also include subjects and items in the 
> fixed effects structure. So I kind of feel that including channels as 
> random effect is correct, and having them nested in longitude:medial 
> allows to relax the assumption that the effect in the EEG has always
> the 
> same longitude:medial distribution. But I might be wrong.
> 
> I thus tested differences in fit (ML) with anova() between 
> (1|longitude:medial:channel) and the same model without the term, and
> a 
> third model with the model with a simpler (1|longitude:medial).
> 
> Fullmod vs Nochannel:
> 
> Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq)
> modnoch 119 969479 970653 -484621 969241
> fullmod 120 968972 970156 -484366 968732 508.73 1 < 2.2e-16 ***
> 
> Differences in fit is remarkable (no variance components with
> estimates 
> close to zero; no correlation parameters with values close to ?1).
> 
> Fullmod vs SimplerMod:
> 
>    Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq)
> 
> fullmod 120 968972 970156 -484366 968732
> simplermod 120 969481 970665 -484621 969241 0 0 1
> 
> Here the number of parameters to estimate in fullmod and simplermod
> is 
> the same but the increase in fit is very consistent (-509 BIC). So I 
> guess although the chisquare is not significant we do have a string 
> increase in fit. As I understand this, a model with better fit will
> find 
> more accurate estimates, and I would be inclined to keep the fullmod 
> random structure.
> 
> But perhaps I am missing something or I am doing something wrong.
> Which 
> is the correct random structure to use?
> 
> Feedbacks are very much appreciated. I often find answers in the
> list, 
> and this is the first time I post a question.
> Thanks,
> Paolo
> 
> 
> 
> 


From macqueen1 at llnl.gov  Thu Sep 24 17:27:35 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 24 Sep 2015 15:27:35 +0000
Subject: [R] boxplot overlap beeswarm
In-Reply-To: <1443079644936-4712727.post@n4.nabble.com>
References: <1443079644936-4712727.post@n4.nabble.com>
Message-ID: <D229620E.1397AE%macqueen1@llnl.gov>

I see that you have used
   add = TRUE
in the boxplot call. Add only makes sense if there is already a plot to
which to add the boxplot. But your boxplot is first, so there isn't
anything to add it to.

Try doing the beeswarm plot first?

Will the two plots will have the same y axis ranges?

Minor notes:
  you don't need both as.is and stringsAsFactors in read.csv
  read.csv by definition uses sep=','; you do not need to specify it.

If you want better help do the following:

  supply example data (use the "dput" function)

  saying it doesn't work is not enough -- describe how it did not work,
and include error messages, if any

  read the Posting Guide (link at the bottom of every email on R-help)

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 9/24/15, 12:27 AM, "R-help on behalf of Julia89"
<r-help-bounces at r-project.org on behalf of Julias_89 at gmx.net> wrote:

>Hi everybody,
>I'm new and i need help very fast.
>I will make a transparent boxplot overlap a beeswarm.
>I don't want to use ggplot2, i will use ggplot.
>Here is my own script but it doesn't worked.
>Maybe you can help me.
>Thanks in advance
>Julia
>
>require(beeswarm)#rohdaten boxplots
>
>rm(list=ls())
>setwd("C:/Dokumente und Einstellungen/jbellsta/Desktop/Paper")
>
>#read data
>data =
>read.csv(file="HypocotylCellLength.csv",stringsAsFactors=FALSE,header=TRUE
>,
>as.is=TRUE, sep=",")
>str(data)
>attach(data)
>
>
>#schreibe gr????ten wert in Variable L.max
>L.max <- max(LengthMM)
>
>boxplot(LengthMM ~ Label,
>boxwex=.3, 		
>outline=F,
>add=T, 	
>ylim=c(0, L.max))		
>
>beeswarm(LengthMM ~ Label,  # Datenpunkte LengthMM~Label
>pch = 20, 	
>cex= .6, 		
>spacing = .3,	
>method =c("swarm"), 	  				
>col= c('grey')	
>)                 
>
>
>
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/boxplot-overlap-beeswarm-tp4712727.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From btyner at gmail.com  Thu Sep 24 19:47:58 2015
From: btyner at gmail.com (Benjamin Tyner)
Date: Thu, 24 Sep 2015 13:47:58 -0400
Subject: [R] backsolve, chol, Matrix, and SparseM
Message-ID: <5604374E.7000301@gmail.com>

Hi

I have some code which does (on a symmetric matrix 'x')

    backsolve(chol(x), diag(nrow(x)))

and I am wondering what is the recommended way to accomplish this when x
is also sparse (from package:Matrix). I know that package:Matrix
provides a chol method for such matrices, but not a backsolve method. On
the other hand, package:SparseM does provide a backsolve method, but
doesn't actually return a sparse matrix. Moreover, I am a little
hesitant to use SparseM, as the vignette seems to be from 2003.

I did notice that help(topic = "solve", package = "Matrix") says "In
?solve(a,b)? in the ?Matrix? package, ?a? may also be a
?MatrixFactorization? instead of directly a matrix." which makes me
think this is the right way:

    Matrix::solve(Cholesky(x), .sparseDiagonal(nrow(x)))

but unfortunately this didn't give the same result as:

    Matrix::solve(chol(x), .sparseDiagonal(nrow(x)))

so I'm asking here in case someone has any suggestions.

Regards,
Ben


From elexira at yahoo.com  Thu Sep 24 19:34:21 2015
From: elexira at yahoo.com (amir)
Date: Thu, 24 Sep 2015 17:34:21 +0000 (UTC)
Subject: [R] Welcome to the "R-help" mailing list (Digest mode)
In-Reply-To: <mailman.0.1443115896.10317.r-help@r-project.org>
References: <mailman.0.1443115896.10317.r-help@r-project.org>
Message-ID: <598062399.469470.1443116061314.JavaMail.yahoo@mail.yahoo.com>

I would greatly appreciate if anybody could help with the stack-overflow question : Updating and Forecasting times series with new data
| ? |
| ? |  | ? | ? | ? | ? | ? |
| Updating and Forecasting times series with new dataWe can fit a time series and use the model to make forecasts, for example using: fit <- auto.arima(WWWusage) plot(forecast(fit,h=20)) Consider the following scenari... |
|  |
| View on stackoverflow.com | Preview by Yahoo |
|  |
| ? |


Thank you for your time,Pemfir 
 


     On Thursday, September 24, 2015 10:31 AM, "r-help-request at r-project.org" <r-help-request at r-project.org> wrote:
   

 Welcome to the R-help at r-project.org mailing list!

To post to this list, send your message to:

? r-help at r-project.org

General information about the mailing list is at:

? https://stat.ethz.ch/mailman/listinfo/r-help

If you ever want to unsubscribe or change your options (eg, switch to
or from digest mode, change your password, etc.), visit your
subscription page at:

? https://stat.ethz.ch/mailman/options/r-help/elexira%40yahoo.com

You can also make such adjustments via email by sending a message to:

? R-help-request at r-project.org

with the word `help' in the subject or body (don't include the
quotes), and you will get back a message with instructions.

You must know your password to change your options (including changing
the password, itself) or to unsubscribe without confirmation.? It is:

? zoha1234

Normally, Mailman will remind you of your r-project.org mailing list
passwords once every month, although you can disable this if you
prefer.? This reminder will also include instructions on how to
unsubscribe or change your account options.? There is also a button on
your options page that will email your current password to you.


  
	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Thu Sep 24 20:19:39 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 24 Sep 2015 14:19:39 -0400
Subject: [R] Welcome to the "R-help" mailing list (Digest mode)
In-Reply-To: <598062399.469470.1443116061314.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.0.1443115896.10317.r-help@r-project.org>
	<598062399.469470.1443116061314.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <FFB33050-D6D4-481E-9B6A-DBC90D1218E2@utoronto.ca>

You're posting in HTML and your link didn't make it  :-(



B.


On Sep 24, 2015, at 1:34 PM, amir via R-help <r-help at r-project.org> wrote:

> I would greatly appreciate if anybody could help with the stack-overflow question : Updating and Forecasting times series with new data
> |   |
> |   |  |   |   |   |   |   |
> | Updating and Forecasting times series with new dataWe can fit a time series and use the model to make forecasts, for example using: fit <- auto.arima(WWWusage) plot(forecast(fit,h=20)) Consider the following scenari... |
> |  |
> | View on stackoverflow.com | Preview by Yahoo |
> |  |
> |   |
> 
> 
> Thank you for your time,Pemfir 
> 
> 
> 
>     On Thursday, September 24, 2015 10:31 AM, "r-help-request at r-project.org" <r-help-request at r-project.org> wrote:
> 
> 
> Welcome to the R-help at r-project.org mailing list!
> 
> To post to this list, send your message to:
> 
>   r-help at r-project.org
> 
> General information about the mailing list is at:
> 
>   https://stat.ethz.ch/mailman/listinfo/r-help
> 
> If you ever want to unsubscribe or change your options (eg, switch to
> or from digest mode, change your password, etc.), visit your
> subscription page at:
> 
>   https://stat.ethz.ch/mailman/options/r-help/elexira%40yahoo.com
> 
> You can also make such adjustments via email by sending a message to:
> 
>   R-help-request at r-project.org
> 
> with the word `help' in the subject or body (don't include the
> quotes), and you will get back a message with instructions.
> 
> You must know your password to change your options (including changing
> the password, itself) or to unsubscribe without confirmation.  It is:
> 
>   zoha1234
> 
> Normally, Mailman will remind you of your r-project.org mailing list
> passwords once every month, although you can disable this if you
> prefer.  This reminder will also include instructions on how to
> unsubscribe or change your account options.  There is also a button on
> your options page that will email your current password to you.
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From neuoneall at yahoo.com  Thu Sep 24 20:31:59 2015
From: neuoneall at yahoo.com (Mohamed A.Abdel-Fattah Mansour)
Date: Thu, 24 Sep 2015 18:31:59 +0000 (UTC)
Subject: [R] Comparing two populations based on the percentile values
 calculated from two independent samples
Message-ID: <486548598.518375.1443119519562.JavaMail.yahoo@mail.yahoo.com>

Dear AllI Have the data for two samples drawn from two different populations
|  
?
  |  
?
  |  
?
  |  
?
  |  
?
  |  Percentiles

  |
|  
?
  |  Mean

  |  STD

  |  Min

  |  Max

  |  5th

  |  25th

  |  50th

  |  75th

  |  95th

  |
|  Sample 1

  |  25

  |  2

  |  16

  |  30

  |  18

  |  20

  |  26

  |  27

  |  29

  |
|  Sample 2

  |  26

  |  2.1

  |  21

  |  32

  |  17

  |  21

  |  27

  |  28

  |  30

  |


? I need to test the equivalence of the two samples regarding percentiles.
?Dr. Mohamed A.Abdel-Fattah Mansour Associate Prof. of Industrial Engineering Industrial Engineering Department Faculty of Engineering KKU
	[[alternative HTML version deleted]]


From JSorkin at grecc.umaryland.edu  Thu Sep 24 20:52:16 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 24 Sep 2015 14:52:16 -0400
Subject: [R] Comparing two populations based on the percentile values
 calculated from two independent samples
In-Reply-To: <486548598.518375.1443119519562.JavaMail.yahoo@mail.yahoo.com>
References: <486548598.518375.1443119519562.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <56040E20020000CB0013A096@smtp.medicine.umaryland.edu>

Mohamed,
You probably should seek the help of a local statistician, and should read about Pearson's chi square test and Fisher's exact test.
John


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 
>>> "Mohamed A.Abdel-Fattah Mansour via R-help" <r-help at r-project.org> 09/24/15 2:42 PM >>>
Dear AllI Have the data for two samples drawn from two different populations
| 

| 

| 

| 

| 

| Percentiles

|
| 

| Mean

| STD

| Min

| Max

| 5th

| 25th

| 50th

| 75th

| 95th

|
| Sample 1

| 25

| 2

| 16

| 30

| 18

| 20

| 26

| 27

| 29

|
| Sample 2

| 26

| 2.1

| 21

| 32

| 17

| 21

| 27

| 28

| 30

|


I need to test the equivalence of the two samples regarding percentiles.
Dr. Mohamed A.Abdel-Fattah Mansour Associate Prof. of Industrial Engineering Industrial Engineering Department Faculty of Engineering KKU
    [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From cadeb at usgs.gov  Thu Sep 24 21:13:27 2015
From: cadeb at usgs.gov (Cade, Brian)
Date: Thu, 24 Sep 2015 13:13:27 -0600
Subject: [R] Comparing two populations based on the percentile values
 calculated from two independent samples
In-Reply-To: <486548598.518375.1443119519562.JavaMail.yahoo@mail.yahoo.com>
References: <486548598.518375.1443119519562.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAM5M9BRdz=sZt_BKNoT86s7BrJ3=X+6JX6icRwNKQDzyj_vsdA@mail.gmail.com>

You can use quantile regression (in quantreg package) to compare
percentiles between two groups in a linear model formulation.  If you are
really interested in "equivalence" testing and not just using the term
informally, you might check out Cade (2011.  Estimating equivalence with
quantile regression.  Ecological Applications 21: 281-289) for some
examples.

Brian

Brian S. Cade, PhD

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  cadeb at usgs.gov <brian_cade at usgs.gov>
tel:  970 226-9326


On Thu, Sep 24, 2015 at 12:31 PM, Mohamed A.Abdel-Fattah Mansour via R-help
<r-help at r-project.org> wrote:

> Dear AllI Have the data for two samples drawn from two different
> populations
> |
>
>   |
>
>   |
>
>   |
>
>   |
>
>   |  Percentiles
>
>   |
> |
>
>   |  Mean
>
>   |  STD
>
>   |  Min
>
>   |  Max
>
>   |  5th
>
>   |  25th
>
>   |  50th
>
>   |  75th
>
>   |  95th
>
>   |
> |  Sample 1
>
>   |  25
>
>   |  2
>
>   |  16
>
>   |  30
>
>   |  18
>
>   |  20
>
>   |  26
>
>   |  27
>
>   |  29
>
>   |
> |  Sample 2
>
>   |  26
>
>   |  2.1
>
>   |  21
>
>   |  32
>
>   |  17
>
>   |  21
>
>   |  27
>
>   |  28
>
>   |  30
>
>   |
>
>
>   I need to test the equivalence of the two samples regarding percentiles.
>  Dr. Mohamed A.Abdel-Fattah Mansour Associate Prof. of Industrial
> Engineering Industrial Engineering Department Faculty of Engineering KKU
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Thu Sep 24 21:36:42 2015
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Thu, 24 Sep 2015 21:36:42 +0200
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
Message-ID: <20150924193642.GA1757@localhost.localdomain>

Hi,
And thanks for your reply.
Essentially, your script gets the job done.
For instance, if I run

mm <- cbind(5/(1:5), -2*sqrt(1:5))
dst <- dist(mm)
dst2 <- as.matrix(dst)
diag(dst2) <- NA
idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))

then it correctly detects the first two rows, where all the values are
larger than 0.9.
In other words, it detects the points that are at least 0.9 units away
from *all* the other points.
My other question (I did not realize this until I got your answer) is
the following: I have the distance matrix of a set of N points.
You gave me an algorithm two find all the points that are at least 0.9
units away from any other points.
However, in some cases, for me it is OK even a weaker condition: find
a subset of k points (with k tunable) whose distance *from each other*
is greater than 0.9 units (even if their distance from some other
points may be smaller than 0.9).
Any idea about how to tackle that? Is it simply a matter of detecting
the row and column numbers of all the entries of the distance matrix
larger than 0.9?
Many thanks

Lorenzo



On Wed, Sep 23, 2015 at 09:23:04PM +0000, David L Carlson wrote:
>I think the OP wanted rows where all values were greater than .9.
>If so, this works:
>
>> set.seed(42)
>> dst <- dist(cbind(rnorm(20), rnorm(20)))
>> dst2 <- as.matrix(dst)
>> diag(dst2) <- NA
>> idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))
>> idx
>13 18 19
>13 18 19
>> dst2[idx, idx]
>         13       18       19
>13       NA 2.272407 3.606054
>18 2.272407       NA 1.578150
>19 3.606054 1.578150       NA
>
>-------------------------------------
>David L Carlson
>Department of Anthropology
>Texas A&M University
>College Station, TX 77840-4352
>
>
>
>-----Original Message-----
>From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of William Dunlap
>Sent: Wednesday, September 23, 2015 3:23 PM
>To: Lorenzo Isella
>Cc: r-help at r-project.org
>Subject: Re: [R] Sampling the Distance Matrix
>
>> mm <- cbind(1/(1:5), sqrt(1:5))
>> d <- dist(mm)
>> d
>          1         2         3         4
>2 0.6492864
>3 0.9901226 0.3588848
>4 1.2500000 0.6369033 0.2806086
>5 1.4723668 0.8748970 0.5213550 0.2413050
>> which(as.matrix(d)>0.9, arr.ind=TRUE)
>  row col
>3   3   1
>4   4   1
>5   5   1
>1   1   3
>1   1   4
>1   1   5
>I.e., the distances between mm's rows 3 & 1, 4 & 1, and 5,1 are more than 0.9
>
>The as.matrix(d) is needed because dist returns the lower triangle of
>the distance
>matrix and an object of class "dist" and as.matrix.dist converts that
>into a matrix.
>
>Bill Dunlap
>TIBCO Software
>wdunlap tibco.com
>
>
>On Wed, Sep 23, 2015 at 12:15 PM, Lorenzo Isella
><lorenzo.isella at gmail.com> wrote:
>> Dear All,
>> Suppose you have a distance matrix stored like a dist object, for
>> instance
>>
>> x<-rnorm(20)
>> y<-rnorm(20)
>>
>> mm<-as.matrix(cbind(x,y))
>>
>> dst<-(dist(mm))
>>
>> Now, my problem is the following: I would like to get the rows of mm
>> corresponding to points whose distance is always larger of, let's say,
>> 0.9.
>> In other words, if I were to compute the distance matrix on those
>> selected rows of mm, apart from the diagonal, I would get all entries
>> larger than 0.9.
>> Any idea about how I can efficiently code that?
>> Regards
>>
>> Lorenzo
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From tanasa at gmail.com  Thu Sep 24 21:43:23 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Thu, 24 Sep 2015 12:43:23 -0700
Subject: [R] comparing 2 long lists in R
Message-ID: <CA+JEM01NV_nONsMk=g+1VVfO0SaaRO0GQ=uNrtBH1bE9djmPgw@mail.gmail.com>

Dear all,

please could you advise on a computationally quick way to compare and merge
2 long lists in R;
the lists are of the following type, for example :

<> in list 1 :

chromosome, coordinateA, coordinateB, value1
chromosome, coordinateC, coordinateC, value2,
etc

<> in list 2 :

chromosome, coordinateX, coordinateY, value6
chromosome, coordinateZ, coordinateT, value8,
etc

In the unified list, if coordinateA=coordinateX, and
coordinateB=coordinateY, then we write :

chromosome, coordinateA, coordinateB, value1, coordinateX, coordinateY,
value6,

otherwise, we write the individual values :

chromosome, coordinateA, coordinateB, value1,
chromosome, coordinateX, coordinateY, value6,

thanks,

bogdan

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Thu Sep 24 21:57:28 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 24 Sep 2015 15:57:28 -0400
Subject: [R] comparing 2 long lists in R
In-Reply-To: <CA+JEM01NV_nONsMk=g+1VVfO0SaaRO0GQ=uNrtBH1bE9djmPgw@mail.gmail.com>
References: <CA+JEM01NV_nONsMk=g+1VVfO0SaaRO0GQ=uNrtBH1bE9djmPgw@mail.gmail.com>
Message-ID: <CAM_vjunkSPd-xH7yGc4D+Tedq9NjeGMusOe72BeDbEgiCBirfQ@mail.gmail.com>

merge() most likely, but: are these really lists in the R sense?

The correct answer depends on what the format actually is; you need to
use dput() or some other unambiguous way of providing sample data.

Without a reproducible example that includes some sample data provided
using dput() (fake is fine), the code you used, and some clear idea of
what output you expect, it's impossible to figure out how to help you.
Here are some suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

Sarah


On Thu, Sep 24, 2015 at 3:43 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
> Dear all,
>
> please could you advise on a computationally quick way to compare and merge
> 2 long lists in R;
> the lists are of the following type, for example :
>
> <> in list 1 :
>
> chromosome, coordinateA, coordinateB, value1
> chromosome, coordinateC, coordinateC, value2,
> etc
>
> <> in list 2 :
>
> chromosome, coordinateX, coordinateY, value6
> chromosome, coordinateZ, coordinateT, value8,
> etc
>
> In the unified list, if coordinateA=coordinateX, and
> coordinateB=coordinateY, then we write :
>
> chromosome, coordinateA, coordinateB, value1, coordinateX, coordinateY,
> value6,
>
> otherwise, we write the individual values :
>
> chromosome, coordinateA, coordinateB, value1,
> chromosome, coordinateX, coordinateY, value6,
>
> thanks,
>
> bogdan
>
>         [[alternative HTML version deleted]]
>
and please don't post in HTML.

-- 
Sarah Goslee
http://www.functionaldiversity.org


From bgunter.4567 at gmail.com  Thu Sep 24 22:17:11 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 24 Sep 2015 13:17:11 -0700
Subject: [R] comparing 2 long lists in R
In-Reply-To: <CAM_vjunkSPd-xH7yGc4D+Tedq9NjeGMusOe72BeDbEgiCBirfQ@mail.gmail.com>
References: <CA+JEM01NV_nONsMk=g+1VVfO0SaaRO0GQ=uNrtBH1bE9djmPgw@mail.gmail.com>
	<CAM_vjunkSPd-xH7yGc4D+Tedq9NjeGMusOe72BeDbEgiCBirfQ@mail.gmail.com>
Message-ID: <CAGxFJbQ3CEa_v7ZEUgg9bO8GU8VG+r7gp8rQSGr=OVUCi3tO4A@mail.gmail.com>

Also, in addition to what Sarah told you, have you checked on the
Bioconductor site, as this sounds like the sort of thing that they may
well have something for already.

... and you've posted here often enough that you shouldn't still be
posting HTML and you should know about toy examples!

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Sep 24, 2015 at 12:57 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> merge() most likely, but: are these really lists in the R sense?
>
> The correct answer depends on what the format actually is; you need to
> use dput() or some other unambiguous way of providing sample data.
>
> Without a reproducible example that includes some sample data provided
> using dput() (fake is fine), the code you used, and some clear idea of
> what output you expect, it's impossible to figure out how to help you.
> Here are some suggestions for creating a good reproducible example:
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>
> Sarah
>
>
> On Thu, Sep 24, 2015 at 3:43 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
>> Dear all,
>>
>> please could you advise on a computationally quick way to compare and merge
>> 2 long lists in R;
>> the lists are of the following type, for example :
>>
>> <> in list 1 :
>>
>> chromosome, coordinateA, coordinateB, value1
>> chromosome, coordinateC, coordinateC, value2,
>> etc
>>
>> <> in list 2 :
>>
>> chromosome, coordinateX, coordinateY, value6
>> chromosome, coordinateZ, coordinateT, value8,
>> etc
>>
>> In the unified list, if coordinateA=coordinateX, and
>> coordinateB=coordinateY, then we write :
>>
>> chromosome, coordinateA, coordinateB, value1, coordinateX, coordinateY,
>> value6,
>>
>> otherwise, we write the individual values :
>>
>> chromosome, coordinateA, coordinateB, value1,
>> chromosome, coordinateX, coordinateY, value6,
>>
>> thanks,
>>
>> bogdan
>>
>>         [[alternative HTML version deleted]]
>>
> and please don't post in HTML.
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Sep 24 22:30:02 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 24 Sep 2015 13:30:02 -0700
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <20150924193642.GA1757@localhost.localdomain>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
	<20150924193642.GA1757@localhost.localdomain>
Message-ID: <BACC74A6-67C2-4B9F-920E-6554B76F5A75@comcast.net>


On Sep 24, 2015, at 12:36 PM, Lorenzo Isella wrote:

> Hi,
> And thanks for your reply.
> Essentially, your script gets the job done.
> For instance, if I run
> 
> mm <- cbind(5/(1:5), -2*sqrt(1:5))
> dst <- dist(mm)
> dst2 <- as.matrix(dst)
> diag(dst2) <- NA
> idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))
> 
> then it correctly detects the first two rows, where all the values are
> larger than 0.9.
> In other words, it detects the points that are at least 0.9 units away
> from *all* the other points.
> My other question (I did not realize this until I got your answer) is
> the following: I have the distance matrix of a set of N points.
> You gave me an algorithm two find all the points that are at least 0.9
> units away from any other points.
> However, in some cases, for me it is OK even a weaker condition: find
> a subset of k points (with k tunable) whose distance *from each other*
> is greater than 0.9 units (even if their distance from some other
> points may be smaller than 0.9).

If I understand ..... Make a matrix of unique combinations, then apply by rows to get the qualifying columns that satisfy the distance criterion:

mtxcomb <- combn(1:20, 5)
goodcls <- apply(mtxcomb , 2, function(idx) all( dist( cbind( x[idx], y[idx]) ) > 0.9))
mtxcomb [ , goodcls]

In my sample it was around 9% of the total 5 item combinations.

snipped a lot of output:
.....
    [,1440] [,1441]
[1,]      12      13
[2,]      13      16
[3,]      16      17
[4,]      19      19
[5,]      20      20
> dim( mtxcomb)
[1]     5 15504


-- 
David

> Any idea about how to tackle that? Is it simply a matter of detecting
> the row and column numbers of all the entries of the distance matrix
> larger than 0.9?
> Many thanks
> 
> Lorenzo
> 
> 
> 
> On Wed, Sep 23, 2015 at 09:23:04PM +0000, David L Carlson wrote:
>> I think the OP wanted rows where all values were greater than .9.
>> If so, this works:
>> 
>>> set.seed(42)
>>> dst <- dist(cbind(rnorm(20), rnorm(20)))
>>> dst2 <- as.matrix(dst)
>>> diag(dst2) <- NA
>>> idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))
>>> idx
>> 13 18 19
>> 13 18 19
>>> dst2[idx, idx]
>>        13       18       19
>> 13       NA 2.272407 3.606054
>> 18 2.272407       NA 1.578150
>> 19 3.606054 1.578150       NA
>> 
>> -------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77840-4352
>> 
>> 
>> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of William Dunlap
>> Sent: Wednesday, September 23, 2015 3:23 PM
>> To: Lorenzo Isella
>> Cc: r-help at r-project.org
>> Subject: Re: [R] Sampling the Distance Matrix
>> 
>>> mm <- cbind(1/(1:5), sqrt(1:5))
>>> d <- dist(mm)
>>> d
>>         1         2         3         4
>> 2 0.6492864
>> 3 0.9901226 0.3588848
>> 4 1.2500000 0.6369033 0.2806086
>> 5 1.4723668 0.8748970 0.5213550 0.2413050
>>> which(as.matrix(d)>0.9, arr.ind=TRUE)
>> row col
>> 3   3   1
>> 4   4   1
>> 5   5   1
>> 1   1   3
>> 1   1   4
>> 1   1   5
>> I.e., the distances between mm's rows 3 & 1, 4 & 1, and 5,1 are more than 0.9
>> 
>> The as.matrix(d) is needed because dist returns the lower triangle of
>> the distance
>> matrix and an object of class "dist" and as.matrix.dist converts that
>> into a matrix.
>> 
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>> 
>> 
>> On Wed, Sep 23, 2015 at 12:15 PM, Lorenzo Isella
>> <lorenzo.isella at gmail.com> wrote:
>>> Dear All,
>>> Suppose you have a distance matrix stored like a dist object, for
>>> instance
>>> 
>>> x<-rnorm(20)
>>> y<-rnorm(20)
>>> 
>>> mm<-as.matrix(cbind(x,y))
>>> 
>>> dst<-(dist(mm))
>>> 
>>> Now, my problem is the following: I would like to get the rows of mm
>>> corresponding to points whose distance is always larger of, let's say,
>>> 0.9.
>>> In other words, if I were to compute the distance matrix on those
>>> selected rows of mm, apart from the diagonal, I would get all entries
>>> larger than 0.9.
>>> Any idea about how I can efficiently code that?
>>> Regards
>>> 
>>> Lorenzo
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From lorenzo.isella at gmail.com  Thu Sep 24 22:54:52 2015
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Thu, 24 Sep 2015 22:54:52 +0200
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <BACC74A6-67C2-4B9F-920E-6554B76F5A75@comcast.net>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
	<20150924193642.GA1757@localhost.localdomain>
	<BACC74A6-67C2-4B9F-920E-6554B76F5A75@comcast.net>
Message-ID: <20150924205452.GB1757@localhost.localdomain>

On Thu, Sep 24, 2015 at 01:30:02PM -0700, David Winsemius wrote:
>
>On Sep 24, 2015, at 12:36 PM, Lorenzo Isella wrote:
>
>> Hi,
>> And thanks for your reply.
>> Essentially, your script gets the job done.
>> For instance, if I run
>>
>> mm <- cbind(5/(1:5), -2*sqrt(1:5))
>> dst <- dist(mm)
>> dst2 <- as.matrix(dst)
>> diag(dst2) <- NA
>> idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))
>>
>> then it correctly detects the first two rows, where all the values are
>> larger than 0.9.
>> In other words, it detects the points that are at least 0.9 units away
>> from *all* the other points.
>> My other question (I did not realize this until I got your answer) is
>> the following: I have the distance matrix of a set of N points.
>> You gave me an algorithm two find all the points that are at least 0.9
>> units away from any other points.
>> However, in some cases, for me it is OK even a weaker condition: find
>> a subset of k points (with k tunable) whose distance *from each other*
>> is greater than 0.9 units (even if their distance from some other
>> points may be smaller than 0.9).
>
>If I understand ..... Make a matrix of unique combinations, then apply by rows to get the qualifying columns that satisfy the distance criterion:
>
>mtxcomb <- combn(1:20, 5)
>goodcls <- apply(mtxcomb , 2, function(idx) all( dist( cbind( x[idx], y[idx]) ) > 0.9))
>mtxcomb [ , goodcls]
>
>In my sample it was around 9% of the total 5 item combinations.
>
>snipped a lot of output:
>.....
>    [,1440] [,1441]
>[1,]      12      13
>[2,]      13      16
>[3,]      16      17
>[4,]      19      19
>[5,]      20      20
>> dim( mtxcomb)
>[1]     5 15504
>

Hi,
Thanks for your reply.
I think I am getting there, but when I run your commands, I get this
error message

Error in cbind(x[idx], y[idx]) : object 'x' not found

Any idea why? Should I combine those 3 lines with something else?
Cheers

Lorenzo


From tanasa at gmail.com  Thu Sep 24 23:06:07 2015
From: tanasa at gmail.com (Bogdan Tanasa)
Date: Thu, 24 Sep 2015 14:06:07 -0700
Subject: [R] comparing 2 long lists in R
In-Reply-To: <CAGxFJbQ3CEa_v7ZEUgg9bO8GU8VG+r7gp8rQSGr=OVUCi3tO4A@mail.gmail.com>
References: <CA+JEM01NV_nONsMk=g+1VVfO0SaaRO0GQ=uNrtBH1bE9djmPgw@mail.gmail.com>
	<CAM_vjunkSPd-xH7yGc4D+Tedq9NjeGMusOe72BeDbEgiCBirfQ@mail.gmail.com>
	<CAGxFJbQ3CEa_v7ZEUgg9bO8GU8VG+r7gp8rQSGr=OVUCi3tO4A@mail.gmail.com>
Message-ID: <CA+JEM00dww04W0Bt7eyXKJsWucyeVgMmWrS9xn2gbqCnKpBf2A@mail.gmail.com>

Dear Bert and Sarah, thank you for your suggestions. Yes, I came across
"dplyr" that has a few functions already implemented, thanks again !

On Thu, Sep 24, 2015 at 1:17 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Also, in addition to what Sarah told you, have you checked on the
> Bioconductor site, as this sounds like the sort of thing that they may
> well have something for already.
>
> ... and you've posted here often enough that you shouldn't still be
> posting HTML and you should know about toy examples!
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Thu, Sep 24, 2015 at 12:57 PM, Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
> > merge() most likely, but: are these really lists in the R sense?
> >
> > The correct answer depends on what the format actually is; you need to
> > use dput() or some other unambiguous way of providing sample data.
> >
> > Without a reproducible example that includes some sample data provided
> > using dput() (fake is fine), the code you used, and some clear idea of
> > what output you expect, it's impossible to figure out how to help you.
> > Here are some suggestions for creating a good reproducible example:
> >
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> >
> > Sarah
> >
> >
> > On Thu, Sep 24, 2015 at 3:43 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
> >> Dear all,
> >>
> >> please could you advise on a computationally quick way to compare and
> merge
> >> 2 long lists in R;
> >> the lists are of the following type, for example :
> >>
> >> <> in list 1 :
> >>
> >> chromosome, coordinateA, coordinateB, value1
> >> chromosome, coordinateC, coordinateC, value2,
> >> etc
> >>
> >> <> in list 2 :
> >>
> >> chromosome, coordinateX, coordinateY, value6
> >> chromosome, coordinateZ, coordinateT, value8,
> >> etc
> >>
> >> In the unified list, if coordinateA=coordinateX, and
> >> coordinateB=coordinateY, then we write :
> >>
> >> chromosome, coordinateA, coordinateB, value1, coordinateX, coordinateY,
> >> value6,
> >>
> >> otherwise, we write the individual values :
> >>
> >> chromosome, coordinateA, coordinateB, value1,
> >> chromosome, coordinateX, coordinateY, value6,
> >>
> >> thanks,
> >>
> >> bogdan
> >>
> >>         [[alternative HTML version deleted]]
> >>
> > and please don't post in HTML.
> >
> > --
> > Sarah Goslee
> > http://www.functionaldiversity.org
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From shidaxia at yahoo.com  Thu Sep 24 23:31:39 2015
From: shidaxia at yahoo.com (Shi, Tao)
Date: Thu, 24 Sep 2015 21:31:39 +0000 (UTC)
Subject: [R] comparing 2 long lists in R
In-Reply-To: <CAM_vjunkSPd-xH7yGc4D+Tedq9NjeGMusOe72BeDbEgiCBirfQ@mail.gmail.com>
References: <CA+JEM01NV_nONsMk=g+1VVfO0SaaRO0GQ=uNrtBH1bE9djmPgw@mail.gmail.com>
	<CAM_vjunkSPd-xH7yGc4D+Tedq9NjeGMusOe72BeDbEgiCBirfQ@mail.gmail.com>
Message-ID: <1457450970.604037.1443130299262.JavaMail.yahoo@mail.yahoo.com>

Bogdan,


I would look into bioconductor for packages handling this type choromosomal range data.  cntools is one poped into my mind.

Tao



On Thursday, September 24, 2015 12:59 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:



merge() most likely, but: are these really lists in the R sense?

The correct answer depends on what the format actually is; you need to
use dput() or some other unambiguous way of providing sample data.

Without a reproducible example that includes some sample data provided
using dput() (fake is fine), the code you used, and some clear idea of
what output you expect, it's impossible to figure out how to help you.
Here are some suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

Sarah


On Thu, Sep 24, 2015 at 3:43 PM, Bogdan Tanasa <tanasa at gmail.com> wrote:
> Dear all,
>
> please could you advise on a computationally quick way to compare and merge
> 2 long lists in R;
> the lists are of the following type, for example :
>
> <> in list 1 :
>
> chromosome, coordinateA, coordinateB, value1
> chromosome, coordinateC, coordinateC, value2,
> etc
>
> <> in list 2 :
>
> chromosome, coordinateX, coordinateY, value6
> chromosome, coordinateZ, coordinateT, value8,
> etc
>
> In the unified list, if coordinateA=coordinateX, and
> coordinateB=coordinateY, then we write :
>
> chromosome, coordinateA, coordinateB, value1, coordinateX, coordinateY,
> value6,
>
> otherwise, we write the individual values :
>
> chromosome, coordinateA, coordinateB, value1,
> chromosome, coordinateX, coordinateY, value6,
>
> thanks,
>
> bogdan
>
>         [[alternative HTML version deleted]]
>
and please don't post in HTML.

-- 
Sarah Goslee
http://www.functionaldiversity.org


______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Fri Sep 25 00:56:10 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 24 Sep 2015 14:56:10 -0800
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <6AEB6B05-8B0F-4FEF-A5CD-36DB7B970C36@gmail.com>
References: <4cc4eadb-60c6-4992-be6e-d692c0b98da6@me.com>
	<5eaec047-aa21-4fe5-b67b-88c4e02870a3@gmail.com>
	<he1pr01mb08098500d1aeacb205ecd48292450@he1pr01mb0809.eurprd01.prod.exchangelabs.com>
	<5601f36d.5060906@gmail.com>
	<vi1pr01mb081311ac5a3874e3f1fe121292450@vi1pr01mb0813.eurprd01.prod.exchangelabs.com>
	<cagxfjbqx0vtbe9jm_y-eza=q4bvm5sdeajgsgf-1oyrqtao+ca@mail.gmail.com>
Message-ID: <4F04888C442.00000F82jrkrideau@inbox.com>


> -----Original Message-----
> From: pdalgd at gmail.com
> Sent: Thu, 24 Sep 2015 11:41:19 +0200
> To: murdoch.duncan at gmail.com
> Subject: Re: [R] 'R' Software Output Plagiarism
> 
> 
> On 23 Sep 2015, at 02:33 , Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
> 
>> I don't see why this puzzles you.  A simple explanation is that Urkund
>> is incompetent.
> 
> That much I figured. What I was puzzled about was _how_ it was being
> incompetent. Also how it could be so in a way that wouldn't be obvious to
> the professor in question.

My guess is that it just ran the scan and emailed back the results with no analysis. It sounds like the software is functioning 'properly' but the humanware is not. But then, there is no reason to expect them to be subject matter experts, either. One should just hope they would supply better reports than it appears Oliver and his professor received.

Re the professor, he/she may have just tossed the report to Oliver and said, "Explain this".  Once Oliver discussed the issue with Ukund it should be blinding obvious to the professor.

____________________________________________________________
Share photos & screenshots in seconds...
TRY FREE IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if1
Works in all emails, instant messengers, blogs, forums and social networks.


From dwinsemius at comcast.net  Fri Sep 25 01:29:51 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 24 Sep 2015 16:29:51 -0700
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <20150924205452.GB1757@localhost.localdomain>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
	<20150924193642.GA1757@localhost.localdomain>
	<BACC74A6-67C2-4B9F-920E-6554B76F5A75@comcast.net>
	<20150924205452.GB1757@localhost.localdomain>
Message-ID: <99D0D5AA-BD45-484F-8FCB-8EA0B00E27A7@comcast.net>


On Sep 24, 2015, at 1:54 PM, Lorenzo Isella wrote:

> On Thu, Sep 24, 2015 at 01:30:02PM -0700, David Winsemius wrote:
>> 
>> On Sep 24, 2015, at 12:36 PM, Lorenzo Isella wrote:
>> 
>>> Hi,
>>> And thanks for your reply.
>>> Essentially, your script gets the job done.
>>> For instance, if I run
>>> 
>>> mm <- cbind(5/(1:5), -2*sqrt(1:5))
>>> dst <- dist(mm)
>>> dst2 <- as.matrix(dst)
>>> diag(dst2) <- NA
>>> idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))
>>> 
>>> then it correctly detects the first two rows, where all the values are
>>> larger than 0.9.
>>> In other words, it detects the points that are at least 0.9 units away
>>> from *all* the other points.
>>> My other question (I did not realize this until I got your answer) is
>>> the following: I have the distance matrix of a set of N points.
>>> You gave me an algorithm two find all the points that are at least 0.9
>>> units away from any other points.
>>> However, in some cases, for me it is OK even a weaker condition: find
>>> a subset of k points (with k tunable) whose distance *from each other*
>>> is greater than 0.9 units (even if their distance from some other
>>> points may be smaller than 0.9).
>> 
>> If I understand ..... Make a matrix of unique combinations, then apply by rows to get the qualifying columns that satisfy the distance criterion:
>> 
>> mtxcomb <- combn(1:20, 5)
>> goodcls <- apply(mtxcomb , 2, function(idx) all( dist( cbind( x[idx], y[idx]) ) > 0.9))
>> mtxcomb [ , goodcls]
>> 
>> In my sample it was around 9% of the total 5 item combinations.
>> 
>> snipped a lot of output:
>> .....
>>   [,1440] [,1441]
>> [1,]      12      13
>> [2,]      13      16
>> [3,]      16      17
>> [4,]      19      19
>> [5,]      20      20
>>> dim( mtxcomb)
>> [1]     5 15504
>> 
> 
> Hi,
> Thanks for your reply.
> I think I am getting there, but when I run your commands, I get this
> error message
> 
> Error in cbind(x[idx], y[idx]) : object 'x' not found
> 
> Any idea why? Should I combine those 3 lines with something else?

No idea. I was running the setup that you asked for in your original message which you have now omitted from the mail chain.



> Cheers
> 
> Lorenzo

David Winsemius
Alameda, CA, USA


From vik at vlr.cc  Fri Sep 25 01:35:30 2015
From: vik at vlr.cc (Vik Rubenfeld)
Date: Thu, 24 Sep 2015 16:35:30 -0700
Subject: [R] 'R' Software Output Plagiarism
In-Reply-To: <HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
References: <VI1PR01MB081311AC5A3874E3F1FE121292450@VI1PR01MB0813.eurprd01.prod.exchangelabs.com>
	<HE1PR01MB08098500D1AEACB205ECD48292450@HE1PR01MB0809.eurprd01.prod.exchangelabs.com>
Message-ID: <6E3C4EB0-2E62-47CA-8307-F761A2A32446@vlr.cc>

Your professor should immediately recognize that the quoted code is standard regression input/output and that the Urkund results in this case are without merit.


> On Sep 22, 2015, at 7:27 AM, BARRETT, Oliver <oliver.barrett at skema.edu> wrote:
> 
> 
> Dear 'R' community support,
> 
> 
> I am a student at Skema business school and I have recently submitted my MSc thesis/dissertation. This has been passed on to an external plagiarism service provider, Urkund, who have scanned my document and returned a plagiarism report to my professor having detected 32% plagiarism.
> 
> 
> I have contacted Urkund regarding this issue having committed no such plagiarism and they have told me that all the plagiarism detected in my document comes from the last 25% which consists only of 'R' regressions like the one I have pasted below:
> 
> lm(formula = Prague50 ~ Fed + Fed.t.1. + Fed.t.2. + Fed.t.3. +
>    Fed.t.4., data = OLS_CAR, x = TRUE)
> 
> Residuals:
>      Min        1Q    Median        3Q       Max
> -0.154587 -0.015961  0.001429  0.017196  0.110907
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) -0.001630   0.001763  -0.925   0.3559
> Fed         -0.121595   0.165359  -0.735   0.4627
> Fed.t.1.     0.344014   0.140979   2.440   0.0153 *
> Fed.t.2.     0.026529   0.143648   0.185   0.8536
> Fed.t.3.     0.622357   0.142021   4.382 1.62e-05 ***
> Fed.t.4.     0.291985   0.158914   1.837   0.0671 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Residual standard error: 0.0293 on 304 degrees of freedom
>  (20 observations deleted due to missingness)
> Multiple R-squared:  0.08629,  Adjusted R-squared:  0.07126
> F-statistic: 5.742 on 5 and 304 DF,  p-value: 4.422e-05
> 
> I have produced all of these regressions myself and pasted them directly from the 'R' software package. My regression methodology is entirely my own along with the sourcing and preperation of the data used to produce these statistics.
> 
> I would be very grateful if you could provide my with some clarity as to why this output from 'R' is reading as plagiarism.
> 
> I would like to thank you in advance,
> 
> Kind regards,
> 
> Oliver Barrett
> (+44) 7341 834 217
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Fri Sep 25 01:38:11 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 24 Sep 2015 15:38:11 -0800
Subject: [R] Welcome to the "R-help" mailing list (Digest mode)
In-Reply-To: <598062399.469470.1443116061314.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.0.1443115896.10317.r-help@r-project.org>
Message-ID: <4F627367032.0000006Fjrkrideau@inbox.com>

You seem to have sent the message in HTML and most of it did not arrive.  R-help is a plain text mailing list and strips out any HTML as a security precaution.  

Please have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-ex
ample and/or http://adv-r.had.co.nz/Reproducibility.html with particular attention to the use of  dput() for supplying sample data.

Welcome to the list. 

John Kane
Kingston ON Canada


> -----Original Message-----
> From: r-help at r-project.org
> Sent: Thu, 24 Sep 2015 17:34:21 +0000 (UTC)
> To: r-help at r-project.org
> Subject: Re: [R] Welcome to the "R-help" mailing list (Digest mode)
> 
> I would greatly appreciate if anybody could help with the stack-overflow
> question : Updating and Forecasting times series with new data
> | ? |
> | ? |  | ? | ? | ? | ? | ? |
> | Updating and Forecasting times series with new dataWe can fit a time
> series and use the model to make forecasts, for example using: fit <-
> auto.arima(WWWusage) plot(forecast(fit,h=20)) Consider the following
> scenari... |
> |  |
> | View on stackoverflow.com | Preview by Yahoo |
> |  |
> | ? |
> 
> 
> Thank you for your time,Pemfir
> 
> 
> 
>      On Thursday, September 24, 2015 10:31 AM,
> "r-help-request at r-project.org" <r-help-request at r-project.org> wrote:
> 
> 
>  Welcome to the R-help at r-project.org mailing list!
> 
> To post to this list, send your message to:
> 
> ? r-help at r-project.org
> 
> General information about the mailing list is at:
> 
> ? https://stat.ethz.ch/mailman/listinfo/r-help
> 
> If you ever want to unsubscribe or change your options (eg, switch to
> or from digest mode, change your password, etc.), visit your
> subscription page at:
> 
> ? https://stat.ethz.ch/mailman/options/r-help/elexira%40yahoo.com
> 
> You can also make such adjustments via email by sending a message to:
> 
> ? R-help-request at r-project.org
> 
> with the word `help' in the subject or body (don't include the
> quotes), and you will get back a message with instructions.
> 
> You must know your password to change your options (including changing
> the password, itself) or to unsubscribe without confirmation.? It is:
> 
> ? zoha1234
> 
> Normally, Mailman will remind you of your r-project.org mailing list
> passwords once every month, although you can disable this if you
> prefer.? This reminder will also include instructions on how to
> unsubscribe or change your account options.? There is also a button on
> your options page that will email your current password to you.
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From hnorpois at gmail.com  Thu Sep 24 23:26:50 2015
From: hnorpois at gmail.com (Hermann Norpois)
Date: Thu, 24 Sep 2015 23:26:50 +0200
Subject: [R] lines - defining the range that is plotted
Message-ID: <CAKyZeBt73t0qLt7+2YzOtpv7_e0=VxO0kbVequUKAZNLw=gL1g@mail.gmail.com>

Hello,

I am looking for a possibility to define something like ylim for lines. I
thought, there might be a possibility to define the range of lines by means
of par ("usr") but I did not find the correct syntax. In my toy example I
would like to stop the red line at y=0.3.
Thanks Hermann

toy example:

x <- seq (-3,3, by=0.1)
y1 <- dnorm (x)
y2 <- pnorm (x)
plot (x,y1)
lines (x,y2, col="red")

	[[alternative HTML version deleted]]


From bgnumis at gmail.com  Thu Sep 24 23:43:03 2015
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Thu, 24 Sep 2015 23:43:03 +0200
Subject: [R] Quantmod several indicators
Message-ID: <CAN25tHSR_dRZotJYyS_RRSU+qt_TPK5Z5xmwpy_cPSJ0_p0TZQ@mail.gmail.com>

Hi all,

I?m trying to use quantmod and to display plot of Bollinger bands is
working properly.

?How should I add for instance addWPR(n =300 ) below the main plot?  Could
it would be plotted independently?

chartSeries(
IB,theme="white",TA = c(addBBands(200,2) )

Thanks in advance.

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Fri Sep 25 08:44:36 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 25 Sep 2015 03:44:36 -0300
Subject: [R] lines - defining the range that is plotted
In-Reply-To: <CAKyZeBt73t0qLt7+2YzOtpv7_e0=VxO0kbVequUKAZNLw=gL1g@mail.gmail.com>
References: <CAKyZeBt73t0qLt7+2YzOtpv7_e0=VxO0kbVequUKAZNLw=gL1g@mail.gmail.com>
Message-ID: <5604ED54.8070102@gmail.com>

On 24/09/2015 6:26 PM, Hermann Norpois wrote:
> Hello,
> 
> I am looking for a possibility to define something like ylim for lines. I
> thought, there might be a possibility to define the range of lines by means
> of par ("usr") but I did not find the correct syntax. In my toy example I
> would like to stop the red line at y=0.3.
> Thanks Hermann
> 
> toy example:
> 
> x <- seq (-3,3, by=0.1)
> y1 <- dnorm (x)
> y2 <- pnorm (x)
> plot (x,y1)
> lines (x,y2, col="red")
> 

The clip() function does what you want.  You could use

clip(min(x), max(x), min(y2), 0.3)

before the call to lines() to get what you want.

See the example in the help page for how to restore the full region.

Duncan Murdoch


From maechler at stat.math.ethz.ch  Fri Sep 25 10:25:02 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 25 Sep 2015 10:25:02 +0200
Subject: [R] backsolve, chol, Matrix, and SparseM
In-Reply-To: <5604374E.7000301@gmail.com>
References: <5604374E.7000301@gmail.com>
Message-ID: <22021.1246.450108.509693@stat.math.ethz.ch>

Dear Ben,

>>>>> Benjamin Tyner <btyner at gmail.com>
>>>>>     on Thu, 24 Sep 2015 13:47:58 -0400 writes:

    > Hi I have some code which does (on a symmetric matrix 'x')

    >     backsolve(chol(x), diag(nrow(x)))

    > and I am wondering what is the recommended way to
    > accomplish this when x is also sparse (from
    > package:Matrix). I know that package:Matrix provides a
    > chol method for such matrices, but not a backsolve
    > method. On the other hand, package:SparseM does provide a
    > backsolve method, but doesn't actually return a sparse
    > matrix. Moreover, I am a little hesitant to use SparseM,
    > as the vignette seems to be from 2003.

Roger Koenker has agreed in the past, that new projects should
rather use Matrix.   SparseM has been the very first R package
providing sparse matrix support.


    > I did notice that help(topic = "solve", package =
    > "Matrix") says "In ?solve(a,b)? in the ?Matrix? package,
    > ?a? may also be a ?MatrixFactorization? instead of
    > directly a matrix." which makes me think this is the right
    > way:

    >     Matrix::solve(Cholesky(x), .sparseDiagonal(nrow(x)))

    > but unfortunately this didn't give the same result as:

    >     Matrix::solve(chol(x), .sparseDiagonal(nrow(x)))

    > so I'm asking here in case someone has any suggestions.

You don't give any examples.
So a few remarks and a reproducible example to get more concrete

A. As the Matrix package has classes for triangular matrices and
  Matrix :: chol() returns them, there   is no need for
  forwardsolve() or backwardsolve(), as just   solve() is always
  enough.

B. As Doug Bates has been teaching for many decennia, "it is
  almost always computationally *wrong* to compute a matrix
  inverse explicitly".
  Rather compute    A^{-1} B   or  A^{-1} x  {for vector x,
  matrix B (but different from Identity).

C. Inspite of B, there are cases (such as computing sandwich
  estimates of covariance matrices) where you do want the inverse.
  In that case,

   solve(A)   		  is semantically equivalent to
   solve(A, diag(.))

   and almost always the *first* form is implempented more
   efficiently than the second.

D. In Matrix,  use chol(.) ... unless you really read a bit
   about Cholesky(.) and its special purpose sparse cholesky decompositions.
   As mentioned above,  Matrix :: chol()  will return a
   "formally triangular" matrix, i.e., inheriting from
   "triangularMatrix"; in the sparse case, very typically of
   specific class "dtCMatrix".

Here's a small reproducible example,
please use it to ask further questions:

*.R:

library(Matrix)
M <- as(diag(4)+1,"dsCMatrix")
m <- as(M, "matrix") # -> traditional R matrix
stopifnot( all(M == m) )
M
L <- Cholesky(M,super=TRUE,perm=FALSE) # a MatrixFactorization ("dCHMsuper")
(L. <- as(L, "Matrix")) #-> lower-triagonal (sparseMatrix, specifically "dtCMatrix")
(cM <- chol(M))# *upper* triagonal ("dtCMatrix")
(cm <- chol(m))#  upper  triagonal traditional matrix -- the same "of course" :
all.equal(as.matrix(cM), cm) # TRUE

(r. <- backsolve(cm, diag(4)))# upper tri. (traditional) matrix
(R. <-     solve(cM) ) ## the "same"  (but nicer printing)
all.equal(as.matrix(R.), r., check.attributes=FALSE) # TRUE
all( abs(R. - r.) <  1e-12 * mean(abs(R.))) # TRUE

*.Rout:

> M <- as(diag(4)+1,"dsCMatrix")
> m <- as(M, "matrix") # -> traditional R matrix
> stopifnot( all(M == m) )
> M
4 x 4 sparse Matrix of class "dsCMatrix"
            
[1,] 2 1 1 1
[2,] 1 2 1 1
[3,] 1 1 2 1
[4,] 1 1 1 2
> L <- Cholesky(M,super=TRUE,perm=FALSE) # a MatrixFactorization ("dCHMsuper")
> (L. <- as(L, "Matrix")) #-> lower-triagonal (sparseMatrix, specifically "dtCMatrix")
4 x 4 sparse Matrix of class "dtCMatrix"
                                           
[1,] 1.4142136 .         .         .       
[2,] 0.7071068 1.2247449 .         .       
[3,] 0.7071068 0.4082483 1.1547005 .       
[4,] 0.7071068 0.4082483 0.2886751 1.118034
> (cM <- chol(M))# *upper* triagonal ("dtCMatrix")
4 x 4 sparse Matrix of class "dtCMatrix"
                                           
[1,] 1.414214 0.7071068 0.7071068 0.7071068
[2,] .        1.2247449 0.4082483 0.4082483
[3,] .        .         1.1547005 0.2886751
[4,] .        .         .         1.1180340
> (cm <- chol(m))#  upper  triagonal traditional matrix -- the same "of course" :
         [,1]      [,2]      [,3]      [,4]
[1,] 1.414214 0.7071068 0.7071068 0.7071068
[2,] 0.000000 1.2247449 0.4082483 0.4082483
[3,] 0.000000 0.0000000 1.1547005 0.2886751
[4,] 0.000000 0.0000000 0.0000000 1.1180340
> all.equal(as.matrix(cM), cm) # TRUE
[1] TRUE
> (r. <- backsolve(cm, diag(4)))# upper tri. (traditional) matrix
          [,1]       [,2]       [,3]       [,4]
[1,] 0.7071068 -0.4082483 -0.2886751 -0.2236068
[2,] 0.0000000  0.8164966 -0.2886751 -0.2236068
[3,] 0.0000000  0.0000000  0.8660254 -0.2236068
[4,] 0.0000000  0.0000000  0.0000000  0.8944272
> (R. <-     solve(cM) ) ## the "same"  (but nicer printing)
4 x 4 sparse Matrix of class "dtCMatrix"
                                               
[1,] 0.7071068 -0.4082483 -0.2886751 -0.2236068
[2,] .          0.8164966 -0.2886751 -0.2236068
[3,] .          .          0.8660254 -0.2236068
[4,] .          .          .          0.8944272
> all.equal(as.matrix(R.), r., check.attributes=FALSE) # TRUE
[1] TRUE
> all( abs(R. - r.) <  1e-12 * mean(abs(R.))) # TRUE
[1] TRUE
>


From 2hanl2da at naver.com  Fri Sep 25 13:44:59 2015
From: 2hanl2da at naver.com (massmatics)
Date: Fri, 25 Sep 2015 04:44:59 -0700 (PDT)
Subject: [R] There is an error when I performed chisquare and postanova
 test...:(
Message-ID: <1443181499770-4712770.post@n4.nabble.com>

I am studying for a quiz and while I was solving two problems, an error
occured.
a) So the problem is we want to test if the block program makes a difference
in retention at ? = 1%
(which is the significance level)
This is the code I used to perform chisquare test.
My solution:
blockprogram <- matrix(c(18,15,5,8,4,10,5,7,18,10), nrow=2, byrow=T)
colnames(blockprogram) <- c("1 yr","2 yr","3 yr","4 yr","5+ yrs")
rownames(blockprogram) <- c("Non-Block","Block")	
chisq.test(blockprogram, conf.level=0.99) ==> This is the part where I can't
run..
chisq.test(blockprogram, conf.level=0.99)$observed
chisq.test(blockprogram, conf.level=0.99)$expected
So my question is is it the *conf.level=0.99* where it is wrongly used?

b)I was able to run the one way anova but wasn't able to run on the tukey's
test. I seriously do not know what went wrong... So, this is the original
problem:
The data set chicken.csv contains weights of chickens which are given 1 of 3
different food rations. Perform an ANOVA procedure to determine if the
weights of the chickens are significantly affected by the food rations they
are provided. In case a significant effect exists, perform a Tukey HSD
post-hoc procedure to identify under which ration the chickens will be
heaviest. Use a 5% level of significance.

SO here's the code I used to perform:
chicken <- read.csv("C:/Users/Win/Desktop/chicken.csv")
attach(chicken)
anova(lm(chicken$ration~chicken$weight))
anova.ration <- aov(chicken$ration~chicken$weight)
*posthoc.ration <- TukeyHSD(anova.ration, 'weight', conf.level=0.95) => This
is the part where I wasn't able to run :(*
tapply(ration,weight, mean, na.rm=T)
plot(posthoc.weight)

Thanks!




--
View this message in context: http://r.789695.n4.nabble.com/There-is-an-error-when-I-performed-chisquare-and-postanova-test-tp4712770.html
Sent from the R help mailing list archive at Nabble.com.


From lists at dewey.myzen.co.uk  Fri Sep 25 14:24:12 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 25 Sep 2015 13:24:12 +0100
Subject: [R] There is an error when I performed chisquare and postanova
 test...:(
In-Reply-To: <1443181499770-4712770.post@n4.nabble.com>
References: <1443181499770-4712770.post@n4.nabble.com>
Message-ID: <56053CEC.8060308@dewey.myzen.co.uk>

Dear massmatics

The list has a no homework policy but in this case I feel I can depart 
from that. Have you tried reading the documentation for chisq.test?

On 25/09/2015 12:44, massmatics wrote:
> I am studying for a quiz and while I was solving two problems, an error
> occured.
> a) So the problem is we want to test if the block program makes a difference
> in retention at ? = 1%
> (which is the significance level)
> This is the code I used to perform chisquare test.
> My solution:
> blockprogram <- matrix(c(18,15,5,8,4,10,5,7,18,10), nrow=2, byrow=T)
> colnames(blockprogram) <- c("1 yr","2 yr","3 yr","4 yr","5+ yrs")
> rownames(blockprogram) <- c("Non-Block","Block")	
> chisq.test(blockprogram, conf.level=0.99) ==> This is the part where I can't
> run..
> chisq.test(blockprogram, conf.level=0.99)$observed
> chisq.test(blockprogram, conf.level=0.99)$expected
> So my question is is it the *conf.level=0.99* where it is wrongly used?
>
> b)I was able to run the one way anova but wasn't able to run on the tukey's
> test. I seriously do not know what went wrong... So, this is the original
> problem:
> The data set chicken.csv contains weights of chickens which are given 1 of 3
> different food rations. Perform an ANOVA procedure to determine if the
> weights of the chickens are significantly affected by the food rations they
> are provided. In case a significant effect exists, perform a Tukey HSD
> post-hoc procedure to identify under which ration the chickens will be
> heaviest. Use a 5% level of significance.
>
> SO here's the code I used to perform:
> chicken <- read.csv("C:/Users/Win/Desktop/chicken.csv")
> attach(chicken)
> anova(lm(chicken$ration~chicken$weight))
> anova.ration <- aov(chicken$ration~chicken$weight)
> *posthoc.ration <- TukeyHSD(anova.ration, 'weight', conf.level=0.95) => This
> is the part where I wasn't able to run :(*
> tapply(ration,weight, mean, na.rm=T)
> plot(posthoc.weight)
>
> Thanks!
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/There-is-an-error-when-I-performed-chisquare-and-postanova-test-tp4712770.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From 2hanl2da at naver.com  Fri Sep 25 15:12:37 2015
From: 2hanl2da at naver.com (massmatics)
Date: Fri, 25 Sep 2015 06:12:37 -0700 (PDT)
Subject: [R] There is an error when I performed chisquare and postanova
 test...:(
In-Reply-To: <56053CEC.8060308@dewey.myzen.co.uk>
References: <1443181499770-4712770.post@n4.nabble.com>
	<56053CEC.8060308@dewey.myzen.co.uk>
Message-ID: <1443186757916-4712772.post@n4.nabble.com>

Yes, so far, we only learned how to use chisq.test when performed at 0.05
significance level
But not when it is at 0.01



--
View this message in context: http://r.789695.n4.nabble.com/There-is-an-error-when-I-performed-chisquare-and-postanova-test-tp4712770p4712772.html
Sent from the R help mailing list archive at Nabble.com.


From lists at dewey.myzen.co.uk  Fri Sep 25 15:37:06 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 25 Sep 2015 14:37:06 +0100
Subject: [R] There is an error when I performed chisquare and postanova
 test...:(
In-Reply-To: <1443186757916-4712772.post@n4.nabble.com>
References: <1443181499770-4712770.post@n4.nabble.com>
	<56053CEC.8060308@dewey.myzen.co.uk>
	<1443186757916-4712772.post@n4.nabble.com>
Message-ID: <56054E02.8060304@dewey.myzen.co.uk>

Let me repeat my advice
Read the documentation for chisq.test and see if it has the parameters 
you think it has.

On 25/09/2015 14:12, massmatics wrote:
> Yes, so far, we only learned how to use chisq.test when performed at 0.05
> significance level
> But not when it is at 0.01
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/There-is-an-error-when-I-performed-chisquare-and-postanova-test-tp4712770p4712772.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dcarlson at tamu.edu  Fri Sep 25 15:54:54 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Fri, 25 Sep 2015 13:54:54 +0000
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <99D0D5AA-BD45-484F-8FCB-8EA0B00E27A7@comcast.net>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
	<20150924193642.GA1757@localhost.localdomain>
	<BACC74A6-67C2-4B9F-920E-6554B76F5A75@comcast.net>
	<20150924205452.GB1757@localhost.localdomain>
	<99D0D5AA-BD45-484F-8FCB-8EA0B00E27A7@comcast.net>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C6144@mb02.ads.tamu.edu>

You defined x and y in your original email as:

> x<-rnorm(20)
> y<-rnorm(20)
>
> mm<-as.matrix(cbind(x,y))
>
> dst<-(dist(mm))

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: David Winsemius [mailto:dwinsemius at comcast.net] 
Sent: Thursday, September 24, 2015 6:30 PM
To: Lorenzo Isella
Cc: David L Carlson; r-help at r-project.org
Subject: Re: [R] Sampling the Distance Matrix


On Sep 24, 2015, at 1:54 PM, Lorenzo Isella wrote:

> On Thu, Sep 24, 2015 at 01:30:02PM -0700, David Winsemius wrote:
>> 
>> On Sep 24, 2015, at 12:36 PM, Lorenzo Isella wrote:
>> 
>>> Hi,
>>> And thanks for your reply.
>>> Essentially, your script gets the job done.
>>> For instance, if I run
>>> 
>>> mm <- cbind(5/(1:5), -2*sqrt(1:5))
>>> dst <- dist(mm)
>>> dst2 <- as.matrix(dst)
>>> diag(dst2) <- NA
>>> idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))
>>> 
>>> then it correctly detects the first two rows, where all the values are
>>> larger than 0.9.
>>> In other words, it detects the points that are at least 0.9 units away
>>> from *all* the other points.
>>> My other question (I did not realize this until I got your answer) is
>>> the following: I have the distance matrix of a set of N points.
>>> You gave me an algorithm two find all the points that are at least 0.9
>>> units away from any other points.
>>> However, in some cases, for me it is OK even a weaker condition: find
>>> a subset of k points (with k tunable) whose distance *from each other*
>>> is greater than 0.9 units (even if their distance from some other
>>> points may be smaller than 0.9).
>> 
>> If I understand ..... Make a matrix of unique combinations, then apply by rows to get the qualifying columns that satisfy the distance criterion:
>> 
>> mtxcomb <- combn(1:20, 5)
>> goodcls <- apply(mtxcomb , 2, function(idx) all( dist( cbind( x[idx], y[idx]) ) > 0.9))
>> mtxcomb [ , goodcls]
>> 
>> In my sample it was around 9% of the total 5 item combinations.
>> 
>> snipped a lot of output:
>> .....
>>   [,1440] [,1441]
>> [1,]      12      13
>> [2,]      13      16
>> [3,]      16      17
>> [4,]      19      19
>> [5,]      20      20
>>> dim( mtxcomb)
>> [1]     5 15504
>> 
> 
> Hi,
> Thanks for your reply.
> I think I am getting there, but when I run your commands, I get this
> error message
> 
> Error in cbind(x[idx], y[idx]) : object 'x' not found
> 
> Any idea why? Should I combine those 3 lines with something else?

No idea. I was running the setup that you asked for in your original message which you have now omitted from the mail chain.



> Cheers
> 
> Lorenzo

David Winsemius
Alameda, CA, USA


From jrkrideau at inbox.com  Fri Sep 25 15:56:26 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 25 Sep 2015 05:56:26 -0800
Subject: [R] There is an error when I performed chisquare and postanova
 test...:(
In-Reply-To: <1443181499770-4712770.post@n4.nabble.com>
Message-ID: <56E0CF0CC53.000007ADjrkrideau@inbox.com>

As Michael suggests carefully read the help file (type ?chisq.test to bring up the page) and match all the elements of your command with the elements listed on the help page.

You do not need to use attach(chicken) given the code you are using and using attach(data)  is generally considered bad practice. The two equations below are equivalent and the second is better practice in R.

anova(lm(chicken$ration~chicken$weight)) # your code

anova(lm(ration ~ weight, data = chickens)) # cleaner code.

Also have a look at ?with

I'd suggest having a look at http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-ex and/or http://adv-r.had.co.nz/Reproducibility.html for some ideas on how to form questions and supply sample data to R-help.

You are posting from nabble which does not supply any context for the help list.  If you are going to post here regularly I think everyone would be grateful if you post directly to R-help from a mail program. Very few people here use nabble and it is generally considered something of a curse.

Good luck on the quizz


John Kane
Kingston ON Canada


> -----Original Message-----
> From: 2hanl2da at naver.com
> Sent: Fri, 25 Sep 2015 04:44:59 -0700 (PDT)
> To: r-help at r-project.org
> Subject: [R] There is an error when I performed chisquare and postanova
> test...:(
> 
> I am studying for a quiz and while I was solving two problems, an error
> occured.
> a) So the problem is we want to test if the block program makes a
> difference
> in retention at ? = 1%
> (which is the significance level)
> This is the code I used to perform chisquare test.
> My solution:
> blockprogram <- matrix(c(18,15,5,8,4,10,5,7,18,10), nrow=2, byrow=T)
> colnames(blockprogram) <- c("1 yr","2 yr","3 yr","4 yr","5+ yrs")
> rownames(blockprogram) <- c("Non-Block","Block")
> chisq.test(blockprogram, conf.level=0.99) ==> This is the part where I
> can't
> run..
> chisq.test(blockprogram, conf.level=0.99)$observed
> chisq.test(blockprogram, conf.level=0.99)$expected
> So my question is is it the *conf.level=0.99* where it is wrongly used?
> 
> b)I was able to run the one way anova but wasn't able to run on the
> tukey's
> test. I seriously do not know what went wrong... So, this is the original
> problem:
> The data set chicken.csv contains weights of chickens which are given 1
> of 3
> different food rations. Perform an ANOVA procedure to determine if the
> weights of the chickens are significantly affected by the food rations
> they
> are provided. In case a significant effect exists, perform a Tukey HSD
> post-hoc procedure to identify under which ration the chickens will be
> heaviest. Use a 5% level of significance.
> 
> SO here's the code I used to perform:
> chicken <- read.csv("C:/Users/Win/Desktop/chicken.csv")
> attach(chicken)
> anova(lm(chicken$ration~chicken$weight))
> anova.ration <- aov(chicken$ration~chicken$weight)
> *posthoc.ration <- TukeyHSD(anova.ration, 'weight', conf.level=0.95) =>
> This
> is the part where I wasn't able to run :(*
> tapply(ration,weight, mean, na.rm=T)
> plot(posthoc.weight)
> 
> Thanks!
> 
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/There-is-an-error-when-I-performed-chisquare-and-postanova-test-tp4712770.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Receive Notifications of Incoming Messages
Easily monitor multiple email accounts & access them with a click.
Visit http://www.inbox.com/notifier and check it out!


From ashenkin at ufl.edu  Fri Sep 25 16:20:15 2015
From: ashenkin at ufl.edu (Alexander Shenkin)
Date: Fri, 25 Sep 2015 15:20:15 +0100
Subject: [R] Quantifying widths of polygons
Message-ID: <5605581F.9030404@ufl.edu>

Hello all,

I am working with data on tree crowns, and this data describes points 
(verticies) around the polyhedron of the crown volume (think of the 
crown as a single volume with vertices and faces).  I can calculate 
maximum z distance between any 2 points (maximum depth) and maximum x/y 
distance (maximum width).  These are useful metrics.  I would also like 
to quantify an "average" width of the polygon in 2D space (x/y only), as 
well as a metric that would describe the "eccentricity" of the polygon. 
  But, I'm not sure how to go about doing that.

In general, I've made the polyhedrons and polygons into convex shapes.

I have considered getting a centroid, intersecting lines every 10 
degrees (for example) going through the centroid with the x/y polygon 
owin in spatstat, and then analyzing those line lengths.  But, I'm not 
sure that's the right way to go, and maybe there are already tools out 
there to do this.  Any thoughts anyone might have would be very welcome!

Thanks,
Allie


library(rgl)
library(spatstat)
library(geometry)

x = 
c(1.9,-1.4,1.5,1.8,2.2,0.2,0.6,-0.9,-3.7,1.3,-1.9,-3.4,3.7,2.1,-2.0,-1.9)
y = 
c(-3.1,3.0,1.1,-1.3,1.0,0.0,1.4,1.6,2.3,-3.6,-1.5,-1.3,0.3,-2.1,0.2,-0.3)
z = c(5.5,4.5,4.3,4.8,6.7,5.8,7.4,6.2,3.5,2.9,4.0,3.7,3.2,3.0,3.1,8.4)
depth = max(z) - min(z)
width_max = max(dist(matrix(c(x,y),ncol=2)))

xy_win = owin(poly=list(x=x,y=y))
conv_win = convexhull(xy_win)
# from here, maybe draw lines every 10 degrees through a centroid?
# avg_width = ??
# eccentricity = ??

# 3D plot of crown polyhedron (convex)
ps = data.frame(x=x,y=y,z=z)
crown.surf = t(convhulln(matrix(c(x,y,z),ncol=3)))
open3d()
rgl.triangles(ps[crown.surf,1],ps[crown.surf,2],ps[crown.surf,3],col=heat.colors(nrow(ps)),alpha=.2)
axes3d()


From fisher at plessthan.com  Fri Sep 25 16:25:33 2015
From: fisher at plessthan.com (Dennis Fisher)
Date: Fri, 25 Sep 2015 10:25:33 -0400
Subject: [R] Accessing defunct package
Message-ID: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>

R 3.2.0
OS X

Colleagues,

In the past, I used a package:
	SASxport
to output files to SAS?s XPT format.  This was useful because FDA requests that data be submitted in that format (even though they typically must reconvert to some other format before the data are used).

It appears that the package is no longer available at CRAN:
	Package ?SASxport? was removed from the CRAN repository.
	Formerly available versions can be obtained from the archive.
	Archived on 2015-06-09 as errors were not corrected despite reminders.

I have a previously-functioning version of the package on my computer.  When I attempt to load it with:
	require("SASxport", lib.loc=?/PATH/TOt/R-Packages")
R responds:
	Loading required package: SASxport 
	Failed with error: ?package ?SASxport? was built before R 3.0.0: please re-install it? 

Other than reinstalling an old version of R (< 3.0.0), is there some way that I can use the package?

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From bgunter.4567 at gmail.com  Fri Sep 25 17:02:38 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 25 Sep 2015 08:02:38 -0700
Subject: [R] Quantifying widths of polygons
In-Reply-To: <5605581F.9030404@ufl.edu>
References: <5605581F.9030404@ufl.edu>
Message-ID: <CAGxFJbSRZME=Q1g9jW1qgOXFirJ67d0xOTxWRqy1_u21xQhABA@mail.gmail.com>

This is not the right list for such substantive questions.

I suggest that you check out the "spatial" and perhaps the
"Environmetrics" Task views on CRAN and/or post to a statistical site
like stats.stackexchange.com or the R-sig-ecology mailing list
instead.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Sep 25, 2015 at 7:20 AM, Alexander Shenkin <ashenkin at ufl.edu> wrote:
> Hello all,
>
> I am working with data on tree crowns, and this data describes points
> (verticies) around the polyhedron of the crown volume (think of the crown as
> a single volume with vertices and faces).  I can calculate maximum z
> distance between any 2 points (maximum depth) and maximum x/y distance
> (maximum width).  These are useful metrics.  I would also like to quantify
> an "average" width of the polygon in 2D space (x/y only), as well as a
> metric that would describe the "eccentricity" of the polygon.  But, I'm not
> sure how to go about doing that.
>
> In general, I've made the polyhedrons and polygons into convex shapes.
>
> I have considered getting a centroid, intersecting lines every 10 degrees
> (for example) going through the centroid with the x/y polygon owin in
> spatstat, and then analyzing those line lengths.  But, I'm not sure that's
> the right way to go, and maybe there are already tools out there to do this.
> Any thoughts anyone might have would be very welcome!
>
> Thanks,
> Allie
>
>
> library(rgl)
> library(spatstat)
> library(geometry)
>
> x =
> c(1.9,-1.4,1.5,1.8,2.2,0.2,0.6,-0.9,-3.7,1.3,-1.9,-3.4,3.7,2.1,-2.0,-1.9)
> y =
> c(-3.1,3.0,1.1,-1.3,1.0,0.0,1.4,1.6,2.3,-3.6,-1.5,-1.3,0.3,-2.1,0.2,-0.3)
> z = c(5.5,4.5,4.3,4.8,6.7,5.8,7.4,6.2,3.5,2.9,4.0,3.7,3.2,3.0,3.1,8.4)
> depth = max(z) - min(z)
> width_max = max(dist(matrix(c(x,y),ncol=2)))
>
> xy_win = owin(poly=list(x=x,y=y))
> conv_win = convexhull(xy_win)
> # from here, maybe draw lines every 10 degrees through a centroid?
> # avg_width = ??
> # eccentricity = ??
>
> # 3D plot of crown polyhedron (convex)
> ps = data.frame(x=x,y=y,z=z)
> crown.surf = t(convhulln(matrix(c(x,y,z),ncol=3)))
> open3d()
> rgl.triangles(ps[crown.surf,1],ps[crown.surf,2],ps[crown.surf,3],col=heat.colors(nrow(ps)),alpha=.2)
> axes3d()
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Fri Sep 25 17:04:07 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 25 Sep 2015 10:04:07 -0500
Subject: [R] Accessing defunct package
In-Reply-To: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
References: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
Message-ID: <A9546FCC-EC48-4BBE-BE43-3797F65AC88D@me.com>


> On Sep 25, 2015, at 9:25 AM, Dennis Fisher <fisher at plessthan.com> wrote:
> 
> R 3.2.0
> OS X
> 
> Colleagues,
> 
> In the past, I used a package:
> 	SASxport
> to output files to SAS?s XPT format.  This was useful because FDA requests that data be submitted in that format (even though they typically must reconvert to some other format before the data are used).
> 
> It appears that the package is no longer available at CRAN:
> 	Package ?SASxport? was removed from the CRAN repository.
> 	Formerly available versions can be obtained from the archive.
> 	Archived on 2015-06-09 as errors were not corrected despite reminders.
> 
> I have a previously-functioning version of the package on my computer.  When I attempt to load it with:
> 	require("SASxport", lib.loc=?/PATH/TOt/R-Packages")
> R responds:
> 	Loading required package: SASxport 
> 	Failed with error: ?package ?SASxport? was built before R 3.0.0: please re-install it? 
> 
> Other than reinstalling an old version of R (< 3.0.0), is there some way that I can use the package?
> 
> Dennis
> 


Dennis, 

A search would seem to suggest that Greg is no longer supporting the package.

Despite the XPORT format being openly documented (as opposed to the SAS7BDAT format), I am not rapidly locating other free resources to *write* xport files, while there are a number of them to *read* xport files.

Thus, one commercial (not free) conversion application that is independent of SAS is StatTransfer, which may be worth your investment if you are doing this quite a bit:

  http://www.stattransfer.com

Regards,

Marc Schwartz


From jdnewmil at dcn.davis.CA.us  Fri Sep 25 17:08:18 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 25 Sep 2015 08:08:18 -0700
Subject: [R] Accessing defunct package
In-Reply-To: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
References: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
Message-ID: <0627FB99-9600-4977-890B-E5767D667B52@dcn.davis.CA.us>

Obtain the source package and fix it? Most errors are relatively minor adjustments that just require reading the updated "Writing R Extensions" document to figure out. You might be unlucky, but I think the odds are in your favor.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 25, 2015 7:25:33 AM PDT, Dennis Fisher <fisher at plessthan.com> wrote:
>R 3.2.0
>OS X
>
>Colleagues,
>
>In the past, I used a package:
>	SASxport
>to output files to SAS?s XPT format.  This was useful because FDA
>requests that data be submitted in that format (even though they
>typically must reconvert to some other format before the data are
>used).
>
>It appears that the package is no longer available at CRAN:
>	Package ?SASxport? was removed from the CRAN repository.
>	Formerly available versions can be obtained from the archive.
>	Archived on 2015-06-09 as errors were not corrected despite reminders.
>
>I have a previously-functioning version of the package on my computer. 
>When I attempt to load it with:
>	require("SASxport", lib.loc=?/PATH/TOt/R-Packages")
>R responds:
>	Loading required package: SASxport 
>	Failed with error: ?package ?SASxport? was built before R 3.0.0:
>please re-install it? 
>
>Other than reinstalling an old version of R (< 3.0.0), is there some
>way that I can use the package?
>
>Dennis
>
>Dennis Fisher MD
>P < (The "P Less Than" Company)
>Phone: 1-866-PLessThan (1-866-753-7784)
>Fax: 1-866-PLessThan (1-866-753-7784)
>www.PLessThan.com
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From giorgio.garziano at ericsson.com  Fri Sep 25 17:27:50 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Fri, 25 Sep 2015 15:27:50 +0000
Subject: [R] Quantmod several indicators
Message-ID: <248E6FA047A8C746BA491485764190F522093D9B@ESESSMB207.ericsson.se>

Hi,

Both following code examples plot Bollinger bands over the ticker main plot
and William's Percent below the main plot.

1. chartSeries(YHOO,theme="white",TA = c(addBBands(200,2), addWPR(n=300)))

2. chartSeries(IB,theme="white",TA = c(addBBands(200,2)))
   addWPR(n=300)

In general, if you like to plot indipendently a trading indicator computed by quantmod,
you can do:

wpr <- addWPR(n=300)
plot(wpr at TA.values, type='l')   -- or any other R plot library you like

as in @TA.values are stored trading indicator values for any quantmod indicator.


Giorgio Garziano



	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Fri Sep 25 18:31:15 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Fri, 25 Sep 2015 16:31:15 +0000
Subject: [R] Randomness tests
Message-ID: <248E6FA047A8C746BA491485764190F522093DE5@ESESSMB207.ericsson.se>

I am interested in any kind of deviation from randomness.

I would like to know if the fact that a time series can take values only from the set {-1, 1} restricts
the type of randomness tests that can be done.

--

Giorgio Garziano



	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Fri Sep 25 18:49:10 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 25 Sep 2015 09:49:10 -0700
Subject: [R] Randomness tests
In-Reply-To: <248E6FA047A8C746BA491485764190F522093DE5@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F522093DE5@ESESSMB207.ericsson.se>
Message-ID: <7877CF90-B8A9-4558-B716-B3E28EB5A187@dcn.davis.CA.us>

You are way off topic for this list. Perhaps stats.stackexchange.com would be a better place to ask such a question.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 25, 2015 9:31:15 AM PDT, Giorgio Garziano <giorgio.garziano at ericsson.com> wrote:
>I am interested in any kind of deviation from randomness.
>
>I would like to know if the fact that a time series can take values
>only from the set {-1, 1} restricts
>the type of randomness tests that can be done.
>
>--
>
>Giorgio Garziano
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From giorgio.garziano at ericsson.com  Fri Sep 25 18:54:00 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Fri, 25 Sep 2015 16:54:00 +0000
Subject: [R] Randomness tests
In-Reply-To: <7877CF90-B8A9-4558-B716-B3E28EB5A187@dcn.davis.CA.us>
References: <248E6FA047A8C746BA491485764190F522093DE5@ESESSMB207.ericsson.se>
	<7877CF90-B8A9-4558-B716-B3E28EB5A187@dcn.davis.CA.us>
Message-ID: <248E6FA047A8C746BA491485764190F522093EE6@ESESSMB207.ericsson.se>

Good suggestion, thanks.

-----Original Message-----
From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.CA.us] 
Sent: venerd? 25 settembre 2015 18:49
To: Giorgio Garziano; r-help at r-project.org
Subject: Re: [R] Randomness tests

You are way off topic for this list. Perhaps stats.stackexchange.com would be a better place to ask such a question.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
---------------------------------------------------------------------------
Sent from my phone. Please excuse my brevity.

On September 25, 2015 9:31:15 AM PDT, Giorgio Garziano <giorgio.garziano at ericsson.com> wrote:
>I am interested in any kind of deviation from randomness.
>
>I would like to know if the fact that a time series can take values 
>only from the set {-1, 1} restricts the type of randomness tests that 
>can be done.
>
>--
>
>Giorgio Garziano
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Sep 25 19:00:45 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 25 Sep 2015 10:00:45 -0700
Subject: [R] Accessing defunct package
In-Reply-To: <0627FB99-9600-4977-890B-E5767D667B52@dcn.davis.CA.us>
References: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
	<0627FB99-9600-4977-890B-E5767D667B52@dcn.davis.CA.us>
Message-ID: <ADF677E9-9BD2-40DF-BF84-007B28478643@comcast.net>

I think it might be even simpler at least for now. The error checking done by CRAN can be more rigorous than that done when an installation is done locally. I don't see a report in the current package checks listing of what error was identified, but experimentation is always an option.  When I download the last archived version and install from source I get no error on R 3.2.2 (Mac-SL fork):

install.packages('~/Downloads/SASxport_1.5.0.tar.gz', repos = NULL , type="source")

Best;
David.



On Sep 25, 2015, at 8:08 AM, Jeff Newmiller wrote:

> Obtain the source package and fix it? Most errors are relatively minor adjustments that just require reading the updated "Writing R Extensions" document to figure out. You might be unlucky, but I think the odds are in your favor.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                      Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> --------------------------------------------------------------------------- 
> Sent from my phone. Please excuse my brevity.
> 
> On September 25, 2015 7:25:33 AM PDT, Dennis Fisher <fisher at plessthan.com> wrote:
>> R 3.2.0
>> OS X
>> 
>> Colleagues,
>> 
>> In the past, I used a package:
>> 	SASxport
>> to output files to SAS?s XPT format.  This was useful because FDA
>> requests that data be submitted in that format (even though they
>> typically must reconvert to some other format before the data are
>> used).
>> 
>> It appears that the package is no longer available at CRAN:
>> 	Package ?SASxport? was removed from the CRAN repository.
>> 	Formerly available versions can be obtained from the archive.
>> 	Archived on 2015-06-09 as errors were not corrected despite reminders.
>> 
>> I have a previously-functioning version of the package on my computer. 
>> When I attempt to load it with:
>> 	require("SASxport", lib.loc=?/PATH/TOt/R-Packages")
>> R responds:
>> 	Loading required package: SASxport 
>> 	Failed with error: ?package ?SASxport? was built before R 3.0.0:
>> please re-install it? 
>> 
>> Other than reinstalling an old version of R (< 3.0.0), is there some
>> way that I can use the package?
>> 
>> Dennis
>> 
>> Dennis Fisher MD
>> P < (The "P Less Than" Company)
>> Phone: 1-866-PLessThan (1-866-753-7784)
>> Fax: 1-866-PLessThan (1-866-753-7784)
>> www.PLessThan.com
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Fri Sep 25 19:17:59 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 25 Sep 2015 10:17:59 -0700
Subject: [R] Quantifying widths of polygons
In-Reply-To: <5605581F.9030404@ufl.edu>
References: <5605581F.9030404@ufl.edu>
Message-ID: <63862E98-D669-4468-9520-85E83EAD1BD4@comcast.net>


On Sep 25, 2015, at 7:20 AM, Alexander Shenkin wrote:

> Hello all,
> 
> I am working with data on tree crowns, and this data describes points (verticies) around the polyhedron of the crown volume (think of the crown as a single volume with vertices and faces).  I can calculate maximum z distance between any 2 points (maximum depth) and maximum x/y distance (maximum width).  These are useful metrics.  I would also like to quantify an "average" width of the polygon in 2D space (x/y only), as well as a metric that would describe the "eccentricity" of the polygon.  But, I'm not sure how to go about doing that.
> 
> In general, I've made the polyhedrons and polygons into convex shapes.
> 
> I have considered getting a centroid, intersecting lines every 10 degrees (for example) going through the centroid with the x/y polygon owin in spatstat, and then analyzing those line lengths.  But, I'm not sure that's the right way to go, and maybe there are already tools out there to do this.  Any thoughts anyone might have would be very welcome!

Frank Harrell does something similar in his rms/Hmisc pkg-duo. I cannot remember which of those tow packages contains the `perimeter` function, but take a look at its approach. It is multi-dimensional. I think either that function or the associated plotting functions (mostly based on the lattice paradigm) must be projecting the boundaries it defines onto 2D regions of the highest densities of points.

Best;
David.

> 
> Thanks,
> Allie
> 
> 
> library(rgl)
> library(spatstat)
> library(geometry)
> 
> x = c(1.9,-1.4,1.5,1.8,2.2,0.2,0.6,-0.9,-3.7,1.3,-1.9,-3.4,3.7,2.1,-2.0,-1.9)
> y = c(-3.1,3.0,1.1,-1.3,1.0,0.0,1.4,1.6,2.3,-3.6,-1.5,-1.3,0.3,-2.1,0.2,-0.3)
> z = c(5.5,4.5,4.3,4.8,6.7,5.8,7.4,6.2,3.5,2.9,4.0,3.7,3.2,3.0,3.1,8.4)
> depth = max(z) - min(z)
> width_max = max(dist(matrix(c(x,y),ncol=2)))
> 
> xy_win = owin(poly=list(x=x,y=y))
> conv_win = convexhull(xy_win)
> # from here, maybe draw lines every 10 degrees through a centroid?
> # avg_width = ??
> # eccentricity = ??
> 
> # 3D plot of crown polyhedron (convex)
> ps = data.frame(x=x,y=y,z=z)
> crown.surf = t(convhulln(matrix(c(x,y,z),ncol=3)))
> open3d()
> rgl.triangles(ps[crown.surf,1],ps[crown.surf,2],ps[crown.surf,3],col=heat.colors(nrow(ps)),alpha=.2)
> axes3d()
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From euthymios.k.kasvikis at gmail.com  Fri Sep 25 13:44:00 2015
From: euthymios.k.kasvikis at gmail.com (euthymios kasvikis)
Date: Fri, 25 Sep 2015 16:44:00 +0500
Subject: [R] ARCHETYPAL ANALYSIS IN R
Message-ID: <CABNxSU0s9s-Bc_qOz7Ebg3CpNr+4d=2J6AZKr4=41_BmFt8ctw@mail.gmail.com>

To whom it may concern
I am a postgraduate student in Aristotle University of Thessaloniki and a
recent user of R also writing my thesis statement on a Sport Analytics tool
based on Pr.Eugster's work on Archetypal Analysis.
The problem is that after importing the data-set i want to work with, i
cannot "run" Pr Eugster's script which he wrote for his own analysis on NBA
Statistics.I am sending you not only the code but also the data-set i use.
How could i adjust this code on my data-set?
Thank you in advance for your patience and cooperation.

@demo Archetypal basketball players based on player statistics;
#'   analysis from the manuscript "Archetypal athletes" by Eugster
#'   (2011)

library("SportsAnalytics")
library("archetypes")
library("RColorBrewer")

col_pal <- brewer.pal(7, "Set1")
col_black <- rgb(0, 0, 0, 0.2)



### Data: ############################################################

data("NBAPlayerStatistics0910")

dat <- subset(NBAPlayerStatistics0910,
              select = -c(Ejections, FlagrantFouls))

mat <- as.matrix(subset(dat, select = -c(League, Name, Team, Position)))


pcplot(mat, col = col_black, las = 2)



### Archetypes: ######################################################

set.seed(4321)
as <- stepArchetypes(mat, k = 1:10)

rss(as)
screeplot(as)


a4 <- bestModel(as[[4]])


### Archetypal basketball players:

parameters(a4)
barplot(a4, mat, percentiles = TRUE)



### Player interpretation: ###########################################

players <- function(which) {
  players <- list()
  players$which <- which
  players$mat <- mat[which, ]
  players$coef <- coef(a4, "alphas")[which, ]
  players$dat <- dat[which, ]

  players
}


### Archetypal players:

which <- apply(coef(a4, "alphas"), 2, which.max)
atypes <- players(which)

cbind(subset(atypes$dat, select = c(Name, Team, Position)),
      atypes$coef)



### Good players:

good_players <- function(atype, threshold) {
  which <- which(coef(a4, "alphas")[, atype] > threshold)

  good_coef <- coef(a4, "alphas")[which, ]
  good_dat <- subset(dat[which, ], select = c(Name, Team, Position))
  good_dat <- cbind(good_dat, good_coef)
  good_dat <- good_dat[order(-good_coef[, atype]), ]

  good_dat
}


good_threshold <- 0.95

players <- lapply(2:4, good_players, good_threshold)
players
-------------- next part --------------
Year	Name	Team	GP	Min	Pts	FG	FGA	TwoPt	TwoPtA	ThreePt	ThreePtA	FTM	FTA	Off	Def	Asts	Stls	Blks	TOs	PFs				
2001	Gregor Fucka	Fortitudo Bologna	17	33,4	335	127	226	115	195	12	31	69	110	42	101	12	39	14	38	37				
2001	Manu Ginobili	Virtus Bologna	22	29,7	334	110	247	80	144	30	103	84	108	17	74	44	64	7	56	70				
2001	Elmer Bennett	Vitoria	22	35,6	314	100	254	78	170	22	84	92	124	12	37	120	43	1	56	58				
2001	Alphonso Ford	Peristeri	12	34,5	312	116	214	101	172	15	42	65	75	18	31	32	13	3	34	33				
2001	Sani Becirovic	Olimpija Ljubljana	15	32,4	310	94	221	66	151	28	70	94	105	13	45	53	26	2	50	47				
2001	Victor Alexander	Vitoria	22	30	297	121	217	120	213	1	4	54	73	41	113	16	20	4	47	74				
2001	Panagiotis Liadelis	PAOK	13	33,4	295	93	185	86	162	7	23	102	121	2	33	19	7	0	24	28				
2001	Rashard Griffith	Virtus Bologna	21	29	292	112	180	112	179	0	1	68	111	55	128	13	36	27	43	76				
2001	Ibrahim Kutluay	AEK Athens	18	29,8	282	83	203	44	91	39	112	77	106	1	43	22	14	0	41	54				
2001	Dejan Tomasevic	Buducnost	12	34,3	275	105	168	105	164	0	4	65	113	60	78	37	24	12	36	34				
2001	Antoine Rigaudeau	Virtus Bologna	20	26,9	272	82	151	43	81	39	70	69	79	9	28	30	26	2	32	59				
2001	Saulius Stombergas	Vitoria	22	27,3	265	89	171	45	85	44	86	43	52	14	38	15	20	4	27	79				
2001	Louis Bullock	Verona	11	31,6	242	78	158	53	98	25	60	61	74	5	23	35	21	2	14	24				
2001	Fabricio Oberto	Vitoria	22	28	240	105	160	105	160	0	0	30	66	68	94	24	44	13	28	74				
2001	Marko Jaric	Virtus Bologna	22	28,7	228	79	180	62	125	17	55	53	76	17	50	50	43	5	58	62				
2001	Petar Naumoski	Treviso	14	33,7	222	66	143	34	66	32	77	58	74	12	22	35	27	0	22	33				
2001	Jemeil Rich	Lugano	10	33,5	215	82	176	65	128	17	48	34	49	13	16	27	37	0	35	30				
2001	Andrew Betts	AEK Athens	18	26,7	213	83	155	83	155	0	0	47	69	51	71	18	10	10	35	65				
2001	Dino Radja	Olympiakos	14	28,8	198	73	136	73	136	0	0	52	90	43	94	14	23	20	38	40				
2001	Luis Scola	Vitoria	22	18,3	192	75	137	75	136	0	1	42	78	36	60	15	21	3	46	79				
2001	Alfonso Reyes	Estudiantes	12	24,5	190	68	115	68	115	0	0	54	66	28	48	5	8	1	17	40				
2001	Josip Sesar	Cibona Zagreb	12	32,6	188	62	190	38	92	24	98	40	63	9	15	22	18	7	30	46				
2001	Marcelo Nicola	Treviso	14	24,7	182	55	128	27	59	28	69	44	49	16	55	33	13	11	26	36				
2001	Henry Williams	Verona	12	30,2	182	60	134	34	66	26	68	36	38	5	17	25	13	0	18	28				
2001	Emilio Kovacic	Olimpija Ljubljana	14	27,2	181	70	132	70	131	0	1	41	59	40	75	11	26	8	22	44				
2001	Marcus Brown	Treviso	9	35	179	59	124	38	71	21	53	40	46	7	21	27	19	0	18	28				
2001	Gianluca Basile	Fortitudo Bologna	18	27,3	179	58	139	30	49	28	90	35	48	10	45	39	27	3	35	46				
2001	Vrbica Stefanov	AEK Athens	18	31,1	179	64	129	45	75	19	54	32	38	16	30	41	26	0	13	50				
2001	Stephane Risacher	Olympiakos	14	22,7	179	60	93	42	57	18	36	41	53	13	26	12	17	4	17	32				
2001	Donatas Slanina	Zalgiris	12	28,3	174	61	108	29	50	32	58	20	23	8	32	12	8	4	14	35				
2001	Laurent Foirest	Vitoria	21	23,3	173	52	125	25	55	27	70	42	57	5	52	19	15	0	36	60				
2001	Kris Hill	Ovarense	10	34,4	172	73	119	72	111	1	8	25	38	34	42	15	19	0	35	29				
2001	Jorge Garbajosa	Treviso	14	24,4	169	58	96	50	79	8	17	45	64	17	53	13	20	12	16	50				
2001	Patrick Baldwin	Zadar	9	32	165	53	99	44	80	9	19	50	56	15	17	21	26	3	20	28				
2001	Erik Meek	Real Madrid	13	25,9	161	59	89	59	89	0	0	43	70	22	57	3	10	4	26	50				
2001	Mike Batiste	Charleroi	10	28,6	161	66	132	63	122	3	10	26	34	31	61	4	9	3	27	32				
2001	Kendrick Warren	London Towers	9	31,7	161	65	132	56	106	9	26	22	41	20	49	7	9	0	29	35				
2001	David Andersen	Virtus Bologna	22	18,1	160	55	100	53	92	2	8	48	58	29	53	10	12	10	34	46				
2001	Marty Conlon	Verona	12	26,8	159	58	107	56	100	2	7	41	49	28	44	19	7	1	22	35				
2001	Nikolaos Oikonomou	Olympiakos	12	27,5	158	47	89	27	47	20	42	44	55	7	30	21	10	0	22	32				
2001	Dusan Vukcevic	Olympiakos	14	23,8	157	54	113	37	55	17	58	32	39	1	14	14	12	5	22	25				
2001	Kebu Stewart	Hapoel Jerusalem	9	31,4	157	46	91	46	91	0	0	65	87	25	49	2	10	2	18	25				
2001	Matjaz Smodis	Virtus Bologna	21	15,2	157	56	111	41	65	15	46	30	39	14	32	8	13	5	19	53				
2001	Carlton Myers	Fortitudo Bologna	10	28	156	45	113	25	57	20	56	46	62	4	17	19	18	0	19	25				
2001	Alessandro Abbio	Virtus Bologna	19	24,7	155	47	110	23	53	24	57	37	52	10	43	25	22	0	27	56				
2001	Arthur Lee	Cibona Zagreb	12	31,4	155	54	106	47	85	7	21	40	44	4	19	20	24	0	28	30				
2001	Alberto Herreros	Real Madrid	14	23,8	154	49	113	28	55	21	58	35	46	9	29	17	16	1	13	25				
2001	Tyron McCoy	Frankfurt	10	35,4	153	49	122	36	75	13	47	42	53	6	35	37	18	1	33	33				
2001	Juan Carlos Navarro	Barcelona	12	27,8	152	59	127	40	78	19	49	15	22	7	27	32	13	0	26	26				
2001	Dimitrios Papanikolaou	Olympiakos	13	23,9	152	54	98	42	72	12	26	32	56	17	34	15	19	5	26	37				
2001	Lee Matthews	Lugano	10	31,2	151	57	107	56	99	1	8	36	53	29	34	3	20	7	17	26				
2001	Martin Muursepp	AEK Athens	17	21,5	147	55	130	43	79	12	51	25	51	34	49	12	15	4	26	63				
2001	Dimos Dikoudis	AEK Athens	17	22,6	147	54	121	52	113	2	8	37	60	33	59	8	12	5	30	60				
2001	Andrius Jurkunas	Zalgiris	12	26,3	145	49	103	25	45	24	58	23	35	7	37	13	15	11	21	45				
2001	Grygorii Khizhniak	Zalgiris	12	29,8	145	41	76	41	73	0	3	63	75	20	66	10	8	38	25	49				
2001	Steve Woodberry	Zalgiris	12	33,8	143	54	125	40	93	14	32	21	28	19	39	34	19	4	27	19				
2001	Steve Bucknall	London Towers	9	35,8	142	42	105	26	62	16	43	42	52	9	16	19	11	1	18	25				
2001	Igor Rakocevic	Buducnost	11	29,8	142	50	120	40	75	10	45	32	49	9	21	20	15	0	16	30				
2001	Darko Krunic	Zadar	10	25,4	141	46	90	33	59	13	31	36	38	2	12	26	7	0	15	20				
2001	Giorgos Sigalas	PAOK	13	28,5	139	38	105	21	58	17	47	46	59	6	38	32	19	3	28	55				
2001	Arturas Karnisovas	Barcelona	10	26,6	139	39	87	27	54	12	33	49	59	18	32	13	8	2	15	24				
2001	Tunji Awojobi	Cibona Zagreb	12	26,3	138	56	112	54	104	2	8	24	39	15	50	18	16	2	17	25				
2001	Byron Dinkins	Peristeri	12	29,6	135	39	103	23	45	16	58	41	49	2	42	43	21	0	27	25				
2001	Corey Albano	Verona	12	24,5	134	51	92	47	75	4	17	28	46	18	45	15	13	3	25	44				
2001	Milan Tomic	Olympiakos	14	30,1	134	42	101	20	41	22	60	28	41	0	32	35	13	0	21	46				
2001	Mindaugas Timinskas	Vitoria	22	22,3	134	45	106	35	63	10	43	34	45	14	52	26	22	4	33	58				
2001	Eric Struelens	Real Madrid	14	23,4	133	50	85	50	84	0	1	33	51	28	60	17	21	19	29	41				
2001	Kestutis Marciulonis	Zalgiris	12	26,8	133	37	92	20	44	17	48	42	45	2	21	30	13	0	32	35				
2001	Nikos Hatzis	AEK Athens	18	20,7	131	37	101	21	47	16	54	41	51	0	31	20	10	0	18	39				
2001	Marko Milic	Real Madrid	14	22,6	131	52	103	49	94	3	9	24	43	29	36	26	26	0	29	32				
2001	Milenko Topic	Buducnost	9	30,9	130	50	84	36	52	14	32	16	17	12	50	26	11	4	21	23				
2001	John Jerome	Charleroi	10	23,2	130	45	86	40	69	5	17	35	50	34	20	3	2	1	22	33				
2001	Adrian Autry	Fortitudo Bologna	18	21,3	130	44	88	31	56	13	32	29	48	9	32	21	17	1	39	46				
2001	Ronald Ellis	Charleroi	10	28,6	128	52	120	37	80	15	40	9	14	34	62	15	15	2	27	37				
2001	Rico Hill	Estudiantes	12	24,1	127	45	111	32	73	13	38	24	27	12	37	9	13	1	27	28				
2001	Sarunas Jasikevicius	Barcelona	9	28,2	126	37	93	21	44	16	49	36	40	4	17	50	9	1	18	31				
2001	Giacomo Galanda	Fortitudo Bologna	18	19,8	126	46	114	38	86	8	28	26	32	27	35	12	21	5	25	53				
2001	Aleksander Djordjevic	Real Madrid	10	21,4	124	37	68	14	24	23	44	27	30	2	15	19	5	2	20	19				
2001	Primoz Brezec	Olimpija Ljubljana	14	21,1	124	50	70	50	70	0	0	24	48	31	39	5	13	14	15	45				
2001	Kostas Tsartsaris	Peristeri	11	30,9	122	41	81	30	56	11	25	29	37	17	51	21	6	3	16	41				
2001	Michail Kakiouzis	AEK Athens	14	21,1	121	42	94	32	65	10	29	27	39	21	44	7	7	1	17	29				
2001	Evangelos Koronios	PAOK	11	26,3	120	33	84	15	39	18	45	36	38	8	14	32	11	1	21	33				
2001	Tony Dorsey	Hapoel Jerusalem	7	33,4	118	48	95	45	82	3	13	19	30	15	20	12	8	1	18	15				
2001	Thomas Adams	Ovarense	10	31,1	118	40	89	27	59	13	30	25	30	16	31	16	10	1	32	28				
2001	Ivica Maric	Zadar	10	31,9	117	35	83	14	31	21	52	26	31	7	22	59	37	0	30	40				
2001	Riccardo Pittis	Treviso	13	33,7	117	42	85	41	82	1	3	32	65	14	47	49	44	3	35	38				
2001	Alessandro Frosini	Virtus Bologna	16	22	116	51	89	51	89	0	0	14	27	29	50	9	18	8	26	48				
2001	Gerrit Terdenge	Frankfurt	10	27,4	115	43	97	40	83	3	14	26	30	17	28	14	4	3	17	37				
2001	Joffre Lleal	Ovarense	10	29	115	36	94	21	54	15	40	28	34	6	17	21	11	2	24	28				
2001	Davor Pejcinovic	Zadar	10	30,6	115	43	95	43	94	0	1	29	52	27	52	18	16	25	19	39				
2001	Casey Schmidt	Verona	12	29,8	114	43	90	29	51	14	39	14	15	6	43	16	9	1	13	31				
2001	Eric Taylor	London Towers	10	28	113	42	100	26	59	16	41	13	15	8	31	14	16	2	19	39				
2001	Giannis Giannoulis	PAOK	8	31,6	113	41	59	41	59	0	0	31	54	17	52	14	6	5	22	25				
2001	Alexey Savrasenko	Peristeri	12	26,6	113	45	69	45	69	0	0	23	42	38	40	7	8	10	27	33				
2001	Jurica Ruzic	Zadar	10	28,6	112	42	96	23	42	19	54	9	19	9	32	2	11	0	7	30				
2001	Mindaugas Zukauskas	Olimpija Ljubljana	15	20,2	112	37	76	13	28	24	48	14	21	13	20	14	12	1	13	41				
2001	Anthony Bowie	Fortitudo Bologna	10	27	111	35	70	20	37	15	33	26	30	8	28	10	8	2	18	20				
2001	Pau Gasol	Barcelona	6	25,7	111	38	57	34	49	4	8	31	42	10	26	4	5	4	10	12				
2001	Andrea Meneghin	Fortitudo Bologna	15	24,3	108	37	100	13	35	24	65	10	14	10	26	20	16	3	20	47				
2001	James Blackwell	Hapoel Jerusalem	8	33,9	108	35	65	25	45	10	20	28	33	4	14	33	15	1	21	28				
2001	Carlos Jimenez	Estudiantes	12	25,6	107	31	66	19	39	12	27	33	40	11	62	15	19	5	34	38				
2001	David Rivers	Olympiakos	11	26,3	106	35	96	19	50	16	46	20	27	8	17	24	18	0	22	36				
2001	Jorge Racca	PAOK	13	17,1	105	39	68	23	41	16	27	11	14	6	10	5	9	3	7	19				
2001	Nenad Markovic	Estudiantes	10	23,7	104	33	83	21	45	12	38	26	32	2	17	16	7	1	10	15				
2001	Shaun Vandiver	Estudiantes	11	27,1	104	45	101	41	82	4	19	10	13	24	34	9	8	4	20	32				
2001	Nikola Prkacin	Cibona Zagreb	11	20,9	100	39	79	39	78	0	1	22	32	14	33	10	21	1	24	43				
2001	Raul Lopez	Real Madrid	12	22,9	98	32	71	16	36	16	35	18	21	4	20	39	18	0	24	30				
2001	Petar Arsic	Olimpija Ljubljana	14	16	97	40	90	34	57	6	33	11	17	10	34	8	9	4	12	34				
2001	Efthimios Rentzias	Barcelona	11	19,2	97	40	79	35	62	5	17	12	17	20	33	5	5	12	18	16				
2001	Dusan Stevic	Lugano	10	22,4	97	38	84	28	53	10	31	11	21	18	29	0	18	1	19	38				
2001	Derrick Taylor	Frankfurt	7	31,6	96	34	69	25	48	9	21	19	22	4	14	21	11	0	12	21				
2001	Felipe Reyes	Estudiantes	12	15,8	95	29	53	29	53	0	0	37	58	20	29	7	9	3	18	38				
2001	Harold Mrazek	Lugano	10	30,2	94	27	80	8	30	19	50	21	26	4	16	14	15	0	29	33				
2001	Alberto Angulo	Real Madrid	14	18,7	94	42	83	39	74	3	9	7	12	3	27	10	20	1	16	18				
2001	George Zidek	Real Madrid	14	15,6	93	31	73	13	30	18	43	13	19	7	28	5	3	1	10	41				
2001	Beno Udrih	Olimpija Ljubljana	13	24,6	93	36	78	31	58	5	20	16	27	6	23	28	15	1	23	37				
2001	Wouter DeWilde	Charleroi	10	24,5	92	32	68	10	25	22	43	6	13	4	14	2	3	2	15	36				
2001	Francisco Elson	Barcelona	12	22,6	91	37	60	37	60	0	0	17	37	30	40	7	18	8	19	50				
2001	Andrea Camata	Verona	11	20,8	89	41	60	41	60	0	0	7	11	24	53	6	15	3	11	44				
2001	Lucio Angulo	Real Madrid	13	17,5	89	25	53	21	44	4	9	35	43	7	20	10	17	3	21	37				
2001	Roberto Duenas	Barcelona	12	18,8	89	43	75	43	74	0	1	3	16	31	31	6	9	9	20	29				
2001	Lenny Brown	Charleroi	7	27,9	87	31	60	23	35	8	25	17	24	3	10	13	8	0	21	20				
2001	Mauro Sartori	Verona	12	15,9	86	29	66	12	20	17	46	11	14	6	20	6	4	0	7	31				
2001	Fabio Ribeiro	Peristeri	12	20,8	85	33	64	30	57	3	7	16	17	17	48	11	8	2	11	26				
2001	Tim Kennedy	Ovarense	10	28,7	85	33	73	22	40	11	33	8	11	5	17	10	6	1	17	19				
2001	Dejan Jovanovski	Lugano	8	25,8	85	28	69	13	28	15	41	14	18	5	16	7	6	1	11	29				
2001	Shalom Turgeman	Hapoel Jerusalem	10	24,8	84	31	71	17	43	14	28	8	9	13	18	18	15	1	27	22				
2001	Jurica Golemac	Olimpija Ljubljana	12	18	82	28	57	21	39	7	18	19	29	17	26	11	4	2	22	42				
2001	Randy Duck	London Towers	5	37,2	82	27	52	18	37	9	15	19	22	1	9	19	3	1	10	19				
2001	Alan Tomidy	Treviso	14	15	81	28	57	27	53	1	4	24	36	27	30	5	5	5	17	28				
2001	Dejan Radonjic	Buducnost	11	21,4	80	23	56	7	11	16	45	18	22	4	14	8	6	0	7	29				
2001	Pedro Robles	Estudiantes	10	15,3	80	30	59	17	30	13	29	7	10	1	4	8	5	0	10	11				
2001	Mate Skelin	Cibona Zagreb	11	22,6	79	29	42	29	42	0	0	21	44	32	50	4	13	10	21	32				
2001	Jiri Welsch	Olimpija Ljubljana	15	16,9	77	29	72	24	52	5	20	14	15	5	12	8	10	0	12	34				
2001	Ignacio Azofra	Estudiantes	12	24,2	76	25	51	7	14	18	37	8	10	9	22	32	17	0	24	32				
2001	Matej Mamic	Cibona Zagreb	12	23,3	75	24	59	17	34	7	25	20	28	30	33	3	9	2	9	33				
2001	Valeri Daineko	PAOK	8	26,1	74	29	65	26	52	3	13	13	16	9	28	1	9	4	9	27				
2001	Tom Wideman	London Towers	10	21,2	72	25	56	25	55	0	1	22	32	26	39	9	12	2	11	29				
2001	Vladimir Kuzmanovic	Buducnost	12	23,8	72	26	69	18	32	8	37	12	15	16	15	9	10	2	14	24				
2001	Richard Mandeville	Zadar	10	18,1	71	28	50	28	48	0	2	15	20	7	26	3	4	0	12	31				
2001	Claudio Coldebella	PAOK	13	26,2	71	19	50	9	23	10	27	23	26	1	39	32	14	2	12	46				
2001	John White	Zalgiris	6	27,8	71	21	62	11	29	10	33	19	21	7	13	16	3	0	11	21				
2001	Mihalis Pelekanos	Peristeri	12	22,4	69	23	53	12	20	11	33	12	15	5	24	4	16	6	15	37				
2001	Eurelijus Zukauskas	Fortitudo Bologna	11	13,1	68	21	52	20	51	1	1	25	44	18	21	2	17	5	12	24				
2001	Sandro Nicevic	Cibona Zagreb	11	16,5	67	25	38	25	38	0	0	17	24	13	24	2	5	3	12	23				
2001	Joseph Blair	PAOK	5	33,4	66	24	46	24	45	0	1	18	37	24	45	3	4	8	8	9				
2001	James Shields	Frankfurt	8	24	66	28	61	27	52	1	9	9	17	12	23	11	5	1	13	37				
2001	Alain Digbeu	Barcelona	11	18,9	65	20	62	11	23	9	39	16	20	6	22	8	7	2	17	32				
2001	Tomas Masiulis	Zalgiris	5	30,8	64	25	39	23	33	2	6	12	19	13	28	10	8	2	5	16				
2001	Denis Marconato	Treviso	14	15,9	63	29	43	29	42	0	1	5	14	18	21	6	16	1	20	35				
2001	Jacques Stas	Charleroi	9	26,2	63	25	49	20	30	5	19	8	11	0	10	11	3	0	14	23				
2001	David Desy	Charleroi	10	23,1	62	17	49	7	23	10	26	18	21	2	8	12	7	0	16	29				
2001	Kevin Rankin	Hapoel Jerusalem	7	23	62	22	45	19	35	3	10	15	20	11	17	1	6	3	16	22				
2001	Perry Carter	London Towers	6	25,3	62	22	47	22	47	0	0	18	26	23	31	2	7	1	12	21				
2001	Stojan Vrankovic	Fortitudo Bologna	10	21,7	62	23	36	23	36	0	0	16	25	11	54	6	19	26	22	29				
2001	Gordan Giricek	Cibona Zagreb	6	27	62	16	69	12	38	4	31	26	33	4	14	5	2	0	16	13				
2001	Sasa Obradovic	Buducnost	8	26,6	61	19	61	11	20	8	41	15	22	2	11	8	12	1	12	23				
2001	Dragan Vukcevic	Buducnost	10	21,5	60	22	65	12	22	10	43	6	8	3	10	4	2	1	10	24				
2001	Robin Grey	Frankfurt	9	24,4	60	20	45	13	32	7	13	13	17	5	17	5	7	1	9	36				
2001	Michael Andersen	Peristeri	12	16	60	25	42	25	42	0	0	10	16	14	23	2	5	4	22	27				
2001	David Berbois	Ovarense	8	17,4	60	20	46	12	22	8	24	12	15	4	9	14	4	0	25	25				
2001	Nikola Jestratijevic	Virtus Bologna	14	8,3	59	20	41	20	41	0	0	19	28	8	21	2	9	1	9	33				
2001	Iker Iturbe	Real Madrid	12	17,3	58	19	52	10	28	9	24	11	16	4	30	6	11	1	10	26				
2001	Rodrigo De La Fuente	Barcelona	10	22,4	57	21	60	16	38	5	22	10	11	11	23	10	5	3	13	14				
2001	Massimo Bulleri	Treviso	14	11,5	56	18	39	12	22	6	17	14	17	3	10	8	6	2	10	24				
2001	Davide Bonora	Virtus Bologna	18	13,2	54	13	25	11	15	2	10	26	36	2	19	9	17	2	11	31				
2001	Robert Maras	Frankfurt	8	20,9	54	20	49	20	49	0	0	14	18	16	36	6	6	1	16	27				
2001	Manolis Papamakarios	Peristeri	11	10,7	54	18	39	9	19	9	20	9	12	0	9	5	5	0	14	23				
2001	Rony Seikaly	Barcelona	4	24,5	52	20	43	20	43	0	0	12	20	5	16	4	5	4	11	12				
2001	Milan Gurovic	AEK Athens	4	21,8	52	14	31	8	16	6	15	18	23	0	10	1	3	0	4	11				
2001	Bostjan Nachbar	Treviso	12	9	52	15	35	9	21	6	14	16	25	4	18	9	6	2	7	18				
2001	Chris Corchiani	Vitoria	17	11,1	52	12	33	9	20	3	13	25	33	8	12	10	19	0	9	26				
2001	Dejan Milojevic	Buducnost	11	13,5	51	23	37	23	32	0	5	5	15	18	17	2	7	0	11	36				
2001	Gonzalo Martinez	Estudiantes	10	18,9	49	13	41	6	17	7	24	16	18	1	7	29	6	0	16	23				
2001	Meir Tapiro	Hapoel Jerusalem	10	16,7	49	19	50	19	41	0	9	11	12	6	11	11	9	0	11	25				
2001	Rodolfo Rombaldoni	Verona	12	13,9	48	18	41	17	34	1	7	11	22	6	12	19	12	1	13	24				
2001	Alessandro De Pol	Fortitudo Bologna	14	12,3	48	9	37	7	21	2	16	28	38	6	14	4	5	0	9	20				
2001	Josep Cargol	PAOK	12	13,4	46	18	37	17	31	1	6	9	12	7	20	5	1	2	7	23				
2001	Barak Peleg	Hapoel Jerusalem	10	19,1	46	20	64	17	41	3	23	3	6	12	14	9	10	2	23	31				
2001	Mustafa Sahin	Ovarense	4	26	46	23	33	23	33	0	0	0	6	1	22	3	2	0	6	11				
2001	Slavko Kotnik	Olimpija Ljubljana	13	10,7	45	17	40	17	40	0	0	11	15	15	23	2	8	1	15	17				
2001	Nuno Manarte	Ovarense	10	23,4	45	17	40	16	32	1	8	10	16	2	19	21	16	0	25	27				
2001	Steven Hansell	AEK Athens	12	11,5	44	15	34	12	24	3	10	11	13	3	6	10	9	1	6	10				
2001	Dainius Salenga	Zalgiris	6	22,2	44	15	32	11	22	4	10	10	12	4	10	7	6	1	10	11				
2001	Tomislav Ruzic	Zadar	7	27	43	16	36	16	36	0	0	11	16	7	23	4	3	4	11	29				
2001	Walsh Jordan	Olimpija Ljubljana	12	22,3	43	17	44	11	23	6	21	3	8	3	9	19	19	1	13	27				
2001	Jerry Hester	London Towers	5	31,8	42	15	55	11	30	4	25	8	12	4	6	9	2	0	8	14				
2001	Geert Hammink	AEK Athens	16	10,4	41	17	46	17	45	0	1	7	11	11	22	4	4	2	11	25				
2001	Martynas Andriukaitis	Zalgiris	4	19	41	15	24	15	24	0	0	11	12	7	9	1	1	2	4	11				
2001	Erik Cleymans	Charleroi	10	22,3	41	10	56	6	21	4	35	17	21	4	8	4	4	0	12	25				
2001	Haris Brkic	Buducnost	4	19,8	40	13	32	9	17	4	15	10	15	1	4	9	1	0	11	7				
2001	Amit Tamir	Hapoel Jerusalem	8	13,8	40	14	28	14	28	0	0	12	17	9	8	2	1	1	11	14				
2001	Toni Dijan	Zadar	6	14,5	40	15	23	13	20	2	3	8	10	3	8	4	2	1	6	4				
2001	Spencer Dunkley	Lugano	9	15,9	40	14	35	14	33	0	2	12	17	12	27	2	11	5	8	16				
2001	Massimo Ruggeri	Fortitudo Bologna	9	10,8	38	12	32	8	14	4	18	10	15	3	14	2	4	1	8	14				
2001	Claudio Pilutti	Fortitudo Bologna	13	17,1	37	13	40	8	21	5	19	6	10	5	20	7	18	0	10	19				
2001	Alexis Papadatos	Peristeri	10	10,4	36	10	30	1	10	9	20	7	8	1	11	6	4	0	10	17				
2001	Patrick Femerling	Olympiakos	12	9,5	36	15	22	15	20	0	2	6	8	14	14	2	6	5	9	19				
2001	David Arigbabu	Verona	9	13,2	36	16	31	16	30	0	1	4	8	9	22	6	3	2	8	17				
2001	Constantin Popa	Hapoel Jerusalem	9	13,8	36	13	20	13	20	0	0	10	13	7	16	1	7	8	7	22				
2001	Walter Palmer	Frankfurt	3	22,3	35	15	36	14	30	1	6	4	6	4	9	1	3	2	14	10				
2001	Niklas Lutcke	Frankfurt	8	21	34	13	43	7	19	6	24	2	3	7	8	12	8	0	4	19				
2001	Hrvoje Perincic	Zadar	8	12,8	33	14	28	11	19	3	9	2	4	3	9	3	1	0	7	14				
2001	Martin Henlan	London Towers	10	15,3	33	12	35	9	26	3	9	6	6	13	20	1	6	4	8	20				
2001	Norbert Valis	Lugano	9	13,6	33	12	23	9	14	3	9	6	9	2	16	7	3	0	15	35				
2001	Zoran Viskovic	Ovarense	7	8,3	32	12	16	12	16	0	0	8	11	4	8	0	4	0	9	15				
2001	Jerome James	Buducnost	4	18,8	32	14	41	14	40	0	1	4	8	6	10	1	2	2	7	18				
2001	Kenny Williams	Hapoel Jerusalem	2	33,5	31	12	26	12	23	0	3	7	13	4	15	3	0	1	4	7				
2001	Nicolas Fernandez	Ovarense	8	13,3	31	12	25	10	21	2	4	5	10	1	12	1	4	8	13	26				
2001	Eric Burks	London Towers	5	17,8	31	10	24	6	10	4	14	7	11	2	2	4	2	0	3	10				
2001	Asier Garcia	Estudiantes	10	11,5	30	11	40	9	31	2	9	6	6	10	14	2	5	4	9	12				
2001	Pascal Roller	Frankfurt	5	27,8	29	13	37	10	20	3	17	0	0	1	7	8	2	0	8	9				
2001	Frederic Weis	PAOK	5	19	28	13	22	13	22	0	0	2	5	7	16	4	2	7	3	15				
2001	Roberto Nunez	Real Madrid	7	11,6	28	9	19	4	5	5	14	5	9	2	10	4	5	0	7	8				
2001	Vassilis Soulis	Olympiakos	11	7,5	28	12	24	12	24	0	0	4	8	11	14	3	0	0	6	21				
2001	Erez Katz	Hapoel Jerusalem	10	8,6	28	8	26	6	17	2	9	10	14	1	5	4	9	1	8	10				
2001	Michael Polite	Lugano	7	11,1	28	8	17	8	17	0	0	12	15	10	10	0	2	0	6	12				
2001	Ismael Santos	Treviso	11	13	27	7	18	2	6	5	12	8	11	3	11	4	4	0	3	24				
2001	Nikolaos Bountouris	Olympiakos	12	8,8	27	7	17	2	4	5	13	8	10	1	4	10	5	1	4	20				
2001	Stipe Modric	Olimpija Ljubljana	7	12,4	27	9	15	6	10	3	5	6	8	10	14	3	7	2	5	24				
2001	Peter Lisicky	Treviso	4	16,8	27	6	24	1	12	5	12	10	12	4	3	3	4	0	6	8				
2001	Ignacio Rodriguez	Barcelona	5	18,8	25	8	19	3	8	5	11	4	6	1	7	15	1	0	5	4				
2001	Haywoode Workman	Hapoel Jerusalem	2	30	25	11	16	8	12	3	4	0	0	0	4	8	6	0	3	4				
2001	Ignacio De Miguel	Olympiakos	11	8,5	24	6	14	6	14	0	0	12	23	13	14	2	3	2	10	19				
2001	Hugo Sconochini	Virtus Bologna	4	15,5	21	5	14	4	9	1	5	10	10	3	6	4	4	0	6	11				
2001	Sergi Vidal	Vitoria	14	6,3	20	6	24	4	15	2	9	6	12	1	5	3	5	0	5	13				
2001	Boris Dzidic	Cibona Zagreb	6	15,5	19	7	24	3	11	4	13	1	1	6	7	0	4	1	6	12				
2001	Bojan Bakic	Buducnost	6	16	19	6	20	5	8	1	12	6	9	2	1	3	9	0	4	12				
2001	Zoran Savic	Barcelona	4	11,5	19	7	15	6	12	1	3	4	8	2	6	1	2	0	6	10				
2001	Daniel Garcia	Vitoria	13	6	17	7	23	5	15	2	8	1	2	2	12	5	0	3	4	12				
2001	Kai Nurnberger	Frankfurt	3	30	16	6	17	4	9	2	8	2	2	1	5	4	5	0	6	10				
2001	Ruslan Boidakov	Frankfurt	4	10,5	16	7	13	7	11	0	2	2	2	5	1	0	2	1	1	2				
2001	Daniel Becker	Ovarense	2	26	16	6	13	2	7	4	6	0	0	1	3	0	0	1	4	7				
2001	Peter Van Elswyk	London Towers	4	14,3	16	7	12	6	9	1	3	1	2	5	7	2	1	0	2	8				
2001	Arturas Masiulis	Zalgiris	6	11,2	15	6	17	6	16	0	1	3	4	1	10	0	1	5	6	13				
2001	Marcelo Damiao	Fortitudo Bologna	9	9,3	14	5	12	5	9	0	3	4	13	5	18	1	6	1	3	12				
2001	Johnathan Edwards	Lugano	5	9	14	6	14	6	12	0	2	2	6	5	5	1	4	1	3	11				
2001	Vassilis Kikilias	AEK Athens	10	12,2	12	5	22	5	12	0	10	2	2	6	19	4	3	0	9	30				
2001	Giedrius Gustas	Zalgiris	5	8,6	12	2	8	1	5	1	3	7	8	2	1	2	1	0	2	6				
2001	Dominik Hennen	Frankfurt	1	32	12	5	6	5	6	0	0	2	3	2	3	1	3	0	2	3				
2001	Ronnie Baker	London Towers	7	7,9	12	4	14	3	8	1	6	3	6	1	4	2	1	0	0	7				
2001	David Evans	Olimpija Ljubljana	4	10,8	11	2	9	1	6	1	3	6	6	1	2	3	4	0	5	8				
2001	Zoran Planinic	Cibona Zagreb	5	16,8	10	5	17	5	10	0	7	0	0	0	7	6	1	1	7	18				
2001	Branimir Longin	Cibona Zagreb	5	9,6	10	5	11	5	8	0	3	0	2	3	1	1	0	0	3	8				
2001	Nerijus Karlikanovas	Zalgiris	2	12	10	5	9	5	9	0	0	0	0	1	3	2	0	0	2	4				
2001	Patrick Koller	Lugano	10	11,3	9	3	18	3	9	0	9	3	6	3	3	6	7	0	6	15				
2001	Wayne Henry	London Towers	10	4,1	9	3	15	1	6	2	9	1	2	4	3	1	1	0	1	12				
2001	Jovan Manovic	Ovarense	2	17,5	9	2	10	1	4	1	6	4	4	0	1	5	0	0	2	8				
2001	Anthony Avent	PAOK	3	19	8	4	11	4	11	0	0	0	0	5	7	4	7	2	4	11				
2001	Davor Kus	Cibona Zagreb	8	5	8	0	4	0	2	0	2	8	8	1	3	2	0	0	1	5				
2001	Spiros Panteliadis	AEK Athens	6	11,5	8	1	5	0	3	1	2	5	10	2	5	7	1	0	6	8				
2001	Fabrizio Ambrassa	Virtus Bologna	6	5,5	8	2	6	1	1	1	5	3	3	0	3	1	2	0	0	1				
2001	Jorge Sing	Ovarense	3	8,3	8	3	5	2	2	1	3	1	1	0	5	3	1	1	3	3				
2001	Balsa Radunovic	Buducnost	3	4,3	7	3	7	2	4	1	3	0	0	0	3	0	0	0	1	2				
2001	Cesar Arranz	Estudiantes	4	10,8	6	2	4	2	4	0	0	2	2	2	5	1	3	0	4	3				
2001	Manuel Raga	Lugano	7	12	6	2	12	0	4	2	8	0	0	2	5	3	3	1	4	9				
2001	Aleksandar Simic	Frankfurt	2	11	6	3	6	3	4	0	2	0	0	0	2	1	1	0	3	1				
2001	Luca Sottana	Treviso	1	3	5	0	3	0	0	0	3	5	5	0	0	0	1	0	1	0				
2001	Jaime Silva	Ovarense	2	4,5	5	2	3	1	1	1	2	0	0	0	0	3	0	0	0	0				
2001	Marco Sassella	Lugano	5	7,2	5	1	7	1	4	0	3	3	4	1	3	1	2	0	1	2				
2001	Nikolaos Zisis	AEK Athens	2	17	4	2	8	2	5	0	3	0	0	0	3	0	0	0	1	6				
2001	Ingmar Janke	Frankfurt	1	9	4	1	2	1	2	0	0	2	3	0	0	0	2	0	1	0				
2001	Lashun McDaniels	Ovarense	1	5	4	1	2	1	2	0	0	2	2	0	0	0	0	0	2	1				
2001	Bernard Thiry	Charleroi	6	9,7	4	2	5	2	5	0	0	0	2	0	3	0	1	0	7	12				
2001	Tomislav Knezevic	Zadar	3	2	3	0	2	0	0	0	2	3	4	0	0	0	0	0	1	1				
2001	Giampaolo Zamberlan	Verona	3	8	3	1	3	0	1	1	2	0	0	0	3	0	0	0	0	4				
2001	Giorgio Boscagin	Verona	2	7	3	1	2	1	1	0	1	1	2	0	0	0	0	0	0	1				
2001	John Brugos	Peristeri	1	1	2	0	0	0	0	0	0	2	2	0	0	0	0	0	1	3				
2001	Giorgos Limniatis	PAOK	6	4,5	2	1	3	1	3	0	0	0	0	0	5	4	1	0	2	3				
2001	Luis Munoz	Estudiantes	2	11	2	1	5	1	4	0	1	0	0	0	2	4	1	0	3	2				
2001	Dimitrios Misiakos	AEK Athens	1	2	2	1	1	1	1	0	0	0	0	0	1	0	0	0	0	0				
2001	Ariel Eslava	Real Madrid	1	3	1	0	0	0	0	0	0	1	2	1	1	1	0	0	0	1				
2001	Stephen Tison	Charleroi	1	1	1	0	0	0	0	0	0	1	2	0	0	0	0	0	0	0				
2001	Sime Spralja	Zadar	1	7	0	0	0	0	0	0	0	0	0	0	2	0	0	1	3	2				
2001	Cristian Akrivos	Virtus Bologna	1	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Ioannis Kritikos	Peristeri	3	7,3	0	0	5	0	0	0	5	0	0	0	2	0	1	1	3	3				
2001	Giorgos Apostolidis	PAOK			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Nikos Pettas	Olympiakos			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Blagota Sekulic	Buducnost	3	7,3	0	0	3	0	2	0	1	0	0	1	2	0	0	0	1	5				
2001	Francesc Puyada	Barcelona	1	2	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Miltiadis Moschou	AEK Athens	2	7	0	0	2	0	2	0	0	0	0	2	2	0	0	0	1	2				
2001	Theodoros Triftanidis	PAOK			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Juan Antonio Jobacho	Barcelona	1	3	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1				
2001	Julien Defosse	Charleroi	1	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0				
2001	Emmanuel Manpuya	Charleroi	2	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0				
2001	Martin Lattibeaudiere	London Towers	1	6	0	0	1	0	0	0	1	0	0	0	1	0	0	0	0	3				
2001	Andre Pinto	Ovarense			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Robert Fultz	Fortitudo Bologna			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Fulvio Candido	Fortitudo Bologna			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Ivan Todorovic	Buducnost			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Vidas Ginevicius	Zalgiris	1	21	0	0	3	0	2	0	1	0	0	0	2	1	1	0	2	2				
2001	Marius Basinskas	Zalgiris			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Tim Sanks	Frankfurt	1	3	0	0	0	0	0	0	0	0	0	0	2	1	0	0	1	0				
2001	Erez Zaichik	Hapoel Jerusalem			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Robert Reisenbuchler	Peristeri	2	8,5	0	0	4	0	3	0	1	0	0	2	0	1	1	0	2	2				
2001	Claudio Nobile	Verona			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Neville Austin	London Towers	2	1,5	0	0	0	0	0	0	0	0	0	0	0	0	1	0	2	1				
2001	David Brkic	Virtus Bologna	1	1	0	0	1	0	1	0	0	0	0	0	0	0	0	0	0	0				
2001	Cesar Bravo	Barcelona	2	5	0	0	0	0	0	0	0	0	2	0	1	0	0	0	1	1				
2001	Maxim Kropatchev	Hapoel Jerusalem			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Oumarou Toure	Vitoria			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Vedran Morovic	Zadar			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	M.A. Beltran	Barcelona			0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0				
2001	Gregory Crucifix	Charleroi	2	2	0	0	2	0	0	0	2	0	1	0	0	0	0	0	1	1				
2001	Costas Vassiliadis	PAOK	2	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0				

From giorgio.garziano at tin.it  Fri Sep 25 17:19:00 2015
From: giorgio.garziano at tin.it (giorgio.garziano at tin.it)
Date: Fri, 25 Sep 2015 17:19:00 +0200 (CEST)
Subject: [R] Quantmod several indicators
Message-ID: <150051603cd.giorgio.garziano@tin.it>

Hi,
Both following code examples plot bollinger band over the ticker main plotand Williams Power below the main plot.
1. chartSeries(YHOO,theme="white",TA = c(addBBands(200,2), addWPR(n=300)))
2. chartSeries(IB,theme="white",TA = c(addBBands(200,2)))   addWPR(n=300)
See:
http://around-r.blogspot.it/2015/04/financial-data-exploratory-analysis_27.html
for examples.
In general, if you like to plot indipendently a trading indicator computed by quantmod, you can do:
wpr <- addWPR(n=300)plot(wpr at TA.values, type='l')   -- or any other R plot library you like
as in @TA.values are stored trading indicator values for any indicator.

Giorgio Garziano 

 
	[[alternative HTML version deleted]]


From bbolker at gmail.com  Fri Sep 25 16:46:45 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 25 Sep 2015 14:46:45 +0000
Subject: [R] Accessing defunct package
References: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
Message-ID: <loom.20150925T164520-926@post.gmane.org>

Dennis Fisher <fisher <at> plessthan.com> writes:


> Colleagues,
> 
> In the past, I used a package:
> 	SASxport
> to output files to SAS?s XPT format.  This was useful because FDA requests
that data be submitted in that
> format (even though they typically must reconvert to some other format
before the data are used).
> 


  [snip]

  Try this?

library("devtools")
install_version("SASxport","1.5.0")

or download the last archived version from

https://cran.r-project.org/src/contrib/Archive/SASxport/SASxport_1.5.0.tar.gz

and use R CMD INSTALL from the command line or
install.packages(<filename>,repos=NULL)

From bgunter.4567 at gmail.com  Fri Sep 25 19:47:13 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 25 Sep 2015 10:47:13 -0700
Subject: [R] Randomness tests
In-Reply-To: <248E6FA047A8C746BA491485764190F522093EE6@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F522093DE5@ESESSMB207.ericsson.se>
	<7877CF90-B8A9-4558-B716-B3E28EB5A187@dcn.davis.CA.us>
	<248E6FA047A8C746BA491485764190F522093EE6@ESESSMB207.ericsson.se>
Message-ID: <CAGxFJbTZdya_3odDBHey2=8GvuGMRo4F=0mkDVrM1-z=225ovQ@mail.gmail.com>

Yes, but I'll just note offlist that a test for "any deviation from
randomness" is mathematically impossibile (ask on stackexchange for
why if you like), so you may wish to think about the issue more
carefully -- or consult a local statistician for advice.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Sep 25, 2015 at 9:54 AM, Giorgio Garziano
<giorgio.garziano at ericsson.com> wrote:
> Good suggestion, thanks.
>
> -----Original Message-----
> From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.CA.us]
> Sent: venerd? 25 settembre 2015 18:49
> To: Giorgio Garziano; r-help at r-project.org
> Subject: Re: [R] Randomness tests
>
> You are way off topic for this list. Perhaps stats.stackexchange.com would be a better place to ask such a question.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On September 25, 2015 9:31:15 AM PDT, Giorgio Garziano <giorgio.garziano at ericsson.com> wrote:
>>I am interested in any kind of deviation from randomness.
>>
>>I would like to know if the fact that a time series can take values
>>only from the set {-1, 1} restricts the type of randomness tests that
>>can be done.
>>
>>--
>>
>>Giorgio Garziano
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fisher at plessthan.com  Fri Sep 25 20:23:34 2015
From: fisher at plessthan.com (Dennis Fisher)
Date: Fri, 25 Sep 2015 14:23:34 -0400
Subject: [R] Accessing defunct package
In-Reply-To: <ADF677E9-9BD2-40DF-BF84-007B28478643@comcast.net>
References: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
	<0627FB99-9600-4977-890B-E5767D667B52@dcn.davis.CA.us>
	<ADF677E9-9BD2-40DF-BF84-007B28478643@comcast.net>
Message-ID: <BFE6C2A4-82EB-408F-A031-317DA5A72BDE@plessthan.com>

David

Thanks for suggesting this.  Three issues of note:

1.  I too work on a Mac.  When I downloaded the last archived version (which has a .gz extension), OS X automatically unzipped the file and removed the .gz extension.  I was able to gzip the file so that I could execute your exact command.  However, I am curious whether you were able to download without OS X unzipping it.

2.  When I executed your exact command, I received one warning message:
	SASxport.c:695:10: warning: unused variable 'dbl' [-Wunused-variable]
I presume that this can be ignored.  Am I correct?

3.  Did you actually run the write.xport command?  I did so and it failed on a number of files (but worked on others).  Failures yielded the following error:
	Error in nchar(var) : invalid multibyte string 3157
I traced the problem in this instance to the following text:
	DIARRH??????A
Other than editing the object to remove errant text, is there some general way to prevent this error?

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com



> On Sep 25, 2015, at 1:00 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> I think it might be even simpler at least for now. The error checking done by CRAN can be more rigorous than that done when an installation is done locally. I don't see a report in the current package checks listing of what error was identified, but experimentation is always an option.  When I download the last archived version and install from source I get no error on R 3.2.2 (Mac-SL fork):
> 
> install.packages('~/Downloads/SASxport_1.5.0.tar.gz', repos = NULL , type="source")
> 
> Best;
> David.
> 
> 
> 
> On Sep 25, 2015, at 8:08 AM, Jeff Newmiller wrote:
> 
>> Obtain the source package and fix it? Most errors are relatively minor adjustments that just require reading the updated "Writing R Extensions" document to figure out. You might be unlucky, but I think the odds are in your favor.
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>>                                     Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> --------------------------------------------------------------------------- 
>> Sent from my phone. Please excuse my brevity.
>> 
>> On September 25, 2015 7:25:33 AM PDT, Dennis Fisher <fisher at plessthan.com> wrote:
>>> R 3.2.0
>>> OS X
>>> 
>>> Colleagues,
>>> 
>>> In the past, I used a package:
>>> 	SASxport
>>> to output files to SAS?s XPT format.  This was useful because FDA
>>> requests that data be submitted in that format (even though they
>>> typically must reconvert to some other format before the data are
>>> used).
>>> 
>>> It appears that the package is no longer available at CRAN:
>>> 	Package ?SASxport? was removed from the CRAN repository.
>>> 	Formerly available versions can be obtained from the archive.
>>> 	Archived on 2015-06-09 as errors were not corrected despite reminders.
>>> 
>>> I have a previously-functioning version of the package on my computer. 
>>> When I attempt to load it with:
>>> 	require("SASxport", lib.loc=?/PATH/TOt/R-Packages")
>>> R responds:
>>> 	Loading required package: SASxport 
>>> 	Failed with error: ?package ?SASxport? was built before R 3.0.0:
>>> please re-install it? 
>>> 
>>> Other than reinstalling an old version of R (< 3.0.0), is there some
>>> way that I can use the package?
>>> 
>>> Dennis
>>> 
>>> Dennis Fisher MD
>>> P < (The "P Less Than" Company)
>>> Phone: 1-866-PLessThan (1-866-753-7784)
>>> Fax: 1-866-PLessThan (1-866-753-7784)
>>> www.PLessThan.com
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 


From sarah.goslee at gmail.com  Fri Sep 25 20:38:25 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 25 Sep 2015 14:38:25 -0400
Subject: [R] Accessing defunct package
In-Reply-To: <BFE6C2A4-82EB-408F-A031-317DA5A72BDE@plessthan.com>
References: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
	<0627FB99-9600-4977-890B-E5767D667B52@dcn.davis.CA.us>
	<ADF677E9-9BD2-40DF-BF84-007B28478643@comcast.net>
	<BFE6C2A4-82EB-408F-A031-317DA5A72BDE@plessthan.com>
Message-ID: <CAM_vjuki760+GrbjHFxK6i1_P-YPo=Qm9p5XF0X-Hmqz8EHnQQ@mail.gmail.com>

Not an R question, but tangentially relevant:

On Fri, Sep 25, 2015 at 2:23 PM, Dennis Fisher <fisher at plessthan.com> wrote:
> David
>
> Thanks for suggesting this.  Three issues of note:
>
> 1.  I too work on a Mac.  When I downloaded the last archived version (which has a .gz extension), OS X automatically unzipped the file and removed the .gz extension.  I was able to gzip the file so that I could execute your exact command.  However, I am curious whether you were able to download without OS X unzipping it.

Forever or for one download, it's a Safari setting:
https://discussions.apple.com/thread/1483114?start=0&tstart=0

And if you forget, the zipped version is in your Trash.

Sarah

-- 
Sarah Goslee
http://www.functionaldiversity.org


From lorenzo.isella at gmail.com  Fri Sep 25 21:15:36 2015
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 25 Sep 2015 21:15:36 +0200
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C6144@mb02.ads.tamu.edu>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
	<20150924193642.GA1757@localhost.localdomain>
	<BACC74A6-67C2-4B9F-920E-6554B76F5A75@comcast.net>
	<20150924205452.GB1757@localhost.localdomain>
	<99D0D5AA-BD45-484F-8FCB-8EA0B00E27A7@comcast.net>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C6144@mb02.ads.tamu.edu>
Message-ID: <20150925191536.GB1617@localhost.localdomain>

Absolutely right!
Thanks to both David for their help.
Cheers

Lorenzo

On Fri, Sep 25, 2015 at 01:54:54PM +0000, David L Carlson wrote:
>You defined x and y in your original email as:
>
>> x<-rnorm(20)
>> y<-rnorm(20)
>>
>> mm<-as.matrix(cbind(x,y))
>>
>> dst<-(dist(mm))
>
>-------------------------------------
>David L Carlson
>Department of Anthropology
>Texas A&M University
>College Station, TX 77840-4352
>
>
>-----Original Message-----
>From: David Winsemius [mailto:dwinsemius at comcast.net]
>Sent: Thursday, September 24, 2015 6:30 PM
>To: Lorenzo Isella
>Cc: David L Carlson; r-help at r-project.org
>Subject: Re: [R] Sampling the Distance Matrix
>
>
>On Sep 24, 2015, at 1:54 PM, Lorenzo Isella wrote:
>
>> On Thu, Sep 24, 2015 at 01:30:02PM -0700, David Winsemius wrote:
>>>
>>> On Sep 24, 2015, at 12:36 PM, Lorenzo Isella wrote:
>>>
>>>> Hi,
>>>> And thanks for your reply.
>>>> Essentially, your script gets the job done.
>>>> For instance, if I run
>>>>
>>>> mm <- cbind(5/(1:5), -2*sqrt(1:5))
>>>> dst <- dist(mm)
>>>> dst2 <- as.matrix(dst)
>>>> diag(dst2) <- NA
>>>> idx <- which(apply(dst2, 1, function(x) all(na.omit(x)>.9)))
>>>>
>>>> then it correctly detects the first two rows, where all the values are
>>>> larger than 0.9.
>>>> In other words, it detects the points that are at least 0.9 units away
>>>> from *all* the other points.
>>>> My other question (I did not realize this until I got your answer) is
>>>> the following: I have the distance matrix of a set of N points.
>>>> You gave me an algorithm two find all the points that are at least 0.9
>>>> units away from any other points.
>>>> However, in some cases, for me it is OK even a weaker condition: find
>>>> a subset of k points (with k tunable) whose distance *from each other*
>>>> is greater than 0.9 units (even if their distance from some other
>>>> points may be smaller than 0.9).
>>>
>>> If I understand ..... Make a matrix of unique combinations, then apply by rows to get the qualifying columns that satisfy the distance criterion:
>>>
>>> mtxcomb <- combn(1:20, 5)
>>> goodcls <- apply(mtxcomb , 2, function(idx) all( dist( cbind( x[idx], y[idx]) ) > 0.9))
>>> mtxcomb [ , goodcls]
>>>
>>> In my sample it was around 9% of the total 5 item combinations.
>>>
>>> snipped a lot of output:
>>> .....
>>>   [,1440] [,1441]
>>> [1,]      12      13
>>> [2,]      13      16
>>> [3,]      16      17
>>> [4,]      19      19
>>> [5,]      20      20
>>>> dim( mtxcomb)
>>> [1]     5 15504
>>>
>>
>> Hi,
>> Thanks for your reply.
>> I think I am getting there, but when I run your commands, I get this
>> error message
>>
>> Error in cbind(x[idx], y[idx]) : object 'x' not found
>>
>> Any idea why? Should I combine those 3 lines with something else?
>
>No idea. I was running the setup that you asked for in your original message which you have now omitted from the mail chain.
>
>
>
>> Cheers
>>
>> Lorenzo
>
>David Winsemius
>Alameda, CA, USA
>


From giorgio.garziano at ericsson.com  Fri Sep 25 21:22:46 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Fri, 25 Sep 2015 19:22:46 +0000
Subject: [R] Randomness tests
Message-ID: <248E6FA047A8C746BA491485764190F522093F84@ESESSMB207.ericsson.se>

By "interested in any kind of deviation from randomness", I mean that
I would like to apply all "randtests" R package randomness tests, if
they give reliable results for {-1, 1} sequences.


	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Fri Sep 25 21:54:33 2015
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 25 Sep 2015 21:54:33 +0200
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C6144@mb02.ads.tamu.edu>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
	<20150924193642.GA1757@localhost.localdomain>
	<BACC74A6-67C2-4B9F-920E-6554B76F5A75@comcast.net>
	<20150924205452.GB1757@localhost.localdomain>
	<99D0D5AA-BD45-484F-8FCB-8EA0B00E27A7@comcast.net>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C6144@mb02.ads.tamu.edu>
Message-ID: <20150925195433.GC1617@localhost.localdomain>

Apologies for not letting this thread rest in peace.
The small script

#########################################################
set.seed(1234)

x <- rnorm(20)
y <- rnorm(20)


goodcls <- apply(mtxcomb , 2, function(idx) all( dist( cbind( x[idx],
y[idx]) ) > 0.9))

mycomb <- mtxcomb [ , goodcls]
#########################################################


is perfect to detects groups of 5 points whose distances to each other
are always above 0.9.
However, in my practical case I have about 500 points and I am looking
for subset of several tens of points whose distance is above a given
threshold.
Unfortunately, the approach above does not scale, so I wonder if
anybody is aware of an alternative approach.
Many thanks

Lorenzo


From dwinsemius at comcast.net  Fri Sep 25 22:13:44 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 25 Sep 2015 13:13:44 -0700
Subject: [R] Accessing defunct package
In-Reply-To: <BFE6C2A4-82EB-408F-A031-317DA5A72BDE@plessthan.com>
References: <B40CF5BC-6AF3-4207-A785-C37C749B3FA3@plessthan.com>
	<0627FB99-9600-4977-890B-E5767D667B52@dcn.davis.CA.us>
	<ADF677E9-9BD2-40DF-BF84-007B28478643@comcast.net>
	<BFE6C2A4-82EB-408F-A031-317DA5A72BDE@plessthan.com>
Message-ID: <914793C1-DC7D-4FB2-9FD4-FA24A3E1404A@comcast.net>


On Sep 25, 2015, at 11:23 AM, Dennis Fisher wrote:

> David
> 
> Thanks for suggesting this.  Three issues of note:
> 
> 1.  I too work on a Mac.  When I downloaded the last archived version (which has a .gz extension), OS X automatically unzipped the file and removed the .gz extension.  I was able to gzip the file so that I could execute your exact command.  However, I am curious whether you were able to download without OS X unzipping it.
> 

I use Firefox and had no trouble. I think Sarah has probably identified the difference in our workflow.

> 2.  When I executed your exact command, I received one warning message:
> 	SASxport.c:695:10: warning: unused variable 'dbl' [-Wunused-variable]
> I presume that this can be ignored.  Am I correct?

I'm not sure. It certainly sounds ignorable to me. I did not see such a warning.

> 
> 3.  Did you actually run the write.xport command?  I did so and it failed on a number of files (but worked on others).  Failures yielded the following error:
> 	Error in nchar(var) : invalid multibyte string 3157
> I traced the problem in this instance to the following text:
> 	DIARRH??????A
> Other than editing the object to remove errant text, is there some general way to prevent this error?

No. I have never used SASxport. I was merely reporting (apparent) success in installing it from source on a Mac SL branch 3.2.1.

I get no error when I run the example on the help page:

> abc <- data.frame( x=c(1, 2, NA, NA ), y=c('a', 'B', NA, '*' ) )
> 
> ## look at it
> abc
   x    y
1  1    a
2  2    B
3 NA <NA>
4 NA    *
> 
> ## add a format specifier (not used by R)
> SASformat(abc$x) <- 'date7.'
> 
> ## add a variable label (not used by R)
> label(abc$y) <- 'character variable'
> 
> ## add a dataset label and type
> label(abc) <- 'Simple example'
> SAStype(abc) <- 'MYTYPE'
> 
> ## verify the additions
> str(abc)
'data.frame':	4 obs. of  2 variables:
 $ x: atomic  1 2 NA NA
  ..- attr(*, "SASformat")= chr "date7."
 $ y: Factor w/ 3 levels "*","a","B": 2 3 NA 1
  ..- attr(*, "label")= chr "character variable"
 - attr(*, "label")= chr "Simple example"
 - attr(*, "SAStype")= chr "MYTYPE"
> 
> # create a SAS XPORT file 
> write.xport( abc, file="xxx.dat" )


I think a minimal example would be needed. I'm wondering if one of those weird `ea`-ligatures might be tripping you up when spelling 'diarrhea'.

-- 
David.


> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> 
> 
>> On Sep 25, 2015, at 1:00 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>> I think it might be even simpler at least for now. The error checking done by CRAN can be more rigorous than that done when an installation is done locally. I don't see a report in the current package checks listing of what error was identified, but experimentation is always an option.  When I download the last archived version and install from source I get no error on R 3.2.2 (Mac-SL fork):
>> 
>> install.packages('~/Downloads/SASxport_1.5.0.tar.gz', repos = NULL , type="source")
>> 
>> Best;
>> David.
>> 
>> 
>> 
>> On Sep 25, 2015, at 8:08 AM, Jeff Newmiller wrote:
>> 
>>> Obtain the source package and fix it? Most errors are relatively minor adjustments that just require reading the updated "Writing R Extensions" document to figure out. You might be unlucky, but I think the odds are in your favor.
>>> ---------------------------------------------------------------------------
>>> Jeff Newmiller                        The     .....       .....  Go Live...
>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>>>                                    Live:   OO#.. Dead: OO#..  Playing
>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>>> --------------------------------------------------------------------------- 
>>> Sent from my phone. Please excuse my brevity.
>>> 
>>> On September 25, 2015 7:25:33 AM PDT, Dennis Fisher <fisher at plessthan.com> wrote:
>>>> R 3.2.0
>>>> OS X
>>>> 
>>>> Colleagues,
>>>> 
>>>> In the past, I used a package:
>>>> 	SASxport
>>>> to output files to SAS?s XPT format.  This was useful because FDA
>>>> requests that data be submitted in that format (even though they
>>>> typically must reconvert to some other format before the data are
>>>> used).
>>>> 
>>>> It appears that the package is no longer available at CRAN:
>>>> 	Package ?SASxport? was removed from the CRAN repository.
>>>> 	Formerly available versions can be obtained from the archive.
>>>> 	Archived on 2015-06-09 as errors were not corrected despite reminders.
>>>> 
>>>> I have a previously-functioning version of the package on my computer. 
>>>> When I attempt to load it with:
>>>> 	require("SASxport", lib.loc=?/PATH/TOt/R-Packages")
>>>> R responds:
>>>> 	Loading required package: SASxport 
>>>> 	Failed with error: ?package ?SASxport? was built before R 3.0.0:
>>>> please re-install it? 
>>>> 
>>>> Other than reinstalling an old version of R (< 3.0.0), is there some
>>>> way that I can use the package?
>>>> 
>>>> Dennis
>>>> 
>>>> Dennis Fisher MD
>>>> P < (The "P Less Than" Company)
>>>> Phone: 1-866-PLessThan (1-866-753-7784)
>>>> Fax: 1-866-PLessThan (1-866-753-7784)
>>>> www.PLessThan.com
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
> 

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Fri Sep 25 22:56:56 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 25 Sep 2015 13:56:56 -0700
Subject: [R] Sampling the Distance Matrix
In-Reply-To: <20150925195433.GC1617@localhost.localdomain>
References: <20150923191537.GA2548@localhost.localdomain>
	<CAF8bMcYHVQiBs2y_nGjeBV+nZgi1WF67gmxRvxCO-fKio2xBdQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C5770@mb02.ads.tamu.edu>
	<20150924193642.GA1757@localhost.localdomain>
	<BACC74A6-67C2-4B9F-920E-6554B76F5A75@comcast.net>
	<20150924205452.GB1757@localhost.localdomain>
	<99D0D5AA-BD45-484F-8FCB-8EA0B00E27A7@comcast.net>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C6144@mb02.ads.tamu.edu>
	<20150925195433.GC1617@localhost.localdomain>
Message-ID: <5C129E88-3531-4595-B20C-D595E316BF8C@comcast.net>


On Sep 25, 2015, at 12:54 PM, Lorenzo Isella wrote:

> Apologies for not letting this thread rest in peace.
> The small script
> 
> #########################################################
> set.seed(1234)
> 
> x <- rnorm(20)
> y <- rnorm(20)
> 
> 
> goodcls <- apply(mtxcomb , 2, function(idx) all( dist( cbind( x[idx],
> y[idx]) ) > 0.9))
> 
> mycomb <- mtxcomb [ , goodcls]
> #########################################################
> 
> 
> is perfect to detects groups of 5 points whose distances to each other
> are always above 0.9.
> However, in my practical case I have about 500 points and I am looking
> for subset of several tens of points whose distance is above a given
> threshold.
> Unfortunately, the approach above does not scale, so I wonder if
> anybody is aware of an alternative approach.

Find the center of the distribution, eliminate all the points within some reasonable radius perhaps sqrt( sd(x)^2 +sd(y)^2 ) and then work on the reduced set. If you needed to reduce it even further I could imagine sampling in sectors defined by tan(x/y).

-- 

David Winsemius
Alameda, CA, USA


From fisher at plessthan.com  Fri Sep 25 23:23:54 2015
From: fisher at plessthan.com (Dennis Fisher)
Date: Fri, 25 Sep 2015 17:23:54 -0400
Subject: [R] Multibyte strings
Message-ID: <C8799AC8-33AB-44F4-9498-88A8A0B2B3BD@plessthan.com>

R 3.2.0
OS X

Colleagues,

Earlier today, I initiated a series of emails regarding SASxport (which was removed from CRAN).  David Winsemius proposed downloading the source code and installing with the following command:
	install.packages('~/Downloads/SASxport_1.5.0.tar.gz', repos = NULL , type="source?)Th

That works and I am grateful to David for his recommendation.  However, the package fails on some of the many objects that I attempted to write with:
	write.xport

The error message was:
	Error in nchar(var) : invalid multibyte string 3157

One work-around would be to edit out multibyte strings.  Is there a simple way to find and replace them?  Or is there some other clever approach that bypasses the problem?

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From dwinsemius at comcast.net  Sat Sep 26 00:20:27 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 25 Sep 2015 15:20:27 -0700
Subject: [R] Multibyte strings
In-Reply-To: <C8799AC8-33AB-44F4-9498-88A8A0B2B3BD@plessthan.com>
References: <C8799AC8-33AB-44F4-9498-88A8A0B2B3BD@plessthan.com>
Message-ID: <4EDE246B-3FDF-4039-B5BD-06DB25D1F214@comcast.net>




On Sep 25, 2015, at 2:23 PM, Dennis Fisher wrote:

> R 3.2.0
> OS X
> 
> Colleagues,
> 
> Earlier today, I initiated a series of emails regarding SASxport (which was removed from CRAN).  David Winsemius proposed downloading the source code and installing with the following command:
> 	install.packages('~/Downloads/SASxport_1.5.0.tar.gz', repos = NULL , type="source?)Th
> 
> That works and I am grateful to David for his recommendation.  However, the package fails on some of the many objects that I attempted to write with:
> 	write.xport
> 
> The error message was:
> 	Error in nchar(var) : invalid multibyte string 3157

Consider using traceback() to see what section of code is actually reporting?

Since the error reported in your earlier message indicated a problem with a particular word starting with DIARRH  and ending in ?????A. When I try to drop that unquoted into an R console line I get:

> DIARRH??????A
Error: unexpected input in "DIARRH?"

My word process tells me that little comma-like glyph is a cedilla.

However I'm not sure this is reproducible problem since I am unable to produce a similar error with the toy file that is built with the write.xport help page code:

> abc <- data.frame( x=c(1, 2, NA, NA ), y=c('a', 'DIARRH??????A', NA, '*' ) )
> abc
   x             y
1  1             a
2  2 DIARRH??????A
3 NA          <NA>
4 NA             *
> SASformat(abc$x) <- 'date7.'
> label(abc$y) <- 'character variable'
> label(abc) <- 'Simple example'
> SAStype(abc) <- 'MYTYPE'
> str(abc)
'data.frame':	4 obs. of  2 variables:
 $ x: atomic  1 2 NA NA
  ..- attr(*, "SASformat")= chr "date7."
 $ y: Factor w/ 3 levels "*","a","DIARRH??????A": 2 3 NA 1
  ..- attr(*, "label")= chr "character variable"
 - attr(*, "label")= chr "Simple example"
 - attr(*, "SAStype")= chr "MYTYPE"
> write.xport( abc, file="xxx.dat" )
> abc <- data.frame( x=c(1, 2, NA, NA ), y=c('a', 'DIARRH??????A', NA, '*' ) )
> abc
   x             y
1  1             a
2  2 DIARRH??????A
3 NA          <NA>
4 NA             *
> SASformat(abc$x) <- 'date7.'
> label(abc$y) <- '"DIARRH??????A"'
> label(abc) <- 'Simple example'
> SAStype(abc) <- 'MYTYPE'
> str(abc)
'data.frame':	4 obs. of  2 variables:
 $ x: atomic  1 2 NA NA
  ..- attr(*, "SASformat")= chr "date7."
 $ y: Factor w/ 3 levels "*","a","DIARRH??????A": 2 3 NA 1
  ..- attr(*, "label")= chr "\"DIARRH??????A\""
 - attr(*, "label")= chr "Simple example"
 - attr(*, "SAStype")= chr "MYTYPE"
> write.xport( abc, file="xxx.dat" )


> 
> One work-around would be to edit out multibyte strings.  Is there a simple way to find and replace them?  

On a Mac I have used the Zap Gremlins option in TextWrangler.app. It would change the spelling of words that were originally constructed using ligature characters.


Best of luck;
David.

> Or is there some other clever approach that bypasses the problem?
> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From btyner at gmail.com  Sat Sep 26 01:36:08 2015
From: btyner at gmail.com (Benjamin Tyner)
Date: Fri, 25 Sep 2015 19:36:08 -0400
Subject: [R] backsolve, chol, Matrix, and SparseM
In-Reply-To: <22021.1246.450108.509693@stat.math.ethz.ch>
References: <5604374E.7000301@gmail.com>
	<22021.1246.450108.509693@stat.math.ethz.ch>
Message-ID: <5605DA68.8050302@gmail.com>

Hi Martin,

Thanks for the remarks and examples, and for confirming that I was
indeed barking up the wrong tree with SparseM.

A. I assume that is a typo and you meant to say, no need for backsolve().
B. Absolutely; however, in this case I am taking advantage of
quadprog::solve.QP(..., factorized = TRUE) which requires the inverse of
the Cholesky factor; it turns out to be faster to compute this one time
upfront rather than have solve.QP(..., factorized = FALSE) do it over
and over again. Of course the holy grail would be a QP solver which
takes advantage of the various innovations from package:Matrix, but I
digress...
C. Agreed, assuming you are talking about Matrix::solve(X) on X of class
Matrix. On the other hand for a regular matrix x it is not difficult to
construct examples where backsolve(chol(x), diag(nrow(x))) is twice as
fast as base::solve(chol(x)), which led me down this path in the first
place.

By the way, is R-forge still the correct place to report bugs in
package:Matrix?

Regards
Ben


On 09/25/2015 04:25 AM, Martin Maechler wrote:
> Dear Ben,
>
>>>>>> Benjamin Tyner <btyner at gmail.com>
>>>>>>     on Thu, 24 Sep 2015 13:47:58 -0400 writes:
>     > Hi I have some code which does (on a symmetric matrix 'x')
>
>     >     backsolve(chol(x), diag(nrow(x)))
>
>     > and I am wondering what is the recommended way to
>     > accomplish this when x is also sparse (from
>     > package:Matrix). I know that package:Matrix provides a
>     > chol method for such matrices, but not a backsolve
>     > method. On the other hand, package:SparseM does provide a
>     > backsolve method, but doesn't actually return a sparse
>     > matrix. Moreover, I am a little hesitant to use SparseM,
>     > as the vignette seems to be from 2003.
>
> Roger Koenker has agreed in the past, that new projects should
> rather use Matrix.   SparseM has been the very first R package
> providing sparse matrix support.
>
>
>     > I did notice that help(topic = "solve", package =
>     > "Matrix") says "In ?solve(a,b)? in the ?Matrix? package,
>     > ?a? may also be a ?MatrixFactorization? instead of
>     > directly a matrix." which makes me think this is the right
>     > way:
>
>     >     Matrix::solve(Cholesky(x), .sparseDiagonal(nrow(x)))
>
>     > but unfortunately this didn't give the same result as:
>
>     >     Matrix::solve(chol(x), .sparseDiagonal(nrow(x)))
>
>     > so I'm asking here in case someone has any suggestions.
>
> You don't give any examples.
> So a few remarks and a reproducible example to get more concrete
>
> A. As the Matrix package has classes for triangular matrices and
>   Matrix :: chol() returns them, there   is no need for
>   forwardsolve() or backwardsolve(), as just   solve() is always
>   enough.
>
> B. As Doug Bates has been teaching for many decennia, "it is
>   almost always computationally *wrong* to compute a matrix
>   inverse explicitly".
>   Rather compute    A^{-1} B   or  A^{-1} x  {for vector x,
>   matrix B (but different from Identity).
>
> C. Inspite of B, there are cases (such as computing sandwich
>   estimates of covariance matrices) where you do want the inverse.
>   In that case,
>
>    solve(A)   		  is semantically equivalent to
>    solve(A, diag(.))
>
>    and almost always the *first* form is implempented more
>    efficiently than the second.
>
> D. In Matrix,  use chol(.) ... unless you really read a bit
>    about Cholesky(.) and its special purpose sparse cholesky decompositions.
>    As mentioned above,  Matrix :: chol()  will return a
>    "formally triangular" matrix, i.e., inheriting from
>    "triangularMatrix"; in the sparse case, very typically of
>    specific class "dtCMatrix".
>
> Here's a small reproducible example,
> please use it to ask further questions:
>
> *.R:
>
> library(Matrix)
> M <- as(diag(4)+1,"dsCMatrix")
> m <- as(M, "matrix") # -> traditional R matrix
> stopifnot( all(M == m) )
> M
> L <- Cholesky(M,super=TRUE,perm=FALSE) # a MatrixFactorization ("dCHMsuper")
> (L. <- as(L, "Matrix")) #-> lower-triagonal (sparseMatrix, specifically "dtCMatrix")
> (cM <- chol(M))# *upper* triagonal ("dtCMatrix")
> (cm <- chol(m))#  upper  triagonal traditional matrix -- the same "of course" :
> all.equal(as.matrix(cM), cm) # TRUE
>
> (r. <- backsolve(cm, diag(4)))# upper tri. (traditional) matrix
> (R. <-     solve(cM) ) ## the "same"  (but nicer printing)
> all.equal(as.matrix(R.), r., check.attributes=FALSE) # TRUE
> all( abs(R. - r.) <  1e-12 * mean(abs(R.))) # TRUE
>
> *.Rout:
>
>> M <- as(diag(4)+1,"dsCMatrix")
>> m <- as(M, "matrix") # -> traditional R matrix
>> stopifnot( all(M == m) )
>> M
> 4 x 4 sparse Matrix of class "dsCMatrix"
>             
> [1,] 2 1 1 1
> [2,] 1 2 1 1
> [3,] 1 1 2 1
> [4,] 1 1 1 2
>> L <- Cholesky(M,super=TRUE,perm=FALSE) # a MatrixFactorization ("dCHMsuper")
>> (L. <- as(L, "Matrix")) #-> lower-triagonal (sparseMatrix, specifically "dtCMatrix")
> 4 x 4 sparse Matrix of class "dtCMatrix"
>                                            
> [1,] 1.4142136 .         .         .       
> [2,] 0.7071068 1.2247449 .         .       
> [3,] 0.7071068 0.4082483 1.1547005 .       
> [4,] 0.7071068 0.4082483 0.2886751 1.118034
>> (cM <- chol(M))# *upper* triagonal ("dtCMatrix")
> 4 x 4 sparse Matrix of class "dtCMatrix"
>                                            
> [1,] 1.414214 0.7071068 0.7071068 0.7071068
> [2,] .        1.2247449 0.4082483 0.4082483
> [3,] .        .         1.1547005 0.2886751
> [4,] .        .         .         1.1180340
>> (cm <- chol(m))#  upper  triagonal traditional matrix -- the same "of course" :
>          [,1]      [,2]      [,3]      [,4]
> [1,] 1.414214 0.7071068 0.7071068 0.7071068
> [2,] 0.000000 1.2247449 0.4082483 0.4082483
> [3,] 0.000000 0.0000000 1.1547005 0.2886751
> [4,] 0.000000 0.0000000 0.0000000 1.1180340
>> all.equal(as.matrix(cM), cm) # TRUE
> [1] TRUE
>> (r. <- backsolve(cm, diag(4)))# upper tri. (traditional) matrix
>           [,1]       [,2]       [,3]       [,4]
> [1,] 0.7071068 -0.4082483 -0.2886751 -0.2236068
> [2,] 0.0000000  0.8164966 -0.2886751 -0.2236068
> [3,] 0.0000000  0.0000000  0.8660254 -0.2236068
> [4,] 0.0000000  0.0000000  0.0000000  0.8944272
>> (R. <-     solve(cM) ) ## the "same"  (but nicer printing)
> 4 x 4 sparse Matrix of class "dtCMatrix"
>                                                
> [1,] 0.7071068 -0.4082483 -0.2886751 -0.2236068
> [2,] .          0.8164966 -0.2886751 -0.2236068
> [3,] .          .          0.8660254 -0.2236068
> [4,] .          .          .          0.8944272
>> all.equal(as.matrix(R.), r., check.attributes=FALSE) # TRUE
> [1] TRUE
>> all( abs(R. - r.) <  1e-12 * mean(abs(R.))) # TRUE
> [1] TRUE



From michael.eisenring at gmx.ch  Sat Sep 26 07:01:11 2015
From: michael.eisenring at gmx.ch (Michael Eisenring)
Date: Fri, 25 Sep 2015 22:01:11 -0700
Subject: [R] How to get significance codes after Kruskal Wallis test
Message-ID: <000601d0f818$5e670de0$1b3529a0$@gmx.ch>

Is there a way to get significance codes after a pairwise comparisons to a
Kruskall wallis test? With significance codes I mean letter codes (a, b,c)
that are assigned to treatments to indicate where differences are
significant.

With a traditional anova such a test can be performed using HSD.test from
the agricolae library but for non parametric counterparts of anova I have
not been able to find anything.

Can anyone help me?

Thanks mike

 

I added two example codes.

First code  represents an ANOVA and a HSD.test() giving me significant codes

 #FIRST CODE USING ANOVA
 

 

library(agricolae)
an.dta<-aov(Gossypol~Treatment,data=dta)
summary(an.dta)

HSD.test(an.dta,"Treatment")
# The level by alpha default is 0.05.
outT<-HSD.test(an.dta,"Treatment", group=T)
outT

#I receive significant codes.

 

 

#SECOND CODE USING KRUSKAL WALLIs

library(agricolae)
an.dta2<-kruskal.test(Heliocide~Treatment,dta)
summary(an.dta2)

HSD.test(an.dta2,"Treatment")

#ERROR MESSAGE no significance codes, why??

 

#DATA FOR CODES

 
structure(list(Treatment = structure(c(1L, 3L, 4L, 2L, 1L, 3L, 
4L, 2L, 5L, 1L, 3L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 
5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 
1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 
3L, 5L), .Label = c("1_2d", "1_7d", "3_2d", "9_2d", "C"), class = "factor"),

    Code = structure(c(1L, 2L, 3L, 4L, 18L, 19L, 20L, 21L, 22L, 
    23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 
    35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 44L, 45L, 46L, 
    47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 56L, 5L, 6L, 
    7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L), .Label =
c("1_2d_1c", 
    "1_2d_3c", "1_2d_9c", "1_7d_1c", "10_2d_1c", "10_2d_3c", 
    "10_2d_9c", "10_7d_1c", "10_C", "11_2d_1c", "11_2d_3c", "11_2d_9c", 
    "11_7d_1c", "11_C", "12_2d_1c", "12_2d_3c", "12_C", "2_2d_1c", 
    "2_2d_3c", "2_2d_9c", "2_7d_1c", "2_C", "3_2d_1c", "3_2d_3c", 
    "3_7d_1c", "3_C", "4_2d_1c", "4_2d_3c", "4_2d_9c", "4_7d_1c", 
    "4_C", "5_2d_1c", "5_2d_3c", "5_2d_9c", "5_7d_1c", "5_C", 
    "6_2d_1c", "6_2d_3c", "6_2d_9c", "6_7d_1c", "6_C", "7_2d_1c", 
    "7_2d_3c", "7_2d_9c", "7_7d_1c", "7_C", "8_2d_1c", "8_2d_3c", 
    "8_2d_9c", "8_7d_1c", "8_C", "9_2d_1c", "9_2d_3c", "9_2d_9c", 
    "9_7d_1c", "9_C"), class = "factor"), Glands = c(165, 289.3333333, 
    319.3333333, 472, 334.6666667, 259, 373.3333333, 525.6666667, 
    275.3333333, 230.6666667, 346.3333333, 377.6666667, 255.3333333, 
    217.6666667, 266, 300.3333333, 354.3333333, 225.3333333, 
    294, 359, 359, 222.6666667, 103, 246.6666667, 324.6666667, 
    277, 460, 163.6666667, 226.3333333, 228, 357.6666667, 505, 
    142.6666667, 324, 278.6666667, 317.3333333, 335.6666667, 
    193.6666667, 188, 255, 252, 393.3333333, 248.3333333, 353, 
    320.6666667, 228.3333333, 497, 165.6666667, 209.3333333, 
    162.3333333, 280, 337, 169.6666667, 231.6666667, 257.6666667, 
    218.6666667), Tannin = c(0.334252451, 1.376077586, 0.896849593, 
    0.888621795, 0.464285714, 0.830236486, 0.870881783, 0.768489583, 
    0.647727273, 0.81372549, 0.51380814, 0.859923246, 0.495265152, 
    0.699932796, 1.09375, 0.785037879, 0.892650463, 0.518963675, 
    1.05859375, 0.447916667, 1.269097222, 1.147522523, 0.391276042, 
    0.883400538, 1.523989899, 0.907930108, 0.749155405, 0.450126263, 
    0.562239583, 0.911151961, 0.611111111, 1.610677083, 0.446428571, 
    0.601151316, 1.073635057, 1.359923246, 1.00154321, 0.90933642, 
    0.012054398, 1.102083333, 1.017361111, 1.052372685, 0.958607456, 
    1.224702381, 0.982291667, 1.045138889, 1.611607143, 0.662574405, 
    1.385416667, 0.464518229, 0.994444444, 1.239583333, 0.877514368, 
    0.74453125, 0.804315476, 1.024066092), H.polone = c(6754.067177, 
    22380.26652, 23622.79158, 23733.77678, 13099.20833, 23564.74907, 
    2725.016387, 18751.03986, 4283.098494, 23008.35336, 10205.56354, 
    19787.63361, 4302.050374, 7400.640798, 22442.86044, 34315.09631, 
    16498.66728, 14170.13252, 9509.1073, 6265.29637, 20671.56905, 
    14517.15648, 2643.950729, 4974.607571, 14782.87029, 13918.82361, 
    12526.27863, 1236.908141, 4854.469195, 4076.396504, 9603.950212, 
    13762.57476, 2298.727719, 3514.186757, 5705.140289, 14178.21668, 
    14277.39878, 2656.552509, 8184.633961, 9931.163373, 21474.90732, 
    18522.74376, 9884.406532, 17242.54114, 8431.506608, 14601.11606, 
    15748.4912, 2849.90903, 16747.27644, 9396.645481, 21996.95822, 
    5767.358748, 5767.358748, 14207.1734, 10353.21833, 2859.51171
    ), Gossypol = c(1036.331811, 4171.427741, 6039.995102, 5909.068158, 
    4140.242559, 4854.985845, 6982.035521, 6132.876396, 948.2418407, 
    3618.448997, 3130.376482, 5113.942098, 1180.171957, 1500.863038, 
    4576.787021, 5629.979049, 3378.151945, 3589.187889, 2508.417927, 
    1989.576826, 5972.926124, 2867.610671, 450.7205451, 1120.955, 
    3470.09352, 3575.043632, 2952.931863, 349.0864019, 1013.807628, 
    910.8879471, 3743.331903, 3350.203452, 592.3403778, 1517.045807, 
    1504.491931, 3736.144027, 2818.419785, 723.885643, 1782.864308, 
    1414.161257, 3723.629772, 3747.076592, 2005.919344, 4198.569251, 
    2228.522959, 3322.115942, 4274.324792, 720.9785449, 2874.651764, 
    2287.228752, 5654.858696, 1247.806111, 1247.806111, 2547.326207, 
    2608.716056, 1079.846532), Heliocide = c(711.1776124, 8559.141828, 
    8014.897387, 3972.305107, 3227.467943, 5778.242027, 3628.427557, 
    3177.426984, 325.1764586, 3774.732152, 3111.880146, 4624.945228, 
    160.8912744, 336.4018128, 5207.091788, 6360.856306, 1740.091298, 
    1588.430761, 3509.141442, 685.6917982, 4664.118976, 1477.26149, 
    75.73956465, 402.1570283, 3703.317553, 4235.211434, 1730.465296, 
    91.53557346, 334.5397274, 698.1713846, 3328.897126, 1742.69355, 
    231.9097243, 513.7933372, 774.6461158, 4687.003829, 1692.296924, 
    179.1968506, 1022.628651, 1199.898583, 6132.303567, 1971.798098, 
    413.3375988, 4072.908467, 615.911814, 4906.642605, 3160.349616, 
    117.642134, 4929.371855, 616.8755006, 7428.352411, 767.2288107, 
    767.2288107, 1078.928494, 730.6740868, 425.9053258), Damage..cm. =
c(0.4955, 
    1.516, 4.409, 3.2665, 0.491, 2.3035, 3.51, 1.8115, 0, 0.4435, 
    1.573, 1.8595, 0, 0.142, 2.171, 4.023, 4.9835, 0, 0.6925, 
    1.989, 5.683, 3.547, 0, 0.756, 2.129, 9.437, 3.211, 0, 0.578, 
    2.966, 4.7245, 1.8185, 0, 1.0475, 1.62, 5.568, 9.7455, 0, 
    0.8295, 2.411, 7.272, 4.516, 0, 0.4035, 2.974, 8.043, 4.809, 
    0, 0.6965, 1.313, 5.681, 3.474, 0, 0.5895, 2.559, 0)), .Names =
c("Treatment", 
"Code", "Glands", "Tannin", "H.polone", "Gossypol", "Heliocide", 
"Damage..cm."), class = "data.frame", row.names = c(NA, -56L))

 


	[[alternative HTML version deleted]]


From michael.eisenring at gmx.ch  Sat Sep 26 07:08:52 2015
From: michael.eisenring at gmx.ch (Michael Eisenring)
Date: Fri, 25 Sep 2015 22:08:52 -0700
Subject: [R] How to calculate standard error of estimate (S) for my
	non-linear regression model?
Message-ID: <000b01d0f819$710b6f30$53224d90$@gmx.ch>

Hi all,

I am looking for something that indicates the goodness of fit for my non
linear regression model (since R2 is not very reliable).

I read that the standard error of estimate (also known as standard error of
the regression) is a good alternative.

 

The standard error of estimate is described on this page (including the
formula) http://onlinestatbook.com/2/regression/accuracy.html
<https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fonlinest
atbook.com%2F2%2Fregression%2Faccuracy.html> 

Unfortunately however, I have no clue how to programm it in R. Does anyone
know and could help me?

Thank you very much.

 

I added an example of my model and a dput() of my data

#CODE

dta<-read.csv("Regression_exp2.csv",header=T, sep = ",")
attach(dta)      # tells R to do the following analyses on this dataset
head(dta)

 

# loading packages: analysis of mixed effect models
library(nls2)#model

#Aim: fit equation to data: y~yo+a*(1-b^x) : Two parameter exp. single rise
to the maximum
# y =Gossypol (from my data set) x= Damage_cm (from my data set)
#The other 3 parameters are unknown: yo=Intercept, a= assymptote ans b=slope

plot(Gossypol~Damage_cm, dta)
# Looking at the plot, 0 is a plausible estimate for y0:
# a+y0 is the asymptote, so estimate about 4000;
# b is between 0 and 1, so estimate .5
dta.nls <- nls(Gossypol~y0+a*(1-b^Damage_cm), dta,
               start=list(y0=0, a=4000, b=.5))

xval <- seq(0, 10, 0.1)
lines(xval, predict(dta.nls, data.frame(Damage_cm=xval)))
profile(dta.nls, alpha= .05)


summary(dta.nls)

 

 

 

#INPUT

structure(list(Gossypol = c(948.2418407, 1180.171957, 3589.187889, 
450.7205451, 349.0864019, 592.3403778, 723.885643, 2005.919344, 
720.9785449, 1247.806111, 1079.846532, 1500.863038, 4198.569251, 
3618.448997, 4140.242559, 1036.331811, 1013.807628, 2547.326207, 
2508.417927, 2874.651764, 1120.955, 1782.864308, 1517.045807, 
2287.228752, 4171.427741, 3130.376482, 1504.491931, 6132.876396, 
3350.203452, 5113.942098, 1989.576826, 3470.09352, 4576.787021, 
4854.985845, 1414.161257, 2608.716056, 910.8879471, 2228.522959, 
2952.931863, 5909.068158, 1247.806111, 6982.035521, 2867.610671, 
5629.979049, 6039.995102, 3747.076592, 3743.331903, 4274.324792, 
3378.151945, 3736.144027, 5654.858696, 5972.926124, 3723.629772, 
3322.115942, 3575.043632, 2818.419785), Treatment = structure(c(5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 4L, 2L, 4L, 4L, 2L, 4L, 2L, 2L, 
4L, 4L, 4L, 4L, 4L, 4L, 2L), .Label = c("1c_2d", "1c_7d", "3c_2d", 
"9c_2d", "C"), class = "factor"), Damage_cm = c(0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0.142, 0.4035, 0.4435, 0.491, 0.4955, 0.578, 
0.5895, 0.6925, 0.6965, 0.756, 0.8295, 1.0475, 1.313, 1.516, 
1.573, 1.62, 1.8115, 1.8185, 1.8595, 1.989, 2.129, 2.171, 2.3035, 
2.411, 2.559, 2.966, 2.974, 3.211, 3.2665, 3.474, 3.51, 3.547, 
4.023, 4.409, 4.516, 4.7245, 4.809, 4.9835, 5.568, 5.681, 5.683, 
7.272, 8.043, 9.437, 9.7455), Damage_groups = c(0.278, 1.616, 
2.501, 3.401, 4.577, 5.644, 7.272, 8.043, 9.591, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), Gossypol_Averaged =
c(1783.211, 
3244.129, 2866.307, 3991.809, 4468.809, 5121.309, 3723.629772, 
3322.115942, 3196.731, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA), Groups = c(42006L, 42038L, 42067L, 42099L, 
42130L, 42162L, 42193L, 42225L, 42257L, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA)), .Names = c("Gossypol", 
"Treatment", "Damage_cm", "Damage_groups", "Gossypol_Averaged", 
"Groups"), class = "data.frame", row.names = c(NA, -56L))

 


	[[alternative HTML version deleted]]


From kmwolf at ucdavis.edu  Sat Sep 26 08:25:41 2015
From: kmwolf at ucdavis.edu (Kristina Wolf)
Date: Fri, 25 Sep 2015 23:25:41 -0700
Subject: [R] How to get significance codes after Kruskal Wallis test
In-Reply-To: <000601d0f818$5e670de0$1b3529a0$@gmx.ch>
References: <000601d0f818$5e670de0$1b3529a0$@gmx.ch>
Message-ID: <CAHq6VpdWyXW2A94jx6_uBMKh7FUEhht8iJf1aw=0phSRWupCnw@mail.gmail.com>

Perhaps look into the function friedman.test.with.post.hoc()
There is more information here:
http://www.r-statistics.com/wp-content/uploads/2010/02/Friedman-Test-with-Post-Hoc.r.txt

Note, this does not handle NA's though, and technically it is for blocked
designs, but maybe it will lead you somewhere useful or could be adapted?


*?~ Kristina*

??
Kristina Wolf
?
?
Ph.D. Candidate, Graduate Group in Ecology
M.S. Soil Science
?,
?
B.S. Animal Science?
?
KristinaMWolf.com
Restoration Ecology Lab
?
Department of Plant Sciences
?
University of California, Davis?
?
(530) 750-9771

"We have to remember that what we observe is not nature herself, but nature
exposed to our method of questioning." ~ Werner Heisenberg


On Fri, Sep 25, 2015 at 10:01 PM, Michael Eisenring <
michael.eisenring at gmx.ch> wrote:

> Is there a way to get significance codes after a pairwise comparisons to a
> Kruskall wallis test? With significance codes I mean letter codes (a, b,c)
> that are assigned to treatments to indicate where differences are
> significant.
>
> With a traditional anova such a test can be performed using HSD.test from
> the agricolae library but for non parametric counterparts of anova I have
> not been able to find anything.
>
> Can anyone help me?
>
> Thanks mike
>
>
>
> I added two example codes.
>
> First code  represents an ANOVA and a HSD.test() giving me significant
> codes
>
>  #FIRST CODE USING ANOVA
>
>
>
>
> library(agricolae)
> an.dta<-aov(Gossypol~Treatment,data=dta)
> summary(an.dta)
>
> HSD.test(an.dta,"Treatment")
> # The level by alpha default is 0.05.
> outT<-HSD.test(an.dta,"Treatment", group=T)
> outT
>
> #I receive significant codes.
>
>
>
>
>
> #SECOND CODE USING KRUSKAL WALLIs
>
> library(agricolae)
> an.dta2<-kruskal.test(Heliocide~Treatment,dta)
> summary(an.dta2)
>
> HSD.test(an.dta2,"Treatment")
>
> #ERROR MESSAGE no significance codes, why??
>
>
>
> #DATA FOR CODES
>
>
> structure(list(Treatment = structure(c(1L, 3L, 4L, 2L, 1L, 3L,
> 4L, 2L, 5L, 1L, 3L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L,
> 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L,
> 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L,
> 3L, 5L), .Label = c("1_2d", "1_7d", "3_2d", "9_2d", "C"), class =
> "factor"),
>
>     Code = structure(c(1L, 2L, 3L, 4L, 18L, 19L, 20L, 21L, 22L,
>     23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L,
>     35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 44L, 45L, 46L,
>     47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 56L, 5L, 6L,
>     7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L), .Label =
> c("1_2d_1c",
>     "1_2d_3c", "1_2d_9c", "1_7d_1c", "10_2d_1c", "10_2d_3c",
>     "10_2d_9c", "10_7d_1c", "10_C", "11_2d_1c", "11_2d_3c", "11_2d_9c",
>     "11_7d_1c", "11_C", "12_2d_1c", "12_2d_3c", "12_C", "2_2d_1c",
>     "2_2d_3c", "2_2d_9c", "2_7d_1c", "2_C", "3_2d_1c", "3_2d_3c",
>     "3_7d_1c", "3_C", "4_2d_1c", "4_2d_3c", "4_2d_9c", "4_7d_1c",
>     "4_C", "5_2d_1c", "5_2d_3c", "5_2d_9c", "5_7d_1c", "5_C",
>     "6_2d_1c", "6_2d_3c", "6_2d_9c", "6_7d_1c", "6_C", "7_2d_1c",
>     "7_2d_3c", "7_2d_9c", "7_7d_1c", "7_C", "8_2d_1c", "8_2d_3c",
>     "8_2d_9c", "8_7d_1c", "8_C", "9_2d_1c", "9_2d_3c", "9_2d_9c",
>     "9_7d_1c", "9_C"), class = "factor"), Glands = c(165, 289.3333333,
>     319.3333333, 472, 334.6666667, 259, 373.3333333, 525.6666667,
>     275.3333333, 230.6666667, 346.3333333, 377.6666667, 255.3333333,
>     217.6666667, 266, 300.3333333, 354.3333333, 225.3333333,
>     294, 359, 359, 222.6666667, 103, 246.6666667, 324.6666667,
>     277, 460, 163.6666667, 226.3333333, 228, 357.6666667, 505,
>     142.6666667, 324, 278.6666667, 317.3333333, 335.6666667,
>     193.6666667, 188, 255, 252, 393.3333333, 248.3333333, 353,
>     320.6666667, 228.3333333, 497, 165.6666667, 209.3333333,
>     162.3333333, 280, 337, 169.6666667, 231.6666667, 257.6666667,
>     218.6666667), Tannin = c(0.334252451, 1.376077586, 0.896849593,
>     0.888621795, 0.464285714, 0.830236486, 0.870881783, 0.768489583,
>     0.647727273, 0.81372549, 0.51380814, 0.859923246, 0.495265152,
>     0.699932796, 1.09375, 0.785037879, 0.892650463, 0.518963675,
>     1.05859375, 0.447916667, 1.269097222, 1.147522523, 0.391276042,
>     0.883400538, 1.523989899, 0.907930108, 0.749155405, 0.450126263,
>     0.562239583, 0.911151961, 0.611111111, 1.610677083, 0.446428571,
>     0.601151316, 1.073635057, 1.359923246, 1.00154321, 0.90933642,
>     0.012054398, 1.102083333, 1.017361111, 1.052372685, 0.958607456,
>     1.224702381, 0.982291667, 1.045138889, 1.611607143, 0.662574405,
>     1.385416667, 0.464518229, 0.994444444, 1.239583333, 0.877514368,
>     0.74453125, 0.804315476, 1.024066092), H.polone = c(6754.067177,
>     22380.26652, 23622.79158, 23733.77678, 13099.20833, 23564.74907,
>     2725.016387, 18751.03986, 4283.098494, 23008.35336, 10205.56354,
>     19787.63361, 4302.050374, 7400.640798, 22442.86044, 34315.09631,
>     16498.66728, 14170.13252, 9509.1073, 6265.29637, 20671.56905,
>     14517.15648, 2643.950729, 4974.607571, 14782.87029, 13918.82361,
>     12526.27863, 1236.908141, 4854.469195, 4076.396504, 9603.950212,
>     13762.57476, 2298.727719, 3514.186757, 5705.140289, 14178.21668,
>     14277.39878, 2656.552509, 8184.633961, 9931.163373, 21474.90732,
>     18522.74376, 9884.406532, 17242.54114, 8431.506608, 14601.11606,
>     15748.4912, 2849.90903, 16747.27644, 9396.645481, 21996.95822,
>     5767.358748, 5767.358748, 14207.1734, 10353.21833, 2859.51171
>     ), Gossypol = c(1036.331811, 4171.427741, 6039.995102, 5909.068158,
>     4140.242559, 4854.985845, 6982.035521, 6132.876396, 948.2418407,
>     3618.448997, 3130.376482, 5113.942098, 1180.171957, 1500.863038,
>     4576.787021, 5629.979049, 3378.151945, 3589.187889, 2508.417927,
>     1989.576826, 5972.926124, 2867.610671, 450.7205451, 1120.955,
>     3470.09352, 3575.043632, 2952.931863, 349.0864019, 1013.807628,
>     910.8879471, 3743.331903, 3350.203452, 592.3403778, 1517.045807,
>     1504.491931, 3736.144027, 2818.419785, 723.885643, 1782.864308,
>     1414.161257, 3723.629772, 3747.076592, 2005.919344, 4198.569251,
>     2228.522959, 3322.115942, 4274.324792, 720.9785449, 2874.651764,
>     2287.228752, 5654.858696, 1247.806111, 1247.806111, 2547.326207,
>     2608.716056, 1079.846532), Heliocide = c(711.1776124, 8559.141828,
>     8014.897387, 3972.305107, 3227.467943, 5778.242027, 3628.427557,
>     3177.426984, 325.1764586, 3774.732152, 3111.880146, 4624.945228,
>     160.8912744, 336.4018128, 5207.091788, 6360.856306, 1740.091298,
>     1588.430761, 3509.141442, 685.6917982, 4664.118976, 1477.26149,
>     75.73956465, 402.1570283, 3703.317553, 4235.211434, 1730.465296,
>     91.53557346, 334.5397274, 698.1713846, 3328.897126, 1742.69355,
>     231.9097243, 513.7933372, 774.6461158, 4687.003829, 1692.296924,
>     179.1968506, 1022.628651, 1199.898583, 6132.303567, 1971.798098,
>     413.3375988, 4072.908467, 615.911814, 4906.642605, 3160.349616,
>     117.642134, 4929.371855, 616.8755006, 7428.352411, 767.2288107,
>     767.2288107, 1078.928494, 730.6740868, 425.9053258), Damage..cm. =
> c(0.4955,
>     1.516, 4.409, 3.2665, 0.491, 2.3035, 3.51, 1.8115, 0, 0.4435,
>     1.573, 1.8595, 0, 0.142, 2.171, 4.023, 4.9835, 0, 0.6925,
>     1.989, 5.683, 3.547, 0, 0.756, 2.129, 9.437, 3.211, 0, 0.578,
>     2.966, 4.7245, 1.8185, 0, 1.0475, 1.62, 5.568, 9.7455, 0,
>     0.8295, 2.411, 7.272, 4.516, 0, 0.4035, 2.974, 8.043, 4.809,
>     0, 0.6965, 1.313, 5.681, 3.474, 0, 0.5895, 2.559, 0)), .Names =
> c("Treatment",
> "Code", "Glands", "Tannin", "H.polone", "Gossypol", "Heliocide",
> "Damage..cm."), class = "data.frame", row.names = c(NA, -56L))
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From markshanks101 at hotmail.com  Sat Sep 26 00:33:40 2015
From: markshanks101 at hotmail.com (markshanks)
Date: Fri, 25 Sep 2015 15:33:40 -0700 (PDT)
Subject: [R] Analysis of causal relations between rare (categorical) events
Message-ID: <1443220420164-4712801.post@n4.nabble.com>

Hi,

I have only a relatively basic background in statistics (e.g., anova,
regression), and the books on R I have read so far have focused on
relatively common statistical analyses (e.g., outlier analysis, trend
forecasting) and haven't helped me with the data problem I am facing.

In short, imagine if the data is date stamped and we are interested in
predicting the occurrence of relatively-rare, categorical events through the
occurrence of other, relatively-rare categorical events. Both the outcomes
and predictors can be a huge number of different types, although it is
possible to group them as well.

One way to go would seem to be to take the difference in time between each
set of predictors and outcomes and see if there is more consistency for some
measures than others?? However, I haven't found a good book or package that
is directly aimed at this type of dataset, although I'm guessing there must
be some...

Thanks,

Mark





--
View this message in context: http://r.789695.n4.nabble.com/Analysis-of-causal-relations-between-rare-categorical-events-tp4712801.html
Sent from the R help mailing list archive at Nabble.com.


From glennmschultz at me.com  Sat Sep 26 03:19:10 2015
From: glennmschultz at me.com (Glenn Schultz)
Date: Sat, 26 Sep 2015 01:19:10 +0000 (GMT)
Subject: [R] R studio and git
Message-ID: <ec201886-b26f-4b85-aaf0-28515665bf09@me.com>

Hello all,

Maybe not the right place. ?I have a project that is under git version control in R studio. ?I just updated to recent OSX build and now the git tab in R studio is not available. ?It seems I can still use version control with source tree but it is strange that I lost git version on the update of OSX. Any ideas?

Glenn

From damien.jourdain at cirad.fr  Sat Sep 26 07:42:02 2015
From: damien.jourdain at cirad.fr (Damien Jourdain)
Date: Fri, 25 Sep 2015 22:42:02 -0700 (PDT)
Subject: [R] flexmix - concomitant model and significance of variables
Message-ID: <1443246122149-4712809.post@n4.nabble.com>

Dear All, 

I am new to this forum and to flexmix. 
I am using flexmix to make a cluster analysis (Model Based). The data for
the clustering are all continuous (although all between 0 and 1).  I also
want to see the correlation between found clusters and some socio economic
variables, so I am using a concomitant model

The formulation is:
Conc<- FLXmultinom(~factor(Area)+factor(income)+Gender+factor(Education))

f2c <- flexmix(cbind(ECO, SOC, ENV, CULT)~1, k=5, 
              model=FLXMCmvnorm(), concomitant=Conc, data=data)

To recover the influence of socio-economic variables from the output I am
using:
f2c at concomitant@coef

However, how do I know which variables are significant?

Any help is welcomed!

Best

Damien





--
View this message in context: http://r.789695.n4.nabble.com/flexmix-concomitant-model-and-significance-of-variables-tp4712809.html
Sent from the R help mailing list archive at Nabble.com.


From dwinsemius at comcast.net  Sat Sep 26 08:40:08 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 25 Sep 2015 23:40:08 -0700
Subject: [R] R studio and git
In-Reply-To: <ec201886-b26f-4b85-aaf0-28515665bf09@me.com>
References: <ec201886-b26f-4b85-aaf0-28515665bf09@me.com>
Message-ID: <7C517683-3D1C-40AE-BF3F-530CC5948160@comcast.net>

This is not the  RStudio support list. That function is offered on a web-hosted support list.

-- 
David

On Sep 25, 2015, at 6:19 PM, Glenn Schultz wrote:

> Hello all,
> 
> Maybe not the right place.  I have a project that is under git version control in R studio.  I just updated to recent OSX build and now the git tab in R studio is not available.  It seems I can still use version control with source tree but it is strange that I lost git version on the update of OSX. Any ideas?
> 
> Glenn
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From giorgio.garziano at ericsson.com  Sat Sep 26 09:55:14 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Sat, 26 Sep 2015 07:55:14 +0000
Subject: [R] Analysis of causal relations between rare (categorical)
 events
Message-ID: <248E6FA047A8C746BA491485764190F522094024@ESESSMB207.ericsson.se>

Hi,

I may suggest the following book introducing event history analysis with R and
showing some datasets to work with:

https://www.crcpress.com/Event-History-Analysis-with-R/Brostrm/9781439831649

I am not sure it can answer all your questions about your specific problem (rare events),
however it may help.

Giorgio Garziano





	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Sat Sep 26 10:42:44 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 26 Sep 2015 10:42:44 +0200
Subject: [R] How to calculate standard error of estimate (S) for my
	non-linear regression model?
In-Reply-To: <000b01d0f819$710b6f30$53224d90$@gmx.ch>
References: <000b01d0f819$710b6f30$53224d90$@gmx.ch>
Message-ID: <582FCB0F-9082-4D83-854B-17F243EEDCBC@gmail.com>

This is one area in which terminology in (computational) statistics has gone a bit crazy. The thing some call "standard error of estimate" is actually the residual standard deviation in the regression model, not to be confused with the standard errors that are associated with parameter estimates. In summary(nls(...)) (and summary(lm()) for that matter), you'll find it as "residual standard error",  and even that is a bit of a misnomer.

-pd 

> On 26 Sep 2015, at 07:08 , Michael Eisenring <michael.eisenring at gmx.ch> wrote:
> 
> Hi all,
> 
> I am looking for something that indicates the goodness of fit for my non
> linear regression model (since R2 is not very reliable).
> 
> I read that the standard error of estimate (also known as standard error of
> the regression) is a good alternative.
> 
> 
> 
> The standard error of estimate is described on this page (including the
> formula) http://onlinestatbook.com/2/regression/accuracy.html
> <https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fonlinest
> atbook.com%2F2%2Fregression%2Faccuracy.html> 
> 
> Unfortunately however, I have no clue how to programm it in R. Does anyone
> know and could help me?
> 
> Thank you very much.
> 
> 
> 
> I added an example of my model and a dput() of my data
> 
> #CODE
> 
> dta<-read.csv("Regression_exp2.csv",header=T, sep = ",")
> attach(dta)      # tells R to do the following analyses on this dataset
> head(dta)
> 
> 
> 
> # loading packages: analysis of mixed effect models
> library(nls2)#model
> 
> #Aim: fit equation to data: y~yo+a*(1-b^x) : Two parameter exp. single rise
> to the maximum
> # y =Gossypol (from my data set) x= Damage_cm (from my data set)
> #The other 3 parameters are unknown: yo=Intercept, a= assymptote ans b=slope
> 
> plot(Gossypol~Damage_cm, dta)
> # Looking at the plot, 0 is a plausible estimate for y0:
> # a+y0 is the asymptote, so estimate about 4000;
> # b is between 0 and 1, so estimate .5
> dta.nls <- nls(Gossypol~y0+a*(1-b^Damage_cm), dta,
>               start=list(y0=0, a=4000, b=.5))
> 
> xval <- seq(0, 10, 0.1)
> lines(xval, predict(dta.nls, data.frame(Damage_cm=xval)))
> profile(dta.nls, alpha= .05)
> 
> 
> summary(dta.nls)
> 
> 
> 
> 
> 
> 
> 
> #INPUT
> 
> structure(list(Gossypol = c(948.2418407, 1180.171957, 3589.187889, 
> 450.7205451, 349.0864019, 592.3403778, 723.885643, 2005.919344, 
> 720.9785449, 1247.806111, 1079.846532, 1500.863038, 4198.569251, 
> 3618.448997, 4140.242559, 1036.331811, 1013.807628, 2547.326207, 
> 2508.417927, 2874.651764, 1120.955, 1782.864308, 1517.045807, 
> 2287.228752, 4171.427741, 3130.376482, 1504.491931, 6132.876396, 
> 3350.203452, 5113.942098, 1989.576826, 3470.09352, 4576.787021, 
> 4854.985845, 1414.161257, 2608.716056, 910.8879471, 2228.522959, 
> 2952.931863, 5909.068158, 1247.806111, 6982.035521, 2867.610671, 
> 5629.979049, 6039.995102, 3747.076592, 3743.331903, 4274.324792, 
> 3378.151945, 3736.144027, 5654.858696, 5972.926124, 3723.629772, 
> 3322.115942, 3575.043632, 2818.419785), Treatment = structure(c(5L, 
> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 1L, 1L, 1L, 1L, 1L, 1L, 
> 1L, 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 
> 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 4L, 2L, 4L, 4L, 2L, 4L, 2L, 2L, 
> 4L, 4L, 4L, 4L, 4L, 4L, 2L), .Label = c("1c_2d", "1c_7d", "3c_2d", 
> "9c_2d", "C"), class = "factor"), Damage_cm = c(0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0.142, 0.4035, 0.4435, 0.491, 0.4955, 0.578, 
> 0.5895, 0.6925, 0.6965, 0.756, 0.8295, 1.0475, 1.313, 1.516, 
> 1.573, 1.62, 1.8115, 1.8185, 1.8595, 1.989, 2.129, 2.171, 2.3035, 
> 2.411, 2.559, 2.966, 2.974, 3.211, 3.2665, 3.474, 3.51, 3.547, 
> 4.023, 4.409, 4.516, 4.7245, 4.809, 4.9835, 5.568, 5.681, 5.683, 
> 7.272, 8.043, 9.437, 9.7455), Damage_groups = c(0.278, 1.616, 
> 2.501, 3.401, 4.577, 5.644, 7.272, 8.043, 9.591, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), Gossypol_Averaged =
> c(1783.211, 
> 3244.129, 2866.307, 3991.809, 4468.809, 5121.309, 3723.629772, 
> 3322.115942, 3196.731, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA), Groups = c(42006L, 42038L, 42067L, 42099L, 
> 42130L, 42162L, 42193L, 42225L, 42257L, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA)), .Names = c("Gossypol", 
> "Treatment", "Damage_cm", "Damage_groups", "Gossypol_Averaged", 
> "Groups"), class = "data.frame", row.names = c(NA, -56L))
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Sat Sep 26 11:52:34 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 26 Sep 2015 11:52:34 +0200
Subject: [R] Multibyte strings
In-Reply-To: <C8799AC8-33AB-44F4-9498-88A8A0B2B3BD@plessthan.com>
References: <C8799AC8-33AB-44F4-9498-88A8A0B2B3BD@plessthan.com>
Message-ID: <8DF6DF26-ECEA-478B-A6AD-6A30DE7859A6@gmail.com>

Dennis,

The invalid multibyte issue is almost certainly a symptom of being in a UTF-8 locale and trying to handle strings that aren't in UTF-8. (UTF uses particular 8 bit patterns to say that the following k bytes contain a Unicode value outside ASCII, other "8 bit ASCII" encodings, like Latin-1, just use the extra 128 character codes for special characters. Treating the latter as the former causes errors, the other way around just looks weird.

So perhaps you should try diddling your locale settings and/or look for encoding arguments for the functions that you use. Then again, the XPT format may not be happy with non-ASCII characters, whatever the encoding, in which case you may need to massage the input data sets and change variable names and factor labels (iconv() should be your friend).

By the way, I don't think the FDA "requests" XPT files. As far as I recall, they say somewhere that they _accept_ them (possibly defending themselves against the platform-specific SAS files that once abunded), but I think even Excel goes for submissions - the important thing is that they can get at the actual data reasonably easy. I can see the attraction of taking the well-trodden path, though.

-pd

> On 25 Sep 2015, at 23:23 , Dennis Fisher <fisher at plessthan.com> wrote:
> 
> R 3.2.0
> OS X
> 
> Colleagues,
> 
> Earlier today, I initiated a series of emails regarding SASxport (which was removed from CRAN).  David Winsemius proposed downloading the source code and installing with the following command:
> 	install.packages('~/Downloads/SASxport_1.5.0.tar.gz', repos = NULL , type="source?)Th
> 
> That works and I am grateful to David for his recommendation.  However, the package fails on some of the many objects that I attempted to write with:
> 	write.xport
> 
> The error message was:
> 	Error in nchar(var) : invalid multibyte string 3157
> 
> One work-around would be to edit out multibyte strings.  Is there a simple way to find and replace them?  Or is there some other clever approach that bypasses the problem?
> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From drjimlemon at gmail.com  Sat Sep 26 12:29:38 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 26 Sep 2015 20:29:38 +1000
Subject: [R] Analysis of causal relations between rare (categorical)
	events
In-Reply-To: <1443220420164-4712801.post@n4.nabble.com>
References: <1443220420164-4712801.post@n4.nabble.com>
Message-ID: <CA+8X3fX5g01+P8+R7TWDxZRKZbvbdjAeOPkfcN1eR3t-AgghSA@mail.gmail.com>

Hi Mark,
You might find the eventInterval package of use.

Jim


On Sat, Sep 26, 2015 at 8:33 AM, markshanks <markshanks101 at hotmail.com>
wrote:

> Hi,
>
> I have only a relatively basic background in statistics (e.g., anova,
> regression), and the books on R I have read so far have focused on
> relatively common statistical analyses (e.g., outlier analysis, trend
> forecasting) and haven't helped me with the data problem I am facing.
>
> In short, imagine if the data is date stamped and we are interested in
> predicting the occurrence of relatively-rare, categorical events through
> the
> occurrence of other, relatively-rare categorical events. Both the outcomes
> and predictors can be a huge number of different types, although it is
> possible to group them as well.
>
> One way to go would seem to be to take the difference in time between each
> set of predictors and outcomes and see if there is more consistency for
> some
> measures than others?? However, I haven't found a good book or package that
> is directly aimed at this type of dataset, although I'm guessing there must
> be some...
>
> Thanks,
>
> Mark
>
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Analysis-of-causal-relations-between-rare-categorical-events-tp4712801.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From fisher at plessthan.com  Sat Sep 26 14:16:20 2015
From: fisher at plessthan.com (Dennis Fisher)
Date: Sat, 26 Sep 2015 08:16:20 -0400
Subject: [R] Multibyte strings
In-Reply-To: <8DF6DF26-ECEA-478B-A6AD-6A30DE7859A6@gmail.com>
References: <C8799AC8-33AB-44F4-9498-88A8A0B2B3BD@plessthan.com>
	<8DF6DF26-ECEA-478B-A6AD-6A30DE7859A6@gmail.com>
Message-ID: <70C29DF6-4AA7-4D2B-B16D-264849AF1543@plessthan.com>

Peter

Thanks for the explanation.  One further comment ? you wrote:
> I don't think the FDA "requests" XPT files 

In fact, they do make such a request.  Here is the actual language received this week (and repeatedly in the past):
> Program/script files should be submitted using text files (*.TXT) and the data should be submitted using SAS transport files (*.XPT).

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com



> On Sep 26, 2015, at 5:52 AM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> Dennis,
> 
> The invalid multibyte issue is almost certainly a symptom of being in a UTF-8 locale and trying to handle strings that aren't in UTF-8. (UTF uses particular 8 bit patterns to say that the following k bytes contain a Unicode value outside ASCII, other "8 bit ASCII" encodings, like Latin-1, just use the extra 128 character codes for special characters. Treating the latter as the former causes errors, the other way around just looks weird.
> 
> So perhaps you should try diddling your locale settings and/or look for encoding arguments for the functions that you use. Then again, the XPT format may not be happy with non-ASCII characters, whatever the encoding, in which case you may need to massage the input data sets and change variable names and factor labels (iconv() should be your friend).
> 
> By the way, I don't think the FDA "requests" XPT files. As far as I recall, they say somewhere that they _accept_ them (possibly defending themselves against the platform-specific SAS files that once abunded), but I think even Excel goes for submissions - the important thing is that they can get at the actual data reasonably easy. I can see the attraction of taking the well-trodden path, though.
> 
> -pd
> 
>> On 25 Sep 2015, at 23:23 , Dennis Fisher <fisher at plessthan.com> wrote:
>> 
>> R 3.2.0
>> OS X
>> 
>> Colleagues,
>> 
>> Earlier today, I initiated a series of emails regarding SASxport (which was removed from CRAN).  David Winsemius proposed downloading the source code and installing with the following command:
>> 	install.packages('~/Downloads/SASxport_1.5.0.tar.gz', repos = NULL , type="source?)Th
>> 
>> That works and I am grateful to David for his recommendation.  However, the package fails on some of the many objects that I attempted to write with:
>> 	write.xport
>> 
>> The error message was:
>> 	Error in nchar(var) : invalid multibyte string 3157
>> 
>> One work-around would be to edit out multibyte strings.  Is there a simple way to find and replace them?  Or is there some other clever approach that bypasses the problem?
>> 
>> Dennis
>> 
>> Dennis Fisher MD
>> P < (The "P Less Than" Company)
>> Phone: 1-866-PLessThan (1-866-753-7784)
>> Fax: 1-866-PLessThan (1-866-753-7784)
>> www.PLessThan.com
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 


From michael.eisenring at gmx.ch  Sat Sep 26 15:48:34 2015
From: michael.eisenring at gmx.ch (Michael Eisenring)
Date: Sat, 26 Sep 2015 06:48:34 -0700
Subject: [R] How to get significance codes after Kruskal Wallis test
In-Reply-To: <CAHq6VpdWyXW2A94jx6_uBMKh7FUEhht8iJf1aw=0phSRWupCnw@mail.gmail.com>
References: <000601d0f818$5e670de0$1b3529a0$@gmx.ch>
	<CAHq6VpdWyXW2A94jx6_uBMKh7FUEhht8iJf1aw=0phSRWupCnw@mail.gmail.com>
Message-ID: <002c01d0f862$0b622810$22267830$@gmx.ch>

Thank you very much Kristina,

Unfortunately that?s not what I am looking for.

I am just very surprised if there would be no possibility to get the significance codes for Kruskal Wallis (I would have suggested that this is a pretty common test.)

 

I found another option called kruskal() which does pairwise comparison, but without significance codes.

Maybe another R-list member knows more.

 

Thank you,

Mike

 

Von: Kristina Wolf [mailto:kmwolf at ucdavis.edu] 
Gesendet: Freitag, 25. September 2015 23:26
An: Michael Eisenring <michael.eisenring at gmx.ch>
Cc: r-help <r-help at r-project.org>
Betreff: Re: [R] How to get significance codes after Kruskal Wallis test

 

Perhaps look into the function friedman.test.with.post.hoc()

There is more information here: http://www.r-statistics.com/wp-content/uploads/2010/02/Friedman-Test-with-Post-Hoc.r.txt

 

Note, this does not handle NA's though, and technically it is for blocked designs, but maybe it will lead you somewhere useful or could be adapted? 

 




?

~ Kristina

 

??

Kristina Wolf

?

?

Ph.D. Candidate, Graduate Group in Ecology

M.S. Soil Science

?, 

?

B.S. Animal Science?

?

KristinaMWolf.com

Restoration Ecology Lab

?

Department of Plant Sciences

?

University of California, Davis?

?

(530) 750-9771

 

"We have to remember that what we observe is not nature herself, but nature exposed to our method of questioning." ~ Werner Heisenberg

 

 

On Fri, Sep 25, 2015 at 10:01 PM, Michael Eisenring <michael.eisenring at gmx.ch <mailto:michael.eisenring at gmx.ch> > wrote:

Is there a way to get significance codes after a pairwise comparisons to a
Kruskall wallis test? With significance codes I mean letter codes (a, b,c)
that are assigned to treatments to indicate where differences are
significant.

With a traditional anova such a test can be performed using HSD.test from
the agricolae library but for non parametric counterparts of anova I have
not been able to find anything.

Can anyone help me?

Thanks mike



I added two example codes.

First code  represents an ANOVA and a HSD.test() giving me significant codes

 #FIRST CODE USING ANOVA




library(agricolae)
an.dta<-aov(Gossypol~Treatment,data=dta)
summary(an.dta)

HSD.test(an.dta,"Treatment")
# The level by alpha default is 0.05.
outT<-HSD.test(an.dta,"Treatment", group=T)
outT

#I receive significant codes.





#SECOND CODE USING KRUSKAL WALLIs

library(agricolae)
an.dta2<-kruskal.test(Heliocide~Treatment,dta)
summary(an.dta2)

HSD.test(an.dta2,"Treatment")

#ERROR MESSAGE no significance codes, why??



#DATA FOR CODES


structure(list(Treatment = structure(c(1L, 3L, 4L, 2L, 1L, 3L,
4L, 2L, 5L, 1L, 3L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L,
5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L,
1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L,
3L, 5L), .Label = c("1_2d", "1_7d", "3_2d", "9_2d", "C"), class = "factor"),

    Code = structure(c(1L, 2L, 3L, 4L, 18L, 19L, 20L, 21L, 22L,
    23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L,
    35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 44L, 45L, 46L,
    47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 56L, 5L, 6L,
    7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L), .Label =
c("1_2d_1c",
    "1_2d_3c", "1_2d_9c", "1_7d_1c", "10_2d_1c", "10_2d_3c",
    "10_2d_9c", "10_7d_1c", "10_C", "11_2d_1c", "11_2d_3c", "11_2d_9c",
    "11_7d_1c", "11_C", "12_2d_1c", "12_2d_3c", "12_C", "2_2d_1c",
    "2_2d_3c", "2_2d_9c", "2_7d_1c", "2_C", "3_2d_1c", "3_2d_3c",
    "3_7d_1c", "3_C", "4_2d_1c", "4_2d_3c", "4_2d_9c", "4_7d_1c",
    "4_C", "5_2d_1c", "5_2d_3c", "5_2d_9c", "5_7d_1c", "5_C",
    "6_2d_1c", "6_2d_3c", "6_2d_9c", "6_7d_1c", "6_C", "7_2d_1c",
    "7_2d_3c", "7_2d_9c", "7_7d_1c", "7_C", "8_2d_1c", "8_2d_3c",
    "8_2d_9c", "8_7d_1c", "8_C", "9_2d_1c", "9_2d_3c", "9_2d_9c",
    "9_7d_1c", "9_C"), class = "factor"), Glands = c(165, 289.3333333,
    319.3333333, 472, 334.6666667, 259, 373.3333333, 525.6666667,
    275.3333333, 230.6666667, 346.3333333, 377.6666667, 255.3333333,
    217.6666667, 266, 300.3333333, 354.3333333, 225.3333333,
    294, 359, 359, 222.6666667, 103, 246.6666667, 324.6666667,
    277, 460, 163.6666667, 226.3333333, 228, 357.6666667, 505,
    142.6666667, 324, 278.6666667, 317.3333333, 335.6666667,
    193.6666667, 188, 255, 252, 393.3333333, 248.3333333, 353,
    320.6666667, 228.3333333, 497, 165.6666667, 209.3333333,
    162.3333333, 280, 337, 169.6666667, 231.6666667, 257.6666667,
    218.6666667), Tannin = c(0.334252451, 1.376077586, 0.896849593,
    0.888621795, 0.464285714, 0.830236486, 0.870881783, 0.768489583,
    0.647727273, 0.81372549, 0.51380814, 0.859923246, 0.495265152,
    0.699932796, 1.09375, 0.785037879, 0.892650463, 0.518963675,
    1.05859375, 0.447916667, 1.269097222, 1.147522523, 0.391276042,
    0.883400538, 1.523989899, 0.907930108, 0.749155405, 0.450126263,
    0.562239583, 0.911151961, 0.611111111, 1.610677083, 0.446428571,
    0.601151316, 1.073635057, 1.359923246, 1.00154321, 0.90933642,
    0.012054398, 1.102083333, 1.017361111, 1.052372685, 0.958607456,
    1.224702381, 0.982291667, 1.045138889, 1.611607143, 0.662574405,
    1.385416667, 0.464518229, 0.994444444, 1.239583333, 0.877514368,
    0.74453125, 0.804315476, 1.024066092), H.polone = c(6754.067177,
    22380.26652, 23622.79158, 23733.77678, 13099.20833, 23564.74907,
    2725.016387, 18751.03986, 4283.098494, 23008.35336, 10205.56354,
    19787.63361, 4302.050374, 7400.640798, 22442.86044, 34315.09631,
    16498.66728, 14170.13252, 9509.1073, 6265.29637, 20671.56905,
    14517.15648, 2643.950729, 4974.607571, 14782.87029, 13918.82361,
    12526.27863, 1236.908141, 4854.469195, 4076.396504, 9603.950212,
    13762.57476, 2298.727719, 3514.186757, 5705.140289, 14178.21668,
    14277.39878, 2656.552509, 8184.633961, 9931.163373, 21474.90732,
    18522.74376, 9884.406532, 17242.54114, 8431.506608, 14601.11606,
    15748.4912, 2849.90903, 16747.27644, 9396.645481, 21996.95822,
    5767.358748, 5767.358748, 14207.1734, 10353.21833, 2859.51171
    ), Gossypol = c(1036.331811, 4171.427741, 6039.995102, 5909.068158,
    4140.242559, 4854.985845, 6982.035521, 6132.876396, 948.2418407,
    3618.448997, 3130.376482, 5113.942098, 1180.171957, 1500.863038,
    4576.787021, 5629.979049, 3378.151945, 3589.187889, 2508.417927,
    1989.576826, 5972.926124, 2867.610671, 450.7205451, 1120.955,
    3470.09352, 3575.043632, 2952.931863, 349.0864019, 1013.807628,
    910.8879471, 3743.331903, 3350.203452, 592.3403778, 1517.045807,
    1504.491931, 3736.144027, 2818.419785, 723.885643, 1782.864308,
    1414.161257, 3723.629772, 3747.076592, 2005.919344, 4198.569251,
    2228.522959, 3322.115942, 4274.324792, 720.9785449, 2874.651764,
    2287.228752, 5654.858696, 1247.806111, 1247.806111, 2547.326207,
    2608.716056, 1079.846532), Heliocide = c(711.1776124, 8559.141828,
    8014.897387, 3972.305107, 3227.467943, 5778.242027, 3628.427557,
    3177.426984, 325.1764586, 3774.732152, 3111.880146, 4624.945228,
    160.8912744, 336.4018128, 5207.091788, 6360.856306, 1740.091298,
    1588.430761, 3509.141442, 685.6917982, 4664.118976, 1477.26149,
    75.73956465, 402.1570283, 3703.317553, 4235.211434, 1730.465296,
    91.53557346, 334.5397274, 698.1713846, 3328.897126, 1742.69355,
    231.9097243, 513.7933372, 774.6461158, 4687.003829, 1692.296924,
    179.1968506, 1022.628651, 1199.898583, 6132.303567, 1971.798098,
    413.3375988, 4072.908467, 615.911814, 4906.642605, 3160.349616,
    117.642134, 4929.371855, 616.8755006, 7428.352411, 767.2288107,
    767.2288107, 1078.928494, 730.6740868, 425.9053258), Damage..cm. =
c(0.4955,
    1.516, 4.409, 3.2665, 0.491, 2.3035, 3.51, 1.8115, 0, 0.4435,
    1.573, 1.8595, 0, 0.142, 2.171, 4.023, 4.9835, 0, 0.6925,
    1.989, 5.683, 3.547, 0, 0.756, 2.129, 9.437, 3.211, 0, 0.578,
    2.966, 4.7245, 1.8185, 0, 1.0475, 1.62, 5.568, 9.7455, 0,
    0.8295, 2.411, 7.272, 4.516, 0, 0.4035, 2.974, 8.043, 4.809,
    0, 0.6965, 1.313, 5.681, 3.474, 0, 0.5895, 2.559, 0)), .Names =
c("Treatment",
"Code", "Glands", "Tannin", "H.polone", "Gossypol", "Heliocide",
"Damage..cm."), class = "data.frame", row.names = c(NA, -56L))




        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




	[[alternative HTML version deleted]]


From michael.eisenring at gmx.ch  Sat Sep 26 16:46:01 2015
From: michael.eisenring at gmx.ch (Michael Eisenring)
Date: Sat, 26 Sep 2015 07:46:01 -0700
Subject: [R] How to calculate standard error of estimate (S) for my
	non-linear regression model?
In-Reply-To: <582FCB0F-9082-4D83-854B-17F243EEDCBC@gmail.com>
References: <000b01d0f819$710b6f30$53224d90$@gmx.ch>
	<582FCB0F-9082-4D83-854B-17F243EEDCBC@gmail.com>
Message-ID: <000001d0f86a$1189cdd0$349d6970$@gmx.ch>

Dear Peter,
Thank you for your answer.
If I look at my summary I see there a Residual standard error: 1394 on 53
degrees of freedom.
This number is very high (the fit of the curve is pretty bad I know but
still...). Are you sure the residual standard error given in the summary is
the same as the one described on this page:
http://onlinestatbook.com/2/regression/accuracy.html
I am basically just looking for a value that describes the goodness of fit
for my non-linear regression model.


This is probably a pretty obvious question, but I am not a statistician and
as you said the terminology is sometimes pretty confusing.
Thanks mike

-----Urspr?ngliche Nachricht-----
Von: peter dalgaard [mailto:pdalgd at gmail.com] 
Gesendet: Samstag, 26. September 2015 01:43
An: Michael Eisenring <michael.eisenring at gmx.ch>
Cc: r-help at r-project.org
Betreff: Re: [R] How to calculate standard error of estimate (S) for my
non-linear regression model?

This is one area in which terminology in (computational) statistics has gone
a bit crazy. The thing some call "standard error of estimate" is actually
the residual standard deviation in the regression model, not to be confused
with the standard errors that are associated with parameter estimates. In
summary(nls(...)) (and summary(lm()) for that matter), you'll find it as
"residual standard error",  and even that is a bit of a misnomer.

-pd 

> On 26 Sep 2015, at 07:08 , Michael Eisenring <michael.eisenring at gmx.ch>
wrote:
> 
> Hi all,
> 
> I am looking for something that indicates the goodness of fit for my 
> non linear regression model (since R2 is not very reliable).
> 
> I read that the standard error of estimate (also known as standard 
> error of the regression) is a good alternative.
> 
> 
> 
> The standard error of estimate is described on this page (including 
> the
> formula) http://onlinestatbook.com/2/regression/accuracy.html
> <https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fon
> linest atbook.com%2F2%2Fregression%2Faccuracy.html>
> 
> Unfortunately however, I have no clue how to programm it in R. Does 
> anyone know and could help me?
> 
> Thank you very much.
> 
> 
> 
> I added an example of my model and a dput() of my data
> 
> #CODE
> 
> dta<-read.csv("Regression_exp2.csv",header=T, sep = ",")
> attach(dta)      # tells R to do the following analyses on this dataset
> head(dta)
> 
> 
> 
> # loading packages: analysis of mixed effect models 
> library(nls2)#model
> 
> #Aim: fit equation to data: y~yo+a*(1-b^x) : Two parameter exp. single 
> rise to the maximum # y =Gossypol (from my data set) x= Damage_cm 
> (from my data set) #The other 3 parameters are unknown: yo=Intercept, 
> a= assymptote ans b=slope
> 
> plot(Gossypol~Damage_cm, dta)
> # Looking at the plot, 0 is a plausible estimate for y0:
> # a+y0 is the asymptote, so estimate about 4000; # b is between 0 and 
> 1, so estimate .5 dta.nls <- nls(Gossypol~y0+a*(1-b^Damage_cm), dta,
>               start=list(y0=0, a=4000, b=.5))
> 
> xval <- seq(0, 10, 0.1)
> lines(xval, predict(dta.nls, data.frame(Damage_cm=xval))) 
> profile(dta.nls, alpha= .05)
> 
> 
> summary(dta.nls)
> 
> 
> 
> 
> 
> 
> 
> #INPUT
> 
> structure(list(Gossypol = c(948.2418407, 1180.171957, 3589.187889, 
> 450.7205451, 349.0864019, 592.3403778, 723.885643, 2005.919344, 
> 720.9785449, 1247.806111, 1079.846532, 1500.863038, 4198.569251, 
> 3618.448997, 4140.242559, 1036.331811, 1013.807628, 2547.326207, 
> 2508.417927, 2874.651764, 1120.955, 1782.864308, 1517.045807, 
> 2287.228752, 4171.427741, 3130.376482, 1504.491931, 6132.876396, 
> 3350.203452, 5113.942098, 1989.576826, 3470.09352, 4576.787021, 
> 4854.985845, 1414.161257, 2608.716056, 910.8879471, 2228.522959, 
> 2952.931863, 5909.068158, 1247.806111, 6982.035521, 2867.610671, 
> 5629.979049, 6039.995102, 3747.076592, 3743.331903, 4274.324792, 
> 3378.151945, 3736.144027, 5654.858696, 5972.926124, 3723.629772, 
> 3322.115942, 3575.043632, 2818.419785), Treatment = structure(c(5L, 
> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
> 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 
> 3L, 3L, 3L, 2L, 2L, 2L, 4L, 2L, 4L, 4L, 2L, 4L, 2L, 2L, 4L, 4L, 4L, 
> 4L, 4L, 4L, 2L), .Label = c("1c_2d", "1c_7d", "3c_2d", "9c_2d", "C"), 
> class = "factor"), Damage_cm = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0.142, 0.4035, 0.4435, 0.491, 0.4955, 0.578, 0.5895, 0.6925, 0.6965, 
> 0.756, 0.8295, 1.0475, 1.313, 1.516, 1.573, 1.62, 1.8115, 1.8185, 
> 1.8595, 1.989, 2.129, 2.171, 2.3035, 2.411, 2.559, 2.966, 2.974, 
> 3.211, 3.2665, 3.474, 3.51, 3.547, 4.023, 4.409, 4.516, 4.7245, 4.809, 
> 4.9835, 5.568, 5.681, 5.683, 7.272, 8.043, 9.437, 9.7455), 
> Damage_groups = c(0.278, 1.616, 2.501, 3.401, 4.577, 5.644, 7.272, 
> 8.043, 9.591, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), 
> Gossypol_Averaged = c(1783.211, 3244.129, 2866.307, 3991.809, 
> 4468.809, 5121.309, 3723.629772, 3322.115942, 3196.731, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), Groups = c(42006L, 42038L, 
> 42067L, 42099L, 42130L, 42162L, 42193L, 42225L, 42257L, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)), .Names = c("Gossypol", 
> "Treatment", "Damage_cm", "Damage_groups", "Gossypol_Averaged", 
> "Groups"), class = "data.frame", row.names = c(NA, -56L))
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000
Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Sat Sep 26 17:56:26 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 26 Sep 2015 17:56:26 +0200
Subject: [R] How to calculate standard error of estimate (S) for my
	non-linear regression model?
In-Reply-To: <000001d0f86a$1189cdd0$349d6970$@gmx.ch>
References: <000b01d0f819$710b6f30$53224d90$@gmx.ch>
	<582FCB0F-9082-4D83-854B-17F243EEDCBC@gmail.com>
	<000001d0f86a$1189cdd0$349d6970$@gmx.ch>
Message-ID: <48347294-6707-42C4-BF44-B1C064CFAB17@gmail.com>


> On 26 Sep 2015, at 16:46 , Michael Eisenring <michael.eisenring at gmx.ch> wrote:
> 
> Dear Peter,
> Thank you for your answer.
> If I look at my summary I see there a Residual standard error: 1394 on 53
> degrees of freedom.
> This number is very high (the fit of the curve is pretty bad I know but
> still...). Are you sure the residual standard error given in the summary is
> the same as the one described on this page:
> http://onlinestatbook.com/2/regression/accuracy.html

Sure I'm sure (& I did check!)... But notice that unlike R^2, Residual SE is not dimensionless. Switch from millimeters to meters in your response measure and the Residual SE instantly turns into 1.394. It's basically saying that your model claims to be able to predict Y within +/-2800 units. How good or bad that is, is your judgment.

> I am basically just looking for a value that describes the goodness of fit
> for my non-linear regression model.
> 
> 
> This is probably a pretty obvious question, but I am not a statistician and
> as you said the terminology is sometimes pretty confusing.
> Thanks mike
> 
> -----Urspr?ngliche Nachricht-----
> Von: peter dalgaard [mailto:pdalgd at gmail.com] 
> Gesendet: Samstag, 26. September 2015 01:43
> An: Michael Eisenring <michael.eisenring at gmx.ch>
> Cc: r-help at r-project.org
> Betreff: Re: [R] How to calculate standard error of estimate (S) for my
> non-linear regression model?
> 
> This is one area in which terminology in (computational) statistics has gone
> a bit crazy. The thing some call "standard error of estimate" is actually
> the residual standard deviation in the regression model, not to be confused
> with the standard errors that are associated with parameter estimates. In
> summary(nls(...)) (and summary(lm()) for that matter), you'll find it as
> "residual standard error",  and even that is a bit of a misnomer.
> 
> -pd 
> 
>> On 26 Sep 2015, at 07:08 , Michael Eisenring <michael.eisenring at gmx.ch>
> wrote:
>> 
>> Hi all,
>> 
>> I am looking for something that indicates the goodness of fit for my 
>> non linear regression model (since R2 is not very reliable).
>> 
>> I read that the standard error of estimate (also known as standard 
>> error of the regression) is a good alternative.
>> 
>> 
>> 
>> The standard error of estimate is described on this page (including 
>> the
>> formula) http://onlinestatbook.com/2/regression/accuracy.html
>> <https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fon
>> linest atbook.com%2F2%2Fregression%2Faccuracy.html>
>> 
>> Unfortunately however, I have no clue how to programm it in R. Does 
>> anyone know and could help me?
>> 
>> Thank you very much.
>> 
>> 
>> 
>> I added an example of my model and a dput() of my data
>> 
>> #CODE
>> 
>> dta<-read.csv("Regression_exp2.csv",header=T, sep = ",")
>> attach(dta)      # tells R to do the following analyses on this dataset
>> head(dta)
>> 
>> 
>> 
>> # loading packages: analysis of mixed effect models 
>> library(nls2)#model
>> 
>> #Aim: fit equation to data: y~yo+a*(1-b^x) : Two parameter exp. single 
>> rise to the maximum # y =Gossypol (from my data set) x= Damage_cm 
>> (from my data set) #The other 3 parameters are unknown: yo=Intercept, 
>> a= assymptote ans b=slope
>> 
>> plot(Gossypol~Damage_cm, dta)
>> # Looking at the plot, 0 is a plausible estimate for y0:
>> # a+y0 is the asymptote, so estimate about 4000; # b is between 0 and 
>> 1, so estimate .5 dta.nls <- nls(Gossypol~y0+a*(1-b^Damage_cm), dta,
>>              start=list(y0=0, a=4000, b=.5))
>> 
>> xval <- seq(0, 10, 0.1)
>> lines(xval, predict(dta.nls, data.frame(Damage_cm=xval))) 
>> profile(dta.nls, alpha= .05)
>> 
>> 
>> summary(dta.nls)
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> #INPUT
>> 
>> structure(list(Gossypol = c(948.2418407, 1180.171957, 3589.187889, 
>> 450.7205451, 349.0864019, 592.3403778, 723.885643, 2005.919344, 
>> 720.9785449, 1247.806111, 1079.846532, 1500.863038, 4198.569251, 
>> 3618.448997, 4140.242559, 1036.331811, 1013.807628, 2547.326207, 
>> 2508.417927, 2874.651764, 1120.955, 1782.864308, 1517.045807, 
>> 2287.228752, 4171.427741, 3130.376482, 1504.491931, 6132.876396, 
>> 3350.203452, 5113.942098, 1989.576826, 3470.09352, 4576.787021, 
>> 4854.985845, 1414.161257, 2608.716056, 910.8879471, 2228.522959, 
>> 2952.931863, 5909.068158, 1247.806111, 6982.035521, 2867.610671, 
>> 5629.979049, 6039.995102, 3747.076592, 3743.331903, 4274.324792, 
>> 3378.151945, 3736.144027, 5654.858696, 5972.926124, 3723.629772, 
>> 3322.115942, 3575.043632, 2818.419785), Treatment = structure(c(5L, 
>> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
>> 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 
>> 3L, 3L, 3L, 2L, 2L, 2L, 4L, 2L, 4L, 4L, 2L, 4L, 2L, 2L, 4L, 4L, 4L, 
>> 4L, 4L, 4L, 2L), .Label = c("1c_2d", "1c_7d", "3c_2d", "9c_2d", "C"), 
>> class = "factor"), Damage_cm = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
>> 0.142, 0.4035, 0.4435, 0.491, 0.4955, 0.578, 0.5895, 0.6925, 0.6965, 
>> 0.756, 0.8295, 1.0475, 1.313, 1.516, 1.573, 1.62, 1.8115, 1.8185, 
>> 1.8595, 1.989, 2.129, 2.171, 2.3035, 2.411, 2.559, 2.966, 2.974, 
>> 3.211, 3.2665, 3.474, 3.51, 3.547, 4.023, 4.409, 4.516, 4.7245, 4.809, 
>> 4.9835, 5.568, 5.681, 5.683, 7.272, 8.043, 9.437, 9.7455), 
>> Damage_groups = c(0.278, 1.616, 2.501, 3.401, 4.577, 5.644, 7.272, 
>> 8.043, 9.591, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), 
>> Gossypol_Averaged = c(1783.211, 3244.129, 2866.307, 3991.809, 
>> 4468.809, 5121.309, 3723.629772, 3322.115942, 3196.731, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), Groups = c(42006L, 42038L, 
>> 42067L, 42099L, 42130L, 42162L, 42193L, 42225L, 42257L, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)), .Names = c("Gossypol", 
>> "Treatment", "Damage_cm", "Damage_groups", "Gossypol_Averaged", 
>> "Groups"), class = "data.frame", row.names = c(NA, -56L))
>> 
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000
> Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From dwinsemius at comcast.net  Sat Sep 26 18:07:09 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 26 Sep 2015 09:07:09 -0700
Subject: [R] How to get significance codes after Kruskal Wallis test
In-Reply-To: <002c01d0f862$0b622810$22267830$@gmx.ch>
References: <000601d0f818$5e670de0$1b3529a0$@gmx.ch>
	<CAHq6VpdWyXW2A94jx6_uBMKh7FUEhht8iJf1aw=0phSRWupCnw@mail.gmail.com>
	<002c01d0f862$0b622810$22267830$@gmx.ch>
Message-ID: <BEE39710-7924-4A47-B6F6-092A61EBAF06@comcast.net>


On Sep 26, 2015, at 6:48 AM, Michael Eisenring wrote:

> Thank you very much Kristina,
> 
> Unfortunately that?s not what I am looking for.
> 
> I am just very surprised if there would be no possibility to get the significance codes for Kruskal Wallis (I would have suggested that this is a pretty common test.)

Well, it is modestly common test but its really a global test, and not a pairwise one.

> I found another option called kruskal() which does pairwise comparison, but without significance codes.
> 
> Maybe another R-list member knows more.
> 

I'm not sure I "know more", and it's very possible I "know less". In particular I don't really know what the term "significance code" actually means. (I'm hoping it's not a request for "significance stars", a feature which is roundly deprecated by more knowledgeable R users.) 

However, looking at the agricolae::kruskal function's help page and submitting the data in the non-formulaic manner it expects, I get this output very similar in form the the HSD.test output that it appeared you considered satisfied satisfactory:

(an.dta3<-kruskal(dta$Heliocide, dta$Treatment))

#--------------------
$statistics
     Chisq      p.chisq
  30.25246 4.348055e-06

$parameters
  Df ntr  t.value
   4   5 2.007584

$means
     dta$Heliocide       std  r        Min      Max
1_2d     1992.7707 1747.1879 12  334.53973 4929.372
1_7d     2368.8057 1187.9285 11  767.22881 4624.945
3_2d     2640.1286 2659.5800 12  615.91181 8559.142
9_2d     5338.6711 1579.4428 10 3328.89713 8014.897
C         397.9086  443.6019 11   75.73956 1588.431

$rankMeans
  dta$Treatment dta$Heliocide  r
1          1_2d     26.000000 12
2          1_7d     32.045455 11
3          3_2d     29.833333 12
4          9_2d     47.500000 10
5             C      8.954545 11

$comparison
NULL

$groups
   trt     means M
1 9_2d 47.500000 a
2 1_7d 32.045455 b
3 3_2d 29.833333 b
4 1_2d 26.000000 b
5 C     8.954545 c

From context I am guessing that the "significance codes" you ask for are the items in the M column of the "groups" element of the list output.

-- 
David.

> 
> 
> Thank you,
> 
> Mike
> 
> 
> 
> Von: Kristina Wolf [mailto:kmwolf at ucdavis.edu] 
> Gesendet: Freitag, 25. September 2015 23:26
> An: Michael Eisenring <michael.eisenring at gmx.ch>
> Cc: r-help <r-help at r-project.org>
> Betreff: Re: [R] How to get significance codes after Kruskal Wallis test
> 
> 
> 
> Perhaps look into the function friedman.test.with.post.hoc()
> 
> There is more information here: http://www.r-statistics.com/wp-content/uploads/2010/02/Friedman-Test-with-Post-Hoc.r.txt
> 
> 
> 
> Note, this does not handle NA's though, and technically it is for blocked designs, but maybe it will lead you somewhere useful or could be adapted? 
> 
> 
> ~ Kristina
> 
> Kristina Wolf
> Ph.D. Candidate, Graduate Group in Ecology
> M.S. Soil Science
> 
> 
> 
> On Fri, Sep 25, 2015 at 10:01 PM, Michael Eisenring <michael.eisenring at gmx.ch <mailto:michael.eisenring at gmx.ch> > wrote:
> 
> Is there a way to get significance codes after a pairwise comparisons to a
> Kruskall wallis test? With significance codes I mean letter codes (a, b,c)
> that are assigned to treatments to indicate where differences are
> significant.
> 
> With a traditional anova such a test can be performed using HSD.test from
> the agricolae library but for non parametric counterparts of anova I have
> not been able to find anything.
> 
> Can anyone help me?
> 
> Thanks mike
> 
> 
> 
> I added two example codes.
> 
> First code  represents an ANOVA and a HSD.test() giving me significant codes
> 
> #FIRST CODE USING ANOVA
> 
> library(agricolae)
> an.dta<-aov(Gossypol~Treatment,data=dta)
> summary(an.dta)
> 
> HSD.test(an.dta,"Treatment")
> # The level by alpha default is 0.05.
> outT<-HSD.test(an.dta,"Treatment", group=T)
> outT
> 
> #I receive significant codes.
> 
> 
> #SECOND CODE USING KRUSKAL WALLIs
> 
> library(agricolae)
> an.dta2<-kruskal.test(Heliocide~Treatment,dta)
> summary(an.dta2)
> 
> HSD.test(an.dta2,"Treatment")
> 
> #ERROR MESSAGE no significance codes, why??
> 
> 
> 
> #DATA FOR CODES
> 
> 
> structure(list(Treatment = structure(c(1L, 3L, 4L, 2L, 1L, 3L,
> 4L, 2L, 5L, 1L, 3L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L,
> 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L,
> 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L,
> 3L, 5L), .Label = c("1_2d", "1_7d", "3_2d", "9_2d", "C"), class = "factor"),
> 
>    Code = structure(c(1L, 2L, 3L, 4L, 18L, 19L, 20L, 21L, 22L,
>    23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L,
>    35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 44L, 45L, 46L,
>    47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 56L, 5L, 6L,
>    7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L), .Label =
> c("1_2d_1c",
>    "1_2d_3c", "1_2d_9c", "1_7d_1c", "10_2d_1c", "10_2d_3c",
>    "10_2d_9c", "10_7d_1c", "10_C", "11_2d_1c", "11_2d_3c", "11_2d_9c",
>    "11_7d_1c", "11_C", "12_2d_1c", "12_2d_3c", "12_C", "2_2d_1c",
>    "2_2d_3c", "2_2d_9c", "2_7d_1c", "2_C", "3_2d_1c", "3_2d_3c",
>    "3_7d_1c", "3_C", "4_2d_1c", "4_2d_3c", "4_2d_9c", "4_7d_1c",
>    "4_C", "5_2d_1c", "5_2d_3c", "5_2d_9c", "5_7d_1c", "5_C",
>    "6_2d_1c", "6_2d_3c", "6_2d_9c", "6_7d_1c", "6_C", "7_2d_1c",
>    "7_2d_3c", "7_2d_9c", "7_7d_1c", "7_C", "8_2d_1c", "8_2d_3c",
>    "8_2d_9c", "8_7d_1c", "8_C", "9_2d_1c", "9_2d_3c", "9_2d_9c",
>    "9_7d_1c", "9_C"), class = "factor"), Glands = c(165, 289.3333333,
>    319.3333333, 472, 334.6666667, 259, 373.3333333, 525.6666667,
>    275.3333333, 230.6666667, 346.3333333, 377.6666667, 255.3333333,
>    217.6666667, 266, 300.3333333, 354.3333333, 225.3333333,
>    294, 359, 359, 222.6666667, 103, 246.6666667, 324.6666667,
>    277, 460, 163.6666667, 226.3333333, 228, 357.6666667, 505,
>    142.6666667, 324, 278.6666667, 317.3333333, 335.6666667,
>    193.6666667, 188, 255, 252, 393.3333333, 248.3333333, 353,
>    320.6666667, 228.3333333, 497, 165.6666667, 209.3333333,
>    162.3333333, 280, 337, 169.6666667, 231.6666667, 257.6666667,
>    218.6666667), Tannin = c(0.334252451, 1.376077586, 0.896849593,
>    0.888621795, 0.464285714, 0.830236486, 0.870881783, 0.768489583,
>    0.647727273, 0.81372549, 0.51380814, 0.859923246, 0.495265152,
>    0.699932796, 1.09375, 0.785037879, 0.892650463, 0.518963675,
>    1.05859375, 0.447916667, 1.269097222, 1.147522523, 0.391276042,
>    0.883400538, 1.523989899, 0.907930108, 0.749155405, 0.450126263,
>    0.562239583, 0.911151961, 0.611111111, 1.610677083, 0.446428571,
>    0.601151316, 1.073635057, 1.359923246, 1.00154321, 0.90933642,
>    0.012054398, 1.102083333, 1.017361111, 1.052372685, 0.958607456,
>    1.224702381, 0.982291667, 1.045138889, 1.611607143, 0.662574405,
>    1.385416667, 0.464518229, 0.994444444, 1.239583333, 0.877514368,
>    0.74453125, 0.804315476, 1.024066092), H.polone = c(6754.067177,
>    22380.26652, 23622.79158, 23733.77678, 13099.20833, 23564.74907,
>    2725.016387, 18751.03986, 4283.098494, 23008.35336, 10205.56354,
>    19787.63361, 4302.050374, 7400.640798, 22442.86044, 34315.09631,
>    16498.66728, 14170.13252, 9509.1073, 6265.29637, 20671.56905,
>    14517.15648, 2643.950729, 4974.607571, 14782.87029, 13918.82361,
>    12526.27863, 1236.908141, 4854.469195, 4076.396504, 9603.950212,
>    13762.57476, 2298.727719, 3514.186757, 5705.140289, 14178.21668,
>    14277.39878, 2656.552509, 8184.633961, 9931.163373, 21474.90732,
>    18522.74376, 9884.406532, 17242.54114, 8431.506608, 14601.11606,
>    15748.4912, 2849.90903, 16747.27644, 9396.645481, 21996.95822,
>    5767.358748, 5767.358748, 14207.1734, 10353.21833, 2859.51171
>    ), Gossypol = c(1036.331811, 4171.427741, 6039.995102, 5909.068158,
>    4140.242559, 4854.985845, 6982.035521, 6132.876396, 948.2418407,
>    3618.448997, 3130.376482, 5113.942098, 1180.171957, 1500.863038,
>    4576.787021, 5629.979049, 3378.151945, 3589.187889, 2508.417927,
>    1989.576826, 5972.926124, 2867.610671, 450.7205451, 1120.955,
>    3470.09352, 3575.043632, 2952.931863, 349.0864019, 1013.807628,
>    910.8879471, 3743.331903, 3350.203452, 592.3403778, 1517.045807,
>    1504.491931, 3736.144027, 2818.419785, 723.885643, 1782.864308,
>    1414.161257, 3723.629772, 3747.076592, 2005.919344, 4198.569251,
>    2228.522959, 3322.115942, 4274.324792, 720.9785449, 2874.651764,
>    2287.228752, 5654.858696, 1247.806111, 1247.806111, 2547.326207,
>    2608.716056, 1079.846532), Heliocide = c(711.1776124, 8559.141828,
>    8014.897387, 3972.305107, 3227.467943, 5778.242027, 3628.427557,
>    3177.426984, 325.1764586, 3774.732152, 3111.880146, 4624.945228,
>    160.8912744, 336.4018128, 5207.091788, 6360.856306, 1740.091298,
>    1588.430761, 3509.141442, 685.6917982, 4664.118976, 1477.26149,
>    75.73956465, 402.1570283, 3703.317553, 4235.211434, 1730.465296,
>    91.53557346, 334.5397274, 698.1713846, 3328.897126, 1742.69355,
>    231.9097243, 513.7933372, 774.6461158, 4687.003829, 1692.296924,
>    179.1968506, 1022.628651, 1199.898583, 6132.303567, 1971.798098,
>    413.3375988, 4072.908467, 615.911814, 4906.642605, 3160.349616,
>    117.642134, 4929.371855, 616.8755006, 7428.352411, 767.2288107,
>    767.2288107, 1078.928494, 730.6740868, 425.9053258), Damage..cm. =
> c(0.4955,
>    1.516, 4.409, 3.2665, 0.491, 2.3035, 3.51, 1.8115, 0, 0.4435,
>    1.573, 1.8595, 0, 0.142, 2.171, 4.023, 4.9835, 0, 0.6925,
>    1.989, 5.683, 3.547, 0, 0.756, 2.129, 9.437, 3.211, 0, 0.578,
>    2.966, 4.7245, 1.8185, 0, 1.0475, 1.62, 5.568, 9.7455, 0,
>    0.8295, 2.411, 7.272, 4.516, 0, 0.4035, 2.974, 8.043, 4.809,
>    0, 0.6965, 1.313, 5.681, 3.474, 0, 0.5895, 2.559, 0)), .Names =
> c("Treatment",
> "Code", "Glands", "Tannin", "H.polone", "Gossypol", "Heliocide",
> "Damage..cm."), class = "data.frame", row.names = c(NA, -56L))
> 
> 
> 	[[alternative HTML version deleted]]
> 

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Sat Sep 26 18:20:29 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 26 Sep 2015 09:20:29 -0700
Subject: [R] How to calculate standard error of estimate (S) for my
 non-linear regression model?
In-Reply-To: <48347294-6707-42C4-BF44-B1C064CFAB17@gmail.com>
References: <000b01d0f819$710b6f30$53224d90$@gmx.ch>
	<582FCB0F-9082-4D83-854B-17F243EEDCBC@gmail.com>
	<000001d0f86a$1189cdd0$349d6970$@gmx.ch>
	<48347294-6707-42C4-BF44-B1C064CFAB17@gmail.com>
Message-ID: <CAGxFJbS6_-Q7xLqaoMrnNWeJ_QhcsPxdH=7hCRDWTQZ6QMjk3w@mail.gmail.com>

Michael:

You appear to be laboring under the illusion that a single numeric
summary (**any summary**)is a useful measure of model adequacy. It is
not; for details about why not, consult any applied statistics text
(e.g. on regression) and/or post on a statistics site, like
stats.stackexchange.com. Better yet, consult a local statistician.

Incidentally, this is even more the case for NON-linear models. Again,
consult appropriate statistical resources. Even googling on "R^2
inadequate for nonlinear models" brought up some interesting
resources, among them:

http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892436/

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Sep 26, 2015 at 8:56 AM, peter dalgaard <pdalgd at gmail.com> wrote:
>
>> On 26 Sep 2015, at 16:46 , Michael Eisenring <michael.eisenring at gmx.ch> wrote:
>>
>> Dear Peter,
>> Thank you for your answer.
>> If I look at my summary I see there a Residual standard error: 1394 on 53
>> degrees of freedom.
>> This number is very high (the fit of the curve is pretty bad I know but
>> still...). Are you sure the residual standard error given in the summary is
>> the same as the one described on this page:
>> http://onlinestatbook.com/2/regression/accuracy.html
>
> Sure I'm sure (& I did check!)... But notice that unlike R^2, Residual SE is not dimensionless. Switch from millimeters to meters in your response measure and the Residual SE instantly turns into 1.394. It's basically saying that your model claims to be able to predict Y within +/-2800 units. How good or bad that is, is your judgment.
>
>> I am basically just looking for a value that describes the goodness of fit
>> for my non-linear regression model.
>>
>>
>> This is probably a pretty obvious question, but I am not a statistician and
>> as you said the terminology is sometimes pretty confusing.
>> Thanks mike
>>
>> -----Urspr?ngliche Nachricht-----
>> Von: peter dalgaard [mailto:pdalgd at gmail.com]
>> Gesendet: Samstag, 26. September 2015 01:43
>> An: Michael Eisenring <michael.eisenring at gmx.ch>
>> Cc: r-help at r-project.org
>> Betreff: Re: [R] How to calculate standard error of estimate (S) for my
>> non-linear regression model?
>>
>> This is one area in which terminology in (computational) statistics has gone
>> a bit crazy. The thing some call "standard error of estimate" is actually
>> the residual standard deviation in the regression model, not to be confused
>> with the standard errors that are associated with parameter estimates. In
>> summary(nls(...)) (and summary(lm()) for that matter), you'll find it as
>> "residual standard error",  and even that is a bit of a misnomer.
>>
>> -pd
>>
>>> On 26 Sep 2015, at 07:08 , Michael Eisenring <michael.eisenring at gmx.ch>
>> wrote:
>>>
>>> Hi all,
>>>
>>> I am looking for something that indicates the goodness of fit for my
>>> non linear regression model (since R2 is not very reliable).
>>>
>>> I read that the standard error of estimate (also known as standard
>>> error of the regression) is a good alternative.
>>>
>>>
>>>
>>> The standard error of estimate is described on this page (including
>>> the
>>> formula) http://onlinestatbook.com/2/regression/accuracy.html
>>> <https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fon
>>> linest atbook.com%2F2%2Fregression%2Faccuracy.html>
>>>
>>> Unfortunately however, I have no clue how to programm it in R. Does
>>> anyone know and could help me?
>>>
>>> Thank you very much.
>>>
>>>
>>>
>>> I added an example of my model and a dput() of my data
>>>
>>> #CODE
>>>
>>> dta<-read.csv("Regression_exp2.csv",header=T, sep = ",")
>>> attach(dta)      # tells R to do the following analyses on this dataset
>>> head(dta)
>>>
>>>
>>>
>>> # loading packages: analysis of mixed effect models
>>> library(nls2)#model
>>>
>>> #Aim: fit equation to data: y~yo+a*(1-b^x) : Two parameter exp. single
>>> rise to the maximum # y =Gossypol (from my data set) x= Damage_cm
>>> (from my data set) #The other 3 parameters are unknown: yo=Intercept,
>>> a= assymptote ans b=slope
>>>
>>> plot(Gossypol~Damage_cm, dta)
>>> # Looking at the plot, 0 is a plausible estimate for y0:
>>> # a+y0 is the asymptote, so estimate about 4000; # b is between 0 and
>>> 1, so estimate .5 dta.nls <- nls(Gossypol~y0+a*(1-b^Damage_cm), dta,
>>>              start=list(y0=0, a=4000, b=.5))
>>>
>>> xval <- seq(0, 10, 0.1)
>>> lines(xval, predict(dta.nls, data.frame(Damage_cm=xval)))
>>> profile(dta.nls, alpha= .05)
>>>
>>>
>>> summary(dta.nls)
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> #INPUT
>>>
>>> structure(list(Gossypol = c(948.2418407, 1180.171957, 3589.187889,
>>> 450.7205451, 349.0864019, 592.3403778, 723.885643, 2005.919344,
>>> 720.9785449, 1247.806111, 1079.846532, 1500.863038, 4198.569251,
>>> 3618.448997, 4140.242559, 1036.331811, 1013.807628, 2547.326207,
>>> 2508.417927, 2874.651764, 1120.955, 1782.864308, 1517.045807,
>>> 2287.228752, 4171.427741, 3130.376482, 1504.491931, 6132.876396,
>>> 3350.203452, 5113.942098, 1989.576826, 3470.09352, 4576.787021,
>>> 4854.985845, 1414.161257, 2608.716056, 910.8879471, 2228.522959,
>>> 2952.931863, 5909.068158, 1247.806111, 6982.035521, 2867.610671,
>>> 5629.979049, 6039.995102, 3747.076592, 3743.331903, 4274.324792,
>>> 3378.151945, 3736.144027, 5654.858696, 5972.926124, 3723.629772,
>>> 3322.115942, 3575.043632, 2818.419785), Treatment = structure(c(5L,
>>> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>>> 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L,
>>> 3L, 3L, 3L, 2L, 2L, 2L, 4L, 2L, 4L, 4L, 2L, 4L, 2L, 2L, 4L, 4L, 4L,
>>> 4L, 4L, 4L, 2L), .Label = c("1c_2d", "1c_7d", "3c_2d", "9c_2d", "C"),
>>> class = "factor"), Damage_cm = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>>> 0.142, 0.4035, 0.4435, 0.491, 0.4955, 0.578, 0.5895, 0.6925, 0.6965,
>>> 0.756, 0.8295, 1.0475, 1.313, 1.516, 1.573, 1.62, 1.8115, 1.8185,
>>> 1.8595, 1.989, 2.129, 2.171, 2.3035, 2.411, 2.559, 2.966, 2.974,
>>> 3.211, 3.2665, 3.474, 3.51, 3.547, 4.023, 4.409, 4.516, 4.7245, 4.809,
>>> 4.9835, 5.568, 5.681, 5.683, 7.272, 8.043, 9.437, 9.7455),
>>> Damage_groups = c(0.278, 1.616, 2.501, 3.401, 4.577, 5.644, 7.272,
>>> 8.043, 9.591, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA),
>>> Gossypol_Averaged = c(1783.211, 3244.129, 2866.307, 3991.809,
>>> 4468.809, 5121.309, 3723.629772, 3322.115942, 3196.731, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), Groups = c(42006L, 42038L,
>>> 42067L, 42099L, 42130L, 42162L, 42193L, 42225L, 42257L, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)), .Names = c("Gossypol",
>>> "Treatment", "Damage_cm", "Damage_groups", "Gossypol_Averaged",
>>> "Groups"), class = "data.frame", row.names = c(NA, -56L))
>>>
>>>
>>>
>>>
>>>      [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000
>> Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>>
>>
>>
>>
>>
>>
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sat Sep 26 22:26:34 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 26 Sep 2015 13:26:34 -0700 (PDT)
Subject: [R] How to calculate standard error of estimate (S) for my
 non-linear regression model?
In-Reply-To: <000001d0f86a$1189cdd0$349d6970$@gmx.ch>
References: <000b01d0f819$710b6f30$53224d90$@gmx.ch>
	<582FCB0F-9082-4D83-854B-17F243EEDCBC@gmail.com>
	<000001d0f86a$1189cdd0$349d6970$@gmx.ch>
Message-ID: <alpine.BSF.2.00.1509261300280.89405@pedal.dcn.davis.ca.us>

You may not be a statistician, but you should at least learn about the 
calculations you are making. You cannot expect to convince others that 
your calculations are right just because "Peter on the internet said they 
were right".

To give you a gentle push in this direction, I have reproduced the 
calculations on that reference web page using R, so you can get a head 
start on understanding how to perform them on your data. (Hint: about all 
you should need to do is give your dta.nls to predict and use your 
column names instead of X and Y.) Once you have convinced yourself that
the pre-defined functions are doing what you expect, then you can omit the 
do-it-yourself option with confidence in the results.

Please note that no use of attach is included here... that usually ends in 
unhappiness at some point, so prefer to use the data argument instead.

###
dta <- data.frame( X=c(1,2,3,4,5), Y=c(1,2,1.3,3.75,2.25) )
nrow( dta )
# linear regression
y.lm <- lm( Y~X, data=dta )
# compute predictions from original data
## Be aware that you can also "predict" using a nonlinear regression fit
## Also be aware that you can compute estimates using different data if you
## specify the "newdata" argument... see the help for predict.lm
## ?predict.lm
dta$Yprime <- predict( y.lm )
# ?with
dta$YlessYprime <- with( dta, Y - Yprime )
dta$YlessYprime2 <- with( dta, YlessYprime^2 )

# confirm sums
# ?sum
sum( dta$X )
sum( dta$Y )
sum( dta$Yprime )
sum( dta$YlessYprime )
sum( dta$YlessYprime2 )

# standard error of the estimate for population data
sigma.est <- sqrt( sum( dta$YlessYprime2 ) / nrow( dta ) )
sigma.est
# sd function assumes sample standard deviation, can correct the result if 
# you want
# ?sd
# ?all.equal
all.equal( sigma.est, sd( dta$YlessYprime ) * sqrt( ( nrow( dta ) - 1 ) / 
nrow( dta ) ) )

# alternate formulation
SSY <- sum( ( dta$Y - mean( dta$Y ) )^2 )
rho <- with( dta, cor( Y, X ) )
all.equal( sigma.est, sqrt( (1-rho^2)*SSY/nrow(dta) ) )

# when working with a sample...
s.est <- sqrt( sum( dta$YlessYprime2 ) / ( nrow( dta ) - 2 ) )
s.est

####
> dta <- data.frame( X=c(1,2,3,4,5), Y=c(1,2,1.3,3.75,2.25) )
> nrow( dta )
[1] 5
> # linear regression
> y.lm <- lm( Y~X, data=dta )
> # compute predictions from original data
> ## Be aware that you can also "predict" using a nonlinear regression fit
> ## Also be aware that you can compute estimates using different data if you
> ## specify the "newdata" argument... see the help for predict.lm
> ## ?predict.lm
> dta$Yprime <- predict( y.lm )
> # ?with
> dta$YlessYprime <- with( dta, Y - Yprime )
> dta$YlessYprime2 <- with( dta, YlessYprime^2 )
>
> # confirm sums
> # ?sum
> sum( dta$X )
[1] 15
> sum( dta$Y )
[1] 10.3
> sum( dta$Yprime )
[1] 10.3
> sum( dta$YlessYprime )
[1] 2.220446e-16
> sum( dta$YlessYprime2 )
[1] 2.79075
>
> # standard error of the estimate for population data
> sigma.est <- sqrt( sum( dta$YlessYprime2 ) / nrow( dta ) )
> sigma.est
[1] 0.7470944
> # sd function assumes sample standard deviation, can correct the result 
> # if you want
> # ?sd
> # ?all.equal
> all.equal( sigma.est, sd( dta$YlessYprime ) * sqrt( ( nrow( dta ) - 1 ) 
/ nrow( dta ) ) )
[1] TRUE
>
> # alternate formulation
> SSY <- sum( ( dta$Y - mean( dta$Y ) )^2 )
> rho <- with( dta, cor( Y, X ) )
> all.equal( sigma.est, sqrt( (1-rho^2)*SSY/nrow(dta) ) )
[1] TRUE
>
> # when working with a sample...
> s.est <- sqrt( sum( dta$YlessYprime2 ) / ( nrow( dta ) - 2 ) )
> s.est
[1] 0.9644947
>


On Sat, 26 Sep 2015, Michael Eisenring wrote:

> Dear Peter,
> Thank you for your answer.
> If I look at my summary I see there a Residual standard error: 1394 on 53
> degrees of freedom.
> This number is very high (the fit of the curve is pretty bad I know but
> still...). Are you sure the residual standard error given in the summary is
> the same as the one described on this page:
> http://onlinestatbook.com/2/regression/accuracy.html
> I am basically just looking for a value that describes the goodness of fit
> for my non-linear regression model.
>
>
> This is probably a pretty obvious question, but I am not a statistician and
> as you said the terminology is sometimes pretty confusing.
> Thanks mike
>
> -----Urspr?ngliche Nachricht-----
> Von: peter dalgaard [mailto:pdalgd at gmail.com]
> Gesendet: Samstag, 26. September 2015 01:43
> An: Michael Eisenring <michael.eisenring at gmx.ch>
> Cc: r-help at r-project.org
> Betreff: Re: [R] How to calculate standard error of estimate (S) for my
> non-linear regression model?
>
> This is one area in which terminology in (computational) statistics has gone
> a bit crazy. The thing some call "standard error of estimate" is actually
> the residual standard deviation in the regression model, not to be confused
> with the standard errors that are associated with parameter estimates. In
> summary(nls(...)) (and summary(lm()) for that matter), you'll find it as
> "residual standard error",  and even that is a bit of a misnomer.
>
> -pd
>
>> On 26 Sep 2015, at 07:08 , Michael Eisenring <michael.eisenring at gmx.ch>
> wrote:
>>
>> Hi all,
>>
>> I am looking for something that indicates the goodness of fit for my
>> non linear regression model (since R2 is not very reliable).
>>
>> I read that the standard error of estimate (also known as standard
>> error of the regression) is a good alternative.
>>
>>
>>
>> The standard error of estimate is described on this page (including
>> the
>> formula) http://onlinestatbook.com/2/regression/accuracy.html
>> <https://3c.gmx.net/mail/client/dereferrer?redirectUrl=http%3A%2F%2Fon
>> linest atbook.com%2F2%2Fregression%2Faccuracy.html>
>>
>> Unfortunately however, I have no clue how to programm it in R. Does
>> anyone know and could help me?
>>
>> Thank you very much.
>>
>>
>>
>> I added an example of my model and a dput() of my data
>>
>> #CODE
>>
>> dta<-read.csv("Regression_exp2.csv",header=T, sep = ",")
>> attach(dta)      # tells R to do the following analyses on this dataset
>> head(dta)
>>
>>
>>
>> # loading packages: analysis of mixed effect models
>> library(nls2)#model
>>
>> #Aim: fit equation to data: y~yo+a*(1-b^x) : Two parameter exp. single
>> rise to the maximum # y =Gossypol (from my data set) x= Damage_cm
>> (from my data set) #The other 3 parameters are unknown: yo=Intercept,
>> a= assymptote ans b=slope
>>
>> plot(Gossypol~Damage_cm, dta)
>> # Looking at the plot, 0 is a plausible estimate for y0:
>> # a+y0 is the asymptote, so estimate about 4000; # b is between 0 and
>> 1, so estimate .5 dta.nls <- nls(Gossypol~y0+a*(1-b^Damage_cm), dta,
>>               start=list(y0=0, a=4000, b=.5))
>>
>> xval <- seq(0, 10, 0.1)
>> lines(xval, predict(dta.nls, data.frame(Damage_cm=xval)))
>> profile(dta.nls, alpha= .05)
>>
>>
>> summary(dta.nls)
>>
>>
>>
>>
>>
>>
>>
>> #INPUT
>>
>> structure(list(Gossypol = c(948.2418407, 1180.171957, 3589.187889,
>> 450.7205451, 349.0864019, 592.3403778, 723.885643, 2005.919344,
>> 720.9785449, 1247.806111, 1079.846532, 1500.863038, 4198.569251,
>> 3618.448997, 4140.242559, 1036.331811, 1013.807628, 2547.326207,
>> 2508.417927, 2874.651764, 1120.955, 1782.864308, 1517.045807,
>> 2287.228752, 4171.427741, 3130.376482, 1504.491931, 6132.876396,
>> 3350.203452, 5113.942098, 1989.576826, 3470.09352, 4576.787021,
>> 4854.985845, 1414.161257, 2608.716056, 910.8879471, 2228.522959,
>> 2952.931863, 5909.068158, 1247.806111, 6982.035521, 2867.610671,
>> 5629.979049, 6039.995102, 3747.076592, 3743.331903, 4274.324792,
>> 3378.151945, 3736.144027, 5654.858696, 5972.926124, 3723.629772,
>> 3322.115942, 3575.043632, 2818.419785), Treatment = structure(c(5L,
>> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L,
>> 3L, 3L, 3L, 2L, 2L, 2L, 4L, 2L, 4L, 4L, 2L, 4L, 2L, 2L, 4L, 4L, 4L,
>> 4L, 4L, 4L, 2L), .Label = c("1c_2d", "1c_7d", "3c_2d", "9c_2d", "C"),
>> class = "factor"), Damage_cm = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> 0.142, 0.4035, 0.4435, 0.491, 0.4955, 0.578, 0.5895, 0.6925, 0.6965,
>> 0.756, 0.8295, 1.0475, 1.313, 1.516, 1.573, 1.62, 1.8115, 1.8185,
>> 1.8595, 1.989, 2.129, 2.171, 2.3035, 2.411, 2.559, 2.966, 2.974,
>> 3.211, 3.2665, 3.474, 3.51, 3.547, 4.023, 4.409, 4.516, 4.7245, 4.809,
>> 4.9835, 5.568, 5.681, 5.683, 7.272, 8.043, 9.437, 9.7455),
>> Damage_groups = c(0.278, 1.616, 2.501, 3.401, 4.577, 5.644, 7.272,
>> 8.043, 9.591, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA),
>> Gossypol_Averaged = c(1783.211, 3244.129, 2866.307, 3991.809,
>> 4468.809, 5121.309, 3723.629772, 3322.115942, 3196.731, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), Groups = c(42006L, 42038L,
>> 42067L, 42099L, 42130L, 42162L, 42193L, 42225L, 42257L, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)), .Names = c("Gossypol",
>> "Treatment", "Damage_cm", "Damage_groups", "Gossypol_Averaged",
>> "Groups"), class = "data.frame", row.names = c(NA, -56L))
>>
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000
> Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From giorgio.garziano at tin.it  Sat Sep 26 18:43:44 2015
From: giorgio.garziano at tin.it (giorgio.garziano at tin.it)
Date: Sat, 26 Sep 2015 18:43:44 +0200 (CEST)
Subject: [R] Analysis of causal relations between rare (categorical)
 events
Message-ID: <1500a89f26c.giorgio.garziano@tin.it>

Also:

https://cran.r-project.org/web/views/Survival.html
https://www.youtube.com/watch?v=1Mt7EuVJf1A

http://gking.harvard.edu/files/gking/files/0s.pdf?m=1360039053
http://biostat.mc.vanderbilt.edu/wiki/pub/Main/RmS/rms.pdf

---
Giorgio Garziano


From haidaharis at gmail.com  Sat Sep 26 18:59:33 2015
From: haidaharis at gmail.com (Haida)
Date: Sun, 27 Sep 2015 00:59:33 +0800
Subject: [R] Rattle installation
Message-ID: <CABjPMZ3Lmmt2ENafk-Qv3qSh5FPXhCxOHi0-9cKFGXja+rzJ2w@mail.gmail.com>

I received below messages during the installation Rattle. please help,
thank you

Warning in install.packages :
  error 1 in extracting from zip file
Warning in install.packages :
  cannot open compressed file 'rJava/DESCRIPTION', probable reason 'No such
file or directory'
Error in install.packages : cannot open the connection

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Sep 26 23:43:04 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 26 Sep 2015 14:43:04 -0700
Subject: [R] Rattle installation
In-Reply-To: <CABjPMZ3Lmmt2ENafk-Qv3qSh5FPXhCxOHi0-9cKFGXja+rzJ2w@mail.gmail.com>
References: <CABjPMZ3Lmmt2ENafk-Qv3qSh5FPXhCxOHi0-9cKFGXja+rzJ2w@mail.gmail.com>
Message-ID: <1043E3A5-0F12-47DB-8DAF-01A2D5E873E0@comcast.net>


On Sep 26, 2015, at 9:59 AM, Haida wrote:

> I received below messages during the installation Rattle. please help,
> thank you
> 
> Warning in install.packages :
>  error 1 in extracting from zip file
> Warning in install.packages :
>  cannot open compressed file 'rJava/DESCRIPTION', probable reason 'No such
> file or directory'
> Error in install.packages : cannot open the connection

Off-hand it would appear you have not yet installed rJava.


> 
> 	[[alternative HTML version deleted]]
> 

You should read the Posting guide.

> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bgnumis at gmail.com  Sat Sep 26 23:31:36 2015
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Sat, 26 Sep 2015 23:31:36 +0200
Subject: [R] Line in hist count
Message-ID: <CAN25tHR_PBSyw4ssKFwj8Hmho65V=E5zMKaApyW+jHdAis_-UA@mail.gmail.com>

Hi all,

Several time ago I used to work with R, now I?m returning to study and work
and searching old file I see that I used this code:


gfhist<-hist(gf,plot=FALSE)

par(mar=c(6,0,6,6))

barplot(gfhist$counts,axes=FALSE, space=0,horiz=TRUE,col="lightgray")

grid()

title("Marginal Distribution Lagged",font=4)

The thing is I would line to plot a bar (horizontal and thing bar that will
be placed on the last gf data but on the barplot

?Do you think is it possible? gf is a matrix.

	[[alternative HTML version deleted]]


From p_connolly at slingshot.co.nz  Sun Sep 27 11:00:30 2015
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Sun, 27 Sep 2015 22:00:30 +1300
Subject: [R] Special characters in regular expressions
In-Reply-To: <2C4D9A7C-5261-4DE9-B947-5A4011107965@gmail.com>
References: <20150924095204.GC5022@slingshot.co.nz>
	<CAJuCY5wU4uWBs6m2m1Rjnqj_K4=ofr0C4wSDYKF2aQWd5Y2wjw@mail.gmail.com>
	<2C4D9A7C-5261-4DE9-B947-5A4011107965@gmail.com>
Message-ID: <20150927090030.GD5022@slingshot.co.nz>

On Thu, 24-Sep-2015 at 12:38PM +0200, peter dalgaard wrote:

|> 
|> On 24 Sep 2015, at 12:05 , Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
|> 
|> > gsub("[A|K]\\|", "", x)
|> 

|> That'll probably do it, but what was the point of the | in [A|K] ??
|> I don't think it does what I think you think it does...

|> Somewhat safer, maybe:
|> 
|> gsub("\\|[AK]\\|","\\|", x)
|> 
|> (avoids surprises from, say, "LBAM 5|A|15A|3h")

Thanks for that suggestion.  Very simple now.

|> 
|> -pd
|> 
|> > [snip]
|> > 2015-09-24 11:52 GMT+02:00 Patrick Connolly <p_connolly at slingshot.co.nz>:
|> > 
|> >> I need to change a vector dd that looks like this:
|> >> c("LBAM 5|A|15C|3h", "LBAM 5|K|15C|2h")
|> >> 
|> >> into this:
|> >> c("LBAM 5|15C|3h", "LBAM 5|15C|2h")
|> >> 
|> >> It's not very imaginative, but I could use a complicated nesting of
|> >> gsub() as so:
|> >> 
|> >> gsub("-", "\\|", gsub("K-", "", gsub("A-", "", gsub("\\|", "-", dd))))
|> >> 
|> >> Or I could make it a bit more readable by using interim objects,
|> >> 
|> >> But I'd prefer to use a single regular expression that can detect "A|"
|> >> *and* "K|" without collateral damage from the impact of special
|> >> characters and regular characters.
|> >> 
|> 
|> -- 
|> Peter Dalgaard, Professor,
|> Center for Statistics, Copenhagen Business School
|> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
|> Phone: (+45)38153501
|> Office: A 4.23
|> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
|> 
|> 
|> 
|> 
|> 
|> 
|> 
|> 
|> 

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From marongiu.luigi at gmail.com  Sun Sep 27 13:56:56 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Sun, 27 Sep 2015 12:56:56 +0100
Subject: [R] Truncation of character fields in a data frame
Message-ID: <CAMk+s2TNFz=95bbxBm_=irWZ+J+2_BDrwLDWu77yCECmr0J7Zw@mail.gmail.com>

Dear all,
I am reading a txt file into the R environment to create a data frame,
however I have notice that some entries have a truncated version of a
field, so for instance I get "Astro" instead of "Astro 1-Astro 1" and
"Sapo" for "Sapo #1-Sapo_1" and "Sapo #2-Sapo_2", but I also get
"Adeno 40/41 EH-Adeno_40-41_EH" so the problem is not in the spaces
between the words. The txt file is a simple tab delimited file
generated from excel which I read with:

bad.data<-read.table(
    "test_df.txt",
    header=TRUE,
    row.names=1,
    dec = ".",
    sep="\t",
    stringsAsFactors = FALSE,
    fill = TRUE
)

[the fill = TRUE was introduced because in the real case I got an
error of a missing line.]

I can recreate this file as follows:
sample <- c(rep("p.001", 48), rep("p.547", 48))
target <- c("Adeno 1-Adeno 1",    "Adeno 40/41 EH-AIQJCT3",    "Astro
1-Astro 1",    "Sapo 1-Sapo 1",    "Sapo 2-Sapo 2",    "Enterovirus
1-Enterovirus 1",    "Parechovirus-Parechovirus",    "HEV 1-HEV 1",
"IC PDV control-AIRSA0B",    "Rotavirus cam-Rotavirus cam",
"18S-Hs99999901_s1",    "Noro gp II-Noro gp II",    "Noro gp 1-Noro gp
1",    "Noro gp 1 mod33-Noro gp 1 mod33",    "C difficile
GDH-AIS086J",    "C difficile Tox B-C difficile Tox B",    "VTX
1-AIT97CR",    "BT control Man-AIVI5IZ",    "E. coli vtx 2-E. coli vtx
2",    "Campy spp-AIWR3O7",    "Salmonella ttr-AIX01VF",    "Crypto
CP2-AIY9Z1N",    "Green Fluorescent Protein-AI0IX7V",    "Adeno
2-Adeno 2",    "Adeno 40_41 Oly-AI1RWD3",    "Astro 2 Liu-AI20UKB",
"Giardia lambia 1-AI39SQJ",    "Rotavirus Liu-Rotavirus Liu 2",
"Enterovirus Bruges-Enterovirus 2 Br",    "HAV 1-Hepatitis A 1",
"HEV 2-AI5IQWR",    "MS2 control-AI6RO2Z",    "Rotarix NSP2-AI70M87",
  "CMV br-CMV br",    "IC Rnase P-AI89LFF",    "Salmonella hil
A-Salmonella hil A",    "Shigella ipa H-AIAA0K8",    "Enteroagg E.
coli-AIBJYRG",    "Campy jejuni-AICSWXO",    "Campy coli-AID1U3W",
"Yersinia enterocolitica-AIFAS94",    "Bacterial 16S-Bacterial 16S",
 "Aeromonas hydrophilia-Aeromonas hydrophilia",    "V
cholerae-AIGJRGC",    "Dientamoeba fragilis-AIHSPMK",    "Entamoeba
histolytica-AII1NSS",    "Crypto 2 J-AIKALY0",    "Giardia lambia
rev-AILJJ48",    "Adeno #1-Adeno_1",    "Adeno 40/41
EH-Adeno_40-41_EH",    "Astro #1-Astro_1",    "Sapo #1-Sapo_1",
"Sapo #2-Sapo_2",    "Enterovirus #1-Enterovirus_1",
"Parechovirus-Parechovirus",    "HEV #1-HEV_1",    "C coli jejuni
Liu-C_coli_jejuni_Li",    "Rotavirus cam-Rotavirus_cam",    "IC 18s-IC
18s",    "Noro gp II-Noro_gp_II",    "Noro gp 1-Noro_gp_1",    "Noro
gp 1 mod33-Noro_gp_1_mod33",    "C difficile GDH-C-difficile_GDH",
"C difficile Tox B-C_difficile_T_B",    "E. coli vtx 1-E_coli_vtx_1",
  "BT control Man-BT_control_Man",    "E. coli vtx 2-E_coli_vtx_2",
"Campy spp NEW-Campy_spp_NEW",    "Salmonella ttr-Salmonella_ttr",
"Cryptosporidium spp CP2-Cryptos_spp_CP2",    "C jejuni
#2-C_jejuni_2",    "Adeno #2-Adeno_2",    "Adeno 40/41
Oly-Adeno_40-41_Oly",    "Astro Liu #2-Astro_Liu_2",    "Giardia
lambia #1-Giardia_lambia_1",    "Rotavirus Liu #2-Rotavirus_Liu_2",
"Enterovirus #2 Br-Enterovirus_2_Br",    "Hepatitis A
#1-Hepatitis_A_1",    "HEV #2-HEV_2",    "MS2 control-MS2_control",
"Rotarix NSP2 Bris-Rotarix_NSP2_Bri",    "CMV br-CMV_br",    "Rnase P
control-Rnase_P_control",    "Salmonella hil A-Salmonella_hil_A",
"Shigella ipa H-Shigella_ipa_H",    "Enteroagg E.
coli-Enteroagg_E_coli",    "V parahaemolyticus-V_p_haemolyticus",
"Campy coli-Campy_coli",    "Yersinia
enterocolitica-Y_enterocolitica",    "Bacterial 16S-Bacterial_16S",
"Aeromonas hydrophilia-Aero_hydrophilia",    "Vibrio
cholerae-Vibrio_cholerae",    "Dientamoeba fragilis-Dien_fragilis",
"Entamoeba histolytica-Enta_histolytica",    "Cryptosporidium spp #2
J-Crypto_spp_2_J",    "Giardia lambia #2 rev-Giardia_lambia_r")
ct <- c(NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,
NA,    18.793,    NA,    NA,    NA,    NA,    NA,    NA,    33.302,
NA,    32.388,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,
   NA,    NA,    NA,    31.398,    NA,    NA,    NA,    NA,    NA,
NA,    NA,    NA,    NA,    8.115,    NA,    NA,    NA,    NA,    NA,
  NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,
 NA,    21.161,    NA,    NA,    NA,    NA,    NA,    NA,    31.302,
 NA,    29.785,    NA,    NA,    NA,    NA,    NA,    NA,    NA,
NA,    NA,    NA,    NA,    31.212,    42.967,    NA,    33.503,
NA,    NA,    NA,    NA,    NA,    NA,    9.584,    NA,    NA,    NA,
  NA,    NA,    NA)

good.data <- data.frame(sample, target, ct, stringsAsFactors = FALSE)

and the structure of these object is the same:
> str(good.data)
'data.frame':    96 obs. of  3 variables:
 $ sample: chr  "p.001" "p.001" "p.001" "p.001" ...
 $ target: chr  "Adeno 1-Adeno 1" "Adeno 40/41 EH-AIQJCT3" "Astro
1-Astro 1" "Sapo 1-Sapo 1" ...
 $ ct    : num  NA NA NA NA NA NA NA NA NA NA ...
> str(bad.data)
'data.frame':    96 obs. of  3 variables:
 $ Sample: chr  "p.001" "p.001" "p.001" "p.001" ...
 $ Target: chr  "Adeno 1-Adeno 1" "Adeno 40/41 EH-AIQJCT3" "Astro
1-Astro 1" "Sapo 1-Sapo 1" ...
 $ Ct    : num  NA NA NA NA NA NA NA NA NA NA ...

however in the good.data case the problem with truncation does not
occur, so for instance I get the required "Astro #1-Astro_1", "Sapo
#1-Sapo_1" and "Sapo #2-Sapo_2 ".
The problem must therefore be in the format of the txt file and the
read function, possibly in the # character present in the names.
Could somebody explain me what such problem is and how to avoid it?
Many thanks
Best regards
Luigi


From murdoch.duncan at gmail.com  Sun Sep 27 14:19:55 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 27 Sep 2015 08:19:55 -0400
Subject: [R] Truncation of character fields in a data frame
In-Reply-To: <CAMk+s2TNFz=95bbxBm_=irWZ+J+2_BDrwLDWu77yCECmr0J7Zw@mail.gmail.com>
References: <CAMk+s2TNFz=95bbxBm_=irWZ+J+2_BDrwLDWu77yCECmr0J7Zw@mail.gmail.com>
Message-ID: <5607DEEB.7070600@gmail.com>

On 27/09/2015 7:56 AM, Luigi Marongiu wrote:
> Dear all,
> I am reading a txt file into the R environment to create a data frame,
> however I have notice that some entries have a truncated version of a
> field, so for instance I get "Astro" instead of "Astro 1-Astro 1" and
> "Sapo" for "Sapo #1-Sapo_1" and "Sapo #2-Sapo_2", but I also get
> "Adeno 40/41 EH-Adeno_40-41_EH" so the problem is not in the spaces
> between the words. The txt file is a simple tab delimited file
> generated from excel which I read with:
> 
> bad.data<-read.table(
>     "test_df.txt",
>     header=TRUE,
>     row.names=1,
>     dec = ".",
>     sep="\t",
>     stringsAsFactors = FALSE,
>     fill = TRUE
> )
> 
> [the fill = TRUE was introduced because in the real case I got an
> error of a missing line.]

See the "comment.char" argument to read.table.  By default the "#"
character marks a comment, as in R code.

Duncan Murdoch

> 
> I can recreate this file as follows:
> sample <- c(rep("p.001", 48), rep("p.547", 48))
> target <- c("Adeno 1-Adeno 1",    "Adeno 40/41 EH-AIQJCT3",    "Astro
> 1-Astro 1",    "Sapo 1-Sapo 1",    "Sapo 2-Sapo 2",    "Enterovirus
> 1-Enterovirus 1",    "Parechovirus-Parechovirus",    "HEV 1-HEV 1",
> "IC PDV control-AIRSA0B",    "Rotavirus cam-Rotavirus cam",
> "18S-Hs99999901_s1",    "Noro gp II-Noro gp II",    "Noro gp 1-Noro gp
> 1",    "Noro gp 1 mod33-Noro gp 1 mod33",    "C difficile
> GDH-AIS086J",    "C difficile Tox B-C difficile Tox B",    "VTX
> 1-AIT97CR",    "BT control Man-AIVI5IZ",    "E. coli vtx 2-E. coli vtx
> 2",    "Campy spp-AIWR3O7",    "Salmonella ttr-AIX01VF",    "Crypto
> CP2-AIY9Z1N",    "Green Fluorescent Protein-AI0IX7V",    "Adeno
> 2-Adeno 2",    "Adeno 40_41 Oly-AI1RWD3",    "Astro 2 Liu-AI20UKB",
> "Giardia lambia 1-AI39SQJ",    "Rotavirus Liu-Rotavirus Liu 2",
> "Enterovirus Bruges-Enterovirus 2 Br",    "HAV 1-Hepatitis A 1",
> "HEV 2-AI5IQWR",    "MS2 control-AI6RO2Z",    "Rotarix NSP2-AI70M87",
>   "CMV br-CMV br",    "IC Rnase P-AI89LFF",    "Salmonella hil
> A-Salmonella hil A",    "Shigella ipa H-AIAA0K8",    "Enteroagg E.
> coli-AIBJYRG",    "Campy jejuni-AICSWXO",    "Campy coli-AID1U3W",
> "Yersinia enterocolitica-AIFAS94",    "Bacterial 16S-Bacterial 16S",
>  "Aeromonas hydrophilia-Aeromonas hydrophilia",    "V
> cholerae-AIGJRGC",    "Dientamoeba fragilis-AIHSPMK",    "Entamoeba
> histolytica-AII1NSS",    "Crypto 2 J-AIKALY0",    "Giardia lambia
> rev-AILJJ48",    "Adeno #1-Adeno_1",    "Adeno 40/41
> EH-Adeno_40-41_EH",    "Astro #1-Astro_1",    "Sapo #1-Sapo_1",
> "Sapo #2-Sapo_2",    "Enterovirus #1-Enterovirus_1",
> "Parechovirus-Parechovirus",    "HEV #1-HEV_1",    "C coli jejuni
> Liu-C_coli_jejuni_Li",    "Rotavirus cam-Rotavirus_cam",    "IC 18s-IC
> 18s",    "Noro gp II-Noro_gp_II",    "Noro gp 1-Noro_gp_1",    "Noro
> gp 1 mod33-Noro_gp_1_mod33",    "C difficile GDH-C-difficile_GDH",
> "C difficile Tox B-C_difficile_T_B",    "E. coli vtx 1-E_coli_vtx_1",
>   "BT control Man-BT_control_Man",    "E. coli vtx 2-E_coli_vtx_2",
> "Campy spp NEW-Campy_spp_NEW",    "Salmonella ttr-Salmonella_ttr",
> "Cryptosporidium spp CP2-Cryptos_spp_CP2",    "C jejuni
> #2-C_jejuni_2",    "Adeno #2-Adeno_2",    "Adeno 40/41
> Oly-Adeno_40-41_Oly",    "Astro Liu #2-Astro_Liu_2",    "Giardia
> lambia #1-Giardia_lambia_1",    "Rotavirus Liu #2-Rotavirus_Liu_2",
> "Enterovirus #2 Br-Enterovirus_2_Br",    "Hepatitis A
> #1-Hepatitis_A_1",    "HEV #2-HEV_2",    "MS2 control-MS2_control",
> "Rotarix NSP2 Bris-Rotarix_NSP2_Bri",    "CMV br-CMV_br",    "Rnase P
> control-Rnase_P_control",    "Salmonella hil A-Salmonella_hil_A",
> "Shigella ipa H-Shigella_ipa_H",    "Enteroagg E.
> coli-Enteroagg_E_coli",    "V parahaemolyticus-V_p_haemolyticus",
> "Campy coli-Campy_coli",    "Yersinia
> enterocolitica-Y_enterocolitica",    "Bacterial 16S-Bacterial_16S",
> "Aeromonas hydrophilia-Aero_hydrophilia",    "Vibrio
> cholerae-Vibrio_cholerae",    "Dientamoeba fragilis-Dien_fragilis",
> "Entamoeba histolytica-Enta_histolytica",    "Cryptosporidium spp #2
> J-Crypto_spp_2_J",    "Giardia lambia #2 rev-Giardia_lambia_r")
> ct <- c(NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,
> NA,    18.793,    NA,    NA,    NA,    NA,    NA,    NA,    33.302,
> NA,    32.388,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,
>    NA,    NA,    NA,    31.398,    NA,    NA,    NA,    NA,    NA,
> NA,    NA,    NA,    NA,    8.115,    NA,    NA,    NA,    NA,    NA,
>   NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,
>  NA,    21.161,    NA,    NA,    NA,    NA,    NA,    NA,    31.302,
>  NA,    29.785,    NA,    NA,    NA,    NA,    NA,    NA,    NA,
> NA,    NA,    NA,    NA,    31.212,    42.967,    NA,    33.503,
> NA,    NA,    NA,    NA,    NA,    NA,    9.584,    NA,    NA,    NA,
>   NA,    NA,    NA)
> 
> good.data <- data.frame(sample, target, ct, stringsAsFactors = FALSE)
> 
> and the structure of these object is the same:
>> str(good.data)
> 'data.frame':    96 obs. of  3 variables:
>  $ sample: chr  "p.001" "p.001" "p.001" "p.001" ...
>  $ target: chr  "Adeno 1-Adeno 1" "Adeno 40/41 EH-AIQJCT3" "Astro
> 1-Astro 1" "Sapo 1-Sapo 1" ...
>  $ ct    : num  NA NA NA NA NA NA NA NA NA NA ...
>> str(bad.data)
> 'data.frame':    96 obs. of  3 variables:
>  $ Sample: chr  "p.001" "p.001" "p.001" "p.001" ...
>  $ Target: chr  "Adeno 1-Adeno 1" "Adeno 40/41 EH-AIQJCT3" "Astro
> 1-Astro 1" "Sapo 1-Sapo 1" ...
>  $ Ct    : num  NA NA NA NA NA NA NA NA NA NA ...
> 
> however in the good.data case the problem with truncation does not
> occur, so for instance I get the required "Astro #1-Astro_1", "Sapo
> #1-Sapo_1" and "Sapo #2-Sapo_2 ".
> The problem must therefore be in the format of the txt file and the
> read function, possibly in the # character present in the names.
> Could somebody explain me what such problem is and how to avoid it?
> Many thanks
> Best regards
> Luigi
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From giorgio.garziano at ericsson.com  Sun Sep 27 18:09:25 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Sun, 27 Sep 2015 16:09:25 +0000
Subject: [R] Rattle installation
Message-ID: <248E6FA047A8C746BA491485764190F522094335@ESESSMB207.ericsson.se>

This is what I can observe for rattle 3.5.0 on Windows

pack <- available.packages()

pack["rattle","Depends"]
[1] "R (>= 2.13.0), RGtk2"


pack["rattle", "Suggests"]



[1] "pmml (>= 1.2.13), bitops, colorspace, ada, amap, arules,\narulesViz, biclust, cairoDevice, cba, corrplot, descr, doBy,\ndplyr, e1071, ellipse, fBasics, foreign, fpc, gdata, ggdendro,\nggplot2, gplots, graph, grid, gtools, gWidgetsRGtk2, hmeasure,\nHmisc, kernlab, Matrix, methods, mice, nnet, odfWeave, party,\nplaywith, plyr, psych, randomForest, RBGL, RColorBrewer,\nreadxl, reshape, rggobi, RGtk2Extras, ROCR, RODBC, rpart,\nrpart.plot, SnowballC, stringr, survival, timeDate, tm,\nverification, wskm, XML, pkgDepTools, Rgraphviz"


pack["rattle", "Imports"]

[1] NA

In general, the package installation by RStudio is straightforward as it takes care of dependencies.

--
Giorgio Garziano

	[[alternative HTML version deleted]]


From amelia_marsh08 at yahoo.com  Sun Sep 27 20:19:01 2015
From: amelia_marsh08 at yahoo.com (Amelia Marsh)
Date: Sun, 27 Sep 2015 18:19:01 +0000 (UTC)
Subject: [R] Excel Price function in R
Message-ID: <969741861.1141678.1443377941319.JavaMail.yahoo@mail.yahoo.com>

Dear Forum,

I am using trying to find price of bond in R. I have written the code in line with Excel PRICE formula. However, whenever the residual maturity is less than a year, my R output tallies with the Excel Price formula. However, moment my residual maturity exceeds 1 year, the R output differs from Excel Price function. I have tried to find out the reason for am not able to figure out. 

Please guide me. Here is my code alongwith illustrative examples -

(I am copying this code from notepad++. Please forgive forgive for any inconvenience caused)


# MY code

add.months = function(date, n) {
  nC <- seq(date, by=paste (n, "months"), length = 2)[2]
  fD <- as.Date(strftime(as.Date(date), format='%Y-%m-01'))
  C  <- (seq(fD, by=paste (n+1, "months"), length = 2)[2])-1
  if(nC>C) return(C)
  return(nC)
}

# ________________________________________________________________________

date.diff = function(end, start, basis=1) {
  if (basis != 0 && basis != 4)
    return(as.numeric(end - start))
  e <- as.POSIXlt(end)
  s <- as.POSIXlt(start)
  d <-   (360 * (e$year - s$year)) + 
    ( 30 * (e$mon  - s$mon )) +
    (min(30, e$mday) - min(30, s$mday))
  return (d)
}

# ________________________________________________________________________


excel.price = function(settlement, maturity, coupon, yield, redemption, frequency, basis=1) 
{
  cashflows   <- 0
  last.coupon <- maturity
  while (last.coupon > settlement) {
    last.coupon <- add.months(last.coupon, -12/frequency)
    cashflows   <- cashflows + 1
  }
  next.coupon <- add.months(last.coupon, 12/frequency)
  
  valueA   <- date.diff(settlement,  last.coupon, basis)
  valueE   <- date.diff(next.coupon, last.coupon, basis)
  valueDSC <- date.diff(next.coupon, settlement,  basis)

  if (cashflows == 0)
    stop('number of coupons payable cannot be zero')else
  if (cashflows == 1)
  {
  valueDSR = valueE - valueA
  T1 = 100 * coupon / frequency + redemption
  T2 = (yield/frequency * valueDSR/valueE) + 1
  T3 = 100 * coupon / frequency * valueA / valueE
  result = (T1 / T2) - T3
  return(result = result)
  }else
  if (cashflows > 1)  
  {  
  expr1    <- 1 + (yield/frequency)
  expr2    <- valueDSC / valueE
  expr3    <- coupon / frequency
  result   <- redemption / (expr1 ^ (cashflows - 1 + expr2))
  for (k in 1:cashflows) {
    result <- result + ( 100 * expr3 / (expr1 ^ (k - 1 + expr2)) )
  }
  result   <- result - ( 100*expr3 * valueA / valueE )
   return(result = result)
   }
}


# ________________________________________________________________________


(ep1 = excel.price(settlement = as.Date(c("09/15/24"), "%m/%y/%d"), maturity = as.Date(c("11/15/4"), "%m/%y/%d"), coupon = 0.065, yield = 0.05904166667, redemption = 100, frequency = 2, basis = 1))

(ep2 = excel.price(settlement = as.Date(c("09/15/24"), "%m/%y/%d"), maturity = as.Date(c("7/16/22"), "%m/%y/%d"), coupon = 0.0725, yield = 0.0969747125, redemption = 100, frequency = 2, basis = 1))

(ep3 = excel.price(settlement = as.Date(c("09/15/24"), "%m/%y/%d"), maturity = as.Date(c("11/16/30"), "%m/%y/%d"), coupon = 0.08, yield = 0.0969747125, redemption = 100, frequency = 2, basis = 1))

# .......................................................................................................................................


# OUTPUT

ep1 = 100.0494
Excel output = 100.0494


ep2 = 98.0815
Excel output = 98.08149


ep3 = 98.12432
Excel output = 98.122795


While ep1 and ep2 match exactly with Excel Price function values, ep3 which has maturity exceeding one year doesnt tally with Excel Price function.



Kindly advise

With regards

Amelia


From mr.luizalberto at gmail.com  Sun Sep 27 07:54:49 2015
From: mr.luizalberto at gmail.com (Luiz Alberto Lima)
Date: Sun, 27 Sep 2015 07:54:49 +0200
Subject: [R] Error finding clusters for new data with FlexMix
Message-ID: <CAJ-NxTzdGskErftvLMuA8n0OK-DsfwKa6YYsEDE2hHyMsAf1tw@mail.gmail.com>

Hello,

I am trying to find the clusters for new data using FlexMix. I have the
following code:

library(flexmix)

x <- c(0.605, 0.523, 0.677, 0.101, 0.687, 0.586, 0.517, 0.592, 0.653,
0.617)
y <- c(0.222, 0.741, 0.182, 0.162, 0.192, 0.254, 0.745, 0.669, 0.198,
0.214)
test <- c(0.720, 0.168, 0.520, 0.134, 0.558)

model <- flexmix(y ~ x, data = data.frame(x = x, y = y), k=2)
pred <- predict(model, newdata=data.frame(x = test))
clusters_train <- clusters(model)
clusters_test <- clusters(model, newdata=data.frame(x = test))

When I execute the last line of the code, I receive the following error
message:

> clusters_test <- clusters(model, newdata=data.frame(x = test))
Error in model.frame.default(model at terms, data = data, na.action = NULL,  :
  variable lengths differ (found for 'x')

What am I doing wrong?

Thanks!

	[[alternative HTML version deleted]]


From james.vordtriede at att.net  Sun Sep 27 09:58:07 2015
From: james.vordtriede at att.net (james.vordtriede at att.net)
Date: Sun, 27 Sep 2015 07:58:07 +0000
Subject: [R] =?utf-8?q?Variable_Class_=22numeric=22_instead_recognized_by_?=
 =?utf-8?q?dplyr_as_a_=27factor=27?=
Message-ID: <557306.92647.bm@smtp205.mail.gq1.yahoo.com>

Hi--I?m new to R.  For a dissertation, my panel data is for 48 Sub-Saharan countries (cross-sectional index=?i?) over 55 years 1960-2014 (time-series index=?t?).  The variables read into R from a text file are levels data.  The 2SLS regression due to reverse causality will be based on change in the levels data, so will need to difference the data grouped by cross-sectional index ?i?.  


There are nearly 50 total variables, but the model essentially will regress the differenced Yit ~ X1it+X2it+X3it+X4it+X5it+X6it, with a dummy variable attached to each of the change-X(s).


Due to missing data, R originally classified each X and Y variable as a ?factor?, subsequently changed to ?numeric? via ?as.numeric? command.  


However, when I write the following command for dplr solely to difference Yit (=Yit-Yi[t-1]) mutated to new variable dYit, I receive error messages to the effect that Yit and each of the X variables are ?factors?.




>library (dplr)

>dt = CSUdata2 %>% group_by (i) %>% (dYit=Yit-lag(Yit))



?CSUdata2? is the object in which the tab-delimited text file dataset is stored.  


Questions:


 Any idea why dplyr reads the variables as ?factors??  A class(*) command per variable shows R to know each Y and X as ?numeric?.


Is the command to difference Yit done correctly?  I plan to use the same command for each variable requiring change until I understand the commands better.



Thank you.









Sent from Windows Mail
	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Sun Sep 27 21:55:48 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Sun, 27 Sep 2015 21:55:48 +0200
Subject: [R] Variable Class "numeric" instead recognized by dplyr as a
	'factor'
In-Reply-To: <557306.92647.bm@smtp205.mail.gq1.yahoo.com>
References: <557306.92647.bm@smtp205.mail.gq1.yahoo.com>
Message-ID: <CAJuCY5xhHsktZTb77RxGQRXKhX984zdzJUcZjxps9o2og245Aw@mail.gmail.com>

I doubt that dplyr is the problem. have a look at the output of
str(CSUdata2) The problem is probably in there.

Sending a reproducible example of the problem makes it easier for us to
help you. Note that this list doesn't accept HTML mail. I suggest that you
read the posting guide carefully.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-09-27 9:58 GMT+02:00 <james.vordtriede at att.net>:

> Hi--I?m new to R.  For a dissertation, my panel data is for 48 Sub-Saharan
> countries (cross-sectional index=?i?) over 55 years 1960-2014 (time-series
> index=?t?).  The variables read into R from a text file are levels data.
> The 2SLS regression due to reverse causality will be based on change in the
> levels data, so will need to difference the data grouped by cross-sectional
> index ?i?.
>
>
> There are nearly 50 total variables, but the model essentially will
> regress the differenced Yit ~ X1it+X2it+X3it+X4it+X5it+X6it, with a dummy
> variable attached to each of the change-X(s).
>
>
> Due to missing data, R originally classified each X and Y variable as a
> ?factor?, subsequently changed to ?numeric? via ?as.numeric? command.
>
>
> However, when I write the following command for dplr solely to difference
> Yit (=Yit-Yi[t-1]) mutated to new variable dYit, I receive error messages
> to the effect that Yit and each of the X variables are ?factors?.
>
>
>
>
> >library (dplr)
>
> >dt = CSUdata2 %>% group_by (i) %>% (dYit=Yit-lag(Yit))
>
>
>
> ?CSUdata2? is the object in which the tab-delimited text file dataset is
> stored.
>
>
> Questions:
>
>
>  Any idea why dplyr reads the variables as ?factors??  A class(*) command
> per variable shows R to know each Y and X as ?numeric?.
>
>
> Is the command to difference Yit done correctly?  I plan to use the same
> command for each variable requiring change until I understand the commands
> better.
>
>
>
> Thank you.
>
>
>
>
>
>
>
>
>
> Sent from Windows Mail
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sun Sep 27 22:12:01 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 27 Sep 2015 13:12:01 -0700
Subject: [R] Variable Class "numeric" instead recognized by dplyr as a
	'factor'
In-Reply-To: <557306.92647.bm@smtp205.mail.gq1.yahoo.com>
References: <557306.92647.bm@smtp205.mail.gq1.yahoo.com>
Message-ID: <CAGxFJbS1Y8pbCNx4=W1FcaAMscFxujrK-_ThR-S0uMUJ-vo8iA@mail.gmail.com>

I believe you need to spend some time with an R tutorial, as I don't
believe what you understand what factors are and how they should be
used."Dummy variables" are also almost certainly unnecessary and
usually undesirable, as well.

A few comments below may help..

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sun, Sep 27, 2015 at 12:58 AM,  <james.vordtriede at att.net> wrote:
> Hi--I?m new to R.  For a dissertation, my panel data is for 48 Sub-Saharan countries (cross-sectional index=?i?) over 55 years 1960-2014 (time-series index=?t?).  The variables read into R from a text file are levels data.  The 2SLS regression due to reverse causality will be based on change in the levels data, so will need to difference the data grouped by cross-sectional index ?i?.
>
>
> There are nearly 50 total variables, but the model essentially will regress the differenced Yit ~ X1it+X2it+X3it+X4it+X5it+X6it, with a dummy variable attached to each of the change-X(s).
>
>
> Due to missing data, R originally classified each X and Y variable as a ?factor?, subsequently changed to ?numeric? via ?as.numeric? command.

No.
a) missing data will not cause numeric data to become factor. There's
something wrong in the data from the beginning (as Thierry said)

b) If f is numeric data that is a factor, as.numeric(f) is almost
certainly **not** the corrrect way to change it to numeric. You will
get garbage, viz.:

> f <- runif(5)
> f
[1] 0.42568762 0.03105132 0.46606135 0.35251240 0.57303571
> as.numeric(factor(f))
[1] 3 1 4 2 5




>
>
> However, when I write the following command for dplr solely to difference Yit (=Yit-Yi[t-1]) mutated to new variable dYit, I receive error messages to the effect that Yit and each of the X variables are ?factors?.
>
>
>
>
>>library (dplr)
>
>>dt = CSUdata2 %>% group_by (i) %>% (dYit=Yit-lag(Yit))
>
>
>
> ?CSUdata2? is the object in which the tab-delimited text file dataset is stored.
>
>
> Questions:
>
>
>  Any idea why dplyr reads the variables as ?factors??  A class(*) command per variable shows R to know each Y and X as ?numeric?.
>
>
> Is the command to difference Yit done correctly?  I plan to use the same command for each variable requiring change until I understand the commands better.

Almost certainly not. See ?diff


>
>
>
> Thank you.
>
>
>
>
>
>
>
>
>
> Sent from Windows Mail
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Sun Sep 27 22:14:30 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 27 Sep 2015 22:14:30 +0200
Subject: [R] Excel Price function in R
In-Reply-To: <969741861.1141678.1443377941319.JavaMail.yahoo@mail.yahoo.com>
References: <969741861.1141678.1443377941319.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <BD29BDD3-1E6B-484B-A4F1-388B90F2D503@gmail.com>

Given that this requires knowledge of both bond theory and Excel plus a fair amount of effort to understand your code, you are likely to be _so_ on your own....

However, I'll venture a guess that it has something to do with whether coupons should be discounted until payout or until maturity.  

There are some fairly straightforward numerical experiments that you could perform to get a handle on what is different in Excel: Graph the price as a function of maturity; do you see an abrupt change or does your curve and Excel's diverge in a smoothish fashion? If the latter, what is the order of magnitude of the divergence? Can you relate it to some of the parameters of your model? What happens if you go beyond 2 years to maturity? 3? 4? Etc.

-pd

> On 27 Sep 2015, at 20:19 , Amelia Marsh via R-help <r-help at r-project.org> wrote:
> 
> Dear Forum,
> 
> I am using trying to find price of bond in R. I have written the code in line with Excel PRICE formula. However, whenever the residual maturity is less than a year, my R output tallies with the Excel Price formula. However, moment my residual maturity exceeds 1 year, the R output differs from Excel Price function. I have tried to find out the reason for am not able to figure out. 
> 
> Please guide me. Here is my code alongwith illustrative examples -
> 
> (I am copying this code from notepad++. Please forgive forgive for any inconvenience caused)
> 
> 
> # MY code
> 
> add.months = function(date, n) {
>  nC <- seq(date, by=paste (n, "months"), length = 2)[2]
>  fD <- as.Date(strftime(as.Date(date), format='%Y-%m-01'))
>  C  <- (seq(fD, by=paste (n+1, "months"), length = 2)[2])-1
>  if(nC>C) return(C)
>  return(nC)
> }
> 
> # ________________________________________________________________________
> 
> date.diff = function(end, start, basis=1) {
>  if (basis != 0 && basis != 4)
>    return(as.numeric(end - start))
>  e <- as.POSIXlt(end)
>  s <- as.POSIXlt(start)
>  d <-   (360 * (e$year - s$year)) + 
>    ( 30 * (e$mon  - s$mon )) +
>    (min(30, e$mday) - min(30, s$mday))
>  return (d)
> }
> 
> # ________________________________________________________________________
> 
> 
> excel.price = function(settlement, maturity, coupon, yield, redemption, frequency, basis=1) 
> {
>  cashflows   <- 0
>  last.coupon <- maturity
>  while (last.coupon > settlement) {
>    last.coupon <- add.months(last.coupon, -12/frequency)
>    cashflows   <- cashflows + 1
>  }
>  next.coupon <- add.months(last.coupon, 12/frequency)
> 
>  valueA   <- date.diff(settlement,  last.coupon, basis)
>  valueE   <- date.diff(next.coupon, last.coupon, basis)
>  valueDSC <- date.diff(next.coupon, settlement,  basis)
> 
>  if (cashflows == 0)
>    stop('number of coupons payable cannot be zero')else
>  if (cashflows == 1)
>  {
>  valueDSR = valueE - valueA
>  T1 = 100 * coupon / frequency + redemption
>  T2 = (yield/frequency * valueDSR/valueE) + 1
>  T3 = 100 * coupon / frequency * valueA / valueE
>  result = (T1 / T2) - T3
>  return(result = result)
>  }else
>  if (cashflows > 1)  
>  {  
>  expr1    <- 1 + (yield/frequency)
>  expr2    <- valueDSC / valueE
>  expr3    <- coupon / frequency
>  result   <- redemption / (expr1 ^ (cashflows - 1 + expr2))
>  for (k in 1:cashflows) {
>    result <- result + ( 100 * expr3 / (expr1 ^ (k - 1 + expr2)) )
>  }
>  result   <- result - ( 100*expr3 * valueA / valueE )
>   return(result = result)
>   }
> }
> 
> 
> # ________________________________________________________________________
> 
> 
> (ep1 = excel.price(settlement = as.Date(c("09/15/24"), "%m/%y/%d"), maturity = as.Date(c("11/15/4"), "%m/%y/%d"), coupon = 0.065, yield = 0.05904166667, redemption = 100, frequency = 2, basis = 1))
> 
> (ep2 = excel.price(settlement = as.Date(c("09/15/24"), "%m/%y/%d"), maturity = as.Date(c("7/16/22"), "%m/%y/%d"), coupon = 0.0725, yield = 0.0969747125, redemption = 100, frequency = 2, basis = 1))
> 
> (ep3 = excel.price(settlement = as.Date(c("09/15/24"), "%m/%y/%d"), maturity = as.Date(c("11/16/30"), "%m/%y/%d"), coupon = 0.08, yield = 0.0969747125, redemption = 100, frequency = 2, basis = 1))
> 
> # .......................................................................................................................................
> 
> 
> # OUTPUT
> 
> ep1 = 100.0494
> Excel output = 100.0494
> 
> 
> ep2 = 98.0815
> Excel output = 98.08149
> 
> 
> ep3 = 98.12432
> Excel output = 98.122795
> 
> 
> While ep1 and ep2 match exactly with Excel Price function values, ep3 which has maturity exceeding one year doesnt tally with Excel Price function.
> 
> 
> 
> Kindly advise
> 
> With regards
> 
> Amelia
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Sun Sep 27 22:29:01 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 27 Sep 2015 22:29:01 +0200
Subject: [R] Variable Class "numeric" instead recognized by dplyr as a
	'factor'
In-Reply-To: <CAGxFJbS1Y8pbCNx4=W1FcaAMscFxujrK-_ThR-S0uMUJ-vo8iA@mail.gmail.com>
References: <557306.92647.bm@smtp205.mail.gq1.yahoo.com>
	<CAGxFJbS1Y8pbCNx4=W1FcaAMscFxujrK-_ThR-S0uMUJ-vo8iA@mail.gmail.com>
Message-ID: <2D8936B3-B0A6-455B-912F-DFCB745964BD@gmail.com>


> On 27 Sep 2015, at 22:12 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
>> 
>> Due to missing data, R originally classified each X and Y variable as a ?factor?, subsequently changed to ?numeric? via ?as.numeric? command.
> 
> No.
> a) missing data will not cause numeric data to become factor. There's
> something wrong in the data from the beginning (as Thierry said)

Well, if you forget to tell R what the input code for missing is (na.strings if you use read.table), then that is de facto what happens: The whole column gets interpreted as character and subsequently converted to a factor. The fix is to _remember_ to tell R what missing value codes are being used.

> 
> b) If f is numeric data that is a factor, as.numeric(f) is almost
> certainly **not** the corrrect way to change it to numeric.

Amen... as.numeric(as.character(f)) if you must, but the proper fix is usually the above.

-pd

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From bgunter.4567 at gmail.com  Sun Sep 27 23:09:12 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 27 Sep 2015 14:09:12 -0700
Subject: [R] Variable Class "numeric" instead recognized by dplyr as a
	'factor'
In-Reply-To: <2D8936B3-B0A6-455B-912F-DFCB745964BD@gmail.com>
References: <557306.92647.bm@smtp205.mail.gq1.yahoo.com>
	<CAGxFJbS1Y8pbCNx4=W1FcaAMscFxujrK-_ThR-S0uMUJ-vo8iA@mail.gmail.com>
	<2D8936B3-B0A6-455B-912F-DFCB745964BD@gmail.com>
Message-ID: <CAGxFJbTOnk4XPnjuggW00P2_QGKvcYRqG2KhiK_9wMVKKH2NBA@mail.gmail.com>

Yes, but I think of numeric data with non-numeric values (e.g. "." for
missing) as character, not numeric.  Missing to me means either empty
or with the missing value code specified as you describe. Ergo my
comment. Your clarification is nevertheless appropriate.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sun, Sep 27, 2015 at 1:29 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>
>> On 27 Sep 2015, at 22:12 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>>>
>>> Due to missing data, R originally classified each X and Y variable as a ?factor?, subsequently changed to ?numeric? via ?as.numeric? command.
>>
>> No.
>> a) missing data will not cause numeric data to become factor. There's
>> something wrong in the data from the beginning (as Thierry said)
>
> Well, if you forget to tell R what the input code for missing is (na.strings if you use read.table), then that is de facto what happens: The whole column gets interpreted as character and subsequently converted to a factor. The fix is to _remember_ to tell R what missing value codes are being used.
>
>>
>> b) If f is numeric data that is a factor, as.numeric(f) is almost
>> certainly **not** the corrrect way to change it to numeric.
>
> Amen... as.numeric(as.character(f)) if you must, but the proper fix is usually the above.
>
> -pd
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>


From trichter at uni-bremen.de  Sun Sep 27 23:22:13 2015
From: trichter at uni-bremen.de (trichter at uni-bremen.de)
Date: Sun, 27 Sep 2015 21:22:13 +0000
Subject: [R] How to find out if two cells in a dataframe belong to the same
 pre-specified factor-level
Message-ID: <20150927212213.Horde.PswsnsW-4aMBBnP0grr3cve@webmail.uni-bremen.de>

Dear list,
I really couldnt find a better way to describe my question, so please  
bear with me.

To illustrate my problem, i have a matrix with ecological distances  
(m1) and one with genetic distances (m2) for a number of biological  
species. I have merged both matrices and want to plot both distances  
versus each other, as illustrated in this example:

library(reshape)
library(ggplot2)
library(dplyr)

dist1 <- matrix(runif(16),4,4)
dist2 <- matrix(runif(16),4,4)
rownames(dist1) <- colnames(dist1) <- paste0("A",1:4)
rownames(dist2) <- colnames(dist2) <- paste0("A",1:4)

m1 <- melt(dist1)
m2 <- melt(dist2)

final <- full_join(m1,m2, by=c("Var1","Var2"))
ggplot(final, aes(value.x,value.y)) + geom_point()

Here is the twist:
The biological species belong to certain groups, which are given in  
the dataframe `species`, for example:

species <- data.frame(spcs=as.character(paste0("A",1:4)),
                       grps=as.factor(c(rep("cat",2),(rep("dog",2)))))

I want to check if a x,y pair in final (as in `final$Var1`,  
`final$Var2`) belongs to the same group of species (here "cat" or  
"dog"), and then want to color all groups specifically in the  
x,y-scatterplot.
Thus, i need an R translation for:

final$group <- If (final$Var1 and final$Var2) belong to the same group  
as specified
       in species, then assign the species group here, else do nothing  
or assign NA

so i can proceed with

ggplot(final, aes(value.x,value.y, col=group)) + geom_point()

So, in the example, the pairs A1-A1, A1-A2, A2-A1, A2-A2 should be  
identified as "both cats", hence should get the factor "cat".

Thank you very much!


Tim


From bgnumis at gmail.com  Sun Sep 27 23:51:23 2015
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Sun, 27 Sep 2015 23:51:23 +0200
Subject: [R] Line in hist count
In-Reply-To: <CAN25tHR_PBSyw4ssKFwj8Hmho65V=E5zMKaApyW+jHdAis_-UA@mail.gmail.com>
References: <CAN25tHR_PBSyw4ssKFwj8Hmho65V=E5zMKaApyW+jHdAis_-UA@mail.gmail.com>
Message-ID: <CAN25tHRnB1OqPYCFfY5sV8Q00p-cp8XPzdm_qqDv__sBYJOyRA@mail.gmail.com>

Hi, all

I have discovered that with abline(h=dataf,col="red") can add a line as I
want in this plot

fhist<-hist(f,plot=FALSE)
par(mar=c(6,0,6,6))
barplot(fhist$counts/ sum(fhist$counts),axes=FALSE,
space=0,horiz=TRUE,col="lightgray")
grid()
title("Marginal Distribution CDS vs. Ibex",font=4)
abline(h=dataf,col="red")

The thing is:

?How can I display the associated fhist$counts/ sum(fhist$counts on the
last value of f?

2015-09-26 23:31 GMT+02:00 bgnumis bgnum <bgnumis at gmail.com>:

> Hi all,
>
> Several time ago I used to work with R, now I?m returning to study and
> work and searching old file I see that I used this code:
>
>
> gfhist<-hist(gf,plot=FALSE)
>
> par(mar=c(6,0,6,6))
>
> barplot(gfhist$counts,axes=FALSE, space=0,horiz=TRUE,col="lightgray")
>
> grid()
>
> title("Marginal Distribution Lagged",font=4)
>
> The thing is I would line to plot a bar (horizontal and thing bar that
> will be placed on the last gf data but on the barplot
>
> ?Do you think is it possible? gf is a matrix.
>
>
>

	[[alternative HTML version deleted]]


From bgnumis at gmail.com  Mon Sep 28 00:35:26 2015
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Mon, 28 Sep 2015 00:35:26 +0200
Subject: [R] Theme white bands blue and grey or other color
Message-ID: <CAN25tHQJyQTgg90C3yyrHasXazAYjh1Tf4-Ax734GbqXoWHS9A@mail.gmail.com>

Hi all,

I want to plot two bands in quant mod but with white theme,

chartSeries(Fond,theme="white",TA = c(addBBands(50,2), addBBands(100,2) )
)

The thing is if I put this "t" it plot with black but two bands can be
shown, ?Is it psiible to put the two different bands with white theme??

t=chartTheme()
t$BBands$fill="#ff666633"
reChart(theme="t")
t$BBands$col=c('red','blue','green')
t$BBands$col='blue'
reChart(theme="t")

Many Thanks in advance

	[[alternative HTML version deleted]]


From dkatz at tibco.com  Mon Sep 28 02:17:03 2015
From: dkatz at tibco.com (Davidwkatz)
Date: Sun, 27 Sep 2015 17:17:03 -0700 (PDT)
Subject: [R] FlexBayes installation from R-Forge Problem R 3.2.2
Message-ID: <1443399423867-4712861.post@n4.nabble.com>

I tried to install FlexBayes like this:

install.packages("FlexBayes", repos="http://R-Forge.R.project.org") but got
errors:

Here's the transcript in R:

R version 3.2.2 (2015-08-14) -- "Fire Safety"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> install.packages("FlexBayes", repos="http://R-Forge.R.project.org")
Installing package into ?C:/Users/dkatz/R/win-library/3.2?
(as ?lib? is unspecified)
Error: Line starting '<!DOCTYPE HTML PUBLI ...' is malformed!


Any help will be much appreciated!

Thanks,



--
View this message in context: http://r.789695.n4.nabble.com/FlexBayes-installation-from-R-Forge-Problem-R-3-2-2-tp4712861.html
Sent from the R help mailing list archive at Nabble.com.


From Phillip.Alday at unisa.edu.au  Mon Sep 28 04:17:12 2015
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Mon, 28 Sep 2015 02:17:12 +0000
Subject: [R] Appropriate specification of random effects structure for
 EEG/ERP data: including Channels or not?
In-Reply-To: <1443100371.4117.28.camel@loki>
References: <mailman.7.1443088802.25661.r-help@r-project.org>
	<1443100371.4117.28.camel@loki>
Message-ID: <C0478D33-24D6-4E06-9DBF-C7C536E1C456@unisa.edu.au>

You might also want to take a look at the recent paper from the Federmeier group, especially the supplementary materials. There are a few technical inaccuracies (ANOVA is a special case of hierarchical modelling, not the other way around), but they discuss some of the issues involved. And relevant for your work: they model channel as a grouping variable in the random-effects structure.

Payne, B. R., Lee, C.-L., and Federmeier, K. D. (2015). Revisiting the incremental effects of context on word processing: Evidence from single-word event-related brain potentials. Psychophysiology.

http://dx.doi.org/10.1111/psyp.12515

Best,
Phillip

> On 24 Sep 2015, at 22:42, Phillip Alday <Phillip.Alday at unisa.edu.au> wrote:
> 
> There is actually a fair amount of ERP literature using mixed-effects
> modelling, though you may have to branch out from the traditional
> psycholinguistics journals a bit (even just more "neurolinguistics" or
> language studies published in "psychology" would get you more!). But
> just in the traditional psycholinguistics journals, there is a wealth of
> literature, see for example the 2008 special issue on mixed models of
> the Journal of Memory and Language.
> 
> I would NOT encode the channels/ROIs/other topographic measures as
> random effects (grouping variables). If you think about the traditional
> ANOVA analysis of ERPs, you'll recall that ROI or some other topographic
> measure (laterality, saggitality) are included in the main effects and
> interactions. As a rule of thumb, this corresponds to a fixed effect in
> random effects models. More specifically, you generally care about
> whether the particular levels of the topographic measure (i.e. you care
> if an ERP component is located left-anterior or what not) and this is
> what fixed effects test. Random effects are more useful when you only
> care about the variance introduced by a particular term but not the
> specific levels (e.g. participants or items -- we don't care about a
> particular participant, but we do care about how much variance there is
> between participants, i.e. how the population of participants looks). 
> 
> Or, another thought: You may have seen ANOVA by-subjects and by-items,
> but I bet you've never seen an ANOVA by-channels. ANOVA "implicitly"
> collapses the channels within ROIs and you can do the same with mixed
> models. (That's an awkward statement technically, but it should help
> with the intuition.)
> 
> There is an another, related important point -- "nuisance parameters"
> aren't necessarily random effects. So even if you're not interested in
> the per-electrode distribution of the ERP component, that doesn't mean
> those should automatically be random effects. It *might* make sense to
> add a channel (as in per-electrode) random effect, if you care to model
> the variation within a given ROI (as you have done), but I haven't seen
> that yet. It is somewhat rare to include a per-channel fixed effect,
> just because you lose a lot of information that way and introduce more
> parameters into the model, but you could include a more fine-grained
> notion of saggital / lateral location based on e.g. the 10-20 system and
> make that into an ordered factor. (Or you could be extreme and even use
> the spherical coordinates that the 10-20 is based on and have continuous
> measures of electrode placement!) The big problem with including
> "channel" as a random-effect grouping variable is that the channels
> would have a very complicated covariance structure (because adjacent
> electrodes are very highly correlated with each other) and I'm not sure
> how to model this in a straightforward way with lme4.
> 
> More generally, in considering your random effects structure, you should
> look at Barr et al (2013, "Random effects structure for confirmatory
> hypothesis testing: Keep it maximal") and the recent reply by Bates et
> al (arXiv, "Parsimonious Mixed Models"). You should read up on the GLMM
> FAQ on testing random effects -- there are different opinions on this
> and not all think that testing them via likelihood-ratio tests makes
> sense.
> 
> That wasn't my most coherent response, but maybe it's still useful. And
> for questions like this on mixed models, do check out the R Special
> Interest Group on Mixed Models. :-)
> 
> Best,
> Phillip
> 
> On Thu, 2015-09-24 at 12:00 +0200, r-help-request at r-project.org wrote:
>> Message: 4
>> Date: Wed, 23 Sep 2015 12:46:46 +0200
>> From: Paolo Canal <paolo.canal at iusspavia.it>
>> To: r-help at r-project.org
>> Subject: [R] Appropriate specification of random effects structure for
>>        EEG/ERP data: including Channels or not?
>> Message-ID: <56028316.2050004 at iusspavia.it>
>> Content-Type: text/plain; charset="UTF-8"
>> 
>> Dear r-help list,
>> 
>> I work with EEG/ERP data and this is the first time I am using LMM to 
>> analyze my data (using lme4).
>> The experimental design is a 2X2: one manipulated factor is
>> agreement, 
>> the other is noun (agreement being within subjects and items, and
>> noun 
>> being within subjects and between items).
>> 
>> The data matrix is 31 subjects * 160 items * 33 channels. In ERP 
>> research, the distribution of the EEG amplitude differences (in a
>> time 
>> window of interest) are important, and we care about knowing whether
>> a 
>> negative difference is occurring in Parietal or Frontal electrodes.
>> At 
>> the same time information from single channel is often too noisy and 
>> channels are organized in topographic factors for evaluating
>> differences 
>> in distribution. In the present case I have assigned each channel to
>> one 
>> of three levels of two factors, i.e., Longitude (Anterior, Central, 
>> Parietal) and Medial (Left, Midline, Right): for instance, one
>> channel 
>> is Anterior and Left. With traditional ANOVAs channels from the same 
>> level of topographic factors are averaged before variance is
>> evaluated 
>> and this also has the benefit of reducing the noise picked up by the 
>> electrodes.
>> 
>> I have troubles in deciding the random structure of my model. Very
>> few 
>> examples on LMM on ERP data exist (e.g., Newman, Tremblay, Nichols, 
>> Neville & Ullman, 2012) and little detail is provided about the 
>> treatment of channel. I feel it is a tricky term but very important
>> to 
>> optimize fit. Newman et al say "data from each electrode within an
>> ROI 
>> were treated as repeated measures of that ROI". In Newman et al, the 
>> ROIs are the 9 regions deriving from Longitude X Medial
>> (Anterior-Left, 
>> Anterior-Midline, Anterior-Right, Central-Left ... and so on), so in
>> a 
>> way they treated each ROI separately and not according to the
>> relevant 
>> dimensions of Longitude and Medial.
>> 
>> We used the following specifications in lmer:
>> 
>> [fixed effects specification: ?V ~ Agreement * Noun * Longitude *
>> Medial 
>> * (cov1 + cov2 + cov3 + cov4)] (the terms within brackets are a
>> series 
>> of individual covariates, most of which are continuous variables)
>> 
>> [random effects specification: (1+Agreement*Type of Noun | subject) + 
>> (1+Agreement | item) + (1|longitude:medial:channel)]
>> 
>> What I care the most about is the last term 
>> (1|longitude:medial:channel). I chose this specification because I 
>> thought that allowing each channel to have different intercepts in
>> the 
>> random structure would affect the estimation of the topographic fixed 
>> effects (Longitude and Medial) in which channel is nested.
>> Unfortunately 
>> a reviewer commented that since "channel is not included in the fixed 
>> effects I would probably leave that out".
>> 
>> But each channel is a repeated measure of the eeg amplitude inside
>> the 
>> two topographic factors, and random terms do not have to be in the
>> fixed 
>> structure, otherwise we would also include subjects and items in the 
>> fixed effects structure. So I kind of feel that including channels as 
>> random effect is correct, and having them nested in longitude:medial 
>> allows to relax the assumption that the effect in the EEG has always
>> the 
>> same longitude:medial distribution. But I might be wrong.
>> 
>> I thus tested differences in fit (ML) with anova() between 
>> (1|longitude:medial:channel) and the same model without the term, and
>> a 
>> third model with the model with a simpler (1|longitude:medial).
>> 
>> Fullmod vs Nochannel:
>> 
>> Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq)
>> modnoch 119 969479 970653 -484621 969241
>> fullmod 120 968972 970156 -484366 968732 508.73 1 < 2.2e-16 ***
>> 
>> Differences in fit is remarkable (no variance components with
>> estimates 
>> close to zero; no correlation parameters with values close to ?1).
>> 
>> Fullmod vs SimplerMod:
>> 
>>   Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq)
>> 
>> fullmod 120 968972 970156 -484366 968732
>> simplermod 120 969481 970665 -484621 969241 0 0 1
>> 
>> Here the number of parameters to estimate in fullmod and simplermod
>> is 
>> the same but the increase in fit is very consistent (-509 BIC). So I 
>> guess although the chisquare is not significant we do have a string 
>> increase in fit. As I understand this, a model with better fit will
>> find 
>> more accurate estimates, and I would be inclined to keep the fullmod 
>> random structure.
>> 
>> But perhaps I am missing something or I am doing something wrong.
>> Which 
>> is the correct random structure to use?
>> 
>> Feedbacks are very much appreciated. I often find answers in the
>> list, 
>> and this is the first time I post a question.
>> Thanks,
>> Paolo
>> 
>> 
>> 
>> 
> 


From dileepkunjaai at gmail.com  Mon Sep 28 08:01:11 2015
From: dileepkunjaai at gmail.com (=?UTF-8?B?4LSV4LWB4LSe4LWN4LSe4LS+4LSv4LS/IGt1bmphYWk=?=)
Date: Mon, 28 Sep 2015 11:31:11 +0530
Subject: [R] Singular Spectrum Analysis Plotting Skipping in a loop,
 but individually it works
Message-ID: <CALTF6smA778Kcgh4qfvsPTqdD_Px3xeqiM3cBX3M-a3+4ZHGbg@mail.gmail.com>

Dear all,

I am trying to plot  Spectrum of Singular Values using "Rssa" package.

I am trying to plot singular spectrum plot  inside a loop, it is not
plotting, but when I am trying to plot individually in terminal it works,
and I can save this as png files.

My code is given below:

*# --------------Calculating and plotting  Spectrum of Singular Values of
Control---------#*
library(ncdf)
library(Rssa)
season_list <-c('YEAR', 'DJF', 'JJA', 'SON', 'MAM')
zone_list <-c('ALLIN', 'WCIND', 'IPIND', 'ECIND', 'NEIND', 'NCIND',
'NWIND', 'WHIND')

for (sns in 1:length(season_list)){
for (rgn in 1:length(zone_list)){
var_noise<-paste(zone_list[rgn], "_",  season_list[sns], sep = "")
noise1<-get.var.ncdf(f_noise1, var_noise)
noise2<-get.var.ncdf(f_noise2, var_noise)
# Calculating Covariance Matrix from 'noise1' matrix
cv_noise_1 = noise1%*%t(noise1)
sigular_spectrum = ssa(cv_noise_1, svd.method = c("eigen"))

out_ssa_png =
paste("/home/dileep/WORK/Research_wind/CMIP_PiCOntrol_Expiriments/Homogenious_zone/homogenious_tempreture_zone/Homogenous_temp_Codes/Optimal_fingerprint_code/R/ECOF-package/SSA_noise_plots/SSA_",
zone_list[rgn], "_",  season_list[sns], "_SET_1.png", sep = "")

png(out_ssa_png, width= 6, height    = 7.0, units = "in", res= 1200,
pointsize = 3)
print ("Created PNG file")
titl = paste(zone_list[rgn],  season_list[sns], sep = " ")
plot(sigular_spectrum , main=titl)
dev.off()
print ("Done !")
}
}
*#####----------------------------------------------------------------------------------------###*

Thank you in advance
-- 
DILEEPKUMAR. R
J R F, IIT DELHI

	[[alternative HTML version deleted]]


From Ramgad82 at gmx.net  Mon Sep 28 08:50:18 2015
From: Ramgad82 at gmx.net (Tagmarie)
Date: Sun, 27 Sep 2015 23:50:18 -0700 (PDT)
Subject: [R] Boxplot: Plot outliners in a different window
Message-ID: <1443423018369-4712869.post@n4.nabble.com>

Hi, 
I want to draw a usual boxplot. I have one outliner way up. It makes my
boxes being drawn tiny. I do not want to delete the outliner as it is also
of ecological importance. 
I know there is a way of drawing a second window on top of the boxplot which
starts at a different y-axis-scales and includes the outliners there. I saw
it on a poster once. I can't find the command though. Does anyone know it?
Best regards, 
Tagmarie



--
View this message in context: http://r.789695.n4.nabble.com/Boxplot-Plot-outliners-in-a-different-window-tp4712869.html
Sent from the R help mailing list archive at Nabble.com.


From lists at dewey.myzen.co.uk  Mon Sep 28 09:54:52 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Mon, 28 Sep 2015 08:54:52 +0100
Subject: [R] Singular Spectrum Analysis Plotting Skipping in a loop,
 but individually it works
In-Reply-To: <CALTF6smA778Kcgh4qfvsPTqdD_Px3xeqiM3cBX3M-a3+4ZHGbg@mail.gmail.com>
References: <CALTF6smA778Kcgh4qfvsPTqdD_Px3xeqiM3cBX3M-a3+4ZHGbg@mail.gmail.com>
Message-ID: <5608F24C.7090209@dewey.myzen.co.uk>

Dear Dileep

What happens if you explicitly print it by wrapping the plot command in 
print(   )


On 28/09/2015 07:01, ???????? kunjaai wrote:
> Dear all,
>
> I am trying to plot  Spectrum of Singular Values using "Rssa" package.
>
> I am trying to plot singular spectrum plot  inside a loop, it is not
> plotting, but when I am trying to plot individually in terminal it works,
> and I can save this as png files.
>
> My code is given below:
>
> *# --------------Calculating and plotting  Spectrum of Singular Values of
> Control---------#*
> library(ncdf)
> library(Rssa)
> season_list <-c('YEAR', 'DJF', 'JJA', 'SON', 'MAM')
> zone_list <-c('ALLIN', 'WCIND', 'IPIND', 'ECIND', 'NEIND', 'NCIND',
> 'NWIND', 'WHIND')
>
> for (sns in 1:length(season_list)){
> for (rgn in 1:length(zone_list)){
> var_noise<-paste(zone_list[rgn], "_",  season_list[sns], sep = "")
> noise1<-get.var.ncdf(f_noise1, var_noise)
> noise2<-get.var.ncdf(f_noise2, var_noise)
> # Calculating Covariance Matrix from 'noise1' matrix
> cv_noise_1 = noise1%*%t(noise1)
> sigular_spectrum = ssa(cv_noise_1, svd.method = c("eigen"))
>
> out_ssa_png =
> paste("/home/dileep/WORK/Research_wind/CMIP_PiCOntrol_Expiriments/Homogenious_zone/homogenious_tempreture_zone/Homogenous_temp_Codes/Optimal_fingerprint_code/R/ECOF-package/SSA_noise_plots/SSA_",
> zone_list[rgn], "_",  season_list[sns], "_SET_1.png", sep = "")
>
> png(out_ssa_png, width= 6, height    = 7.0, units = "in", res= 1200,
> pointsize = 3)
> print ("Created PNG file")
> titl = paste(zone_list[rgn],  season_list[sns], sep = " ")
> plot(sigular_spectrum , main=titl)
> dev.off()
> print ("Done !")
> }
> }
> *#####----------------------------------------------------------------------------------------###*
>
> Thank you in advance
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From paolo.canal at iusspavia.it  Mon Sep 28 10:42:34 2015
From: paolo.canal at iusspavia.it (Paolo Canal)
Date: Mon, 28 Sep 2015 10:42:34 +0200
Subject: [R] Appropriate specification of random effects structure for
 EEG/ERP data: including Channels or not?
In-Reply-To: <C0478D33-24D6-4E06-9DBF-C7C536E1C456@unisa.edu.au>
References: <mailman.7.1443088802.25661.r-help@r-project.org>
	<1443100371.4117.28.camel@loki>
	<C0478D33-24D6-4E06-9DBF-C7C536E1C456@unisa.edu.au>
Message-ID: <5608FD7A.6060208@iusspavia.it>

Thank you Philllip,

And sorry being late in the response. Thanks for the reference, I 
believe that many of the published papers on ERPs with mixed models have 
descriptions of the analysis that often lacks of detail (even when 
looking for non *.linguistic journals).

Concerning your comments: I understand that nuisance factors are not 
random effects. At the same time the eeg amplitude is recorded from 
several electrodes which are units of observations around which the data 
are clustered. In the paper you suggested they write:

"In simplified models, by-channel random slope parameters were estimated 
at zero, resulting in failures to converge to an optimal solution. This 
likely reflects the limited variance in effects across the selected 
centro-parietal channels due to volume conduction. Therefore, random 
slopes of effects across channels were not fit in final models." This 
observation is not far from your point about adjacent electrodes being 
very highly correlated with each other, but I guess this would emerge in 
the random effects covariance matrix, only when asking for the 
calculation of by-channel random slopes or intercepts for some of the 
factors.

Therefore they used the term, only calculating adjustments to the 
intercepts, and I would have done the same just to increase fit, because 
I would say 1|ch better describes the data structure to lmer. But for 
pragmatism or necessity or parsimony, I'll likely forget about this (I 
am already trying to fit more than 80 parameters in the random structure 
so I do not want to be too strict about the inclusion of channels).

I keep your suggestion about modeling the topographic factors using the 
XYZ scalp-coordinates for the next studies when I'll have a better grasp 
on mixed models and some more computational power ;).

Thanks again for all your insights (I will refer to the special interest 
group in the next future).
best
Paolo


On 28/09/2015 04:17, Phillip Alday wrote:
> You might also want to take a look at the recent paper from the Federmeier group, especially the supplementary materials. There are a few technical inaccuracies (ANOVA is a special case of hierarchical modelling, not the other way around), but they discuss some of the issues involved. And relevant for your work: they model channel as a grouping variable in the random-effects structure.
>
> Payne, B. R., Lee, C.-L., and Federmeier, K. D. (2015). Revisiting the incremental effects of context on word processing: Evidence from single-word event-related brain potentials. Psychophysiology.
>
> http://dx.doi.org/10.1111/psyp.12515
>
> Best,
> Phillip
>
>> On 24 Sep 2015, at 22:42, Phillip Alday <Phillip.Alday at unisa.edu.au> wrote:
>>
>> There is actually a fair amount of ERP literature using mixed-effects
>> modelling, though you may have to branch out from the traditional
>> psycholinguistics journals a bit (even just more "neurolinguistics" or
>> language studies published in "psychology" would get you more!). But
>> just in the traditional psycholinguistics journals, there is a wealth of
>> literature, see for example the 2008 special issue on mixed models of
>> the Journal of Memory and Language.
>>
>> I would NOT encode the channels/ROIs/other topographic measures as
>> random effects (grouping variables). If you think about the traditional
>> ANOVA analysis of ERPs, you'll recall that ROI or some other topographic
>> measure (laterality, saggitality) are included in the main effects and
>> interactions. As a rule of thumb, this corresponds to a fixed effect in
>> random effects models. More specifically, you generally care about
>> whether the particular levels of the topographic measure (i.e. you care
>> if an ERP component is located left-anterior or what not) and this is
>> what fixed effects test. Random effects are more useful when you only
>> care about the variance introduced by a particular term but not the
>> specific levels (e.g. participants or items -- we don't care about a
>> particular participant, but we do care about how much variance there is
>> between participants, i.e. how the population of participants looks).
>>
>> Or, another thought: You may have seen ANOVA by-subjects and by-items,
>> but I bet you've never seen an ANOVA by-channels. ANOVA "implicitly"
>> collapses the channels within ROIs and you can do the same with mixed
>> models. (That's an awkward statement technically, but it should help
>> with the intuition.)
>>
>> There is an another, related important point -- "nuisance parameters"
>> aren't necessarily random effects. So even if you're not interested in
>> the per-electrode distribution of the ERP component, that doesn't mean
>> those should automatically be random effects. It *might* make sense to
>> add a channel (as in per-electrode) random effect, if you care to model
>> the variation within a given ROI (as you have done), but I haven't seen
>> that yet. It is somewhat rare to include a per-channel fixed effect,
>> just because you lose a lot of information that way and introduce more
>> parameters into the model, but you could include a more fine-grained
>> notion of saggital / lateral location based on e.g. the 10-20 system and
>> make that into an ordered factor. (Or you could be extreme and even use
>> the spherical coordinates that the 10-20 is based on and have continuous
>> measures of electrode placement!) The big problem with including
>> "channel" as a random-effect grouping variable is that the channels
>> would have a very complicated covariance structure (because adjacent
>> electrodes are very highly correlated with each other) and I'm not sure
>> how to model this in a straightforward way with lme4.
>>
>> More generally, in considering your random effects structure, you should
>> look at Barr et al (2013, "Random effects structure for confirmatory
>> hypothesis testing: Keep it maximal") and the recent reply by Bates et
>> al (arXiv, "Parsimonious Mixed Models"). You should read up on the GLMM
>> FAQ on testing random effects -- there are different opinions on this
>> and not all think that testing them via likelihood-ratio tests makes
>> sense.
>>
>> That wasn't my most coherent response, but maybe it's still useful. And
>> for questions like this on mixed models, do check out the R Special
>> Interest Group on Mixed Models. :-)
>>
>> Best,
>> Phillip
>>
>> On Thu, 2015-09-24 at 12:00 +0200, r-help-request at r-project.org wrote:
>>> Message: 4
>>> Date: Wed, 23 Sep 2015 12:46:46 +0200
>>> From: Paolo Canal <paolo.canal at iusspavia.it>
>>> To: r-help at r-project.org
>>> Subject: [R] Appropriate specification of random effects structure for
>>>         EEG/ERP data: including Channels or not?
>>> Message-ID: <56028316.2050004 at iusspavia.it>
>>> Content-Type: text/plain; charset="UTF-8"
>>>
>>> Dear r-help list,
>>>
>>> I work with EEG/ERP data and this is the first time I am using LMM to
>>> analyze my data (using lme4).
>>> The experimental design is a 2X2: one manipulated factor is
>>> agreement,
>>> the other is noun (agreement being within subjects and items, and
>>> noun
>>> being within subjects and between items).
>>>
>>> The data matrix is 31 subjects * 160 items * 33 channels. In ERP
>>> research, the distribution of the EEG amplitude differences (in a
>>> time
>>> window of interest) are important, and we care about knowing whether
>>> a
>>> negative difference is occurring in Parietal or Frontal electrodes.
>>> At
>>> the same time information from single channel is often too noisy and
>>> channels are organized in topographic factors for evaluating
>>> differences
>>> in distribution. In the present case I have assigned each channel to
>>> one
>>> of three levels of two factors, i.e., Longitude (Anterior, Central,
>>> Parietal) and Medial (Left, Midline, Right): for instance, one
>>> channel
>>> is Anterior and Left. With traditional ANOVAs channels from the same
>>> level of topographic factors are averaged before variance is
>>> evaluated
>>> and this also has the benefit of reducing the noise picked up by the
>>> electrodes.
>>>
>>> I have troubles in deciding the random structure of my model. Very
>>> few
>>> examples on LMM on ERP data exist (e.g., Newman, Tremblay, Nichols,
>>> Neville & Ullman, 2012) and little detail is provided about the
>>> treatment of channel. I feel it is a tricky term but very important
>>> to
>>> optimize fit. Newman et al say "data from each electrode within an
>>> ROI
>>> were treated as repeated measures of that ROI". In Newman et al, the
>>> ROIs are the 9 regions deriving from Longitude X Medial
>>> (Anterior-Left,
>>> Anterior-Midline, Anterior-Right, Central-Left ... and so on), so in
>>> a
>>> way they treated each ROI separately and not according to the
>>> relevant
>>> dimensions of Longitude and Medial.
>>>
>>> We used the following specifications in lmer:
>>>
>>> [fixed effects specification: ?V ~ Agreement * Noun * Longitude *
>>> Medial
>>> * (cov1 + cov2 + cov3 + cov4)] (the terms within brackets are a
>>> series
>>> of individual covariates, most of which are continuous variables)
>>>
>>> [random effects specification: (1+Agreement*Type of Noun | subject) +
>>> (1+Agreement | item) + (1|longitude:medial:channel)]
>>>
>>> What I care the most about is the last term
>>> (1|longitude:medial:channel). I chose this specification because I
>>> thought that allowing each channel to have different intercepts in
>>> the
>>> random structure would affect the estimation of the topographic fixed
>>> effects (Longitude and Medial) in which channel is nested.
>>> Unfortunately
>>> a reviewer commented that since "channel is not included in the fixed
>>> effects I would probably leave that out".
>>>
>>> But each channel is a repeated measure of the eeg amplitude inside
>>> the
>>> two topographic factors, and random terms do not have to be in the
>>> fixed
>>> structure, otherwise we would also include subjects and items in the
>>> fixed effects structure. So I kind of feel that including channels as
>>> random effect is correct, and having them nested in longitude:medial
>>> allows to relax the assumption that the effect in the EEG has always
>>> the
>>> same longitude:medial distribution. But I might be wrong.
>>>
>>> I thus tested differences in fit (ML) with anova() between
>>> (1|longitude:medial:channel) and the same model without the term, and
>>> a
>>> third model with the model with a simpler (1|longitude:medial).
>>>
>>> Fullmod vs Nochannel:
>>>
>>> Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq)
>>> modnoch 119 969479 970653 -484621 969241
>>> fullmod 120 968972 970156 -484366 968732 508.73 1 < 2.2e-16 ***
>>>
>>> Differences in fit is remarkable (no variance components with
>>> estimates
>>> close to zero; no correlation parameters with values close to ?1).
>>>
>>> Fullmod vs SimplerMod:
>>>
>>>    Df AIC BIC logLik deviance Chisq Chi Df Pr(>Chisq)
>>>
>>> fullmod 120 968972 970156 -484366 968732
>>> simplermod 120 969481 970665 -484621 969241 0 0 1
>>>
>>> Here the number of parameters to estimate in fullmod and simplermod
>>> is
>>> the same but the increase in fit is very consistent (-509 BIC). So I
>>> guess although the chisquare is not significant we do have a string
>>> increase in fit. As I understand this, a model with better fit will
>>> find
>>> more accurate estimates, and I would be inclined to keep the fullmod
>>> random structure.
>>>
>>> But perhaps I am missing something or I am doing something wrong.
>>> Which
>>> is the correct random structure to use?
>>>
>>> Feedbacks are very much appreciated. I often find answers in the
>>> list,
>>> and this is the first time I post a question.
>>> Thanks,
>>> Paolo
>>>
>>>
>>>
>>>


From dstr7320 at uni.sydney.edu.au  Mon Sep 28 09:00:06 2015
From: dstr7320 at uni.sydney.edu.au (Dario Strbenac)
Date: Mon, 28 Sep 2015 07:00:06 +0000
Subject: [R] Find Crossover Points of Two Spline Functions
Message-ID: <SN1PR0101MB143936DF4AA8F6C4C8F8DAE3CD4F0@SN1PR0101MB1439.prod.exchangelabs.com>

Good day,

I have two probability densities, each with a function determined by splinefun(densityResult[['x']], densityResult[['y']], "natural"), where densityResult is the output of the density function in stats. How can I determine all of the x values at which the densities cross ?

--------------------------------------
Dario Strbenac
PhD Student
University of Sydney
Camperdown NSW 2050
Australia


From tclwarrior at gmail.com  Mon Sep 28 10:23:43 2015
From: tclwarrior at gmail.com (Ali M.)
Date: Mon, 28 Sep 2015 04:23:43 -0400
Subject: [R] R applications deployment models?
Message-ID: <CA+ftKJ7o0NGozM-vyG=zCuzzpJ9NG1i2uRR53gno17w14K7hjg@mail.gmail.com>

I am new to R
And while learning the language inside the dev tools is easy and fun

I wonder how R applications are deployed and distributed to the typical
business users

I searched online of course and found some commercial options
The revolution r enterprise platform
Shiny r server from the makers of r-studio
There was also a video on youtube about a company wrapping their R
application in tcl/tk gui apps

But what else is available, what are the best practices ? is there free
alternatives to the commercial options i mentioned above?

Thanks
Ali

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Mon Sep 28 12:19:37 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 28 Sep 2015 20:19:37 +1000
Subject: [R] Boxplot: Plot outliners in a different window
In-Reply-To: <1443423018369-4712869.post@n4.nabble.com>
References: <1443423018369-4712869.post@n4.nabble.com>
Message-ID: <CA+8X3fU=i6eBo=EWggxNGHbCdX9jGhg=1T72WzdGOEA3T26pbg@mail.gmail.com>

Hi Tagmarie,
Have a look at gap.boxplot (plotrix).

JIim


On Mon, Sep 28, 2015 at 4:50 PM, Tagmarie <Ramgad82 at gmx.net> wrote:

> Hi,
> I want to draw a usual boxplot. I have one outliner way up. It makes my
> boxes being drawn tiny. I do not want to delete the outliner as it is also
> of ecological importance.
> I know there is a way of drawing a second window on top of the boxplot
> which
> starts at a different y-axis-scales and includes the outliners there. I saw
> it on a poster once. I can't find the command though. Does anyone know it?
> Best regards,
> Tagmarie
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Boxplot-Plot-outliners-in-a-different-window-tp4712869.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Mon Sep 28 15:15:01 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 28 Sep 2015 06:15:01 -0700
Subject: [R] R applications deployment models?
In-Reply-To: <CA+ftKJ7o0NGozM-vyG=zCuzzpJ9NG1i2uRR53gno17w14K7hjg@mail.gmail.com>
References: <CA+ftKJ7o0NGozM-vyG=zCuzzpJ9NG1i2uRR53gno17w14K7hjg@mail.gmail.com>
Message-ID: <59CED643-8197-4F61-99DA-98852FAB3BB4@dcn.davis.CA.us>

R is not designed as an application development programming language. Your question is a bit like asking why a car does not float like a boat. If you want to distribute analyses broadly then you are likely to either need to do it using a server or to expect users to become somewhat familiar with R.
Also, IANAL but don't forget that you will probably have obligations under the GPL if you modify R to fit it into a deployable application.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 28, 2015 1:23:43 AM PDT, "Ali M." <tclwarrior at gmail.com> wrote:
>I am new to R
>And while learning the language inside the dev tools is easy and fun
>
>I wonder how R applications are deployed and distributed to the typical
>business users
>
>I searched online of course and found some commercial options
>The revolution r enterprise platform
>Shiny r server from the makers of r-studio
>There was also a video on youtube about a company wrapping their R
>application in tcl/tk gui apps
>
>But what else is available, what are the best practices ? is there free
>alternatives to the commercial options i mentioned above?
>
>Thanks
>Ali
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From john.archie.mckown at gmail.com  Mon Sep 28 15:26:57 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Mon, 28 Sep 2015 08:26:57 -0500
Subject: [R] R applications deployment models?
In-Reply-To: <59CED643-8197-4F61-99DA-98852FAB3BB4@dcn.davis.CA.us>
References: <CA+ftKJ7o0NGozM-vyG=zCuzzpJ9NG1i2uRR53gno17w14K7hjg@mail.gmail.com>
	<59CED643-8197-4F61-99DA-98852FAB3BB4@dcn.davis.CA.us>
Message-ID: <CAAJSdjhtAAwMTChc2Ejx3mP2CMkrav_K9Pi5k4hE_4SL1uZvdQ@mail.gmail.com>

On Mon, Sep 28, 2015 at 8:15 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> R is not designed as an application development programming language.


?This is an interesting statement to me. I don't really understand it. I
have developed some applications in R. Do do you mean _commercial_
applications (i.e. something paid for)?? I think of R a bit like I think of
SAS (which may be stupid of me). There are some commercial SAS applications
(one that I know of is MXG for doing performance analysis and reporting on
a specific OS - z/OS, which runs on IBM z series "mainframes").

?<snip>?


-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From therneau at mayo.edu  Mon Sep 28 15:31:21 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Mon, 28 Sep 2015 08:31:21 -0500
Subject: [R] XPT files
In-Reply-To: <mailman.7.1443348002.26299.r-help@r-project.org>
References: <mailman.7.1443348002.26299.r-help@r-project.org>
Message-ID: <c10f8b$1h9kr2@ironport10.mayo.edu>

This was an FDA/SAS bargain a long while ago.  SAS made the XPT format publicly available 
and unchanging in return for it becoming a standard for submission.  Many packages can 
reliably read or write these files.  (The same is not true for other SAS file formats, nor 
is xport the SAS default.)  I do not know how good the R routines are, having never used them.

The following snippit is taken from
   http://www.fda.gov/downloads/ForIndustry/DataStandards/StudyDataStandards/UCM312964.pdf

2 Dataset Specifications
2.1 File Format

SAS XPORT transport file format, also called Version 5 SAS transport format, is an open 
format published by the SAS Institute. The description of this SAS transport file format 
is in the public domain. Data can be translated to and from this SAS transport format to 
other commonly usedformats without the use of programs from SAS Institute or any specific 
vendor.

Sponsors can find the record layout for SAS XPORT transport files through SAS technical 
support technical document TS-140. This document and additional information about the SAS 
Transport file layout can be found on the SAS World Wide Web page at 
http://www.sas.com/fda-esub.

---
Said document TS-140  talks about IBM 360 and Dec VAX machines but no others, which should 
give you an idea of its age.

Terry Therneau


On 09/27/2015 05:00 AM, r-help-request at r-project.org wrote:
> Peter
>
> Thanks for the explanation.  One further comment ? you wrote:
>> >I don't think the FDA "requests" XPT files
> In fact, they do make such a request.  Here is the actual language received this week (and repeatedly in the past):
>> >Program/script files should be submitted using text files (*.TXT) and the data should be submitted using SAS transport files (*.XPT).
> Dennis


From murdoch.duncan at gmail.com  Mon Sep 28 15:33:05 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 28 Sep 2015 09:33:05 -0400
Subject: [R] R applications deployment models?
In-Reply-To: <CA+ftKJ7o0NGozM-vyG=zCuzzpJ9NG1i2uRR53gno17w14K7hjg@mail.gmail.com>
References: <CA+ftKJ7o0NGozM-vyG=zCuzzpJ9NG1i2uRR53gno17w14K7hjg@mail.gmail.com>
Message-ID: <56094191.4030200@gmail.com>

On 28/09/2015 4:23 AM, Ali M. wrote:
> I am new to R
> And while learning the language inside the dev tools is easy and fun
>
> I wonder how R applications are deployed and distributed to the typical
> business users

Typically as web applications, rather than standalone executables.

>
> I searched online of course and found some commercial options
> The revolution r enterprise platform
> Shiny r server from the makers of r-studio
> There was also a video on youtube about a company wrapping their R
> application in tcl/tk gui apps
>
> But what else is available, what are the best practices ? is there free
> alternatives to the commercial options i mentioned above?

Shiny is free.  You can pay them to run the server hosting your 
application and give you access to support, but you don't need to do 
that if you don't mind running it yourself and asking the community for 
help.  I'm not familiar with the others.

Duncan Murdoch


From jdnewmil at dcn.davis.CA.us  Mon Sep 28 16:19:12 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 28 Sep 2015 07:19:12 -0700
Subject: [R] R applications deployment models?
In-Reply-To: <CAAJSdjhtAAwMTChc2Ejx3mP2CMkrav_K9Pi5k4hE_4SL1uZvdQ@mail.gmail.com>
References: <CA+ftKJ7o0NGozM-vyG=zCuzzpJ9NG1i2uRR53gno17w14K7hjg@mail.gmail.com>
	<59CED643-8197-4F61-99DA-98852FAB3BB4@dcn.davis.CA.us>
	<CAAJSdjhtAAwMTChc2Ejx3mP2CMkrav_K9Pi5k4hE_4SL1uZvdQ@mail.gmail.com>
Message-ID: <0668B786-EBEC-4A85-8F6D-7225BDB5E0E7@dcn.davis.CA.us>

I am not necessarily referring to the business model (though many people asking this question are), but rather the install-to-bare-os deployment model that controls the user experience throughout. You typically need to install R as a separate product and use it interactively to kick your "application" into gear, should you choose to develop such.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 28, 2015 6:26:57 AM PDT, John McKown <john.archie.mckown at gmail.com> wrote:
>On Mon, Sep 28, 2015 at 8:15 AM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> R is not designed as an application development programming language.
>
>
>?This is an interesting statement to me. I don't really understand it.
>I
>have developed some applications in R. Do do you mean _commercial_
>applications (i.e. something paid for)?? I think of R a bit like I
>think of
>SAS (which may be stupid of me). There are some commercial SAS
>applications
>(one that I know of is MXG for doing performance analysis and reporting
>on
>a specific OS - z/OS, which runs on IBM z series "mainframes").
>
>?<snip>?


From john.archie.mckown at gmail.com  Mon Sep 28 16:21:55 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Mon, 28 Sep 2015 09:21:55 -0500
Subject: [R] R applications deployment models?
In-Reply-To: <0668B786-EBEC-4A85-8F6D-7225BDB5E0E7@dcn.davis.CA.us>
References: <CA+ftKJ7o0NGozM-vyG=zCuzzpJ9NG1i2uRR53gno17w14K7hjg@mail.gmail.com>
	<59CED643-8197-4F61-99DA-98852FAB3BB4@dcn.davis.CA.us>
	<CAAJSdjhtAAwMTChc2Ejx3mP2CMkrav_K9Pi5k4hE_4SL1uZvdQ@mail.gmail.com>
	<0668B786-EBEC-4A85-8F6D-7225BDB5E0E7@dcn.davis.CA.us>
Message-ID: <CAAJSdjjqOhS1NtTpGBuX8WN2UzDvntSNejQ5hq6=iYavqxR0PQ@mail.gmail.com>

On Mon, Sep 28, 2015 at 9:19 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> I am not necessarily referring to the business model (though many people
> asking this question are), but rather the install-to-bare-os deployment
> model that controls the user experience throughout. You typically need to
> install R as a separate product and use it interactively to kick your
> "application" into gear, should you choose to develop such.
>

?Thanks. That helps me understand better. A difficult task on a Monday
morning after coming back from vacation! <grin/>?


-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Mon Sep 28 16:28:31 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 28 Sep 2015 14:28:31 +0000
Subject: [R] How to get significance codes after Kruskal Wallis test
In-Reply-To: <BEE39710-7924-4A47-B6F6-092A61EBAF06@comcast.net>
References: <000601d0f818$5e670de0$1b3529a0$@gmx.ch>
	<CAHq6VpdWyXW2A94jx6_uBMKh7FUEhht8iJf1aw=0phSRWupCnw@mail.gmail.com>
	<002c01d0f862$0b622810$22267830$@gmx.ch>
	<BEE39710-7924-4A47-B6F6-092A61EBAF06@comcast.net>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C6DE6@mb02.ads.tamu.edu>

The kruskalmc() function in package pgirmess performs a multiple comparisons analysis using the kruskal-wallis test. It indicates which pairs are significantly different, but it does not summarize the results in a compact letter display. 

> library(pgirmess)
> kruskalmc(Heliocide~Treatment, dta)
Multiple comparison test after Kruskal-Wallis 
p.value: 0.05 
Comparisons
            obs.dif critical.dif difference
1_2d-1_7d  6.045455     19.11021      FALSE
1_2d-3_2d  3.833333     18.69015      FALSE
1_2d-9_2d 21.500000     19.60240       TRUE
1_2d-C    17.045455     19.11021      FALSE
1_7d-3_2d  2.212121     19.11021      FALSE
1_7d-9_2d 15.454545     20.00331      FALSE
1_7d-C    23.090909     19.52123       TRUE
3_2d-9_2d 17.666667     19.60240      FALSE
3_2d-C    20.878788     19.11021       TRUE
9_2d-C    38.545455     20.00331       TRUE

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David Winsemius
Sent: Saturday, September 26, 2015 11:07 AM
To: Michael Eisenring
Cc: 'r-help'
Subject: Re: [R] How to get significance codes after Kruskal Wallis test


On Sep 26, 2015, at 6:48 AM, Michael Eisenring wrote:

> Thank you very much Kristina,
> 
> Unfortunately that's not what I am looking for.
> 
> I am just very surprised if there would be no possibility to get the significance codes for Kruskal Wallis (I would have suggested that this is a pretty common test.)

Well, it is modestly common test but its really a global test, and not a pairwise one.

> I found another option called kruskal() which does pairwise comparison, but without significance codes.
> 
> Maybe another R-list member knows more.
> 

I'm not sure I "know more", and it's very possible I "know less". In particular I don't really know what the term "significance code" actually means. (I'm hoping it's not a request for "significance stars", a feature which is roundly deprecated by more knowledgeable R users.) 

However, looking at the agricolae::kruskal function's help page and submitting the data in the non-formulaic manner it expects, I get this output very similar in form the the HSD.test output that it appeared you considered satisfied satisfactory:

(an.dta3<-kruskal(dta$Heliocide, dta$Treatment))

#--------------------
$statistics
     Chisq      p.chisq
  30.25246 4.348055e-06

$parameters
  Df ntr  t.value
   4   5 2.007584

$means
     dta$Heliocide       std  r        Min      Max
1_2d     1992.7707 1747.1879 12  334.53973 4929.372
1_7d     2368.8057 1187.9285 11  767.22881 4624.945
3_2d     2640.1286 2659.5800 12  615.91181 8559.142
9_2d     5338.6711 1579.4428 10 3328.89713 8014.897
C         397.9086  443.6019 11   75.73956 1588.431

$rankMeans
  dta$Treatment dta$Heliocide  r
1          1_2d     26.000000 12
2          1_7d     32.045455 11
3          3_2d     29.833333 12
4          9_2d     47.500000 10
5             C      8.954545 11

$comparison
NULL

$groups
   trt     means M
1 9_2d 47.500000 a
2 1_7d 32.045455 b
3 3_2d 29.833333 b
4 1_2d 26.000000 b
5 C     8.954545 c

>From context I am guessing that the "significance codes" you ask for are the items in the M column of the "groups" element of the list output.

-- 
David.

> 
> 
> Thank you,
> 
> Mike
> 
> 
> 
> Von: Kristina Wolf [mailto:kmwolf at ucdavis.edu] 
> Gesendet: Freitag, 25. September 2015 23:26
> An: Michael Eisenring <michael.eisenring at gmx.ch>
> Cc: r-help <r-help at r-project.org>
> Betreff: Re: [R] How to get significance codes after Kruskal Wallis test
> 
> 
> 
> Perhaps look into the function friedman.test.with.post.hoc()
> 
> There is more information here: http://www.r-statistics.com/wp-content/uploads/2010/02/Friedman-Test-with-Post-Hoc.r.txt
> 
> 
> 
> Note, this does not handle NA's though, and technically it is for blocked designs, but maybe it will lead you somewhere useful or could be adapted?
> 
> 
> ~ Kristina
> 
> Kristina Wolf
> Ph.D. Candidate, Graduate Group in Ecology
> M.S. Soil Science
> 
> 
> 
> On Fri, Sep 25, 2015 at 10:01 PM, Michael Eisenring <michael.eisenring at gmx.ch <mailto:michael.eisenring at gmx.ch> > wrote:
> 
> Is there a way to get significance codes after a pairwise comparisons to a
> Kruskall wallis test? With significance codes I mean letter codes (a, b,c)
> that are assigned to treatments to indicate where differences are
> significant.
> 
> With a traditional anova such a test can be performed using HSD.test from
> the agricolae library but for non parametric counterparts of anova I have
> not been able to find anything.
> 
> Can anyone help me?
> 
> Thanks mike
> 
> 
> 
> I added two example codes.
> 
> First code  represents an ANOVA and a HSD.test() giving me significant codes
> 
> #FIRST CODE USING ANOVA
> 
> library(agricolae)
> an.dta<-aov(Gossypol~Treatment,data=dta)
> summary(an.dta)
> 
> HSD.test(an.dta,"Treatment")
> # The level by alpha default is 0.05.
> outT<-HSD.test(an.dta,"Treatment", group=T)
> outT
> 
> #I receive significant codes.
> 
> 
> #SECOND CODE USING KRUSKAL WALLIs
> 
> library(agricolae)
> an.dta2<-kruskal.test(Heliocide~Treatment,dta)
> summary(an.dta2)
> 
> HSD.test(an.dta2,"Treatment")
> 
> #ERROR MESSAGE no significance codes, why??
> 
> 
> 
> #DATA FOR CODES
> 
> 
> structure(list(Treatment = structure(c(1L, 3L, 4L, 2L, 1L, 3L,
> 4L, 2L, 5L, 1L, 3L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L,
> 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L,
> 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L,
> 3L, 5L), .Label = c("1_2d", "1_7d", "3_2d", "9_2d", "C"), class = "factor"),
> 
>    Code = structure(c(1L, 2L, 3L, 4L, 18L, 19L, 20L, 21L, 22L,
>    23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L,
>    35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 44L, 45L, 46L,
>    47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 56L, 5L, 6L,
>    7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L), .Label =
> c("1_2d_1c",
>    "1_2d_3c", "1_2d_9c", "1_7d_1c", "10_2d_1c", "10_2d_3c",
>    "10_2d_9c", "10_7d_1c", "10_C", "11_2d_1c", "11_2d_3c", "11_2d_9c",
>    "11_7d_1c", "11_C", "12_2d_1c", "12_2d_3c", "12_C", "2_2d_1c",
>    "2_2d_3c", "2_2d_9c", "2_7d_1c", "2_C", "3_2d_1c", "3_2d_3c",
>    "3_7d_1c", "3_C", "4_2d_1c", "4_2d_3c", "4_2d_9c", "4_7d_1c",
>    "4_C", "5_2d_1c", "5_2d_3c", "5_2d_9c", "5_7d_1c", "5_C",
>    "6_2d_1c", "6_2d_3c", "6_2d_9c", "6_7d_1c", "6_C", "7_2d_1c",
>    "7_2d_3c", "7_2d_9c", "7_7d_1c", "7_C", "8_2d_1c", "8_2d_3c",
>    "8_2d_9c", "8_7d_1c", "8_C", "9_2d_1c", "9_2d_3c", "9_2d_9c",
>    "9_7d_1c", "9_C"), class = "factor"), Glands = c(165, 289.3333333,
>    319.3333333, 472, 334.6666667, 259, 373.3333333, 525.6666667,
>    275.3333333, 230.6666667, 346.3333333, 377.6666667, 255.3333333,
>    217.6666667, 266, 300.3333333, 354.3333333, 225.3333333,
>    294, 359, 359, 222.6666667, 103, 246.6666667, 324.6666667,
>    277, 460, 163.6666667, 226.3333333, 228, 357.6666667, 505,
>    142.6666667, 324, 278.6666667, 317.3333333, 335.6666667,
>    193.6666667, 188, 255, 252, 393.3333333, 248.3333333, 353,
>    320.6666667, 228.3333333, 497, 165.6666667, 209.3333333,
>    162.3333333, 280, 337, 169.6666667, 231.6666667, 257.6666667,
>    218.6666667), Tannin = c(0.334252451, 1.376077586, 0.896849593,
>    0.888621795, 0.464285714, 0.830236486, 0.870881783, 0.768489583,
>    0.647727273, 0.81372549, 0.51380814, 0.859923246, 0.495265152,
>    0.699932796, 1.09375, 0.785037879, 0.892650463, 0.518963675,
>    1.05859375, 0.447916667, 1.269097222, 1.147522523, 0.391276042,
>    0.883400538, 1.523989899, 0.907930108, 0.749155405, 0.450126263,
>    0.562239583, 0.911151961, 0.611111111, 1.610677083, 0.446428571,
>    0.601151316, 1.073635057, 1.359923246, 1.00154321, 0.90933642,
>    0.012054398, 1.102083333, 1.017361111, 1.052372685, 0.958607456,
>    1.224702381, 0.982291667, 1.045138889, 1.611607143, 0.662574405,
>    1.385416667, 0.464518229, 0.994444444, 1.239583333, 0.877514368,
>    0.74453125, 0.804315476, 1.024066092), H.polone = c(6754.067177,
>    22380.26652, 23622.79158, 23733.77678, 13099.20833, 23564.74907,
>    2725.016387, 18751.03986, 4283.098494, 23008.35336, 10205.56354,
>    19787.63361, 4302.050374, 7400.640798, 22442.86044, 34315.09631,
>    16498.66728, 14170.13252, 9509.1073, 6265.29637, 20671.56905,
>    14517.15648, 2643.950729, 4974.607571, 14782.87029, 13918.82361,
>    12526.27863, 1236.908141, 4854.469195, 4076.396504, 9603.950212,
>    13762.57476, 2298.727719, 3514.186757, 5705.140289, 14178.21668,
>    14277.39878, 2656.552509, 8184.633961, 9931.163373, 21474.90732,
>    18522.74376, 9884.406532, 17242.54114, 8431.506608, 14601.11606,
>    15748.4912, 2849.90903, 16747.27644, 9396.645481, 21996.95822,
>    5767.358748, 5767.358748, 14207.1734, 10353.21833, 2859.51171
>    ), Gossypol = c(1036.331811, 4171.427741, 6039.995102, 5909.068158,
>    4140.242559, 4854.985845, 6982.035521, 6132.876396, 948.2418407,
>    3618.448997, 3130.376482, 5113.942098, 1180.171957, 1500.863038,
>    4576.787021, 5629.979049, 3378.151945, 3589.187889, 2508.417927,
>    1989.576826, 5972.926124, 2867.610671, 450.7205451, 1120.955,
>    3470.09352, 3575.043632, 2952.931863, 349.0864019, 1013.807628,
>    910.8879471, 3743.331903, 3350.203452, 592.3403778, 1517.045807,
>    1504.491931, 3736.144027, 2818.419785, 723.885643, 1782.864308,
>    1414.161257, 3723.629772, 3747.076592, 2005.919344, 4198.569251,
>    2228.522959, 3322.115942, 4274.324792, 720.9785449, 2874.651764,
>    2287.228752, 5654.858696, 1247.806111, 1247.806111, 2547.326207,
>    2608.716056, 1079.846532), Heliocide = c(711.1776124, 8559.141828,
>    8014.897387, 3972.305107, 3227.467943, 5778.242027, 3628.427557,
>    3177.426984, 325.1764586, 3774.732152, 3111.880146, 4624.945228,
>    160.8912744, 336.4018128, 5207.091788, 6360.856306, 1740.091298,
>    1588.430761, 3509.141442, 685.6917982, 4664.118976, 1477.26149,
>    75.73956465, 402.1570283, 3703.317553, 4235.211434, 1730.465296,
>    91.53557346, 334.5397274, 698.1713846, 3328.897126, 1742.69355,
>    231.9097243, 513.7933372, 774.6461158, 4687.003829, 1692.296924,
>    179.1968506, 1022.628651, 1199.898583, 6132.303567, 1971.798098,
>    413.3375988, 4072.908467, 615.911814, 4906.642605, 3160.349616,
>    117.642134, 4929.371855, 616.8755006, 7428.352411, 767.2288107,
>    767.2288107, 1078.928494, 730.6740868, 425.9053258), Damage..cm. =
> c(0.4955,
>    1.516, 4.409, 3.2665, 0.491, 2.3035, 3.51, 1.8115, 0, 0.4435,
>    1.573, 1.8595, 0, 0.142, 2.171, 4.023, 4.9835, 0, 0.6925,
>    1.989, 5.683, 3.547, 0, 0.756, 2.129, 9.437, 3.211, 0, 0.578,
>    2.966, 4.7245, 1.8185, 0, 1.0475, 1.62, 5.568, 9.7455, 0,
>    0.8295, 2.411, 7.272, 4.516, 0, 0.4035, 2.974, 8.043, 4.809,
>    0, 0.6965, 1.313, 5.681, 3.474, 0, 0.5895, 2.559, 0)), .Names =
> c("Treatment",
> "Code", "Glands", "Tannin", "H.polone", "Gossypol", "Heliocide",
> "Damage..cm."), class = "data.frame", row.names = c(NA, -56L))
> 
> 
> 	[[alternative HTML version deleted]]
> 

David Winsemius
Alameda, CA, USA

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marammagdysalem at gmail.com  Mon Sep 28 17:26:56 2015
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Mon, 28 Sep 2015 17:26:56 +0200
Subject: [R] writing an equation with multiple summation
Message-ID: <CAPLSCn0Tw73tuyNELz7+dYoaKuWdUGHAJLkBFuANHzKmzxJz2A@mail.gmail.com>

Dear All,

I'm trying to write and evaluate an equation which involves multiple
summations but can't figure out how to do it.

I've an numeric vector r
r<-vector(mode = "numeric", length = m)
 and I have multiple summations (for ex.) of the form:

[(sum from r[1]=0 to g(r[1])) (sum from r[2] =0 to g(r[2]))......(sum from
r[m] to g(r[m]))] {the sum is over some complicated expression in
r[1],r[2],.....,r[m]},

where g(r[i]) = m- (r[1] +r[2]+...+r[i-1])
Any suggestions for some function or a package that can help me with this?

Many Thanks,
Maram

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Sep 28 18:17:59 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 28 Sep 2015 16:17:59 +0000
Subject: [R] Find Crossover Points of Two Spline Functions
References: <SN1PR0101MB143936DF4AA8F6C4C8F8DAE3CD4F0@SN1PR0101MB1439.prod.exchangelabs.com>
Message-ID: <loom.20150928T181139-845@post.gmane.org>

Dario Strbenac <dstr7320 <at> uni.sydney.edu.au> writes:

> 
> Good day,
> 
> I have two probability densities, each with a function determined
> by splinefun(densityResult[['x']],
> densityResult[['y']], "natural"), where densityResult is the
> output of the density function in stats.
> How can I determine all of the x values at which the densities cross ?
> 

  My initial thought was this is non-trivial, because the two densities could
cross (or nearly-but-not-quite cross) at an unlimited number of points.
I thought it would essentially boils down to "how do I find all 
the roots of an arbitrary (continuous, smooth) function?

  However, after thinking about it for a few more seconds I realize
that at least the functions are piecewise cubic.  I still don't see
a *convenient* way to do it ... if the knots were all coincident between
the two densities (maybe you could constrain them to be so?) then you
just have a difference of cubics within each segment, and you can use
polyroot() to find the roots (and throw out any that are complex or
don't fall within the segment).
  If the knots are not coincident it's more of a pain but you should
still be able to do it by considering overlapping segments ...


From jdnewmil at dcn.davis.CA.us  Mon Sep 28 18:26:30 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 28 Sep 2015 09:26:30 -0700
Subject: [R] writing an equation with multiple summation
In-Reply-To: <CAPLSCn0Tw73tuyNELz7+dYoaKuWdUGHAJLkBFuANHzKmzxJz2A@mail.gmail.com>
References: <CAPLSCn0Tw73tuyNELz7+dYoaKuWdUGHAJLkBFuANHzKmzxJz2A@mail.gmail.com>
Message-ID: <45AE1D8B-63D1-4AC3-8BA8-272590688061@dcn.davis.CA.us>

The brute force answer involves for loops or apply functions, with or without defining your own functions for calculating terms. More optimized methods usually require understanding the specific structure of the summations and unwinding them with functions like expand.grid. For more specific help, read the Posting Guide (which among other things warns you to post in plain text) and post a specific example of the type of problem that you want to solve.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 28, 2015 8:26:56 AM PDT, Maram SAlem <marammagdysalem at gmail.com> wrote:
>Dear All,
>
>I'm trying to write and evaluate an equation which involves multiple
>summations but can't figure out how to do it.
>
>I've an numeric vector r
>r<-vector(mode = "numeric", length = m)
> and I have multiple summations (for ex.) of the form:
>
>[(sum from r[1]=0 to g(r[1])) (sum from r[2] =0 to g(r[2]))......(sum
>from
>r[m] to g(r[m]))] {the sum is over some complicated expression in
>r[1],r[2],.....,r[m]},
>
>where g(r[i]) = m- (r[1] +r[2]+...+r[i-1])
>Any suggestions for some function or a package that can help me with
>this?
>
>Many Thanks,
>Maram
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Mon Sep 28 18:28:32 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 28 Sep 2015 16:28:32 +0000
Subject: [R] How to get significance codes after Kruskal Wallis test
In-Reply-To: <trinity-bb1e2d3e-1223-49e2-8d8c-848ba9230f30-1443454805747@3capp-gmx-bs33>
References: <000601d0f818$5e670de0$1b3529a0$@gmx.ch>
	<CAHq6VpdWyXW2A94jx6_uBMKh7FUEhht8iJf1aw=0phSRWupCnw@mail.gmail.com>
	<002c01d0f862$0b622810$22267830$@gmx.ch>
	<BEE39710-7924-4A47-B6F6-092A61EBAF06@comcast.net>,
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6C6DE6@mb02.ads.tamu.edu>
	<trinity-bb1e2d3e-1223-49e2-8d8c-848ba9230f30-1443454805747@3capp-gmx-bs33>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C813E@mb02.ads.tamu.edu>

It may be, but the documentation for kruskal() seems to have fallen behind updates to the function. The kruskal() manual page indicates values returned by the function that have no connection to what is actually returned and M is not described at all. 

There is another version of the test, DunnTest() in the package DescTools. It appears to be less conservative than kruskalmc(). DunnTest() finds 6 significant differences between the 10 pairs at .05 to the 4 identified by kruskalmc().

> library(DescTools)
> DunnTest(Heliocide~Treatment, dta)

 Dunn's test of multiple comparisons using rank sums : holm  

          mean.rank.diff    pval    
1_7d-1_2d       6.045455  0.5618    
3_2d-1_2d       3.833333  0.5648    
9_2d-1_2d      21.500000  0.0083 ** 
C-1_2d        -17.045455  0.0342 *  
3_2d-1_7d      -2.212121  0.5648    
9_2d-1_7d      15.454545  0.0602 .  
C-1_7d        -23.090909  0.0040 ** 
9_2d-3_2d      17.666667  0.0342 *  
C-3_2d        -20.878788  0.0083 ** 
C-9_2d        -38.545455 3.2e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

David C

From: Michael Eisenring [mailto:Michael.Eisenring at gmx.ch] 
Sent: Monday, September 28, 2015 10:40 AM
To: David L Carlson
Cc: David Winsemius; 'r-help'
Subject: Aw: RE: [R] How to get significance codes after Kruskal Wallis test

Dear David,
Thanks for your answer.
Another member of the R list pointed out that one can actually use the function
kk<-kruskal(dta$Heliocide,dta$Treatment,group=TRUE,p.adj="bonferroni")
R gives then in the Results a section$groups
$groups
? ?trt? ? ?means M
1 9_2d 47.500000 a
2 1_7d 32.045455 b
3 3_2d 29.833333 b
4 1_2d 26.000000 b
5 C? ? ?8.954545 c
?
I guess the codes under $groups in Column M are the significance codes I am looking for.
?
Thanks,
Mike
? 
Gesendet:?Montag, 28. September 2015 um 07:28 Uhr
Von:?"David L Carlson" <dcarlson at tamu.edu>
An:?"David Winsemius" <dwinsemius at comcast.net>, "Michael Eisenring" <michael.eisenring at gmx.ch>
Cc:?'r-help' <r-help at r-project.org>
Betreff:?RE: [R] How to get significance codes after Kruskal Wallis test
The kruskalmc() function in package pgirmess performs a multiple comparisons analysis using the kruskal-wallis test. It indicates which pairs are significantly different, but it does not summarize the results in a compact letter display.

> library(pgirmess)
> kruskalmc(Heliocide~Treatment, dta)
Multiple comparison test after Kruskal-Wallis
p.value: 0.05
Comparisons
obs.dif critical.dif difference
1_2d-1_7d 6.045455 19.11021 FALSE
1_2d-3_2d 3.833333 18.69015 FALSE
1_2d-9_2d 21.500000 19.60240 TRUE
1_2d-C 17.045455 19.11021 FALSE
1_7d-3_2d 2.212121 19.11021 FALSE
1_7d-9_2d 15.454545 20.00331 FALSE
1_7d-C 23.090909 19.52123 TRUE
3_2d-9_2d 17.666667 19.60240 FALSE
3_2d-C 20.878788 19.11021 TRUE
9_2d-C 38.545455 20.00331 TRUE

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David Winsemius
Sent: Saturday, September 26, 2015 11:07 AM
To: Michael Eisenring
Cc: 'r-help'
Subject: Re: [R] How to get significance codes after Kruskal Wallis test


On Sep 26, 2015, at 6:48 AM, Michael Eisenring wrote:

> Thank you very much Kristina,
>
> Unfortunately that's not what I am looking for.
>
> I am just very surprised if there would be no possibility to get the significance codes for Kruskal Wallis (I would have suggested that this is a pretty common test.)

Well, it is modestly common test but its really a global test, and not a pairwise one.

> I found another option called kruskal() which does pairwise comparison, but without significance codes.
>
> Maybe another R-list member knows more.
>

I'm not sure I "know more", and it's very possible I "know less". In particular I don't really know what the term "significance code" actually means. (I'm hoping it's not a request for "significance stars", a feature which is roundly deprecated by more knowledgeable R users.)

However, looking at the agricolae::kruskal function's help page and submitting the data in the non-formulaic manner it expects, I get this output very similar in form the the HSD.test output that it appeared you considered satisfied satisfactory:

(an.dta3<-kruskal(dta$Heliocide, dta$Treatment))

#--------------------
$statistics
Chisq p.chisq
30.25246 4.348055e-06

$parameters
Df ntr t.value
4 5 2.007584

$means
dta$Heliocide std r Min Max
1_2d 1992.7707 1747.1879 12 334.53973 4929.372
1_7d 2368.8057 1187.9285 11 767.22881 4624.945
3_2d 2640.1286 2659.5800 12 615.91181 8559.142
9_2d 5338.6711 1579.4428 10 3328.89713 8014.897
C 397.9086 443.6019 11 75.73956 1588.431

$rankMeans
dta$Treatment dta$Heliocide r
1 1_2d 26.000000 12
2 1_7d 32.045455 11
3 3_2d 29.833333 12
4 9_2d 47.500000 10
5 C 8.954545 11

$comparison
NULL

$groups
trt means M
1 9_2d 47.500000 a
2 1_7d 32.045455 b
3 3_2d 29.833333 b
4 1_2d 26.000000 b
5 C 8.954545 c

From context I am guessing that the "significance codes" you ask for are the items in the M column of the "groups" element of the list output.

--
David.

>
>
> Thank you,
>
> Mike
>
>
>
> Von: Kristina Wolf [mailto:kmwolf at ucdavis.edu]
> Gesendet: Freitag, 25. September 2015 23:26
> An: Michael Eisenring <michael.eisenring at gmx.ch>
> Cc: r-help <r-help at r-project.org>
> Betreff: Re: [R] How to get significance codes after Kruskal Wallis test
>
>
>
> Perhaps look into the function friedman.test.with.post.hoc()
>
> There is more information here: http://www.r-statistics.com/wp-content/uploads/2010/02/Friedman-Test-with-Post-Hoc.r.txt
>
>
>
> Note, this does not handle NA's though, and technically it is for blocked designs, but maybe it will lead you somewhere useful or could be adapted?
>
>
> ~ Kristina
>
> Kristina Wolf
> Ph.D. Candidate, Graduate Group in Ecology
> M.S. Soil Science
>
>
>
> On Fri, Sep 25, 2015 at 10:01 PM, Michael Eisenring <michael.eisenring at gmx.ch <mailto:michael.eisenring at gmx.ch> > wrote:
>
> Is there a way to get significance codes after a pairwise comparisons to a
> Kruskall wallis test? With significance codes I mean letter codes (a, b,c)
> that are assigned to treatments to indicate where differences are
> significant.
>
> With a traditional anova such a test can be performed using HSD.test from
> the agricolae library but for non parametric counterparts of anova I have
> not been able to find anything.
>
> Can anyone help me?
>
> Thanks mike
>
>
>
> I added two example codes.
>
> First code represents an ANOVA and a HSD.test() giving me significant codes
>
> #FIRST CODE USING ANOVA
>
> library(agricolae)
> an.dta<-aov(Gossypol~Treatment,data=dta)
> summary(an.dta)
>
> HSD.test(an.dta,"Treatment")
> # The level by alpha default is 0.05.
> outT<-HSD.test(an.dta,"Treatment", group=T)
> outT
>
> #I receive significant codes.
>
>
> #SECOND CODE USING KRUSKAL WALLIs
>
> library(agricolae)
> an.dta2<-kruskal.test(Heliocide~Treatment,dta)
> summary(an.dta2)
>
> HSD.test(an.dta2,"Treatment")
>
> #ERROR MESSAGE no significance codes, why??
>
>
>
> #DATA FOR CODES
>
>
> structure(list(Treatment = structure(c(1L, 3L, 4L, 2L, 1L, 3L,
> 4L, 2L, 5L, 1L, 3L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L,
> 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L,
> 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L, 3L, 4L, 2L, 5L, 1L,
> 3L, 5L), .Label = c("1_2d", "1_7d", "3_2d", "9_2d", "C"), class = "factor"),
>
> Code = structure(c(1L, 2L, 3L, 4L, 18L, 19L, 20L, 21L, 22L,
> 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L,
> 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 44L, 45L, 46L,
> 47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 56L, 5L, 6L,
> 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L), .Label =
> c("1_2d_1c",
> "1_2d_3c", "1_2d_9c", "1_7d_1c", "10_2d_1c", "10_2d_3c",
> "10_2d_9c", "10_7d_1c", "10_C", "11_2d_1c", "11_2d_3c", "11_2d_9c",
> "11_7d_1c", "11_C", "12_2d_1c", "12_2d_3c", "12_C", "2_2d_1c",
> "2_2d_3c", "2_2d_9c", "2_7d_1c", "2_C", "3_2d_1c", "3_2d_3c",
> "3_7d_1c", "3_C", "4_2d_1c", "4_2d_3c", "4_2d_9c", "4_7d_1c",
> "4_C", "5_2d_1c", "5_2d_3c", "5_2d_9c", "5_7d_1c", "5_C",
> "6_2d_1c", "6_2d_3c", "6_2d_9c", "6_7d_1c", "6_C", "7_2d_1c",
> "7_2d_3c", "7_2d_9c", "7_7d_1c", "7_C", "8_2d_1c", "8_2d_3c",
> "8_2d_9c", "8_7d_1c", "8_C", "9_2d_1c", "9_2d_3c", "9_2d_9c",
> "9_7d_1c", "9_C"), class = "factor"), Glands = c(165, 289.3333333,
> 319.3333333, 472, 334.6666667, 259, 373.3333333, 525.6666667,
> 275.3333333, 230.6666667, 346.3333333, 377.6666667, 255.3333333,
> 217.6666667, 266, 300.3333333, 354.3333333, 225.3333333,
> 294, 359, 359, 222.6666667, 103, 246.6666667, 324.6666667,
> 277, 460, 163.6666667, 226.3333333, 228, 357.6666667, 505,
> 142.6666667, 324, 278.6666667, 317.3333333, 335.6666667,
> 193.6666667, 188, 255, 252, 393.3333333, 248.3333333, 353,
> 320.6666667, 228.3333333, 497, 165.6666667, 209.3333333,
> 162.3333333, 280, 337, 169.6666667, 231.6666667, 257.6666667,
> 218.6666667), Tannin = c(0.334252451, 1.376077586, 0.896849593,
> 0.888621795, 0.464285714, 0.830236486, 0.870881783, 0.768489583,
> 0.647727273, 0.81372549, 0.51380814, 0.859923246, 0.495265152,
> 0.699932796, 1.09375, 0.785037879, 0.892650463, 0.518963675,
> 1.05859375, 0.447916667, 1.269097222, 1.147522523, 0.391276042,
> 0.883400538, 1.523989899, 0.907930108, 0.749155405, 0.450126263,
> 0.562239583, 0.911151961, 0.611111111, 1.610677083, 0.446428571,
> 0.601151316, 1.073635057, 1.359923246, 1.00154321, 0.90933642,
> 0.012054398, 1.102083333, 1.017361111, 1.052372685, 0.958607456,
> 1.224702381, 0.982291667, 1.045138889, 1.611607143, 0.662574405,
> 1.385416667, 0.464518229, 0.994444444, 1.239583333, 0.877514368,
> 0.74453125, 0.804315476, 1.024066092), H.polone = c(6754.067177,
> 22380.26652, 23622.79158, 23733.77678, 13099.20833, 23564.74907,
> 2725.016387, 18751.03986, 4283.098494, 23008.35336, 10205.56354,
> 19787.63361, 4302.050374, 7400.640798, 22442.86044, 34315.09631,
> 16498.66728, 14170.13252, 9509.1073, 6265.29637, 20671.56905,
> 14517.15648, 2643.950729, 4974.607571, 14782.87029, 13918.82361,
> 12526.27863, 1236.908141, 4854.469195, 4076.396504, 9603.950212,
> 13762.57476, 2298.727719, 3514.186757, 5705.140289, 14178.21668,
> 14277.39878, 2656.552509, 8184.633961, 9931.163373, 21474.90732,
> 18522.74376, 9884.406532, 17242.54114, 8431.506608, 14601.11606,
> 15748.4912, 2849.90903, 16747.27644, 9396.645481, 21996.95822,
> 5767.358748, 5767.358748, 14207.1734, 10353.21833, 2859.51171
> ), Gossypol = c(1036.331811, 4171.427741, 6039.995102, 5909.068158,
> 4140.242559, 4854.985845, 6982.035521, 6132.876396, 948.2418407,
> 3618.448997, 3130.376482, 5113.942098, 1180.171957, 1500.863038,
> 4576.787021, 5629.979049, 3378.151945, 3589.187889, 2508.417927,
> 1989.576826, 5972.926124, 2867.610671, 450.7205451, 1120.955,
> 3470.09352, 3575.043632, 2952.931863, 349.0864019, 1013.807628,
> 910.8879471, 3743.331903, 3350.203452, 592.3403778, 1517.045807,
> 1504.491931, 3736.144027, 2818.419785, 723.885643, 1782.864308,
> 1414.161257, 3723.629772, 3747.076592, 2005.919344, 4198.569251,
> 2228.522959, 3322.115942, 4274.324792, 720.9785449, 2874.651764,
> 2287.228752, 5654.858696, 1247.806111, 1247.806111, 2547.326207,
> 2608.716056, 1079.846532), Heliocide = c(711.1776124, 8559.141828,
> 8014.897387, 3972.305107, 3227.467943, 5778.242027, 3628.427557,
> 3177.426984, 325.1764586, 3774.732152, 3111.880146, 4624.945228,
> 160.8912744, 336.4018128, 5207.091788, 6360.856306, 1740.091298,
> 1588.430761, 3509.141442, 685.6917982, 4664.118976, 1477.26149,
> 75.73956465, 402.1570283, 3703.317553, 4235.211434, 1730.465296,
> 91.53557346, 334.5397274, 698.1713846, 3328.897126, 1742.69355,
> 231.9097243, 513.7933372, 774.6461158, 4687.003829, 1692.296924,
> 179.1968506, 1022.628651, 1199.898583, 6132.303567, 1971.798098,
> 413.3375988, 4072.908467, 615.911814, 4906.642605, 3160.349616,
> 117.642134, 4929.371855, 616.8755006, 7428.352411, 767.2288107,
> 767.2288107, 1078.928494, 730.6740868, 425.9053258), Damage..cm. =
> c(0.4955,
> 1.516, 4.409, 3.2665, 0.491, 2.3035, 3.51, 1.8115, 0, 0.4435,
> 1.573, 1.8595, 0, 0.142, 2.171, 4.023, 4.9835, 0, 0.6925,
> 1.989, 5.683, 3.547, 0, 0.756, 2.129, 9.437, 3.211, 0, 0.578,
> 2.966, 4.7245, 1.8185, 0, 1.0475, 1.62, 5.568, 9.7455, 0,
> 0.8295, 2.411, 7.272, 4.516, 0, 0.4035, 2.974, 8.043, 4.809,
> 0, 0.6965, 1.313, 5.681, 3.474, 0, 0.5895, 2.559, 0)), .Names =
> c("Treatment",
> "Code", "Glands", "Tannin", "H.polone", "Gossypol", "Heliocide",
> "Damage..cm."), class = "data.frame", row.names = c(NA, -56L))
>
>
> [[alternative HTML version deleted]]
>

David Winsemius
Alameda, CA, USA

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From bgunter.4567 at gmail.com  Mon Sep 28 18:36:10 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 28 Sep 2015 09:36:10 -0700
Subject: [R] Find Crossover Points of Two Spline Functions
In-Reply-To: <loom.20150928T181139-845@post.gmane.org>
References: <SN1PR0101MB143936DF4AA8F6C4C8F8DAE3CD4F0@SN1PR0101MB1439.prod.exchangelabs.com>
	<loom.20150928T181139-845@post.gmane.org>
Message-ID: <CAGxFJbRrCzTqkN6cx0CjhZp3vPtONH32xSPDkxkeSFsq_BV8_w@mail.gmail.com>

Use ?uniroot to do it numerically instead of polyroot()?

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Sep 28, 2015 at 9:17 AM, Ben Bolker <bbolker at gmail.com> wrote:
> Dario Strbenac <dstr7320 <at> uni.sydney.edu.au> writes:
>
>>
>> Good day,
>>
>> I have two probability densities, each with a function determined
>> by splinefun(densityResult[['x']],
>> densityResult[['y']], "natural"), where densityResult is the
>> output of the density function in stats.
>> How can I determine all of the x values at which the densities cross ?
>>
>
>   My initial thought was this is non-trivial, because the two densities could
> cross (or nearly-but-not-quite cross) at an unlimited number of points.
> I thought it would essentially boils down to "how do I find all
> the roots of an arbitrary (continuous, smooth) function?
>
>   However, after thinking about it for a few more seconds I realize
> that at least the functions are piecewise cubic.  I still don't see
> a *convenient* way to do it ... if the knots were all coincident between
> the two densities (maybe you could constrain them to be so?) then you
> just have a difference of cubics within each segment, and you can use
> polyroot() to find the roots (and throw out any that are complex or
> don't fall within the segment).
>   If the knots are not coincident it's more of a pain but you should
> still be able to do it by considering overlapping segments ...
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Mon Sep 28 18:47:21 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 28 Sep 2015 09:47:21 -0700
Subject: [R] Find Crossover Points of Two Spline Functions
In-Reply-To: <CAGxFJbRrCzTqkN6cx0CjhZp3vPtONH32xSPDkxkeSFsq_BV8_w@mail.gmail.com>
References: <SN1PR0101MB143936DF4AA8F6C4C8F8DAE3CD4F0@SN1PR0101MB1439.prod.exchangelabs.com>
	<loom.20150928T181139-845@post.gmane.org>
	<CAGxFJbRrCzTqkN6cx0CjhZp3vPtONH32xSPDkxkeSFsq_BV8_w@mail.gmail.com>
Message-ID: <CAGxFJbTH23Yg-H5XL=2pz-y2wkadFSQFX-ghuXq=+c=YDwUhqw@mail.gmail.com>

... (should have added)

However one might ask: Isn't this just a bit silly? The density()
function gives kernel density estimates (perhaps interpolated by
?approx -- see ?density) on as fine a grid as one likes, so why use
splines thereafter? And since these are density estimates -- i.e.
fitted approximations -- anyway, why search for "exact" solutions? Of
course I don't know the detailed context, but this whole question
sounds somewhat bogus.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Sep 28, 2015 at 9:36 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Use ?uniroot to do it numerically instead of polyroot()?
>
> Cheers,
> Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Mon, Sep 28, 2015 at 9:17 AM, Ben Bolker <bbolker at gmail.com> wrote:
>> Dario Strbenac <dstr7320 <at> uni.sydney.edu.au> writes:
>>
>>>
>>> Good day,
>>>
>>> I have two probability densities, each with a function determined
>>> by splinefun(densityResult[['x']],
>>> densityResult[['y']], "natural"), where densityResult is the
>>> output of the density function in stats.
>>> How can I determine all of the x values at which the densities cross ?
>>>
>>
>>   My initial thought was this is non-trivial, because the two densities could
>> cross (or nearly-but-not-quite cross) at an unlimited number of points.
>> I thought it would essentially boils down to "how do I find all
>> the roots of an arbitrary (continuous, smooth) function?
>>
>>   However, after thinking about it for a few more seconds I realize
>> that at least the functions are piecewise cubic.  I still don't see
>> a *convenient* way to do it ... if the knots were all coincident between
>> the two densities (maybe you could constrain them to be so?) then you
>> just have a difference of cubics within each segment, and you can use
>> polyroot() to find the roots (and throw out any that are complex or
>> don't fall within the segment).
>>   If the knots are not coincident it's more of a pain but you should
>> still be able to do it by considering overlapping segments ...
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From patrick-breheny at uiowa.edu  Mon Sep 28 16:56:42 2015
From: patrick-breheny at uiowa.edu (Patrick Breheny)
Date: Mon, 28 Sep 2015 09:56:42 -0500
Subject: [R] ASA Statistical Computing & Statistical Graphics Award
Message-ID: <5609552A.1000104@uiowa.edu>

Statistical Computing and Statistical Graphics Sections
American Statistical Association

The Statistical Computing and Graphics Award

The ASA Sections of Statistical Computing and Statistical Graphics have 
established the Statistical Computing and Graphics Award to recognize an 
individual or team for innovation in computing, software, or graphics 
that has had a great impact on statistical practice or research.

The prize carries with it a cash award of $5,000 plus an allowance of up 
to $1,000 for travel to the annual Joint Statistical Meetings (JSM) 
where the award will be presented.

The prize-winning contribution will have had significant and lasting 
impact on statistical computing, software or graphics.  The Awards 
Committee depends on the American Statistical Association membership to 
submit nominations. Committee members will review the nominations and 
make the final determination of who, if any, should receive the award. 
The award may not be given to a sitting member of the Awards Committee 
or a sitting member of the Executive Committee of the Section of 
Statistical Computing or the Section of Statistical Graphics.

Nominations are due by December 15, 2015.  The award will be presented 
at the Joint Statistical Meetings in August 2016.

Nominations should be submitted as a complete packet, consisting of:

* nomination letter, no longer than four pages, addressing the impact of 
the nominee's contribution
* nominee's curriculum vita(e)
* maximum of four supporting letters, each no longer than two pages

Nominations and questions should be sent to the Awards Chair of the 
Statistical Computing Section at the e-mail address below.

Patrick Breheny
Department of Biostatistics
University of Iowa
patrick-breheny at uiowa.edu

-- 
Patrick Breheny
Assistant Professor
Department of Biostatistics
University of Iowa
N336 College of Public Health Building
319-384-1584


From bgnumis at gmail.com  Mon Sep 28 18:07:01 2015
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Mon, 28 Sep 2015 18:07:01 +0200
Subject: [R] Hide Legend
Message-ID: <CAN25tHTt1fqmoVXCnbe8x13-ULuM1fGWjM9MPT4MHbZJiCAj1w@mail.gmail.com>

Hi all,

I want to plot with quantmode but I want to ommit levels on Lavel, showing
only Last Price, not the bands (in the case f.e. in Bollinger Bands) ?Is it
Possible? And to ommit hide all legend?

I dont find docutmentation or example.

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Mon Sep 28 19:59:35 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Mon, 28 Sep 2015 17:59:35 +0000
Subject: [R] Hide Legend
Message-ID: <248E6FA047A8C746BA491485764190F5220948DB@ESESSMB207.ericsson.se>

library(quantmod)



getSymbols("YHOO")

chartSeries(YHOO, theme="white", type='line')

chartSeries(YHOO, theme="white", type='line', TA=NULL)

chartSeries(Cl(YHOO), theme="white", type='line')

chartSeries(YHOO, theme="white", type='line', name="")


chartSeries(YHOO, type='line', theme='white')

b <- BBands(HLC(YHOO))
addTa(b, legend=NULL)

--

Giorgio Garziano



	[[alternative HTML version deleted]]


From jvadams at usgs.gov  Mon Sep 28 20:15:09 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Mon, 28 Sep 2015 13:15:09 -0500
Subject: [R] How to find out if two cells in a dataframe belong to the
 same pre-specified factor-level
In-Reply-To: <20150927212213.Horde.PswsnsW-4aMBBnP0grr3cve@webmail.uni-bremen.de>
References: <20150927212213.Horde.PswsnsW-4aMBBnP0grr3cve@webmail.uni-bremen.de>
Message-ID: <CAN5YmCEHGUVcuybrLAqmGaKrj2BuDnKrdNEOJVzJmQAdQ5m_2Q@mail.gmail.com>

Here's one approach that works.  I made some changes to the code you
provided.  Full working example code given below.

library(reshape)
library(ggplot2)
library(dplyr)

dist1 <- matrix(runif(16), 4, 4)
dist2 <- matrix(runif(16), 4, 4)
rownames(dist1) <- colnames(dist1) <- paste0("A", 1:4)
rownames(dist2) <- colnames(dist2) <- paste0("A", 1:4)
m1 <- melt(dist1)
m2 <- melt(dist2)
# I changed the by= argument here
final <- full_join(m1, m2, by=c("X1", "X2"))

# I made some changes to keep spcs character and grps factor
species <- data.frame(spcs=paste0("A", 1:4),
  grps=as.factor(c(rep("cat", 2), (rep("dog", 2)))), stringsAsFactors=FALSE)

# define new variables for final indicating group membership
final$g1 <- species$grps[match(final$X1, species$spcs)]
final$g2 <- species$grps[match(final$X2, species$spcs)]
final$group <- as.factor(with(final, ifelse(g1==g2, as.character(g1),
"dif")))

# plot just the rows with matching groups
ggplot(final[final$group!="dif", ], aes(value.x, value.y, col=group)) +
  geom_point()
# plot all the rows
ggplot(final, aes(value.x, value.y, col=group)) + geom_point()

Jean


On Sun, Sep 27, 2015 at 4:22 PM, <trichter at uni-bremen.de> wrote:

> Dear list,
> I really couldnt find a better way to describe my question, so please bear
> with me.
>
> To illustrate my problem, i have a matrix with ecological distances (m1)
> and one with genetic distances (m2) for a number of biological species. I
> have merged both matrices and want to plot both distances versus each
> other, as illustrated in this example:
>
> library(reshape)
> library(ggplot2)
> library(dplyr)
>
> dist1 <- matrix(runif(16),4,4)
> dist2 <- matrix(runif(16),4,4)
> rownames(dist1) <- colnames(dist1) <- paste0("A",1:4)
> rownames(dist2) <- colnames(dist2) <- paste0("A",1:4)
>
> m1 <- melt(dist1)
> m2 <- melt(dist2)
>
> final <- full_join(m1,m2, by=c("Var1","Var2"))
> ggplot(final, aes(value.x,value.y)) + geom_point()
>
> Here is the twist:
> The biological species belong to certain groups, which are given in the
> dataframe `species`, for example:
>
> species <- data.frame(spcs=as.character(paste0("A",1:4)),
>                       grps=as.factor(c(rep("cat",2),(rep("dog",2)))))
>
> I want to check if a x,y pair in final (as in `final$Var1`, `final$Var2`)
> belongs to the same group of species (here "cat" or "dog"), and then want
> to color all groups specifically in the x,y-scatterplot.
> Thus, i need an R translation for:
>
> final$group <- If (final$Var1 and final$Var2) belong to the same group as
> specified
>       in species, then assign the species group here, else do nothing or
> assign NA
>
> so i can proceed with
>
> ggplot(final, aes(value.x,value.y, col=group)) + geom_point()
>
> So, in the example, the pairs A1-A1, A1-A2, A2-A1, A2-A2 should be
> identified as "both cats", hence should get the factor "cat".
>
> Thank you very much!
>
>
> Tim
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From aolinto.lst at gmail.com  Mon Sep 28 20:31:38 2015
From: aolinto.lst at gmail.com (Antonio Silva)
Date: Mon, 28 Sep 2015 15:31:38 -0300
Subject: [R] a question on write.table
Message-ID: <CAE8g1gPK0KoDGD+d4rA_N96sK9U8O6wYO11KfMFR-BOnk3N46Q@mail.gmail.com>

Dear R users

I want to write a file that contains several data frames generated in a loop
ing.
I also want the column names be written to file only when it is created in
first loop.

In the example below, when I run each line separately without "for (i in
...) { }"  it works, but when I run the looping I get an error message

X<-c("A","B","C","D","E")
Y<-c(0,1,2,3,4)

for (i in 0:3) {
Y<-Y+i
data<-data.frame(X,Y)
ifelse(file.exists("test.csv"),
 write.table(data,"test.csv",row.names =
FALSE,col.names=FALSE,sep=";",append=TRUE),
 write.table(data,"test.csv",row.names = FALSE,sep=";")
)}

Error in ifelse(file.exists("test.csv"), write.table(data, "test.csv",  :
  substituto tem comprimento zero
Al?m disso: Warning message:
In rep(yes, length.out = length(ans)) :
  'x' is NULL so the result will be NULL

What is going wrong here? Thanks for any comments or suggestions.

All the best.

Antonio Olinto

	[[alternative HTML version deleted]]


From jvadams at usgs.gov  Mon Sep 28 20:30:51 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Mon, 28 Sep 2015 13:30:51 -0500
Subject: [R] Line in hist count
In-Reply-To: <CAN25tHRnB1OqPYCFfY5sV8Q00p-cp8XPzdm_qqDv__sBYJOyRA@mail.gmail.com>
References: <CAN25tHR_PBSyw4ssKFwj8Hmho65V=E5zMKaApyW+jHdAis_-UA@mail.gmail.com>
	<CAN25tHRnB1OqPYCFfY5sV8Q00p-cp8XPzdm_qqDv__sBYJOyRA@mail.gmail.com>
Message-ID: <CAN5YmCHLegFv6SWCM98TQC44r+60Wyj-YBZyAoJN_uqSDe+UzA@mail.gmail.com>

I'm not sure what you want when you refer to the "last value of f", but
perhaps this will help you out.  I simplified your example since the other
parts of the code you submitted (par, grid, title, etc.) do not have
anything to do with your question.

# some fake data for f
f <- rnorm(20)

# save the counts using default breaks
fhist <- hist(f, plot=FALSE)

# calculate the proportion
x <- fhist$counts/sum(fhist$counts)

# plot the results, and save the y values
b <- barplot(x, horiz=TRUE)

# add text to the bars
text(x, b, b, pos=2)

Jean


On Sun, Sep 27, 2015 at 4:51 PM, bgnumis bgnum <bgnumis at gmail.com> wrote:

> Hi, all
>
> I have discovered that with abline(h=dataf,col="red") can add a line as I
> want in this plot
>
> fhist<-hist(f,plot=FALSE)
> par(mar=c(6,0,6,6))
> barplot(fhist$counts/ sum(fhist$counts),axes=FALSE,
> space=0,horiz=TRUE,col="lightgray")
> grid()
> title("Marginal Distribution CDS vs. Ibex",font=4)
> abline(h=dataf,col="red")
>
> The thing is:
>
> ?How can I display the associated fhist$counts/ sum(fhist$counts on the
> last value of f?
>
> 2015-09-26 23:31 GMT+02:00 bgnumis bgnum <bgnumis at gmail.com>:
>
> > Hi all,
> >
> > Several time ago I used to work with R, now I?m returning to study and
> > work and searching old file I see that I used this code:
> >
> >
> > gfhist<-hist(gf,plot=FALSE)
> >
> > par(mar=c(6,0,6,6))
> >
> > barplot(gfhist$counts,axes=FALSE, space=0,horiz=TRUE,col="lightgray")
> >
> > grid()
> >
> > title("Marginal Distribution Lagged",font=4)
> >
> > The thing is I would line to plot a bar (horizontal and thing bar that
> > will be placed on the last gf data but on the barplot
> >
> > ?Do you think is it possible? gf is a matrix.
> >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Sep 28 20:39:08 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 28 Sep 2015 11:39:08 -0700
Subject: [R] a question on write.table
In-Reply-To: <CAE8g1gPK0KoDGD+d4rA_N96sK9U8O6wYO11KfMFR-BOnk3N46Q@mail.gmail.com>
References: <CAE8g1gPK0KoDGD+d4rA_N96sK9U8O6wYO11KfMFR-BOnk3N46Q@mail.gmail.com>
Message-ID: <4C57CF84-0858-4268-9B9E-2FE19D016091@comcast.net>


On Sep 28, 2015, at 11:31 AM, Antonio Silva wrote:

> Dear R users
> 
> I want to write a file that contains several data frames generated in a loop
> ing.
> I also want the column names be written to file only when it is created in
> first loop.
> 
> In the example below, when I run each line separately without "for (i in
> ...) { }"  it works, but when I run the looping I get an error message
> 
> X<-c("A","B","C","D","E")
> Y<-c(0,1,2,3,4)
> 
> for (i in 0:3) {
> Y<-Y+i
> data<-data.frame(X,Y)
> ifelse(file.exists("test.csv"),
> write.table(data,"test.csv",row.names =
> FALSE,col.names=FALSE,sep=";",append=TRUE),
> write.table(data,"test.csv",row.names = FALSE,sep=";")

The basic problem is that you are using ifelse(test , cons , alt) when you should be using if(test){cons}else{alt}

`ifelse` will evaluate both cons and alt. You don't want that to happen.

-- 
David.


> )}
> 
> Error in ifelse(file.exists("test.csv"), write.table(data, "test.csv",  :
>  substituto tem comprimento zero
> Al?m disso: Warning message:
> In rep(yes, length.out = length(ans)) :
>  'x' is NULL so the result will be NULL
> 
> What is going wrong here? Thanks for any comments or suggestions.
> 
> All the best.
> 
> Antonio Olinto
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ruipbarradas at sapo.pt  Mon Sep 28 20:43:33 2015
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Mon, 28 Sep 2015 19:43:33 +0100
Subject: [R] a question on write.table
In-Reply-To: <CAE8g1gPK0KoDGD+d4rA_N96sK9U8O6wYO11KfMFR-BOnk3N46Q@mail.gmail.com>
Message-ID: <20150928194333.Horde.q2l55KP9zFEDotBjNg8Qudl@mail.sapo.pt>

Hello,

ifelse is a vectorized version of if/else, you want the normal if/else.

if(file.exists(... etc ...)
??? [...]
else
??? [...]

Hope this helps,

Rui Barradas
?

Citando Antonio Silva <aolinto.lst at gmail.com>:

> Dear R users
>
> I want to write a file that contains several data frames generated in a
> loop
> ing.
> I also want the column names be written to file only when it is created
in
> first loop.
>
> In the example below, when I run each line separately without "for (i in
> ...) { }"? it works, but when I run the looping I get an error message
>
> X<-c("A","B","C","D","E")
> Y<-c(0,1,2,3,4)
>
> for (i in 0:3) {
> Y<-Y+i
> data<-data.frame(X,Y)
> ifelse(file.exists("test.csv"),
> write.table(data,"test.csv",row.names =
> FALSE,col.names=FALSE,sep=";",append=TRUE),
> write.table(data,"test.csv",row.names = FALSE,sep=";")
> )}
>
> Error in ifelse(file.exists("test.csv"), write.table(data, "test.csv",?
:
> substituto tem comprimento zero
> Al?m disso: Warning message:
> In rep(yes, length.out = length(ans)) :
> 'x' is NULL so the result will be NULL
>
> What is going wrong here? Thanks for any comments or suggestions.
>
> All the best.
>
> Antonio Olinto
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.htmland provide commented,
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Mon Sep 28 20:52:06 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Mon, 28 Sep 2015 18:52:06 +0000
Subject: [R] a question on write.table
Message-ID: <248E6FA047A8C746BA491485764190F52209494E@ESESSMB207.ericsson.se>

Try this:

X<-c("A","B","C","D","E")
Y<-c(0,1,2,3,4)

for (i in 0:3) {
  Y<-Y+i
  data<-data.frame(X,Y)
  fe.flag <- file.exists("test.csv")
  write.table(data, "test.csv", row.names = FALSE, col.names = !fe.flag, sep=";", append = fe.flag)
}




	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Mon Sep 28 21:07:48 2015
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Mon, 28 Sep 2015 21:07:48 +0200
Subject: [R] Creating World Map with Points
Message-ID: <20150928190748.GB1580@localhost.localdomain>

Dear All,
Please have a look at the snippet at the end of the email.
Essentially, I am trying to combine google maps with ggplot2.
The idea is to simply plot some points, whose size depend on a scalar,
on a google map.
My question is how I can extend the map in the snippet below in order
to plot the whole world.
Even at the lowest allowed zoom (=2), there are some continents left
out.
I cannot believe there is not a workaround for this while using the
google maps, but so far I have not made any progress at all.
Any suggestion is welcome.
Cheers

Lorenzo


##############################################################
library(ggmap)
map <- get_map(location = 'India', zoom = 2)

n <- 1000

set.seed(1234)

long <- runif(n,-180, 180)

lat <- runif(n,-90, 90)

size <- runif(n, 1,5)

data <- cbind(long, lat, size)

data <- as.data.frame(data)

gpl <- ggmap(map) +
geom_point(data = data,aes(x = long, y = lat),size=data$size,  alpha
=1, color="blue",show.legend  = F)


ggsave("test-map.pdf", gpl,width=10,height=10)


From dileepkunjaai at gmail.com  Mon Sep 28 21:11:07 2015
From: dileepkunjaai at gmail.com (=?UTF-8?B?4LSV4LWB4LSe4LWN4LSe4LS+4LSv4LS/IGt1bmphYWk=?=)
Date: Tue, 29 Sep 2015 00:41:07 +0530
Subject: [R] Singular Spectrum Analysis Plotting Skipping in a loop,
 but individually it works
In-Reply-To: <5608F24C.7090209@dewey.myzen.co.uk>
References: <CALTF6smA778Kcgh4qfvsPTqdD_Px3xeqiM3cBX3M-a3+4ZHGbg@mail.gmail.com>
	<5608F24C.7090209@dewey.myzen.co.uk>
Message-ID: <CALTF6s=u8dLq2Sn98+X2yxmZA+7Bs2FuthekvaT7dk5GPgsJgw@mail.gmail.com>

Dear Anton & Michael,

 Thanks a lot for your  nice suggestion,

Now its working....

cheers  :)

On Mon, Sep 28, 2015 at 1:24 PM, Michael Dewey <lists at dewey.myzen.co.uk>
wrote:

> Dear Dileep
>
> What happens if you explicitly print it by wrapping the plot command in
> print(   )
>
>
> On 28/09/2015 07:01, ???????? kunjaai wrote:
>
>> Dear all,
>>
>> I am trying to plot  Spectrum of Singular Values using "Rssa" package.
>>
>> I am trying to plot singular spectrum plot  inside a loop, it is not
>> plotting, but when I am trying to plot individually in terminal it works,
>> and I can save this as png files.
>>
>> My code is given below:
>>
>> *# --------------Calculating and plotting  Spectrum of Singular Values of
>> Control---------#*
>> library(ncdf)
>> library(Rssa)
>> season_list <-c('YEAR', 'DJF', 'JJA', 'SON', 'MAM')
>> zone_list <-c('ALLIN', 'WCIND', 'IPIND', 'ECIND', 'NEIND', 'NCIND',
>> 'NWIND', 'WHIND')
>>
>> for (sns in 1:length(season_list)){
>> for (rgn in 1:length(zone_list)){
>> var_noise<-paste(zone_list[rgn], "_",  season_list[sns], sep = "")
>> noise1<-get.var.ncdf(f_noise1, var_noise)
>> noise2<-get.var.ncdf(f_noise2, var_noise)
>> # Calculating Covariance Matrix from 'noise1' matrix
>> cv_noise_1 = noise1%*%t(noise1)
>> sigular_spectrum = ssa(cv_noise_1, svd.method = c("eigen"))
>>
>> out_ssa_png =
>>
>> paste("/home/dileep/WORK/Research_wind/CMIP_PiCOntrol_Expiriments/Homogenious_zone/homogenious_tempreture_zone/Homogenous_temp_Codes/Optimal_fingerprint_code/R/ECOF-package/SSA_noise_plots/SSA_",
>> zone_list[rgn], "_",  season_list[sns], "_SET_1.png", sep = "")
>>
>> png(out_ssa_png, width= 6, height    = 7.0, units = "in", res= 1200,
>> pointsize = 3)
>> print ("Created PNG file")
>> titl = paste(zone_list[rgn],  season_list[sns], sep = " ")
>> plot(sigular_spectrum , main=titl)
>> dev.off()
>> print ("Done !")
>> }
>> }
>>
>> *#####----------------------------------------------------------------------------------------###*
>>
>> Thank you in advance
>>
>>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>



-- 
DILEEPKUMAR. R
J R F, IIT DELHI

	[[alternative HTML version deleted]]


From baccts at hotmail.com  Mon Sep 28 20:46:18 2015
From: baccts at hotmail.com (C Lin)
Date: Mon, 28 Sep 2015 18:46:18 +0000
Subject: [R] merging tables based on both row and column names
In-Reply-To: <SN1PR19MB0559A6FE7AAE1B92A616E8B4CB4F0@SN1PR19MB0559.namprd19.prod.outlook.com>
References: <SN1PR19MB0559A6FE7AAE1B92A616E8B4CB4F0@SN1PR19MB0559.namprd19.prod.outlook.com>
Message-ID: <SN1PR19MB0559D1D88EA5B7F787EFCED6CB4F0@SN1PR19MB0559.namprd19.prod.outlook.com>

Dear R users,

I am trying to merge tables based on both their row names and column names.
My ultimate goal is to build a distribution table of values for each combination of row and column names. 
I have more test tables, more x's and y's than in the toy example below. 
Thanks in advance for your help.

For example :
test1 <- data.frame(rbind(c(0.1,0.2),0.3,0.1))
rownames(test1)=c('y1','y2','y3')
colnames(test1) = c('x1','x2');
test2 <- data.frame(rbind(c(0.8,0.9,0.5),c(0.5,0.1,0.6)))
rownames(test2) = c('y2','y5')
colnames(test2) = c('x1','x3','x2')

test1
       x1   x2
y1  0.1  0.2
y2  0.3  0.3
y3  0.1  0.1

test2
       x1   x3   x2
y2  0.8  0.9  0.5
y5  0.5  0.1  0.6

I would like to combine test1 and test2 such that if the column name and row name are both the same they are combined.

combined_test
           x1              x2             x3
y1      0.1              0.2           NA
y2  (0.3,0.8)    (0.3,0.5)      0.9
y3      0.1              0.1           NA
y5      0.5              0.6           0.1


From aolinto.lst at gmail.com  Mon Sep 28 22:23:59 2015
From: aolinto.lst at gmail.com (Antonio Silva)
Date: Mon, 28 Sep 2015 17:23:59 -0300
Subject: [R] a question on write.table
In-Reply-To: <248E6FA047A8C746BA491485764190F52209494E@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F52209494E@ESESSMB207.ericsson.se>
Message-ID: <CAE8g1gOA2FfTVOk_7DfPVrMmuNoqso98-_uyf7GUdkfP-eAO0w@mail.gmail.com>

Thanks Giorgio, David and Rui

With the suggestions my problem was solved in different ways.

Best regards

Antonio


2015-09-28 15:52 GMT-03:00 Giorgio Garziano <giorgio.garziano at ericsson.com>:

> Try this:
>
> X<-c("A","B","C","D","E")
> Y<-c(0,1,2,3,4)
>
> for (i in 0:3) {
>   Y<-Y+i
>   data<-data.frame(X,Y)
>   fe.flag <- file.exists("test.csv")
>   write.table(data, "test.csv", row.names = FALSE, col.names = !fe.flag,
> sep=";", append = fe.flag)
> }
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Ant?nio Olinto ?vila da Silva
Bi?logo / Ocean?grafo
Instituto de Pesca (Fisheries Institute)
S?o Paulo, Brasil

	[[alternative HTML version deleted]]


From schwidom at gmx.net  Mon Sep 28 23:23:54 2015
From: schwidom at gmx.net (Frank Schwidom)
Date: Mon, 28 Sep 2015 23:23:54 +0200
Subject: [R] merging tables based on both row and column names
In-Reply-To: <SN1PR19MB0559D1D88EA5B7F787EFCED6CB4F0@SN1PR19MB0559.namprd19.prod.outlook.com>
References: <SN1PR19MB0559A6FE7AAE1B92A616E8B4CB4F0@SN1PR19MB0559.namprd19.prod.outlook.com>
	<SN1PR19MB0559D1D88EA5B7F787EFCED6CB4F0@SN1PR19MB0559.namprd19.prod.outlook.com>
Message-ID: <20150928212354.GB4170@debian64>


test1 <- (rbind(c(0.1,0.2),0.3,0.1))
rownames(test1)=c('y1','y2','y3')
colnames(test1) = c('x1','x2');
test2 <- (rbind(c(0.8,0.9,0.5),c(0.5,0.1,0.6)))
rownames(test2) = c('y2','y5')
colnames(test2) = c('x1','x3','x2')


lTest12 <- list( test1, test2)
namesRow <- unique( unlist( lapply( lTest12, rownames)))
namesCol <- unique( unlist( lapply( lTest12, colnames)))
tmp1 <- do.call( cbind, lapply( lTest12, function( x) as.vector( x[ match( namesRow, rownames( x)), match( namesCol, colnames( x))])))
tmp2 <- apply( tmp1, 1, list)
tmp3 <- lapply( tmp2, function( x) x[[1]][ !is.na( x[[1]])])
dimnames1 <- list( namesRow, namesCol)
tmp4 <- array( data= tmp3, dim= sapply( dimnames1, length), dimnames= dimnames1)

paste( tmp4)

 [1] "0.1"         "c(0.3, 0.8)" "0.1"         "0.5"         "0.2"        
 [6] "c(0.3, 0.5)" "0.1"         "0.6"         "numeric(0)"  "0.9"        
[11] "numeric(0)"  "0.1"        

tmp4
   x1        x2        x3       
y1 0.1       0.2       Numeric,0
y2 Numeric,2 Numeric,2 0.9      
y3 0.1       0.1       Numeric,0
y5 0.5       0.6       0.1      


Regards.


On 2015-09-28 18:46:18, C Lin wrote:
> Dear R users,
> 
> I am trying to merge tables based on both their row names and column names.
> My ultimate goal is to build a distribution table of values for each combination of row and column names. 
> I have more test tables, more x's and y's than in the toy example below. 
> Thanks in advance for your help.
> 
> For example :
> test1 <- data.frame(rbind(c(0.1,0.2),0.3,0.1))
> rownames(test1)=c('y1','y2','y3')
> colnames(test1) = c('x1','x2');
> test2 <- data.frame(rbind(c(0.8,0.9,0.5),c(0.5,0.1,0.6)))
> rownames(test2) = c('y2','y5')
> colnames(test2) = c('x1','x3','x2')
> 
> test1
>        x1   x2
> y1  0.1  0.2
> y2  0.3  0.3
> y3  0.1  0.1
> 
> test2
>        x1   x3   x2
> y2  0.8  0.9  0.5
> y5  0.5  0.1  0.6
> 
> I would like to combine test1 and test2 such that if the column name and row name are both the same they are combined.
> 
> combined_test
>            x1              x2             x3
> y1      0.1              0.2           NA
> y2  (0.3,0.8)    (0.3,0.5)      0.9
> y3      0.1              0.1           NA
> y5      0.5              0.6           0.1
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bbolker at gmail.com  Tue Sep 29 00:02:20 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 28 Sep 2015 22:02:20 +0000
Subject: [R] Find Crossover Points of Two Spline Functions
References: <SN1PR0101MB143936DF4AA8F6C4C8F8DAE3CD4F0@SN1PR0101MB1439.prod.exchangelabs.com>
	<loom.20150928T181139-845@post.gmane.org>
	<CAGxFJbRrCzTqkN6cx0CjhZp3vPtONH32xSPDkxkeSFsq_BV8_w@mail.gmail.com>
Message-ID: <loom.20150929T000029-680@post.gmane.org>

Bert Gunter <bgunter.4567 <at> gmail.com> writes:

> 
> Use ?uniroot to do it numerically instead of polyroot()?
> 
> Cheers,
> Bert
> Bert Gunter

  The problem with uniroot() is that we don't know how many intersections/
roots we might be looking for. With polyroot(), we know that there can
be at most 3 roots that lie within a particular cubic segment.

  (I'm not arguing that doing this is necessarily a good idea in
the larger context.)


From ebs15242 at gmail.com  Tue Sep 29 02:33:39 2015
From: ebs15242 at gmail.com (Ed Siefker)
Date: Mon, 28 Sep 2015 19:33:39 -0500
Subject: [R] plot changes usr?
Message-ID: <CALRb-oe0DP+0zu7rWSg1rOz-5p7WMEKZ0EBv+KeH3OEkNL9vaA@mail.gmail.com>

I'm trying to plot() over an existing plot() like this:

> attach(mtcars)
> plot(mpg, hp)
> par(new=TRUE)
> par("usr")
[1]   9.46  34.84  40.68 346.32
> plot(mpg, hp, col="red", axes=FALSE, xlim=par("usr")[1:2], ylim=par("usr")[3:4], xlab="", ylab="")
> par("usr")
[1]   8.4448  35.8552  28.4544 358.5456

For some reason "usr" is changing, and so it's not plotting over the
existing data in the right place.


From kridox at ymail.com  Tue Sep 29 02:47:59 2015
From: kridox at ymail.com (Pascal Oettli)
Date: Tue, 29 Sep 2015 09:47:59 +0900
Subject: [R] FlexBayes installation from R-Forge Problem R 3.2.2
In-Reply-To: <1443399423867-4712861.post@n4.nabble.com>
References: <1443399423867-4712861.post@n4.nabble.com>
Message-ID: <CAAcyNCzfyptwATTJn6Yhq3BFhfDO-=+n57iVQXrT_cOKhy=4uQ@mail.gmail.com>

You misspelled the web address. It is "R-project", not "R.project".
Thus, the command line should be:

install.packages("FlexBayes", repos="http://R-Forge.R-project.org")

Regards,
Pascal

On Mon, Sep 28, 2015 at 9:17 AM, Davidwkatz <dkatz at tibco.com> wrote:
> I tried to install FlexBayes like this:
>
> install.packages("FlexBayes", repos="http://R-Forge.R.project.org") but got
> errors:
>
> Here's the transcript in R:
>
> R version 3.2.2 (2015-08-14) -- "Fire Safety"
> Copyright (C) 2015 The R Foundation for Statistical Computing
> Platform: x86_64-w64-mingw32/x64 (64-bit)
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
>
>   Natural language support but running in an English locale
>
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
>
>> install.packages("FlexBayes", repos="http://R-Forge.R.project.org")
> Installing package into ?C:/Users/dkatz/R/win-library/3.2?
> (as ?lib? is unspecified)
> Error: Line starting '<!DOCTYPE HTML PUBLI ...' is malformed!
>
>
> Any help will be much appreciated!
>
> Thanks,
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/FlexBayes-installation-from-R-Forge-Problem-R-3-2-2-tp4712861.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Pascal Oettli
Project Scientist
JAMSTEC
Yokohama, Japan


From dwinsemius at comcast.net  Tue Sep 29 02:57:47 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 28 Sep 2015 17:57:47 -0700
Subject: [R] plot changes usr?
In-Reply-To: <CALRb-oe0DP+0zu7rWSg1rOz-5p7WMEKZ0EBv+KeH3OEkNL9vaA@mail.gmail.com>
References: <CALRb-oe0DP+0zu7rWSg1rOz-5p7WMEKZ0EBv+KeH3OEkNL9vaA@mail.gmail.com>
Message-ID: <DE9AD64A-D0EC-4CA0-8F5E-DCDCF73D6AC6@comcast.net>


On Sep 28, 2015, at 5:33 PM, Ed Siefker wrote:

> I'm trying to plot() over an existing plot() like this:
> 
>> attach(mtcars)
>> plot(mpg, hp)
>> par(new=TRUE)
>> par("usr")
> [1]   9.46  34.84  40.68 346.32
>> plot(mpg, hp, col="red", axes=FALSE, xlim=par("usr")[1:2], ylim=par("usr")[3:4], xlab="", ylab="")
>> par("usr")
> [1]   8.4448  35.8552  28.4544 358.5456

The default usr ranges are some factor (my hazy memory says 104%) of the range of thex- and y-values unless you specify otherwise. This choice allows round data-points to be displayed at the extremes. Why are you trying to muck with the plot setup? The right way would be to use points().


> 
> For some reason "usr" is changing, and so it's not plotting over the
> existing data in the right place.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From hannah.hlx at gmail.com  Tue Sep 29 03:58:43 2015
From: hannah.hlx at gmail.com (li li)
Date: Mon, 28 Sep 2015 21:58:43 -0400
Subject: [R] cumulative distribtuion function for multinomial distribution
	in R
Message-ID: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>

Hi all,
  In R, is there a function for the cumulative distribution function
for multinomial distribution? I only see pmultinom and rmultinom which
are the prabability mass function and the function for generating
multinomial random variables respectively.
  Thanks!
    Hanna


From r.turner at auckland.ac.nz  Tue Sep 29 04:16:46 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 29 Sep 2015 15:16:46 +1300
Subject: [R] [FORGED] cumulative distribtuion function for multinomial
 distribution in R
In-Reply-To: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
References: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
Message-ID: <5609F48E.6000504@auckland.ac.nz>

On 29/09/15 14:58, li li wrote:
> Hi all,
>    In R, is there a function for the cumulative distribution function
> for multinomial distribution? I only see pmultinom and rmultinom which
> are the prabability mass function and the function for generating
> multinomial random variables respectively.

A moment's Googling would have led you to pmvnorm in package "mvtnorm".

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dkatz at tibco.com  Tue Sep 29 03:07:46 2015
From: dkatz at tibco.com (David Katz)
Date: Mon, 28 Sep 2015 18:07:46 -0700
Subject: [R] FlexBayes installation from R-Forge Problem R 3.2.2
In-Reply-To: <CAAcyNCzfyptwATTJn6Yhq3BFhfDO-=+n57iVQXrT_cOKhy=4uQ@mail.gmail.com>
References: <1443399423867-4712861.post@n4.nabble.com>
	<CAAcyNCzfyptwATTJn6Yhq3BFhfDO-=+n57iVQXrT_cOKhy=4uQ@mail.gmail.com>
Message-ID: <CAA+Ab90SGootqFgCJDSj6yKFoc2rgW8z1txeasJzG_UHDx-FcQ@mail.gmail.com>

Pascal,

Oops. Thanks!


*David Katz*| IAG, TIBCO Spotfire

O: 1.541.203.7084 | M: 1.541.324.7417



On Mon, Sep 28, 2015 at 5:47 PM, Pascal Oettli <kridox at ymail.com> wrote:

> You misspelled the web address. It is "R-project", not "R.project".
> Thus, the command line should be:
>
> install.packages("FlexBayes", repos="http://R-Forge.R-project.org")
>
> Regards,
> Pascal
>
> On Mon, Sep 28, 2015 at 9:17 AM, Davidwkatz <dkatz at tibco.com> wrote:
> > I tried to install FlexBayes like this:
> >
> > install.packages("FlexBayes", repos="http://R-Forge.R.project.org") but
> got
> > errors:
> >
> > Here's the transcript in R:
> >
> > R version 3.2.2 (2015-08-14) -- "Fire Safety"
> > Copyright (C) 2015 The R Foundation for Statistical Computing
> > Platform: x86_64-w64-mingw32/x64 (64-bit)
> >
> > R is free software and comes with ABSOLUTELY NO WARRANTY.
> > You are welcome to redistribute it under certain conditions.
> > Type 'license()' or 'licence()' for distribution details.
> >
> >   Natural language support but running in an English locale
> >
> > R is a collaborative project with many contributors.
> > Type 'contributors()' for more information and
> > 'citation()' on how to cite R or R packages in publications.
> >
> > Type 'demo()' for some demos, 'help()' for on-line help, or
> > 'help.start()' for an HTML browser interface to help.
> > Type 'q()' to quit R.
> >
> >> install.packages("FlexBayes", repos="http://R-Forge.R.project.org")
> > Installing package into ?C:/Users/dkatz/R/win-library/3.2?
> > (as ?lib? is unspecified)
> > Error: Line starting '<!DOCTYPE HTML PUBLI ...' is malformed!
> >
> >
> > Any help will be much appreciated!
> >
> > Thanks,
> >
> >
> >
> > --
> > View this message in context:
> http://r.789695.n4.nabble.com/FlexBayes-installation-from-R-Forge-Problem-R-3-2-2-tp4712861.html
> > Sent from the R help mailing list archive at Nabble.com.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Pascal Oettli
> Project Scientist
> JAMSTEC
> Yokohama, Japan
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Tue Sep 29 08:53:48 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 29 Sep 2015 16:53:48 +1000
Subject: [R] plot changes usr?
In-Reply-To: <DE9AD64A-D0EC-4CA0-8F5E-DCDCF73D6AC6@comcast.net>
References: <CALRb-oe0DP+0zu7rWSg1rOz-5p7WMEKZ0EBv+KeH3OEkNL9vaA@mail.gmail.com>
	<DE9AD64A-D0EC-4CA0-8F5E-DCDCF73D6AC6@comcast.net>
Message-ID: <CA+8X3fWQ5A2jPLz=c3CtPM0gZzAkikduQxdCac26tNMuMX95mQ@mail.gmail.com>

Hi Ed,
While David's suggestion is correct, the change in par("usr") results from
your resetting the plot limits.

data(mtcars)
plot(mtcars$mpg, mtcars$hp)
par("usr")
[1]   9.46  34.84  40.68 346.32
par(new=TRUE)
plot(mtcars$mpg, mtcars$hp,col="red",axes=FALSE)
par("usr")
[1]   9.46  34.84  40.68 346.32

Jim

On Tue, Sep 29, 2015 at 10:57 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> On Sep 28, 2015, at 5:33 PM, Ed Siefker wrote:
>
> > I'm trying to plot() over an existing plot() like this:
> >
> >> attach(mtcars)
> >> plot(mpg, hp)
> >> par(new=TRUE)
> >> par("usr")
> > [1]   9.46  34.84  40.68 346.32
> >> plot(mpg, hp, col="red", axes=FALSE, xlim=par("usr")[1:2],
> ylim=par("usr")[3:4], xlab="", ylab="")
> >> par("usr")
> > [1]   8.4448  35.8552  28.4544 358.5456
>
> The default usr ranges are some factor (my hazy memory says 104%) of the
> range of thex- and y-values unless you specify otherwise. This choice
> allows round data-points to be displayed at the extremes. Why are you
> trying to muck with the plot setup? The right way would be to use points().
>
>
> >
> > For some reason "usr" is changing, and so it's not plotting over the
> > existing data in the right place.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Tue Sep 29 10:52:52 2015
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Tue, 29 Sep 2015 09:52:52 +0100
Subject: [R] cumulative distribtuion function for multinomial
 distribution in R
In-Reply-To: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
Message-ID: <20150929095252.Horde.4EzjaUQqMrD0vrK-9rA_L-A@mail.sapo.pt>

Hello,

Actually, the probability mass function should be dmultinom. I think
pmultinom is what you want.

Hope this helps,

Rui Barradas
?

Citando li li <hannah.hlx at gmail.com>:

> Hi all,
> In R, is there a function for the cumulative distribution function
> for multinomial distribution? I only see pmultinom and rmultinom which
> are the prabability mass function and the function for generating
> multinomial random variables respectively.
> Thanks!
> ? ?Hanna
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.htmland provide commented,
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From highstat at highstat.com  Tue Sep 29 11:12:08 2015
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Tue, 29 Sep 2015 10:12:08 +0100
Subject: [R] Stats course: Lisbon, Portugal
Message-ID: <560A55E8.3080807@highstat.com>

Apologies for cross-posting

We would like to announce the following statistics course:

Course: Data exploration, regression, GLM & GAM with introduction to R
Where: Lisbon, Portugal
When: 8-12 February 2016

Course website: http://www.highstat.com/statscourse.htm
Flyer: http://highstat.com/Courses/Flyers/Flyer2016_02Lisbon_RGG.pdf



Kind regards,

Alain Zuur




-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Tue Sep 29 11:16:59 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 29 Sep 2015 22:16:59 +1300
Subject: [R] [FORGED] cumulative distribtuion function for multinomial
 distribution in R
In-Reply-To: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
References: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
Message-ID: <560A570B.50807@auckland.ac.nz>

On 29/09/15 14:58, li li wrote:
> Hi all,
>    In R, is there a function for the cumulative distribution function
> for multinomial distribution? I only see pmultinom and rmultinom which
> are the prabability mass function and the function for generating
> multinomial random variables respectively.


Dennis Murphy has kindly pointed out to me, off-list, that I miss-read 
your message.  You *clearly* say "multinomial" and *NOT* "multivariate 
normal"!!!  Psigh!

OTOH upon reading your message properly I see that it says you have 
*found* pmultinom.  Did you really mean that?  The function pmultinom
is or would be the cumulative distribution function that you desire.
The probability mass function would be called *dmultinom*.  That is 
probably (???) what you found.  There seems to be no pmultinom in the
default packages or in MASS.

However, if one Googles on "multinomial cumulative distribution R" one 
is quickly led to the pmultinom() function from the "MFSAS" package.

Yayyyy!

*HOWEVER* (!!!) if one tries to install that package one is informed 
that it is not available for R 3.3.2.

Booooo!

I found the archived source of the package OK, and it seems to install 
OK except for a minor squawk about vignettes.  So if you have the 
capability to install from source you would appear to be home and hosed.

I could not find "MFSAS" amongst the orphaned packages.  It has not 
clear why it isn't among them.  Can anyone enlighten me about that?

The DESCRIPTION file in the source package from the Archives says 
nothing about "orphaned".

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From trichter at uni-bremen.de  Tue Sep 29 11:27:33 2015
From: trichter at uni-bremen.de (Tim Richter-Heitmann)
Date: Tue, 29 Sep 2015 11:27:33 +0200
Subject: [R] How to find out if two cells in a dataframe belong to the
 same pre-specified factor-level
In-Reply-To: <CAN5YmCEHGUVcuybrLAqmGaKrj2BuDnKrdNEOJVzJmQAdQ5m_2Q@mail.gmail.com>
References: <20150927212213.Horde.PswsnsW-4aMBBnP0grr3cve@webmail.uni-bremen.de>
	<CAN5YmCEHGUVcuybrLAqmGaKrj2BuDnKrdNEOJVzJmQAdQ5m_2Q@mail.gmail.com>
Message-ID: <560A5985.8060108@uni-bremen.de>

Thank you, that turned out to work very well.

If you want to, you can answer it here:

http://stackoverflow.com/questions/32809249/how-to-find-out-if-two-cells-in-a-dataframe-belong-to-the-same-pre-specified-fac/

The question wasnt properly answered, which is why i switched to R-list.


On 28.09.2015 20:15, Adams, Jean wrote:
> Here's one approach that works.  I made some changes to the code you 
> provided.  Full working example code given below.
>
> library(reshape)
> library(ggplot2)
> library(dplyr)
>
> dist1 <- matrix(runif(16), 4, 4)
> dist2 <- matrix(runif(16), 4, 4)
> rownames(dist1) <- colnames(dist1) <- paste0("A", 1:4)
> rownames(dist2) <- colnames(dist2) <- paste0("A", 1:4)
> m1 <- melt(dist1)
> m2 <- melt(dist2)
> # I changed the by= argument here
> final <- full_join(m1, m2, by=c("X1", "X2"))
>
> # I made some changes to keep spcs character and grps factor
> species <- data.frame(spcs=paste0("A", 1:4),
>   grps=as.factor(c(rep("cat", 2), (rep("dog", 2)))), 
> stringsAsFactors=FALSE)
>
> # define new variables for final indicating group membership
> final$g1 <- species$grps[match(final$X1, species$spcs)]
> final$g2 <- species$grps[match(final$X2, species$spcs)]
> final$group <- as.factor(with(final, ifelse(g1==g2, as.character(g1), 
> "dif")))
>
> # plot just the rows with matching groups
> ggplot(final[final$group!="dif", ], aes(value.x, value.y, col=group)) +
>   geom_point()
> # plot all the rows
> ggplot(final, aes(value.x, value.y, col=group)) + geom_point()
>
> Jean
>
>
> On Sun, Sep 27, 2015 at 4:22 PM, <trichter at uni-bremen.de 
> <mailto:trichter at uni-bremen.de>> wrote:
>
>     Dear list,
>     I really couldnt find a better way to describe my question, so
>     please bear with me.
>
>     To illustrate my problem, i have a matrix with ecological
>     distances (m1) and one with genetic distances (m2) for a number of
>     biological species. I have merged both matrices and want to plot
>     both distances versus each other, as illustrated in this example:
>
>     library(reshape)
>     library(ggplot2)
>     library(dplyr)
>
>     dist1 <- matrix(runif(16),4,4)
>     dist2 <- matrix(runif(16),4,4)
>     rownames(dist1) <- colnames(dist1) <- paste0("A",1:4)
>     rownames(dist2) <- colnames(dist2) <- paste0("A",1:4)
>
>     m1 <- melt(dist1)
>     m2 <- melt(dist2)
>
>     final <- full_join(m1,m2, by=c("Var1","Var2"))
>     ggplot(final, aes(value.x,value.y)) + geom_point()
>
>     Here is the twist:
>     The biological species belong to certain groups, which are given
>     in the dataframe `species`, for example:
>
>     species <- data.frame(spcs=as.character(paste0("A",1:4)),
>     grps=as.factor(c(rep("cat",2),(rep("dog",2)))))
>
>     I want to check if a x,y pair in final (as in `final$Var1`,
>     `final$Var2`) belongs to the same group of species (here "cat" or
>     "dog"), and then want to color all groups specifically in the
>     x,y-scatterplot.
>     Thus, i need an R translation for:
>
>     final$group <- If (final$Var1 and final$Var2) belong to the same
>     group as specified
>           in species, then assign the species group here, else do
>     nothing or assign NA
>
>     so i can proceed with
>
>     ggplot(final, aes(value.x,value.y, col=group)) + geom_point()
>
>     So, in the example, the pairs A1-A1, A1-A2, A2-A1, A2-A2 should be
>     identified as "both cats", hence should get the factor "cat".
>
>     Thank you very much!
>
>
>     Tim
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Tim Richter-Heitmann (M.Sc.)
PhD Candidate



International Max-Planck Research School for Marine Microbiology
University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069


From giorgio.garziano at ericsson.com  Tue Sep 29 13:22:14 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 29 Sep 2015 11:22:14 +0000
Subject: [R] merging tables based on both row and column names
Message-ID: <248E6FA047A8C746BA491485764190F522094D00@ESESSMB207.ericsson.se>

Another approach:

test1 <- data.frame(rbind(c(0.1,0.2),0.3,0.1))
rownames(test1) = c('y1','y2','y3')
colnames(test1) = c('x1','x2');
test2 <- data.frame(rbind(c(0.8,0.9,0.5),c(0.5,0.1,0.6)))
rownames(test2) = c('y2','y5')
colnames(test2) = c('x1','x3','x2')

> test1
    x1  x2
y1 0.1 0.2
y2 0.3 0.3
y3 0.1 0.1

> test2
    x1  x3  x2
y2 0.8 0.9 0.5
y5 0.5 0.1 0.6

t1.r <- rownames(test1)
t2.r <- rownames(test2)
t1.c <- colnames(test1)
t2.c <- colnames(test2)

col <- unique(union(t1.c, t2.c))
ncol <- length(col)
row <- unique(union(t1.r, t2.r))
nrow <- length(row)

m <- matrix(list(), nrow=nrow, ncol=ncol)
rownames(m) <- row
colnames(m) <- col

for (i in 1:nrow) {
 for (j in 1:ncol) {
     rowname <- row[i]
     colname <- col[j]
     v <- c()
     if (!is.null(test1[rowname, colname]) && !is.na(test1[rowname, colname])) {
       v <- c(test1[rowname, colname])
     }
     if (!is.null(test2[rowname, colname]) && !is.na(test2[rowname, colname])) {
       v <- c(v, test2[rowname, colname])
     }
     if (!is.null(v)) {
       m[rowname, colname] <- list(v)
     } else {
       m[rowname, colname] <- NA
     }
  }
}


> m
x1        x2        x3
y1 0.1       0.2       NA
y2 Numeric,2 Numeric,2 0.9
y3 0.1       0.1       NA
y5 0.5       0.6       0.1


> m["y2",]
$x1
[1] 0.3 0.8

$x2
[1] 0.3 0.5

$x3
[1] 0.9

--
Giorgio Garziano

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Tue Sep 29 14:35:52 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 29 Sep 2015 14:35:52 +0200
Subject: [R] [FORGED] cumulative distribtuion function for multinomial
	distribution in R
In-Reply-To: <5609F48E.6000504@auckland.ac.nz>
References: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
	<5609F48E.6000504@auckland.ac.nz>
Message-ID: <147DEAAA-6759-4403-87E4-7CDDD0E9646D@gmail.com>


On 29 Sep 2015, at 04:16 , Rolf Turner <r.turner at auckland.ac.nz> wrote:

> On 29/09/15 14:58, li li wrote:
>> Hi all,
>>   In R, is there a function for the cumulative distribution function
>> for multinomial distribution? I only see pmultinom and rmultinom which
>> are the prabability mass function and the function for generating
>> multinomial random variables respectively.
> 
> A moment's Googling would have led you to pmvnorm in package "mvtnorm".

A moment spent with a cup of strong coffee would have led you to realize that multinomial and multivariate normal are two different things....  ;-)

- pd

> 
> cheers,
> 
> Rolf Turner
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From giorgio.garziano at ericsson.com  Tue Sep 29 15:22:09 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 29 Sep 2015 13:22:09 +0000
Subject: [R] Creating World Map with Points
Message-ID: <248E6FA047A8C746BA491485764190F522094DD6@ESESSMB207.ericsson.se>

I had to update low/up longitude and latitude attributes of the zoom=1 map
with those of zoom=2 map.

library(ggmap)

# the zoom=2 map works with ggmap(), however it does not show Americas and all Pacific Ocean

map <- get_map(location = 'India', zoom=2)
bb <- attr(map, "bb")
bb

# the zoom=1 shows all entire world

map <- get_map(location = 'India', zoom=1)
attr(map, "bb")

# changing latitude and longitude upper and lower bounds
attr(map, "bb") <- bb
bb

# now your code
n <- 1000
set.seed(1234)
long <- runif(n,-180, 180)
lat <- runif(n,-90, 90)
size <- runif(n, 1,5)
data <- cbind(long, lat, size)
data <- as.data.frame(data)

gpl <- ggmap(map) +
  geom_point(data = data, aes(x = long, y = lat), size=data$size,
             alpha=1, color="blue", show.legend  = F)
plot(gpl)


--
Giorgio Garziano


	[[alternative HTML version deleted]]


From marammagdysalem at gmail.com  Tue Sep 29 16:59:23 2015
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Tue, 29 Sep 2015 16:59:23 +0200
Subject: [R] writing an equation with multiple summation
In-Reply-To: <CAPLSCn0Tw73tuyNELz7+dYoaKuWdUGHAJLkBFuANHzKmzxJz2A@mail.gmail.com>
References: <CAPLSCn0Tw73tuyNELz7+dYoaKuWdUGHAJLkBFuANHzKmzxJz2A@mail.gmail.com>
Message-ID: <CAPLSCn1Gj+7zBshBBK9CHU6k2iFiW80c2=i7BHKvGwLUyPer1g@mail.gmail.com>

My first problem is that the counters involved in the summation are
dependent, For instance for each r[i], 0<r[i]< m-(r[1] +r[2]+...+r[i-1])
 so I tried using seq() and for loops to express all the possible
combinations but I couldn't.


On 28 September 2015 at 17:26, Maram SAlem <marammagdysalem at gmail.com>
wrote:

> Dear All,
>
> I'm trying to write and evaluate an equation which involves multiple
> summations but can't figure out how to do it.
>
> I've an numeric vector r
> r<-vector(mode = "numeric", length = m)
>  and I have multiple summations (for ex.) of the form:
>
> [(sum from r[1]=0 to g(r[1])) (sum from r[2] =0 to g(r[2]))......(sum from
> r[m] to g(r[m]))] {the sum is over some complicated expression in
> r[1],r[2],.....,r[m]},
>
> where g(r[i]) = m- (r[1] +r[2]+...+r[i-1])
> Any suggestions for some function or a package that can help me with this?
>
> Many Thanks,
> Maram
>
>

	[[alternative HTML version deleted]]


From therneau at mayo.edu  Tue Sep 29 17:00:30 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Tue, 29 Sep 2015 15:00:30 +0000
Subject: [R] flatten a list
Message-ID: <c10f8b$1hk83e@ironport10.mayo.edu>

I'd like to flatten a list from 2 levels to 1 level.  This has to be easy, but is currently opaque to me.

temp <- list(1:3, list(letters[1:3], duh= 5:8),  zed=15:17)

Desired result would be a 4 element list.
[[1]] 1:3
[[2]] "a", "b", "c"
[[duh]] 5:8
[[zed]] 15:17

(Preservation of the names is not important)

Terry T

From oma.gonzales at gmail.com  Tue Sep 29 17:24:52 2015
From: oma.gonzales at gmail.com (=?UTF-8?B?T21hciBBbmRyw6kgR29uesOhbGVzIETDrWF6?=)
Date: Tue, 29 Sep 2015 10:24:52 -0500
Subject: [R] Regex: just keep the money and not the description
Message-ID: <CAM-xyZjKJVSmPMmOs1eWvC9o=HBY1LFaVnbd1fBg3GG9MZHfEw@mail.gmail.com>

Hi R users, I have a character vector with 2 numbers: old price, new
price. The problem is that some rows (4,23, for example) contain a
little description of the product, which I don't need.

I've tried a lot of thins, like this one:

TV_Precios3 <- gsub("^S ^[0-9]{2}\\$","",TV_Precios2)

Without result. Any help is welcome.

After that I want to separate them with colsplit (have this solved)
based on "S/.".



head(TV_Precios3,50)
 [1] "S/. 2,499.00S/. 1,999.00"
 [2] "S/. 2,299.00  S/. 1,599.00"
 [3] "S/. 2,299.00  S/. 1,599.00"
 [4] "S 40\" FULL HD 40LF6350S/. 1,999.00S/. 1,699.00"
 [5] "S/. 5,999.00S/. 4,799.00"
 [6] "S/. 3,499.00S/. 2,999.00"
 [7] "S/. 4,799.00S/. 3,699.00"
 [8] "S/. 599.00"
 [9] "S/. 1,299.00"
[10] "S/. 999.00  S/. 799.00"
[11] "S/. 1,999.00  S/. 1,699.00"
[12] "S/. 999.00  S/. 849.00"
[13] "S/. 499.00  S/. 439.00"
[14] "S610S/. 1,899.00"
[15] "S/. 1,799.00S/. 1,699.00"
[16] "S/. 2,299.00S/. 1,699.00"
[17] "S/. 8,999.00S/. 7,299.00"
[18] "S9000S/. 10,999.00S/. 8,999.00"
[19] "S9000S/. 14,999.00S/. 12,999.00"
[20] "S/. 6,999.00S/. 5,999.00"
[21] "S/. 2,799.00S/. 2,299.00"
[22] "S/. 2,999.00S/. 2,649.00"
[23] "SMART 49LF5900S/. 2,399.00S/. 2,149.00"
[24] "S/. 2,299.00  S/. 1,599.00"


From ggrothendieck at gmail.com  Tue Sep 29 17:26:57 2015
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Sep 2015 11:26:57 -0400
Subject: [R] flatten a list
In-Reply-To: <c10f8b$1hk83e@ironport10.mayo.edu>
References: <c10f8b$1hk83e@ironport10.mayo.edu>
Message-ID: <CAP01uRkRwzH-0a9JrBEkgTBWMb0B5csADy3xS+M+YdZ8Gn8WbQ@mail.gmail.com>

> do.call(c, lapply(temp, function(x) if (is.list(x)) x else list(x)))
[[1]]
[1] 1 2 3

[[2]]
[1] "a" "b" "c"

$duh
[1] 5 6 7 8

$zed
[1] 15 16 17


On Tue, Sep 29, 2015 at 11:00 AM, Therneau, Terry M., Ph.D. <
therneau at mayo.edu> wrote:

> I'd like to flatten a list from 2 levels to 1 level.  This has to be easy,
> but is currently opaque to me.
>
> temp <- list(1:3, list(letters[1:3], duh= 5:8),  zed=15:17)
>
> Desired result would be a 4 element list.
> [[1]] 1:3
> [[2]] "a", "b", "c"
> [[duh]] 5:8
> [[zed]] 15:17
>
> (Preservation of the names is not important)
>
> Terry T
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From oma.gonzales at gmail.com  Tue Sep 29 17:30:32 2015
From: oma.gonzales at gmail.com (=?UTF-8?B?T21hciBBbmRyw6kgR29uesOhbGVzIETDrWF6?=)
Date: Tue, 29 Sep 2015 10:30:32 -0500
Subject: [R] Fwd: Regex: just keep the money and not the description
In-Reply-To: <CAM-xyZjKJVSmPMmOs1eWvC9o=HBY1LFaVnbd1fBg3GG9MZHfEw@mail.gmail.com>
References: <CAM-xyZjKJVSmPMmOs1eWvC9o=HBY1LFaVnbd1fBg3GG9MZHfEw@mail.gmail.com>
Message-ID: <CAM-xyZhqYTqrN3Yu313KwH0oPZ0eCR5XPQNHEAMB5kBomFuczg@mail.gmail.com>

Excuse me, here is the vector:

TV_Precios2 <- c("S/. 2,499.00S/. 1,999.00", "S/. 2,299.00  S/.
1,599.00", "S/. 2,299.00  S/. 1,599.00",
"S 40\" FULL HD 40LF6350S/. 1,999.00S/. 1,699.00", "S/. 5,999.00S/. 4,799.00",
"S/. 3,499.00S/. 2,999.00", "S/. 4,799.00S/. 3,699.00", "S/. 599.00",
"S/. 1,299.00", "S/. 999.00  S/. 799.00", "S/. 1,999.00  S/. 1,699.00",
"S/. 999.00  S/. 849.00", "S/. 499.00  S/. 439.00", "S610S/. 1,899.00",
"S/. 1,799.00S/. 1,699.00", "S/. 2,299.00S/. 1,699.00", "S/.
8,999.00S/. 7,299.00",
"S9000S/. 10,999.00S/. 8,999.00", "S9000S/. 14,999.00S/. 12,999.00",
"S/. 6,999.00S/. 5,999.00", "S/. 2,799.00S/. 2,299.00", "S/.
2,999.00S/. 2,649.00",
"SMART 49LF5900S/. 2,399.00S/. 2,149.00", "S/. 2,299.00  S/. 1,599.00"
)

---------- Forwarded message ----------
From: Omar Andr? Gonz?les D?az <oma.gonzales at gmail.com>
Date: 2015-09-29 10:24 GMT-05:00
Subject: Regex: just keep the money and not the description
To: "r-help at R-project.org" <r-help at r-project.org>


Hi R users, I have a character vector with 2 numbers: old price, new
price. The problem is that some rows (4,23, for example) contain a
little description of the product, which I don't need.

I've tried a lot of thins, like this one:

TV_Precios3 <- gsub("^S ^[0-9]{2}\\$","",TV_Precios2)

Without result. Any help is welcome.

After that I want to separate them with colsplit (have this solved)
based on "S/.".



head(TV_Precios3,50)
 [1] "S/. 2,499.00S/. 1,999.00"
 [2] "S/. 2,299.00  S/. 1,599.00"
 [3] "S/. 2,299.00  S/. 1,599.00"
 [4] "S 40\" FULL HD 40LF6350S/. 1,999.00S/. 1,699.00"
 [5] "S/. 5,999.00S/. 4,799.00"
 [6] "S/. 3,499.00S/. 2,999.00"
 [7] "S/. 4,799.00S/. 3,699.00"
 [8] "S/. 599.00"
 [9] "S/. 1,299.00"
[10] "S/. 999.00  S/. 799.00"
[11] "S/. 1,999.00  S/. 1,699.00"
[12] "S/. 999.00  S/. 849.00"
[13] "S/. 499.00  S/. 439.00"
[14] "S610S/. 1,899.00"
[15] "S/. 1,799.00S/. 1,699.00"
[16] "S/. 2,299.00S/. 1,699.00"
[17] "S/. 8,999.00S/. 7,299.00"
[18] "S9000S/. 10,999.00S/. 8,999.00"
[19] "S9000S/. 14,999.00S/. 12,999.00"
[20] "S/. 6,999.00S/. 5,999.00"
[21] "S/. 2,799.00S/. 2,299.00"
[22] "S/. 2,999.00S/. 2,649.00"
[23] "SMART 49LF5900S/. 2,399.00S/. 2,149.00"
[24] "S/. 2,299.00  S/. 1,599.00"


From giorgio.garziano at ericsson.com  Tue Sep 29 17:58:23 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 29 Sep 2015 15:58:23 +0000
Subject: [R] flatten a list
Message-ID: <248E6FA047A8C746BA491485764190F522094ED9@ESESSMB207.ericsson.se>

To mention also:

temp <- list(1:3, list(letters[1:3], duh= 5:8),  zed=15:17)

library(rlist)
list.flatten(temp)


[[1]]

[1] 1 2 3



[[2]]

[1] "a" "b" "c"



$duh

[1] 5 6 7 8



$zed

[1] 15 16 17


---

Giorgio Garziano

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Tue Sep 29 18:10:26 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 29 Sep 2015 16:10:26 +0000
Subject: [R] flatten a list
Message-ID: <248E6FA047A8C746BA491485764190F522094F0C@ESESSMB207.ericsson.se>

If you need further info on flattening a list, check this out:

http://stackoverflow.com/questions/8139677/how-to-flatten-a-list-to-a-list-without-coercion/8139959#8139959




	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Tue Sep 29 18:31:23 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 29 Sep 2015 09:31:23 -0700
Subject: [R] plot changes usr?
In-Reply-To: <CA+8X3fWQ5A2jPLz=c3CtPM0gZzAkikduQxdCac26tNMuMX95mQ@mail.gmail.com>
References: <CALRb-oe0DP+0zu7rWSg1rOz-5p7WMEKZ0EBv+KeH3OEkNL9vaA@mail.gmail.com>
	<DE9AD64A-D0EC-4CA0-8F5E-DCDCF73D6AC6@comcast.net>
	<CA+8X3fWQ5A2jPLz=c3CtPM0gZzAkikduQxdCac26tNMuMX95mQ@mail.gmail.com>
Message-ID: <CAF8bMcbz0TU9O4p8KfTw66QO2Z9fidPZzeLnceVeaCvTLg9nAw@mail.gmail.com>

If you want to set the axis limits to exactly your given xlim and ylim, also use
xaxs="i" and yaxs="i" (x or y "axis style" is "internal") in your plot
command.  E.g.,

 plot(xaxs="i", yaxs="i", 1:10, 87:96, xlim=c(-0.75, 13.2), ylim=c(81.03,100.2))
 par("usr")
 #[1]  -0.75  13.20  81.03 100.20

as compared to

 plot(1:10, 87:96, xlim=c(-0.75, 13.2), ylim=c(81.03,100.2))
 par("usr")
 #[1]  -1.3080  13.7580  80.2632 100.9668

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Sep 28, 2015 at 11:53 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi Ed,
> While David's suggestion is correct, the change in par("usr") results from
> your resetting the plot limits.
>
> data(mtcars)
> plot(mtcars$mpg, mtcars$hp)
> par("usr")
> [1]   9.46  34.84  40.68 346.32
> par(new=TRUE)
> plot(mtcars$mpg, mtcars$hp,col="red",axes=FALSE)
> par("usr")
> [1]   9.46  34.84  40.68 346.32
>
> Jim
>
> On Tue, Sep 29, 2015 at 10:57 AM, David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>>
>> On Sep 28, 2015, at 5:33 PM, Ed Siefker wrote:
>>
>> > I'm trying to plot() over an existing plot() like this:
>> >
>> >> attach(mtcars)
>> >> plot(mpg, hp)
>> >> par(new=TRUE)
>> >> par("usr")
>> > [1]   9.46  34.84  40.68 346.32
>> >> plot(mpg, hp, col="red", axes=FALSE, xlim=par("usr")[1:2],
>> ylim=par("usr")[3:4], xlab="", ylab="")
>> >> par("usr")
>> > [1]   8.4448  35.8552  28.4544 358.5456
>>
>> The default usr ranges are some factor (my hazy memory says 104%) of the
>> range of thex- and y-values unless you specify otherwise. This choice
>> allows round data-points to be displayed at the extremes. Why are you
>> trying to muck with the plot setup? The right way would be to use points().
>>
>>
>> >
>> > For some reason "usr" is changing, and so it's not plotting over the
>> > existing data in the right place.
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From markshanks101 at hotmail.com  Tue Sep 29 18:49:50 2015
From: markshanks101 at hotmail.com (markshanks)
Date: Tue, 29 Sep 2015 09:49:50 -0700 (PDT)
Subject: [R] Analysis of causal relations between rare (categorical)
	events
In-Reply-To: <1500a89f26c.giorgio.garziano@tin.it>
References: <1443220420164-4712801.post@n4.nabble.com>
	<1500a89f26c.giorgio.garziano@tin.it>
Message-ID: <1443545390587-4712951.post@n4.nabble.com>

Thanks Giorgio and Jim. This info is exactly what I needed.



--
View this message in context: http://r.789695.n4.nabble.com/Analysis-of-causal-relations-between-rare-categorical-events-tp4712801p4712951.html
Sent from the R help mailing list archive at Nabble.com.


From engin.toksoz at gm.com  Tue Sep 29 19:25:37 2015
From: engin.toksoz at gm.com (Engin Toksoz)
Date: Tue, 29 Sep 2015 17:25:37 +0000
Subject: [R] Fwd: Regex: just keep the money and not the description
In-Reply-To: <CAM-xyZhqYTqrN3Yu313KwH0oPZ0eCR5XPQNHEAMB5kBomFuczg@mail.gmail.com>
References: <CAM-xyZjKJVSmPMmOs1eWvC9o=HBY1LFaVnbd1fBg3GG9MZHfEw@mail.gmail.com>
	<CAM-xyZhqYTqrN3Yu313KwH0oPZ0eCR5XPQNHEAMB5kBomFuczg@mail.gmail.com>
Message-ID: <9cb8df4ae21c4c74a57cee6c242cb137@DCWIPPEXCH079.nam.corp.gm.com>

gsub(pattern = "(S[^/]{0,}/.)([:space:]{0,})([0-9,]{0,})(\n){0,}",replace="", TV_Precios2)

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Omar Andr? Gonz?les D?az
Sent: Tuesday, September 29, 2015 11:31 AM
To: r-help at R-project.org
Subject: [R] Fwd: Regex: just keep the money and not the description

Excuse me, here is the vector:

TV_Precios2 <- c("S/. 2,499.00S/. 1,999.00", "S/. 2,299.00  S/.
1,599.00", "S/. 2,299.00  S/. 1,599.00", "S 40\" FULL HD 40LF6350S/. 1,999.00S/. 1,699.00", "S/. 5,999.00S/. 4,799.00", "S/. 3,499.00S/. 2,999.00", "S/. 4,799.00S/. 3,699.00", "S/. 599.00", "S/. 1,299.00", "S/. 999.00  S/. 799.00", "S/. 1,999.00  S/. 1,699.00", "S/. 999.00  S/. 849.00", "S/. 499.00  S/. 439.00", "S610S/. 1,899.00", "S/. 1,799.00S/. 1,699.00", "S/. 2,299.00S/. 1,699.00", "S/.
8,999.00S/. 7,299.00",
"S9000S/. 10,999.00S/. 8,999.00", "S9000S/. 14,999.00S/. 12,999.00", "S/. 6,999.00S/. 5,999.00", "S/. 2,799.00S/. 2,299.00", "S/.
2,999.00S/. 2,649.00",
"SMART 49LF5900S/. 2,399.00S/. 2,149.00", "S/. 2,299.00  S/. 1,599.00"
)

---------- Forwarded message ----------
From: Omar Andr? Gonz?les D?az <oma.gonzales at gmail.com>
Date: 2015-09-29 10:24 GMT-05:00
Subject: Regex: just keep the money and not the description
To: "r-help at R-project.org" <r-help at r-project.org>


Hi R users, I have a character vector with 2 numbers: old price, new price. The problem is that some rows (4,23, for example) contain a little description of the product, which I don't need.

I've tried a lot of thins, like this one:

TV_Precios3 <- gsub("^S ^[0-9]{2}\\$","",TV_Precios2)

Without result. Any help is welcome.

After that I want to separate them with colsplit (have this solved) based on "S/.".



head(TV_Precios3,50)
 [1] "S/. 2,499.00S/. 1,999.00"
 [2] "S/. 2,299.00  S/. 1,599.00"
 [3] "S/. 2,299.00  S/. 1,599.00"
 [4] "S 40\" FULL HD 40LF6350S/. 1,999.00S/. 1,699.00"
 [5] "S/. 5,999.00S/. 4,799.00"
 [6] "S/. 3,499.00S/. 2,999.00"
 [7] "S/. 4,799.00S/. 3,699.00"
 [8] "S/. 599.00"
 [9] "S/. 1,299.00"
[10] "S/. 999.00  S/. 799.00"
[11] "S/. 1,999.00  S/. 1,699.00"
[12] "S/. 999.00  S/. 849.00"
[13] "S/. 499.00  S/. 439.00"
[14] "S610S/. 1,899.00"
[15] "S/. 1,799.00S/. 1,699.00"
[16] "S/. 2,299.00S/. 1,699.00"
[17] "S/. 8,999.00S/. 7,299.00"
[18] "S9000S/. 10,999.00S/. 8,999.00"
[19] "S9000S/. 14,999.00S/. 12,999.00"
[20] "S/. 6,999.00S/. 5,999.00"
[21] "S/. 2,799.00S/. 2,299.00"
[22] "S/. 2,999.00S/. 2,649.00"
[23] "SMART 49LF5900S/. 2,399.00S/. 2,149.00"
[24] "S/. 2,299.00  S/. 1,599.00"

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


Nothing in this message is intended to constitute an electronic signature unless a specific statement to the contrary is included in this message.

Confidentiality Note: This message is intended only for the person or entity to which it is addressed. It may contain confidential and/or privileged material. Any review, transmission, dissemination or other use, or taking of any action in reliance upon this message by persons or entities other than the intended recipient is prohibited and may be unlawful. If you received this message in error, please contact the sender and delete it from your computer.

From dale_ramsden at med.unc.edu  Tue Sep 29 20:54:10 2015
From: dale_ramsden at med.unc.edu (Daler)
Date: Tue, 29 Sep 2015 11:54:10 -0700 (PDT)
Subject: [R] X Stacked bars of Y values,
 with bar segments colored according to Z
Message-ID: <1443552850609-4712955.post@n4.nabble.com>

I'm trying to generate a bar plot from the following data frame
seq	left	deleted	right	mh
1	125	175	132	0
2	125	225	82	3
3	200	150	82	3
4	300	80	52	2
5	165	205	62	7
where x="seq", y is a stack of "left", "deleted", and "right" values. then
color left and right segments with color from palette (e.g. brewer 
yellow-orange-red) as determined by z="mh"
Any help appreciated



--
View this message in context: http://r.789695.n4.nabble.com/X-Stacked-bars-of-Y-values-with-bar-segments-colored-according-to-Z-tp4712955.html
Sent from the R help mailing list archive at Nabble.com.


From r.turner at auckland.ac.nz  Tue Sep 29 22:44:55 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 30 Sep 2015 09:44:55 +1300
Subject: [R] [FORGED] cumulative distribtuion function for multinomial
 distribution in R
In-Reply-To: <147DEAAA-6759-4403-87E4-7CDDD0E9646D@gmail.com>
References: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
	<5609F48E.6000504@auckland.ac.nz>
	<147DEAAA-6759-4403-87E4-7CDDD0E9646D@gmail.com>
Message-ID: <560AF847.1050407@auckland.ac.nz>

On 30/09/15 01:35, peter dalgaard wrote:
>
> On 29 Sep 2015, at 04:16 , Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
>> On 29/09/15 14:58, li li wrote:
>>> Hi all, In R, is there a function for the cumulative distribution
>>> function for multinomial distribution? I only see pmultinom and
>>> rmultinom which are the prabability mass function and the
>>> function for generating multinomial random variables
>>> respectively.
>>
>> A moment's Googling would have led you to pmvnorm in package
>> "mvtnorm".
>
> A moment spent with a cup of strong coffee would have led you to
> realize that multinomial and multivariate normal are two different
> things....  ;-)


Yeah, well, some brains would help too.

Be that as it were, did you see my follow-up post in respect of the 
(actually relevant) MFSAS package and it's "orphaned" (or not) status?

The package (source version in the Archives) seems to be basically OK. 
Did the maintainer just disappear off the face of the earth?

According to my reading of the README, this should instigate an 
"orphaned" annotation in the DESCRIPTION file, but there doesn't seem to 
be one.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From waddawanna at hotmail.com  Wed Sep 30 01:49:18 2015
From: waddawanna at hotmail.com (waddawanna)
Date: Tue, 29 Sep 2015 16:49:18 -0700 (PDT)
Subject: [R] Matrix element-by-element multiplication
In-Reply-To: <1988657a-bd29-47f6-a85e-a088c62b85b5@kedge2.utk.tennessee.edu>
References: <1988657a-bd29-47f6-a85e-a088c62b85b5@kedge2.utk.tennessee.edu>
Message-ID: <1443570558062-4712964.post@n4.nabble.com>

Hello Steven,

It looks like, there is no in-built function that can do GAUSS ".*"
element-wise multiplication.
Now, if you want to make the desired computations in R, it is actually
preatty straightforward.

> a<-c(1,2,3)
> b<-matrix(rep(1:9,1),3,3,byrow=TRUE)
> a*b

That, should work fine. But, suppose that for some reason you have following
situation, which can make you trip for hours of sleepless nights. That is,
you have a matrix "b", where number of columns equal to number of columns of
your vector "a". That is

> b<-matrix(rep(1:9,1),3,3,byrow=TRUE);b
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9

> a <- matrix(rep(1:3,1),1,3,byrow=TRUE)
     [,1] [,2] [,3]
[1,]    1    2    3

If you try to do elementwise multilication, i.e., of those two
> b*a

You get an error that they are not comfomable, that is why, you have to
write your own function (here, 
I just write the for-loop):
> for ( i in 1:3 ) {
     foo[ ,i] = ( foo[ ,i] * bar[1,i] ) ;
   }
       [,1] [,2] [,3]
[1,]    1    4    9
[2,]    4   10   18
[3,]    7   16   27

I hope that this helped
Serge Boris Nesterenko
CEO 50 Pence Music Production
https://www.linkedin.com/in/sergenesterenko



--
View this message in context: http://r.789695.n4.nabble.com/Matrix-element-by-element-multiplication-tp3992206p4712964.html
Sent from the R help mailing list archive at Nabble.com.


From michael.eisenring at gmx.ch  Wed Sep 30 02:08:22 2015
From: michael.eisenring at gmx.ch (Michael Eisenring)
Date: Tue, 29 Sep 2015 17:08:22 -0700
Subject: [R] Calculate Total CORRECTED SS for non linear regression
Message-ID: <001401d0fb14$201466b0$603d3410$@gmx.ch>

Hello

I started to work on nonlinear regressions recently,

I want to compare a set of models and would like to use an equivalent of R2
for  nonlinear regression models.

I learned that a good alternative is to calculate the following corrected
formula:

 

 1 - (Residual SS/Total Corrected SS)

 

This provides a measure of how well the model fits versus just the overall
mean for the response variable.

 

With some help I could program the formula for my model and , but I am not
quite sure if it is correct and what exactly the total CORRECTED SS is
(compared to the Total SS)

Can anyone tell me how to calculate the Total Corrected SS in R and how it
can be implemented in my code?

 

CODE and INPUT:

 

 


# REGRESSION NON-LINEAR REGRESSION MODEL
FIT------------------------------------
dta<-read.csv("input.csv",header=T, sep = ",")
dput(dta)
head(dta)

 

# loading packages: analysis of mixed effect models
library(nls2)#model

 

 


#EQUATION 1: y~yo+a*(1-b^x)
#Aim: fit equation to data: y~yo+a*(1-b^x)
# y =Compound (from my data set) x= Damage (from my data set)
#The other 3 parameters are unknown: yo=Intercept, a= assymptote ans b=slope

plot(Compound~Damage, dta)
# Looking at the plot, 0 is a plausible estimate for y0:
# a+y0 is the asymptote, so estimate about 4000;
# b is between 0 and 1, so estimate .5
dta.nls <- nls(Compound~y0+a*(1-b^Damage), dta,
               start=list(y0=0, a=4000, b=.5))

xval <- seq(0, 10, 0.1)
lines(xval, predict(dta.nls, data.frame(Damage=xval)))
profile(dta.nls, alpha= .05)


summary(dta.nls)


#Calculation R2 Variation expl: total Variation--> Is deviance(dta.nls) the
same as Total Corrected SS???
CompoundSS <- sum((dta$Compound - mean(dta$Compound))^2)
R2 <- deviance(dta.nls)/CompoundSS
R2
#R2=0.5723023

 

 

 

structure(list(Treatment = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 
4L, 1L, 2L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 
1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 
1L, 2L, 3L, 4L, 1L, 2L, 4L), .Label = c("1_2d", "3_2d", "9_2d", 
"C"), class = "factor"), Compound = c(1036.331811, 4171.427741, 
6039.995102, 4140.242559, 4854.985845, 6982.035521, 948.2418407, 
3618.448997, 3130.376482, 1180.171957, 1500.863038, 4576.787021, 
5629.979049, 3589.187889, 2508.417927, 1989.576826, 5972.926124, 
450.7205451, 1120.955, 3470.09352, 3575.043632, 349.0864019, 
1013.807628, 910.8879471, 3743.331903, 592.3403778, 1517.045807, 
1504.491931, 3736.144027, 723.885643, 1782.864308, 1414.161257, 
3723.629772, 2005.919344, 4198.569251, 2228.522959, 3322.115942, 
720.9785449, 2874.651764, 2287.228752, 5654.858696, 1247.806111, 
2547.326207, 2608.716056, 1079.846532), Damage = c(0.4955, 1.516, 
4.409, 0.491, 2.3035, 3.51, 0, 0.4435, 1.573, 0, 0.142, 2.171, 
4.023, 0, 0.6925, 1.989, 5.683, 0, 0.756, 2.129, 9.437, 0, 0.578, 
2.966, 4.7245, 0, 1.0475, 1.62, 5.568, 0, 0.8295, 2.411, 7.272, 
0, 0.4035, 2.974, 8.043, 0, 0.6965, 1.313, 5.681, 0, 0.5895, 
2.559, 0)), .Names = c("Treatment", "Compound", "Damage"), class =
"data.frame", row.names = c(NA, 
-45L))

 

 

 


	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Wed Sep 30 02:14:56 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 30 Sep 2015 13:14:56 +1300
Subject: [R] Matrix element-by-element multiplication
In-Reply-To: <1443570558062-4712964.post@n4.nabble.com>
References: <1988657a-bd29-47f6-a85e-a088c62b85b5@kedge2.utk.tennessee.edu>
	<1443570558062-4712964.post@n4.nabble.com>
Message-ID: <560B2980.2020600@auckland.ac.nz>

On 30/09/15 12:49, waddawanna wrote:
> Hello Steven,
>
> It looks like, there is no in-built function that can do GAUSS ".*"
> element-wise multiplication.
> Now, if you want to make the desired computations in R, it is actually
> preatty straightforward.
>
>> a<-c(1,2,3)
>> b<-matrix(rep(1:9,1),3,3,byrow=TRUE)
>> a*b
>
> That, should work fine. But, suppose that for some reason you have following
> situation, which can make you trip for hours of sleepless nights. That is,
> you have a matrix "b", where number of columns equal to number of columns of
> your vector "a". That is
>
>> b<-matrix(rep(1:9,1),3,3,byrow=TRUE);b
>       [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    4    5    6
> [3,]    7    8    9
>
>> a <- matrix(rep(1:3,1),1,3,byrow=TRUE)
>       [,1] [,2] [,3]
> [1,]    1    2    3
>
> If you try to do elementwise multilication, i.e., of those two
>> b*a
>
> You get an error that they are not comfomable, that is why, you have to
> write your own function (here,
> I just write the for-loop):
>> for ( i in 1:3 ) {
>       foo[ ,i] = ( foo[ ,i] * bar[1,i] ) ;
>     }
>         [,1] [,2] [,3]
> [1,]    1    4    9
> [2,]    4   10   18
> [3,]    7   16   27
>
> I hope that this helped.


(1) You're making heavy weather by using rep() totally unnecessarily.


(2) The example you give can be done much more succinctly; for-loops
are unnecessary:

    t(as.vector(a)*t(b))

(3) Please don't post to R-help via nabble (WTF ever that is).  It 
messes up everything.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Wed Sep 30 06:25:38 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 29 Sep 2015 21:25:38 -0700
Subject: [R] Matrix element-by-element multiplication
In-Reply-To: <1443570558062-4712964.post@n4.nabble.com>
References: <1988657a-bd29-47f6-a85e-a088c62b85b5@kedge2.utk.tennessee.edu>
	<1443570558062-4712964.post@n4.nabble.com>
Message-ID: <3B87F9B7-AA7F-4430-BFA4-387EE0F37A40@comcast.net>


On Sep 29, 2015, at 4:49 PM, waddawanna wrote:

> Hello Steven,
> 
> It looks like, there is no in-built function that can do GAUSS ".*"
> element-wise multiplication.
> Now, if you want to make the desired computations in R, it is actually
> preatty straightforward.
> 
>> a<-c(1,2,3)
>> b<-matrix(rep(1:9,1),3,3,byrow=TRUE)
>> a*b
> 
> That, should work fine. But, suppose that for some reason you have following
> situation, which can make you trip for hours of sleepless nights. That is,
> you have a matrix "b", where number of columns equal to number of columns of
> your vector "a". That is
> 
>> b<-matrix(rep(1:9,1),3,3,byrow=TRUE);b
>     [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    4    5    6
> [3,]    7    8    9
> 
>> a <- matrix(rep(1:3,1),1,3,byrow=TRUE)
>     [,1] [,2] [,3]
> [1,]    1    2    3
> 
> If you try to do elementwise multilication, i.e., of those two
>> b*a
> 
> You get an error that they are not comfomable,

You should not have gotten that error with either `*` or `%*%`.

> that is why, you have to
> write your own function (here, 
> I just write the for-loop):
>> for ( i in 1:3 ) {
>     foo[ ,i] = ( foo[ ,i] * bar[1,i] ) ;
>   }
>       [,1] [,2] [,3]
> [1,]    1    4    9
> [2,]    4   10   18
> [3,]    7   16   27

You were expecting c(1,2,3) to be a row-vector. That leads to disappointment. If anything it will be a column-vector.

Notice .... no error:
> a*b
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    8   10   12
[3,]   21   24   27


You really wanted to sweep that vector

> sweep(b, 1,a,'*')   # Sweeping by rows is same a multiplying by recycle column vector 
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    8   10   12
[3,]   21   24   27

> sweep(b, 2, a, '*')  # You wanted to "sweep" across columns
     [,1] [,2] [,3]
[1,]    1    4    9
[2,]    4   10   18
[3,]    7   16   27

-- 

David Winsemius
Alameda, CA, USA


From tiago.d.henriques at tecnico.ulisboa.pt  Wed Sep 30 02:56:05 2015
From: tiago.d.henriques at tecnico.ulisboa.pt (tiago)
Date: Tue, 29 Sep 2015 17:56:05 -0700 (PDT)
Subject: [R] Self-starting nonlinear power law function
In-Reply-To: <529F00FB.3030707@gwdg.de>
References: <529F00FB.3030707@gwdg.de>
Message-ID: <1443574565862-4712967.post@n4.nabble.com>

Hi Christoph! Actually I had this doubt and thanks for the help!

Although i have kind of a problem: while you just have one variable (x) i
have three and respective interactions (let's say x1, x2 and x3 where x2 and
x3 are categorical variables) the self-construct is still applicable? 

Best regards,
Tiago Henriques



--
View this message in context: http://r.789695.n4.nabble.com/Self-starting-nonlinear-power-law-function-tp4681626p4712967.html
Sent from the R help mailing list archive at Nabble.com.


From pdalgd at gmail.com  Wed Sep 30 09:55:13 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 30 Sep 2015 09:55:13 +0200
Subject: [R] [FORGED] cumulative distribtuion function for multinomial
	distribution in R
In-Reply-To: <560AF847.1050407@auckland.ac.nz>
References: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
	<5609F48E.6000504@auckland.ac.nz>
	<147DEAAA-6759-4403-87E4-7CDDD0E9646D@gmail.com>
	<560AF847.1050407@auckland.ac.nz>
Message-ID: <BEBF7404-8F3D-4F6E-BDAB-C8131D2D5D83@gmail.com>

I know exactly this:

https://cran.r-project.org/web/packages/MFSAS/index.html

As I understand things (but I am not a CRAN maintainer): This is compatible with a situation where the maintainer exists but doesn't update the package. I think a package only gets to orphaned status if the maintainer steps down or disappears. 

-pd

> On 29 Sep 2015, at 22:44 , Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> On 30/09/15 01:35, peter dalgaard wrote:
>> 
>> On 29 Sep 2015, at 04:16 , Rolf Turner <r.turner at auckland.ac.nz> wrote:
>> 
>>> On 29/09/15 14:58, li li wrote:
>>>> Hi all, In R, is there a function for the cumulative distribution
>>>> function for multinomial distribution? I only see pmultinom and
>>>> rmultinom which are the prabability mass function and the
>>>> function for generating multinomial random variables
>>>> respectively.
>>> 
>>> A moment's Googling would have led you to pmvnorm in package
>>> "mvtnorm".
>> 
>> A moment spent with a cup of strong coffee would have led you to
>> realize that multinomial and multivariate normal are two different
>> things....  ;-)
> 
> 
> Yeah, well, some brains would help too.
> 
> Be that as it were, did you see my follow-up post in respect of the (actually relevant) MFSAS package and it's "orphaned" (or not) status?
> 
> The package (source version in the Archives) seems to be basically OK. Did the maintainer just disappear off the face of the earth?
> 
> According to my reading of the README, this should instigate an "orphaned" annotation in the DESCRIPTION file, but there doesn't seem to be one.
> 
> cheers,
> 
> Rolf
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Wed Sep 30 10:09:04 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 30 Sep 2015 10:09:04 +0200
Subject: [R] Calculate Total CORRECTED SS for non linear regression
In-Reply-To: <001401d0fb14$201466b0$603d3410$@gmx.ch>
References: <001401d0fb14$201466b0$603d3410$@gmx.ch>
Message-ID: <0A3CE045-B316-424B-B724-C6C5AB829ED1@gmail.com>


> On 30 Sep 2015, at 02:08 , Michael Eisenring <michael.eisenring at gmx.ch> wrote:
> 
> Can anyone tell me how to calculate the Total Corrected SS in R and how it
> can be implemented in my code?

It is just sum((y-mean(y))^2).

Beware that a fair amount of (somewhat silly) contention is going on in this area, though. In particular, the formula can give negative values, which is unfortunate if you try calling it "R^2".
 
-pd

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From drjimlemon at gmail.com  Wed Sep 30 10:44:51 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 30 Sep 2015 18:44:51 +1000
Subject: [R] X Stacked bars of Y values,
 with bar segments colored according to Z
In-Reply-To: <1443552850609-4712955.post@n4.nabble.com>
References: <1443552850609-4712955.post@n4.nabble.com>
Message-ID: <CA+8X3fX=oVzqkGV6aGdLxQxQD+dxHtkWk+8udG-nRdjqcP0o_g@mail.gmail.com>

Hi Dale,
This apparently simple plot turned out to be fairly messy. "barplot"
doesn't seem to accept a matrix of colors, and "barp" doesn't yet do a
stacked plot, so I can only suggest a fairly labor intensive one-off plot:

barcol<-color.scale(x$mh,extremes=c("yellow","red"))
barheights<-rowSums(x[,2:4])
library(plotrix)
barp(barheights,xlab="x",main="Stacked bar plot")
rect(0.6,0,1.4,x[1,2],col=barcol[1])
rect(0.6,x[1,2]+x[1,3],1.4,barheights[1],col=barcol[1])
rect(1.6,0,2.4,x[2,2],col=barcol[2])
rect(1.6,x[2,2]+x[2,3],2.4,barheights[2],col=barcol[2])
rect(2.6,0,3.4,x[3,2],col=barcol[3])
rect(2.6,x[3,2]+x[3,3],3.4,barheights[3],col=barcol[3])
rect(3.6,0,4.4,x[4,2],col=barcol[4])
rect(3.6,x[4,2]+x[4,3],4.4,barheights[4],col=barcol[4])
rect(4.6,0,5.4,x[5,2],col=barcol[5])
rect(4.6,x[5,2]+x[5,3],5.4,barheights[5],col=barcol[5])

Can be streamlined a bit if it's worth the trouble

Jim


On Wed, Sep 30, 2015 at 4:54 AM, Daler <dale_ramsden at med.unc.edu> wrote:

> I'm trying to generate a bar plot from the following data frame
> seq     left    deleted right   mh
> 1       125     175     132     0
> 2       125     225     82      3
> 3       200     150     82      3
> 4       300     80      52      2
> 5       165     205     62      7
> where x="seq", y is a stack of "left", "deleted", and "right" values. then
> color left and right segments with color from palette (e.g. brewer
> yellow-orange-red) as determined by z="mh"
> Any help appreciated
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/X-Stacked-bars-of-Y-values-with-bar-segments-colored-according-to-Z-tp4712955.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Rainer at krugs.de  Wed Sep 30 10:46:59 2015
From: Rainer at krugs.de (Rainer M Krug)
Date: Wed, 30 Sep 2015 10:46:59 +0200
Subject: [R] optimizing with non-linear constraints
Message-ID: <m24mic2t18.fsf@krugs.de>

Hi

I have posted the following question on stackoverflow [1] but haven't
received a response yet. Has somebody here any idea, how to formulate
non-linear constraints using e.g. the packages alabama or nlopr? The
question is pasted below.

I am really stuck with this.

Thanks,

Rainer

,----
| I am successful in using constrOptim() for linear constraints (thanks to
| a comment in my earlier question), but I now have a problem with
| non-linear constraints.
| 
| Consider the following example:
| 
| I want to fit a function FUN(X1, X2, ... X12)
| 
|  X1 = dep.a
|  X2 = dep.b
|  X3 = dep.c
|  ... for all z0. , na. and zjoint. 
| 
| and have the following constraint
| (link to formulas:
| http://stackoverflow.com/questions/32845169/how-to-optimize-with-non-linear-constraints-in-r
| )
| | 
| With some re-formating and h > 0 and LAI > 0 I get for thew first one
| 
| -X1 <= LAI^X2 / X3 < 1 - X1
| 
| where LAI is simply a number which constant for each fit.
| 
| I saw the packages alabama and nloptr but I don't manage to get my head
| around how I can specify these types of constraints?
| 
| Also a web search did not yield any helpful information for me.
| 
| Could somebody provide some info how I can convert these constraints
| into arguments for the fitting functions (e.g. hin, heq in the alabama
| package)?
`----


Footnotes: 
[1]  http://stackoverflow.com/questions/32845169/how-to-optimize-with-non-linear-constraints-in-r

-- 
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys. (Germany)

Centre of Excellence for Invasion Biology
Stellenbosch University
South Africa

Tel :       +33 - (0)9 53 10 27 44
Cell:       +33 - (0)6 85 62 59 98
Fax :       +33 - (0)9 58 10 27 44

Fax (D):    +49 - (0)3 21 21 25 22 44

email:      Rainer at krugs.de

Skype:      RMkrug

PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 454 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150930/a30cef3d/attachment.bin>

From profjcnash at gmail.com  Wed Sep 30 13:56:08 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Wed, 30 Sep 2015 07:56:08 -0400
Subject: [R] Calculate Total CORRECTED SS for non linear regression
In-Reply-To: <0A3CE045-B316-424B-B724-C6C5AB829ED1@gmail.com>
References: <001401d0fb14$201466b0$603d3410$@gmx.ch>
	<0A3CE045-B316-424B-B724-C6C5AB829ED1@gmail.com>
Message-ID: <560BCDD8.1060003@gmail.com>

Some workers consider it bad practise to compute what is called
R-squared for a nonlinear model. I find it useful for nonlinear models
as a signpost of how good a fit has been found. But just as signposts
can be turned around by vandals, nonlinear models can give a misleading
indication. With linear models, sum((y - model(y))^2) must be smaller
than sum((y - mean(y))^2). That is not necessary for nonlinear models.
Also for linear models there are many equivalent formulas due to the
many identities that go with the linear algebra. Those are not in force
for nonlinear models. So what in linear models is "R-squared" is just a
comparison to the model that is the mean of the predicted variable,
i.e., the best single number model.

JN

On 15-09-30 04:09 AM, peter dalgaard wrote:
> 
>> On 30 Sep 2015, at 02:08 , Michael Eisenring <michael.eisenring at gmx.ch> wrote:
>>
>> Can anyone tell me how to calculate the Total Corrected SS in R and how it
>> can be implemented in my code?
> 
> It is just sum((y-mean(y))^2).
> 
> Beware that a fair amount of (somewhat silly) contention is going on in this area, though. In particular, the formula can give negative values, which is unfortunate if you try calling it "R^2".
>  
> -pd
>


From michael.eisenring at gmx.ch  Wed Sep 30 13:57:12 2015
From: michael.eisenring at gmx.ch (Michael Eisenring)
Date: Wed, 30 Sep 2015 04:57:12 -0700
Subject: [R] Calculate Total CORRECTED SS for non linear regression
In-Reply-To: <0A3CE045-B316-424B-B724-C6C5AB829ED1@gmail.com>
References: <001401d0fb14$201466b0$603d3410$@gmx.ch>
	<0A3CE045-B316-424B-B724-C6C5AB829ED1@gmail.com>
Message-ID: <001d01d0fb77$26976380$73c62a80$@gmx.ch>

Hi Peter,
Thanks for your answer.
I am still a bit confused- So that means that the calculation of the " R2"
for my non-linear model is in fact the formula I used in my eample:

CompoundSS <- sum((dta$Compound - mean(dta$Compound))^2)
R2 <- deviance(dta.nls)/CompoundSS
R2


What's then the difference to the normally calculated R2 used for linear
regression? To me it looks the same.



-----Urspr?ngliche Nachricht-----
Von: peter dalgaard [mailto:pdalgd at gmail.com] 
Gesendet: Mittwoch, 30. September 2015 01:09
An: Michael Eisenring <michael.eisenring at gmx.ch>
Cc: r-help at r-project.org
Betreff: Re: [R] Calculate Total CORRECTED SS for non linear regression


> On 30 Sep 2015, at 02:08 , Michael Eisenring <michael.eisenring at gmx.ch>
wrote:
> 
> Can anyone tell me how to calculate the Total Corrected SS in R and 
> how it can be implemented in my code?

It is just sum((y-mean(y))^2).

Beware that a fair amount of (somewhat silly) contention is going on in this
area, though. In particular, the formula can give negative values, which is
unfortunate if you try calling it "R^2".
 
-pd

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000
Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mestre.frederico at gmail.com  Wed Sep 30 14:38:49 2015
From: mestre.frederico at gmail.com (Frederico Mestre)
Date: Wed, 30 Sep 2015 13:38:49 +0100
Subject: [R] External functions called by my functions
Message-ID: <CAPfBvqwxYuQ5VV4rB98wXTDcs3H31HD7M=o1K99-FMcdV7tD8Q@mail.gmail.com>

Hello,

Is there any way to list all the functions called inside
?a given set of?
 functions?

Something like the function foodweb from mvbutils.
? This function maps the relations between the functions of a package, but
what I need is something to show me the relations with external functions.

Thanks,


?
?Frederico Mestre ? mestre.frederico at gmail.com
ResearchGate: https://www.researchgate.net/profile/Frederico_Mestre
Conservation Biology Unit (UBC)
Research Center in Biodiversity and Genetic Resources ? ?vora  (CIBIO-UE)
Research Group in Applied Ecology (ApplEcol)
University of ?vora
Biology Department
N?cleo da Mitra
7002 ? 554 ? ?vora
PORTUGAL?

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Wed Sep 30 15:01:34 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 30 Sep 2015 13:01:34 +0000
Subject: [R] External functions called by my functions
Message-ID: <248E6FA047A8C746BA491485764190F522095494@ESESSMB207.ericsson.se>

See if this may help:


http://stackoverflow.com/questions/11872879/finding-out-which-functions-are-called-within-a-given-function


--

GG

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Wed Sep 30 15:51:20 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 30 Sep 2015 06:51:20 -0700
Subject: [R] Calculate Total CORRECTED SS for non linear regression
In-Reply-To: <001d01d0fb77$26976380$73c62a80$@gmx.ch>
References: <001401d0fb14$201466b0$603d3410$@gmx.ch>
	<0A3CE045-B316-424B-B724-C6C5AB829ED1@gmail.com>
	<001d01d0fb77$26976380$73c62a80$@gmx.ch>
Message-ID: <FA5FFA19-AA88-49F3-9FE2-90DAA278F256@dcn.davis.CA.us>

Michael... this is an example of a thread that is quickly headed off-topic for R-help because there are lots of resources for learning "why" one formula might be better than another elsewhere and the use of the R language is not central to that discussion (and yet this topic has been discussed before on R-help). It is common to get friendly warnings that your proposed calculations might not do what you think they will around here, but you should supplement those warnings with reading elsewhere to keep the discussion here mostly on the topic of R.

In this case you ought to read up at [1], [2] and [3] at least, and in the future try to ask "how to calculate" questions by including references to the formulas or theory you are interested in. Terms like "adjusted" and "corrected" may not mean the same thing in all contexts, so knowing what YOU are referring to can avoid confusion.

[1]  https://en.m.wikipedia.org/wiki/Coefficient_of_determination
[2] stats.stackexchange.com
[3] http://www.R-project.org/posting-guide.html
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On September 30, 2015 4:57:12 AM PDT, Michael Eisenring <michael.eisenring at gmx.ch> wrote:
>Hi Peter,
>Thanks for your answer.
>I am still a bit confused- So that means that the calculation of the "
>R2"
>for my non-linear model is in fact the formula I used in my eample:
>
>CompoundSS <- sum((dta$Compound - mean(dta$Compound))^2)
>R2 <- deviance(dta.nls)/CompoundSS
>R2
>
>
>What's then the difference to the normally calculated R2 used for
>linear
>regression? To me it looks the same.
>
>
>
>-----Urspr?ngliche Nachricht-----
>Von: peter dalgaard [mailto:pdalgd at gmail.com] 
>Gesendet: Mittwoch, 30. September 2015 01:09
>An: Michael Eisenring <michael.eisenring at gmx.ch>
>Cc: r-help at r-project.org
>Betreff: Re: [R] Calculate Total CORRECTED SS for non linear regression
>
>
>> On 30 Sep 2015, at 02:08 , Michael Eisenring
><michael.eisenring at gmx.ch>
>wrote:
>> 
>> Can anyone tell me how to calculate the Total Corrected SS in R and 
>> how it can be implemented in my code?
>
>It is just sum((y-mean(y))^2).
>
>Beware that a fair amount of (somewhat silly) contention is going on in
>this
>area, though. In particular, the formula can give negative values,
>which is
>unfortunate if you try calling it "R^2".
> 
>-pd
>
>--
>Peter Dalgaard, Professor,
>Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
>2000
>Frederiksberg, Denmark
>Phone: (+45)38153501
>Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From giorgio.garziano at ericsson.com  Wed Sep 30 15:58:32 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 30 Sep 2015 13:58:32 +0000
Subject: [R] Theme white bands blue and grey or other color
Message-ID: <248E6FA047A8C746BA491485764190F5220954E9@ESESSMB207.ericsson.se>

  library(quantmod)
  getSymbols("YHOO")

  chartSeries(YHOO, theme="white")

  b1 <- addBBands(50,2)
  b1 at params$colors$bg.col="#FFFFFF"
  b1

  b2 <- addBBands(100,2)
  b2 at params$colors$bg.col="#FFFFFF"
  b2


	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Wed Sep 30 16:04:25 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 30 Sep 2015 10:04:25 -0400
Subject: [R] dplyr complete.cases(.) works one way but not another
Message-ID: <CAN2xGJYhXvPFCKW=Zdy06HoaJmM=g70c1_YSXsAgTz07sUmP6Q@mail.gmail.com>

Hello!
I don't have a data set, but my question is very clear without it.
I have a data frame 'mydata' and want to reproduce in dplyr the
following R base command:

mydata[complete.cases(mydata), ]

This dplyr command produces the expected result:

library(dplyr)
mydata %>% filter(complete.cases(.))

But this command doesn't work:

filter(mydata, complete.cases(.))
Error: object '.' not found

Why doesn't it work?
Thank you!

-- 
Dimitri Liakhovitski


From giorgio.garziano at ericsson.com  Wed Sep 30 16:15:12 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 30 Sep 2015 14:15:12 +0000
Subject: [R] dplyr complete.cases(.) works one way but not another
Message-ID: <248E6FA047A8C746BA491485764190F5220954F6@ESESSMB207.ericsson.se>

This works:

filter(mydata, complete.cases(mydata))

About dplyr "pronoun dot", see:

http://www.r-bloggers.com/dplyr-0-2/


--
GG




	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Wed Sep 30 16:24:16 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 30 Sep 2015 10:24:16 -0400
Subject: [R] dplyr complete.cases(.) works one way but not another
In-Reply-To: <248E6FA047A8C746BA491485764190F5220954F6@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F5220954F6@ESESSMB207.ericsson.se>
Message-ID: <CAN2xGJYvhsh03-Yt3qkZTp8oXVFFe6dXdktCPcNfL=7VwtxPJg@mail.gmail.com>

Thank you very much, got it: It's because complete.cases is an R base command.

On Wed, Sep 30, 2015 at 10:15 AM, Giorgio Garziano
<giorgio.garziano at ericsson.com> wrote:
> This works:
>
> filter(mydata, complete.cases(mydata))
>
> About dplyr "pronoun dot", see:
>
> http://www.r-bloggers.com/dplyr-0-2/
>
>
> --
> GG
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Dimitri Liakhovitski


From giorgio.garziano at ericsson.com  Wed Sep 30 16:38:26 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 30 Sep 2015 14:38:26 +0000
Subject: [R] dplyr complete.cases(.) works one way but not another
In-Reply-To: <CAN2xGJYvhsh03-Yt3qkZTp8oXVFFe6dXdktCPcNfL=7VwtxPJg@mail.gmail.com>
References: <248E6FA047A8C746BA491485764190F5220954F6@ESESSMB207.ericsson.se>
	<CAN2xGJYvhsh03-Yt3qkZTp8oXVFFe6dXdktCPcNfL=7VwtxPJg@mail.gmail.com>
Message-ID: <248E6FA047A8C746BA491485764190F522095519@ESESSMB207.ericsson.se>

The "pronoun dot"  is used in conjunction with %>% in dplyr (which imports magrittr).

See pag.9, paragraph "Placing lhs elsewhere in rhs call" of the document:

https://cran.r-project.org/web/packages/magrittr/magrittr.pdf

--
GG

From dimitri.liakhovitski at gmail.com  Wed Sep 30 16:56:50 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 30 Sep 2015 10:56:50 -0400
Subject: [R] na.action in xtabs - how to include NAs?
Message-ID: <CAN2xGJa5Z0QfYN9npy+tCpH=K6V2G46T5xf5=EpowQcupm1BYQ@mail.gmail.com>

Please help:

# I have a data frame x:
x <- data.frame(a = c(1:3, NA), b = c(NA, 2:4))

# I run a cross-tab keeping NAs, like this:
table(x[c("a", "b")], useNA = "ifany")

# I want to reproduce it using xtabs, but it ignores NAs:
xtabs(~ a + b, x)

# I can't figure out how to force xtabs to include NAs.
# All my attempts below fail to include NAs:
xtabs(~ a + b, x, na.action(na.pass))
xtabs(~ a + b, x, na.action = "na.pass")
xtabs(~ a + b, x, na.action(na.pass(x)))
xtabs(~ a + b, x, exclude = NULL)

Thank you for your hints!
-- 
Dimitri Liakhovitski


From wdunlap at tibco.com  Wed Sep 30 17:09:27 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 30 Sep 2015 08:09:27 -0700
Subject: [R] na.action in xtabs - how to include NAs?
In-Reply-To: <CAN2xGJa5Z0QfYN9npy+tCpH=K6V2G46T5xf5=EpowQcupm1BYQ@mail.gmail.com>
References: <CAN2xGJa5Z0QfYN9npy+tCpH=K6V2G46T5xf5=EpowQcupm1BYQ@mail.gmail.com>
Message-ID: <CAF8bMcaxTyUm0k+kJtGQRK1YebsTuZyAaip=HuhuuTH26SRNrQ@mail.gmail.com>

Try both na.action=na.pass and exclude=NULL, the first is for
xtabs' call to model.frame and the second for when it prepares
the data from model.frame's output for a call to table.

xtabs(formula = ~a + b, data = x, na.action = na.pass, exclude = NULL)
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Sep 30, 2015 at 7:56 AM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Please help:
>
> # I have a data frame x:
> x <- data.frame(a = c(1:3, NA), b = c(NA, 2:4))
>
> # I run a cross-tab keeping NAs, like this:
> table(x[c("a", "b")], useNA = "ifany")
>
> # I want to reproduce it using xtabs, but it ignores NAs:
> xtabs(~ a + b, x)
>
> # I can't figure out how to force xtabs to include NAs.
> # All my attempts below fail to include NAs:
> xtabs(~ a + b, x, na.action(na.pass))
> xtabs(~ a + b, x, na.action = "na.pass")
> xtabs(~ a + b, x, na.action(na.pass(x)))
> xtabs(~ a + b, x, exclude = NULL)
>
> Thank you for your hints!
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitri.liakhovitski at gmail.com  Wed Sep 30 17:21:33 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 30 Sep 2015 11:21:33 -0400
Subject: [R] na.action in xtabs - how to include NAs?
In-Reply-To: <CAF8bMcaxTyUm0k+kJtGQRK1YebsTuZyAaip=HuhuuTH26SRNrQ@mail.gmail.com>
References: <CAN2xGJa5Z0QfYN9npy+tCpH=K6V2G46T5xf5=EpowQcupm1BYQ@mail.gmail.com>
	<CAF8bMcaxTyUm0k+kJtGQRK1YebsTuZyAaip=HuhuuTH26SRNrQ@mail.gmail.com>
Message-ID: <CAN2xGJYMOiLybLPAoaj_8uQ=Lj7xBgjMKEcsaxo_fbGzfcQNWQ@mail.gmail.com>

Thank you very much, Bill - it worked.
Wow, that's very wordy!

On Wed, Sep 30, 2015 at 11:09 AM, William Dunlap <wdunlap at tibco.com> wrote:
> Try both na.action=na.pass and exclude=NULL, the first is for
> xtabs' call to model.frame and the second for when it prepares
> the data from model.frame's output for a call to table.
>
> xtabs(formula = ~a + b, data = x, na.action = na.pass, exclude = NULL)
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Wed, Sep 30, 2015 at 7:56 AM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Please help:
>>
>> # I have a data frame x:
>> x <- data.frame(a = c(1:3, NA), b = c(NA, 2:4))
>>
>> # I run a cross-tab keeping NAs, like this:
>> table(x[c("a", "b")], useNA = "ifany")
>>
>> # I want to reproduce it using xtabs, but it ignores NAs:
>> xtabs(~ a + b, x)
>>
>> # I can't figure out how to force xtabs to include NAs.
>> # All my attempts below fail to include NAs:
>> xtabs(~ a + b, x, na.action(na.pass))
>> xtabs(~ a + b, x, na.action = "na.pass")
>> xtabs(~ a + b, x, na.action(na.pass(x)))
>> xtabs(~ a + b, x, exclude = NULL)
>>
>> Thank you for your hints!
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
Dimitri Liakhovitski


From marammagdysalem at gmail.com  Wed Sep 30 17:33:23 2015
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Wed, 30 Sep 2015 17:33:23 +0200
Subject: [R] (subscript) logical subscript too long
Message-ID: <CAPLSCn37A6fRG_Jdj=9xRZFXC3rFdZQiXo1=EzPMceQcGGvAMA@mail.gmail.com>

Dear All,

I'm trying to write a function in the values of some numeric vectors
(d1,d2,...,d(m-1)). This function  should be applied on some combinations
of the elements of the (m-1) d vectors that satisfy the condition of having
a sum less than or equal to (n-m). I've tried the following code, but got
an error:(subscript) logical subscript too long.

> n=20

> m=6

> D<-matrix(0,nrow=n-m+1,ncol=m-1)

> for (i in 1:m-1)

+  {

+ D[,i]<-seq(0,n-m,1)

+  }

> ED <- do.call(`expand.grid`,as.data.frame(D))

> ED<-as.matrix(ED)

> s<-w[rowSums(ED)<=(n-m),]

Error in w[rowSums(ED) <= (n - m), ] :

  (subscript) logical subscript too long


Bearing in mind that the true values of n and m might be way larger than 20
and 6, consecutively.

Any Suggestions please?
Thanks.

Maram

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Wed Sep 30 17:41:50 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 30 Sep 2015 15:41:50 +0000
Subject: [R] (subscript) logical subscript too long
Message-ID: <248E6FA047A8C746BA491485764190F52209557A@ESESSMB207.ericsson.se>

Be:

log <- (rowSums(ED) <= (n - m))


Compare the following two values:





length(log)





nrow(w)





--

GG


	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Wed Sep 30 18:08:31 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 30 Sep 2015 16:08:31 +0000
Subject: [R] na.action in xtabs - how to include NAs?
In-Reply-To: <CAN2xGJYMOiLybLPAoaj_8uQ=Lj7xBgjMKEcsaxo_fbGzfcQNWQ@mail.gmail.com>
References: <CAN2xGJa5Z0QfYN9npy+tCpH=K6V2G46T5xf5=EpowQcupm1BYQ@mail.gmail.com>
	<CAF8bMcaxTyUm0k+kJtGQRK1YebsTuZyAaip=HuhuuTH26SRNrQ@mail.gmail.com>
	<CAN2xGJYMOiLybLPAoaj_8uQ=Lj7xBgjMKEcsaxo_fbGzfcQNWQ@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6C920D@mb02.ads.tamu.edu>

If you are going to do this routinely or with many factors, you can build NA into each of the factors, but that will affect other operations:

> xNA <- lapply(x, factor, exclude=NULL)
> xtabs(~a+b, xNA)
      b
a      2 3 4 <NA>
  1    0 0 0    1
  2    1 0 0    0
  3    0 1 0    0
  <NA> 0 0 1    0

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dimitri Liakhovitski
Sent: Wednesday, September 30, 2015 10:22 AM
To: William Dunlap
Cc: r-help
Subject: Re: [R] na.action in xtabs - how to include NAs?

Thank you very much, Bill - it worked.
Wow, that's very wordy!

On Wed, Sep 30, 2015 at 11:09 AM, William Dunlap <wdunlap at tibco.com> wrote:
> Try both na.action=na.pass and exclude=NULL, the first is for
> xtabs' call to model.frame and the second for when it prepares
> the data from model.frame's output for a call to table.
>
> xtabs(formula = ~a + b, data = x, na.action = na.pass, exclude = NULL)
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Wed, Sep 30, 2015 at 7:56 AM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Please help:
>>
>> # I have a data frame x:
>> x <- data.frame(a = c(1:3, NA), b = c(NA, 2:4))
>>
>> # I run a cross-tab keeping NAs, like this:
>> table(x[c("a", "b")], useNA = "ifany")
>>
>> # I want to reproduce it using xtabs, but it ignores NAs:
>> xtabs(~ a + b, x)
>>
>> # I can't figure out how to force xtabs to include NAs.
>> # All my attempts below fail to include NAs:
>> xtabs(~ a + b, x, na.action(na.pass))
>> xtabs(~ a + b, x, na.action = "na.pass")
>> xtabs(~ a + b, x, na.action(na.pass(x)))
>> xtabs(~ a + b, x, exclude = NULL)
>>
>> Thank you for your hints!
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
Dimitri Liakhovitski

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ravi.varadhan at jhu.edu  Wed Sep 30 19:21:56 2015
From: ravi.varadhan at jhu.edu (Ravi Varadhan)
Date: Wed, 30 Sep 2015 17:21:56 +0000
Subject: [R] optimizing with non-linear constraints
Message-ID: <da9cabb003544e4cb1f59c02aabee698@DOM-EB1-2013.win.ad.jhu.edu>

Hi Rainer,
It is very simple to specify the constraints (linear or nonlinear) in "alabama" .  They are specified in a function called `hin', where the constraints are written such that they are positive.  Your two nonlinear constraints would be written as follows:

hin <- function(x, LAI) {
h <- rep(NA, 2)
h[1] <- LAI^x[2] / x[3] + x[1]
h[2] <- 1 - x[1] - LAI^x[2] / x[3]
h
}

Please take a look at the help page.  If it is still not clear, you can contact me offline.
Best,
Ravi

Ravi Varadhan, Ph.D. (Biostatistics), Ph.D. (Environmental Engg)
Associate Professor,  Department of Oncology
Division of Biostatistics & Bionformatics
Sidney Kimmel Comprehensive Cancer Center
Johns Hopkins University
550 N. Broadway, Suite 1111-E
Baltimore, MD 21205
410-502-2619


	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Wed Sep 30 22:52:00 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 1 Oct 2015 09:52:00 +1300
Subject: [R] [FORGED] cumulative distribtuion function for multinomial
 distribution in R
In-Reply-To: <BEBF7404-8F3D-4F6E-BDAB-C8131D2D5D83@gmail.com>
References: <CAHLnndYBNZb1xffw75M+ymLpZ71f38prHUX+Lj6oYnk7P_VGaA@mail.gmail.com>
	<5609F48E.6000504@auckland.ac.nz>
	<147DEAAA-6759-4403-87E4-7CDDD0E9646D@gmail.com>
	<560AF847.1050407@auckland.ac.nz>
	<BEBF7404-8F3D-4F6E-BDAB-C8131D2D5D83@gmail.com>
Message-ID: <560C4B70.70309@auckland.ac.nz>

On 30/09/15 20:55, peter dalgaard wrote:
> I know exactly this:
>
> https://cran.r-project.org/web/packages/MFSAS/index.html
>
> As I understand things (but I am not a CRAN maintainer): This is
> compatible with a situation where the maintainer exists but doesn't
> update the package. I think a package only gets to orphaned status if
> the maintainer steps down or disappears.

Gotcha.  Bewdy, ta.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From saahmed2002 at gmail.com  Wed Sep 30 19:51:48 2015
From: saahmed2002 at gmail.com (Syed Abrar Ahmed)
Date: Wed, 30 Sep 2015 12:51:48 -0500
Subject: [R] R help - "UCL at different levels"
Message-ID: <CA+wv8OyL_m4fWOcKAB_n9kVG2LjueqvaEVQJb8qs65gH0HP_tg@mail.gmail.com>

Hello Experts,

I need your help to understand the how can we calculate the UCL values at
different levels dynamically?

Customer No       Month       Sales          UCL 100 201501 500 550.75 100
201502 100 550.75 101 201501 400 425.25 101 201502 50 425.25
In the above scenario i need calculate UCL at Customer level.

Any help is appreciated.

Thanks,
Abrar

	[[alternative HTML version deleted]]


From baccts at hotmail.com  Wed Sep 30 22:16:00 2015
From: baccts at hotmail.com (baccts)
Date: Wed, 30 Sep 2015 13:16:00 -0700 (PDT)
Subject: [R] merging tables based on both row and column names
In-Reply-To: <248E6FA047A8C746BA491485764190F522094D00@ESESSMB207.ericsson.se>
References: <SN1PR19MB0559D1D88EA5B7F787EFCED6CB4F0@SN1PR19MB0559.namprd19.prod.outlook.com>
	<248E6FA047A8C746BA491485764190F522094D00@ESESSMB207.ericsson.se>
Message-ID: <1443644160359-4712995.post@n4.nabble.com>

Thank you, *Frank* and *Giorgio* for your replies. Both of your solutions
work for my need.
*Frank*, I ended up using your codes. Like that it's short.

For multiple test data frame, I ended up creating a list in the beginning
and put each test data frame into the list as such:

ltest = list();
ltest[[i]]=test;

Thanks again for your help.

Lin



--
View this message in context: http://r.789695.n4.nabble.com/merging-tables-based-on-both-row-and-column-names-tp4712905p4712995.html
Sent from the R help mailing list archive at Nabble.com.


From alex.deckmyn at meteo.be  Sun Sep 27 10:47:35 2015
From: alex.deckmyn at meteo.be (Alex Deckmyn)
Date: Sun, 27 Sep 2015 10:47:35 +0200
Subject: [R] [R-pkgs] Major update to 'maps': v3.0.0
Message-ID: <1416191663.7823498.1443343655806.JavaMail.zimbra@meteo.be>

Hi, 

I am pleased to announce the availability of 'maps' v3.0.0 
This is a major update, mainly because of a new 'world' map. The new data is adapted from the public domain GIS project Natural Earth (the 1:50m admin-0 countries map). 

This change in data has a number of consequences for users. Most significantly, many new country names have appeared while some other no longer exist. So code that explicitely calls, for instance, 'USSR' or 'Yugoslavia' will no longer work correctly. On the other hand, I have tried to keep other country names more or less identical. Besides the updated political borders, the map also has a slightly higher resolution than before. 

I understand that this change (which by its nature can not be completely backward compatible) may cause some problems for existing code. There are however various ways to fall back to the old legacy world map. It can be addressed directly as map("legacy_world",...). But for quickly fixing existing code, there is also the possibility to have 'world' point at the legacy data base in two different ways: 
- by calling the function world.legacy(TRUE) 
- be setting the environment variable R_MAP_DATA_LEGACY=TRUE prior to loading the maps package 
These 2 solutions should be seen as temporary fixes. 

Other notable changes and additions: 
- a new data set 'iso3166' with 2- and 3-letter ISO codes of all countries plus the sovereignty. A few simple functions allow to map or label coutries by their ISO codes: 
* iso.expand(c("BE","NL","LU")) 
* iso.alpha(c("Belgium","Netherlands","Luxembourg"),n=2) 
* sov.expand("France") 

- internally, these functions and the basic name matching now use perl-style regular expressions. This may be exploited by using regular expressions to (un)select regions in the 'map()' functions directly, e.g. map(regions="(?!Antarctica)",fill=T) 

The README file and NEWS.Rd contain more details and examples. 

I hope this update proves useful. Please inform me of any errors you may find. 

Alex 
--- 
Dr. Alex Deckmyn e-mail: alex.deckmyn at meteo.be 
Royal Meteorological Institute http://www.meteo.be 
Ringlaan 3, 1180 Ukkel, Belgium tel. (32)(2)3730646 

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From alexzemnitskiy at gmail.com  Wed Sep 30 22:26:55 2015
From: alexzemnitskiy at gmail.com (Alexey Zemnitskiy)
Date: Wed, 30 Sep 2015 16:26:55 -0400
Subject: [R] [R-pkgs] PortfolioEffectHFT - High Frequency Portfolio Analytics
Message-ID: <CADY2e71WW2nxtGnSpTSrFP70AD+0=izrtsPHqf0n_CK3VxMqbA@mail.gmail.com>

Dear R enthusiasts,

I would like to announce PortfolioEffectHFT package availability on CRAN:
https://cran.r-project.org/web/packages/PortfolioEffectHFT/

It is an R interface to PortfolioEffect Quant service for backtesting high
frequency trading (HFT)  strategies, intraday portfolio analysis and
optimization. PortfolioEffect is a cloud-based service, which is free to
use with your own market data, but also has an integrated (optional) access
to high frequency prices for all major US Equities (8,000+ symbols).

Package features:

- Auto-calibrating model pipeline for market microstructure noise, risk
factors, price jumps/outliers, tail risk (high-order moments), price
fractality (long memory) and was designed to give tick-resolution
analytics.

- Over 40+ portfolio and position-level metrics to compute intraday risk
and performance from modern and post-modern portfolio theory.

- Single-period constraint portfolio optimization (classic Markowitz and
extensions for tail risk) with scalar, vector-based and user-defined
functional constraints.

- Multi-period constraint portfolio optimization that accounts for previous
portfolio rebalancing (trading strategy optimization).

- Transactional costs were also implemented in this release.

More details in the package manual:
https://cran.r-project.org/web/packages/PortfolioEffectHFT/vignettes/PortfolioEffectHFT.pdf

Or on the website (nightly builds and latest updates):
https://www.portfolioeffect.com/docs/platform/quant/


Sincerely,

Aleksey Zemnitskiy

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


