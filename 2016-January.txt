From ragia11 at hotmail.com  Fri Jan  1 01:46:06 2016
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Fri, 1 Jan 2016 02:46:06 +0200
Subject: [R] save screen printed data frames and ggplots into a file
Message-ID: <DUB125-W90ECDED878F7AC5B18736CB3FF0@phx.gbl>

Dear group
I have a script that prints data frames and plot using ggplot 2 while running, ?
how can I do this
thanks in advance
Ragia 		 	   		  

From bgunter.4567 at gmail.com  Fri Jan  1 02:19:35 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 31 Dec 2015 17:19:35 -0800
Subject: [R] save screen printed data frames and ggplots into a file
In-Reply-To: <DUB125-W90ECDED878F7AC5B18736CB3FF0@phx.gbl>
References: <DUB125-W90ECDED878F7AC5B18736CB3FF0@phx.gbl>
Message-ID: <CAGxFJbS7ovHOqqE4kGtrj4D2bdzh80G2hdjaFyB7_MHQhuc-JA@mail.gmail.com>

Does what? You said you already have a script.

**If** you want to program it as a function, then I suggest that you
do some homework and go through an R tutorial, e.g. the "Intro to R"
that ships with R or one of the many available on the web.

If that is not what you mean, then clarify.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 31, 2015 at 4:46 PM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
> Dear group
> I have a script that prints data frames and plot using ggplot 2 while running,
> how can I do this
> thanks in advance
> Ragia
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Fri Jan  1 02:36:33 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 1 Jan 2016 12:36:33 +1100
Subject: [R] save screen printed data frames and ggplots into a file
In-Reply-To: <DUB125-W90ECDED878F7AC5B18736CB3FF0@phx.gbl>
References: <DUB125-W90ECDED878F7AC5B18736CB3FF0@phx.gbl>
Message-ID: <CA+8X3fVikR9sF747_gZnzDpmUw+z_fDyEs+m9qXpY8njKARDDQ@mail.gmail.com>

Hi Ragia,
I may be missing your point, but try the "htmlize" function in the prettyR
package. This creates an HTML file containing both the text output and
images from an R script.

Jim

On Fri, Jan 1, 2016 at 11:46 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:

> Dear group
> I have a script that prints data frames and plot using ggplot 2 while
> running,
> how can I do this
> thanks in advance
> Ragia
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From FarkasA at optimum-dosing-strategies.org  Fri Jan  1 02:35:54 2016
From: FarkasA at optimum-dosing-strategies.org (Andras Farkas)
Date: Fri, 1 Jan 2016 01:35:54 +0000 (UTC)
Subject: [R] log log regression model
References: <292488627.5669565.1451612154726.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <292488627.5669565.1451612154726.JavaMail.yahoo@mail.yahoo.com>



Dear All,

wonder if you have a thought on the followimg: if I have a simple model like model <- lm(log(y)~log(x)+log(z),data=data), where both, the dependent and independent variables are log transformed, is it ok just to use ypred <- predict(model,type=response) to get the predictions , then transform ypred with exp(ypred)  to y's original scale to compare observed or known data (y) with model predicted (ypred) on the original scale?

appreciate your thoughts...

Andras


From sstoline at gmail.com  Fri Jan  1 12:45:13 2016
From: sstoline at gmail.com (Steven Stoline)
Date: Fri, 1 Jan 2016 06:45:13 -0500
Subject: [R] [FORGED]  Histogram for Left Censored Data
In-Reply-To: <56859B36.4010202@auckland.ac.nz>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
	<56859B36.4010202@auckland.ac.nz>
Message-ID: <CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>

Dear Rolf:


The histogram should contain a bar(s) for the censored data values replaced
by their detection limit(s) with different color than other bars for the
noncensored values . In this example there are only 3 censored values with
only one detection limit of DL = 1450.


with many thanks
steve



On Thu, Dec 31, 2015 at 4:16 PM, Rolf Turner <r.turner at auckland.ac.nz>
wrote:

> On 31/12/15 23:20, Steven Stoline wrote:
>
>> Dear All:
>>
>> I need helps with creating histograms for data that include left
>> censored observations.
>>
>> Here is an example of left censored data
>>
>>
>>
>> *Sulfate.Concentration*
>> <-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,
>> 1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
>> 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)
>>
>>
>> *Column 2* is an indicator for censoring "*1*" for left censored
>> observations and "*0*" for non-censored (fully measured)
>> observations.
>>
>
> And what, pray tell, do you want the resulting histogram to look like?
> See e.g. fortune("mind_read").
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>



-- 
Steven M. Stoline
1123 Forest Avenue
Portland, ME 04112
sstoline at gmail.com

	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Fri Jan  1 14:54:08 2016
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 1 Jan 2016 05:54:08 -0800 (PST)
Subject: [R] [FORGED]  Histogram for Left Censored Data
In-Reply-To: <CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
	<56859B36.4010202@auckland.ac.nz>
	<CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1601010551150.15291@localhost>

On Fri, 1 Jan 2016, Steven Stoline wrote:

> The histogram should contain a bar(s) for the censored data values
> replaced by their detection limit(s) with different color than other bars
> for the noncensored values. In this example there are only 3 censored
> values with only one detection limit of DL = 1450.

steve,

   Consider using the NADA package. While your current sulfate data set might
have only three non-detects, other data sets might have more.

   My question to you is why you want a histogram that cuts off at the
detection limit. What question are you trying to answer with these data?

Rich


From ragia11 at hotmail.com  Fri Jan  1 19:25:19 2016
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Fri, 1 Jan 2016 20:25:19 +0200
Subject: [R] save screen printed data frames and ggplots into a file
In-Reply-To: <CAGxFJbS7ovHOqqE4kGtrj4D2bdzh80G2hdjaFyB7_MHQhuc-JA@mail.gmail.com>
References: <DUB125-W90ECDED878F7AC5B18736CB3FF0@phx.gbl>,
	<CAGxFJbS7ovHOqqE4kGtrj4D2bdzh80G2hdjaFyB7_MHQhuc-JA@mail.gmail.com>
Message-ID: <DUB125-W70EC720B33389D714D927CB3FF0@phx.gbl>

yes, the script prints that on the scree, when I used pd file to do the job only ggplots appears on it !

hope this clarifies, thanks .
Ragia

----------------------------------------
> Date: Thu, 31 Dec 2015 17:19:35 -0800
> Subject: Re: [R] save screen printed data frames and ggplots into a file
> From: bgunter.4567 at gmail.com
> To: ragia11 at hotmail.com
> CC: r-help at r-project.org
>
> Does what? You said you already have a script.
>
> **If** you want to program it as a function, then I suggest that you
> do some homework and go through an R tutorial, e.g. the "Intro to R"
> that ships with R or one of the many available on the web.
>
> If that is not what you mean, then clarify.
>
> Cheers,
> Bert
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Dec 31, 2015 at 4:46 PM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
>> Dear group
>> I have a script that prints data frames and plot using ggplot 2 while running,
>> how can I do this
>> thanks in advance
>> Ragia
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
 		 	   		  

From bgunter.4567 at gmail.com  Fri Jan  1 20:18:27 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 1 Jan 2016 11:18:27 -0800
Subject: [R] save screen printed data frames and ggplots into a file
In-Reply-To: <DUB125-W70EC720B33389D714D927CB3FF0@phx.gbl>
References: <DUB125-W90ECDED878F7AC5B18736CB3FF0@phx.gbl>
	<CAGxFJbS7ovHOqqE4kGtrj4D2bdzh80G2hdjaFyB7_MHQhuc-JA@mail.gmail.com>
	<DUB125-W70EC720B33389D714D927CB3FF0@phx.gbl>
Message-ID: <CAGxFJbSsREVTz1eRLCtxYXt87HTy04e3K1SRWywyNGZdw3qORg@mail.gmail.com>

Still not clear (to me).

But perhaps FAQ 7.16. You need to explicitly print or use echo = TRUE
when sourcing from a file.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jan 1, 2016 at 10:25 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
> yes, the script prints that on the scree, when I used pd file to do the job only ggplots appears on it !
>
> hope this clarifies, thanks .
> Ragia
>
> ----------------------------------------
>> Date: Thu, 31 Dec 2015 17:19:35 -0800
>> Subject: Re: [R] save screen printed data frames and ggplots into a file
>> From: bgunter.4567 at gmail.com
>> To: ragia11 at hotmail.com
>> CC: r-help at r-project.org
>>
>> Does what? You said you already have a script.
>>
>> **If** you want to program it as a function, then I suggest that you
>> do some homework and go through an R tutorial, e.g. the "Intro to R"
>> that ships with R or one of the many available on the web.
>>
>> If that is not what you mean, then clarify.
>>
>> Cheers,
>> Bert
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Thu, Dec 31, 2015 at 4:46 PM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
>>> Dear group
>>> I have a script that prints data frames and plot using ggplot 2 while running,
>>> how can I do this
>>> thanks in advance
>>> Ragia
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Fri Jan  1 21:42:37 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 1 Jan 2016 12:42:37 -0800
Subject: [R] [FORGED]  Histogram for Left Censored Data
In-Reply-To: <CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
	<56859B36.4010202@auckland.ac.nz>
	<CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
Message-ID: <2255B030-4A19-48E4-B3B5-CF67B216C37A@comcast.net>


> On Jan 1, 2016, at 3:45 AM, Steven Stoline <sstoline at gmail.com> wrote:
> 
> Dear Rolf:
> 
> 
> The histogram should contain a bar(s) for the censored data values replaced
> by their detection limit(s) with different color than other bars for the
> noncensored values . In this example there are only 3 censored values with
> only one detection limit of DL = 1450.
> 
> 
> with many thanks
> steve
> 
> 
> 
> On Thu, Dec 31, 2015 at 4:16 PM, Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
> 
>> On 31/12/15 23:20, Steven Stoline wrote:
>> 
>>> Dear All:
>>> 
>>> I need helps with creating histograms for data that include left
>>> censored observations.
>>> 
>>> Here is an example of left censored data
>>> 
>>> 
>>> 
>>> *Sulfate.Concentration*
>>> <-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,
>>> 1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
>>> 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)
>>> 

 myhist <- hist(sulfate[,1], breaks=c(1400,1451,1500,1600,1700,1800,1900), col=c(1,rep(2,5)), xaxt="n")
#  plots with no x axis labeling
 myhist
#------------------
$breaks
[1] 1400 1451 1500 1600 1700 1800 1900

$counts
[1]  3  1  1  0 14  5

$density
[1] 0.0024509804 0.0008503401 0.0004166667 0.0000000000 0.0058333333 0.0020833333

$mids
[1] 1425.5 1475.5 1550.0 1650.0 1750.0 1850.0

$xname
[1] "sulfate[, 1]"

$equidist
[1] FALSE

attr(,"class")
[1] "histogram"
#---rebuild the x-axis ----------------
 axis(1, at=c(myhist$mids[1],myhist$breaks[-(1:2)]), labels=c("<1450", myhist$breaks[-(1:2)]))



-- 
David.

>>> 
>>> *Column 2* is an indicator for censoring "*1*" for left censored
>>> observations and "*0*" for non-censored (fully measured)
>>> observations.
>>> 
>> 
>> And what, pray tell, do you want the resulting histogram to look like?
>> See e.g. fortune("mind_read").
>> 
>> cheers,
>> 
>> Rolf Turner
>> 
>> --
>> Technical Editor ANZJS
>> Department of Statistics
>> University of Auckland
>> Phone: +64-9-373-7599 ext. 88276
>> 
> 
> 
> 
> -- 
> Steven M. Stoline
> 1123 Forest Avenue
> Portland, ME 04112
> sstoline at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From mortezafirouzi at yahoo.com  Fri Jan  1 19:32:00 2016
From: mortezafirouzi at yahoo.com (Morteza Firouzi)
Date: Fri, 1 Jan 2016 18:32:00 +0000 (UTC)
Subject: [R] missForest: Looping through files in a folder
References: <1396565104.5992614.1451673120715.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1396565104.5992614.1451673120715.JavaMail.yahoo@mail.yahoo.com>

Dear members,
Could you please help me on this issue. I've already searched and I watched some videos, but it was not useful.I need help to loop through the files in a folder (200+ csv files). I am using missForest() to impute missing values.?If I run the code for each single file, I have to do as following:
## main script for each single file
G1334108 <- read.csv(file.choose(), header = T)

G1334108.F <- missForest(G1334108, verbose = TRUE, maxiter = 5)

write.csv(G1334108.F$ximp, file = 'G1334108_F.csv')

I tried these below script codes to loop the function before writing here:
# 1st tryall.files <- list.files()
   my.files <- grep(".*csv", all_files, value=T)
            for(i in my.files){
    # do your operations here
    G1344108.Forest <- missForest(G1344108, verbose = TRUE, maxiter = 5)
    # save
    output.filename <- gsub("(.*?).csv", "\\1.csv", i)
    write.table(G1344108.Forest$ximp, output.filename)
}## ?2nd try
files <- list.files()lapply(files, function(x) {my.files <- read.csv("*.csv", header = T)missforest.out <- missForest(my.files, verbose = TRUE, maxiter = 5)write.csv(missforest.out$ximp, file = '*_F.csv')
}Thank you for the time.Best regards,Morteza

	[[alternative HTML version deleted]]


From arkay7777 at gmail.com  Fri Jan  1 19:36:26 2016
From: arkay7777 at gmail.com (Arkay)
Date: Fri, 1 Jan 2016 18:36:26 +0000
Subject: [R] text duplication bug in mtext?
Message-ID: <loom.20160101T192220-115@post.gmane.org>

Not sure if this has already been reported but I think that there might 
be a bug in mtext that causes the text in a plot to be duplicated under 
a narrow set of circumstances.

Here is a reproducible example.

df1 <- data.frame(V1=rnorm(100))
hist(df1$V1)
mtext("Test", side=1, line=4, adj=c(1,0)) # causes text to appear 
twice

This does not appear to happen if adj=c(0,0) or if the adj argument is 
removed altogether:

hist(df1$V1)
mtext("Test", side=1, line=4, adj=c(0,0))

Furthermore, it looks like using the "at" argument causes the spacing 
between the duplicated text to disappear:

hist(df1$V1)
mtext("Test", side=1, line=4, at=0, adj=c(1,0))

Some additional information related to my R installation.

sessionInfo()
R version 3.2.3 (2015-12-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8.1 x64 (build 9600)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United 
States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United 
States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] rj_2.0.3-2

loaded via a namespace (and not attached):
[1] tools_3.2.3


From dwinsemius at comcast.net  Fri Jan  1 21:57:54 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 1 Jan 2016 12:57:54 -0800
Subject: [R] text duplication bug in mtext?
In-Reply-To: <loom.20160101T192220-115@post.gmane.org>
References: <loom.20160101T192220-115@post.gmane.org>
Message-ID: <A8AEDCAB-7C26-4024-82D9-3B10632B9893@comcast.net>


> On Jan 1, 2016, at 10:36 AM, Arkay <arkay7777 at gmail.com> wrote:
> 
> Not sure if this has already been reported but I think that there might 
> be a bug in mtext that causes the text in a plot to be duplicated under 
> a narrow set of circumstances.
> 
> Here is a reproducible example.
> 
> df1 <- data.frame(V1=rnorm(100))
> hist(df1$V1)
> mtext("Test", side=1, line=4, adj=c(1,0)) # causes text to appear 
> twice
> 

If you do this:

mtext("Test", side=1, line=4, adj=seq(0,1, length=5) )


You get 5 items.  It's arguably a "feature", and arguably documented where the help page says: "All of the named arguments can be vectors, and recycling will take place to plot as many strings as the longest of the vector arguments."

David.

> This does not appear to happen if adj=c(0,0) or if the adj argument is 
> removed altogether:
> 
> hist(df1$V1)
> mtext("Test", side=1, line=4, adj=c(0,0))
> 
> Furthermore, it looks like using the "at" argument causes the spacing 
> between the duplicated text to disappear:
> 
> hist(df1$V1)
> mtext("Test", side=1, line=4, at=0, adj=c(1,0))
> 
> Some additional information related to my R installation.
> 
> sessionInfo()
> R version 3.2.3 (2015-12-10)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 8.1 x64 (build 9600)
> 
> locale:
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United 
> States.1252    LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C                           LC_TIME=English_United 
> States.1252    
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base     
> 
> other attached packages:
> [1] rj_2.0.3-2
> 
> loaded via a namespace (and not attached):
> [1] tools_3.2.3
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Fri Jan  1 22:06:59 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 2 Jan 2016 08:06:59 +1100
Subject: [R] [FORGED] Histogram for Left Censored Data
In-Reply-To: <CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
	<56859B36.4010202@auckland.ac.nz>
	<CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
Message-ID: <CA+8X3fWU=Tw+-5RQxQ=FrCMRYGGinQkO7KmRpoQj=U14r4V77A@mail.gmail.com>

How about this?

Sconc<-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,
 1770,1790,1780,1850,1760,1450,1710,1575,1475,1780,1790,
 1780,1450,1790,1800,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),
 24,2)
Sconc.tab<-table(cut(Sconc[,1],breaks=seq(1450,1900,by=50),
 include.lowest=TRUE))
Sconc.cens<-table(cut(Sconc[,1][Sconc[,2]],breaks=seq(1450,1900,by=50),
 include.lowest=TRUE))
require(plotrix)
barp(c(Sconc.tab,Sconc.cens),names.arg=c(seq(1500,1900,by=50),rep("",9)),
 main="Sulfate concentrations",xlab="Concentration",ylab="Frequency",
 x=rep(1:length(Sconc.tab),2),col=rep(c("green","red"),each=9),
 staxx=TRUE)
legend(2,10,c("Non-censored","Censored"),fill=c("green","red"))

Jim


On Fri, Jan 1, 2016 at 10:45 PM, Steven Stoline <sstoline at gmail.com> wrote:

> Dear Rolf:
>
>
> The histogram should contain a bar(s) for the censored data values replaced
> by their detection limit(s) with different color than other bars for the
> noncensored values . In this example there are only 3 censored values with
> only one detection limit of DL = 1450.
>
>
> with many thanks
> steve
>
>
>
> On Thu, Dec 31, 2015 at 4:16 PM, Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
>
> > On 31/12/15 23:20, Steven Stoline wrote:
> >
> >> Dear All:
> >>
> >> I need helps with creating histograms for data that include left
> >> censored observations.
> >>
> >> Here is an example of left censored data
> >>
> >>
> >>
> >> *Sulfate.Concentration*
> >> <-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,
> >> 1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
> >> 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)
> >>
> >>
> >> *Column 2* is an indicator for censoring "*1*" for left censored
> >> observations and "*0*" for non-censored (fully measured)
> >> observations.
> >>
> >
> > And what, pray tell, do you want the resulting histogram to look like?
> > See e.g. fortune("mind_read").
> >
> > cheers,
> >
> > Rolf Turner
> >
> > --
> > Technical Editor ANZJS
> > Department of Statistics
> > University of Auckland
> > Phone: +64-9-373-7599 ext. 88276
> >
>
>
>
> --
> Steven M. Stoline
> 1123 Forest Avenue
> Portland, ME 04112
> sstoline at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Jan  1 22:33:24 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 2 Jan 2016 08:33:24 +1100
Subject: [R] missForest: Looping through files in a folder
In-Reply-To: <1396565104.5992614.1451673120715.JavaMail.yahoo@mail.yahoo.com>
References: <1396565104.5992614.1451673120715.JavaMail.yahoo.ref@mail.yahoo.com>
	<1396565104.5992614.1451673120715.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+8X3fXt=QrixO5wC=kY5Eob57tGScojp7yQDNs1Sa2i2-ACvA@mail.gmail.com>

Hi Morteza,
What you may want is this:

my.files<-list.files(pattern=".csv")
newfiles<-gsub(".","_F.",my.files,fixed=TRUE)
for(i in 1:length(my.files)) {
 mydat<-read.csv(my.files[i])
 mydatimp<-missForest(mydat,verbose=TRUE,maxiter=5)
 write.csv(mydatimp$ximp,newfiles[i])
}

Jim


On Sat, Jan 2, 2016 at 5:32 AM, Morteza Firouzi via R-help <
r-help at r-project.org> wrote:

> Dear members,
> Could you please help me on this issue. I've already searched and I
> watched some videos, but it was not useful.I need help to loop through the
> files in a folder (200+ csv files). I am using missForest() to impute
> missing values. If I run the code for each single file, I have to do as
> following:
> ## main script for each single file
> G1334108 <- read.csv(file.choose(), header = T)
>
> G1334108.F <- missForest(G1334108, verbose = TRUE, maxiter = 5)
>
> write.csv(G1334108.F$ximp, file = 'G1334108_F.csv')
>
> I tried these below script codes to loop the function before writing here:
> # 1st tryall.files <- list.files()
>    my.files <- grep(".*csv", all_files, value=T)
>             for(i in my.files){
>     # do your operations here
>     G1344108.Forest <- missForest(G1344108, verbose = TRUE, maxiter = 5)
>     # save
>     output.filename <- gsub("(.*?).csv", "\\1.csv", i)
>     write.table(G1344108.Forest$ximp, output.filename)
> }##  2nd try
> files <- list.files()lapply(files, function(x) {my.files <-
> read.csv("*.csv", header = T)missforest.out <- missForest(my.files, verbose
> = TRUE, maxiter = 5)write.csv(missforest.out$ximp, file = '*_F.csv')
> }Thank you for the time.Best regards,Morteza
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Fri Jan  1 22:56:49 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 1 Jan 2016 16:56:49 -0500
Subject: [R] text duplication bug in mtext?
In-Reply-To: <loom.20160101T192220-115@post.gmane.org>
References: <loom.20160101T192220-115@post.gmane.org>
Message-ID: <5686F621.9080502@gmail.com>

On 01/01/2016 1:36 PM, Arkay wrote:
> Not sure if this has already been reported but I think that there might
> be a bug in mtext that causes the text in a plot to be duplicated under
> a narrow set of circumstances.

As David said, this is a feature, not a bug.

You were probably confused by the different treatment of "adj" in text() 
and mtext().  The former uses one or two values to determine placement 
of all strings.  The latter uses one value per string.

Duncan Murdoch

>
> Here is a reproducible example.
>
> df1 <- data.frame(V1=rnorm(100))
> hist(df1$V1)
> mtext("Test", side=1, line=4, adj=c(1,0)) # causes text to appear
> twice
>
> This does not appear to happen if adj=c(0,0) or if the adj argument is
> removed altogether:
>
> hist(df1$V1)
> mtext("Test", side=1, line=4, adj=c(0,0))
>
> Furthermore, it looks like using the "at" argument causes the spacing
> between the duplicated text to disappear:
>
> hist(df1$V1)
> mtext("Test", side=1, line=4, at=0, adj=c(1,0))
>
> Some additional information related to my R installation.
>
> sessionInfo()
> R version 3.2.3 (2015-12-10)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 8.1 x64 (build 9600)
>
> locale:
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
> States.1252    LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C                           LC_TIME=English_United
> States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rj_2.0.3-2
>
> loaded via a namespace (and not attached):
> [1] tools_3.2.3
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jvadams at usgs.gov  Fri Jan  1 23:09:12 2016
From: jvadams at usgs.gov (Adams, Jean)
Date: Fri, 1 Jan 2016 16:09:12 -0600
Subject: [R] log log regression model
In-Reply-To: <292488627.5669565.1451612154726.JavaMail.yahoo@mail.yahoo.com>
References: <292488627.5669565.1451612154726.JavaMail.yahoo.ref@mail.yahoo.com>
	<292488627.5669565.1451612154726.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAN5YmCFVJN1zYuA=m8oBZwz=4x+u4Tf6A5EY6MhsRV17YnTUJw@mail.gmail.com>

This is more of a statistics question than an R programming question.  I
suggest you look at Cross Validated for an answer.  I found this in a quick
search,

http://stats.stackexchange.com/questions/115571/back-transforming-regression-results-when-modeling-logy

Jean

On Thu, Dec 31, 2015 at 7:35 PM, Andras Farkas <
FarkasA at optimum-dosing-strategies.org> wrote:

>
>
> Dear All,
>
> wonder if you have a thought on the followimg: if I have a simple model
> like model <- lm(log(y)~log(x)+log(z),data=data), where both, the dependent
> and independent variables are log transformed, is it ok just to use ypred
> <- predict(model,type=response) to get the predictions , then transform
> ypred with exp(ypred)  to y's original scale to compare observed or known
> data (y) with model predicted (ypred) on the original scale?
>
> appreciate your thoughts...
>
> Andras
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Fri Jan  1 23:27:17 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 2 Jan 2016 11:27:17 +1300
Subject: [R] [FORGED]  log log regression model
In-Reply-To: <292488627.5669565.1451612154726.JavaMail.yahoo@mail.yahoo.com>
References: <292488627.5669565.1451612154726.JavaMail.yahoo.ref@mail.yahoo.com>
	<292488627.5669565.1451612154726.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5686FD45.6060301@auckland.ac.nz>

On 01/01/16 14:35, Andras Farkas wrote:
>

>
> Dear All,
>
> wonder if you have a thought on the followimg: if I have a simple
> model like model <- lm(log(y)~log(x)+log(z),data=data), where both,
> the dependent and independent variables are log transformed, is it ok
> just to use ypred <- predict(model,type=response) to get the
> predictions , then transform ypred with exp(ypred)  to y's original
> scale to compare observed or known data (y) with model predicted
> (ypred) on the original scale?
>
> appreciate your thoughts...

Is it OK?  In what sense?  It's a free country --- at least the country 
that I live in is still free (more or less) so you can do whatever you 
feel like.

However it helps to do a little elementary mathematics, rather than just 
hammering and hoping.  When in doubt, do the maths.

You are fitting the model

   log(y) = beta_0 + beta_1 * log(x) + beta_2 * log(z) + E

where it is tacitly assumed that E ~ N(0,sigma^2) and that the E's 
corresponding to different observations are independent. Using predict() 
will get you

    log(y).hat = b_0 + b_1 * log(x) + b_2 * log(z)

where the b_i are the *estimates* of the beta_i.  Under the given 
assumptions, the b_i are unbiased so you get the expected value of 
log(y).hat being equal to the expected value of log(y) and the universe 
is in harmony.  OMMMMMMMMM!

However when you exponentiate everything, things change; expected values 
are not preserved since exponentiation is *not* a linear function!

If you set y.hat = exp(log(y).hat) you get

    y.hat = exp(b_0) * x^{b_1}* z^{b_2}

It is not obvious what the expected value of this expression is, but it 
is pretty sure not to be the same as the expected value of y.

Note that the expected value of y is *not* equal to:

     exp(beta_0) * x^{beta_1} * z^{beta_2}  (1)

Rather, it is equal to this expression multiplied by exp(sigma^2/2) 
(under the stated assumptions).

I *think* (I have not checked this) that the expected value of y.hat 
will be equal "asymptotically" to (1) --- that is, to the expected value 
of y divided by exp(sigma^2/2).

Other younger and wiser heads who read this list might like to chime in 
here and possibly correct me.

So, bottom line, my guess is that if your sample size is "large" then 
you won't go too far wrong by predicting y by

     exp(log(y).hat)*exp(s^2/2)

i.e. by exp(predict(model))*exp(s^2/2)

where s^2 is the estimated variance from the model that you fitted on 
the log scale using lm().

I would advise doing some fairly thorough simulations to get a feel for 
the size of the bias.  (There will always be *some* bias.)

You might also think about fitting the non-linear model

     y = gamma_0 * x^{gamma_1} * z^{gamma_2} + E

if you think that additive errors make more sense than multiplicative 
errors for your setting.

You can do this "fairly easily" using the function nls(), if you choose 
to go this way.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From fisher at plessthan.com  Sat Jan  2 03:10:53 2016
From: fisher at plessthan.com (Fisher Dennis)
Date: Fri, 1 Jan 2016 18:10:53 -0800
Subject: [R] plotmath problem
Message-ID: <6862B51B-B249-4BED-B605-C756BF7A659B@plessthan.com>

R 3.2
OS X

Colleagues,

This should be very simple but the solution eludes me.
I have a polymath label for a graphic:
	bquote(AUC[0-infinity]~(ng/ml~x~hours))
I would like the ?x? to be replaced with a bullet.  

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From dwinsemius at comcast.net  Sat Jan  2 03:39:25 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 1 Jan 2016 18:39:25 -0800
Subject: [R] plotmath problem
In-Reply-To: <6862B51B-B249-4BED-B605-C756BF7A659B@plessthan.com>
References: <6862B51B-B249-4BED-B605-C756BF7A659B@plessthan.com>
Message-ID: <C2247569-DB36-45E0-B5FD-0ED57260280C@comcast.net>


> On Jan 1, 2016, at 6:10 PM, Fisher Dennis <fisher at plessthan.com> wrote:
> 
> R 3.2
> OS X
> 
> Colleagues,
> 
> This should be very simple but the solution eludes me.
> I have a polymath label for a graphic:
> 	bquote(AUC[0-infinity]~(ng/ml~x~hours))
> I would like the ?x? to be replaced with a bullet.  

If by "bullet" you mean the mathematical symbol for multiplication, then it's just `%.%`

plot(1,1,xlab=bquote(AUC[0-infinity]~(ng/ml %.% hours))  )
> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From mortezafirouzi at yahoo.com  Sat Jan  2 06:56:42 2016
From: mortezafirouzi at yahoo.com (Morteza Firouzi)
Date: Sat, 2 Jan 2016 05:56:42 +0000 (UTC)
Subject: [R] missForest: Looping through files in a folder
In-Reply-To: <CA+8X3fXt=QrixO5wC=kY5Eob57tGScojp7yQDNs1Sa2i2-ACvA@mail.gmail.com>
References: <CA+8X3fXt=QrixO5wC=kY5Eob57tGScojp7yQDNs1Sa2i2-ACvA@mail.gmail.com>
Message-ID: <147162594.5958488.1451714202758.JavaMail.yahoo@mail.yahoo.com>

Hi Jim,Thank you very much for the time.You saved me 3 days!
Best regards,
Morteza 

    On Saturday, January 2, 2016 5:33 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
 

 Hi Morteza,What you may want is this:
my.files<-list.files(pattern=".csv")newfiles<-gsub(".","_F.",my.files,fixed=TRUE)for(i in 1:length(my.files)) {?mydat<-read.csv(my.files[i])?mydatimp<-missForest(mydat,verbose=TRUE,maxiter=5)?write.csv(mydatimp$ximp,newfiles[i])}
Jim

On Sat, Jan 2, 2016 at 5:32 AM, Morteza Firouzi via R-help <r-help at r-project.org> wrote:

Dear members,
Could you please help me on this issue. I've already searched and I watched some videos, but it was not useful.I need help to loop through the files in a folder (200+ csv files). I am using missForest() to impute missing values.?If I run the code for each single file, I have to do as following:
## main script for each single file
G1334108 <- read.csv(file.choose(), header = T)

G1334108.F <- missForest(G1334108, verbose = TRUE, maxiter = 5)

write.csv(G1334108.F$ximp, file = 'G1334108_F.csv')

I tried these below script codes to loop the function before writing here:
# 1st tryall.files <- list.files()
? ?my.files <- grep(".*csv", all_files, value=T)
? ? ? ? ? ? for(i in my.files){
? ? # do your operations here
? ? G1344108.Forest <- missForest(G1344108, verbose = TRUE, maxiter = 5)
? ? # save
? ? output.filename <- gsub("(.*?).csv", "\\1.csv", i)
? ? write.table(G1344108.Forest$ximp, output.filename)
}## ?2nd try
files <- list.files()lapply(files, function(x) {my.files <- read.csv("*.csv", header = T)missforest.out <- missForest(my.files, verbose = TRUE, maxiter = 5)write.csv(missforest.out$ximp, file = '*_F.csv')
}Thank you for the time.Best regards,Morteza

? ? ? ? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



  
	[[alternative HTML version deleted]]


From phschlesinger at gmail.com  Sat Jan  2 03:54:38 2016
From: phschlesinger at gmail.com (Paul Schlesinger)
Date: Fri, 1 Jan 2016 20:54:38 -0600
Subject: [R] R under OS X El Capitan 10.11.2
Message-ID: <CAG8VtR1=QnLeBmu8dtN+sHEX88NT-izbQvmZC5Pvh+V765T0=A@mail.gmail.com>

Since Apple upgrade to 10.11.2 and the new version ggplot2 running
library(ggplot2) gives the following

Error in library.dynam(lib, package, package.lib) :
  shared object ?Rcpp.so? not found
Error: package or namespace load failed for ?ggplot2?

Updating all packages went without errors but did not correct this error.
Re-installing R did not correct the situation and similar errors occur in
Studio.  This same R version and ggplot2 on Windows 7 did not produce these
error and is why I do not suspect ggplot2.

Thank you
-- 
Paul H. Schlesinger MD, PhD
Washington University School of Medicine

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Jan  2 09:04:04 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 2 Jan 2016 00:04:04 -0800
Subject: [R] R under OS X El Capitan 10.11.2
In-Reply-To: <CAG8VtR1=QnLeBmu8dtN+sHEX88NT-izbQvmZC5Pvh+V765T0=A@mail.gmail.com>
References: <CAG8VtR1=QnLeBmu8dtN+sHEX88NT-izbQvmZC5Pvh+V765T0=A@mail.gmail.com>
Message-ID: <6D4A69DE-0C0C-4FE4-8F4F-5CAE14B71AF5@comcast.net>


> On Jan 1, 2016, at 6:54 PM, Paul Schlesinger <phschlesinger at gmail.com> wrote:
> 
> Since Apple upgrade to 10.11.2 and the new version ggplot2 running
> library(ggplot2) gives the following
> 
> Error in library.dynam(lib, package, package.lib) :
>  shared object ?Rcpp.so? not found
> Error: package or namespace load failed for ?ggplot2?

Do you have Rcpp_0.12.2 installed?


> Updating all packages went without errors but did not correct this error.

If you used `update.packages, did you use checkBuilt=TRUE?

> Re-installing R did not correct the situation and similar errors occur in
> Studio.  This same R version and ggplot2 on Windows 7 did not produce these
> error and is why I do not suspect ggplot2.
> 
> Thank you
> -- 
> Paul H. Schlesinger MD, PhD
> Washington University School of Medicine
> 
> 	[[alternative HTML version deleted]]

Please read the Posting Guide. You are asked to post questions suspected of being due to Mac-related issues on R-SIG-Mac and you are asked to post in HTNL. You are further asked to post a full description of your setup including output from sessionInfo()

> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Sat Jan  2 09:05:17 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 2 Jan 2016 00:05:17 -0800
Subject: [R] R under OS X El Capitan 10.11.2
In-Reply-To: <6D4A69DE-0C0C-4FE4-8F4F-5CAE14B71AF5@comcast.net>
References: <CAG8VtR1=QnLeBmu8dtN+sHEX88NT-izbQvmZC5Pvh+V765T0=A@mail.gmail.com>
	<6D4A69DE-0C0C-4FE4-8F4F-5CAE14B71AF5@comcast.net>
Message-ID: <AFF84915-5F0F-413D-8960-9309A0EA46F0@comcast.net>


> On Jan 2, 2016, at 12:04 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Jan 1, 2016, at 6:54 PM, Paul Schlesinger <phschlesinger at gmail.com> wrote:
>> 
>> Since Apple upgrade to 10.11.2 and the new version ggplot2 running
>> library(ggplot2) gives the following
>> 
>> Error in library.dynam(lib, package, package.lib) :
>> shared object ?Rcpp.so? not found
>> Error: package or namespace load failed for ?ggplot2?
> 
> Do you have Rcpp_0.12.2 installed?
> 
> 
>> Updating all packages went without errors but did not correct this error.
> 
> If you used `update.packages, did you use checkBuilt=TRUE?
> 
>> Re-installing R did not correct the situation and similar errors occur in
>> Studio.  This same R version and ggplot2 on Windows 7 did not produce these
>> error and is why I do not suspect ggplot2.
>> 
>> Thank you
>> -- 
>> Paul H. Schlesinger MD, PhD
>> Washington University School of Medicine
>> 
>> 	[[alternative HTML version deleted]]
> 
> Please read the Posting Guide. You are asked to post questions suspected of being due to Mac-related issues on R-SIG-Mac and you are asked to post in HTNL.

rather ........................., and you are asked NOT to post in HTML.

> You are further asked to post a full description of your setup including output from sessionInfo()
> 
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> David Winsemius
> Alameda, CA, USA
> 

David Winsemius
Alameda, CA, USA


From jfox at mcmaster.ca  Sat Jan  2 17:25:18 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Sat, 2 Jan 2016 16:25:18 +0000
Subject: [R] R under OS X El Capitan 10.11.2
In-Reply-To: <CAG8VtR1=QnLeBmu8dtN+sHEX88NT-izbQvmZC5Pvh+V765T0=A@mail.gmail.com>
References: <CAG8VtR1=QnLeBmu8dtN+sHEX88NT-izbQvmZC5Pvh+V765T0=A@mail.gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F49080@FHSDB2D11-2.csu.mcmaster.ca>

Dear Paul,

I haven't seen this error myself, so this may be off-base, but the first thing I'd try (if you haven't already done it) is to (re)install Rcpp directly: install.packages("Rcpp"), possibly using a different CRAN mirror.

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Paul
> Schlesinger
> Sent: January 1, 2016 9:55 PM
> To: r-help at r-project.org
> Subject: [R] R under OS X El Capitan 10.11.2
> 
> Since Apple upgrade to 10.11.2 and the new version ggplot2 running
> library(ggplot2) gives the following
> 
> Error in library.dynam(lib, package, package.lib) :
>   shared object ?Rcpp.so? not found
> Error: package or namespace load failed for ?ggplot2?
> 
> Updating all packages went without errors but did not correct this error.
> Re-installing R did not correct the situation and similar errors occur in Studio.
> This same R version and ggplot2 on Windows 7 did not produce these error and
> is why I do not suspect ggplot2.
> 
> Thank you
> --
> Paul H. Schlesinger MD, PhD
> Washington University School of Medicine
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From dwinsemius at comcast.net  Sat Jan  2 17:38:51 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 2 Jan 2016 08:38:51 -0800
Subject: [R] [FORGED] Histogram for Left Censored Data
In-Reply-To: <CAHDp66D-f84foqjHZPcL8JdQ9vGEWiQxPmOBkRo1QJ93LqAj_Q@mail.gmail.com>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
	<56859B36.4010202@auckland.ac.nz>
	<CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
	<2255B030-4A19-48E4-B3B5-CF67B216C37A@comcast.net>
	<CAHDp66D-f84foqjHZPcL8JdQ9vGEWiQxPmOBkRo1QJ93LqAj_Q@mail.gmail.com>
Message-ID: <3CECCEE3-11F7-4F52-BBA4-B42114F57DBF@comcast.net>


> On Jan 2, 2016, at 2:24 AM, Steven Stoline <sstoline at gmail.com> wrote:
> 
> Dear David:
> 
> Thank you very much for the code, it works very good for this data set.
> 
> I just have one more thing (if not bothered you).
> 
> how about if some of the non-censored (fully measured) data equal to the detection limit?
> 
> As an example, in the data set below, there are 16 censored observations with detection limit of 0.01, and there are some non-censored data observation equal to 0.01 (equal to the detection limit). I am wondering if we still can distinguish  between them in the histogram. I tried to modify your code, but I could not make it work for this situation.

I would probably construct an intermediate dataset copy where you "lowered" the items that were below the detection limit to a value .... below the detection limit, and then set the breaks parameter so that the real 0.01 items were included in the second bin.

(That actually mimics what I usually do with the actual values in regression situations. I consider the measurements "below the detection limit" to still be meaningful.)

-- 
David.
> 
> I crated a data frame, I want to create histogram for the variable "NH3Nconcentrations" (second column in the data frame).
> 
> 
> Once again, thank you very much for your helps.
> 
> 
> 
> 
> cen<-c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
> 
> censored<-c(TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE)
> 
> data.original<-c("<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01",
> "<0.01","<0.01","<0.01","<0.01","<0.01","<0.01",0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
> 0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,
> 0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.05,0.05,0.05,0.06,0.47)
> 
> NH3Nconcentrations<-c(0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01
> ,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,
> 0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.05,
> 0.05,0.05,0.06,0.47)
> 
> NH3N.concentrations<-data.frame(data.original,NH3Nconcentrations,cen,censored)
> 
> attach(NH3N.concentrations)
> 
> 
> NH3N.concentrations
> 
> 
> 
> with many thanks
> steve
> 
> On Fri, Jan 1, 2016 at 3:42 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
>> On Jan 1, 2016, at 3:45 AM, Steven Stoline <sstoline at gmail.com> wrote:
>> 
>> Dear Rolf:
>> 
>> 
>> The histogram should contain a bar(s) for the censored data values replaced
>> by their detection limit(s) with different color than other bars for the
>> noncensored values . In this example there are only 3 censored values with
>> only one detection limit of DL = 1450.
>> 
>> 
>> with many thanks
>> steve
>> 
>> 
>> 
>> On Thu, Dec 31, 2015 at 4:16 PM, Rolf Turner <r.turner at auckland.ac.nz>
>> wrote:
>> 
>>> On 31/12/15 23:20, Steven Stoline wrote:
>>> 
>>>> Dear All:
>>>> 
>>>> I need helps with creating histograms for data that include left
>>>> censored observations.
>>>> 
>>>> Here is an example of left censored data
>>>> 
>>>> 
>>>> 
>>>> *Sulfate.Concentration*
>>>> <-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,
>>>> 1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
>>>> 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)
>>>> 
> 
>  myhist <- hist(sulfate[,1], breaks=c(1400,1451,1500,1600,1700,1800,1900), col=c(1,rep(2,5)), xaxt="n")
> #  plots with no x axis labeling
>  myhist
> #------------------
> $breaks
> [1] 1400 1451 1500 1600 1700 1800 1900
> 
> $counts
> [1]  3  1  1  0 14  5
> 
> $density
> [1] 0.0024509804 0.0008503401 0.0004166667 0.0000000000 0.0058333333 0.0020833333
> 
> $mids
> [1] 1425.5 1475.5 1550.0 1650.0 1750.0 1850.0
> 
> $xname
> [1] "sulfate[, 1]"
> 
> $equidist
> [1] FALSE
> 
> attr(,"class")
> [1] "histogram"
> #---rebuild the x-axis ----------------
>  axis(1, at=c(myhist$mids[1],myhist$breaks[-(1:2)]), labels=c("<1450", myhist$breaks[-(1:2)]))
> 
> <Rplot001.png>
> 
> -- 
> David.
> 
>>>> 
>>>> *Column 2* is an indicator for censoring "*1*" for left censored
>>>> observations and "*0*" for non-censored (fully measured)
>>>> observations.
>>>> 
>>> 
>>> And what, pray tell, do you want the resulting histogram to look like?
>>> See e.g. fortune("mind_read").
>>> 
>>> cheers,
>>> 
>>> Rolf Turner
>>> 
>>> --
>>> Technical Editor ANZJS
>>> Department of Statistics
>>> University of Auckland
>>> Phone: +64-9-373-7599 ext. 88276
>>> 
>> 
>> 
>> 
>> -- 
>> Steven M. Stoline
>> 1123 Forest Avenue
>> Portland, ME 04112
>> sstoline at gmail.com
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 
> 
> 
> -- 
> Steven M. Stoline
> 1123 Forest Avenue
> Portland, ME 04112
> sstoline at gmail.com

David Winsemius
Alameda, CA, USA


From fisher at plessthan.com  Sat Jan  2 17:52:22 2016
From: fisher at plessthan.com (Fisher Dennis)
Date: Sat, 2 Jan 2016 08:52:22 -0800
Subject: [R] Additional polymath query
Message-ID: <18AD9A07-E30B-452E-BC79-237C846136FE@plessthan.com>

R 3.2
OS X

Colleagues

Yesterday, I asked:
	I have a polymath label for a graphic:
		bquote(AUC[0-infinity]~(ng/ml~x~hours))
	I would like the ?x? to be replaced with a bullet.  

David Winsemius provided an answer:
	%.%
(which is also known as ?cdot?)

That works but the dot is small ? I would prefer something bolder.  
Further searching led to the following:
	plot(1, xlab="\u25CF") 

Now I can?t figure out how to incorporate
	\u25CF
into the original command (to replace the ?x? below):
	bquote(AUC[0-infinity]~(ng/ml~x~hours))

As much as I enjoy plotmath, it remains a bit of a mystery to me.  Any advice would be appreciated.

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From bgunter.4567 at gmail.com  Sat Jan  2 18:38:11 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 2 Jan 2016 09:38:11 -0800
Subject: [R] Additional polymath query
In-Reply-To: <18AD9A07-E30B-452E-BC79-237C846136FE@plessthan.com>
References: <18AD9A07-E30B-452E-BC79-237C846136FE@plessthan.com>
Message-ID: <CAGxFJbSFuq1NzHCPiuE-jXuJzE=4X8J2Q3xKtcsiifXuxhy8yA@mail.gmail.com>

Quote the unicode -- it's just a way to write a text character:

bquote(AUC[0-infinity]~(ng/ml~"\u25CF"~hours)


Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Jan 2, 2016 at 8:52 AM, Fisher Dennis <fisher at plessthan.com> wrote:
> R 3.2
> OS X
>
> Colleagues
>
> Yesterday, I asked:
>         I have a polymath label for a graphic:
>                 bquote(AUC[0-infinity]~(ng/ml~x~hours))
>         I would like the ?x? to be replaced with a bullet.
>
> David Winsemius provided an answer:
>         %.%
> (which is also known as ?cdot?)
>
> That works but the dot is small ? I would prefer something bolder.
> Further searching led to the following:
>         plot(1, xlab="\u25CF")
>
> Now I can?t figure out how to incorporate
>         \u25CF
> into the original command (to replace the ?x? below):
>         bquote(AUC[0-infinity]~(ng/ml~x~hours))
>
> As much as I enjoy plotmath, it remains a bit of a mystery to me.  Any advice would be appreciated.
>
> Dennis
>
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Sat Jan  2 18:39:31 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 2 Jan 2016 12:39:31 -0500
Subject: [R] Additional polymath query
In-Reply-To: <18AD9A07-E30B-452E-BC79-237C846136FE@plessthan.com>
References: <18AD9A07-E30B-452E-BC79-237C846136FE@plessthan.com>
Message-ID: <56880B53.50600@gmail.com>

On 02/01/2016 11:52 AM, Fisher Dennis wrote:
> R 3.2
> OS X
>
> Colleagues
>
> Yesterday, I asked:
> 	I have a polymath label for a graphic:
> 		bquote(AUC[0-infinity]~(ng/ml~x~hours))
> 	I would like the ?x? to be replaced with a bullet.
>
> David Winsemius provided an answer:
> 	%.%
> (which is also known as ?cdot?)
>
> That works but the dot is small ? I would prefer something bolder.
> Further searching led to the following:
> 	plot(1, xlab="\u25CF")
>
> Now I can?t figure out how to incorporate
> 	\u25CF
> into the original command (to replace the ?x? below):
> 	bquote(AUC[0-infinity]~(ng/ml~x~hours))
>
> As much as I enjoy plotmath, it remains a bit of a mystery to me.  Any advice would be appreciated.

Just replace x with "\u25CF" (including the quotes).  You do need to be 
a little careful here:  not all graphics devices support all Unicode 
characters.  That one works on my OSX system running in RStudio, but you 
didn't say what particular output format you wanted, so you'll need to 
check it yourself.

Duncan Murdoch


From dwinsemius at comcast.net  Sat Jan  2 18:53:37 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 2 Jan 2016 09:53:37 -0800
Subject: [R] Additional polymath query
In-Reply-To: <18AD9A07-E30B-452E-BC79-237C846136FE@plessthan.com>
References: <18AD9A07-E30B-452E-BC79-237C846136FE@plessthan.com>
Message-ID: <713A9C1D-3CF9-4AF3-BC93-7D0605B9E47E@comcast.net>


> On Jan 2, 2016, at 8:52 AM, Fisher Dennis <fisher at plessthan.com> wrote:
> 
> R 3.2
> OS X
> 
> Colleagues
> 
> Yesterday, I asked:
> 	I have a polymath label for a graphic:
> 		bquote(AUC[0-infinity]~(ng/ml~x~hours))
> 	I would like the ?x? to be replaced with a bullet.  
> 
> David Winsemius provided an answer:
> 	%.%
> (which is also known as ?cdot?)
> 
> That works but the dot is small ? I would prefer something bolder.  
> Further searching led to the following:
> 	plot(1, xlab="\u25CF") 
> 
> Now I can?t figure out how to incorporate
> 	\u25CF
> into the original command (to replace the ?x? below):
> 	bquote(AUC[0-infinity]~(ng/ml~x~hours))
> 
> As much as I enjoy plotmath, it remains a bit of a mystery to me.  Any advice would be appreciated.

The `?plotmath` help page also refers you to the help page on `?points`. I have on several occasions used the last example on that page to create an annotated table of glyphs associated with the Symbol font "characters" in the higher ranges of values:

pdf(); TestChars <- function(sign = 1, font = 1, ...)
{
   MB <- l10n_info()$MBCS
   r <- if(font == 5) { sign <- 1; c(32:126, 160:254)
       } else if(MB) 32:126 else 32:255
   if (sign == -1) r <- c(32:126, 160:255)
   par(pty = "s")
   plot(c(-1,16), c(-1,16), type = "n", xlab = "", ylab = "",
        xaxs = "i", yaxs = "i",
        main = paste( "Mac Symbol: points example", sprintf("sign = %d, font = %d", sign, font)))
   grid(17, 17, lty = 1) ; mtext(paste("MBCS:", MB))
   for(i in r) {
         try(points(i%%16, i%/%16, pch = sign*i, font = font,...))
         try(text(i%%16 -.3, i%/%16-.3, labels = i,cex=.6)) }
}

TestChars(font = 5); dev.off()

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplots.pdf
Type: application/pdf
Size: 7945 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160102/b561edaf/attachment.pdf>
-------------- next part --------------


Looks to me that the 183 glyph might be to your liking. `points` has a 'cex' parameter that could be used to embolden it further if needed.

-- 
David.

> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ulrik.stervbo at gmail.com  Sat Jan  2 13:24:11 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Sat, 02 Jan 2016 12:24:11 +0000
Subject: [R] save screen printed data frames and ggplots into a file
In-Reply-To: <CAGxFJbSsREVTz1eRLCtxYXt87HTy04e3K1SRWywyNGZdw3qORg@mail.gmail.com>
References: <DUB125-W90ECDED878F7AC5B18736CB3FF0@phx.gbl>
	<CAGxFJbS7ovHOqqE4kGtrj4D2bdzh80G2hdjaFyB7_MHQhuc-JA@mail.gmail.com>
	<DUB125-W70EC720B33389D714D927CB3FF0@phx.gbl>
	<CAGxFJbSsREVTz1eRLCtxYXt87HTy04e3K1SRWywyNGZdw3qORg@mail.gmail.com>
Message-ID: <CAKVAULOYztHoLUr6AorMbiYtZe+4Zg8YHTjjGRDT89cp024UQQ@mail.gmail.com>

No clear to me either but if you want to create dynamic reports you can
look at the knitr package.

Or gridextra to have a table next to a plot.

HTH
Ulrik

On Fri, 1 Jan 2016 20:20 Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Still not clear (to me).
>
> But perhaps FAQ 7.16. You need to explicitly print or use echo = TRUE
> when sourcing from a file.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Fri, Jan 1, 2016 at 10:25 AM, Ragia Ibrahim <ragia11 at hotmail.com>
> wrote:
> > yes, the script prints that on the scree, when I used pd file to do the
> job only ggplots appears on it !
> >
> > hope this clarifies, thanks .
> > Ragia
> >
> > ----------------------------------------
> >> Date: Thu, 31 Dec 2015 17:19:35 -0800
> >> Subject: Re: [R] save screen printed data frames and ggplots into a file
> >> From: bgunter.4567 at gmail.com
> >> To: ragia11 at hotmail.com
> >> CC: r-help at r-project.org
> >>
> >> Does what? You said you already have a script.
> >>
> >> **If** you want to program it as a function, then I suggest that you
> >> do some homework and go through an R tutorial, e.g. the "Intro to R"
> >> that ships with R or one of the many available on the web.
> >>
> >> If that is not what you mean, then clarify.
> >>
> >> Cheers,
> >> Bert
> >> Bert Gunter
> >>
> >> "The trouble with having an open mind is that people keep coming along
> >> and sticking things into it."
> >> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >>
> >>
> >> On Thu, Dec 31, 2015 at 4:46 PM, Ragia Ibrahim <ragia11 at hotmail.com>
> wrote:
> >>> Dear group
> >>> I have a script that prints data frames and plot using ggplot 2 while
> running,
> >>> how can I do this
> >>> thanks in advance
> >>> Ragia
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Jan  2 19:17:03 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 2 Jan 2016 10:17:03 -0800
Subject: [R] Additional polymath query
In-Reply-To: <713A9C1D-3CF9-4AF3-BC93-7D0605B9E47E@comcast.net>
References: <18AD9A07-E30B-452E-BC79-237C846136FE@plessthan.com>
	<713A9C1D-3CF9-4AF3-BC93-7D0605B9E47E@comcast.net>
Message-ID: <7C3979B3-9716-44E3-949C-56B52D6727FE@comcast.net>


> On Jan 2, 2016, at 9:53 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Jan 2, 2016, at 8:52 AM, Fisher Dennis <fisher at plessthan.com> wrote:
>> 
>> R 3.2
>> OS X
>> 
>> Colleagues
>> 
>> Yesterday, I asked:
>> 	I have a polymath label for a graphic:
>> 		bquote(AUC[0-infinity]~(ng/ml~x~hours))
>> 	I would like the ?x? to be replaced with a bullet.  
>> 
>> David Winsemius provided an answer:
>> 	%.%
>> (which is also known as ?cdot?)
>> 
>> That works but the dot is small ? I would prefer something bolder.  
>> Further searching led to the following:
>> 	plot(1, xlab="\u25CF") 
>> 
>> Now I can?t figure out how to incorporate
>> 	\u25CF
>> into the original command (to replace the ?x? below):
>> 	bquote(AUC[0-infinity]~(ng/ml~x~hours))
>> 
>> As much as I enjoy plotmath, it remains a bit of a mystery to me.  Any advice would be appreciated.
> 
> The `?plotmath` help page also refers you to the help page on `?points`. I have on several occasions used the last example on that page to create an annotated table of glyphs associated with the Symbol font "characters" in the higher ranges of values:
> 
> pdf(); TestChars <- function(sign = 1, font = 1, ...)
> {
>   MB <- l10n_info()$MBCS
>   r <- if(font == 5) { sign <- 1; c(32:126, 160:254)
>       } else if(MB) 32:126 else 32:255
>   if (sign == -1) r <- c(32:126, 160:255)
>   par(pty = "s")
>   plot(c(-1,16), c(-1,16), type = "n", xlab = "", ylab = "",
>        xaxs = "i", yaxs = "i",
>        main = paste( "Mac Symbol: points example", sprintf("sign = %d, font = %d", sign, font)))
>   grid(17, 17, lty = 1) ; mtext(paste("MBCS:", MB))
>   for(i in r) {
>         try(points(i%%16, i%/%16, pch = sign*i, font = font,...))
>         try(text(i%%16 -.3, i%/%16-.3, labels = i,cex=.6)) }
> }
> 
> TestChars(font = 5); dev.off()
> 
> <Rplots.pdf>

Duncan Murdoch's advice is relevant here: " You do need to be a little careful here: not all graphics devices support all Unicode characters."

The code above prompted a waring that:

1: In plot.xy(xy.coords(x, y), type = type, ...) :
  font width unknown for character 0xf0

Character 0xF0 is decimal 240 which is ironically enough the Apple logo symbol. It does print to the interactive device on a Mac and to the png()-device but not to the pdf()-device.

I get this for the first few names of pdfFonts:

>  names(pdfFonts())
 [1] "serif"                "sans"                 "mono"                
 [4] "AvantGarde"           "Bookman"              "Courier"    

So maybe my table was mislabeled and it is for "Bookman" rather than Symbol? We've now exceeded my knowledge of the various moving parts of the Mac-R graphics interface.



-- 
David.

> 
> Looks to me that the 183 glyph might be to your liking. `points` has a 'cex' parameter that could be used to embolden it further if needed.
> 
> -- 
> David.
> 
>> 
>> Dennis
>> 
>> Dennis Fisher MD
>> P < (The "P Less Than" Company)
>> Phone: 1-866-PLessThan (1-866-753-7784)
>> Fax: 1-866-PLessThan (1-866-753-7784)
>> www.PLessThan.com
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From Randy.Bangert at nau.edu  Sat Jan  2 20:50:23 2016
From: Randy.Bangert at nau.edu (Randy Kip Bangert)
Date: Sat, 2 Jan 2016 19:50:23 +0000
Subject: [R] R package
Message-ID: <B6DAFF37-1CDB-41A4-8E2C-377E28BC89FC@nau.edu>

I really like the R package and find it extremely useful. I also find the R project impossible to use as I am not a programmer. Is there a version of the R package that will run on OS X?
_____________________________________________________
Randy Bangert
Cortez, CO 81321
Northern Arizona University
rkb at nau.edu
http://oak.ucc.nau.edu/rkb/RKB/Home.html


From jdnewmil at dcn.davis.ca.us  Sat Jan  2 21:24:26 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 2 Jan 2016 12:24:26 -0800
Subject: [R] R package
In-Reply-To: <B6DAFF37-1CDB-41A4-8E2C-377E28BC89FC@nau.edu>
References: <B6DAFF37-1CDB-41A4-8E2C-377E28BC89FC@nau.edu>
Message-ID: <7276A75C-9E59-4C41-9FCA-085A2128BB9E@dcn.davis.ca.us>

Of course there is [1], but the fact that it works on a Mac does not change the fact that R is a programming language. Until you begin the process of becoming a programmer you won't be able to make much use of R regardless of which computer or operating system you run it on. Fortunately such a transformation is doable,  but you have to believe it is possible or you won't work to make it happen... and transformation does take some work.

[1] https://cran.r-project.org/bin/macosx/

-- 
Sent from my phone. Please excuse my brevity.

On January 2, 2016 11:50:23 AM PST, Randy Kip Bangert <Randy.Bangert at nau.edu> wrote:
>I really like the R package and find it extremely useful. I also find
>the R project impossible to use as I am not a programmer. Is there a
>version of the R package that will run on OS X?
>_____________________________________________________
>Randy Bangert
>Cortez, CO 81321
>Northern Arizona University
>rkb at nau.edu
>http://oak.ucc.nau.edu/rkb/RKB/Home.html
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Jan  2 23:04:54 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 2 Jan 2016 14:04:54 -0800
Subject: [R] R package
In-Reply-To: <7276A75C-9E59-4C41-9FCA-085A2128BB9E@dcn.davis.ca.us>
References: <B6DAFF37-1CDB-41A4-8E2C-377E28BC89FC@nau.edu>
	<7276A75C-9E59-4C41-9FCA-085A2128BB9E@dcn.davis.ca.us>
Message-ID: <E04616ED-49BB-4D86-98AD-E1716754B30D@comcast.net>


> On Jan 2, 2016, at 12:24 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> Of course there is [1], but the fact that it works on a Mac does not change the fact that R is a programming language. Until you begin the process of becoming a programmer you won't be able to make much use of R regardless of which computer or operating system you run it on. Fortunately such a transformation is doable,  but you have to believe it is possible or you won't work to make it happen... and transformation does take some work.
> 
> [1] https://cran.r-project.org/bin/macosx/


All true. Some of us who are running R on Macs chose to do so at a time when Windoze machines were limited to 4G (actually 2.5 Gigs effectively) and we wanted a bit more headroom for larger objects. (I started back in the days of CP/M.)  It wasn't the possibility of having dropdown menus, but rather the fact that it was a reasonable GUI bolted on top of a Unix box.


If you want dropdown menus, then consider the Rcmdr package which runs in all of the three strains of OSes that are supported by the R Core. The RStudio effort is also a nice facility to ease the transition to "real programming".

But if you really don't want to exert any effort and simply want a "non-programming package", then buy a seat for the JMP program. (Not cheap.)

-- 
David.

> 
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On January 2, 2016 11:50:23 AM PST, Randy Kip Bangert <Randy.Bangert at nau.edu> wrote:
>> I really like the R package and find it extremely useful. I also find
>> the R project impossible to use as I am not a programmer. Is there a
>> version of the R package that will run on OS X?
>> _____________________________________________________
>> Randy Bangert
>> Cortez, CO 81321
>> Northern Arizona University
>> rkb at nau.edu
>> http://oak.ucc.nau.edu/rkb/RKB/Home.html
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.ca.us  Sat Jan  2 23:20:51 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 02 Jan 2016 14:20:51 -0800
Subject: [R] R package
In-Reply-To: <3A1B3BC1-4D8E-40E7-B20B-A2521AA6359E@nau.edu>
References: <B6DAFF37-1CDB-41A4-8E2C-377E28BC89FC@nau.edu>
	<7276A75C-9E59-4C41-9FCA-085A2128BB9E@dcn.davis.ca.us>
	<3A1B3BC1-4D8E-40E7-B20B-A2521AA6359E@nau.edu>
Message-ID: <A5161C00-706C-42E3-933D-C1692F6995FA@dcn.davis.ca.us>

Your distinction makes no sense to me. 

"Package" in the context of R is a set of useful related functions usable within R. There is no "R" package in this sense. 

"Package" is also sometimes used to refer to installable software. The link I pointed you to has just such an item, specifically for MacOSX.

So... can you clarify what you really mean?
-- 
Sent from my phone. Please excuse my brevity.

On January 2, 2016 1:33:59 PM PST, Randy Kip Bangert <Randy.Bangert at nau.edu> wrote:
>R package not R project.
>_____________________________________________________
>Randy Bangert
>Cortez, CO 81321
>Northern Arizona University
>rkb at nau.edu<mailto:rkb at nau.edu>
>http://oak.ucc.nau.edu/rkb/RKB/Home.html
>
>On Jan 2, 2016, at 1:24 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
>
>Of course there is [1], but the fact that it works on a Mac does not
>change the fact that R is a programming language. Until you begin the
>process of becoming a programmer you won't be able to make much use of
>R regardless of which computer or operating system you run it on.
>Fortunately such a transformation is doable, but you have to believe it
>is possible or you won't work to make it happen... and transformation
>does take some work.
>
>[1] https://cran.r-project.org/bin/macosx/
>
>--
>Sent from my phone. Please excuse my brevity.
>
>On January 2, 2016 11:50:23 AM PST, Randy Kip Bangert
><Randy.Bangert at nau.edu<mailto:Randy.Bangert at nau.edu>> wrote:
>
>I really like the R package and find it extremely useful. I also find
>the R project impossible to use as I am not a programmer. Is there a
>version of the R package that will run on OS X?
>________________________________
>
>Randy Bangert
>Cortez, CO 81321
>Northern Arizona University
>rkb at nau.edu<mailto:rkb at nau.edu>
>http://oak.ucc.nau.edu/rkb/RKB/Home.html
>
>________________________________
>
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sun Jan  3 00:18:10 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 02 Jan 2016 15:18:10 -0800
Subject: [R] R package
In-Reply-To: <386E0674-5E64-4075-A0C7-54B3820DDF4A@nau.edu>
References: <B6DAFF37-1CDB-41A4-8E2C-377E28BC89FC@nau.edu>
	<7276A75C-9E59-4C41-9FCA-085A2128BB9E@dcn.davis.ca.us>
	<3A1B3BC1-4D8E-40E7-B20B-A2521AA6359E@nau.edu>
	<A5161C00-706C-42E3-933D-C1692F6995FA@dcn.davis.ca.us>
	<386E0674-5E64-4075-A0C7-54B3820DDF4A@nau.edu>
Message-ID: <CDDB83A6-7001-49A0-A9C0-879EE8BADD68@dcn.davis.ca.us>

Google turned up [1], but it is old and advertised as alpha. Perhaps someone with a Mac can add more clarity. 

[1] http://adn.biol.umontreal.ca/~numericalecology/old/R/v4/telecharger.html
-- 
Sent from my phone. Please excuse my brevity.

On January 2, 2016 2:30:32 PM PST, Randy Kip Bangert <Randy.Bangert at nau.edu> wrote:
>Sorry, I thought this was to go to Philippe Casgrain based on
>information on the web comparing the R package to the R project. I am
>interested in the R package. I will try to contact Philippe Casgrain.
>_____________________________________________________
>Randy Bangert
>Cortez, CO 81321
>Northern Arizona University
>rkb at nau.edu<mailto:rkb at nau.edu>
>http://oak.ucc.nau.edu/rkb/RKB/Home.html
>
>On Jan 2, 2016, at 3:20 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
>
>Your distinction makes no sense to me.
>
>"Package" in the context of R is a set of useful related functions
>usable within R. There is no "R" package in this sense.
>
>"Package" is also sometimes used to refer to installable software. The
>link I pointed you to has just such an item, specifically for MacOSX.
>
>So... can you clarify what you really mean?
>--
>Sent from my phone. Please excuse my brevity.
>
>On January 2, 2016 1:33:59 PM PST, Randy Kip Bangert
><Randy.Bangert at nau.edu<mailto:Randy.Bangert at nau.edu>> wrote:
>R package not R project.
>_____________________________________________________
>Randy Bangert
>Cortez, CO 81321
>Northern Arizona University
>rkb at nau.edu<mailto:rkb at nau.edu>
>http://oak.ucc.nau.edu/rkb/RKB/Home.html
>
>On Jan 2, 2016, at 1:24 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
>
>Of course there is [1], but the fact that it works on a Mac does not
>change the fact that R is a programming language. Until you begin the
>process of becoming a programmer you won't be able to make much use of
>R regardless of which computer or operating system you run it on.
>Fortunately such a transformation is doable, but you have to believe it
>is possible or you won't work to make it happen... and transformation
>does take some work.
>
>[1] https://cran.r-project.org/bin/macosx/
>
>--
>Sent from my phone. Please excuse my brevity.
>
>On January 2, 2016 11:50:23 AM PST, Randy Kip Bangert
><Randy.Bangert at nau.edu<mailto:Randy.Bangert at nau.edu>> wrote:
>
>I really like the R package and find it extremely useful. I also find
>the R project impossible to use as I am not a programmer. Is there a
>version of the R package that will run on OS X?
>________________________________
>
>Randy Bangert
>Cortez, CO 81321
>Northern Arizona University
>rkb at nau.edu<mailto:rkb at nau.edu>
>http://oak.ucc.nau.edu/rkb/RKB/Home.html
>
>________________________________
>
>R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
>UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From mkashif at uaf.edu.pk  Sun Jan  3 01:02:33 2016
From: mkashif at uaf.edu.pk (Muhammad  Kashif)
Date: Sun, 3 Jan 2016 00:02:33 +0000
Subject: [R] (no subject)
Message-ID: <DBXPR07MB223F67F80D41354BA51102994F10@DBXPR07MB223.eurprd07.prod.outlook.com>

Dear i optimized the gama and beta value using MLE via simmulation of birnbaum saunders distribution. if i run this code it generate very small value of beta. Could any one help me in this regard. i use gbs package to generate data.

gama=1.0
beta=1.3
n=25
iterCount=1000
for(i in 1:iterCount){
x<-rgbs(n,gama,beta)
P<-function(theta,x){
n<-length(x)
gama<-theta[1]
beta<-theta[2]
-n*log(5.013)+ n*(theta[1])^-2-n*log(theta[1])-n/2*log(theta[2])+ sum(log(theta[2]+x)-0.5/theta[1]^2*(x/theta[2]+theta[2]/x))}}
P.out<-optim(theta<-c(gama,beta),ll.wd,x=x,method = "Nelder-Mead",hessian=FALSE)
gamhat<-P.out$par[1]
betahat<-P.out$par[2]

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Sun Jan  3 14:21:54 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Sun, 3 Jan 2016 13:21:54 +0000
Subject: [R] R under OS X El Capitan 10.11.2
In-Reply-To: <CAG8VtR0__w+xd9Eh-uaoXB16ykC5_FN1BXZHAGSOiFzEy6fibg@mail.gmail.com>
References: <CAG8VtR1=QnLeBmu8dtN+sHEX88NT-izbQvmZC5Pvh+V765T0=A@mail.gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC810F49080@FHSDB2D11-2.csu.mcmaster.ca>
	<CAG8VtR0__w+xd9Eh-uaoXB16ykC5_FN1BXZHAGSOiFzEy6fibg@mail.gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F4930C@FHSDB2D11-2.csu.mcmaster.ca>

Dear Paul,

> -----Original Message-----
> From: Paul Schlesinger [mailto:phschlesinger at gmail.com]
> Sent: January 2, 2016 9:23 PM
> To: Fox, John <jfox at mcmaster.ca>
> Subject: Re: [R] R under OS X El Capitan 10.11.2
> 
> Thanks
> 
> Installing from another mirror  corrected the error.  This is troubling, how often
> have you found a mirror to be unreliable.  The mirror that did not work was the
> one ant my university.  I need to make local inquiries.

I haven't experienced this kind of problem myself, but have occasionally noticed that students failed to install a dependency during a package install. The problem can be as simple as a mirror lagging slightly behind CRAN.

I'm cc'ing this back to the r-help list where you posted your initial question, so that the solution will be in the list archives.

Best,
 John

> 
> thank you again
> Paul
> 
> On Sat, Jan 2, 2016 at 10:25 AM, Fox, John <jfox at mcmaster.ca
> <mailto:jfox at mcmaster.ca> > wrote:
> 
> 
> 	Dear Paul,
> 
> 	I haven't seen this error myself, so this may be off-base, but the first
> thing I'd try (if you haven't already done it) is to (re)install Rcpp directly:
> install.packages("Rcpp"), possibly using a different CRAN mirror.
> 
> 	I hope this helps,
> 	 John
> 
> 	-----------------------------
> 	John Fox, Professor
> 	McMaster University
> 	Hamilton, Ontario
> 	Canada L8S 4M4
> 	Web: socserv.mcmaster.ca/jfox <http://socserv.mcmaster.ca/jfox>
> 
> 
> 
> 
> 	> -----Original Message-----
> 	> From: R-help [mailto:r-help-bounces at r-project.org <mailto:r-help-
> bounces at r-project.org> ] On Behalf Of Paul
> 	> Schlesinger
> 	> Sent: January 1, 2016 9:55 PM
> 	> To: r-help at r-project.org <mailto:r-help at r-project.org>
> 	> Subject: [R] R under OS X El Capitan 10.11.2
> 	>
> 	> Since Apple upgrade to 10.11.2 and the new version ggplot2 running
> 	> library(ggplot2) gives the following
> 	>
> 	> Error in library.dynam(lib, package, package.lib) :
> 	>   shared object ?Rcpp.so? not found
> 	> Error: package or namespace load failed for ?ggplot2?
> 	>
> 	> Updating all packages went without errors but did not correct this
> error.
> 	> Re-installing R did not correct the situation and similar errors occur
> in Studio.
> 	> This same R version and ggplot2 on Windows 7 did not produce these
> error and
> 	> is why I do not suspect ggplot2.
> 	>
> 	> Thank you
> 	> --
> 	> Paul H. Schlesinger MD, PhD
> 	> Washington University School of Medicine
> 	>
> 
> 	>       [[alternative HTML version deleted]]
> 	>
> 	> ______________________________________________
> 	> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list --
> To UNSUBSCRIBE and more, see
> 	> https://stat.ethz.ch/mailman/listinfo/r-help
> 
> 	> PLEASE do read the posting guide http://www.R-project.org/posting-
> 	> guide.html
> 	> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 
> 
> --
> 
> Paul H. Schlesinger MD, PhD
> Washington University School of Medicine

From pdalgd at gmail.com  Sun Jan  3 14:41:14 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 3 Jan 2016 14:41:14 +0100
Subject: [R] (no subject)
In-Reply-To: <DBXPR07MB223F67F80D41354BA51102994F10@DBXPR07MB223.eurprd07.prod.outlook.com>
References: <DBXPR07MB223F67F80D41354BA51102994F10@DBXPR07MB223.eurprd07.prod.outlook.com>
Message-ID: <AF23D5EB-B520-4743-9F6F-D9DF670B6729@gmail.com>

Your code could do with some revision: If I read the curly braces correectly, you simulate 1000 data sets and define the same function 1000 times, then use the function on the last simulated data set. Also, inside the function, you define gama and beta, but do not actually use them.

However, none of that would cause your symptoms. More likely something is wrong in in your function P so that it does not actually compute the negative log likelihood.

I'm not familiar with this particular distribution, and I think it would be your job and not mine to check against the theory. However, looking at the function as written, I see two divergent terms as theta[2] approaches zero: 

-n/2*log(theta[2]) goes to plus infinity

sum(-0.5/theta[1]^2*(x/theta[2])) goes to minus infinity,
if theta[1] > 0

The latter term is faster so it seems that your function has no lower limit as theta[2] approaches zero. And optim looks for a minimum.

This suggests a sign error somewhere. However, is there not a dgbs function in the gbs package, to compute the density of the distribution? If so, it would be much easier to minimize 

-sum(dgbs(x, gama, beta, log=TRUE))  

or, in case the log argument doesn't work

-sum(log(dgbs(x, gama, beta))

-pd

> On 03 Jan 2016, at 01:02 , Muhammad Kashif <mkashif at uaf.edu.pk> wrote:
> 
> Dear i optimized the gama and beta value using MLE via simmulation of birnbaum saunders distribution. if i run this code it generate very small value of beta. Could any one help me in this regard. i use gbs package to generate data.
> 
> gama=1.0
> beta=1.3
> n=25
> iterCount=1000
> for(i in 1:iterCount){
> x<-rgbs(n,gama,beta)
> P<-function(theta,x){
> n<-length(x)
> gama<-theta[1]
> beta<-theta[2]
> -n*log(5.013)+ n*(theta[1])^-2-n*log(theta[1])-n/2*log(theta[2])+ sum(log(theta[2]+x)-0.5/theta[1]^2*(x/theta[2]+theta[2]/x))}}
> P.out<-optim(theta<-c(gama,beta),ll.wd,x=x,method = "Nelder-Mead",hessian=FALSE)
> gamhat<-P.out$par[1]
> betahat<-P.out$par[2]
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From sstoline at gmail.com  Sun Jan  3 15:33:27 2016
From: sstoline at gmail.com (Steven Stoline)
Date: Sun, 3 Jan 2016 09:33:27 -0500
Subject: [R] [FORGED] Histogram for Left Censored Data
In-Reply-To: <3CECCEE3-11F7-4F52-BBA4-B42114F57DBF@comcast.net>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
	<56859B36.4010202@auckland.ac.nz>
	<CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
	<2255B030-4A19-48E4-B3B5-CF67B216C37A@comcast.net>
	<CAHDp66D-f84foqjHZPcL8JdQ9vGEWiQxPmOBkRo1QJ93LqAj_Q@mail.gmail.com>
	<3CECCEE3-11F7-4F52-BBA4-B42114F57DBF@comcast.net>
Message-ID: <CAHDp66Crotp5txqDtjO8mOChnU+=ySO02DG-CyEWz2CUi2kxhA@mail.gmail.com>

Dear David:

Could you please check what I did to create a histogram for this data set.
Actually, I used barplot for myhist$counts. But I have two problems:

*1-* when I rebuild the x-axis, only the label of the first bar appear, but
not for the others.

*2-* I tried to add frequencies at top of bars, but I could not.


any helps will be highly appreciated



*First*, I replaced the detection limit values of 0.01 by a smaller value
of 0.009.


NH3Nconcentrations.hist<-c(0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,
0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.05,
0.05,0.05,0.06,0.47)



myhist <- hist(NH3Nconcentrations.hist,
breaks=c(0.005,0.0099,0.01,0.02,0.03,0.04,0.05,0.06,0.40,0.50),
col=c(1,rep(2,8)), xaxt="n",ylim=c(0,20)) ### ,



*Second*, I used barplot as follows:

colors = c("red","gray","gray","gray","gray","gray","gray","gray","blue")

barplot(myhist$counts, space=0 ,ylim=c(0,20), col=colors)




####---rebuild the x-axis , But not work as it should be

axis(1, at=c(myhist$mids[1], myhist$breaks[-(1:2)]), labels=c("<0.01",
myhist$breaks[-(1:2)]))




with many thanks
steve

On Sat, Jan 2, 2016 at 11:38 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jan 2, 2016, at 2:24 AM, Steven Stoline <sstoline at gmail.com> wrote:
> >
> > Dear David:
> >
> > Thank you very much for the code, it works very good for this data set.
> >
> > I just have one more thing (if not bothered you).
> >
> > how about if some of the non-censored (fully measured) data equal to the
> detection limit?
> >
> > As an example, in the data set below, there are 16 censored observations
> with detection limit of 0.01, and there are some non-censored data
> observation equal to 0.01 (equal to the detection limit). I am wondering if
> we still can distinguish  between them in the histogram. I tried to modify
> your code, but I could not make it work for this situation.
>
> I would probably construct an intermediate dataset copy where you
> "lowered" the items that were below the detection limit to a value ....
> below the detection limit, and then set the breaks parameter so that the
> real 0.01 items were included in the second bin.
>
> (That actually mimics what I usually do with the actual values in
> regression situations. I consider the measurements "below the detection
> limit" to still be meaningful.)
>
> --
> David.
> >
> > I crated a data frame, I want to create histogram for the variable
> "NH3Nconcentrations" (second column in the data frame).
> >
> >
> > Once again, thank you very much for your helps.
> >
> >
> >
> >
> >
> cen<-c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
> > 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
> >
> >
> censored<-c(TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,
> >
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,
> >
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,
> >
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE)
> >
> >
> data.original<-c("<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01",
> >
> "<0.01","<0.01","<0.01","<0.01","<0.01","<0.01",0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
> >
> 0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,
> > 0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.05,0.05,0.05,0.06,0.47)
> >
> >
> NH3Nconcentrations<-c(0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01
> >
> ,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,
> >
> 0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.05,
> > 0.05,0.05,0.06,0.47)
> >
> >
> NH3N.concentrations<-data.frame(data.original,NH3Nconcentrations,cen,censored)
> >
> > attach(NH3N.concentrations)
> >
> >
> > NH3N.concentrations
> >
> >
> >
> > with many thanks
> > steve
> >
> > On Fri, Jan 1, 2016 at 3:42 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >> On Jan 1, 2016, at 3:45 AM, Steven Stoline <sstoline at gmail.com> wrote:
> >>
> >> Dear Rolf:
> >>
> >>
> >> The histogram should contain a bar(s) for the censored data values
> replaced
> >> by their detection limit(s) with different color than other bars for the
> >> noncensored values . In this example there are only 3 censored values
> with
> >> only one detection limit of DL = 1450.
> >>
> >>
> >> with many thanks
> >> steve
> >>
> >>
> >>
> >> On Thu, Dec 31, 2015 at 4:16 PM, Rolf Turner <r.turner at auckland.ac.nz>
> >> wrote:
> >>
> >>> On 31/12/15 23:20, Steven Stoline wrote:
> >>>
> >>>> Dear All:
> >>>>
> >>>> I need helps with creating histograms for data that include left
> >>>> censored observations.
> >>>>
> >>>> Here is an example of left censored data
> >>>>
> >>>>
> >>>>
> >>>> *Sulfate.Concentration*
> >>>> <-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,
> >>>> 1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
> >>>> 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)
> >>>>
> >
> >  myhist <- hist(sulfate[,1],
> breaks=c(1400,1451,1500,1600,1700,1800,1900), col=c(1,rep(2,5)), xaxt="n")
> > #  plots with no x axis labeling
> >  myhist
> > #------------------
> > $breaks
> > [1] 1400 1451 1500 1600 1700 1800 1900
> >
> > $counts
> > [1]  3  1  1  0 14  5
> >
> > $density
> > [1] 0.0024509804 0.0008503401 0.0004166667 0.0000000000 0.0058333333
> 0.0020833333
> >
> > $mids
> > [1] 1425.5 1475.5 1550.0 1650.0 1750.0 1850.0
> >
> > $xname
> > [1] "sulfate[, 1]"
> >
> > $equidist
> > [1] FALSE
> >
> > attr(,"class")
> > [1] "histogram"
> > #---rebuild the x-axis ----------------
> >  axis(1, at=c(myhist$mids[1],myhist$breaks[-(1:2)]), labels=c("<1450",
> myhist$breaks[-(1:2)]))
> >
> > <Rplot001.png>
> >
> > --
> > David.
> >
> >>>>
> >>>> *Column 2* is an indicator for censoring "*1*" for left censored
> >>>> observations and "*0*" for non-censored (fully measured)
> >>>> observations.
> >>>>
> >>>
> >>> And what, pray tell, do you want the resulting histogram to look like?
> >>> See e.g. fortune("mind_read").
> >>>
> >>> cheers,
> >>>
> >>> Rolf Turner
> >>>
> >>> --
> >>> Technical Editor ANZJS
> >>> Department of Statistics
> >>> University of Auckland
> >>> Phone: +64-9-373-7599 ext. 88276
> >>>
> >>
> >>
> >>
> >> --
> >> Steven M. Stoline
> >> 1123 Forest Avenue
> >> Portland, ME 04112
> >> sstoline at gmail.com
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
> >
> >
> > --
> > Steven M. Stoline
> > 1123 Forest Avenue
> > Portland, ME 04112
> > sstoline at gmail.com
>
> David Winsemius
> Alameda, CA, USA
>
>


-- 
Steven M. Stoline
1123 Forest Avenue
Portland, ME 04112
sstoline at gmail.com

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Sun Jan  3 22:15:45 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 3 Jan 2016 22:15:45 +0100
Subject: [R] Thanks and further question
In-Reply-To: <DB4PR07MB379050E8BE75B687CF6AF9894F10@DB4PR07MB379.eurprd07.prod.outlook.com>
References: <DB4PR07MB379050E8BE75B687CF6AF9894F10@DB4PR07MB379.eurprd07.prod.outlook.com>
Message-ID: <36F3AFBD-5D83-41B2-9A81-AEA2CAF049A9@gmail.com>

Please keep on-list (cc'ed), for various good reasons. Comments inline.

-pd

> On 03 Jan 2016, at 22:02 , Muhammad Kashif <mkashif at uaf.edu.pk> wrote:
> 
> Dear Peter dalgarrd 
> 
> Thanks and i really appreciate your answer. Actually i am new in r programming. using your answer i run the following code which generate the results
> 
> gama=1.0
> beta=1.3
> x<-rgbs(n,gama,beta)
> ll.wd<-function(theta,x){
>   n<-length(x)
>   gama<-theta[1]
>   beta<-theta[2]
>   sum(-dgbs(x, gama, beta,log = TRUE))}
> out.wd<-optim(theta<-c(gama,beta),ll.wd,x=x,method = "Nelder-Mead",hessian=FALSE)
> gamhat<-out.wd$par[1]
> betahat<-out.wd$par[2]
> gamhat
> betahat
> 
> Can you help me to solve the issue. if i am correct 
> 
> this code minimized the loglikelihood function of gbs using Nelder-Mead method. 

the negative log likelihood, yes (i.e. maximizes the likelihood). Assuming that it converged, of course.

> 
> if yes then further if i wanted to simulate this (say 2000) time and for every simulation i wanted the value of estimated parameter (gamhat and betahat). then what i do 

My standard idiom for that sort of thing is

res <- replicate(2000, {
  x<-rgbs(n,gama,beta)
  optim(c(gama,beta), ll.wd, x=x, 
      method = "Nelder-Mead",hessian=FALSE)$par
})

which should give you a 2x2000 matrix with each column containing the parameter estimates for a simulation.


> 
> Please help me in this regard i am very thankful to you. 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mkashif at uaf.edu.pk  Sun Jan  3 22:19:56 2016
From: mkashif at uaf.edu.pk (Muhammad  Kashif)
Date: Sun, 3 Jan 2016 21:19:56 +0000
Subject: [R] Thanks and further question
In-Reply-To: <36F3AFBD-5D83-41B2-9A81-AEA2CAF049A9@gmail.com>
References: <DB4PR07MB379050E8BE75B687CF6AF9894F10@DB4PR07MB379.eurprd07.prod.outlook.com>,
	<36F3AFBD-5D83-41B2-9A81-AEA2CAF049A9@gmail.com>
Message-ID: <DB4PR07MB37995E6DE8341DDD4F535CB94F10@DB4PR07MB379.eurprd07.prod.outlook.com>

Thank you Professor and sorry for question on personal email. 

________________________________________
From: peter dalgaard <pdalgd at gmail.com>
Sent: Monday, January 4, 2016 2:15 AM
To: Muhammad  Kashif
Cc: Group R-help
Subject: Re: Thanks and further question

Please keep on-list (cc'ed), for various good reasons. Comments inline.

-pd

> On 03 Jan 2016, at 22:02 , Muhammad Kashif <mkashif at uaf.edu.pk> wrote:
>
> Dear Peter dalgarrd
>
> Thanks and i really appreciate your answer. Actually i am new in r programming. using your answer i run the following code which generate the results
>
> gama=1.0
> beta=1.3
> x<-rgbs(n,gama,beta)
> ll.wd<-function(theta,x){
>   n<-length(x)
>   gama<-theta[1]
>   beta<-theta[2]
>   sum(-dgbs(x, gama, beta,log = TRUE))}
> out.wd<-optim(theta<-c(gama,beta),ll.wd,x=x,method = "Nelder-Mead",hessian=FALSE)
> gamhat<-out.wd$par[1]
> betahat<-out.wd$par[2]
> gamhat
> betahat
>
> Can you help me to solve the issue. if i am correct
>
> this code minimized the loglikelihood function of gbs using Nelder-Mead method.

the negative log likelihood, yes (i.e. maximizes the likelihood). Assuming that it converged, of course.

>
> if yes then further if i wanted to simulate this (say 2000) time and for every simulation i wanted the value of estimated parameter (gamhat and betahat). then what i do

My standard idiom for that sort of thing is

res <- replicate(2000, {
  x<-rgbs(n,gama,beta)
  optim(c(gama,beta), ll.wd, x=x,
      method = "Nelder-Mead",hessian=FALSE)$par
})

which should give you a 2x2000 matrix with each column containing the parameter estimates for a simulation.


>
> Please help me in this regard i am very thankful to you.

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From liuwensui at gmail.com  Sun Jan  3 23:57:11 2016
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 3 Jan 2016 16:57:11 -0600
Subject: [R] How to calculate the prediction interval without knowing the
 functional form
Message-ID: <CAKyN3iC9H6LK6vtH9tPd8X0V7WfpCZQD4sQOm=K2oqE=Bg=9QQ@mail.gmail.com>

If I have predictions derived empirically without knowing the functional
form, is there a way to calculate the prediction interval?

Thanks


-- 
WenSui Liu
https://statcompute.wordpress.com/

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Jan  4 00:05:14 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 3 Jan 2016 15:05:14 -0800
Subject: [R] How to calculate the prediction interval without knowing
 the functional form
In-Reply-To: <CAKyN3iC9H6LK6vtH9tPd8X0V7WfpCZQD4sQOm=K2oqE=Bg=9QQ@mail.gmail.com>
References: <CAKyN3iC9H6LK6vtH9tPd8X0V7WfpCZQD4sQOm=K2oqE=Bg=9QQ@mail.gmail.com>
Message-ID: <CAGxFJbRXXCduQbWDEO-ECA02hG+zivvmSyWpSKk2WnwGSky4NA@mail.gmail.com>

Standard answer: bootstrap. However, "derived empirically" is too
vague to know whether the standard answer applies.

As this appears to be primarily a statistics, not an R issue, I
suggest that you post on a statistics list like
stats.stackexchange.com instead, perhaps with some more details on
what "derived empirically" means.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Jan 3, 2016 at 2:57 PM, Wensui Liu <liuwensui at gmail.com> wrote:
> If I have predictions derived empirically without knowing the functional
> form, is there a way to calculate the prediction interval?
>
> Thanks
>
>
> --
> WenSui Liu
> https://statcompute.wordpress.com/
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Mon Jan  4 00:21:10 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 3 Jan 2016 18:21:10 -0500
Subject: [R] How to calculate the prediction interval without knowing
 the functional form
In-Reply-To: <CAKyN3iC9H6LK6vtH9tPd8X0V7WfpCZQD4sQOm=K2oqE=Bg=9QQ@mail.gmail.com>
References: <CAKyN3iC9H6LK6vtH9tPd8X0V7WfpCZQD4sQOm=K2oqE=Bg=9QQ@mail.gmail.com>
Message-ID: <5689ACE6.5000002@gmail.com>

On 03/01/2016 5:57 PM, Wensui Liu wrote:
> If I have predictions derived empirically without knowing the functional
> form, is there a way to calculate the prediction interval?

As Bert said, this isn't really an R question, but I'd say the answer is 
"probably not".  The prediction interval depends on the uncertainty of 
individual observations, and that isn't usually reflected in 
predictions.  For example, if y_i is N(mu, sigma^2), and we fit the 
model to N observations, all of our predictions will be ybar, and there 
will be no indication of sigma.

Duncan Murdoch


From sstoline at gmail.com  Sun Jan  3 22:38:39 2016
From: sstoline at gmail.com (Steven Stoline)
Date: Sun, 3 Jan 2016 16:38:39 -0500
Subject: [R] [FORGED] Histogram for Left Censored Data
In-Reply-To: <3CECCEE3-11F7-4F52-BBA4-B42114F57DBF@comcast.net>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
	<56859B36.4010202@auckland.ac.nz>
	<CAHDp66BJwQZTN3EPR60FFHeatUXWaf8bDs-T7vMkag8VY_Qpqw@mail.gmail.com>
	<2255B030-4A19-48E4-B3B5-CF67B216C37A@comcast.net>
	<CAHDp66D-f84foqjHZPcL8JdQ9vGEWiQxPmOBkRo1QJ93LqAj_Q@mail.gmail.com>
	<3CECCEE3-11F7-4F52-BBA4-B42114F57DBF@comcast.net>
Message-ID: <CAHDp66BU2+JrNKZU4Sq2KW8S2qg_zhWD5L-+4obhp3J-7PoP4w@mail.gmail.com>

Dear All:

This is what I was able to come-up with. It looks work good. But not
sure!!!!



NH3Nconcentrations.hist<-c(0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,0.009,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,
0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.05,
0.05,0.05,0.06,0.47)



myhist <- hist(NH3Nconcentrations.hist,
breaks=c(0.005,0.0099,0.01,0.02,0.03,0.04,0.05,0.06,0.40,0.50),
col=c(1,rep(2,8)), xaxt="n",ylim=c(0,20)) ### ,


freq<-myhist$counts


colors = c("red","gray","gray","gray","gray","gray","gray","gray","blue")


xx<-barplot(myhist$counts, space=0 ,ylim=c(0,20), col=colors)


text(xx, myhist$counts, labels=myhist$counts, pos = 3, cex = 0.8)


####---rebuild the x-axis ----------------

labelsx<-c("<0.01", "0.01", "0.02", "0.03", "0.04", "0.05", "0.06", "0.40",
"0.50")

axis(1, at = xx, labels = labelsx, cex.axis = 0.75, srt = 45)




with thanks
steve

On Sat, Jan 2, 2016 at 11:38 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jan 2, 2016, at 2:24 AM, Steven Stoline <sstoline at gmail.com> wrote:
> >
> > Dear David:
> >
> > Thank you very much for the code, it works very good for this data set.
> >
> > I just have one more thing (if not bothered you).
> >
> > how about if some of the non-censored (fully measured) data equal to the
> detection limit?
> >
> > As an example, in the data set below, there are 16 censored observations
> with detection limit of 0.01, and there are some non-censored data
> observation equal to 0.01 (equal to the detection limit). I am wondering if
> we still can distinguish  between them in the histogram. I tried to modify
> your code, but I could not make it work for this situation.
>
> I would probably construct an intermediate dataset copy where you
> "lowered" the items that were below the detection limit to a value ....
> below the detection limit, and then set the breaks parameter so that the
> real 0.01 items were included in the second bin.
>
> (That actually mimics what I usually do with the actual values in
> regression situations. I consider the measurements "below the detection
> limit" to still be meaningful.)
>
> --
> David.
> >
> > I crated a data frame, I want to create histogram for the variable
> "NH3Nconcentrations" (second column in the data frame).
> >
> >
> > Once again, thank you very much for your helps.
> >
> >
> >
> >
> >
> cen<-c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
> > 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
> >
> >
> censored<-c(TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,
> >
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,
> >
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,
> >
> FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE)
> >
> >
> data.original<-c("<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01","<0.01",
> >
> "<0.01","<0.01","<0.01","<0.01","<0.01","<0.01",0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
> >
> 0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,
> > 0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.05,0.05,0.05,0.06,0.47)
> >
> >
> NH3Nconcentrations<-c(0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01
> >
> ,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,
> >
> 0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.05,
> > 0.05,0.05,0.06,0.47)
> >
> >
> NH3N.concentrations<-data.frame(data.original,NH3Nconcentrations,cen,censored)
> >
> > attach(NH3N.concentrations)
> >
> >
> > NH3N.concentrations
> >
> >
> >
> > with many thanks
> > steve
> >
> > On Fri, Jan 1, 2016 at 3:42 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >> On Jan 1, 2016, at 3:45 AM, Steven Stoline <sstoline at gmail.com> wrote:
> >>
> >> Dear Rolf:
> >>
> >>
> >> The histogram should contain a bar(s) for the censored data values
> replaced
> >> by their detection limit(s) with different color than other bars for the
> >> noncensored values . In this example there are only 3 censored values
> with
> >> only one detection limit of DL = 1450.
> >>
> >>
> >> with many thanks
> >> steve
> >>
> >>
> >>
> >> On Thu, Dec 31, 2015 at 4:16 PM, Rolf Turner <r.turner at auckland.ac.nz>
> >> wrote:
> >>
> >>> On 31/12/15 23:20, Steven Stoline wrote:
> >>>
> >>>> Dear All:
> >>>>
> >>>> I need helps with creating histograms for data that include left
> >>>> censored observations.
> >>>>
> >>>> Here is an example of left censored data
> >>>>
> >>>>
> >>>>
> >>>> *Sulfate.Concentration*
> >>>> <-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,
> >>>> 1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
> >>>> 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)
> >>>>
> >
> >  myhist <- hist(sulfate[,1],
> breaks=c(1400,1451,1500,1600,1700,1800,1900), col=c(1,rep(2,5)), xaxt="n")
> > #  plots with no x axis labeling
> >  myhist
> > #------------------
> > $breaks
> > [1] 1400 1451 1500 1600 1700 1800 1900
> >
> > $counts
> > [1]  3  1  1  0 14  5
> >
> > $density
> > [1] 0.0024509804 0.0008503401 0.0004166667 0.0000000000 0.0058333333
> 0.0020833333
> >
> > $mids
> > [1] 1425.5 1475.5 1550.0 1650.0 1750.0 1850.0
> >
> > $xname
> > [1] "sulfate[, 1]"
> >
> > $equidist
> > [1] FALSE
> >
> > attr(,"class")
> > [1] "histogram"
> > #---rebuild the x-axis ----------------
> >  axis(1, at=c(myhist$mids[1],myhist$breaks[-(1:2)]), labels=c("<1450",
> myhist$breaks[-(1:2)]))
> >
> > <Rplot001.png>
> >
> > --
> > David.
> >
> >>>>
> >>>> *Column 2* is an indicator for censoring "*1*" for left censored
> >>>> observations and "*0*" for non-censored (fully measured)
> >>>> observations.
> >>>>
> >>>
> >>> And what, pray tell, do you want the resulting histogram to look like?
> >>> See e.g. fortune("mind_read").
> >>>
> >>> cheers,
> >>>
> >>> Rolf Turner
> >>>
> >>> --
> >>> Technical Editor ANZJS
> >>> Department of Statistics
> >>> University of Auckland
> >>> Phone: +64-9-373-7599 ext. 88276
> >>>
> >>
> >>
> >>
> >> --
> >> Steven M. Stoline
> >> 1123 Forest Avenue
> >> Portland, ME 04112
> >> sstoline at gmail.com
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
> >
> >
> > --
> > Steven M. Stoline
> > 1123 Forest Avenue
> > Portland, ME 04112
> > sstoline at gmail.com
>
> David Winsemius
> Alameda, CA, USA
>
>


-- 
Steven M. Stoline
1123 Forest Avenue
Portland, ME 04112
sstoline at gmail.com

	[[alternative HTML version deleted]]


From kunalshah305 at gmail.com  Mon Jan  4 11:47:19 2016
From: kunalshah305 at gmail.com (Kunal Shah)
Date: Mon, 4 Jan 2016 16:17:19 +0530
Subject: [R] Better scrolling feature in ggplot using Shiny???
Message-ID: <CAOPdpkbC46gER9phfNzXfeY-EctfeTWvb19xz+1=XTmC7OpoQA@mail.gmail.com>

Hello,

I have plotted a ggplot of large data around 30000 points. I opened it in
Shiny. I want a scrolling feature so that I can just scroll the data.

I tried to write a code in Shiny where the user can select the slider
range. But "scrolling" by that is not efficient and not at all smooth

Any help is appreciated


Regards

	[[alternative HTML version deleted]]


From stefano.sofia at regione.marche.it  Mon Jan  4 15:25:14 2016
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Mon, 4 Jan 2016 14:25:14 +0000
Subject: [R] Estimating MA parameters through arima or through package "dlm"
Message-ID: <8B435C9568170B469AE31E8891E8CC4F3DB7529C@ESINO.regionemarche.intra>

Dear list users,
I want to use apply a MA(2) process (x=beta1*epsilon_(t-1) + beta2*epsilon_(t-1) + epsilon_(t)) to a given time series (x), and I want to estimate the two parameters beta1, beta2 and the variance of the random variable epsilon_(t).

If I use
MA2_1 <- Arima(x, order=c(0,0,2))
I get the following result

[1] "MA2_1"
Series: x
ARIMA(0,0,2) with non-zero mean

Coefficients:
          ma1     ma2  intercept
      -0.0279  0.0783     5.3737
s.e.   0.0667  0.0622     0.0245

sigma^2 estimated as 0.1284:  log likelihood=-92.63
AIC=193.25   AICc=193.43   BIC=207.11
[1] 0 2 0 0 1 0 0

From this straightforward analysis V[epsilon]=0.1284, beta1=-0.0279 and beta2=0.0783.

I also tried to use a DLM representation of ARIMA models and estimate the unknown parameters by maximum likelihood through the dlm package (in particular applying the example at section 3.2.6, page 115, of "Dynamic Linear Models with R" by Petris, Petrone and Campagnoli:

arma_parameters <- function(x)
{
  buildGap <- function(u)
  {
    gap <- dlmModARMA(ma = u[2 : 3], sigma2 = u[1])
    return(gap)
   }
   init <- c(0.005, 0.004, 0.003)
   outMLE <- dlmMLE(x, init, buildGap)
   dlmGap <- buildGap(outMLE$par)
}

and this gives:
[1] "outMLE"
$par
[1] 1.00816794 0.02349296 0.02364788

$value
[1] 3089.196

$counts
function gradient
      10       10

$convergence
[1] 0

$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

[1] "dlmGap"
$FF
     [,1] [,2] [,3]
[1,]    1    0    0

$V
     [,1]
[1,]    0

$GG
     [,1] [,2] [,3]
[1,]    0    1    0
[2,]    0    0    1
[3,]    0    0    0

$W
           [,1]         [,2]         [,3]
[1,] 1.00816794 0.0236848488 0.0238410337
[2,] 0.02368485 0.0005564272 0.0005600964
[3,] 0.02384103 0.0005600964 0.0005637899

$m0
[1] 0 0 0

$C0
      [,1]  [,2]  [,3]
[1,] 1e+07 0e+00 0e+00
[2,] 0e+00 1e+07 0e+00
[3,] 0e+00 0e+00 1e+07

In this case
V[epsilon]=W[1,1]=1.00816794
beta1=W[2,1]/W[1,1]=0.02349296
beta2=W[3,1]/W[1,1]=0.02364788

I presume that these two approaches should give comparable results, but this does not happen.
Is the model that I used correct? And does it make sense to perform this kind of comparison?

This is the log of a rainfall time series (which has already been deseasonalised):
[1] 6.014937 4.978801 5.654592 5.616771 5.612398 5.837147 5.121580 5.832176
[9] 5.205654 5.355642 5.405376 6.257859 5.516247 5.500850 4.708629 5.482304
[17] 5.689684 5.727824 4.779123 5.289277 5.217107 5.976351 4.630838 5.683240
[25] 5.345678 5.906179 5.605434 5.497578 5.898801 5.660875 5.111988 5.571013
[33] 5.949340 5.374352 4.841033 5.995706 5.661223 5.458734 4.454347 5.795754
[41] 5.995706 5.596939 5.399971 5.908898 5.282696 5.438514 5.528635 6.022721
[49] 5.524257 5.519459 4.957235 5.547518 5.080783 5.411200 5.056883 5.798183
[57] 5.086361 5.536547 5.220356 5.141664 5.847017 5.052417 5.734635 5.340419
[65] 5.724238 5.634432 5.685958 5.307773 5.817706 5.134032 4.987708 5.110179
[73] 5.423628 5.347108 4.859037 5.556828 5.487283 5.661223 5.732370 5.469325
[81] 5.726848 5.419207 5.172187 5.608006 5.130490 5.586874 5.171052 5.683240
[89] 4.674696 5.286245 5.342813 5.370638 5.432411 5.748118 6.355239 5.557986
[97] 5.399067 5.222516 5.279644 5.425390 5.540871 5.917818 5.132853 5.689007
[105] 5.900993 5.007296 5.102911 5.778271 5.318120 5.927726 5.066385 5.716699
[113] 5.511815 4.714921 5.383577 5.319100 5.269403 5.354698 5.145749 5.204556
[121] 5.878296 5.070161 5.441552 5.213304 5.450180 5.695750 4.893352 5.425390
[129] 5.682559 5.487283 4.213608 5.751620 5.432411 5.379436 5.700444 5.580484
[137] 5.357529 5.319100 4.532599 5.603225 5.208393 5.254888 5.017280 5.349961
[145] 4.374498 5.187944 5.585374 5.716370 3.561046 5.119789 5.163070 5.422745
[153] 5.863915 5.651436 4.762174 5.655642 4.797442 5.735927 4.911183 5.240688
[161] 5.148076 5.477300 4.572647 5.493473 5.437644 4.854371 4.908233 4.755313
[169] 5.582744 5.527841 5.613128 5.211124 5.275049 5.462984 5.016617 5.981919
[177] 5.566817 5.094364 5.314191 5.712742 5.299317 5.452325 4.691348 5.851628
[185] 5.410753 5.488938 5.660179 5.900993 5.380819 5.256453 4.781641 5.531807
[193] 5.497578 5.274537 4.325456 5.271973 5.077047 5.258536 5.280662 5.247024
[201] 5.995208 4.700480 4.991113 5.457029 5.194622 5.487283 5.197391 5.747161
[209] 5.842094 5.372497 5.306781 5.641907 5.565286 5.259057 5.241218 4.759607
[217] 4.550714 5.230574 4.470495 5.664348 4.846547 5.771130 4.823502 5.598422
[225] 5.627621 5.547518 5.596939 5.468482 5.536940 5.606170 5.281680 5.656691
[233] 5.283204 5.752255 5.192401 4.550714


Thank you for your attention and your help
Stefano


________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

	[[alternative HTML version deleted]]


From markleeds2 at gmail.com  Mon Jan  4 17:31:22 2016
From: markleeds2 at gmail.com (Mark Leeds)
Date: Mon, 4 Jan 2016 11:31:22 -0500
Subject: [R] Estimating MA parameters through arima or through package
	"dlm"
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F3DB7529C@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F3DB7529C@ESINO.regionemarche.intra>
Message-ID: <CAHz+bWYXanjp4ksu2SzMbFRLaKH8_ASwN07s_3eKYjeyWuAG1A@mail.gmail.com>

Hi: I don't have time to look at the details of what you're doing but the
"equivalence"
between state space and arima ( as paul gilbert pointed out a few weeks ago
) is not a true equivalence.

 if you are in an area of the parameter space that the state space
formulation
 can't reach, then you won't get the same parameter estimates. so, what
you're doing
might be okay or might not be, depending on whether the state space
formulation
can reach that area of the parameter space. there's another state space
formulation that is truly equivalent which is called the SSOE formulation
or innovations representation but
I don't know if you want to get into that. google "SSOE state space" if
you're interested.


Mark






On Mon, Jan 4, 2016 at 9:25 AM, Stefano Sofia <
stefano.sofia at regione.marche.it> wrote:

> Dear list users,
> I want to use apply a MA(2) process (x=beta1*epsilon_(t-1) +
> beta2*epsilon_(t-1) + epsilon_(t)) to a given time series (x), and I want
> to estimate the two parameters beta1, beta2 and the variance of the random
> variable epsilon_(t).
>
> If I use
> MA2_1 <- Arima(x, order=c(0,0,2))
> I get the following result
>
> [1] "MA2_1"
> Series: x
> ARIMA(0,0,2) with non-zero mean
>
> Coefficients:
>           ma1     ma2  intercept
>       -0.0279  0.0783     5.3737
> s.e.   0.0667  0.0622     0.0245
>
> sigma^2 estimated as 0.1284:  log likelihood=-92.63
> AIC=193.25   AICc=193.43   BIC=207.11
> [1] 0 2 0 0 1 0 0
>
> From this straightforward analysis V[epsilon]=0.1284, beta1=-0.0279 and
> beta2=0.0783.
>
> I also tried to use a DLM representation of ARIMA models and estimate the
> unknown parameters by maximum likelihood through the dlm package (in
> particular applying the example at section 3.2.6, page 115, of "Dynamic
> Linear Models with R" by Petris, Petrone and Campagnoli:
>
> arma_parameters <- function(x)
> {
>   buildGap <- function(u)
>   {
>     gap <- dlmModARMA(ma = u[2 : 3], sigma2 = u[1])
>     return(gap)
>    }
>    init <- c(0.005, 0.004, 0.003)
>    outMLE <- dlmMLE(x, init, buildGap)
>    dlmGap <- buildGap(outMLE$par)
> }
>
> and this gives:
> [1] "outMLE"
> $par
> [1] 1.00816794 0.02349296 0.02364788
>
> $value
> [1] 3089.196
>
> $counts
> function gradient
>       10       10
>
> $convergence
> [1] 0
>
> $message
> [1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
>
> [1] "dlmGap"
> $FF
>      [,1] [,2] [,3]
> [1,]    1    0    0
>
> $V
>      [,1]
> [1,]    0
>
> $GG
>      [,1] [,2] [,3]
> [1,]    0    1    0
> [2,]    0    0    1
> [3,]    0    0    0
>
> $W
>            [,1]         [,2]         [,3]
> [1,] 1.00816794 0.0236848488 0.0238410337
> [2,] 0.02368485 0.0005564272 0.0005600964
> [3,] 0.02384103 0.0005600964 0.0005637899
>
> $m0
> [1] 0 0 0
>
> $C0
>       [,1]  [,2]  [,3]
> [1,] 1e+07 0e+00 0e+00
> [2,] 0e+00 1e+07 0e+00
> [3,] 0e+00 0e+00 1e+07
>
> In this case
> V[epsilon]=W[1,1]=1.00816794
> beta1=W[2,1]/W[1,1]=0.02349296
> beta2=W[3,1]/W[1,1]=0.02364788
>
> I presume that these two approaches should give comparable results, but
> this does not happen.
> Is the model that I used correct? And does it make sense to perform this
> kind of comparison?
>
> This is the log of a rainfall time series (which has already been
> deseasonalised):
> [1] 6.014937 4.978801 5.654592 5.616771 5.612398 5.837147 5.121580 5.832176
> [9] 5.205654 5.355642 5.405376 6.257859 5.516247 5.500850 4.708629 5.482304
> [17] 5.689684 5.727824 4.779123 5.289277 5.217107 5.976351 4.630838
> 5.683240
> [25] 5.345678 5.906179 5.605434 5.497578 5.898801 5.660875 5.111988
> 5.571013
> [33] 5.949340 5.374352 4.841033 5.995706 5.661223 5.458734 4.454347
> 5.795754
> [41] 5.995706 5.596939 5.399971 5.908898 5.282696 5.438514 5.528635
> 6.022721
> [49] 5.524257 5.519459 4.957235 5.547518 5.080783 5.411200 5.056883
> 5.798183
> [57] 5.086361 5.536547 5.220356 5.141664 5.847017 5.052417 5.734635
> 5.340419
> [65] 5.724238 5.634432 5.685958 5.307773 5.817706 5.134032 4.987708
> 5.110179
> [73] 5.423628 5.347108 4.859037 5.556828 5.487283 5.661223 5.732370
> 5.469325
> [81] 5.726848 5.419207 5.172187 5.608006 5.130490 5.586874 5.171052
> 5.683240
> [89] 4.674696 5.286245 5.342813 5.370638 5.432411 5.748118 6.355239
> 5.557986
> [97] 5.399067 5.222516 5.279644 5.425390 5.540871 5.917818 5.132853
> 5.689007
> [105] 5.900993 5.007296 5.102911 5.778271 5.318120 5.927726 5.066385
> 5.716699
> [113] 5.511815 4.714921 5.383577 5.319100 5.269403 5.354698 5.145749
> 5.204556
> [121] 5.878296 5.070161 5.441552 5.213304 5.450180 5.695750 4.893352
> 5.425390
> [129] 5.682559 5.487283 4.213608 5.751620 5.432411 5.379436 5.700444
> 5.580484
> [137] 5.357529 5.319100 4.532599 5.603225 5.208393 5.254888 5.017280
> 5.349961
> [145] 4.374498 5.187944 5.585374 5.716370 3.561046 5.119789 5.163070
> 5.422745
> [153] 5.863915 5.651436 4.762174 5.655642 4.797442 5.735927 4.911183
> 5.240688
> [161] 5.148076 5.477300 4.572647 5.493473 5.437644 4.854371 4.908233
> 4.755313
> [169] 5.582744 5.527841 5.613128 5.211124 5.275049 5.462984 5.016617
> 5.981919
> [177] 5.566817 5.094364 5.314191 5.712742 5.299317 5.452325 4.691348
> 5.851628
> [185] 5.410753 5.488938 5.660179 5.900993 5.380819 5.256453 4.781641
> 5.531807
> [193] 5.497578 5.274537 4.325456 5.271973 5.077047 5.258536 5.280662
> 5.247024
> [201] 5.995208 4.700480 4.991113 5.457029 5.194622 5.487283 5.197391
> 5.747161
> [209] 5.842094 5.372497 5.306781 5.641907 5.565286 5.259057 5.241218
> 4.759607
> [217] 4.550714 5.230574 4.470495 5.664348 4.846547 5.771130 4.823502
> 5.598422
> [225] 5.627621 5.547518 5.596939 5.468482 5.536940 5.606170 5.281680
> 5.656691
> [233] 5.283204 5.752255 5.192401 4.550714
>
>
> Thank you for your attention and your help
> Stefano
>
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone autorizzate
> alla ricezione. I messaggi di posta elettronica per i client di Regione
> Marche possono contenere informazioni confidenziali e con privilegi legali.
> Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o
> archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore,
> inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio
> computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in
> caso di necessit? ed urgenza, la risposta al presente messaggio di posta
> elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by
> persons entitled to receive the confidential information it may contain.
> E-mail messages to clients of Regione Marche may contain information that
> is confidential and legally privileged. Please do not read, copy, forward,
> or store this message unless you are an intended recipient of it. If you
> have received this message in error, please forward it to the sender and
> delete it completely from your computer system.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Jan  4 17:41:52 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 04 Jan 2016 08:41:52 -0800
Subject: [R] Better scrolling feature in ggplot using Shiny???
In-Reply-To: <CAOPdpkbC46gER9phfNzXfeY-EctfeTWvb19xz+1=XTmC7OpoQA@mail.gmail.com>
References: <CAOPdpkbC46gER9phfNzXfeY-EctfeTWvb19xz+1=XTmC7OpoQA@mail.gmail.com>
Message-ID: <B77B3271-6480-4AD2-BFC5-07721FA1C6FA@dcn.davis.ca.us>

Server-side rendering of large amounts of data is often criticized this way.  In general, the answer lies in client-side rendering, which these days usually means serving a Web page with embedded data and Javascript (e.g. D3), not ggplot images. The drawback seems to be a significant amount of extra effort and learning JS (off-topic here), or paying someone else to do that grunt-work.

The CRAN Task View on graphics is a little dated in this respect, but may have some options if Web pages are not required.  The "plotly" package works with the plotly Web service, but your tool then becomes tied with that service and its licensing requirements, though they do make it easier to get an interactive plot. The "googleVis" package offers some similar features,  with similar baggage. 

Please (re-)read the Posting Guide mentioned at the bottom of every r-help mailing list, which for one thing mentions that this is a plain text mailing list.  Posting in HTML is bound to lead to corrupted communication (us not being able to decipher your post) sooner or later,  and only you can prevent that by adjusting your email client when you send to this list.

[1] https://cran.r-project.org/web/views/Graphics.html

-- 
Sent from my phone. Please excuse my brevity.

On January 4, 2016 2:47:19 AM PST, Kunal Shah <kunalshah305 at gmail.com> wrote:
>Hello,
>
>I have plotted a ggplot of large data around 30000 points. I opened it
>in
>Shiny. I want a scrolling feature so that I can just scroll the data.
>
>I tried to write a code in Shiny where the user can select the slider
>range. But "scrolling" by that is not efficient and not at all smooth
>
>Any help is appreciated
>
>
>Regards
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From swatijaiswal.bhu at gmail.com  Mon Jan  4 08:19:26 2016
From: swatijaiswal.bhu at gmail.com (swati j)
Date: Mon, 4 Jan 2016 12:49:26 +0530
Subject: [R] problem in installing package xpose4 in Rstudio
Message-ID: <CAEQ8nwQSUYgv6k8NQvCYMrZ=JMr44jQooLL6Dvs31TYqHe7WSA@mail.gmail.com>

With R, package xpose4 is working well, but when I open Rstudio and try to
install package xpose4

following error message is displayed

> install.packages("C:/Users/om/Downloads/xpose4_4.5.3.tar.gz", repos =
> NULL, type = "source")
Installing package(s) into ?C:/Users/om/Documents/R/win-library/2.15?
(as ?lib? is unspecified)
ERROR: dependencies 'gam', 'Hmisc' are not available for package 'xpose4'
* removing 'C:/Users/om/Documents/R/win-library/2.15/xpose4'
Warning in install.packages :
  running command 'C:/PROGRA~1/R/R-215~1.1/bin/i386/R CMD INSTALL -l
"C:/Users/om/Documents/R/win-library/2.15"
"C:/Users/om/Downloads/xpose4_4.5.3.tar.gz"' had status 1
Warning in install.packages :
  installation of package ?C:/Users/om/Downloads/xpose4_4.5.3.tar.gz? had
non-zero exit status

please help me to sort out this problem.....

Swati Jaiswal

PhD Scholar (CSIR-Senior Research Fellow)

Pharmacokinetics & Metabolism Division

CSIR-Central Drug Research Institute

*Lucknow-226031, India*

*Mobile +91 9473837970*

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Jan  4 18:01:41 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 4 Jan 2016 09:01:41 -0800
Subject: [R] problem in installing package xpose4 in Rstudio
In-Reply-To: <CAEQ8nwQSUYgv6k8NQvCYMrZ=JMr44jQooLL6Dvs31TYqHe7WSA@mail.gmail.com>
References: <CAEQ8nwQSUYgv6k8NQvCYMrZ=JMr44jQooLL6Dvs31TYqHe7WSA@mail.gmail.com>
Message-ID: <7251DFCF-1B58-401B-8FF1-47BDB56EECC7@comcast.net>


> On Jan 3, 2016, at 11:19 PM, swati j <swatijaiswal.bhu at gmail.com> wrote:
> 
> With R, package xpose4 is working well, but when I open Rstudio and try to
> install package xpose4
> 
> following error message is displayed
> 
>> install.packages("C:/Users/om/Downloads/xpose4_4.5.3.tar.gz", repos =
>> NULL, type = "source")
> Installing package(s) into ?C:/Users/om/Documents/R/win-library/2.15?
> (as ?lib? is unspecified)
> ERROR: dependencies 'gam', 'Hmisc' are not available for package 'xpose4'
^ ^ ^ ^              ^^^^^   ^^^^^
| | | |      ^       |||||   ||||| 

Please READ error messages. Don't just freak out when you see the work "error". The rest of the message has meaning.

-- 
David.

> * removing 'C:/Users/om/Documents/R/win-library/2.15/xpose4'



> Warning in install.packages :
>  running command 'C:/PROGRA~1/R/R-215~1.1/bin/i386/R CMD INSTALL -l
> "C:/Users/om/Documents/R/win-library/2.15"
> "C:/Users/om/Downloads/xpose4_4.5.3.tar.gz"' had status 1
> Warning in install.packages :
>  installation of package ?C:/Users/om/Downloads/xpose4_4.5.3.tar.gz? had
> non-zero exit status
> 
> please help me to sort out this problem.....
> 
> Swati Jaiswal
> 
> PhD Scholar (CSIR-Senior Research Fellow)
> 
> Pharmacokinetics & Metabolism Division
> 
> CSIR-Central Drug Research Institute
> 
> *Lucknow-226031, India*
> 
> *Mobile +91 9473837970*
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From auerbecktj at gmail.com  Mon Jan  4 20:02:01 2016
From: auerbecktj at gmail.com (Tyler Auerbeck)
Date: Mon, 4 Jan 2016 14:02:01 -0500
Subject: [R] R package built using newer version of R
Message-ID: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>

We're currently looking at using the R eclipse plugin StatET as our
development environment. Due to certain requirements, we're still using
2.15.1. However a required package of StatET was built using 2.15.3, which
results in the following warning:

Warning message:
package 'rj' was built under R version 2.15.3

I'm still fairly new to R, but is there any way for us to rebuild this
package using 2.15.1? It doesn't appear to cause us any issues, but it's
still not desirable for users to see that warning.

Any help would be appreciated.

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Jan  4 20:15:52 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 4 Jan 2016 14:15:52 -0500
Subject: [R] R package built using newer version of R
In-Reply-To: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>
References: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>
Message-ID: <568AC4E8.2000302@gmail.com>

On 04/01/2016 2:02 PM, Tyler Auerbeck wrote:
> We're currently looking at using the R eclipse plugin StatET as our
> development environment. Due to certain requirements, we're still using
> 2.15.1. However a required package of StatET was built using 2.15.3, which
> results in the following warning:
>
> Warning message:
> package 'rj' was built under R version 2.15.3
>
> I'm still fairly new to R, but is there any way for us to rebuild this
> package using 2.15.1? It doesn't appear to cause us any issues, but it's
> still not desirable for users to see that warning.
>
> Any help would be appreciated.

Yes, it's quite easy to do so.  StatET probably gives menu options to do 
it, but I don't know them:  you might want to ask them.  From the R 
console, try

install.packages("pkgname", type="source")

and if you have the necessary prerequisites (e.g. compilers), you'll get 
it installed from source.   If it fails, post the errors and the results 
of sessionInfo() here, and we'll probably be able to tell you what to do 
next.

Duncan Murdoch


From amos.elberg at gmail.com  Mon Jan  4 20:26:21 2016
From: amos.elberg at gmail.com (Amos B. Elberg)
Date: Mon, 4 Jan 2016 14:26:21 -0500
Subject: [R] rZeppelin: Easy Spark for R Data Scientists
Message-ID: <etPan.568ac75d.37852978.1c0@amosmacpro.fios-router.home>

rZeppelin is an R Interpreter for the Apache (Incubating) Zeppelin project. ?

The intention of rZeppelin is to make it possible for regular R-using non-programmer to integrate the power of Spark, and the wide range of ML packages available for Python and scala, into their day-to-day toolbox ? without having to learn a new language, without any learning curve beyond a review of the SparkR API, and without the budget needs or administrative overhead of setting up a Spark or hadoop infrastructure. ?

Zeppelin is a notebook (like iPython) built on top of Spark. ?Zeppelin provides interactive data visualization and other features, and interpreters for a wide variety of ?big data? stores.?

rZeppelin makes it possible to combine R, scala, and Python code in a single data/ML pipeline, seamlessly, from a single, familiar, interface. ?(And without breaking lazy evaluation!)

This means that you can use the Spark package-base of ultra-fast implementations of popular ML algorithms optimized for clusters, as well as python packages, as an extension of your existing work with R. ?

For example, imagine loading text data in R, running LDA on the text using the distributed implementation of LDA in Spark?s MLLIB, tagging the text using advanced Python NLP packages such as gensim, and then visualizing and further processing the results in R ? all from the same interface, in the same session.?

rZeppelin lets you do this because the R interpreter, along with Zeppelin?s scala and Python interpreters, share the same Spark backend. ?

Apart from Spark, most common datatypes can be moved among R, scala, and Python through the ?ZeppelinContext,? a shared environment. ?

rZeppelin is integrated with Zeppelin?s interactive visualization features. ?It also uses knitr for compatibility with most R data visualization and interactive visualization packages, such as ggplot2 and rCharts. ?

rZeppelin is available here: ?https://github.com/elbamos/Zeppelin-With-R??
	[[alternative HTML version deleted]]


From Haiko.Lietz at gesis.org  Tue Jan  5 08:17:37 2016
From: Haiko.Lietz at gesis.org (Lietz, Haiko)
Date: Tue, 5 Jan 2016 07:17:37 +0000
Subject: [R] dput sparseMatrix list
Message-ID: <D57FB3A7BE5E14479A21F7832D5F1B34BB3FE22A@svboexc02.gesis.intra>

hi all,

when dputting a list of sparse matrices (Matrix package), the output does not contain the data but the information that the list contains sparse matrices.

M <- sparseMatrix(i = c(2, 1), j = c(1, 2), x = c(1, 1))

dput(M) ... works.

dput(list(M, M)) ... does not work.

how can I dput a list of sparse matrices?

thx

haiko


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Jan  5 08:30:47 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 4 Jan 2016 23:30:47 -0800
Subject: [R] dput sparseMatrix list
In-Reply-To: <D57FB3A7BE5E14479A21F7832D5F1B34BB3FE22A@svboexc02.gesis.intra>
References: <D57FB3A7BE5E14479A21F7832D5F1B34BB3FE22A@svboexc02.gesis.intra>
Message-ID: <62E24ED7-96C6-421A-B4BE-7A3E26A0DDBF@comcast.net>


> On Jan 4, 2016, at 11:17 PM, Lietz, Haiko <Haiko.Lietz at gesis.org> wrote:
> 
> hi all,
> 
> when dputting a list of sparse matrices (Matrix package), the output does not contain the data but the information that the list contains sparse matrices.
> 
> M <- sparseMatrix(i = c(2, 1), j = c(1, 2), x = c(1, 1))
> 
> dput(M) ... works.
> 
> dput(list(M, M)) ... does not work.
> 
> how can I dput a list of sparse matrices?

> MM <- list(M,M)
> dput(MM)
list(<S4 object of class structure("dgCMatrix", package = "Matrix")>, 
    <S4 object of class structure("dgCMatrix", package = "Matrix")>)

No problem.

I do get an error (clarifying the "did not work" statement), as (perhaps) did you?

> dput(M,M)
Error in cat("new(\"", clx, "\"\n", file = file, sep = "") : 
  invalid connection

Perhaps the `dput` function was not configured to handle two S4 objects in  a list?

> [[alternative HTML version deleted]]

Please do better next time.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From maechler at stat.math.ethz.ch  Tue Jan  5 10:14:49 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 5 Jan 2016 10:14:49 +0100
Subject: [R] dput( list(<S4>,..) )  could be improved
In-Reply-To: <62E24ED7-96C6-421A-B4BE-7A3E26A0DDBF@comcast.net>
References: <D57FB3A7BE5E14479A21F7832D5F1B34BB3FE22A@svboexc02.gesis.intra>
	<62E24ED7-96C6-421A-B4BE-7A3E26A0DDBF@comcast.net>
Message-ID: <22155.35209.318794.910799@stat.math.ethz.ch>


> > On Jan 4, 2016, at 11:17 PM, Lietz, Haiko <Haiko.Lietz at gesis.org> wrote:
> > 
> > hi all,
> > 
> > when dputting a list of sparse matrices (Matrix package), the output does not contain the data but the information that the list contains sparse matrices.
> > 
> > M <- sparseMatrix(i = c(2, 1), j = c(1, 2), x = c(1, 1))
> > 
> > dput(M) ... works.
> > 
> > dput(list(M, M)) ... does not work.
> > 
> > how can I dput a list of sparse matrices?

> > MM <- list(M,M)
> > dput(MM)
> list(<S4 object of class structure("dgCMatrix", package = "Matrix")>, 
>     <S4 object of class structure("dgCMatrix", package = "Matrix")>)

> No problem.

> I do get an error (clarifying the "did not work" statement), as (perhaps) did you?

> > dput(M,M)
> Error in cat("new(\"", clx, "\"\n", file = file, sep = "") : 
>   invalid connection

> Perhaps the `dput` function was not configured to handle two S4 objects in  a list?

Indeed.  As a matter of fact,  a long time I ago I had added a 'FIXME'
comment to the R source code of dput:

see  https://svn.r-project.org/R/trunk/src/library/base/R/dput.R

which contains

    ## FIXME: this should happen in C {deparse2() in ../../../main/deparse.c}
    ##        but we are missing a C-level slotNames()
    ## Fails e.g. if an S3 list-like object has S4 components
    if(isS4(x)) {
	clx <- class(x)
	cat('new("', clx,'"\n', file = file, sep = "")
	for(n in methods::.slotNames(clx)) {
	    cat("    ,", n, "= ", file = file)
	    dput(methods::slot(x, n), file = file, control = control)
	}
	cat(")\n", file = file)
	invisible()
    }
    else .Internal(dput(x, file, opts))

Ideally the above code would be replaced by simply

    .Internal(dput(x, file, opts))

and the C code called in do_dput()  in
  https://svn.r-project.org/R/trunk/src/main/deparse.c
would be improved to do the above in C and be properly
recursive.

(Tested) patches are very welcome!

Martin Maechler,
ETH Zurich and R Core Team


From pmilin at gmail.com  Tue Jan  5 13:23:20 2016
From: pmilin at gmail.com (Petar Milin)
Date: Tue, 5 Jan 2016 13:23:20 +0100
Subject: [R] SNG uz malo regresije
In-Reply-To: <568BA288.3040204@gmail.com>
References: <568BA288.3040204@gmail.com>
Message-ID: <F0954624-5FB2-4A1B-ADA9-85952A765BE2@gmail.com>


> On Jan 5, 2016, at 12:01 PM, Bojana Dinic <bojana.dinic at gmail.com> wrote:
> 
> najpre, srecna Nova godina i sve najbolje tbi i tvojoj porodici. Nadam se da ces se za praznike lepo odmoriti.
> Mene su vec zadesile neke obaveze, pa sam htela da te pitam da li znas neki paket, makro, algoritam ili nesto slicno za hijerarhijsku ridge regresiju, ili za hijerarhijski random forest, gde moze da se ubaci vise seta prediktora? 

Zdravo Bojana!
Najlepse ti hvala! I ja tebi zelim sve naj naj naj! :-)

Sto se tice tvog pitanja, ne umem da ti odgovorim ? ne znam mnogo o ridge regresiji, tj. ne koristim je cesto i nisam imao potrebe za necim sofisticiranim. Takodje, ne znam ni za hijerarhijski RF. Koristim obicni i mislim da i ne postoji mogucnost da se uvedu slucajni efekti. Koristim randomForest iz istoimenog paketa i cforest iz party paketa.

Mozda bi mogla da postavis pitanje na R-HELP listi <r-help at stat.math.ethz.ch <mailto:r-help at stat.math.ethz.ch>> ili na http://stackoverflow.com/ <http://stackoverflow.com/> ? Na oba, naravno, moras da se prijavis.


PS: Nikako da ti pisem i da te pitam zbog cega je i kako ispao problem s Platonom i testiranjem instrumenta? Ja sam to tebi uredno prijavio. Istina, mnogo davno (mislim u maju-junu). Takodje, prijavio sam i Miladinu i poslao zahtev u dekanat. To se sve desavalo krajem juna, pocetkom jula. Jedina greska je do toga sto je Platon to odradio mnogo kasnije od planiranog. Ja sam cak i odustao od toga da ce se bilo sta realizovati, pa sam i zaboravio? Ispalo je da ti nisi nista znala, pa si digla paniku, pa ja (ponovo i ponovo) dobio po nosu? Malko mi dosadilo? (da dobijam po nosu ako se nesto (u)radi)? :-)

Pozdrav,
P



===
Eberhard Karls Universit?t T?bingen
Seminar f?r Sprachwissenschaft | Quantitative Linguistik
http://www.sfs.uni-tuebingen <http://www.sfs.uni-tuebingen/>.de/~pmilin/




	[[alternative HTML version deleted]]


From unwin at math.uni-augsburg.de  Tue Jan  5 13:51:48 2016
From: unwin at math.uni-augsburg.de (Antony Unwin)
Date: Tue, 5 Jan 2016 13:51:48 +0100
Subject: [R] R Course in Dublin (February 3rd-5th)
Message-ID: <D62586A7-C1D4-4164-BD54-4DDCCAC55A62@math.uni-augsburg.de>

The course will be given by Louis Aslett (Oxford University, author of the packages PhaseType and ReliabilityTheory) and Antony Unwin (author of the book ?Graphical Data Analysis with R? CRC Press 2015).

Details at  

http://insightsc.ie/training/r-statistical-software/ <http://insightsc.ie/training/r-statistical-software/>

Antony Unwin
University of Augsburg, Germany and Insight Statistical Consulting, Dublin, Ireland
	[[alternative HTML version deleted]]


From harrie at eyequestion.nl  Tue Jan  5 09:41:13 2016
From: harrie at eyequestion.nl (Harrie Robins)
Date: Tue, 5 Jan 2016 09:41:13 +0100
Subject: [R] R package built using newer version of R
In-Reply-To: <568AC4E8.2000302@gmail.com>
References: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>
	<568AC4E8.2000302@gmail.com>
Message-ID: <000201d14794$d5e8c9f0$81ba5dd0$@eyequestion.nl>

If that fails (sometimes R gives a version error, package not available for
R version X.X.X), you could try downloading the source package
(package.tar.gz) and compile it with running from console (or prompt):

R CMD INSTALL packagename.tar.gz library-location

Regards,

Harrie

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Duncan
Murdoch
Sent: maandag 4 januari 2016 20:16
To: Tyler Auerbeck <auerbecktj at gmail.com>; r-help at r-project.org
Subject: Re: [R] R package built using newer version of R

On 04/01/2016 2:02 PM, Tyler Auerbeck wrote:
> We're currently looking at using the R eclipse plugin StatET as our 
> development environment. Due to certain requirements, we're still 
> using 2.15.1. However a required package of StatET was built using 
> 2.15.3, which results in the following warning:
>
> Warning message:
> package 'rj' was built under R version 2.15.3
>
> I'm still fairly new to R, but is there any way for us to rebuild this 
> package using 2.15.1? It doesn't appear to cause us any issues, but 
> it's still not desirable for users to see that warning.
>
> Any help would be appreciated.

Yes, it's quite easy to do so.  StatET probably gives menu options to do it,
but I don't know them:  you might want to ask them.  From the R console, try

install.packages("pkgname", type="source")

and if you have the necessary prerequisites (e.g. compilers), you'll get 
it installed from source.   If it fails, post the errors and the results 
of sessionInfo() here, and we'll probably be able to tell you what to do
next.

Duncan Murdoch

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Santosh.Aryal at csiro.au  Tue Jan  5 07:19:26 2016
From: Santosh.Aryal at csiro.au (Santosh.Aryal at csiro.au)
Date: Tue, 5 Jan 2016 06:19:26 +0000
Subject: [R] Merging two data files based on common dates
Message-ID: <2459D081E089474298BBE091760B7F9E2ACCC6C1@exmbx03-cdc.nexus.csiro.au>

Hello there
Pardon my ignorance but, I have two data files with different series of dates and x and y values.
There are common dates in both files. For example
>datafile1
date1                     xval
31/12/1982         20
1/01/1983            30
2/01/1983            40
3/01/1983            50
4/01/1983            60
5/01/1983          70
...
01/01/2010       77
...
31/12/1012       99

>datafile2
date2                  yval
3/01/1983            0.4
4/01/1983            0.5
5/01/1983            0.6
..
01/01/2010        88

All I want is a file/object that merges the two files with data for common dates to look like this.
date                    yval  xval
3/01/1983            0.4  50
4/01/1983            0.5  60
5/01/1983            0.6  70
..
01/01/2010        88     77

I tried ' merge' and ' join' commands but somehow I have not been able to get that. Any help will be appreciated.
Thank you.


Best regards

Santosh Aryal
CSIRO Land and Water
GPO Box 1666, Canberra ACT 2601
Ph:   02 6246 5963
Email: santosh.aryal at csiro.au<mailto:santosh.aryal at csiro.au>







	[[alternative HTML version deleted]]


From tawandatizora at gmail.com  Tue Jan  5 10:06:52 2016
From: tawandatizora at gmail.com (Tawanda Tarakini)
Date: Tue, 5 Jan 2016 11:06:52 +0200
Subject: [R] Subsetting a square marix
Message-ID: <CABOtOy+Q1_10YsrCDC=QikbKwF-yvG3tE=HzWJD1ZkOp=fpR8w@mail.gmail.com>

I have a global matrix (e.g. table below) of species feeding. I am trying
to create specific matrix for specific sites. If for example a subset is to
have sp1, sp3 and spp only these 3 species should be appearing in the
subset (both column and rows).

I have been checking online help but I seem not to get my scenario



Sp1

Sp2

Sp3

Sp4

Sp5

Sp6

Sp1

0

0

1

0

0

0

Sp2

1

0

0

0

1

0

Sp3

0

0

0

1

0

0

Sp4

0

1

0

1

0

0

Sp5

0

0

1

0

0

0

Sp6

0

0

0

1

1

0

-- 
Kind Regards

Tawanda Tarakini

Lecturer and Industrial attachment coordinator
Department of Wildlife, Ecology and Conservation
Chinhoyi University of Technology
Bag 7724, Chinhoyi
Cell: +263 775 321 722
Alternative email: ttarakini at cut.ac.zw

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Jan  5 16:51:18 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 5 Jan 2016 10:51:18 -0500
Subject: [R] Merging two data files based on common dates
In-Reply-To: <2459D081E089474298BBE091760B7F9E2ACCC6C1@exmbx03-cdc.nexus.csiro.au>
References: <2459D081E089474298BBE091760B7F9E2ACCC6C1@exmbx03-cdc.nexus.csiro.au>
Message-ID: <CAM_vjunEQQU_Y5j7ggiJS2GQXz1Z11rGSu0vfNrpU0mu=uY-nQ@mail.gmail.com>

Since the date columns have different names, you need to specify the
by.x and by.y arguments to merge().

Other than that, it should work.

If you need more help, please use dput() to provide some of your data,
and include both the code you used and the error message or incorrect
result you got (that is, a reproducible example). And also, post this
information in plain text rather than HTML.

Sarah

On Tue, Jan 5, 2016 at 1:19 AM,  <Santosh.Aryal at csiro.au> wrote:
> Hello there
> Pardon my ignorance but, I have two data files with different series of dates and x and y values.
> There are common dates in both files. For example
>>datafile1
> date1                     xval
> 31/12/1982         20
> 1/01/1983            30
> 2/01/1983            40
> 3/01/1983            50
> 4/01/1983            60
> 5/01/1983          70
> ...
> 01/01/2010       77
> ...
> 31/12/1012       99
>
>>datafile2
> date2                  yval
> 3/01/1983            0.4
> 4/01/1983            0.5
> 5/01/1983            0.6
> ..
> 01/01/2010        88
>
> All I want is a file/object that merges the two files with data for common dates to look like this.
> date                    yval  xval
> 3/01/1983            0.4  50
> 4/01/1983            0.5  60
> 5/01/1983            0.6  70
> ..
> 01/01/2010        88     77
>
> I tried ' merge' and ' join' commands but somehow I have not been able to get that. Any help will be appreciated.
> Thank you.
>
>
> Best regards
>
> Santosh Aryal
> CSIRO Land and Water
> GPO Box 1666, Canberra ACT 2601
> Ph:   02 6246 5963
> Email: santosh.aryal at csiro.au<mailto:santosh.aryal at csiro.au>
>
>
>

-- 
Sarah Goslee
http://www.numberwright.com


From wdunlap at tibco.com  Tue Jan  5 17:00:02 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 5 Jan 2016 08:00:02 -0800
Subject: [R] Merging two data files based on common dates
In-Reply-To: <2459D081E089474298BBE091760B7F9E2ACCC6C1@exmbx03-cdc.nexus.csiro.au>
References: <2459D081E089474298BBE091760B7F9E2ACCC6C1@exmbx03-cdc.nexus.csiro.au>
Message-ID: <CAF8bMcb2URmLH1y4_3Oa5XT6E9ZobfAQQ=66bw0RaPApYiS7vw@mail.gmail.com>

You did not show the structure of your datasets (with, e.g.,
 dump(c("datafile1","datafile2"),file=stdout())) nor what your call to
merge() was.  However, it may be that you did not use the by.x and by.y
arguments to merge() to specify which columns to match.

txt1 <- "date1                     xval
31/12/1982         20
1/01/1983            30
2/01/1983            40
3/01/1983            50
4/01/1983            60
5/01/1983          70"

txt2 <- "date2                  yval
3/01/1983            0.4
4/01/1983            0.5
5/01/1983            0.6"

df1 <- read.table(text=txt1, header=TRUE)
df2 <- read.table(text=txt2, header=TRUE)
merge(x=df2, y=df1, by.x="date2", by.y="date1")
#       date2 yval xval
# 1 3/01/1983  0.4   50
# 2 4/01/1983  0.5   60
# 3 5/01/1983  0.6   70


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jan 4, 2016 at 10:19 PM, <Santosh.Aryal at csiro.au> wrote:

> Hello there
> Pardon my ignorance but, I have two data files with different series of
> dates and x and y values.
> There are common dates in both files. For example
> >datafile1
> date1                     xval
> 31/12/1982         20
> 1/01/1983            30
> 2/01/1983            40
> 3/01/1983            50
> 4/01/1983            60
> 5/01/1983          70
> ...
> 01/01/2010       77
> ...
> 31/12/1012       99
>
> >datafile2
> date2                  yval
> 3/01/1983            0.4
> 4/01/1983            0.5
> 5/01/1983            0.6
> ..
> 01/01/2010        88
>
> All I want is a file/object that merges the two files with data for common
> dates to look like this.
> date                    yval  xval
> 3/01/1983            0.4  50
> 4/01/1983            0.5  60
> 5/01/1983            0.6  70
> ..
> 01/01/2010        88     77
>
> I tried ' merge' and ' join' commands but somehow I have not been able to
> get that. Any help will be appreciated.
> Thank you.
>
>
> Best regards
>
> Santosh Aryal
> CSIRO Land and Water
> GPO Box 1666, Canberra ACT 2601
> Ph:   02 6246 5963
> Email: santosh.aryal at csiro.au<mailto:santosh.aryal at csiro.au>
>
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Jan  5 17:15:19 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 5 Jan 2016 11:15:19 -0500
Subject: [R] Subsetting a square marix
In-Reply-To: <CABOtOy+Q1_10YsrCDC=QikbKwF-yvG3tE=HzWJD1ZkOp=fpR8w@mail.gmail.com>
References: <CABOtOy+Q1_10YsrCDC=QikbKwF-yvG3tE=HzWJD1ZkOp=fpR8w@mail.gmail.com>
Message-ID: <CAM_vju=ZTmhpsG1wvAWVdD86bQjn20KdyYDJr4-gZFVCkzSMVg@mail.gmail.com>

It really isn't clear what you want, and posting in HTML has mangled
what you did provide.

Please use dput() to provide sample data, and give us a clear idea of
what you want, ideally an example of what the output should look like.
Adding the R code you've tried to use is also a good idea.

Sarah

On Tue, Jan 5, 2016 at 4:06 AM, Tawanda Tarakini
<tawandatizora at gmail.com> wrote:
> I have a global matrix (e.g. table below) of species feeding. I am trying
> to create specific matrix for specific sites. If for example a subset is to
> have sp1, sp3 and spp only these 3 species should be appearing in the
> subset (both column and rows).
>
> I have been checking online help but I seem not to get my scenario
>
>
>
> Sp1
>
> Sp2
>
> Sp3
>
> Sp4
>
> Sp5
>
> Sp6
>
> Sp1
>
> 0
>
> 0
>
> 1
>
> 0
>
> 0
>
> 0
>
> Sp2
>
> 1
>
> 0
>
> 0
>
> 0
>
> 1
>
> 0
>
> Sp3
>
> 0
>
> 0
>
> 0
>
> 1
>
> 0
>
> 0
>
> Sp4
>
> 0
>
> 1
>
> 0
>
> 1
>
> 0
>
> 0
>
> Sp5
>
> 0
>
> 0
>
> 1
>
> 0
>
> 0
>
> 0
>
> Sp6
>
> 0
>
> 0
>
> 0
>
> 1
>
> 1
>
> 0
>
> --
> Kind Regards
>
> Tawanda Tarakini
>
> Lecturer and Industrial attachment coordinator
> Department of Wildlife, Ecology and Conservation
> Chinhoyi University of Technology
> Bag 7724, Chinhoyi
> Cell: +263 775 321 722
> Alternative email: ttarakini at cut.ac.zw
>
>         [[alternative HTML version deleted]]
>


-- 
Sarah Goslee
http://www.numberwright.com


From martincmd at hotmail.com  Tue Jan  5 16:58:39 2016
From: martincmd at hotmail.com (=?iso-8859-1?B?TWFydO1uIENh8fNu?=)
Date: Tue, 5 Jan 2016 10:58:39 -0500
Subject: [R] Equality of factor levels problem
Message-ID: <COL126-W505882DA771F11F6036CCAA8F30@phx.gbl>

Hi to all useRs on the list.

I want to know if there is a function in R which evaluates if observations of two factors (similar but with unequal levels) in a data frame are equal at row level.

Example of dataset:

var1 <- c(1, 2, 3, 2, 1)
var2 <- c(1, 3, 3, 4, 2)
dat <- data.frame(var1, var2)
dat$var1 <- factor(dat$var1, labels = c("low", "medium", "high"))
dat$var2 <- factor(dat$var2, labels = c("low", "medium", "high", "very high"))

What I want as a result is a logical vector that says if observation [i, 1] is equal to observation [i, 2].

Something like...

var1[1] == var2[1]  # TRUE
var1[2] == var2[2]  # FALSE
var1[3] == var2[3]  # TRUE
var1[4] == var2[4]  # FALSE
var1[4] == var2[4]  # FALSE

I've tried equal signs (==), commands such as "identical" and "all.equal" in "for loops" and I can't make it work for factors. 

It seems very simple but I just don't find the way to do it.

Regards,

Mart?n 

 		 	   		  
	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Jan  5 17:39:18 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 5 Jan 2016 11:39:18 -0500
Subject: [R] Equality of factor levels problem
In-Reply-To: <COL126-W505882DA771F11F6036CCAA8F30@phx.gbl>
References: <COL126-W505882DA771F11F6036CCAA8F30@phx.gbl>
Message-ID: <CAM_vju=dt626HgG+MSX1usJzVt5G9RUbkpmHY4E3W4nibBcFeg@mail.gmail.com>

You want to compare the values of the levels, I think?

What about:
> as.character(dat$var1) == as.character(dat$var2)
[1]  TRUE FALSE  TRUE FALSE FALSE

Sarah


On Tue, Jan 5, 2016 at 10:58 AM, Mart?n Ca??n <martincmd at hotmail.com> wrote:
> Hi to all useRs on the list.
>
> I want to know if there is a function in R which evaluates if observations of two factors (similar but with unequal levels) in a data frame are equal at row level.
>
> Example of dataset:
>
> var1 <- c(1, 2, 3, 2, 1)
> var2 <- c(1, 3, 3, 4, 2)
> dat <- data.frame(var1, var2)
> dat$var1 <- factor(dat$var1, labels = c("low", "medium", "high"))
> dat$var2 <- factor(dat$var2, labels = c("low", "medium", "high", "very high"))
>
> What I want as a result is a logical vector that says if observation [i, 1] is equal to observation [i, 2].
>
> Something like...
>
> var1[1] == var2[1]  # TRUE
> var1[2] == var2[2]  # FALSE
> var1[3] == var2[3]  # TRUE
> var1[4] == var2[4]  # FALSE
> var1[4] == var2[4]  # FALSE
>
> I've tried equal signs (==), commands such as "identical" and "all.equal" in "for loops" and I can't make it work for factors.
>
> It seems very simple but I just don't find the way to do it.
>
> Regards,
>
> Mart?n
>
-- 
Sarah Goslee
http://www.numberwright.com


From nilesh.dighe at monsanto.com  Tue Jan  5 17:48:40 2016
From: nilesh.dighe at monsanto.com (DIGHE, NILESH [AG/2362])
Date: Tue, 5 Jan 2016 16:48:40 +0000
Subject: [R] store results from loop into a dataframe
Message-ID: <24156952D190E841BF8E66CB59FAB94A4870B346@STLWEXMBXPRD14.na.ds.monsanto.com>

Dear R users:

I am trying to create a function that will loop over three dependent variables in my aov model, and then get the HSD.test for each variable.  I like to store the results from each loop in a data frame.



When I run my function (funx) on my data (dat), results from only yield gets populated in all three columns of the dataframe.  I am not able to store the results for each variable in a dataframe. Any help will be highly appreciated.







function (x)

{

    trait_names <- c("yield", "lp", "lnth")

    d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,

        6))

    for (i in trait_names) {

        mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD + PEDIGREE*FIELD + FIELD%in%REP")),

            data = x)

        out <- HSD.test(mod, "PEDIGREE", group = TRUE, console = FALSE)

        d[, i] <- out$means[, 1]

    }

    d

}


structure(list(FIELD = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L), .Label = c("FYLS", "HKI1", "KIS1", "LMLS",
"SELS", "SGL1"), class = "factor"), REP = structure(c(1L, 2L,
3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L,
1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L,
2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L,
3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L,
1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L,
2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L,
3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2",
"3"), class = "factor"), PEDIGREE = structure(c(1L, 1L, 1L, 2L,
2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L,
1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L,
1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L,
6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L,
5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L,
5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L,
4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L), .Label = c("A", "B", "C", "D",
"E", "F"), class = "factor"), yield = c(1003L, 923L, 1268L, 1226L,
1059L, 1150L, 900L, 816L, 1072L, 1158L, 1026L, 1299L, 1083L,
1038L, 1236L, 1287L, 1270L, 1612L, 1513L, 1676L, 1504L, 1417L,
1932L, 1644L, 1293L, 1542L, 1452L, 1180L, 1248L, 1764L, 1326L,
1877L, 1788L, 1606L, 1809L, 1791L, 2294L, 2315L, 2320L, 2083L,
1895L, 2284L, 2000L, 2380L, 1952L, 2414L, 2354L, 2095L, 2227L,
2093L, 2019L, 2505L, 2410L, 2287L, 2507L, 2507L, 2349L, 2162L,
2108L, 2319L, 2028L, 1947L, 2352L, 2698L, 2369L, 1798L, 2422L,
2509L, 2234L, 2451L, 2139L, 1957L, 799L, 787L, 701L, 781L, 808L,
582L, 770L, 752L, 801L, 865L, 608L, 620L, 677L, 775L, 722L, 1030L,
606L, 729L, 1638L, 1408L, 1045L, 1685L, 1109L, 1210L, 1419L,
1048L, 1129L, 1549L, 1325L, 1315L, 1838L, 1066L, 1295L, 1499L,
1472L, 1139L), lp = c(NA, NA, 46.31, NA, NA, 43.8, NA, NA, 43.91,
NA, NA, 44.47, NA, NA, 45.16, NA, NA, 43.57, 40.65, NA, NA, 40.04,
NA, NA, 41.33, NA, NA, 40.75, NA, NA, 42.04, NA, NA, 40.35, NA,
NA, 43.682, NA, NA, 41.712, NA, NA, 42.566, NA, NA, 43.228, NA,
NA, 43.63, NA, NA, 42.058, NA, NA, NA, 45.19, NA, NA, 41.91,
NA, NA, 43.86, NA, NA, 44.48, NA, NA, 44.34, NA, NA, 43.03, NA,
NA, NA, 44.08, NA, NA, 41.39, NA, NA, 42.48, NA, NA, 44.13, NA,
NA, 43.39, NA, NA, 42.82, 42.18, NA, NA, 41.42, NA, NA, 41.25,
NA, NA, 42.31, NA, NA, 43.22, NA, NA, 40.52, NA, NA), lnth = c(NA,
NA, 1.151, NA, NA, 1.135, NA, NA, 1.109, NA, NA, 1.117, NA, NA,
1.107, NA, NA, 1.196, 1.255, NA, NA, 1.229, NA, NA, 1.158, NA,
NA, 1.214, NA, NA, 1.152, NA, NA, 1.194, NA, NA, NA, NA, NA,
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
1.2, NA, NA, 1.219, NA, NA, 1.115, NA, NA, 1.205, NA, NA, 1.238,
NA, NA, 1.244, NA, NA, NA, 1.096, NA, NA, 1.021, NA, NA, 1.055,
NA, NA, 1.058, NA, NA, 1.026, NA, NA, 1.115, 1.202, NA, NA, 1.161,
NA, NA, 1.168, NA, NA, 1.189, NA, NA, 1.204, NA, NA, 1.277, NA,
NA)), .Names = c("FIELD", "REP", "PEDIGREE", "yield", "lp", "lnth"
), row.names = c(NA, -108L), class = "data.frame")






R version 3.2.1 (2015-06-18)

Platform: i386-w64-mingw32/i386 (32-bit)

Running under: Windows 7 x64 (build 7601) Service Pack 1



locale:

[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252

[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C

[5] LC_TIME=English_United States.1252



attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base



other attached packages:

[1] agricolae_1.2-1 asreml_3.0      lattice_0.20-31 ggplot2_1.0.1   dplyr_0.4.2     plyr_1.8.3



loaded via a namespace (and not attached):

 [1] spdep_0.5-88     Rcpp_0.12.1      cluster_2.0.2    magrittr_1.5     splines_3.2.1    MASS_7.3-41

 [7] munsell_0.4.2    colorspace_1.2-6 R6_2.0.1         stringr_1.0.0    tools_3.2.1      parallel_3.2.1

[13] grid_3.2.1       gtable_0.1.2     nlme_3.1-122     coda_0.17-1      DBI_0.3.1        deldir_0.1-9

[19] lazyeval_0.1.10  assertthat_0.1   digest_0.6.8     Matrix_1.2-1     reshape2_1.4.1   sp_1.2-1

[25] stringi_1.0-1    klaR_0.6-12      LearnBayes_2.15  scales_0.3.0     boot_1.3-17      combinat_0.0-8

[31] proto_0.3-10

Thanks.
Nilesh

Nilesh Dighe
(806)-252-7492 (Cell)
(806)-741-2019 (Office)


This e-mail message may contain privileged and/or confidential information, and is intended to be received only by persons entitled
to receive such information. If you have received this e-mail in error, please notify the sender immediately. Please delete it and
all attachments from any servers, hard drives or any other media. Other use of this e-mail by you is strictly prohibited.

All e-mails and attachments sent and received are subject to monitoring, reading and archival by Monsanto, including its
subsidiaries. The recipient of this e-mail is solely responsible for checking for the presence of "Viruses" or other "Malware".
Monsanto, along with its subsidiaries, accepts no liability for any damage caused by any such code transmitted by or accompanying
this e-mail or any attachment.


The information contained in this email may be subject to the export control laws and regulations of the United States, potentially
including but not limited to the Export Administration Regulations (EAR) and sanctions regulations issued by the U.S. Department of
Treasury, Office of Foreign Asset Controls (OFAC).  As a recipient of this information you are obligated to comply with all
applicable U.S. export laws and regulations.

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Jan  5 18:19:46 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 5 Jan 2016 12:19:46 -0500
Subject: [R] store results from loop into a dataframe
In-Reply-To: <24156952D190E841BF8E66CB59FAB94A4870B346@STLWEXMBXPRD14.na.ds.monsanto.com>
References: <24156952D190E841BF8E66CB59FAB94A4870B346@STLWEXMBXPRD14.na.ds.monsanto.com>
Message-ID: <CAM_vju=5HhiKqDcPiEiqg+D0k6yAcoPWfx9NJ_LMUWxbtXU6OA@mail.gmail.com>

Leaving aside the question of whether this is the best way to approach
your problem (unlikely), there's a couple of errors in your code
involving indexing. Once fixed, the code demonstrates some errors in
your use of HSD.test that will be harder for you to deal with.

Thanks for the complete reproducible example.

fun2 <- function (x)

{

    trait_names <- c("yield", "lp", "lnth")

    d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,

        6))

    for (i in trait_names) {
# your formula has all the trait names, not the selected one
        # mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD +
PEDIGREE*FIELD + FIELD%in%REP")), data = x)
        mod <- aov(formula(paste(i, "~ PEDIGREE + FIELD +
PEDIGREE*FIELD + FIELD%in%REP")), data = x)

        out <- HSD.test(mod, "PEDIGREE", group = TRUE, console = FALSE)

# you're indexing by the trait name, instead of its position
        # d[, i] <- out$means[, 1]
        d[, which(trait_names == i)] <- out$means[, 1]

    }

    d

}

Sarah

On Tue, Jan 5, 2016 at 11:48 AM, DIGHE, NILESH [AG/2362]
<nilesh.dighe at monsanto.com> wrote:
> Dear R users:
>
> I am trying to create a function that will loop over three dependent variables in my aov model, and then get the HSD.test for each variable.  I like to store the results from each loop in a data frame.
>
>
>
> When I run my function (funx) on my data (dat), results from only yield gets populated in all three columns of the dataframe.  I am not able to store the results for each variable in a dataframe. Any help will be highly appreciated.
>
>
>
>
>
>
>
> function (x)
>
> {
>
>     trait_names <- c("yield", "lp", "lnth")
>
>     d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,
>
>         6))
>
>     for (i in trait_names) {
>
>         mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD + PEDIGREE*FIELD + FIELD%in%REP")),
>
>             data = x)
>
>         out <- HSD.test(mod, "PEDIGREE", group = TRUE, console = FALSE)
>
>         d[, i] <- out$means[, 1]
>
>     }
>
>     d
>
> }
>
>
> structure(list(FIELD = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L,
> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
> 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
> 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
> 6L, 6L, 6L, 6L, 6L), .Label = c("FYLS", "HKI1", "KIS1", "LMLS",
> "SELS", "SGL1"), class = "factor"), REP = structure(c(1L, 2L,
> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L,
> 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L,
> 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L,
> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L,
> 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L,
> 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L,
> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2",
> "3"), class = "factor"), PEDIGREE = structure(c(1L, 1L, 1L, 2L,
> 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L,
> 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L,
> 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L,
> 6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L,
> 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L,
> 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L,
> 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L), .Label = c("A", "B", "C", "D",
> "E", "F"), class = "factor"), yield = c(1003L, 923L, 1268L, 1226L,
> 1059L, 1150L, 900L, 816L, 1072L, 1158L, 1026L, 1299L, 1083L,
> 1038L, 1236L, 1287L, 1270L, 1612L, 1513L, 1676L, 1504L, 1417L,
> 1932L, 1644L, 1293L, 1542L, 1452L, 1180L, 1248L, 1764L, 1326L,
> 1877L, 1788L, 1606L, 1809L, 1791L, 2294L, 2315L, 2320L, 2083L,
> 1895L, 2284L, 2000L, 2380L, 1952L, 2414L, 2354L, 2095L, 2227L,
> 2093L, 2019L, 2505L, 2410L, 2287L, 2507L, 2507L, 2349L, 2162L,
> 2108L, 2319L, 2028L, 1947L, 2352L, 2698L, 2369L, 1798L, 2422L,
> 2509L, 2234L, 2451L, 2139L, 1957L, 799L, 787L, 701L, 781L, 808L,
> 582L, 770L, 752L, 801L, 865L, 608L, 620L, 677L, 775L, 722L, 1030L,
> 606L, 729L, 1638L, 1408L, 1045L, 1685L, 1109L, 1210L, 1419L,
> 1048L, 1129L, 1549L, 1325L, 1315L, 1838L, 1066L, 1295L, 1499L,
> 1472L, 1139L), lp = c(NA, NA, 46.31, NA, NA, 43.8, NA, NA, 43.91,
> NA, NA, 44.47, NA, NA, 45.16, NA, NA, 43.57, 40.65, NA, NA, 40.04,
> NA, NA, 41.33, NA, NA, 40.75, NA, NA, 42.04, NA, NA, 40.35, NA,
> NA, 43.682, NA, NA, 41.712, NA, NA, 42.566, NA, NA, 43.228, NA,
> NA, 43.63, NA, NA, 42.058, NA, NA, NA, 45.19, NA, NA, 41.91,
> NA, NA, 43.86, NA, NA, 44.48, NA, NA, 44.34, NA, NA, 43.03, NA,
> NA, NA, 44.08, NA, NA, 41.39, NA, NA, 42.48, NA, NA, 44.13, NA,
> NA, 43.39, NA, NA, 42.82, 42.18, NA, NA, 41.42, NA, NA, 41.25,
> NA, NA, 42.31, NA, NA, 43.22, NA, NA, 40.52, NA, NA), lnth = c(NA,
> NA, 1.151, NA, NA, 1.135, NA, NA, 1.109, NA, NA, 1.117, NA, NA,
> 1.107, NA, NA, 1.196, 1.255, NA, NA, 1.229, NA, NA, 1.158, NA,
> NA, 1.214, NA, NA, 1.152, NA, NA, 1.194, NA, NA, NA, NA, NA,
> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> 1.2, NA, NA, 1.219, NA, NA, 1.115, NA, NA, 1.205, NA, NA, 1.238,
> NA, NA, 1.244, NA, NA, NA, 1.096, NA, NA, 1.021, NA, NA, 1.055,
> NA, NA, 1.058, NA, NA, 1.026, NA, NA, 1.115, 1.202, NA, NA, 1.161,
> NA, NA, 1.168, NA, NA, 1.189, NA, NA, 1.204, NA, NA, 1.277, NA,
> NA)), .Names = c("FIELD", "REP", "PEDIGREE", "yield", "lp", "lnth"
> ), row.names = c(NA, -108L), class = "data.frame")
>
>
>
>
>
>
> R version 3.2.1 (2015-06-18)
>
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
>
>
> locale:
>
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252
>
> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>
> [5] LC_TIME=English_United States.1252
>
>
>
> attached base packages:
>
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
>
> other attached packages:
>
> [1] agricolae_1.2-1 asreml_3.0      lattice_0.20-31 ggplot2_1.0.1   dplyr_0.4.2     plyr_1.8.3
>
>
>
> loaded via a namespace (and not attached):
>
>  [1] spdep_0.5-88     Rcpp_0.12.1      cluster_2.0.2    magrittr_1.5     splines_3.2.1    MASS_7.3-41
>
>  [7] munsell_0.4.2    colorspace_1.2-6 R6_2.0.1         stringr_1.0.0    tools_3.2.1      parallel_3.2.1
>
> [13] grid_3.2.1       gtable_0.1.2     nlme_3.1-122     coda_0.17-1      DBI_0.3.1        deldir_0.1-9
>
> [19] lazyeval_0.1.10  assertthat_0.1   digest_0.6.8     Matrix_1.2-1     reshape2_1.4.1   sp_1.2-1
>
> [25] stringi_1.0-1    klaR_0.6-12      LearnBayes_2.15  scales_0.3.0     boot_1.3-17      combinat_0.0-8
>
> [31] proto_0.3-10
>
> Thanks.
> Nilesh
>

Sarah Goslee
http://www.numberwright.com


From nilesh.dighe at monsanto.com  Tue Jan  5 18:39:11 2016
From: nilesh.dighe at monsanto.com (DIGHE, NILESH [AG/2362])
Date: Tue, 5 Jan 2016 17:39:11 +0000
Subject: [R] store results from loop into a dataframe
In-Reply-To: <CAM_vju=5HhiKqDcPiEiqg+D0k6yAcoPWfx9NJ_LMUWxbtXU6OA@mail.gmail.com>
References: <24156952D190E841BF8E66CB59FAB94A4870B346@STLWEXMBXPRD14.na.ds.monsanto.com>
	<CAM_vju=5HhiKqDcPiEiqg+D0k6yAcoPWfx9NJ_LMUWxbtXU6OA@mail.gmail.com>
Message-ID: <24156952D190E841BF8E66CB59FAB94A4870C4E5@STLWEXMBXPRD14.na.ds.monsanto.com>

Sarah: Thanks for pointing out the errors in my function.

Below are the errors I am getting after I run the corrected quote:
Error in if (s) { : missing value where TRUE/FALSE needed
In addition: Warning message:
In qtukey(1 - alpha, ntr, DFerror) : NaNs produced

You are right, I have no idea to handle these errors.

Do you recommend any other approach to solve my problem? 

Thanks for your time.
Nilesh 



-----Original Message-----
From: Sarah Goslee [mailto:sarah.goslee at gmail.com] 
Sent: Tuesday, January 05, 2016 11:20 AM
To: DIGHE, NILESH [AG/2362]
Cc: r-help at r-project.org
Subject: Re: [R] store results from loop into a dataframe

Leaving aside the question of whether this is the best way to approach your problem (unlikely), there's a couple of errors in your code involving indexing. Once fixed, the code demonstrates some errors in your use of HSD.test that will be harder for you to deal with.

Thanks for the complete reproducible example.

fun2 <- function (x)

{

    trait_names <- c("yield", "lp", "lnth")

    d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,

        6))

    for (i in trait_names) {
# your formula has all the trait names, not the selected one
        # mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD + PEDIGREE*FIELD + FIELD%in%REP")), data = x)
        mod <- aov(formula(paste(i, "~ PEDIGREE + FIELD + PEDIGREE*FIELD + FIELD%in%REP")), data = x)

        out <- HSD.test(mod, "PEDIGREE", group = TRUE, console = FALSE)

# you're indexing by the trait name, instead of its position
        # d[, i] <- out$means[, 1]
        d[, which(trait_names == i)] <- out$means[, 1]

    }

    d

}

Sarah

On Tue, Jan 5, 2016 at 11:48 AM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com> wrote:
> Dear R users:
>
> I am trying to create a function that will loop over three dependent variables in my aov model, and then get the HSD.test for each variable.  I like to store the results from each loop in a data frame.
>
>
>
> When I run my function (funx) on my data (dat), results from only yield gets populated in all three columns of the dataframe.  I am not able to store the results for each variable in a dataframe. Any help will be highly appreciated.
>
>
>
>
>
>
>
> function (x)
>
> {
>
>     trait_names <- c("yield", "lp", "lnth")
>
>     d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,
>
>         6))
>
>     for (i in trait_names) {
>
>         mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD + 
> PEDIGREE*FIELD + FIELD%in%REP")),
>
>             data = x)
>
>         out <- HSD.test(mod, "PEDIGREE", group = TRUE, console = 
> FALSE)
>
>         d[, i] <- out$means[, 1]
>
>     }
>
>     d
>
> }
>
>
> structure(list(FIELD = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 
> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 
> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 
> 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = 
> c("FYLS", "HKI1", "KIS1", "LMLS", "SELS", "SGL1"), class = "factor"), 
> REP = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 
> 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 
> 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 
> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 
> 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 
> 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 
> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2", "3"), 
> class = "factor"), PEDIGREE = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 
> 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L, 
> 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 
> 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 
> 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 
> 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 
> 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L), 
> .Label = c("A", "B", "C", "D", "E", "F"), class = "factor"), yield = 
> c(1003L, 923L, 1268L, 1226L, 1059L, 1150L, 900L, 816L, 1072L, 1158L, 
> 1026L, 1299L, 1083L, 1038L, 1236L, 1287L, 1270L, 1612L, 1513L, 1676L, 
> 1504L, 1417L, 1932L, 1644L, 1293L, 1542L, 1452L, 1180L, 1248L, 1764L, 
> 1326L, 1877L, 1788L, 1606L, 1809L, 1791L, 2294L, 2315L, 2320L, 2083L, 
> 1895L, 2284L, 2000L, 2380L, 1952L, 2414L, 2354L, 2095L, 2227L, 2093L, 
> 2019L, 2505L, 2410L, 2287L, 2507L, 2507L, 2349L, 2162L, 2108L, 2319L, 
> 2028L, 1947L, 2352L, 2698L, 2369L, 1798L, 2422L, 2509L, 2234L, 2451L, 
> 2139L, 1957L, 799L, 787L, 701L, 781L, 808L, 582L, 770L, 752L, 801L, 
> 865L, 608L, 620L, 677L, 775L, 722L, 1030L, 606L, 729L, 1638L, 1408L, 
> 1045L, 1685L, 1109L, 1210L, 1419L, 1048L, 1129L, 1549L, 1325L, 1315L, 
> 1838L, 1066L, 1295L, 1499L, 1472L, 1139L), lp = c(NA, NA, 46.31, NA, 
> NA, 43.8, NA, NA, 43.91, NA, NA, 44.47, NA, NA, 45.16, NA, NA, 43.57, 
> 40.65, NA, NA, 40.04, NA, NA, 41.33, NA, NA, 40.75, NA, NA, 42.04, NA, 
> NA, 40.35, NA, NA, 43.682, NA, NA, 41.712, NA, NA, 42.566, NA, NA, 
> 43.228, NA, NA, 43.63, NA, NA, 42.058, NA, NA, NA, 45.19, NA, NA, 
> 41.91, NA, NA, 43.86, NA, NA, 44.48, NA, NA, 44.34, NA, NA, 43.03, NA, 
> NA, NA, 44.08, NA, NA, 41.39, NA, NA, 42.48, NA, NA, 44.13, NA, NA, 
> 43.39, NA, NA, 42.82, 42.18, NA, NA, 41.42, NA, NA, 41.25, NA, NA, 
> 42.31, NA, NA, 43.22, NA, NA, 40.52, NA, NA), lnth = c(NA, NA, 1.151, 
> NA, NA, 1.135, NA, NA, 1.109, NA, NA, 1.117, NA, NA, 1.107, NA, NA, 
> 1.196, 1.255, NA, NA, 1.229, NA, NA, 1.158, NA, NA, 1.214, NA, NA, 
> 1.152, NA, NA, 1.194, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
> NA, NA, NA, NA, NA, NA, NA, NA, NA, 1.2, NA, NA, 1.219, NA, NA, 1.115, 
> NA, NA, 1.205, NA, NA, 1.238, NA, NA, 1.244, NA, NA, NA, 1.096, NA, 
> NA, 1.021, NA, NA, 1.055, NA, NA, 1.058, NA, NA, 1.026, NA, NA, 1.115, 
> 1.202, NA, NA, 1.161, NA, NA, 1.168, NA, NA, 1.189, NA, NA, 1.204, NA, 
> NA, 1.277, NA, NA)), .Names = c("FIELD", "REP", "PEDIGREE", "yield", 
> "lp", "lnth"
> ), row.names = c(NA, -108L), class = "data.frame")
>
>
>
>
>
>
> R version 3.2.1 (2015-06-18)
>
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
>
>
> locale:
>
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United 
> States.1252
>
> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>
> [5] LC_TIME=English_United States.1252
>
>
>
> attached base packages:
>
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
>
> other attached packages:
>
> [1] agricolae_1.2-1 asreml_3.0      lattice_0.20-31 ggplot2_1.0.1   dplyr_0.4.2     plyr_1.8.3
>
>
>
> loaded via a namespace (and not attached):
>
>  [1] spdep_0.5-88     Rcpp_0.12.1      cluster_2.0.2    magrittr_1.5     splines_3.2.1    MASS_7.3-41
>
>  [7] munsell_0.4.2    colorspace_1.2-6 R6_2.0.1         stringr_1.0.0    tools_3.2.1      parallel_3.2.1
>
> [13] grid_3.2.1       gtable_0.1.2     nlme_3.1-122     coda_0.17-1      DBI_0.3.1        deldir_0.1-9
>
> [19] lazyeval_0.1.10  assertthat_0.1   digest_0.6.8     Matrix_1.2-1     reshape2_1.4.1   sp_1.2-1
>
> [25] stringi_1.0-1    klaR_0.6-12      LearnBayes_2.15  scales_0.3.0     boot_1.3-17      combinat_0.0-8
>
> [31] proto_0.3-10
>
> Thanks.
> Nilesh
>

Sarah Goslee
http://www.numberwright.com
This e-mail message may contain privileged and/or confidential information, and is intended to be received only by persons entitled
to receive such information. If you have received this e-mail in error, please notify the sender immediately. Please delete it and
all attachments from any servers, hard drives or any other media. Other use of this e-mail by you is strictly prohibited.

All e-mails and attachments sent and received are subject to monitoring, reading and archival by Monsanto, including its
subsidiaries. The recipient of this e-mail is solely responsible for checking for the presence of "Viruses" or other "Malware".
Monsanto, along with its subsidiaries, accepts no liability for any damage caused by any such code transmitted by or accompanying
this e-mail or any attachment.


The information contained in this email may be subject to the export control laws and regulations of the United States, potentially
including but not limited to the Export Administration Regulations (EAR) and sanctions regulations issued by the U.S. Department of
Treasury, Office of Foreign Asset Controls (OFAC).  As a recipient of this information you are obligated to comply with all
applicable U.S. export laws and regulations.

From martincmd at hotmail.com  Tue Jan  5 17:59:34 2016
From: martincmd at hotmail.com (=?iso-8859-1?B?TWFydO1uIENh8fNu?=)
Date: Tue, 5 Jan 2016 11:59:34 -0500
Subject: [R] Equality of factor levels problem
In-Reply-To: <CAM_vju=dt626HgG+MSX1usJzVt5G9RUbkpmHY4E3W4nibBcFeg@mail.gmail.com>
References: <COL126-W505882DA771F11F6036CCAA8F30@phx.gbl>,
	<CAM_vju=dt626HgG+MSX1usJzVt5G9RUbkpmHY4E3W4nibBcFeg@mail.gmail.com>
Message-ID: <COL126-W3913373D31D6674E7EF7F5A8F30@phx.gbl>

Thank you, Sarah.

This works just fine. 

So simple. 

I didn't know I had to change the factor to character before making the comparison.

Regards,

Mart?n


 		 	   		  
	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Jan  5 19:13:23 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 5 Jan 2016 13:13:23 -0500
Subject: [R] store results from loop into a dataframe
In-Reply-To: <24156952D190E841BF8E66CB59FAB94A4870C4E5@STLWEXMBXPRD14.na.ds.monsanto.com>
References: <24156952D190E841BF8E66CB59FAB94A4870B346@STLWEXMBXPRD14.na.ds.monsanto.com>
	<CAM_vju=5HhiKqDcPiEiqg+D0k6yAcoPWfx9NJ_LMUWxbtXU6OA@mail.gmail.com>
	<24156952D190E841BF8E66CB59FAB94A4870C4E5@STLWEXMBXPRD14.na.ds.monsanto.com>
Message-ID: <CAM_vjukGG63zgr=brgEwoLRtOs4MHDEjE1FGGXS4sp72TA2Edw@mail.gmail.com>

If you run each variable individually, you'll discover that the NAs in
your data are causing problems. It's up to you to figure out what the
best way to handle those missing values for your research is.

Sarah

On Tue, Jan 5, 2016 at 12:39 PM, DIGHE, NILESH [AG/2362]
<nilesh.dighe at monsanto.com> wrote:
> Sarah: Thanks for pointing out the errors in my function.
>
> Below are the errors I am getting after I run the corrected quote:
> Error in if (s) { : missing value where TRUE/FALSE needed
> In addition: Warning message:
> In qtukey(1 - alpha, ntr, DFerror) : NaNs produced
>
> You are right, I have no idea to handle these errors.
>
> Do you recommend any other approach to solve my problem?
>
> Thanks for your time.
> Nilesh
>
>
>
> -----Original Message-----
> From: Sarah Goslee [mailto:sarah.goslee at gmail.com]
> Sent: Tuesday, January 05, 2016 11:20 AM
> To: DIGHE, NILESH [AG/2362]
> Cc: r-help at r-project.org
> Subject: Re: [R] store results from loop into a dataframe
>
> Leaving aside the question of whether this is the best way to approach your problem (unlikely), there's a couple of errors in your code involving indexing. Once fixed, the code demonstrates some errors in your use of HSD.test that will be harder for you to deal with.
>
> Thanks for the complete reproducible example.
>
> fun2 <- function (x)
>
> {
>
>     trait_names <- c("yield", "lp", "lnth")
>
>     d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,
>
>         6))
>
>     for (i in trait_names) {
> # your formula has all the trait names, not the selected one
>         # mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD + PEDIGREE*FIELD + FIELD%in%REP")), data = x)
>         mod <- aov(formula(paste(i, "~ PEDIGREE + FIELD + PEDIGREE*FIELD + FIELD%in%REP")), data = x)
>
>         out <- HSD.test(mod, "PEDIGREE", group = TRUE, console = FALSE)
>
> # you're indexing by the trait name, instead of its position
>         # d[, i] <- out$means[, 1]
>         d[, which(trait_names == i)] <- out$means[, 1]
>
>     }
>
>     d
>
> }
>
> Sarah
>
> On Tue, Jan 5, 2016 at 11:48 AM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com> wrote:
>> Dear R users:
>>
>> I am trying to create a function that will loop over three dependent variables in my aov model, and then get the HSD.test for each variable.  I like to store the results from each loop in a data frame.
>>
>>
>>
>> When I run my function (funx) on my data (dat), results from only yield gets populated in all three columns of the dataframe.  I am not able to store the results for each variable in a dataframe. Any help will be highly appreciated.
>>
>>
>>
>>
>>
>>
>>
>> function (x)
>>
>> {
>>
>>     trait_names <- c("yield", "lp", "lnth")
>>
>>     d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,
>>
>>         6))
>>
>>     for (i in trait_names) {
>>
>>         mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD +
>> PEDIGREE*FIELD + FIELD%in%REP")),
>>
>>             data = x)
>>
>>         out <- HSD.test(mod, "PEDIGREE", group = TRUE, console =
>> FALSE)
>>
>>         d[, i] <- out$means[, 1]
>>
>>     }
>>
>>     d
>>
>> }
>>
>>
>> structure(list(FIELD = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L,
>> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L,
>> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L,
>> 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label =
>> c("FYLS", "HKI1", "KIS1", "LMLS", "SELS", "SGL1"), class = "factor"),
>> REP = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L,
>> 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L,
>> 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L,
>> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L,
>> 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L,
>> 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L,
>> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2", "3"),
>> class = "factor"), PEDIGREE = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L,
>> 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L,
>> 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L,
>> 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L,
>> 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L,
>> 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L,
>> 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L),
>> .Label = c("A", "B", "C", "D", "E", "F"), class = "factor"), yield =
>> c(1003L, 923L, 1268L, 1226L, 1059L, 1150L, 900L, 816L, 1072L, 1158L,
>> 1026L, 1299L, 1083L, 1038L, 1236L, 1287L, 1270L, 1612L, 1513L, 1676L,
>> 1504L, 1417L, 1932L, 1644L, 1293L, 1542L, 1452L, 1180L, 1248L, 1764L,
>> 1326L, 1877L, 1788L, 1606L, 1809L, 1791L, 2294L, 2315L, 2320L, 2083L,
>> 1895L, 2284L, 2000L, 2380L, 1952L, 2414L, 2354L, 2095L, 2227L, 2093L,
>> 2019L, 2505L, 2410L, 2287L, 2507L, 2507L, 2349L, 2162L, 2108L, 2319L,
>> 2028L, 1947L, 2352L, 2698L, 2369L, 1798L, 2422L, 2509L, 2234L, 2451L,
>> 2139L, 1957L, 799L, 787L, 701L, 781L, 808L, 582L, 770L, 752L, 801L,
>> 865L, 608L, 620L, 677L, 775L, 722L, 1030L, 606L, 729L, 1638L, 1408L,
>> 1045L, 1685L, 1109L, 1210L, 1419L, 1048L, 1129L, 1549L, 1325L, 1315L,
>> 1838L, 1066L, 1295L, 1499L, 1472L, 1139L), lp = c(NA, NA, 46.31, NA,
>> NA, 43.8, NA, NA, 43.91, NA, NA, 44.47, NA, NA, 45.16, NA, NA, 43.57,
>> 40.65, NA, NA, 40.04, NA, NA, 41.33, NA, NA, 40.75, NA, NA, 42.04, NA,
>> NA, 40.35, NA, NA, 43.682, NA, NA, 41.712, NA, NA, 42.566, NA, NA,
>> 43.228, NA, NA, 43.63, NA, NA, 42.058, NA, NA, NA, 45.19, NA, NA,
>> 41.91, NA, NA, 43.86, NA, NA, 44.48, NA, NA, 44.34, NA, NA, 43.03, NA,
>> NA, NA, 44.08, NA, NA, 41.39, NA, NA, 42.48, NA, NA, 44.13, NA, NA,
>> 43.39, NA, NA, 42.82, 42.18, NA, NA, 41.42, NA, NA, 41.25, NA, NA,
>> 42.31, NA, NA, 43.22, NA, NA, 40.52, NA, NA), lnth = c(NA, NA, 1.151,
>> NA, NA, 1.135, NA, NA, 1.109, NA, NA, 1.117, NA, NA, 1.107, NA, NA,
>> 1.196, 1.255, NA, NA, 1.229, NA, NA, 1.158, NA, NA, 1.214, NA, NA,
>> 1.152, NA, NA, 1.194, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, 1.2, NA, NA, 1.219, NA, NA, 1.115,
>> NA, NA, 1.205, NA, NA, 1.238, NA, NA, 1.244, NA, NA, NA, 1.096, NA,
>> NA, 1.021, NA, NA, 1.055, NA, NA, 1.058, NA, NA, 1.026, NA, NA, 1.115,
>> 1.202, NA, NA, 1.161, NA, NA, 1.168, NA, NA, 1.189, NA, NA, 1.204, NA,
>> NA, 1.277, NA, NA)), .Names = c("FIELD", "REP", "PEDIGREE", "yield",
>> "lp", "lnth"
>> ), row.names = c(NA, -108L), class = "data.frame")
>>
>>
>>
>>
>>
>>
>> R version 3.2.1 (2015-06-18)
>>
>> Platform: i386-w64-mingw32/i386 (32-bit)
>>
>> Running under: Windows 7 x64 (build 7601) Service Pack 1
>>
>>
>>
>> locale:
>>
>> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
>> States.1252
>>
>> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>>
>> [5] LC_TIME=English_United States.1252
>>
>>
>>
>> attached base packages:
>>
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>
>>
>> other attached packages:
>>
>> [1] agricolae_1.2-1 asreml_3.0      lattice_0.20-31 ggplot2_1.0.1   dplyr_0.4.2     plyr_1.8.3
>>
>>
>>
>> loaded via a namespace (and not attached):
>>
>>  [1] spdep_0.5-88     Rcpp_0.12.1      cluster_2.0.2    magrittr_1.5     splines_3.2.1    MASS_7.3-41
>>
>>  [7] munsell_0.4.2    colorspace_1.2-6 R6_2.0.1         stringr_1.0.0    tools_3.2.1      parallel_3.2.1
>>
>> [13] grid_3.2.1       gtable_0.1.2     nlme_3.1-122     coda_0.17-1      DBI_0.3.1        deldir_0.1-9
>>
>> [19] lazyeval_0.1.10  assertthat_0.1   digest_0.6.8     Matrix_1.2-1     reshape2_1.4.1   sp_1.2-1
>>
>> [25] stringi_1.0-1    klaR_0.6-12      LearnBayes_2.15  scales_0.3.0     boot_1.3-17      combinat_0.0-8
>>
>> [31] proto_0.3-10
>>
>> Thanks.
>> Nilesh
>>
>


From dcarlson at tamu.edu  Tue Jan  5 19:23:08 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 5 Jan 2016 18:23:08 +0000
Subject: [R] Subsetting a square marix
In-Reply-To: <CAM_vju=ZTmhpsG1wvAWVdD86bQjn20KdyYDJr4-gZFVCkzSMVg@mail.gmail.com>
References: <CABOtOy+Q1_10YsrCDC=QikbKwF-yvG3tE=HzWJD1ZkOp=fpR8w@mail.gmail.com>
	<CAM_vju=ZTmhpsG1wvAWVdD86bQjn20KdyYDJr4-gZFVCkzSMVg@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6FA886@mb02.ads.tamu.edu>

Assuming I've reconstructed your data correctly:

> dta
    Sp1 Sp2 Sp3 Sp4 Sp5 Sp6
Sp1   0   1   0   0   0   0
Sp2   0   0   0   1   0   0
Sp3   1   0   0   0   1   0
Sp4   0   0   1   1   0   1
Sp5   0   1   0   0   0   1
Sp6   0   0   0   0   0   0
> dput(dta)
structure(list(Sp1 = c(0L, 0L, 1L, 0L, 0L, 0L), Sp2 = c(1L, 0L, 
0L, 0L, 1L, 0L), Sp3 = c(0L, 0L, 0L, 1L, 0L, 0L), Sp4 = c(0L, 
1L, 0L, 1L, 0L, 0L), Sp5 = c(0L, 0L, 1L, 0L, 0L, 0L), Sp6 = c(0L, 
0L, 0L, 1L, 1L, 0L)), .Names = c("Sp1", "Sp2", "Sp3", "Sp4", 
"Sp5", "Sp6"), class = "data.frame", row.names = c("Sp1", "Sp2", 
"Sp3", "Sp4", "Sp5", "Sp6"))

The results of dput(dta) is what you should include in your plain text email.

As for the subset, your email indicated: sp1, sp3 and spp. But none of these are labels in your data set since R is case sensitive. Try for example:

> sub <- c("Sp1", "Sp3", "Sp5") 
> dta[sub, sub]
    Sp1 Sp3 Sp5
Sp1   0   0   0
Sp3   1   0   1
Sp5   0   0   0

And definitely spend some time with the available free R tutorials so that you understand how R works.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sarah Goslee
Sent: Tuesday, January 5, 2016 10:15 AM
To: Tawanda Tarakini
Cc: r-help
Subject: Re: [R] Subsetting a square marix

It really isn't clear what you want, and posting in HTML has mangled
what you did provide.

Please use dput() to provide sample data, and give us a clear idea of
what you want, ideally an example of what the output should look like.
Adding the R code you've tried to use is also a good idea.

Sarah

On Tue, Jan 5, 2016 at 4:06 AM, Tawanda Tarakini
<tawandatizora at gmail.com> wrote:
> I have a global matrix (e.g. table below) of species feeding. I am trying
> to create specific matrix for specific sites. If for example a subset is to
> have sp1, sp3 and spp only these 3 species should be appearing in the
> subset (both column and rows).
>
> I have been checking online help but I seem not to get my scenario
>
>
>
> Sp1
>
> Sp2
>
> Sp3
>
> Sp4
>
> Sp5
>
> Sp6
>
> Sp1
>
> 0
>
> 0
>
> 1
>
> 0
>
> 0
>
> 0
>
> Sp2
>
> 1
>
> 0
>
> 0
>
> 0
>
> 1
>
> 0
>
> Sp3
>
> 0
>
> 0
>
> 0
>
> 1
>
> 0
>
> 0
>
> Sp4
>
> 0
>
> 1
>
> 0
>
> 1
>
> 0
>
> 0
>
> Sp5
>
> 0
>
> 0
>
> 1
>
> 0
>
> 0
>
> 0
>
> Sp6
>
> 0
>
> 0
>
> 0
>
> 1
>
> 1
>
> 0
>
> --
> Kind Regards
>
> Tawanda Tarakini
>
> Lecturer and Industrial attachment coordinator
> Department of Wildlife, Ecology and Conservation
> Chinhoyi University of Technology
> Bag 7724, Chinhoyi
> Cell: +263 775 321 722
> Alternative email: ttarakini at cut.ac.zw
>
>         [[alternative HTML version deleted]]
>


-- 
Sarah Goslee
http://www.numberwright.com

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From nilesh.dighe at monsanto.com  Tue Jan  5 20:12:43 2016
From: nilesh.dighe at monsanto.com (DIGHE, NILESH [AG/2362])
Date: Tue, 5 Jan 2016 19:12:43 +0000
Subject: [R] store results from loop into a dataframe
In-Reply-To: <CAM_vjukGG63zgr=brgEwoLRtOs4MHDEjE1FGGXS4sp72TA2Edw@mail.gmail.com>
References: <24156952D190E841BF8E66CB59FAB94A4870B346@STLWEXMBXPRD14.na.ds.monsanto.com>
	<CAM_vju=5HhiKqDcPiEiqg+D0k6yAcoPWfx9NJ_LMUWxbtXU6OA@mail.gmail.com>
	<24156952D190E841BF8E66CB59FAB94A4870C4E5@STLWEXMBXPRD14.na.ds.monsanto.com>
	<CAM_vjukGG63zgr=brgEwoLRtOs4MHDEjE1FGGXS4sp72TA2Edw@mail.gmail.com>
Message-ID: <24156952D190E841BF8E66CB59FAB94A4870C891@STLWEXMBXPRD14.na.ds.monsanto.com>

Sarah:  Thanks a lot for taking time to guide me to the right direction.  I now see how the missing data is causing the problem.
Thanks again!
Nilesh

-----Original Message-----
From: Sarah Goslee [mailto:sarah.goslee at gmail.com] 
Sent: Tuesday, January 05, 2016 12:13 PM
To: DIGHE, NILESH [AG/2362]
Cc: r-help at r-project.org
Subject: Re: [R] store results from loop into a dataframe

If you run each variable individually, you'll discover that the NAs in your data are causing problems. It's up to you to figure out what the best way to handle those missing values for your research is.

Sarah

On Tue, Jan 5, 2016 at 12:39 PM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com> wrote:
> Sarah: Thanks for pointing out the errors in my function.
>
> Below are the errors I am getting after I run the corrected quote:
> Error in if (s) { : missing value where TRUE/FALSE needed In addition: 
> Warning message:
> In qtukey(1 - alpha, ntr, DFerror) : NaNs produced
>
> You are right, I have no idea to handle these errors.
>
> Do you recommend any other approach to solve my problem?
>
> Thanks for your time.
> Nilesh
>
>
>
> -----Original Message-----
> From: Sarah Goslee [mailto:sarah.goslee at gmail.com]
> Sent: Tuesday, January 05, 2016 11:20 AM
> To: DIGHE, NILESH [AG/2362]
> Cc: r-help at r-project.org
> Subject: Re: [R] store results from loop into a dataframe
>
> Leaving aside the question of whether this is the best way to approach your problem (unlikely), there's a couple of errors in your code involving indexing. Once fixed, the code demonstrates some errors in your use of HSD.test that will be harder for you to deal with.
>
> Thanks for the complete reproducible example.
>
> fun2 <- function (x)
>
> {
>
>     trait_names <- c("yield", "lp", "lnth")
>
>     d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,
>
>         6))
>
>     for (i in trait_names) {
> # your formula has all the trait names, not the selected one
>         # mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD + PEDIGREE*FIELD + FIELD%in%REP")), data = x)
>         mod <- aov(formula(paste(i, "~ PEDIGREE + FIELD + 
> PEDIGREE*FIELD + FIELD%in%REP")), data = x)
>
>         out <- HSD.test(mod, "PEDIGREE", group = TRUE, console = 
> FALSE)
>
> # you're indexing by the trait name, instead of its position
>         # d[, i] <- out$means[, 1]
>         d[, which(trait_names == i)] <- out$means[, 1]
>
>     }
>
>     d
>
> }
>
> Sarah
>
> On Tue, Jan 5, 2016 at 11:48 AM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com> wrote:
>> Dear R users:
>>
>> I am trying to create a function that will loop over three dependent variables in my aov model, and then get the HSD.test for each variable.  I like to store the results from each loop in a data frame.
>>
>>
>>
>> When I run my function (funx) on my data (dat), results from only yield gets populated in all three columns of the dataframe.  I am not able to store the results for each variable in a dataframe. Any help will be highly appreciated.
>>
>>
>>
>>
>>
>>
>>
>> function (x)
>>
>> {
>>
>>     trait_names <- c("yield", "lp", "lnth")
>>
>>     d = data.frame(yield = rep(0, 6), lp = rep(0, 6), lnth = rep(0,
>>
>>         6))
>>
>>     for (i in trait_names) {
>>
>>         mod <- aov(formula(paste(trait_names, "~ PEDIGREE + FIELD + 
>> PEDIGREE*FIELD + FIELD%in%REP")),
>>
>>             data = x)
>>
>>         out <- HSD.test(mod, "PEDIGREE", group = TRUE, console =
>> FALSE)
>>
>>         d[, i] <- out$means[, 1]
>>
>>     }
>>
>>     d
>>
>> }
>>
>>
>> structure(list(FIELD = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 
>> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 
>> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 
>> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 
>> 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = 
>> c("FYLS", "HKI1", "KIS1", "LMLS", "SELS", "SGL1"), class = "factor"), 
>> REP = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 
>> 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 
>> 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 
>> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 
>> 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 
>> 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 
>> 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2", "3"), 
>> class = "factor"), PEDIGREE = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 
>> 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 2L, 
>> 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 2L, 
>> 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 2L, 
>> 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 1L, 
>> 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 1L, 1L, 
>> 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L), 
>> .Label = c("A", "B", "C", "D", "E", "F"), class = "factor"), yield = 
>> c(1003L, 923L, 1268L, 1226L, 1059L, 1150L, 900L, 816L, 1072L, 1158L, 
>> 1026L, 1299L, 1083L, 1038L, 1236L, 1287L, 1270L, 1612L, 1513L, 1676L, 
>> 1504L, 1417L, 1932L, 1644L, 1293L, 1542L, 1452L, 1180L, 1248L, 1764L, 
>> 1326L, 1877L, 1788L, 1606L, 1809L, 1791L, 2294L, 2315L, 2320L, 2083L, 
>> 1895L, 2284L, 2000L, 2380L, 1952L, 2414L, 2354L, 2095L, 2227L, 2093L, 
>> 2019L, 2505L, 2410L, 2287L, 2507L, 2507L, 2349L, 2162L, 2108L, 2319L, 
>> 2028L, 1947L, 2352L, 2698L, 2369L, 1798L, 2422L, 2509L, 2234L, 2451L, 
>> 2139L, 1957L, 799L, 787L, 701L, 781L, 808L, 582L, 770L, 752L, 801L, 
>> 865L, 608L, 620L, 677L, 775L, 722L, 1030L, 606L, 729L, 1638L, 1408L, 
>> 1045L, 1685L, 1109L, 1210L, 1419L, 1048L, 1129L, 1549L, 1325L, 1315L, 
>> 1838L, 1066L, 1295L, 1499L, 1472L, 1139L), lp = c(NA, NA, 46.31, NA, 
>> NA, 43.8, NA, NA, 43.91, NA, NA, 44.47, NA, NA, 45.16, NA, NA, 43.57, 
>> 40.65, NA, NA, 40.04, NA, NA, 41.33, NA, NA, 40.75, NA, NA, 42.04, 
>> NA, NA, 40.35, NA, NA, 43.682, NA, NA, 41.712, NA, NA, 42.566, NA, 
>> NA, 43.228, NA, NA, 43.63, NA, NA, 42.058, NA, NA, NA, 45.19, NA, NA, 
>> 41.91, NA, NA, 43.86, NA, NA, 44.48, NA, NA, 44.34, NA, NA, 43.03, 
>> NA, NA, NA, 44.08, NA, NA, 41.39, NA, NA, 42.48, NA, NA, 44.13, NA, 
>> NA, 43.39, NA, NA, 42.82, 42.18, NA, NA, 41.42, NA, NA, 41.25, NA, 
>> NA, 42.31, NA, NA, 43.22, NA, NA, 40.52, NA, NA), lnth = c(NA, NA, 
>> 1.151, NA, NA, 1.135, NA, NA, 1.109, NA, NA, 1.117, NA, NA, 1.107, 
>> NA, NA, 1.196, 1.255, NA, NA, 1.229, NA, NA, 1.158, NA, NA, 1.214, 
>> NA, NA, 1.152, NA, NA, 1.194, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1.2, NA, NA, 1.219, NA, 
>> NA, 1.115, NA, NA, 1.205, NA, NA, 1.238, NA, NA, 1.244, NA, NA, NA, 
>> 1.096, NA, NA, 1.021, NA, NA, 1.055, NA, NA, 1.058, NA, NA, 1.026, 
>> NA, NA, 1.115, 1.202, NA, NA, 1.161, NA, NA, 1.168, NA, NA, 1.189, 
>> NA, NA, 1.204, NA, NA, 1.277, NA, NA)), .Names = c("FIELD", "REP", 
>> "PEDIGREE", "yield", "lp", "lnth"
>> ), row.names = c(NA, -108L), class = "data.frame")
>>
>>
>>
>>
>>
>>
>> R version 3.2.1 (2015-06-18)
>>
>> Platform: i386-w64-mingw32/i386 (32-bit)
>>
>> Running under: Windows 7 x64 (build 7601) Service Pack 1
>>
>>
>>
>> locale:
>>
>> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
>> States.1252
>>
>> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>>
>> [5] LC_TIME=English_United States.1252
>>
>>
>>
>> attached base packages:
>>
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>
>>
>> other attached packages:
>>
>> [1] agricolae_1.2-1 asreml_3.0      lattice_0.20-31 ggplot2_1.0.1   dplyr_0.4.2     plyr_1.8.3
>>
>>
>>
>> loaded via a namespace (and not attached):
>>
>>  [1] spdep_0.5-88     Rcpp_0.12.1      cluster_2.0.2    magrittr_1.5     splines_3.2.1    MASS_7.3-41
>>
>>  [7] munsell_0.4.2    colorspace_1.2-6 R6_2.0.1         stringr_1.0.0    tools_3.2.1      parallel_3.2.1
>>
>> [13] grid_3.2.1       gtable_0.1.2     nlme_3.1-122     coda_0.17-1      DBI_0.3.1        deldir_0.1-9
>>
>> [19] lazyeval_0.1.10  assertthat_0.1   digest_0.6.8     Matrix_1.2-1     reshape2_1.4.1   sp_1.2-1
>>
>> [25] stringi_1.0-1    klaR_0.6-12      LearnBayes_2.15  scales_0.3.0     boot_1.3-17      combinat_0.0-8
>>
>> [31] proto_0.3-10
>>
>> Thanks.
>> Nilesh
>>
>
This e-mail message may contain privileged and/or confidential information, and is intended to be received only by persons entitled
to receive such information. If you have received this e-mail in error, please notify the sender immediately. Please delete it and
all attachments from any servers, hard drives or any other media. Other use of this e-mail by you is strictly prohibited.

All e-mails and attachments sent and received are subject to monitoring, reading and archival by Monsanto, including its
subsidiaries. The recipient of this e-mail is solely responsible for checking for the presence of "Viruses" or other "Malware".
Monsanto, along with its subsidiaries, accepts no liability for any damage caused by any such code transmitted by or accompanying
this e-mail or any attachment.


The information contained in this email may be subject to the export control laws and regulations of the United States, potentially
including but not limited to the Export Administration Regulations (EAR) and sanctions regulations issued by the U.S. Department of
Treasury, Office of Foreign Asset Controls (OFAC).  As a recipient of this information you are obligated to comply with all
applicable U.S. export laws and regulations.

From auerbecktj at gmail.com  Tue Jan  5 20:15:20 2016
From: auerbecktj at gmail.com (Tyler Auerbeck)
Date: Tue, 5 Jan 2016 14:15:20 -0500
Subject: [R] R package built using newer version of R
In-Reply-To: <000201d14794$d5e8c9f0$81ba5dd0$@eyequestion.nl>
References: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>
	<568AC4E8.2000302@gmail.com>
	<000201d14794$d5e8c9f0$81ba5dd0$@eyequestion.nl>
Message-ID: <CAETY7qOKZF6MwmBDOVPkXS+dXaBuF8OoP=wFJusvDAZcDPXmzA@mail.gmail.com>

When I run the install.packages("rj",type="source") I get the following:

> install.packages("rj",type="source")
Warning message:
package ?rj? is not available (for R version 2.15.1)

I believe this is because this is a package available directly from the
creators of StatET. I tried pulling the zip down directly from their
website and ran the following:

>
install.packages("C:\\users\\admin\\Downloads\\rj_2.0.3-1.zip",type="source",repos=NULL)
package 'rj' successfully unpacked and MD5 sums checked

This installs it directly, but it still installs it as compiled for 2.15.3,
which we see the same warning I originally mentioned.

Here is the sessionInfo() you asked for:

> sessionInfo()
R version 2.15.1 (2012-06-22)
Platform: x86_64-pc-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United
States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_2.15.1


If there isn't a good way to compile this for 2.15.1, is there any way to
just ignore the warning? I've seen that you can do something like

options( warn = -1 )

I know this isn't recommended to do on an extended timeframe, but this
message only occurs during the first command that you run. Even if we
could set up some sort of profile that would set this suppression, run
a dummy command and then unset the suppression. I know this is a
workaround, but I just wasn't sure what would be the simpler solution.

Let me know what you think or what I may be missing.

On Tue, Jan 5, 2016 at 3:41 AM, Harrie Robins <harrie at eyequestion.nl> wrote:

> If that fails (sometimes R gives a version error, package not available for
> R version X.X.X), you could try downloading the source package
> (package.tar.gz) and compile it with running from console (or prompt):
>
> R CMD INSTALL packagename.tar.gz library-location
>
> Regards,
>
> Harrie
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Duncan
> Murdoch
> Sent: maandag 4 januari 2016 20:16
> To: Tyler Auerbeck <auerbecktj at gmail.com>; r-help at r-project.org
> Subject: Re: [R] R package built using newer version of R
>
> On 04/01/2016 2:02 PM, Tyler Auerbeck wrote:
> > We're currently looking at using the R eclipse plugin StatET as our
> > development environment. Due to certain requirements, we're still
> > using 2.15.1. However a required package of StatET was built using
> > 2.15.3, which results in the following warning:
> >
> > Warning message:
> > package 'rj' was built under R version 2.15.3
> >
> > I'm still fairly new to R, but is there any way for us to rebuild this
> > package using 2.15.1? It doesn't appear to cause us any issues, but
> > it's still not desirable for users to see that warning.
> >
> > Any help would be appreciated.
>
> Yes, it's quite easy to do so.  StatET probably gives menu options to do
> it,
> but I don't know them:  you might want to ask them.  From the R console,
> try
>
> install.packages("pkgname", type="source")
>
> and if you have the necessary prerequisites (e.g. compilers), you'll get
> it installed from source.   If it fails, post the errors and the results
> of sessionInfo() here, and we'll probably be able to tell you what to do
> next.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Tue Jan  5 20:21:51 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 05 Jan 2016 13:21:51 -0600
Subject: [R] R package built using newer version of R
In-Reply-To: <CAETY7qOKZF6MwmBDOVPkXS+dXaBuF8OoP=wFJusvDAZcDPXmzA@mail.gmail.com>
References: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>
	<568AC4E8.2000302@gmail.com>
	<000201d14794$d5e8c9f0$81ba5dd0$@eyequestion.nl>
	<CAETY7qOKZF6MwmBDOVPkXS+dXaBuF8OoP=wFJusvDAZcDPXmzA@mail.gmail.com>
Message-ID: <AA97AD48-B94A-44AE-84E2-91B1BDC8412D@me.com>

Hi,

You appear to have downloaded and attempted to install the '.zip' version of the package, which is the pre-built Windows **binary** version of the package.

As Harrie noted below, you want to download the '.tar.gz' version of the package, which is the "source" version.

Regards,

Marc Schwartz


> On Jan 5, 2016, at 1:15 PM, Tyler Auerbeck <auerbecktj at gmail.com> wrote:
> 
> When I run the install.packages("rj",type="source") I get the following:
> 
>> install.packages("rj",type="source")
> Warning message:
> package ?rj? is not available (for R version 2.15.1)
> 
> I believe this is because this is a package available directly from the
> creators of StatET. I tried pulling the zip down directly from their
> website and ran the following:
> 
>> 
> install.packages("C:\\users\\admin\\Downloads\\rj_2.0.3-1.zip",type="source",repos=NULL)
> package 'rj' successfully unpacked and MD5 sums checked
> 
> This installs it directly, but it still installs it as compiled for 2.15.3,
> which we see the same warning I originally mentioned.
> 
> Here is the sessionInfo() you asked for:
> 
>> sessionInfo()
> R version 2.15.1 (2012-06-22)
> Platform: x86_64-pc-mingw32/x64 (64-bit)
> 
> locale:
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
> States.1252    LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C                           LC_TIME=English_United
> States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] tools_2.15.1
> 
> 
> If there isn't a good way to compile this for 2.15.1, is there any way to
> just ignore the warning? I've seen that you can do something like
> 
> options( warn = -1 )
> 
> I know this isn't recommended to do on an extended timeframe, but this
> message only occurs during the first command that you run. Even if we
> could set up some sort of profile that would set this suppression, run
> a dummy command and then unset the suppression. I know this is a
> workaround, but I just wasn't sure what would be the simpler solution.
> 
> Let me know what you think or what I may be missing.
> 
> On Tue, Jan 5, 2016 at 3:41 AM, Harrie Robins <harrie at eyequestion.nl> wrote:
> 
>> If that fails (sometimes R gives a version error, package not available for
>> R version X.X.X), you could try downloading the source package
>> (package.tar.gz) and compile it with running from console (or prompt):
>> 
>> R CMD INSTALL packagename.tar.gz library-location
>> 
>> Regards,
>> 
>> Harrie
>> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Duncan
>> Murdoch
>> Sent: maandag 4 januari 2016 20:16
>> To: Tyler Auerbeck <auerbecktj at gmail.com>; r-help at r-project.org
>> Subject: Re: [R] R package built using newer version of R
>> 
>> On 04/01/2016 2:02 PM, Tyler Auerbeck wrote:
>>> We're currently looking at using the R eclipse plugin StatET as our
>>> development environment. Due to certain requirements, we're still
>>> using 2.15.1. However a required package of StatET was built using
>>> 2.15.3, which results in the following warning:
>>> 
>>> Warning message:
>>> package 'rj' was built under R version 2.15.3
>>> 
>>> I'm still fairly new to R, but is there any way for us to rebuild this
>>> package using 2.15.1? It doesn't appear to cause us any issues, but
>>> it's still not desirable for users to see that warning.
>>> 
>>> Any help would be appreciated.
>> 
>> Yes, it's quite easy to do so.  StatET probably gives menu options to do
>> it,
>> but I don't know them:  you might want to ask them.  From the R console,
>> try
>> 
>> install.packages("pkgname", type="source")
>> 
>> and if you have the necessary prerequisites (e.g. compilers), you'll get
>> it installed from source.   If it fails, post the errors and the results
>> of sessionInfo() here, and we'll probably be able to tell you what to do
>> next.
>> 
>> Duncan Murdoch


From dwinsemius at comcast.net  Tue Jan  5 20:34:23 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 5 Jan 2016 11:34:23 -0800
Subject: [R] R package built using newer version of R
In-Reply-To: <CAETY7qOKZF6MwmBDOVPkXS+dXaBuF8OoP=wFJusvDAZcDPXmzA@mail.gmail.com>
References: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>
	<568AC4E8.2000302@gmail.com>
	<000201d14794$d5e8c9f0$81ba5dd0$@eyequestion.nl>
	<CAETY7qOKZF6MwmBDOVPkXS+dXaBuF8OoP=wFJusvDAZcDPXmzA@mail.gmail.com>
Message-ID: <CEB4FEC3-3FEA-436F-8D07-DBF7937DC36A@comcast.net>


> On Jan 5, 2016, at 11:15 AM, Tyler Auerbeck <auerbecktj at gmail.com> wrote:
> 
> When I run the install.packages("rj",type="source") I get the following:
> 
>> install.packages("rj",type="source")
> Warning message:
> package ?rj? is not available (for R version 2.15.1)
> 
> I believe this is because this is a package available directly from the
> creators of StatET. I tried pulling the zip down directly from their
> website and ran the following:
> 
>> 
> install.packages("C:\\users\\admin\\Downloads\\rj_2.0.3-1.zip",type="source",repos=NULL)
> package 'rj' successfully unpacked and MD5 sums checked

Despite the your "source" for the type parameter, you still gave it a Windows binary file. This is the place to get a source version of rj-2.0.3-1:

http://download.walware.de/rj-2.0/src/contrib/rj_2.0.3-2.tar.gz

-- 
David.


> 
> This installs it directly, but it still installs it as compiled for 2.15.3,
> which we see the same warning I originally mentioned.
> 
> Here is the sessionInfo() you asked for:
> 
>> sessionInfo()
> R version 2.15.1 (2012-06-22)
> Platform: x86_64-pc-mingw32/x64 (64-bit)
> 
> locale:
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
> States.1252    LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C                           LC_TIME=English_United
> States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] tools_2.15.1
> 
> 
> If there isn't a good way to compile this for 2.15.1, is there any way to
> just ignore the warning? I've seen that you can do something like
> 
> options( warn = -1 )
> 
> I know this isn't recommended to do on an extended timeframe, but this
> message only occurs during the first command that you run. Even if we
> could set up some sort of profile that would set this suppression, run
> a dummy command and then unset the suppression. I know this is a
> workaround, but I just wasn't sure what would be the simpler solution.
> 
> Let me know what you think or what I may be missing.
> 
> On Tue, Jan 5, 2016 at 3:41 AM, Harrie Robins <harrie at eyequestion.nl> wrote:
> 
>> If that fails (sometimes R gives a version error, package not available for
>> R version X.X.X), you could try downloading the source package
>> (package.tar.gz) and compile it with running from console (or prompt):
>> 
>> R CMD INSTALL packagename.tar.gz library-location
>> 
>> Regards,
>> 
>> Harrie
>> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Duncan
>> Murdoch
>> Sent: maandag 4 januari 2016 20:16
>> To: Tyler Auerbeck <auerbecktj at gmail.com>; r-help at r-project.org
>> Subject: Re: [R] R package built using newer version of R
>> 
>> On 04/01/2016 2:02 PM, Tyler Auerbeck wrote:
>>> We're currently looking at using the R eclipse plugin StatET as our
>>> development environment. Due to certain requirements, we're still
>>> using 2.15.1. However a required package of StatET was built using
>>> 2.15.3, which results in the following warning:
>>> 
>>> Warning message:
>>> package 'rj' was built under R version 2.15.3
>>> 
>>> I'm still fairly new to R, but is there any way for us to rebuild this
>>> package using 2.15.1? It doesn't appear to cause us any issues, but
>>> it's still not desirable for users to see that warning.
>>> 
>>> Any help would be appreciated.
>> 
>> Yes, it's quite easy to do so.  StatET probably gives menu options to do
>> it,
>> but I don't know them:  you might want to ask them.  From the R console,
>> try
>> 
>> install.packages("pkgname", type="source")
>> 
>> and if you have the necessary prerequisites (e.g. compilers), you'll get
>> it installed from source.   If it fails, post the errors and the results
>> of sessionInfo() here, and we'll probably be able to tell you what to do
>> next.
>> 
>> Duncan Murdoch
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dimitri.liakhovitski at gmail.com  Tue Jan  5 23:15:08 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 5 Jan 2016 17:15:08 -0500
Subject: [R] (no subject)
In-Reply-To: <CAN5YmCEm1OSz0CJsBfu+zVENJpqBe9YxP2T8xMQPTQqct2PK0w@mail.gmail.com>
References: <CAN2xGJZ7y_8=ow+-kGfWg-gSyBafOy1HJUiw8mbhyoK5MX5D8w@mail.gmail.com>
	<CAN5YmCEm1OSz0CJsBfu+zVENJpqBe9YxP2T8xMQPTQqct2PK0w@mail.gmail.com>
Message-ID: <CAN2xGJYko7BNsHsL8_5hgngOwLhEr5BQTLcdMUDWvsybsbowpg@mail.gmail.com>

Jean and Dennis, thank you so much for your solutions!
They are super-fast, thanks a lot!
Happy new year!
Dimitri

On Thu, Dec 24, 2015 at 2:50 PM, Adams, Jean <jvadams at usgs.gov> wrote:
> Excellent job providing example data and a desired result.
>
> This code works on the example you provided.  Hope it helps.
>
> Jean
>
> # reshape the info data frame
> library(tidyr)
> info2 <- gather(myinfo, set, val, -game)
> info2$set <- as.numeric(gsub("[[:alpha:]]", "", info2$set))
>
> # add a new column to the x data frame
> y <- t(x[,grep("char", names(x))])
> newx <- x
> newx$char <- row(y)[y==1]
>
> # merge and define winner
> res <- merge(newx, info2)
> res$winner <- with(res, ifelse(char==val, 1, 0))
> res
>
>
> On Wed, Dec 23, 2015 at 3:35 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>>
>> Merry upcoming Christmas - for those of us who celebrate it!
>>
>> # I have data frame x.
>> x <- data.frame(game = c(rep(1, 4), rep(2, 4)), set = rep(c(1,1,2,2),
>> 2), player = rep(1:2, 4),
>>                 char1 = c(1,0,0,0,0,0,0,1), char2 =
>> c(0,0,1,0,0,1,0,0), char3 = c(0,1,0,1,0,0,0,0),
>>                 char4 = c(0,0,0,0,1,0,1,0))
>> x
>> # There are several games (2 here). Each game has several sets (2
>> here. In each set participate
>> # several players (2 here).
>> # Each player possesses 1 or 4 possible characteristics.
>> # For example, in game 1, set 1, player 1 has characteristic 1 and player
>> 2 -
>> # characteristic 3
>>
>> # I also have data frame myinfo:
>> (myinfo <- data.frame(game = 1:2, set1 = c(3, 4), set2 = c(2, 1)))
>> # It tells me:
>> # in game 1, set 1 the winner was the player with characteristic 3
>> # in game 1, set 2 the winner was the player with characteristic 2, etc.
>>
>> # I need to merge the 2 to produce the result below.
>> # I just need an additional column that - for each game and each set -
>> # has a 1 in the row of the player who had the winning characteristic
>> # (as per myinfo) and has a 0 otherwise.
>>
>> result <- x
>> result$winner <- c(0, 1, 1, 0, 1, 0, 0, 1)
>> result
>>
>> # I have written a long loop that loops through each set of each game,
>> identifies
>> # which characteristic wins in myinfo, and puts a 1 against the winning
>> row.
>> # But it's taking forever. Could it be done faster? Thanks a lot!
>>
>> # Important: In my real game the number of players could be more than
>> 2 and so can
>> # the number of games and the number of sets per game.
>> # However, we can assume that the number of sets per game is always the
>> same,
>> # and the number of players per set is always the same.
>>
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>



-- 
Dimitri Liakhovitski


From auerbecktj at gmail.com  Wed Jan  6 07:34:23 2016
From: auerbecktj at gmail.com (Tyler Auerbeck)
Date: Wed, 6 Jan 2016 01:34:23 -0500
Subject: [R] R package built using newer version of R
In-Reply-To: <CEB4FEC3-3FEA-436F-8D07-DBF7937DC36A@comcast.net>
References: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>
	<568AC4E8.2000302@gmail.com>
	<000201d14794$d5e8c9f0$81ba5dd0$@eyequestion.nl>
	<CAETY7qOKZF6MwmBDOVPkXS+dXaBuF8OoP=wFJusvDAZcDPXmzA@mail.gmail.com>
	<CEB4FEC3-3FEA-436F-8D07-DBF7937DC36A@comcast.net>
Message-ID: <CAETY7qOMC6kKtSL9-RAtDijX8LoYwyMYQeWjJ3fSu8RsYO9RHA@mail.gmail.com>

Alright, I believe I'm making some progress. I'm now running into the
following error. As I mentioned, I'm new to this whole process, so this may
be something simple. When I run the following:

R.exe CMD INSTALL rj_2.0.3-1.tar.gz

I get the following error:

C:\users\admin\> R.exe CMD INSTALL rj_2.0.3-1.tar.gz
* installing to library 'C:/Program Files/R/R-2.15.1/library'
* installing *source* package 'rj' ...
/bin/sh: h.exe: No such file or directory
ERROR: configuration failed for package 'rj'

In case I hadn't mentioned, I am attempting to do all of this on windows. I
had looked around a little more and noticed that there is a lot of mention
of installing Rtools on the machine as well and making sure certain bin
directories are added to the path. I've ensured the following directories
were added to my path:

C:\Program Files\R\R-2.15.1\bin\x64;
C:\Program Files\R\Rtools\bin
;C:\Program Files\R\Rtools\gcc-4.6.3\bin;
C:\Program Files\R\Rtools\gcc-4.6.3\bin64;
C:\Program Files\R\Rtools\gcc-4.6.3\i686-w64-mingw32\bin

Is there anything that I may be missing that is required. I see the main
problem is that it can't seem to locate h.exe. Is there something I need to
pull down that would provide this? Do I need to add something to my path in
order for the install to find this?

As always, any help would be greatly appreciated.

On Tue, Jan 5, 2016 at 2:34 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jan 5, 2016, at 11:15 AM, Tyler Auerbeck <auerbecktj at gmail.com>
> wrote:
> >
> > When I run the install.packages("rj",type="source") I get the following:
> >
> >> install.packages("rj",type="source")
> > Warning message:
> > package ?rj? is not available (for R version 2.15.1)
> >
> > I believe this is because this is a package available directly from the
> > creators of StatET. I tried pulling the zip down directly from their
> > website and ran the following:
> >
> >>
> >
> install.packages("C:\\users\\admin\\Downloads\\rj_2.0.3-1.zip",type="source",repos=NULL)
> > package 'rj' successfully unpacked and MD5 sums checked
>
> Despite the your "source" for the type parameter, you still gave it a
> Windows binary file. This is the place to get a source version of
> rj-2.0.3-1:
>
> http://download.walware.de/rj-2.0/src/contrib/rj_2.0.3-2.tar.gz
>
> --
> David.
>
>
> >
> > This installs it directly, but it still installs it as compiled for
> 2.15.3,
> > which we see the same warning I originally mentioned.
> >
> > Here is the sessionInfo() you asked for:
> >
> >> sessionInfo()
> > R version 2.15.1 (2012-06-22)
> > Platform: x86_64-pc-mingw32/x64 (64-bit)
> >
> > locale:
> > [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
> > States.1252    LC_MONETARY=English_United States.1252
> > [4] LC_NUMERIC=C                           LC_TIME=English_United
> > States.1252
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > loaded via a namespace (and not attached):
> > [1] tools_2.15.1
> >
> >
> > If there isn't a good way to compile this for 2.15.1, is there any way to
> > just ignore the warning? I've seen that you can do something like
> >
> > options( warn = -1 )
> >
> > I know this isn't recommended to do on an extended timeframe, but this
> > message only occurs during the first command that you run. Even if we
> > could set up some sort of profile that would set this suppression, run
> > a dummy command and then unset the suppression. I know this is a
> > workaround, but I just wasn't sure what would be the simpler solution.
> >
> > Let me know what you think or what I may be missing.
> >
> > On Tue, Jan 5, 2016 at 3:41 AM, Harrie Robins <harrie at eyequestion.nl>
> wrote:
> >
> >> If that fails (sometimes R gives a version error, package not available
> for
> >> R version X.X.X), you could try downloading the source package
> >> (package.tar.gz) and compile it with running from console (or prompt):
> >>
> >> R CMD INSTALL packagename.tar.gz library-location
> >>
> >> Regards,
> >>
> >> Harrie
> >>
> >> -----Original Message-----
> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Duncan
> >> Murdoch
> >> Sent: maandag 4 januari 2016 20:16
> >> To: Tyler Auerbeck <auerbecktj at gmail.com>; r-help at r-project.org
> >> Subject: Re: [R] R package built using newer version of R
> >>
> >> On 04/01/2016 2:02 PM, Tyler Auerbeck wrote:
> >>> We're currently looking at using the R eclipse plugin StatET as our
> >>> development environment. Due to certain requirements, we're still
> >>> using 2.15.1. However a required package of StatET was built using
> >>> 2.15.3, which results in the following warning:
> >>>
> >>> Warning message:
> >>> package 'rj' was built under R version 2.15.3
> >>>
> >>> I'm still fairly new to R, but is there any way for us to rebuild this
> >>> package using 2.15.1? It doesn't appear to cause us any issues, but
> >>> it's still not desirable for users to see that warning.
> >>>
> >>> Any help would be appreciated.
> >>
> >> Yes, it's quite easy to do so.  StatET probably gives menu options to do
> >> it,
> >> but I don't know them:  you might want to ask them.  From the R console,
> >> try
> >>
> >> install.packages("pkgname", type="source")
> >>
> >> and if you have the necessary prerequisites (e.g. compilers), you'll get
> >> it installed from source.   If it fails, post the errors and the results
> >> of sessionInfo() here, and we'll probably be able to tell you what to do
> >> next.
> >>
> >> Duncan Murdoch
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From auerbecktj at gmail.com  Wed Jan  6 08:13:20 2016
From: auerbecktj at gmail.com (Tyler Auerbeck)
Date: Wed, 6 Jan 2016 02:13:20 -0500
Subject: [R] R package built using newer version of R
In-Reply-To: <CAETY7qOMC6kKtSL9-RAtDijX8LoYwyMYQeWjJ3fSu8RsYO9RHA@mail.gmail.com>
References: <CAETY7qPxze46Ur6EkVbASOAJANdCttnYFL=wiirE3iRJ_H9LEw@mail.gmail.com>
	<568AC4E8.2000302@gmail.com>
	<000201d14794$d5e8c9f0$81ba5dd0$@eyequestion.nl>
	<CAETY7qOKZF6MwmBDOVPkXS+dXaBuF8OoP=wFJusvDAZcDPXmzA@mail.gmail.com>
	<CEB4FEC3-3FEA-436F-8D07-DBF7937DC36A@comcast.net>
	<CAETY7qOMC6kKtSL9-RAtDijX8LoYwyMYQeWjJ3fSu8RsYO9RHA@mail.gmail.com>
Message-ID: <CAETY7qO=5bDDvsknB-OAa12K67kXsgzSr7Cju2rU7pGkBbG=3Q@mail.gmail.com>

We can go ahead and ignore that last email. It looks like I had just
configured Rtools incorrectly. Once I resolved that issue I was able to get
this compiled appropriately. Thanks to everyone for the help!

On Wed, Jan 6, 2016 at 1:34 AM, Tyler Auerbeck <auerbecktj at gmail.com> wrote:

> Alright, I believe I'm making some progress. I'm now running into the
> following error. As I mentioned, I'm new to this whole process, so this may
> be something simple. When I run the following:
>
> R.exe CMD INSTALL rj_2.0.3-1.tar.gz
>
> I get the following error:
>
> C:\users\admin\> R.exe CMD INSTALL rj_2.0.3-1.tar.gz
> * installing to library 'C:/Program Files/R/R-2.15.1/library'
> * installing *source* package 'rj' ...
> /bin/sh: h.exe: No such file or directory
> ERROR: configuration failed for package 'rj'
>
> In case I hadn't mentioned, I am attempting to do all of this on windows.
> I had looked around a little more and noticed that there is a lot of
> mention of installing Rtools on the machine as well and making sure certain
> bin directories are added to the path. I've ensured the following
> directories were added to my path:
>
> C:\Program Files\R\R-2.15.1\bin\x64;
> C:\Program Files\R\Rtools\bin
> ;C:\Program Files\R\Rtools\gcc-4.6.3\bin;
> C:\Program Files\R\Rtools\gcc-4.6.3\bin64;
> C:\Program Files\R\Rtools\gcc-4.6.3\i686-w64-mingw32\bin
>
> Is there anything that I may be missing that is required. I see the main
> problem is that it can't seem to locate h.exe. Is there something I need to
> pull down that would provide this? Do I need to add something to my path in
> order for the install to find this?
>
> As always, any help would be greatly appreciated.
>
> On Tue, Jan 5, 2016 at 2:34 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>>
>> > On Jan 5, 2016, at 11:15 AM, Tyler Auerbeck <auerbecktj at gmail.com>
>> wrote:
>> >
>> > When I run the install.packages("rj",type="source") I get the following:
>> >
>> >> install.packages("rj",type="source")
>> > Warning message:
>> > package ?rj? is not available (for R version 2.15.1)
>> >
>> > I believe this is because this is a package available directly from the
>> > creators of StatET. I tried pulling the zip down directly from their
>> > website and ran the following:
>> >
>> >>
>> >
>> install.packages("C:\\users\\admin\\Downloads\\rj_2.0.3-1.zip",type="source",repos=NULL)
>> > package 'rj' successfully unpacked and MD5 sums checked
>>
>> Despite the your "source" for the type parameter, you still gave it a
>> Windows binary file. This is the place to get a source version of
>> rj-2.0.3-1:
>>
>> http://download.walware.de/rj-2.0/src/contrib/rj_2.0.3-2.tar.gz
>>
>> --
>> David.
>>
>>
>> >
>> > This installs it directly, but it still installs it as compiled for
>> 2.15.3,
>> > which we see the same warning I originally mentioned.
>> >
>> > Here is the sessionInfo() you asked for:
>> >
>> >> sessionInfo()
>> > R version 2.15.1 (2012-06-22)
>> > Platform: x86_64-pc-mingw32/x64 (64-bit)
>> >
>> > locale:
>> > [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
>> > States.1252    LC_MONETARY=English_United States.1252
>> > [4] LC_NUMERIC=C                           LC_TIME=English_United
>> > States.1252
>> >
>> > attached base packages:
>> > [1] stats     graphics  grDevices utils     datasets  methods   base
>> >
>> > loaded via a namespace (and not attached):
>> > [1] tools_2.15.1
>> >
>> >
>> > If there isn't a good way to compile this for 2.15.1, is there any way
>> to
>> > just ignore the warning? I've seen that you can do something like
>> >
>> > options( warn = -1 )
>> >
>> > I know this isn't recommended to do on an extended timeframe, but this
>> > message only occurs during the first command that you run. Even if we
>> > could set up some sort of profile that would set this suppression, run
>> > a dummy command and then unset the suppression. I know this is a
>> > workaround, but I just wasn't sure what would be the simpler solution.
>> >
>> > Let me know what you think or what I may be missing.
>> >
>> > On Tue, Jan 5, 2016 at 3:41 AM, Harrie Robins <harrie at eyequestion.nl>
>> wrote:
>> >
>> >> If that fails (sometimes R gives a version error, package not
>> available for
>> >> R version X.X.X), you could try downloading the source package
>> >> (package.tar.gz) and compile it with running from console (or prompt):
>> >>
>> >> R CMD INSTALL packagename.tar.gz library-location
>> >>
>> >> Regards,
>> >>
>> >> Harrie
>> >>
>> >> -----Original Message-----
>> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Duncan
>> >> Murdoch
>> >> Sent: maandag 4 januari 2016 20:16
>> >> To: Tyler Auerbeck <auerbecktj at gmail.com>; r-help at r-project.org
>> >> Subject: Re: [R] R package built using newer version of R
>> >>
>> >> On 04/01/2016 2:02 PM, Tyler Auerbeck wrote:
>> >>> We're currently looking at using the R eclipse plugin StatET as our
>> >>> development environment. Due to certain requirements, we're still
>> >>> using 2.15.1. However a required package of StatET was built using
>> >>> 2.15.3, which results in the following warning:
>> >>>
>> >>> Warning message:
>> >>> package 'rj' was built under R version 2.15.3
>> >>>
>> >>> I'm still fairly new to R, but is there any way for us to rebuild this
>> >>> package using 2.15.1? It doesn't appear to cause us any issues, but
>> >>> it's still not desirable for users to see that warning.
>> >>>
>> >>> Any help would be appreciated.
>> >>
>> >> Yes, it's quite easy to do so.  StatET probably gives menu options to
>> do
>> >> it,
>> >> but I don't know them:  you might want to ask them.  From the R
>> console,
>> >> try
>> >>
>> >> install.packages("pkgname", type="source")
>> >>
>> >> and if you have the necessary prerequisites (e.g. compilers), you'll
>> get
>> >> it installed from source.   If it fails, post the errors and the
>> results
>> >> of sessionInfo() here, and we'll probably be able to tell you what to
>> do
>> >> next.
>> >>
>> >> Duncan Murdoch
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >>
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>>
>

	[[alternative HTML version deleted]]


From dusa.adrian at unibuc.ro  Wed Jan  6 08:56:35 2016
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 6 Jan 2016 09:56:35 +0200
Subject: [R] Bezier to line segments
Message-ID: <CAJ=0CtCUh35L3XjFdqBoc7y+epzLzVo-76Y3aPmmkETS7zu0qA@mail.gmail.com>

Dear All,

I am interested into transforming Bezier curves (or general splines) to a
series of line segments.
For simplicity, the Bezier curves are either cubic (arches, no inflection
points) or they have at most one inflection point.

The entry parameters are exactly four points (with x and y coordinates):
- start point
- end point
- and two control points to define the curve.

I read a lot about parabolic approximation, and there is also a famous
deCasteljau algorithm.

Before attempting to create my own function, I wonder if something like
this already exists in R (or can easily be adapted to R).

Thanks in advance for any hint,
Adrian

-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From Bettina.Gruen at jku.at  Tue Jan  5 14:43:57 2016
From: Bettina.Gruen at jku.at (Bettina Gruen)
Date: Tue, 5 Jan 2016 14:43:57 +0100
Subject: [R] The R Journal, Volume 7, Issue 2
Message-ID: <568BC89D.6090909@jku.at>

Dear all,

The latest issue of The R Journal is now available at
http://journal.r-project.org/archive/2015-2/

Many thanks to all contributors.

Regards,
Bettina

-- 
-------------------------------------------------------------------
Bettina Gr?n
Department of Applied Statistics

JOHANNES KEPLER
UNIVERSITY LINZ
Altenbergerstra?e 69
Science Park 3, 627
4040 Linz, Austria
P +43 732 2468 6829
F +43 732 2468 6800
Bettina.Gruen at jku.at
www.jku.at

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce

From dusa.adrian at unibuc.ro  Wed Jan  6 14:43:33 2016
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 6 Jan 2016 15:43:33 +0200
Subject: [R] Bezier to line segments
In-Reply-To: <CAJ=0CtCUh35L3XjFdqBoc7y+epzLzVo-76Y3aPmmkETS7zu0qA@mail.gmail.com>
References: <CAJ=0CtCUh35L3XjFdqBoc7y+epzLzVo-76Y3aPmmkETS7zu0qA@mail.gmail.com>
Message-ID: <CAJ=0CtA99P=3ESdrBFVEVKwT94nqQK9GmWFx8UxX2vBNa3D1Ug@mail.gmail.com>

I just found the package "bezier".
Trying to find the needle, I missed the haystack...

On Wed, Jan 6, 2016 at 9:56 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:

> Dear All,
>
> I am interested into transforming Bezier curves (or general splines) to a
> series of line segments.
> For simplicity, the Bezier curves are either cubic (arches, no inflection
> points) or they have at most one inflection point.
>
> The entry parameters are exactly four points (with x and y coordinates):
> - start point
> - end point
> - and two control points to define the curve.
>
> I read a lot about parabolic approximation, and there is also a famous
> deCasteljau algorithm.
>
> Before attempting to create my own function, I wonder if something like
> this already exists in R (or can easily be adapted to R).
>
> Thanks in advance for any hint,
> Adrian
>
> --
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr.90
> 050663 Bucharest sector 5
> Romania
>



-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From riabchenko.kateryna at gmail.com  Wed Jan  6 14:02:52 2016
From: riabchenko.kateryna at gmail.com (Kateryna Riabchenko)
Date: Wed, 6 Jan 2016 14:02:52 +0100
Subject: [R] HELP - Hausman Test + systemfit
Message-ID: <CACzc+AFsBszzSt5A2R+w7cOqc9-NuvuPpyB0rkidBb+PzEvfkA@mail.gmail.com>

Hello, I am not advanced in R (that is why my questions can sound stupid).
I apologize for that in advance. But that is how my brain works - I need to
ask questions to understand. I was searching the answer everywhere  -
without result. So I am asking You.

*Given:*

I have 8 regression Models:
Model1 <- lrm(ACINTENSITY ~ GDPGROWTH, data = ACU)
Model2<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION, data = ACU)
Model3 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION, data = ACU)
Model4<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION + CENTRALBANK,
data = ACU)
Model5 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION + CENTRALBANK
+ CPI, data = ACU)
Model6 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION + CENTRALBANK
+ CPI + INTERATE, data = ACU)
Model7<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION + CENTRALBANK
+ CPI + INTERATE + UNEMPL, data = ACU)
Model8 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION + CENTRALBANK
+ CPI + INTERATE + UNEMPL + INTERNET, data = ACU)
As you can see, each time a new explanatory variable is added.
I want to perform the Hausman Test
for that I installed systemfit package
My idea was to compare Model1 with Model2, Model2 with model3, ... and so on

*I started with this*:
inst <- ~ POPULATION  - *this is Question1:* I don't know what I need to
mention in *"inst"*. I put additional variable in Model2 which does not
exist in Model1, but I ma not sure that is correct! Or maybe I need to put
all variables, which were used in 2 models. if You can explain - thank you!
system <- list(Model1, Model2)

# perform the estimations
fit2sls <- systemfit(system, "2SLS", inst = inst, data = ACU)

but R responded:

Error in systemfit(system, "2SLS", inst = inst, data = ACU) :
  the list of argument 'formula' must contain only objects of class 'formula'



Please, help me to understand What I do wrong!
Best,
Kateryna

	[[alternative HTML version deleted]]


From wornimatthias at gmail.com  Wed Jan  6 11:28:31 2016
From: wornimatthias at gmail.com (Matthias Worni)
Date: Wed, 6 Jan 2016 11:28:31 +0100
Subject: [R] hello everyone and happy new year
Message-ID: <CAM0cwgXy+XufrrUCZaS=uvONF7CUUPDXQ-FdxK21TgVtd67sjQ@mail.gmail.com>

I got the following problem in R studio. Im trying to make a plot of
different time series using different colours, which so far worked finde
with the ggplot tool. However I did not manage to accually write a name to
the correct layers. Below you can see the code that I wrote and also woks.
Remember, the code works, meaning I get a plot with all 9 timeseires from
aa until ii. I just could not figure out how to label the different layers.
What should I do when I would like to add a name to the red line, the green
line and so on beside the plot (in al list). Thank you for your time.

Best Matthias

aa <- OHC_d[40:70]
bb <- OHC_d[50:80]
cc <- OHC_d[173:203]
dd <- OHC_d[205:235]
ee <- OHC_d[273:303]
ff <- OHC_d[292:322]
gg <- OHC_d[302:332]
hh <- OHC_d[370:400]
ii <- OHC_d[381:411]

s3 <- data.frame(aa,bb,cc,dd,ee,ff,gg,hh,ii)
y2 <- ggplot(s3,aes(x=time2,y=aa),colour="black") +
  geom_line() +
  geom_line(data=s3,aes(y=bb),colour="red") +
  geom_line() +
  geom_line(data=s3,aes(y=cc),colour="green") +
  geom_line() +
  geom_line(data=s3,aes(y=dd),colour="blue") +
  geom_line() +
  geom_line(data=s3,aes(y=ee),colour="yellow") +
  geom_line() +
  geom_line(data=s3,aes(y=ff),colour="pink") +
  geom_line() +
  geom_line(data=s3,aes(y=gg),colour="purple")+
  geom_line()+
  geom_line(data=s3,aes(y=hh),colour="violet")
  geom_line()+
  geom_line(data=s3,aes(y=ii),colour="grey")
y2

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Wed Jan  6 17:49:41 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Wed, 06 Jan 2016 16:49:41 +0000
Subject: [R] hello everyone and happy new year
In-Reply-To: <CAM0cwgXy+XufrrUCZaS=uvONF7CUUPDXQ-FdxK21TgVtd67sjQ@mail.gmail.com>
References: <CAM0cwgXy+XufrrUCZaS=uvONF7CUUPDXQ-FdxK21TgVtd67sjQ@mail.gmail.com>
Message-ID: <CAKVAULPE6W+TG44CGrF_cTYPdYZycuY7Z6nx_w6UybSYMod8zg@mail.gmail.com>

Melt your data.frame

library(reshape2)
s3.melted <- melt(s3)

and plot using something along the lines of

ggplot(s3.melted,aes(x=time2,y=value, colour=variable) +  geom_line()

(not tested)

Hope this helps
Ulrik



On Wed, 6 Jan 2016 at 17:01 Matthias Worni <wornimatthias at gmail.com> wrote:

> I got the following problem in R studio. Im trying to make a plot of
> different time series using different colours, which so far worked finde
> with the ggplot tool. However I did not manage to accually write a name to
> the correct layers. Below you can see the code that I wrote and also woks.
> Remember, the code works, meaning I get a plot with all 9 timeseires from
> aa until ii. I just could not figure out how to label the different layers.
> What should I do when I would like to add a name to the red line, the green
> line and so on beside the plot (in al list). Thank you for your time.
>
> Best Matthias
>
> aa <- OHC_d[40:70]
> bb <- OHC_d[50:80]
> cc <- OHC_d[173:203]
> dd <- OHC_d[205:235]
> ee <- OHC_d[273:303]
> ff <- OHC_d[292:322]
> gg <- OHC_d[302:332]
> hh <- OHC_d[370:400]
> ii <- OHC_d[381:411]
>
> s3 <- data.frame(aa,bb,cc,dd,ee,ff,gg,hh,ii)
> y2 <- ggplot(s3,aes(x=time2,y=aa),colour="black") +
>   geom_line() +
>   geom_line(data=s3,aes(y=bb),colour="red") +
>   geom_line() +
>   geom_line(data=s3,aes(y=cc),colour="green") +
>   geom_line() +
>   geom_line(data=s3,aes(y=dd),colour="blue") +
>   geom_line() +
>   geom_line(data=s3,aes(y=ee),colour="yellow") +
>   geom_line() +
>   geom_line(data=s3,aes(y=ff),colour="pink") +
>   geom_line() +
>   geom_line(data=s3,aes(y=gg),colour="purple")+
>   geom_line()+
>   geom_line(data=s3,aes(y=hh),colour="violet")
>   geom_line()+
>   geom_line(data=s3,aes(y=ii),colour="grey")
> y2
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Wed Jan  6 18:44:43 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 06 Jan 2016 09:44:43 -0800
Subject: [R] HELP - Hausman Test + systemfit
In-Reply-To: <CACzc+AFsBszzSt5A2R+w7cOqc9-NuvuPpyB0rkidBb+PzEvfkA@mail.gmail.com>
References: <CACzc+AFsBszzSt5A2R+w7cOqc9-NuvuPpyB0rkidBb+PzEvfkA@mail.gmail.com>
Message-ID: <63630345-6772-4F9D-A3FE-56A4C4CF7989@dcn.davis.ca.us>

To post appropriately, use less apologizing and more reading of the Posting Guide that is mentioned in the footer of all messages on this list (including this one). Note that HTML does not work well on the list, so be sure to turn it off in your email program (at least when sending emails here).

I have never used this function,  but the error message is pretty clear. If you read the help for the systemfit function it states "list of formulas", not "list of model objects". The objects returned from lrm are not formulas. Perhaps look at some examples of using that function, such as the ones in the help (type ?systemfit after you have loaded the systemfit package into R).
-- 
Sent from my phone. Please excuse my brevity.

On January 6, 2016 5:02:52 AM PST, Kateryna Riabchenko <riabchenko.kateryna at gmail.com> wrote:
>Hello, I am not advanced in R (that is why my questions can sound
>stupid).
>I apologize for that in advance. But that is how my brain works - I
>need to
>ask questions to understand. I was searching the answer everywhere  -
>without result. So I am asking You.
>
>*Given:*
>
>I have 8 regression Models:
>Model1 <- lrm(ACINTENSITY ~ GDPGROWTH, data = ACU)
>Model2<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION, data = ACU)
>Model3 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION, data =
>ACU)
>Model4<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>CENTRALBANK,
>data = ACU)
>Model5 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>CENTRALBANK
>+ CPI, data = ACU)
>Model6 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>CENTRALBANK
>+ CPI + INTERATE, data = ACU)
>Model7<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>CENTRALBANK
>+ CPI + INTERATE + UNEMPL, data = ACU)
>Model8 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>CENTRALBANK
>+ CPI + INTERATE + UNEMPL + INTERNET, data = ACU)
>As you can see, each time a new explanatory variable is added.
>I want to perform the Hausman Test
>for that I installed systemfit package
>My idea was to compare Model1 with Model2, Model2 with model3, ... and
>so on
>
>*I started with this*:
>inst <- ~ POPULATION  - *this is Question1:* I don't know what I need
>to
>mention in *"inst"*. I put additional variable in Model2 which does not
>exist in Model1, but I ma not sure that is correct! Or maybe I need to
>put
>all variables, which were used in 2 models. if You can explain - thank
>you!
>system <- list(Model1, Model2)
>
># perform the estimations
>fit2sls <- systemfit(system, "2SLS", inst = inst, data = ACU)
>
>but R responded:
>
>Error in systemfit(system, "2SLS", inst = inst, data = ACU) :
>the list of argument 'formula' must contain only objects of class
>'formula'
>
>
>
>Please, help me to understand What I do wrong!
>Best,
>Kateryna
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jfhenson1 at gmail.com  Wed Jan  6 18:57:35 2016
From: jfhenson1 at gmail.com (James Henson)
Date: Wed, 6 Jan 2016 11:57:35 -0600
Subject: [R] package broom
Message-ID: <CABPq8JOm4=rMeFauYjSaSUtaWUTBgm_no7UirgmvQ5BFCAzmPA@mail.gmail.com>

Dear R community

My version is R version 3.2.3.
The package "broom" appears to install, but it will not load.
The error message is below.


> library(broom)
Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck
= vI[[j]]) :
  there is no package called ?mnormt?
Error: package or namespace load failed for ?broom?


Thanks,
James F. Henson

	[[alternative HTML version deleted]]


From kevin.thorpe at utoronto.ca  Wed Jan  6 19:06:37 2016
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Wed, 06 Jan 2016 13:06:37 -0500
Subject: [R] package broom
In-Reply-To: <CABPq8JOm4=rMeFauYjSaSUtaWUTBgm_no7UirgmvQ5BFCAzmPA@mail.gmail.com>
References: <CABPq8JOm4=rMeFauYjSaSUtaWUTBgm_no7UirgmvQ5BFCAzmPA@mail.gmail.com>
Message-ID: <568D57AD.9030003@utoronto.ca>

On 01/06/2016 12:57 PM, James Henson wrote:
> Dear R community
>
> My version is R version 3.2.3.
> The package "broom" appears to install, but it will not load.
> The error message is below.
>
>
>> library(broom)
> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck
> = vI[[j]]) :
>    there is no package called ?mnormt?
> Error: package or namespace load failed for ?broom?
>
>

It looks like you need to install the package mnormt.

> Thanks,
> James F. Henson
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Kevin E. Thorpe
Head of Biostatistics,  Applied Health Research Centre (AHRC)
Li Ka Shing Knowledge Institute of St. Michael's
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016


From NordlDJ at dshs.wa.gov  Wed Jan  6 19:06:58 2016
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Wed, 6 Jan 2016 18:06:58 +0000
Subject: [R] package broom
In-Reply-To: <CABPq8JOm4=rMeFauYjSaSUtaWUTBgm_no7UirgmvQ5BFCAzmPA@mail.gmail.com>
References: <CABPq8JOm4=rMeFauYjSaSUtaWUTBgm_no7UirgmvQ5BFCAzmPA@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27662EDDAB90@WAXMXOLYMB025.WAX.wa.lcl>

So, you need to download and install the mnormt package.

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of James
> Henson
> Sent: Wednesday, January 06, 2016 9:58 AM
> To: R-help at r-project.org
> Subject: [R] package broom
> 
> Dear R community
> 
> My version is R version 3.2.3.
> The package "broom" appears to install, but it will not load.
> The error message is below.
> 
> 
> > library(broom)
> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
> vI[[j]]) :
>   there is no package called ?mnormt?
> Error: package or namespace load failed for ?broom?
> 
> 
> Thanks,
> James F. Henson
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From sarah.goslee at gmail.com  Wed Jan  6 19:08:25 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 6 Jan 2016 13:08:25 -0500
Subject: [R] package broom
In-Reply-To: <CABPq8JOm4=rMeFauYjSaSUtaWUTBgm_no7UirgmvQ5BFCAzmPA@mail.gmail.com>
References: <CABPq8JOm4=rMeFauYjSaSUtaWUTBgm_no7UirgmvQ5BFCAzmPA@mail.gmail.com>
Message-ID: <CAM_vjukesDuBjYgyLsFaP_LXBVpO1oTBTNUsrzmMw0CafUCBKQ@mail.gmail.com>

That means you didn't install the dependencies when you installed
broom, and now need to install the package mnormt.

No big deal, just do what the error message tells you.

Sarah

On Wed, Jan 6, 2016 at 12:57 PM, James Henson <jfhenson1 at gmail.com> wrote:
> Dear R community
>
> My version is R version 3.2.3.
> The package "broom" appears to install, but it will not load.
> The error message is below.
>
>
>> library(broom)
> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck
> = vI[[j]]) :
>   there is no package called ?mnormt?
> Error: package or namespace load failed for ?broom?
>
>
-- 
Sarah Goslee
http://www.numberwright.com


From saptarshi.guha at gmail.com  Wed Jan  6 19:55:08 2016
From: saptarshi.guha at gmail.com (Saptarshi Guha)
Date: Wed, 6 Jan 2016 10:55:08 -0800
Subject: [R] A question about corAR1 and grouping
Message-ID: <CAJDot1qUbu=mw+_Lhbuewxx+WypJUKGyT55+vqG5YqHk8jSwwA@mail.gmail.com>

Hello,

I was under the impression that in a call to gls, if i specify

gls(.., cor=AR1(form= ~ 1 | subject))

and suppose there are N subjects, then i would have obtained estimates
for N Phi (s) (the AR1 coefficient) - but I only get 1 estimate.

I then checked "Linear Mixed-Effects Models Using R" and it i must
have confused it for the weights(e.g. varIdent) where there are indeed
group(strata) specific estimates.

Is there a way to include correlations  and have group specific estimates?

Regards
Saptarshi


From dwinsemius at comcast.net  Wed Jan  6 20:02:03 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Jan 2016 11:02:03 -0800
Subject: [R] HELP - Hausman Test + systemfit
In-Reply-To: <63630345-6772-4F9D-A3FE-56A4C4CF7989@dcn.davis.ca.us>
References: <CACzc+AFsBszzSt5A2R+w7cOqc9-NuvuPpyB0rkidBb+PzEvfkA@mail.gmail.com>
	<63630345-6772-4F9D-A3FE-56A4C4CF7989@dcn.davis.ca.us>
Message-ID: <67F319A0-FBB5-40A7-92DD-F25DA2BB7ADB@comcast.net>


> On Jan 6, 2016, at 9:44 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> To post appropriately, use less apologizing and more reading of the Posting Guide that is mentioned in the footer of all messages on this list (including this one). Note that HTML does not work well on the list, so be sure to turn it off in your email program (at least when sending emails here).
> 
> I have never used this function,  but the error message is pretty clear. If you read the help for the systemfit function it states "list of formulas", not "list of model objects". The objects returned from lrm are not formulas. Perhaps look at some examples of using that function, such as the ones in the help (type ?systemfit after you have loaded the systemfit package into R).

To go a bit further in explaining what might have better prepared responders to offer a specific and tested reply, please note as you read through the Posting Guide that a data example is strongly suggested. Also suggested is that you post all `library` calls for any packages beyond the base set. The `lrm` function, for instance` is most likely from the 'rms'-package. That raises a further concern about which I have limited experience to offer. The rms-regression functions may or may not have methods defined that allow use of functions from other packages. I see that when 'systemfit' is loaded, the `lrtest`-function from rms is listed as being masked, and I further see that lrtest {lmtest} assumes there will be an `update`-method, but I do not see an `update.lrm` function when I execute:

methods(update)

On the other hand there may be grounds for guarded optimism. After preparing the example on the help page for `lrm` and creating the lrm example 'fit' we see that lrm objects may be constructed so that the can inherit the glm class and therefore use the update.glm method.

> class(fit)
[1] "lrm" "rms" "glm"

So to proceed, you might make a list of those models and then run `lapply` with the `formula` function to extract the needed formula-objects:

formula(fit)
#y ~ blood.pressure + sex * (age + rcs(cholesterol, 4))

-- 
David.


> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On January 6, 2016 5:02:52 AM PST, Kateryna Riabchenko <riabchenko.kateryna at gmail.com> wrote:
>> Hello, I am not advanced in R (that is why my questions can sound
>> stupid).
>> I apologize for that in advance. But that is how my brain works - I
>> need to
>> ask questions to understand. I was searching the answer everywhere  -
>> without result. So I am asking You.
>> 
>> *Given:*
>> 
>> I have 8 regression Models:
>> Model1 <- lrm(ACINTENSITY ~ GDPGROWTH, data = ACU)
>> Model2<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION, data = ACU)
>> Model3 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION, data =
>> ACU)
>> Model4<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>> CENTRALBANK,
>> data = ACU)
>> Model5 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>> CENTRALBANK
>> + CPI, data = ACU)
>> Model6 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>> CENTRALBANK
>> + CPI + INTERATE, data = ACU)
>> Model7<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>> CENTRALBANK
>> + CPI + INTERATE + UNEMPL, data = ACU)
>> Model8 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>> CENTRALBANK
>> + CPI + INTERATE + UNEMPL + INTERNET, data = ACU)
>> As you can see, each time a new explanatory variable is added.
>> I want to perform the Hausman Test
>> for that I installed systemfit package
>> My idea was to compare Model1 with Model2, Model2 with model3, ... and
>> so on
>> 
>> *I started with this*:
>> inst <- ~ POPULATION  - *this is Question1:* I don't know what I need
>> to
>> mention in *"inst"*. I put additional variable in Model2 which does not
>> exist in Model1, but I ma not sure that is correct! Or maybe I need to
>> put
>> all variables, which were used in 2 models. if You can explain - thank
>> you!
>> system <- list(Model1, Model2)
>> 
>> # perform the estimations
>> fit2sls <- systemfit(system, "2SLS", inst = inst, data = ACU)
>> 
>> but R responded:
>> 
>> Error in systemfit(system, "2SLS", inst = inst, data = ACU) :
>> the list of argument 'formula' must contain only objects of class
>> 'formula'
>> 
>> 
>> 
>> Please, help me to understand What I do wrong!
>> Best,
>> Kateryna
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bobaronoff at gmail.com  Wed Jan  6 19:19:29 2016
From: bobaronoff at gmail.com (Bob Aronoff)
Date: Wed, 6 Jan 2016 18:19:29 +0000
Subject: [R] Mixed Beta Disrubutions
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
Message-ID: <loom.20160106T190610-525@post.gmane.org>

I am working to understand the same issues with my datasets.  
Adapting Dr Zeileis' posting I have written a humble function.
It creates a two cluster beta mix on a vector of data.  
It seems to be working well on my datasets. 
You are welcome to try on yours.  

regards,
Bob

_____

bi.modal.beta<-function (vals){
  
  # vals is a vector data points in range (0,1).  There should not be any 0,1, or NA in vals

  library(betareg)
  library(flexmix)
  
  d <- data.frame(y = vals)

  #create bimodal beta model
  m <- betamix(y ~ 1 | 1, data = d, k = 2)

  #extract beta parameters (mean and precision) with anti-link functions
  mu <- plogis(coef(m)[,1])
  phi <- exp(coef(m)[,2])

  #convert mean & precision to alpha & beta
  a <- mu * phi
  b <- (1 - mu) * phi

  #report parameters
  print("alpha:")
  print( a)
  print("beta:")
  print(b)

  #plot in order to inspect result
  ys <- seq(0, 1, by = 0.01)
  p <- prior(m$flexmix)
       # p is equivalent to the percentage of each distribution

  hist(d$y, breaks = 0:25/25, freq = FALSE,
       main = "Bimodal Betamix", xlab = "values")
  lines(ys, p[1] * dbeta(ys, shape1 = a[1], shape2 = b[1]) , lwd = 2, col="red")
  lines(ys, p[2] * dbeta(ys, shape1 = a[2], shape2 = b[2]) , lwd = 2,col="blue")
}

____


From arne.henningsen at gmail.com  Wed Jan  6 22:08:42 2016
From: arne.henningsen at gmail.com (Arne Henningsen)
Date: Wed, 6 Jan 2016 22:08:42 +0100
Subject: [R] HELP - Hausman Test + systemfit
In-Reply-To: <67F319A0-FBB5-40A7-92DD-F25DA2BB7ADB@comcast.net>
References: <CACzc+AFsBszzSt5A2R+w7cOqc9-NuvuPpyB0rkidBb+PzEvfkA@mail.gmail.com>
	<63630345-6772-4F9D-A3FE-56A4C4CF7989@dcn.davis.ca.us>
	<67F319A0-FBB5-40A7-92DD-F25DA2BB7ADB@comcast.net>
Message-ID: <CAMTWbJhVNCJomnN0LLcPG6kYVWhcfJB3rmpue_zQ9QfrS6j4bw@mail.gmail.com>

Dear Kateryna

The hausman() method for objects of class "systemfit" tests
multiple-equation models estimated by 3SLS against multiple-equation
models estimated by 2SLS (see documentation, e.g. [1]). It seems to me
that this is not the type of Hausman test that you want to conduct.

[1] http://www.inside-r.org/packages/cran/systemfit/docs/hausman.systemfit

Best regards,
Arne



On 6 January 2016 at 20:02, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Jan 6, 2016, at 9:44 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>
>> To post appropriately, use less apologizing and more reading of the Posting Guide that is mentioned in the footer of all messages on this list (including this one). Note that HTML does not work well on the list, so be sure to turn it off in your email program (at least when sending emails here).
>>
>> I have never used this function,  but the error message is pretty clear. If you read the help for the systemfit function it states "list of formulas", not "list of model objects". The objects returned from lrm are not formulas. Perhaps look at some examples of using that function, such as the ones in the help (type ?systemfit after you have loaded the systemfit package into R).
>
> To go a bit further in explaining what might have better prepared responders to offer a specific and tested reply, please note as you read through the Posting Guide that a data example is strongly suggested. Also suggested is that you post all `library` calls for any packages beyond the base set. The `lrm` function, for instance` is most likely from the 'rms'-package. That raises a further concern about which I have limited experience to offer. The rms-regression functions may or may not have methods defined that allow use of functions from other packages. I see that when 'systemfit' is loaded, the `lrtest`-function from rms is listed as being masked, and I further see that lrtest {lmtest} assumes there will be an `update`-method, but I do not see an `update.lrm` function when I execute:
>
> methods(update)
>
> On the other hand there may be grounds for guarded optimism. After preparing the example on the help page for `lrm` and creating the lrm example 'fit' we see that lrm objects may be constructed so that the can inherit the glm class and therefore use the update.glm method.
>
>> class(fit)
> [1] "lrm" "rms" "glm"
>
> So to proceed, you might make a list of those models and then run `lapply` with the `formula` function to extract the needed formula-objects:
>
> formula(fit)
> #y ~ blood.pressure + sex * (age + rcs(cholesterol, 4))
>
> --
> David.
>
>
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On January 6, 2016 5:02:52 AM PST, Kateryna Riabchenko <riabchenko.kateryna at gmail.com> wrote:
>>> Hello, I am not advanced in R (that is why my questions can sound
>>> stupid).
>>> I apologize for that in advance. But that is how my brain works - I
>>> need to
>>> ask questions to understand. I was searching the answer everywhere  -
>>> without result. So I am asking You.
>>>
>>> *Given:*
>>>
>>> I have 8 regression Models:
>>> Model1 <- lrm(ACINTENSITY ~ GDPGROWTH, data = ACU)
>>> Model2<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION, data = ACU)
>>> Model3 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION, data =
>>> ACU)
>>> Model4<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>>> CENTRALBANK,
>>> data = ACU)
>>> Model5 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>>> CENTRALBANK
>>> + CPI, data = ACU)
>>> Model6 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>>> CENTRALBANK
>>> + CPI + INTERATE, data = ACU)
>>> Model7<- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>>> CENTRALBANK
>>> + CPI + INTERATE + UNEMPL, data = ACU)
>>> Model8 <- lrm(ACINTENSITY ~ GDPGROWTH + POPULATION + FEDUNION +
>>> CENTRALBANK
>>> + CPI + INTERATE + UNEMPL + INTERNET, data = ACU)
>>> As you can see, each time a new explanatory variable is added.
>>> I want to perform the Hausman Test
>>> for that I installed systemfit package
>>> My idea was to compare Model1 with Model2, Model2 with model3, ... and
>>> so on
>>>
>>> *I started with this*:
>>> inst <- ~ POPULATION  - *this is Question1:* I don't know what I need
>>> to
>>> mention in *"inst"*. I put additional variable in Model2 which does not
>>> exist in Model1, but I ma not sure that is correct! Or maybe I need to
>>> put
>>> all variables, which were used in 2 models. if You can explain - thank
>>> you!
>>> system <- list(Model1, Model2)
>>>
>>> # perform the estimations
>>> fit2sls <- systemfit(system, "2SLS", inst = inst, data = ACU)
>>>
>>> but R responded:
>>>
>>> Error in systemfit(system, "2SLS", inst = inst, data = ACU) :
>>> the list of argument 'formula' must contain only objects of class
>>> 'formula'
>>>
>>>
>>>
>>> Please, help me to understand What I do wrong!
>>> Best,
>>> Kateryna
>>>
>>>      [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Arne Henningsen
http://www.arne-henningsen.name


From hhaussmann at arcor.de  Wed Jan  6 19:27:58 2016
From: hhaussmann at arcor.de (=?UTF-8?Q?Hans_Hau=c3=9fmann?=)
Date: Wed, 6 Jan 2016 19:27:58 +0100
Subject: [R] par(las=1) wirkt nicht
Message-ID: <568D5CAE.3070203@arcor.de>

Hallo R-Experten,

ich habe seit Kurzem einige Grafiken entwickelt mit R.
Wenn ich Grafikparameter wie z.B. "las" oder "tcl" im plot-Befehl 
definiere, funktioniert das. Wenn ich aber im Skript "par(las=1, 
tcl=0.5)" schreibe, bleibt das ohne Wirkung. Woran kann das liegen?

Mit freundlichen Gr??en

Hans Hau?mann


From andyschneider85 at gmail.com  Wed Jan  6 21:01:17 2016
From: andyschneider85 at gmail.com (Andy Schneider)
Date: Wed, 6 Jan 2016 15:01:17 -0500
Subject: [R] HELP - as.numeric changing column data
Message-ID: <CAHytW+9dNPwVuc1O0CtALc=CtKcbhuLJKTKNm0DC1wu8+Rmwbw@mail.gmail.com>

Hi -

I'm trying to plot some data and having a lot of trouble! I have a simple
dataset consisting of two columns - income_per_capita and mass_beauty_value.
When I read the data in and plot it, I get the attached plot Mass Beauty
Non-Numeric:
<http://r.789695.n4.nabble.com/file/n4716202/Mass_Beauty_Non-Numeric.jpg> .
You can see that, while it contains all the values, the income_per_capita
axis is out of order and there are some weird vertical lines happening.

To fix this, I converted both columns to numerics using:

mass_beauty$income_per_capita <- as.numeric(mass_beauty$income_per_capita)
mass_beauty$mass_beauty_value <- as.numeric(mass_beauty$mass_beauty_value)

When I did this, I noticed that my income_per_capita column's values
suddenly changed. Whereas I have values extending all the way to 30,000 or
so before, now they maxed out at around 1,400. While at first I thought they
might at least have changed to scale, it unfortunately looks like changes
were more or less random. But, they plotted much better:
<http://r.789695.n4.nabble.com/file/n4716202/Mass_Beauty_Plot.jpg> .

Does anyone have any solution for how I can convert my income_per_capita
column to a plottable numeric without changing up its values? I've tried
doing as.numeric(as.character(mass_beauty_value$income_per_capita)) but it
didn't work.

Thanks so much for your help!

From wdunlap at tibco.com  Wed Jan  6 22:23:38 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 6 Jan 2016 13:23:38 -0800
Subject: [R] HELP - as.numeric changing column data
In-Reply-To: <CAHytW+9dNPwVuc1O0CtALc=CtKcbhuLJKTKNm0DC1wu8+Rmwbw@mail.gmail.com>
References: <CAHytW+9dNPwVuc1O0CtALc=CtKcbhuLJKTKNm0DC1wu8+Rmwbw@mail.gmail.com>
Message-ID: <CAF8bMcYqzye=pepee2epUqLOFFfGrv2HSs2VG4CygwAxHxCObQ@mail.gmail.com>

You may have read in your data incorrectly - a column you expected to be
numeric was not recognized as such so it was read in a character and then
converted to a 'factor'.

FAQ 7.10 tells how to work around the problem

https://cran.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-convert-factors-to-numeric_003f
but a better solution is to repeatedly call read.table with various
parameters (esp. colClasses=c(...), header=TRUE/FALSE, dec=","/".",
stringsAsFactors=FALSE) until str(yourData) shows you that all the column
types are what you expect.

It is a waste of time to do much of anything with your data until
str(yourData) and some simple plots of it show you that it was read into R
correctly.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Jan 6, 2016 at 12:01 PM, Andy Schneider <andyschneider85 at gmail.com>
wrote:

> Hi -
>
> I'm trying to plot some data and having a lot of trouble! I have a simple
> dataset consisting of two columns - income_per_capita and
> mass_beauty_value.
> When I read the data in and plot it, I get the attached plot Mass Beauty
> Non-Numeric:
> <http://r.789695.n4.nabble.com/file/n4716202/Mass_Beauty_Non-Numeric.jpg>
> .
> You can see that, while it contains all the values, the income_per_capita
> axis is out of order and there are some weird vertical lines happening.
>
> To fix this, I converted both columns to numerics using:
>
> mass_beauty$income_per_capita <- as.numeric(mass_beauty$income_per_capita)
> mass_beauty$mass_beauty_value <- as.numeric(mass_beauty$mass_beauty_value)
>
> When I did this, I noticed that my income_per_capita column's values
> suddenly changed. Whereas I have values extending all the way to 30,000 or
> so before, now they maxed out at around 1,400. While at first I thought
> they
> might at least have changed to scale, it unfortunately looks like changes
> were more or less random. But, they plotted much better:
> <http://r.789695.n4.nabble.com/file/n4716202/Mass_Beauty_Plot.jpg> .
>
> Does anyone have any solution for how I can convert my income_per_capita
> column to a plottable numeric without changing up its values? I've tried
> doing as.numeric(as.character(mass_beauty_value$income_per_capita)) but it
> didn't work.
>
> Thanks so much for your help!
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ligges at statistik.tu-dortmund.de  Wed Jan  6 22:39:13 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 6 Jan 2016 22:39:13 +0100
Subject: [R] package broom
In-Reply-To: <CAM_vjukesDuBjYgyLsFaP_LXBVpO1oTBTNUsrzmMw0CafUCBKQ@mail.gmail.com>
References: <CABPq8JOm4=rMeFauYjSaSUtaWUTBgm_no7UirgmvQ5BFCAzmPA@mail.gmail.com>
	<CAM_vjukesDuBjYgyLsFaP_LXBVpO1oTBTNUsrzmMw0CafUCBKQ@mail.gmail.com>
Message-ID: <568D8981.8030807@statistik.tu-dortmund.de>

Yes, it depends on psych whoich depends on mnormt and all perfectly well 
declared, henc install.packages() should have done the job for you and 
installed the dependencies. Probably you did not use it for installing 
the package.

Best,
Uwe Ligges



On 06.01.2016 19:08, Sarah Goslee wrote:
> That means you didn't install the dependencies when you installed
> broom, and now need to install the package mnormt.
>
> No big deal, just do what the error message tells you.
>
> Sarah
>
> On Wed, Jan 6, 2016 at 12:57 PM, James Henson <jfhenson1 at gmail.com> wrote:
>> Dear R community
>>
>> My version is R version 3.2.3.
>> The package "broom" appears to install, but it will not load.
>> The error message is below.
>>
>>
>>> library(broom)
>> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck
>> = vI[[j]]) :
>>    there is no package called ?mnormt?
>> Error: package or namespace load failed for ?broom?
>>
>>


From lorenzo.isella at gmail.com  Wed Jan  6 22:42:53 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Wed, 6 Jan 2016 22:42:53 +0100
Subject: [R] Amelia: Inputing Integers
Message-ID: <20160106214253.GA1661@localhost.localdomain>

Dear All,
I can provide a numerical example if needed.
My problem is the following: I am using amelia to input some missing
data in a dataset.
The problem is that some columns consist of integer numbers only and
amelia inputs some real numbers with decimals.
Is there a way to tell amelia that certain columns are of type int?
Many thanks

Lorenzo


From wdunlap at tibco.com  Wed Jan  6 22:48:01 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 6 Jan 2016 13:48:01 -0800
Subject: [R] HELP - as.numeric changing column data
In-Reply-To: <CAHytW+-V+ANBGUd0jOkowj9PjpBLq_iCqUd9t1uigeOMPHQpyg@mail.gmail.com>
References: <CAHytW+9dNPwVuc1O0CtALc=CtKcbhuLJKTKNm0DC1wu8+Rmwbw@mail.gmail.com>
	<CAF8bMcYqzye=pepee2epUqLOFFfGrv2HSs2VG4CygwAxHxCObQ@mail.gmail.com>
	<CAHytW+-V+ANBGUd0jOkowj9PjpBLq_iCqUd9t1uigeOMPHQpyg@mail.gmail.com>
Message-ID: <CAF8bMcYxmARzMLL80q1bfTanxhdo6Gw758ULU=Z5+F8QuGsTLA@mail.gmail.com>

By the way, here is an example where the advice in FAQ 7.10 (change the
factor
columns to numeric) would give incorrect results.  The incorrect header
setting
in the call to read.table causes an extra row of non-numeric data to appear
at the
start of the imported data.

  > txt <- "ColA ColB\n101 102\n201 202\n"
  > str(read.table(text=txt))
'  data.frame':   3 obs. of  2 variables:
   $ V1: Factor w/ 3 levels "101","201","ColA": 3 1 2
   $ V2: Factor w/ 3 levels "102","202","ColB": 3 1 2
  > str(read.table(text=txt, header=TRUE))
  'data.frame':   2 obs. of  2 variables:
   $ ColA: int  101 201
   $ ColB: int  102 202


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Jan 6, 2016 at 1:26 PM, Andy Schneider <andyschneider85 at gmail.com>
wrote:

>
> Hi Bill -
>
> Thanks so much! This was actually a great help, and problem worked out.
>
> Cheers,
> Andy
>
> On Wed, Jan 6, 2016 at 4:23 PM, William Dunlap <wdunlap at tibco.com> wrote:
>
>> You may have read in your data incorrectly - a column you expected to be
>> numeric was not recognized as such so it was read in a character and then
>> converted to a 'factor'.
>>
>> FAQ 7.10 tells how to work around the problem
>>
>> https://cran.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-convert-factors-to-numeric_003f
>> but a better solution is to repeatedly call read.table with various
>> parameters (esp. colClasses=c(...), header=TRUE/FALSE, dec=","/".",
>> stringsAsFactors=FALSE) until str(yourData) shows you that all the column
>> types are what you expect.
>>
>> It is a waste of time to do much of anything with your data until
>> str(yourData) and some simple plots of it show you that it was read into R
>> correctly.
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Wed, Jan 6, 2016 at 12:01 PM, Andy Schneider <
>> andyschneider85 at gmail.com> wrote:
>>
>>> Hi -
>>>
>>> I'm trying to plot some data and having a lot of trouble! I have a simple
>>> dataset consisting of two columns - income_per_capita and
>>> mass_beauty_value.
>>> When I read the data in and plot it, I get the attached plot Mass Beauty
>>> Non-Numeric:
>>> <http://r.789695.n4.nabble.com/file/n4716202/Mass_Beauty_Non-Numeric.jpg>
>>> .
>>> You can see that, while it contains all the values, the income_per_capita
>>> axis is out of order and there are some weird vertical lines happening.
>>>
>>> To fix this, I converted both columns to numerics using:
>>>
>>> mass_beauty$income_per_capita <-
>>> as.numeric(mass_beauty$income_per_capita)
>>> mass_beauty$mass_beauty_value <-
>>> as.numeric(mass_beauty$mass_beauty_value)
>>>
>>> When I did this, I noticed that my income_per_capita column's values
>>> suddenly changed. Whereas I have values extending all the way to 30,000
>>> or
>>> so before, now they maxed out at around 1,400. While at first I thought
>>> they
>>> might at least have changed to scale, it unfortunately looks like changes
>>> were more or less random. But, they plotted much better:
>>> <http://r.789695.n4.nabble.com/file/n4716202/Mass_Beauty_Plot.jpg> .
>>>
>>> Does anyone have any solution for how I can convert my income_per_capita
>>> column to a plottable numeric without changing up its values? I've tried
>>> doing as.numeric(as.character(mass_beauty_value$income_per_capita)) but
>>> it
>>> didn't work.
>>>
>>> Thanks so much for your help!
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Jan  6 23:03:22 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 7 Jan 2016 09:03:22 +1100
Subject: [R] Amelia: Inputing Integers
In-Reply-To: <20160106214253.GA1661@localhost.localdomain>
References: <20160106214253.GA1661@localhost.localdomain>
Message-ID: <CA+8X3fXe0mOTQ4HewbFBYhztP_kgeqysyecgAcPMFd+npFZvDQ@mail.gmail.com>

Hi Lorenzo,
Perhaps rounding the values in the imputed dataset?

Jim


On Thu, Jan 7, 2016 at 8:42 AM, Lorenzo Isella <lorenzo.isella at gmail.com>
wrote:

> Dear All,
> I can provide a numerical example if needed.
> My problem is the following: I am using amelia to input some missing
> data in a dataset.
> The problem is that some columns consist of integer numbers only and
> amelia inputs some real numbers with decimals.
> Is there a way to tell amelia that certain columns are of type int?
> Many thanks
>
> Lorenzo
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mesudebayrakci at gmail.com  Thu Jan  7 00:22:02 2016
From: mesudebayrakci at gmail.com (mesude bayrakci)
Date: Wed, 6 Jan 2016 18:22:02 -0500
Subject: [R] Mixed Beta Disrubutions
In-Reply-To: <loom.20160106T190610-525@post.gmane.org>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<loom.20160106T190610-525@post.gmane.org>
Message-ID: <CAH6SreyyM0mCERT9x5WxdEXN2F1Tgp+a8P9gs2gnWhqiT+gMYw@mail.gmail.com>

Thank you Bob.

I already had it worked.

Best,

M.







On Wed, Jan 6, 2016 at 1:19 PM, Bob Aronoff <bobaronoff at gmail.com> wrote:

> I am working to understand the same issues with my datasets.
> Adapting Dr Zeileis' posting I have written a humble function.
> It creates a two cluster beta mix on a vector of data.
> It seems to be working well on my datasets.
> You are welcome to try on yours.
>
> regards,
> Bob
>
> _____
>
> bi.modal.beta<-function (vals){
>
>   # vals is a vector data points in range (0,1).  There should not be any
> 0,1, or NA in vals
>
>   library(betareg)
>   library(flexmix)
>
>   d <- data.frame(y = vals)
>
>   #create bimodal beta model
>   m <- betamix(y ~ 1 | 1, data = d, k = 2)
>
>   #extract beta parameters (mean and precision) with anti-link functions
>   mu <- plogis(coef(m)[,1])
>   phi <- exp(coef(m)[,2])
>
>   #convert mean & precision to alpha & beta
>   a <- mu * phi
>   b <- (1 - mu) * phi
>
>   #report parameters
>   print("alpha:")
>   print( a)
>   print("beta:")
>   print(b)
>
>   #plot in order to inspect result
>   ys <- seq(0, 1, by = 0.01)
>   p <- prior(m$flexmix)
>        # p is equivalent to the percentage of each distribution
>
>   hist(d$y, breaks = 0:25/25, freq = FALSE,
>        main = "Bimodal Betamix", xlab = "values")
>   lines(ys, p[1] * dbeta(ys, shape1 = a[1], shape2 = b[1]) , lwd = 2,
> col="red")
>   lines(ys, p[2] * dbeta(ys, shape1 = a[2], shape2 = b[2]) , lwd =
> 2,col="blue")
> }
>
> ____
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Jan  7 03:14:55 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Jan 2016 18:14:55 -0800
Subject: [R] par(las=1) wirkt nicht
In-Reply-To: <568D5CAE.3070203@arcor.de>
References: <568D5CAE.3070203@arcor.de>
Message-ID: <B541BBEC-EDAF-4A00-B475-C8AC53618AFB@comcast.net>


> On Jan 6, 2016, at 10:27 AM, Hans Hau?mann <hhaussmann at arcor.de> wrote:
> 
> Hallo R-Experten,
> 
> ich habe seit Kurzem einige Grafiken entwickelt mit R.
> Wenn ich Grafikparameter wie z.B. "las" oder "tcl" im plot-Befehl definiere, funktioniert das. Wenn ich aber im Skript "par(las=1, tcl=0.5)" schreibe, bleibt das ohne Wirkung. Woran kann das liegen?

Ich sprechen ein bisschen Deutsch. ... and my spelling is quite suspect. So, dropping this into google-translate and correcting the obvious mistakes, I get:
----------
Hi R experts ,

I have recently developed some graphics with R.
If I [use] graphics parameters such as "las" or "tcl" defining the plot command , it works . But if I [use] in the script " par (las = 1 , tcl = 0.5 ) " writing that has no effect. Why is that ?
-----------

I'm perhaps confused about what is actually occurring. Are you expecting an existing plot to be affected by executing a call to par()? You will be disappointed if that is the case. R graphics are ink-on-paper; one cannot expect an existing plot to be affected by simply using par(...). Existing plots in hte interactive device wiill be replace or overwritten depending on the settings of par's new parameter or possibly depending on a graphics functions add parameter setting.



Best regards

-- 
David Winsemius
Alameda, CA, USA


From luisamrfreitas at gmail.com  Thu Jan  7 01:03:38 2016
From: luisamrfreitas at gmail.com (=?UTF-8?Q?Lu=C3=ADsa_Freitas?=)
Date: Thu, 7 Jan 2016 00:03:38 +0000
Subject: [R] Multiple Integrals
Message-ID: <CAG93X4Fibw2m8wK1PvOnNJS12-L90EjPBK2f4RA9ZeG2yPJwRg@mail.gmail.com>

Dear R-helpers,

I'm looking for a quick way to calculate triple integrals.

I have tried something like this example:

f <- function(x,y,z) dnorm(x)*dnorm(y)*dnorm(z)
llim <- -Inf
ulim <- Inf
integrate(function(z)
{
  sapply(z,function(z)
  {
    integrate(function(y)
    {
      sapply(y, function(y)
      {
        integrate(function(x) f(x,y,z), llim, ulim)$value
      })
    }, llim, ulim)$value
  })
},llim,ulim)

I'm not sure if there are other ways faster than this one.
Any help will be appreciated.

Thanks,
L.M.

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Thu Jan  7 05:16:51 2016
From: hannah.hlx at gmail.com (li li)
Date: Wed, 6 Jan 2016 23:16:51 -0500
Subject: [R] exact trend test
Message-ID: <CAHLnndaQzB5Vyka_QDvOCTydXLyAz+QCrQd2FpWPL-NDuJKd7Q@mail.gmail.com>

Hi all,
  Is there an R function that does exact randomization trend test?
  For example, consider the 2 by 5 contingency table below:

            dose0    dose 0.15    dose 0.5    dose 1.5    dose 5       row
margin
 Yes          4                3                  4               5
     8                   24
  No          4                5                   4               3
       0                  16
col sum    8                8                   8               8
   8                   40

To do the exact trend test, we need to enumerate all the contingency table
with the
row and column margins fixed. Find the probability corresponding to
obtaining
the corresponding contingency tables based on the multivariate
hypergeometric distribution. Finally the pvalue is obtained by adding
relevant probabilities.

Is there an R function that does this? if not, I am wondering whether it is
possible to
enumerate all possible contingency tables that has column sun and row sum
fixed?

Thanks very much!!

   Hanna

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Thu Jan  7 05:37:37 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 7 Jan 2016 17:37:37 +1300
Subject: [R] [FORGED]  Multiple Integrals
In-Reply-To: <CAG93X4Fibw2m8wK1PvOnNJS12-L90EjPBK2f4RA9ZeG2yPJwRg@mail.gmail.com>
References: <CAG93X4Fibw2m8wK1PvOnNJS12-L90EjPBK2f4RA9ZeG2yPJwRg@mail.gmail.com>
Message-ID: <568DEB91.20007@auckland.ac.nz>

On 07/01/16 13:03, Lu?sa Freitas wrote:
> Dear R-helpers,
>
> I'm looking for a quick way to calculate triple integrals.
>
> I have tried something like this example:
>
> f <- function(x,y,z) dnorm(x)*dnorm(y)*dnorm(z)
> llim <- -Inf
> ulim <- Inf
> integrate(function(z)
> {
>    sapply(z,function(z)
>    {
>      integrate(function(y)
>      {
>        sapply(y, function(y)
>        {
>          integrate(function(x) f(x,y,z), llim, ulim)$value
>        })
>      }, llim, ulim)$value
>    })
> },llim,ulim)
>
> I'm not sure if there are other ways faster than this one.
> Any help will be appreciated.

GIYF.  Searching on "multiple integration r" led me very quickly to the 
R2Cuba package for multidimensional numerical integration, which will 
presumably do what you want.  (I have not tried it out; that's your job.)

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Thu Jan  7 08:31:14 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Jan 2016 23:31:14 -0800
Subject: [R] exact trend test
In-Reply-To: <CAHLnndaQzB5Vyka_QDvOCTydXLyAz+QCrQd2FpWPL-NDuJKd7Q@mail.gmail.com>
References: <CAHLnndaQzB5Vyka_QDvOCTydXLyAz+QCrQd2FpWPL-NDuJKd7Q@mail.gmail.com>
Message-ID: <550606A1-02D9-413D-9DEA-3D2DC783CED8@comcast.net>


> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
> 
> Hi all,
>  Is there an R function that does exact randomization trend test?
>  For example, consider the 2 by 5 contingency table below:
> 
>            dose0    dose 0.15    dose 0.5    dose 1.5    dose 5       row
> margin
> Yes          4                3                  4               5
>     8                   24
>  No          4                5                   4               3
>       0                  16
> col sum    8                8                   8               8
>   8                   40

Your data presentation has been distorted by your failure to post in plain text. Surely you have been asked in the past to correct this issue?

> 
> To do the exact trend test, we need to enumerate all the contingency table
> with the
> row and column margins fixed.

Er, how should that be done? A trend test? What is described above would be a general test of no association rather than a trend test. Please use clear language and be as specific as possible if you choose to respond. 

> Find the probability corresponding to
> obtaining
> the corresponding contingency tables based on the multivariate
> hypergeometric distribution. Finally the pvalue is obtained by adding
> relevant probabilities.

If there is a trend under consideration, then I do not understand such a trend would be modeled under a hypergeometric distribution? A hypergeometic distribution would suggest no trend, at least to my current understanding.

> 
> Is there an R function that does this? if not, I am wondering whether it is
> possible to
> enumerate all possible contingency tables that has column sun and row sum
> fixed?

Wel, yes, that is possible and routinely done with `fisher.test`, but it is up to you to describe how that activity leads to a trend test.

If you assume Poisson distributed errors a trend test is fairly easy to construct with glm.

-- 
David.
> 
> Thanks very much!!
> 
>   Hanna
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Thu Jan  7 08:33:14 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Jan 2016 23:33:14 -0800
Subject: [R] [FORGED]  Multiple Integrals
In-Reply-To: <568DEB91.20007@auckland.ac.nz>
References: <CAG93X4Fibw2m8wK1PvOnNJS12-L90EjPBK2f4RA9ZeG2yPJwRg@mail.gmail.com>
	<568DEB91.20007@auckland.ac.nz>
Message-ID: <0CBBDE56-49B9-4B34-A982-EC0E9A90FE29@comcast.net>


> On Jan 6, 2016, at 8:37 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> On 07/01/16 13:03, Lu?sa Freitas wrote:
>> Dear R-helpers,
>> 
>> I'm looking for a quick way to calculate triple integrals.
>> 
>> I have tried something like this example:
>> 
>> f <- function(x,y,z) dnorm(x)*dnorm(y)*dnorm(z)
>> llim <- -Inf
>> ulim <- Inf
>> integrate(function(z)
>> {
>>   sapply(z,function(z)
>>   {
>>     integrate(function(y)
>>     {
>>       sapply(y, function(y)
>>       {
>>         integrate(function(x) f(x,y,z), llim, ulim)$value
>>       })
>>     }, llim, ulim)$value
>>   })
>> },llim,ulim)
>> 
>> I'm not sure if there are other ways faster than this one.
>> Any help will be appreciated.
> 
> GIYF.  Searching on "multiple integration r" led me very quickly to the R2Cuba package for multidimensional numerical integration, which will presumably do what you want.  (I have not tried it out; that's your job.)

I have not tried that package out but I can report satisfactory experience with the r package 'cubature'.

-- 
David.
> 
> cheers,
> 
> Rolf Turner
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From murdoch.duncan at gmail.com  Thu Jan  7 12:24:59 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 7 Jan 2016 06:24:59 -0500
Subject: [R] exact trend test
In-Reply-To: <550606A1-02D9-413D-9DEA-3D2DC783CED8@comcast.net>
References: <CAHLnndaQzB5Vyka_QDvOCTydXLyAz+QCrQd2FpWPL-NDuJKd7Q@mail.gmail.com>
	<550606A1-02D9-413D-9DEA-3D2DC783CED8@comcast.net>
Message-ID: <568E4B0B.70402@gmail.com>

On 07/01/2016 2:31 AM, David Winsemius wrote:
>
>> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
>>
>> Hi all,
>>   Is there an R function that does exact randomization trend test?
>>   For example, consider the 2 by 5 contingency table below:
>>
>>             dose0    dose 0.15    dose 0.5    dose 1.5    dose 5       row
>> margin
>> Yes          4                3                  4               5
>>      8                   24
>>   No          4                5                   4               3
>>        0                  16
>> col sum    8                8                   8               8
>>    8                   40
>
> Your data presentation has been distorted by your failure to post in plain text. Surely you have been asked in the past to correct this issue?
>
>>
>> To do the exact trend test, we need to enumerate all the contingency table
>> with the
>> row and column margins fixed.
>
> Er, how should that be done? A trend test? What is described above would be a general test of no association rather than a trend test. Please use clear language and be as specific as possible if you choose to respond.

Under (one version of) the null hypothesis of no trend, the distribution 
should show no association.  The difference from the Fisher's test is in 
the statistic:  rather than a measure of lack of association, you want a 
measure of trend.  It's been a long time since I worked on this kind of 
thing, but I believe Tarone's test gives an appropriate statistic.  I 
don't know which package calculates its permutation distribution with 
fixed margins, but there's probably one out there somewhere.

Duncan Murdoch

>> Find the probability corresponding to
>> obtaining
>> the corresponding contingency tables based on the multivariate
>> hypergeometric distribution. Finally the pvalue is obtained by adding
>> relevant probabilities.
>
> If there is a trend under consideration, then I do not understand such a trend would be modeled under a hypergeometric distribution? A hypergeometic distribution would suggest no trend, at least to my current understanding.
>
>>
>> Is there an R function that does this? if not, I am wondering whether it is
>> possible to
>> enumerate all possible contingency tables that has column sun and row sum
>> fixed?
>
> Wel, yes, that is possible and routinely done with `fisher.test`, but it is up to you to describe how that activity leads to a trend test.
>
> If you assume Poisson distributed errors a trend test is fairly easy to construct with glm.
>


From lists at dewey.myzen.co.uk  Thu Jan  7 12:53:44 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Thu, 7 Jan 2016 11:53:44 +0000
Subject: [R] exact trend test
In-Reply-To: <CAHLnndaQzB5Vyka_QDvOCTydXLyAz+QCrQd2FpWPL-NDuJKd7Q@mail.gmail.com>
References: <CAHLnndaQzB5Vyka_QDvOCTydXLyAz+QCrQd2FpWPL-NDuJKd7Q@mail.gmail.com>
Message-ID: <568E51C8.9030802@dewey.myzen.co.uk>

Have you tried Google?
exact randomisation trend test r
finds what look like clearly relevant packages

On 07/01/2016 04:16, li li wrote:
> Hi all,
>    Is there an R function that does exact randomization trend test?
>    For example, consider the 2 by 5 contingency table below:
>
>              dose0    dose 0.15    dose 0.5    dose 1.5    dose 5       row
> margin
>   Yes          4                3                  4               5
>       8                   24
>    No          4                5                   4               3
>         0                  16
> col sum    8                8                   8               8
>     8                   40
>
> To do the exact trend test, we need to enumerate all the contingency table
> with the
> row and column margins fixed. Find the probability corresponding to
> obtaining
> the corresponding contingency tables based on the multivariate
> hypergeometric distribution. Finally the pvalue is obtained by adding
> relevant probabilities.
>
> Is there an R function that does this? if not, I am wondering whether it is
> possible to
> enumerate all possible contingency tables that has column sun and row sum
> fixed?
>
> Thanks very much!!
>
>     Hanna
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From pdalgd at gmail.com  Thu Jan  7 13:15:20 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 7 Jan 2016 13:15:20 +0100
Subject: [R] exact trend test
In-Reply-To: <550606A1-02D9-413D-9DEA-3D2DC783CED8@comcast.net>
References: <CAHLnndaQzB5Vyka_QDvOCTydXLyAz+QCrQd2FpWPL-NDuJKd7Q@mail.gmail.com>
	<550606A1-02D9-413D-9DEA-3D2DC783CED8@comcast.net>
Message-ID: <9B50AB7B-260E-46DA-8F09-10745848C28B@gmail.com>


On 07 Jan 2016, at 08:31 , David Winsemius <dwinsemius at comcast.net> wrote:

>> 
>> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
>> 
>> Hi all,
>> Is there an R function that does exact randomization trend test?
>> For example, consider the 2 by 5 contingency table below:
>> 
>>           dose0    dose 0.15    dose 0.5    dose 1.5    dose 5       row
>> margin
>> Yes          4                3                  4               5
>>    8                   24
>> No          4                5                   4               3
>>      0                  16
>> col sum    8                8                   8               8
>>  8                   40
> 
> Your data presentation has been distorted by your failure to post in plain text. Surely you have been asked in the past to correct this issue?
> 
>> 
>> To do the exact trend test, we need to enumerate all the contingency table
>> with the
>> row and column margins fixed.
> 
> Er, how should that be done? A trend test? What is described above would be a general test of no association rather than a trend test. Please use clear language and be as specific as possible if you choose to respond. 
> 
>> Find the probability corresponding to
>> obtaining
>> the corresponding contingency tables based on the multivariate
>> hypergeometric distribution. Finally the pvalue is obtained by adding
>> relevant probabilities.
> 
> If there is a trend under consideration, then I do not understand such a trend would be modeled under a hypergeometric distribution? A hypergeometic distribution would suggest no trend, at least to my current understanding.

I'd expect that there is such a beast as a noncentral multivariate hypergeometric (for the 2x2 case that is what we use to get the CI for the odds ratio), but usually, one just wants the null distribution of the test statistic.


> 
>> 
>> Is there an R function that does this? if not, I am wondering whether it is
>> possible to
>> enumerate all possible contingency tables that has column sun and row sum
>> fixed?
> 
> Wel, yes, that is possible and routinely done with `fisher.test`, but it is up to you to describe how that activity leads to a trend test.
> 
> If you assume Poisson distributed errors a trend test is fairly easy to construct with glm.
> 

Or, more to the point, there is prop.trend.test(). Neither are exact tests, though. 

I think package "coin" may something relevant.

-pd 


> -- 
> David.
>> 
>> Thanks very much!!
>> 
>>  Hanna
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From luke-tierney at uiowa.edu  Tue Jan  5 21:29:14 2016
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Tue, 5 Jan 2016 14:29:14 -0600
Subject: [R] [R-pkgs] new release of proftools package.
Message-ID: <alpine.DEB.2.10.1601051427490.2461@luke-Latitude>

Version 0.99-0 of the proftools package for analyzing and
visualizing R profiling data has been uploaded to CRAN. This is a
major update that adds new summary functions, new and improved
visualizations, and tools for subsetting and filtering profile
data. A vignette illustrates the use of the package.

Luke Tierney
Riad Jarjour

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From hhaussmann at arcor.de  Thu Jan  7 09:30:35 2016
From: hhaussmann at arcor.de (=?UTF-8?Q?Hans_Hau=c3=9fmann?=)
Date: Thu, 7 Jan 2016 09:30:35 +0100
Subject: [R] "par(tcl=0.5,las=1)" doesn't work
Message-ID: <568E222B.2060409@arcor.de>

Hi Bert Gunter and David Winsemius,

sorry for sending my first post to R-help in German and thank you David 
for your efforts to translate.

Here are two scripts now. In the first one the graphics parameters "tcl" 
and "las" are not recognized, in the second one they are. Why not in the 
first one?

I am not sure whether I may attach the outputs of the two scripts, I 
just try.

Kind regards,

Hans Hau?mann (Haussmann)
-------------- n?chster Teil --------------
S=read.table("Sarrazin-2.5",as.is = default.stringsAsFactors())
par(tcl=0.5,las=1)
png(file="partest-1.png",width=580,height=580)
dev.control("enable") 
plot(V3,V2,xlab=" ",ylab="BIP, rel. zu Deutschland", pch=16,xlim=c(630,930),ylim=c(95,135))
dev.off()
-------------- n?chster Teil --------------
S=read.table("Sarrazin-2.5",as.is = default.stringsAsFactors())
# par(tcl=0.5,las=1)
png(file="partest-2.png",width=580,height=580)
dev.control("enable") 
plot(V3,V2,xlab=" ",ylab="BIP, rel. zu Deutschland", pch=16,xlim=c(630,930),ylim=c(95,135),tcl=0.5,las=1)
dev.off()
-------------- n?chster Teil --------------
Ein Dateianhang mit Bin?rdaten wurde abgetrennt...
Dateiname   : partest-1.png
Dateityp    : image/png
Dateigr??e  : 4330 bytes
Beschreibung: nicht verf?gbar
URL         : <https://stat.ethz.ch/pipermail/r-help/attachments/20160107/57e400e9/attachment.png>
-------------- n?chster Teil --------------
Ein Dateianhang mit Bin?rdaten wurde abgetrennt...
Dateiname   : partest-2.png
Dateityp    : image/png
Dateigr??e  : 4296 bytes
Beschreibung: nicht verf?gbar
URL         : <https://stat.ethz.ch/pipermail/r-help/attachments/20160107/57e400e9/attachment-0001.png>

From 362205285 at qq.com  Thu Jan  7 09:50:02 2016
From: 362205285 at qq.com (=?ISO-8859-1?B?UmluY3kgRmVuZw==?=)
Date: Thu, 7 Jan 2016 16:50:02 +0800
Subject: [R] kernlab - laplacedot kernel error
Message-ID: <tencent_0D8ADACC62B32D730D1EE244@qq.com>

Hi,
  I am using R's "kernlab" package. I am doing classification  using ksvm().
 
  My train data is 2455obs 5397variables and no NA data in there.


  And my code is:
  train$tag <- as.factor(train$tag)
  k<-ksvm(tag~.,train,kernel="laplacedot",C=100,cross=10)




  But it returns the following error: 
  Error in votematrix[i, ret < 0] <- votematrix[i, ret < 0] + 1 : 
  NAs are not allowed in subscripted assignments
  In addition: Warning messages:
  1: In sqrt(-round(2 * x[lowerl:upperl, ] %*% t(y) - dotbb - dota[lowerl:upperl] %*%  :
  NaNs produced
  2: ...(the same as 1)
  ...




 But when I choose another kernel, such as rbfdot, vanilladot, polydot and tanhdot, it returns the ksvm model to me.
 So can you tell me where is wrong?
 Thank you very much for your time and attention.
 
Sincerely,
Rincy
	[[alternative HTML version deleted]]


From sstoline at gmail.com  Thu Jan  7 13:24:42 2016
From: sstoline at gmail.com (Steven Stoline)
Date: Thu, 7 Jan 2016 07:24:42 -0500
Subject: [R] Solve an Equation Including Integral
Message-ID: <CAHDp66DCvinYmdzgr7FbJJo0VHGyRpz6DEPxqDbN_bUpVVmkew@mail.gmail.com>

Dear All: I submitted this post to the R-sig-Teaching List too


I am trying to solve an equation including an integral for unknown
parameter.


Is this can be done in R? if so, any helps will be highly appreciated



### The following function is a function of t, given exi, nu, and alpha.


fun<-function(exi,nu,alpha)
(nu+t^2)^(-(nu+1)/2)*exp(((nu+1)*exi*t)/((nu+t^2)^0.5))


### I want to solve "[Integral(-Inf,100) fun(exi,nu,alpha)]-(1-alpha)=0"
for t.

### That is, find the value of t which satisfy the equation:

             [Integral(-Inf,100) fun(exi,nu,alpha)]-(1-alpha)=0


### for example given: alpha = 0.05, nu = 20 , exi = 0.5, Solve for t.


with many thanks
steve

--------------------------
Steven M. Stoline
1123 Forest Avenue
Portland, ME 04112
sstoline at gmail.com

	[[alternative HTML version deleted]]


From shea at eagleseven.com  Thu Jan  7 14:09:05 2016
From: shea at eagleseven.com (Shea Lutton)
Date: Thu, 7 Jan 2016 13:09:05 +0000
Subject: [R] Question about Kolmogorov-Smirnov test behavior
Message-ID: <3155F7C8B928B741AC08CA3B3CD4169A68BD8A78@HUBBARD.eagleseven.com>

Dear R-Help,
       I am trying to understand the output of the KS test on a pair of files. I am trying to determine if the CDF of one distribution is less than (to the left of) the CDF of a second distribution. My problem is that regardless of whether I run A against B, or B against A, the KS output seems to indicate significance that A is less than B AND B is less than A. Can anybody help me understand where my mistake is or if I am misinterpreting the results? 


Here is my code:

file_a = readLines("./file_a.txt")
file_b = readLines("./file_b.txt")
a <- as.numeric(file_a)
b <- as.numeric(file_b)
ks.test(b, a, alternative = "less")
ks.test(a, b, alternative = "less")


And here is the output:

	Two-sample Kolmogorov-Smirnov test

data:  b and a
D^- = 0.087769, p-value < 2.2e-16
alternative hypothesis: the CDF of x lies below that of y

	Two-sample Kolmogorov-Smirnov test

data:  a and b
D^- = 0.085083, p-value < 2.2e-16
alternative hypothesis: the CDF of x lies below that of y

> plot(ecdf(a), col = "blue")
> plot(ecdf(b), add = TRUE, col = "red", lty = 1, pch = 26)
> plot(density(a))
> lines(density(b), col = "red")


My data files can be found here, they are simple columns of numbers. 
     file_a.txt : http://pastebin.com/e3bmnEDt
     file_b.txt : http://pastebin.com/5VBzHRXZ


Many thanks,
____________________________
Shea Lutton
Chicago, IL



From marc_schwartz at me.com  Thu Jan  7 15:22:22 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Thu, 07 Jan 2016 08:22:22 -0600
Subject: [R] "par(tcl=0.5,las=1)" doesn't work
In-Reply-To: <568E222B.2060409@arcor.de>
References: <568E222B.2060409@arcor.de>
Message-ID: <7C613136-DD8A-477F-85D4-AE2734D46F97@me.com>


> On Jan 7, 2016, at 2:30 AM, Hans Hau?mann <hhaussmann at arcor.de> wrote:
> 
> Hi Bert Gunter and David Winsemius,
> 
> sorry for sending my first post to R-help in German and thank you David for your efforts to translate.
> 
> Here are two scripts now. In the first one the graphics parameters "tcl" and "las" are not recognized, in the second one they are. Why not in the first one?
> 
> I am not sure whether I may attach the outputs of the two scripts, I just try.
> 
> Kind regards,
> 
> Hans Hau?mann (Haussmann)
> <partest-1.txt><partest-2.txt><partest-1.png><partest-2.png>______________________________________________


Hi,

Are you sure that partest-1 does not work and partest-2 does work, presuming those are the first and second scripts, respectively, you refer to above?

In partest-2, you have the line:

  # par(tcl=0.5,las=1)

which is commented out with the '#' and therefore will not be interpreted by R, thus non-functional.

In partest-1, the line is:

  par(tcl=0.5,las=1)

where it will be interpreted by R and thus functional.

Regards,

Marc Schwartz


From pdalgd at gmail.com  Thu Jan  7 15:29:13 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 7 Jan 2016 15:29:13 +0100
Subject: [R] Question about Kolmogorov-Smirnov test behavior
In-Reply-To: <3155F7C8B928B741AC08CA3B3CD4169A68BD8A78@HUBBARD.eagleseven.com>
References: <3155F7C8B928B741AC08CA3B3CD4169A68BD8A78@HUBBARD.eagleseven.com>
Message-ID: <FF7D2735-9EC0-40EB-ACB3-DC4FC00451AB@gmail.com>


On 07 Jan 2016, at 14:09 , Shea Lutton <shea at eagleseven.com> wrote:

> Dear R-Help,
>       I am trying to understand the output of the KS test on a pair of files. I am trying to determine if the CDF of one distribution is less than (to the left of) the CDF of a second distribution. My problem is that regardless of whether I run A against B, or B against A, the KS output seems to indicate significance that A is less than B AND B is less than A. Can anybody help me understand where my mistake is or if I am misinterpreting the results? 
> 
> 
> Here is my code:
> 
> file_a = readLines("./file_a.txt")
> file_b = readLines("./file_b.txt")
> a <- as.numeric(file_a)
> b <- as.numeric(file_b)
> ks.test(b, a, alternative = "less")
> ks.test(a, b, alternative = "less")
> 
> 
> And here is the output:
> 
> 	Two-sample Kolmogorov-Smirnov test
> 
> data:  b and a
> D^- = 0.087769, p-value < 2.2e-16
> alternative hypothesis: the CDF of x lies below that of y
> 
> 	Two-sample Kolmogorov-Smirnov test
> 
> data:  a and b
> D^- = 0.085083, p-value < 2.2e-16
> alternative hypothesis: the CDF of x lies below that of y
> 
>> plot(ecdf(a), col = "blue")
>> plot(ecdf(b), add = TRUE, col = "red", lty = 1, pch = 26)
>> plot(density(a))
>> lines(density(b), col = "red")
> 
> 
> My data files can be found here, they are simple columns of numbers. 
>     file_a.txt : http://pastebin.com/e3bmnEDt
>     file_b.txt : http://pastebin.com/5VBzHRXZ
> 


This effect can be generated quite easily by simulation:

> a <- rnorm(1000) ; b <-rnorm(1000, sd=10)
> ks.test(a, b, alternative="less")

	Two-sample Kolmogorov-Smirnov test

data:  a and b
D^- = 0.394, p-value < 2.2e-16
alternative hypothesis: the CDF of x lies below that of y

> ks.test(b, a, alternative="less")

	Two-sample Kolmogorov-Smirnov test

data:  b and a
D^- = 0.412, p-value < 2.2e-16
alternative hypothesis: the CDF of x lies below that of y


The cause should be quite apparent if you do

 plot(ecdf(b))
 plot(ecdf(a), add=T)

and 

plot(function(x)ecdf(a)(x)-ecdf(b)(x), from=-10, to=10)

The basic point is that since KS looks at a maximum difference, two CDFs may deviate in bothe the positive and the negative direction at the same time.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From rbaer at atsu.edu  Thu Jan  7 15:30:15 2016
From: rbaer at atsu.edu (Robert Baer)
Date: Thu, 7 Jan 2016 08:30:15 -0600
Subject: [R] "par(tcl=0.5,las=1)" doesn't work
In-Reply-To: <568E222B.2060409@arcor.de>
References: <568E222B.2060409@arcor.de>
Message-ID: <568E7677.2070208@atsu.edu>



On 1/7/2016 2:30 AM, Hans Hau?mann wrote:
> Hi Bert Gunter and David Winsemius,
>
> sorry for sending my first post to R-help in German and thank you 
> David for your efforts to translate.
>
> Here are two scripts now. In the first one the graphics parameters 
> "tcl" and "las" are not recognized, in the second one they are. Why 
> not in the first one?
To make your par() work the way you expect, you could try moving it 
below the png() command.  Create the device, then set the par().
>
> I am not sure whether I may attach the outputs of the two scripts, I 
> just try.
>
> Kind regards,
>
> Hans Hau?mann (Haussmann)
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From axel.urbiz at gmail.com  Thu Jan  7 16:14:04 2016
From: axel.urbiz at gmail.com (Axel Urbiz)
Date: Thu, 7 Jan 2016 10:14:04 -0500
Subject: [R] Order of terms and model.matrix()
Message-ID: <CAAyVsXJYA1fFvyxrFE==7SxgphbgZjSnxHXXjXt-p33pnL04_g@mail.gmail.com>

Hello,

It is not very clear to me from the model.matrix documentation, why simply
changing the order of terms in the formula may give a different design
matrix.  Please note I?m purposely not including main effects in the model
formulae.

set.seed(1)
x1 <- rnorm(100)
f1 <- factor(sample(letters[1:3], 100, replace = TRUE))
trt <- sample(c(-1,1), 100, replace = TRUE)
df <- data.frame(y=y, x1=x1, f1=f1, trt=trt)

head(model.matrix( ~ x1:trt + f1:trt, data = df))
(Intercept)      x1:trt trt:f1b trt:f1c
1           1 -0.62036668       1       0
2           1  0.04211587       0       0
3           1 -0.91092165       0       1
4           1  0.15802877       0       1
5           1  0.65458464       0      -1
6           1  1.76728727       0       1

head(model.matrix(~ f1:trt + x1:trt, data = df)) #terms reversed
(Intercept) f1a:trt f1b:trt f1c:trt      trt:x1
1           1       0       1       0 -0.62036668
2           1       1       0       0  0.04211587
3           1       0       0       1 -0.91092165
4           1       0       0       1  0.15802877
5           1       0       0      -1  0.65458464
6           1       0       0       1  1.76728727

Thanks,
Axel.

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Thu Jan  7 17:29:29 2016
From: hannah.hlx at gmail.com (li li)
Date: Thu, 7 Jan 2016 11:29:29 -0500
Subject: [R] exact trend test (enumerate all possible contingency tables
 with fixed row and column margins)
Message-ID: <CAHLnndZkaE92n+F1=U7yUO-1b6QdafU5YcGxh-zp0YA1wbRbMg@mail.gmail.com>

Thanks for all the reply. Below is the data in a better format.

> addmargins(dat)

    dose 0 dose 0.15 dose 0.5 dose 1.5 dose 5 Sum

yes      4         3        4        5      8  24

no       4         5        4        3      0  16

Sum      8         8        8        8      8  40

I think it is easier and better that I rephrase my question. I would like
to enumerate all possible
contingency tables with the row margins and column margins fixed as in the
above table. Yes. In fisher's exact test, this should have been done
internally. But I need explicitly find all such tables. Need some help on
this and thanks very much in advance.

    Hanna


2016-01-07 7:15 GMT-05:00 peter dalgaard <pdalgd at gmail.com>:

>
> On 07 Jan 2016, at 08:31 , David Winsemius <dwinsemius at comcast.net> wrote:
>
> >>
> >> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
> >>
> >> Hi all,
> >> Is there an R function that does exact randomization trend test?
> >> For example, consider the 2 by 5 contingency table below:
> >>
> >>           dose0    dose 0.15    dose 0.5    dose 1.5    dose 5       row
> >> margin
> >> Yes          4                3                  4               5
> >>    8                   24
> >> No          4                5                   4               3
> >>      0                  16
> >> col sum    8                8                   8               8
> >>  8                   40
> >
> > Your data presentation has been distorted by your failure to post in
> plain text. Surely you have been asked in the past to correct this issue?
> >
> >>
> >> To do the exact trend test, we need to enumerate all the contingency
> table
> >> with the
> >> row and column margins fixed.
> >
> > Er, how should that be done? A trend test? What is described above would
> be a general test of no association rather than a trend test. Please use
> clear language and be as specific as possible if you choose to respond.
> >
> >> Find the probability corresponding to
> >> obtaining
> >> the corresponding contingency tables based on the multivariate
> >> hypergeometric distribution. Finally the pvalue is obtained by adding
> >> relevant probabilities.
> >
> > If there is a trend under consideration, then I do not understand such a
> trend would be modeled under a hypergeometric distribution? A hypergeometic
> distribution would suggest no trend, at least to my current understanding.
>
> I'd expect that there is such a beast as a noncentral multivariate
> hypergeometric (for the 2x2 case that is what we use to get the CI for the
> odds ratio), but usually, one just wants the null distribution of the test
> statistic.
>
>
> >
> >>
> >> Is there an R function that does this? if not, I am wondering whether
> it is
> >> possible to
> >> enumerate all possible contingency tables that has column sun and row
> sum
> >> fixed?
> >
> > Wel, yes, that is possible and routinely done with `fisher.test`, but it
> is up to you to describe how that activity leads to a trend test.
> >
> > If you assume Poisson distributed errors a trend test is fairly easy to
> construct with glm.
> >
>
> Or, more to the point, there is prop.trend.test(). Neither are exact
> tests, though.
>
> I think package "coin" may something relevant.
>
> -pd
>
>
> > --
> > David.
> >>
> >> Thanks very much!!
> >>
> >>  Hanna
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Jan  7 17:34:59 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 7 Jan 2016 08:34:59 -0800
Subject: [R] "par(tcl=0.5,las=1)" doesn't work
In-Reply-To: <568E222B.2060409@arcor.de>
References: <568E222B.2060409@arcor.de>
Message-ID: <5357649C-622E-4912-BE56-55144B73005E@comcast.net>


> On Jan 7, 2016, at 12:30 AM, Hans Hau?mann <hhaussmann at arcor.de> wrote:
> 
> Hi Bert Gunter and David Winsemius,
> 
> sorry for sending my first post to R-help in German and thank you David for your efforts to translate.
> 
> Here are two scripts now. In the first one the graphics parameters "tcl" and "las" are not recognized, in the second one they are. Why not in the first one?
> 
> I am not sure whether I may attach the outputs of the two scripts, I just try.

Dear Hans;

When using "external" or "non-interactive graphics devices, you need to put the par call after the device is opened rather than before it. 

Try instead:

S=read.table("Sarrazin-2.5",as.is = default.stringsAsFactors())
# moved this call after the device opening call

png(file="partest-2.png",width=580,height=580)
par(tcl=0.5,las=1)
dev.control("enable") 
plot(V3,V2,xlab=" ",ylab="BIP, rel. zu Deutschland", pch=16,xlim=c(630,930),ylim=c(95,135))
dev.off()


Best;
David.

> 
> Kind regards,
> 
> Hans Hau?mann (Haussmann)
> <partest-1.txt><partest-2.txt><partest-1.png><partest-2.png>______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From 538280 at gmail.com  Thu Jan  7 17:54:00 2016
From: 538280 at gmail.com (Greg Snow)
Date: Thu, 7 Jan 2016 09:54:00 -0700
Subject: [R] Bezier to line segments
In-Reply-To: <CAJ=0CtA99P=3ESdrBFVEVKwT94nqQK9GmWFx8UxX2vBNa3D1Ug@mail.gmail.com>
References: <CAJ=0CtCUh35L3XjFdqBoc7y+epzLzVo-76Y3aPmmkETS7zu0qA@mail.gmail.com>
	<CAJ=0CtA99P=3ESdrBFVEVKwT94nqQK9GmWFx8UxX2vBNa3D1Ug@mail.gmail.com>
Message-ID: <CAFEqCdywbFnp2EG7_BPPWvSELqG--GBELRwVi6hb+fDcDchM4w@mail.gmail.com>

You may also be interested in the xspline function (graphics package,
so you don't need to install or load anything extra) since you mention
general splines.  These splines can be made similar to Bezier curves
(but not exactly the same).  The function returns a set of coordinates
(when draw=FALSE) that represent line segments for drawing the
approximate curve.

On Wed, Jan 6, 2016 at 6:43 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
> I just found the package "bezier".
> Trying to find the needle, I missed the haystack...
>
> On Wed, Jan 6, 2016 at 9:56 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
>
>> Dear All,
>>
>> I am interested into transforming Bezier curves (or general splines) to a
>> series of line segments.
>> For simplicity, the Bezier curves are either cubic (arches, no inflection
>> points) or they have at most one inflection point.
>>
>> The entry parameters are exactly four points (with x and y coordinates):
>> - start point
>> - end point
>> - and two control points to define the curve.
>>
>> I read a lot about parabolic approximation, and there is also a famous
>> deCasteljau algorithm.
>>
>> Before attempting to create my own function, I wonder if something like
>> this already exists in R (or can easily be adapted to R).
>>
>> Thanks in advance for any hint,
>> Adrian
>>
>> --
>> Adrian Dusa
>> University of Bucharest
>> Romanian Social Data Archive
>> Soseaua Panduri nr.90
>> 050663 Bucharest sector 5
>> Romania
>>
>
>
>
> --
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr.90
> 050663 Bucharest sector 5
> Romania
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From lists at dewey.myzen.co.uk  Thu Jan  7 18:20:37 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Thu, 7 Jan 2016 17:20:37 +0000
Subject: [R] exact trend test (enumerate all possible contingency tables
 with fixed row and column margins)
In-Reply-To: <CAHLnndZkaE92n+F1=U7yUO-1b6QdafU5YcGxh-zp0YA1wbRbMg@mail.gmail.com>
References: <CAHLnndZkaE92n+F1=U7yUO-1b6QdafU5YcGxh-zp0YA1wbRbMg@mail.gmail.com>
Message-ID: <568E9E65.3010607@dewey.myzen.co.uk>

You received a number of suggestions about where to look and packages 
that might be suitable. Did you do that? If you did which ones did you 
look at and why did you reject them?

On 07/01/2016 16:29, li li wrote:
> Thanks for all the reply. Below is the data in a better format.
>
>> addmargins(dat)
>
>      dose 0 dose 0.15 dose 0.5 dose 1.5 dose 5 Sum
>
> yes      4         3        4        5      8  24
>
> no       4         5        4        3      0  16
>
> Sum      8         8        8        8      8  40
>
> I think it is easier and better that I rephrase my question. I would like
> to enumerate all possible
> contingency tables with the row margins and column margins fixed as in the
> above table. Yes. In fisher's exact test, this should have been done
> internally. But I need explicitly find all such tables. Need some help on
> this and thanks very much in advance.
>
>      Hanna
>
>
> 2016-01-07 7:15 GMT-05:00 peter dalgaard <pdalgd at gmail.com>:
>
>>
>> On 07 Jan 2016, at 08:31 , David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>>>
>>>> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
>>>>
>>>> Hi all,
>>>> Is there an R function that does exact randomization trend test?
>>>> For example, consider the 2 by 5 contingency table below:
>>>>
>>>>            dose0    dose 0.15    dose 0.5    dose 1.5    dose 5       row
>>>> margin
>>>> Yes          4                3                  4               5
>>>>     8                   24
>>>> No          4                5                   4               3
>>>>       0                  16
>>>> col sum    8                8                   8               8
>>>>   8                   40
>>>
>>> Your data presentation has been distorted by your failure to post in
>> plain text. Surely you have been asked in the past to correct this issue?
>>>
>>>>
>>>> To do the exact trend test, we need to enumerate all the contingency
>> table
>>>> with the
>>>> row and column margins fixed.
>>>
>>> Er, how should that be done? A trend test? What is described above would
>> be a general test of no association rather than a trend test. Please use
>> clear language and be as specific as possible if you choose to respond.
>>>
>>>> Find the probability corresponding to
>>>> obtaining
>>>> the corresponding contingency tables based on the multivariate
>>>> hypergeometric distribution. Finally the pvalue is obtained by adding
>>>> relevant probabilities.
>>>
>>> If there is a trend under consideration, then I do not understand such a
>> trend would be modeled under a hypergeometric distribution? A hypergeometic
>> distribution would suggest no trend, at least to my current understanding.
>>
>> I'd expect that there is such a beast as a noncentral multivariate
>> hypergeometric (for the 2x2 case that is what we use to get the CI for the
>> odds ratio), but usually, one just wants the null distribution of the test
>> statistic.
>>
>>
>>>
>>>>
>>>> Is there an R function that does this? if not, I am wondering whether
>> it is
>>>> possible to
>>>> enumerate all possible contingency tables that has column sun and row
>> sum
>>>> fixed?
>>>
>>> Wel, yes, that is possible and routinely done with `fisher.test`, but it
>> is up to you to describe how that activity leads to a trend test.
>>>
>>> If you assume Poisson distributed errors a trend test is fairly easy to
>> construct with glm.
>>>
>>
>> Or, more to the point, there is prop.trend.test(). Neither are exact
>> tests, though.
>>
>> I think package "coin" may something relevant.
>>
>> -pd
>>
>>
>>> --
>>> David.
>>>>
>>>> Thanks very much!!
>>>>
>>>>   Hanna
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From hannah.hlx at gmail.com  Thu Jan  7 19:18:54 2016
From: hannah.hlx at gmail.com (li li)
Date: Thu, 7 Jan 2016 13:18:54 -0500
Subject: [R] exact trend test (enumerate all possible contingency tables
 with fixed row and column margins)
In-Reply-To: <568E9E65.3010607@dewey.myzen.co.uk>
References: <CAHLnndZkaE92n+F1=U7yUO-1b6QdafU5YcGxh-zp0YA1wbRbMg@mail.gmail.com>
	<568E9E65.3010607@dewey.myzen.co.uk>
Message-ID: <CAHLnnda13-hNSX+CJTTZEe0G6jgU8zhgHV5HLw4oXsugz-omhw@mail.gmail.com>

I did check the coin package before. I did not see a function in that
package that can be used to list all the possible contingency tables with
fixed margins.
Of course I googled "exact trend test using R". There is not enough help
there.
For up to three groups, I can easily enumerate all the contingency table
with fixed margins, but with 5 groups it is not that easy.
But as mentioned before, this is done implicitly and routinely in
fisher.test function in R. So if anyone who have done this in R before,
please help.
Thanks.
   Hanna


2016-01-07 12:20 GMT-05:00 Michael Dewey <lists at dewey.myzen.co.uk>:

> You received a number of suggestions about where to look and packages that
> might be suitable. Did you do that? If you did which ones did you look at
> and why did you reject them?
>
>
> On 07/01/2016 16:29, li li wrote:
>
>> Thanks for all the reply. Below is the data in a better format.
>>
>> addmargins(dat)
>>>
>>
>>      dose 0 dose 0.15 dose 0.5 dose 1.5 dose 5 Sum
>>
>> yes      4         3        4        5      8  24
>>
>> no       4         5        4        3      0  16
>>
>> Sum      8         8        8        8      8  40
>>
>> I think it is easier and better that I rephrase my question. I would like
>> to enumerate all possible
>> contingency tables with the row margins and column margins fixed as in the
>> above table. Yes. In fisher's exact test, this should have been done
>> internally. But I need explicitly find all such tables. Need some help on
>> this and thanks very much in advance.
>>
>>      Hanna
>>
>>
>> 2016-01-07 7:15 GMT-05:00 peter dalgaard <pdalgd at gmail.com>:
>>
>>
>>> On 07 Jan 2016, at 08:31 , David Winsemius <dwinsemius at comcast.net>
>>> wrote:
>>>
>>>
>>>>> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
>>>>>
>>>>> Hi all,
>>>>> Is there an R function that does exact randomization trend test?
>>>>> For example, consider the 2 by 5 contingency table below:
>>>>>
>>>>>            dose0    dose 0.15    dose 0.5    dose 1.5    dose 5
>>>>>  row
>>>>> margin
>>>>> Yes          4                3                  4               5
>>>>>     8                   24
>>>>> No          4                5                   4               3
>>>>>       0                  16
>>>>> col sum    8                8                   8               8
>>>>>   8                   40
>>>>>
>>>>
>>>> Your data presentation has been distorted by your failure to post in
>>>>
>>> plain text. Surely you have been asked in the past to correct this issue?
>>>
>>>>
>>>>
>>>>> To do the exact trend test, we need to enumerate all the contingency
>>>>>
>>>> table
>>>
>>>> with the
>>>>> row and column margins fixed.
>>>>>
>>>>
>>>> Er, how should that be done? A trend test? What is described above would
>>>>
>>> be a general test of no association rather than a trend test. Please use
>>> clear language and be as specific as possible if you choose to respond.
>>>
>>>>
>>>> Find the probability corresponding to
>>>>> obtaining
>>>>> the corresponding contingency tables based on the multivariate
>>>>> hypergeometric distribution. Finally the pvalue is obtained by adding
>>>>> relevant probabilities.
>>>>>
>>>>
>>>> If there is a trend under consideration, then I do not understand such a
>>>>
>>> trend would be modeled under a hypergeometric distribution? A
>>> hypergeometic
>>> distribution would suggest no trend, at least to my current
>>> understanding.
>>>
>>> I'd expect that there is such a beast as a noncentral multivariate
>>> hypergeometric (for the 2x2 case that is what we use to get the CI for
>>> the
>>> odds ratio), but usually, one just wants the null distribution of the
>>> test
>>> statistic.
>>>
>>>
>>>
>>>>
>>>>> Is there an R function that does this? if not, I am wondering whether
>>>>>
>>>> it is
>>>
>>>> possible to
>>>>> enumerate all possible contingency tables that has column sun and row
>>>>>
>>>> sum
>>>
>>>> fixed?
>>>>>
>>>>
>>>> Wel, yes, that is possible and routinely done with `fisher.test`, but it
>>>>
>>> is up to you to describe how that activity leads to a trend test.
>>>
>>>>
>>>> If you assume Poisson distributed errors a trend test is fairly easy to
>>>>
>>> construct with glm.
>>>
>>>>
>>>>
>>> Or, more to the point, there is prop.trend.test(). Neither are exact
>>> tests, though.
>>>
>>> I think package "coin" may something relevant.
>>>
>>> -pd
>>>
>>>
>>> --
>>>> David.
>>>>
>>>>>
>>>>> Thanks very much!!
>>>>>
>>>>>   Hanna
>>>>>
>>>>>       [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>>
>>>> http://www.R-project.org/posting-guide.html
>>>
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>> David Winsemius
>>>> Alameda, CA, USA
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>>
>>> http://www.R-project.org/posting-guide.html
>>>
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> --
>>> Peter Dalgaard, Professor,
>>> Center for Statistics, Copenhagen Business School
>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>> Phone: (+45)38153501
>>> Office: A 4.23
>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>>
>>>
>>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Jan  7 19:39:31 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 7 Jan 2016 10:39:31 -0800
Subject: [R] Fwd: exact trend test (enumerate all possible contingency
 tables with fixed row and column margins)
In-Reply-To: <CAGxFJbQ9PmHLyx9zSC8XGe+UymPxjJ5WtLEbHzhA-W5qS+AN2Q@mail.gmail.com>
References: <CAHLnndZkaE92n+F1=U7yUO-1b6QdafU5YcGxh-zp0YA1wbRbMg@mail.gmail.com>
	<568E9E65.3010607@dewey.myzen.co.uk>
	<CAHLnnda13-hNSX+CJTTZEe0G6jgU8zhgHV5HLw4oXsugz-omhw@mail.gmail.com>
	<CAGxFJbQ9PmHLyx9zSC8XGe+UymPxjJ5WtLEbHzhA-W5qS+AN2Q@mail.gmail.com>
Message-ID: <CAGxFJbR2ETcOEnisB3qodwj7ZkGPa4iumvZb2nwY9EQJu4Kn9g@mail.gmail.com>

Sorry -- neglected to reply to the list. -- Bert



---------- Forwarded message ----------
From: Bert Gunter <bgunter.4567 at gmail.com>
Date: Thu, Jan 7, 2016 at 10:38 AM
Subject: Re: [R] exact trend test (enumerate all possible contingency
tables with fixed row and column margins)
To: li li <hannah.hlx at gmail.com>


I do not know whether there is any package to do what you want.

I **do** know that the algorithms required to do this are very
sophisticated and that with more than a few groups, all possible
enumerations are out of the question so that approximating shortcuts
must be used. See http://www.cytel.com/software-solutions/statxact for
some background.

I **suspect** that you have no need to do what you have requested and
**suggest** that you consult a local statistician or
stats.stackexchange.com for another approach to whatever your
underlying issue is.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Jan 7, 2016 at 10:18 AM, li li <hannah.hlx at gmail.com> wrote:
> I did check the coin package before. I did not see a function in that
> package that can be used to list all the possible contingency tables with
> fixed margins.
> Of course I googled "exact trend test using R". There is not enough help
> there.
> For up to three groups, I can easily enumerate all the contingency table
> with fixed margins, but with 5 groups it is not that easy.
> But as mentioned before, this is done implicitly and routinely in
> fisher.test function in R. So if anyone who have done this in R before,
> please help.
> Thanks.
>    Hanna
>
>
> 2016-01-07 12:20 GMT-05:00 Michael Dewey <lists at dewey.myzen.co.uk>:
>
>> You received a number of suggestions about where to look and packages that
>> might be suitable. Did you do that? If you did which ones did you look at
>> and why did you reject them?
>>
>>
>> On 07/01/2016 16:29, li li wrote:
>>
>>> Thanks for all the reply. Below is the data in a better format.
>>>
>>> addmargins(dat)
>>>>
>>>
>>>      dose 0 dose 0.15 dose 0.5 dose 1.5 dose 5 Sum
>>>
>>> yes      4         3        4        5      8  24
>>>
>>> no       4         5        4        3      0  16
>>>
>>> Sum      8         8        8        8      8  40
>>>
>>> I think it is easier and better that I rephrase my question. I would like
>>> to enumerate all possible
>>> contingency tables with the row margins and column margins fixed as in the
>>> above table. Yes. In fisher's exact test, this should have been done
>>> internally. But I need explicitly find all such tables. Need some help on
>>> this and thanks very much in advance.
>>>
>>>      Hanna
>>>
>>>
>>> 2016-01-07 7:15 GMT-05:00 peter dalgaard <pdalgd at gmail.com>:
>>>
>>>
>>>> On 07 Jan 2016, at 08:31 , David Winsemius <dwinsemius at comcast.net>
>>>> wrote:
>>>>
>>>>
>>>>>> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
>>>>>>
>>>>>> Hi all,
>>>>>> Is there an R function that does exact randomization trend test?
>>>>>> For example, consider the 2 by 5 contingency table below:
>>>>>>
>>>>>>            dose0    dose 0.15    dose 0.5    dose 1.5    dose 5
>>>>>>  row
>>>>>> margin
>>>>>> Yes          4                3                  4               5
>>>>>>     8                   24
>>>>>> No          4                5                   4               3
>>>>>>       0                  16
>>>>>> col sum    8                8                   8               8
>>>>>>   8                   40
>>>>>>
>>>>>
>>>>> Your data presentation has been distorted by your failure to post in
>>>>>
>>>> plain text. Surely you have been asked in the past to correct this issue?
>>>>
>>>>>
>>>>>
>>>>>> To do the exact trend test, we need to enumerate all the contingency
>>>>>>
>>>>> table
>>>>
>>>>> with the
>>>>>> row and column margins fixed.
>>>>>>
>>>>>
>>>>> Er, how should that be done? A trend test? What is described above would
>>>>>
>>>> be a general test of no association rather than a trend test. Please use
>>>> clear language and be as specific as possible if you choose to respond.
>>>>
>>>>>
>>>>> Find the probability corresponding to
>>>>>> obtaining
>>>>>> the corresponding contingency tables based on the multivariate
>>>>>> hypergeometric distribution. Finally the pvalue is obtained by adding
>>>>>> relevant probabilities.
>>>>>>
>>>>>
>>>>> If there is a trend under consideration, then I do not understand such a
>>>>>
>>>> trend would be modeled under a hypergeometric distribution? A
>>>> hypergeometic
>>>> distribution would suggest no trend, at least to my current
>>>> understanding.
>>>>
>>>> I'd expect that there is such a beast as a noncentral multivariate
>>>> hypergeometric (for the 2x2 case that is what we use to get the CI for
>>>> the
>>>> odds ratio), but usually, one just wants the null distribution of the
>>>> test
>>>> statistic.
>>>>
>>>>
>>>>
>>>>>
>>>>>> Is there an R function that does this? if not, I am wondering whether
>>>>>>
>>>>> it is
>>>>
>>>>> possible to
>>>>>> enumerate all possible contingency tables that has column sun and row
>>>>>>
>>>>> sum
>>>>
>>>>> fixed?
>>>>>>
>>>>>
>>>>> Wel, yes, that is possible and routinely done with `fisher.test`, but it
>>>>>
>>>> is up to you to describe how that activity leads to a trend test.
>>>>
>>>>>
>>>>> If you assume Poisson distributed errors a trend test is fairly easy to
>>>>>
>>>> construct with glm.
>>>>
>>>>>
>>>>>
>>>> Or, more to the point, there is prop.trend.test(). Neither are exact
>>>> tests, though.
>>>>
>>>> I think package "coin" may something relevant.
>>>>
>>>> -pd
>>>>
>>>>
>>>> --
>>>>> David.
>>>>>
>>>>>>
>>>>>> Thanks very much!!
>>>>>>
>>>>>>   Hanna
>>>>>>
>>>>>>       [[alternative HTML version deleted]]
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>>
>>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>
>>>>> David Winsemius
>>>>> Alameda, CA, USA
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>>
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>> --
>>>> Peter Dalgaard, Professor,
>>>> Center for Statistics, Copenhagen Business School
>>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>>> Phone: (+45)38153501
>>>> Office: A 4.23
>>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>>>
>>>>
>>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ben.bighair at gmail.com  Thu Jan  7 15:27:33 2016
From: ben.bighair at gmail.com (Ben Tupper)
Date: Thu, 7 Jan 2016 09:27:33 -0500
Subject: [R] "par(tcl=0.5,las=1)" doesn't work
In-Reply-To: <568E222B.2060409@arcor.de>
References: <568E222B.2060409@arcor.de>
Message-ID: <F151ED6A-7616-4989-A54C-F09ED2369E79@gmail.com>

Hi,

I think it may be the order in which your par() configuration occurs relative to png().  When you open a new device, as you have with png(), the default par() values for that device are exposed. If you move your par() statement to *after* your call to png() you get the result you desire.

S=read.table("Sarrazin-2.5",as.is = default.stringsAsFactors())
#par(tcl=0.5,las=1)   # - move to after you open the device
png(file="partest-1.png",width=580,height=580)
par(tcl=0.5,las=1)    # - configure the device after opening it
dev.control("enable") 
plot(V3,V2,xlab=" ",ylab="BIP, rel. zu Deutschland", pch=16,xlim=c(630,930),ylim=c(95,135))
dev.off()

See the 'details' section of ?par  - "Each device has its own set of graphical parameters."  I take that to mean a new set of parameters are exposed.

Cheers,
Ben

P.S.  I wonder if that sentence might have more value if it read like this, "Configure each device after opening as each device has its own set of graphical parameters."

> On Jan 7, 2016, at 3:30 AM, Hans Hau?mann <hhaussmann at arcor.de> wrote:
> 
> Hi Bert Gunter and David Winsemius,
> 
> sorry for sending my first post to R-help in German and thank you David for your efforts to translate.
> 
> Here are two scripts now. In the first one the graphics parameters "tcl" and "las" are not recognized, in the second one they are. Why not in the first one?
> 
> I am not sure whether I may attach the outputs of the two scripts, I just try.
> 
> Kind regards,
> 
> Hans Hau?mann (Haussmann)
> <partest-1.txt><partest-2.txt><partest-1.png><partest-2.png>______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From alaasindi at gmail.com  Thu Jan  7 21:43:53 2016
From: alaasindi at gmail.com (Alaa Sindi)
Date: Thu, 7 Jan 2016 15:43:53 -0500
Subject: [R] mlogit.optim
Message-ID: <AA715E5E-2F48-47ED-A695-7C072055F393@gmail.com>

Hi All,

do anyone have an example about mlogit.optim function that explain how to use this function other than the one in the manual. 

Thanks

From axel.urbiz at gmail.com  Fri Jan  8 00:34:44 2016
From: axel.urbiz at gmail.com (Axel Urbiz)
Date: Thu, 7 Jan 2016 18:34:44 -0500
Subject: [R] model.matrix behaviour
Message-ID: <E7DE7A27-7C2C-4CD0-9AD0-BC73398F1101@gmail.com>

Hello,

I apologize my prior email was sent in html. 

It is not very clear to me from the model.matrix documentation, why simply changing the order of terms in the formula may give a different design matrix. Please note I?m purposely not including main effects in the model formulae.

set.seed(1)
x1 <- rnorm(100)
f1 <- factor(sample(letters[1:3], 100, replace = TRUE))
trt <- sample(c(-1,1), 100, replace = TRUE)
df <- data.frame(x1=x1, f1=f1, trt=trt)

head(model.matrix( ~ x1:trt + f1:trt, data = df))
(Intercept)     x1:trt trt:f1b trt:f1c
1           1 -0.6264538       0       0
2           1 -0.1836433       0       0
3           1  0.8356286      -1       0
4           1 -1.5952808       0       0
5           1 -0.3295078       0       0
6           1 -0.8204684       1       0

head(model.matrix(~ f1:trt + x1:trt, data = df)) 
(Intercept) f1a:trt f1b:trt f1c:trt     trt:x1
1           1       1       0       0 -0.6264538
2           1      -1       0       0 -0.1836433
3           1       0      -1       0  0.8356286
4           1      -1       0       0 -1.5952808
5           1      -1       0       0 -0.3295078
6           1       0       1       0 -0.8204684

Thanks,
Axel.


From manishm at dbs.com  Fri Jan  8 09:35:45 2016
From: manishm at dbs.com (Manish MAHESHWARI)
Date: Fri, 8 Jan 2016 08:35:45 +0000
Subject: [R] NAs introduced by coercion
Message-ID: <8B5BC7735651764E884E3651BAA48D9A08D8C4D3@W01GMAILDAGA52.reg1.1bank.dbs.com>

Hi,

In glmnet, while using a matrix as an input, I get an error of NA's introduced by coercion. However in the input there is no NA value.

cvfit = cv.glmnet( x = mat1, y = train$response,family="multinomial", type.multinomial = "grouped", parallel = TRUE)
Error in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  :
  NA/NaN/Inf in foreign function call (arg 5)
In addition: Warning message:
In lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  :
  NAs introduced by coercion

> anyisna = function(x) {any(is.na(x))}

> nas = apply(mat1, MARGIN = 2, FUN = anyisna)



> nas[nas==T]

named logical(0)



> any(is.na(train$response))

[1] FALSE


Can you pls throw some light on where is the NA found.

Thanks,
Manish

CONFIDENTIAL NOTE:
The information contained in this email is intended only...{{dropped:11}}


From jdnewmil at dcn.davis.ca.us  Fri Jan  8 09:43:58 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 08 Jan 2016 00:43:58 -0800
Subject: [R] A question about corAR1 and grouping
In-Reply-To: <CAJDot1qUbu=mw+_Lhbuewxx+WypJUKGyT55+vqG5YqHk8jSwwA@mail.gmail.com>
References: <CAJDot1qUbu=mw+_Lhbuewxx+WypJUKGyT55+vqG5YqHk8jSwwA@mail.gmail.com>
Message-ID: <ADCD7976-F9AB-4062-BE9C-7C77C8BE95B6@dcn.davis.ca.us>

I don't understand your question, but lacking a reproducible example you may not get the attention of those who might be more likely to. 
-- 
Sent from my phone. Please excuse my brevity.

On January 6, 2016 10:55:08 AM PST, Saptarshi Guha <saptarshi.guha at gmail.com> wrote:
>Hello,
>
>I was under the impression that in a call to gls, if i specify
>
>gls(.., cor=AR1(form= ~ 1 | subject))
>
>and suppose there are N subjects, then i would have obtained estimates
>for N Phi (s) (the AR1 coefficient) - but I only get 1 estimate.
>
>I then checked "Linear Mixed-Effects Models Using R" and it i must
>have confused it for the weights(e.g. varIdent) where there are indeed
>group(strata) specific estimates.
>
>Is there a way to include correlations  and have group specific
>estimates?
>
>Regards
>Saptarshi
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Jan  8 12:01:43 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 8 Jan 2016 11:01:43 +0000
Subject: [R] NAs introduced by coercion
In-Reply-To: <8B5BC7735651764E884E3651BAA48D9A08D8C4D3@W01GMAILDAGA52.reg1.1bank.dbs.com>
References: <8B5BC7735651764E884E3651BAA48D9A08D8C4D3@W01GMAILDAGA52.reg1.1bank.dbs.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5009081@SRVEXCHMBX.precheza.cz>

Hi

The values in mat1 are not NA. OK. However they shall be numbers. Are they numbers?

What is result of

str(mat1)

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Manish
> MAHESHWARI
> Sent: Friday, January 08, 2016 9:36 AM
> To: r-help at r-project.org
> Subject: [R] NAs introduced by coercion
>
> Hi,
>
> In glmnet, while using a matrix as an input, I get an error of NA's
> introduced by coercion. However in the input there is no NA value.
>
> cvfit = cv.glmnet( x = mat1, y = train$response,family="multinomial",
> type.multinomial = "grouped", parallel = TRUE) Error in lognet(x,
> is.sparse, ix, jx, y, weights, offset, alpha, nobs,  :
>   NA/NaN/Inf in foreign function call (arg 5) In addition: Warning
> message:
> In lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  :
>   NAs introduced by coercion
>
> > anyisna = function(x) {any(is.na(x))}
>
> > nas = apply(mat1, MARGIN = 2, FUN = anyisna)
>
>
>
> > nas[nas==T]
>
> named logical(0)
>
>
>
> > any(is.na(train$response))
>
> [1] FALSE
>
>
> Can you pls throw some light on where is the NA found.
>
> Thanks,
> Manish
>
> CONFIDENTIAL NOTE:
> The information contained in this email is intended
> only...{{dropped:11}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From drjimlemon at gmail.com  Fri Jan  8 12:20:57 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 8 Jan 2016 22:20:57 +1100
Subject: [R] NAs introduced by coercion
In-Reply-To: <8B5BC7735651764E884E3651BAA48D9A08D8C4D3@W01GMAILDAGA52.reg1.1bank.dbs.com>
References: <8B5BC7735651764E884E3651BAA48D9A08D8C4D3@W01GMAILDAGA52.reg1.1bank.dbs.com>
Message-ID: <CA+8X3fXJPCBfNUrATcp_QgdR4vxhYy0ufUp9d6BB2q5fX0n5pg@mail.gmail.com>

Hi Manish,
Most likely there are one or more negative numbers in mat1.

Jim


On Fri, Jan 8, 2016 at 7:35 PM, Manish MAHESHWARI <manishm at dbs.com> wrote:

> Hi,
>
> In glmnet, while using a matrix as an input, I get an error of NA's
> introduced by coercion. However in the input there is no NA value.
>
> cvfit = cv.glmnet( x = mat1, y = train$response,family="multinomial",
> type.multinomial = "grouped", parallel = TRUE)
> Error in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  :
>   NA/NaN/Inf in foreign function call (arg 5)
> In addition: Warning message:
> In lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  :
>   NAs introduced by coercion
>
> > anyisna = function(x) {any(is.na(x))}
>
> > nas = apply(mat1, MARGIN = 2, FUN = anyisna)
>
>
>
> > nas[nas==T]
>
> named logical(0)
>
>
>
> > any(is.na(train$response))
>
> [1] FALSE
>
>
> Can you pls throw some light on where is the NA found.
>
> Thanks,
> Manish
>
> CONFIDENTIAL NOTE:
> The information contained in this email is intended on...{{dropped:13}}


From hannah.hlx at gmail.com  Fri Jan  8 15:30:10 2016
From: hannah.hlx at gmail.com (li li)
Date: Fri, 8 Jan 2016 09:30:10 -0500
Subject: [R] Fwd: exact trend test (enumerate all possible contingency
 tables with fixed row and column margins)
In-Reply-To: <CAGxFJbR2ETcOEnisB3qodwj7ZkGPa4iumvZb2nwY9EQJu4Kn9g@mail.gmail.com>
References: <CAHLnndZkaE92n+F1=U7yUO-1b6QdafU5YcGxh-zp0YA1wbRbMg@mail.gmail.com>
	<568E9E65.3010607@dewey.myzen.co.uk>
	<CAHLnnda13-hNSX+CJTTZEe0G6jgU8zhgHV5HLw4oXsugz-omhw@mail.gmail.com>
	<CAGxFJbQ9PmHLyx9zSC8XGe+UymPxjJ5WtLEbHzhA-W5qS+AN2Q@mail.gmail.com>
	<CAGxFJbR2ETcOEnisB3qodwj7ZkGPa4iumvZb2nwY9EQJu4Kn9g@mail.gmail.com>
Message-ID: <CAHLnndaJHVL41e50Mvt8TzLPRoNJ-qy2cfnLcUV685E7+UoohA@mail.gmail.com>

Thanks Bert.


2016-01-07 13:39 GMT-05:00 Bert Gunter <bgunter.4567 at gmail.com>:

> Sorry -- neglected to reply to the list. -- Bert
>
>
>
> ---------- Forwarded message ----------
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Date: Thu, Jan 7, 2016 at 10:38 AM
> Subject: Re: [R] exact trend test (enumerate all possible contingency
> tables with fixed row and column margins)
> To: li li <hannah.hlx at gmail.com>
>
>
> I do not know whether there is any package to do what you want.
>
> I **do** know that the algorithms required to do this are very
> sophisticated and that with more than a few groups, all possible
> enumerations are out of the question so that approximating shortcuts
> must be used. See http://www.cytel.com/software-solutions/statxact for
> some background.
>
> I **suspect** that you have no need to do what you have requested and
> **suggest** that you consult a local statistician or
> stats.stackexchange.com for another approach to whatever your
> underlying issue is.
>
> Cheers,
> Bert
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Jan 7, 2016 at 10:18 AM, li li <hannah.hlx at gmail.com> wrote:
> > I did check the coin package before. I did not see a function in that
> > package that can be used to list all the possible contingency tables with
> > fixed margins.
> > Of course I googled "exact trend test using R". There is not enough help
> > there.
> > For up to three groups, I can easily enumerate all the contingency table
> > with fixed margins, but with 5 groups it is not that easy.
> > But as mentioned before, this is done implicitly and routinely in
> > fisher.test function in R. So if anyone who have done this in R before,
> > please help.
> > Thanks.
> >    Hanna
> >
> >
> > 2016-01-07 12:20 GMT-05:00 Michael Dewey <lists at dewey.myzen.co.uk>:
> >
> >> You received a number of suggestions about where to look and packages
> that
> >> might be suitable. Did you do that? If you did which ones did you look
> at
> >> and why did you reject them?
> >>
> >>
> >> On 07/01/2016 16:29, li li wrote:
> >>
> >>> Thanks for all the reply. Below is the data in a better format.
> >>>
> >>> addmargins(dat)
> >>>>
> >>>
> >>>      dose 0 dose 0.15 dose 0.5 dose 1.5 dose 5 Sum
> >>>
> >>> yes      4         3        4        5      8  24
> >>>
> >>> no       4         5        4        3      0  16
> >>>
> >>> Sum      8         8        8        8      8  40
> >>>
> >>> I think it is easier and better that I rephrase my question. I would
> like
> >>> to enumerate all possible
> >>> contingency tables with the row margins and column margins fixed as in
> the
> >>> above table. Yes. In fisher's exact test, this should have been done
> >>> internally. But I need explicitly find all such tables. Need some help
> on
> >>> this and thanks very much in advance.
> >>>
> >>>      Hanna
> >>>
> >>>
> >>> 2016-01-07 7:15 GMT-05:00 peter dalgaard <pdalgd at gmail.com>:
> >>>
> >>>
> >>>> On 07 Jan 2016, at 08:31 , David Winsemius <dwinsemius at comcast.net>
> >>>> wrote:
> >>>>
> >>>>
> >>>>>> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
> >>>>>>
> >>>>>> Hi all,
> >>>>>> Is there an R function that does exact randomization trend test?
> >>>>>> For example, consider the 2 by 5 contingency table below:
> >>>>>>
> >>>>>>            dose0    dose 0.15    dose 0.5    dose 1.5    dose 5
> >>>>>>  row
> >>>>>> margin
> >>>>>> Yes          4                3                  4               5
> >>>>>>     8                   24
> >>>>>> No          4                5                   4               3
> >>>>>>       0                  16
> >>>>>> col sum    8                8                   8               8
> >>>>>>   8                   40
> >>>>>>
> >>>>>
> >>>>> Your data presentation has been distorted by your failure to post in
> >>>>>
> >>>> plain text. Surely you have been asked in the past to correct this
> issue?
> >>>>
> >>>>>
> >>>>>
> >>>>>> To do the exact trend test, we need to enumerate all the contingency
> >>>>>>
> >>>>> table
> >>>>
> >>>>> with the
> >>>>>> row and column margins fixed.
> >>>>>>
> >>>>>
> >>>>> Er, how should that be done? A trend test? What is described above
> would
> >>>>>
> >>>> be a general test of no association rather than a trend test. Please
> use
> >>>> clear language and be as specific as possible if you choose to
> respond.
> >>>>
> >>>>>
> >>>>> Find the probability corresponding to
> >>>>>> obtaining
> >>>>>> the corresponding contingency tables based on the multivariate
> >>>>>> hypergeometric distribution. Finally the pvalue is obtained by
> adding
> >>>>>> relevant probabilities.
> >>>>>>
> >>>>>
> >>>>> If there is a trend under consideration, then I do not understand
> such a
> >>>>>
> >>>> trend would be modeled under a hypergeometric distribution? A
> >>>> hypergeometic
> >>>> distribution would suggest no trend, at least to my current
> >>>> understanding.
> >>>>
> >>>> I'd expect that there is such a beast as a noncentral multivariate
> >>>> hypergeometric (for the 2x2 case that is what we use to get the CI for
> >>>> the
> >>>> odds ratio), but usually, one just wants the null distribution of the
> >>>> test
> >>>> statistic.
> >>>>
> >>>>
> >>>>
> >>>>>
> >>>>>> Is there an R function that does this? if not, I am wondering
> whether
> >>>>>>
> >>>>> it is
> >>>>
> >>>>> possible to
> >>>>>> enumerate all possible contingency tables that has column sun and
> row
> >>>>>>
> >>>>> sum
> >>>>
> >>>>> fixed?
> >>>>>>
> >>>>>
> >>>>> Wel, yes, that is possible and routinely done with `fisher.test`,
> but it
> >>>>>
> >>>> is up to you to describe how that activity leads to a trend test.
> >>>>
> >>>>>
> >>>>> If you assume Poisson distributed errors a trend test is fairly easy
> to
> >>>>>
> >>>> construct with glm.
> >>>>
> >>>>>
> >>>>>
> >>>> Or, more to the point, there is prop.trend.test(). Neither are exact
> >>>> tests, though.
> >>>>
> >>>> I think package "coin" may something relevant.
> >>>>
> >>>> -pd
> >>>>
> >>>>
> >>>> --
> >>>>> David.
> >>>>>
> >>>>>>
> >>>>>> Thanks very much!!
> >>>>>>
> >>>>>>   Hanna
> >>>>>>
> >>>>>>       [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>>>
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>
> >>>>> David Winsemius
> >>>>> Alameda, CA, USA
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>>
> >>>> http://www.R-project.org/posting-guide.html
> >>>>
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>
> >>>> --
> >>>> Peter Dalgaard, Professor,
> >>>> Center for Statistics, Copenhagen Business School
> >>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >>>> Phone: (+45)38153501
> >>>> Office: A 4.23
> >>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>>>
> >>>>
> >>>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >> --
> >> Michael
> >> http://www.dewey.myzen.co.uk/home.html
> >>
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From batholdy at googlemail.com  Fri Jan  8 16:07:08 2016
From: batholdy at googlemail.com (Martin Batholdy)
Date: Fri, 8 Jan 2016 16:07:08 +0100
Subject: [R] plot 2D matrix of RGB values
Message-ID: <3AA05027-DBEB-4873-BB3E-E091E57147EF@googlemail.com>

Hi,

I have a 2 dimensional matrix with RGB values and would like to plot it as a two dimensional surface.

I am aware of functions like image() that plot a matrix of values as a grid of coloured rectangles.
But I can not directly feed in the specific color value for each of these rectangles, as far as I understand.

Is there an easy way to just take a matrix of color values and plot it in R?

Thank You!

From mdsumner at gmail.com  Fri Jan  8 16:34:45 2016
From: mdsumner at gmail.com (Michael Sumner)
Date: Fri, 08 Jan 2016 15:34:45 +0000
Subject: [R] plot 2D matrix of RGB values
In-Reply-To: <3AA05027-DBEB-4873-BB3E-E091E57147EF@googlemail.com>
References: <3AA05027-DBEB-4873-BB3E-E091E57147EF@googlemail.com>
Message-ID: <CAAcGz98PO_zBo12-LVbx41KNwpfAw42xFJXJ8sj01nLH7FZ62w@mail.gmail.com>

There is ?rasterImage which can take a matrix of hex values, or a 3D array
of individual colour dimension values in RGB. It's pretty low-level, used
to add to an existing plot. The ancient ?image function leverages this
facility if you use "useRaster = TRUE".

The "raster" package (no relation), uses the rasterImage engine to
efficiently plot high-level raster objects, and provides a wide range of
tools for upgrading a matrix to raster objects. There is a floating naming
clash with the "raster" class of grDevices and the RasterLayer and
raster::raster() functions to be aware of.

Confusing? Yes, but so is "2 dimensional matrix with RGB values". In R,
such a matrix would be 3D - two for the spatial X/Y matrix part, and one
for each of the R, G, B values. In R-context, it could be 2D by storing as
a hex character string. Both objects are understood by ?rasterImage and
there are facilities for conversion.

"Plotting as a surface" is also ambiguous, but we can use ?persp  for a
perspective view analogous to ?image (including par-ameter-like
transformations available for "overplotting"), or rgl::quads3d and friends
for interactive use.

Cheers, Mike.





On Sat, 9 Jan 2016 at 02:09 Martin Batholdy via R-help <r-help at r-project.org>
wrote:

> Hi,
>
> I have a 2 dimensional matrix with RGB values and would like to plot it as
> a two dimensional surface.
>
> I am aware of functions like image() that plot a matrix of values as a
> grid of coloured rectangles.
> But I can not directly feed in the specific color value for each of these
> rectangles, as far as I understand.
>
> Is there an easy way to just take a matrix of color values and plot it in
> R?
>
> Thank You!
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From wornimatthias at gmail.com  Fri Jan  8 17:07:28 2016
From: wornimatthias at gmail.com (Matthias Worni)
Date: Fri, 8 Jan 2016 17:07:28 +0100
Subject: [R] (no subject)
Message-ID: <CAM0cwgUHUf0L8Xf-KgOfMvMoj_sPexjyHgOZN_fnfNs=eNwCpw@mail.gmail.com>

Thank you for the help. It worked out fine. However trying to add
additional aesthetics I came to another point, where I couldnt figure out a
way to solve it. Perhaps you run into similar problems since your familiar
with the ggplot package.

The code is the following:

runmean_10yr_Nino_3_Index_17th_century <-
SMA(SST_NINO3_detrended[0:100],n=10)
runmean_10yr_Nino_3_Index_18th_century <-
SMA(SST_NINO3_detrended[101:200],n=10)
runmean_10yr_Nino_3_Index_19th_century <-
SMA(SST_NINO3_detrended[201:300],n=10)
runmean_10yr_Nino_3_Index_20th_century <-
SMA(SST_NINO3_detrended[301:400],n=10)


time_Nino3 <- c(1:100)
runmean_yearly_decadal_Nino_Index <-
data.frame(runmean_10yr_Nino_3_Index_17th_century,runmean_10yr_Nino_3_Index_18th_century,runmean_10yr_Nino_3_Index_19th_century,runmean_10yr_Nino_3_Index_20th_century,time_Nino3)
melted3 <- melt(runmean_yearly_decadal_Nino_Index , id.vars="time_Nino3")

ggplot(data=melted3, aes(x=time_Nino3,y=value,colour=variable))+
  geom_line()+ ylab("SST Anomaly") + xlab("year") + ggtitle("Decadal El
Nino Index ")+ facet_grid(variable ~ .) +
  geom_vline(aes(xintercept = xp),data=dat.vline)

dat.vline <- data.frame( xp = c(15, 23))


As you may have noticed I created several plots below each other and now I
would like  to add vertical lines to this plots, therefore adding
"geom_vline(aes(xintercept = xp),data=dat.vline). This worked so far,
however I would like to add vertical lines for these four different plots
at different places. Do you know how I could do this? I added the plot I
created at this e-mail. You can see two lines on every of those 4 plots (at
the same place). What I want is the same thing but the lines at different
places for every of those 4 plots (independently). Thank you for the help!!
If you need to create the plots you can simply change the
"runmean_10yr_Nino_3_Index_17th_century" to any other vector.

Best Matthias

From Larry.John at anser.org  Thu Jan  7 21:22:22 2016
From: Larry.John at anser.org (John, Larry)
Date: Thu, 7 Jan 2016 20:22:22 +0000
Subject: [R] Scripting Problem--Linear Regression Using Logit of the
 Response Variable
Message-ID: <F26D18C3423DD149B0C3203622C7270F06CE20CE@HQNEX1.anser.org>

Dear fellow citizens of the R-verse,

I'm a non-quant analyst trying to do some statistical analyses on a large data set unimaginatively named "data."

  sympathy trust fear greed sharer_prob
[1]      3     2    0     0    0.669593
[2]      2     1    2     3    0.669593
[3]      2     2    2     3    0.494675
[4]      2     2    1     2    0.494675
[5[      2     2    2     0    0.556837
[6]      2     2    1     1    0.556837

"sharer_prob" is the continuous dependent variable with values 0 > sharer_prob > 1, so I know need to run a logit transformation before I can do any linear regressions to study the main and interaction effects of the four independent variables.

I've loaded "car" and tried for several hours to create a one-line script that will give me the overall linear model, but keep having problems. It seems to me that something like the following should work


> EvacLM <- lm((logit(sharer_prob) ~ sympathy + trust + fear + greed) adjust=TRUE) na.action=NULL)

But R politely tells me I've gotten something wrong:


Error: unexpected symbol in "EvacLM <- lm((logit(sharer_prob) ~ sympathy + trust + fear + greed) adjust"

I'm hoping one of you can spot and help me correct my mistake-I sure can't figure it out.

Thanks for any help you can offer!

Larry John
Principal Analyst
ANSER (www.anser.org)

	[[alternative HTML version deleted]]


From rmcgu at doh.health.nsw.gov.au  Fri Jan  8 00:15:26 2016
From: rmcgu at doh.health.nsw.gov.au (MCGUIRE, Rhydwyn)
Date: Thu, 7 Jan 2016 23:15:26 +0000
Subject: [R] mlogit.optim
In-Reply-To: <AA715E5E-2F48-47ED-A695-7C072055F393@gmail.com>
References: <AA715E5E-2F48-47ED-A695-7C072055F393@gmail.com>
Message-ID: <AF36C32BE015CB48883C4A9F73CC8C320174892552@DOHNSMXDB03.doh.health.nsw.gov.au>

Hi Alaa, 

I haven't personally used the mlogit.optim fuction. Have you tried searching github or stack exchange for the string "mlogit.optim" . I find they can be very useful for finding examples of a function being used in practice. 

Cheers 
Rhydwyn 


Rhydwyn McGuire
Senior Biostatistician | Health Statistics NSW
Level 7, 73 Miller St, North Sydney 2060
Tel 02 9391 9781 | rmcgu at doh.health.nsw.gov.au
www.health.nsw.gov.au




-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Alaa Sindi
Sent: Friday, 8 January 2016 7:44 AM
To: r-help at r-project.org
Subject: [R] mlogit.optim

Hi All,

do anyone have an example about mlogit.optim function that explain how to use this function other than the one in the manual. 

Thanks
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
__________________________________________________________________________________________________________
This email has been scanned for the NSW Ministry of Health by the Websense Hosted Email Security System.
Emails and attachments are monitored to ensure compliance with the NSW Ministry of health's Electronic Messaging Policy.
__________________________________________________________________________________________________________
_______________________________________________________________________________________________________
Disclaimer: This message is intended for the addressee named and may contain confidential information.
If you are not the intended recipient, please delete it and notify the sender.
Views expressed in this message are those of the individual sender, and are not necessarily the views of the NSW Ministry of Health.
_______________________________________________________________________________________________________
This email has been scanned for the NSW Ministry of Health by the Websense Hosted Email Security System.
Emails and attachments are monitored to ensure compliance with the NSW Ministry of Health's Electronic Messaging Policy.


From m.dogan at mail.com  Fri Jan  8 01:23:47 2016
From: m.dogan at mail.com (Mehmet Dogan)
Date: Fri, 8 Jan 2016 01:23:47 +0100
Subject: [R] Please help with volatility calculation in R (For my thesis)
Message-ID: <568F0193.3050404@mail.com>

Dear Sir or Madam,

Since long time I am trying to complete my thesis, but was not able to 
complete due to the problem with R program. I do have R_codes 
(attached), but I just can calculate the parameters with the attached 
code. (EUR/USD currency pair, weekends are excluded thanks to code 
attached). I could not calculate the average volatility for 10 minutes 
intervals.

I need to calculate average volatility for 10 minutes time intervals of 
my data set and plot them the results in one plot as in the sample 
attached(plot_example.png based on the GARCH(1,1), EGARCH(1,1) and 
TGARCH(1,1) models.  My codes are not enough to do it.

I am wondering if you could help me with my thesis.

Many many thanks in advance,

Yours sincerely,

Mehmet Dogan
+48 732 926 877
mehmetdogan_07 at windowslive.com

-- 
Mehmet Dogan
m.dogan at mail.com
+48 732 926 877



---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus
-------------- next part --------------

setwd("D:\\NewThesis")

# installation of needed packages
install.packages("fBasics")
install.packages("tseries")
install.packages("car")
install.packages("FinTS")
install.packages("fGarch")
install.packages("rugarch")


# Library Installations
library(xts)
library(fBasics) # e.g. basicStats()
library(tseries)# e.g. jarque.bera.test()
library(car) # e.g. durbinWatsonTest()
library(FinTS) # e.g. ArchTest()
library(fGarch) # e.g. garchFit()
library(rugarch) # e.g. ugarchfit()

source("functions/TSA_lab06_functions.R")

# Data import
EUR <- read.csv("Data_Forecast/EURUSD.csv",
                stringsAsFactors = FALSE)  #o import data

# First and last 6 data
head(EUR)
tail(EUR)

# formatting the date and time - without the need for a lubridate package
EUR$Date <- strptime(EUR$Date, format = "%d.%m.%Y %H:%M:%S")

str(EUR)

# The code was not working as you should not use the as.Date() function 
# which only works on dates (not times)

EUR <- EUR[EUR$Date >= strptime("2015-01-01 23:00:00", format = "%Y-%m-%d %H:%M:%S"),]

# now it works


# we need just weekdays
wday_ <- as.POSIXlt(EUR$Date)$wday
EUR <- EUR[!wday_ %in% c(0,6) ,]

# lets create vector of hours and minutes based on the existing data
hour_ <- as.POSIXlt(EUR$Date)$hour
minute_ <- as.POSIXlt(EUR$Date)$min

###########################################################################
# Exclude first and last 5 minutes
exclude_ <- ((hour_ == 23 & minute_ < 6) | (hour_ == 22 & minute_ > 55) )*1

EUR <- EUR[exclude_ == 0,]

head(EUR)
tail(EUR)


# xts object creation
EUR.xts<-xts(EUR$Close,EUR$Date)

# plot of original close pricess
plot(EUR$Date,EUR$Close,type="l", col="blue",
     main="EUR/USD")

# log-returns calculation
EUR$r<-diff.xts(log(EUR$Close))

# log returns plot
plot(EUR$Date,EUR$r,type="l", col="red",
     main="Log-returns of USD/JPY")
abline(h=0,col="gray",lty=2) #abline functions adds line to the plot


#############################################################################
#GARCH MODEL
# remove missing values and zeroes
data_ <- EUR[!is.na(EUR$r) & EUR$r!=0,]

k.garch11<-garchFit(~garch(1,1),
                    data = 100*data_$r,
                    include.mean=F,
                    cond.dist= "norm", # conditional distribution of errors
                    trace=FALSE) # if we don't want to see history of iterations


summary(k.garch11)
# lets see if the model converges on a shorter sample
k.garch11<-garchFit(~garch(1,1),
                    data = 100*data_$r[1:15000],
                    include.mean=F,
                    cond.dist= "norm", # conditional distribution of errors
                    trace=FALSE) # if we don't want to see history of iterations

k.garch11<-garchFit(~garch(1,1),
                    data = 100*data_$r[-c(1:15000)],
                    include.mean=F,
                    cond.dist= "norm", # conditional distribution of errors
                    trace=FALSE) # if we don't want to see history of iterations

plot(100*data_$r[30000:32000], type="l")

which(100*data_$r > 0.9)

k.garch11<-garchFit(~garch(1,1),
                    data = 100*data_$r[-15000],
                    include.mean=F,
                    cond.dist= "norm", # conditional distribution of errors
                    trace=FALSE) # if we don't want to see history of iterations

summary(k.garch11)


# Does the best model have all parameters significant? 

# Let's assume that the final model is GARCH(1,1) 
str(k.garch11)

# 7.
# Plot of conditional variance estimates
par(mfrow=c(2,1))
plot(k.garch11 at data, # @data = original data values
     type="l",col="red",ylab="r",main="Log-returns of EUR/USD")
plot(k.garch11 at h.t, # @h.t = conditional variance
     type="l",col="blue",
     ylab="cvar",main="Estimated Volatility of EUR/USD")
par(mfrow=c(1,1))


# Do standardized residuals come from normal distribution?
stdres<-k.garch11 at residuals/sqrt(k.garch11 at h.t)

hist(stdres,breaks=20,prob=T,
     main="Histogram of standardized residuals \n from GARCH(1,1) for EUR/USD")
# lets add a normal density curve for 
# sample mean and variance 
curve(dnorm(x, mean=mean(stdres,na.rm=T), 
            sd=sd(stdres,na.rm=T)), 
      col="darkblue", lwd=2, add=TRUE)

# Jarque-Bera test
jarque.bera.test(stdres)

# normalit yrejected

# Durbin Watson test
durbinWatsonTest(lm(stdres~1),
                 max.lag=5) # lets check first 5 orders

# ARCH effects among standardized residuals
ArchTest(stdres,lags=5)

# How should we interpret these statistics?

###########################################################################################################       
# The EGARCH model     
#########################################################################

# Let's examine whether conditional variance reacts asymmetrically 
# to the news arriving to the market.
# Below estimation of the EGARCH(1,1) model.

# ugarchfit() from rugarch package

# lets first define a model specification
spec = ugarchspec(# variance equation
  variance.model=list(model="eGARCH",garchOrder=c(1,1)),
  # sGARCH would stand for standard GARCH model
  # mean equation
  mean.model=list(armaOrder=c(0,0),include.mean=F), 
  # assumed distribution of errors
  distribution.model="norm")

# function doesn't accept missing values
k.egarch11 = ugarchfit( spec=spec, data=na.omit(EUR$r))

k.egarch11


# coefficient alpha captures the sign effect and gamma the size effect
# alpha is negative and significant, so the asymmetry is found

# Plot of conditional standard deviation estimates (3)
# and News-Impact curve (12).
# ESC to exit

plot(k.egarch11)



##########################################################################                                   
# The TGARCH model
###################################################################

# lets first define a model specification
spec = ugarchspec(# variance equation
  variance.model=list(model="fGARCH",garchOrder=c(1,1),
                      submodel="TGARCH"),
  # model="fGARCH" (family GARCH) together with submodel="TGARCH"
  # mean equation
  mean.model=list(armaOrder=c(0,0),include.mean=F), 
  # assumed distribution of errors
  distribution.model="norm")

# function doesn't accept missing values
k.tgarch11 = ugarchfit( spec=spec, data=na.omit(EUR$r))

k.tgarch11


# Plot of News-Impact curve (12).
# ESC to exit
plot(k.tgarch11)

# What is our conclusion? 
# Do we see asymmetry in the conditional variance function?


# CAUTION !!!
### all above options can be combined !!!   HOW????


-------------- next part --------------
A non-text attachment was scrubbed...
Name: plot_example.png
Type: image/png
Size: 112997 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160108/cbf646db/attachment.png>

From hiroysato at gmail.com  Fri Jan  8 04:12:35 2016
From: hiroysato at gmail.com (Hiroyuki Sato)
Date: Fri, 08 Jan 2016 03:12:35 +0000
Subject: [R] [Q] It it possible to create the data frame only non-zero data
	column?
Message-ID: <CA+Tq-RpThb89BpYofhvAxDAZA_t+geEMxGyye4ZuHngfVHbBwg@mail.gmail.com>

Hello all.

I re-post this question by e-mail.
(I posted via google-group. But It's not posted yet.)

I'm newbie GNU R.

I would like to compare two datas.
How to select columns which has non-zero datas?.
It it possible to create the data frame only VAL3(non-zero data) column
with command?

Formatted sample.
https://gist.github.com/hiroyuki-sato/cb36584f6cd5845b6c3e

sample1.txt

  ID,VAL1,VAL2,VAL3
  ID1,0,2,3
  ID2,0,2,3
  ID3,0,2,3

  real data has 5000 columns.

sample2.txt

  ID,VAL1,VAL2,VAL3
  ID1,0,2,3
  ID2,0,2,3
  ID3,0,2,2

  The difference sample1 and sample2 is ID3/VAL3.
    sample1: 3
    sample2: 2

R commands.

  sample1 <- read.table("sample1.txt",header=T,sep=',')
  sample2 <- read.table("sample2.txt",header=T,sep=',')

  result <- sample1[,2:4] - sample2[,2:4]
  result
    VAL1 VAL2 VAL3
  1    0    0    0
  2    0    0    0
  3    0    0    1

I would like to create data frame which has non-zero value columns.
Could you tell me how to do it?

Best regards.

--
Hiroyuki Sato.

	[[alternative HTML version deleted]]


From ravi.varadhan at jhu.edu  Fri Jan  8 17:28:52 2016
From: ravi.varadhan at jhu.edu (Ravi Varadhan)
Date: Fri, 8 Jan 2016 16:28:52 +0000
Subject: [R] Solve an equation including integral
Message-ID: <8600377588b441e685d0f8ed1bd7bb7b@ESGEBEX10.win.ad.jhu.edu>

I think this is what you want:

myroot <- function(t, nu, exi, alpha){
  fun <- function(t, exi, nu) (nu+t^2)^(-(nu+1)/2)*exp(((nu+1)*exi*t)/((nu+t^2)^0.5))
  res <- alpha - integrate(fun, -Inf, t, nu=2, exi=0.5)$value
  return(res)
}

uniroot(myroot, c(-2, 2), nu=2, exi=0.5, alpha=.05)

Hope this is helpful,
Ravi

Ravi Varadhan, Ph.D. (Biostatistics), Ph.D. (Environmental Engg)
Associate Professor,  Department of Oncology
Division of Biostatistics & Bionformatics
Sidney Kimmel Comprehensive Cancer Center
Johns Hopkins University
550 N. Broadway, Suite 1111-E
Baltimore, MD 21205
410-502-2619


	[[alternative HTML version deleted]]


From ravi.varadhan at jhu.edu  Fri Jan  8 17:47:00 2016
From: ravi.varadhan at jhu.edu (Ravi Varadhan)
Date: Fri, 8 Jan 2016 16:47:00 +0000
Subject: [R] Solve an equation including integral
Message-ID: <94a5b4ba9cc4475292805d533b975bde@ESGEBEX10.win.ad.jhu.edu>

There was a mistake in the previously sent function.  I had hard coded the values of parameters `nu' and `exi'.

Use this one:

myroot <- function(t, nu, exi, alpha){
  fun <- function(t, exi, nu) (nu+t^2)^(-(nu+1)/2)*exp(((nu+1)*exi*t)/((nu+t^2)^0.5))
  res <- alpha - integrate(fun, -Inf, t, nu=nu, exi=exi)$value
  return(res)
}

uniroot(myroot, c(-2, 2), nu=2, exi=0.5, alpha=.05)

Ravi


From: Ravi Varadhan
Sent: Friday, January 08, 2016 11:29 AM
To: r-help at r-project.org; 'sstoline at gmail.com' <sstoline at gmail.com>
Subject: Re: Solve an equation including integral

I think this is what you want:

myroot <- function(t, nu, exi, alpha){
  fun <- function(t, exi, nu) (nu+t^2)^(-(nu+1)/2)*exp(((nu+1)*exi*t)/((nu+t^2)^0.5))
  res <- alpha - integrate(fun, -Inf, t, nu=2, exi=0.5)$value
  return(res)
}

uniroot(myroot, c(-2, 2), nu=2, exi=0.5, alpha=.05)

Hope this is helpful,
Ravi

Ravi Varadhan, Ph.D. (Biostatistics), Ph.D. (Environmental Engg)
Associate Professor,  Department of Oncology
Division of Biostatistics & Bionformatics
Sidney Kimmel Comprehensive Cancer Center
Johns Hopkins University
550 N. Broadway, Suite 1111-E
Baltimore, MD 21205
410-502-2619


	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Fri Jan  8 18:02:25 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Fri, 08 Jan 2016 17:02:25 +0000
Subject: [R] (no subject)
In-Reply-To: <CAM0cwgUHUf0L8Xf-KgOfMvMoj_sPexjyHgOZN_fnfNs=eNwCpw@mail.gmail.com>
References: <CAM0cwgUHUf0L8Xf-KgOfMvMoj_sPexjyHgOZN_fnfNs=eNwCpw@mail.gmail.com>
Message-ID: <CAKVAULMY5hprOuugsjaCTF2Xk-hBDz6b8mUwpEZ_Edj_mdXStA@mail.gmail.com>

Hi Matthias,

Your dat.vline should include the 'variable' as you facet on this.

HTH

On Fri, 8 Jan 2016 at 17:49 Matthias Worni <wornimatthias at gmail.com> wrote:

> Thank you for the help. It worked out fine. However trying to add
> additional aesthetics I came to another point, where I couldnt figure out a
> way to solve it. Perhaps you run into similar problems since your familiar
> with the ggplot package.
>
> The code is the following:
>
> runmean_10yr_Nino_3_Index_17th_century <-
> SMA(SST_NINO3_detrended[0:100],n=10)
> runmean_10yr_Nino_3_Index_18th_century <-
> SMA(SST_NINO3_detrended[101:200],n=10)
> runmean_10yr_Nino_3_Index_19th_century <-
> SMA(SST_NINO3_detrended[201:300],n=10)
> runmean_10yr_Nino_3_Index_20th_century <-
> SMA(SST_NINO3_detrended[301:400],n=10)
>
>
> time_Nino3 <- c(1:100)
> runmean_yearly_decadal_Nino_Index <-
>
> data.frame(runmean_10yr_Nino_3_Index_17th_century,runmean_10yr_Nino_3_Index_18th_century,runmean_10yr_Nino_3_Index_19th_century,runmean_10yr_Nino_3_Index_20th_century,time_Nino3)
> melted3 <- melt(runmean_yearly_decadal_Nino_Index , id.vars="time_Nino3")
>
> ggplot(data=melted3, aes(x=time_Nino3,y=value,colour=variable))+
>   geom_line()+ ylab("SST Anomaly") + xlab("year") + ggtitle("Decadal El
> Nino Index ")+ facet_grid(variable ~ .) +
>   geom_vline(aes(xintercept = xp),data=dat.vline)
>
> dat.vline <- data.frame( xp = c(15, 23))
>
>
> As you may have noticed I created several plots below each other and now I
> would like  to add vertical lines to this plots, therefore adding
> "geom_vline(aes(xintercept = xp),data=dat.vline). This worked so far,
> however I would like to add vertical lines for these four different plots
> at different places. Do you know how I could do this? I added the plot I
> created at this e-mail. You can see two lines on every of those 4 plots (at
> the same place). What I want is the same thing but the lines at different
> places for every of those 4 plots (independently). Thank you for the help!!
> If you need to create the plots you can simply change the
> "runmean_10yr_Nino_3_Index_17th_century" to any other vector.
>
> Best Matthias
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Fri Jan  8 18:06:25 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Fri, 08 Jan 2016 17:06:25 +0000
Subject: [R] [Q] It it possible to create the data frame only non-zero
 data column?
In-Reply-To: <CA+Tq-RpThb89BpYofhvAxDAZA_t+geEMxGyye4ZuHngfVHbBwg@mail.gmail.com>
References: <CA+Tq-RpThb89BpYofhvAxDAZA_t+geEMxGyye4ZuHngfVHbBwg@mail.gmail.com>
Message-ID: <CAKVAULM9n-0Limf1_7U5sS7w-JmW8Cz64PmZquwwppTbY8oT-Q@mail.gmail.com>

Could you use rowsum and select rows larger then 0?

Something like:

result[rowsum(as.matrix(result) > 0, ]

On Fri, 8 Jan 2016 at 18:00 Hiroyuki Sato <hiroysato at gmail.com> wrote:

> Hello all.
>
> I re-post this question by e-mail.
> (I posted via google-group. But It's not posted yet.)
>
> I'm newbie GNU R.
>
> I would like to compare two datas.
> How to select columns which has non-zero datas?.
> It it possible to create the data frame only VAL3(non-zero data) column
> with command?
>
> Formatted sample.
> https://gist.github.com/hiroyuki-sato/cb36584f6cd5845b6c3e
>
> sample1.txt
>
>   ID,VAL1,VAL2,VAL3
>   ID1,0,2,3
>   ID2,0,2,3
>   ID3,0,2,3
>
>   real data has 5000 columns.
>
> sample2.txt
>
>   ID,VAL1,VAL2,VAL3
>   ID1,0,2,3
>   ID2,0,2,3
>   ID3,0,2,2
>
>   The difference sample1 and sample2 is ID3/VAL3.
>     sample1: 3
>     sample2: 2
>
> R commands.
>
>   sample1 <- read.table("sample1.txt",header=T,sep=',')
>   sample2 <- read.table("sample2.txt",header=T,sep=',')
>
>   result <- sample1[,2:4] - sample2[,2:4]
>   result
>     VAL1 VAL2 VAL3
>   1    0    0    0
>   2    0    0    0
>   3    0    0    1
>
> I would like to create data frame which has non-zero value columns.
> Could you tell me how to do it?
>
> Best regards.
>
> --
> Hiroyuki Sato.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Fri Jan  8 18:07:38 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 8 Jan 2016 12:07:38 -0500
Subject: [R] Scripting Problem--Linear Regression Using Logit of the
 Response Variable
In-Reply-To: <F26D18C3423DD149B0C3203622C7270F06CE20CE@HQNEX1.anser.org>
References: <F26D18C3423DD149B0C3203622C7270F06CE20CE@HQNEX1.anser.org>
Message-ID: <CAM_vjukfueD1K6Pg6yECWGVRBjHNJgdL6OMDo4o8mXsqXB8+Uw@mail.gmail.com>

>From a basic R syntax perspective, you're missing several commas, and
you don't specify your data. You also have a substantial problem with
parenthesis matching, with 3 ( and 4 )

adjust is an argument to logit(), and I'm assuming na.action is an
argument to lm().

>> EvacLM <- lm((logit(sharer_prob) ~ sympathy + trust + fear + greed) adjust=TRUE) na.action=NULL)

     EvacLM <- lm(logit(sharer_prob, adjust=TRUE) ~ sympathy + trust +
fear + greed, na.action=NULL, data=data)

If this is foreign to you, you should probably go read one of the many
good intro to R lessons online.

Sarah

On Thu, Jan 7, 2016 at 3:22 PM, John, Larry <Larry.John at anser.org> wrote:
> Dear fellow citizens of the R-verse,
>
> I'm a non-quant analyst trying to do some statistical analyses on a large data set unimaginatively named "data."
>
>   sympathy trust fear greed sharer_prob
> [1]      3     2    0     0    0.669593
> [2]      2     1    2     3    0.669593
> [3]      2     2    2     3    0.494675
> [4]      2     2    1     2    0.494675
> [5[      2     2    2     0    0.556837
> [6]      2     2    1     1    0.556837
>
> "sharer_prob" is the continuous dependent variable with values 0 > sharer_prob > 1, so I know need to run a logit transformation before I can do any linear regressions to study the main and interaction effects of the four independent variables.
>
> I've loaded "car" and tried for several hours to create a one-line script that will give me the overall linear model, but keep having problems. It seems to me that something like the following should work
>
>
>> EvacLM <- lm((logit(sharer_prob) ~ sympathy + trust + fear + greed) adjust=TRUE) na.action=NULL)
>
> But R politely tells me I've gotten something wrong:
>
>
> Error: unexpected symbol in "EvacLM <- lm((logit(sharer_prob) ~ sympathy + trust + fear + greed) adjust"
>
> I'm hoping one of you can spot and help me correct my mistake-I sure can't figure it out.
>
> Thanks for any help you can offer!
>
> Larry John
> Principal Analyst
> ANSER (www.anser.org)

-- 
Sarah Goslee
http://www.numberwright.com


From ulrik.stervbo at gmail.com  Fri Jan  8 18:53:50 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Fri, 08 Jan 2016 17:53:50 +0000
Subject: [R] (no subject)
In-Reply-To: <E9DA6CABC0F0734A9980C6E78E9024BA070665CD@aai-exch-mbx8.campus.unibe.ch>
References: <CAM0cwgUHUf0L8Xf-KgOfMvMoj_sPexjyHgOZN_fnfNs=eNwCpw@mail.gmail.com>
	<CAKVAULMY5hprOuugsjaCTF2Xk-hBDz6b8mUwpEZ_Edj_mdXStA@mail.gmail.com>
	<E9DA6CABC0F0734A9980C6E78E9024BA070665CD@aai-exch-mbx8.campus.unibe.ch>
Message-ID: <CAKVAULOHG+-_TX2zfmLHGYLBMvsM=Uvh+rSzwi6iVh602ZWvAQ@mail.gmail.com>

Please keep the list in CC.

Maybe something like

dat.vline <- data.frame( variable =
c("runmean_10yr_Nino_3_Index_17th_century",
"runmean_10yr_Nino_3_Index_17th_century",
"runmean_10yr_Nino_3_Index_18th_century",
"runmean_10yr_Nino_3_Index_17th_century"), xval = c(55, 65, 72, 82))

and
geom_vline(aes(xintercept = xval),data=dat.vline)

will work. If not take a look from the melted data.frame - the dat.vline
should be of similar structure.

On Fri, 8 Jan 2016 at 18:41 <maettuw at students.unibe.ch> wrote:

> I tried the following. Still isnt working, now having 8 lines on ever
> plot. I dont get the structure of the functions here..
>
> Matthias
>
> ggplot(data=melted3, aes(x=time_Nino3,y=value,colour=variable))+
>   geom_line()+ ylab("SST Anomaly") + xlab("year") + ggtitle("Decadal El
> Nino Index ")+ facet_grid(variable ~ .) +
>   geom_vline(aes(xintercept =
> runmean_10yr_Nino_3_Index_18th_century),data=dat.vline)
>
> a<- c(55,65)
> b<- c(72,82)
> c<- c(10,80)
> d <- c(57,97)
> dat.vline <- data.frame( runmean_10yr_Nino_3_Index_17th_century = c(a, b),
> runmean_10yr_Nino_3_Index_18th_century=c(c,d))
> ________________________________________
> Von: R-help [r-help-bounces at r-project.org]&quot; im Auftrag von
> &quot;Ulrik Stervbo [ulrik.stervbo at gmail.com]
> Gesendet: Freitag, 8. Januar 2016 18:02
> An: Matthias Worni; r-help at r-project.org
> Betreff: Re: [R] (no subject)
>
> Hi Matthias,
>
> Your dat.vline should include the 'variable' as you facet on this.
>
> HTH
>
> On Fri, 8 Jan 2016 at 17:49 Matthias Worni <wornimatthias at gmail.com>
> wrote:
>
> > Thank you for the help. It worked out fine. However trying to add
> > additional aesthetics I came to another point, where I couldnt figure
> out a
> > way to solve it. Perhaps you run into similar problems since your
> familiar
> > with the ggplot package.
> >
> > The code is the following:
> >
> > runmean_10yr_Nino_3_Index_17th_century <-
> > SMA(SST_NINO3_detrended[0:100],n=10)
> > runmean_10yr_Nino_3_Index_18th_century <-
> > SMA(SST_NINO3_detrended[101:200],n=10)
> > runmean_10yr_Nino_3_Index_19th_century <-
> > SMA(SST_NINO3_detrended[201:300],n=10)
> > runmean_10yr_Nino_3_Index_20th_century <-
> > SMA(SST_NINO3_detrended[301:400],n=10)
> >
> >
> > time_Nino3 <- c(1:100)
> > runmean_yearly_decadal_Nino_Index <-
> >
> >
> data.frame(runmean_10yr_Nino_3_Index_17th_century,runmean_10yr_Nino_3_Index_18th_century,runmean_10yr_Nino_3_Index_19th_century,runmean_10yr_Nino_3_Index_20th_century,time_Nino3)
> > melted3 <- melt(runmean_yearly_decadal_Nino_Index , id.vars="time_Nino3")
> >
> > ggplot(data=melted3, aes(x=time_Nino3,y=value,colour=variable))+
> >   geom_line()+ ylab("SST Anomaly") + xlab("year") + ggtitle("Decadal El
> > Nino Index ")+ facet_grid(variable ~ .) +
> >   geom_vline(aes(xintercept = xp),data=dat.vline)
> >
> > dat.vline <- data.frame( xp = c(15, 23))
> >
> >
> > As you may have noticed I created several plots below each other and now
> I
> > would like  to add vertical lines to this plots, therefore adding
> > "geom_vline(aes(xintercept = xp),data=dat.vline). This worked so far,
> > however I would like to add vertical lines for these four different plots
> > at different places. Do you know how I could do this? I added the plot I
> > created at this e-mail. You can see two lines on every of those 4 plots
> (at
> > the same place). What I want is the same thing but the lines at different
> > places for every of those 4 plots (independently). Thank you for the
> help!!
> > If you need to create the plots you can simply change the
> > "runmean_10yr_Nino_3_Index_17th_century" to any other vector.
> >
> > Best Matthias
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ragia11 at hotmail.com  Fri Jan  8 21:16:40 2016
From: ragia11 at hotmail.com (Ragia .)
Date: Fri, 8 Jan 2016 22:16:40 +0200
Subject: [R] r output as html or pdf without rstudio
Message-ID: <DUB125-W62BC3D13CE6535FBE96526B3F60@phx.gbl>

dear group,
is there a way to write my outputs to any kind of files ( the output contains text and graph) using R only without installing rstudio and using rmd ?files.
thanks in advance
Ragia 		 	   		  

From bhh at xs4all.nl  Fri Jan  8 21:17:05 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 8 Jan 2016 21:17:05 +0100
Subject: [R] Solve an equation including integral
In-Reply-To: <94a5b4ba9cc4475292805d533b975bde@ESGEBEX10.win.ad.jhu.edu>
References: <94a5b4ba9cc4475292805d533b975bde@ESGEBEX10.win.ad.jhu.edu>
Message-ID: <00C416CF-3507-4317-94BE-2C131213E64B@xs4all.nl>


> On 8 Jan 2016, at 17:47, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
> 
> There was a mistake in the previously sent function.  I had hard coded the values of parameters `nu' and `exi'.
> 
> Use this one:
> 
> myroot <- function(t, nu, exi, alpha){
>  fun <- function(t, exi, nu) (nu+t^2)^(-(nu+1)/2)*exp(((nu+1)*exi*t)/((nu+t^2)^0.5))
>  res <- alpha - integrate(fun, -Inf, t, nu=nu, exi=exi)$value
>  return(res)
> }
> 
> uniroot(myroot, c(-2, 2), nu=2, exi=0.5, alpha=.05)
> 

Given the original question, shouldn't alpha in the myroot function be replaced by (1-alpha)?
Like so

myroot <- function(t, nu, exi, alpha){
 fun <- function(t, exi, nu) (nu+t^2)^(-(nu+1)/2)*exp(((nu+1)*exi*t)/((nu+t^2)^0.5))
 res <- (1-alpha) - integrate(fun, -Inf, t, nu=nu, exi=exi)$value
 return(res)
}

If you have to do this frequently it would be better to move the declaration of fun to outside of myroot.

Berend

> Ravi
> 
> 
> From: Ravi Varadhan
> Sent: Friday, January 08, 2016 11:29 AM
> To: r-help at r-project.org; 'sstoline at gmail.com' <sstoline at gmail.com>
> Subject: Re: Solve an equation including integral
> 
> I think this is what you want:
> 
> myroot <- function(t, nu, exi, alpha){
>  fun <- function(t, exi, nu) (nu+t^2)^(-(nu+1)/2)*exp(((nu+1)*exi*t)/((nu+t^2)^0.5))
>  res <- alpha - integrate(fun, -Inf, t, nu=2, exi=0.5)$value
>  return(res)
> }
> 
> uniroot(myroot, c(-2, 2), nu=2, exi=0.5, alpha=.05)
> 
> Hope this is helpful,
> Ravi
> 
> Ravi Varadhan, Ph.D. (Biostatistics), Ph.D. (Environmental Engg)
> Associate Professor,  Department of Oncology
> Division of Biostatistics & Bionformatics
> Sidney Kimmel Comprehensive Cancer Center
> Johns Hopkins University
> 550 N. Broadway, Suite 1111-E
> Baltimore, MD 21205
> 410-502-2619
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From 538280 at gmail.com  Fri Jan  8 21:46:20 2016
From: 538280 at gmail.com (Greg Snow)
Date: Fri, 8 Jan 2016 13:46:20 -0700
Subject: [R] r output as html or pdf without rstudio
In-Reply-To: <DUB125-W62BC3D13CE6535FBE96526B3F60@phx.gbl>
References: <DUB125-W62BC3D13CE6535FBE96526B3F60@phx.gbl>
Message-ID: <CAFEqCdxn5LbsWvhJnkk9jTpR-RR+9mO+j4kjToynd0SP-1Qd7Q@mail.gmail.com>

Yes, you can use the knitr package directly (that is what Rstudio
uses, but it is its own package).

On Fri, Jan 8, 2016 at 1:16 PM, Ragia . <ragia11 at hotmail.com> wrote:
> dear group,
> is there a way to write my outputs to any kind of files ( the output contains text and graph) using R only without installing rstudio and using rmd  files.
> thanks in advance
> Ragia
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From hannah.hlx at gmail.com  Fri Jan  8 22:10:19 2016
From: hannah.hlx at gmail.com (li li)
Date: Fri, 8 Jan 2016 16:10:19 -0500
Subject: [R] Fwd: exact trend test (enumerate all possible contingency
 tables with fixed row and column margins)
In-Reply-To: <CAHLnndaJHVL41e50Mvt8TzLPRoNJ-qy2cfnLcUV685E7+UoohA@mail.gmail.com>
References: <CAHLnndZkaE92n+F1=U7yUO-1b6QdafU5YcGxh-zp0YA1wbRbMg@mail.gmail.com>
	<568E9E65.3010607@dewey.myzen.co.uk>
	<CAHLnnda13-hNSX+CJTTZEe0G6jgU8zhgHV5HLw4oXsugz-omhw@mail.gmail.com>
	<CAGxFJbQ9PmHLyx9zSC8XGe+UymPxjJ5WtLEbHzhA-W5qS+AN2Q@mail.gmail.com>
	<CAGxFJbR2ETcOEnisB3qodwj7ZkGPa4iumvZb2nwY9EQJu4Kn9g@mail.gmail.com>
	<CAHLnndaJHVL41e50Mvt8TzLPRoNJ-qy2cfnLcUV685E7+UoohA@mail.gmail.com>
Message-ID: <CAHLnndZ8Rp-6=CMHhOTbUyb8d0_t1ZKnfJxD-9Or_A4uSFLDJw@mail.gmail.com>

As a follow up, I found out the proc freq in SAS can perform the exact
permutation trend test.

  Hanna

2016-01-08 9:30 GMT-05:00 li li <hannah.hlx at gmail.com>:

> Thanks Bert.
>
>
> 2016-01-07 13:39 GMT-05:00 Bert Gunter <bgunter.4567 at gmail.com>:
>
>> Sorry -- neglected to reply to the list. -- Bert
>>
>>
>>
>> ---------- Forwarded message ----------
>> From: Bert Gunter <bgunter.4567 at gmail.com>
>> Date: Thu, Jan 7, 2016 at 10:38 AM
>> Subject: Re: [R] exact trend test (enumerate all possible contingency
>> tables with fixed row and column margins)
>> To: li li <hannah.hlx at gmail.com>
>>
>>
>> I do not know whether there is any package to do what you want.
>>
>> I **do** know that the algorithms required to do this are very
>> sophisticated and that with more than a few groups, all possible
>> enumerations are out of the question so that approximating shortcuts
>> must be used. See http://www.cytel.com/software-solutions/statxact for
>> some background.
>>
>> I **suspect** that you have no need to do what you have requested and
>> **suggest** that you consult a local statistician or
>> stats.stackexchange.com for another approach to whatever your
>> underlying issue is.
>>
>> Cheers,
>> Bert
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Thu, Jan 7, 2016 at 10:18 AM, li li <hannah.hlx at gmail.com> wrote:
>> > I did check the coin package before. I did not see a function in that
>> > package that can be used to list all the possible contingency tables
>> with
>> > fixed margins.
>> > Of course I googled "exact trend test using R". There is not enough help
>> > there.
>> > For up to three groups, I can easily enumerate all the contingency table
>> > with fixed margins, but with 5 groups it is not that easy.
>> > But as mentioned before, this is done implicitly and routinely in
>> > fisher.test function in R. So if anyone who have done this in R before,
>> > please help.
>> > Thanks.
>> >    Hanna
>> >
>> >
>> > 2016-01-07 12:20 GMT-05:00 Michael Dewey <lists at dewey.myzen.co.uk>:
>> >
>> >> You received a number of suggestions about where to look and packages
>> that
>> >> might be suitable. Did you do that? If you did which ones did you look
>> at
>> >> and why did you reject them?
>> >>
>> >>
>> >> On 07/01/2016 16:29, li li wrote:
>> >>
>> >>> Thanks for all the reply. Below is the data in a better format.
>> >>>
>> >>> addmargins(dat)
>> >>>>
>> >>>
>> >>>      dose 0 dose 0.15 dose 0.5 dose 1.5 dose 5 Sum
>> >>>
>> >>> yes      4         3        4        5      8  24
>> >>>
>> >>> no       4         5        4        3      0  16
>> >>>
>> >>> Sum      8         8        8        8      8  40
>> >>>
>> >>> I think it is easier and better that I rephrase my question. I would
>> like
>> >>> to enumerate all possible
>> >>> contingency tables with the row margins and column margins fixed as
>> in the
>> >>> above table. Yes. In fisher's exact test, this should have been done
>> >>> internally. But I need explicitly find all such tables. Need some
>> help on
>> >>> this and thanks very much in advance.
>> >>>
>> >>>      Hanna
>> >>>
>> >>>
>> >>> 2016-01-07 7:15 GMT-05:00 peter dalgaard <pdalgd at gmail.com>:
>> >>>
>> >>>
>> >>>> On 07 Jan 2016, at 08:31 , David Winsemius <dwinsemius at comcast.net>
>> >>>> wrote:
>> >>>>
>> >>>>
>> >>>>>> On Jan 6, 2016, at 8:16 PM, li li <hannah.hlx at gmail.com> wrote:
>> >>>>>>
>> >>>>>> Hi all,
>> >>>>>> Is there an R function that does exact randomization trend test?
>> >>>>>> For example, consider the 2 by 5 contingency table below:
>> >>>>>>
>> >>>>>>            dose0    dose 0.15    dose 0.5    dose 1.5    dose 5
>> >>>>>>  row
>> >>>>>> margin
>> >>>>>> Yes          4                3                  4               5
>> >>>>>>     8                   24
>> >>>>>> No          4                5                   4               3
>> >>>>>>       0                  16
>> >>>>>> col sum    8                8                   8               8
>> >>>>>>   8                   40
>> >>>>>>
>> >>>>>
>> >>>>> Your data presentation has been distorted by your failure to post in
>> >>>>>
>> >>>> plain text. Surely you have been asked in the past to correct this
>> issue?
>> >>>>
>> >>>>>
>> >>>>>
>> >>>>>> To do the exact trend test, we need to enumerate all the
>> contingency
>> >>>>>>
>> >>>>> table
>> >>>>
>> >>>>> with the
>> >>>>>> row and column margins fixed.
>> >>>>>>
>> >>>>>
>> >>>>> Er, how should that be done? A trend test? What is described above
>> would
>> >>>>>
>> >>>> be a general test of no association rather than a trend test. Please
>> use
>> >>>> clear language and be as specific as possible if you choose to
>> respond.
>> >>>>
>> >>>>>
>> >>>>> Find the probability corresponding to
>> >>>>>> obtaining
>> >>>>>> the corresponding contingency tables based on the multivariate
>> >>>>>> hypergeometric distribution. Finally the pvalue is obtained by
>> adding
>> >>>>>> relevant probabilities.
>> >>>>>>
>> >>>>>
>> >>>>> If there is a trend under consideration, then I do not understand
>> such a
>> >>>>>
>> >>>> trend would be modeled under a hypergeometric distribution? A
>> >>>> hypergeometic
>> >>>> distribution would suggest no trend, at least to my current
>> >>>> understanding.
>> >>>>
>> >>>> I'd expect that there is such a beast as a noncentral multivariate
>> >>>> hypergeometric (for the 2x2 case that is what we use to get the CI
>> for
>> >>>> the
>> >>>> odds ratio), but usually, one just wants the null distribution of the
>> >>>> test
>> >>>> statistic.
>> >>>>
>> >>>>
>> >>>>
>> >>>>>
>> >>>>>> Is there an R function that does this? if not, I am wondering
>> whether
>> >>>>>>
>> >>>>> it is
>> >>>>
>> >>>>> possible to
>> >>>>>> enumerate all possible contingency tables that has column sun and
>> row
>> >>>>>>
>> >>>>> sum
>> >>>>
>> >>>>> fixed?
>> >>>>>>
>> >>>>>
>> >>>>> Wel, yes, that is possible and routinely done with `fisher.test`,
>> but it
>> >>>>>
>> >>>> is up to you to describe how that activity leads to a trend test.
>> >>>>
>> >>>>>
>> >>>>> If you assume Poisson distributed errors a trend test is fairly
>> easy to
>> >>>>>
>> >>>> construct with glm.
>> >>>>
>> >>>>>
>> >>>>>
>> >>>> Or, more to the point, there is prop.trend.test(). Neither are exact
>> >>>> tests, though.
>> >>>>
>> >>>> I think package "coin" may something relevant.
>> >>>>
>> >>>> -pd
>> >>>>
>> >>>>
>> >>>> --
>> >>>>> David.
>> >>>>>
>> >>>>>>
>> >>>>>> Thanks very much!!
>> >>>>>>
>> >>>>>>   Hanna
>> >>>>>>
>> >>>>>>       [[alternative HTML version deleted]]
>> >>>>>>
>> >>>>>> ______________________________________________
>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>> PLEASE do read the posting guide
>> >>>>>>
>> >>>>> http://www.R-project.org/posting-guide.html
>> >>>>
>> >>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>>>>
>> >>>>>
>> >>>>> David Winsemius
>> >>>>> Alameda, CA, USA
>> >>>>>
>> >>>>> ______________________________________________
>> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>> PLEASE do read the posting guide
>> >>>>>
>> >>>> http://www.R-project.org/posting-guide.html
>> >>>>
>> >>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>>>
>> >>>>
>> >>>> --
>> >>>> Peter Dalgaard, Professor,
>> >>>> Center for Statistics, Copenhagen Business School
>> >>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> >>>> Phone: (+45)38153501
>> >>>> Office: A 4.23
>> >>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> >>>>
>> >>>>
>> >>>>
>> >>>         [[alternative HTML version deleted]]
>> >>>
>> >>> ______________________________________________
>> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> PLEASE do read the posting guide
>> >>> http://www.R-project.org/posting-guide.html
>> >>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>>
>> >> --
>> >> Michael
>> >> http://www.dewey.myzen.co.uk/home.html
>> >>
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From Wittner.Ben at mgh.harvard.edu  Fri Jan  8 22:13:09 2016
From: Wittner.Ben at mgh.harvard.edu (Wittner, Ben, Ph.D.)
Date: Fri, 8 Jan 2016 21:13:09 +0000
Subject: [R] rgl.snapshot only captures a small portion what's visible in
 the RGL device window on CentOS 7
Message-ID: <C402B4BC4499874FBA9869DA6E7A13F964B18628@PHSX10MB8.partners.org>

Hello,

As an example, I ran the following code:

library("rgl")                                                                  
example(plot3d)                                                                 
rgl.snapshot("test.png")                                                        

The full plot is visible in the window titled RGL device 1 [Focus], but only a small portion of the upper left part of the plot is visible in test.png (see attached test.png, if the list server attaches it. Otherwise, email me if you like and I'll send it directly to you.)

The output of sessionInfo() is as follows:

R version 3.2.3 (2015-12-10)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: CentOS Linux 7 (Core)

locale:
 [1] LC_CTYPE=en_US.utf8       LC_NUMERIC=C             
 [3] LC_TIME=en_US.utf8        LC_COLLATE=en_US.utf8    
 [5] LC_MONETARY=en_US.utf8    LC_MESSAGES=en_US.utf8   
 [7] LC_PAPER=en_US.utf8       LC_NAME=C                
 [9] LC_ADDRESS=C              LC_TELEPHONE=C           
[11] LC_MEASUREMENT=en_US.utf8 LC_IDENTIFICATION=C      

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] rgl_0.95.1441

loaded via a namespace (and not attached):
[1] tools_3.2.3

In addition to the CentOS 7 machine, I have a CentOS 5.3 machine, which has R 3.0.2 and rgl 0.93.996. I tried the code above on it and it captured the full window in the output of rgl.snapshot (i.e., it worked properly).

To see whether the difference could be attributed to the CentOS version or the R/rgl version, I put R 3.0.2 with rgl 0.93.996 on the CentOS 7 machine and ran the code above. As with the earlier CentOS 7 run, only a small portion of the plot was visible in the output of rgl.snapshot. So it seems the difference is due to a difference in the CentOS versions and not the R/rgl versions.

Thanks in advance for any help.

-Ben




The information in this e-mail is intended only for the person to whom it is
addressed. If you believe this e-mail was sent to you in error and the e-mail
contains patient information, please contact the Partners Compliance HelpLine at
http://www.partners.org/complianceline . If the e-mail was sent to you in error
but does not contain patient information, please contact the sender and properly
dispose of the e-mail.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.png
Type: image/png
Size: 968 bytes
Desc: test.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160108/f20302e3/attachment.png>

From murdoch.duncan at gmail.com  Sat Jan  9 02:12:37 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 8 Jan 2016 20:12:37 -0500
Subject: [R] rgl.snapshot only captures a small portion what's visible
 in the RGL device window on CentOS 7
In-Reply-To: <C402B4BC4499874FBA9869DA6E7A13F964B18628@PHSX10MB8.partners.org>
References: <C402B4BC4499874FBA9869DA6E7A13F964B18628@PHSX10MB8.partners.org>
Message-ID: <56905E85.8040604@gmail.com>

On 08/01/2016 4:13 PM, Wittner, Ben, Ph.D. wrote:
> Hello,
>
> As an example, I ran the following code:
>
> library("rgl")
> example(plot3d)
> rgl.snapshot("test.png")
>
> The full plot is visible in the window titled RGL device 1 [Focus], but only a small portion of the upper left part of the plot is visible in test.png (see attached test.png, if the list server attaches it. Otherwise, email me if you like and I'll send it directly to you.)
>
> The output of sessionInfo() is as follows:
>
> R version 3.2.3 (2015-12-10)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: CentOS Linux 7 (Core)
>
> locale:
>   [1] LC_CTYPE=en_US.utf8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.utf8        LC_COLLATE=en_US.utf8
>   [5] LC_MONETARY=en_US.utf8    LC_MESSAGES=en_US.utf8
>   [7] LC_PAPER=en_US.utf8       LC_NAME=C
>   [9] LC_ADDRESS=C              LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.utf8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rgl_0.95.1441
>
> loaded via a namespace (and not attached):
> [1] tools_3.2.3
>
> In addition to the CentOS 7 machine, I have a CentOS 5.3 machine, which has R 3.0.2 and rgl 0.93.996. I tried the code above on it and it captured the full window in the output of rgl.snapshot (i.e., it worked properly).
>
> To see whether the difference could be attributed to the CentOS version or the R/rgl version, I put R 3.0.2 with rgl 0.93.996 on the CentOS 7 machine and ran the code above. As with the earlier CentOS 7 run, only a small portion of the plot was visible in the output of rgl.snapshot. So it seems the difference is due to a difference in the CentOS versions and not the R/rgl versions.
>
> Thanks in advance for any help.
>

I think you'll need to contact CentOS about this.  That code hasn't 
changed in rgl in a long time.  It's possible that rgl is wrong, but I'd 
need more information from them to convince me.

Duncan Murdoch


From setareh227 at gmail.com  Fri Jan  8 22:44:55 2016
From: setareh227 at gmail.com (maryam moazam)
Date: Sat, 9 Jan 2016 01:14:55 +0330
Subject: [R] Combining dataframes with different row numbers and plotting
	with ggplot2
Message-ID: <CAM=b0pfwier1Kqy2BEfOr-++AbbxFDuEMCe-9Ec7Vr_Q8s8rSw@mail.gmail.com>

Dear Sir / Madam,

I have just come to the amazing R software, so please be patient if my
question is basic for you. I have 2 text file (say 1.txt and 2.txt), each
file containing 2 columns and different row numbers, like below

case size
case1 120
case2 120
case3 121
case4 121
case5 121
case6 122
case7 122
case8 123

I would like to have a one plot for all text files, with x-axis shows the
size between 300-1200 with the interval of 200 (300,500,700,900,1200) and
size between 1201-1500 with the interval of 1000. For dataframes with the
equal row numbers, the following codes worked well,

df1 = data.frame("1.txt", header=T)
df2 = data.frame("2.txt", header=T)
*combining two dataframes with equal row number*

df = data.frame(df1$size,df2$size)
library(reshape)
melted <- melt(df)

ggplot(data=melted, aes(value))+aes(fill=variable)+ geom_histogram
(binwidth =500)+

+scale_x_continuous(breaks=c(seq(300,1000,by=200),seq(1001,15000,by=1000)))


but I couldn't reproduce the plot with these codes for dataframes with
different row number. I think the problem is* how to combine datafrmaes
with the different row number*, could you please help me out on this issue?

Thank you in advance

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Sat Jan  9 02:24:11 2016
From: hannah.hlx at gmail.com (li li)
Date: Fri, 8 Jan 2016 20:24:11 -0500
Subject: [R] permTREND function in package "perm"
Message-ID: <CAHLnndZX+PW9Kos8MCkFRiei_Fg0o_8VXQojeBd5J5h9P8ESNA@mail.gmail.com>

Hi all,
  I am trying to figure out how to use the function permTREND correctly in
package "perm". There does not seem to be any examples given for this
function. In the help file, it says the following:

## Default S3 method:
permTREND(x, y, alternative = c("two.sided", "less", "greater"), exact =
NULL, method = NULL, methodRule = methodRuleTREND1,
control=permControl(),...)

## S3 method for class 'formula'
permTREND(formula,data,subset,na.action,...)

where,

x

numeric vector of response scores for the first group
y

numeric vector of either response scores for the second group (for permTS)
or trend scores for each observation (for permTREND)
g

a factor or character vector denoting group membership

But I am still not sure. What does it mean by S3  method for formula?
also, trend score in the explanation for "y" mean what?

Take the data below for example, x should be the count value for the row
corresponding to "Yes", and "y" should be the dose levels "0, 0.15. 0.5,
1.5, 5"?  Need some help on this. Thanks!
    Hanna


> addmargins(dat)

    dose 0 dose 0.15 dose 0.5 dose 1.5 dose 5 Sum

yes      4         3        4        5      8  24

no       4         5        4        3      0  16

Sum      8         8        8        8      8  40

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Jan  9 08:15:50 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 8 Jan 2016 23:15:50 -0800
Subject: [R] permTREND function in package "perm"
In-Reply-To: <CAHLnndZX+PW9Kos8MCkFRiei_Fg0o_8VXQojeBd5J5h9P8ESNA@mail.gmail.com>
References: <CAHLnndZX+PW9Kos8MCkFRiei_Fg0o_8VXQojeBd5J5h9P8ESNA@mail.gmail.com>
Message-ID: <CAGxFJbTCB_6=xw3QnrsoXj-R+-5ysHeAk-dcf0ARvrVoAvY__Q@mail.gmail.com>

?UseMethod  ## for S3 method help.

In addition, please read An Introduction to R (ships with R) or other
online tutorial to learn about S3 methods. You should not expect this
forum to provide you information that you should learn about yourself.

As for your other queries, someone who is familiar with the package
will have to reply. I am not.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jan 8, 2016 at 5:24 PM, li li <hannah.hlx at gmail.com> wrote:
> Hi all,
>   I am trying to figure out how to use the function permTREND correctly in
> package "perm". There does not seem to be any examples given for this
> function. In the help file, it says the following:
>
> ## Default S3 method:
> permTREND(x, y, alternative = c("two.sided", "less", "greater"), exact =
> NULL, method = NULL, methodRule = methodRuleTREND1,
> control=permControl(),...)
>
> ## S3 method for class 'formula'
> permTREND(formula,data,subset,na.action,...)
>
> where,
>
> x
>
> numeric vector of response scores for the first group
> y
>
> numeric vector of either response scores for the second group (for permTS)
> or trend scores for each observation (for permTREND)
> g
>
> a factor or character vector denoting group membership
>
> But I am still not sure. What does it mean by S3  method for formula?
> also, trend score in the explanation for "y" mean what?
>
> Take the data below for example, x should be the count value for the row
> corresponding to "Yes", and "y" should be the dose levels "0, 0.15. 0.5,
> 1.5, 5"?  Need some help on this. Thanks!
>     Hanna
>
>
>> addmargins(dat)
>
>     dose 0 dose 0.15 dose 0.5 dose 1.5 dose 5 Sum
>
> yes      4         3        4        5      8  24
>
> no       4         5        4        3      0  16
>
> Sum      8         8        8        8      8  40
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tcmuigai at gmail.com  Sat Jan  9 10:55:52 2016
From: tcmuigai at gmail.com (Charles Thuo)
Date: Sat, 9 Jan 2016 12:55:52 +0300
Subject: [R] Which GLM to use with mixed predictor variables i.e. both
 discrete and continous
Message-ID: <CAAJc=rN6s-W706WStDuPgAncibC9Uw7KnKeMkKRxF+TYa9sc_Q@mail.gmail.com>

I have the following glm

v_glm <-
glm(claim_count~ave_age+cost+inc.count+p.statcount,data=v,family=poisson)

claim_count - dependent variable and is discrete

ave_age - predictor variable and is continuous

cost - predictor variable and is continuous

inc_count - predictor variable and is  discrete

p.statcount - predictor variable and is  discrete

Which is the most appropriate glm to use for such a model.

Charles

	[[alternative HTML version deleted]]


From peterenos at ymail.com  Sat Jan  9 12:28:00 2016
From: peterenos at ymail.com (Peter Tuju)
Date: Sat, 9 Jan 2016 11:28:00 +0000 (UTC)
Subject: [R] Extracting point data using longitude and latitude from
 netcdf file using R
In-Reply-To: <mailman.7.1452250802.3001.r-help@r-project.org>
References: <mailman.7.1452250802.3001.r-help@r-project.org>
Message-ID: <1252388692.2613064.1452338880762.JavaMail.yahoo@mail.yahoo.com>

I have data file in netcdf with three dimensions (x, y, t) and I want to extract a variable RAINC and RAINNC 
using longitude and latitude for a single point location with all the time, but no lucky. The syntax is as follows;;
setwd( "/run/media/tuju/0767090047/extract_wrf_txt_file" )
rm( list = ls() )??? ??? ??? ??? ????????????????????????????? 
library( ncdf4 )???????????????????????????????????? 
inp_file <- nc_open( "wrfout_d01_2016-01-07.nc" )time <- ncvar_get( inp_file, "Times" )????????????????????? # Reading the time variabledar_lon <- 39.2dar_lat <- -6.866667
RAINC <- ncvar_get( inp_file, varid = "RAINC", start? = c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
RAINNC <- ncvar_get( inp_file, varid = "RAINNC", start? =? c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
RAIN <- RAINC + RAINNCRAIN_TABLE <- cbind( time, RAIN )
write.table( RAIN_TABLE, "Dar_es_Salaam.txt", row.names = FALSE, 
???????????? col.names = c( "Valid Forecast Time",? "Rain (mm)", sep = "\t " )

# But no lucky with the red bolded syntax as I end up with the following error message> RAINC <- ncvar_get( inp_file, varid = "RAINC", start? = c( Lon[2], Lat[2], 1 ), count = c( 1, 1, -1 ) )
Error in Rsx_nc4_get_vara_double: NetCDF: Index exceeds dimension bound
Var: RAINC? Ndims: 3?? Start: 0,4294967289,38 Count: 17,1,1
Error in ncvar_get_inner(ncid2use, varid2use, nc$var[[li]]$missval, addOffset,? : 
? C function R_nc4_get_vara_double returned error
However when I cahnge the latitude to postive it works fine. Note latitudes in the file data ranges from -16.71505 to 7.787529 as shown below;
head(ncvar_get(inp_file, "XLAT"))
[1] -16.71505 -16.71505 -16.71505 -16.71505 -16.71505 -16.71505
> tail(ncvar_get(inp_file, "XLAT"))
[1] 7.787529 7.787529 7.787529 7.787529 7.787529 7.787529
## So, how can I get the syntax correct? Please help?_____________
Peter? E. Tuju
Dar es Salaam
T A N Z A N I A
----------------------




  
	[[alternative HTML version deleted]]


From peter.anthoni at kit.edu  Sat Jan  9 13:14:43 2016
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Sat, 9 Jan 2016 12:14:43 +0000
Subject: [R] Extracting point data using longitude and latitude from
 netcdf file using R
In-Reply-To: <1252388692.2613064.1452338880762.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.7.1452250802.3001.r-help@r-project.org>
	<1252388692.2613064.1452338880762.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <7CD94835-C004-4E20-9117-1E9AE42981D8@kit.edu>

Hi Peter,

the start in nc_varget requires a latitude and longitude index, not the latitude and longitude in double format.
So you need to figure out what index your latitude and longitude correspond to, which will depends on what data are in your netCDF.

it might have looked like that it worked for a positive latitude, but you got the data from the latitude index 6 or 7, depends on how the double was transformed into an integer.

best regards
Peter

> On 09 Jan 2016, at 12:28, Peter Tuju via R-help <r-help at r-project.org> wrote:
> 
> I have data file in netcdf with three dimensions (x, y, t) and I want to extract a variable RAINC and RAINNC 
> using longitude and latitude for a single point location with all the time, but no lucky. The syntax is as follows;;
> setwd( "/run/media/tuju/0767090047/extract_wrf_txt_file" )
> rm( list = ls() )                                              
> library( ncdf4 )                                     
> inp_file <- nc_open( "wrfout_d01_2016-01-07.nc" )time <- ncvar_get( inp_file, "Times" )                      # Reading the time variabledar_lon <- 39.2dar_lat <- -6.866667
> RAINC <- ncvar_get( inp_file, varid = "RAINC", start  = c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
> RAINNC <- ncvar_get( inp_file, varid = "RAINNC", start  =  c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
> RAIN <- RAINC + RAINNCRAIN_TABLE <- cbind( time, RAIN )
> write.table( RAIN_TABLE, "Dar_es_Salaam.txt", row.names = FALSE, 
>              col.names = c( "Valid Forecast Time",  "Rain (mm)", sep = "\t " )
> 
> # But no lucky with the red bolded syntax as I end up with the following error message> RAINC <- ncvar_get( inp_file, varid = "RAINC", start  = c( Lon[2], Lat[2], 1 ), count = c( 1, 1, -1 ) )
> Error in Rsx_nc4_get_vara_double: NetCDF: Index exceeds dimension bound
> Var: RAINC  Ndims: 3   Start: 0,4294967289,38 Count: 17,1,1
> Error in ncvar_get_inner(ncid2use, varid2use, nc$var[[li]]$missval, addOffset,  : 
>   C function R_nc4_get_vara_double returned error
> However when I cahnge the latitude to postive it works fine. Note latitudes in the file data ranges from -16.71505 to 7.787529 as shown below;
> head(ncvar_get(inp_file, "XLAT"))
> [1] -16.71505 -16.71505 -16.71505 -16.71505 -16.71505 -16.71505
>> tail(ncvar_get(inp_file, "XLAT"))
> [1] 7.787529 7.787529 7.787529 7.787529 7.787529 7.787529
> ## So, how can I get the syntax correct? Please help _____________
> Peter  E. Tuju
> Dar es Salaam
> T A N Z A N I A
> ----------------------
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From peterenos at ymail.com  Sat Jan  9 13:54:33 2016
From: peterenos at ymail.com (Peter Tuju)
Date: Sat, 9 Jan 2016 12:54:33 +0000 (UTC)
Subject: [R] Extracting point data using longitude and latitude from
 netcdf file using R
In-Reply-To: <7CD94835-C004-4E20-9117-1E9AE42981D8@kit.edu>
References: <7CD94835-C004-4E20-9117-1E9AE42981D8@kit.edu>
Message-ID: <493316923.2578453.1452344073136.JavaMail.yahoo@mail.yahoo.com>

Thank you Mr. Anthon for your feedback. So, how can? I extract the data at the specified? latitude and longitude?Or, How can I get the latitude and longitude index which corresponds to the following coordinates from the netcdf file?
dar_lon <- 39.2
dar_lat <- -6.866667?_____________
Peter? E. Tuju
Dar es Salaam
T A N Z A N I A
----------------------
 

      From: "Anthoni, Peter (IMK)" <peter.anthoni at kit.edu>
 To: Peter Tuju <peterenos at ymail.com> 
Cc: "r-help at r-project.org" <r-help at r-project.org>
 Sent: Saturday, January 9, 2016 3:14 PM
 Subject: Re: [R] Extracting point data using longitude and latitude from netcdf file using R
   
Hi Peter,

the start in nc_varget requires a latitude and longitude index, not the latitude and longitude in double format.
So you need to figure out what index your latitude and longitude correspond to, which will depends on what data are in your netCDF.

it might have looked like that it worked for a positive latitude, but you got the data from the latitude index 6 or 7, depends on how the double was transformed into an integer.

best regards
Peter

> On 09 Jan 2016, at 12:28, Peter Tuju via R-help <r-help at r-project.org> wrote:
> 
> I have data file in netcdf with three dimensions (x, y, t) and I want to extract a variable RAINC and RAINNC 
> using longitude and latitude for a single point location with all the time, but no lucky. The syntax is as follows;;
> setwd( "/run/media/tuju/0767090047/extract_wrf_txt_file" )
> rm( list = ls() )? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
> library( ncdf4 )? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
> inp_file <- nc_open( "wrfout_d01_2016-01-07.nc" )time <- ncvar_get( inp_file, "Times" )? ? ? ? ? ? ? ? ? ? ? # Reading the time variabledar_lon <- 39.2dar_lat <- -6.866667
> RAINC <- ncvar_get( inp_file, varid = "RAINC", start? = c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
> RAINNC <- ncvar_get( inp_file, varid = "RAINNC", start? =? c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
> RAIN <- RAINC + RAINNCRAIN_TABLE <- cbind( time, RAIN )
> write.table( RAIN_TABLE, "Dar_es_Salaam.txt", row.names = FALSE, 
>? ? ? ? ? ? ? col.names = c( "Valid Forecast Time",? "Rain (mm)", sep = "\t " )
> 
> # But no lucky with the red bolded syntax as I end up with the following error message> RAINC <- ncvar_get( inp_file, varid = "RAINC", start? = c( Lon[2], Lat[2], 1 ), count = c( 1, 1, -1 ) )
> Error in Rsx_nc4_get_vara_double: NetCDF: Index exceeds dimension bound
> Var: RAINC? Ndims: 3? Start: 0,4294967289,38 Count: 17,1,1
> Error in ncvar_get_inner(ncid2use, varid2use, nc$var[[li]]$missval, addOffset,? : 
>? C function R_nc4_get_vara_double returned error
> However when I cahnge the latitude to postive it works fine. Note latitudes in the file data ranges from -16.71505 to 7.787529 as shown below;
> head(ncvar_get(inp_file, "XLAT"))
> [1] -16.71505 -16.71505 -16.71505 -16.71505 -16.71505 -16.71505
>> tail(ncvar_get(inp_file, "XLAT"))
> [1] 7.787529 7.787529 7.787529 7.787529 7.787529 7.787529
> ## So, how can I get the syntax correct? Please help _____________
> Peter? E. Tuju
> Dar es Salaam
> T A N Z A N I A
> ----------------------
> 
> 
> 
> 
> 
> ??? [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


  
	[[alternative HTML version deleted]]


From btupper at bigelow.org  Sat Jan  9 14:01:14 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Sat, 9 Jan 2016 08:01:14 -0500
Subject: [R] Extracting point data using longitude and latitude from
	netcdf file using R
In-Reply-To: <7CD94835-C004-4E20-9117-1E9AE42981D8@kit.edu>
References: <mailman.7.1452250802.3001.r-help@r-project.org>
	<1252388692.2613064.1452338880762.JavaMail.yahoo@mail.yahoo.com>
	<7CD94835-C004-4E20-9117-1E9AE42981D8@kit.edu>
Message-ID: <29773798-C6C0-4D52-8243-67C7F23B895A@bigelow.org>

Hi,

This post gives more details on how to transform your lat/lon values to row/column indices. The question and answer are specifically about the ncdf package, but the workflow is identical when using the ncfd4 package.

https://stat.ethz.ch/pipermail/r-help/2011-March/272641.html

Cheers,
Ben

> On Jan 9, 2016, at 7:14 AM, Anthoni, Peter (IMK) <peter.anthoni at kit.edu> wrote:
> 
> Hi Peter,
> 
> the start in nc_varget requires a latitude and longitude index, not the latitude and longitude in double format.
> So you need to figure out what index your latitude and longitude correspond to, which will depends on what data are in your netCDF.
> 
> it might have looked like that it worked for a positive latitude, but you got the data from the latitude index 6 or 7, depends on how the double was transformed into an integer.
> 
> best regards
> Peter
> 
>> On 09 Jan 2016, at 12:28, Peter Tuju via R-help <r-help at r-project.org> wrote:
>> 
>> I have data file in netcdf with three dimensions (x, y, t) and I want to extract a variable RAINC and RAINNC 
>> using longitude and latitude for a single point location with all the time, but no lucky. The syntax is as follows;;
>> setwd( "/run/media/tuju/0767090047/extract_wrf_txt_file" )
>> rm( list = ls() )                                              
>> library( ncdf4 )                                     
>> inp_file <- nc_open( "wrfout_d01_2016-01-07.nc" )time <- ncvar_get( inp_file, "Times" )                      # Reading the time variabledar_lon <- 39.2dar_lat <- -6.866667
>> RAINC <- ncvar_get( inp_file, varid = "RAINC", start  = c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
>> RAINNC <- ncvar_get( inp_file, varid = "RAINNC", start  =  c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
>> RAIN <- RAINC + RAINNCRAIN_TABLE <- cbind( time, RAIN )
>> write.table( RAIN_TABLE, "Dar_es_Salaam.txt", row.names = FALSE, 
>>             col.names = c( "Valid Forecast Time",  "Rain (mm)", sep = "\t " )
>> 
>> # But no lucky with the red bolded syntax as I end up with the following error message> RAINC <- ncvar_get( inp_file, varid = "RAINC", start  = c( Lon[2], Lat[2], 1 ), count = c( 1, 1, -1 ) )
>> Error in Rsx_nc4_get_vara_double: NetCDF: Index exceeds dimension bound
>> Var: RAINC  Ndims: 3   Start: 0,4294967289,38 Count: 17,1,1
>> Error in ncvar_get_inner(ncid2use, varid2use, nc$var[[li]]$missval, addOffset,  : 
>>  C function R_nc4_get_vara_double returned error
>> However when I cahnge the latitude to postive it works fine. Note latitudes in the file data ranges from -16.71505 to 7.787529 as shown below;
>> head(ncvar_get(inp_file, "XLAT"))
>> [1] -16.71505 -16.71505 -16.71505 -16.71505 -16.71505 -16.71505
>>> tail(ncvar_get(inp_file, "XLAT"))
>> [1] 7.787529 7.787529 7.787529 7.787529 7.787529 7.787529
>> ## So, how can I get the syntax correct? Please help _____________
>> Peter  E. Tuju
>> Dar es Salaam
>> T A N Z A N I A
>> ----------------------
>> 
>> 
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From peterenos at ymail.com  Sat Jan  9 15:01:04 2016
From: peterenos at ymail.com (Peter Tuju)
Date: Sat, 9 Jan 2016 14:01:04 +0000 (UTC)
Subject: [R] Extracting point data using longitude and latitude from
 netcdf file using R
In-Reply-To: <29773798-C6C0-4D52-8243-67C7F23B895A@bigelow.org>
References: <29773798-C6C0-4D52-8243-67C7F23B895A@bigelow.org>
Message-ID: <826855243.76630.1452348064541.JavaMail.yahoo@mail.yahoo.com>

Thank you Be for the good guide, however no luck with the syntax used namely;
ix0 = wherenearest( lower_left_lon_lat[1],? lon )
ix1 = wherenearest( upper_right_lon_lat[1], lon )
iy0 = wherenearest( lower_left_lon_lat[2],? lat )
iy1 = wherenearest( upper_right_lon_lat[2], lat )
# I end up with this error,? "Error: could not find function "wherenearest"
Is there any other way I can get the index corresponding/or rearing to thelongitude and latitude of interests??_____________
Peter? E. Tuju
Dar es Salaam
T A N Z A N I A
----------------------
 

      From: Ben Tupper <btupper at bigelow.org>
 To: "r-help at r-project.org" <r-help at r-project.org> 
Cc: Peter Tuju <peterenos at ymail.com>
 Sent: Saturday, January 9, 2016 4:01 PM
 Subject: Re: [R] Extracting point data using longitude and latitude from netcdf file using R
   
Hi,

This post gives more details on how to transform your lat/lon values to row/column indices. The question and answer are specifically about the ncdf package, but the workflow is identical when using the ncfd4 package.

https://stat.ethz.ch/pipermail/r-help/2011-March/272641.html

Cheers,
Ben

> On Jan 9, 2016, at 7:14 AM, Anthoni, Peter (IMK) <peter.anthoni at kit.edu> wrote:
> 
> Hi Peter,
> 
> the start in nc_varget requires a latitude and longitude index, not the latitude and longitude in double format.
> So you need to figure out what index your latitude and longitude correspond to, which will depends on what data are in your netCDF.
> 
> it might have looked like that it worked for a positive latitude, but you got the data from the latitude index 6 or 7, depends on how the double was transformed into an integer.
> 
> best regards
> Peter
> 
>> On 09 Jan 2016, at 12:28, Peter Tuju via R-help <r-help at r-project.org> wrote:
>> 
>> I have data file in netcdf with three dimensions (x, y, t) and I want to extract a variable RAINC and RAINNC 
>> using longitude and latitude for a single point location with all the time, but no lucky. The syntax is as follows;;
>> setwd( "/run/media/tuju/0767090047/extract_wrf_txt_file" )
>> rm( list = ls() )? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
>> library( ncdf4 )? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
>> inp_file <- nc_open( "wrfout_d01_2016-01-07.nc" )time <- ncvar_get( inp_file, "Times" )? ? ? ? ? ? ? ? ? ? ? # Reading the time variabledar_lon <- 39.2dar_lat <- -6.866667
>> RAINC <- ncvar_get( inp_file, varid = "RAINC", start? = c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
>> RAINNC <- ncvar_get( inp_file, varid = "RAINNC", start? =? c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
>> RAIN <- RAINC + RAINNCRAIN_TABLE <- cbind( time, RAIN )
>> write.table( RAIN_TABLE, "Dar_es_Salaam.txt", row.names = FALSE, 
>>? ? ? ? ? ? col.names = c( "Valid Forecast Time",? "Rain (mm)", sep = "\t " )
>> 
>> # But no lucky with the red bolded syntax as I end up with the following error message> RAINC <- ncvar_get( inp_file, varid = "RAINC", start? = c( Lon[2], Lat[2], 1 ), count = c( 1, 1, -1 ) )
>> Error in Rsx_nc4_get_vara_double: NetCDF: Index exceeds dimension bound
>> Var: RAINC? Ndims: 3? Start: 0,4294967289,38 Count: 17,1,1
>> Error in ncvar_get_inner(ncid2use, varid2use, nc$var[[li]]$missval, addOffset,? : 
>>? C function R_nc4_get_vara_double returned error
>> However when I cahnge the latitude to postive it works fine. Note latitudes in the file data ranges from -16.71505 to 7.787529 as shown below;
>> head(ncvar_get(inp_file, "XLAT"))
>> [1] -16.71505 -16.71505 -16.71505 -16.71505 -16.71505 -16.71505
>>> tail(ncvar_get(inp_file, "XLAT"))
>> [1] 7.787529 7.787529 7.787529 7.787529 7.787529 7.787529
>> ## So, how can I get the syntax correct? Please help _____________
>> Peter? E. Tuju
>> Dar es Salaam
>> T A N Z A N I A
>> ----------------------
>> 
>> 
>> 
>> 
>> 
>> ??? [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


  
	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Sat Jan  9 15:08:05 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 9 Jan 2016 14:08:05 +0000
Subject: [R] Combining dataframes with different row numbers and
 plotting with ggplot2
In-Reply-To: <CAM=b0pfwier1Kqy2BEfOr-++AbbxFDuEMCe-9Ec7Vr_Q8s8rSw@mail.gmail.com>
References: <CAM=b0pfwier1Kqy2BEfOr-++AbbxFDuEMCe-9Ec7Vr_Q8s8rSw@mail.gmail.com>
Message-ID: <56911445.4060003@dewey.myzen.co.uk>

Dear Maryam

If you just need all the values of size would
c(df1$size, df2$size)
work?

On 08/01/2016 21:44, maryam moazam wrote:
> Dear Sir / Madam,
>
> I have just come to the amazing R software, so please be patient if my
> question is basic for you. I have 2 text file (say 1.txt and 2.txt), each
> file containing 2 columns and different row numbers, like below
>
> case size
> case1 120
> case2 120
> case3 121
> case4 121
> case5 121
> case6 122
> case7 122
> case8 123
>
> I would like to have a one plot for all text files, with x-axis shows the
> size between 300-1200 with the interval of 200 (300,500,700,900,1200) and
> size between 1201-1500 with the interval of 1000. For dataframes with the
> equal row numbers, the following codes worked well,
>
> df1 = data.frame("1.txt", header=T)
> df2 = data.frame("2.txt", header=T)
> *combining two dataframes with equal row number*
>
> df = data.frame(df1$size,df2$size)
> library(reshape)
> melted <- melt(df)
>
> ggplot(data=melted, aes(value))+aes(fill=variable)+ geom_histogram
> (binwidth =500)+
>
> +scale_x_continuous(breaks=c(seq(300,1000,by=200),seq(1001,15000,by=1000)))
>
>
> but I couldn't reproduce the plot with these codes for dataframes with
> different row number. I think the problem is* how to combine datafrmaes
> with the different row number*, could you please help me out on this issue?
>
> Thank you in advance
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From lists at dewey.myzen.co.uk  Sat Jan  9 15:09:51 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 9 Jan 2016 14:09:51 +0000
Subject: [R] Which GLM to use with mixed predictor variables i.e. both
 discrete and continous
In-Reply-To: <CAAJc=rN6s-W706WStDuPgAncibC9Uw7KnKeMkKRxF+TYa9sc_Q@mail.gmail.com>
References: <CAAJc=rN6s-W706WStDuPgAncibC9Uw7KnKeMkKRxF+TYa9sc_Q@mail.gmail.com>
Message-ID: <569114AF.3050803@dewey.myzen.co.uk>

Dear Charles

You showed us a call of glm with which you are presumably not satisfied. 
What happened to disappoint you?

On 09/01/2016 09:55, Charles Thuo wrote:
> I have the following glm
>
> v_glm <-
> glm(claim_count~ave_age+cost+inc.count+p.statcount,data=v,family=poisson)
>
> claim_count - dependent variable and is discrete
>
> ave_age - predictor variable and is continuous
>
> cost - predictor variable and is continuous
>
> inc_count - predictor variable and is  discrete
>
> p.statcount - predictor variable and is  discrete
>
> Which is the most appropriate glm to use for such a model.
>
> Charles
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From lists at dewey.myzen.co.uk  Sat Jan  9 16:11:10 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 9 Jan 2016 15:11:10 +0000
Subject: [R] Combining dataframes with different row numbers and
 plotting with ggplot2
In-Reply-To: <CAM=b0peX9vX-viQO8TPHpTyzWg1DZk92=wbRPjV+F0aPV3v9Zw@mail.gmail.com>
References: <CAM=b0pfwier1Kqy2BEfOr-++AbbxFDuEMCe-9Ec7Vr_Q8s8rSw@mail.gmail.com>
	<56911445.4060003@dewey.myzen.co.uk>
	<CAM=b0peX9vX-viQO8TPHpTyzWg1DZk92=wbRPjV+F0aPV3v9Zw@mail.gmail.com>
Message-ID: <5691230E.80907@dewey.myzen.co.uk>

Sorry Maryam but I use neither reshape nor ggplot2 so I will leave it to 
others to advise you.

On 09/01/2016 14:29, maryam moazam wrote:
>
> Dear Michael,
>
> Thanks for your feedback. Actually, I would like to show (and compare)
> size distribution of df1 and df2 in the single plot using ggplot2,
> something like the attached picture. The command dosesn't lead me to
> this purpose. However, I'm really new here, could you please help
> me more on this?
>
>
> Thanks in advance,
> Maryam
>
>
>
>
>
> On Sat, Jan 9, 2016 at 5:38 PM, Michael Dewey <lists at dewey.myzen.co.uk
> <mailto:lists at dewey.myzen.co.uk>> wrote:
>
>     Dear Maryam
>
>     If you just need all the values of size would
>     c(df1$size, df2$size)
>     work?
>
>     On 08/01/2016 21:44, maryam moazam wrote:
>
>         Dear Sir / Madam,
>
>         I have just come to the amazing R software, so please be patient
>         if my
>         question is basic for you. I have 2 text file (say 1.txt and
>         2.txt), each
>         file containing 2 columns and different row numbers, like below
>
>         case size
>         case1 120
>         case2 120
>         case3 121
>         case4 121
>         case5 121
>         case6 122
>         case7 122
>         case8 123
>
>         I would like to have a one plot for all text files, with x-axis
>         shows the
>         size between 300-1200 with the interval of 200
>         (300,500,700,900,1200) and
>         size between 1201-1500 with the interval of 1000. For dataframes
>         with the
>         equal row numbers, the following codes worked well,
>
>         df1 = data.frame("1.txt", header=T)
>         df2 = data.frame("2.txt", header=T)
>         *combining two dataframes with equal row number*
>
>         df = data.frame(df1$size,df2$size)
>         library(reshape)
>         melted <- melt(df)
>
>         ggplot(data=melted, aes(value))+aes(fill=variable)+ geom_histogram
>         (binwidth =500)+
>
>         +scale_x_continuous(breaks=c(seq(300,1000,by=200),seq(1001,15000,by=1000)))
>
>
>         but I couldn't reproduce the plot with these codes for
>         dataframes with
>         different row number. I think the problem is* how to combine
>         datafrmaes
>         with the different row number*, could you please help me out on
>         this issue?
>
>         Thank you in advance
>
>                  [[alternative HTML version deleted]]
>
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>         -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         and provide commented, minimal, self-contained, reproducible code.
>
>
>     --
>     Michael
>     http://www.dewey.myzen.co.uk/home.html
>
>
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From btupper at bigelow.org  Sat Jan  9 16:27:41 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Sat, 9 Jan 2016 10:27:41 -0500
Subject: [R] Extracting point data using longitude and latitude from
	netcdf file using R
In-Reply-To: <826855243.76630.1452348064541.JavaMail.yahoo@mail.yahoo.com>
References: <29773798-C6C0-4D52-8243-67C7F23B895A@bigelow.org>
	<826855243.76630.1452348064541.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <A2EC8E66-097A-4A00-8D76-74EC4FFBEF2B@bigelow.org>

Hi,

Well, you have to extrapolate from that post that wherenearest is a function you must create on your own.  Maybe something like this?

#' Find the index into a dataset that is 'closest' to a specified point.
#' 
#' Adpated from https://stat.ethz.ch/pipermail/r-help/2011-March/272641.html
#' @param myPoint numeric, one point
#' @param allpoints numeric, one or more points
#' @return index into allPoints that myPoint falls closest to
wherenearest <- function(myPoint, allPoints){
    d <- abs(allPoints-myPoint[1])
    index <- which.min(d)
    return( index )
}

I haven't tried the above.

You haven't provided much detail on what's in the file, but if it is a grid then perhaps you would find it easier to use the raster package.  It has functions to read gridded data stored in ncdf files.  It comes with a very good tutorial and makes working with gridded data a breeze. Using raster, you can bypass the nitty-gritty of getting data out of a ncdf file and just get to work on your data.

https://cran.r-project.org/web/packages/raster/index.html 
https://cran.r-project.org/web/packages/raster/vignettes/Raster.pdf

Bon chance!
Ben

P.S.  Do yourself (and everyone else on the list) a favor by making your email client use plain text rather than html or rich text when sending messages to the list.  The html/rich text scrambles your code making it hard to read.

> On Jan 9, 2016, at 9:01 AM, Peter Tuju <peterenos at ymail.com> wrote:
> 
> Thank you Be for the good guide, however no luck with the syntax used namely;
> 
> ix0 = wherenearest( lower_left_lon_lat[1],  lon )
> ix1 = wherenearest( upper_right_lon_lat[1], lon )
> iy0 = wherenearest( lower_left_lon_lat[2],  lat )
> iy1 = wherenearest( upper_right_lon_lat[2], lat )
> 
> # I end up with this error,  "Error: could not find function "wherenearest"
> 
> Is there any other way I can get the index corresponding/or rearing to the
> longitude and latitude of interests?
>  
> _____________
> Peter  E. Tuju
> Dar es Salaam
> T A N Z A N I A
> ----------------------
> 
> 
> 
> From: Ben Tupper <btupper at bigelow.org>
> To: "r-help at r-project.org" <r-help at r-project.org> 
> Cc: Peter Tuju <peterenos at ymail.com>
> Sent: Saturday, January 9, 2016 4:01 PM
> Subject: Re: [R] Extracting point data using longitude and latitude from netcdf file using R
> 
> Hi,
> 
> This post gives more details on how to transform your lat/lon values to row/column indices. The question and answer are specifically about the ncdf package, but the workflow is identical when using the ncfd4 package.
> 
> https://stat.ethz.ch/pipermail/r-help/2011-March/272641.html
> 
> Cheers,
> Ben
> 
> > On Jan 9, 2016, at 7:14 AM, Anthoni, Peter (IMK) <peter.anthoni at kit.edu> wrote:
> > 
> > Hi Peter,
> > 
> > the start in nc_varget requires a latitude and longitude index, not the latitude and longitude in double format.
> > So you need to figure out what index your latitude and longitude correspond to, which will depends on what data are in your netCDF.
> > 
> > it might have looked like that it worked for a positive latitude, but you got the data from the latitude index 6 or 7, depends on how the double was transformed into an integer.
> > 
> > best regards
> > Peter
> > 
> >> On 09 Jan 2016, at 12:28, Peter Tuju via R-help <r-help at r-project.org> wrote:
> >> 
> >> I have data file in netcdf with three dimensions (x, y, t) and I want to extract a variable RAINC and RAINNC 
> >> using longitude and latitude for a single point location with all the time, but no lucky. The syntax is as follows;;
> >> setwd( "/run/media/tuju/0767090047/extract_wrf_txt_file" )
> >> rm( list = ls() )                                              
> >> library( ncdf4 )                                    
> >> inp_file <- nc_open( "wrfout_d01_2016-01-07.nc" )time <- ncvar_get( inp_file, "Times" )                      # Reading the time variabledar_lon <- 39.2dar_lat <- -6.866667
> >> RAINC <- ncvar_get( inp_file, varid = "RAINC", start  = c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
> >> RAINNC <- ncvar_get( inp_file, varid = "RAINNC", start  =  c( dar_lon, dar_lat, 1 ), count = c( 1, 1, -1 ) )
> >> RAIN <- RAINC + RAINNCRAIN_TABLE <- cbind( time, RAIN )
> >> write.table( RAIN_TABLE, "Dar_es_Salaam.txt", row.names = FALSE, 
> >>            col.names = c( "Valid Forecast Time",  "Rain (mm)", sep = "\t " )
> >> 
> >> # But no lucky with the red bolded syntax as I end up with the following error message> RAINC <- ncvar_get( inp_file, varid = "RAINC", start  = c( Lon[2], Lat[2], 1 ), count = c( 1, 1, -1 ) )
> >> Error in Rsx_nc4_get_vara_double: NetCDF: Index exceeds dimension bound
> >> Var: RAINC  Ndims: 3  Start: 0,4294967289,38 Count: 17,1,1
> >> Error in ncvar_get_inner(ncid2use, varid2use, nc$var[[li]]$missval, addOffset,  : 
> >>  C function R_nc4_get_vara_double returned error
> >> However when I cahnge the latitude to postive it works fine. Note latitudes in the file data ranges from -16.71505 to 7.787529 as shown below;
> >> head(ncvar_get(inp_file, "XLAT"))
> >> [1] -16.71505 -16.71505 -16.71505 -16.71505 -16.71505 -16.71505
> >>> tail(ncvar_get(inp_file, "XLAT"))
> >> [1] 7.787529 7.787529 7.787529 7.787529 7.787529 7.787529
> >> ## So, how can I get the syntax correct? Please help _____________
> >> Peter  E. Tuju
> >> Dar es Salaam
> >> T A N Z A N I A
> >> ----------------------
> >> 
> >> 
> >> 
> >> 
> >> 
> >>     [[alternative HTML version deleted]]
> >> 
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> 
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
> 
> 
> 

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From dusa.adrian at unibuc.ro  Sat Jan  9 16:35:32 2016
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Sat, 9 Jan 2016 17:35:32 +0200
Subject: [R] Bezier to line segments
In-Reply-To: <CAFEqCdywbFnp2EG7_BPPWvSELqG--GBELRwVi6hb+fDcDchM4w@mail.gmail.com>
References: <CAJ=0CtCUh35L3XjFdqBoc7y+epzLzVo-76Y3aPmmkETS7zu0qA@mail.gmail.com>
	<CAJ=0CtA99P=3ESdrBFVEVKwT94nqQK9GmWFx8UxX2vBNa3D1Ug@mail.gmail.com>
	<CAFEqCdywbFnp2EG7_BPPWvSELqG--GBELRwVi6hb+fDcDchM4w@mail.gmail.com>
Message-ID: <CAJ=0CtAStGuXvoBGKAxyQGd8HKHKT1YOd0AnfXweiiSj7wTCfQ@mail.gmail.com>

Thanks Greg, I'll take a look on that package as well.
I'm still unsure whether to calculate these splines at each function run,
or to store the coordinates of each spline (curve) and read them when
needed. There are performance issues to take into account, but from my
current tests it seems that calculating the coordinates each time decreases
timing performance.

For the moment I opted to store the values, but if I will find a quick(er)
alternative, will change it at the next version.

This is for a new package called venn (now on CRAN), to which I am going to
send an announcement to r-packages.

Best wishes,
Adrian



On Thu, Jan 7, 2016 at 6:54 PM, Greg Snow <538280 at gmail.com> wrote:

> You may also be interested in the xspline function (graphics package,
> so you don't need to install or load anything extra) since you mention
> general splines.  These splines can be made similar to Bezier curves
> (but not exactly the same).  The function returns a set of coordinates
> (when draw=FALSE) that represent line segments for drawing the
> approximate curve.
>
> On Wed, Jan 6, 2016 at 6:43 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
> > I just found the package "bezier".
> > Trying to find the needle, I missed the haystack...
> >
> > On Wed, Jan 6, 2016 at 9:56 AM, Adrian Du?a <dusa.adrian at unibuc.ro>
> wrote:
> >
> >> Dear All,
> >>
> >> I am interested into transforming Bezier curves (or general splines) to
> a
> >> series of line segments.
> >> For simplicity, the Bezier curves are either cubic (arches, no
> inflection
> >> points) or they have at most one inflection point.
> >>
> >> The entry parameters are exactly four points (with x and y coordinates):
> >> - start point
> >> - end point
> >> - and two control points to define the curve.
> >>
> >> I read a lot about parabolic approximation, and there is also a famous
> >> deCasteljau algorithm.
> >>
> >> Before attempting to create my own function, I wonder if something like
> >> this already exists in R (or can easily be adapted to R).
> >>
> >> Thanks in advance for any hint,
> >> Adrian
> >>
> >> --
> >> Adrian Dusa
> >> University of Bucharest
> >> Romanian Social Data Archive
> >> Soseaua Panduri nr.90
> >> 050663 Bucharest sector 5
> >> Romania
> >>
> >
> >
> >
> > --
> > Adrian Dusa
> > University of Bucharest
> > Romanian Social Data Archive
> > Soseaua Panduri nr.90
> > 050663 Bucharest sector 5
> > Romania
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>



-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Sat Jan  9 17:59:13 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 9 Jan 2016 16:59:13 +0000
Subject: [R] Which GLM to use with mixed predictor variables i.e.
 bothdiscrete and continous
In-Reply-To: <CAAJc=rPZSoi_cuDH5kGy7EYeJcDM1zoDEonj3J8M7BmTAQ87OA@mail.gmail.com>
References: <CAAJc=rN6s-W706WStDuPgAncibC9Uw7KnKeMkKRxF+TYa9sc_Q@mail.gmail.com>
	<1452349633050.1626226653@boxbe>
	<CAAJc=rPZSoi_cuDH5kGy7EYeJcDM1zoDEonj3J8M7BmTAQ87OA@mail.gmail.com>
Message-ID: <56913C61.8010308@dewey.myzen.co.uk>

Please reply to the list as well as others may be able to help you. 
Comments below.

On 09/01/2016 16:08, Charles Thuo wrote:
> I read that glm poisson should have continous predictor variables but my
> model has both continous and discrete predictor variables.

I think you need to read more widely.

  In addition
> am not sure which link function to use.

If the people who wrote R chose a default link then, since they know 
more about statistics than you or I, I suggest you use it unless you can 
think of a good reason to change.

>
> Charles
>
> On 9 Jan 2016 17:27, "Michael Dewey" <lists at dewey.myzen.co.uk
> <mailto:lists at dewey.myzen.co.uk>> wrote:
>
>     Boxbe <https://www.boxbe.com/overview> Michael Dewey
>     (lists at dewey.myzen.co.uk <mailto:lists at dewey.myzen.co.uk>) is not on
>     your Guest List
>     <https://www.boxbe.com/approved-list?tc_serial=23948366964&tc_rand=865530497&utm_source=stf&utm_medium=email&utm_campaign=ANNO_MWTP&utm_content=001&token=z0E8RY73ui9gXsH7imOJSvT3I5lZh72UaIrMqObc2fbFhrX9P3xMHHwvotPXEIWH&key=TSFxbER%2F6YngyPWC4RAfJB5uEmZ50rxhXypc%2F8oestk%3D>
>     | Approve sender
>     <https://www.boxbe.com/anno?tc_serial=23948366964&tc_rand=865530497&utm_source=stf&utm_medium=email&utm_campaign=ANNO_MWTP&utm_content=001&token=z0E8RY73ui9gXsH7imOJSvT3I5lZh72UaIrMqObc2fbFhrX9P3xMHHwvotPXEIWH&key=TSFxbER%2F6YngyPWC4RAfJB5uEmZ50rxhXypc%2F8oestk%3D>
>     | Approve domain
>     <https://www.boxbe.com/anno?tc_serial=23948366964&tc_rand=865530497&utm_source=stf&utm_medium=email&utm_campaign=ANNO_MWTP&utm_content=001&dom&token=z0E8RY73ui9gXsH7imOJSvT3I5lZh72UaIrMqObc2fbFhrX9P3xMHHwvotPXEIWH&key=TSFxbER%2F6YngyPWC4RAfJB5uEmZ50rxhXypc%2F8oestk%3D>
>
>
>     Dear Charles
>
>     You showed us a call of glm with which you are presumably not
>     satisfied. What happened to disappoint you?
>
>     On 09/01/2016 09:55, Charles Thuo wrote:
>
>         I have the following glm
>
>         v_glm <-
>         glm(claim_count~ave_age+cost+inc.count+p.statcount,data=v,family=poisson)
>
>         claim_count - dependent variable and is discrete
>
>         ave_age - predictor variable and is continuous
>
>         cost - predictor variable and is continuous
>
>         inc_count - predictor variable and is  discrete
>
>         p.statcount - predictor variable and is  discrete
>
>         Which is the most appropriate glm to use for such a model.
>
>         Charles
>
>                  [[alternative HTML version deleted]]
>
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>         -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         and provide commented, minimal, self-contained, reproducible code.
>
>
>     --
>     Michael
>     http://www.dewey.myzen.co.uk/home.html
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From setareh227 at gmail.com  Sat Jan  9 15:29:51 2016
From: setareh227 at gmail.com (maryam moazam)
Date: Sat, 9 Jan 2016 17:59:51 +0330
Subject: [R] Combining dataframes with different row numbers and
 plotting with ggplot2
In-Reply-To: <56911445.4060003@dewey.myzen.co.uk>
References: <CAM=b0pfwier1Kqy2BEfOr-++AbbxFDuEMCe-9Ec7Vr_Q8s8rSw@mail.gmail.com>
	<56911445.4060003@dewey.myzen.co.uk>
Message-ID: <CAM=b0peX9vX-viQO8TPHpTyzWg1DZk92=wbRPjV+F0aPV3v9Zw@mail.gmail.com>

Dear Michael,

Thanks for your feedback. Actually, I would like to show (and compare) size
distribution of df1 and df2 in the single plot using ggplot2, something
like the attached picture. The command dosesn't lead me to this purpose.
However, I'm really new here, could you please help me more on this?


Thanks in advance,
Maryam





On Sat, Jan 9, 2016 at 5:38 PM, Michael Dewey <lists at dewey.myzen.co.uk>
wrote:

> Dear Maryam
>
> If you just need all the values of size would
> c(df1$size, df2$size)
> work?
>
> On 08/01/2016 21:44, maryam moazam wrote:
>
>> Dear Sir / Madam,
>>
>> I have just come to the amazing R software, so please be patient if my
>> question is basic for you. I have 2 text file (say 1.txt and 2.txt), each
>> file containing 2 columns and different row numbers, like below
>>
>> case size
>> case1 120
>> case2 120
>> case3 121
>> case4 121
>> case5 121
>> case6 122
>> case7 122
>> case8 123
>>
>> I would like to have a one plot for all text files, with x-axis shows the
>> size between 300-1200 with the interval of 200 (300,500,700,900,1200) and
>> size between 1201-1500 with the interval of 1000. For dataframes with the
>> equal row numbers, the following codes worked well,
>>
>> df1 = data.frame("1.txt", header=T)
>> df2 = data.frame("2.txt", header=T)
>> *combining two dataframes with equal row number*
>>
>> df = data.frame(df1$size,df2$size)
>> library(reshape)
>> melted <- melt(df)
>>
>> ggplot(data=melted, aes(value))+aes(fill=variable)+ geom_histogram
>> (binwidth =500)+
>>
>>
>> +scale_x_continuous(breaks=c(seq(300,1000,by=200),seq(1001,15000,by=1000)))
>>
>>
>> but I couldn't reproduce the plot with these codes for dataframes with
>> different row number. I think the problem is* how to combine datafrmaes
>> with the different row number*, could you please help me out on this
>> issue?
>>
>> Thank you in advance
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: plot.png
Type: image/png
Size: 18641 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160109/b721b1c6/attachment.png>

From jdnewmil at dcn.davis.ca.us  Sat Jan  9 20:05:49 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 9 Jan 2016 11:05:49 -0800 (PST)
Subject: [R] Combining dataframes with different row numbers and
 plotting with ggplot2
In-Reply-To: <CAM=b0peX9vX-viQO8TPHpTyzWg1DZk92=wbRPjV+F0aPV3v9Zw@mail.gmail.com>
References: <CAM=b0pfwier1Kqy2BEfOr-++AbbxFDuEMCe-9Ec7Vr_Q8s8rSw@mail.gmail.com>
	<56911445.4060003@dewey.myzen.co.uk>
	<CAM=b0peX9vX-viQO8TPHpTyzWg1DZk92=wbRPjV+F0aPV3v9Zw@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1601091050050.23549@pedal.dcn.davis.ca.us>

Please study each line of code, and use the str command to study the 
intermediate data objects... the examples on this list are almost never 
plug-and-play for your real work. Note that while you provided some of the 
code necessary to make your example reproducible, I had to fill in blanks 
with additional code... the Posting Guide asks you to make your example 
run as-is to get us to the point where you are having problems. The below 
code is a model for posing your future questions as well as an answer to 
this one.

library(ggplot2)

DF1 <- read.table( text =
"case size
case1 120
case2 120
case3 121
case4 121
case5 121
case6 122
case7 122
case8 123
", header=TRUE, as.is=TRUE )

# note the fewer records below
DF2 <- read.table( text =
"case size
case1 120
case2 120
case3 121
case4 121
case5 121
case6 122
case7 122
", header=TRUE, as.is=TRUE )

# While you CAN use reshape to make long data out of wide data, that 
# method for making long data will always presume you have the same number 
# of records for each case. Combine your data directly into long form if 
# that is how it is best represented.
# Below note the use of labels such as "Source" to organize the data
# Also note the use of "stringsAsFactors = FALSE" because concatenating
# factors is almost never a good idea... go read (again?) about what
# factors are if you don't understand why concatenating factors doesn't
# work well
DFL <- rbind( data.frame( Source = "DF1"
                         , size = DF1$size
                         , stringsAsFactors = FALSE
                         )
             , data.frame( Source = "DF2"
                         , size = DF2$size
                         , stringsAsFactors = FALSE
                         )
             )

# Your intent in making this graph is still a little opaque to me.. the 
# breaks are causing logarithmic axis labels, but not all of the breaks 
# show up
ggplot( data = DFL
       , aes( x=size, fill=Source ) ) +
     geom_histogram( binwidth = 500 ) + # might want "position='dodge`"?
     scale_x_continuous( breaks = c( seq( 300, 800, by = 200 )
                                   , seq( 1000, 15000, by = 1000 )
                                   )
                       )

On Sat, 9 Jan 2016, maryam moazam wrote:

> Dear Michael,
>
> Thanks for your feedback. Actually, I would like to show (and compare) size
> distribution of df1 and df2 in the single plot using ggplot2, something
> like the attached picture. The command dosesn't lead me to this purpose.
> However, I'm really new here, could you please help me more on this?
>
>
> Thanks in advance,
> Maryam
>
>
>
>
>
> On Sat, Jan 9, 2016 at 5:38 PM, Michael Dewey <lists at dewey.myzen.co.uk>
> wrote:
>
>> Dear Maryam
>>
>> If you just need all the values of size would
>> c(df1$size, df2$size)
>> work?
>>
>> On 08/01/2016 21:44, maryam moazam wrote:
>>
>>> Dear Sir / Madam,
>>>
>>> I have just come to the amazing R software, so please be patient if my
>>> question is basic for you. I have 2 text file (say 1.txt and 2.txt), each
>>> file containing 2 columns and different row numbers, like below
>>>
>>> case size
>>> case1 120
>>> case2 120
>>> case3 121
>>> case4 121
>>> case5 121
>>> case6 122
>>> case7 122
>>> case8 123
>>>
>>> I would like to have a one plot for all text files, with x-axis shows the
>>> size between 300-1200 with the interval of 200 (300,500,700,900,1200) and
>>> size between 1201-1500 with the interval of 1000. For dataframes with the
>>> equal row numbers, the following codes worked well,
>>>
>>> df1 = data.frame("1.txt", header=T)
>>> df2 = data.frame("2.txt", header=T)
>>> *combining two dataframes with equal row number*
>>>
>>> df = data.frame(df1$size,df2$size)
>>> library(reshape)
>>> melted <- melt(df)
>>>
>>> ggplot(data=melted, aes(value))+aes(fill=variable)+ geom_histogram
>>> (binwidth =500)+
>>>
>>>
>>> +scale_x_continuous(breaks=c(seq(300,1000,by=200),seq(1001,15000,by=1000)))
>>>
>>>
>>> but I couldn't reproduce the plot with these codes for dataframes with
>>> different row number. I think the problem is* how to combine datafrmaes
>>> with the different row number*, could you please help me out on this
>>> issue?
>>>
>>> Thank you in advance
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>>
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From gublaghose at gmail.com  Sun Jan 10 02:13:13 2016
From: gublaghose at gmail.com (Amur Ghose)
Date: Sun, 10 Jan 2016 06:43:13 +0530
Subject: [R] How do I start contributing to R ?
Message-ID: <CAP7avErT2WagNzvwUuspAuzyx9zBP9yULFQ30oLL_P2bNDBb+A@mail.gmail.com>

Hi,

I'm a newcomer to R and have been picking it up for the last month. I
really like it and recently I've been trying the bug tracker to see if
there's any way I can help.

I'm confused what to do after fixing a bug on my local machine. Since
the github mirror says read only, are changes done via pull requests
there or through the main SVN repository ? In that case should I be
just putting code on the mailing lists / post comments on the bug
tracker page ?

I'd also like to know how else to contribute to R , as a person new to
the project.


From robgrantj at yahoo.com.au  Sun Jan 10 02:38:42 2016
From: robgrantj at yahoo.com.au (Rob Grant)
Date: Sun, 10 Jan 2016 12:38:42 +1100
Subject: [R] How to suppress console output when using choose.dir()?
Message-ID: <000301d14b47$a4fb5130$eef1f390$@yahoo.com.au>

Hi,

I am wondering how to suppress console output when using choose.dir() on
Windows 10 with RStudio. 
Am getting a log output when using this function. Example: 

... 
22:07:18.873 INFO
|9824|ccdi_client_protorpc.cpp:413:ccdi::client::CCDIGetSyncState| The CCD
process doesn't appear to be running. 
22:07:18.873##ERR##|9824|ACloudToBeSynced.cpp:80:CACloudToBeSynced::IsMember
Of| CCDIGetSyncState for syncbox fail rv -9055 
... 


Have tried sink(), invisible(), options(echo = FALSE), capture.output().
None working for me.

Regards,

Rob Grant


	[[alternative HTML version deleted]]


From ligges at statistik.tu-dortmund.de  Sun Jan 10 08:50:30 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sun, 10 Jan 2016 08:50:30 +0100
Subject: [R] How to suppress console output when using choose.dir()?
In-Reply-To: <000301d14b47$a4fb5130$eef1f390$@yahoo.com.au>
References: <000301d14b47$a4fb5130$eef1f390$@yahoo.com.au>
Message-ID: <56920D46.7050707@statistik.tu-dortmund.de>



On 10.01.2016 02:38, Rob Grant wrote:
> Hi,
>
> I am wondering how to suppress console output when using choose.dir() on
> Windows 10 with RStudio.
> Am getting a log output when using this function. Example:
>
> ...
> 22:07:18.873 INFO
> |9824|ccdi_client_protorpc.cpp:413:ccdi::client::CCDIGetSyncState| The CCD
> process doesn't appear to be running.
> 22:07:18.873##ERR##|9824|ACloudToBeSynced.cpp:80:CACloudToBeSynced::IsMember
> Of| CCDIGetSyncState for syncbox fail rv -9055
> ...
>

This output does not come from R, since we do not have any 
ACloudToBeSynced.cpp ....

So perhaps from a contributed package or RStudio, in each case you need 
to talk to the correpsonding maintainer.

Best,
Uwe Ligges

>
> Have tried sink(), invisible(), options(echo = FALSE), capture.output().
> None working for me.
>
> Regards,
>
> Rob Grant
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.tu-dortmund.de  Sun Jan 10 08:52:54 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sun, 10 Jan 2016 08:52:54 +0100
Subject: [R] How do I start contributing to R ?
In-Reply-To: <CAP7avErT2WagNzvwUuspAuzyx9zBP9yULFQ30oLL_P2bNDBb+A@mail.gmail.com>
References: <CAP7avErT2WagNzvwUuspAuzyx9zBP9yULFQ30oLL_P2bNDBb+A@mail.gmail.com>
Message-ID: <56920DD6.5010106@statistik.tu-dortmund.de>

Thanks for your offer to contribute.

R is developed via the svn repository. If you want to change patches for 
bugs from the repository. Please use a curent version of R-devel for 
fixing the issue and submit the patch as a diff against the R-devel 
sources and post it in form of an attachment in the bug tracker.

Best,
Uwe Ligges


On 10.01.2016 02:13, Amur Ghose wrote:
> Hi,
>
> I'm a newcomer to R and have been picking it up for the last month. I
> really like it and recently I've been trying the bug tracker to see if
> there's any way I can help.
>
> I'm confused what to do after fixing a bug on my local machine. Since
> the github mirror says read only, are changes done via pull requests
> there or through the main SVN repository ? In that case should I be
> just putting code on the mailing lists / post comments on the bug
> tracker page ?
>
> I'd also like to know how else to contribute to R , as a person new to
> the project.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From soni.archit1989 at gmail.com  Sun Jan 10 15:06:49 2016
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Sun, 10 Jan 2016 08:06:49 -0600
Subject: [R] Fraud detection Model in R
Message-ID: <CAJ7HxByZufnwMLT77MQV5=h6cWkzg_aDSZUGsLu_nnQNt-DpUQ@mail.gmail.com>

Hello R lovers,

For the upcoming hackathon I have thought of an idea to flag fraudulent
claims with the probability and send an email to the designated parties
that a fraudulent claim has been submitted and the probability is x.

I am new to R, but what I have researched on the internet is that I should
have a data model to fit my data into that + machine learning and Spotfire
to visualise that.

I need your guidance and suggestion as how to take on this challenge, what
documents i can read or any sample fraud detection model in place.

Any help or guidance is much needed and appreciated.

Best Regards,
Archit

	[[alternative HTML version deleted]]


From mkashif at uaf.edu.pk  Sun Jan 10 15:56:26 2016
From: mkashif at uaf.edu.pk (Muhammad  Kashif)
Date: Sun, 10 Jan 2016 14:56:26 +0000
Subject: [R] Calculation of time for 5000 simulation study
Message-ID: <DB4PR07MB379E23F0731D162237F339F94C80@DB4PR07MB379.eurprd07.prod.outlook.com>

Dear

Is there any function which calculate time for one simulation  when we use  " asim=2000" under any simulation study. I run one simulation code on R and i have core i 5 laptop. Each simulation take about 30 -40 minutes.

Is there any function which calculate time of each output.


thanks

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Sun Jan 10 16:07:19 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Sun, 10 Jan 2016 15:07:19 +0000
Subject: [R] Calculation of time for 5000 simulation study
In-Reply-To: <DB4PR07MB379E23F0731D162237F339F94C80@DB4PR07MB379.eurprd07.prod.outlook.com>
References: <DB4PR07MB379E23F0731D162237F339F94C80@DB4PR07MB379.eurprd07.prod.outlook.com>
Message-ID: <CAKVAULMLpEh9SmewP3hjG7_gyhS9g91au9j=1goHZ1xLCETtFg@mail.gmail.com>

Hi Muhammad,

is the system.time() function what you are looking for?

You can use it like this:

system.time(for(i in 1:100) mad(runif(1000)))

or

f1 <- function(){
for(i in 1:100) mad(runif(1000))
}

system.time(f1)

HTH
Ulrik

On Sun, 10 Jan 2016 at 15:58 Muhammad Kashif <mkashif at uaf.edu.pk> wrote:

> Dear
>
> Is there any function which calculate time for one simulation  when we
> use  " asim=2000" under any simulation study. I run one simulation code on
> R and i have core i 5 laptop. Each simulation take about 30 -40 minutes.
>
> Is there any function which calculate time of each output.
>
>
> thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sjo at incois.gov.in  Mon Jan 11 05:58:17 2016
From: sjo at incois.gov.in (Sudheer Joseph)
Date: Mon, 11 Jan 2016 04:58:17 +0000
Subject: [R] cross correlation of filtered Time series
Message-ID: <fa171f4b60d3407cbc73069de327a535@INCOISEXCMB02.incois.gov.in>

Dear Experts,

I have been trying to find lagged correlation between two time series  which are band pass filtered  for 30-90 day band. I would like to understand the lagged correlation between these 2 series in this band.

The issue is that auto-correlations exist in both series and( also both have the effect of filtering if any).  I am unclear on how to separate out these factors and find the true lagged relation if any among the 2 series. Apparently there is a strong lagged correlation between the two series around 10 days lag. And there is strong ACF in both series at around 20 days. So it is difficult for me to interpret the cross correlation.

If your time permits can you please help me with your advice on this?

The plot is at below link:

https://drive.google.com/file/d/0B3heUQNme7G5UEpzWjd2WmxYdVk/view?usp=sharing

The data is at below link:

https://drive.google.com/file/d/0B3heUQNme7G5T2JjeTlDNzdkX00/view?usp=sharing

R code:

dc = read.csv('test_dat.csv', header=TRUE)
par(mfrow=c(3,1))
acf(dc$sst, 36)
acf(dc$t2m, 36)
ccf(dc$sst, dc$t2m, 36)

with best regards,
Sudheer

From ALPAY.KOCAK at tuik.gov.tr  Sun Jan 10 21:21:18 2016
From: ALPAY.KOCAK at tuik.gov.tr (=?iso-8859-9?Q?NECMETT=DDN_ALPAY_KO=C7AK?=)
Date: Sun, 10 Jan 2016 20:21:18 +0000
Subject: [R] Help request from Ph.D. Students
Message-ID: <57B547CBF14C9947A3B0B8DDD15BEED3E4378754@TUIKEXCMAIL04.tuikmerkez.tuik.gov.tr>

Dear All,
I am Ph.D. student in Econometrics. My thesis is about "Linear Filtering on a Time Series"  which R has already a nice package, namely "Filter". This package is really helpful for my study. But, I really help from you to create two filter using with "filter" package.
I want to create two filter desribed in attachment (effects word file) using "filter" package. But, I dont know how?

I really need your reply,
Sincerely,

Alpay KOCAK

The "filter" package usage is given below.
filter(x, filter, method = c("convolution", "recursive"),
                sides = 2, circular = FALSE, init)
And arguments,
x : a univariate or multivariate time series.
filter : a vector of filter coefficients in reverse time order (as for AR or MA coefficients).
method : Either "convolution" or "recursive" (and can be abbreviated). If "convolution" a moving average is used: if "recursive" an autoregression is used.
sides : for convolution filters only. If sides = 1 the filter coefficients are for past values only; if sides = 2 they are centred around lag 0. In this case the length of the filter should be odd, but if it is even, more of the filter is forward in time than backward.
circular : for convolution filters only. If TRUE, wrap the filter around the ends of the series, otherwise assume external values are missing (NA).
init : for recursive filters only. Specifies the initial values of the time series just prior to the start value, in reverse time order. The default is a set of zeros.


From msg.contact at laposte.net  Sun Jan 10 18:56:38 2016
From: msg.contact at laposte.net (msg.contact at laposte.net)
Date: Sun, 10 Jan 2016 18:56:38 +0100 (CET)
Subject: [R] Package "boot" and Bca for %
In-Reply-To: <2079914578.1755497.1452448433833.JavaMail.zimbra@laposte.net>
Message-ID: <874828437.1761467.1452448598687.JavaMail.zimbra@laposte.net>



Dear all, 

? 

Beginner with R, I need somme help to write a correct R script using package "boot" to calculate % and Bca confidence intervals for percentages to describe a population structure. 

? 

I have the following strucure data set: 

data<-c(A, A, A, A, A, A, B, B, B, B, B, C, C, C, C, C, C, C, C, C) 

? 

where A, B and C are different species. 

? 

I want to calculate robust % of A; % of B and % of C with Bca confidance intervals using package "boot" and boostraps of the initial data set. 

? 

Thanks for your help. 

All the best. 
Steeve M. 

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Jan 11 08:17:46 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 11 Jan 2016 07:17:46 +0000
Subject: [R] [Q] It it possible to create the data frame only non-zero
 data column?
In-Reply-To: <CAKVAULM9n-0Limf1_7U5sS7w-JmW8Cz64PmZquwwppTbY8oT-Q@mail.gmail.com>
References: <CA+Tq-RpThb89BpYofhvAxDAZA_t+geEMxGyye4ZuHngfVHbBwg@mail.gmail.com>
	<CAKVAULM9n-0Limf1_7U5sS7w-JmW8Cz64PmZquwwppTbY8oT-Q@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C50092B7@SRVEXCHMBX.precheza.cz>

Hi

as.matrix is rather dangerous, it converts all values to lowest mode which, if there is text column, is character.

And if I understand correctly original post was about removing all zero columns.

Using plain zero comparison can be dangerous due to possibility of compensating negative and positive values.
> dat<-data.frame(a=c(-1,0,1), b=c(0,0,0), c=c(0,0,1))
> dat
   a b c
1 -1 0 0
2  0 0 0
3  1 0 1
> colSums(dat)==0
    a     b     c
 TRUE  TRUE FALSE

Remove incorrectly first column too.

I would proceed with NA substitution, which is safer.

> dat[dat==0]<-NA
> colSums(is.na(dat))==nrow(dat)
    a     b     c
FALSE  TRUE FALSE
>

And even this can be problematic if values in dat are not integers.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ulrik
> Stervbo
> Sent: Friday, January 08, 2016 6:06 PM
> To: Hiroyuki Sato; r-help at r-project.org
> Subject: Re: [R] [Q] It it possible to create the data frame only non-
> zero data column?
>
> Could you use rowsum and select rows larger then 0?
>
> Something like:
>
> result[rowsum(as.matrix(result) > 0, ]
>
> On Fri, 8 Jan 2016 at 18:00 Hiroyuki Sato <hiroysato at gmail.com> wrote:
>
> > Hello all.
> >
> > I re-post this question by e-mail.
> > (I posted via google-group. But It's not posted yet.)
> >
> > I'm newbie GNU R.
> >
> > I would like to compare two datas.
> > How to select columns which has non-zero datas?.
> > It it possible to create the data frame only VAL3(non-zero data)
> > column with command?
> >
> > Formatted sample.
> > https://gist.github.com/hiroyuki-sato/cb36584f6cd5845b6c3e
> >
> > sample1.txt
> >
> >   ID,VAL1,VAL2,VAL3
> >   ID1,0,2,3
> >   ID2,0,2,3
> >   ID3,0,2,3
> >
> >   real data has 5000 columns.
> >
> > sample2.txt
> >
> >   ID,VAL1,VAL2,VAL3
> >   ID1,0,2,3
> >   ID2,0,2,3
> >   ID3,0,2,2
> >
> >   The difference sample1 and sample2 is ID3/VAL3.
> >     sample1: 3
> >     sample2: 2
> >
> > R commands.
> >
> >   sample1 <- read.table("sample1.txt",header=T,sep=',')
> >   sample2 <- read.table("sample2.txt",header=T,sep=',')
> >
> >   result <- sample1[,2:4] - sample2[,2:4]
> >   result
> >     VAL1 VAL2 VAL3
> >   1    0    0    0
> >   2    0    0    0
> >   3    0    0    1
> >
> > I would like to create data frame which has non-zero value columns.
> > Could you tell me how to do it?
> >
> > Best regards.
> >
> > --
> > Hiroyuki Sato.
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From suparna.mitra.sm at gmail.com  Mon Jan 11 11:00:33 2016
From: suparna.mitra.sm at gmail.com (Suparna Mitra)
Date: Mon, 11 Jan 2016 10:00:33 +0000
Subject: [R] Propensity score matching with MatchIt
Message-ID: <CAFdg=fWu3jt-cfi0OmjE8j+21gdxBN1o-WbFwH8D4LFugWnW=Q@mail.gmail.com>

Hello R experts,
I am trying to do Propensity score matching for a medical data with two
types of surgery.
But somehow I am getting Summary of balance for all data and the matched
data exactly similar resulting the Percent Balance Improvement as zero.

> surgery.data<-read.csv(file.choose(), head = TRUE)
> surgery.data
   Sample Surgerytype Age ASAgrade  BMI FIGOstage PreviousAbdoSurgery
1       2           1  41        1 22.3         3                   0
2       4           1  49        2 19.5         3                   0
3       5           1  58        2 28.8         3                   0
4       8           1  34        1 29.1         3                   0
5       9           1  49        1 25.1         3                   0
6      13           1  30        2 29.0         3                   0
7      14           1  31        1 23.6         3                   0
8      15           1  29        1 33.7         3                   2
9      20           1  25        1 24.6         3                   0
10     28           1  28        1 21.0         3                   0
11     29           1  29        2 21.4         3                   0
12     30           1  61        1 25.2         3                   3
13     32           1  48        1 22.7         3                   0
14     33           1  24        1 26.1         3                   3
15     34           1  39        1 23.7         3                   0
16     36           1  39        2 34.6         3                   1
17     37           1  68        2 27.0         3                   0
18     49           1  71        2 30.8         3                   3
19     50           1  73        2 25.8         3                   0
20     54           1  30        2 23.1         3                   0
21     65           1  45        2 34.6         3                   0
22     77           1  41        1 29.8         3                   3
23     82           1  41        2 33.8         3                   0
24     86           1  34        1 34.7         3                   0
25     87           1  28        2 21.4         3                   0
26     88           1  35        1 25.5         3                   2
27     89           1  46        1 31.9         3                   1
28     91           1  48        2 20.7         3                   0
29     92           1  28        2 22.4         3                   2
30     96           1  45        1 22.7         3                   1
31     97           1  39        2 19.7         3                   1
32     98           1  34        1 27.6         3                   2
33    101           1  41        1 22.5         3                   0
34    107           1  31        2 31.0         3                   0
35    113           1  51        2 33.2         3                   0
36    114           1  43        2 22.5         3                   2
37      6           0  50        1 22.9         3                   0
38      7           0  43        2 25.6         3                   0
39     11           0  43        1 23.8         3                   2
40     12           0  31        1 22.0         3                   0
41     16           0  31        1 27.2         3                   2
42     17           0  34        1 19.6         3                   0
43     18           0  56        3 25.2         3                   0
44     21           0  39        1 26.6         3                   0
45     25           0  64        2 24.5         3                   0
46     45           0  61        1 21.9         3                   0
47     47           0  64        1 28.5         3                   0
48     53           0  54        2 26.8         5                   0
49     55           0  40        1 23.1         3                   0
50     57           0  46        1 26.2         3                   3
51     59           0  34        1 21.5         3                   0
52     62           0  25        2 23.8         3                   0
53     63           0  56        2 24.6         3                   0
54     64           0  45        1 24.2         3                   0
55     66           0  42        1 30.4         3                   0
56     67           0  49        2 35.8         2                   0
57     69           0  63        1 24.7         3                   0
58     70           0  29        1 29.7         5                   0
59     71           0  39        1 19.9         3                   3
60     73           0  62        1 28.0         3                   0
61     74           0  24        1 26.7         3                   0
62     75           0  70        2 31.2         3                   4
63     76           0  42        2 23.0         3                   0
64     79           0  56        1 34.9         3                   0
65     81           0  40        1 25.0         3                   0
66     83           0  39        2 29.6         3                   4
67     84           0  58        1 22.1         1                   0
68    104           0  36        1 28.6         3                   0
69    105           0  37        1 31.2         3                   0
70    109           0  33        1 25.0         3                   0
71    110           0  37        1 25.8         3                   0
72    111           0  34        1 21.0         3                   2
> m.out1 <- matchit(Surgerytype ~ Age + ASAgrade + BMI + FIGOstage +
PreviousAbdoSurgery, data = surgery.data, method = "nearest", distance =
"logit")
> summary(m.out1) # check balance

Call:
matchit(formula = Surgerytype ~ Age + ASAgrade + BMI + FIGOstage +
    PreviousAbdoSurgery, data = surgery.data, method = "nearest",
    distance = "logit")

Summary of balance for all data:
                    Means Treated Means Control SD Control Mean Diff eQQ
Med eQQ Mean eQQ Max
distance                   0.5426        0.4574     0.1429    0.0853
 0.0913   0.0867  0.1686
Age                       41.2778       44.6111    12.2528   -3.3333
 4.0000   4.1111 10.0000
ASAgrade                   1.5000        1.3056     0.5248    0.1944
 0.0000   0.2500  1.0000
BMI                       26.4194       25.8500     3.8345    0.5694
 0.8500   1.1472  3.5000
FIGOstage                  3.0000        3.0278     0.6088   -0.0278
 0.0000   0.1944  2.0000
PreviousAbdoSurgery        0.7222        0.5556     1.2058    0.1667
 0.0000   0.2778  2.0000


Summary of balance for matched data:
                    Means Treated Means Control SD Control Mean Diff eQQ
Med eQQ Mean eQQ Max
distance                   0.5426        0.4574     0.1429    0.0853
 0.0913   0.0867  0.1686
Age                       41.2778       44.6111    12.2528   -3.3333
 4.0000   4.1111 10.0000
ASAgrade                   1.5000        1.3056     0.5248    0.1944
 0.0000   0.2500  1.0000
BMI                       26.4194       25.8500     3.8345    0.5694
 0.8500   1.1472  3.5000
FIGOstage                  3.0000        3.0278     0.6088   -0.0278
 0.0000   0.1944  2.0000
PreviousAbdoSurgery        0.7222        0.5556     1.2058    0.1667
 0.0000   0.2778  2.0000

Percent Balance Improvement:
                    Mean Diff. eQQ Med eQQ Mean eQQ Max
distance                     0       0        0       0
Age                          0       0        0       0
ASAgrade                     0       0        0       0
BMI                          0       0        0       0
FIGOstage                    0       0        0       0
PreviousAbdoSurgery          0       0        0       0

Sample sizes:
          Control Treated
All            36      36
Matched        36      36
Unmatched       0       0
Discarded       0       0

But if I test separately for Age or BMI, I know there are differences in
these two groups. As results shows here:
> summary(lm(Age~Surgerytype,data=surgery.data))

Call:
lm(formula = Age ~ Surgerytype, data = surgery.data)

Residuals:
    Min      1Q  Median      3Q     Max
-20.611 -10.361  -2.278   7.722  31.722

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   37.944      4.663   8.138 1.02e-11 ***
Surgerytype    3.333      2.949   1.130    0.262
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 12.51 on 70 degrees of freedom
Multiple R-squared: 0.01792, Adjusted R-squared: 0.003895
F-statistic: 1.278 on 1 and 70 DF,  p-value: 0.2622


######
> summary(lm(BMI~Surgerytype,data=surgery.data))

Call:
lm(formula = BMI ~ Surgerytype, data = surgery.data)

Residuals:
   Min     1Q Median     3Q    Max
-6.919 -3.719 -0.850  2.698  9.950

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  26.9889     1.6089   16.77   <2e-16 ***
Surgerytype  -0.5694     1.0176   -0.56    0.578
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 4.317 on 70 degrees of freedom
Multiple R-squared: 0.004454, Adjusted R-squared: -0.009768
F-statistic: 0.3132 on 1 and 70 DF,  p-value: 0.5775

##Or a t-test for Age
> t.test(surgery.data$Age[surgery.data $Surgerytype ==1], surgery.data
$Age[surgery.data $Surgerytype ==2],paired=FALSE)

Welch Two Sample t-test

data:  surgery.data$Age[surgery.data$Surgerytype == 1] and
surgery.data$Age[surgery.data$Surgerytype == 2]
t = -1.1303, df = 69.883, p-value = 0.2622
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -9.215118  2.548451
sample estimates:
mean of x mean of y
 41.27778  44.61111

======
May be I am doing a silly mistake. Can anybody please help me?
Thanks a lot,
Mitra

	[[alternative HTML version deleted]]


From bretschr at xs4all.nl  Mon Jan 11 13:07:15 2016
From: bretschr at xs4all.nl (Franklin Bretschneider)
Date: Mon, 11 Jan 2016 13:07:15 +0100
Subject: [R] Help request from Ph.D. Students
In-Reply-To: <57B547CBF14C9947A3B0B8DDD15BEED3E4378754@TUIKEXCMAIL04.tuikmerkez.tuik.gov.tr>
References: <57B547CBF14C9947A3B0B8DDD15BEED3E4378754@TUIKEXCMAIL04.tuikmerkez.tuik.gov.tr>
Message-ID: <E5605E29-BBAF-4EEE-9C0D-71EDDEC4692F@xs4all.nl>

Dear NECMETT?N ALPAY KO?AK,

Re:

> Dear All,
> I am Ph.D. student in Econometrics. My thesis is about "Linear Filtering on a Time Series"  which R has already a nice package, namely "Filter". This package is really helpful for my study. But, I really help from you to create two filter using with "filter" package.
> I want to create two filter desribed in attachment (effects word file) using "filter" package. But, I dont know how?
> 
> I really need your reply,
> Sincerely,
> 
> Alpay KOCAK
> (etc...)


I don't know a package called "filter", but a function "filter" is in the "stats" package
There are however more packages to filter time data. I use "signal", which also has a "filter" function in addition to functions to design filters yourself (from simple first-order to higher-order butterworth etc).
This package is intended to filter (electrical) signals, but might be used for any time series.
In addition, there are several packages for the analysis of seasonal data. You might search CRAN for the names.
Success and
Best Wishes,


Frank
------



Franklin Bretschneider
Dept of Biology
Utrecht University
bretschr at xs4all.nl


From dusa.adrian at unibuc.ro  Sat Jan  9 16:56:25 2016
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Sat, 9 Jan 2016 17:56:25 +0200
Subject: [R] [R-pkgs] new package: venn
Message-ID: <CAJ=0CtBzXpeHiKvJKuiK45xFTgLXyU0P-XfR98Xrj6O3x5J-pg@mail.gmail.com>

Dear R users,

I would like to announce a new package that has just the appeared on CRAN,
called "venn" version 1.0:
http://cran.r-project.org/web/packages/venn/
(binaries will appear in one or two days)

Although there are quite a few packages that draw Venn diagrams, there are
a number of reasons for yet another one:

- this package draws diagrams up to 7 sets (!) while other packages top at 5

- in addition, this package is also capable to draw any boolean union of
set intersections, using different colors (transparency included), using a
meta-command

- efforts were employed to create these diagrams using base R, without
using any dependencies to other graphics oriented packages

- there are a variety of input data which are automatically recognised,
making the package user friendly

- the technology behind this package is completely different from the other
Venn diagrams functions, similar to geographical maps where polygons can be
constructed on a hierarchical order, the way administrative units belong to
superior, higher units.

The most impressive diagram to show off is the so-called "Adelaide" for 7
sets, which can be viewed simply with:

> venn(7)

Comments and suggestions are, as always, welcome.

Best wishes,
Adrian

--
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From msg.contact at laposte.net  Mon Jan 11 11:01:50 2016
From: msg.contact at laposte.net (msg.contact at laposte.net)
Date: Mon, 11 Jan 2016 11:01:50 +0100 (CET)
Subject: [R] [FORGED]  Package "boot" and Bca for %
In-Reply-To: <56934711.1020104@auckland.ac.nz>
References: <874828437.1761467.1452448598687.JavaMail.zimbra@laposte.net>
	<56934711.1020104@auckland.ac.nz>
Message-ID: <1186835459.656743.1452506510005.JavaMail.zimbra@laposte.net>

This question is not for homework... 
This question is about a real question about population structure with different morphotypes... 
? 
Thanks. 

----- Mail original -----

De: "Rolf Turner" <r.turner at auckland.ac.nz> 
?: "msg contact" <msg.contact at laposte.net> 
Envoy?: Lundi 11 Janvier 2016 07:09:21 
Objet: Re: [FORGED] [R] Package "boot" and Bca for % 


On 11/01/16 06:56, Steeve M. via R-help wrote: 

> 
> Dear all, 
> 
> Beginner with R, I need somme help to write a correct R script using 
> package "boot" to calculate % and Bca confidence intervals for 
> percentages to describe a population structure. 
> 
> I have the following strucure data set: 
> 
> data<-c(A, A, A, A, A, A, B, B, B, B, B, C, C, C, C, C, C, C, C, C) 
> 
> where A, B and C are different species. 
> 
> I want to calculate robust % of A; % of B and % of C with Bca 
> confidance intervals using package "boot" and boostraps of the initial 
> data set. 
> 
> Thanks for your help. 

Homework? ?This list has a "no homework" policy. 

cheers, 

Rolf Turner 

-- 
Technical Editor ANZJS 
Department of Statistics 
University of Auckland 
Phone: +64-9-373-7599 ext. 88276 


	[[alternative HTML version deleted]]


From robgrantj at yahoo.com.au  Mon Jan 11 13:11:26 2016
From: robgrantj at yahoo.com.au (Rob Grant)
Date: Mon, 11 Jan 2016 23:11:26 +1100
Subject: [R] How to suppress console output when using choose.dir()?
In-Reply-To: <56920D46.7050707@statistik.tu-dortmund.de>
References: <000301d14b47$a4fb5130$eef1f390$@yahoo.com.au>
	<56920D46.7050707@statistik.tu-dortmund.de>
Message-ID: <001301d14c69$33a58e60$9af0ab20$@yahoo.com.au>

Thank you.

Found uninstalling PC bloatware 'Acer Portal' rectified the problem.

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.tu-dortmund.de] 
Sent: Sunday, 10 January 2016 6:51 PM
To: Rob Grant; r-help at r-project.org
Subject: Re: [R] How to suppress console output when using choose.dir()?



On 10.01.2016 02:38, Rob Grant wrote:
> Hi,
>
> I am wondering how to suppress console output when using choose.dir() 
> on Windows 10 with RStudio.
> Am getting a log output when using this function. Example:
>
> ...
> 22:07:18.873 INFO
> |9824|ccdi_client_protorpc.cpp:413:ccdi::client::CCDIGetSyncState| The 
> |9824|CCD
> process doesn't appear to be running.
> 22:07:18.873##ERR##|9824|ACloudToBeSynced.cpp:80:CACloudToBeSynced::Is
> Member
> Of| CCDIGetSyncState for syncbox fail rv -9055
> ...
>

This output does not come from R, since we do not have any ACloudToBeSynced.cpp ....

So perhaps from a contributed package or RStudio, in each case you need to talk to the correpsonding maintainer.

Best,
Uwe Ligges

>
> Have tried sink(), invisible(), options(echo = FALSE), capture.output().
> None working for me.
>
> Regards,
>
> Rob Grant
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mohammed.ouassou at statkart.no  Mon Jan 11 15:43:04 2016
From: mohammed.ouassou at statkart.no (Mohammed Ouassou)
Date: Mon, 11 Jan 2016 15:43:04 +0100
Subject: [R] Help request from Ph.D. Students
In-Reply-To: <E5605E29-BBAF-4EEE-9C0D-71EDDEC4692F@xs4all.nl>
References: <57B547CBF14C9947A3B0B8DDD15BEED3E4378754@TUIKEXCMAIL04.tuikmerkez.tuik.gov.tr>
	<E5605E29-BBAF-4EEE-9C0D-71EDDEC4692F@xs4all.nl>
Message-ID: <1452523384.3993.0.camel@localhost.localdomain>

Try DLM package :

 dlm: Bayesian and Likelihood Analysis of Dynamic Linear Models

M.O

On Mon, 2016-01-11 at 13:07 +0100, Franklin Bretschneider wrote:
> Dear NECMETT?N ALPAY KO?AK,
> 
> Re:
> 
> > Dear All,
> > I am Ph.D. student in Econometrics. My thesis is about "Linear Filtering on a Time Series"  which R has already a nice package, namely "Filter". This package is really helpful for my study. But, I really help from you to create two filter using with "filter" package.
> > I want to create two filter desribed in attachment (effects word file) using "filter" package. But, I dont know how?
> > 
> > I really need your reply,
> > Sincerely,
> > 
> > Alpay KOCAK
> > (etc...)
> 
> 
> I don't know a package called "filter", but a function "filter" is in the "stats" package
> There are however more packages to filter time data. I use "signal", which also has a "filter" function in addition to functions to design filters yourself (from simple first-order to higher-order butterworth etc).
> This package is intended to filter (electrical) signals, but might be used for any time series.
> In addition, there are several packages for the analysis of seasonal data. You might search CRAN for the names.
> Success and
> Best Wishes,
> 
> 
> Frank
> ------
> 
> 
> 
> Franklin Bretschneider
> Dept of Biology
> Utrecht University
> bretschr at xs4all.nl
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From d_margarita_p at hotmail.com  Mon Jan 11 13:28:56 2016
From: d_margarita_p at hotmail.com (DIANA MARGARITA)
Date: Mon, 11 Jan 2016 20:28:56 +0800
Subject: [R] OpenAir package help!!
Message-ID: <BLU436-SMTP2214BAE622BF3556D90D781ADC90@phx.gbl>

Dear all,

I just wondered if anyone could help me on how to change the color of labels of a polar coordinate in a polar plot using the openair package of R.

Kindest regards,

Diana 

From Wittner.Ben at mgh.harvard.edu  Mon Jan 11 16:38:41 2016
From: Wittner.Ben at mgh.harvard.edu (Wittner, Ben, Ph.D.)
Date: Mon, 11 Jan 2016 15:38:41 +0000
Subject: [R] rgl.snapshot only captures a small portion what's visible
 in the RGL device window on CentOS 7
In-Reply-To: <56905E85.8040604@gmail.com>
References: <C402B4BC4499874FBA9869DA6E7A13F964B18628@PHSX10MB8.partners.org>
	<56905E85.8040604@gmail.com>
Message-ID: <C402B4BC4499874FBA9869DA6E7A13F964B1974B@PHSX10MB8.partners.org>

Dear Duncan, Thanks very much for your reply. I guess I'll start with R-SIG-Fedora. I'll keep you cc'd unless you tell me you would rather I not. -Ben

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com] 
Sent: Friday, January 08, 2016 8:13 PM
To: Wittner, Ben, Ph.D.; r-help at r-project.org
Subject: Re: [R] rgl.snapshot only captures a small portion what's visible in the RGL device window on CentOS 7

On 08/01/2016 4:13 PM, Wittner, Ben, Ph.D. wrote:
> Hello,
>
> As an example, I ran the following code:
>
> library("rgl")
> example(plot3d)
> rgl.snapshot("test.png")
>
> The full plot is visible in the window titled RGL device 1 [Focus], 
> but only a small portion of the upper left part of the plot is visible 
> in test.png (see attached test.png, if the list server attaches it. 
> Otherwise, email me if you like and I'll send it directly to you.)
>
> The output of sessionInfo() is as follows:
>
> R version 3.2.3 (2015-12-10)
> Platform: x86_64-pc-linux-gnu (64-bit) Running under: CentOS Linux 7 
> (Core)
>
> locale:
>   [1] LC_CTYPE=en_US.utf8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.utf8        LC_COLLATE=en_US.utf8
>   [5] LC_MONETARY=en_US.utf8    LC_MESSAGES=en_US.utf8
>   [7] LC_PAPER=en_US.utf8       LC_NAME=C
>   [9] LC_ADDRESS=C              LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.utf8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rgl_0.95.1441
>
> loaded via a namespace (and not attached):
> [1] tools_3.2.3
>
> In addition to the CentOS 7 machine, I have a CentOS 5.3 machine, which has R 3.0.2 and rgl 0.93.996. I tried the code above on it and it captured the full window in the output of rgl.snapshot (i.e., it worked properly).
>
> To see whether the difference could be attributed to the CentOS version or the R/rgl version, I put R 3.0.2 with rgl 0.93.996 on the CentOS 7 machine and ran the code above. As with the earlier CentOS 7 run, only a small portion of the plot was visible in the output of rgl.snapshot. So it seems the difference is due to a difference in the CentOS versions and not the R/rgl versions.
>
> Thanks in advance for any help.
>

I think you'll need to contact CentOS about this.  That code hasn't changed in rgl in a long time.  It's possible that rgl is wrong, but I'd need more information from them to convince me.

Duncan Murdoch



The information in this e-mail is intended only for the ...{{dropped:11}}


From evan.cooch at gmail.com  Mon Jan 11 16:44:12 2016
From: evan.cooch at gmail.com (Evan Cooch)
Date: Mon, 11 Jan 2016 10:44:12 -0500
Subject: [R] different coloured axis title labels for different axes
Message-ID: <5693CDCC.8010602@gmail.com>

Consider a simple plot of X vs Y.  There are elements on the plot that 
represent X, or Y, that are presented in different colours (say, blue 
for X,   red for Y). Rather than use a legend, I would like to have the 
title label for the X-axis be in blue, and the title label for the 
Y-axis be in red.

While it is trivial to change the color of the axis title labels for 
*both* axes at the same time, I haven't figured out how to trick thing 
into generating a blue title label for the X-axis, and a red title label 
for the Y- axis (i.e., different colours on different axes).

I'm sure this is out there on searchable pages, but, I haven't managed 
to stumble across the appropriate search phrase(s).

Pointers to the obvious solution welcomed in advance.


Cheers....

	[[alternative HTML version deleted]]


From evan.cooch at gmail.com  Mon Jan 11 16:59:08 2016
From: evan.cooch at gmail.com (Evan Cooch)
Date: Mon, 11 Jan 2016 10:59:08 -0500
Subject: [R] embedding expression into title in R plot
Message-ID: <5693D14C.6050401@gmail.com>

Suppose I've specified that the xlab for a plot is

expression(bold(species~(italic(N1))))

In other words, I want the axis label to be bold, italic 'species (N1)'

Now, I want the title for the plot to be have this label embedded in the 
title.

Say, 'This is the plot for Species (N1)'.

For a variety of reasons, I've set this up so that the xlab is a global 
parameter (basically, because the labels are set in a function which 
when called, generates various plots):

x_label <<- expression(bold(species~(italic(N1))))

So, in the title, I've tried

  title(main=paste("This is the plot for ",x_label,"nullcline", sep=" "));

but what this does is generate something like

'This is the plot for bold(species~(italic(N1)))'

In other words, it pastes the text of the expression into the title, but 
not what the expression 'evaluates' to.

Is there any way around this?

Thanks in advance...


	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Mon Jan 11 18:52:55 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Mon, 11 Jan 2016 09:52:55 -0800
Subject: [R] different coloured axis title labels for different axes
In-Reply-To: <5693CDCC.8010602@gmail.com>
References: <5693CDCC.8010602@gmail.com>
Message-ID: <CAJeYpE-+1LYjVpqm+cFbwNNKKNn+Ei8NRT397vby=bZmayGSqQ@mail.gmail.com>

How about this:

plot(0,0,xlab='',ylab='')
mtext(side=1,line=3,text='x axis',col=4)
mtext(side=2,line=3,text='y axis',col=2)

-Dan

On Mon, Jan 11, 2016 at 7:44 AM, Evan Cooch <evan.cooch at gmail.com> wrote:

> Consider a simple plot of X vs Y.  There are elements on the plot that
> represent X, or Y, that are presented in different colours (say, blue
> for X,   red for Y). Rather than use a legend, I would like to have the
> title label for the X-axis be in blue, and the title label for the
> Y-axis be in red.
>
> While it is trivial to change the color of the axis title labels for
> *both* axes at the same time, I haven't figured out how to trick thing
> into generating a blue title label for the X-axis, and a red title label
> for the Y- axis (i.e., different colours on different axes).
>
> I'm sure this is out there on searchable pages, but, I haven't managed
> to stumble across the appropriate search phrase(s).
>
> Pointers to the obvious solution welcomed in advance.
>
>
> Cheers....
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Jan 11 19:01:39 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 11 Jan 2016 10:01:39 -0800
Subject: [R] embedding expression into title in R plot
In-Reply-To: <5693D14C.6050401@gmail.com>
References: <5693D14C.6050401@gmail.com>
Message-ID: <87F2827F-574E-49E2-824F-0752497B9B69@comcast.net>


> On Jan 11, 2016, at 7:59 AM, Evan Cooch <evan.cooch at gmail.com> wrote:
> 
> Suppose I've specified that the xlab for a plot is
> 
> expression(bold(species~(italic(N1))))
> 
> In other words, I want the axis label to be bold, italic 'species (N1)'
> 
> Now, I want the title for the plot to be have this label embedded in the 
> title.
> 
> Say, 'This is the plot for Species (N1)'.
> 
> For a variety of reasons, I've set this up so that the xlab is a global 
> parameter (basically, because the labels are set in a function which 
> when called, generates various plots):
> 
> x_label <<- expression(bold(species~(italic(N1))))

> 
> So, in the title, I've tried
> 
>  title(main=paste("This is the plot for ",x_label,"nullcline", sep=" "));
> 
> but what this does is generate something like
> 
> 'This is the plot for bold(species~(italic(N1)))'
> 
> In other words, it pastes the text of the expression into the title, but 
> not what the expression 'evaluates' to.
> 
> Is there any way around this?

You instead need the `bquote` function. The `paste` function will only confuse things. In this particular instance it is embedding the literal as.character result of 'x_label'-value in a character object rather than in an expression-object. Since you have not offered a full example it remains unclear whether you want the words: "species" or "N1" rather than the values of those names.


> 
> Thanks in advance...
> 
> 
> 	[[alternative HTML version deleted]]

This is a plain text mailing list.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Mon Jan 11 19:08:03 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 11 Jan 2016 10:08:03 -0800
Subject: [R] different coloured axis title labels for different axes
In-Reply-To: <5693CDCC.8010602@gmail.com>
References: <5693CDCC.8010602@gmail.com>
Message-ID: <0D594A73-044D-450F-BE28-253875731F53@comcast.net>


> On Jan 11, 2016, at 7:44 AM, Evan Cooch <evan.cooch at gmail.com> wrote:
> 
> Consider a simple plot of X vs Y.  There are elements on the plot that 
> represent X, or Y, that are presented in different colours (say, blue 
> for X,   red for Y). Rather than use a legend, I would like to have the 
> title label for the X-axis be in blue, and the title label for the 
> Y-axis be in red.
> 
> While it is trivial to change the color of the axis title labels for 
> *both* axes at the same time, I haven't figured out how to trick thing 
> into generating a blue title label for the X-axis, and a red title label 
> for the Y- axis (i.e., different colours on different axes).
> 
> I'm sure this is out there on searchable pages, but, I haven't managed 
> to stumble across the appropriate search phrase(s).

Suppress the 'label' production for both x-axis and y-axis "titles" (by assigning them to "") and then use the `title` function (twice) to separately construct your colored labels. I had difficulty learning that the term "label" in hte plot-function documentation referred only to the single valued axis title and not to the multiple numeric or text axis annotations at the tick marks. The word "lable" is used to refer to both sets of values..
> 
> Pointers to the obvious solution welcomed in advance.
> 
> 
> Cheers....
> 
> 	[[alternative HTML version deleted]]

Plain text.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From wdunlap at tibco.com  Mon Jan 11 19:54:20 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 11 Jan 2016 10:54:20 -0800
Subject: [R] different coloured axis title labels for different axes
In-Reply-To: <5693CDCC.8010602@gmail.com>
References: <5693CDCC.8010602@gmail.com>
Message-ID: <CAF8bMcYfP-ECjgNuBCyoBF_=O4sSgR3TBCSp8pNv0XmwZL+qTg@mail.gmail.com>

The following shows how to get different colors for most features of a
scatterplot:

plot(1:11,log(1:11),ann=FALSE,axes=FALSE,col="pink",pch=16)
box(col="gray")
title(xlab="X Axis Label", col.lab="light blue")
title(ylab="Y Axis Label", col.lab="light green")
axis(side=1, at=c(2,3,5,7,11), lab=as.expression(lapply(1:5,
function(i)bquote(pi[.(i)]))), col.axis="red", col="orange")
axis(side=2, at=log(c(2,3,5,7,11)), lab=as.expression(lapply(1:5,
function(i)bquote(lambda[.(i)]))), col.axis="blue", col="green")
title(main="Main Title", col.main="magenta", sub="(subtitle)",
col.sub="yellow")

See help(par) for details.  The 'cex' and 'font' parameters have same
subtypes as 'col'.




Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jan 11, 2016 at 7:44 AM, Evan Cooch <evan.cooch at gmail.com> wrote:

> Consider a simple plot of X vs Y.  There are elements on the plot that
> represent X, or Y, that are presented in different colours (say, blue
> for X,   red for Y). Rather than use a legend, I would like to have the
> title label for the X-axis be in blue, and the title label for the
> Y-axis be in red.
>
> While it is trivial to change the color of the axis title labels for
> *both* axes at the same time, I haven't figured out how to trick thing
> into generating a blue title label for the X-axis, and a red title label
> for the Y- axis (i.e., different colours on different axes).
>
> I'm sure this is out there on searchable pages, but, I haven't managed
> to stumble across the appropriate search phrase(s).
>
> Pointers to the obvious solution welcomed in advance.
>
>
> Cheers....
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Jan 11 20:08:14 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 11 Jan 2016 11:08:14 -0800
Subject: [R] embedding expression into title in R plot
In-Reply-To: <5693D14C.6050401@gmail.com>
References: <5693D14C.6050401@gmail.com>
Message-ID: <CAF8bMcZCJShU0zAOUPmkKh-Qnvvd_Jvx-ewYU6KsNcCtVY+qFQ@mail.gmail.com>

I tend to use bquote, as in

  x_label <- bquote(bold(species) ~ (italic(N1)))
  plot(1:10,main=bquote("This is the expression for" ~ .(x_label) * "!"))


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jan 11, 2016 at 7:59 AM, Evan Cooch <evan.cooch at gmail.com> wrote:

> Suppose I've specified that the xlab for a plot is
>
> expression(bold(species~(italic(N1))))
>
> In other words, I want the axis label to be bold, italic 'species (N1)'
>
> Now, I want the title for the plot to be have this label embedded in the
> title.
>
> Say, 'This is the plot for Species (N1)'.
>
> For a variety of reasons, I've set this up so that the xlab is a global
> parameter (basically, because the labels are set in a function which
> when called, generates various plots):
>
> x_label <<- expression(bold(species~(italic(N1))))
>
> So, in the title, I've tried
>
>   title(main=paste("This is the plot for ",x_label,"nullcline", sep=" "));
>
> but what this does is generate something like
>
> 'This is the plot for bold(species~(italic(N1)))'
>
> In other words, it pastes the text of the expression into the title, but
> not what the expression 'evaluates' to.
>
> Is there any way around this?
>
> Thanks in advance...
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From leo.guelman at rbc.com  Mon Jan 11 20:24:29 2016
From: leo.guelman at rbc.com (Guelman, Leo)
Date: Mon, 11 Jan 2016 19:24:29 +0000
Subject: [R] Order of factor levels
Message-ID: <22572F76F41BBF45A4F1192762097145601C59F1@SXSCM704.fg.rbc.com>

Dear list,

What is a better way relative to the one below to keep the order of factor levels created from cut()? Notice, I'm simply pasting letters to levels before converting to character so to keep the desired order of levels. This is not very elegant... I'm converting to character so I can call the helper fun with vapply() from the main fun.

Removing this line of code "  levels(xc) <- paste(letters[1:nlevels(xc)], levels(xc), sep=":")" would result in factor levels that are not ordered according to x1.

set.seed(1)
df <- data.frame(x1 = rnorm(1000), x2 = rnorm(1000))

main_fun <- function(data) {
  data.frame(vapply(data, helper_fun, character(nrow(df))))
}

helper_fun <- function(x) {
  xc <-  cut(x, breaks = unique(quantile(x, seq(0, 1, 1/10), na.rm = TRUE)),
             include.lowest = TRUE)
  levels(xc) <- paste(letters[1:nlevels(xc)], levels(xc), sep=":")
  as.character(xc)

}


res <- main_fun(df)
levels(res$x1)
levels(res$x1)
 [1] "a:[-3.01,-1.34]"    "b:(-1.34,-0.882]"   "c:(-0.882,-0.511]"  "d:(-0.511,-0.296]"  "e:(-0.296,-0.0353]"
 [6] "f:(-0.0353,0.245]"  "g:(0.245,0.536]"    "h:(0.536,0.854]"    "i:(0.854,1.32]"     "j:(1.32,3.81]"
>

Thanks
Leo.

_______________________________________________________________________
If you received this email in error, please advise the sender (by return email or otherwise) immediately. You have consented to receive the attached electronically at the above-noted email address; please retain a copy of this confirmation for future reference.  

Si vous recevez ce courriel par erreur, veuillez en aviser l'exp?diteur imm?diatement, par retour de courriel ou par un autre moyen. Vous avez accept? de recevoir le(s) document(s) ci-joint(s) par voie ?lectronique ? l'adresse courriel indiqu?e ci-dessus; veuillez conserver une copie de cette confirmation pour les fins de reference future.

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Jan 11 20:34:36 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 11 Jan 2016 11:34:36 -0800
Subject: [R] Order of factor levels
In-Reply-To: <22572F76F41BBF45A4F1192762097145601C59F1@SXSCM704.fg.rbc.com>
References: <22572F76F41BBF45A4F1192762097145601C59F1@SXSCM704.fg.rbc.com>
Message-ID: <CAF8bMcbTM_ExohPkYwos7rsLGLR-t6ngFOtpcRw2Hj3CkKcrTw@mail.gmail.com>

Don't use vapply() here - use lapply() instead and then leave cut's output
alone.

vapply() will combine its outputs to create a character matrix and
data.frame will pull apart the character matrix into its columns.  Skipping
the matrix intermediary solves
lots of issues.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jan 11, 2016 at 11:24 AM, Guelman, Leo <leo.guelman at rbc.com> wrote:

> Dear list,
>
> What is a better way relative to the one below to keep the order of factor
> levels created from cut()? Notice, I'm simply pasting letters to levels
> before converting to character so to keep the desired order of levels. This
> is not very elegant... I'm converting to character so I can call the helper
> fun with vapply() from the main fun.
>
> Removing this line of code "  levels(xc) <- paste(letters[1:nlevels(xc)],
> levels(xc), sep=":")" would result in factor levels that are not ordered
> according to x1.
>
> set.seed(1)
> df <- data.frame(x1 = rnorm(1000), x2 = rnorm(1000))
>
> main_fun <- function(data) {
>   data.frame(vapply(data, helper_fun, character(nrow(df))))
> }
>
> helper_fun <- function(x) {
>   xc <-  cut(x, breaks = unique(quantile(x, seq(0, 1, 1/10), na.rm =
> TRUE)),
>              include.lowest = TRUE)
>   levels(xc) <- paste(letters[1:nlevels(xc)], levels(xc), sep=":")
>   as.character(xc)
>
> }
>
>
> res <- main_fun(df)
> levels(res$x1)
> levels(res$x1)
>  [1] "a:[-3.01,-1.34]"    "b:(-1.34,-0.882]"   "c:(-0.882,-0.511]"
> "d:(-0.511,-0.296]"  "e:(-0.296,-0.0353]"
>  [6] "f:(-0.0353,0.245]"  "g:(0.245,0.536]"    "h:(0.536,0.854]"
> "i:(0.854,1.32]"     "j:(1.32,3.81]"
> >
>
> Thanks
> Leo.
>
> _______________________________________________________________________
> If you received this email in error, please advise the sender (by return
> email or otherwise) immediately. You have consented to receive the attached
> electronically at the above-noted email address; please retain a copy of
> this confirmation for future reference.
>
> Si vous recevez ce courriel par erreur, veuillez en aviser l'exp?diteur
> imm?diatement, par retour de courriel ou par un autre moyen. Vous avez
> accept? de recevoir le(s) document(s) ci-joint(s) par voie ?lectronique ?
> l'adresse courriel indiqu?e ci-dessus; veuillez conserver une copie de
> cette confirmation pour les fins de reference future.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Jan 11 20:47:46 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 11 Jan 2016 20:47:46 +0100
Subject: [R] Order of factor levels
In-Reply-To: <CAF8bMcbTM_ExohPkYwos7rsLGLR-t6ngFOtpcRw2Hj3CkKcrTw@mail.gmail.com>
References: <22572F76F41BBF45A4F1192762097145601C59F1@SXSCM704.fg.rbc.com>
	<CAF8bMcbTM_ExohPkYwos7rsLGLR-t6ngFOtpcRw2Hj3CkKcrTw@mail.gmail.com>
Message-ID: <CAJuCY5y7Xvey1xfXOE-ENRYPMXvUosEfbgECY1MaNNAc+QBN0w@mail.gmail.com>

Here's a solution with dplyr

my_cut <- function(x){
  breaks <- quantile(x, seq(0, 1, by = 0.1))
  y <- cut(x, breaks = breaks, include.lowest = TRUE)
  levels(y) <- paste(head(letters, length(breaks) - 1), levels(y), sep = ":
")
  return(y)
}

library(dplyr)
mutate_each(df, funs = funs(my_cut))


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-01-11 20:34 GMT+01:00 William Dunlap via R-help <r-help at r-project.org>:

> Don't use vapply() here - use lapply() instead and then leave cut's output
> alone.
>
> vapply() will combine its outputs to create a character matrix and
> data.frame will pull apart the character matrix into its columns.  Skipping
> the matrix intermediary solves
> lots of issues.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Mon, Jan 11, 2016 at 11:24 AM, Guelman, Leo <leo.guelman at rbc.com>
> wrote:
>
> > Dear list,
> >
> > What is a better way relative to the one below to keep the order of
> factor
> > levels created from cut()? Notice, I'm simply pasting letters to levels
> > before converting to character so to keep the desired order of levels.
> This
> > is not very elegant... I'm converting to character so I can call the
> helper
> > fun with vapply() from the main fun.
> >
> > Removing this line of code "  levels(xc) <- paste(letters[1:nlevels(xc)],
> > levels(xc), sep=":")" would result in factor levels that are not ordered
> > according to x1.
> >
> > set.seed(1)
> > df <- data.frame(x1 = rnorm(1000), x2 = rnorm(1000))
> >
> > main_fun <- function(data) {
> >   data.frame(vapply(data, helper_fun, character(nrow(df))))
> > }
> >
> > helper_fun <- function(x) {
> >   xc <-  cut(x, breaks = unique(quantile(x, seq(0, 1, 1/10), na.rm =
> > TRUE)),
> >              include.lowest = TRUE)
> >   levels(xc) <- paste(letters[1:nlevels(xc)], levels(xc), sep=":")
> >   as.character(xc)
> >
> > }
> >
> >
> > res <- main_fun(df)
> > levels(res$x1)
> > levels(res$x1)
> >  [1] "a:[-3.01,-1.34]"    "b:(-1.34,-0.882]"   "c:(-0.882,-0.511]"
> > "d:(-0.511,-0.296]"  "e:(-0.296,-0.0353]"
> >  [6] "f:(-0.0353,0.245]"  "g:(0.245,0.536]"    "h:(0.536,0.854]"
> > "i:(0.854,1.32]"     "j:(1.32,3.81]"
> > >
> >
> > Thanks
> > Leo.
> >
> > _______________________________________________________________________
> > If you received this email in error, please advise the sender (by return
> > email or otherwise) immediately. You have consented to receive the
> attached
> > electronically at the above-noted email address; please retain a copy of
> > this confirmation for future reference.
> >
> > Si vous recevez ce courriel par erreur, veuillez en aviser l'exp?diteur
> > imm?diatement, par retour de courriel ou par un autre moyen. Vous avez
> > accept? de recevoir le(s) document(s) ci-joint(s) par voie ?lectronique ?
> > l'adresse courriel indiqu?e ci-dessus; veuillez conserver une copie de
> > cette confirmation pour les fins de reference future.
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Wittner.Ben at mgh.harvard.edu  Mon Jan 11 23:39:59 2016
From: Wittner.Ben at mgh.harvard.edu (Wittner, Ben, Ph.D.)
Date: Mon, 11 Jan 2016 22:39:59 +0000
Subject: [R] rgl.snapshot only captures a small portion what's visible
 in the RGL device window on CentOS 7
In-Reply-To: <C402B4BC4499874FBA9869DA6E7A13F964B18628@PHSX10MB8.partners.org>
References: <C402B4BC4499874FBA9869DA6E7A13F964B18628@PHSX10MB8.partners.org>
Message-ID: <C402B4BC4499874FBA9869DA6E7A13F964B1A044@PHSX10MB8.partners.org>

The problem discussed below was fixed on my computer with much help from Tom Callaway, of RedHat.
My computer has an NVIDIA graphics card and CentOS 7 comes with an open-source driver for NVIDIA graphic cards called nouveau. When I replaced the nouveau driver with a driver from NVIDIA, the problem went away.

To see whether your machine has an NVIDIA graphics card, execute the command
lspci -v
and search in the output for VGA. The top of that block of output will tell you what graphics card you have and the last line of the block will tell you what driver is in use.
In my case the top of that block of output was the following:

07:00.0 VGA compatible controller: NVIDIA Corporation G84GL [Quadro FX 370] (rev a1)

So I knew I had an NVIDIA graphics card and that the model was Quadro FX 370, which was important when trying to determine which driver to download from http://www.nvidia.com/object/unix.html .

After downloading the driver, I was not able to get it installed just using the instructions from the NVIDIA download page. The key for me was to follow the instructions in http://www.dedoimedo.com/computers/centos-7-nvidia.html .

Good luck!

-Ben

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wittner, Ben, Ph.D.
Sent: Friday, January 08, 2016 4:13 PM
To: r-help at r-project.org
Subject: [R] rgl.snapshot only captures a small portion what's visible in the RGL device window on CentOS 7

Hello,

As an example, I ran the following code:

library("rgl")                                                                  
example(plot3d)                                                                 
rgl.snapshot("test.png")                                                        

The full plot is visible in the window titled RGL device 1 [Focus], but only a small portion of the upper left part of the plot is visible in test.png (see attached test.png, if the list server attaches it. Otherwise, email me if you like and I'll send it directly to you.)

The output of sessionInfo() is as follows:

R version 3.2.3 (2015-12-10)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: CentOS Linux 7 (Core)

locale:
 [1] LC_CTYPE=en_US.utf8       LC_NUMERIC=C             
 [3] LC_TIME=en_US.utf8        LC_COLLATE=en_US.utf8    
 [5] LC_MONETARY=en_US.utf8    LC_MESSAGES=en_US.utf8   
 [7] LC_PAPER=en_US.utf8       LC_NAME=C                
 [9] LC_ADDRESS=C              LC_TELEPHONE=C           
[11] LC_MEASUREMENT=en_US.utf8 LC_IDENTIFICATION=C      

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] rgl_0.95.1441

loaded via a namespace (and not attached):
[1] tools_3.2.3

In addition to the CentOS 7 machine, I have a CentOS 5.3 machine, which has R 3.0.2 and rgl 0.93.996. I tried the code above on it and it captured the full window in the output of rgl.snapshot (i.e., it worked properly).

To see whether the difference could be attributed to the CentOS version or the R/rgl version, I put R 3.0.2 with rgl 0.93.996 on the CentOS 7 machine and ran the code above. As with the earlier CentOS 7 run, only a small portion of the plot was visible in the output of rgl.snapshot. So it seems the difference is due to a difference in the CentOS versions and not the R/rgl versions.

Thanks in advance for any help.

-Ben



The information in this e-mail is intended only for the ...{{dropped:11}}


From murdoch.duncan at gmail.com  Tue Jan 12 00:31:21 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 11 Jan 2016 18:31:21 -0500
Subject: [R] rgl.snapshot only captures a small portion what's visible
 in the RGL device window on CentOS 7
In-Reply-To: <C402B4BC4499874FBA9869DA6E7A13F964B1A044@PHSX10MB8.partners.org>
References: <C402B4BC4499874FBA9869DA6E7A13F964B18628@PHSX10MB8.partners.org>
	<C402B4BC4499874FBA9869DA6E7A13F964B1A044@PHSX10MB8.partners.org>
Message-ID: <56943B49.4000907@gmail.com>

On 11/01/2016 5:39 PM, Wittner, Ben, Ph.D. wrote:
> The problem discussed below was fixed on my computer with much help from Tom Callaway, of RedHat.
> My computer has an NVIDIA graphics card and CentOS 7 comes with an open-source driver for NVIDIA graphic cards called nouveau. When I replaced the nouveau driver with a driver from NVIDIA, the problem went away.
>
> To see whether your machine has an NVIDIA graphics card, execute the command
> lspci -v
> and search in the output for VGA. The top of that block of output will tell you what graphics card you have and the last line of the block will tell you what driver is in use.
> In my case the top of that block of output was the following:
>
> 07:00.0 VGA compatible controller: NVIDIA Corporation G84GL [Quadro FX 370] (rev a1)
>
> So I knew I had an NVIDIA graphics card and that the model was Quadro FX 370, which was important when trying to determine which driver to download from http://www.nvidia.com/object/unix.html .
>
> After downloading the driver, I was not able to get it installed just using the instructions from the NVIDIA download page. The key for me was to follow the instructions in http://www.dedoimedo.com/computers/centos-7-nvidia.html .

Thanks for following up on this.  If I get any other similar reports, 
I'll be able to point them to your message.

Duncan Murdoch

> Good luck!
>
> -Ben
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wittner, Ben, Ph.D.
> Sent: Friday, January 08, 2016 4:13 PM
> To: r-help at r-project.org
> Subject: [R] rgl.snapshot only captures a small portion what's visible in the RGL device window on CentOS 7
>
> Hello,
>
> As an example, I ran the following code:
>
> library("rgl")
> example(plot3d)
> rgl.snapshot("test.png")
>
> The full plot is visible in the window titled RGL device 1 [Focus], but only a small portion of the upper left part of the plot is visible in test.png (see attached test.png, if the list server attaches it. Otherwise, email me if you like and I'll send it directly to you.)
>
> The output of sessionInfo() is as follows:
>
> R version 3.2.3 (2015-12-10)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: CentOS Linux 7 (Core)
>
> locale:
>   [1] LC_CTYPE=en_US.utf8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.utf8        LC_COLLATE=en_US.utf8
>   [5] LC_MONETARY=en_US.utf8    LC_MESSAGES=en_US.utf8
>   [7] LC_PAPER=en_US.utf8       LC_NAME=C
>   [9] LC_ADDRESS=C              LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.utf8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rgl_0.95.1441
>
> loaded via a namespace (and not attached):
> [1] tools_3.2.3
>
> In addition to the CentOS 7 machine, I have a CentOS 5.3 machine, which has R 3.0.2 and rgl 0.93.996. I tried the code above on it and it captured the full window in the output of rgl.snapshot (i.e., it worked properly).
>
> To see whether the difference could be attributed to the CentOS version or the R/rgl version, I put R 3.0.2 with rgl 0.93.996 on the CentOS 7 machine and ran the code above. As with the earlier CentOS 7 run, only a small portion of the plot was visible in the output of rgl.snapshot. So it seems the difference is due to a difference in the CentOS versions and not the R/rgl versions.
>
> Thanks in advance for any help.
>
> -Ben
>
>
>
> The information in this e-mail is intended only for the ...{{dropped:11}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From veroandreo at gmail.com  Tue Jan 12 01:23:10 2016
From: veroandreo at gmail.com (Veronica Andreo)
Date: Mon, 11 Jan 2016 21:23:10 -0300
Subject: [R] multiple model selection for vglm models
Message-ID: <CAAMki4G6VjyejQXaYGU2zGNmkmd3UyZuRD8A9p+bvLKKgvDV=Q@mail.gmail.com>

Hi list,

I'm working with vglm (family tobit) models and I need to perform multiple
model selection. Something similar to step or stepAIC, is there anything
alike?

I found that neither step nor stepAIC work for objects of class vglm. Has
anybody worked with them and can give a hint on how to select among vglm
models?

Thanks a lot in advance,
Vero

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Tue Jan 12 02:52:49 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 11 Jan 2016 17:52:49 -0800
Subject: [R] Order of factor levels
In-Reply-To: <CAF8bMcbTM_ExohPkYwos7rsLGLR-t6ngFOtpcRw2Hj3CkKcrTw@mail.gmail.com>
References: <22572F76F41BBF45A4F1192762097145601C59F1@SXSCM704.fg.rbc.com>
	<CAF8bMcbTM_ExohPkYwos7rsLGLR-t6ngFOtpcRw2Hj3CkKcrTw@mail.gmail.com>
Message-ID: <CAF8bMcavf5L3a28sY0VLjUnGoPoD7SQ6_3bgF2E6hQ8UTxqBWA@mail.gmail.com>

I left out the example:

> set.seed(1)
> df <- data.frame(x1 = rpois(1000,4), x2 = rpois(1000,8))
> helper_fun <- function(x) {
+     cut(x, breaks = unique(quantile(x, seq(0, 1, 1/10), na.rm = TRUE)),
+              include.lowest = TRUE)
+ }
> df2 <- data.frame(lapply(df, helper_fun))
> lapply(df2, levels)
$x1
[1] "[0,2]"  "(2,3]"  "(3,4]"  "(4,5]"  "(5,6]"  "(6,7]"  "(7,14]"

$x2
[1] "[1,4]"   "(4,5]"   "(5,6]"   "(6,7]"   "(7,8]"   "(8,9]"   "(9,10]"
[8] "(10,12]" "(12,18]"


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jan 11, 2016 at 11:34 AM, William Dunlap <wdunlap at tibco.com> wrote:

> Don't use vapply() here - use lapply() instead and then leave cut's output
> alone.
>
> vapply() will combine its outputs to create a character matrix and
> data.frame will pull apart the character matrix into its columns.  Skipping
> the matrix intermediary solves
> lots of issues.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Mon, Jan 11, 2016 at 11:24 AM, Guelman, Leo <leo.guelman at rbc.com>
> wrote:
>
>> Dear list,
>>
>> What is a better way relative to the one below to keep the order of
>> factor levels created from cut()? Notice, I'm simply pasting letters to
>> levels before converting to character so to keep the desired order of
>> levels. This is not very elegant... I'm converting to character so I can
>> call the helper fun with vapply() from the main fun.
>>
>> Removing this line of code "  levels(xc) <- paste(letters[1:nlevels(xc)],
>> levels(xc), sep=":")" would result in factor levels that are not ordered
>> according to x1.
>>
>> set.seed(1)
>> df <- data.frame(x1 = rnorm(1000), x2 = rnorm(1000))
>>
>> main_fun <- function(data) {
>>   data.frame(vapply(data, helper_fun, character(nrow(df))))
>> }
>>
>> helper_fun <- function(x) {
>>   xc <-  cut(x, breaks = unique(quantile(x, seq(0, 1, 1/10), na.rm =
>> TRUE)),
>>              include.lowest = TRUE)
>>   levels(xc) <- paste(letters[1:nlevels(xc)], levels(xc), sep=":")
>>   as.character(xc)
>>
>> }
>>
>>
>> res <- main_fun(df)
>> levels(res$x1)
>> levels(res$x1)
>>  [1] "a:[-3.01,-1.34]"    "b:(-1.34,-0.882]"   "c:(-0.882,-0.511]"
>> "d:(-0.511,-0.296]"  "e:(-0.296,-0.0353]"
>>  [6] "f:(-0.0353,0.245]"  "g:(0.245,0.536]"    "h:(0.536,0.854]"
>> "i:(0.854,1.32]"     "j:(1.32,3.81]"
>> >
>>
>> Thanks
>> Leo.
>>
>> _______________________________________________________________________
>> If you received this email in error, please advise the sender (by return
>> email or otherwise) immediately. You have consented to receive the attached
>> electronically at the above-noted email address; please retain a copy of
>> this confirmation for future reference.
>>
>> Si vous recevez ce courriel par erreur, veuillez en aviser l'exp?diteur
>> imm?diatement, par retour de courriel ou par un autre moyen. Vous avez
>> accept? de recevoir le(s) document(s) ci-joint(s) par voie ?lectronique ?
>> l'adresse courriel indiqu?e ci-dessus; veuillez conserver une copie de
>> cette confirmation pour les fins de reference future.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From evan.cooch at gmail.com  Mon Jan 11 19:52:04 2016
From: evan.cooch at gmail.com (Evan Cooch)
Date: Mon, 11 Jan 2016 13:52:04 -0500
Subject: [R] embedding expression into title in R plot
In-Reply-To: <87F2827F-574E-49E2-824F-0752497B9B69@comcast.net>
References: <5693D14C.6050401@gmail.com>
	<87F2827F-574E-49E2-824F-0752497B9B69@comcast.net>
Message-ID: <5693F9D4.4020409@gmail.com>

David --

On 1/11/2016 1:01 PM, David Winsemius wrote:
>
>> On Jan 11, 2016, at 7:59 AM, Evan Cooch <evan.cooch at gmail.com> wrote:
>>
>> Suppose I've specified that the xlab for a plot is
>>
>> expression(bold(species~(italic(N1))))
>>
>> In other words, I want the axis label to be bold, italic 'species (N1)'
>>
>> Now, I want the title for the plot to be have this label embedded in the
>> title.
>>
>> Say, 'This is the plot for Species (N1)'.
>>
>> For a variety of reasons, I've set this up so that the xlab is a global
>> parameter (basically, because the labels are set in a function which
>> when called, generates various plots):
>>
>> x_label <<- expression(bold(species~(italic(N1))))
>
>>
>> So, in the title, I've tried
>>
>>   title(main=paste("This is the plot for ",x_label,"nullcline", sep=" "));
>>
>> but what this does is generate something like
>>
>> 'This is the plot for bold(species~(italic(N1)))'
>>
>> In other words, it pastes the text of the expression into the title, but
>> not what the expression 'evaluates' to.
>>
>> Is there any way around this?
>
> You instead need the `bquote` function. The `paste` function will only confuse things. In this particular instance it is embedding the literal as.character result of 'x_label'-value in a character object rather than in an expression-object. Since you have not offered a full example it remains unclear whether you want the words: "species" or "N1" rather than the values of those names.
>

Thanks. I had wondered if bquote was the solution, but wasn't sure. Have 
followed your suggestion, but for the moment, am having problems getting 
it to work. I haven't figured out how to get bquote to actually evaulate 
the expression.

>
>>
>> Thanks in advance...
>>
>>
>> 	[[alternative HTML version deleted]]
>
> This is a plain text mailing list.
>>

Indeed -- forgot to flip the switch on my email client, which defaults 
to sending both plain text and HTML email.

>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>


From evan.cooch at gmail.com  Mon Jan 11 20:05:34 2016
From: evan.cooch at gmail.com (Evan Cooch)
Date: Mon, 11 Jan 2016 14:05:34 -0500
Subject: [R] different coloured axis title labels for different axes
In-Reply-To: <CAF8bMcYfP-ECjgNuBCyoBF_=O4sSgR3TBCSp8pNv0XmwZL+qTg@mail.gmail.com>
References: <5693CDCC.8010602@gmail.com>
	<CAF8bMcYfP-ECjgNuBCyoBF_=O4sSgR3TBCSp8pNv0XmwZL+qTg@mail.gmail.com>
Message-ID: <5693FCFE.5060902@gmail.com>



On 1/11/2016 1:54 PM, William Dunlap wrote:
> The following shows how to get different colors for most features of a
> scatterplot:
>
> plot(1:11,log(1:11),ann=FALSE,axes=FALSE,col="pink",pch=16)
> box(col="gray")
> title(xlab="X Axis Label", col.lab="light blue")
> title(ylab="Y Axis Label", col.lab="light green")
> axis(side=1, at=c(2,3,5,7,11), lab=as.expression(lapply(1:5,
> function(i)bquote(pi[.(i)]))), col.axis="red", col="orange")
> axis(side=2, at=log(c(2,3,5,7,11)), lab=as.expression(lapply(1:5,
> function(i)bquote(lambda[.(i)]))), col.axis="blue", col="green")
> title(main="Main Title", col.main="magenta", sub="(subtitle)",
> col.sub="yellow")
>
> See help(par) for details.  The 'cex' and 'font' parameters have same
> subtypes as 'col'.
>
>
>


Thanks -- very helpful.


From evan.cooch at gmail.com  Mon Jan 11 20:20:07 2016
From: evan.cooch at gmail.com (Evan Cooch)
Date: Mon, 11 Jan 2016 14:20:07 -0500
Subject: [R] embedding expression into title in R plot
In-Reply-To: <CAF8bMcZCJShU0zAOUPmkKh-Qnvvd_Jvx-ewYU6KsNcCtVY+qFQ@mail.gmail.com>
References: <5693D14C.6050401@gmail.com>
	<CAF8bMcZCJShU0zAOUPmkKh-Qnvvd_Jvx-ewYU6KsNcCtVY+qFQ@mail.gmail.com>
Message-ID: <56940067.1000403@gmail.com>



On 1/11/2016 2:08 PM, William Dunlap wrote:
> I tend to use bquote, as in
>
>    x_label <- bquote(bold(species) ~ (italic(N1)))
>    plot(1:10,main=bquote("This is the expression for" ~ .(x_label) * "!"))
>
>
>

Thanks -- I thought I'd tried something very close to this in my various 
attempts, but it would seem, not close enough.

I'll give your suggestion a try. It isn't a critical feature/need on my 
end (since I can live without elements of the title being bold, or 
italic), but I'm always happier knowing what to do if I *need* do - in 
future.


From hiroysato at gmail.com  Tue Jan 12 03:37:12 2016
From: hiroysato at gmail.com (Hiroyuki Sato)
Date: Tue, 12 Jan 2016 02:37:12 +0000
Subject: [R] [Q] It it possible to create the data frame only non-zero
 data column?
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C50092B7@SRVEXCHMBX.precheza.cz>
References: <CA+Tq-RpThb89BpYofhvAxDAZA_t+geEMxGyye4ZuHngfVHbBwg@mail.gmail.com>
	<CAKVAULM9n-0Limf1_7U5sS7w-JmW8Cz64PmZquwwppTbY8oT-Q@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C50092B7@SRVEXCHMBX.precheza.cz>
Message-ID: <CA+Tq-RqA4JpzfA7dmy7hexMH=iNt8wKUmf+18Pjbk2g4ptktVw@mail.gmail.com>

Hello Ulrik, Ashis and Petr

Thank you for replying.
I'll use dat[dat==0] <- NA.

Thanks



2016?1?11?(?) 16:17 PIKAL Petr <petr.pikal at precheza.cz>:

> Hi
>
> as.matrix is rather dangerous, it converts all values to lowest mode
> which, if there is text column, is character.
>
> And if I understand correctly original post was about removing all zero
> columns.
>
> Using plain zero comparison can be dangerous due to possibility of
> compensating negative and positive values.
> > dat<-data.frame(a=c(-1,0,1), b=c(0,0,0), c=c(0,0,1))
> > dat
>    a b c
> 1 -1 0 0
> 2  0 0 0
> 3  1 0 1
> > colSums(dat)==0
>     a     b     c
>  TRUE  TRUE FALSE
>
> Remove incorrectly first column too.
>
> I would proceed with NA substitution, which is safer.
>
> > dat[dat==0]<-NA
> > colSums(is.na(dat))==nrow(dat)
>     a     b     c
> FALSE  TRUE FALSE
> >
>
> And even this can be problematic if values in dat are not integers.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ulrik
> > Stervbo
> > Sent: Friday, January 08, 2016 6:06 PM
> > To: Hiroyuki Sato; r-help at r-project.org
> > Subject: Re: [R] [Q] It it possible to create the data frame only non-
> > zero data column?
> >
> > Could you use rowsum and select rows larger then 0?
> >
> > Something like:
> >
> > result[rowsum(as.matrix(result) > 0, ]
> >
> > On Fri, 8 Jan 2016 at 18:00 Hiroyuki Sato <hiroysato at gmail.com> wrote:
> >
> > > Hello all.
> > >
> > > I re-post this question by e-mail.
> > > (I posted via google-group. But It's not posted yet.)
> > >
> > > I'm newbie GNU R.
> > >
> > > I would like to compare two datas.
> > > How to select columns which has non-zero datas?.
> > > It it possible to create the data frame only VAL3(non-zero data)
> > > column with command?
> > >
> > > Formatted sample.
> > > https://gist.github.com/hiroyuki-sato/cb36584f6cd5845b6c3e
> > >
> > > sample1.txt
> > >
> > >   ID,VAL1,VAL2,VAL3
> > >   ID1,0,2,3
> > >   ID2,0,2,3
> > >   ID3,0,2,3
> > >
> > >   real data has 5000 columns.
> > >
> > > sample2.txt
> > >
> > >   ID,VAL1,VAL2,VAL3
> > >   ID1,0,2,3
> > >   ID2,0,2,3
> > >   ID3,0,2,2
> > >
> > >   The difference sample1 and sample2 is ID3/VAL3.
> > >     sample1: 3
> > >     sample2: 2
> > >
> > > R commands.
> > >
> > >   sample1 <- read.table("sample1.txt",header=T,sep=',')
> > >   sample2 <- read.table("sample2.txt",header=T,sep=',')
> > >
> > >   result <- sample1[,2:4] - sample2[,2:4]
> > >   result
> > >     VAL1 VAL2 VAL3
> > >   1    0    0    0
> > >   2    0    0    0
> > >   3    0    0    1
> > >
> > > I would like to create data frame which has non-zero value columns.
> > > Could you tell me how to do it?
> > >
> > > Best regards.
> > >
> > > --
> > > Hiroyuki Sato.
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Tue Jan 12 04:25:55 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 12 Jan 2016 14:25:55 +1100
Subject: [R] OpenAir package help!!
In-Reply-To: <BLU436-SMTP2214BAE622BF3556D90D781ADC90@phx.gbl>
References: <BLU436-SMTP2214BAE622BF3556D90D781ADC90@phx.gbl>
Message-ID: <CA+8X3fX+CKFBWX9rZjT_TcZ6MesiJs4ygjajZqbxyAGuJvCXEg@mail.gmail.com>

Hi Diana,
As far as I can see, polarPlot (openair) does not specify the colors for
the labels. One possibility is to redefine the default foreground color and
see what happens:

par(fg="red")

but this will almost certainly change other elements in the plot.
Unfortunately the panel.levelplot  function in lattice which is called
doesn't seem to have an argument for label colors either. I cannot even try
to persuade you to use the polar.plot function in the plotrix package as
the radial.grid function does not have a label color argument. If you are
really desperate for this, it can probably be programmed...

Jim


On Mon, Jan 11, 2016 at 11:28 PM, DIANA MARGARITA <d_margarita_p at hotmail.com
> wrote:

> Dear all,
>
> I just wondered if anyone could help me on how to change the color of
> labels of a polar coordinate in a polar plot using the openair package of R.
>
> Kindest regards,
>
> Diana
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Tue Jan 12 17:08:46 2016
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 12 Jan 2016 16:08:46 +0000
Subject: [R] cross correlation of filtered Time series
Message-ID: <248E6FA047A8C746BA491485764190F537163C74@ESESSMB207.ericsson.se>

Hello,

I think that the package "seewave" may help you. See corenv() function.

https://cran.r-project.org/web/packages/seewave/seewave.pdf


library(seewave)
sst.ts <- ts(dc$sst, frequency=60)
t2m.ts <- ts(dc$t2m, frequency=60)
corenv(sst.ts, t2m.ts)
corenv.res <- corenv(sst.ts, t2m.ts, plot=FALSE)
corenv.res

Best,

--
GG




	[[alternative HTML version deleted]]


From firoozi_maryam6858 at yahoo.com  Tue Jan 12 06:53:06 2016
From: firoozi_maryam6858 at yahoo.com (maryam firoozi)
Date: Tue, 12 Jan 2016 09:23:06 +0330
Subject: [R] for loop
Message-ID: <E9304ED1-A010-45F5-891C-66248E186622@yahoo.com>


Dear mr/madam
I want to mak a matrix with 10 row and 3 column . this matrix is pedigree. my input
sire<- c(1,2,3,4,5)
count<- 0
sire<- cbind(sire,count)
dam<- c(1,2,3,4,5,6,7,8,9,10)
ped<-mstrix(NA,nrow=10,ncol=3)
for(i in 1:10){
Sire<- sample(sire[,1],1)
a<- which(sire[,1]==Sire)
if(a){sire[a,2]<-sire[a,2]+1}

Dam<- sample(dam,1)
ped[i,1]<- 1:10
ped[i,2]<- Sire
ped[i,3]<- Dam}
i cant write a code that each sire use only twic not more in ped and Dam use only once.
can you help me?
sincerely
firoozi
Sent from my iPhone

From lorenzo.isella at gmail.com  Tue Jan 12 17:16:16 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Tue, 12 Jan 2016 17:16:16 +0100
Subject: [R] Forecasting with Timeseries with Different Frequency
Message-ID: <20160112161616.GB11070@localhost.localdomain>

Dear All,
Suppose you have some time series, e.g. monthly data about
profits in a company.
I am trying to develop a model to predict what the profit will be the
next month or two.
Other useful data is for sure the number of employes in the company,
the volume of product purchases etc...but they are available at best
on a quarterly basis.
I am sure I am not the first one that has this problem: I would like
*not* to neglect this auxiliary info, but I am sure about how to
proceed with monthly and quarterly data.
Any suggestion is appreciated.

Lorenzo


From lists at dewey.myzen.co.uk  Tue Jan 12 17:22:31 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 12 Jan 2016 16:22:31 +0000
Subject: [R] for loop
In-Reply-To: <E9304ED1-A010-45F5-891C-66248E186622@yahoo.com>
References: <E9304ED1-A010-45F5-891C-66248E186622@yahoo.com>
Message-ID: <56952847.2070406@dewey.myzen.co.uk>

Dear Maryam

sample(dam) would give you a random permutation of dams
sample(c(sire, sire)) would give you a random permutation of sires, each 
twice.

Does that help?

On 12/01/2016 05:53, maryam firoozi via R-help wrote:
>
> Dear mr/madam
> I want to mak a matrix with 10 row and 3 column . this matrix is pedigree. my input
> sire<- c(1,2,3,4,5)
> count<- 0
> sire<- cbind(sire,count)
> dam<- c(1,2,3,4,5,6,7,8,9,10)
> ped<-mstrix(NA,nrow=10,ncol=3)
> for(i in 1:10){
> Sire<- sample(sire[,1],1)
> a<- which(sire[,1]==Sire)
> if(a){sire[a,2]<-sire[a,2]+1}
>
> Dam<- sample(dam,1)
> ped[i,1]<- 1:10
> ped[i,2]<- Sire
> ped[i,3]<- Dam}
> i cant write a code that each sire use only twic not more in ped and Dam use only once.
> can you help me?
> sincerely
> firoozi
> Sent from my iPhone
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From lists at dewey.myzen.co.uk  Tue Jan 12 18:02:53 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 12 Jan 2016 17:02:53 +0000
Subject: [R] for loop
In-Reply-To: <0C568710-2EE4-42E3-9F72-741A0EC75F9D@yahoo.com>
References: <E9304ED1-A010-45F5-891C-66248E186622@yahoo.com>
	<56952847.2070406@dewey.myzen.co.uk>
	<0C568710-2EE4-42E3-9F72-741A0EC75F9D@yahoo.com>
Message-ID: <569531BD.9030404@dewey.myzen.co.uk>

Dear Maryam

Please keep the list cc'ed in as others will have better answers than me.

If dam has 700 members then sample (dam) gives you a random permutation 
of dams, each once.

I did not understand the second part as i do not think you can have 30 
sires each occurring 20 times. Did you mean 35 sires? You can follow the 
same procedure as I suggested to do that.

I think you perhaps need to re-read some introductory material as you 
are struggling over some fairly basic concepts here.

Michael.

On 12/01/2016 16:54, MARYAM wrote:
> Dear Michael,
> thanks a lot for your answering. I have question if i have 700 dam and 30 sire. how can i write my ped thad dam use only once and sire use 20 time or 1:20 dam with sire 1 then 21:40 dam with sire 2 and ?
>
>
> On Dey 22, 1394 AP, at 19:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
>> Dear Maryam
>>
>> sample(dam) would give you a random permutation of dams
>> sample(c(sire, sire)) would give you a random permutation of sires, each twice.
>>
>> Does that help?
>>
>> On 12/01/2016 05:53, maryam firoozi via R-help wrote:
>>>
>>> Dear mr/madam
>>> I want to mak a matrix with 10 row and 3 column . this matrix is pedigree. my input
>>> sire<- c(1,2,3,4,5)
>>> count<- 0
>>> sire<- cbind(sire,count)
>>> dam<- c(1,2,3,4,5,6,7,8,9,10)
>>> ped<-mstrix(NA,nrow=10,ncol=3)
>>> for(i in 1:10){
>>> Sire<- sample(sire[,1],1)
>>> a<- which(sire[,1]==Sire)
>>> if(a){sire[a,2]<-sire[a,2]+1}
>>>
>>> Dam<- sample(dam,1)
>>> ped[i,1]<- 1:10
>>> ped[i,2]<- Sire
>>> ped[i,3]<- Dam}
>>> i cant write a code that each sire use only twic not more in ped and Dam use only once.
>>> can you help me?
>>> sincerely
>>> firoozi
>>> Sent from my iPhone
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From jenny at stat.ubc.ca  Tue Jan 12 18:15:14 2016
From: jenny at stat.ubc.ca (Jenny Bryan)
Date: Tue, 12 Jan 2016 09:15:14 -0800
Subject: [R] printing a data.frame that contains a list-column of S4 objects
Message-ID: <0D60E6F4-3F9E-4268-A9E3-F3FDD8AE4FCC@stat.ubc.ca>

Is there a general problem with printing a data.frame when it has a
list-column of S4 objects? Or am I just unlucky in my life choices?

I ran across this with objects from the git2r package but maintainer
Stefan Widgren points out this example below from Matrix as well. I note
that the offending object can be printed if sent through
dplyr::tbl_df(). I accept that that printing doesn't provide much info
on S4 objects. I'd just like those vars to not prevent data.frame-style
inpsection of the entire object.

I asked this on stack overflow, where commenter provided the lead to the
workaround below. Is that the best solution?

library(Matrix)

m <- new("dgCMatrix")
isS4(m)
#> [1] TRUE
df <- data.frame(id = 1:2)
df$matrices <- list(m, m)
df
#> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic
#> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic

## fairly costly workaround
df2 <- df
df2[] <- lapply(df2, as.character)
df2
#>   id                         matrices
#> 1  1 <S4 object of class "dgCMatrix">
#> 2  2 <S4 object of class "dgCMatrix">

## dplyr handles original object better but not as well as workaround
library(dplyr)
## use select to force dplyr to show the tricky column
tbl_df(select(df, matrices))
#> Source: local data frame [2 x 1]
#> 
#>                                                                      matrices
#>                                                                        (list)
#> 1 <S4:dgCMatrix, CsparseMatrix, dsparseMatrix, generalMatrix, dCsparseMatrix,
#> 2 <S4:dgCMatrix, CsparseMatrix, dsparseMatrix, generalMatrix, dCsparseMatrix,

Thanks,
Jenny

Jennifer Bryan
Associate Professor
Department of Statistics and
   the Michael Smith Laboratories
University of British Columbia
Vancouver, BC Canada


From bhh at xs4all.nl  Tue Jan 12 19:03:06 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Tue, 12 Jan 2016 19:03:06 +0100
Subject: [R] Forecasting with Timeseries with Different Frequency
In-Reply-To: <20160112161616.GB11070@localhost.localdomain>
References: <20160112161616.GB11070@localhost.localdomain>
Message-ID: <C15EA91D-5718-486B-9ECF-93CA7EF8554B@xs4all.nl>


Have a look at the midasr package available on CRAN.
It might provide what you need.

Berend

> On 12 Jan 2016, at 17:16, Lorenzo Isella <lorenzo.isella at gmail.com> wrote:
> 
> Dear All,
> Suppose you have some time series, e.g. monthly data about
> profits in a company.
> I am trying to develop a model to predict what the profit will be the
> next month or two.
> Other useful data is for sure the number of employes in the company,
> the volume of product purchases etc...but they are available at best
> on a quarterly basis.
> I am sure I am not the first one that has this problem: I would like
> *not* to neglect this auxiliary info, but I am sure about how to
> proceed with monthly and quarterly data.
> Any suggestion is appreciated.
> 
> Lorenzo
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Tue Jan 12 19:26:06 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 12 Jan 2016 10:26:06 -0800
Subject: [R] Forecasting with Timeseries with Different Frequency
In-Reply-To: <20160112161616.GB11070@localhost.localdomain>
References: <20160112161616.GB11070@localhost.localdomain>
Message-ID: <CAGxFJbRdJCLQ5Pe8BV390h1VmOgdC+S-MERenMLVCqsXFqRxbQ@mail.gmail.com>

A statistics, not an R question, and hence OT here.

However, look at the CRAN Time Series Task View and/or post on a
statistics site like stats.stackexchange.com

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Jan 12, 2016 at 8:16 AM, Lorenzo Isella
<lorenzo.isella at gmail.com> wrote:
> Dear All,
> Suppose you have some time series, e.g. monthly data about
> profits in a company.
> I am trying to develop a model to predict what the profit will be the
> next month or two.
> Other useful data is for sure the number of employes in the company,
> the volume of product purchases etc...but they are available at best
> on a quarterly basis.
> I am sure I am not the first one that has this problem: I would like
> *not* to neglect this auxiliary info, but I am sure about how to
> proceed with monthly and quarterly data.
> Any suggestion is appreciated.
>
> Lorenzo
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bob at rudis.net  Tue Jan 12 19:51:50 2016
From: bob at rudis.net (boB Rudis)
Date: Tue, 12 Jan 2016 13:51:50 -0500
Subject: [R] printing a data.frame that contains a list-column of S4
	objects
In-Reply-To: <0D60E6F4-3F9E-4268-A9E3-F3FDD8AE4FCC@stat.ubc.ca>
References: <0D60E6F4-3F9E-4268-A9E3-F3FDD8AE4FCC@stat.ubc.ca>
Message-ID: <CAJ4QxaPbw_tXfdfmhtAMQPKs8oe5suCBA5SyPzU_dSqxXsFnew@mail.gmail.com>

I wonder if something like:

format.list <- function(x, ...) {
  rep(class(x[[1]]), length(x))
}

would be sufficient? (prbly needs more 'if's though)

On Tue, Jan 12, 2016 at 12:15 PM, Jenny Bryan <jenny at stat.ubc.ca> wrote:
> Is there a general problem with printing a data.frame when it has a
> list-column of S4 objects? Or am I just unlucky in my life choices?
>
> I ran across this with objects from the git2r package but maintainer
> Stefan Widgren points out this example below from Matrix as well. I note
> that the offending object can be printed if sent through
> dplyr::tbl_df(). I accept that that printing doesn't provide much info
> on S4 objects. I'd just like those vars to not prevent data.frame-style
> inpsection of the entire object.
>
> I asked this on stack overflow, where commenter provided the lead to the
> workaround below. Is that the best solution?
>
> library(Matrix)
>
> m <- new("dgCMatrix")
> isS4(m)
> #> [1] TRUE
> df <- data.frame(id = 1:2)
> df$matrices <- list(m, m)
> df
> #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic
> #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic
>
> ## fairly costly workaround
> df2 <- df
> df2[] <- lapply(df2, as.character)
> df2
> #>   id                         matrices
> #> 1  1 <S4 object of class "dgCMatrix">
> #> 2  2 <S4 object of class "dgCMatrix">
>
> ## dplyr handles original object better but not as well as workaround
> library(dplyr)
> ## use select to force dplyr to show the tricky column
> tbl_df(select(df, matrices))
> #> Source: local data frame [2 x 1]
> #>
> #>                                                                      matrices
> #>                                                                        (list)
> #> 1 <S4:dgCMatrix, CsparseMatrix, dsparseMatrix, generalMatrix, dCsparseMatrix,
> #> 2 <S4:dgCMatrix, CsparseMatrix, dsparseMatrix, generalMatrix, dCsparseMatrix,
>
> Thanks,
> Jenny
>
> Jennifer Bryan
> Associate Professor
> Department of Statistics and
>    the Michael Smith Laboratories
> University of British Columbia
> Vancouver, BC Canada
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Tue Jan 12 21:17:44 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 13 Jan 2016 07:17:44 +1100
Subject: [R] for loop
In-Reply-To: <569531BD.9030404@dewey.myzen.co.uk>
References: <E9304ED1-A010-45F5-891C-66248E186622@yahoo.com>
	<56952847.2070406@dewey.myzen.co.uk>
	<0C568710-2EE4-42E3-9F72-741A0EC75F9D@yahoo.com>
	<569531BD.9030404@dewey.myzen.co.uk>
Message-ID: <CA+8X3fW52uG6uj_pJARLD2B=83XUyu95K4CWu6GCwqRjiB74bw@mail.gmail.com>

Hi maryam (firoozi),
Apart from the fact that you are overworking your sires (or the more
realistic scenario of differential mating success) you can achieve the
700:30 ratio in this simple way:

sires<-paste("Sire",1:30,sep="")
dams<-paste("Dam",1:700,sep="")
ped<-data.frame(offspring=1:700,sire=sample(rep(sires,length.out=700),700),dams)

This mates your 30 sires randomly with the 700 dams. As Michael has pointed
out, each sire will get lucky 23.33 times on average. With just another 5
sires, you can get this down to 20. Of course if we want to adopt the
"anything goes" condition of a one in seven chance of parthenogenesis, try
this:

sires<-paste("Sire",1:30,sep="")
dams<-paste("Dam",1:700,sep="")
miracles<-paste("None",1:100,sep="")
ped<-data.frame(offspring=1:700,sire=sample(c(rep(sires,each=20),miracles),700),dams)

Jim



On Wed, Jan 13, 2016 at 4:02 AM, Michael Dewey <lists at dewey.myzen.co.uk>
wrote:

> Dear Maryam
>
> Please keep the list cc'ed in as others will have better answers than me.
>
> If dam has 700 members then sample (dam) gives you a random permutation of
> dams, each once.
>
> I did not understand the second part as i do not think you can have 30
> sires each occurring 20 times. Did you mean 35 sires? You can follow the
> same procedure as I suggested to do that.
>
> I think you perhaps need to re-read some introductory material as you are
> struggling over some fairly basic concepts here.
>
> Michael.
>
> On 12/01/2016 16:54, MARYAM wrote:
>
>> Dear Michael,
>> thanks a lot for your answering. I have question if i have 700 dam and 30
>> sire. how can i write my ped thad dam use only once and sire use 20 time or
>> 1:20 dam with sire 1 then 21:40 dam with sire 2 and ?
>>
>>
>>
>> On Dey 22, 1394 AP, at 19:52, Michael Dewey <lists at dewey.myzen.co.uk>
>> wrote:
>>
>> Dear Maryam
>>>
>>> sample(dam) would give you a random permutation of dams
>>> sample(c(sire, sire)) would give you a random permutation of sires, each
>>> twice.
>>>
>>> Does that help?
>>>
>>> On 12/01/2016 05:53, maryam firoozi via R-help wrote:
>>>
>>>>
>>>> Dear mr/madam
>>>> I want to mak a matrix with 10 row and 3 column . this matrix is
>>>> pedigree. my input
>>>> sire<- c(1,2,3,4,5)
>>>> count<- 0
>>>> sire<- cbind(sire,count)
>>>> dam<- c(1,2,3,4,5,6,7,8,9,10)
>>>> ped<-mstrix(NA,nrow=10,ncol=3)
>>>> for(i in 1:10){
>>>> Sire<- sample(sire[,1],1)
>>>> a<- which(sire[,1]==Sire)
>>>> if(a){sire[a,2]<-sire[a,2]+1}
>>>>
>>>> Dam<- sample(dam,1)
>>>> ped[i,1]<- 1:10
>>>> ped[i,2]<- Sire
>>>> ped[i,3]<- Dam}
>>>> i cant write a code that each sire use only twic not more in ped and
>>>> Dam use only once.
>>>> can you help me?
>>>> sincerely
>>>> firoozi
>>>> Sent from my iPhone
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>> --
>>> Michael
>>> http://www.dewey.myzen.co.uk/home.html
>>>
>>
>>
>>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From johnbeyer at me.com  Tue Jan 12 22:22:27 2016
From: johnbeyer at me.com (John Beyer)
Date: Tue, 12 Jan 2016 16:22:27 -0500
Subject: [R] Running R 3.2.2 on Mac OS X 10.11.3 - not getting proper
 response to ls and dir commands ... launching error scripts. See below:
Message-ID: <78DB1E3C-9EC8-491B-A7B7-19CA7F637776@me.com>

This is what I get when i try to run dir or ls on my R Console:


 getwd()
[1] "/Users/johnbeyer"
> dir
function (path = ".", pattern = NULL, all.files = FALSE, full.names = FALSE, 
    recursive = FALSE, ignore.case = FALSE, include.dirs = FALSE, 
    no.. = FALSE) 
.Internal(list.files(path, pattern, all.files, full.names, recursive, 
    ignore.case, include.dirs, no..))
<bytecode: 0x7f8b48c1d428>
<environment: namespace:base>
> ls
function (name, pos = -1L, envir = as.environment(pos), all.names = FALSE, 
    pattern, sorted = TRUE) 
{
    if (!missing(name)) {
        pos <- tryCatch(name, error = function(e) e)
        if (inherits(pos, "error")) {
            name <- substitute(name)
            if (!is.character(name)) 
                name <- deparse(name)
            warning(gettextf("%s converted to character string", 
                sQuote(name)), domain = NA)
            pos <- name
        }
    }
    all.names <- .Internal(ls(envir, all.names, sorted))
    if (!missing(pattern)) {
        if ((ll <- length(grep("[", pattern, fixed = TRUE))) && 
            ll != length(grep("]", pattern, fixed = TRUE))) {
            if (pattern == "[") {
                pattern <- "\\["
                warning("replaced regular expression pattern '[' by  '\\\\['")
            }
            else if (length(grep("[^\\\\]\\[<-", pattern))) {
                pattern <- sub("\\[<-", "\\\\\\[<-", pattern)
                warning("replaced '[<-' by '\\\\[<-' in regular expression pattern")
            }
        }
        grep(pattern, all.names, value = TRUE)
    }
    else all.names
}
<bytecode: 0x7f8b4a861350>
<environment: namespace:base>


	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Tue Jan 12 23:58:54 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 12 Jan 2016 14:58:54 -0800
Subject: [R] Running R 3.2.2 on Mac OS X 10.11.3 - not getting proper
 response to ls and dir commands ... launching error scripts. See below:
In-Reply-To: <78DB1E3C-9EC8-491B-A7B7-19CA7F637776@me.com>
References: <78DB1E3C-9EC8-491B-A7B7-19CA7F637776@me.com>
Message-ID: <CAF8bMca3FpvV0tG7-e45FxmW7Zi80mdH_zHiRuQSL6sQ6e6X2g@mail.gmail.com>

   > getwd()
   [1] "/Users/johnbeyer"
   > dir
   function (path = ".", pattern = NULL, all.files = FALSE, full.names =
FALSE,
       recursive = FALSE, ignore.case = FALSE, include.dirs = FALSE,

Note what a difference the parentheses after the function name make.  With
parentheses,
often with arguments in the parentheses, you call the function; without
parentheses you
get the function definition,



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Jan 12, 2016 at 1:22 PM, John Beyer <johnbeyer at me.com> wrote:

> This is what I get when i try to run dir or ls on my R Console:
>
>
>  getwd()
> [1] "/Users/johnbeyer"
> > dir
> function (path = ".", pattern = NULL, all.files = FALSE, full.names =
> FALSE,
>     recursive = FALSE, ignore.case = FALSE, include.dirs = FALSE,
>     no.. = FALSE)
> .Internal(list.files(path, pattern, all.files, full.names, recursive,
>     ignore.case, include.dirs, no..))
> <bytecode: 0x7f8b48c1d428>
> <environment: namespace:base>
> > ls
> function (name, pos = -1L, envir = as.environment(pos), all.names = FALSE,
>     pattern, sorted = TRUE)
> {
>     if (!missing(name)) {
>         pos <- tryCatch(name, error = function(e) e)
>         if (inherits(pos, "error")) {
>             name <- substitute(name)
>             if (!is.character(name))
>                 name <- deparse(name)
>             warning(gettextf("%s converted to character string",
>                 sQuote(name)), domain = NA)
>             pos <- name
>         }
>     }
>     all.names <- .Internal(ls(envir, all.names, sorted))
>     if (!missing(pattern)) {
>         if ((ll <- length(grep("[", pattern, fixed = TRUE))) &&
>             ll != length(grep("]", pattern, fixed = TRUE))) {
>             if (pattern == "[") {
>                 pattern <- "\\["
>                 warning("replaced regular expression pattern '[' by
> '\\\\['")
>             }
>             else if (length(grep("[^\\\\]\\[<-", pattern))) {
>                 pattern <- sub("\\[<-", "\\\\\\[<-", pattern)
>                 warning("replaced '[<-' by '\\\\[<-' in regular expression
> pattern")
>             }
>         }
>         grep(pattern, all.names, value = TRUE)
>     }
>     else all.names
> }
> <bytecode: 0x7f8b4a861350>
> <environment: namespace:base>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jan 13 00:00:13 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 12 Jan 2016 15:00:13 -0800
Subject: [R] Running R 3.2.2 on Mac OS X 10.11.3 - not getting proper
	response to ls and dir commands ... launching error scripts.
	See below:
In-Reply-To: <78DB1E3C-9EC8-491B-A7B7-19CA7F637776@me.com>
References: <78DB1E3C-9EC8-491B-A7B7-19CA7F637776@me.com>
Message-ID: <A3D95EFC-CECF-4D89-9AE9-0F2F2B02CA6E@comcast.net>


> On Jan 12, 2016, at 1:22 PM, John Beyer <johnbeyer at me.com> wrote:
> 
> This is what I get when i try to run dir or ls on my R Console:
> 
> 
> getwd()
> [1] "/Users/johnbeyer"
>> dir
> function (path = ".", pattern = NULL, all.files = FALSE, full.names = FALSE, 
>    recursive = FALSE, ignore.case = FALSE, include.dirs = FALSE, 
>    no.. = FALSE) 
> .Internal(list.files(path, pattern, all.files, full.names, recursive, 
>    ignore.case, include.dirs, no..))
> <bytecode: 0x7f8b48c1d428>
> <environment: namespace:base>
>> ls

If you want to use the `ls` function, at a minimum you must enter 4 keystrokes:

ls()

R is a functional language and has a syntax that requires that all functions have a name, opening parenthesis, argument list (possibly empty as in this case) and then closing parenthesis. This should have become apparent as you worked through the material in "Introduction to R".

Just typing a literal function name with no parentheses returns the value of the function name which is code.

-- 
David.


> function (name, pos = -1L, envir = as.environment(pos), all.names = FALSE, 
>    pattern, sorted = TRUE) 
> {
>    if (!missing(name)) {
>        pos <- tryCatch(name, error = function(e) e)
>        if (inherits(pos, "error")) {
>            name <- substitute(name)
>            if (!is.character(name)) 
>                name <- deparse(name)
>            warning(gettextf("%s converted to character string", 
>                sQuote(name)), domain = NA)
>            pos <- name
>        }
>    }
>    all.names <- .Internal(ls(envir, all.names, sorted))
>    if (!missing(pattern)) {
>        if ((ll <- length(grep("[", pattern, fixed = TRUE))) && 
>            ll != length(grep("]", pattern, fixed = TRUE))) {
>            if (pattern == "[") {
>                pattern <- "\\["
>                warning("replaced regular expression pattern '[' by  '\\\\['")
>            }
>            else if (length(grep("[^\\\\]\\[<-", pattern))) {
>                pattern <- sub("\\[<-", "\\\\\\[<-", pattern)
>                warning("replaced '[<-' by '\\\\[<-' in regular expression pattern")
>            }
>        }
>        grep(pattern, all.names, value = TRUE)
>    }
>    else all.names
> }
> <bytecode: 0x7f8b4a861350>
> <environment: namespace:base>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jsorkin at grecc.umaryland.edu  Wed Jan 13 00:10:20 2016
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 12 Jan 2016 18:10:20 -0500
Subject: [R] Trying to use name of difference variable created in a function
 in call to wilcox.test function.
Message-ID: <5695418C020000CB0014650C@smtp.medicine.umaryland.edu>

I am trying to write a function which will allow me to analyze many variables all of which have the same form, Base.stem and Disch.stem, where stem varies from variable to variable, e.g. Base.rolling and Disch.rolling, Base.standing, Disch.standing. 
 
I want to pass a dataframe and the stem of the name of the pre and post intervention variables. In my function I create the full pre and post intervention Base.stem and Disch.stem variables names (by using the paste function, varpre=paste("Base.",stem,sep="") and varpost=paste("Disch.",stem,sep="")). I also create and add to the dataframe the change variable (i.e. post-pre) formed as ch<-paste("Change.",stem,sep="") and compute the change in the variable:    data[,ch]=data[,varpo]-data[,varpre]. 
 
If, for example, I run the function doit(rolling, data), the function creates the full pre-intervention variable name Base.rolling, the post-intervention variable name Disch.rolling, and computes Change.rolling = Disch.rolling-Base.rolling. I want to use Change.rolling in a function, but I don't want to have to expressly specify the name of the outcome variable, i.e.
 
  fitwilcox<-wilcox.test(Change.rolling~RANDOMIZED,data=data,na.action=na.omit)
 
I want to specify the constructed variable, i.e.
 
 fitwilcox<-wilcox.test(ch~RANDOMIZED,data=data,na.action=na.omit)
 
This does not work because ch is not in the dataframe, but Change.rolling is. How can I modify the call to the Wilcox.test function so I can specify the change variable constructor. I tried the following but it does not work:
 
 fitwilcox<-wilcox.test(eval(ch, parent.frame())~RANDOMIZED,data=data,na.action=na.omit) 
 
DATA:
 
> dput(data2)structure(list(Subj = c(115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147),     RANDOMIZED = c(1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2,     1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2),     Base.rolling = c(0, 6, 6, 3, 3, 1, 4, 6, 4, 6, 4, 1, 3, NA,     1, 2, 4, 1, 4, 4, 4, 1, 5, 3, 2, 3, 1, 4, 0, 4, 1, NA, 3),     Disch.rolling = c(6, NA, NA, 6, 6, 3, 6, 6, 6, NA, NA, 6,     1, NA, 6, 6, 6, NA, 6, 6, 6, NA, 4, NA, 6, 6, NA, 6, NA,     6, 6, NA, NA)), .Names = c("Subj", "RANDOMIZED", "Base.rolling", "Disch.rolling"), class = "data.frame", row.names = c(NA, -33L))

 
CODE:

doit <- function(var,data){
  var <-    deparse(substitute(var))
  print(var)
  # Create name for basline variable.
  varpre<-paste("Base.",var,sep="")
  # Create name for post variable.
  varpo<-paste("Disch.",var,sep="")
  # Create a name for the change variable.
  ch<-paste("Change.",var,sep="")
  print(ch)
  # Compute change.
  data[,ch]=data[,varpo]-data[,varpre]
  
  cat("\nData used in the analyses\n")
  print(data[,c("Subj","RANDOMIZED",ch)])
  # This works when I expressly specify the change variable.
  cat("This works, expressly specify change variable\n")
  fitwilcox1<-wilcox.test(Change.rolling~RANDOMIZED,data=data,na.action=na.omit)
  print(fitwilcox1)
  
  # This does not work.
  cat("This does NOT work, try to use created change variable.\n")
  fitwilcox2<-wilcox.test(eval(ch, parent.frame())~RANDOMIZED,data=data,na.action=na.omit)
  print(fitwilcox2)
  # Compute wilcoxon statistic.
}
doit(rolling,data2)
 
 
Thank you,
John
John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From wdunlap at tibco.com  Wed Jan 13 00:27:49 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 12 Jan 2016 15:27:49 -0800
Subject: [R] Trying to use name of difference variable created in a
 function in call to wilcox.test function.
In-Reply-To: <5695418C020000CB0014650C@smtp.medicine.umaryland.edu>
References: <5695418C020000CB0014650C@smtp.medicine.umaryland.edu>
Message-ID: <CAF8bMcb8TovtodU7+0o1d9jMZeSjvk3LYzbhi9ewLkYXXm19gQ@mail.gmail.com>

Is the following function, myFormula(), what you are looking for?

myFormula <- function(stem, env = parent.frame()) {
    eval(bquote(.(as.name(paste0("Disch.",stem))) ~
.(as.name(paste0("Base.",stem))),
list(stem=as.name("rolling"))), envir=env)
}
str(myFormula("myStem"))
#Class "formula" language Disch.rolling ~ Base.rolling
# - attr(*, ".Environment")= <environment: R_GlobalEnv>


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Jan 12, 2016 at 3:10 PM, John Sorkin <jsorkin at grecc.umaryland.edu>
wrote:

> I am trying to write a function which will allow me to analyze many
> variables all of which have the same form, Base.stem and Disch.stem, where
> stem varies from variable to variable, e.g. Base.rolling and Disch.rolling,
> Base.standing, Disch.standing.
>
> I want to pass a dataframe and the stem of the name of the pre and post
> intervention variables. In my function I create the full pre and post
> intervention Base.stem and Disch.stem variables names (by using the paste
> function, varpre=paste("Base.",stem,sep="") and
> varpost=paste("Disch.",stem,sep="")). I also create and add to the
> dataframe the change variable (i.e. post-pre) formed as
> ch<-paste("Change.",stem,sep="") and compute the change in the variable:
> data[,ch]=data[,varpo]-data[,varpre].
>
> If, for example, I run the function doit(rolling, data), the function
> creates the full pre-intervention variable name Base.rolling, the
> post-intervention variable name Disch.rolling, and computes Change.rolling
> = Disch.rolling-Base.rolling. I want to use Change.rolling in a function,
> but I don't want to have to expressly specify the name of the outcome
> variable, i.e.
>
>
> fitwilcox<-wilcox.test(Change.rolling~RANDOMIZED,data=data,na.action=na.omit)
>
> I want to specify the constructed variable, i.e.
>
>  fitwilcox<-wilcox.test(ch~RANDOMIZED,data=data,na.action=na.omit)
>
> This does not work because ch is not in the dataframe, but Change.rolling
> is. How can I modify the call to the Wilcox.test function so I can specify
> the change variable constructor. I tried the following but it does not work:
>
>  fitwilcox<-wilcox.test(eval(ch,
> parent.frame())~RANDOMIZED,data=data,na.action=na.omit)
>
> DATA:
>
> > dput(data2)structure(list(Subj = c(115, 116, 117, 118, 119, 120, 121,
> 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136,
> 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147),     RANDOMIZED =
> c(1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2,     1, 1, 2, 2, 1, 2, 1, 1, 2,
> 2, 1, 2, 1, 2, 2, 2, 2, 2, 2),     Base.rolling = c(0, 6, 6, 3, 3, 1, 4, 6,
> 4, 6, 4, 1, 3, NA,     1, 2, 4, 1, 4, 4, 4, 1, 5, 3, 2, 3, 1, 4, 0, 4, 1,
> NA, 3),     Disch.rolling = c(6, NA, NA, 6, 6, 3, 6, 6, 6, NA, NA, 6,
>  1, NA, 6, 6, 6, NA, 6, 6, 6, NA, 4, NA, 6, 6, NA, 6, NA,     6, 6, NA,
> NA)), .Names = c("Subj", "RANDOMIZED", "Base.rolling", "Disch.rolling"),
> class = "data.frame", row.names = c(NA, -33L))
>
>
> CODE:
>
> doit <- function(var,data){
>   var <-    deparse(substitute(var))
>   print(var)
>   # Create name for basline variable.
>   varpre<-paste("Base.",var,sep="")
>   # Create name for post variable.
>   varpo<-paste("Disch.",var,sep="")
>   # Create a name for the change variable.
>   ch<-paste("Change.",var,sep="")
>   print(ch)
>   # Compute change.
>   data[,ch]=data[,varpo]-data[,varpre]
>
>   cat("\nData used in the analyses\n")
>   print(data[,c("Subj","RANDOMIZED",ch)])
>   # This works when I expressly specify the change variable.
>   cat("This works, expressly specify change variable\n")
>
> fitwilcox1<-wilcox.test(Change.rolling~RANDOMIZED,data=data,na.action=na.omit)
>   print(fitwilcox1)
>
>   # This does not work.
>   cat("This does NOT work, try to use created change variable.\n")
>   fitwilcox2<-wilcox.test(eval(ch,
> parent.frame())~RANDOMIZED,data=data,na.action=na.omit)
>   print(fitwilcox2)
>   # Compute wilcoxon statistic.
> }
> doit(rolling,data2)
>
>
> Thank you,
> John
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:16}}


From bgunter.4567 at gmail.com  Wed Jan 13 01:31:26 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 12 Jan 2016 16:31:26 -0800
Subject: [R] Trying to use name of difference variable created in a
 function in call to wilcox.test function.
In-Reply-To: <5695418C020000CB0014650C@smtp.medicine.umaryland.edu>
References: <5695418C020000CB0014650C@smtp.medicine.umaryland.edu>
Message-ID: <CAGxFJbR-kVyqcDa2pan0BE7xWJKqtYwULt7cn2gtWDqDN7gZcA@mail.gmail.com>

I think you're looking for ?bquote .

Something like this should give you the idea:

nm <- "yvar"
bquote( .(y)~x, list(y = as.symbol(nm )))

The key is distinguishing between the character string "yvar" and the
(language) object, as.symbol("yvar").

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Jan 12, 2016 at 3:10 PM, John Sorkin
<jsorkin at grecc.umaryland.edu> wrote:
> I am trying to write a function which will allow me to analyze many variables all of which have the same form, Base.stem and Disch.stem, where stem varies from variable to variable, e.g. Base.rolling and Disch.rolling, Base.standing, Disch.standing.
>
> I want to pass a dataframe and the stem of the name of the pre and post intervention variables. In my function I create the full pre and post intervention Base.stem and Disch.stem variables names (by using the paste function, varpre=paste("Base.",stem,sep="") and varpost=paste("Disch.",stem,sep="")). I also create and add to the dataframe the change variable (i.e. post-pre) formed as ch<-paste("Change.",stem,sep="") and compute the change in the variable:    data[,ch]=data[,varpo]-data[,varpre].
>
> If, for example, I run the function doit(rolling, data), the function creates the full pre-intervention variable name Base.rolling, the post-intervention variable name Disch.rolling, and computes Change.rolling = Disch.rolling-Base.rolling. I want to use Change.rolling in a function, but I don't want to have to expressly specify the name of the outcome variable, i.e.
>
>   fitwilcox<-wilcox.test(Change.rolling~RANDOMIZED,data=data,na.action=na.omit)
>
> I want to specify the constructed variable, i.e.
>
>  fitwilcox<-wilcox.test(ch~RANDOMIZED,data=data,na.action=na.omit)
>
> This does not work because ch is not in the dataframe, but Change.rolling is. How can I modify the call to the Wilcox.test function so I can specify the change variable constructor. I tried the following but it does not work:
>
>  fitwilcox<-wilcox.test(eval(ch, parent.frame())~RANDOMIZED,data=data,na.action=na.omit)
>
> DATA:
>
>> dput(data2)structure(list(Subj = c(115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147),     RANDOMIZED = c(1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2,     1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2),     Base.rolling = c(0, 6, 6, 3, 3, 1, 4, 6, 4, 6, 4, 1, 3, NA,     1, 2, 4, 1, 4, 4, 4, 1, 5, 3, 2, 3, 1, 4, 0, 4, 1, NA, 3),     Disch.rolling = c(6, NA, NA, 6, 6, 3, 6, 6, 6, NA, NA, 6,     1, NA, 6, 6, 6, NA, 6, 6, 6, NA, 4, NA, 6, 6, NA, 6, NA,     6, 6, NA, NA)), .Names = c("Subj", "RANDOMIZED", "Base.rolling", "Disch.rolling"), class = "data.frame", row.names = c(NA, -33L))
>
>
> CODE:
>
> doit <- function(var,data){
>   var <-    deparse(substitute(var))
>   print(var)
>   # Create name for basline variable.
>   varpre<-paste("Base.",var,sep="")
>   # Create name for post variable.
>   varpo<-paste("Disch.",var,sep="")
>   # Create a name for the change variable.
>   ch<-paste("Change.",var,sep="")
>   print(ch)
>   # Compute change.
>   data[,ch]=data[,varpo]-data[,varpre]
>
>   cat("\nData used in the analyses\n")
>   print(data[,c("Subj","RANDOMIZED",ch)])
>   # This works when I expressly specify the change variable.
>   cat("This works, expressly specify change variable\n")
>   fitwilcox1<-wilcox.test(Change.rolling~RANDOMIZED,data=data,na.action=na.omit)
>   print(fitwilcox1)
>
>   # This does not work.
>   cat("This does NOT work, try to use created change variable.\n")
>   fitwilcox2<-wilcox.test(eval(ch, parent.frame())~RANDOMIZED,data=data,na.action=na.omit)
>   print(fitwilcox2)
>   # Compute wilcoxon statistic.
> }
> doit(rolling,data2)
>
>
> Thank you,
> John
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:12}}


From sjo at incois.gov.in  Wed Jan 13 06:10:44 2016
From: sjo at incois.gov.in (Sudheer Joseph)
Date: Wed, 13 Jan 2016 05:10:44 +0000
Subject: [R] cross correlation of filtered Time series
In-Reply-To: <248E6FA047A8C746BA491485764190F537163C74@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F537163C74@ESESSMB207.ericsson.se>
Message-ID: <2dc5d67a3f214e2082691e0addd09cdb@INCOISEXCMB02.incois.gov.in>

Thank you,
                 May I know the reason for keeping frequency as 60, the package says  it expects frequency in hertz, how does it work if I input daily data?. The correlation max shown by the plot is at around 55 -ve lag (0.36). which is not the actual case, so there is some thing which I am not understanding in this case.
With best regards,
Sudheer




________________________________________
From: Giorgio Garziano [giorgio.garziano at ericsson.com]
Sent: Tuesday, January 12, 2016 9:38 PM
To: r-help at r-project.org
Cc: Sudheer Joseph
Subject: Re: [R] cross correlation of filtered Time series

Hello,

I think that the package ?seewave? may help you. See corenv() function.

https://cran.r-project.org/web/packages/seewave/seewave.pdf


library(seewave)
sst.ts <- ts(dc$sst, frequency=60)
t2m.ts <- ts(dc$t2m, frequency=60)
corenv(sst.ts, t2m.ts)
corenv.res <- corenv(sst.ts, t2m.ts, plot=FALSE)
corenv.res

Best,

--
GG





Email secured by Check Point


From tomdharray at gmail.com  Wed Jan 13 02:50:54 2016
From: tomdharray at gmail.com (tomdharray at gmail.com)
Date: Tue, 12 Jan 2016 20:50:54 -0500
Subject: [R] Scaling rows of a large Matrix::sparseMatrix()
Message-ID: <5695AD7E.60201@gmail.com>

Hello R-Users,

I'm looking for a way to scale the rows of a sparse matrix M with about
57,000 rows, 14,000 columns, and 238,000 non-zero matrix elements; see
example code below.

Usually I'd use the base::scale() function (see sample code), but it
freezes my computer. The same happens when I try to run a for loop over
the matrix rows.

The conversion with as.matrix() yields a 5.8 Gb large object, which
appears too large for scale().


So my question is: How can the rows of a large sparse matrix be
efficiently scaled?

Thanks and regards,

Dirk


### Hardware/Session Info
Intel Core i7 w/ 12 Gb RAM
R version 3.2.1 (2015-06-18)
Platform: x86_64-unknown-linux-gnu (64-bit)
Running under: Ubuntu 14.04.3 LTS

### Example Code
library(Matrix)
set.seed(42)

## These are exemplary values for my real "problem matrix"
N_ROW <- 56743
N_COL <- 13648
SIZE  <- 238283
PROB <- c(0.050, 0.050, 0.099, 0.149, 0.198, 0.178, 0.119,
          0.079, 0.0297, 0.0198, 0.001, 0.001, 0.001)

## get some random values to populate the sparse matrix
x <- do.call(
  what = rbind,
  args = lapply(X = 1:N_ROW,
                FUN = function(i)
                  expand.grid(i,
                    sample(x = 1:N_COL,
                      size = sample(1:15, 1),
                      replace = TRUE)
                  )
         )
)
x[,3] <- sample(x = 1:13, size = nrow(x),
           replace = TRUE, prob = PROB)

## build the sparse matrix
M <- Matrix::sparseMatrix(
       dims = c(N_ROW, N_COL),
       i = x[,1],
       j = x[,2],
       x = x[,3]
)
print(format(object.size(M), units = "auto"))

## *******************************************
## Scaling the rows of M

## scale() lets my computer freeze
# M <- scale(t(M), center = FALSE, scale(Matrix::rowSums(M)))

## this appears to be not elegant at all and takes forever
# rwsms <- Matrix::rowSums(M)
# for (i in 1:nrow(M)) M[i,] <- M[i,]/rwsms[[i]]


From stefanML at collocations.de  Wed Jan 13 09:07:24 2016
From: stefanML at collocations.de (Stefan Evert)
Date: Wed, 13 Jan 2016 09:07:24 +0100
Subject: [R] Scaling rows of a large Matrix::sparseMatrix()
In-Reply-To: <5695AD7E.60201@gmail.com>
References: <5695AD7E.60201@gmail.com>
Message-ID: <C1AACF3D-2B81-46A3-A2B1-8A3621D2F9F4@collocations.de>


> On 13 Jan 2016, at 02:50, tomdharray at gmail.com wrote:
> 
> So my question is: How can the rows of a large sparse matrix be
> efficiently scaled?

If you're not picky about the particular storage format, the "wordspace" package

	http://wordspace.r-forge.r-project.org/

has an efficient scaleMargins() function, which can be made to do what you need in combination with rowNorms() and colNorms(); cf. the trivial implementation of normalize.rows().

These functions only work with a dgCMatrix and will try to coerce any other sparseMatrix to this format.

Best,
Stefan


From giorgio.garziano at ericsson.com  Wed Jan 13 09:20:26 2016
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 13 Jan 2016 08:20:26 +0000
Subject: [R] cross correlation of filtered Time series
In-Reply-To: <2dc5d67a3f214e2082691e0addd09cdb@INCOISEXCMB02.incois.gov.in>
References: <248E6FA047A8C746BA491485764190F537163C74@ESESSMB207.ericsson.se>
	<2dc5d67a3f214e2082691e0addd09cdb@INCOISEXCMB02.incois.gov.in>
Message-ID: <248E6FA047A8C746BA491485764190F537163F6B@ESESSMB207.ericsson.se>

I used frequency=60 to make it quickly work in order to show some sample code.
About the frequency parameter in time series, i.e. ts() stats package,  you may take a look at:

http://stats.stackexchange.com/questions/120806/frequency-value-for-seconds-minutes-intervals-data-in-r

corenv() f parameter unit measure is Hertz as seewave use case is about time waves.
About the corenv() results based on your data, I do not have the chance to investigate further.

Best,

--
GG

-----Original Message-----
From: Sudheer Joseph [mailto:sjo at incois.gov.in] 
Sent: mercoled? 13 gennaio 2016 06:11
To: Giorgio Garziano; r-help at r-project.org
Subject: RE: [R] cross correlation of filtered Time series

Thank you,
                 May I know the reason for keeping frequency as 60, the package says  it expects frequency in hertz, how does it work if I input daily data?. The correlation max shown by the plot is at around 55 -ve lag (0.36). which is not the actual case, so there is some thing which I am not understanding in this case.
With best regards,
Sudheer




________________________________________
From: Giorgio Garziano [giorgio.garziano at ericsson.com]
Sent: Tuesday, January 12, 2016 9:38 PM
To: r-help at r-project.org
Cc: Sudheer Joseph
Subject: Re: [R] cross correlation of filtered Time series

Hello,

I think that the package "seewave" may help you. See corenv() function.

https://cran.r-project.org/web/packages/seewave/seewave.pdf


library(seewave)
sst.ts <- ts(dc$sst, frequency=60)
t2m.ts <- ts(dc$t2m, frequency=60)
corenv(sst.ts, t2m.ts)
corenv.res <- corenv(sst.ts, t2m.ts, plot=FALSE) corenv.res

Best,

--
GG





Email secured by Check Point


From Gerrit.Eichner at math.uni-giessen.de  Wed Jan 13 09:23:38 2016
From: Gerrit.Eichner at math.uni-giessen.de (Gerrit Eichner)
Date: Wed, 13 Jan 2016 09:23:38 +0100 (MET)
Subject: [R] Scaling rows of a large Matrix::sparseMatrix()
In-Reply-To: <5695AD7E.60201@gmail.com>
References: <5695AD7E.60201@gmail.com>
Message-ID: <Pine.SOC.4.64.1601130918340.26481@solcom.hrz.uni-giessen.de>

Hello, Dirk,

maybe I'm missing something, but to avoid your for-loop-approach doesn't

M <- M/Matrix::rowSums(M)

do what you want?

  Hth  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 212
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
Fax: +49-(0)641-99-32109            http://www.uni-giessen.de/eichner
---------------------------------------------------------------------

> Hello R-Users,
>
> I'm looking for a way to scale the rows of a sparse matrix M with about
> 57,000 rows, 14,000 columns, and 238,000 non-zero matrix elements; see
> example code below.
>
> Usually I'd use the base::scale() function (see sample code), but it
> freezes my computer. The same happens when I try to run a for loop over
> the matrix rows.
>
> The conversion with as.matrix() yields a 5.8 Gb large object, which
> appears too large for scale().
>
>
> So my question is: How can the rows of a large sparse matrix be
> efficiently scaled?
>
> Thanks and regards,
>
> Dirk
>
>
> ### Hardware/Session Info
> Intel Core i7 w/ 12 Gb RAM
> R version 3.2.1 (2015-06-18)
> Platform: x86_64-unknown-linux-gnu (64-bit)
> Running under: Ubuntu 14.04.3 LTS
>
> ### Example Code
> library(Matrix)
> set.seed(42)
>
> ## These are exemplary values for my real "problem matrix"
> N_ROW <- 56743
> N_COL <- 13648
> SIZE  <- 238283
> PROB <- c(0.050, 0.050, 0.099, 0.149, 0.198, 0.178, 0.119,
>          0.079, 0.0297, 0.0198, 0.001, 0.001, 0.001)
>
> ## get some random values to populate the sparse matrix
> x <- do.call(
>  what = rbind,
>  args = lapply(X = 1:N_ROW,
>                FUN = function(i)
>                  expand.grid(i,
>                    sample(x = 1:N_COL,
>                      size = sample(1:15, 1),
>                      replace = TRUE)
>                  )
>         )
> )
> x[,3] <- sample(x = 1:13, size = nrow(x),
>           replace = TRUE, prob = PROB)
>
> ## build the sparse matrix
> M <- Matrix::sparseMatrix(
>       dims = c(N_ROW, N_COL),
>       i = x[,1],
>       j = x[,2],
>       x = x[,3]
> )
> print(format(object.size(M), units = "auto"))
>
> ## *******************************************
> ## Scaling the rows of M
>
> ## scale() lets my computer freeze
> # M <- scale(t(M), center = FALSE, scale(Matrix::rowSums(M)))
>
> ## this appears to be not elegant at all and takes forever
> # rwsms <- Matrix::rowSums(M)
> # for (i in 1:nrow(M)) M[i,] <- M[i,]/rwsms[[i]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sjo at incois.gov.in  Wed Jan 13 09:49:35 2016
From: sjo at incois.gov.in (Sudheer Joseph)
Date: Wed, 13 Jan 2016 08:49:35 +0000
Subject: [R] cross correlation of filtered Time series
In-Reply-To: <248E6FA047A8C746BA491485764190F537163F6B@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F537163C74@ESESSMB207.ericsson.se>
	<2dc5d67a3f214e2082691e0addd09cdb@INCOISEXCMB02.incois.gov.in>,
	<248E6FA047A8C746BA491485764190F537163F6B@ESESSMB207.ericsson.se>
Message-ID: <05de004a5b424d4d9784f15e77e41523@INCOISEXCMB02.incois.gov.in>

Thank you Giorgio
With best regards,
Sudheer





________________________________________
From: Giorgio Garziano [giorgio.garziano at ericsson.com]
Sent: Wednesday, January 13, 2016 1:50 PM
To: Sudheer Joseph; r-help at r-project.org
Subject: RE: [R] cross correlation of filtered Time series

I used frequency=60 to make it quickly work in order to show some sample code.
About the frequency parameter in time series, i.e. ts() stats package,  you may take a look at:

http://stats.stackexchange.com/questions/120806/frequency-value-for-seconds-minutes-intervals-data-in-r

corenv() f parameter unit measure is Hertz as seewave use case is about time waves.
About the corenv() results based on your data, I do not have the chance to investigate further.

Best,

--
GG

-----Original Message-----
From: Sudheer Joseph [mailto:sjo at incois.gov.in]
Sent: mercoled? 13 gennaio 2016 06:11
To: Giorgio Garziano; r-help at r-project.org
Subject: RE: [R] cross correlation of filtered Time series

Thank you,
                 May I know the reason for keeping frequency as 60, the package says  it expects frequency in hertz, how does it work if I input daily data?. The correlation max shown by the plot is at around 55 -ve lag (0.36). which is not the actual case, so there is some thing which I am not understanding in this case.
With best regards,
Sudheer




________________________________________
From: Giorgio Garziano [giorgio.garziano at ericsson.com]
Sent: Tuesday, January 12, 2016 9:38 PM
To: r-help at r-project.org
Cc: Sudheer Joseph
Subject: Re: [R] cross correlation of filtered Time series

Hello,

I think that the package "seewave" may help you. See corenv() function.

https://cran.r-project.org/web/packages/seewave/seewave.pdf


library(seewave)
sst.ts <- ts(dc$sst, frequency=60)
t2m.ts <- ts(dc$t2m, frequency=60)
corenv(sst.ts, t2m.ts)
corenv.res <- corenv(sst.ts, t2m.ts, plot=FALSE) corenv.res

Best,

--
GG





Email secured by Check Point


Email secured by Check Point


From jafarikia at gmail.com  Wed Jan 13 15:18:32 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Wed, 13 Jan 2016 09:18:32 -0500
Subject: [R] Multiple CSV files in different sheets of an Excel file
Message-ID: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>

I have multiple CSV files that I would like to have them in a single Excel
file. For example, I have file1.csv and file2.csv and I want to have
file.xls where file1.csv and file2.csv each have been copied to a single
sheet of the file.xls file.

Thanks,
Mohsen

	[[alternative HTML version deleted]]


From fransiepansiekevertje at gmail.com  Wed Jan 13 15:44:58 2016
From: fransiepansiekevertje at gmail.com (Frans Marcelissen)
Date: Wed, 13 Jan 2016 15:44:58 +0100
Subject: [R] Multiple CSV files in different sheets of an Excel file
In-Reply-To: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
References: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
Message-ID: <CAFFQM6Z1zOJCWp0pqif_3o5UC2b1f8wZ14sKx=A4gLHPtbHW7Q@mail.gmail.com>

Hi Mohse,
You can do that with the append parameter of the write.xlsx routine in the
xlsx package:

xlsx::write.xlsx(file1,file='XXXXX.xlsx',sheetName = '1')
xlsx::write.xlsx(file2,file='XXXXX.xlsx',sheetName = '2',append = T)

Success!
Frans

2016-01-13 15:18 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:

> I have multiple CSV files that I would like to have them in a single Excel
> file. For example, I have file1.csv and file2.csv and I want to have
> file.xls where file1.csv and file2.csv each have been copied to a single
> sheet of the file.xls file.
>
> Thanks,
> Mohsen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jafarikia at gmail.com  Wed Jan 13 15:47:53 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Wed, 13 Jan 2016 09:47:53 -0500
Subject: [R] Multiple CSV files in different sheets of an Excel file
In-Reply-To: <CAFFQM6Z1zOJCWp0pqif_3o5UC2b1f8wZ14sKx=A4gLHPtbHW7Q@mail.gmail.com>
References: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
	<CAFFQM6Z1zOJCWp0pqif_3o5UC2b1f8wZ14sKx=A4gLHPtbHW7Q@mail.gmail.com>
Message-ID: <CADs3iXmm=G2oQf83-o0dgJNJYGqHO94e4LqAzuOSa9xEdWS0=w@mail.gmail.com>

Thanks Frans,

My files are CSV. If presume first I should convert them to Excel format
and run the code you have suggested. Am I right?

Thanks again,
Mohsen


On Wed, Jan 13, 2016 at 9:44 AM, Frans Marcelissen <
fransiepansiekevertje at gmail.com> wrote:

> Hi Mohse,
> You can do that with the append parameter of the write.xlsx routine in the
> xlsx package:
>
> xlsx::write.xlsx(file1,file='XXXXX.xlsx',sheetName = '1')
> xlsx::write.xlsx(file2,file='XXXXX.xlsx',sheetName = '2',append = T)
>
> Success!
> Frans
>
> 2016-01-13 15:18 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:
>
>> I have multiple CSV files that I would like to have them in a single Excel
>> file. For example, I have file1.csv and file2.csv and I want to have
>> file.xls where file1.csv and file2.csv each have been copied to a single
>> sheet of the file.xls file.
>>
>> Thanks,
>> Mohsen
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From fransiepansiekevertje at gmail.com  Wed Jan 13 15:53:22 2016
From: fransiepansiekevertje at gmail.com (Frans Marcelissen)
Date: Wed, 13 Jan 2016 15:53:22 +0100
Subject: [R] Multiple CSV files in different sheets of an Excel file
In-Reply-To: <CADs3iXmm=G2oQf83-o0dgJNJYGqHO94e4LqAzuOSa9xEdWS0=w@mail.gmail.com>
References: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
	<CAFFQM6Z1zOJCWp0pqif_3o5UC2b1f8wZ14sKx=A4gLHPtbHW7Q@mail.gmail.com>
	<CADs3iXmm=G2oQf83-o0dgJNJYGqHO94e4LqAzuOSa9xEdWS0=w@mail.gmail.com>
Message-ID: <CAFFQM6b-uOep=-bmmC1+5kVJC4oxQSZU8ks95R9HM4JePZyqyw@mail.gmail.com>

Hi Mohsen,
Just read them with read.csv or read.table (file1<-read.csv(file=.....),
and you can write them.
Success!
Frams

2016-01-13 15:47 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:

> Thanks Frans,
>
> My files are CSV. If presume first I should convert them to Excel format
> and run the code you have suggested. Am I right?
>
> Thanks again,
> Mohsen
>
>
> On Wed, Jan 13, 2016 at 9:44 AM, Frans Marcelissen <
> fransiepansiekevertje at gmail.com> wrote:
>
>> Hi Mohse,
>> You can do that with the append parameter of the write.xlsx routine in
>> the xlsx package:
>>
>> xlsx::write.xlsx(file1,file='XXXXX.xlsx',sheetName = '1')
>> xlsx::write.xlsx(file2,file='XXXXX.xlsx',sheetName = '2',append = T)
>>
>> Success!
>> Frans
>>
>> 2016-01-13 15:18 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:
>>
>>> I have multiple CSV files that I would like to have them in a single
>>> Excel
>>> file. For example, I have file1.csv and file2.csv and I want to have
>>> file.xls where file1.csv and file2.csv each have been copied to a single
>>> sheet of the file.xls file.
>>>
>>> Thanks,
>>> Mohsen
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Wed Jan 13 16:07:43 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 13 Jan 2016 09:07:43 -0600
Subject: [R] Multiple CSV files in different sheets of an Excel file
In-Reply-To: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
References: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
Message-ID: <48CDFDB8-2A63-4F08-8837-708A000F22AA@me.com>


> On Jan 13, 2016, at 8:18 AM, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
> 
> I have multiple CSV files that I would like to have them in a single Excel
> file. For example, I have file1.csv and file2.csv and I want to have
> file.xls where file1.csv and file2.csv each have been copied to a single
> sheet of the file.xls file.
> 
> Thanks,
> Mohsen


Hi,

In general there are several options to create XLS[X] files from within R. 

Most, like my own WriteXLS package, will require the use of external functionality, typically Perl, Python or Java. WriteXLS requires Perl and there is an installation guide for the package on the CRAN page:

   https://cran.r-project.org/web/packages/WriteXLS/index.html

next to the Materials header:

  https://cran.r-project.org/web/packages/WriteXLS/INSTALL


If you elect to go with WriteXLS and satisfy the Perl requirements indicated, you can then use something like the following, after installing the package:

require(WriteXLS)

CSVFiles <- c("file1.csv", "file2.csv")

# Use ?lapply to read in each of the files to a data frame
# results in a list of the data frames

FILE.List <- lapply(CSVFiles, function(x) read.csv(x))

# write the list of data frames to an Excel file called CSVFiles.xlsx
# which will contain 2 worksheets, one per data frame

WriteXLS(File.List, "CSVFiles.xlsx")


If you have a series of files in a folder to read in, you may also wish to look at ?list.files to fetch a listing of file names by pattern into the CSVFiles vector rather than creating it manually as above.

Regards,

Marc Schwartz


From jafarikia at gmail.com  Wed Jan 13 17:30:42 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Wed, 13 Jan 2016 11:30:42 -0500
Subject: [R] Multiple CSV files in different sheets of an Excel file
In-Reply-To: <CAFFQM6b-uOep=-bmmC1+5kVJC4oxQSZU8ks95R9HM4JePZyqyw@mail.gmail.com>
References: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
	<CAFFQM6Z1zOJCWp0pqif_3o5UC2b1f8wZ14sKx=A4gLHPtbHW7Q@mail.gmail.com>
	<CADs3iXmm=G2oQf83-o0dgJNJYGqHO94e4LqAzuOSa9xEdWS0=w@mail.gmail.com>
	<CAFFQM6b-uOep=-bmmC1+5kVJC4oxQSZU8ks95R9HM4JePZyqyw@mail.gmail.com>
Message-ID: <CADs3iXkrSUVhpce+2QoRsYe-X7bXpASsc9eiE-+2ZJQhuwujJQ@mail.gmail.com>

Hi Frans,

There is a problem that my csv files are not just several columns of data.
I have some data and then a couple of tables after my data. I can't use
read.table to read my files. There should be something different to look at
the whole csv sheet.

Thanks again,
Mohsen


On Wed, Jan 13, 2016 at 9:53 AM, Frans Marcelissen <
fransiepansiekevertje at gmail.com> wrote:

> Hi Mohsen,
> Just read them with read.csv or read.table (file1<-read.csv(file=.....),
> and you can write them.
> Success!
> Frams
>
> 2016-01-13 15:47 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:
>
>> Thanks Frans,
>>
>> My files are CSV. If presume first I should convert them to Excel format
>> and run the code you have suggested. Am I right?
>>
>> Thanks again,
>> Mohsen
>>
>>
>> On Wed, Jan 13, 2016 at 9:44 AM, Frans Marcelissen <
>> fransiepansiekevertje at gmail.com> wrote:
>>
>>> Hi Mohse,
>>> You can do that with the append parameter of the write.xlsx routine in
>>> the xlsx package:
>>>
>>> xlsx::write.xlsx(file1,file='XXXXX.xlsx',sheetName = '1')
>>> xlsx::write.xlsx(file2,file='XXXXX.xlsx',sheetName = '2',append = T)
>>>
>>> Success!
>>> Frans
>>>
>>> 2016-01-13 15:18 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:
>>>
>>>> I have multiple CSV files that I would like to have them in a single
>>>> Excel
>>>> file. For example, I have file1.csv and file2.csv and I want to have
>>>> file.xls where file1.csv and file2.csv each have been copied to a single
>>>> sheet of the file.xls file.
>>>>
>>>> Thanks,
>>>> Mohsen
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From cstrato at aon.at  Wed Jan 13 18:57:52 2016
From: cstrato at aon.at (cstrato)
Date: Wed, 13 Jan 2016 18:57:52 +0100
Subject: [R] xps installation error on Mac OS X 10.11
In-Reply-To: <D2BBF369.29BE9%zhujack@mail.nih.gov>
References: <D2BBF369.29BE9%zhujack@mail.nih.gov>
Message-ID: <56969020.6040603@aon.at>

Dear Jack,

First, please do not use r-help or r-devel to ask Bioconductor-related 
questions, Bioconductor has its own support site for BioC packages:
https://support.bioconductor.org/

Second, please note that ROOT_6.x cannot be used for xps.

In the README file I mention for Yosemite:
ftp://root.cern.ch/root/root_v5.34.30.macosx64-10.10-clang61.tar.gz
However, you can probably also use the newest version:
ftp://root.cern.ch/root/root_v5.34.34.macosx64-10.10-clang61.tar.gz

Before installing ROOT again please make sure to delete all versions of 
ROOT that you have already installed.

Best regards,
Christian




On 01/13/16 18:28, Zhu, Jack (NIH/NCI) [E] wrote:
>   Dear Christian,
>
> Sorry for bothering you, but it looks like I really need your help with
> the xps installation on my Mac OS X 10.11 system.  I have been
> struggling for a few days and tried all the options listed in your
> README file:
>
> http://bioconductor.org/packages/devel/bioc/readmes/xps/README
>
> I also read all the online posts related to the xps installation.  But
> still have problem.  By strictly following the instructions,  I
> re-installed ROOT from both source and binary for Mac OS X 10.11 and
> added ROOT environmental variables in my .bashrc (all the four lines).
>   The ROOT itself seems working totally fine by different tests and the
>
>     $ echo $ROOTSYS
>     /Applications/root_v6.06.00
>
>     $ echo $LD_LIBRARY_PATH
>     /Applications/root_v6.06.00/lib
>     $ echo $DYLD_LIBRARY_PATH
>     /Applications/root_v6.06.00
>     $echo $PATH
>     /Applications/root_v6.06.00/bin:?
>
> But I always get an error with LD_LIBRARY_PATH when I try to install the
> xps package from the source:
>
>     -------------------
>     $R CMD INSTALL -d -l /Users/zhujack/Library/R/3.3/library
>     xps_1.31.0.tar.gz
>
>     found ROOT version 6.06/00 in directory /Applications/root_v6.06.00
>
>     xps configuration error:
>
>         You must set the shell variable LD_LIBRARY_PATH to the
>         directory where ROOT resides and re-run R CMD INSTALL
>         e.g., (using Bourne shell syntax):
>
>            export "LD_LIBRARY_PATH=$ROOTSYS/lib:$LD_LIBRARY_PATH"
>            R CMD INSTALL xps
>
>         Please consult the README file for more information
>
>     ERROR: configuration failed for package ?xps?
>     * removing ?/Users/zhujack/Library/R/3.3/library/xps?
>     * restoring previous ?/Users/zhujack/Library/R/3.3/library/xps?
>
> I also tried to add the following line to the /usr/local/bin/R:
>
>     LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/Applications/root_v6.06.00/lib"
>     export LD_LIBRARY_PATH
>
>
> But I still get the same error.  Your help and insigts will be greatly
> appreciared.
>
> Jack
>
>  > sessionInfo()
> R Under development (unstable) (2016-01-11 r69918)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.11.2 (El Capitan)
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] BiocInstaller_1.21.2


From jdnewmil at dcn.davis.ca.us  Wed Jan 13 19:16:27 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 13 Jan 2016 10:16:27 -0800
Subject: [R] Multiple CSV files in different sheets of an Excel file
In-Reply-To: <CADs3iXkrSUVhpce+2QoRsYe-X7bXpASsc9eiE-+2ZJQhuwujJQ@mail.gmail.com>
References: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
	<CAFFQM6Z1zOJCWp0pqif_3o5UC2b1f8wZ14sKx=A4gLHPtbHW7Q@mail.gmail.com>
	<CADs3iXmm=G2oQf83-o0dgJNJYGqHO94e4LqAzuOSa9xEdWS0=w@mail.gmail.com>
	<CAFFQM6b-uOep=-bmmC1+5kVJC4oxQSZU8ks95R9HM4JePZyqyw@mail.gmail.com>
	<CADs3iXkrSUVhpce+2QoRsYe-X7bXpASsc9eiE-+2ZJQhuwujJQ@mail.gmail.com>
Message-ID: <1F644129-72DD-48AE-8C2B-2B4E35E2B71E@dcn.davis.ca.us>

There is no such thing as a "csv sheet"... if you have a complex layout of data in one CSV file then if you want to import it into R then you are going to have to cobble together something that reads the file as character data and parses the pieces out using functions from the grep or sub category. If your goal truly is to go from these files into Excel files with no data processing in R then perhaps you should be asking for help in a forum dedicated to Excel or OpenOffice.
-- 
Sent from my phone. Please excuse my brevity.

On January 13, 2016 8:30:42 AM PST, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
>Hi Frans,
>
>There is a problem that my csv files are not just several columns of
>data.
>I have some data and then a couple of tables after my data. I can't use
>read.table to read my files. There should be something different to
>look at
>the whole csv sheet.
>
>Thanks again,
>Mohsen
>
>
>On Wed, Jan 13, 2016 at 9:53 AM, Frans Marcelissen <
>fransiepansiekevertje at gmail.com> wrote:
>
>> Hi Mohsen,
>> Just read them with read.csv or read.table
>(file1<-read.csv(file=.....),
>> and you can write them.
>> Success!
>> Frams
>>
>> 2016-01-13 15:47 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:
>>
>>> Thanks Frans,
>>>
>>> My files are CSV. If presume first I should convert them to Excel
>format
>>> and run the code you have suggested. Am I right?
>>>
>>> Thanks again,
>>> Mohsen
>>>
>>>
>>> On Wed, Jan 13, 2016 at 9:44 AM, Frans Marcelissen <
>>> fransiepansiekevertje at gmail.com> wrote:
>>>
>>>> Hi Mohse,
>>>> You can do that with the append parameter of the write.xlsx routine
>in
>>>> the xlsx package:
>>>>
>>>> xlsx::write.xlsx(file1,file='XXXXX.xlsx',sheetName = '1')
>>>> xlsx::write.xlsx(file2,file='XXXXX.xlsx',sheetName = '2',append =
>T)
>>>>
>>>> Success!
>>>> Frans
>>>>
>>>> 2016-01-13 15:18 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:
>>>>
>>>>> I have multiple CSV files that I would like to have them in a
>single
>>>>> Excel
>>>>> file. For example, I have file1.csv and file2.csv and I want to
>have
>>>>> file.xls where file1.csv and file2.csv each have been copied to a
>single
>>>>> sheet of the file.xls file.
>>>>>
>>>>> Thanks,
>>>>> Mohsen
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>>
>>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From davidsmi at microsoft.com  Wed Jan 13 21:42:12 2016
From: davidsmi at microsoft.com (David Smith)
Date: Wed, 13 Jan 2016 20:42:12 +0000
Subject: [R] Revolutions blog: December 2015 roundup
Message-ID: <DM2PR0301MB0848737867D61C5A3F203936C8CB0@DM2PR0301MB0848.namprd03.prod.outlook.com>

Since 2008, Microsoft (formerly Revolution Analytics) staff and guests have written about R every weekday at the
Revolutions blog: http://blog.revolutionanalytics.com
and every month I post a summary of articles from the previous month of particular interest to readers of r-help.

Some recent news: Revolution R is now Microsoft R: 
http://blog.revolutionanalytics.com/2016/01/microsoft-r-open.html

And in case you missed them, here are some articles related to R from the month of December:

A look back at accomplishments for the R Project and community in 2015:
http://blog.revolutionanalytics.com/2015/12/rs-year-in-review.html

Segmented regression with the "segmented" package, applied to long-distance running records:
http://blog.revolutionanalytics.com/2015/12/using-segmented-regression-to-analyse-world-record-running-times.html 

Creating multi-tab reports in R with knitr and jQuery UI:
http://blog.revolutionanalytics.com/2015/12/reports-r-jquery.html

New version 2.0 update to ggplot2 adds extensibility and many improvements:
http://blog.revolutionanalytics.com/2015/12/gggplot-version-2-new-features.html

A circle diagram of translations of "Merry Christmas":
http://blog.revolutionanalytics.com/2015/12/all-i-want-for-christmas.html

Upcoming R events and conferences, and sponsorship for R user groups:
http://blog.revolutionanalytics.com/2015/12/looking-forward-to-2016.html

How to embed images in R help pages:
http://blog.revolutionanalytics.com/2015/12/embedding-images-in-r-package-help-pages.html

An Azure ML Studio fraud detection template relies heavily on R components:
http://blog.revolutionanalytics.com/2015/12/fraud-detection-with-r-and-azure.html

R is the fastest-growing language on Stackoverflow, as shown in a subway-style rank chart from ggplot2:
http://blog.revolutionanalytics.com/2015/12/r-is-the-fastest-growing-language-on-stackoverflow.html

Buzzfeed is using R for some (serious!) data journalism:
http://blog.revolutionanalytics.com/2015/12/buzzfeed-uses-r-for-data-journalism.html

A tutorial on using SQL Server R Services to analyze a billion taxi rides:
http://blog.revolutionanalytics.com/2015/12/sql-rre-tutorial.html

Some suggestions on how to cryptographically store secrets in R code:
http://blog.revolutionanalytics.com/2015/12/securely-storing-your-secrets-in-r-code.html

Some tips and trade-offs to consider when reading large data files with the RevoScaleR package:
http://blog.revolutionanalytics.com/2015/12/trade-offs-to-consider-when-reading-a-large-dataset-into-r-using-the-revoscaler-package.html

A brief summary of improvements in R 3.2.3: http://blog.revolutionanalytics.com/2015/12/r-323-released.html

Implementing Wald's sequential analysis test in R:
http://blog.revolutionanalytics.com/2015/12/walds-graphical-sequential-inspection-procedure.html

Using the gtrendsR package to download and chart Google Trends data:
http://blog.revolutionanalytics.com/2015/12/download-and-plot-google-trends-data-with-r.html

Distributed data structures in R with the ddR package:
http://blog.revolutionanalytics.com/2015/12/fun-with-ddr-using-distributed-data-structures-in-r.html

Using the leaflet package to create an interactive, photo-annotated map of GPS data from a hike:
http://blog.revolutionanalytics.com/2015/12/document-hikes-with-interactive-leaftlets-and-r.html

Microsoft Azure's Data Science Virtual Machine includes R:
http://blog.revolutionanalytics.com/2015/12/microsoft-data-science.html

Feature selection when modeling wide data sets with genetic algorithms using the caret package:
http://blog.revolutionanalytics.com/2015/12/caret-genetic.html

Tips on setting up a virtual machine with RStudio in Azure:
http://blog.revolutionanalytics.com/2015/12/setting-up-an-azure-resource-manager-virtual-machine-with-rstudio.html

Querying recursive CTEs (common table expressions) in a database with the sqldf package:
http://blog.revolutionanalytics.com/2015/12/exploring-recursive-ctes-with-sqldf.html

General interest stories (not related to R) in the past month included: your Macbook charger has more CPU than the
original Macintosh (http://blog.revolutionanalytics.com/2015/12/because-its-friday-smart-charger.html), how 5 particles
can jam a hopper (http://blog.revolutionanalytics.com/2015/12/because-its-friday-hopper-jams.html), and a film based on
the NASA photo archive (http://blog.revolutionanalytics.com/2015/12/apollo-film.html).

Meeting times for local R user groups (http://blog.revolutionanalytics.com/local-r-groups.html) can be found on the
updated R Community Calendar at: http://blog.revolutionanalytics.com/calendar.html

If you're looking for more articles about R, you can find summaries from previous months at
http://blog.revolutionanalytics.com/roundups/. You can receive daily blog posts via email using services like
blogtrottr.com.

As always, thanks for the comments and please keep sending suggestions to me at davidsmi at microsoft.com or via Twitter
(I'm @revodavid).

Cheers,
# David

-- 
David M Smith <davidsmi at microsoft.com>
R Community Lead, Microsoft? 
Tel: +1 (312) 9205766 (Chicago IL, USA)
Twitter: @revodavid | Blog: ?http://blog.revolutionanalytics.com


From lorenzo.isella at gmail.com  Wed Jan 13 22:12:52 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Wed, 13 Jan 2016 22:12:52 +0100
Subject: [R] Forecast and xreg
Message-ID: <20160113211252.GB1838@localhost.localdomain>

Dear All,
Please consider the small self-contained code at the end of the email.
It is an artificial arimax model with a matrix of regressors xreg.
In the script, xreg has as many rows as the number of data points in
the time series "visits" I want to model.
Now, my problem is the following: I am in a similar situation, as in
the example, but my matrix of auxiliary regressors is "shorter" than
the time series, e.g. I am trying to do something like (see the last
line of my script)

modArima <- auto.arima(visits, xreg=xreg[1:40, ])

Which is simply not allowed by the forecast package.
Is there any workaround?
I do not want to throw away the information, so I would prefer *not*
to disregard the predictors just because they are not synchronized to
the time series, nor shorten artificially the time series because it
is longer than my predictor matrix.
Any suggestion is appreciated.
Regards

Lorenzo


########################################################################
library(forecast)
# create some artifical data
modelfitsample <-
data.frame(Customer_Visit=rpois(49,3000),Weekday=rep(1:7,7),
                             Christmas=c(rep(0,40),1,rep(0,8)),Day=1:49)

# Create matrix of numeric predictors
xreg <-
cbind(Weekday=model.matrix(~as.factor(modelfitsample$Weekday)),
                  Day=modelfitsample$Day,
		                Christmas=modelfitsample$Christmas)

# Remove intercept
xreg <- xreg[,-1]

# Rename columns
colnames(xreg) <-
c("Mon","Tue","Wed","Thu","Fri","Sat","Day","Christmas")

# Variable to be modelled
visits <- ts(modelfitsample$Customer_Visit, frequency=7)

# Find ARIMAX model
modArima <- auto.arima(visits, xreg=xreg)


From marc_schwartz at me.com  Wed Jan 13 23:05:08 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 13 Jan 2016 16:05:08 -0600
Subject: [R] Multiple CSV files in different sheets of an Excel file
In-Reply-To: <1F644129-72DD-48AE-8C2B-2B4E35E2B71E@dcn.davis.ca.us>
References: <CADs3iX=J1x5Mr-f+8fD2f2XzSjSnsrmX6cRNdFzKMxRaGTdN6w@mail.gmail.com>
	<CAFFQM6Z1zOJCWp0pqif_3o5UC2b1f8wZ14sKx=A4gLHPtbHW7Q@mail.gmail.com>
	<CADs3iXmm=G2oQf83-o0dgJNJYGqHO94e4LqAzuOSa9xEdWS0=w@mail.gmail.com>
	<CAFFQM6b-uOep=-bmmC1+5kVJC4oxQSZU8ks95R9HM4JePZyqyw@mail.gmail.com>
	<CADs3iXkrSUVhpce+2QoRsYe-X7bXpASsc9eiE-+2ZJQhuwujJQ@mail.gmail.com>
	<1F644129-72DD-48AE-8C2B-2B4E35E2B71E@dcn.davis.ca.us>
Message-ID: <4DB9C8EF-D13C-4736-AF27-19C696CF9CED@me.com>

Hi,

Just to augment Jeff's comments, if you envision taking one of your CSV files (presuming that they are truly a CSV format) and opening it with Excel without any other pre-processing, that is what would happen by default with the majority of R tools that can create Excel files.

Excel, as you may be aware, can directly open CSV files and place the incoming data into a single worksheet. The comma delimiters and line breaks would be parsed and the data would be placed into appropriate rows and columns. You might even want to try that with one of your files, just to see what happens.

You need to think of the worksheets in Excel as being like an R data frame, which has a regular rectangular structure with rows and columns. Sure, you can do things like merge individual cells, etc. in Excel, but the essential structure is a row/column rectangle.

If your CSV files do not conform to that general structure, then you are going to have to do some pre-processing, either in R or elsewhere, before writing the data to worksheets in an Excel file using R.

In the case of WriteXLS, which I mentioned earlier, you are limited to essentially exporting data frame objects to the Excel worksheets. That is the primary goal of the package.

Some of the other Excel file writing packages provide for additional functionality, including formatting and manipulating cells and similar things. But, you would have to code around any uniqueness in each of your CSV files, if they do not have a consistent format within each file and from file to file.

One alternative in R, would be to use ?readLines (note capital 'L'), which will simply read each line of the CSV files into an R vector. You would then have to parse each line as may be required, into a format that would then conform to the requirements of the Excel row/column cell structure. Perhaps even consider writing the rectangular data in each file to one worksheet and the tables contained into separate worksheets. 

That all being said, you may want to take a step back and re-consider, whether or not this process is really what you want to do and whether or not there is a solution to your problem that does not involve using Excel as the end result.

Regards,

Marc Schwartz


> On Jan 13, 2016, at 12:16 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> There is no such thing as a "csv sheet"... if you have a complex layout of data in one CSV file then if you want to import it into R then you are going to have to cobble together something that reads the file as character data and parses the pieces out using functions from the grep or sub category. If your goal truly is to go from these files into Excel files with no data processing in R then perhaps you should be asking for help in a forum dedicated to Excel or OpenOffice.
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On January 13, 2016 8:30:42 AM PST, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
>> Hi Frans,
>> 
>> There is a problem that my csv files are not just several columns of
>> data.
>> I have some data and then a couple of tables after my data. I can't use
>> read.table to read my files. There should be something different to
>> look at
>> the whole csv sheet.
>> 
>> Thanks again,
>> Mohsen
>> 
>> 
>> On Wed, Jan 13, 2016 at 9:53 AM, Frans Marcelissen <
>> fransiepansiekevertje at gmail.com> wrote:
>> 
>>> Hi Mohsen,
>>> Just read them with read.csv or read.table
>> (file1<-read.csv(file=.....),
>>> and you can write them.
>>> Success!
>>> Frams
>>> 
>>> 2016-01-13 15:47 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:
>>> 
>>>> Thanks Frans,
>>>> 
>>>> My files are CSV. If presume first I should convert them to Excel
>> format
>>>> and run the code you have suggested. Am I right?
>>>> 
>>>> Thanks again,
>>>> Mohsen
>>>> 
>>>> 
>>>> On Wed, Jan 13, 2016 at 9:44 AM, Frans Marcelissen <
>>>> fransiepansiekevertje at gmail.com> wrote:
>>>> 
>>>>> Hi Mohse,
>>>>> You can do that with the append parameter of the write.xlsx routine
>> in
>>>>> the xlsx package:
>>>>> 
>>>>> xlsx::write.xlsx(file1,file='XXXXX.xlsx',sheetName = '1')
>>>>> xlsx::write.xlsx(file2,file='XXXXX.xlsx',sheetName = '2',append =
>> T)
>>>>> 
>>>>> Success!
>>>>> Frans
>>>>> 
>>>>> 2016-01-13 15:18 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:
>>>>> 
>>>>>> I have multiple CSV files that I would like to have them in a
>> single
>>>>>> Excel
>>>>>> file. For example, I have file1.csv and file2.csv and I want to
>> have
>>>>>> file.xls where file1.csv and file2.csv each have been copied to a
>> single
>>>>>> sheet of the file.xls file.
>>>>>> 
>>>>>> Thanks,
>>>>>> Mohsen
>>>>>> 
>>>>>>        [[alternative HTML version deleted]]
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>> 
>>>>> 


From cg.pettersson at lantmannen.com  Wed Jan 13 20:02:25 2016
From: cg.pettersson at lantmannen.com (CG Pettersson)
Date: Wed, 13 Jan 2016 19:02:25 +0000
Subject: [R] Problems with data structure when using plsr() from package pls
Message-ID: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>

R version 3.2.3, W7 64bit.

Dear all!

I am trying to make pls-regression using plsr() from package pls, with Mevik & Wehrens (2007) as tutorial and the datasets from the package.
Everything works real nice as long as I use the supplied datasets, but I don?t understand how to prepare my own data.
This is what I have done:

> frame1 <- data.frame(gushVM, I(n96))

Where gushVM is a vector with fifteen reference analysis values of a quality problem in grain and n96 is a matrix with fifteen rows and 96 columns from an electronic nose. I try to copy the methods as in 3.2 in Mevik & Wehrens, and want to keep n96 as one variable to avoid addressing 96 different variables in the plsr call. If I don?t use I() in the call I get 96 variables instead.
Looking at the dataframe by summary(frame1) get a return quite like summary(gasoline) from the package (not shown here).
But when I try to use plsr() with my own data it doesn?t work due to an error in the data structure:

> pls1 <- plsr(gushVM ~ n96, data = frame1)
Error in model.frame.default(formula = gushVM ~ n96, data = frame1) :
  invalid type (list) for variable 'n96'
>
So, n96 has turned into a list, and that is a problem. If gushVM is a vector (one variable) och a matrix (five variables) does not seem to change anything, managing n96 is the problem
I have tried all alternative ways of creating a proper data frame suggested in the article with exactly the same result.
I have tried the documentation for data.frame() but I probably don?t understand what it says.

What should I do to change "n96" into something better than "list"?

Thanks
/CG

Med v?nlig h?lsning/Best regards
CG Pettersson
Scientific Project Manager, PhD
______________________
Lantm?nnen Corporate R&D
Phone:  +46 10 556 19 85
Mobile: + 46 70 330 66 85
Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
Visiting Address: S:t G?ransgatan 160 A
Address: Box 30192, SE-104 25 Stockholm
Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
Registered Office: Stockholm
Before printing, think about the environment


	[[alternative HTML version deleted]]


From marongiu.luigi at gmail.com  Wed Jan 13 23:55:57 2016
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Wed, 13 Jan 2016 22:55:57 +0000
Subject: [R] Improve training of predictive neural networks
Message-ID: <CAMk+s2SC=ex-Y9XMmueYGPpb9aZ-6iPOuXa2qZReWj121rThMQ@mail.gmail.com>

Dear all,
I am trying to train a predictive neural network to guess the
positivity/negativity of some tests I have done. I am using the
library pnn and I have empirically identified the best sigma to use in
the algorithm but the results I got are affected by false positive
results.
The actual dataset I am using is formed by 14,000 values, which I have
subdivided in a training and query subsets. Since the numbers are high
I can't really use them for the present example, just suffice to say
that the values less than y = 0.035 are negative in the training
subset. However in the query subset many values with y < 0.035 and x
between about 10 and 30 are given as positive.
Since the only parameter that I can change in pnn is essentially sigma
and the training subset is properly dividing the positive and negative
populations, how can I further train the system to reduce the false
positive rate?
In the example I am using here, which is just an approximation of the
real thing, red and black circles indicates the positive and negative
values of the training subset, respectively. Squares comes from the
query subset, red/black = positive/negative by pnn. The line indicates
the cut-off that I was expecting the model to guess.
Many thanks
Luigi

>>>
x <- c(3.15,    2.97,    3.21,    45,    2,    2.47,    2.97,    2.6,
  7.35,    4.11,    37.12,    2.73,    36.36,    2.4,    2.74,    45,
  2.47,    37.4,    45,    2.97,    2,    2,    2.55,    2.51,
2.68,    2.31,    2.6,    2,    2.57,    37.05,    13.84,    19.18,
21.94,    28.61,    38.01,    38.24,    38.33,    29.01,    24.64,
10.03,    10.12,    10.29,    10.32,    10.39,    10.41,    10.44,
10.51,    10.64,    10.65,    10.67,    10.83,    10.85,    10.97,
11.24,    11.43,    11.85,    11.87,    12.02,    12.03,    12.05,
12.12,    12.22,    12.29,    12.3,    12.33,    12.62,    12.62,
12.64,    12.69)
y <- c(0.014,    0.008,    0.008,    0.001,    0.002,    0,    0.013,
  0.008,    0.001,    0.011,    0.076,    0.005,    0.045,    0.002,
 0.016,    0.001,    0.002,    0.086,    0.002,    0.019,    0,
0.002,    0.024,    0.015,    0.009,    0.013,    0.017,    0.009,
0.012,    0.088,    0.129,    0.097,    0.085,    0.096,    0.087,
0.103,    0.066,    0.11,    0.11, 0.001,    0.002,    0.002,
0.104,    0,    0.003,    0.116,    0.001,    0.002,    -0.001,
0.116,    0.124,    0.004,    0.116,    0.124,    0.119,    0.003,
0.112,    0.003,    0.002,    0.092,    0.118,    0.108,    0.104,
0,    0.112,    0.131,    0.001,    0.125,    0.005)
z <- c(0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,
  0,    1,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0,
   0,    0,    0,    0,    0,    0,    1,    1,    1,    1,    1,
1,    1,    1,    1,    1, 0,    0,    0,    1,    0,    0,    1,
0,    0,    0,    1,    1,    0,    1,    1,    1,    0,    1,    0,
 0,    1,    1,    1,    1,    0,    1,    1,    0,    1,    0)
t.df <- data.frame(z, x, y)

t.pos <- subset(t.df, z == 1)
t.neg <- subset(t.df, z == 0)
plot(t.pos$y ~ t.pos$x,
     col = "red", xlab="x", ylab="y",
     xlim = c(min(t.df$x), max(t.df$x)),
     ylim = c(min(t.df$y), max(t.df$y))
)
points(t.neg$x, t.neg$y)

x <- c(38.01,    2.7,    4.89,    2.76,    2.96,    2.91,    38.61,
2.89,    2.07,    2.72,    2.77,    4.49,    3.06,    3.1,    2,
2.95,    3.37,    4.7,    2.98,    2.89,    44.6,    3.09,    28.05,
 2.8,    4.76,    4.91,    3.04,    2.79,    3.1,    37.62,    5.49,
 3.17,    4.53,    2.77,    2.87,    4.91,    3.08,    3.04,    3.03,
  3.09,    4.74,    2.74,    4.25,    3.31,    28.22,    3.05,
4.68,    4.8,    3.12,    2.65,    2.62,    2.91,    38.32,    2.86,
 2.96,    2.95,    16.24,    3.01,    3.25,    2.93,    2.92,    2.93,
   2.99,    4.79,    3.13,    3.01,    3.29,    2.76,    3.44,
4.91,    3.14,    2.9,    3.03,    2.51,    2.91,    2.52,    45,
3.15,    44.48,    3.03,    2.76,    4.81,    14.97,    2.8, 12, 20,
25, 30)
y <- c(0.082,    0.007,    0.034,    0.007,    0.027,    0.009,
0.057,    0.028,    0.007,    0.02,    0.022,    0.021,    0.012,
0.018,    0,    0.021,    0.041,    0.021,    0.021,    0.011,
0.025,    0.011,    0.102,    0.016,    0.035,    0.015,    0.008,
0.017,    0.028,    0.084,    0.013,    0.032,    0.004,    0.006,
0.025,    0.019,    0.006,    0.018,    0.019,    0.02,    0.021,
0.009,    0.015,    0.023,    0.089,    0.023,    0.025,    0.034,
0.035,    0.009,    0.006,    0.007,    0.056,    0.025,    0.016,
0.012,    0.101,    0.019,    0.017,    0.031,    0.019,    0.014,
0.044,    0.02,    0.018,    0.017,    0.018,    0.008,    0.02,
0.017,    0.016,    0.021,    0.02,    0.009,    0.019,    0.006,
0.002,    0.012,    0.016,    0.013,    0.016,    0.013,    0.101,
0.027, 0.01, 0.02, 0.03, 0.025)
q.df <- data.frame(x, y)
points(q.df$x, q.df$y, col="green")

library(pnn)
train <- learn(t.df[,1:3])
fit <- smooth(train)
# guess
n <- nrow(q.df)
Q <- rep(-1, n)
for (i in 1:n) {
    print(i)
    Q[i] <- guess(fit, as.matrix(q.df[i,1:2]))$category
}
R <- data.frame(q.df, Q, stringsAsFactors = FALSE)

P <- subset(R, Q == 1)
N <- subset(R, Q == 0)
points (P$x, P$y, pch=5, col = "red")
points (N$x, N$y, pch=5, col = "black")
abline(h=0.035)


From bgunter.4567 at gmail.com  Thu Jan 14 00:08:25 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 13 Jan 2016 15:08:25 -0800
Subject: [R] Improve training of predictive neural networks
In-Reply-To: <CAMk+s2SC=ex-Y9XMmueYGPpb9aZ-6iPOuXa2qZReWj121rThMQ@mail.gmail.com>
References: <CAMk+s2SC=ex-Y9XMmueYGPpb9aZ-6iPOuXa2qZReWj121rThMQ@mail.gmail.com>
Message-ID: <CAGxFJbQYvOv5NqBEVJKF9a7r4gqsgPe4o2g8o92WM60+JkQ==Q@mail.gmail.com>

Inline.

-- Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Jan 13, 2016 at 2:55 PM, Luigi Marongiu
<marongiu.luigi at gmail.com> wrote:
> Dear all,
> I am trying to train a predictive neural network to guess the
> positivity/negativity of some tests I have done. I am using the
> library pnn and I have empirically identified the best sigma to use in
> the algorithm but the results I got are affected by false positive
> results.
> The actual dataset I am using is formed by 14,000 values, which I have
> subdivided in a training and query subsets. Since the numbers are high
> I can't really use them for the present example, just suffice to say
> that the values less than y = 0.035 are negative in the training
> subset. However in the query subset many values with y < 0.035 and x
> between about 10 and 30 are given as positive.
> Since the only parameter that I can change in pnn is essentially sigma
> and the training subset is properly dividing the positive and negative
> populations, how can I further train the system to reduce the false
> positive rate?

You cannot. You have already allowed the results on the query subset
to feed back to your training procedure, which completely invalidates
the procedure. You get one bite at the apple, no more.  From here on,
you will most likely produce irreproducible nonsense.

As this is all about the proper use of statistical procedures and not
at all about R, per se, I suggest you follow up by posting on a
statistical site like stats.stackexchange.com, not here.

Cheers,
Bert



> In the example I am using here, which is just an approximation of the
> real thing, red and black circles indicates the positive and negative
> values of the training subset, respectively. Squares comes from the
> query subset, red/black = positive/negative by pnn. The line indicates
> the cut-off that I was expecting the model to guess.
> Many thanks
> Luigi
>
>>>>
> x <- c(3.15,    2.97,    3.21,    45,    2,    2.47,    2.97,    2.6,
>   7.35,    4.11,    37.12,    2.73,    36.36,    2.4,    2.74,    45,
>   2.47,    37.4,    45,    2.97,    2,    2,    2.55,    2.51,
> 2.68,    2.31,    2.6,    2,    2.57,    37.05,    13.84,    19.18,
> 21.94,    28.61,    38.01,    38.24,    38.33,    29.01,    24.64,
> 10.03,    10.12,    10.29,    10.32,    10.39,    10.41,    10.44,
> 10.51,    10.64,    10.65,    10.67,    10.83,    10.85,    10.97,
> 11.24,    11.43,    11.85,    11.87,    12.02,    12.03,    12.05,
> 12.12,    12.22,    12.29,    12.3,    12.33,    12.62,    12.62,
> 12.64,    12.69)
> y <- c(0.014,    0.008,    0.008,    0.001,    0.002,    0,    0.013,
>   0.008,    0.001,    0.011,    0.076,    0.005,    0.045,    0.002,
>  0.016,    0.001,    0.002,    0.086,    0.002,    0.019,    0,
> 0.002,    0.024,    0.015,    0.009,    0.013,    0.017,    0.009,
> 0.012,    0.088,    0.129,    0.097,    0.085,    0.096,    0.087,
> 0.103,    0.066,    0.11,    0.11, 0.001,    0.002,    0.002,
> 0.104,    0,    0.003,    0.116,    0.001,    0.002,    -0.001,
> 0.116,    0.124,    0.004,    0.116,    0.124,    0.119,    0.003,
> 0.112,    0.003,    0.002,    0.092,    0.118,    0.108,    0.104,
> 0,    0.112,    0.131,    0.001,    0.125,    0.005)
> z <- c(0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,
>   0,    1,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0,
>    0,    0,    0,    0,    0,    0,    1,    1,    1,    1,    1,
> 1,    1,    1,    1,    1, 0,    0,    0,    1,    0,    0,    1,
> 0,    0,    0,    1,    1,    0,    1,    1,    1,    0,    1,    0,
>  0,    1,    1,    1,    1,    0,    1,    1,    0,    1,    0)
> t.df <- data.frame(z, x, y)
>
> t.pos <- subset(t.df, z == 1)
> t.neg <- subset(t.df, z == 0)
> plot(t.pos$y ~ t.pos$x,
>      col = "red", xlab="x", ylab="y",
>      xlim = c(min(t.df$x), max(t.df$x)),
>      ylim = c(min(t.df$y), max(t.df$y))
> )
> points(t.neg$x, t.neg$y)
>
> x <- c(38.01,    2.7,    4.89,    2.76,    2.96,    2.91,    38.61,
> 2.89,    2.07,    2.72,    2.77,    4.49,    3.06,    3.1,    2,
> 2.95,    3.37,    4.7,    2.98,    2.89,    44.6,    3.09,    28.05,
>  2.8,    4.76,    4.91,    3.04,    2.79,    3.1,    37.62,    5.49,
>  3.17,    4.53,    2.77,    2.87,    4.91,    3.08,    3.04,    3.03,
>   3.09,    4.74,    2.74,    4.25,    3.31,    28.22,    3.05,
> 4.68,    4.8,    3.12,    2.65,    2.62,    2.91,    38.32,    2.86,
>  2.96,    2.95,    16.24,    3.01,    3.25,    2.93,    2.92,    2.93,
>    2.99,    4.79,    3.13,    3.01,    3.29,    2.76,    3.44,
> 4.91,    3.14,    2.9,    3.03,    2.51,    2.91,    2.52,    45,
> 3.15,    44.48,    3.03,    2.76,    4.81,    14.97,    2.8, 12, 20,
> 25, 30)
> y <- c(0.082,    0.007,    0.034,    0.007,    0.027,    0.009,
> 0.057,    0.028,    0.007,    0.02,    0.022,    0.021,    0.012,
> 0.018,    0,    0.021,    0.041,    0.021,    0.021,    0.011,
> 0.025,    0.011,    0.102,    0.016,    0.035,    0.015,    0.008,
> 0.017,    0.028,    0.084,    0.013,    0.032,    0.004,    0.006,
> 0.025,    0.019,    0.006,    0.018,    0.019,    0.02,    0.021,
> 0.009,    0.015,    0.023,    0.089,    0.023,    0.025,    0.034,
> 0.035,    0.009,    0.006,    0.007,    0.056,    0.025,    0.016,
> 0.012,    0.101,    0.019,    0.017,    0.031,    0.019,    0.014,
> 0.044,    0.02,    0.018,    0.017,    0.018,    0.008,    0.02,
> 0.017,    0.016,    0.021,    0.02,    0.009,    0.019,    0.006,
> 0.002,    0.012,    0.016,    0.013,    0.016,    0.013,    0.101,
> 0.027, 0.01, 0.02, 0.03, 0.025)
> q.df <- data.frame(x, y)
> points(q.df$x, q.df$y, col="green")
>
> library(pnn)
> train <- learn(t.df[,1:3])
> fit <- smooth(train)
> # guess
> n <- nrow(q.df)
> Q <- rep(-1, n)
> for (i in 1:n) {
>     print(i)
>     Q[i] <- guess(fit, as.matrix(q.df[i,1:2]))$category
> }
> R <- data.frame(q.df, Q, stringsAsFactors = FALSE)
>
> P <- subset(R, Q == 1)
> N <- subset(R, Q == 0)
> points (P$x, P$y, pch=5, col = "red")
> points (N$x, N$y, pch=5, col = "black")
> abline(h=0.035)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tomdharray at gmail.com  Thu Jan 14 02:11:28 2016
From: tomdharray at gmail.com (tomdharray at gmail.com)
Date: Wed, 13 Jan 2016 20:11:28 -0500
Subject: [R] Scaling rows of a large Matrix::sparseMatrix()
In-Reply-To: <Pine.SOC.4.64.1601130918340.26481@solcom.hrz.uni-giessen.de>
References: <5695AD7E.60201@gmail.com>
	<Pine.SOC.4.64.1601130918340.26481@solcom.hrz.uni-giessen.de>
Message-ID: <5696F5C0.2060404@gmail.com>

Hello Gerrit,

Thanks. Your proposal works in general, but I get memory allocation
errors with my actual 57,000 x 14,000 matrix.

The fix which I now use is to scale the data before I build the matrix;
see below.


Cheers,

Dirk


## Code Start -----------------------------

library(parallel)

rowscale <- function(.x) cbind(.x[,1:3], .x[,3] / sum(.x[,3]))

y <- split(x = x, f = x[,1])


localSocketCluster <- parallel::makeCluster(spec = 4, type = "SOCK")

y <- parallel::parLapply(cl = localSocketCluster, X = y, fun = rowscale)

parallel::stopCluster(cl = localSocketCluster)

x <- do.call(what = rbind, args = y)

## build the sparse matrix
M <- Matrix::sparseMatrix(dims = c(N_ROW, N_COL),
       i = x[,1], j = x[,2], x = x[,4])


## Code End -----------------------------

On 16-01-13 03:23 AM, Gerrit Eichner wrote:
> Hello, Dirk,
> 
> maybe I'm missing something, but to avoid your for-loop-approach doesn't
> 
> M <- M/Matrix::rowSums(M)
> 
> do what you want?
> 
>  Hth  --  Gerrit
> 
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> Fax: +49-(0)641-99-32109            http://www.uni-giessen.de/eichner
> ---------------------------------------------------------------------
>


From r.turner at auckland.ac.nz  Thu Jan 14 02:53:02 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 14 Jan 2016 14:53:02 +1300
Subject: [R] [FORGED]  Forecast and xreg
In-Reply-To: <20160113211252.GB1838@localhost.localdomain>
References: <20160113211252.GB1838@localhost.localdomain>
Message-ID: <5696FF7E.4000808@auckland.ac.nz>



I know from nothing about arimax or the forecast package, but it sounds 
to me like you are expecting magic.  An analogy:

Suppose you are doing a simple linear regression of y on x1 and x2 with
y and x1 having 49 entries each, but with x2 having only 40.  Assume 
that y, x1 and x2 are properly aligned but there are no x2 data for the 
last 9 entries of x2.  Would you really expect to be able to make use of 
the last 9 entries of y and x1 in fitting your model?

I could be wrong (I was once; back in 1968 --- I thought I'd made a 
mistake and I hadn't; :-) ) but I don't think there is any scope for 
applying an EM algorithm, since x2 is not a random variable so taking 
expectations of the likelihood w.r.t. the missing data makes no sense.


I guess if you assumed (e.g.) a joint Gaussian distribution for 
(y,x1,x2) then you could "do EM".  But that would not be a realistic 
assumption in most instances.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

On 14/01/16 10:12, Lorenzo Isella wrote:
> Dear All,
> Please consider the small self-contained code at the end of the email.
> It is an artificial arimax model with a matrix of regressors xreg.
> In the script, xreg has as many rows as the number of data points in
> the time series "visits" I want to model.
> Now, my problem is the following: I am in a similar situation, as in
> the example, but my matrix of auxiliary regressors is "shorter" than
> the time series, e.g. I am trying to do something like (see the last
> line of my script)
>
> modArima <- auto.arima(visits, xreg=xreg[1:40, ])
>
> Which is simply not allowed by the forecast package.
> Is there any workaround?
> I do not want to throw away the information, so I would prefer *not*
> to disregard the predictors just because they are not synchronized to
> the time series, nor shorten artificially the time series because it
> is longer than my predictor matrix.
> Any suggestion is appreciated.
> Regards
>
> Lorenzo
>
>
> ########################################################################
> library(forecast)
> # create some artifical data
> modelfitsample <-
> data.frame(Customer_Visit=rpois(49,3000),Weekday=rep(1:7,7),
>                              Christmas=c(rep(0,40),1,rep(0,8)),Day=1:49)
>
> # Create matrix of numeric predictors
> xreg <-
> cbind(Weekday=model.matrix(~as.factor(modelfitsample$Weekday)),
>                   Day=modelfitsample$Day,
>                          Christmas=modelfitsample$Christmas)
>
> # Remove intercept
> xreg <- xreg[,-1]
>
> # Rename columns
> colnames(xreg) <-
> c("Mon","Tue","Wed","Thu","Fri","Sat","Day","Christmas")
>
> # Variable to be modelled
> visits <- ts(modelfitsample$Customer_Visit, frequency=7)
>
> # Find ARIMAX model
> modArima <- auto.arima(visits, xreg=xreg)


From miaojpm at gmail.com  Thu Jan 14 04:49:12 2016
From: miaojpm at gmail.com (jpm miao)
Date: Wed, 13 Jan 2016 19:49:12 -0800
Subject: [R] Could the function "addPlot.RTF {rtf}" add multiplots to Word?
Message-ID: <CABcx46Ao8Wvtbrma30+3rbUgSTvjkpUBkkTvzcQ5nFsep6haqQ@mail.gmail.com>

Hi,

   I am outputting a large number of graphs (made by ggplot2) to a Word
file. The function "addPlot" in the package "rtf" does help output the
graphs sequentially in one column; can it be modified so that it outputs
graphs in more than one columns?

   (I try R2wd package, but it does not work well on my computer)

   Could we let the ggplot2-related graph function return more than one
graph (e.g., cowplot package or "multiplot.R" function

http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/

and then use addPlot?

   Any suggested alternative would help!

Thanks!!

Miao

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Thu Jan 14 05:16:27 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 13 Jan 2016 20:16:27 -0800
Subject: [R] Problems with data structure when using plsr() from package
	pls
In-Reply-To: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
Message-ID: <F8D1CA9A-6B00-4BB1-B00F-4C6234624C4B@dcn.davis.ca.us>

Using I() in the data.frame seems ill-advised to me. You complain about 96 variables but from reading your explanation that seems to be what your data are. I have no idea whether it makes sense to NOT have 96 variables if that is what your data are. Note that a reproducible example supplied by you might help us guess better,  but it might just be that your expectations are wrong. 
-- 
Sent from my phone. Please excuse my brevity.

On January 13, 2016 11:02:25 AM PST, CG Pettersson <cg.pettersson at lantmannen.com> wrote:
>R version 3.2.3, W7 64bit.
>
>Dear all!
>
>I am trying to make pls-regression using plsr() from package pls, with
>Mevik & Wehrens (2007) as tutorial and the datasets from the package.
>Everything works real nice as long as I use the supplied datasets, but
>I don?t understand how to prepare my own data.
>This is what I have done:
>
>> frame1 <- data.frame(gushVM, I(n96))
>
>Where gushVM is a vector with fifteen reference analysis values of a
>quality problem in grain and n96 is a matrix with fifteen rows and 96
>columns from an electronic nose. I try to copy the methods as in 3.2 in
>Mevik & Wehrens, and want to keep n96 as one variable to avoid
>addressing 96 different variables in the plsr call. If I don?t use I()
>in the call I get 96 variables instead.
>Looking at the dataframe by summary(frame1) get a return quite like
>summary(gasoline) from the package (not shown here).
>But when I try to use plsr() with my own data it doesn?t work due to an
>error in the data structure:
>
>> pls1 <- plsr(gushVM ~ n96, data = frame1)
>Error in model.frame.default(formula = gushVM ~ n96, data = frame1) :
>  invalid type (list) for variable 'n96'
>>
>So, n96 has turned into a list, and that is a problem. If gushVM is a
>vector (one variable) och a matrix (five variables) does not seem to
>change anything, managing n96 is the problem
>I have tried all alternative ways of creating a proper data frame
>suggested in the article with exactly the same result.
>I have tried the documentation for data.frame() but I probably don?t
>understand what it says.
>
>What should I do to change "n96" into something better than "list"?
>
>Thanks
>/CG
>
>Med v?nlig h?lsning/Best regards
>CG Pettersson
>Scientific Project Manager, PhD
>______________________
>Lantm?nnen Corporate R&D
>Phone:  +46 10 556 19 85
>Mobile: + 46 70 330 66 85
>Email:
>cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
>Visiting Address: S:t G?ransgatan 160 A
>Address: Box 30192, SE-104 25 Stockholm
>Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
>Registered Office: Stockholm
>Before printing, think about the environment
>
>
>	[[alternative HTML version deleted]]
>
>
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From teotjunk at hotmail.com  Thu Jan 14 09:10:36 2016
From: teotjunk at hotmail.com (TJUN KIAT TEO)
Date: Thu, 14 Jan 2016 16:10:36 +0800
Subject: [R] Writing a matrix of lists as a CSV file
Message-ID: <SNT152-W1321729BA569D4ADC52F9EDFCC0@phx.gbl>



I
have a matrix of lists in R that looks like this

 

   Crabs  Glass nnet List,2 List,2rf   List,1 List,1

 

An
example of what a list looks like in the matrix

 

size decay6    3   0.1

 

How can I write it into a csv file in R so I can retrieve it
in the same format?

Thanks

 		 	   		  
	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Thu Jan 14 09:34:57 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 14 Jan 2016 09:34:57 +0100
Subject: [R] printing a data.frame that contains a list-column of
	S4	objects
In-Reply-To: <CAJ4QxaPbw_tXfdfmhtAMQPKs8oe5suCBA5SyPzU_dSqxXsFnew@mail.gmail.com>
References: <0D60E6F4-3F9E-4268-A9E3-F3FDD8AE4FCC@stat.ubc.ca>
	<CAJ4QxaPbw_tXfdfmhtAMQPKs8oe5suCBA5SyPzU_dSqxXsFnew@mail.gmail.com>
Message-ID: <22167.23985.491538.833141@stat.math.ethz.ch>

>>>>> boB Rudis <bob at rudis.net>
>>>>>     on Tue, 12 Jan 2016 13:51:50 -0500 writes:

    > I wonder if something like:
    > format.list <- function(x, ...) {
    > rep(class(x[[1]]), length(x))
    > }

    > would be sufficient? (prbly needs more 'if's though)

Dear Jenny,
for a different perspective (and a lot of musings), see inline below

    > On Tue, Jan 12, 2016 at 12:15 PM, Jenny Bryan <jenny at stat.ubc.ca> wrote:
    >> Is there a general problem with printing a data.frame when it has a
    >> list-column of S4 objects? Or am I just unlucky in my life choices?
    >> 
    >> I ran across this with objects from the git2r package but maintainer
    >> Stefan Widgren points out this example below from Matrix as well. I note
    >> that the offending object can be printed if sent through
    >> dplyr::tbl_df(). I accept that that printing doesn't provide much info
    >> on S4 objects. I'd just like those vars to not prevent data.frame-style
    >> inpsection of the entire object.
    >> 
    >> I asked this on stack overflow, where commenter provided the lead to the
    >> workaround below. Is that the best solution?
    >> 
    >> library(Matrix)
    >> 
    >> m <- new("dgCMatrix")
    >> isS4(m)
    >> #> [1] TRUE
    >> df <- data.frame(id = 1:2)
    >> df$matrices <- list(m, m)

This only works by accident (I think), and fails for

  df <- data.frame(id = 1)
  df$matrices <- list(m, m)

    > df <- data.frame(id = 1)
    > df$matrices <- list(m, m)
    Error in `$<-.data.frame`(`*tmp*`, "matrices", value = list(<S4 object of class "dgCMatrix">,  : 
    replacement has 2 rows, data has 1
    > 


    >> df
    >> #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic
    >> #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic

Hmm,
As 'data.frame' is just an S3 class there is no formal
definition to go with and in this sense you are of course entitled
to all expectations. ;-)
Even though data frames are internally coded as lists, I
strongly believe data frames should be taught as (and thought of)
	 "generalized matrices"
in the sense that data frames should be thought of n (say) rows
and p (say) columns.

The help pages  for  data.frame()  and as.data.frame()
should make it clear that you can *not* put all kinds of entries
into data frame columns, but I agree the documentation is vague
and probably has to remain vague,
because if you provide  as.data.frame()  methods for your class
you should be able to go quite far.

In addition, the data frame columns need to fulfill properties, e.g.,
subsetting (aka "indexing") and also subassignment ( df[i,j] <- v )

Now the real "problem" here is that the '$<-' and '[<-'  methods
for data frames which you call via  df$m <- v  or  df[,co] <- V
are too "forgiving". They only check that NROW(.) of the new
entry corresponds to the nrow(<data.frame>).
Currently they allow very easy construction of illegal data
frames(*), as in your present case.

--
*) Yes, it is hard to say when a data.frame is illegal, as there
   is no formal definition

There is more to be said and thought about if you really want
sparse matrices in a data frame, and as 'Matrix' maintainers,
I'm quite interested *why* you'd want that, but I won't go there
now.

One last issue though: The idea of allowing to put 'matrix' or
'array' into data frames is that each column of the matrix
becomes a separate column of the data frame

> data.frame(D = diag(3), M = matrix(1:12, 3,4))
  D.1 D.2 D.3 M.1 M.2 M.3 M.4
1   1   0   0   1   4   7  10
2   0   1   0   2   5   8  11
3   0   0   1   3   6   9  12

.... and that would be quite inefficient for large sparse matrices.

---------

Final recommendation as a summary:

If  data.frame(.., .., ..) does not work to put entries into a
data frame, then don't do it, but rather think about how to make
data.frame() work with your objects -- namely by ensuring that
as.data.frame() works .. possibly by providing an
as.data.frame() method.

Best regards,
Martin Maechler


From cg.pettersson at lantmannen.com  Thu Jan 14 11:33:51 2016
From: cg.pettersson at lantmannen.com (CG Pettersson)
Date: Thu, 14 Jan 2016 10:33:51 +0000
Subject: [R] Problems with data structure when using plsr() from package
 pls
In-Reply-To: <F8D1CA9A-6B00-4BB1-B00F-4C6234624C4B@dcn.davis.ca.us>
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
	<F8D1CA9A-6B00-4BB1-B00F-4C6234624C4B@dcn.davis.ca.us>
Message-ID: <HE1PR02MB0987DEFE44C06F8BFD164C608CCC0@HE1PR02MB0987.eurprd02.prod.outlook.com>

Dear Jeff, 
thanks for the effort, but the use of I() when preparing the dataset is suggested by the authors (Mevik & Wehrens, section 3.2):

+If Z is a matrix, it has to be protected by the ?protect function? I() in calls
+to data.frame: mydata <- data.frame(..., Z = I(Z)). Otherwise, it will be split into
+separate variables for each column, and there will be no variable called Z in the data frame,
+so we cannot use Z in the formula. One can also add the matrix to an existing data frame:
+R> mydata <- data.frame(...)
+R> mydata$Z <- Z

In the dataset "gasoline" that is supplied with the pls package, there are two variables; octane and NIR, where NIR is a frame with 401 columns and possible to work with like: 
 plsr(octane ~NIR, data = gasoline)
I thought "gasoline" was made like the example above, but I must be missing something else.

Whatever I do ends with " invalid type (list) for variable 'n96'"

So I am still stuck
/CG

Fr?n: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us] 
Skickat: den 14 januari 2016 05:16
Till: CG Pettersson; r-help at r-project.org
?mne: Re: [R] Problems with data structure when using plsr() from package pls

Using I() in the data.frame seems ill-advised to me. You complain about 96 variables but from reading your explanation that seems to be what your data are. I have no idea whether it makes sense to NOT have 96 variables if that is what your data are. Note that a reproducible example supplied by you might help us guess better, but it might just be that your expectations are wrong. 
-- 
Sent from my phone. Please excuse my brevity.
On January 13, 2016 11:02:25 AM PST, CG Pettersson <cg.pettersson at lantmannen.com> wrote:
R version 3.2.3, W7 64bit.

Dear all!

I am trying to make pls-regression using plsr() from package pls, with Mevik & Wehrens (2007) as tutorial and the datasets from the package.
Everything works real nice as long as I use the supplied datasets, but I don?t understand how to prepare my own data.
This is what I have done:
 frame1 <- data.frame(gushVM, I(n96))

Where gushVM is a vector with fifteen reference analysis values of a quality problem in grain and n96 is a matrix with fifteen rows and 96 columns from an electronic nose. I try to copy the methods as in 3.2 in Mevik & Wehrens, and want to keep n96 as one variable to avoid addressing 96 different variables in the plsr call. If I don?t use I() in the call I get 96 variables instead.
Looking at the data
frame by
summary(frame1) get a return quite like summary(gasoline) from the package (not shown here).
But when I try to use plsr() with my own data it doesn?t work due to an error in the data structure:
 pls1 <- plsr(gushVM ~ n96, data = frame1)
Error in model.frame.default(formula = gushVM ~ n96, data = frame1) :
  invalid type (list) for variable 'n96'

So, n96 has turned into a list, and that is a problem. If gushVM is a vector (one variable) och a matrix (five variables) does not seem to change anything, managing n96 is the problem
I have tried all alternative ways of creating a proper data frame suggested in the article with exactly the same result.
I have tried the docum
entation
for data.frame() but I probably don?t understand what it says.

What should I do to change "n96" into something better than "list"?

Thanks
/CG

Med v?nlig h?lsning/Best regards
CG Pettersson
Scientific Project Manager, PhD
______________________
Lantm?nnen Corporate R&D
Phone:  +46 10 556 19 85
Mobile: + 46 70 330 66 85
Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
Visiting Address: S:t G?ransgatan 160 A
Address: Box 30192, SE-104 25 Stockholm
Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
Registered Office: Stockholm
Before printing, think about the environment


 [[alternative HTML version deleted]]

R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From petr.pikal at precheza.cz  Thu Jan 14 11:47:20 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 14 Jan 2016 10:47:20 +0000
Subject: [R] Writing a matrix of lists as a CSV file
In-Reply-To: <SNT152-W1321729BA569D4ADC52F9EDFCC0@phx.gbl>
References: <SNT152-W1321729BA569D4ADC52F9EDFCC0@phx.gbl>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5009BB6@SRVEXCHMBX.precheza.cz>

Hi

I have no idea what you do want. You have several options how to save data in R. First three hits after my search "R save data" are

https://stat.ethz.ch/R-manual/R-devel/library/base/html/save.html
https://stat.ethz.ch/R-manual/R-devel/library/base/html/write.html
http://www.statmethods.net/input/exportingdata.html

Why any of those options do not work for you?

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of TJUN
> KIAT TEO
> Sent: Thursday, January 14, 2016 9:11 AM
> To: r-help at r-project.org
> Subject: [R] Writing a matrix of lists as a CSV file
>
>
>
> I
> have a matrix of lists in R that looks like this
>
>
>
>    Crabs  Glass nnet List,2 List,2rf   List,1 List,1
>
>
>
> An
> example of what a list looks like in the matrix
>
>
>
> size decay6    3   0.1
>
>
>
> How can I write it into a csv file in R so I can retrieve it in the
> same format?
>
> Thanks
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From drjimlemon at gmail.com  Thu Jan 14 11:49:06 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 14 Jan 2016 21:49:06 +1100
Subject: [R] Writing a matrix of lists as a CSV file
In-Reply-To: <SNT152-W1321729BA569D4ADC52F9EDFCC0@phx.gbl>
References: <SNT152-W1321729BA569D4ADC52F9EDFCC0@phx.gbl>
Message-ID: <CA+8X3fV2V2mSw7u+WVP5wrEB-JDipRH4EdfXuifFJAK+kh2BHA@mail.gmail.com>

Hi Teo,
Your "list" looks suspiciously like a vector. It might help if you provided
the output of:

str(my_matrix)

(or whatever your matrix is named) as that would ensure that we were
talking about the same objects.

Jim


On Thu, Jan 14, 2016 at 7:10 PM, TJUN KIAT TEO <teotjunk at hotmail.com> wrote:

>
>
> I
> have a matrix of lists in R that looks like this
>
>
>
>    Crabs  Glass nnet List,2 List,2rf   List,1 List,1
>
>
>
> An
> example of what a list looks like in the matrix
>
>
>
> size decay6    3   0.1
>
>
>
> How can I write it into a csv file in R so I can retrieve it
> in the same format?
>
> Thanks
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bob at rudis.net  Thu Jan 14 12:26:34 2016
From: bob at rudis.net (boB Rudis)
Date: Thu, 14 Jan 2016 06:26:34 -0500
Subject: [R] printing a data.frame that contains a list-column of S4
	objects
In-Reply-To: <22167.23985.491538.833141@stat.math.ethz.ch>
References: <0D60E6F4-3F9E-4268-A9E3-F3FDD8AE4FCC@stat.ubc.ca>
	<CAJ4QxaPbw_tXfdfmhtAMQPKs8oe5suCBA5SyPzU_dSqxXsFnew@mail.gmail.com>
	<22167.23985.491538.833141@stat.math.ethz.ch>
Message-ID: <CAJ4QxaMBDbmpPeBz2Wsy4dgpe4ejT8=7gsoXR+aq79njfd3C9A@mail.gmail.com>

Martin, I'm pretty sure the use of Matrix here (actually by someone
else than Dr Bryan) was to make an easy, inline, reproducible example.
The actual "ugh" column comes from using git2r. I'm assuming there's
an API call returning some pretty gnarly structures that are getting
shoehorned into a data.frame. That happens more often than I'd like in
modern API calls (really complex/nested JSON being returned).

On Thu, Jan 14, 2016 at 3:34 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> boB Rudis <bob at rudis.net>
>>>>>>     on Tue, 12 Jan 2016 13:51:50 -0500 writes:
>
>     > I wonder if something like:
>     > format.list <- function(x, ...) {
>     > rep(class(x[[1]]), length(x))
>     > }
>
>     > would be sufficient? (prbly needs more 'if's though)
>
> Dear Jenny,
> for a different perspective (and a lot of musings), see inline below
>
>     > On Tue, Jan 12, 2016 at 12:15 PM, Jenny Bryan <jenny at stat.ubc.ca> wrote:
>     >> Is there a general problem with printing a data.frame when it has a
>     >> list-column of S4 objects? Or am I just unlucky in my life choices?
>     >>
>     >> I ran across this with objects from the git2r package but maintainer
>     >> Stefan Widgren points out this example below from Matrix as well. I note
>     >> that the offending object can be printed if sent through
>     >> dplyr::tbl_df(). I accept that that printing doesn't provide much info
>     >> on S4 objects. I'd just like those vars to not prevent data.frame-style
>     >> inpsection of the entire object.
>     >>
>     >> I asked this on stack overflow, where commenter provided the lead to the
>     >> workaround below. Is that the best solution?
>     >>
>     >> library(Matrix)
>     >>
>     >> m <- new("dgCMatrix")
>     >> isS4(m)
>     >> #> [1] TRUE
>     >> df <- data.frame(id = 1:2)
>     >> df$matrices <- list(m, m)
>
> This only works by accident (I think), and fails for
>
>   df <- data.frame(id = 1)
>   df$matrices <- list(m, m)
>
>     > df <- data.frame(id = 1)
>     > df$matrices <- list(m, m)
>     Error in `$<-.data.frame`(`*tmp*`, "matrices", value = list(<S4 object of class "dgCMatrix">,  :
>     replacement has 2 rows, data has 1
>     >
>
>
>     >> df
>     >> #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic
>     >> #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic
>
> Hmm,
> As 'data.frame' is just an S3 class there is no formal
> definition to go with and in this sense you are of course entitled
> to all expectations. ;-)
> Even though data frames are internally coded as lists, I
> strongly believe data frames should be taught as (and thought of)
>          "generalized matrices"
> in the sense that data frames should be thought of n (say) rows
> and p (say) columns.
>
> The help pages  for  data.frame()  and as.data.frame()
> should make it clear that you can *not* put all kinds of entries
> into data frame columns, but I agree the documentation is vague
> and probably has to remain vague,
> because if you provide  as.data.frame()  methods for your class
> you should be able to go quite far.
>
> In addition, the data frame columns need to fulfill properties, e.g.,
> subsetting (aka "indexing") and also subassignment ( df[i,j] <- v )
>
> Now the real "problem" here is that the '$<-' and '[<-'  methods
> for data frames which you call via  df$m <- v  or  df[,co] <- V
> are too "forgiving". They only check that NROW(.) of the new
> entry corresponds to the nrow(<data.frame>).
> Currently they allow very easy construction of illegal data
> frames(*), as in your present case.
>
> --
> *) Yes, it is hard to say when a data.frame is illegal, as there
>    is no formal definition
>
> There is more to be said and thought about if you really want
> sparse matrices in a data frame, and as 'Matrix' maintainers,
> I'm quite interested *why* you'd want that, but I won't go there
> now.
>
> One last issue though: The idea of allowing to put 'matrix' or
> 'array' into data frames is that each column of the matrix
> becomes a separate column of the data frame
>
>> data.frame(D = diag(3), M = matrix(1:12, 3,4))
>   D.1 D.2 D.3 M.1 M.2 M.3 M.4
> 1   1   0   0   1   4   7  10
> 2   0   1   0   2   5   8  11
> 3   0   0   1   3   6   9  12
>
> .... and that would be quite inefficient for large sparse matrices.
>
> ---------
>
> Final recommendation as a summary:
>
> If  data.frame(.., .., ..) does not work to put entries into a
> data frame, then don't do it, but rather think about how to make
> data.frame() work with your objects -- namely by ensuring that
> as.data.frame() works .. possibly by providing an
> as.data.frame() method.
>
> Best regards,
> Martin Maechler
>


From f_j_rod at hotmail.com  Thu Jan 14 12:47:37 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Thu, 14 Jan 2016 12:47:37 +0100
Subject: [R] Overlapping subject-specific histograms
Message-ID: <BAY168-W415D1696BC42DE5E907D3FBACC0@phx.gbl>

Dear R users,
 
First of all, excuse me if my doubt is very trivial, but so far I haven't been able to solve it.
My question is this: I have a data frame which contains repeated measurements on 4 subjects coded
as "id", and I want to plot, for each subject, not only the corresponding "counts" variable histogram, 
but also overlapping to the right side the corresponding results of "sim" variable (I want to do it in basic
R code, i.e., without any specific R package). I have almost the right code (see the example code below), 
but I can not overlap the "sim" variable.

Thanks in advance for suggestions!!
 
Frank

data <- data.frame(id =  rep(c(1,3,4,7), c(9,5,3,3)),
    count = c(0, 10, 15, 0, 16, 7, 14, 11, 12, 1, 8, 17, 19, 0, 9, 10, 14, 2, 3, 10),
    sims =  c(1, 9, 15, 1, 14, 5, 12, 10, 12, 2, 6, 15, 18, 1, 9, 9, 12, 5, 3, 9)) 
 
# The actual code I have
# ------------------------------
windows(height = 5, width = 5)
par(mfrow = c(2, 2), oma = c(1, 2, 2, 1), mar=c(3, 2, 1, 1), las = 1)    
for(i in 1:length(unique(data$id))){
   kat <- factor(data$id, labels = 1:length(unique(data$id)))
   plot(data$count[kat == i], 
   type = "h", col = 1, lwd = 3, xaxt = "n", xlab ="", main = "",
   xlim = c(1, max(table(data$id))), ylim = c(0, 20))
   axis(1, at = 1:max(table(data$id)))
   mtext( bquote(paste("id = ", .(unique(data$id)[i]))), side = 3, cex = 0.9, line = 0.5)
   tab <- table( as.matrix( data$id ) )
   dist.overlap <- 0.4 # Distance of right overlapping of the "sim" variable
   # points( factor(names(tab)) + dist.overlap, data$sim[kat == i] , type="h", col=2, lw =4)  ##  =======>  Line I can not solve
 }
 		 	   		  
	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Thu Jan 14 13:03:16 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 14 Jan 2016 12:03:16 +0000
Subject: [R] Overlapping subject-specific histograms
In-Reply-To: <BAY168-W415D1696BC42DE5E907D3FBACC0@phx.gbl>
References: <BAY168-W415D1696BC42DE5E907D3FBACC0@phx.gbl>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5009C38@SRVEXCHMBX.precheza.cz>

Hi

change

points( factor(names(tab)) + dist.overlap, data$sim[kat == i] ,
> type="h", col=2, lw =4)

to

points( 1:length(data$count[kat == i]) + dist.overlap, data$sims[kat == i] , type="h", col=2, lw =4)

And do not use html post, your code could be scrammbled.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank
> S.
> Sent: Thursday, January 14, 2016 12:48 PM
> To: r-help at r-project.org
> Subject: [R] Overlapping subject-specific histograms
>
> Dear R users,
>
> First of all, excuse me if my doubt is very trivial, but so far I
> haven't been able to solve it.
> My question is this: I have a data frame which contains repeated
> measurements on 4 subjects coded
> as "id", and I want to plot, for each subject, not only the
> corresponding "counts" variable histogram,
> but also overlapping to the right side the corresponding results of
> "sim" variable (I want to do it in basic
> R code, i.e., without any specific R package). I have almost the right
> code (see the example code below),
> but I can not overlap the "sim" variable.
>
> Thanks in advance for suggestions!!
>
> Frank
>
> data <- data.frame(id =  rep(c(1,3,4,7), c(9,5,3,3)),
>     count = c(0, 10, 15, 0, 16, 7, 14, 11, 12, 1, 8, 17, 19, 0, 9, 10,
> 14, 2, 3, 10),
>     sims =  c(1, 9, 15, 1, 14, 5, 12, 10, 12, 2, 6, 15, 18, 1, 9, 9,
> 12, 5, 3, 9))
>
> # The actual code I have
> # ------------------------------
> windows(height = 5, width = 5)
> par(mfrow = c(2, 2), oma = c(1, 2, 2, 1), mar=c(3, 2, 1, 1), las = 1)
> for(i in 1:length(unique(data$id))){
>    kat <- factor(data$id, labels = 1:length(unique(data$id)))
>    plot(data$count[kat == i],
>    type = "h", col = 1, lwd = 3, xaxt = "n", xlab ="", main = "",
>    xlim = c(1, max(table(data$id))), ylim = c(0, 20))
>    axis(1, at = 1:max(table(data$id)))
>    mtext( bquote(paste("id = ", .(unique(data$id)[i]))), side = 3, cex
> = 0.9, line = 0.5)
>    tab <- table( as.matrix( data$id ) )
>    dist.overlap <- 0.4 # Distance of right overlapping of the "sim"
> variable
>    # points( factor(names(tab)) + dist.overlap, data$sim[kat == i] ,
> type="h", col=2, lw =4)  ##  =======>  Line I can not solve
>  }
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From olivier.eterradossi at mines-ales.fr  Thu Jan 14 16:05:39 2016
From: olivier.eterradossi at mines-ales.fr (Olivier ETERRADOSSI)
Date: Thu, 14 Jan 2016 16:05:39 +0100 (CET)
Subject: [R] unexpected behaviour of an extended time series (using packages
	spuRs and xts)
Message-ID: <00a401d14edc$ffef75b0$ffce6110$@mines-ales.fr>

Hi list,



I thought I knew how to use extended time series (package xts), but I was
wrong  J  ?



While preparing a toy example for something else, using data provided in
R, I run into an unexpected problem and can?t figure by myself what is
happening below, can anyone of you tell ? I searched the archives but
didn?t locate any answer. Probably it?s trivial, so please forgive  :



I?m using :

R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree" / Platform:
x86_64-w64-mingw32/x64 (64-bit)

Packages are updated weekly, sometimes daily.



I take some data from package spuRs :

> library(spuRs)

> data(kew)



I turn the dataframe into time series (by combining each kew[,2:13] one
after each other into a vector, and turning the vector into time series).



One is ts :



>
kew.ts<-ts(data=stock,start=kew$year[1],end=kew$year[length(kew$year)],fre
quency=12)



And the other is xts, it looks fine at first :



> kew.xts<-as.xts(kew.ts)

> periodicity(kew.xts)

Monthly periodicity from janv. 1697 to janv. 1999  # OK

> hist(kew.xts) # OK

> summary(kew.xts)

     Index         kew.xts

 Min.   :1697   Min.   :  0.00

 1st Qu.:1772   1st Qu.: 29.70

 Median :1848   Median : 47.00

 Mean   :1848   Mean   : 51.14

 3rd Qu.:1924   3rd Qu.: 67.60

 Max.   :1999   Max.   :189.00  # OK



> gdata::is.what(kew.xts)

[1] "is.array"        "is.atomic"       "is.double"
"is.index.unique"

[5] "is.matrix"       "is.numeric"      "is.object"       "is.regular"


 [9] "is.time.unique"  "is.unsorted"     "is.xts"          "is.zoo"
# seems OK





# But now, first try :

> plot(kew.xts)

Error in if (on == "years") { :

  valeur manquante l? o? TRUE / FALSE est requis     # french for ?
missing value where TRUE/FALSE is required ?



# hmmmm, let?s try something else :

> plot(kew.xts['1697-01/1979/']) # OK



> plot(kew.xts['1697-01/1980/'])

Error in if (on == "years") { :

  valeur manquante l? o? TRUE / FALSE est requis



> plot(kew.xts['1697-01/1979-12/']) # OK



> plot(kew.xts['1697-01/1980-01/'])

Error in if (on == "years") { :

  valeur manquante l? o? TRUE / FALSE est requis



# but?!  :



> plot(kew.xts['1979-01/1980/']) # OK !!!!!



And so are :

> plot (kew.xts['1978/1980/'])

> plot(kew.xts['1977/1982/'])

> plot(kew.xts['1977-01/1982-12'])  # and so on?



I?m puzzled ! I have probably missed a trivial point? Can someone tell ?



Thanks a lot list, regards, Olivier



--------------------------

Olivier ETERRADOSSI

Ma?tre-Assistant, HDR

Ecole des Mines d?Al?s (C2MA, site de Pau)

Ing?nierie de l'aspect visuel et tactile des mat?riaux

P?le ? Recherche sur les Interactions des Mat?riaux avec leur
Environnement ? (RIME)

H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9

Tel : 05 59 30 90 35 (direct) - 05 59 30  54 25 (std)

Fax : 05 59 30 63 68

 <http://www.mines-ales.fr/> http://www.mines-ales.fr

 <http://www.mines-telecom.fr/> http://www.mines-telecom.fr








	[[alternative HTML version deleted]]


From f_j_rod at hotmail.com  Thu Jan 14 16:20:29 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Thu, 14 Jan 2016 16:20:29 +0100
Subject: [R] Overlapping subject-specific histograms
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5009C38@SRVEXCHMBX.precheza.cz>
References: <BAY168-W415D1696BC42DE5E907D3FBACC0@phx.gbl>,
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5009C38@SRVEXCHMBX.precheza.cz>
Message-ID: <BAY168-W5BE014AB0F53728926B0DBACC0@phx.gbl>

Many thanks Petr!
 
Best,
 
Frank
 
> From: petr.pikal at precheza.cz
> To: f_j_rod at hotmail.com; r-help at r-project.org
> Subject: RE: [R] Overlapping subject-specific histograms
> Date: Thu, 14 Jan 2016 12:03:16 +0000
> 
> Hi
> 
> change
> 
> points( factor(names(tab)) + dist.overlap, data$sim[kat == i] ,
> > type="h", col=2, lw =4)
> 
> to
> 
> points( 1:length(data$count[kat == i]) + dist.overlap, data$sims[kat == i] , type="h", col=2, lw =4)
> 
> And do not use html post, your code could be scrammbled.
> 
> Cheers
> Petr
> 
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank
> > S.
> > Sent: Thursday, January 14, 2016 12:48 PM
> > To: r-help at r-project.org
> > Subject: [R] Overlapping subject-specific histograms
> >
> > Dear R users,
> >
> > First of all, excuse me if my doubt is very trivial, but so far I
> > haven't been able to solve it.
> > My question is this: I have a data frame which contains repeated
> > measurements on 4 subjects coded
> > as "id", and I want to plot, for each subject, not only the
> > corresponding "counts" variable histogram,
> > but also overlapping to the right side the corresponding results of
> > "sim" variable (I want to do it in basic
> > R code, i.e., without any specific R package). I have almost the right
> > code (see the example code below),
> > but I can not overlap the "sim" variable.
> >
> > Thanks in advance for suggestions!!
> >
> > Frank
> >
> > data <- data.frame(id =  rep(c(1,3,4,7), c(9,5,3,3)),
> >     count = c(0, 10, 15, 0, 16, 7, 14, 11, 12, 1, 8, 17, 19, 0, 9, 10,
> > 14, 2, 3, 10),
> >     sims =  c(1, 9, 15, 1, 14, 5, 12, 10, 12, 2, 6, 15, 18, 1, 9, 9,
> > 12, 5, 3, 9))
> >
> > # The actual code I have
> > # ------------------------------
> > windows(height = 5, width = 5)
> > par(mfrow = c(2, 2), oma = c(1, 2, 2, 1), mar=c(3, 2, 1, 1), las = 1)
> > for(i in 1:length(unique(data$id))){
> >    kat <- factor(data$id, labels = 1:length(unique(data$id)))
> >    plot(data$count[kat == i],
> >    type = "h", col = 1, lwd = 3, xaxt = "n", xlab ="", main = "",
> >    xlim = c(1, max(table(data$id))), ylim = c(0, 20))
> >    axis(1, at = 1:max(table(data$id)))
> >    mtext( bquote(paste("id = ", .(unique(data$id)[i]))), side = 3, cex
> > = 0.9, line = 0.5)
> >    tab <- table( as.matrix( data$id ) )
> >    dist.overlap <- 0.4 # Distance of right overlapping of the "sim"
> > variable
> >    # points( factor(names(tab)) + dist.overlap, data$sim[kat == i] ,
> > type="h", col=2, lw =4)  ##  =======>  Line I can not solve
> >  }
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> 
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> 
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
 		 	   		  
	[[alternative HTML version deleted]]


From mortezafirouzi at yahoo.com  Thu Jan 14 16:18:54 2016
From: mortezafirouzi at yahoo.com (Morteza Firouzi)
Date: Thu, 14 Jan 2016 15:18:54 +0000 (UTC)
Subject: [R] zyp Vs. kendall package for Time Series Trend
In-Reply-To: <SG2PR01MB0895A783FD22EDECBC3D254BBACC0@SG2PR01MB0895.apcprd01.prod.exchangelabs.com>
References: <SG2PR01MB0895A783FD22EDECBC3D254BBACC0@SG2PR01MB0895.apcprd01.prod.exchangelabs.com>
Message-ID: <875108057.660916.1452784734814.JavaMail.yahoo@mail.yahoo.com>

Dear members,
I need to detect trends in time series. To remove the effect of "Lag-1 serial correlation", it is suggested to use either Yue&Pilon or Zhang method. Both methods are available in "zyp" package. The package uses "kendall" package for trend analysis. ?


Based on Yue&Pilon (2002), if the lag-1 serial correlation is significant, TFPW method will remove the effects of it prior to the?trend test; otherwise trend test will be applied on original time series.?

I've compared the results of a sample time series with non-significant lag-1 serial correlation, using both zyp & kendall packages.?"yuepilon" method in "zyp" gives me the following results:tau: 0.075 & sig: 0.388
while "kendall"?package gives me this:?
tau: 0.109 & sig: 0.216



The question is :?Does "zyp"?change the significance of the trend?in this case as well? Is this a malfunction or did I miss something?
I've checked the script and it is mentioned (ln 65)?:?# Prewhiten the original series
c <- acf(data,lag.max=1,plot=FALSE,na.action=na.pass)$acf[2]


Thank you for your consideration.
Best regards,
Morteza





   <!--#yiv5285967173 P {margin-top:0;margin-bottom:0;}-->
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 1-10-2016 6-09-04 PM.png
Type: image/png
Size: 6799 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160114/54f347d5/attachment.png>

From jafarikia at gmail.com  Thu Jan 14 17:20:17 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Thu, 14 Jan 2016 11:20:17 -0500
Subject: [R] read.xlsx - write.xlsx: reading/writing numbers as character
Message-ID: <CADs3iXmGUYWvxUzkDsxxGRcaroP29bFBPi8nwB16ncxjZ1QbQw@mail.gmail.com>

Hello:

I am reading some excel files (each with one sheet) and trying to write
them all in one file. I am not sure if read.xlsx reads some of the columns
as character or write.xlsx writes them as character where they are not
characters. I have 12 columns (2 character and 10 numbers). From 10 number
columns, with float and integer numbers, only 3 of them are recognized
correctly. I was wondering how can I define the the column format for
read.xlsx - write.xlsx.

Here comes a simple example of my code:

ifn1 <- "A.xlsx"
dat1  <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)

ifn2 <- "F.xlsx"
dat2  <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)

write.xlsx(dat1,  file="AF.xlsx", sheetName="A",   showNA=FALSE,
row.names=FALSE, append=FALSE)
write.xlsx(dat2,  file="AF.xlsx", sheetName="F",   showNA=FALSE,
row.names=FALSE, append= TRUE)

Thanks in advance!
Mohsen

	[[alternative HTML version deleted]]


From mara.pfleiderer at uni-ulm.de  Thu Jan 14 13:37:42 2016
From: mara.pfleiderer at uni-ulm.de (mara.pfleiderer at uni-ulm.de)
Date: Thu, 14 Jan 2016 13:37:42 +0100
Subject: [R] parameter constraints in glm() and Bayesian version
Message-ID: <20160114133742.rmeptmtu74woc0k0@imap.uni-ulm.de>

Hello,

I'm a mathematics student at Ulm University and currently I am working  
on my bachelor thesis about a Poisson regression model.

For this, I am using the function glm () in R which is working very well.
But still I have two questions to improve my model and I hope that you  
could help me:

(i) Is there a possibility to set constraints on the regression  
parameters in glm() or is there another function in R?
Specifically, my paramters should be constrained to be positive as  
negative parameters wouldn't make sense. How can I do this in R  
(preferably with glm() or similar functions)?

(ii) Is there a Bayesian version of the glm()-function where I can  
specify the prior distribution for my regression parameters?

Thanks in advance!
Kind regards,
Mara Pfleiderer


From mgperry32 at gmail.com  Thu Jan 14 14:42:10 2016
From: mgperry32 at gmail.com (Malcolm Perry)
Date: Thu, 14 Jan 2016 13:42:10 +0000
Subject: [R] R: layout() affects margin size in subfigures [unexpected
	behaviour]
Message-ID: <CADfHdv6=kuXszvVnEvgd9Kxx18ckit6vEGPsN7a2i+Q6Mfz+0A@mail.gmail.com>

The absolute margin size of figures in R seems to be affected by the layout
of the plot, which i think is surprising (not sure if it qualifies as a
bug). The following plots have different margins sizes, with the 1x3 plot
margins being smaller (thus giving a larger plot area). This is causing
havoc with a package I am writing to automatically generate composite
figures, since labels are positioned differently depending on the number of
panels.

plot_box <- function() {
        plot(1, 1, type='n', bty='n', xaxt='n', yaxt='n', xlab='', ylab='')
        box(lwd = 6)
        box("figure", lwd=6, col='red')
}

png("margin_test_1.png", width=1000, height=500)
par(oma=c(0,0,0,0))
layout(t(1:2))
par(mar=c(3, 3, 3, 3))
plot_box()
par(mar=c(3, 3, 3, 3))
plot_box()
dev.off()

png("margin_test_2.png", width=1500, height=500)
par(oma=c(0,0,0,0))
layout(t(1:3))
par(mar=c(3, 3, 3, 3))
plot_box()
par(mar=c(3, 3, 3, 3))
plot_box()
par(mar=c(3, 3, 3, 3))
plot_box()
dev.off()

I have also posted this question to StackOverflow, and it has images of the
graphical output which illustrate the problem better:
http://stackoverflow.com/questions/34790682/r-layout-affects-margin-size-in-plot-regions

Thanks,

Malcolm

PS I was unsure if this question belonged to help or devel - I will repost
on devel if it is likely to get better answers.

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Thu Jan 14 17:29:16 2016
From: jholtman at gmail.com (jim holtman)
Date: Thu, 14 Jan 2016 11:29:16 -0500
Subject: [R] read.xlsx - write.xlsx: reading/writing numbers as character
In-Reply-To: <CADs3iXmGUYWvxUzkDsxxGRcaroP29bFBPi8nwB16ncxjZ1QbQw@mail.gmail.com>
References: <CADs3iXmGUYWvxUzkDsxxGRcaroP29bFBPi8nwB16ncxjZ1QbQw@mail.gmail.com>
Message-ID: <CAAxdm-7tTcbs-KvCwoR-ctDcq2t3UzeUFuz65aXm=RjoUFU23Q@mail.gmail.com>

Take a look at the data coming in since you may have something that looks
like characters (maybe 'blanks').  What you think is numeric in EXCEL might
not be.?


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
wrote:

> Hello:
>
> I am reading some excel files (each with one sheet) and trying to write
> them all in one file. I am not sure if read.xlsx reads some of the columns
> as character or write.xlsx writes them as character where they are not
> characters. I have 12 columns (2 character and 10 numbers). From 10 number
> columns, with float and integer numbers, only 3 of them are recognized
> correctly. I was wondering how can I define the the column format for
> read.xlsx - write.xlsx.
>
> Here comes a simple example of my code:
>
> ifn1 <- "A.xlsx"
> dat1  <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>
> ifn2 <- "F.xlsx"
> dat2  <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>
> write.xlsx(dat1,  file="AF.xlsx", sheetName="A",   showNA=FALSE,
> row.names=FALSE, append=FALSE)
> write.xlsx(dat2,  file="AF.xlsx", sheetName="F",   showNA=FALSE,
> row.names=FALSE, append= TRUE)
>
> Thanks in advance!
> Mohsen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jafarikia at gmail.com  Thu Jan 14 17:36:26 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Thu, 14 Jan 2016 11:36:26 -0500
Subject: [R] read.xlsx - write.xlsx: reading/writing numbers as character
In-Reply-To: <CAAxdm-7tTcbs-KvCwoR-ctDcq2t3UzeUFuz65aXm=RjoUFU23Q@mail.gmail.com>
References: <CADs3iXmGUYWvxUzkDsxxGRcaroP29bFBPi8nwB16ncxjZ1QbQw@mail.gmail.com>
	<CAAxdm-7tTcbs-KvCwoR-ctDcq2t3UzeUFuz65aXm=RjoUFU23Q@mail.gmail.com>
Message-ID: <CADs3iXkxOrxfvUwQVz7ddQMkkRmDMBrMZUjd8XiFcqiwxn6Ndw@mail.gmail.com>

Thanks for the comment Jim. There is no blank cell. All have numbers.

Mohsen

On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:

> Take a look at the data coming in since you may have something that looks
> like characters (maybe 'blanks').  What you think is numeric in EXCEL might
> not be.?
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
> wrote:
>
>> Hello:
>>
>> I am reading some excel files (each with one sheet) and trying to write
>> them all in one file. I am not sure if read.xlsx reads some of the columns
>> as character or write.xlsx writes them as character where they are not
>> characters. I have 12 columns (2 character and 10 numbers). From 10 number
>> columns, with float and integer numbers, only 3 of them are recognized
>> correctly. I was wondering how can I define the the column format for
>> read.xlsx - write.xlsx.
>>
>> Here comes a simple example of my code:
>>
>> ifn1 <- "A.xlsx"
>> dat1  <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>>
>> ifn2 <- "F.xlsx"
>> dat2  <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>>
>> write.xlsx(dat1,  file="AF.xlsx", sheetName="A",   showNA=FALSE,
>> row.names=FALSE, append=FALSE)
>> write.xlsx(dat2,  file="AF.xlsx", sheetName="F",   showNA=FALSE,
>> row.names=FALSE, append= TRUE)
>>
>> Thanks in advance!
>> Mohsen
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Thu Jan 14 17:48:39 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 14 Jan 2016 11:48:39 -0500
Subject: [R] R: layout() affects margin size in subfigures [unexpected
	behaviour]
In-Reply-To: <CADfHdv6=kuXszvVnEvgd9Kxx18ckit6vEGPsN7a2i+Q6Mfz+0A@mail.gmail.com>
References: <CADfHdv6=kuXszvVnEvgd9Kxx18ckit6vEGPsN7a2i+Q6Mfz+0A@mail.gmail.com>
Message-ID: <CAM_vju=W5jOuYwkRNaM6n7yH4oygo6xf63SbgmQPGpTFB02BiA@mail.gmail.com>

You're setting margin using mar, which is in terms of lines, which is,
well, difficult to manage properly.

     ?mar? A numerical vector of the form ?c(bottom, left, top, right)?
          which gives the number of lines of margin to be specified on
          the four sides of the plot.  The default is ?c(5, 4, 4, 2) +
          0.1?.

If you use mai instead, you will get a consistent physical size.


     ?mai? A numerical vector of the form ?c(bottom, left, top, right)?
          which gives the margin size specified in inches.

See also this bit of ?par:

     The meaning of ?character size? is not well-defined: this is set
     up for the device taking ?pointsize? into account but often not
     the actual font family in use.  Internally the corresponding pars
     (?cra?, ?cin?, ?cxy? and ?csi?) are used only to set the
     inter-line spacing used to convert ?mar? and ?oma? to physical
     margins.  (The same inter-line spacing multiplied by ?lheight? is
     used for multi-line strings in ?text? and ?strheight?.)

Sarah



On Thu, Jan 14, 2016 at 8:42 AM, Malcolm Perry <mgperry32 at gmail.com> wrote:
> The absolute margin size of figures in R seems to be affected by the layout
> of the plot, which i think is surprising (not sure if it qualifies as a
> bug). The following plots have different margins sizes, with the 1x3 plot
> margins being smaller (thus giving a larger plot area). This is causing
> havoc with a package I am writing to automatically generate composite
> figures, since labels are positioned differently depending on the number of
> panels.
>
> plot_box <- function() {
>         plot(1, 1, type='n', bty='n', xaxt='n', yaxt='n', xlab='', ylab='')
>         box(lwd = 6)
>         box("figure", lwd=6, col='red')
> }
>
> png("margin_test_1.png", width=1000, height=500)
> par(oma=c(0,0,0,0))
> layout(t(1:2))
> par(mar=c(3, 3, 3, 3))
> plot_box()
> par(mar=c(3, 3, 3, 3))
> plot_box()
> dev.off()
>
> png("margin_test_2.png", width=1500, height=500)
> par(oma=c(0,0,0,0))
> layout(t(1:3))
> par(mar=c(3, 3, 3, 3))
> plot_box()
> par(mar=c(3, 3, 3, 3))
> plot_box()
> par(mar=c(3, 3, 3, 3))
> plot_box()
> dev.off()
>
> I have also posted this question to StackOverflow, and it has images of the
> graphical output which illustrate the problem better:
> http://stackoverflow.com/questions/34790682/r-layout-affects-margin-size-in-plot-regions
>
> Thanks,
>
> Malcolm
>
> PS I was unsure if this question belonged to help or devel - I will repost
> on devel if it is likely to get better answers.
>
-- 
Sarah Goslee
http://www.numberwright.com


From jholtman at gmail.com  Thu Jan 14 18:17:48 2016
From: jholtman at gmail.com (Jim Holtman)
Date: Thu, 14 Jan 2016 12:17:48 -0500
Subject: [R] read.xlsx - write.xlsx: reading/writing numbers as character
Message-ID: <wu2yr8ggciqwr86bpal4oemf.1452791789815@email.android.com>

Write it out as a csv from excel and see what the data looks like. ?That may help ?in seeing what the problem is.


Sent from my Verizon Wireless 4G LTE Smartphone

<div>-------- Original message --------</div><div>From: Mohsen Jafarikia <jafarikia at gmail.com> </div><div>Date:01/14/2016  11:36  (GMT-05:00) </div><div>To: jim holtman <jholtman at gmail.com> </div><div>Cc: r-help <r-help at stat.math.ethz.ch> </div><div>Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as character </div><div>
</div>Thanks for the comment Jim. There is no blank cell. All have numbers. 

Mohsen

On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:
Take a look at the data coming in since you may have something that looks like characters (maybe 'blanks').  What you think is numeric in EXCEL might not be.?


Jim Holtman
Data Munger Guru
 
What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
Hello:

I am reading some excel files (each with one sheet) and trying to write
them all in one file. I am not sure if read.xlsx reads some of the columns
as character or write.xlsx writes them as character where they are not
characters. I have 12 columns (2 character and 10 numbers). From 10 number
columns, with float and integer numbers, only 3 of them are recognized
correctly. I was wondering how can I define the the column format for
read.xlsx - write.xlsx.

Here comes a simple example of my code:

ifn1 <- "A.xlsx"
dat1  <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)

ifn2 <- "F.xlsx"
dat2  <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)

write.xlsx(dat1,  file="AF.xlsx", sheetName="A",   showNA=FALSE,
row.names=FALSE, append=FALSE)
write.xlsx(dat2,  file="AF.xlsx", sheetName="F",   showNA=FALSE,
row.names=FALSE, append= TRUE)

Thanks in advance!
Mohsen

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Jan 14 18:36:26 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 14 Jan 2016 09:36:26 -0800
Subject: [R] Problems with data structure when using plsr() from package
	pls
In-Reply-To: <HE1PR02MB0987DEFE44C06F8BFD164C608CCC0@HE1PR02MB0987.eurprd02.prod.outlook.com>
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
	<F8D1CA9A-6B00-4BB1-B00F-4C6234624C4B@dcn.davis.ca.us>
	<HE1PR02MB0987DEFE44C06F8BFD164C608CCC0@HE1PR02MB0987.eurprd02.prod.outlook.com>
Message-ID: <47A5E3B0-B334-43BF-9A5D-60FADB167948@comcast.net>


> On Jan 14, 2016, at 2:33 AM, CG Pettersson <cg.pettersson at lantmannen.com> wrote:
> 
> Dear Jeff, 
> thanks for the effort, but the use of I() when preparing the dataset is suggested by the authors (Mevik & Wehrens, section 3.2):
> 
> +If Z is a matrix, it has to be protected by the ?protect function? I() in calls
> +to data.frame: mydata <- data.frame(..., Z = I(Z)). Otherwise, it will be split into
> +separate variables for each column, and there will be no variable called Z in the data frame,
> +so we cannot use Z in the formula. One can also add the matrix to an existing data frame:
> +R> mydata <- data.frame(...)
> +R> mydata$Z <- Z
> 
> In the dataset "gasoline" that is supplied with the pls package, there are two variables; octane and NIR, where NIR is a frame with 401 columns and possible to work with like: 
> plsr(octane ~NIR, data = gasoline)
> I thought "gasoline" was made like the example above, but I must be missing something else.
> 
> Whatever I do ends with " invalid type (list) for variable 'n96'"

Was `n96` a list before you put a copy of it into the `frame1`-object? Maybe it wasn't a simple matrix. You need at the very least to post the output of str(n96). Also .... never use attach().

-- 
David.

> 
> So I am still stuck
> /CG
> 
> Fr?n: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us] 
> Skickat: den 14 januari 2016 05:16
> Till: CG Pettersson; r-help at r-project.org
> ?mne: Re: [R] Problems with data structure when using plsr() from package pls
> 
> Using I() in the data.frame seems ill-advised to me. You complain about 96 variables but from reading your explanation that seems to be what your data are. I have no idea whether it makes sense to NOT have 96 variables if that is what your data are. Note that a reproducible example supplied by you might help us guess better, but it might just be that your expectations are wrong. 
> -- 
> Sent from my phone. Please excuse my brevity.
> On January 13, 2016 11:02:25 AM PST, CG Pettersson <cg.pettersson at lantmannen.com> wrote:
> R version 3.2.3, W7 64bit.
> 
> Dear all!
> 
> I am trying to make pls-regression using plsr() from package pls, with Mevik & Wehrens (2007) as tutorial and the datasets from the package.
> Everything works real nice as long as I use the supplied datasets, but I don?t understand how to prepare my own data.
> This is what I have done:
> frame1 <- data.frame(gushVM, I(n96))
> 
> Where gushVM is a vector with fifteen reference analysis values of a quality problem in grain and n96 is a matrix with fifteen rows and 96 columns from an electronic nose. I try to copy the methods as in 3.2 in Mevik & Wehrens, and want to keep n96 as one variable to avoid addressing 96 different variables in the plsr call. If I don?t use I() in the call I get 96 variables instead.
> Looking at the data
> frame by
> summary(frame1) get a return quite like summary(gasoline) from the package (not shown here).
> But when I try to use plsr() with my own data it doesn?t work due to an error in the data structure:
> pls1 <- plsr(gushVM ~ n96, data = frame1)
> Error in model.frame.default(formula = gushVM ~ n96, data = frame1) :
>  invalid type (list) for variable 'n96'
> 
> So, n96 has turned into a list, and that is a problem. If gushVM is a vector (one variable) och a matrix (five variables) does not seem to change anything, managing n96 is the problem
> I have tried all alternative ways of creating a proper data frame suggested in the article with exactly the same result.
> I have tried the docum
> entation
> for data.frame() but I probably don?t understand what it says.
> 
> What should I do to change "n96" into something better than "list"?
> 
> Thanks
> /CG
> 
> Med v?nlig h?lsning/Best regards
> CG Pettersson
> Scientific Project Manager, PhD
> ______________________
> Lantm?nnen Corporate R&D
> Phone:  +46 10 556 19 85
> Mobile: + 46 70 330 66 85
> Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
> Visiting Address: S:t G?ransgatan 160 A
> Address: Box 30192, SE-104 25 Stockholm
> Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
> Registered Office: Stockholm
> Before printing, think about the environment
> 
> 
> [[alternative HTML version deleted]]
> 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jafarikia at gmail.com  Thu Jan 14 18:59:49 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Thu, 14 Jan 2016 12:59:49 -0500
Subject: [R] read.xlsx - write.xlsx: reading/writing numbers as character
In-Reply-To: <wu2yr8ggciqwr86bpal4oemf.1452791789815@email.android.com>
References: <wu2yr8ggciqwr86bpal4oemf.1452791789815@email.android.com>
Message-ID: <CADs3iX=P+F-09ewUaNaeAiRfc+o33pSD1jXR=UpTd2AO6OadHA@mail.gmail.com>

It looks okay on CSV too.

On Thu, Jan 14, 2016 at 12:17 PM, Jim Holtman <jholtman at gmail.com> wrote:

> Write it out as a csv from excel and see what the data looks like.  That
> may help  in seeing what the problem is.
>
>
> Sent from my Verizon Wireless 4G LTE Smartphone
>
>
> -------- Original message --------
> From: Mohsen Jafarikia
> Date:01/14/2016 11:36 (GMT-05:00)
> To: jim holtman
> Cc: r-help
> Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
> character
>
> Thanks for the comment Jim. There is no blank cell. All have numbers.
>
> Mohsen
>
> On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:
>
>> Take a look at the data coming in since you may have something that looks
>> like characters (maybe 'blanks').  What you think is numeric in EXCEL might
>> not be.?
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
>> wrote:
>>
>>> Hello:
>>>
>>> I am reading some excel files (each with one sheet) and trying to write
>>> them all in one file. I am not sure if read.xlsx reads some of the
>>> columns
>>> as character or write.xlsx writes them as character where they are not
>>> characters. I have 12 columns (2 character and 10 numbers). From 10
>>> number
>>> columns, with float and integer numbers, only 3 of them are recognized
>>> correctly. I was wondering how can I define the the column format for
>>> read.xlsx - write.xlsx.
>>>
>>> Here comes a simple example of my code:
>>>
>>> ifn1 <- "A.xlsx"
>>> dat1  <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>>>
>>> ifn2 <- "F.xlsx"
>>> dat2  <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>>>
>>> write.xlsx(dat1,  file="AF.xlsx", sheetName="A",   showNA=FALSE,
>>> row.names=FALSE, append=FALSE)
>>> write.xlsx(dat2,  file="AF.xlsx", sheetName="F",   showNA=FALSE,
>>> row.names=FALSE, append= TRUE)
>>>
>>> Thanks in advance!
>>> Mohsen
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From jafarikia at gmail.com  Thu Jan 14 20:30:59 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Thu, 14 Jan 2016 14:30:59 -0500
Subject: [R] read.xlsx - write.xlsx: reading/writing numbers as character
In-Reply-To: <wu2yr8ggciqwr86bpal4oemf.1452791789815@email.android.com>
References: <wu2yr8ggciqwr86bpal4oemf.1452791789815@email.android.com>
Message-ID: <CADs3iXnux3ZDOn0c3VD14n3oW4SVUM5WmYYBptm01xLjf1bFOg@mail.gmail.com>

Hi Jim,

I found where the problem is. I had some characters in my list.

Thanks very much for your attention and help!

Mohsen

On Thu, Jan 14, 2016 at 12:17 PM, Jim Holtman <jholtman at gmail.com> wrote:

> Write it out as a csv from excel and see what the data looks like.  That
> may help  in seeing what the problem is.
>
>
> Sent from my Verizon Wireless 4G LTE Smartphone
>
>
> -------- Original message --------
> From: Mohsen Jafarikia
> Date:01/14/2016 11:36 (GMT-05:00)
> To: jim holtman
> Cc: r-help
> Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
> character
>
> Thanks for the comment Jim. There is no blank cell. All have numbers.
>
> Mohsen
>
> On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:
>
>> Take a look at the data coming in since you may have something that looks
>> like characters (maybe 'blanks').  What you think is numeric in EXCEL might
>> not be.?
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
>> wrote:
>>
>>> Hello:
>>>
>>> I am reading some excel files (each with one sheet) and trying to write
>>> them all in one file. I am not sure if read.xlsx reads some of the
>>> columns
>>> as character or write.xlsx writes them as character where they are not
>>> characters. I have 12 columns (2 character and 10 numbers). From 10
>>> number
>>> columns, with float and integer numbers, only 3 of them are recognized
>>> correctly. I was wondering how can I define the the column format for
>>> read.xlsx - write.xlsx.
>>>
>>> Here comes a simple example of my code:
>>>
>>> ifn1 <- "A.xlsx"
>>> dat1  <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>>>
>>> ifn2 <- "F.xlsx"
>>> dat2  <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>>>
>>> write.xlsx(dat1,  file="AF.xlsx", sheetName="A",   showNA=FALSE,
>>> row.names=FALSE, append=FALSE)
>>> write.xlsx(dat2,  file="AF.xlsx", sheetName="F",   showNA=FALSE,
>>> row.names=FALSE, append= TRUE)
>>>
>>> Thanks in advance!
>>> Mohsen
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Thu Jan 14 20:51:58 2016
From: jholtman at gmail.com (jim holtman)
Date: Thu, 14 Jan 2016 14:51:58 -0500
Subject: [R] read.xlsx - write.xlsx: reading/writing numbers as character
In-Reply-To: <CADs3iXnux3ZDOn0c3VD14n3oW4SVUM5WmYYBptm01xLjf1bFOg@mail.gmail.com>
References: <wu2yr8ggciqwr86bpal4oemf.1452791789815@email.android.com>
	<CADs3iXnux3ZDOn0c3VD14n3oW4SVUM5WmYYBptm01xLjf1bFOg@mail.gmail.com>
Message-ID: <CAAxdm-6YMe2KnZuL9pfMJY2+uVAM3Vtq86Zu_F+7-pLfWfMNCw@mail.gmail.com>

That is what usually caused me problems in the past.  Excel does not have
the requirement that a column contain the same 'mode' of a variable.  You
can mix numeric and character in the same column and things will work fine
in Excel; conversion to a data.frame does require the same mode in a
column.?


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jan 14, 2016 at 2:30 PM, Mohsen Jafarikia <jafarikia at gmail.com>
wrote:

> Hi Jim,
>
> I found where the problem is. I had some characters in my list.
>
> Thanks very much for your attention and help!
>
> Mohsen
>
> On Thu, Jan 14, 2016 at 12:17 PM, Jim Holtman <jholtman at gmail.com> wrote:
>
>> Write it out as a csv from excel and see what the data looks like.  That
>> may help  in seeing what the problem is.
>>
>>
>> Sent from my Verizon Wireless 4G LTE Smartphone
>>
>>
>> -------- Original message --------
>> From: Mohsen Jafarikia
>> Date:01/14/2016 11:36 (GMT-05:00)
>> To: jim holtman
>> Cc: r-help
>> Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
>> character
>>
>> Thanks for the comment Jim. There is no blank cell. All have numbers.
>>
>> Mohsen
>>
>> On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:
>>
>>> Take a look at the data coming in since you may have something that
>>> looks like characters (maybe 'blanks').  What you think is numeric in EXCEL
>>> might not be.?
>>>
>>>
>>> Jim Holtman
>>> Data Munger Guru
>>>
>>> What is the problem that you are trying to solve?
>>> Tell me what you want to do, not how you want to do it.
>>>
>>> On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
>>> wrote:
>>>
>>>> Hello:
>>>>
>>>> I am reading some excel files (each with one sheet) and trying to write
>>>> them all in one file. I am not sure if read.xlsx reads some of the
>>>> columns
>>>> as character or write.xlsx writes them as character where they are not
>>>> characters. I have 12 columns (2 character and 10 numbers). From 10
>>>> number
>>>> columns, with float and integer numbers, only 3 of them are recognized
>>>> correctly. I was wondering how can I define the the column format for
>>>> read.xlsx - write.xlsx.
>>>>
>>>> Here comes a simple example of my code:
>>>>
>>>> ifn1 <- "A.xlsx"
>>>> dat1  <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>>>>
>>>> ifn2 <- "F.xlsx"
>>>> dat2  <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>>>>
>>>> write.xlsx(dat1,  file="AF.xlsx", sheetName="A",   showNA=FALSE,
>>>> row.names=FALSE, append=FALSE)
>>>> write.xlsx(dat2,  file="AF.xlsx", sheetName="F",   showNA=FALSE,
>>>> row.names=FALSE, append= TRUE)
>>>>
>>>> Thanks in advance!
>>>> Mohsen
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Thu Jan 14 22:36:00 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Thu, 14 Jan 2016 22:36:00 +0100
Subject: [R] Updating a Time Series After Forecast()
Message-ID: <20160114213600.GB1913@localhost.localdomain>

Dear All,
Perhaps I am drowning in a cup of water, since I am positive that the
answer will be a one-liner.
Consider the following short script


########################################################
library(forecast)

ts2<-structure(c(339130, 356462, 363234, 378179, 367864, 378337, 392157,
402153, 376361, 392204, 403483, 414034, 391967, 406067, 419464,
434913, 410102, 424795, 437073, 448827, 415569, 430561, 444719,
455764, 419892, 444190, 454648, 466312, 439922, 448963, 465153,
475621, 445502, 457198, 473573, 485764, 463895, 470274, 484390,
490678, 478003, 483570, 499141, 509216, 481395, 492345, 511184,
513420, 483757, 490884, 514966, 515457, 497614, 510139, 523467,
526406, 499784, 519033, 532009, 531260, 521539, 532590, 553118,
557725, 548321, 556832, 578087, 578120, 566116, 580571, 587993,
569985, 534326, 539641, 564824, 568445, 558614, 570192, 594584,
598305, 593769, 598278, 620147, 615884, 611033, 609304, 630458,
624325, 614356, 627192, 649324, 645988, 642965, 645125, 669471,
665529, 664248, 669670, 694719), na.action = structure(1:64, class =
"omit"), .Tsp = c(1991,
2015.5, 4), class = "ts")

fit2 <- auto.arima(ts2, approximation=FALSE,trace=FALSE)

pred2 <- forecast(fit2, h=2)

#######################################################

So, I have an original quarterly time series ts2 and a forecast for 2
quarters pred2.

I would like to combine ts2 and pred2 (just the prediction) into a new
time series (in other words, just stretch a bit ts2).
How can I do that?
Many thanks

Lorenzo


From moscoe at wisc.edu  Thu Jan 14 22:34:40 2016
From: moscoe at wisc.edu (Lauren Moscoe)
Date: Thu, 14 Jan 2016 21:34:40 +0000
Subject: [R] Tukey and extracting letters in multcomp
Message-ID: <CY1PR0601MB143762017C05ECDDED789BE4B6CC0@CY1PR0601MB1437.namprd06.prod.outlook.com>

Hello,


I have the following model:


model <- lmer (Y  ~ gen + (1|env) + (1|gen:env))


I would like to use Tukey's method to identify significant pairwise differences among levels of the factor gen, which has 18 levels. (Env has 3 levels.)


I have been trying to do this using glht and cld in the package multcomp.


This is what I ran, with notes about the output.


tukey.gen <- glht(model, linfct=mcp(gen = "Tukey))


summary(tukey.gen)

# This resulted in 46 warnings that said, "In RET$pfunction("adjusted", ...) : Completion with error > abseps."


cld(tukey.gen, level=0.05)

# This was my attempt to extract letters to represent Tukey results. I did get letters for each level of gen, but they came alongside the same warning as above, repeated over 50 times.


Why am I getting these warnings? Should I not use the results of cld, given these warnings? How can I avoid the warnings?


I would be very grateful for any advice for how to successfully use Tukey's method for gen in this model, as well as for how to extract letters to denote significant differences. Either suggestions for adjusting my code or for a new approach entirely would be fine.


Thank you,


Lauren


PhD Candidate

Department of Botany

University of Wisconsin-Madison


	[[alternative HTML version deleted]]


From ajain at umn.edu  Thu Jan 14 23:45:44 2016
From: ajain at umn.edu (AASHISH JAIN)
Date: Thu, 14 Jan 2016 16:45:44 -0600
Subject: [R] Problem with rJava
Message-ID: <121B32DB-615E-4148-B8BE-5D6E47C8624D@umn.edu>

Hello,

I am using an R package called Rknots, which uses rJava (and others like rSymPy, rjson, rJython) and I am getting some error due to rJava. When I run my R code, the execution gets halted with the following error:

Error in .jcheck() : No running JVM detected. Maybe .jinit() would help. (found this line in rjava.c)
Calls: computeInvariant ... sympy -> $ -> $ -> hasField -> .jcall -> .jcheck -> .Call (this line comes from Rknots package when the function computeInvariant is called)
Execution halted

Note that since I could not install these packages on root level, I installed them locally on unix OS. Since the other Java and R software appears to be working, it seems like the error is specific to the rJava package. Possibly rJava makes some assumptions about the Java installation (perhaps its location) that cause it to be confused. I would like to know if a non-root Java installation would cause problems. 

FYI, here are the versions of different packages that I currently have:
1. rSymPy_0.2-1.1.tar
2. rJava_0.9-8.tar
3. rjson_0.2.15.tar.gz
4. rJython_0.0-4.tar
5. Rknots_1.2.1.tar

I would really appreciate any help. 

I want to apologize if this forum is not a right place to discuss about rJava.

Thanks!
Aashish Jain
Postdoctoral Associate
Department of Chemical Engineering and Material Science
151 Amundson Hall
421 Washington Ave SE
Minneapolis, MN 55455 USA
Ph: +1 612-806-7154


From profjcnash at gmail.com  Fri Jan 15 01:24:51 2016
From: profjcnash at gmail.com (ProfJCNash)
Date: Thu, 14 Jan 2016 19:24:51 -0500
Subject: [R] Problem with rJava
In-Reply-To: <121B32DB-615E-4148-B8BE-5D6E47C8624D@umn.edu>
References: <121B32DB-615E-4148-B8BE-5D6E47C8624D@umn.edu>
Message-ID: <56983C53.4060102@gmail.com>

Your post does not have the requested session information that will tell
us your computing environment, nor the version of R.

However, I'm experiencing at least a related problem, as this morning I
updated R (in Linux Mind Rafaela 17.2, so I get an operating system
notice to update via the package manager). Afterwards I ran
update.packages() but could not get rJava to carry out the update,
though other packages did complete. Possibly there is some mismatch
between rJava and R 3.2.3 and/or gcj. I'm a bit surprised this hasn't
surfaced before, as I'm a "slow updater" and 3.2.3 is over a month old now.

Below is the output from the update -- AFTER I ran "R CMD javareconf" as
root -- along with the session information, which shows Ubuntu rather
than the derivative Linux Mint.

Cheers, JN

> update.packages()
rJava :
 Version 0.9-6 installed in /usr/lib/R/site-library
 Version 0.9-8 available at https://rweb.crmda.ku.edu/cran
Update (y/N/c)?  y
trying URL 'https://rweb.crmda.ku.edu/cran/src/contrib/rJava_0.9-8.tar.gz'
Content type 'application/x-gzip' length 656615 bytes (641 KB)
==================================================
downloaded 641 KB

* installing *source* package ?rJava? ...
** package ?rJava? successfully unpacked and MD5 sums checked
checking for gcc... gcc -std=gnu99
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables...
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc -std=gnu99 accepts -g... yes
checking for gcc -std=gnu99 option to accept ISO C89... none needed
checking how to run the C preprocessor... gcc -std=gnu99 -E
checking for grep that handles long lines and -e... /bin/grep
checking for egrep... /bin/grep -E
checking for ANSI C header files... yes
checking for sys/wait.h that is POSIX.1 compatible... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking for string.h... (cached) yes
checking sys/time.h usability... yes
checking sys/time.h presence... yes
checking for sys/time.h... yes
checking for unistd.h... (cached) yes
checking for an ANSI C-conforming const... yes
checking whether time.h and sys/time.h may both be included... yes
configure: checking whether gcc -std=gnu99 supports static inline...
yes
checking whether setjmp.h is POSIX.1 compatible... yes
checking whether sigsetjmp is declared... yes
checking whether siglongjmp is declared... yes
checking Java support in R... present:
interpreter : '/usr/lib/jvm/default-java/jre/bin/java'
archiver    : '/usr/bin/jar'
compiler    : '/usr/bin/javac'
header prep.: '/usr/bin/javah'
cpp flags   : ''
java libs   : '-L/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server
-ljvm'
configure: error: One or more Java configuration variables are not set.
Make sure R is configured with full Java support (including JDK). Run
R CMD javareconf
as root to add Java support to R.

If you don't have root privileges, run
R CMD javareconf -e
to set all Java-related variables and then install rJava.

ERROR: configuration failed for package ?rJava?
* removing ?/usr/lib/R/site-library/rJava?
* restoring previous ?/usr/lib/R/site-library/rJava?

The downloaded source packages are in
	?/tmp/RtmpXCs91E/downloaded_packages?
Warning message:
In install.packages(update[instlib == l, "Package"], l, contriburl =
contriburl,  :
  installation of package ?rJava? had non-zero exit status
>
> sessionInfo()
R version 3.2.3 (2015-12-10)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.3 LTS

locale:
 [1] LC_CTYPE=en_CA.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_CA.UTF-8        LC_COLLATE=en_CA.UTF-8
 [5] LC_MONETARY=en_CA.UTF-8    LC_MESSAGES=en_CA.UTF-8
 [7] LC_PAPER=en_CA.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_3.2.3 tcltk_3.2.3
>



On 16-01-14 05:45 PM, AASHISH JAIN wrote:
> Hello,
> 
> I am using an R package called Rknots, which uses rJava (and others like rSymPy, rjson, rJython) and I am getting some error due to rJava. When I run my R code, the execution gets halted with the following error:
> 
> Error in .jcheck() : No running JVM detected. Maybe .jinit() would help. (found this line in rjava.c)
> Calls: computeInvariant ... sympy -> $ -> $ -> hasField -> .jcall -> .jcheck -> .Call (this line comes from Rknots package when the function computeInvariant is called)
> Execution halted
> 
> Note that since I could not install these packages on root level, I installed them locally on unix OS. Since the other Java and R software appears to be working, it seems like the error is specific to the rJava package. Possibly rJava makes some assumptions about the Java installation (perhaps its location) that cause it to be confused. I would like to know if a non-root Java installation would cause problems. 
> 
> FYI, here are the versions of different packages that I currently have:
> 1. rSymPy_0.2-1.1.tar
> 2. rJava_0.9-8.tar
> 3. rjson_0.2.15.tar.gz
> 4. rJython_0.0-4.tar
> 5. Rknots_1.2.1.tar
> 
> I would really appreciate any help. 
> 
> I want to apologize if this forum is not a right place to discuss about rJava.
> 
> Thanks!
> Aashish Jain
> Postdoctoral Associate
> Department of Chemical Engineering and Material Science
> 151 Amundson Hall
> 421 Washington Ave SE
> Minneapolis, MN 55455 USA
> Ph: +1 612-806-7154
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From guoxiongj at yahoo.com  Fri Jan 15 05:18:35 2016
From: guoxiongj at yahoo.com (guoxiong)
Date: Thu, 14 Jan 2016 23:18:35 -0500
Subject: [R] Multiplication of high dimensional array
Message-ID: <D04350EC-78BA-4353-B399-8F55603AAAD8@yahoo.com>

In one of my applications, I need to perform following task in R:
 
    
    svector <- array(0, dim=c(5, 100000, 360), dimnames=list(c('s1','s2','s3','s4','s5'), NULL, NULL)))
 
    
    tmatrix <- array(0, dim=c(100000, 360, 5, 5), dimnames=list(NULL, NULL, c('s1','s2','s3','s4','s5'), c('s1','s2','s3','s4','s5'))))
 
    #My algorithms compute all the elements in tmatrix  (transition matrix among states from time t to t+1, for all entities represented by l index)
 
    #do transition for all l for t=1 to 359
    for (l in 1:100000)
    {
       for(t in 1:359)
       {
          svector [, l,t+1] <- tmatrix[l,t,,] %*% svector [,l,t]
      }
   }
 
The double loops make computation slow. I have been trying to see I can treat the svector and tmatrix as tensors, and use mul.tensor in tensorR to vectorize the computation, but so far I ways get one message or another indicating my incorrect usage. Can tensorR or any other package be used here to simply the calculation? If it can, would you kindly give some sample code
 
Thank you in advance!


From miaojpm at gmail.com  Fri Jan 15 06:26:49 2016
From: miaojpm at gmail.com (jpm miao)
Date: Thu, 14 Jan 2016 21:26:49 -0800
Subject: [R] How can we let "multiplot.R" return a plot?
Message-ID: <CABcx46CjrFiZ0-anXMZ32QP84g2Q05XED9S1b1iGyTu0v4NDsA@mail.gmail.com>

Hi,

   The function "ggplot" does plot and return a plot. For example, we can
write:

y = ggplot(.. .....)  Then y is  a plot.

   How can we modify the multiplot function so that it can also return a
plot?  Multiplot.R is here:

   http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/,
and the code is as below.

   Any advice on returning a multi-ggplot would help!

   Thanks,

Miao

# Multiple plot function # # ggplot objects can be passed in ..., or to
plotlist (as a list of ggplot objects) # - cols: Number of columns in layout #
- layout: A matrix specifying the layout. If present, 'cols' is ignored. # #
If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), #
then plot 1 will go in the upper left, 2 will go in the upper right, and #
3 will go all the way across the bottom. # multiplot <- function(...,
plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list
from the ... arguments and plotlist plots <- c(list(...), plotlist)
numPlots = length(plots) # If layout is NULL, then use 'cols' to determine
layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of
plots # nrow: Number of rows needed, calculated from # of cols layout <-
matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow =
ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { #
Set up the page grid.newpage() pushViewport(viewport(layout =
grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct
location for (i in 1:numPlots) { # Get the i,j matrix positions of the
regions that contain this subplot matchidx <- as.data.frame(which(layout ==
i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row =
matchidx$row, layout.pos.col = matchidx$col)) } } }

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jan 15 06:43:20 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 14 Jan 2016 21:43:20 -0800
Subject: [R] How can we let "multiplot.R" return a plot?
In-Reply-To: <CABcx46CjrFiZ0-anXMZ32QP84g2Q05XED9S1b1iGyTu0v4NDsA@mail.gmail.com>
References: <CABcx46CjrFiZ0-anXMZ32QP84g2Q05XED9S1b1iGyTu0v4NDsA@mail.gmail.com>
Message-ID: <3D69E829-76AC-402B-A82B-DE249052C601@comcast.net>


> On Jan 14, 2016, at 9:26 PM, jpm miao <miaojpm at gmail.com> wrote:
> 
> Hi,
> 
>   The function "ggplot" does plot and return a plot. For example, we can
> write:
> 
> y = ggplot(.. .....)  Then y is  a plot.
> 
>   How can we modify the multiplot function so that it can also return a
> plot?  Multiplot.R is here:
> 
>   http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/,
> and the code is as below.
> 
>   Any advice on returning a multi-ggplot would help!
> 
>   Thanks,
> 
> Miao
> 
> # Multiple plot function # # ggplot objects can be passed in ..., or to
> plotlist (as a list of ggplot objects) # - cols: Number of columns in layout #
> - layout: A matrix specifying the layout. If present, 'cols' is ignored. # #
> If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), #
> then plot 1 will go in the upper left, 2 will go in the upper right, and #
> 3 will go all the way across the bottom. # multiplot <- function(...,
> plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list
> from the ... arguments and plotlist plots <- c(list(...), plotlist)
> numPlots = length(plots) # If layout is NULL, then use 'cols' to determine
> layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of
> plots # nrow: Number of rows needed, calculated from # of cols layout <-
> matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow =
> ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { #
> Set up the page grid.newpage() pushViewport(viewport(layout =
> grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct
> location for (i in 1:numPlots) { # Get the i,j matrix positions of the
> regions that contain this subplot matchidx <- as.data.frame(which(layout ==
> i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row =
> matchidx$row, layout.pos.col = matchidx$col)) } } }
> 
> 	[[alternative HTML version deleted]]

What part of NO HTML don't you understand????

Read the .... posting guide:
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Fri Jan 15 07:24:11 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 15 Jan 2016 17:24:11 +1100
Subject: [R] How can we let "multiplot.R" return a plot?
In-Reply-To: <CABcx46CjrFiZ0-anXMZ32QP84g2Q05XED9S1b1iGyTu0v4NDsA@mail.gmail.com>
References: <CABcx46CjrFiZ0-anXMZ32QP84g2Q05XED9S1b1iGyTu0v4NDsA@mail.gmail.com>
Message-ID: <CA+8X3fUcLEthU8DsdGnUrCDBanJYeQFhtnSz_T-WQPj0Pqk9kw@mail.gmail.com>

Hi Miao,
If I understand your question correctly, you want to get a return value
from the "multiplot" function that you have copied into your message. You
could simply add:

return(plotlist)

just before the final right brace in the function and it would return the
list of plots that you have created. However, as you already have this, I
think you probably want to get a single plot object that has all the
information in the original plotlist. This doesn't seem possible to me as I
don't think that the ggplot objects can be merged. I may be mistaken, so I
will defer to anyone more knowledgeable.

Jim

On Fri, Jan 15, 2016 at 4:26 PM, jpm miao <miaojpm at gmail.com> wrote:

> Hi,
>
>    The function "ggplot" does plot and return a plot. For example, we can
> write:
>
> y = ggplot(.. .....)  Then y is  a plot.
>
>    How can we modify the multiplot function so that it can also return a
> plot?  Multiplot.R is here:
>
>    http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
> ,
> and the code is as below.
>
>    Any advice on returning a multi-ggplot would help!
>
>    Thanks,
>
> Miao
>
> # Multiple plot function # # ggplot objects can be passed in ..., or to
> plotlist (as a list of ggplot objects) # - cols: Number of columns in
> layout #
> - layout: A matrix specifying the layout. If present, 'cols' is ignored. #
> #
> If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), #
> then plot 1 will go in the upper left, 2 will go in the upper right, and #
> 3 will go all the way across the bottom. # multiplot <- function(...,
> plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list
> from the ... arguments and plotlist plots <- c(list(...), plotlist)
> numPlots = length(plots) # If layout is NULL, then use 'cols' to determine
> layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of
> plots # nrow: Number of rows needed, calculated from # of cols layout <-
> matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow =
> ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { #
> Set up the page grid.newpage() pushViewport(viewport(layout =
> grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct
> location for (i in 1:numPlots) { # Get the i,j matrix positions of the
> regions that contain this subplot matchidx <- as.data.frame(which(layout ==
> i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row =
> matchidx$row, layout.pos.col = matchidx$col)) } } }
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From peter.crowther at melandra.com  Fri Jan 15 11:41:18 2016
From: peter.crowther at melandra.com (Peter Crowther)
Date: Fri, 15 Jan 2016 10:41:18 +0000
Subject: [R] R 3.2.3 on Windows 8.1 with interface scaling: how do I produce
 metafiles that fill the whole canvas?
Message-ID: <CALhdq6uebO=zDmUSodhO-qbnJeVSBKYb9LW0D2HyvC1ZGkHBcg@mail.gmail.com>

To reproduce (requires Windows):

1) Set your desktop scaling to 100%;
2) run the script below;
3) observe that the chart in scaled-example.wmf takes up the whole canvas.
4) Set your desktop scaling to greater than 100% (try 150% or 200%);
5) again run the script below;
6) observe that the chart in scaled-example.wmf takes up less than the
whole canvas.

Increasingly, we're seeing users run R on machines with very
high-resolution screens and scaled displays.  How can we correct for
this effect within R?

Cheers,

- Peter

Script:

-- start --
egfr <- c(222.6,176.4)
outcome <- data.frame(egfr)
hgb <- c(141,134)
predictors <- data.frame(hgb)
title <- "eGFR vs. Hgb"
ylab <- "eGFR"
xlab <- "Hgb"
gr1<-file.path(path.expand('~'),"scaled-example.wmf")
win.metafile(gr1)
plot(predictors[[1]],outcome[[1]],main=title,ylab=ylab,xlab=xlab)
dev.off()
-- end --


From bhh at xs4all.nl  Fri Jan 15 13:02:58 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 15 Jan 2016 13:02:58 +0100
Subject: [R] Updating a Time Series After Forecast()
In-Reply-To: <20160114213600.GB1913@localhost.localdomain>
References: <20160114213600.GB1913@localhost.localdomain>
Message-ID: <AC8189E0-4F20-479E-A75F-65A5CEE5F150@xs4all.nl>


> On 14 Jan 2016, at 22:36, Lorenzo Isella <lorenzo.isella at gmail.com> wrote:
> 
> Dear All,
> Perhaps I am drowning in a cup of water, since I am positive that the
> answer will be a one-liner.
> Consider the following short script
> 
> 
> ########################################################
> library(forecast)
> 
> ts2<-structure(c(339130, 356462, 363234, 378179, 367864, 378337, 392157,
> 402153, 376361, 392204, 403483, 414034, 391967, 406067, 419464,
> 434913, 410102, 424795, 437073, 448827, 415569, 430561, 444719,
> 455764, 419892, 444190, 454648, 466312, 439922, 448963, 465153,
> 475621, 445502, 457198, 473573, 485764, 463895, 470274, 484390,
> 490678, 478003, 483570, 499141, 509216, 481395, 492345, 511184,
> 513420, 483757, 490884, 514966, 515457, 497614, 510139, 523467,
> 526406, 499784, 519033, 532009, 531260, 521539, 532590, 553118,
> 557725, 548321, 556832, 578087, 578120, 566116, 580571, 587993,
> 569985, 534326, 539641, 564824, 568445, 558614, 570192, 594584,
> 598305, 593769, 598278, 620147, 615884, 611033, 609304, 630458,
> 624325, 614356, 627192, 649324, 645988, 642965, 645125, 669471,
> 665529, 664248, 669670, 694719), na.action = structure(1:64, class =
> "omit"), .Tsp = c(1991,
> 2015.5, 4), class = "ts")
> 
> fit2 <- auto.arima(ts2, approximation=FALSE,trace=FALSE)
> 
> pred2 <- forecast(fit2, h=2)
> 
> #######################################################
> 
> So, I have an original quarterly time series ts2 and a forecast for 2
> quarters pred2.
> 
> I would like to combine ts2 and pred2 (just the prediction) into a new
> time series (in other words, just stretch a bit ts2).
> How can I do that?

A possible way is this

ts3 <- ts(c(ts2,pred2$mean),start=start(ts2),frequency=frequency(ts2))

Most  likely there are more ways of getting what you want.

Berend

> Many thanks
> 
> Lorenzo
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From b.h.mevik at usit.uio.no  Fri Jan 15 13:33:10 2016
From: b.h.mevik at usit.uio.no (=?utf-8?Q?Bj=C3=B8rn-Helge_Mevik?=)
Date: Fri, 15 Jan 2016 13:33:10 +0100
Subject: [R] Problems with data structure when using plsr() from package
	pls
In-Reply-To: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
	(CG Pettersson's message of "Wed, 13 Jan 2016 19:02:25 +0000")
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
Message-ID: <s3s37tzxbsp.fsf@varelg.uio.no>

CG Pettersson <cg.pettersson at lantmannen.com> writes:

>> frame1 <- data.frame(gushVM, I(n96))

[...]

>> pls1 <- plsr(gushVM ~ n96, data = frame1)
> Error in model.frame.default(formula = gushVM ~ n96, data = frame1) :
>   invalid type (list) for variable 'n96'

As far as I can remember, you get this error if the n96 object was a
data.frame instead of a matrix.  Can you check with, e.g.,

> class(n96)

If it says "data.frame", try using I(as.matrix(n96)).

-- 
Regards,
Bj?rn-Helge Mevik


From b.h.mevik at usit.uio.no  Fri Jan 15 13:37:30 2016
From: b.h.mevik at usit.uio.no (=?utf-8?Q?Bj=C3=B8rn-Helge_Mevik?=)
Date: Fri, 15 Jan 2016 13:37:30 +0100
Subject: [R] Problems with data structure when using plsr() from package
	pls
In-Reply-To: <F8D1CA9A-6B00-4BB1-B00F-4C6234624C4B@dcn.davis.ca.us> (Jeff
	Newmiller's message of "Wed, 13 Jan 2016 20:16:27 -0800")
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
	<F8D1CA9A-6B00-4BB1-B00F-4C6234624C4B@dcn.davis.ca.us>
Message-ID: <s3sy4brvx11.fsf@varelg.uio.no>

Jeff Newmiller <jdnewmil at dcn.davis.ca.us> writes:

> Using I() in the data.frame seems ill-advised to me. You complain about 96
> variables but from reading your explanation that seems to be what your data
> are.

In PSLR, it is common to regress a variable against matrices with very
many coloumns, often several thousands.  Using a data frame with one
predictor variable for each coloumn is going to make the formula
handling very slow.  And if you have several such predictor matrices, it
is very practical to keep them as single variables in the data frame, so
you easily can select/deselect which groups of variables you want in the
model.

-- 
Regards,
Bj?rn-Helge Mevik


From sarah.goslee at gmail.com  Fri Jan 15 13:55:52 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 15 Jan 2016 07:55:52 -0500
Subject: [R] Problems with data structure when using plsr() from package
	pls
In-Reply-To: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
Message-ID: <CAM_vju=QwHPrjFgpv7+DDbqJf2AZ5z7OKZGKGG-oS7wjru9g+g@mail.gmail.com>

Backing up a step:

On Wednesday, January 13, 2016, CG Pettersson <cg.pettersson at lantmannen.com>
wrote:

> R version 3.2.3, W7 64bit.
>
> Dear all!
>
> I am trying to make pls-regression using plsr() from package pls, with
> Mevik & Wehrens (2007) as tutorial and the datasets from the package.
> Everything works real nice as long as I use the supplied datasets, but I
> don?t understand how to prepare my own data.
> This is what I have done:
>
> > frame1 <- data.frame(gushVM, I(n96))


Which ISN'T what the example you're following did. You didn't name the
construct.

frame1 <- data.frame(gushVM, n96 = I(n96))

so R can't find anything named n96 within frame1 because it's probably
named some variant on I(n96).

str(frame1) would have told you this.

Sarah



>
> Where gushVM is a vector with fifteen reference analysis values of a
> quality problem in grain and n96 is a matrix with fifteen rows and 96
> columns from an electronic nose. I try to copy the methods as in 3.2 in
> Mevik & Wehrens, and want to keep n96 as one variable to avoid addressing
> 96 different variables in the plsr call. If I don?t use I() in the call I
> get 96 variables instead.
> Looking at the dataframe by summary(frame1) get a return quite like
> summary(gasoline) from the package (not shown here).
> But when I try to use plsr() with my own data it doesn?t work due to an
> error in the data structure:
>
> > pls1 <- plsr(gushVM ~ n96, data = frame1)
> Error in model.frame.default(formula = gushVM ~ n96, data = frame1) :
>   invalid type (list) for variable 'n96'
> >
> So, n96 has turned into a list, and that is a problem. If gushVM is a
> vector (one variable) och a matrix (five variables) does not seem to change
> anything, managing n96 is the problem
> I have tried all alternative ways of creating a proper data frame
> suggested in the article with exactly the same result.
> I have tried the documentation for data.frame() but I probably don?t
> understand what it says.
>
> What should I do to change "n96" into something better than "list"?
>
> Thanks
> /CG
>
> Med v?nlig h?lsning/Best regards
> CG Pettersson
> Scientific Project Manager, PhD
> ______________________
> Lantm?nnen Corporate R&D
> Phone:  +46 10 556 19 85
> Mobile: + 46 70 330 66 85
> Email: cg.pettersson at lantmannen.com <javascript:;><mailto:
> cg.pettersson at lantmannen.com <javascript:;>>
> Visiting Address: S:t G?ransgatan 160 A
> Address: Box 30192, SE-104 25 Stockholm
> Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
> Registered Office: Stockholm
> Before printing, think about the environment
>
>
>         [[alternative HTML version deleted]]
>
>

-- 
Sarah Goslee
http://www.stringpage.com
http://www.sarahgoslee.com
http://www.functionaldiversity.org

	[[alternative HTML version deleted]]


From kroberts012 at gmail.com  Fri Jan 15 13:16:11 2016
From: kroberts012 at gmail.com (Kieran)
Date: Fri, 15 Jan 2016 13:16:11 +0100
Subject: [R] Panel plots for means of cyclical observations
Message-ID: <CAMVnMy2RRGB1e5LJje7P7LTR_aXO7kkT73qntGoQQEjpFGyNMw@mail.gmail.com>

I want to create a panel plot using xyplot of a line graph whose
x-axis is months of the year and y-axis is the average rainfall in a
given month over the 6 years the data spans.

There should be two levels in this panel plot: odd and even months.

Creating this plot without splitting it into levels is quite
straightforward (creating a for loop to compute a vector of averages)
but the approach is not useful if you want to split the plots into
different levels.

Here is the code:

dfmt <- "%d/%m/%Y"
date <- seq(as.Date("01/01/2010", dfmt), as.Date("31/12/2015", dfmt),
    "day")
month <- months(date)
rainfall <- runif(2191, 0, 150)
monthsOfYear <- c("January", "February", "March", "April",
    "May", "June", "July", "August", "September", "October",
    "November", "December")

parity <- match(month, monthsOfYear) %% 2
# even parity = 0, odd parity = 1

z <- data.frame(rainfall, date, month, parity)

The problem with using xyplot( y ~ x | f, data=z .. ) is the x and y I
want to plot are not columns in z but rather some kind of statistical
summary of columns.


From S.Ellison at LGCGroup.com  Fri Jan 15 14:38:35 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Fri, 15 Jan 2016 13:38:35 +0000
Subject: [R] Problems with data structure when using plsr() from
	package	pls
In-Reply-To: <CAM_vju=QwHPrjFgpv7+DDbqJf2AZ5z7OKZGKGG-oS7wjru9g+g@mail.gmail.com>
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
	<CAM_vju=QwHPrjFgpv7+DDbqJf2AZ5z7OKZGKGG-oS7wjru9g+g@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403C998D72F@GBTEDVPEXCMB04.corp.lgc-group.com>


> > I am trying to make pls-regression using plsr() from package pls, with
> > Mevik & Wehrens (2007) as tutorial and the datasets from the package.
> > Everything works real nice as long as I use the supplied datasets, but
> > I don?t understand how to prepare my own data.
> > This is what I have done:
> >
> > > frame1 <- data.frame(gushVM, I(n96))

Reading ?plsr examples and inspecting the data they use, you need to arrange frame1 so that it has the data from n96 included as columns with names of the from "n96.xxx" whre xxx can be numbers, names etc.

If n96 is a data frame, try something like
names(n96) <- paste("n96", 1:96) 
frame1 <- cbind(gushVM, n96)

pls1 <- plsr(gushVM ~ n96, data = frame1)


If n96 is a matrix, 

frame1 <- data.frame(gushVM, n96=n96)

should also give you a data frame with names of the right format.

I() wrapped round a matrix or data frame does nothing like what is needed if you include it in a data frame construction, so either things have changed since the tutorial was written, or the authors were not handling a matrix or data frame with I().

S Ellison






*******************************************************************
This email and any attachments are confidential. Any use, copying or
disclosure other than by the intended recipient is unauthorised. If 
you have received this message in error, please notify the sender 
immediately via +44(0)20 8943 7000 or notify postmaster at lgcgroup.com 
and delete this message and any copies from your computer and network. 
LGC Limited. Registered in England 2991879. 
Registered office: Queens Road, Teddington, Middlesex, TW11 0LY, UK

From jafarikia at gmail.com  Fri Jan 15 15:43:25 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Fri, 15 Jan 2016 09:43:25 -0500
Subject: [R] write.xlsx- writing in a single sheet
Message-ID: <CADs3iXkMy30kzUBqTd2h9Pgtvq5hXVm2tnSiwQEP__ToF1ENHA@mail.gmail.com>

Hello all:

I am having problem writing a few files in a single sheet of excel. It
seems R has problem writing on the same sheet. Maybe there is a command
that I am missing. Here is the code I am using:

library(xlsx)

ifn11 <- "A1.xlsx"
dat11  <- read.xlsx(ifn11, sheetName="A.csv", header = TRUE)

ifn12 <- "A2.xlsx"
dat12  <- read.xlsx(ifn12, sheetName="A.csv", header = TRUE)

ifn13 <- "A3.xlsx"
dat13  <- read.xlsx(ifn13, sheetName="A.csv", header = TRUE)

ifn21 <- "F1.xlsx"
dat21 <- read.xlsx(ifn21, sheetName="F.csv",header = TRUE)

ifn22 <- "F2.xlsx"
dat22  <- read.xlsx(ifn22, sheetName="F.csv",header = TRUE)

ifn23 <- "F3.xlsx"
dat23 <- read.xlsx(ifn23, sheetName="F.csv",header = TRUE)

write.xlsx(dat11,  file="AC.xlsx", sheetName="A",  append=FALSE)
write.xlsx(dat12,  file="AC.xlsx",                             append= TRUE)
write.xlsx(dat13,  file="AC.xlsx",                             append= TRUE)
write.xlsx(dat21,  file="AC.xlsx", sheetName="F",  append= TRUE)
write.xlsx(dat22,  file="AC.xlsx",                             append= TRUE)
write.xlsx(dat23,  file="AC.xlsx",                             append= TRUE)

And here is the error message I am having:

Error in .jcall(wb, "Lorg/apache/poi/ss/usermodel/Sheet;", "createSheet",
 :
  java.lang.IllegalArgumentException: The workbook already contains a sheet
of this name

This error message comes after running the write.xlsx(dat13,
 file="AC.xlsx", showNA=FALSE, row.names=FALSE, append= TRUE) line. Program
creates a sheet named "A" when writes dat11, then creates "sheet1" after
writing dat12 and when tries to write dat13, it gives me error. It seems it
tries to write on "sheet1" which already exists. I would like dat11, dat12
and dat13 will be all written after each other on sheet "A" and dat12,
dat22 and dat23 in sheet "F".

Anybody has any comments please.

Regards,
Mohsen

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Fri Jan 15 16:11:10 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 15 Jan 2016 09:11:10 -0600
Subject: [R] write.xlsx- writing in a single sheet
In-Reply-To: <CADs3iXkMy30kzUBqTd2h9Pgtvq5hXVm2tnSiwQEP__ToF1ENHA@mail.gmail.com>
References: <CADs3iXkMy30kzUBqTd2h9Pgtvq5hXVm2tnSiwQEP__ToF1ENHA@mail.gmail.com>
Message-ID: <64749096-28B2-4009-8C7D-D5FFA02E3E0E@me.com>


> On Jan 15, 2016, at 8:43 AM, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
> 
> Hello all:
> 
> I am having problem writing a few files in a single sheet of excel. It
> seems R has problem writing on the same sheet. Maybe there is a command
> that I am missing. Here is the code I am using:
> 
> library(xlsx)
> 
> ifn11 <- "A1.xlsx"
> dat11  <- read.xlsx(ifn11, sheetName="A.csv", header = TRUE)
> 
> ifn12 <- "A2.xlsx"
> dat12  <- read.xlsx(ifn12, sheetName="A.csv", header = TRUE)
> 
> ifn13 <- "A3.xlsx"
> dat13  <- read.xlsx(ifn13, sheetName="A.csv", header = TRUE)
> 
> ifn21 <- "F1.xlsx"
> dat21 <- read.xlsx(ifn21, sheetName="F.csv",header = TRUE)
> 
> ifn22 <- "F2.xlsx"
> dat22  <- read.xlsx(ifn22, sheetName="F.csv",header = TRUE)
> 
> ifn23 <- "F3.xlsx"
> dat23 <- read.xlsx(ifn23, sheetName="F.csv",header = TRUE)
> 
> write.xlsx(dat11,  file="AC.xlsx", sheetName="A",  append=FALSE)
> write.xlsx(dat12,  file="AC.xlsx",                             append= TRUE)
> write.xlsx(dat13,  file="AC.xlsx",                             append= TRUE)
> write.xlsx(dat21,  file="AC.xlsx", sheetName="F",  append= TRUE)
> write.xlsx(dat22,  file="AC.xlsx",                             append= TRUE)
> write.xlsx(dat23,  file="AC.xlsx",                             append= TRUE)
> 
> And here is the error message I am having:
> 
> Error in .jcall(wb, "Lorg/apache/poi/ss/usermodel/Sheet;", "createSheet",
> :
>  java.lang.IllegalArgumentException: The workbook already contains a sheet
> of this name
> 
> This error message comes after running the write.xlsx(dat13,
> file="AC.xlsx", showNA=FALSE, row.names=FALSE, append= TRUE) line. Program
> creates a sheet named "A" when writes dat11, then creates "sheet1" after
> writing dat12 and when tries to write dat13, it gives me error. It seems it
> tries to write on "sheet1" which already exists. I would like dat11, dat12
> and dat13 will be all written after each other on sheet "A" and dat12,
> dat22 and dat23 in sheet "F".
> 
> Anybody has any comments please.
> 
> Regards,
> Mohsen

Hi,

From a review of the package documentation (hint...hint), the write.xlsx() function can add new worksheets to a new or existing Excel file. write.xlsx() cannot append data to an existing worksheet.

The 'append = TRUE' argument enables you to add a new worksheet to an existing Excel file, as opposed to creating a new Excel file or overwriting an existing Excel file.

It appears that the addDataFrame() function might support the approach of adding the contents of a data frame object to an existing worksheet.

I have not used the xlsx package, but note that the XLConnect package, which I have not used either, also seems to support the ability to append data to an existing worksheet.

I would recommend spending more time reviewing the package documentation.

Regards,

Marc Schwartz


From fransiepansiekevertje at gmail.com  Fri Jan 15 16:31:31 2016
From: fransiepansiekevertje at gmail.com (Frans Marcelissen)
Date: Fri, 15 Jan 2016 16:31:31 +0100
Subject: [R] write.xlsx- writing in a single sheet
In-Reply-To: <CADs3iXkMy30kzUBqTd2h9Pgtvq5hXVm2tnSiwQEP__ToF1ENHA@mail.gmail.com>
References: <CADs3iXkMy30kzUBqTd2h9Pgtvq5hXVm2tnSiwQEP__ToF1ENHA@mail.gmail.com>
Message-ID: <CAFFQM6YxS+3wPg9G_pOEEvyJNAxU9P09GgP1hbqNerqYKJx6jA@mail.gmail.com>

Do you mean that you try to write several dataframes to the same sheet? You
have asked this before, and I think it has been said that that is not
possible with write.xls. I suppose that you still try to write rows of data
of varying length to one sheet. I think that is possible with the lowlevel
functions of xlsx (like createRow, createWorkbook), but not with
write.xlsx. Read the documentation of xlsx for this. I have no experience
with this.

In addition to that: would you please stop sending messages in html? This
has been asked to you before, and in general people are very irritated
about this, and won't answer your questions.
Frans

2016-01-15 15:43 GMT+01:00 Mohsen Jafarikia <jafarikia at gmail.com>:

> Hello all:
>
> I am having problem writing a few files in a single sheet of excel. It
> seems R has problem writing on the same sheet. Maybe there is a command
> that I am missing. Here is the code I am using:
>
> library(xlsx)
>
> ifn11 <- "A1.xlsx"
> dat11  <- read.xlsx(ifn11, sheetName="A.csv", header = TRUE)
>
> ifn12 <- "A2.xlsx"
> dat12  <- read.xlsx(ifn12, sheetName="A.csv", header = TRUE)
>
> ifn13 <- "A3.xlsx"
> dat13  <- read.xlsx(ifn13, sheetName="A.csv", header = TRUE)
>
> ifn21 <- "F1.xlsx"
> dat21 <- read.xlsx(ifn21, sheetName="F.csv",header = TRUE)
>
> ifn22 <- "F2.xlsx"
> dat22  <- read.xlsx(ifn22, sheetName="F.csv",header = TRUE)
>
> ifn23 <- "F3.xlsx"
> dat23 <- read.xlsx(ifn23, sheetName="F.csv",header = TRUE)
>
> write.xlsx(dat11,  file="AC.xlsx", sheetName="A",  append=FALSE)
> write.xlsx(dat12,  file="AC.xlsx",                             append=
> TRUE)
> write.xlsx(dat13,  file="AC.xlsx",                             append=
> TRUE)
> write.xlsx(dat21,  file="AC.xlsx", sheetName="F",  append= TRUE)
> write.xlsx(dat22,  file="AC.xlsx",                             append=
> TRUE)
> write.xlsx(dat23,  file="AC.xlsx",                             append=
> TRUE)
>
> And here is the error message I am having:
>
> Error in .jcall(wb, "Lorg/apache/poi/ss/usermodel/Sheet;", "createSheet",
>  :
>   java.lang.IllegalArgumentException: The workbook already contains a sheet
> of this name
>
> This error message comes after running the write.xlsx(dat13,
>  file="AC.xlsx", showNA=FALSE, row.names=FALSE, append= TRUE) line. Program
> creates a sheet named "A" when writes dat11, then creates "sheet1" after
> writing dat12 and when tries to write dat13, it gives me error. It seems it
> tries to write on "sheet1" which already exists. I would like dat11, dat12
> and dat13 will be all written after each other on sheet "A" and dat12,
> dat22 and dat23 in sheet "F".
>
> Anybody has any comments please.
>
> Regards,
> Mohsen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rni.boh at gmail.com  Fri Jan 15 16:48:44 2016
From: rni.boh at gmail.com (Bob O'Hara)
Date: Fri, 15 Jan 2016 16:48:44 +0100
Subject: [R] Ordinal regression with some categories combined for some data
Message-ID: <CAN-Z0xWF42sioxrAdUphnK6KR9ZvHseEcF+tDND=mophWUHQEg@mail.gmail.com>

Hi!

I've been asked about a problem where I think I can see how to write
the model, but don't know if it's been implemented in R. It's not
something I work on a lot, so I'm hoping someone else can point me to
an answer straight away.

The researcher has been carrying out germination experiments: lost of
seeds are put in several conditions (temperature humidity etc.), and
every few days they are checked to see if they have germinated.
Because the days are discrete I think it makes sense to view this as
an ordinal regression problem (rather than as an interval censored
survival analysis). But what makes this tricky is that there are days
when the researcher only checked some seeds. So for some seeds the
germination might fall into more than one category.

Is there a package in R that can handle this, i.e. do an ordinal
regression where for some observations the categories are interval
censored? Or is it easier to go straight to a full interval-censored
survival analysis?

Bob

-- 
Bob O'Hara

Biodiversity and Climate Research Centre
Senckenberganlage 25
D-60325 Frankfurt am Main,
Germany

Tel: +49 69 798 40226
Mobile: +49 1515 888 5440
WWW:   http://www.bik-f.de/root/index.php?page_id=219
Blog: http://occamstypewriter.org/boboh/
Journal of Negative Results - EEB: www.jnr-eeb.org


From rmh at temple.edu  Fri Jan 15 17:06:34 2016
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 15 Jan 2016 11:06:34 -0500
Subject: [R] Panel plots for means of cyclical observations
In-Reply-To: <CAMVnMy2RRGB1e5LJje7P7LTR_aXO7kkT73qntGoQQEjpFGyNMw@mail.gmail.com>
References: <CAMVnMy2RRGB1e5LJje7P7LTR_aXO7kkT73qntGoQQEjpFGyNMw@mail.gmail.com>
Message-ID: <CAGx1TMBZY++UVxDdxs-PT8Xb9Xh_KtE8wfiVtaLKDdmJ-+7hAA@mail.gmail.com>

## Kieran,

## I think the root problem is that you allowed the levels of month to
be alphabetical.

## continuing with your example
##You need to take control of the levels with a statement like
levels(z$month)
z$month <- factor(z$month, levels=monthsOfYear)
levels(z$month)

## now you can write something like
z$month.parity <- factor(z$month,
levels=unlist(matrix(levels(z$month), 6, 2, byrow=TRUE)))
levels(z$month.parity)
##  [1] "January"   "March"     "May"       "July"      "September" "November"
##  [7] "February"  "April"     "June"      "August"    "October"   "December"
xyplot(rainfall ~ date | month, layout=c(2, 6), data=z)
xyplot(rainfall ~ date | month.parity, layout=c(6, 2), data=z)

## I hope the modifications to deal with your summary measures will be
straightforward.

## Rich


On Fri, Jan 15, 2016 at 7:16 AM, Kieran <kroberts012 at gmail.com> wrote:
> I want to create a panel plot using xyplot of a line graph whose
> x-axis is months of the year and y-axis is the average rainfall in a
> given month over the 6 years the data spans.
>
> There should be two levels in this panel plot: odd and even months.
>
> Creating this plot without splitting it into levels is quite
> straightforward (creating a for loop to compute a vector of averages)
> but the approach is not useful if you want to split the plots into
> different levels.
>
> Here is the code:
>
> dfmt <- "%d/%m/%Y"
> date <- seq(as.Date("01/01/2010", dfmt), as.Date("31/12/2015", dfmt),
>     "day")
> month <- months(date)
> rainfall <- runif(2191, 0, 150)
> monthsOfYear <- c("January", "February", "March", "April",
>     "May", "June", "July", "August", "September", "October",
>     "November", "December")
>
> parity <- match(month, monthsOfYear) %% 2
> # even parity = 0, odd parity = 1
>
> z <- data.frame(rainfall, date, month, parity)
>
> The problem with using xyplot( y ~ x | f, data=z .. ) is the x and y I
> want to plot are not columns in z but rather some kind of statistical
> summary of columns.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From peterenos at ymail.com  Fri Jan 15 15:16:05 2016
From: peterenos at ymail.com (Peter Tuju)
Date: Fri, 15 Jan 2016 14:16:05 +0000 (UTC)
Subject: [R] EXTRACT POINT DATA FROM NETCDF FILE
In-Reply-To: <mailman.5.1452855602.18847.r-help@r-project.org>
References: <mailman.5.1452855602.18847.r-help@r-project.org>
Message-ID: <1106920986.6201043.1452867365383.JavaMail.yahoo@mail.yahoo.com>

Daer R, usersI want to get a point data from netcdf file using the longitude and latitude, 
but have no luck with the code I'm using. Here is the code. Please help!

rm( list = ls() )??? ??? ??? ??? ?????????????????????????????? # Clearing the workspace
setwd( "/run/media/tuju/0767090047/extract_wrf_txt_file" )
library( ncdf4 )?????????????????????????????????????? # Loading the ncdf4 package to read the netcdf data
library(ncdf)
inp_file <- open.ncdf( "wrfout_d01_2015-12-30.nc" )????? # Reading the netcdf data
time <- get.var.ncdf( inp_file, "Times" )???????????????? # Extracting the forecasts time

sites <- read.csv("Station_Coordinates_TMA.csv", sep = "\t")
attach(sites)

source( "whereis.R" )
lat = get.var.ncdf(inp_file, "XLAT")? 
lon = get.var.ncdf(inp_file, "XLONG")

lower_left_lon_lat = c( 22, -16 )
upper_right_lon_lat = c( 56, 7 )

ix0 = wherenearest( lower_left_lon_lat[1],? lon )
ix1 = wherenearest( upper_right_lon_lat[1], lon )
iy0 = wherenearest( lower_left_lon_lat[2],? lat )
iy1 = wherenearest( upper_right_lon_lat[2], lat )

countx = ix1 - ix0 + 1
county = iy1 - iy0 + 1

rainc = get.var.ncdf( inp_file, "RAINC", start = c( ix0, iy0, 1 ), count = c( countx, county, 1 ))
?_____________
Peter? E. Tuju
Dar es Salaam
T A N Z A N I A
----------------------
 

      From: "r-help-request at r-project.org" <r-help-request at r-project.org>
 To: r-help at r-project.org 
 Sent: Friday, January 15, 2016 2:00 PM
 Subject: R-help Digest, Vol 155, Issue 15
   
Send R-help mailing list submissions to
??? r-help at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
??? https://stat.ethz.ch/mailman/listinfo/r-help
or, via email, send a message with subject or body 'help' to
??? r-help-request at r-project.org

You can reach the person managing the list at
??? r-help-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-help digest..."


Today's Topics:

? 1. Re: printing a data.frame that contains a list-column of S4
? ? ? objects (boB Rudis)
? 2. Overlapping subject-specific histograms (Frank S.)
? 3. Re: Overlapping subject-specific histograms (PIKAL Petr)
? 4. unexpected behaviour of an extended time series (using
? ? ? packages??? spuRs and xts) (Olivier ETERRADOSSI)
? 5. Re: Overlapping subject-specific histograms (Frank S.)
? 6. zyp Vs. kendall package for Time Series Trend (Morteza Firouzi)
? 7. read.xlsx - write.xlsx: reading/writing numbers as character
? ? ? (Mohsen Jafarikia)
? 8. parameter constraints in glm() and Bayesian version
? ? ? (mara.pfleiderer at uni-ulm.de)
? 9. R: layout() affects margin size in subfigures [unexpected
? ? ? behaviour] (Malcolm Perry)
? 10. Re: read.xlsx - write.xlsx: reading/writing numbers as
? ? ? character (jim holtman)
? 11. Re: read.xlsx - write.xlsx: reading/writing numbers as
? ? ? character (Mohsen Jafarikia)
? 12. Re: R: layout() affects margin size in subfigures [unexpected
? ? ? behaviour] (Sarah Goslee)
? 13. Re: read.xlsx - write.xlsx: reading/writing numbers as
? ? ? character (Jim Holtman)
? 14. Re: Problems with data structure when using plsr() from
? ? ? package??? pls (David Winsemius)
? 15. Re: read.xlsx - write.xlsx: reading/writing numbers as
? ? ? character (Mohsen Jafarikia)
? 16. Re: read.xlsx - write.xlsx: reading/writing numbers as
? ? ? character (Mohsen Jafarikia)
? 17. Re: read.xlsx - write.xlsx: reading/writing numbers as
? ? ? character (jim holtman)
? 18. Updating a Time Series After Forecast() (Lorenzo Isella)
? 19. Tukey and extracting letters in multcomp (Lauren Moscoe)
? 20. Problem with rJava (AASHISH JAIN)
? 21. Re: Problem with rJava (ProfJCNash)
? 22. Multiplication of high dimensional array (guoxiong)
? 23. How can we let "multiplot.R" return a plot? (jpm miao)
? 24. Re: How can we let "multiplot.R" return a plot? (David Winsemius)
? 25. Re: How can we let "multiplot.R" return a plot? (Jim Lemon)
? 26. R 3.2.3 on Windows 8.1 with interface scaling: how do I
? ? ? produce metafiles that fill the whole canvas? (Peter Crowther)


----------------------------------------------------------------------

Message: 1
Date: Thu, 14 Jan 2016 06:26:34 -0500
From: boB Rudis <bob at rudis.net>
To: Martin Maechler <maechler at stat.math.ethz.ch>
Cc: r-help mailing list <r-help at r-project.org>, Jenny Bryan
??? <jenny at stat.ubc.ca>
Subject: Re: [R] printing a data.frame that contains a list-column of
??? S4??? objects
Message-ID:
??? <CAJ4QxaMBDbmpPeBz2Wsy4dgpe4ejT8=7gsoXR+aq79njfd3C9A at mail.gmail.com>
Content-Type: text/plain; charset=UTF-8

Martin, I'm pretty sure the use of Matrix here (actually by someone
else than Dr Bryan) was to make an easy, inline, reproducible example.
The actual "ugh" column comes from using git2r. I'm assuming there's
an API call returning some pretty gnarly structures that are getting
shoehorned into a data.frame. That happens more often than I'd like in
modern API calls (really complex/nested JSON being returned).

On Thu, Jan 14, 2016 at 3:34 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> boB Rudis <bob at rudis.net>
>>>>>>? ? on Tue, 12 Jan 2016 13:51:50 -0500 writes:
>
>? ? > I wonder if something like:
>? ? > format.list <- function(x, ...) {
>? ? > rep(class(x[[1]]), length(x))
>? ? > }
>
>? ? > would be sufficient? (prbly needs more 'if's though)
>
> Dear Jenny,
> for a different perspective (and a lot of musings), see inline below
>
>? ? > On Tue, Jan 12, 2016 at 12:15 PM, Jenny Bryan <jenny at stat.ubc.ca> wrote:
>? ? >> Is there a general problem with printing a data.frame when it has a
>? ? >> list-column of S4 objects? Or am I just unlucky in my life choices?
>? ? >>
>? ? >> I ran across this with objects from the git2r package but maintainer
>? ? >> Stefan Widgren points out this example below from Matrix as well. I note
>? ? >> that the offending object can be printed if sent through
>? ? >> dplyr::tbl_df(). I accept that that printing doesn't provide much info
>? ? >> on S4 objects. I'd just like those vars to not prevent data.frame-style
>? ? >> inpsection of the entire object.
>? ? >>
>? ? >> I asked this on stack overflow, where commenter provided the lead to the
>? ? >> workaround below. Is that the best solution?
>? ? >>
>? ? >> library(Matrix)
>? ? >>
>? ? >> m <- new("dgCMatrix")
>? ? >> isS4(m)
>? ? >> #> [1] TRUE
>? ? >> df <- data.frame(id = 1:2)
>? ? >> df$matrices <- list(m, m)
>
> This only works by accident (I think), and fails for
>
>? df <- data.frame(id = 1)
>? df$matrices <- list(m, m)
>
>? ? > df <- data.frame(id = 1)
>? ? > df$matrices <- list(m, m)
>? ? Error in `$<-.data.frame`(`*tmp*`, "matrices", value = list(<S4 object of class "dgCMatrix">,? :
>? ? replacement has 2 rows, data has 1
>? ? >
>
>
>? ? >> df
>? ? >> #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic
>? ? >> #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, : first argument must be atomic
>
> Hmm,
> As 'data.frame' is just an S3 class there is no formal
> definition to go with and in this sense you are of course entitled
> to all expectations. ;-)
> Even though data frames are internally coded as lists, I
> strongly believe data frames should be taught as (and thought of)
>? ? ? ? ? "generalized matrices"
> in the sense that data frames should be thought of n (say) rows
> and p (say) columns.
>
> The help pages? for? data.frame()? and as.data.frame()
> should make it clear that you can *not* put all kinds of entries
> into data frame columns, but I agree the documentation is vague
> and probably has to remain vague,
> because if you provide? as.data.frame()? methods for your class
> you should be able to go quite far.
>
> In addition, the data frame columns need to fulfill properties, e.g.,
> subsetting (aka "indexing") and also subassignment ( df[i,j] <- v )
>
> Now the real "problem" here is that the '$<-' and '[<-'? methods
> for data frames which you call via? df$m <- v? or? df[,co] <- V
> are too "forgiving". They only check that NROW(.) of the new
> entry corresponds to the nrow(<data.frame>).
> Currently they allow very easy construction of illegal data
> frames(*), as in your present case.
>
> --
> *) Yes, it is hard to say when a data.frame is illegal, as there
>? ? is no formal definition
>
> There is more to be said and thought about if you really want
> sparse matrices in a data frame, and as 'Matrix' maintainers,
> I'm quite interested *why* you'd want that, but I won't go there
> now.
>
> One last issue though: The idea of allowing to put 'matrix' or
> 'array' into data frames is that each column of the matrix
> becomes a separate column of the data frame
>
>> data.frame(D = diag(3), M = matrix(1:12, 3,4))
>? D.1 D.2 D.3 M.1 M.2 M.3 M.4
> 1? 1? 0? 0? 1? 4? 7? 10
> 2? 0? 1? 0? 2? 5? 8? 11
> 3? 0? 0? 1? 3? 6? 9? 12
>
> .... and that would be quite inefficient for large sparse matrices.
>
> ---------
>
> Final recommendation as a summary:
>
> If? data.frame(.., .., ..) does not work to put entries into a
> data frame, then don't do it, but rather think about how to make
> data.frame() work with your objects -- namely by ensuring that
> as.data.frame() works .. possibly by providing an
> as.data.frame() method.
>
> Best regards,
> Martin Maechler
>



------------------------------

Message: 2
Date: Thu, 14 Jan 2016 12:47:37 +0100
From: "Frank S." <f_j_rod at hotmail.com>
To: "r-help at r-project.org" <r-help at r-project.org>
Subject: [R] Overlapping subject-specific histograms
Message-ID: <BAY168-W415D1696BC42DE5E907D3FBACC0 at phx.gbl>
Content-Type: text/plain; charset="UTF-8"

Dear R users,

First of all, excuse me if my doubt is very trivial, but so far I haven't been able to solve it.
My question is this: I have a data frame which contains repeated measurements on 4 subjects coded
as "id", and I want to plot, for each subject, not only the corresponding "counts" variable histogram, 
but also overlapping to the right side the corresponding results of "sim" variable (I want to do it in basic
R code, i.e., without any specific R package). I have almost the right code (see the example code below), 
but I can not overlap the "sim" variable.

Thanks in advance for suggestions!!

Frank

data <- data.frame(id =? rep(c(1,3,4,7), c(9,5,3,3)),
? ? count = c(0, 10, 15, 0, 16, 7, 14, 11, 12, 1, 8, 17, 19, 0, 9, 10, 14, 2, 3, 10),
? ? sims =? c(1, 9, 15, 1, 14, 5, 12, 10, 12, 2, 6, 15, 18, 1, 9, 9, 12, 5, 3, 9)) 

# The actual code I have
# ------------------------------
windows(height = 5, width = 5)
par(mfrow = c(2, 2), oma = c(1, 2, 2, 1), mar=c(3, 2, 1, 1), las = 1)? ? 
for(i in 1:length(unique(data$id))){
? kat <- factor(data$id, labels = 1:length(unique(data$id)))
? plot(data$count[kat == i], 
? type = "h", col = 1, lwd = 3, xaxt = "n", xlab ="", main = "",
? xlim = c(1, max(table(data$id))), ylim = c(0, 20))
? axis(1, at = 1:max(table(data$id)))
? mtext( bquote(paste("id = ", .(unique(data$id)[i]))), side = 3, cex = 0.9, line = 0.5)
? tab <- table( as.matrix( data$id ) )
? dist.overlap <- 0.4 # Distance of right overlapping of the "sim" variable
? # points( factor(names(tab)) + dist.overlap, data$sim[kat == i] , type="h", col=2, lw =4)? ##? =======>? Line I can not solve
 }
 ??? ??? ??? ? ??? ??? ? 
??? [[alternative HTML version deleted]]



------------------------------

Message: 3
Date: Thu, 14 Jan 2016 12:03:16 +0000
From: PIKAL Petr <petr.pikal at precheza.cz>
To: "Frank S." <f_j_rod at hotmail.com>, "r-help at r-project.org"
??? <r-help at r-project.org>
Subject: Re: [R] Overlapping subject-specific histograms
Message-ID:
??? <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5009C38 at SRVEXCHMBX.precheza.cz>
Content-Type: text/plain; charset="utf-8"

Hi

change

points( factor(names(tab)) + dist.overlap, data$sim[kat == i] ,
> type="h", col=2, lw =4)

to

points( 1:length(data$count[kat == i]) + dist.overlap, data$sims[kat == i] , type="h", col=2, lw =4)

And do not use html post, your code could be scrammbled.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank
> S.
> Sent: Thursday, January 14, 2016 12:48 PM
> To: r-help at r-project.org
> Subject: [R] Overlapping subject-specific histograms
>
> Dear R users,
>
> First of all, excuse me if my doubt is very trivial, but so far I
> haven't been able to solve it.
> My question is this: I have a data frame which contains repeated
> measurements on 4 subjects coded
> as "id", and I want to plot, for each subject, not only the
> corresponding "counts" variable histogram,
> but also overlapping to the right side the corresponding results of
> "sim" variable (I want to do it in basic
> R code, i.e., without any specific R package). I have almost the right
> code (see the example code below),
> but I can not overlap the "sim" variable.
>
> Thanks in advance for suggestions!!
>
> Frank
>
> data <- data.frame(id =? rep(c(1,3,4,7), c(9,5,3,3)),
>? ? count = c(0, 10, 15, 0, 16, 7, 14, 11, 12, 1, 8, 17, 19, 0, 9, 10,
> 14, 2, 3, 10),
>? ? sims =? c(1, 9, 15, 1, 14, 5, 12, 10, 12, 2, 6, 15, 18, 1, 9, 9,
> 12, 5, 3, 9))
>
> # The actual code I have
> # ------------------------------
> windows(height = 5, width = 5)
> par(mfrow = c(2, 2), oma = c(1, 2, 2, 1), mar=c(3, 2, 1, 1), las = 1)
> for(i in 1:length(unique(data$id))){
>? ? kat <- factor(data$id, labels = 1:length(unique(data$id)))
>? ? plot(data$count[kat == i],
>? ? type = "h", col = 1, lwd = 3, xaxt = "n", xlab ="", main = "",
>? ? xlim = c(1, max(table(data$id))), ylim = c(0, 20))
>? ? axis(1, at = 1:max(table(data$id)))
>? ? mtext( bquote(paste("id = ", .(unique(data$id)[i]))), side = 3, cex
> = 0.9, line = 0.5)
>? ? tab <- table( as.matrix( data$id ) )
>? ? dist.overlap <- 0.4 # Distance of right overlapping of the "sim"
> variable
>? ? # points( factor(names(tab)) + dist.overlap, data$sim[kat == i] ,
> type="h", col=2, lw =4)? ##? =======>? Line I can not solve
>? }
>
>? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

------------------------------

Message: 4
Date: Thu, 14 Jan 2016 16:05:39 +0100 (CET)
From: Olivier ETERRADOSSI <olivier.eterradossi at mines-ales.fr>
To: <r-help at r-project.org>
Subject: [R] unexpected behaviour of an extended time series (using
??? packages??? spuRs and xts)
Message-ID: <00a401d14edc$ffef75b0$ffce6110$@mines-ales.fr>
Content-Type: text/plain; charset="UTF-8"

Hi list,



I thought I knew how to use extended time series (package xts), but I was
wrong? J? ?



While preparing a toy example for something else, using data provided in
R, I run into an unexpected problem and can?t figure by myself what is
happening below, can anyone of you tell ? I searched the archives but
didn?t locate any answer. Probably it?s trivial, so please forgive? :



I?m using :

R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree" / Platform:
x86_64-w64-mingw32/x64 (64-bit)

Packages are updated weekly, sometimes daily.



I take some data from package spuRs :

> library(spuRs)

> data(kew)



I turn the dataframe into time series (by combining each kew[,2:13] one
after each other into a vector, and turning the vector into time series).



One is ts :



>
kew.ts<-ts(data=stock,start=kew$year[1],end=kew$year[length(kew$year)],fre
quency=12)



And the other is xts, it looks fine at first :



> kew.xts<-as.xts(kew.ts)

> periodicity(kew.xts)

Monthly periodicity from janv. 1697 to janv. 1999? # OK

> hist(kew.xts) # OK

> summary(kew.xts)

? ? Index? ? ? ? kew.xts

 Min.? :1697? Min.? :? 0.00

 1st Qu.:1772? 1st Qu.: 29.70

 Median :1848? Median : 47.00

 Mean? :1848? Mean? : 51.14

 3rd Qu.:1924? 3rd Qu.: 67.60

 Max.? :1999? Max.? :189.00? # OK



> gdata::is.what(kew.xts)

[1] "is.array"? ? ? ? "is.atomic"? ? ? "is.double"
"is.index.unique"

[5] "is.matrix"? ? ? "is.numeric"? ? ? "is.object"? ? ? "is.regular"


 [9] "is.time.unique"? "is.unsorted"? ? "is.xts"? ? ? ? ? "is.zoo"
# seems OK





# But now, first try :

> plot(kew.xts)

Error in if (on == "years") { :

? valeur manquante l??E / FALSE est requis? ? # french for ?
missing value where TRUE/FALSE is required ?



# hmmmm, let?s try something else :

> plot(kew.xts['1697-01/1979/']) # OK



> plot(kew.xts['1697-01/1980/'])

Error in if (on == "years") { :

? valeur manquante l??E / FALSE est requis



> plot(kew.xts['1697-01/1979-12/']) # OK



> plot(kew.xts['1697-01/1980-01/'])

Error in if (on == "years") { :

? valeur manquante l??E / FALSE est requis



# but?!? :



> plot(kew.xts['1979-01/1980/']) # OK !!!!!



And so are :

> plot (kew.xts['1978/1980/'])

> plot(kew.xts['1977/1982/'])

> plot(kew.xts['1977-01/1982-12'])? # and so on?



I?m puzzled ! I have probably missed a trivial point? Can someone tell ?



Thanks a lot list, regards, Olivier



--------------------------

Olivier ETERRADOSSI

Ma?e-Assistant, HDR

Ecole des Mines d?Al?(C2MA, site de Pau)

Ing?erie de l'aspect visuel et tactile des mat?aux

P?? Recherche sur les Interactions des Mat?aux avec leur
Environnement ? (RIME)

H?oparc, 2 av. P. Angot, F-64053 PAU CEDEX 9

Tel : 05 59 30 90 35 (direct) - 05 59 30? 54 25 (std)

Fax : 05 59 30 63 68

 <http://www.mines-ales.fr/> http://www.mines-ales.fr

 <http://www.mines-telecom.fr/> http://www.mines-telecom.fr








??? [[alternative HTML version deleted]]



------------------------------

Message: 5
Date: Thu, 14 Jan 2016 16:20:29 +0100
From: "Frank S." <f_j_rod at hotmail.com>
To: PIKAL Petr <petr.pikal at precheza.cz>, "r-help at r-project.org"
??? <r-help at r-project.org>
Subject: Re: [R] Overlapping subject-specific histograms
Message-ID: <BAY168-W5BE014AB0F53728926B0DBACC0 at phx.gbl>
Content-Type: text/plain; charset="UTF-8"

Many thanks Petr!

Best,

Frank

> From: petr.pikal at precheza.cz
> To: f_j_rod at hotmail.com; r-help at r-project.org
> Subject: RE: [R] Overlapping subject-specific histograms
> Date: Thu, 14 Jan 2016 12:03:16 +0000
> 
> Hi
> 
> change
> 
> points( factor(names(tab)) + dist.overlap, data$sim[kat == i] ,
> > type="h", col=2, lw =4)
> 
> to
> 
> points( 1:length(data$count[kat == i]) + dist.overlap, data$sims[kat == i] , type="h", col=2, lw =4)
> 
> And do not use html post, your code could be scrammbled.
> 
> Cheers
> Petr
> 
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank
> > S.
> > Sent: Thursday, January 14, 2016 12:48 PM
> > To: r-help at r-project.org
> > Subject: [R] Overlapping subject-specific histograms
> >
> > Dear R users,
> >
> > First of all, excuse me if my doubt is very trivial, but so far I
> > haven't been able to solve it.
> > My question is this: I have a data frame which contains repeated
> > measurements on 4 subjects coded
> > as "id", and I want to plot, for each subject, not only the
> > corresponding "counts" variable histogram,
> > but also overlapping to the right side the corresponding results of
> > "sim" variable (I want to do it in basic
> > R code, i.e., without any specific R package). I have almost the right
> > code (see the example code below),
> > but I can not overlap the "sim" variable.
> >
> > Thanks in advance for suggestions!!
> >
> > Frank
> >
> > data <- data.frame(id =? rep(c(1,3,4,7), c(9,5,3,3)),
> >? ? count = c(0, 10, 15, 0, 16, 7, 14, 11, 12, 1, 8, 17, 19, 0, 9, 10,
> > 14, 2, 3, 10),
> >? ? sims =? c(1, 9, 15, 1, 14, 5, 12, 10, 12, 2, 6, 15, 18, 1, 9, 9,
> > 12, 5, 3, 9))
> >
> > # The actual code I have
> > # ------------------------------
> > windows(height = 5, width = 5)
> > par(mfrow = c(2, 2), oma = c(1, 2, 2, 1), mar=c(3, 2, 1, 1), las = 1)
> > for(i in 1:length(unique(data$id))){
> >? ? kat <- factor(data$id, labels = 1:length(unique(data$id)))
> >? ? plot(data$count[kat == i],
> >? ? type = "h", col = 1, lwd = 3, xaxt = "n", xlab ="", main = "",
> >? ? xlim = c(1, max(table(data$id))), ylim = c(0, 20))
> >? ? axis(1, at = 1:max(table(data$id)))
> >? ? mtext( bquote(paste("id = ", .(unique(data$id)[i]))), side = 3, cex
> > = 0.9, line = 0.5)
> >? ? tab <- table( as.matrix( data$id ) )
> >? ? dist.overlap <- 0.4 # Distance of right overlapping of the "sim"
> > variable
> >? ? # points( factor(names(tab)) + dist.overlap, data$sim[kat == i] ,
> > type="h", col=2, lw =4)? ##? =======>? Line I can not solve
> >? }
> >
> >? ? ? [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ________________________________
> Tento e-mail a jak?liv k n? p?en?okumenty jsou d?? jsou ur?y pouze jeho adres?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?eprodlen?eho odes?tele. Obsah tohoto emailu i s p?ami a jeho kopie vyma?te ze sv? syst?.
> Nejste-li zam?m adres?m tohoto emailu, nejste opr?? tento email jakkoliv u??t, roz?i?, kop?vat ?zve?vat.
> Odes?tel e-mailu neodpov? za eventu??kodu zp?nou modifikacemi ?zpo?d?m p?u e-mailu.
> 
> V p???e je tento e-mail sou?t?bchodn? jedn?:
> - vyhrazuje si odes?tel pr? ukon? kdykoliv jedn? o uzav?smlouvy, a to z jak?koliv d? i bez uveden??.
> - a obsahuje-li nab?u, je adres?opr??nab?u bezodkladn??ut; Odes?tel tohoto e-mailu (nab?y) vylu?e p??ab?y ze strany p?ce s dodatkem ?odchylkou.
> - trv?des?tel na tom, ?e p??n?mlouva je uzav?teprve v??a?en?shody na v?ech jej? n??itostech.
> - odes?tel tohoto emailu informuje, ?e nen?pr??uzav?t za spole?st ???mlouvy s v?u p??y k tomu byl p?mn?mocn?nebo p?mn?ov?n a takov?ov?n?ebo pln?oc byly adres?vi tohoto emailu p?n?sob?kterou adres?zastupuje, p??eny nebo jejich existence je adres?vi ?osob??zastoupen?n?.
> 
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
 ??? ??? ??? ? ??? ??? ? 
??? [[alternative HTML version deleted]]



------------------------------

Message: 6
Date: Thu, 14 Jan 2016 15:18:54 +0000 (UTC)
From: Morteza Firouzi <mortezafirouzi at yahoo.com>
To: R-help Mailing List <r-help at r-project.org>
Subject: [R] zyp Vs. kendall package for Time Series Trend
Message-ID:
??? <875108057.660916.1452784734814.JavaMail.yahoo at mail.yahoo.com>
Content-Type: text/plain; charset="utf-8"

Dear members,
I need to detect trends in time series. To remove the effect of "Lag-1 serial correlation", it is suggested to use either Yue&Pilon or Zhang method. Both methods are available in "zyp" package. The package uses "kendall" package for trend analysis. ?


Based on Yue&Pilon (2002), if the lag-1 serial correlation is significant, TFPW method will remove the effects of it prior to the?trend test; otherwise trend test will be applied on original time series.?

I've compared the results of a sample time series with non-significant lag-1 serial correlation, using both zyp & kendall packages.?"yuepilon" method in "zyp" gives me the following results:tau: 0.075 & sig: 0.388
while "kendall"?package gives me this:?
tau: 0.109 & sig: 0.216



The question is :?Does "zyp"?change the significance of the trend?in this case as well? Is this a malfunction or did I miss something?
I've checked the script and it is mentioned (ln 65)?:?# Prewhiten the original series
c <- acf(data,lag.max=1,plot=FALSE,na.action=na.pass)$acf[2]


Thank you for your consideration.
Best regards,
Morteza





? <!--#yiv5285967173 P {margin-top:0;margin-bottom:0;}-->
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 1-10-2016 6-09-04 PM.png
Type: image/png
Size: 6799 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160114/54f347d5/attachment-0001.png>

------------------------------

Message: 7
Date: Thu, 14 Jan 2016 11:20:17 -0500
From: Mohsen Jafarikia <jafarikia at gmail.com>
To: <r-help at stat.math.ethz.ch>
Subject: [R] read.xlsx - write.xlsx: reading/writing numbers as
??? character
Message-ID:
??? <CADs3iXmGUYWvxUzkDsxxGRcaroP29bFBPi8nwB16ncxjZ1QbQw at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

Hello:

I am reading some excel files (each with one sheet) and trying to write
them all in one file. I am not sure if read.xlsx reads some of the columns
as character or write.xlsx writes them as character where they are not
characters. I have 12 columns (2 character and 10 numbers). From 10 number
columns, with float and integer numbers, only 3 of them are recognized
correctly. I was wondering how can I define the the column format for
read.xlsx - write.xlsx.

Here comes a simple example of my code:

ifn1 <- "A.xlsx"
dat1? <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)

ifn2 <- "F.xlsx"
dat2? <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)

write.xlsx(dat1,? file="AF.xlsx", sheetName="A",? showNA=FALSE,
row.names=FALSE, append=FALSE)
write.xlsx(dat2,? file="AF.xlsx", sheetName="F",? showNA=FALSE,
row.names=FALSE, append= TRUE)

Thanks in advance!
Mohsen

??? [[alternative HTML version deleted]]



------------------------------

Message: 8
Date: Thu, 14 Jan 2016 13:37:42 +0100
From: mara.pfleiderer at uni-ulm.de
To: r-help at r-project.org
Subject: [R] parameter constraints in glm() and Bayesian version
Message-ID: <20160114133742.rmeptmtu74woc0k0 at imap.uni-ulm.de>
Content-Type: text/plain;??? charset=ISO-8859-1;??? DelSp="Yes";
??? format="flowed"

Hello,

I'm a mathematics student at Ulm University and currently I am working? 
on my bachelor thesis about a Poisson regression model.

For this, I am using the function glm () in R which is working very well.
But still I have two questions to improve my model and I hope that you? 
could help me:

(i) Is there a possibility to set constraints on the regression? 
parameters in glm() or is there another function in R?
Specifically, my paramters should be constrained to be positive as? 
negative parameters wouldn't make sense. How can I do this in R? 
(preferably with glm() or similar functions)?

(ii) Is there a Bayesian version of the glm()-function where I can? 
specify the prior distribution for my regression parameters?

Thanks in advance!
Kind regards,
Mara Pfleiderer



------------------------------

Message: 9
Date: Thu, 14 Jan 2016 13:42:10 +0000
From: Malcolm Perry <mgperry32 at gmail.com>
To: r-help at r-project.org
Subject: [R] R: layout() affects margin size in subfigures [unexpected
??? behaviour]
Message-ID:
??? <CADfHdv6=kuXszvVnEvgd9Kxx18ckit6vEGPsN7a2i+Q6Mfz+0A at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

The absolute margin size of figures in R seems to be affected by the layout
of the plot, which i think is surprising (not sure if it qualifies as a
bug). The following plots have different margins sizes, with the 1x3 plot
margins being smaller (thus giving a larger plot area). This is causing
havoc with a package I am writing to automatically generate composite
figures, since labels are positioned differently depending on the number of
panels.

plot_box <- function() {
? ? ? ? plot(1, 1, type='n', bty='n', xaxt='n', yaxt='n', xlab='', ylab='')
? ? ? ? box(lwd = 6)
? ? ? ? box("figure", lwd=6, col='red')
}

png("margin_test_1.png", width=1000, height=500)
par(oma=c(0,0,0,0))
layout(t(1:2))
par(mar=c(3, 3, 3, 3))
plot_box()
par(mar=c(3, 3, 3, 3))
plot_box()
dev.off()

png("margin_test_2.png", width=1500, height=500)
par(oma=c(0,0,0,0))
layout(t(1:3))
par(mar=c(3, 3, 3, 3))
plot_box()
par(mar=c(3, 3, 3, 3))
plot_box()
par(mar=c(3, 3, 3, 3))
plot_box()
dev.off()

I have also posted this question to StackOverflow, and it has images of the
graphical output which illustrate the problem better:
http://stackoverflow.com/questions/34790682/r-layout-affects-margin-size-in-plot-regions

Thanks,

Malcolm

PS I was unsure if this question belonged to help or devel - I will repost
on devel if it is likely to get better answers.

??? [[alternative HTML version deleted]]



------------------------------

Message: 10
Date: Thu, 14 Jan 2016 11:29:16 -0500
From: jim holtman <jholtman at gmail.com>
To: Mohsen Jafarikia <jafarikia at gmail.com>
Cc: r-help <r-help at stat.math.ethz.ch>
Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
??? character
Message-ID:
??? <CAAxdm-7tTcbs-KvCwoR-ctDcq2t3UzeUFuz65aXm=RjoUFU23Q at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

Take a look at the data coming in since you may have something that looks
like characters (maybe 'blanks').? What you think is numeric in EXCEL might
not be.?


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
wrote:

> Hello:
>
> I am reading some excel files (each with one sheet) and trying to write
> them all in one file. I am not sure if read.xlsx reads some of the columns
> as character or write.xlsx writes them as character where they are not
> characters. I have 12 columns (2 character and 10 numbers). From 10 number
> columns, with float and integer numbers, only 3 of them are recognized
> correctly. I was wondering how can I define the the column format for
> read.xlsx - write.xlsx.
>
> Here comes a simple example of my code:
>
> ifn1 <- "A.xlsx"
> dat1? <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>
> ifn2 <- "F.xlsx"
> dat2? <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>
> write.xlsx(dat1,? file="AF.xlsx", sheetName="A",? showNA=FALSE,
> row.names=FALSE, append=FALSE)
> write.xlsx(dat2,? file="AF.xlsx", sheetName="F",? showNA=FALSE,
> row.names=FALSE, append= TRUE)
>
> Thanks in advance!
> Mohsen
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

??? [[alternative HTML version deleted]]



------------------------------

Message: 11
Date: Thu, 14 Jan 2016 11:36:26 -0500
From: Mohsen Jafarikia <jafarikia at gmail.com>
To: jim holtman <jholtman at gmail.com>
Cc: r-help <r-help at stat.math.ethz.ch>
Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
??? character
Message-ID:
??? <CADs3iXkxOrxfvUwQVz7ddQMkkRmDMBrMZUjd8XiFcqiwxn6Ndw at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

Thanks for the comment Jim. There is no blank cell. All have numbers.

Mohsen

On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:

> Take a look at the data coming in since you may have something that looks
> like characters (maybe 'blanks').? What you think is numeric in EXCEL might
> not be.?
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
> wrote:
>
>> Hello:
>>
>> I am reading some excel files (each with one sheet) and trying to write
>> them all in one file. I am not sure if read.xlsx reads some of the columns
>> as character or write.xlsx writes them as character where they are not
>> characters. I have 12 columns (2 character and 10 numbers). From 10 number
>> columns, with float and integer numbers, only 3 of them are recognized
>> correctly. I was wondering how can I define the the column format for
>> read.xlsx - write.xlsx.
>>
>> Here comes a simple example of my code:
>>
>> ifn1 <- "A.xlsx"
>> dat1? <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>>
>> ifn2 <- "F.xlsx"
>> dat2? <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>>
>> write.xlsx(dat1,? file="AF.xlsx", sheetName="A",? showNA=FALSE,
>> row.names=FALSE, append=FALSE)
>> write.xlsx(dat2,? file="AF.xlsx", sheetName="F",? showNA=FALSE,
>> row.names=FALSE, append= TRUE)
>>
[[elided Yahoo spam]]
>> Mohsen
>>
>>? ? ? ? [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

??? [[alternative HTML version deleted]]



------------------------------

Message: 12
Date: Thu, 14 Jan 2016 11:48:39 -0500
From: Sarah Goslee <sarah.goslee at gmail.com>
To: Malcolm Perry <mgperry32 at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] R: layout() affects margin size in subfigures
??? [unexpected??? behaviour]
Message-ID:
??? <CAM_vju=W5jOuYwkRNaM6n7yH4oygo6xf63SbgmQPGpTFB02BiA at mail.gmail.com>
Content-Type: text/plain; charset=UTF-8

You're setting margin using mar, which is in terms of lines, which is,
well, difficult to manage properly.

? ? ?mar? A numerical vector of the form ?c(bottom, left, top, right)?
? ? ? ? ? which gives the number of lines of margin to be specified on
? ? ? ? ? the four sides of the plot.? The default is ?c(5, 4, 4, 2) +
? ? ? ? ? 0.1?.

If you use mai instead, you will get a consistent physical size.


? ? ?mai? A numerical vector of the form ?c(bottom, left, top, right)?
? ? ? ? ? which gives the margin size specified in inches.

See also this bit of ?par:

? ? The meaning of ?character size? is not well-defined: this is set
? ? up for the device taking ?pointsize? into account but often not
? ? the actual font family in use.? Internally the corresponding pars
? ? (?cra?, ?cin?, ?cxy? and ?csi?) are used only to set the
? ? inter-line spacing used to convert ?mar? and ?oma? to physical
? ? margins.? (The same inter-line spacing multiplied by ?lheight? is
? ? used for multi-line strings in ?text? and ?strheight?.)

Sarah



On Thu, Jan 14, 2016 at 8:42 AM, Malcolm Perry <mgperry32 at gmail.com> wrote:
> The absolute margin size of figures in R seems to be affected by the layout
> of the plot, which i think is surprising (not sure if it qualifies as a
> bug). The following plots have different margins sizes, with the 1x3 plot
> margins being smaller (thus giving a larger plot area). This is causing
> havoc with a package I am writing to automatically generate composite
> figures, since labels are positioned differently depending on the number of
> panels.
>
> plot_box <- function() {
>? ? ? ? plot(1, 1, type='n', bty='n', xaxt='n', yaxt='n', xlab='', ylab='')
>? ? ? ? box(lwd = 6)
>? ? ? ? box("figure", lwd=6, col='red')
> }
>
> png("margin_test_1.png", width=1000, height=500)
> par(oma=c(0,0,0,0))
> layout(t(1:2))
> par(mar=c(3, 3, 3, 3))
> plot_box()
> par(mar=c(3, 3, 3, 3))
> plot_box()
> dev.off()
>
> png("margin_test_2.png", width=1500, height=500)
> par(oma=c(0,0,0,0))
> layout(t(1:3))
> par(mar=c(3, 3, 3, 3))
> plot_box()
> par(mar=c(3, 3, 3, 3))
> plot_box()
> par(mar=c(3, 3, 3, 3))
> plot_box()
> dev.off()
>
> I have also posted this question to StackOverflow, and it has images of the
> graphical output which illustrate the problem better:
> http://stackoverflow.com/questions/34790682/r-layout-affects-margin-size-in-plot-regions
>
> Thanks,
>
> Malcolm
>
> PS I was unsure if this question belonged to help or devel - I will repost
> on devel if it is likely to get better answers.
>
-- 
Sarah Goslee
http://www.numberwright.com



------------------------------

Message: 13
Date: Thu, 14 Jan 2016 12:17:48 -0500
From: Jim Holtman <jholtman at gmail.com>
To: Mohsen Jafarikia <jafarikia at gmail.com>
Cc: r-help <r-help at stat.math.ethz.ch>
Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
??? character
Message-ID: <wu2yr8ggciqwr86bpal4oemf.1452791789815 at email.android.com>
Content-Type: text/plain; charset="UTF-8"

Write it out as a csv from excel and see what the data looks like. ?That may help ?in seeing what the problem is.


Sent from my Verizon Wireless 4G LTE Smartphone

<div>-------- Original message --------</div><div>From: Mohsen Jafarikia <jafarikia at gmail.com> </div><div>Date:01/14/2016? 11:36? (GMT-05:00) </div><div>To: jim holtman <jholtman at gmail.com> </div><div>Cc: r-help <r-help at stat.math.ethz.ch> </div><div>Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as character </div><div>
</div>Thanks for the comment Jim. There is no blank cell. All have numbers.

Mohsen

On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:
Take a look at the data coming in since you may have something that looks like characters (maybe 'blanks').? What you think is numeric in EXCEL might not be.?


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
Hello:

I am reading some excel files (each with one sheet) and trying to write
them all in one file. I am not sure if read.xlsx reads some of the columns
as character or write.xlsx writes them as character where they are not
characters. I have 12 columns (2 character and 10 numbers). From 10 number
columns, with float and integer numbers, only 3 of them are recognized
correctly. I was wondering how can I define the the column format for
read.xlsx - write.xlsx.

Here comes a simple example of my code:

ifn1 <- "A.xlsx"
dat1? <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)

ifn2 <- "F.xlsx"
dat2? <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)

write.xlsx(dat1,? file="AF.xlsx", sheetName="A",? showNA=FALSE,
row.names=FALSE, append=FALSE)
write.xlsx(dat2,? file="AF.xlsx", sheetName="F",? showNA=FALSE,
row.names=FALSE, append= TRUE)

Thanks in advance!
Mohsen

? ? ? ? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



??? [[alternative HTML version deleted]]


------------------------------

Message: 14
Date: Thu, 14 Jan 2016 09:36:26 -0800
From: David Winsemius <dwinsemius at comcast.net>
To: CG Pettersson <cg.pettersson at lantmannen.com>
Cc: "r-help at r-project.org" <r-help at r-project.org>
Subject: Re: [R] Problems with data structure when using plsr() from
??? package??? pls
Message-ID: <47A5E3B0-B334-43BF-9A5D-60FADB167948 at comcast.net>
Content-Type: text/plain; charset=utf-8


> On Jan 14, 2016, at 2:33 AM, CG Pettersson <cg.pettersson at lantmannen.com> wrote:
> 
> Dear Jeff, 
> thanks for the effort, but the use of I() when preparing the dataset is suggested by the authors (Mevik & Wehrens, section 3.2):
> 
> +If Z is a matrix, it has to be protected by the ?protect function? I() in calls
> +to data.frame: mydata <- data.frame(..., Z = I(Z)). Otherwise, it will be split into
> +separate variables for each column, and there will be no variable called Z in the data frame,
> +so we cannot use Z in the formula. One can also add the matrix to an existing data frame:
> +R> mydata <- data.frame(...)
> +R> mydata$Z <- Z
> 
> In the dataset "gasoline" that is supplied with the pls package, there are two variables; octane and NIR, where NIR is a frame with 401 columns and possible to work with like: 
> plsr(octane ~NIR, data = gasoline)
> I thought "gasoline" was made like the example above, but I must be missing something else.
> 
> Whatever I do ends with " invalid type (list) for variable 'n96'"

Was `n96` a list before you put a copy of it into the `frame1`-object? Maybe it wasn't a simple matrix. You need at the very least to post the output of str(n96). Also .... never use attach().

-- 
David.

> 
> So I am still stuck
> /CG
> 
> Fr?n: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us] 
> Skickat: den 14 januari 2016 05:16
> Till: CG Pettersson; r-help at r-project.org
> ?mne: Re: [R] Problems with data structure when using plsr() from package pls
> 
> Using I() in the data.frame seems ill-advised to me. You complain about 96 variables but from reading your explanation that seems to be what your data are. I have no idea whether it makes sense to NOT have 96 variables if that is what your data are. Note that a reproducible example supplied by you might help us guess better, but it might just be that your expectations are wrong. 
> -- 
> Sent from my phone. Please excuse my brevity.
> On January 13, 2016 11:02:25 AM PST, CG Pettersson <cg.pettersson at lantmannen.com> wrote:
> R version 3.2.3, W7 64bit.
> 
> Dear all!
> 
> I am trying to make pls-regression using plsr() from package pls, with Mevik & Wehrens (2007) as tutorial and the datasets from the package.
> Everything works real nice as long as I use the supplied datasets, but I don?t understand how to prepare my own data.
> This is what I have done:
> frame1 <- data.frame(gushVM, I(n96))
> 
> Where gushVM is a vector with fifteen reference analysis values of a quality problem in grain and n96 is a matrix with fifteen rows and 96 columns from an electronic nose. I try to copy the methods as in 3.2 in Mevik & Wehrens, and want to keep n96 as one variable to avoid addressing 96 different variables in the plsr call. If I don?t use I() in the call I get 96 variables instead.
> Looking at the data
> frame by
> summary(frame1) get a return quite like summary(gasoline) from the package (not shown here).
> But when I try to use plsr() with my own data it doesn?t work due to an error in the data structure:
> pls1 <- plsr(gushVM ~ n96, data = frame1)
> Error in model.frame.default(formula = gushVM ~ n96, data = frame1) :
>? invalid type (list) for variable 'n96'
> 
> So, n96 has turned into a list, and that is a problem. If gushVM is a vector (one variable) och a matrix (five variables) does not seem to change anything, managing n96 is the problem
> I have tried all alternative ways of creating a proper data frame suggested in the article with exactly the same result.
> I have tried the docum
> entation
> for data.frame() but I probably don?t understand what it says.
> 
> What should I do to change "n96" into something better than "list"?
> 
> Thanks
> /CG
> 
> Med v?nlig h?lsning/Best regards
> CG Pettersson
> Scientific Project Manager, PhD
> ______________________
> Lantm?nnen Corporate R&D
> Phone:? +46 10 556 19 85
> Mobile: + 46 70 330 66 85
> Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
> Visiting Address: S:t G?ransgatan 160 A
> Address: Box 30192, SE-104 25 Stockholm
> Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
> Registered Office: Stockholm
> Before printing, think about the environment
> 
> 
> [[alternative HTML version deleted]]
> 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA



------------------------------

Message: 15
Date: Thu, 14 Jan 2016 12:59:49 -0500
From: Mohsen Jafarikia <jafarikia at gmail.com>
To: Jim Holtman <jholtman at gmail.com>
Cc: r-help <r-help at stat.math.ethz.ch>
Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
??? character
Message-ID:
??? <CADs3iX=P+F-09ewUaNaeAiRfc+o33pSD1jXR=UpTd2AO6OadHA at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

It looks okay on CSV too.

On Thu, Jan 14, 2016 at 12:17 PM, Jim Holtman <jholtman at gmail.com> wrote:

> Write it out as a csv from excel and see what the data looks like.? That
> may help? in seeing what the problem is.
>
>
> Sent from my Verizon Wireless 4G LTE Smartphone
>
>
> -------- Original message --------
> From: Mohsen Jafarikia
> Date:01/14/2016 11:36 (GMT-05:00)
> To: jim holtman
> Cc: r-help
> Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
> character
>
> Thanks for the comment Jim. There is no blank cell. All have numbers.
>
> Mohsen
>
> On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:
>
>> Take a look at the data coming in since you may have something that looks
>> like characters (maybe 'blanks').? What you think is numeric in EXCEL might
>> not be.?
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
>> wrote:
>>
>>> Hello:
>>>
>>> I am reading some excel files (each with one sheet) and trying to write
>>> them all in one file. I am not sure if read.xlsx reads some of the
>>> columns
>>> as character or write.xlsx writes them as character where they are not
>>> characters. I have 12 columns (2 character and 10 numbers). From 10
>>> number
>>> columns, with float and integer numbers, only 3 of them are recognized
>>> correctly. I was wondering how can I define the the column format for
>>> read.xlsx - write.xlsx.
>>>
>>> Here comes a simple example of my code:
>>>
>>> ifn1 <- "A.xlsx"
>>> dat1? <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>>>
>>> ifn2 <- "F.xlsx"
>>> dat2? <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>>>
>>> write.xlsx(dat1,? file="AF.xlsx", sheetName="A",? showNA=FALSE,
>>> row.names=FALSE, append=FALSE)
>>> write.xlsx(dat2,? file="AF.xlsx", sheetName="F",? showNA=FALSE,
>>> row.names=FALSE, append= TRUE)
>>>
[[elided Yahoo spam]]
>>> Mohsen
>>>
>>>? ? ? ? [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

??? [[alternative HTML version deleted]]



------------------------------

Message: 16
Date: Thu, 14 Jan 2016 14:30:59 -0500
From: Mohsen Jafarikia <jafarikia at gmail.com>
To: Jim Holtman <jholtman at gmail.com>
Cc: r-help <r-help at stat.math.ethz.ch>
Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
??? character
Message-ID:
??? <CADs3iXnux3ZDOn0c3VD14n3oW4SVUM5WmYYBptm01xLjf1bFOg at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

Hi Jim,

I found where the problem is. I had some characters in my list.

[[elided Yahoo spam]]

Mohsen

On Thu, Jan 14, 2016 at 12:17 PM, Jim Holtman <jholtman at gmail.com> wrote:

> Write it out as a csv from excel and see what the data looks like.? That
> may help? in seeing what the problem is.
>
>
> Sent from my Verizon Wireless 4G LTE Smartphone
>
>
> -------- Original message --------
> From: Mohsen Jafarikia
> Date:01/14/2016 11:36 (GMT-05:00)
> To: jim holtman
> Cc: r-help
> Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
> character
>
> Thanks for the comment Jim. There is no blank cell. All have numbers.
>
> Mohsen
>
> On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:
>
>> Take a look at the data coming in since you may have something that looks
>> like characters (maybe 'blanks').? What you think is numeric in EXCEL might
>> not be.?
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
>> wrote:
>>
>>> Hello:
>>>
>>> I am reading some excel files (each with one sheet) and trying to write
>>> them all in one file. I am not sure if read.xlsx reads some of the
>>> columns
>>> as character or write.xlsx writes them as character where they are not
>>> characters. I have 12 columns (2 character and 10 numbers). From 10
>>> number
>>> columns, with float and integer numbers, only 3 of them are recognized
>>> correctly. I was wondering how can I define the the column format for
>>> read.xlsx - write.xlsx.
>>>
>>> Here comes a simple example of my code:
>>>
>>> ifn1 <- "A.xlsx"
>>> dat1? <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>>>
>>> ifn2 <- "F.xlsx"
>>> dat2? <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>>>
>>> write.xlsx(dat1,? file="AF.xlsx", sheetName="A",? showNA=FALSE,
>>> row.names=FALSE, append=FALSE)
>>> write.xlsx(dat2,? file="AF.xlsx", sheetName="F",? showNA=FALSE,
>>> row.names=FALSE, append= TRUE)
>>>
[[elided Yahoo spam]]
>>> Mohsen
>>>
>>>? ? ? ? [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

??? [[alternative HTML version deleted]]



------------------------------

Message: 17
Date: Thu, 14 Jan 2016 14:51:58 -0500
From: jim holtman <jholtman at gmail.com>
To: Mohsen Jafarikia <jafarikia at gmail.com>
Cc: r-help <r-help at stat.math.ethz.ch>
Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
??? character
Message-ID:
??? <CAAxdm-6YMe2KnZuL9pfMJY2+uVAM3Vtq86Zu_F+7-pLfWfMNCw at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

That is what usually caused me problems in the past.? Excel does not have
the requirement that a column contain the same 'mode' of a variable.? You
can mix numeric and character in the same column and things will work fine
in Excel; conversion to a data.frame does require the same mode in a
column.?


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jan 14, 2016 at 2:30 PM, Mohsen Jafarikia <jafarikia at gmail.com>
wrote:

> Hi Jim,
>
> I found where the problem is. I had some characters in my list.
>
[[elided Yahoo spam]]
>
> Mohsen
>
> On Thu, Jan 14, 2016 at 12:17 PM, Jim Holtman <jholtman at gmail.com> wrote:
>
>> Write it out as a csv from excel and see what the data looks like.? That
>> may help? in seeing what the problem is.
>>
>>
>> Sent from my Verizon Wireless 4G LTE Smartphone
>>
>>
>> -------- Original message --------
>> From: Mohsen Jafarikia
>> Date:01/14/2016 11:36 (GMT-05:00)
>> To: jim holtman
>> Cc: r-help
>> Subject: Re: [R] read.xlsx - write.xlsx: reading/writing numbers as
>> character
>>
>> Thanks for the comment Jim. There is no blank cell. All have numbers.
>>
>> Mohsen
>>
>> On Thu, Jan 14, 2016 at 11:29 AM, jim holtman <jholtman at gmail.com> wrote:
>>
>>> Take a look at the data coming in since you may have something that
>>> looks like characters (maybe 'blanks').? What you think is numeric in EXCEL
>>> might not be.?
>>>
>>>
>>> Jim Holtman
>>> Data Munger Guru
>>>
>>> What is the problem that you are trying to solve?
>>> Tell me what you want to do, not how you want to do it.
>>>
>>> On Thu, Jan 14, 2016 at 11:20 AM, Mohsen Jafarikia <jafarikia at gmail.com>
>>> wrote:
>>>
>>>> Hello:
>>>>
>>>> I am reading some excel files (each with one sheet) and trying to write
>>>> them all in one file. I am not sure if read.xlsx reads some of the
>>>> columns
>>>> as character or write.xlsx writes them as character where they are not
>>>> characters. I have 12 columns (2 character and 10 numbers). From 10
>>>> number
>>>> columns, with float and integer numbers, only 3 of them are recognized
>>>> correctly. I was wondering how can I define the the column format for
>>>> read.xlsx - write.xlsx.
>>>>
>>>> Here comes a simple example of my code:
>>>>
>>>> ifn1 <- "A.xlsx"
>>>> dat1? <- read.xlsx(ifn1, sheetName="A.csv", header = TRUE)
>>>>
>>>> ifn2 <- "F.xlsx"
>>>> dat2? <- read.xlsx(ifn2, sheetName="F.csv",header = TRUE)
>>>>
>>>> write.xlsx(dat1,? file="AF.xlsx", sheetName="A",? showNA=FALSE,
>>>> row.names=FALSE, append=FALSE)
>>>> write.xlsx(dat2,? file="AF.xlsx", sheetName="F",? showNA=FALSE,
>>>> row.names=FALSE, append= TRUE)
>>>>
[[elided Yahoo spam]]
>>>> Mohsen
>>>>
>>>>? ? ? ? [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

??? [[alternative HTML version deleted]]



------------------------------

Message: 18
Date: Thu, 14 Jan 2016 22:36:00 +0100
From: Lorenzo Isella <lorenzo.isella at gmail.com>
To: "r-help at r-project.org" <r-help at r-project.org>
Subject: [R] Updating a Time Series After Forecast()
Message-ID: <20160114213600.GB1913 at localhost.localdomain>
Content-Type: text/plain; charset=us-ascii; format=flowed

Dear All,
Perhaps I am drowning in a cup of water, since I am positive that the
answer will be a one-liner.
Consider the following short script


########################################################
library(forecast)

ts2<-structure(c(339130, 356462, 363234, 378179, 367864, 378337, 392157,
402153, 376361, 392204, 403483, 414034, 391967, 406067, 419464,
434913, 410102, 424795, 437073, 448827, 415569, 430561, 444719,
455764, 419892, 444190, 454648, 466312, 439922, 448963, 465153,
475621, 445502, 457198, 473573, 485764, 463895, 470274, 484390,
490678, 478003, 483570, 499141, 509216, 481395, 492345, 511184,
513420, 483757, 490884, 514966, 515457, 497614, 510139, 523467,
526406, 499784, 519033, 532009, 531260, 521539, 532590, 553118,
557725, 548321, 556832, 578087, 578120, 566116, 580571, 587993,
569985, 534326, 539641, 564824, 568445, 558614, 570192, 594584,
598305, 593769, 598278, 620147, 615884, 611033, 609304, 630458,
624325, 614356, 627192, 649324, 645988, 642965, 645125, 669471,
665529, 664248, 669670, 694719), na.action = structure(1:64, class =
"omit"), .Tsp = c(1991,
2015.5, 4), class = "ts")

fit2 <- auto.arima(ts2, approximation=FALSE,trace=FALSE)

pred2 <- forecast(fit2, h=2)

#######################################################

So, I have an original quarterly time series ts2 and a forecast for 2
quarters pred2.

I would like to combine ts2 and pred2 (just the prediction) into a new
time series (in other words, just stretch a bit ts2).
How can I do that?
Many thanks

Lorenzo



------------------------------

Message: 19
Date: Thu, 14 Jan 2016 21:34:40 +0000
From: Lauren Moscoe <moscoe at wisc.edu>
To: "r-help at r-project.org" <r-help at r-project.org>
Subject: [R] Tukey and extracting letters in multcomp
Message-ID:
??? <CY1PR0601MB143762017C05ECDDED789BE4B6CC0 at CY1PR0601MB1437.namprd06.prod.outlook.com>
??? 
Content-Type: text/plain; charset="UTF-8"

Hello,


I have the following model:


model <- lmer (Y? ~ gen + (1|env) + (1|gen:env))


I would like to use Tukey's method to identify significant pairwise differences among levels of the factor gen, which has 18 levels. (Env has 3 levels.)


I have been trying to do this using glht and cld in the package multcomp.


This is what I ran, with notes about the output.


tukey.gen <- glht(model, linfct=mcp(gen = "Tukey))


summary(tukey.gen)

# This resulted in 46 warnings that said, "In RET$pfunction("adjusted", ...) : Completion with error > abseps."


cld(tukey.gen, level=0.05)

# This was my attempt to extract letters to represent Tukey results. I did get letters for each level of gen, but they came alongside the same warning as above, repeated over 50 times.


Why am I getting these warnings? Should I not use the results of cld, given these warnings? How can I avoid the warnings?


I would be very grateful for any advice for how to successfully use Tukey's method for gen in this model, as well as for how to extract letters to denote significant differences. Either suggestions for adjusting my code or for a new approach entirely would be fine.


Thank you,


Lauren


PhD Candidate

Department of Botany

University of Wisconsin-Madison


??? [[alternative HTML version deleted]]



------------------------------

Message: 20
Date: Thu, 14 Jan 2016 16:45:44 -0600
From: AASHISH JAIN <ajain at umn.edu>
To: r-help at r-project.org
Subject: [R] Problem with rJava
Message-ID: <121B32DB-615E-4148-B8BE-5D6E47C8624D at umn.edu>
Content-Type: text/plain; charset=us-ascii

Hello,

I am using an R package called Rknots, which uses rJava (and others like rSymPy, rjson, rJython) and I am getting some error due to rJava. When I run my R code, the execution gets halted with the following error:

Error in .jcheck() : No running JVM detected. Maybe .jinit() would help. (found this line in rjava.c)
Calls: computeInvariant ... sympy -> $ -> $ -> hasField -> .jcall -> .jcheck -> .Call (this line comes from Rknots package when the function computeInvariant is called)
Execution halted

Note that since I could not install these packages on root level, I installed them locally on unix OS. Since the other Java and R software appears to be working, it seems like the error is specific to the rJava package. Possibly rJava makes some assumptions about the Java installation (perhaps its location) that cause it to be confused. I would like to know if a non-root Java installation would cause problems. 

FYI, here are the versions of different packages that I currently have:
1. rSymPy_0.2-1.1.tar
2. rJava_0.9-8.tar
3. rjson_0.2.15.tar.gz
4. rJython_0.0-4.tar
5. Rknots_1.2.1.tar

I would really appreciate any help. 

I want to apologize if this forum is not a right place to discuss about rJava.

Thanks!
Aashish Jain
Postdoctoral Associate
Department of Chemical Engineering and Material Science
151 Amundson Hall
421 Washington Ave SE
Minneapolis, MN 55455 USA
Ph: +1 612-806-7154



------------------------------

Message: 21
Date: Thu, 14 Jan 2016 19:24:51 -0500
From: ProfJCNash <profjcnash at gmail.com>
To: r-help at r-project.org
Subject: Re: [R] Problem with rJava
Message-ID: <56983C53.4060102 at gmail.com>
Content-Type: text/plain; charset=windows-1252

Your post does not have the requested session information that will tell
us your computing environment, nor the version of R.

However, I'm experiencing at least a related problem, as this morning I
updated R (in Linux Mind Rafaela 17.2, so I get an operating system
notice to update via the package manager). Afterwards I ran
update.packages() but could not get rJava to carry out the update,
though other packages did complete. Possibly there is some mismatch
between rJava and R 3.2.3 and/or gcj. I'm a bit surprised this hasn't
surfaced before, as I'm a "slow updater" and 3.2.3 is over a month old now.

Below is the output from the update -- AFTER I ran "R CMD javareconf" as
root -- along with the session information, which shows Ubuntu rather
than the derivative Linux Mint.

Cheers, JN

> update.packages()
rJava :
 Version 0.9-6 installed in /usr/lib/R/site-library
 Version 0.9-8 available at https://rweb.crmda.ku.edu/cran
Update (y/N/c)?? y
trying URL 'https://rweb.crmda.ku.edu/cran/src/contrib/rJava_0.9-8.tar.gz'
Content type 'application/x-gzip' length 656615 bytes (641 KB)
==================================================
downloaded 641 KB

* installing *source* package ?rJava? ...
** package ?rJava? successfully unpacked and MD5 sums checked
checking for gcc... gcc -std=gnu99
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables...
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc -std=gnu99 accepts -g... yes
checking for gcc -std=gnu99 option to accept ISO C89... none needed
checking how to run the C preprocessor... gcc -std=gnu99 -E
checking for grep that handles long lines and -e... /bin/grep
checking for egrep... /bin/grep -E
checking for ANSI C header files... yes
checking for sys/wait.h that is POSIX.1 compatible... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking for string.h... (cached) yes
checking sys/time.h usability... yes
checking sys/time.h presence... yes
checking for sys/time.h... yes
checking for unistd.h... (cached) yes
checking for an ANSI C-conforming const... yes
checking whether time.h and sys/time.h may both be included... yes
configure: checking whether gcc -std=gnu99 supports static inline...
yes
checking whether setjmp.h is POSIX.1 compatible... yes
checking whether sigsetjmp is declared... yes
checking whether siglongjmp is declared... yes
checking Java support in R... present:
interpreter : '/usr/lib/jvm/default-java/jre/bin/java'
archiver? ? : '/usr/bin/jar'
compiler? ? : '/usr/bin/javac'
header prep.: '/usr/bin/javah'
cpp flags? : ''
java libs? : '-L/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server
-ljvm'
configure: error: One or more Java configuration variables are not set.
Make sure R is configured with full Java support (including JDK). Run
R CMD javareconf
as root to add Java support to R.

If you don't have root privileges, run
R CMD javareconf -e
to set all Java-related variables and then install rJava.

ERROR: configuration failed for package ?rJava?
* removing ?/usr/lib/R/site-library/rJava?
* restoring previous ?/usr/lib/R/site-library/rJava?

The downloaded source packages are in
??? ?/tmp/RtmpXCs91E/downloaded_packages?
Warning message:
In install.packages(update[instlib == l, "Package"], l, contriburl =
contriburl,? :
? installation of package ?rJava? had non-zero exit status
>
> sessionInfo()
R version 3.2.3 (2015-12-10)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.3 LTS

locale:
 [1] LC_CTYPE=en_CA.UTF-8? ? ? LC_NUMERIC=C
 [3] LC_TIME=en_CA.UTF-8? ? ? ? LC_COLLATE=en_CA.UTF-8
 [5] LC_MONETARY=en_CA.UTF-8? ? LC_MESSAGES=en_CA.UTF-8
 [7] LC_PAPER=en_CA.UTF-8? ? ? LC_NAME=C
 [9] LC_ADDRESS=C? ? ? ? ? ? ? LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats? ? graphics? grDevices utils? ? datasets? methods? base

loaded via a namespace (and not attached):
[1] tools_3.2.3 tcltk_3.2.3
>



On 16-01-14 05:45 PM, AASHISH JAIN wrote:
> Hello,
> 
> I am using an R package called Rknots, which uses rJava (and others like rSymPy, rjson, rJython) and I am getting some error due to rJava. When I run my R code, the execution gets halted with the following error:
> 
> Error in .jcheck() : No running JVM detected. Maybe .jinit() would help. (found this line in rjava.c)
> Calls: computeInvariant ... sympy -> $ -> $ -> hasField -> .jcall -> .jcheck -> .Call (this line comes from Rknots package when the function computeInvariant is called)
> Execution halted
> 
> Note that since I could not install these packages on root level, I installed them locally on unix OS. Since the other Java and R software appears to be working, it seems like the error is specific to the rJava package. Possibly rJava makes some assumptions about the Java installation (perhaps its location) that cause it to be confused. I would like to know if a non-root Java installation would cause problems. 
> 
> FYI, here are the versions of different packages that I currently have:
> 1. rSymPy_0.2-1.1.tar
> 2. rJava_0.9-8.tar
> 3. rjson_0.2.15.tar.gz
> 4. rJython_0.0-4.tar
> 5. Rknots_1.2.1.tar
> 
> I would really appreciate any help. 
> 
> I want to apologize if this forum is not a right place to discuss about rJava.
> 
> Thanks!
> Aashish Jain
> Postdoctoral Associate
> Department of Chemical Engineering and Material Science
> 151 Amundson Hall
> 421 Washington Ave SE
> Minneapolis, MN 55455 USA
> Ph: +1 612-806-7154
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



------------------------------

Message: 22
Date: Thu, 14 Jan 2016 23:18:35 -0500

To: "r-help at r-project.org" <r-help at r-project.org>
Subject: [R] Multiplication of high dimensional array

Content-Type: text/plain; charset=us-ascii

In one of my applications, I need to perform following task in R:

? ? 
? ? svector <- array(0, dim=c(5, 100000, 360), dimnames=list(c('s1','s2','s3','s4','s5'), NULL, NULL)))

? ? 
? ? tmatrix <- array(0, dim=c(100000, 360, 5, 5), dimnames=list(NULL, NULL, c('s1','s2','s3','s4','s5'), c('s1','s2','s3','s4','s5'))))

? ? #My algorithms compute all the elements in tmatrix? (transition matrix among states from time t to t+1, for all entities represented by l index)

? ? #do transition for all l for t=1 to 359
? ? for (l in 1:100000)
? ? {
? ? ? for(t in 1:359)
? ? ? {
? ? ? ? ? svector [, l,t+1] <- tmatrix[l,t,,] %*% svector [,l,t]
? ? ? }
? }

The double loops make computation slow. I have been trying to see I can treat the svector and tmatrix as tensors, and use mul.tensor in tensorR to vectorize the computation, but so far I ways get one message or another indicating my incorrect usage. Can tensorR or any other package be used here to simply the calculation? If it can, would you kindly give some sample code

[[elided Yahoo spam]]



------------------------------

Message: 23
Date: Thu, 14 Jan 2016 21:26:49 -0800
From: jpm miao <miaojpm at gmail.com>
To: r-help <r-help at r-project.org>
Subject: [R] How can we let "multiplot.R" return a plot?
Message-ID:
??? <CABcx46CjrFiZ0-anXMZ32QP84g2Q05XED9S1b1iGyTu0v4NDsA at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

Hi,

? The function "ggplot" does plot and return a plot. For example, we can
write:

y = ggplot(.. .....)? Then y is? a plot.

? How can we modify the multiplot function so that it can also return a
plot?? Multiplot.R is here:

? http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/,
and the code is as below.

[[elided Yahoo spam]]

? Thanks,

Miao

# Multiple plot function # # ggplot objects can be passed in ..., or to
plotlist (as a list of ggplot objects) # - cols: Number of columns in layout #
- layout: A matrix specifying the layout. If present, 'cols' is ignored. # #
If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), #
then plot 1 will go in the upper left, 2 will go in the upper right, and #
3 will go all the way across the bottom. # multiplot <- function(...,
plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list
from the ... arguments and plotlist plots <- c(list(...), plotlist)
numPlots = length(plots) # If layout is NULL, then use 'cols' to determine
layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of
plots # nrow: Number of rows needed, calculated from # of cols layout <-
matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow =
ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { #
Set up the page grid.newpage() pushViewport(viewport(layout =
grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct
location for (i in 1:numPlots) { # Get the i,j matrix positions of the
regions that contain this subplot matchidx <- as.data.frame(which(layout ==
i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row =
matchidx$row, layout.pos.col = matchidx$col)) } } }

??? [[alternative HTML version deleted]]



------------------------------

Message: 24
Date: Thu, 14 Jan 2016 21:43:20 -0800
From: David Winsemius <dwinsemius at comcast.net>
To: jpm miao <miaojpm at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] How can we let "multiplot.R" return a plot?
Message-ID: <3D69E829-76AC-402B-A82B-DE249052C601 at comcast.net>
Content-Type: text/plain; charset=us-ascii


> On Jan 14, 2016, at 9:26 PM, jpm miao <miaojpm at gmail.com> wrote:
> 
> Hi,
> 
>? The function "ggplot" does plot and return a plot. For example, we can
> write:
> 
> y = ggplot(.. .....)? Then y is? a plot.
> 
>? How can we modify the multiplot function so that it can also return a
> plot?? Multiplot.R is here:
> 
>? http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/,
> and the code is as below.
> 
[[elided Yahoo spam]]
> 
>? Thanks,
> 
> Miao
> 
> # Multiple plot function # # ggplot objects can be passed in ..., or to
> plotlist (as a list of ggplot objects) # - cols: Number of columns in layout #
> - layout: A matrix specifying the layout. If present, 'cols' is ignored. # #
> If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), #
> then plot 1 will go in the upper left, 2 will go in the upper right, and #
> 3 will go all the way across the bottom. # multiplot <- function(...,
> plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list
> from the ... arguments and plotlist plots <- c(list(...), plotlist)
> numPlots = length(plots) # If layout is NULL, then use 'cols' to determine
> layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of
> plots # nrow: Number of rows needed, calculated from # of cols layout <-
> matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow =
> ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { #
> Set up the page grid.newpage() pushViewport(viewport(layout =
> grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct
> location for (i in 1:numPlots) { # Get the i,j matrix positions of the
> regions that contain this subplot matchidx <- as.data.frame(which(layout ==
> i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row =
> matchidx$row, layout.pos.col = matchidx$col)) } } }
> 
> ??? [[alternative HTML version deleted]]

What part of NO HTML don't you understand????

Read the .... posting guide:
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA



------------------------------

Message: 25
Date: Fri, 15 Jan 2016 17:24:11 +1100
From: Jim Lemon <drjimlemon at gmail.com>
To: jpm miao <miaojpm at gmail.com>, r-help mailing list
??? <r-help at r-project.org>
Subject: Re: [R] How can we let "multiplot.R" return a plot?
Message-ID:
??? <CA+8X3fUcLEthU8DsdGnUrCDBanJYeQFhtnSz_T-WQPj0Pqk9kw at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

Hi Miao,
If I understand your question correctly, you want to get a return value
from the "multiplot" function that you have copied into your message. You
could simply add:

return(plotlist)

just before the final right brace in the function and it would return the
list of plots that you have created. However, as you already have this, I
think you probably want to get a single plot object that has all the
information in the original plotlist. This doesn't seem possible to me as I
don't think that the ggplot objects can be merged. I may be mistaken, so I
will defer to anyone more knowledgeable.

Jim

On Fri, Jan 15, 2016 at 4:26 PM, jpm miao <miaojpm at gmail.com> wrote:

> Hi,
>
>? ? The function "ggplot" does plot and return a plot. For example, we can
> write:
>
> y = ggplot(.. .....)? Then y is? a plot.
>
>? ? How can we modify the multiplot function so that it can also return a
> plot?? Multiplot.R is here:
>
>? ? http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
> ,
> and the code is as below.
>
[[elided Yahoo spam]]
>
>? ? Thanks,
>
> Miao
>
> # Multiple plot function # # ggplot objects can be passed in ..., or to
> plotlist (as a list of ggplot objects) # - cols: Number of columns in
> layout #
> - layout: A matrix specifying the layout. If present, 'cols' is ignored. #
> #
> If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), #
> then plot 1 will go in the upper left, 2 will go in the upper right, and #
> 3 will go all the way across the bottom. # multiplot <- function(...,
> plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list
> from the ... arguments and plotlist plots <- c(list(...), plotlist)
> numPlots = length(plots) # If layout is NULL, then use 'cols' to determine
> layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of
> plots # nrow: Number of rows needed, calculated from # of cols layout <-
> matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow =
> ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { #
> Set up the page grid.newpage() pushViewport(viewport(layout =
> grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct
> location for (i in 1:numPlots) { # Get the i,j matrix positions of the
> regions that contain this subplot matchidx <- as.data.frame(which(layout ==
> i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row =
> matchidx$row, layout.pos.col = matchidx$col)) } } }
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

??? [[alternative HTML version deleted]]



------------------------------

Message: 26
Date: Fri, 15 Jan 2016 10:41:18 +0000
From: Peter Crowther <peter.crowther at melandra.com>
To: R list <r-help at r-project.org>
Subject: [R] R 3.2.3 on Windows 8.1 with interface scaling: how do I
??? produce metafiles that fill the whole canvas?
Message-ID:
??? <CALhdq6uebO=zDmUSodhO-qbnJeVSBKYb9LW0D2HyvC1ZGkHBcg at mail.gmail.com>
Content-Type: text/plain; charset=UTF-8

To reproduce (requires Windows):

1) Set your desktop scaling to 100%;
2) run the script below;
3) observe that the chart in scaled-example.wmf takes up the whole canvas.
4) Set your desktop scaling to greater than 100% (try 150% or 200%);
5) again run the script below;
6) observe that the chart in scaled-example.wmf takes up less than the
whole canvas.

Increasingly, we're seeing users run R on machines with very
high-resolution screens and scaled displays.? How can we correct for
this effect within R?

Cheers,

- Peter

Script:

-- start --
egfr <- c(222.6,176.4)
outcome <- data.frame(egfr)
hgb <- c(141,134)
predictors <- data.frame(hgb)
title <- "eGFR vs. Hgb"
ylab <- "eGFR"
xlab <- "Hgb"
gr1<-file.path(path.expand('~'),"scaled-example.wmf")
win.metafile(gr1)
plot(predictors[[1]],outcome[[1]],main=title,ylab=ylab,xlab=xlab)
dev.off()
-- end --



------------------------------

Subject: Digest Footer

_______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

------------------------------

End of R-help Digest, Vol 155, Issue 15
***************************************


  
	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Fri Jan 15 19:38:18 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 15 Jan 2016 19:38:18 +0100
Subject: [R] Segmentation Fault on Debian
Message-ID: <20160115183818.GA1829@localhost.localdomain>

Dear All,
I am running R on a debian testing machine and lately I have
experienced several segmentation faults (often when running Amelia on
some large data set).
However, please have a look at the script pasted at the end of the
email.

If I uncomment the line about the RJSDMX library (which does precisely
nothing in this script), the script causes a segmentation fault
killing my R session.
Anybody else experiences this?
Here is my session_info()

sessionInfo()
R version 3.2.3 (2015-12-10)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux stretch/sid

locale:
 [1] LC_CTYPE=en_GB.utf8          LC_NUMERIC=C
  [3] LC_TIME=en_GB.utf8           LC_COLLATE=en_GB.utf8
   [5] LC_MONETARY=en_GB.utf8       LC_MESSAGES=en_GB.utf8
    [7] LC_PAPER=en_GB.utf8          LC_NAME=en_GB.utf8
     [9] LC_ADDRESS=en_GB.utf8        LC_TELEPHONE=en_GB.utf8
     [11] LC_MEASUREMENT=en_GB.utf8    LC_IDENTIFICATION=en_GB.utf8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] RJSDMX_1.5        zoo_1.7-12        rJava_0.9-8
tempdisagg_0.24.0

loaded via a namespace (and not attached):
[1] grid_3.2.3      lattice_0.20-33


Regards

Lorenzo


#################################################
rm(list=ls())

library(tempdisagg)


## library(RJSDMX)

ts2 <- structure(c(339130, 356462, 363234, 378179, 367864, 378337,
392157,
402153, 376361, 392204, 403483, 414034, 391967, 406067, 419464,
434913, 410102, 424795, 437073, 448827, 415569, 430561, 444719,
455764, 419892, 444190, 454648, 466312, 439922, 448963, 465153,
475621, 445502, 457198, 473573, 485764, 463895, 470274, 484390,
490678, 478003, 483570, 499141, 509216, 481395, 492345, 511184,
513420, 483757, 490884, 514966, 515457, 497614, 510139, 523467,
526406, 499784, 519033, 532009, 531260, 521539, 532590, 553118,
557725, 548321, 556832, 578087, 578120, 566116, 580571, 587993,
569985, 534326, 539641, 564824, 568445, 558614, 570192, 594584,
598305, 593769, 598278, 620147, 615884, 611033, 609304, 630458,
624325, 614356, 627192, 649324, 645988, 642965, 645125, 669471,
665529, 664248, 669670, 694719), na.action = structure(1:64, class =
"omit"), .Tsp = c(1991,
2015.5, 4), class = "ts")


tsq <- td(ts2 ~ 1, to = "monthly", method = "denton-cholette")

tsm <- predict(tsq)


From drjimlemon at gmail.com  Fri Jan 15 21:41:31 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 16 Jan 2016 07:41:31 +1100
Subject: [R] write.xlsx- writing in a single sheet
In-Reply-To: <CADs3iXkMy30kzUBqTd2h9Pgtvq5hXVm2tnSiwQEP__ToF1ENHA@mail.gmail.com>
References: <CADs3iXkMy30kzUBqTd2h9Pgtvq5hXVm2tnSiwQEP__ToF1ENHA@mail.gmail.com>
Message-ID: <CA+8X3fX_MUFeyLFgu6J2cBH931Y9-=+kUX-b=-kTB4c76rXtwg@mail.gmail.com>

Hi Mohsen,
I can guess two things that you might want. One is to join all of the data
in the "CSV" files into a single data frame and then write that to a single
XLSX sheet. If the names and data structure of these files are similar
enough to "rbind" them, just do this in R and then write the result.

If the names are different, you can change the names to be consistent
across the data frames or reorder them.

If the structures are different, add columns of NAs where the numbers of
columns are different and "rbind" or "merge".

Jim

On Sat, Jan 16, 2016 at 1:43 AM, Mohsen Jafarikia <jafarikia at gmail.com>
wrote:

> Hello all:
>
> I am having problem writing a few files in a single sheet of excel. It
> seems R has problem writing on the same sheet. Maybe there is a command
> that I am missing. Here is the code I am using:
>
> library(xlsx)
>
> ifn11 <- "A1.xlsx"
> dat11  <- read.xlsx(ifn11, sheetName="A.csv", header = TRUE)
>
> ifn12 <- "A2.xlsx"
> dat12  <- read.xlsx(ifn12, sheetName="A.csv", header = TRUE)
>
> ifn13 <- "A3.xlsx"
> dat13  <- read.xlsx(ifn13, sheetName="A.csv", header = TRUE)
>
> ifn21 <- "F1.xlsx"
> dat21 <- read.xlsx(ifn21, sheetName="F.csv",header = TRUE)
>
> ifn22 <- "F2.xlsx"
> dat22  <- read.xlsx(ifn22, sheetName="F.csv",header = TRUE)
>
> ifn23 <- "F3.xlsx"
> dat23 <- read.xlsx(ifn23, sheetName="F.csv",header = TRUE)
>
> write.xlsx(dat11,  file="AC.xlsx", sheetName="A",  append=FALSE)
> write.xlsx(dat12,  file="AC.xlsx",                             append=
> TRUE)
> write.xlsx(dat13,  file="AC.xlsx",                             append=
> TRUE)
> write.xlsx(dat21,  file="AC.xlsx", sheetName="F",  append= TRUE)
> write.xlsx(dat22,  file="AC.xlsx",                             append=
> TRUE)
> write.xlsx(dat23,  file="AC.xlsx",                             append=
> TRUE)
>
> And here is the error message I am having:
>
> Error in .jcall(wb, "Lorg/apache/poi/ss/usermodel/Sheet;", "createSheet",
>  :
>   java.lang.IllegalArgumentException: The workbook already contains a sheet
> of this name
>
> This error message comes after running the write.xlsx(dat13,
>  file="AC.xlsx", showNA=FALSE, row.names=FALSE, append= TRUE) line. Program
> creates a sheet named "A" when writes dat11, then creates "sheet1" after
> writing dat12 and when tries to write dat13, it gives me error. It seems it
> tries to write on "sheet1" which already exists. I would like dat11, dat12
> and dat13 will be all written after each other on sheet "A" and dat12,
> dat22 and dat23 in sheet "F".
>
> Anybody has any comments please.
>
> Regards,
> Mohsen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From matteo.richiardi at gmail.com  Fri Jan 15 23:20:23 2016
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Fri, 15 Jan 2016 22:20:23 +0000
Subject: [R] creating cubes
Message-ID: <CABSrU1JAu5-UkZ1f-B_dDaKd+MqDAW981Dm9rta_Bw6+QGqdGw@mail.gmail.com>

What is the best way to store data in a cube? That is, I need to create a
data structure D with three indexes, say i,j,h, so that I can access each
data point D[i,j,h], and visualise sections like D[i,j,] or D[,,h].

I have tried to create an array of matrixes:

D <-matrix(matrix(NA,i,j),h)

but then D[i] returns a number, and not a matrix.

I imagine this is a dummy question, but I did search for an answer on
various R help sites, and found nothing straightforward. Being an
inexperienced R user, I prefer a simple solution, even at some efficiency
cost.

Many thanks for your help.
Matteo

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Fri Jan 15 23:24:16 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Fri, 15 Jan 2016 14:24:16 -0800
Subject: [R] rjags loading error
Message-ID: <CAJeYpE-chs_s6XQH7rrJOGXcsU=R300UD7oQ7JphhkdR0S9BFw@mail.gmail.com>

What has happened?!

I get the following error message when I try to load rjags package (w/
Windows 7)...


> require(rjags)
Loading required package: rjags
Error in get(method, envir = home) :
  lazy-load database 'C:/Program Files/R/R-3.2.3/library/rjags/R/rjags.rdb'
is corrupt
In addition: Warning messages:
1: In .registerS3method(fin[i, 1], fin[i, 2], fin[i, 3], fin[i, 4],  :
  restarting interrupted promise evaluation
2: In get(method, envir = home) :
  restarting interrupted promise evaluation
3: In get(method, envir = home) : internal error -3 in R_decompress1

I was able to use this package successfully a few months ago. Has something
changed?

Yes, I upgraded to R 3.2.3, but would that break the package?

Any help would be greatly appreciated!

Thanks.

-Dan

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Fri Jan 15 23:26:21 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Fri, 15 Jan 2016 14:26:21 -0800
Subject: [R] creating cubes
In-Reply-To: <CABSrU1JAu5-UkZ1f-B_dDaKd+MqDAW981Dm9rta_Bw6+QGqdGw@mail.gmail.com>
References: <CABSrU1JAu5-UkZ1f-B_dDaKd+MqDAW981Dm9rta_Bw6+QGqdGw@mail.gmail.com>
Message-ID: <CAJeYpE_Ls6xWoRp8b+7XgGFn==2BP8wc38w1p5hBS8nKQxADWw@mail.gmail.com>

How about: D<-array(dim=c(d1, d2, d3))?



On Fri, Jan 15, 2016 at 2:20 PM, Matteo Richiardi <
matteo.richiardi at gmail.com> wrote:

> What is the best way to store data in a cube? That is, I need to create a
> data structure D with three indexes, say i,j,h, so that I can access each
> data point D[i,j,h], and visualise sections like D[i,j,] or D[,,h].
>
> I have tried to create an array of matrixes:
>
> D <-matrix(matrix(NA,i,j),h)
>
> but then D[i] returns a number, and not a matrix.
>
> I imagine this is a dummy question, but I did search for an answer on
> various R help sites, and found nothing straightforward. Being an
> inexperienced R user, I prefer a simple solution, even at some efficiency
> cost.
>
> Many thanks for your help.
> Matteo
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From btupper at bigelow.org  Fri Jan 15 23:30:57 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Fri, 15 Jan 2016 17:30:57 -0500
Subject: [R] EXTRACT POINT DATA FROM NETCDF FILE
In-Reply-To: <1106920986.6201043.1452867365383.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.5.1452855602.18847.r-help@r-project.org>
	<1106920986.6201043.1452867365383.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <59BFA7C3-59CF-45C9-B04E-E498C9014E3D@bigelow.org>

Hello Peter,

It's not possible to help you resolve your difficulties with what you have shared. How we know what is mean to 'have no luck'?  What you need to do is provide a reproducible example - something anyone with R might be able to replicate.  Here is some terrific advice on how to make it easy for people to help you.

http://adv-r.had.co.nz/Reproducibility.html

In addition to the above, can you type at the R command line this...

> inp_file

And share the complete output.

Ben

> On Jan 15, 2016, at 9:16 AM, Peter Tuju <peterenos at ymail.com> wrote:
> 
> Daer R, usersI want to get a point data from netcdf file using the longitude and latitude, 
> but have no luck with the code I'm using. Here is the code. Please help!
> 
> rm( list = ls() )                                               # Clearing the workspace
> setwd( "/run/media/tuju/0767090047/extract_wrf_txt_file" )
> library( ncdf4 )                                       # Loading the ncdf4 package to read the netcdf data
> library(ncdf)
> inp_file <- open.ncdf( "wrfout_d01_2015-12-30.nc" )      # Reading the netcdf data
> time <- get.var.ncdf( inp_file, "Times" )                 # Extracting the forecasts time
> 
> sites <- read.csv("Station_Coordinates_TMA.csv", sep = "\t")
> attach(sites)
> 
> source( "whereis.R" )
> lat = get.var.ncdf(inp_file, "XLAT")  
> lon = get.var.ncdf(inp_file, "XLONG")
> 
> lower_left_lon_lat = c( 22, -16 )
> upper_right_lon_lat = c( 56, 7 )
> 
> ix0 = wherenearest( lower_left_lon_lat[1],  lon )
> ix1 = wherenearest( upper_right_lon_lat[1], lon )
> iy0 = wherenearest( lower_left_lon_lat[2],  lat )
> iy1 = wherenearest( upper_right_lon_lat[2], lat )
> 
> countx = ix1 - ix0 + 1
> county = iy1 - iy0 + 1
> 
> rainc = get.var.ncdf( inp_file, "RAINC", start = c( ix0, iy0, 1 ), count = c( countx, county, 1 ))
>  _____________
> Peter  E. Tuju
> Dar es Salaam
> T A N Z A N I A
> ----------------------

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From matteo.richiardi at gmail.com  Fri Jan 15 23:57:55 2016
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Fri, 15 Jan 2016 22:57:55 +0000
Subject: [R] creating cubes
In-Reply-To: <CAJeYpE_Ls6xWoRp8b+7XgGFn==2BP8wc38w1p5hBS8nKQxADWw@mail.gmail.com>
References: <CABSrU1JAu5-UkZ1f-B_dDaKd+MqDAW981Dm9rta_Bw6+QGqdGw@mail.gmail.com>
	<CAJeYpE_Ls6xWoRp8b+7XgGFn==2BP8wc38w1p5hBS8nKQxADWw@mail.gmail.com>
Message-ID: <CABSrU1L+6yqKx=Y+LHkg+49NgS4LE-FyOPkO3m_5PMWZ_zFNBg@mail.gmail.com>

Hi Daniel,
thanks for your answer. How can I populate the array with the
matrixes? Suppose I want to populate it with 10 matrixes

matrix(NA,5,5)

Matteo


On 15 January 2016 at 22:26, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> How about: D<-array(dim=c(d1, d2, d3))?
>
>
>
> On Fri, Jan 15, 2016 at 2:20 PM, Matteo Richiardi
> <matteo.richiardi at gmail.com> wrote:
>>
>> What is the best way to store data in a cube? That is, I need to create a
>> data structure D with three indexes, say i,j,h, so that I can access each
>> data point D[i,j,h], and visualise sections like D[i,j,] or D[,,h].
>>
>> I have tried to create an array of matrixes:
>>
>> D <-matrix(matrix(NA,i,j),h)
>>
>> but then D[i] returns a number, and not a matrix.
>>
>> I imagine this is a dummy question, but I did search for an answer on
>> various R help sites, and found nothing straightforward. Being an
>> inexperienced R user, I prefer a simple solution, even at some efficiency
>> cost.
>>
>> Many thanks for your help.
>> Matteo
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
>


From ddalthorp at usgs.gov  Sat Jan 16 00:17:02 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Fri, 15 Jan 2016 15:17:02 -0800
Subject: [R] creating cubes
In-Reply-To: <CABSrU1L+6yqKx=Y+LHkg+49NgS4LE-FyOPkO3m_5PMWZ_zFNBg@mail.gmail.com>
References: <CABSrU1JAu5-UkZ1f-B_dDaKd+MqDAW981Dm9rta_Bw6+QGqdGw@mail.gmail.com>
	<CAJeYpE_Ls6xWoRp8b+7XgGFn==2BP8wc38w1p5hBS8nKQxADWw@mail.gmail.com>
	<CABSrU1L+6yqKx=Y+LHkg+49NgS4LE-FyOPkO3m_5PMWZ_zFNBg@mail.gmail.com>
Message-ID: <CAJeYpE-75DD0OCP_shrQ-KDTP6mAbQ=UaE8m1fc3bznYgTdtHg@mail.gmail.com>

One possibility is to arrange it as a 3-dimensional array with two 5 x 5
matrices (i.e. a 5 x 5 x 2 array):

arrD3<-array(dim=c(5,5,2))

# the first matrix is (e.g.) random normal deviates:
mat1<-matrix(rnorm(25),nrow=5,ncol=5)

# the second matrix is (e.g.) random uniform deviates:
mat2<-matrix(runif(25),nrow=5,ncol=5)

# populating your 3-d array:
arrD3[,,1]<-mat1
arrD3[,,2]<-mat2

# to see what it looks like:
arrD3



On Fri, Jan 15, 2016 at 2:57 PM, Matteo Richiardi <
matteo.richiardi at gmail.com> wrote:

> Hi Daniel,
> thanks for your answer. How can I populate the array with the
> matrixes? Suppose I want to populate it with 10 matrixes
>
> matrix(NA,5,5)
>
> Matteo
>
>
> On 15 January 2016 at 22:26, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> > How about: D<-array(dim=c(d1, d2, d3))?
> >
> >
> >
> > On Fri, Jan 15, 2016 at 2:20 PM, Matteo Richiardi
> > <matteo.richiardi at gmail.com> wrote:
> >>
> >> What is the best way to store data in a cube? That is, I need to create
> a
> >> data structure D with three indexes, say i,j,h, so that I can access
> each
> >> data point D[i,j,h], and visualise sections like D[i,j,] or D[,,h].
> >>
> >> I have tried to create an array of matrixes:
> >>
> >> D <-matrix(matrix(NA,i,j),h)
> >>
> >> but then D[i] returns a number, and not a matrix.
> >>
> >> I imagine this is a dummy question, but I did search for an answer on
> >> various R help sites, and found nothing straightforward. Being an
> >> inexperienced R user, I prefer a simple solution, even at some
> efficiency
> >> cost.
> >>
> >> Many thanks for your help.
> >> Matteo
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> > --
> > Dan Dalthorp, PhD
> > USGS Forest and Rangeland Ecosystem Science Center
> > Forest Sciences Lab, Rm 189
> > 3200 SW Jefferson Way
> > Corvallis, OR 97331
> > ph: 541-750-0953
> > ddalthorp at usgs.gov
> >
>



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From rmh at temple.edu  Sat Jan 16 01:09:26 2016
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 15 Jan 2016 19:09:26 -0500
Subject: [R] creating cubes
In-Reply-To: <CABSrU1L+6yqKx=Y+LHkg+49NgS4LE-FyOPkO3m_5PMWZ_zFNBg@mail.gmail.com>
References: <CABSrU1JAu5-UkZ1f-B_dDaKd+MqDAW981Dm9rta_Bw6+QGqdGw@mail.gmail.com>
	<CAJeYpE_Ls6xWoRp8b+7XgGFn==2BP8wc38w1p5hBS8nKQxADWw@mail.gmail.com>
	<CABSrU1L+6yqKx=Y+LHkg+49NgS4LE-FyOPkO3m_5PMWZ_zFNBg@mail.gmail.com>
Message-ID: <CAGx1TMAyosEvXWE7K30uexr603VZS9ExU23b31y6qBeW4QdQNg@mail.gmail.com>

## I think you should look at the abind ## array bind
## function in the abind package.

install.packages("abind")
library(abind)
? abind
## the examples are comprehensive


## It allows constructions like these
w <- matrix(1:6, 2, 3, dimnames=list(letters[1:2], LETTERS[3:5]))
x <- w+10
y <- w+20
z <- w+30
wxyz <- abind(W=w, X=x, Y=y, Z=z, along=.5)
dim(wxyz)
wxyz
wxyz["W", , ]

## Rich

On Fri, Jan 15, 2016 at 5:57 PM, Matteo Richiardi
<matteo.richiardi at gmail.com> wrote:
> Hi Daniel,
> thanks for your answer. How can I populate the array with the
> matrixes? Suppose I want to populate it with 10 matrixes
>
> matrix(NA,5,5)
>
> Matteo
>
>
> On 15 January 2016 at 22:26, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
>> How about: D<-array(dim=c(d1, d2, d3))?
>>
>>
>>
>> On Fri, Jan 15, 2016 at 2:20 PM, Matteo Richiardi
>> <matteo.richiardi at gmail.com> wrote:
>>>
>>> What is the best way to store data in a cube? That is, I need to create a
>>> data structure D with three indexes, say i,j,h, so that I can access each
>>> data point D[i,j,h], and visualise sections like D[i,j,] or D[,,h].
>>>
>>> I have tried to create an array of matrixes:
>>>
>>> D <-matrix(matrix(NA,i,j),h)
>>>
>>> but then D[i] returns a number, and not a matrix.
>>>
>>> I imagine this is a dummy question, but I did search for an answer on
>>> various R help sites, and found nothing straightforward. Being an
>>> inexperienced R user, I prefer a simple solution, even at some efficiency
>>> cost.
>>>
>>> Many thanks for your help.
>>> Matteo
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>> --
>> Dan Dalthorp, PhD
>> USGS Forest and Rangeland Ecosystem Science Center
>> Forest Sciences Lab, Rm 189
>> 3200 SW Jefferson Way
>> Corvallis, OR 97331
>> ph: 541-750-0953
>> ddalthorp at usgs.gov
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From amoy_y at yahoo.com  Fri Jan 15 18:59:39 2016
From: amoy_y at yahoo.com (Amoy Yang)
Date: Fri, 15 Jan 2016 17:59:39 +0000 (UTC)
Subject: [R] Use SQL in R environment
References: <733209980.4690835.1452880779186.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <733209980.4690835.1452880779186.JavaMail.yahoo@mail.yahoo.com>

 Hi All, 
I am new here and a beginner for R. Can I use SQL procedure in R environment as it can be done in SAS starting with?PROC SQL;
Thanks for helps!

Amoy
	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Sat Jan 16 01:52:50 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Fri, 15 Jan 2016 16:52:50 -0800
Subject: [R] Use SQL in R environment
In-Reply-To: <733209980.4690835.1452880779186.JavaMail.yahoo@mail.yahoo.com>
References: <733209980.4690835.1452880779186.JavaMail.yahoo.ref@mail.yahoo.com>
	<733209980.4690835.1452880779186.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAJeYpE9_X+dvxj1cWPVH23G5y+e5Ter6RbXbo1JGY2drDzRKbw@mail.gmail.com>

Very general question...try searching "R SQL" on google for a start.

On Fri, Jan 15, 2016 at 9:59 AM, Amoy Yang via R-help <r-help at r-project.org>
wrote:

>  Hi All,
> I am new here and a beginner for R. Can I use SQL procedure in R
> environment as it can be done in SAS starting with PROC SQL;
> Thanks for helps!
>
> Amoy
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From miaojpm at gmail.com  Sat Jan 16 04:18:56 2016
From: miaojpm at gmail.com (jpm miao)
Date: Fri, 15 Jan 2016 19:18:56 -0800
Subject: [R] readWorksheetFromFile error message: invalid 'envir' argument
Message-ID: <CABcx46CAwoPvJSXg4MsvVgrMD+VVqpOyCDrUpN-OabS4fkqFWQ@mail.gmail.com>

Hi,

   I try to read a large xlsx file from by the function
"readWorksheetFromFile" in "XLConnect" package. Sometimes it works, but
sometimes it gives an error message. When it does not work and I try to run
the program line by line (in RStudio), then it would sometimes work, but
sometimes it does not. When it does not run,  restarting RStudio sometimes
works. How can I fix the problem?
   Thanks,


> date_col<-readWorksheetFromFile("dt-160FXO_many_xyz_data.xlsx", sheet=2,
region="A1:A3137")
Error in ls(envir = envir, all.names = private) :
  invalid 'envir' argument



Miao

	[[alternative HTML version deleted]]


From liuwensui at gmail.com  Sat Jan 16 04:24:41 2016
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 15 Jan 2016 21:24:41 -0600
Subject: [R] Use SQL in R environment
In-Reply-To: <733209980.4690835.1452880779186.JavaMail.yahoo@mail.yahoo.com>
References: <733209980.4690835.1452880779186.JavaMail.yahoo.ref@mail.yahoo.com>
	<733209980.4690835.1452880779186.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAKyN3iBOPQcLtxjR3FkrLwM9tGvrkqUBqmLtcXtkFM+bwi4yDQ@mail.gmail.com>

Check sqldf

On Friday, January 15, 2016, Amoy Yang via R-help <r-help at r-project.org>
wrote:

>  Hi All,
> I am new here and a beginner for R. Can I use SQL procedure in R
> environment as it can be done in SAS starting with PROC SQL;
> Thanks for helps!
>
> Amoy
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
WenSui Liu
https://statcompute.wordpress.com/

	[[alternative HTML version deleted]]


From maitra.mbox.ignored at inbox.com  Sat Jan 16 04:25:06 2016
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Fri, 15 Jan 2016 21:25:06 -0600
Subject: [R] installing tikzDevices for R 3.2.3
Message-ID: <20160115212506.884b617cd9ea18c0c17c6efc@inbox.com>

Hi,

I wanted to install tikzDevices on a installation of R (3.2.3) on a new machine. However, I am getting:

> install.packages('tikzDevices')

Warning message:
package ?tikzDevices? is not available (for R version 3.2.3) 

Is there any way out for me other than wait for the tikzDevices to be updated in the repos? I am on Fedora 23 with everything up to date.

Many thanks in advance for any suggestions and best wishes,
Ranjan

____________________________________________________________
Receive Notifications of Incoming Messages
Easily monitor multiple email accounts & access them with a click.
Visit http://www.inbox.com/notifier and check it out!


From maitra.mbox.ignored at inbox.com  Sat Jan 16 04:37:07 2016
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Fri, 15 Jan 2016 21:37:07 -0600
Subject: [R] installing tikzDevices for R 3.2.3
In-Reply-To: <20160115212506.884b617cd9ea18c0c17c6efc@inbox.com>
References: <20160115212506.884b617cd9ea18c0c17c6efc@inbox.com>
Message-ID: <20160115213707.8231923d32de773283d237bd@inbox.com>

On Fri, 15 Jan 2016 21:25:06 -0600 Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:

> Hi,
> 
> I wanted to install tikzDevices on a installation of R (3.2.3) on a new machine. However, I am getting:
> 
> > install.packages('tikzDevices')
> 
> Warning message:
> package ?tikzDevices? is not available (for R version 3.2.3) 
> 
> Is there any way out for me other than wait for the tikzDevices to be updated in the repos? I am on Fedora 23 with everything up to date.
> 
> Many thanks in advance for any suggestions and best wishes,
> Ranjan


Sorry to answer my own question, but I found a way out (only specific to tikzDevices):

install.packages("tikzDevice", repos="http://R-Forge.R-project.org")
 	
It would be nice to have a general approach, especially when R updated does not mostly mean that packages installed under and earlier version of R stop working.

Best wiehes,
Ranjan

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From xie at yihui.name  Sat Jan 16 05:42:41 2016
From: xie at yihui.name (Yihui Xie)
Date: Fri, 15 Jan 2016 22:42:41 -0600
Subject: [R] installing tikzDevices for R 3.2.3
In-Reply-To: <20160115213707.8231923d32de773283d237bd@inbox.com>
References: <20160115212506.884b617cd9ea18c0c17c6efc@inbox.com>
	<20160115213707.8231923d32de773283d237bd@inbox.com>
Message-ID: <CANROs4ckFjBUFhYzudK=LrLS7BWUjaTpt5VcQSjWh59E4X8ptQ@mail.gmail.com>

Please do not use http://r-forge.r-project.org/ to install tikzDevice.
We no longer update tikzDevice on R-Forge. It may be a problem of your
CRAN mirror. When in doubt, try the RStudio mirror (the chance that
Amazon CloudFront is down should be much smaller than a single metal
server somewhere):

install.packages("tikzDevice", repos="https://cran.rstudio.com")
# make sure your version of R supports https

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Fri, Jan 15, 2016 at 9:37 PM, Ranjan Maitra
<maitra.mbox.ignored at inbox.com> wrote:
> On Fri, 15 Jan 2016 21:25:06 -0600 Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
>
>> Hi,
>>
>> I wanted to install tikzDevices on a installation of R (3.2.3) on a new machine. However, I am getting:
>>
>> > install.packages('tikzDevices')
>>
>> Warning message:
>> package ?tikzDevices? is not available (for R version 3.2.3)
>>
>> Is there any way out for me other than wait for the tikzDevices to be updated in the repos? I am on Fedora 23 with everything up to date.
>>
>> Many thanks in advance for any suggestions and best wishes,
>> Ranjan
>
>
> Sorry to answer my own question, but I found a way out (only specific to tikzDevices):
>
> install.packages("tikzDevice", repos="http://R-Forge.R-project.org")
>
> It would be nice to have a general approach, especially when R updated does not mostly mean that packages installed under and earlier version of R stop working.
>
> Best wiehes,
> Ranjan
>
> ____________________________________________________________
> Can't remember your password? Do you need a strong and secure password?
> Use Password manager! It stores your passwords & protects your account.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maitra.mbox.ignored at inbox.com  Sat Jan 16 06:46:19 2016
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Fri, 15 Jan 2016 23:46:19 -0600
Subject: [R] installing tikzDevice for R 3.2.3
In-Reply-To: <CANROs4ckFjBUFhYzudK=LrLS7BWUjaTpt5VcQSjWh59E4X8ptQ@mail.gmail.com>
References: <20160115212506.884b617cd9ea18c0c17c6efc@inbox.com>
	<20160115213707.8231923d32de773283d237bd@inbox.com>
	<CANROs4ckFjBUFhYzudK=LrLS7BWUjaTpt5VcQSjWh59E4X8ptQ@mail.gmail.com>
Message-ID: <20160115234619.2110d07463badbca77abcc57@inbox.com>

Yihui,

Thanks very much! Explicitly specifying the RStudio server works, and I have reinstalled tikzDevice

Best wishes,
Ranjan



On Fri, 15 Jan 2016 22:42:41 -0600 Yihui Xie <xie at yihui.name> wrote:

> Please do not use http://r-forge.r-project.org/ to install tikzDevice.
> We no longer update tikzDevice on R-Forge. It may be a problem of your
> CRAN mirror. When in doubt, try the RStudio mirror (the chance that
> Amazon CloudFront is down should be much smaller than a single metal
> server somewhere):
> 
> install.packages("tikzDevice", repos="https://cran.rstudio.com")
> # make sure your version of R supports https
> 
> Regards,
> Yihui
> --
> Yihui Xie <xieyihui at gmail.com>
> Web: http://yihui.name
> 
> 
> On Fri, Jan 15, 2016 at 9:37 PM, Ranjan Maitra
> <maitra.mbox.ignored at inbox.com> wrote:
> > On Fri, 15 Jan 2016 21:25:06 -0600 Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
> >
> >> Hi,
> >>
> >> I wanted to install tikzDevices on a installation of R (3.2.3) on a new machine. However, I am getting:
> >>
> >> > install.packages('tikzDevices')
> >>
> >> Warning message:
> >> package ?tikzDevices? is not available (for R version 3.2.3)
> >>
> >> Is there any way out for me other than wait for the tikzDevices to be updated in the repos? I am on Fedora 23 with everything up to date.
> >>
> >> Many thanks in advance for any suggestions and best wishes,
> >> Ranjan
> >
> >
> > Sorry to answer my own question, but I found a way out (only specific to tikzDevices):
> >
> > install.packages("tikzDevice", repos="http://R-Forge.R-project.org")
> >
> > It would be nice to have a general approach, especially when R updated does not mostly mean that packages installed under and earlier version of R stop working.
> >
> > Best wiehes,
> > Ranjan
> >
> > ____________________________________________________________
> > Can't remember your password? Do you need a strong and secure password?
> > Use Password manager! It stores your passwords & protects your account.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From bbolker at gmail.com  Sat Jan 16 05:45:58 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 16 Jan 2016 04:45:58 +0000
Subject: [R] installing tikzDevices for R 3.2.3
References: <20160115212506.884b617cd9ea18c0c17c6efc@inbox.com>
	<20160115213707.8231923d32de773283d237bd@inbox.com>
Message-ID: <loom.20160116T054330-923@post.gmane.org>

Ranjan Maitra <maitra.mbox.ignored <at> inbox.com> writes:

> 
> On Fri, 15 Jan 2016 21:25:06 -0600 Ranjan Maitra <maitra.mbox.ignored <at>
inbox.com> wrote:
> 
> > Hi,
> > 
> > I wanted to install tikzDevices on a installation of 
> R (3.2.3) on a new machine. However, I am getting:
> > 
> > > install.packages('tikzDevices')
> > 
> > Warning message:
> > package ?tikzDevices? is not available (for R version 3.2.3) 
> > 
> > Is there any way out for me other than wait for the tikzDevices to be
updated in the repos? I am on Fedora 23
> with everything up to date.
> > 
> > Many thanks in advance for any suggestions and best wishes,
> > Ranjan
> 
> Sorry to answer my own question, but I found a way out 
> (only specific to tikzDevices):
> 
> install.packages("tikzDevice", repos="http://R-Forge.R-project.org")

  This is all a little bit surprising since tizkDevice seems
perfectly fine on CRAN: 
https://cran.rstudio.com/web/packages/tikzDevice/index.html

  What are getOption("repos") and sessionInfo() ?
  Perhaps there is some temporary binary-building delay because
R 3.2.3 just came out?

   Did you try install.packages("tikzDevice", type="source") ?


From sunnysingha.analytics at gmail.com  Sat Jan 16 12:22:59 2016
From: sunnysingha.analytics at gmail.com (Sandeep Rana)
Date: Sat, 16 Jan 2016 16:52:59 +0530
Subject: [R] featurised matrix factorisation in R
Message-ID: <3403DC79-41AC-4861-997C-E64918A9FDC9@gmail.com>

Hi,
I want to perform non-negative featurised Matrix factorisation in ?R?. Is there any ?R' package or documentation that explains on this ?

Regards,
Sunny
	[[alternative HTML version deleted]]


From maitra.mbox.ignored at inbox.com  Sat Jan 16 16:09:02 2016
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Sat, 16 Jan 2016 09:09:02 -0600
Subject: [R] installing tikzDevice for R 3.2.3
In-Reply-To: <loom.20160116T054330-923@post.gmane.org>
References: <20160115212506.884b617cd9ea18c0c17c6efc@inbox.com>
	<20160115213707.8231923d32de773283d237bd@inbox.com>
	<loom.20160116T054330-923@post.gmane.org>
Message-ID: <20160116090902.966313f3bb27fc68ae335913@inbox.com>

On Sat, 16 Jan 2016 04:45:58 +0000 Ben Bolker <bbolker at gmail.com> wrote:

> Ranjan Maitra <maitra.mbox.ignored <at> inbox.com> writes:
> 
> > 
> > On Fri, 15 Jan 2016 21:25:06 -0600 Ranjan Maitra <maitra.mbox.ignored <at>
> inbox.com> wrote:
> > 
> > > Hi,
> > > 
> > > I wanted to install tikzDevices on a installation of 
> > R (3.2.3) on a new machine. However, I am getting:
> > > 
> > > > install.packages('tikzDevices')
> > > 
> > > Warning message:
> > > package ?tikzDevices? is not available (for R version 3.2.3) 
> > > 
> > > Is there any way out for me other than wait for the tikzDevices to be
> updated in the repos? I am on Fedora 23
> > with everything up to date.
> > > 
> > > Many thanks in advance for any suggestions and best wishes,
> > > Ranjan
> > 
> > Sorry to answer my own question, but I found a way out 
> > (only specific to tikzDevices):
> > 
> > install.packages("tikzDevice", repos="http://R-Forge.R-project.org")
> 
>   This is all a little bit surprising since tizkDevice seems
> perfectly fine on CRAN: 
> https://cran.rstudio.com/web/packages/tikzDevice/index.html
> 
>   What are getOption("repos") and sessionInfo() ?
>   Perhaps there is some temporary binary-building delay because
> R 3.2.3 just came out?
> 
>    Did you try install.packages("tikzDevice", type="source") ?

I am sorry: I made an error earlier -- I made a mistake (used the plural instead of 'tikzDevices'). 

This was a false alarm. Apologies for that!

However, while I don't know if this would be possible to get from an error message but it would have been preferred if the message had said that there was no such package in R, rather than say that the package is not available for R version 3.2.3 which led me to believe that an updated version was not available.

However, at the end of a long day, it was my mistake: sorry again for that!

Best wishes,
Ranjan


> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From bhh at xs4all.nl  Sat Jan 16 16:36:09 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Sat, 16 Jan 2016 16:36:09 +0100
Subject: [R] featurised matrix factorisation in R
In-Reply-To: <3403DC79-41AC-4861-997C-E64918A9FDC9@gmail.com>
References: <3403DC79-41AC-4861-997C-E64918A9FDC9@gmail.com>
Message-ID: <CBA67C6E-949A-482F-8479-B917C45A18A3@xs4all.nl>


> On 16 Jan 2016, at 12:22, Sandeep Rana <sunnysingha.analytics at gmail.com> wrote:
> 
> Hi,
> I want to perform non-negative featurised Matrix factorisation in ?R?. Is there any ?R' package or documentation that explains on this ?
> 

Well I don't know about the "featurised" but googling on (you didn't do that?)

non-negative featurised Matrix factorisation R

found this:  https://cran.r-project.org/web/packages/NMF/index.html

Berend

> Regards,
> Sunny
> 	[[alternative HTML version deleted]]
> 


No html please.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tonja.krueger at web.de  Sat Jan 16 15:53:54 2016
From: tonja.krueger at web.de (tonja.krueger at web.de)
Date: Sat, 16 Jan 2016 15:53:54 +0100
Subject: [R] use gcheckbox in gWidgets to switch on/off gframe
Message-ID: <trinity-af63e592-8d73-4373-99a2-4cdbadd416c0-1452956034921@3capp-webde-bap32>



Dear All,
I?m trying to create a GUI using gWidgets.
I would like to have a checkbox to ?switch on/off? different frames within the GUI. Ideally if one frame is switched on, all other frames would be switched off.
Unfortunatly I only came as far as this:?

library(gWidgets)?
Population <- c("A","B","C","D","E","F")
w = gwindow("")
g1 = ggroup(horizontal = F, cont=w)
g2 = ggroup(horizontal = T, cont=g1)
glabel("Population:", cont=g2)
Station = gcombobox(Population, editable=F, cont=g2, handler=NULL)
gseparator(horizontal=T, container=g1, expand=F)
gcheckbox("checked", container=g1, handler=function(h,...) {
enabled ( frame1 ) <- ?cat(svalue(h$obj))
})
frame1 <- gframe ( "A:" , cont = g1 , horizontal=FALSE )
lyt1 <- glayout ( cont = frame1)
widget_list <- list ( )
lyt1 [1,1] <- "A1:"
lyt1 [1,2,expand = TRUE] <- (widget_list$A1 <- gedit(" ", cont=lyt1, handler=NULL))
lyt1 [2,1] <- "A2:"
lyt1 [2,2,expand = TRUE] <- (widget_list$A2 <- gedit(" ", cont=lyt1, handler=NULL))
gcheckbox("checked", container=g1, handler=function(h,...) {
enabled ( frame2 ) <- ?cat(svalue(h$obj))
})
frame2 <- gframe ( "B:" , cont = g1 , horizontal=FALSE )
lyt2 <- glayout ( cont = frame2)
widget_list <- list ( )
lyt2 [1,1] <- "B1:"
lyt2 [1,2, expand = TRUE] <- (widget_list$B1 <- gedit(" ", cont=lyt2, handler=NULL))

When I type in:?
enabled ( frame2 ) <- ?F; enabled ( frame2 ) <- ?T
it does what I would like it to do. But when I check the checkbox it will only work once. Thank you for any suggestions!
Tonja


From maechler at stat.math.ethz.ch  Sat Jan 16 19:34:49 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 16 Jan 2016 19:34:49 +0100
Subject: [R] installing tikzDevice for R 3.2.3
In-Reply-To: <20160115234619.2110d07463badbca77abcc57@inbox.com>
References: <20160115212506.884b617cd9ea18c0c17c6efc@inbox.com>
	<20160115213707.8231923d32de773283d237bd@inbox.com>
	<CANROs4ckFjBUFhYzudK=LrLS7BWUjaTpt5VcQSjWh59E4X8ptQ@mail.gmail.com>
	<20160115234619.2110d07463badbca77abcc57@inbox.com>
Message-ID: <22170.36169.10783.153504@stat.math.ethz.ch>

>>>>> Ranjan Maitra <maitra.mbox.ignored at inbox.com>
>>>>>     on Fri, 15 Jan 2016 23:46:19 -0600 writes:

    > Yihui, Thanks very much! Explicitly specifying the RStudio
    > server works, and I have reinstalled tikzDevice

    > Best wishes, Ranjan

Good,  and thanks to Yihui indeed!

One note:  The  "cloud" CRAN server is nowadays reachable as

    https://cloud.r-project.org

and that is now the preferred URL.  Rstudio is currently still
paying the service -> Thank you JJ and Rstudio staff! -- but
that may not have to remain so, and the  R Foundation and R Core team
are much happier to have a generic

    https://cloud.r-project.org   as "default CRAN mirror".

Martin Maechler
(as 'Secreatry General' of the R Foundation)

    > On Fri, 15 Jan 2016 22:42:41 -0600 Yihui Xie
    > <xie at yihui.name> wrote:

    >> Please do not use http://r-forge.r-project.org/ to
    >> install tikzDevice.  We no longer update tikzDevice on
    >> R-Forge. It may be a problem of your CRAN mirror. When in
    >> doubt, try the RStudio mirror (the chance that Amazon
    >> CloudFront is down should be much smaller than a single
    >> metal server somewhere):
    >> 
    >> install.packages("tikzDevice",
    >> repos="https://cran.rstudio.com") # make sure your
    >> version of R supports https
    >> 
    >> Regards, Yihui
    >> --
    >> Yihui Xie <xieyihui at gmail.com> Web: http://yihui.name
    >> 
    >> 
    >> On Fri, Jan 15, 2016 at 9:37 PM, Ranjan Maitra
    >> <maitra.mbox.ignored at inbox.com> wrote: > On Fri, 15 Jan
    >> 2016 21:25:06 -0600 Ranjan Maitra
    >> <maitra.mbox.ignored at inbox.com> wrote:
    >> >
    >> >> Hi,
    >> >>
    >> >> I wanted to install tikzDevices on a installation of R
    >> (3.2.3) on a new machine. However, I am getting:
    >> >>
    >> >> > install.packages('tikzDevices')
    >> >>
    >> >> Warning message: >> package ?tikzDevices? is not
    >> available (for R version 3.2.3)
    >> >>
    >> >> Is there any way out for me other than wait for the
    >> tikzDevices to be updated in the repos? I am on Fedora 23
    >> with everything up to date.
    >> >>
    >> >> Many thanks in advance for any suggestions and best
    >> wishes, >> Ranjan
    >> >
    >> >
    >> > Sorry to answer my own question, but I found a way out
    >> (only specific to tikzDevices):
    >> >
    >> > install.packages("tikzDevice",
    >> repos="http://R-Forge.R-project.org")
    >> >
    >> > It would be nice to have a general approach, especially
    >> when R updated does not mostly mean that packages
    >> installed under and earlier version of R stop working.
    >> >
    >> > Best wiehes, > Ranjan


From rolf.fankhauser at gepdata.ch  Sat Jan 16 19:45:47 2016
From: rolf.fankhauser at gepdata.ch (Rolf Fankhauser)
Date: Sat, 16 Jan 2016 19:45:47 +0100
Subject: [R] Aggregate records to 10min
Message-ID: <569A8FDB.9070607@gepdata.ch>

Hi

I would like to aggregate a rainfall series with 1min records (timestamp 
and value of 0.1mm from a tipping bucket raingauge) to 10min values by 
summing up the values.

# ptime is a POSIXlt datetime value with tz="GMT"

t10min <- 600*floor(as.integer(as.POSIXct(data$ptime))/600)
w10min <- tapply(data$value, format(as.POSIXct(t10min, tz="GMT", origin 
= "1970-01-01"), "%Y-%m-%d %H:%M"), sum)
write.table(as.matrix(w10min),"data 10min.txt", row.names=TRUE, 
col.names=FALSE, quote=FALSE)

This code works but I would like to have the result in datetime format 
of %m/%d/%Y %H:%M. When I output this format the records are not 
chronologically sorted but text-sorted because dimnames of w10min is of 
type character (because of the format function).
Is there an easier way summing up the records to 10min records?

Thanks,
Rolf


From thierry.onkelinx at inbo.be  Sat Jan 16 20:11:24 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Sat, 16 Jan 2016 20:11:24 +0100
Subject: [R] Ordinal regression with some categories combined for some
	data
In-Reply-To: <CAN-Z0xWF42sioxrAdUphnK6KR9ZvHseEcF+tDND=mophWUHQEg@mail.gmail.com>
References: <CAN-Z0xWF42sioxrAdUphnK6KR9ZvHseEcF+tDND=mophWUHQEg@mail.gmail.com>
Message-ID: <CAJuCY5wyNEMNmOjso7Y2eA4m6dXQkscTGZ1kYFTn4MzQkBehXA@mail.gmail.com>

Dear Bob,

I don't know any package that handles ordinal data the way you are looking
for. I'd just would comment on the ordinal regression. Would the time of
loss be the ordinal response? That seems inefficient to me when you have a
lot of time points (= lots of ordinal classes). IMHO the survival analysis
would make more sense.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-01-15 16:48 GMT+01:00 Bob O'Hara <rni.boh at gmail.com>:

> Hi!
>
> I've been asked about a problem where I think I can see how to write
> the model, but don't know if it's been implemented in R. It's not
> something I work on a lot, so I'm hoping someone else can point me to
> an answer straight away.
>
> The researcher has been carrying out germination experiments: lost of
> seeds are put in several conditions (temperature humidity etc.), and
> every few days they are checked to see if they have germinated.
> Because the days are discrete I think it makes sense to view this as
> an ordinal regression problem (rather than as an interval censored
> survival analysis). But what makes this tricky is that there are days
> when the researcher only checked some seeds. So for some seeds the
> germination might fall into more than one category.
>
> Is there a package in R that can handle this, i.e. do an ordinal
> regression where for some observations the categories are interval
> censored? Or is it easier to go straight to a full interval-censored
> survival analysis?
>
> Bob
>
> --
> Bob O'Hara
>
> Biodiversity and Climate Research Centre
> Senckenberganlage 25
> D-60325 Frankfurt am Main,
> Germany
>
> Tel: +49 69 798 40226
> Mobile: +49 1515 888 5440
> WWW:   http://www.bik-f.de/root/index.php?page_id=219
> Blog: http://occamstypewriter.org/boboh/
> Journal of Negative Results - EEB: www.jnr-eeb.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rolf.fankhauser at gepdata.ch  Sat Jan 16 22:50:43 2016
From: rolf.fankhauser at gepdata.ch (Rolf Fankhauser)
Date: Sat, 16 Jan 2016 22:50:43 +0100
Subject: [R] use gcheckbox in gWidgets to switch on/off gframe
In-Reply-To: <trinity-af63e592-8d73-4373-99a2-4cdbadd416c0-1452956034921@3capp-webde-bap32>
References: <trinity-af63e592-8d73-4373-99a2-4cdbadd416c0-1452956034921@3capp-webde-bap32>
Message-ID: <569ABB33.6040505@gepdata.ch>

Hi Tonja

I removed the cat(...) in the event handlers because cat writes TRUE or 
FALSE to the console but doesn't seem to return a value.
Then it worked. You should initialize the frames to be disabled or the 
checkboxes to be checked that both states are consistant.

Regards, Rolf


tonja.krueger at web.de wrote:
>
> Dear All,
> I?m trying to create a GUI using gWidgets.
> I would like to have a checkbox to ?switch on/off? different frames within the GUI. Ideally if one frame is switched on, all other frames would be switched off.
> Unfortunatly I only came as far as this:
>
> library(gWidgets)
> Population <- c("A","B","C","D","E","F")
> w = gwindow("")
> g1 = ggroup(horizontal = F, cont=w)
> g2 = ggroup(horizontal = T, cont=g1)
> glabel("Population:", cont=g2)
> Station = gcombobox(Population, editable=F, cont=g2, handler=NULL)
> gseparator(horizontal=T, container=g1, expand=F)
> gcheckbox("checked", container=g1, handler=function(h,...) {
> enabled ( frame1 ) <-  cat(svalue(h$obj))
> })
> frame1 <- gframe ( "A:" , cont = g1 , horizontal=FALSE )
> lyt1 <- glayout ( cont = frame1)
> widget_list <- list ( )
> lyt1 [1,1] <- "A1:"
> lyt1 [1,2,expand = TRUE] <- (widget_list$A1 <- gedit(" ", cont=lyt1, handler=NULL))
> lyt1 [2,1] <- "A2:"
> lyt1 [2,2,expand = TRUE] <- (widget_list$A2 <- gedit(" ", cont=lyt1, handler=NULL))
> gcheckbox("checked", container=g1, handler=function(h,...) {
> enabled ( frame2 ) <-  cat(svalue(h$obj))
> })
> frame2 <- gframe ( "B:" , cont = g1 , horizontal=FALSE )
> lyt2 <- glayout ( cont = frame2)
> widget_list <- list ( )
> lyt2 [1,1] <- "B1:"
> lyt2 [1,2, expand = TRUE] <- (widget_list$B1 <- gedit(" ", cont=lyt2, handler=NULL))
>
> When I type in:
> enabled ( frame2 ) <-  F; enabled ( frame2 ) <-  T
> it does what I would like it to do. But when I check the checkbox it will only work once. Thank you for any suggestions!
> Tonja
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Rolf Fankhauser
Hegenheimerstrasse 129
4055 Basel
Tel. 061 321 45 25


From dulcalma at bigpond.com  Sun Jan 17 01:38:12 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Sun, 17 Jan 2016 10:38:12 +1000
Subject: [R] Ordinal regression with some categories combined for some
	data
In-Reply-To: <CAN-Z0xWF42sioxrAdUphnK6KR9ZvHseEcF+tDND=mophWUHQEg@mail.gmail.com>
References: <CAN-Z0xWF42sioxrAdUphnK6KR9ZvHseEcF+tDND=mophWUHQEg@mail.gmail.com>
Message-ID: <000001d150bf$5955dcf0$0c0196d0$@bigpond.com>

Hi

I have never seen germination experiments carried out as ordinal regression.
Most germination tests are done using a nls model.
For some species germination may only start a week after planting and then
germinate over 2 or 3 days.
If all germinated over the experimental period and  there is no change in
germination between readings then read them accordingly. 
If there are ungerminated seeds at the end; have these then had germinable
tests been applied to them?.

I agree with Thierry about survival analysis.
If however you are wanting to get into latency then ordinal models may be OK
Another package to do ordinal regression is VGAM 

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bob O'Hara
Sent: Saturday, 16 January 2016 01:49
To: r-help
Subject: [R] Ordinal regression with some categories combined for some data

Hi!

I've been asked about a problem where I think I can see how to write
the model, but don't know if it's been implemented in R. It's not
something I work on a lot, so I'm hoping someone else can point me to
an answer straight away.

The researcher has been carrying out germination experiments: lost of
seeds are put in several conditions (temperature humidity etc.), and
every few days they are checked to see if they have germinated.
Because the days are discrete I think it makes sense to view this as
an ordinal regression problem (rather than as an interval censored
survival analysis). But what makes this tricky is that there are days
when the researcher only checked some seeds. So for some seeds the
germination might fall into more than one category.

Is there a package in R that can handle this, i.e. do an ordinal
regression where for some observations the categories are interval
censored? Or is it easier to go straight to a full interval-censored
survival analysis?

Bob

-- 
Bob O'Hara

Biodiversity and Climate Research Centre
Senckenberganlage 25
D-60325 Frankfurt am Main,
Germany

Tel: +49 69 798 40226
Mobile: +49 1515 888 5440
WWW:   http://www.bik-f.de/root/index.php?page_id=219
Blog: http://occamstypewriter.org/boboh/
Journal of Negative Results - EEB: www.jnr-eeb.org

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Sun Jan 17 02:09:30 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Sun, 17 Jan 2016 11:09:30 +1000
Subject: [R] Panel plots for means of cyclical observations
In-Reply-To: <CAMVnMy2RRGB1e5LJje7P7LTR_aXO7kkT73qntGoQQEjpFGyNMw@mail.gmail.com>
References: <CAMVnMy2RRGB1e5LJje7P7LTR_aXO7kkT73qntGoQQEjpFGyNMw@mail.gmail.com>
Message-ID: <000101d150c3$b8fda350$2af8e9f0$@bigpond.com>

Hi

Continuing on from the data.frame code for z that you supplied
Using the data without summarizing first

z$mth = format(date "%m")
z$mth = format(date, "%m")

xyplot(rainfall ~ as.numeric(mth), z,
       groups = yr,
       type = "l",
       auto.key = T,
       scales = list(x = list(at = 1:12,
                              labels = month.abb,
                              rot = 60)),
       panel = panel.superpose,
       panel.groups = function(x,y, ...){
                        panel.average(x,y, fun = mean, horizontal = FALSE,
...)
                      }
) ## xyplot

You may want to look into the zoo package as it has several date grouping
functions.

untested
z.tom <- aggregate(rainfall ~ year +month, z, mean, na.rm = T)

xyplot(rainfall ~ month, z.tom, groups = year, 
              scales = ... ,
             panel = panel.superpose)

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Kieran
Sent: Friday, 15 January 2016 22:16
To: R-help at r-project.org
Subject: [R] Panel plots for means of cyclical observations

I want to create a panel plot using xyplot of a line graph whose
x-axis is months of the year and y-axis is the average rainfall in a
given month over the 6 years the data spans.

There should be two levels in this panel plot: odd and even months.

Creating this plot without splitting it into levels is quite
straightforward (creating a for loop to compute a vector of averages)
but the approach is not useful if you want to split the plots into
different levels.

Here is the code:

dfmt <- "%d/%m/%Y"
date <- seq(as.Date("01/01/2010", dfmt), as.Date("31/12/2015", dfmt),
    "day")
month <- months(date)
rainfall <- runif(2191, 0, 150)
monthsOfYear <- c("January", "February", "March", "April",
    "May", "June", "July", "August", "September", "October",
    "November", "December")

parity <- match(month, monthsOfYear) %% 2
# even parity = 0, odd parity = 1

z <- data.frame(rainfall, date, month, parity)

The problem with using xyplot( y ~ x | f, data=z .. ) is the x and y I
want to plot are not columns in z but rather some kind of statistical
summary of columns.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Sun Jan 17 02:14:44 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Sun, 17 Jan 2016 11:14:44 +1000
Subject: [R] rjags loading error
In-Reply-To: <CAJeYpE-chs_s6XQH7rrJOGXcsU=R300UD7oQ7JphhkdR0S9BFw@mail.gmail.com>
References: <CAJeYpE-chs_s6XQH7rrJOGXcsU=R300UD7oQ7JphhkdR0S9BFw@mail.gmail.com>
Message-ID: <000201d150c4$737cdcf0$5a7696d0$@bigpond.com>

Hi

What version of JAGS are you using

JAGS 4.0.0 is current

library(rjags)
Loading required package: coda
Linked to JAGS 4.0.0
Loaded modules: basemod,bugs

Regards

Duncan


Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dalthorp,
Daniel
Sent: Saturday, 16 January 2016 08:24
To: r-help at r-project.org
Subject: [R] rjags loading error

What has happened?!

I get the following error message when I try to load rjags package (w/
Windows 7)...


> require(rjags)
Loading required package: rjags
Error in get(method, envir = home) :
  lazy-load database 'C:/Program Files/R/R-3.2.3/library/rjags/R/rjags.rdb'
is corrupt
In addition: Warning messages:
1: In .registerS3method(fin[i, 1], fin[i, 2], fin[i, 3], fin[i, 4],  :
  restarting interrupted promise evaluation
2: In get(method, envir = home) :
  restarting interrupted promise evaluation
3: In get(method, envir = home) : internal error -3 in R_decompress1

I was able to use this package successfully a few months ago. Has something
changed?

Yes, I upgraded to R 3.2.3, but would that break the package?

Any help would be greatly appreciated!

Thanks.

-Dan

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Sun Jan 17 05:07:49 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 17 Jan 2016 17:07:49 +1300
Subject: [R] The "cloud" CRAN server.
Message-ID: <569B1395.80300@auckland.ac.nz>


In another thread you wrote:

> One note:  The  "cloud" CRAN server is nowadays reachable as
>
>     https://cloud.r-project.org
>
> and that is now the preferred URL.

Should I understand from this that it is considered advisable that I use 
this URL as my default repository for downloading R and contributed 
packages from CRAN?  Rather than making use of my "local mirror" (in my 
case http://cran.stat.auckland.ac.nz)?

I have set

    options(repos="https://cloud.r-project.org")

in my .Rprofile, and everything seems to work, and go like a train.  Are 
there any disadvantages to so doing?

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From pdalgd at gmail.com  Sun Jan 17 11:51:49 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 17 Jan 2016 11:51:49 +0100
Subject: [R] The "cloud" CRAN server.
In-Reply-To: <569B1395.80300@auckland.ac.nz>
References: <569B1395.80300@auckland.ac.nz>
Message-ID: <262AD88F-F12D-4834-9A3C-8EA10AE6D872@gmail.com>


> On 17 Jan 2016, at 05:07 , Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> 
> In another thread you wrote:
> 
>> One note:  The  "cloud" CRAN server is nowadays reachable as
>> 
>>    https://cloud.r-project.org
>> 
>> and that is now the preferred URL.
> 
> Should I understand from this that it is considered advisable that I use this URL as my default repository for downloading R and contributed packages from CRAN?  Rather than making use of my "local mirror" (in my case http://cran.stat.auckland.ac.nz)?

No, not necessarily. It just means that the cloud service now has an official name as an R-project server. The main issue is that the name no longer implies a reliance on Rstudio as a company, nor can it be interpreted as advertisment of their services. 

(Rstudio still pioneered the service and, at least for now, also provides the infrastructure, and they deserve a lot of thanks for that. It was just that people were being apprehensive about using Rstudio services without using their products and about the long-term reliability of infrastructure rooted in a single commercial company.)

- Peter D.

> 
> I have set
> 
>   options(repos="https://cloud.r-project.org")
> 
> in my .Rprofile, and everything seems to work, and go like a train.  Are there any disadvantages to so doing?
> 
> cheers,
> 
> Rolf
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From cg.pettersson at lantmannen.com  Sun Jan 17 13:29:36 2016
From: cg.pettersson at lantmannen.com (CG Pettersson)
Date: Sun, 17 Jan 2016 12:29:36 +0000
Subject: [R] Problems with data structure when using plsr() from
	package	pls
In-Reply-To: <1A8C1289955EF649A09086A153E2672403C998D72F@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
	<CAM_vju=QwHPrjFgpv7+DDbqJf2AZ5z7OKZGKGG-oS7wjru9g+g@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C998D72F@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <HE1PR02MB098744D830CA9950BC9A19B38CCF0@HE1PR02MB0987.eurprd02.prod.outlook.com>

Thanks a lot to both of you for the effort and nice suggestions.

I have tested all suggested coding variants and I do succeed in changing the data structure, for example like this (with a test using str())

> names(n96) <- paste("n96", 1:96)
> frame1 <- cbind(gushVM, n96)
>  
> str(frame1)
'data.frame':   15 obs. of  97 variables:
 $ gushVM: num  2 23.2 14.3 40.9 32.8 29.1 0 79 0 0 ...
 $ n96 1 : num  34.2 31.8 52 36.1 47 ...
 $ n96 2 : num  23.3 21.1 33.6 24.3 31.5 ...
 $ n96 3 : num  97.5 80.6 89.5 97.3 122.4 ...
 $ n96 4 : num  79.2 64.4 79.9 79.4 99.7 ...
 $ n96 5 : num  24.1 25.8 45.3 25.8 45.2 ...
 $ n96 6 : num  83.1 81.7 119 82.6 140.2 ...
 $ n96 7 : num  10 10.8 16.2 10.9 15.9 ...
 $ n96 8 : num  129 124 180 135 202 ...
 $ n96 9 : num  58.8 52.9 84 61.7 76.7 ...   and so on until row variable 96 inside n96

But using this dataframe with plsr() results in the same error message, like:

> pls1 <- plsr(gushVM ~ n96, data = frame1)
Error in model.frame.default(formula = gushVM ~ n96, data = frame1) : 
  invalid type (list) for variable 'n96'

Which is the same message as ever. The same happens with the other two suggestions, even if the detail structure of "n96" inside "frame" differ a little whe I test it with str().
Still stuck, but thanks all the same!

/CG

-----Ursprungligt meddelande-----
Fr?n: S Ellison [mailto:S.Ellison at LGCGroup.com] 
Skickat: den 15 januari 2016 14:39
Till: Sarah Goslee; CG Pettersson
Kopia: r-help at r-project.org
?mne: RE: [R] Problems with data structure when using plsr() from package pls


> > I am trying to make pls-regression using plsr() from package pls, 
> > with Mevik & Wehrens (2007) as tutorial and the datasets from the package.
> > Everything works real nice as long as I use the supplied datasets, 
> > but I don?t understand how to prepare my own data.
> > This is what I have done:
> >
> > > frame1 <- data.frame(gushVM, I(n96))

Reading ?plsr examples and inspecting the data they use, you need to arrange frame1 so that it has the data from n96 included as columns with names of the from "n96.xxx" whre xxx can be numbers, names etc.

If n96 is a data frame, try something like
names(n96) <- paste("n96", 1:96)
frame1 <- cbind(gushVM, n96)

pls1 <- plsr(gushVM ~ n96, data = frame1)


If n96 is a matrix, 

frame1 <- data.frame(gushVM, n96=n96)

should also give you a data frame with names of the right format.

I() wrapped round a matrix or data frame does nothing like what is needed if you include it in a data frame construction, so either things have changed since the tutorial was written, or the authors were not handling a matrix or data frame with I().

S Ellison






*******************************************************************
This email and any attachments are confidential. Any use, copying or disclosure other than by the intended recipient is unauthorised. If you have received this message in error, please notify the sender immediately via +44(0)20 8943 7000 or notify postmaster at lgcgroup.com and delete this message and any copies from your computer and network. 
LGC Limited. Registered in England 2991879. 
Registered office: Queens Road, Teddington, Middlesex, TW11 0LY, UK

From lars52r at gmail.com  Sun Jan 17 14:42:51 2016
From: lars52r at gmail.com (Lars Bishop)
Date: Sun, 17 Jan 2016 08:42:51 -0500
Subject: [R] Order of formula terms in model.matrix
Message-ID: <DF70D770-C30D-425D-93AE-B10BC832C661@gmail.com>

I?d appreciate your help on understanding the following. 

It is not very clear to me from the model.matrix documentation, why simply changing the order of terms in the formula may change the number of resulting columns. Please note I?m purposely not including main effects in the model formula in this case. 
 

set.seed(1)
x1 <- rnorm(100)
f1 <- factor(sample(letters[1:3], 100, replace = TRUE))
trt <- sample(c(-1,1), 100, replace = TRUE)
df <- data.frame(x1=x1, f1=f1, trt=trt)

dim(model.matrix( ~ x1:trt + f1:trt, data = df))
[1] 100 4

dim(model.matrix(~ f1:trt + x1:trt, data = df))
[1] 100 5


Thanks,
Lars. 

From f.harrell at Vanderbilt.Edu  Sun Jan 17 15:58:15 2016
From: f.harrell at Vanderbilt.Edu (Frank Harrell)
Date: Sun, 17 Jan 2016 08:58:15 -0600
Subject: [R] Ordinal regression with some categories combined for some
	data
Message-ID: <569BAC07.2050603@vanderbilt.edu>

This does seem to be a good situation for ordinal regression.  The R rms 
package's orm function allows for thousands of categories in Y. But it 
doesn't handle censoring.

This discussion would be better for stats.stackexchange.com

Frank
-- 
------------------------------------------------------------------------
Frank E Harrell Jr 	Professor and Chairman 	School of Medicine

	Department of *Biostatistics* 	*Vanderbilt University*


	[[alternative HTML version deleted]]


From rolf.fankhauser at gepdata.ch  Sun Jan 17 16:17:08 2016
From: rolf.fankhauser at gepdata.ch (Fankhauser GEP Data Consulting)
Date: Sun, 17 Jan 2016 16:17:08 +0100
Subject: [R] Aggregate records to 10min
In-Reply-To: <CA+8X3fXhGFtgXes2vynq4O7eYjECboSsJAmzQqPennL7+rHgWQ@mail.gmail.com>
References: <569A8FDB.9070607@gepdata.ch>
	<CA+8X3fXhGFtgXes2vynq4O7eYjECboSsJAmzQqPennL7+rHgWQ@mail.gmail.com>
Message-ID: <569BB074.4000307@gepdata.ch>

Hi Jim,
Thanks a lot! It works now. I didn't remember how to access the 
datetimes in w10min. names(...) is the solution!

Rolf

Jim Lemon wrote:
> Hi Rolf,
> If I get the above, perhaps if you change the names of w10min after 
> applying the calculation:
>
> raindata<-data.frame(value=round(runif(60,0,4),1),
> ptime=paste("2016-01-17 ","15:",0:59,sep=""))
> t10min <- 600*floor(as.integer(as.POSIXct(raindata$ptime))/600)
> w10min <- tapply(raindata$value,t10min,sum)
> names(w10min)<-format(as.POSIXct(as.numeric(names(w10min)),
>  tz="AEST",origin="1970-01-01"),"%m/%d/%Y %H:%M")
>
> Jim
>
>
> On Sun, Jan 17, 2016 at 5:45 AM, Rolf Fankhauser 
> <rolf.fankhauser at gepdata.ch <mailto:rolf.fankhauser at gepdata.ch>> wrote:
>
>     Hi
>
>     I would like to aggregate a rainfall series with 1min records
>     (timestamp and value of 0.1mm from a tipping bucket raingauge) to
>     10min values by summing up the values.
>
>     # ptime is a POSIXlt datetime value with tz="GMT"
>
>     t10min <- 600*floor(as.integer(as.POSIXct(data$ptime))/600)
>     w10min <- tapply(data$value, format(as.POSIXct(t10min, tz="GMT",
>     origin = "1970-01-01"), "%Y-%m-%d %H:%M"), sum)
>     write.table(as.matrix(w10min),"data 10min.txt", row.names=TRUE,
>     col.names=FALSE, quote=FALSE)
>
>     This code works but I would like to have the result in datetime
>     format of %m/%d/%Y %H:%M. When I output this format the records
>     are not chronologically sorted but text-sorted because dimnames of
>     w10min is of type character (because of the format function).
>     Is there an easier way summing up the records to 10min records?
>
>     Thanks,
>     Rolf
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
______________________________

Fankhauser GEP Data Consulting
Hegenheimerstrasse 129
4055 Basel

Tel:    ++41-(0)61-321-4525
Mobile: ++41-(0)79-440-7706
rolf.fankhauser at gepdata.ch
www.gepdata.ch


From rni.boh at gmail.com  Sun Jan 17 19:29:21 2016
From: rni.boh at gmail.com (Bob O'Hara)
Date: Sun, 17 Jan 2016 19:29:21 +0100
Subject: [R] Ordinal regression with some categories combined for some
	data
In-Reply-To: <000001d150bf$5955dcf0$0c0196d0$@bigpond.com>
References: <CAN-Z0xWF42sioxrAdUphnK6KR9ZvHseEcF+tDND=mophWUHQEg@mail.gmail.com>
	<000001d150bf$5955dcf0$0c0196d0$@bigpond.com>
Message-ID: <CAN-Z0xVs8oxpRkictbW7FrqAm1sY6KxjiMp=1r_XRdm-j6Ykjg@mail.gmail.com>

Thanks, Thierry & Duncan. I'll go down the survival analysis route.

The data are for central American epiphytes, so not your usual
species. Visually there's definitely differences in the times of
germination, but not in eventual germination, so that's
straightforward.

Bob

On 17 January 2016 at 01:38, Duncan Mackay <dulcalma at bigpond.com> wrote:
> Hi
>
> I have never seen germination experiments carried out as ordinal regression.
> Most germination tests are done using a nls model.
> For some species germination may only start a week after planting and then
> germinate over 2 or 3 days.
> If all germinated over the experimental period and  there is no change in
> germination between readings then read them accordingly.
> If there are ungerminated seeds at the end; have these then had germinable
> tests been applied to them?.
>
> I agree with Thierry about survival analysis.
> If however you are wanting to get into latency then ordinal models may be OK
> Another package to do ordinal regression is VGAM
>
> Regards
>
> Duncan
>
> Duncan Mackay
> Department of Agronomy and Soil Science
> University of New England
> Armidale NSW 2351
> Email: home: mackay at northnet.com.au
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bob O'Hara
> Sent: Saturday, 16 January 2016 01:49
> To: r-help
> Subject: [R] Ordinal regression with some categories combined for some data
>
> Hi!
>
> I've been asked about a problem where I think I can see how to write
> the model, but don't know if it's been implemented in R. It's not
> something I work on a lot, so I'm hoping someone else can point me to
> an answer straight away.
>
> The researcher has been carrying out germination experiments: lost of
> seeds are put in several conditions (temperature humidity etc.), and
> every few days they are checked to see if they have germinated.
> Because the days are discrete I think it makes sense to view this as
> an ordinal regression problem (rather than as an interval censored
> survival analysis). But what makes this tricky is that there are days
> when the researcher only checked some seeds. So for some seeds the
> germination might fall into more than one category.
>
> Is there a package in R that can handle this, i.e. do an ordinal
> regression where for some observations the categories are interval
> censored? Or is it easier to go straight to a full interval-censored
> survival analysis?
>
> Bob
>
> --
> Bob O'Hara
>
> Biodiversity and Climate Research Centre
> Senckenberganlage 25
> D-60325 Frankfurt am Main,
> Germany
>
> Tel: +49 69 798 40226
> Mobile: +49 1515 888 5440
> WWW:   http://www.bik-f.de/root/index.php?page_id=219
> Blog: http://occamstypewriter.org/boboh/
> Journal of Negative Results - EEB: www.jnr-eeb.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Bob O'Hara

Biodiversity and Climate Research Centre
Senckenberganlage 25
D-60325 Frankfurt am Main,
Germany

Tel: +49 69 798 40226
Mobile: +49 1515 888 5440
WWW:   http://www.bik-f.de/root/index.php?page_id=219
Blog: http://occamstypewriter.org/boboh/
Journal of Negative Results - EEB: www.jnr-eeb.org


From ccberry at ucsd.edu  Sun Jan 17 19:34:40 2016
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Sun, 17 Jan 2016 10:34:40 -0800
Subject: [R] Order of formula terms in model.matrix
In-Reply-To: <DF70D770-C30D-425D-93AE-B10BC832C661@gmail.com>
References: <DF70D770-C30D-425D-93AE-B10BC832C661@gmail.com>
Message-ID: <alpine.OSX.2.20.1601171015270.673@charles-berrys-macbook.local>

On Sun, 17 Jan 2016, Lars Bishop wrote:

> I?d appreciate your help on understanding the following. 
>

> It is not very clear to me from the model.matrix documentation, why 
> simply changing the order of terms in the formula may change the number 
> of resulting columns. Please note I?m purposely not including main 
> effects in the model formula in this case.


IIRC, there are some heuristics involved harking back to the White Book. I 
recall there have been discussions of whether and how this could be fixed 
before on this list and or R-devel, but I cannot seem to lay my browser on 
them right now.


> 
>
> set.seed(1)
> x1 <- rnorm(100)
> f1 <- factor(sample(letters[1:3], 100, replace = TRUE))
> trt <- sample(c(-1,1), 100, replace = TRUE)
> df <- data.frame(x1=x1, f1=f1, trt=trt)
>
> dim(model.matrix( ~ x1:trt + f1:trt, data = df))
> [1] 100 4
>
> dim(model.matrix(~ f1:trt + x1:trt, data = df))
> [1] 100 5
>

By `x1:trt' I guess you mean the same thing as `I(x1*trt)'.

If you use the latter form, the issue you raise goes away.

Note that `I(some.expr)' gives you the ability to force the behavior of 
model.matrix to be exactly what you want by suitably crafting `some.expr', 
heuristics notwithstanding.

HTH,

Chuck

From r.turner at auckland.ac.nz  Sun Jan 17 20:49:10 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 18 Jan 2016 08:49:10 +1300
Subject: [R] The "cloud" CRAN server.
In-Reply-To: <262AD88F-F12D-4834-9A3C-8EA10AE6D872@gmail.com>
References: <569B1395.80300@auckland.ac.nz>
	<262AD88F-F12D-4834-9A3C-8EA10AE6D872@gmail.com>
Message-ID: <569BF036.9010203@auckland.ac.nz>

On 17/01/16 23:51, peter dalgaard wrote:
>

>> On 17 Jan 2016, at 05:07 , Rolf Turner <r.turner at auckland.ac.nz>
>> wrote:
>>
>>
>> In another thread you wrote:
>>
>>> One note:  The  "cloud" CRAN server is nowadays reachable as
>>>
>>> https://cloud.r-project.org
>>>
>>> and that is now the preferred URL.
>>
>> Should I understand from this that it is considered advisable that
>> I use this URL as my default repository for downloading R and
>> contributed packages from CRAN?  Rather than making use of my
>> "local mirror" (in my case http://cran.stat.auckland.ac.nz)?
>
> No, not necessarily. It just means that the cloud service now has an
> official name as an R-project server. The main issue is that the name
> no longer implies a reliance on Rstudio as a company, nor can it be
> interpreted as advertisment of their services.
>
> (Rstudio still pioneered the service and, at least for now, also
> provides the infrastructure, and they deserve a lot of thanks for
> that. It was just that people were being apprehensive about using
> Rstudio services without using their products and about the long-term
> reliability of infrastructure rooted in a single commercial
> company.)

Okay,  gottit.  Thanks Peter.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From lars52r at gmail.com  Sun Jan 17 20:53:55 2016
From: lars52r at gmail.com (Lars Bishop)
Date: Sun, 17 Jan 2016 14:53:55 -0500
Subject: [R] Order of formula terms in model.matrix
In-Reply-To: <alpine.OSX.2.20.1601171015270.673@charles-berrys-macbook.local>
References: <DF70D770-C30D-425D-93AE-B10BC832C661@gmail.com>
	<alpine.OSX.2.20.1601171015270.673@charles-berrys-macbook.local>
Message-ID: <AB5C2093-376B-4BAB-9832-C5441E64C6DA@gmail.com>

This is very helpful, thanks!

Lars.


> On Jan 17, 2016, at 1:34 PM, Charles C. Berry <ccberry at ucsd.edu> wrote:
> 
> On Sun, 17 Jan 2016, Lars Bishop wrote:
> 
>> I?d appreciate your help on understanding the following. 
> 
>> It is not very clear to me from the model.matrix documentation, why simply changing the order of terms in the formula may change the number of resulting columns. Please note I?m purposely not including main effects in the model formula in this case.
> 
> 
> IIRC, there are some heuristics involved harking back to the White Book. I recall there have been discussions of whether and how this could be fixed before on this list and or R-devel, but I cannot seem to lay my browser on them right now.
> 
> 
>> 
>> set.seed(1)
>> x1 <- rnorm(100)
>> f1 <- factor(sample(letters[1:3], 100, replace = TRUE))
>> trt <- sample(c(-1,1), 100, replace = TRUE)
>> df <- data.frame(x1=x1, f1=f1, trt=trt)
>> 
>> dim(model.matrix( ~ x1:trt + f1:trt, data = df))
>> [1] 100 4
>> 
>> dim(model.matrix(~ f1:trt + x1:trt, data = df))
>> [1] 100 5
>> 
> 
> By `x1:trt' I guess you mean the same thing as `I(x1*trt)'.
> 
> If you use the latter form, the issue you raise goes away.
> 
> Note that `I(some.expr)' gives you the ability to force the behavior of model.matrix to be exactly what you want by suitably crafting `some.expr', heuristics notwithstanding.
> 
> HTH,
> 
> Chuck
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lars52r at gmail.com  Sun Jan 17 21:10:19 2016
From: lars52r at gmail.com (Lars Bishop)
Date: Sun, 17 Jan 2016 15:10:19 -0500
Subject: [R] Order of formula terms in model.matrix
In-Reply-To: <AB5C2093-376B-4BAB-9832-C5441E64C6DA@gmail.com>
References: <DF70D770-C30D-425D-93AE-B10BC832C661@gmail.com>
	<alpine.OSX.2.20.1601171015270.673@charles-berrys-macbook.local>
	<AB5C2093-376B-4BAB-9832-C5441E64C6DA@gmail.com>
Message-ID: <F11C48E4-8A4D-4E1F-BF79-4CA15B6BC550@gmail.com>

Question: I(a*b) would work as long as both ?a" and ?b? are numeric. Is there a way I can force the behaviour of model.matrix when one of these variables is a factor (as in "f1:trt" from my example below)? 

Specifically, based on my example below, I would like to always return the first matrix (where the first level of ?f1? is omitted in the resulting matrix). This would?t happen if the user specifies the terms in reverse order (as per the second matrix). 

Thanks again,
Lars. 


> On Jan 17, 2016, at 2:53 PM, Lars Bishop <lars52r at gmail.com> wrote:
> 
> This is very helpful, thanks!
> 
> Lars.
> 
> 
>> On Jan 17, 2016, at 1:34 PM, Charles C. Berry <ccberry at ucsd.edu> wrote:
>> 
>> On Sun, 17 Jan 2016, Lars Bishop wrote:
>> 
>>> I?d appreciate your help on understanding the following. 
>> 
>>> It is not very clear to me from the model.matrix documentation, why simply changing the order of terms in the formula may change the number of resulting columns. Please note I?m purposely not including main effects in the model formula in this case.
>> 
>> 
>> IIRC, there are some heuristics involved harking back to the White Book. I recall there have been discussions of whether and how this could be fixed before on this list and or R-devel, but I cannot seem to lay my browser on them right now.
>> 
>> 
>>> 
>>> set.seed(1)
>>> x1 <- rnorm(100)
>>> f1 <- factor(sample(letters[1:3], 100, replace = TRUE))
>>> trt <- sample(c(-1,1), 100, replace = TRUE)
>>> df <- data.frame(x1=x1, f1=f1, trt=trt)
>>> 
>>> dim(model.matrix( ~ x1:trt + f1:trt, data = df))
>>> [1] 100 4
>>> 
>>> dim(model.matrix(~ f1:trt + x1:trt, data = df))
>>> [1] 100 5
>>> 
>> 
>> By `x1:trt' I guess you mean the same thing as `I(x1*trt)'.
>> 
>> If you use the latter form, the issue you raise goes away.
>> 
>> Note that `I(some.expr)' gives you the ability to force the behavior of model.matrix to be exactly what you want by suitably crafting `some.expr', heuristics notwithstanding.
>> 
>> HTH,
>> 
>> Chuck
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 


From naresh_gurbuxani at hotmail.com  Sun Jan 17 21:25:47 2016
From: naresh_gurbuxani at hotmail.com (Naresh Gurbuxani)
Date: Sun, 17 Jan 2016 15:25:47 -0500
Subject: [R] tikzDevice and Sweave
Message-ID: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>

I want to use tikz() function in tikzDevice package so that it generates a pdf file to be included in the bigger tex file.  Below code works, but directly inserts tikz commands in the output tex file.  
This works:
<<name = tikzFig, echo = FALSE, results = tex>>= 
This does not work:
<<name = tikzFig, echo = FALSE, fig = TRUE>>=
Full code is given below:
\documentclass{article}\usepackage{tikz}
Figure~\ref{tikzExampleFig} is and an example of \texttt{tikzDevice} package. 
\begin{figure}\begin{center}
<<name = tikzFig, echo = FALSE, results = tex>>=
library(tikzDevice)tikz(console = TRUE)plot(sin, -pi, pi, main = "A stand alone TikZ plot", xlab = "x", ylab = "sin(x)")dummy <- dev.off()@
\caption{Example of tikz graph}\label{tikzExampleFig}\end{center}\end{figure}
\end{document}


 		 	   		  
	[[alternative HTML version deleted]]


From pradeep.bisht0303 at gmail.com  Sun Jan 17 16:31:08 2016
From: pradeep.bisht0303 at gmail.com (Pradeep Bisht)
Date: Sun, 17 Jan 2016 10:31:08 -0500
Subject: [R] Reading a tab delimted file of varying length using read.table
Message-ID: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>

Hello Experts  ,

Being a SAS developer I am finding it difficult to perform some of data
cleaning in R that are quite easy to perform in SAS .

I have been trying to read a .dat file and after a lot of attempts have
failed to find a solution . Maybe R doesn't have the functionality right
now or I am not looking in the right place . Here is my code .

f5=read.table("http://data.princeton.edu/wws509/datasets/divorce.dat
<http://www.linkedin.com/redir/redirect?url=http%3A%2F%2Fdata%2Eprinceton%2Eedu%2Fwws509%2Fdatasets%2Fdivorce%2Edat&urlhash=GVbR&_t=tracking_anet>
",
header=T,
sep="\t",
colClasses = c("numeric", "character", "character","character", "double",
"character" ) )
The error i get i
?s?
this .
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,
:
scan() expected 'a real', got '912-15yearsNoNo10.546No'

Also does read.table always calls scan in background to do its job . If so
why use read.table in first place .

Pradeep?

	[[alternative HTML version deleted]]


From sunnysingha.analytics at gmail.com  Sun Jan 17 08:38:44 2016
From: sunnysingha.analytics at gmail.com (Sandeep Rana)
Date: Sun, 17 Jan 2016 13:08:44 +0530
Subject: [R] featurised matrix factorisation in R
In-Reply-To: <CBA67C6E-949A-482F-8479-B917C45A18A3@xs4all.nl>
References: <3403DC79-41AC-4861-997C-E64918A9FDC9@gmail.com>
	<CBA67C6E-949A-482F-8479-B917C45A18A3@xs4all.nl>
Message-ID: <9873019D-E6C0-4BBF-B3FC-E6F13583A75B@gmail.com>

Thanks Berend,
Through NMF we would be able to capture relationship between users and items and discover latent factors/topics of those users items. 
I want to combine the above with effect of user features(e.g: gender, age, location, time? etc.) and item features (e.g.: item_category, price, location, etc?.) hence, the name featurised 
matrix factorisation. I didn?t find yet reference to package that takes in user and item features too.

Regards,
Sunny

> On 16-Jan-2016, at 9:06 PM, Berend Hasselman <bhh at xs4all.nl> wrote:
> 
> 
>> On 16 Jan 2016, at 12:22, Sandeep Rana <sunnysingha.analytics at gmail.com> wrote:
>> 
>> Hi,
>> I want to perform non-negative featurised Matrix factorisation in ?R?. Is there any ?R' package or documentation that explains on this ?
>> 
> 
> Well I don't know about the "featurised" but googling on (you didn't do that?)
> 
> non-negative featurised Matrix factorisation R
> 
> found this:  https://cran.r-project.org/web/packages/NMF/index.html
> 
> Berend
> 
>> Regards,
>> Sunny
>> 	[[alternative HTML version deleted]]
>> 
> 
> 
> No html please.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 


	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sun Jan 17 21:40:24 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 17 Jan 2016 15:40:24 -0500
Subject: [R] tikzDevice and Sweave
In-Reply-To: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>
References: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>
Message-ID: <569BFC38.90405@gmail.com>

On 17/01/2016 3:25 PM, Naresh Gurbuxani wrote:
> I want to use tikz() function in tikzDevice package so that it generates a pdf file to be included in the bigger tex file.  Below code works, but directly inserts tikz commands in the output tex file.
> This works:
> <<name = tikzFig, echo = FALSE, results = tex>>=
> This does not work:
> <<name = tikzFig, echo = FALSE, fig = TRUE>>=
> Full code is given below:
> \documentclass{article}\usepackage{tikz}
> Figure~\ref{tikzExampleFig} is and an example of \texttt{tikzDevice} package.
> \begin{figure}\begin{center}
> <<name = tikzFig, echo = FALSE, results = tex>>=
> library(tikzDevice)tikz(console = TRUE)plot(sin, -pi, pi, main = "A stand alone TikZ plot", xlab = "x", ylab = "sin(x)")dummy <- dev.off()@
> \caption{Example of tikz graph}\label{tikzExampleFig}\end{center}\end{figure}
> \end{document}

Your example isn't usable -- please post in plain text, not HTML.

I can't tell whether you are trying to use Sweave or knitr.  If you're 
using knitr, see the discussion of dev = "tikz" in 
<http://yihui.name/knitr/>.  If you're using Sweave, you probably need 
pgfSweave.

Duncan Murdoch


From murdoch.duncan at gmail.com  Sun Jan 17 21:50:01 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 17 Jan 2016 15:50:01 -0500
Subject: [R] tikzDevice and Sweave
In-Reply-To: <569BFC38.90405@gmail.com>
References: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>
	<569BFC38.90405@gmail.com>
Message-ID: <569BFE79.7090607@gmail.com>

On 17/01/2016 3:40 PM, Duncan Murdoch wrote:
> On 17/01/2016 3:25 PM, Naresh Gurbuxani wrote:
>> I want to use tikz() function in tikzDevice package so that it generates a pdf file to be included in the bigger tex file.  Below code works, but directly inserts tikz commands in the output tex file.
>> This works:
>> <<name = tikzFig, echo = FALSE, results = tex>>=
>> This does not work:
>> <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>> Full code is given below:
>> \documentclass{article}\usepackage{tikz}
>> Figure~\ref{tikzExampleFig} is and an example of \texttt{tikzDevice} package.
>> \begin{figure}\begin{center}
>> <<name = tikzFig, echo = FALSE, results = tex>>=
>> library(tikzDevice)tikz(console = TRUE)plot(sin, -pi, pi, main = "A stand alone TikZ plot", xlab = "x", ylab = "sin(x)")dummy <- dev.off()@
>> \caption{Example of tikz graph}\label{tikzExampleFig}\end{center}\end{figure}
>> \end{document}
>
> Your example isn't usable -- please post in plain text, not HTML.
>
> I can't tell whether you are trying to use Sweave or knitr.  If you're
> using knitr, see the discussion of dev = "tikz" in
> <http://yihui.name/knitr/>.  If you're using Sweave, you probably need
> pgfSweave.
>

Okay, I see from your subject line that you're using Sweave.  I don't 
think you can do better than your "This works" example without switching 
to knitr.

Duncan Murdoch


From naresh_gurbuxani at hotmail.com  Sun Jan 17 21:54:20 2016
From: naresh_gurbuxani at hotmail.com (Naresh Gurbuxani)
Date: Sun, 17 Jan 2016 15:54:20 -0500
Subject: [R] tikzDevice and Sweave
In-Reply-To: <569BFC38.90405@gmail.com>
References: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>,
	<569BFC38.90405@gmail.com>
Message-ID: <SNT150-W750181125F1A3C387B5109FACF0@phx.gbl>

Resending as a useable example

\documentclass{article}
\usepackage{tikz}

\begin{document}

Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package.?

\begin{figure}
\begin{center}

<<name = tikzFig, echo = FALSE, results = tex>>=
# % <<name = tikzFig, echo = FALSE, fig = TRUE>>=

# %<<tikzFig, echo = FALSE, fig = TRUE>>=
# setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
library(tikzDevice)

tikz(console = TRUE)
plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab = "sin(x)")
dummy <- dev.off()
@

\caption{Example of tikz graph}
\label{tikzExampleFig}
\end{center}
\end{figure}

\end{document}

----------------------------------------
> Subject: Re: [R] tikzDevice and Sweave
> To: naresh_gurbuxani at hotmail.com; r-help at r-project.org
> From: murdoch.duncan at gmail.com
> Date: Sun, 17 Jan 2016 15:40:24 -0500
>
> On 17/01/2016 3:25 PM, Naresh Gurbuxani wrote:
>> I want to use tikz() function in tikzDevice package so that it generates a pdf file to be included in the bigger tex file. Below code works, but directly inserts tikz commands in the output tex file.
>> This works:
>> <<name = tikzFig, echo = FALSE, results = tex>>=
>> This does not work:
>> <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>> Full code is given below:
>> \documentclass{article}\usepackage{tikz}
>> Figure~\ref{tikzExampleFig} is and an example of \texttt{tikzDevice} package.
>> \begin{figure}\begin{center}
>> <<name = tikzFig, echo = FALSE, results = tex>>=
>> library(tikzDevice)tikz(console = TRUE)plot(sin, -pi, pi, main = "A stand alone TikZ plot", xlab = "x", ylab = "sin(x)")dummy <- dev.off()@
>> \caption{Example of tikz graph}\label{tikzExampleFig}\end{center}\end{figure}
>> \end{document}
>
> Your example isn't usable -- please post in plain text, not HTML.
>
> I can't tell whether you are trying to use Sweave or knitr. If you're
> using knitr, see the discussion of dev = "tikz" in
> <http://yihui.name/knitr/>. If you're using Sweave, you probably need
> pgfSweave.
>
> Duncan Murdoch
>
 		 	   		  

From milujisb at gmail.com  Sun Jan 17 21:56:18 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Sun, 17 Jan 2016 21:56:18 +0100
Subject: [R] Split Strings
Message-ID: <CAMLwc7Phkmrvm+PBYHmcwXHiUOqFZ1cxm9Tf3w7VAifMN60C+w@mail.gmail.com>

I have a list of strings of different lengths and would like to split each
string by underscore "_"

pc_m2_45_ssp3_wheat
pc_m2_45_ssp3_wheat
ssp3_maize
m2_wheat

I would like to separate each part of the string into different columns
such as

pc m2 45 ssp3 wheat

But because of the different lengths - I would like NA in the columns for
the variables have fewer parts such as

NA NA NA m2 wheat

I have tried unlist(strsplit(x, "_")) to split, it works for one variable
but not for the list - gives me "non-character argument" error. I would
highly appreciate any help. Thank you!

	[[alternative HTML version deleted]]


From dusa.adrian at unibuc.ro  Sun Jan 17 22:17:15 2016
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Sun, 17 Jan 2016 23:17:15 +0200
Subject: [R] Split Strings
In-Reply-To: <CAMLwc7Phkmrvm+PBYHmcwXHiUOqFZ1cxm9Tf3w7VAifMN60C+w@mail.gmail.com>
References: <CAMLwc7Phkmrvm+PBYHmcwXHiUOqFZ1cxm9Tf3w7VAifMN60C+w@mail.gmail.com>
Message-ID: <CAJ=0CtCkvnx=dgyXCpKoN4dXcw8s6NaOr-C-82MgjKCNAdqDOw@mail.gmail.com>

Try this:

mylist <- list("pc_m2_45_ssp3_wheat", "pc_m2_45_ssp3_wheat", "ssp3_maize",
"m2_wheat")
mylist <- lapply(mylist, function(x) unlist(strsplit(x, split="_")))
allstrings <- unique(unlist(mylist))
lapply(mylist, function(x) allstrings[match(allstrings, x)])

[[1]]
[1] "pc"    "m2"    "45"    "ssp3"  "wheat" NA

[[2]]
[1] "pc"    "m2"    "45"    "ssp3"  "wheat" NA

[[3]]
[1] NA   NA   NA   "pc" NA   "m2"

[[4]]
[1] NA   "pc" NA   NA   "m2" NA

Hope this helps,
Adrian

On Sun, Jan 17, 2016 at 10:56 PM, Miluji Sb <milujisb at gmail.com> wrote:

> I have a list of strings of different lengths and would like to split each
> string by underscore "_"
>
> pc_m2_45_ssp3_wheat
> pc_m2_45_ssp3_wheat
> ssp3_maize
> m2_wheat
>
> I would like to separate each part of the string into different columns
> such as
>
> pc m2 45 ssp3 wheat
>
> But because of the different lengths - I would like NA in the columns for
> the variables have fewer parts such as
>
> NA NA NA m2 wheat
>
> I have tried unlist(strsplit(x, "_")) to split, it works for one variable
> but not for the list - gives me "non-character argument" error. I would
> highly appreciate any help. Thank you!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From rolf.fankhauser at gepdata.ch  Sun Jan 17 22:42:43 2016
From: rolf.fankhauser at gepdata.ch (Rolf Fankhauser)
Date: Sun, 17 Jan 2016 22:42:43 +0100
Subject: [R] Reading a tab delimted file of varying length using
 read.table
In-Reply-To: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
References: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
Message-ID: <569C0AD3.6060704@gepdata.ch>

Hello Pradeep

I downloaded divorce.dat but I could not find tabs between the columns.
You defined tab as separator, so your columns should be separated by tabs.
Therefore read.table reads the whole first line and wants to save the 
result as numeric because you defined the first column as numeric.

That's my interpretation
So, use tab, comma or semicolon as delimiter then it should work.

Rolf

Pradeep Bisht wrote:
> Hello Experts  ,
>
> Being a SAS developer I am finding it difficult to perform some of data
> cleaning in R that are quite easy to perform in SAS .
>
> I have been trying to read a .dat file and after a lot of attempts have
> failed to find a solution . Maybe R doesn't have the functionality right
> now or I am not looking in the right place . Here is my code .
>
> f5=read.table("http://data.princeton.edu/wws509/datasets/divorce.dat
> <http://www.linkedin.com/redir/redirect?url=http%3A%2F%2Fdata%2Eprinceton%2Eedu%2Fwws509%2Fdatasets%2Fdivorce%2Edat&urlhash=GVbR&_t=tracking_anet>
> ",
> header=T,
> sep="\t",
> colClasses = c("numeric", "character", "character","character", "double",
> "character" ) )
> The error i get i
> ?s?
> this .
> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,
> :
> scan() expected 'a real', got '912-15yearsNoNo10.546No'
>
> Also does read.table always calls scan in background to do its job . If so
> why use read.table in first place .
>
> Pradeep?


From btupper at bigelow.org  Sun Jan 17 22:46:36 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Sun, 17 Jan 2016 16:46:36 -0500
Subject: [R] Reading a tab delimted file of varying length using
	read.table
In-Reply-To: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
References: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
Message-ID: <88AA6F12-0169-434B-8420-F85AFBA06EDB@bigelow.org>

Hi Pradeep,

Any software would be challenged to determine the boundaries between your columns.

ff <- 'http://data.princeton.edu/wws509/datasets/divorce.dat'
txt <- readLines(ff)
head(txt)
# [1] "       id        heduc   heblack   mixed     years   div  " "       9   12-15 years        No      No    10.546    No  "
# [3] "      11    < 12 years        No      No    34.943    No  " "      13    < 12 years        No      No     2.834   Yes  "
# [5] "      15    < 12 years        No      No    17.532   Yes  " "      33   12-15 years        No      No     1.418    No  

You don't have tab delimiters but instead have space delimiters (well sort of).  Your second column has either one ("12-15 years") or two ("< 12 years") spaces embedded in the values.  That will mess up any scheme using spaces to delineate the columns.  

Perhaps you can read this as fixed width - see ?read.fwf - but you'll have to fiddle with the width specifications.

Cheers,
Ben


> On Jan 17, 2016, at 10:31 AM, Pradeep Bisht <pradeep.bisht0303 at gmail.com> wrote:
> 
> Hello Experts  ,
> 
> Being a SAS developer I am finding it difficult to perform some of data
> cleaning in R that are quite easy to perform in SAS .
> 
> I have been trying to read a .dat file and after a lot of attempts have
> failed to find a solution . Maybe R doesn't have the functionality right
> now or I am not looking in the right place . Here is my code .
> 
> f5=read.table("http://data.princeton.edu/wws509/datasets/divorce.dat
> <http://www.linkedin.com/redir/redirect?url=http%3A%2F%2Fdata%2Eprinceton%2Eedu%2Fwws509%2Fdatasets%2Fdivorce%2Edat&urlhash=GVbR&_t=tracking_anet>
> ",
> header=T,
> sep="\t",
> colClasses = c("numeric", "character", "character","character", "double",
> "character" ) )
> The error i get i
> ?s?
> this .
> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,
> :
> scan() expected 'a real', got '912-15yearsNoNo10.546No'
> 
> Also does read.table always calls scan in background to do its job . If so
> why use read.table in first place .
> 
> Pradeep?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From ligges at statistik.tu-dortmund.de  Sun Jan 17 22:48:37 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sun, 17 Jan 2016 22:48:37 +0100
Subject: [R] Reading a tab delimted file of varying length using
 read.table
In-Reply-To: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
References: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
Message-ID: <569C0C35.3050600@statistik.tu-dortmund.de>

This is not a tab delimited file (as you apparently assume given the 
code), but a fixed width format, hence I'd try:

url <- "http://data.princeton.edu/wws509/datasets/divorce.dat"
widths <- c(9, 13, 10, 8, 10, 6)
f5 <- read.fwf(url, widths = widths, skip = 1, strip.white = TRUE)

names(f5) <- as.character(unlist(read.fwf(url, widths = widths, 
strip.white=TRUE, n=1)))

Not sure why reading it simply with header=TRUE des not work, but no 
time to investiagte this now.

Best,
Uwe Ligges



On 17.01.2016 16:31, Pradeep Bisht wrote:
> Hello Experts  ,
>
> Being a SAS developer I am finding it difficult to perform some of data
> cleaning in R that are quite easy to perform in SAS .
>
> I have been trying to read a .dat file and after a lot of attempts have
> failed to find a solution . Maybe R doesn't have the functionality right
> now or I am not looking in the right place . Here is my code .
>
> f5=read.table("http://data.princeton.edu/wws509/datasets/divorce.dat
> <http://www.linkedin.com/redir/redirect?url=http%3A%2F%2Fdata%2Eprinceton%2Eedu%2Fwws509%2Fdatasets%2Fdivorce%2Edat&urlhash=GVbR&_t=tracking_anet>
> ",
> header=T,
> sep="\t",
> colClasses = c("numeric", "character", "character","character", "double",
> "character" ) )
> The error i get i
> ?s?
> this .
> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,
> :
> scan() expected 'a real', got '912-15yearsNoNo10.546No'
>
> Also does read.table always calls scan in background to do its job . If so
> why use read.table in first place .
>
> Pradeep?
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.turner at auckland.ac.nz  Mon Jan 18 00:01:27 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 18 Jan 2016 12:01:27 +1300
Subject: [R] Reading a tab delimted file of varying length using
 read.table
In-Reply-To: <569C0C35.3050600@statistik.tu-dortmund.de>
References: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
	<569C0C35.3050600@statistik.tu-dortmund.de>
Message-ID: <569C1D47.9080907@auckland.ac.nz>

On 18/01/16 10:48, Uwe Ligges wrote:
> This is not a tab delimited file (as you apparently assume given the
> code), but a fixed width format, hence I'd try:
>
> url <- "http://data.princeton.edu/wws509/datasets/divorce.dat"
> widths <- c(9, 13, 10, 8, 10, 6)
> f5 <- read.fwf(url, widths = widths, skip = 1, strip.white = TRUE)
>
> names(f5) <- as.character(unlist(read.fwf(url, widths = widths,
> strip.white=TRUE, n=1)))
>
> Not sure why reading it simply with header=TRUE des not work, but no
> time to investiagte this now.

Dear Uwe,

I have fiddled around a bit and the situation seems to me to be of the 
nature of a bug in read.fwf.  It would seem that in order for 
header=TRUE to work, the entries of the header need to be separated by
the sep delimiter which defaults to "\t".  In the case in question the 
entries are separated by blanks, so presumably the header gets read in 
as a single entity, rather than 6 such, leading to a mismatch between 
the length of the header and the number of columns.

It seems that the specified widths get ignored when the header line is 
dealt with.

It also seems that if one specifies sep="" then the header gets read 
correctly but then strings of blanks get interpreted as field separators 
throughout and then blanks within the fields result in the
wrong number of columns.

I think that the code of read.fwf is easy enough to fix; a slight 
adjustment will make the header get treated the same way as the body of 
the file.

I don't see any problems/drawbacks with so-doing, and experimenting with 
my modified function resulted in the divorce data being read in with 
header=TRUE with no problems.

If this mod is made, I see no reason to keep the "sep" argument in 
read.fwf --- except maybe for backward compatibility issues, and I don't 
think there would be any since it never worked properly anyhow.

cheers,

Rolf

P. S. I can send you my modified version of read.fwf off-list if this 
would be of any use to you.

R.

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From pradeep.bisht0303 at gmail.com  Sun Jan 17 23:52:25 2016
From: pradeep.bisht0303 at gmail.com (Pradeep Bisht)
Date: Sun, 17 Jan 2016 17:52:25 -0500
Subject: [R] Reading a tab delimted file of varying length using
	read.table
In-Reply-To: <569C0C35.3050600@statistik.tu-dortmund.de>
References: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
	<569C0C35.3050600@statistik.tu-dortmund.de>
Message-ID: <CAA1oRtGCwnBZDvB_FYj-Twkp_B3Y+iFbBdVNPs3yQNzX6eXoig@mail.gmail.com>

A Big thanks to everyone to help me solve this problem .
My bad I assumed the file is delimited by tab which it was not . Its a
fixed width file and the code that Uwe gave is just perfect .
It was cleaver to skip the first row since the delimiter cannot be
specified in this case .I added few more things to it and got the desired
solution .
Here is the code

??
url <- "http://data.princeton.edu/wws509/datasets/divorce.dat"
widths <- c(9, 13, 10, 8, 10, 6)
f5 <- read.fwf(url, widths = widths,
               skip = 1,
               nrow=10,
               strip.white = TRUE,
               col.names=c("id","heduc","heblack","mixed","years","div"),
               colClasses = c("numeric", "character",
"character","character", "double", "character" )
               )

Regards
Pradeep Singh

On Sun, Jan 17, 2016 at 4:48 PM, Uwe Ligges <ligges at statistik.tu-dortmund.de
> wrote:

> This is not a tab delimited file (as you apparently assume given the
> code), but a fixed width format, hence I'd try:
>
> url <- "http://data.princeton.edu/wws509/datasets/divorce.dat"
> widths <- c(9, 13, 10, 8, 10, 6)
> f5 <- read.fwf(url, widths = widths, skip = 1, strip.white = TRUE)
>
> names(f5) <- as.character(unlist(read.fwf(url, widths = widths,
> strip.white=TRUE, n=1)))
>
> Not sure why reading it simply with header=TRUE des not work, but no time
> to investiagte this now.
>
> Best,
> Uwe Ligges
>
>
>
> On 17.01.2016 16:31, Pradeep Bisht wrote:
>
>> Hello Experts  ,
>>
>> Being a SAS developer I am finding it difficult to perform some of data
>> cleaning in R that are quite easy to perform in SAS .
>>
>> I have been trying to read a .dat file and after a lot of attempts have
>> failed to find a solution . Maybe R doesn't have the functionality right
>> now or I am not looking in the right place . Here is my code .
>>
>> f5=read.table("http://data.princeton.edu/wws509/datasets/divorce.dat
>> <
>> http://www.linkedin.com/redir/redirect?url=http%3A%2F%2Fdata%2Eprinceton%2Eedu%2Fwws509%2Fdatasets%2Fdivorce%2Edat&urlhash=GVbR&_t=tracking_anet
>> >
>> ",
>> header=T,
>> sep="\t",
>> colClasses = c("numeric", "character", "character","character", "double",
>> "character" ) )
>> The error i get i
>> ?s?
>> this .
>> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,
>> :
>> scan() expected 'a real', got '912-15yearsNoNo10.546No'
>>
>> Also does read.table always calls scan in background to do its job . If so
>> why use read.table in first place .
>>
>> Pradeep?
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From firoozi_maryam6858 at yahoo.com  Sun Jan 17 22:44:56 2016
From: firoozi_maryam6858 at yahoo.com (maryam firoozi)
Date: Mon, 18 Jan 2016 01:14:56 +0330
Subject: [R] (no subject)
Message-ID: <6F174D07-953F-40F7-AB7D-4641AF45913F@yahoo.com>

hello,
we want to do genomic blup in r.i know that use pedigree package.
the formule is
gblup( P~1,data=ped[,c('ID','P')],M=M,lambda=1/h2-1)
P:phenotype variance
ped:pedigree
M: matrix marker or genotype
my ped has 4500 ID.but my M has 9000 individual.becasue i have two row for each ID in M matrix becasue each ID has two haplotype.how can i solve it.the formula didnt solve.
sincerely


Sent from my iPhone

From pdalgd at gmail.com  Mon Jan 18 00:39:21 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 18 Jan 2016 00:39:21 +0100
Subject: [R] Order of formula terms in model.matrix
In-Reply-To: <alpine.OSX.2.20.1601171015270.673@charles-berrys-macbook.local>
References: <DF70D770-C30D-425D-93AE-B10BC832C661@gmail.com>
	<alpine.OSX.2.20.1601171015270.673@charles-berrys-macbook.local>
Message-ID: <25DD720E-619A-458E-BE4B-5327F9AE6353@gmail.com>


> On 17 Jan 2016, at 19:34 , Charles C. Berry <ccberry at ucsd.edu> wrote:
> 
> 
> IIRC, there are some heuristics involved harking back to the White Book. I recall there have been discussions of whether and how this could be fixed before on this list and or R-devel, but I cannot seem to lay my browser on them right now.
> 

And IIRC: yup, and one of the issues is that 
(a) some rules work left-to-right
(b) the logic is oblivious to the factor/vector distinction

For factors a,b,c, what happens for ~a:b + b:c is that a:b gets the full term expansion since the marginals a and b are not in the model but since b is part of the fully expanded a:b,  b:c gets the reduced form expansion as it would in ~b + b:c (the c-within-b thing). Swapping the terms gives you a different result, but at least it is the same model in the sense that the columns span the same subspace. 

If a and b are vectors, and c is a factor, you get the same logic: expand a:b fully, then treat b:c as in b + b:c. Unfortunately, a:b is just the product of a and b, whether or not it is fully expanded, so it doesn't really make sense to proceed as if b is contained in a preceding term. So the net result is that you end up with one column less than you probably wanted.



-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ligges at statistik.tu-dortmund.de  Mon Jan 18 00:43:05 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Mon, 18 Jan 2016 00:43:05 +0100
Subject: [R] Reading a tab delimted file of varying length using
 read.table
In-Reply-To: <569C1D47.9080907@auckland.ac.nz>
References: <CAA1oRtEMuVJprQJjF1rK5Y6C2YsktrBhZrO5NXCregDExZYXTQ@mail.gmail.com>
	<569C0C35.3050600@statistik.tu-dortmund.de>
	<569C1D47.9080907@auckland.ac.nz>
Message-ID: <569C2709.4050609@statistik.tu-dortmund.de>

Dear Rolf,

I'll take a look how to fix it tomorrow, your proposal is very welocme, 
of course,

Best,
Uwe


On 18.01.2016 00:01, Rolf Turner wrote:
> On 18/01/16 10:48, Uwe Ligges wrote:
>> This is not a tab delimited file (as you apparently assume given the
>> code), but a fixed width format, hence I'd try:
>>
>> url <- "http://data.princeton.edu/wws509/datasets/divorce.dat"
>> widths <- c(9, 13, 10, 8, 10, 6)
>> f5 <- read.fwf(url, widths = widths, skip = 1, strip.white = TRUE)
>>
>> names(f5) <- as.character(unlist(read.fwf(url, widths = widths,
>> strip.white=TRUE, n=1)))
>>
>> Not sure why reading it simply with header=TRUE des not work, but no
>> time to investiagte this now.
>
> Dear Uwe,
>
> I have fiddled around a bit and the situation seems to me to be of the
> nature of a bug in read.fwf.  It would seem that in order for
> header=TRUE to work, the entries of the header need to be separated by
> the sep delimiter which defaults to "\t".  In the case in question the
> entries are separated by blanks, so presumably the header gets read in
> as a single entity, rather than 6 such, leading to a mismatch between
> the length of the header and the number of columns.
>
> It seems that the specified widths get ignored when the header line is
> dealt with.
>
> It also seems that if one specifies sep="" then the header gets read
> correctly but then strings of blanks get interpreted as field separators
> throughout and then blanks within the fields result in the
> wrong number of columns.
>
> I think that the code of read.fwf is easy enough to fix; a slight
> adjustment will make the header get treated the same way as the body of
> the file.
>
> I don't see any problems/drawbacks with so-doing, and experimenting with
> my modified function resulted in the divorce data being read in with
> header=TRUE with no problems.
>
> If this mod is made, I see no reason to keep the "sep" argument in
> read.fwf --- except maybe for backward compatibility issues, and I don't
> think there would be any since it never worked properly anyhow.
>
> cheers,
>
> Rolf
>
> P. S. I can send you my modified version of read.fwf off-list if this
> would be of any use to you.
>
> R.
>


From wjm1 at caa.columbia.edu  Mon Jan 18 00:50:36 2016
From: wjm1 at caa.columbia.edu (William Michels)
Date: Sun, 17 Jan 2016 15:50:36 -0800
Subject: [R] Split Strings
In-Reply-To: <CAMLwc7Phkmrvm+PBYHmcwXHiUOqFZ1cxm9Tf3w7VAifMN60C+w@mail.gmail.com>
References: <CAMLwc7Phkmrvm+PBYHmcwXHiUOqFZ1cxm9Tf3w7VAifMN60C+w@mail.gmail.com>
Message-ID: <CAA99HCxxC9KsHyWoYnrk4NE7M9AOFVsO+cj7geYByJ=5qT2jow@mail.gmail.com>

> str_1 <- list("pc_m2_45_ssp3_wheat", "pc_m2_45_ssp3_wheat", "ssp3_maize", "m2_wheat")
> str_2 <- strsplit(unlist(str_1), "_")
> max.length <- max(sapply(str_2,length))
> str_3 <- lapply(lapply(str_2, unlist), "length<-", max.length)
> str_3

See: http://stackoverflow.com/questions/27995639/i-have-a-numeric-list-where-id-like-to-add-0-or-na-to-extend-the-length-of-the

Hope this helps,

Bill

William Michels, Ph.D.


On Sun, Jan 17, 2016 at 12:56 PM, Miluji Sb <milujisb at gmail.com> wrote:
> I have a list of strings of different lengths and would like to split each
> string by underscore "_"
>
> pc_m2_45_ssp3_wheat
> pc_m2_45_ssp3_wheat
> ssp3_maize
> m2_wheat
>
> I would like to separate each part of the string into different columns
> such as
>
> pc m2 45 ssp3 wheat
>
> But because of the different lengths - I would like NA in the columns for
> the variables have fewer parts such as
>
> NA NA NA m2 wheat
>
> I have tried unlist(strsplit(x, "_")) to split, it works for one variable
> but not for the list - gives me "non-character argument" error. I would
> highly appreciate any help. Thank you!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Mon Jan 18 05:17:42 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Mon, 18 Jan 2016 14:17:42 +1000
Subject: [R] tikzDevice and Sweave
In-Reply-To: <SNT150-W750181125F1A3C387B5109FACF0@phx.gbl>
References: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>,
	<569BFC38.90405@gmail.com>
	<SNT150-W750181125F1A3C387B5109FACF0@phx.gbl>
Message-ID: <000401d151a7$2dffeb70$89ffc250$@bigpond.com>

Hi 

I use Sweave  and some tikz in latex but not in Sweave

Your problem is that you left out Sweave in the preamble
It must be in the preamble of any Sweave document

I added sizing so that it is not off the page. 
I do not know if Sweave options will cover this or you have to set it.
eg
\setkeys{Gin}{width=1.0\textwidth}   

\documentclass{article}
\usepackage{tikz}
\usepackage{Sweave}

\begin{document}

Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package. 

\begin{figure}
\begin{center}

<<name = tikzFig, echo = FALSE, results = tex>>=
# % <<name = tikzFig, echo = FALSE, fig = TRUE>>=

# %<<tikzFig, echo = FALSE, fig = TRUE>>=
# setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
library(tikzDevice)

# added height and width
tikz(console = TRUE, width = 4, height = 3)
plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
"sin(x)")
dummy <- dev.off()
@

\caption{Example of tikz graph}
\label{tikzExampleFig}
\end{center}
\end{figure}

\end{document}

If this is an example for a larger document then have a look at the latex
hyperref package

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Naresh
Gurbuxani
Sent: Monday, 18 January 2016 06:54
To: R-help at r-project.org
Subject: Re: [R] tikzDevice and Sweave

Resending as a useable example

\documentclass{article}
\usepackage{tikz}

\begin{document}

Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package.?

\begin{figure}
\begin{center}

<<name = tikzFig, echo = FALSE, results = tex>>=
# % <<name = tikzFig, echo = FALSE, fig = TRUE>>=

# %<<tikzFig, echo = FALSE, fig = TRUE>>=
# setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
library(tikzDevice)

tikz(console = TRUE)
plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
"sin(x)")
dummy <- dev.off()
@

\caption{Example of tikz graph}
\label{tikzExampleFig}
\end{center}
\end{figure}

\end{document}

----------------------------------------
> Subject: Re: [R] tikzDevice and Sweave
> To: naresh_gurbuxani at hotmail.com; r-help at r-project.org
> From: murdoch.duncan at gmail.com
> Date: Sun, 17 Jan 2016 15:40:24 -0500
>
> On 17/01/2016 3:25 PM, Naresh Gurbuxani wrote:
>> I want to use tikz() function in tikzDevice package so that it generates
a pdf file to be included in the bigger tex file. Below code works, but
directly inserts tikz commands in the output tex file.
>> This works:
>> <<name = tikzFig, echo = FALSE, results = tex>>=
>> This does not work:
>> <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>> Full code is given below:
>> \documentclass{article}\usepackage{tikz}
>> Figure~\ref{tikzExampleFig} is and an example of \texttt{tikzDevice}
package.
>> \begin{figure}\begin{center}
>> <<name = tikzFig, echo = FALSE, results = tex>>=
>> library(tikzDevice)tikz(console = TRUE)plot(sin, -pi, pi, main = "A stand
alone TikZ plot", xlab = "x", ylab = "sin(x)")dummy <- dev.off()@
>> \caption{Example of tikz
graph}\label{tikzExampleFig}\end{center}\end{figure}
>> \end{document}
>
> Your example isn't usable -- please post in plain text, not HTML.
>
> I can't tell whether you are trying to use Sweave or knitr. If you're
> using knitr, see the discussion of dev = "tikz" in
> <http://yihui.name/knitr/>. If you're using Sweave, you probably need
> pgfSweave.
>
> Duncan Murdoch
>
 		 	   		  
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Mon Jan 18 09:19:24 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 18 Jan 2016 19:19:24 +1100
Subject: [R] Split Strings
In-Reply-To: <CAMLwc7Phkmrvm+PBYHmcwXHiUOqFZ1cxm9Tf3w7VAifMN60C+w@mail.gmail.com>
References: <CAMLwc7Phkmrvm+PBYHmcwXHiUOqFZ1cxm9Tf3w7VAifMN60C+w@mail.gmail.com>
Message-ID: <CA+8X3fX3AWs_mcoh3B3tqeQUsM67k=Z3oSQxsYubEAwK6yperg@mail.gmail.com>

Hi Miluji,
While the other answers are correct in general, I noticed that your request
was for the elements of an incomplete string to be placed in the same
positions as in the complete strings. Perhaps this will help:

strings<-list("pc_m2_45_ssp3_wheat","pc_m2_45_ssp3_wheat",
 "ssp3_maize","m2_wheat","pc_m2_45_ssp3_maize")
split_strings<-strsplit(unlist(strings),"_")
max_length <- max(sapply(split_strings,length))
complete_sets<-split_strings[sapply(split_strings,length)==max_length]
element_sets<-list()

# build a list with the unique elements of each complete string
for(i in 1:max_length)
 element_sets[[i]]<-unique(sapply(complete_sets,"[",i))

# function to guess the position of the elements in a partial string
# and return them in the hopefully correct positions
fill_strings<-function(split_string,max_length,element_sets) {
 if(length(split_string) < max_length) {
  new_split_string<-rep(NA,max_length)
  for(i in 1:length(split_string)) {
   for(j in 1:length(complete_sets)) {
    if(grep(split_string[i],element_sets[j]))
     new_split_string[j]<-split_string[i]
   }
  }
  return(new_split_string)
 }
 return(split_string)
}

# however, if you know that the incomplete strings will always
# be composed of the last elements in the complete strings
fill_strings<-function(split_string,max_length) {
 lenstring<-length(split_string)
 if(lenstring < max_length)
  split_string<-c(rep(NA,max_length-lenstring),split_string)
 return(split_string)
}

sapply(split_strings,fill_strings,list(max_length,element_sets))

Jim

On Mon, Jan 18, 2016 at 7:56 AM, Miluji Sb <milujisb at gmail.com> wrote:

> I have a list of strings of different lengths and would like to split each
> string by underscore "_"
>
> pc_m2_45_ssp3_wheat
> pc_m2_45_ssp3_wheat
> ssp3_maize
> m2_wheat
>
> I would like to separate each part of the string into different columns
> such as
>
> pc m2 45 ssp3 wheat
>
> But because of the different lengths - I would like NA in the columns for
> the variables have fewer parts such as
>
> NA NA NA m2 wheat
>
> I have tried unlist(strsplit(x, "_")) to split, it works for one variable
> but not for the list - gives me "non-character argument" error. I would
> highly appreciate any help. Thank you!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From milujisb at gmail.com  Mon Jan 18 09:46:10 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Mon, 18 Jan 2016 09:46:10 +0100
Subject: [R] Split Strings
In-Reply-To: <CA+8X3fX3AWs_mcoh3B3tqeQUsM67k=Z3oSQxsYubEAwK6yperg@mail.gmail.com>
References: <CAMLwc7Phkmrvm+PBYHmcwXHiUOqFZ1cxm9Tf3w7VAifMN60C+w@mail.gmail.com>
	<CA+8X3fX3AWs_mcoh3B3tqeQUsM67k=Z3oSQxsYubEAwK6yperg@mail.gmail.com>
Message-ID: <CAMLwc7OL0ynjyjofdhiyPw0HB+f4wYQXvCC55rno4gMUpq76yw@mail.gmail.com>

Thank you everyone for the codes and the link. They work well!

Mr. Lemon, thank you for the detailed code and the explanations. I
appreciate it. One thing though, in the last line

sapply(split_strings,fill_strings,list(max_length,element_sets))

should it be unlist instead of list - I get this error "Error in
FUN(X[[i]], ...) : (list) object cannot be coerced to type 'integer'".
Thanks again!



On Mon, Jan 18, 2016 at 9:19 AM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Miluji,
> While the other answers are correct in general, I noticed that your
> request was for the elements of an incomplete string to be placed in the
> same positions as in the complete strings. Perhaps this will help:
>
> strings<-list("pc_m2_45_ssp3_wheat","pc_m2_45_ssp3_wheat",
>  "ssp3_maize","m2_wheat","pc_m2_45_ssp3_maize")
> split_strings<-strsplit(unlist(strings),"_")
> max_length <- max(sapply(split_strings,length))
> complete_sets<-split_strings[sapply(split_strings,length)==max_length]
> element_sets<-list()
>
> # build a list with the unique elements of each complete string
> for(i in 1:max_length)
>  element_sets[[i]]<-unique(sapply(complete_sets,"[",i))
>
> # function to guess the position of the elements in a partial string
> # and return them in the hopefully correct positions
> fill_strings<-function(split_string,max_length,element_sets) {
>  if(length(split_string) < max_length) {
>   new_split_string<-rep(NA,max_length)
>   for(i in 1:length(split_string)) {
>    for(j in 1:length(complete_sets)) {
>     if(grep(split_string[i],element_sets[j]))
>      new_split_string[j]<-split_string[i]
>    }
>   }
>   return(new_split_string)
>  }
>  return(split_string)
> }
>
> # however, if you know that the incomplete strings will always
> # be composed of the last elements in the complete strings
> fill_strings<-function(split_string,max_length) {
>  lenstring<-length(split_string)
>  if(lenstring < max_length)
>   split_string<-c(rep(NA,max_length-lenstring),split_string)
>  return(split_string)
> }
>
> sapply(split_strings,fill_strings,list(max_length,element_sets))
>
> Jim
>
> On Mon, Jan 18, 2016 at 7:56 AM, Miluji Sb <milujisb at gmail.com> wrote:
>
>> I have a list of strings of different lengths and would like to split each
>> string by underscore "_"
>>
>> pc_m2_45_ssp3_wheat
>> pc_m2_45_ssp3_wheat
>> ssp3_maize
>> m2_wheat
>>
>> I would like to separate each part of the string into different columns
>> such as
>>
>> pc m2 45 ssp3 wheat
>>
>> But because of the different lengths - I would like NA in the columns for
>> the variables have fewer parts such as
>>
>> NA NA NA m2 wheat
>>
>> I have tried unlist(strsplit(x, "_")) to split, it works for one variable
>> but not for the list - gives me "non-character argument" error. I would
>> highly appreciate any help. Thank you!
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From b.h.mevik at usit.uio.no  Mon Jan 18 10:26:10 2016
From: b.h.mevik at usit.uio.no (=?utf-8?Q?Bj=C3=B8rn-Helge_Mevik?=)
Date: Mon, 18 Jan 2016 10:26:10 +0100
Subject: [R] Problems with data structure when using plsr() from
	package	pls
In-Reply-To: <1A8C1289955EF649A09086A153E2672403C998D72F@GBTEDVPEXCMB04.corp.lgc-group.com>
	(S. Ellison's message of "Fri, 15 Jan 2016 13:38:35 +0000")
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
	<CAM_vju=QwHPrjFgpv7+DDbqJf2AZ5z7OKZGKGG-oS7wjru9g+g@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C998D72F@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <s3segdfutl9.fsf@varelg.uio.no>

S Ellison <S.Ellison at lgcgroup.com> writes:

> Reading ?plsr examples and inspecting the data they use, you need to arrange
> frame1 so that it has the data from n96 included as columns with names of the
> from "n96.xxx" whre xxx can be numbers, names etc.

No, you do not. :)  plsr() is happy with a data frame where n96 is a
single variable consisting of a matrix.  And this is the recommended way
for matrices with a lot of coloumns.  Which is what you get with

frame1 <- data.frame(gushVM, n96 = I(n96))

if n96 is a matrix, or

frame1 <- data.frame(gushVM, n96 = I(as.matrix(n96)))

if it is a data.frame.

> If n96 is a data frame, try something like
> names(n96) <- paste("n96", 1:96) 
> frame1 <- cbind(gushVM, n96)
>
> pls1 <- plsr(gushVM ~ n96, data = frame1)

Have you actually tried this?  It doesn't work:  For instance:

> gushVM <- 1:5
> n96 <- data.frame(a=1:5, b=2:6)
> names(n96) <- paste("n96", 1:2)
> n96
  n96 1 n96 2
1     1     2
2     2     3
3     3     4
4     4     5
5     5     6
> frame1 <- cbind(gushVM, n96)
> frame1
  gushVM n96 1 n96 2
1      1     1     2
2      2     2     3
3      3     3     4
4      4     4     5
5      5     5     6
> dim(frame1)
[1] 5 3
> pls1 <- plsr(gushVM ~ n96, data = frame1)
Error in model.frame.default(formula = gushVM ~ n96, data = frame1) : 
  invalid type (list) for variable 'n96'

The reason is that frame1 does _not_ contain a variable called 'n96', so
plsr() (or actually model.frame.default()) searches in the global work
space, where it finds a _data.frame_ n96.  A data.frame is a list.
Hence the error message.

> If n96 is a matrix, 
>
> frame1 <- data.frame(gushVM, n96=n96)
>
> should also give you a data frame with names of the right format.

It does not:

> n96 <- as.matrix(n96)
> frame1 <- data.frame(gushVM, n96=n96)
> frame1
  gushVM n96.n96.1 n96.n96.2
1      1         1         2
2      2         2         3
3      3         3         4
4      4         4         5
5      5         5         6
> dim(frame1)
[1] 5 3
> names(frame1)
[1] "gushVM"    "n96.n96.1" "n96.n96.2"

So the data frame still does not have any variable named 'n96'.  The
only reason

> pls1 <- plsr(gushVM ~ n96, data = frame1)

seems to work, is that the 'n96' variable it now finds in the global
environment, happens to be a matrix

> class(n96)
[1] "matrix"

If that wasn't there, you would get an error:

> rm(n96)
> pls1 <- plsr(gushVM ~ n96, data = frame1)
Error in eval(expr, envir, enclos) : object 'n96' not found

> I() wrapped round a matrix or data frame does nothing like what is needed if
> you include it in a data frame construction, so either things have changed
> since the tutorial was written, or the authors were not handling a matrix or
> data frame with I().

Yes it does. :)  Nothing (substantial) has changed, and we did/do handle
matrices with I():

> n96 <- matrix(1:10, ncol=2)
> n96
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
> frame1 <- data.frame(gushVM, I(n96))
> frame1
  gushVM n96.1 n96.2
1      1     1     6
2      2     2     7
3      3     3     8
4      4     4     9
5      5     5    10
> dim(frame1)
[1] 5 2
> names(frame1)
[1] "gushVM" "n96"   
> rm(n96)
> pls1 <- plsr(gushVM ~ n96, data = frame1)
> pls1
Partial least squares regression , fitted with the kernel algorithm.
Call:
plsr(formula = gushVM ~ n96, data = frame1)

-- 
Regards,
Bj?rn-Helge Mevik


From cg.pettersson at lantmannen.com  Mon Jan 18 11:25:14 2016
From: cg.pettersson at lantmannen.com (CG Pettersson)
Date: Mon, 18 Jan 2016 10:25:14 +0000
Subject: [R] Problems with data structure when using plsr() from
	package	pls
References: <DB5PR02MB09836F5AFEBC1E802C6F8A938CCB0@DB5PR02MB0983.eurprd02.prod.outlook.com>
	<CAM_vju=QwHPrjFgpv7+DDbqJf2AZ5z7OKZGKGG-oS7wjru9g+g@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C998D72F@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <HE1PR02MB09872CD280850FBE17C608978CC00@HE1PR02MB0987.eurprd02.prod.outlook.com>

Again: thanks a lot for all your suggestions, the problem is now solved.

This combination of calls finally did the trick:

> n96 <- as.matrix(n96)
> frame2 <- data.frame(gushVM, n96=I(n96))
> str(frame2)
'data.frame':   15 obs. of  2 variables:
 $ gushVM: num  2 23.2 14.3 40.9 32.8 29.1 0 79 0 0 ...
 $ n96   : AsIs [1:15, 1:96] 34.24092067 31.79041592 52.00961175 36.10294958 47.02360183 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr  "1" "2" "3" "4" ...
  .. ..$ : chr  "n96 1" "n96 2" "n96 3" "n96 4" ...

Which is exactly the same structure as the supplied datasets in package pls, and works well with plsr()

Thanks a lot for the help!
/CG


-----Ursprungligt meddelande-----
Fr?n: CG Pettersson 
Skickat: den 17 januari 2016 13:30
Till: 'S Ellison'; Sarah Goslee
Kopia: r-help at r-project.org
?mne: SV: [R] Problems with data structure when using plsr() from package pls

Thanks a lot to both of you for the effort and nice suggestions.

I have tested all suggested coding variants and I do succeed in changing the data structure, for example like this (with a test using str())

> names(n96) <- paste("n96", 1:96)
> frame1 <- cbind(gushVM, n96)
>  
> str(frame1)
'data.frame':   15 obs. of  97 variables:
 $ gushVM: num  2 23.2 14.3 40.9 32.8 29.1 0 79 0 0 ...
 $ n96 1 : num  34.2 31.8 52 36.1 47 ...
 $ n96 2 : num  23.3 21.1 33.6 24.3 31.5 ...
 $ n96 3 : num  97.5 80.6 89.5 97.3 122.4 ...
 $ n96 4 : num  79.2 64.4 79.9 79.4 99.7 ...
 $ n96 5 : num  24.1 25.8 45.3 25.8 45.2 ...
 $ n96 6 : num  83.1 81.7 119 82.6 140.2 ...
 $ n96 7 : num  10 10.8 16.2 10.9 15.9 ...
 $ n96 8 : num  129 124 180 135 202 ...
 $ n96 9 : num  58.8 52.9 84 61.7 76.7 ...   and so on until row variable 96 inside n96

But using this dataframe with plsr() results in the same error message, like:

> pls1 <- plsr(gushVM ~ n96, data = frame1)
Error in model.frame.default(formula = gushVM ~ n96, data = frame1) : 
  invalid type (list) for variable 'n96'

Which is the same message as ever. The same happens with the other two suggestions, even if the detail structure of "n96" inside "frame" differ a little whe I test it with str().
Still stuck, but thanks all the same!

/CG

-----Ursprungligt meddelande-----
Fr?n: S Ellison [mailto:S.Ellison at LGCGroup.com] 
Skickat: den 15 januari 2016 14:39
Till: Sarah Goslee; CG Pettersson
Kopia: r-help at r-project.org
?mne: RE: [R] Problems with data structure when using plsr() from package pls


> > I am trying to make pls-regression using plsr() from package pls, 
> > with Mevik & Wehrens (2007) as tutorial and the datasets from the package.
> > Everything works real nice as long as I use the supplied datasets, 
> > but I don?t understand how to prepare my own data.
> > This is what I have done:
> >
> > > frame1 <- data.frame(gushVM, I(n96))

Reading ?plsr examples and inspecting the data they use, you need to arrange frame1 so that it has the data from n96 included as columns with names of the from "n96.xxx" whre xxx can be numbers, names etc.

If n96 is a data frame, try something like
names(n96) <- paste("n96", 1:96)
frame1 <- cbind(gushVM, n96)

pls1 <- plsr(gushVM ~ n96, data = frame1)


If n96 is a matrix, 

frame1 <- data.frame(gushVM, n96=n96)

should also give you a data frame with names of the right format.

I() wrapped round a matrix or data frame does nothing like what is needed if you include it in a data frame construction, so either things have changed since the tutorial was written, or the authors were not handling a matrix or data frame with I().

S Ellison






*******************************************************************
This email and any attachments are confidential. Any use, copying or disclosure other than by the intended recipient is unauthorised. If you have received this message in error, please notify the sender immediately via +44(0)20 8943 7000 or notify postmaster at lgcgroup.com and delete this message and any copies from your computer and network. 
LGC Limited. Registered in England 2991879. 
Registered office: Queens Road, Teddington, Middlesex, TW11 0LY, UK

From naresh_gurbuxani at hotmail.com  Mon Jan 18 12:12:40 2016
From: naresh_gurbuxani at hotmail.com (Naresh Gurbuxani)
Date: Mon, 18 Jan 2016 06:12:40 -0500
Subject: [R] tikzDevice and Sweave
In-Reply-To: <000401d151a7$2dffeb70$89ffc250$@bigpond.com>
References: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>, ,
	<569BFC38.90405@gmail.com>,
	<SNT150-W750181125F1A3C387B5109FACF0@phx.gbl>,
	<000401d151a7$2dffeb70$89ffc250$@bigpond.com>
Message-ID: <SNT150-W8007901527B18EF163A9B9FAC00@phx.gbl>

Duncan,

Many thanks for looking at my code and for your suggestion.

Your solution works. ?But my problem is different. ?This code gives me a tex file with a lot of tikz code. ?If there are several graphs in the document, then tex file become very large. ?I would like the code to result in a pdf file for each graph. ?When this pdf file is included in the tex file, the tex file is more readable. ?

Naresh

----------------------------------------
> From: dulcalma at bigpond.com
> To: r-help at r-project.org
> Date: Mon, 18 Jan 2016 14:17:42 +1000
> Subject: Re: [R] tikzDevice and Sweave
>
> Hi
>
> I use Sweave and some tikz in latex but not in Sweave
>
> Your problem is that you left out Sweave in the preamble
> It must be in the preamble of any Sweave document
>
> I added sizing so that it is not off the page.
> I do not know if Sweave options will cover this or you have to set it.
> eg
> \setkeys{Gin}{width=1.0\textwidth}
>
> \documentclass{article}
> \usepackage{tikz}
> \usepackage{Sweave}
>
> \begin{document}
>
> Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package.
>
> \begin{figure}
> \begin{center}
>
> <<name = tikzFig, echo = FALSE, results = tex>>=
> # % <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>
> # %<<tikzFig, echo = FALSE, fig = TRUE>>=
> # setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
> library(tikzDevice)
>
> # added height and width
> tikz(console = TRUE, width = 4, height = 3)
> plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
> "sin(x)")
> dummy <- dev.off()
> @
>
> \caption{Example of tikz graph}
> \label{tikzExampleFig}
> \end{center}
> \end{figure}
>
> \end{document}
>
> If this is an example for a larger document then have a look at the latex
> hyperref package
>
> Regards
>
> Duncan
>
> Duncan Mackay
> Department of Agronomy and Soil Science
> University of New England
> Armidale NSW 2351
> Email: home: mackay at northnet.com.au
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Naresh
> Gurbuxani
> Sent: Monday, 18 January 2016 06:54
> To: R-help at r-project.org
> Subject: Re: [R] tikzDevice and Sweave
>
> Resending as a useable example
>
> \documentclass{article}
> \usepackage{tikz}
>
> \begin{document}
>
> Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package.
>
> \begin{figure}
> \begin{center}
>
> <<name = tikzFig, echo = FALSE, results = tex>>=
> # % <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>
> # %<<tikzFig, echo = FALSE, fig = TRUE>>=
> # setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
> library(tikzDevice)
>
> tikz(console = TRUE)
> plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
> "sin(x)")
> dummy <- dev.off()
> @
>
> \caption{Example of tikz graph}
> \label{tikzExampleFig}
> \end{center}
> \end{figure}
>
> \end{document}
>
> ----------------------------------------
>> Subject: Re: [R] tikzDevice and Sweave
>> To: naresh_gurbuxani at hotmail.com; r-help at r-project.org
>> From: murdoch.duncan at gmail.com
>> Date: Sun, 17 Jan 2016 15:40:24 -0500
>>
>> On 17/01/2016 3:25 PM, Naresh Gurbuxani wrote:
>>> I want to use tikz() function in tikzDevice package so that it generates
> a pdf file to be included in the bigger tex file. Below code works, but
> directly inserts tikz commands in the output tex file.
>>> This works:
>>> <<name = tikzFig, echo = FALSE, results = tex>>=
>>> This does not work:
>>> <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>>> Full code is given below:
>>> \documentclass{article}\usepackage{tikz}
>>> Figure~\ref{tikzExampleFig} is and an example of \texttt{tikzDevice}
> package.
>>> \begin{figure}\begin{center}
>>> <<name = tikzFig, echo = FALSE, results = tex>>=
>>> library(tikzDevice)tikz(console = TRUE)plot(sin, -pi, pi, main = "A stand
> alone TikZ plot", xlab = "x", ylab = "sin(x)")dummy <- dev.off()@
>>> \caption{Example of tikz
> graph}\label{tikzExampleFig}\end{center}\end{figure}
>>> \end{document}
>>
>> Your example isn't usable -- please post in plain text, not HTML.
>>
>> I can't tell whether you are trying to use Sweave or knitr. If you're
>> using knitr, see the discussion of dev = "tikz" in
>> <http://yihui.name/knitr/>. If you're using Sweave, you probably need
>> pgfSweave.
>>
>> Duncan Murdoch
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
 		 	   		  

From pmassicotte at hotmail.com  Mon Jan 18 12:59:12 2016
From: pmassicotte at hotmail.com (philippe massicotte)
Date: Mon, 18 Jan 2016 11:59:12 +0000
Subject: [R] Strange behavior with S3 class
Message-ID: <COL127-W250C62A3DA01C87FADA140B3C00@phx.gbl>

Hi all.
For the development of a package I created a S3 class with a "setter" operator "<-".
Running the following code will create two objects: eem and eem2. Additionally, the class eem has a names.eem function used to retrieve the value of the sample field.
names.eem <- function(x, ...){  x$sample}
# First constructoreem1 <- function(sample){  eem <- list(sample = sample)  class(eem) <- "eem"  return(eem)}
# Second constructoreem2 <- function(sample){  eem <- list(sample = sample)  class(eem) <- "eem2"  return(eem)}
test1 <- eem1("justaname")test2 <- eem2("justaname")

This is "bugged":
  > str(test1)List of 1$ justaname: chr "justaname"- attr(*, "class")= chr "eem"
This is ok:    > str(test2)List of 1$ sample: chr "justaname"- attr(*, "class")= chr "eem2"
It seems that override of the "<-" is causing problem since in str(test1) the variable name is not preserved.
Any idea?
Thank you,Philippe  		 	   		  
	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Jan 18 13:30:43 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 18 Jan 2016 07:30:43 -0500
Subject: [R] tikzDevice and Sweave
In-Reply-To: <SNT150-W8007901527B18EF163A9B9FAC00@phx.gbl>
References: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>
	<569BFC38.90405@gmail.com>
	<SNT150-W750181125F1A3C387B5109FACF0@phx.gbl>
	<000401d151a7$2dffeb70$89ffc250$@bigpond.com>
	<SNT150-W8007901527B18EF163A9B9FAC00@phx.gbl>
Message-ID: <569CDAF3.4040805@gmail.com>

On 18/01/2016 6:12 AM, Naresh Gurbuxani wrote:
> Duncan,
>
> Many thanks for looking at my code and for your suggestion.
>
> Your solution works.  But my problem is different.  This code gives me a tex file with a lot of tikz code.  If there are several graphs in the document, then tex file become very large.  I would like the code to result in a pdf file for each graph.  When this pdf file is included in the tex file, the tex file is more readable.

You probably don't want to do that -- tikz outputs LaTeX code, so you'd 
need to run pdflatex once in every figure in your document.  And you 
probably shouldn't care:  when using Sweave, the .tex file is not really 
of interest.  Concentrate on the .Rnw file as the source file.
However, sometimes you need to deal with other people...

You can easily redirect tikz output to a file, and \input{} that file. 
Just change the figure chunk to

<<name = tikzFig, echo = FALSE, results = tex>>=
library(tikzDevice)
# added height and width
tikz(file = "tikzFig.tex", width = 4, height = 3)
plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab 
= "sin(x)")
dummy <- dev.off()
cat("\\input{tikzFig.tex}")
@

As mentioned, this is a bit simpler in knitr.

Duncan Murdoch

> Naresh
>
> ----------------------------------------
>> From: dulcalma at bigpond.com
>> To: r-help at r-project.org
>> Date: Mon, 18 Jan 2016 14:17:42 +1000
>> Subject: Re: [R] tikzDevice and Sweave
>>
>> Hi
>>
>> I use Sweave and some tikz in latex but not in Sweave
>>
>> Your problem is that you left out Sweave in the preamble
>> It must be in the preamble of any Sweave document
>>
>> I added sizing so that it is not off the page.
>> I do not know if Sweave options will cover this or you have to set it.
>> eg
>> \setkeys{Gin}{width=1.0\textwidth}
>>
>> \documentclass{article}
>> \usepackage{tikz}
>> \usepackage{Sweave}
>>
>> \begin{document}
>>
>> Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package.
>>
>> \begin{figure}
>> \begin{center}
>>
>> <<name = tikzFig, echo = FALSE, results = tex>>=
>> # % <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>>
>> # %<<tikzFig, echo = FALSE, fig = TRUE>>=
>> # setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
>> library(tikzDevice)
>>
>> # added height and width
>> tikz(console = TRUE, width = 4, height = 3)
>> plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
>> "sin(x)")
>> dummy <- dev.off()
>> @
>>
>> \caption{Example of tikz graph}
>> \label{tikzExampleFig}
>> \end{center}
>> \end{figure}
>>
>> \end{document}
>>
>> If this is an example for a larger document then have a look at the latex
>> hyperref package
>>
>> Regards
>>
>> Duncan
>>
>> Duncan Mackay
>> Department of Agronomy and Soil Science
>> University of New England
>> Armidale NSW 2351
>> Email: home: mackay at northnet.com.au
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Naresh
>> Gurbuxani
>> Sent: Monday, 18 January 2016 06:54
>> To: R-help at r-project.org
>> Subject: Re: [R] tikzDevice and Sweave
>>
>> Resending as a useable example
>>
>> \documentclass{article}
>> \usepackage{tikz}
>>
>> \begin{document}
>>
>> Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package.
>>
>> \begin{figure}
>> \begin{center}
>>
>> <<name = tikzFig, echo = FALSE, results = tex>>=
>> # % <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>>
>> # %<<tikzFig, echo = FALSE, fig = TRUE>>=
>> # setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
>> library(tikzDevice)
>>
>> tikz(console = TRUE)
>> plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
>> "sin(x)")
>> dummy <- dev.off()
>> @
>>
>> \caption{Example of tikz graph}
>> \label{tikzExampleFig}
>> \end{center}
>> \end{figure}
>>
>> \end{document}
>>
>> ----------------------------------------
>>> Subject: Re: [R] tikzDevice and Sweave
>>> To: naresh_gurbuxani at hotmail.com; r-help at r-project.org
>>> From: murdoch.duncan at gmail.com
>>> Date: Sun, 17 Jan 2016 15:40:24 -0500
>>>
>>> On 17/01/2016 3:25 PM, Naresh Gurbuxani wrote:
>>>> I want to use tikz() function in tikzDevice package so that it generates
>> a pdf file to be included in the bigger tex file. Below code works, but
>> directly inserts tikz commands in the output tex file.
>>>> This works:
>>>> <<name = tikzFig, echo = FALSE, results = tex>>=
>>>> This does not work:
>>>> <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>>>> Full code is given below:
>>>> \documentclass{article}\usepackage{tikz}
>>>> Figure~\ref{tikzExampleFig} is and an example of \texttt{tikzDevice}
>> package.
>>>> \begin{figure}\begin{center}
>>>> <<name = tikzFig, echo = FALSE, results = tex>>=
>>>> library(tikzDevice)tikz(console = TRUE)plot(sin, -pi, pi, main = "A stand
>> alone TikZ plot", xlab = "x", ylab = "sin(x)")dummy <- dev.off()@
>>>> \caption{Example of tikz
>> graph}\label{tikzExampleFig}\end{center}\end{figure}
>>>> \end{document}
>>>
>>> Your example isn't usable -- please post in plain text, not HTML.
>>>
>>> I can't tell whether you are trying to use Sweave or knitr. If you're
>>> using knitr, see the discussion of dev = "tikz" in
>>> <http://yihui.name/knitr/>. If you're using Sweave, you probably need
>>> pgfSweave.
>>>
>>> Duncan Murdoch
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>   		 	   		
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dmcp at webmail.co.za  Mon Jan 18 10:14:50 2016
From: dmcp at webmail.co.za (David McPearson)
Date: Mon, 18 Jan 2016 11:14:50 +0200
Subject: [R] read in csv.-data file with blanks and missing values
Message-ID: <519121db21fe979369361918c5e4758b@www.webmail.co.za>

swizz-john wrote:

> Hi people,
> 
> my task is to analyse data that is formatted like this.
> 
>
date,bid,name,w1,w2,w3,m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11,m12,debt2mkt,cds,equity
> 28jul2009,1,"ABN Amro",,,,,,,,,,,,,,,,,56.5,
> 29jul2009,1,"ABN Amro",,,,,,,,,,,,,,,,,56.5,
> 30jul2009,1,"ABN Amro",,,,,,,,,,,,,,,,,55,
> 31jul2009,1,"ABN Amro",,,,,,,,,,,,,,,,,55,
> 03aug2009,1,"ABN
Amro",.35,.4,.45,,.71,.96,1.05,1.14,1.22,1.26,1.3,1.35,1.38,1.41,1.44,,55,
> 06aug2009,1,"ABN
Amro",.35,.4,.45,,.72,.92,1,1.1,1.18,1.2,1.23,1.28,1.32,1.35,1.38,,53.75,

##
## Er - no, it'snot.
##

> 
> It has 21 columns but not every column has a value, some only have commas
instead.
> 
> My file is named: test_data.csv and it is attached here:
> test_data.csv

If I open your attachment with Excel I see what you have above. If I open it
with LibreOffice Calc I see the same as you are getting in R.

When I open test_dta.csv with a plain text editor (I used Tinn-R, Notepad
should also work) I see why you are not getting wjat you expect. Take a
careful look at the data: All the commas are contained withing matching pairs
of quotation marks. Hence they are all treated as plain text by sensible
software.

Hope this helps,
Dave.

PS - If this comes through as html can someone flame me, please. I'm using a
web based service and don't yet know what format it sends...
Thanks.

____________________________________________________________
South Africas premier free email service - www.webmail.co.za


From S.Ellison at LGCGroup.com  Mon Jan 18 15:28:08 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Mon, 18 Jan 2016 14:28:08 +0000
Subject: [R] Strange behavior with S3 class
In-Reply-To: <COL127-W250C62A3DA01C87FADA140B3C00@phx.gbl>
References: <COL127-W250C62A3DA01C87FADA140B3C00@phx.gbl>
Message-ID: <1A8C1289955EF649A09086A153E2672403C9A5DB4C@GBTEDVPEXCMB04.corp.lgc-group.com>

> This is "bugged":
>   > str(test1)List of 1$ justaname: chr "justaname"- attr(*, "class")= chr "eem"
> This is ok:    > str(test2)List of 1$ sample: chr "justaname"- attr(*, "class")= chr
> "eem2"
> It seems that override of the "<-" is causing problem since in str(test1) the
> variable name is not preserved.
> Any idea?

Compare 
str(test1)
# List of 1
#  $ justaname: chr "justaname"
#  - attr(*, "class")= chr "eem"

str(unclass(test1))
# List of 1
#  $ sample: chr "justaname"

That says the internal variable name is preserved but while it still has the 'eem' class the name is reported differently.
That is presumably because you defined a 'names' function for objects of class 'eem' which returns the contents of $sample as the name, not "sample". Looks like str is using your names.eem method to extract the name of each component of your object, which is kind of what redefining 'names' asks for. And just to demonstrate that str does indeed use the current class's names method: 

names.eem <- function(x, ...){  "AnotherName"}
str(test1)
# List of 1
#  $ AnotherName: chr "justaname"
#  - attr(*, "class")= chr "eem"

So it isn't your '<-', it's because you overrode 'names'


S Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From leo.guelman at rbc.com  Mon Jan 18 16:49:27 2016
From: leo.guelman at rbc.com (Guelman, Leo)
Date: Mon, 18 Jan 2016 15:49:27 +0000
Subject: [R] Order of formula terms in model.matrix
Message-ID: <22572F76F41BBF45A4F1192762097145601CFE71@SXSCM702.fg.rbc.com>

Is it really the same model?

Following the example provided by Lars:

set.seed(1)
x1 <- rnorm(100)
f1 <- factor(sample(letters[1:3], 100, replace = TRUE))
trt <- sample(c(-1,1), 100, replace = TRUE)
y <- factor(sample(c(0,1), 100, T))
df <- data.frame(y=y, x1=x1, f1=f1, trt=trt)

fit1 <- glm(y ~ x1:trt + f1:trt, data = df, family = binomial)
coef(fit1)

fit2 <- glm(y ~ f1:trt + x1:trt, data = df, family = binomial)
coef(fit2)

identical(fitted(fit1), fitted(fit2))
[1] FALSE



_______________________________________________________________________
If you received this email in error, please advise the sender (by return email or otherwise) immediately. You have consented to receive the attached electronically at the above-noted email address; please retain a copy of this confirmation for future reference.  

Si vous recevez ce courriel par erreur, veuillez en aviser l'exp?diteur imm?diatement, par retour de courriel ou par un autre moyen. Vous avez accept? de recevoir le(s) document(s) ci-joint(s) par voie ?lectronique ? l'adresse courriel indiqu?e ci-dessus; veuillez conserver une copie de cette confirmation pour les fins de reference future.

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Mon Jan 18 17:15:56 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 18 Jan 2016 17:15:56 +0100
Subject: [R] Order of formula terms in model.matrix
In-Reply-To: <22572F76F41BBF45A4F1192762097145601CFE71@SXSCM702.fg.rbc.com>
References: <22572F76F41BBF45A4F1192762097145601CFE71@SXSCM702.fg.rbc.com>
Message-ID: <A433082C-881D-4E1E-8B99-751B2B8EF188@gmail.com>


On 18 Jan 2016, at 16:49 , Guelman, Leo <leo.guelman at rbc.com> wrote:

> Is it really the same model?


No, and I didn't say that they would be. I did say that they would be in the all-factor case, which does seem to be right:

> df$trt <- factor(df$trt)
> fit1 <- glm(y ~ x1:trt + f1:trt, data = df, family = binomial)
> fit2 <- glm(y ~ f1:trt + x1:trt, data = df, family = binomial)
> plot(fitted(fit1), fitted(fit2)) # still differs
> df$x1 <- factor(sample(c(-1,1), 100, replace = TRUE))
> fit1 <- glm(y ~ x1:trt + f1:trt, data = df, family = binomial)
> fit2 <- glm(y ~ f1:trt + x1:trt, data = df, family = binomial)
> plot(fitted(fit1), fitted(fit2)) # looks like it's on diagonal
> identical(fitted(fit1), fitted(fit2)) # wrong check
[1] FALSE
> all.equal(fitted(fit1), fitted(fit2)) # better
[1] TRUE


-pd


>  
> Following the example provided by Lars:
>  
> set.seed(1)
> x1 <- rnorm(100)
> f1 <- factor(sample(letters[1:3], 100, replace = TRUE))
> trt <- sample(c(-1,1), 100, replace = TRUE)
> y <- factor(sample(c(0,1), 100, T))
> df <- data.frame(y=y, x1=x1, f1=f1, trt=trt)
>  
> fit1 <- glm(y ~ x1:trt + f1:trt, data = df, family = binomial)
> coef(fit1)
>  
> fit2 <- glm(y ~ f1:trt + x1:trt, data = df, family = binomial)
> coef(fit2)
>  
> identical(fitted(fit1), fitted(fit2))
> [1] FALSE
>  
>  
>  
> _______________________________________________________________________
> 
> If you received this email in error, please advise the sender (by return email or otherwise) immediately. You have consented to receive the attached electronically at the above-noted email address; please retain a copy of this confirmation for future reference.
> 
> Si vous recevez ce courriel par erreur, veuillez en aviser l'exp?diteur imm?diatement, par retour de courriel ou par un autre moyen. Vous avez accept? de recevoir le(s) document(s) ci-joint(s) par voie ?lectronique ? l'adresse courriel indiqu?e ci-dessus; veuillez conserver une copie de cette confirmation pour les fins de reference future.
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From leo.guelman at rbc.com  Mon Jan 18 17:22:42 2016
From: leo.guelman at rbc.com (Guelman, Leo)
Date: Mon, 18 Jan 2016 16:22:42 +0000
Subject: [R] Order of formula terms in model.matrix
In-Reply-To: <A433082C-881D-4E1E-8B99-751B2B8EF188@gmail.com>
References: <22572F76F41BBF45A4F1192762097145601CFE71@SXSCM702.fg.rbc.com>
	<A433082C-881D-4E1E-8B99-751B2B8EF188@gmail.com>
Message-ID: <22572F76F41BBF45A4F1192762097145601CFE9B@SXSCM702.fg.rbc.com>

Thanks Peter. That make sense. Nevertheless, what comes at a surprise to me (and maybe to others) is that one can potentially get different fits by simply swapping the terms in the model formula. 

Best,
Leo. 

-----Original Message-----
From: peter dalgaard [mailto:pdalgd at gmail.com] 
Sent: 2016, January, 18 11:16 AM
To: Guelman, Leo
Cc: r-help at r-project.org; Charles C. Berry
Subject: Re: [R] Order of formula terms in model.matrix


On 18 Jan 2016, at 16:49 , Guelman, Leo <leo.guelman at rbc.com> wrote:

> Is it really the same model?


No, and I didn't say that they would be. I did say that they would be in the all-factor case, which does seem to be right:

> df$trt <- factor(df$trt)
> fit1 <- glm(y ~ x1:trt + f1:trt, data = df, family = binomial)
> fit2 <- glm(y ~ f1:trt + x1:trt, data = df, family = binomial) 
> plot(fitted(fit1), fitted(fit2)) # still differs
> df$x1 <- factor(sample(c(-1,1), 100, replace = TRUE))
> fit1 <- glm(y ~ x1:trt + f1:trt, data = df, family = binomial)
> fit2 <- glm(y ~ f1:trt + x1:trt, data = df, family = binomial) 
> plot(fitted(fit1), fitted(fit2)) # looks like it's on diagonal 
> identical(fitted(fit1), fitted(fit2)) # wrong check
[1] FALSE
> all.equal(fitted(fit1), fitted(fit2)) # better
[1] TRUE


-pd


>  
> Following the example provided by Lars:
>  
> set.seed(1)
> x1 <- rnorm(100)
> f1 <- factor(sample(letters[1:3], 100, replace = TRUE)) trt <- 
> sample(c(-1,1), 100, replace = TRUE) y <- factor(sample(c(0,1), 100, 
> T)) df <- data.frame(y=y, x1=x1, f1=f1, trt=trt)
>  
> fit1 <- glm(y ~ x1:trt + f1:trt, data = df, family = binomial)
> coef(fit1)
>  
> fit2 <- glm(y ~ f1:trt + x1:trt, data = df, family = binomial)
> coef(fit2)
>  
> identical(fitted(fit1), fitted(fit2))
> [1] FALSE
>  
>  
>  
> ______________________________________________________________________
> _
> 
> If you received this email in error, please advise the sender (by return email or otherwise) immediately. You have consented to receive the attached electronically at the above-noted email address; please retain a copy of this confirmation for future reference.
> 
> Si vous recevez ce courriel par erreur, veuillez en aviser l'exp?diteur imm?diatement, par retour de courriel ou par un autre moyen. Vous avez accept? de recevoir le(s) document(s) ci-joint(s) par voie ?lectronique ? l'adresse courriel indiqu?e ci-dessus; veuillez conserver une copie de cette confirmation pour les fins de reference future.
> 

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

_______________________________________________________________________
If you received this email in error, please advise the sender (by return email or otherwise) immediately. You have consented to receive the attached electronically at the above-noted email address; please retain a copy of this confirmation for future reference.  

Si vous recevez ce courriel par erreur, veuillez en aviser l'exp?diteur imm?diatement, par retour de courriel ou par un autre moyen. Vous avez accept? de recevoir le(s) document(s) ci-joint(s) par voie ?lectronique ? l'adresse courriel indiqu?e ci-dessus; veuillez conserver une copie de cette confirmation pour les fins de reference future.


From fanjianling at gmail.com  Mon Jan 18 17:46:34 2016
From: fanjianling at gmail.com (Jianling Fan)
Date: Mon, 18 Jan 2016 10:46:34 -0600
Subject: [R] LSD value
Message-ID: <CAJ7mryKgwGa9R5kt273qq9sjmCRT2G8K0+S6o3znt_z3ohiyTQ@mail.gmail.com>

Hello, everyone,

I am using LSD.test() from package "agricolae" to do my anova
analysis. I know I can calculate LSD value by its equation
t*sqrt(MSE*2/n), but I am wondering if there is a code or something
that can give the LSD value more directly in R?

Thanks!

Julian


From xie at yihui.name  Mon Jan 18 18:22:07 2016
From: xie at yihui.name (Yihui Xie)
Date: Mon, 18 Jan 2016 11:22:07 -0600
Subject: [R] tikzDevice and Sweave
In-Reply-To: <569CDAF3.4040805@gmail.com>
References: <SNT150-W43F2CE4AC90233E0D2AB1DFACF0@phx.gbl>
	<569BFC38.90405@gmail.com>
	<SNT150-W750181125F1A3C387B5109FACF0@phx.gbl>
	<000401d151a7$2dffeb70$89ffc250$@bigpond.com>
	<SNT150-W8007901527B18EF163A9B9FAC00@phx.gbl>
	<569CDAF3.4040805@gmail.com>
Message-ID: <CANROs4co+zqeGeiwZkVEFX8U5D5DKLK4Ok7u5a=x4qGevKiaSg@mail.gmail.com>

Yeah, the philosophy of knitr from the very beginning is that if you
want to draw a plot, simply draw it, and knitr will take care of the
rest of work (http://i.imgur.com/jrwbX.jpg). You rarely need to think
about graphical devices or LaTeX or a specific output format. With
knitr, the example can be reduced to the absolutely minimal:

\documentclass{article}
\begin{document}
<<tikzFig, echo=FALSE, dev='tikz', fig.width=4, fig.height=3>>=
plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x",
ylab = "sin(x)")
@
\end{document}


Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Mon, Jan 18, 2016 at 6:30 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 18/01/2016 6:12 AM, Naresh Gurbuxani wrote:
>>
>> Duncan,
>>
>> Many thanks for looking at my code and for your suggestion.
>>
>> Your solution works.  But my problem is different.  This code gives me a
>> tex file with a lot of tikz code.  If there are several graphs in the
>> document, then tex file become very large.  I would like the code to result
>> in a pdf file for each graph.  When this pdf file is included in the tex
>> file, the tex file is more readable.
>
>
> You probably don't want to do that -- tikz outputs LaTeX code, so you'd need
> to run pdflatex once in every figure in your document.  And you probably
> shouldn't care:  when using Sweave, the .tex file is not really of interest.
> Concentrate on the .Rnw file as the source file.
> However, sometimes you need to deal with other people...
>
> You can easily redirect tikz output to a file, and \input{} that file. Just
> change the figure chunk to
>
> <<name = tikzFig, echo = FALSE, results = tex>>=
> library(tikzDevice)
> # added height and width
> tikz(file = "tikzFig.tex", width = 4, height = 3)
> plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
> "sin(x)")
> dummy <- dev.off()
> cat("\\input{tikzFig.tex}")
> @
>
> As mentioned, this is a bit simpler in knitr.
>
> Duncan Murdoch
>
>
>> Naresh
>>
>> ----------------------------------------
>>>
>>> From: dulcalma at bigpond.com
>>> To: r-help at r-project.org
>>> Date: Mon, 18 Jan 2016 14:17:42 +1000
>>> Subject: Re: [R] tikzDevice and Sweave
>>>
>>> Hi
>>>
>>> I use Sweave and some tikz in latex but not in Sweave
>>>
>>> Your problem is that you left out Sweave in the preamble
>>> It must be in the preamble of any Sweave document
>>>
>>> I added sizing so that it is not off the page.
>>> I do not know if Sweave options will cover this or you have to set it.
>>> eg
>>> \setkeys{Gin}{width=1.0\textwidth}
>>>
>>> \documentclass{article}
>>> \usepackage{tikz}
>>> \usepackage{Sweave}
>>>
>>> \begin{document}
>>>
>>> Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package.
>>>
>>> \begin{figure}
>>> \begin{center}
>>>
>>> <<name = tikzFig, echo = FALSE, results = tex>>=
>>> # % <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>>>
>>> # %<<tikzFig, echo = FALSE, fig = TRUE>>=
>>> # setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
>>> library(tikzDevice)
>>>
>>> # added height and width
>>> tikz(console = TRUE, width = 4, height = 3)
>>> plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
>>> "sin(x)")
>>> dummy <- dev.off()
>>> @
>>>
>>> \caption{Example of tikz graph}
>>> \label{tikzExampleFig}
>>> \end{center}
>>> \end{figure}
>>>
>>> \end{document}
>>>
>>> If this is an example for a larger document then have a look at the latex
>>> hyperref package
>>>
>>> Regards
>>>
>>> Duncan
>>>
>>> Duncan Mackay
>>> Department of Agronomy and Soil Science
>>> University of New England
>>> Armidale NSW 2351
>>> Email: home: mackay at northnet.com.au
>>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Naresh
>>> Gurbuxani
>>> Sent: Monday, 18 January 2016 06:54
>>> To: R-help at r-project.org
>>> Subject: Re: [R] tikzDevice and Sweave
>>>
>>> Resending as a useable example
>>>
>>> \documentclass{article}
>>> \usepackage{tikz}
>>>
>>> \begin{document}
>>>
>>> Figure~\ref{tikzExampleFig} is an example of \texttt{tikzDevice} package.
>>>
>>> \begin{figure}
>>> \begin{center}
>>>
>>> <<name = tikzFig, echo = FALSE, results = tex>>=
>>> # % <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>>>
>>> # %<<tikzFig, echo = FALSE, fig = TRUE>>=
>>> # setwd("/Users/nareshgurbuxani/Documents/tex/tikz")
>>> library(tikzDevice)
>>>
>>> tikz(console = TRUE)
>>> plot(sin, -pi, 2*pi, main = "A stand alone TikZ plot", xlab = "x", ylab =
>>> "sin(x)")
>>> dummy <- dev.off()
>>> @
>>>
>>> \caption{Example of tikz graph}
>>> \label{tikzExampleFig}
>>> \end{center}
>>> \end{figure}
>>>
>>> \end{document}
>>>
>>> ----------------------------------------
>>>>
>>>> Subject: Re: [R] tikzDevice and Sweave
>>>> To: naresh_gurbuxani at hotmail.com; r-help at r-project.org
>>>> From: murdoch.duncan at gmail.com
>>>> Date: Sun, 17 Jan 2016 15:40:24 -0500
>>>>
>>>> On 17/01/2016 3:25 PM, Naresh Gurbuxani wrote:
>>>>>
>>>>> I want to use tikz() function in tikzDevice package so that it
>>>>> generates
>>>
>>> a pdf file to be included in the bigger tex file. Below code works, but
>>> directly inserts tikz commands in the output tex file.
>>>>>
>>>>> This works:
>>>>> <<name = tikzFig, echo = FALSE, results = tex>>=
>>>>> This does not work:
>>>>> <<name = tikzFig, echo = FALSE, fig = TRUE>>=
>>>>> Full code is given below:
>>>>> \documentclass{article}\usepackage{tikz}
>>>>> Figure~\ref{tikzExampleFig} is and an example of \texttt{tikzDevice}
>>>
>>> package.
>>>>>
>>>>> \begin{figure}\begin{center}
>>>>> <<name = tikzFig, echo = FALSE, results = tex>>=
>>>>> library(tikzDevice)tikz(console = TRUE)plot(sin, -pi, pi, main = "A
>>>>> stand
>>>
>>> alone TikZ plot", xlab = "x", ylab = "sin(x)")dummy <- dev.off()@
>>>>>
>>>>> \caption{Example of tikz
>>>
>>> graph}\label{tikzExampleFig}\end{center}\end{figure}
>>>>>
>>>>> \end{document}
>>>>
>>>>
>>>> Your example isn't usable -- please post in plain text, not HTML.
>>>>
>>>> I can't tell whether you are trying to use Sweave or knitr. If you're
>>>> using knitr, see the discussion of dev = "tikz" in
>>>> <http://yihui.name/knitr/>. If you're using Sweave, you probably need
>>>> pgfSweave.
>>>>
>>>> Duncan Murdoch


From dcarlson at tamu.edu  Mon Jan 18 20:00:45 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 18 Jan 2016 19:00:45 +0000
Subject: [R] LSD value
In-Reply-To: <CAJ7mryKgwGa9R5kt273qq9sjmCRT2G8K0+S6o3znt_z3ohiyTQ@mail.gmail.com>
References: <CAJ7mryKgwGa9R5kt273qq9sjmCRT2G8K0+S6o3znt_z3ohiyTQ@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6FC4B4@mb02.ads.tamu.edu>

Providing us with a reproducible example, makes it much easier to answer your question. The object returned by LSD.test() is completely different from the object described in the "Value" section of its manual page which makes things confusing. If you look at the first example on the manual page:

> library(agricolae)
> data(sweetpotato)
> model<-aov(yield~virus, data=sweetpotato)
> out <- LSD.test(model,"virus", p.adj="bonferroni")
> out
$statistics
    Mean      CV  MSerror     LSD
  27.625 17.1666 22.48917 13.4704

$parameters
  Df ntr bonferroni alpha       test name.t
   8   4   3.478879  0.05 bonferroni  virus

$means
      yield      std r       LCL      UCL  Min  Max
cc 24.40000 3.609709 3 18.086268 30.71373 21.7 28.5
fc 12.86667 2.159475 3  6.552935 19.18040 10.6 14.9
ff 36.33333 7.333030 3 30.019601 42.64707 28.0 41.8
oo 36.90000 4.300000 3 30.586268 43.21373 32.1 40.4

$comparison
NULL

$groups
  trt    means  M
1  oo 36.90000  a
2  ff 36.33333  a
3  cc 24.40000 ab
4  fc 12.86667  b

So you can get the LSD statistic with

> out$statistics$LSD
[1] 13.4704
> out$statistics[4]
      LSD
  13.4704
> out[[1]][4]
      LSD
  13.4704

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jianling Fan
Sent: Monday, January 18, 2016 10:47 AM
To: r-help
Subject: [R] LSD value

Hello, everyone,

I am using LSD.test() from package "agricolae" to do my anova
analysis. I know I can calculate LSD value by its equation
t*sqrt(MSE*2/n), but I am wondering if there is a code or something
that can give the LSD value more directly in R?

Thanks!

Julian

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Andrea.Segovia at ssc-spc.gc.ca  Mon Jan 18 19:51:59 2016
From: Andrea.Segovia at ssc-spc.gc.ca (Segovia, Andrea)
Date: Mon, 18 Jan 2016 18:51:59 +0000
Subject: [R] Installation of R Rmpi and snow on cluster running Platform MPI
 and PBS Pro
Message-ID: <5319C6CC5F7E774DA0C5532A43FA8A6539CFC685@SVNSBIOMBX01.ENT.dfo-mpo.ca>

Hello,

I am new to R support. I am trying to install R on a compute cluster running Platform MPI and PBS Pro.

I have installed R, Rmpi and snow - at first glance successfully.

When I try to run a simple Rmpi program, however, I get a Segmentation fault on mpi send.?

The error messages reads as follows:

segoviaa at c01 Rmpi_examples]$ mpirun -e PATH=/export/opt/R-3.2.2/bin:$PATH -hostfile myhostfile -np 8 R --no-save -q ?<
mpi_test2.r

*** caught segfault ***
ddress 0x78, cause 'memory not mapped'

raceback:
1: .Call("mpi_send", .force.type(x, type), as.integer(type), as.integer(dest), ? ? as.integer(tag), as.integer(comm), P
CKAGE = "Rmpi")
2: mpi.send(x = scmd.arg, type = 4, dest = i, tag = 50000 + i, comm = comm)
3: mpi.bcast.cmd(.mpi.worker.exec, tag = tag, ret = ret, simplify = simplify, ? ? comm = comm)
4: mpi.remote.exec(mpi.get.processor.name(), comm = comm)
5: unlist(mpi.remote.exec(mpi.get.processor.name(), comm = comm))
6: slave.hostinfo(1)
borting ...
PI Application rank 0 killed before MPI_Finalize() with signal 11

The program I am running is:

[segoviaa at c01 Rmpi_examples]$ more Rmpi_test2.r
# The corresponding cluster of processes is already pre-constructed with mpirun.

# This where the main work happens
mpi.remote.exec(paste("Rank",mpi.comm.rank(),"on",Sys.info()[c("nodename")]))

# Shut down the cluster and clean up any remaining connections between machines.
mpi.close.Rslaves()
mpi.quit()

The Rprofile is located in the running directory, it is being run, and looks like this:

# This R profile can be used when a cluster does not allow spawning or a job
# scheduler is required to launch any parallel jobs. Saving this file as
# .Rprofile in the working directory or root directory. For unix platform, run
# mpirun -n [cpu numbers] R --no-save -q

# Another way is to modify R_home_dir/bin/R by adding the following line after
# R_HOME_DIR
# R_PROFILE=${R_HOME_DIR}/library/Rmpi/Rprofile; export R_PROFILE

# For windows platform with mpich2, use mpiexec wrapper and specify a working
# directory where .Rprofile is inside.

# Cannot be used as Rprofile.site because it will not work

# If no CPU consumptions of slaves while waiting are desirable, change
# nonblocak=FALSE to nonblock=TRUE and change sleep time accordingly

# Following system libraries are not loaded automatically. So manual loads are
# needed.
#
library(utils)
library(stats)
library(datasets)
library(grDevices)
library(graphics)
library(methods)

#Change to TRUE if you don't want any slave host info
quiet=FALSE

if (!invisible(library(Rmpi,logical.return = TRUE))){
? ? warning("Rmpi cannot be loaded")
? ? q(save = "no")
}

options(error=quote(assign(".mpi.err", FALSE, envir = .GlobalEnv)))

if (mpi.comm.size(0) > 1)
? ? invisible(mpi.comm.dup(0,1))

if (mpi.comm.rank(0) >0){
? ? #sys.load.image(".RData",TRUE)
? ? options(echo=FALSE)
? ? .comm <- 1
? ? mpi.barrier(0)
? ? repeat
? ? ? ? ? ? ? ? try(eval(mpi.bcast.cmd(rank=0,comm=.comm, nonblock=FALSE, sleep=0.1)),TRUE)
? ? ? ? if (is.loaded("mpi_comm_disconnect"))
? ? ? ? mpi.comm.disconnect(.comm)
? ? else mpi.comm.free(.comm)
? ? mpi.quit()
}

if (mpi.comm.rank(0)==0) {
? ? #options(echo=TRUE)
? ? mpi.barrier(0)
? ? if(mpi.comm.size(0) > 1 && !quiet)
? ? ? ? slave.hostinfo(1)
}

.Last <- function(){
? ? if (is.loaded("mpi_initialize")){
? ? ? ? if (mpi.comm.size(1) > 1){
? ? ? ? ? ? print("Please use mpi.close.Rslaves() to close slaves")
? ? ? ? ? ? mpi.close.Rslaves(comm=1)
? ? ? ? }
? ? }
? ? print("Please use mpi.quit() to quit R")
? ? mpi.quit()
}

I am not certain that the R installation was successful, either. I installed R as follows:

./configure --prefix=/export/opt/R-3.2.2
make
make check
make install
make install-pdf
make install-tests


I installed Rmpi as follows:

[root at c01 R-3.2.2]# R CMD INSTALL Rmpi_0.6-5.tar.gz "--configure-args=--with-Rmpi-include=/export/opt/platform_mpi/include/ --with-Rmpi-libpath=/export/opt/platform_mpi/lib/linux_amd64/ --with-Rmpi-type=OPENMPI"

I installed snow as follows:

[root at c01 R-3.2.2]# R CMD INSTALL snow -l /export/opt/R-3.2.2
* installing *source* package 'snow' ...
** package 'snow' successfully unpacked and MD5 sums checked
** R
** inst
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (snow)

Any suggestions would be greatly appreciated.

Thank you,
Andrea

Andrea Segovia - Team Lead, HPC Services 
Science Portfolio/Portfolio de la Science 
Shared Services Canada/Services partag?s Canada 


From fanjianling at gmail.com  Mon Jan 18 20:29:42 2016
From: fanjianling at gmail.com (Jianling Fan)
Date: Mon, 18 Jan 2016 13:29:42 -0600
Subject: [R] LSD value
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6FC4B4@mb02.ads.tamu.edu>
References: <CAJ7mryKgwGa9R5kt273qq9sjmCRT2G8K0+S6o3znt_z3ohiyTQ@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6FC4B4@mb02.ads.tamu.edu>
Message-ID: <CAJ7mryJKyzrAGpafKEhE4Fec+jWJN_Lj2stC3DZeF=Q8t_mQLw@mail.gmail.com>

Thanks David,

That's exactly what I need! Thanks very much for your example.

Best regards,

Julian






On 18 January 2016 at 13:00, David L Carlson <dcarlson at tamu.edu> wrote:
> Providing us with a reproducible example, makes it much easier to answer your question. The object returned by LSD.test() is completely different from the object described in the "Value" section of its manual page which makes things confusing. If you look at the first example on the manual page:
>
>> library(agricolae)
>> data(sweetpotato)
>> model<-aov(yield~virus, data=sweetpotato)
>> out <- LSD.test(model,"virus", p.adj="bonferroni")
>> out
> $statistics
>     Mean      CV  MSerror     LSD
>   27.625 17.1666 22.48917 13.4704
>
> $parameters
>   Df ntr bonferroni alpha       test name.t
>    8   4   3.478879  0.05 bonferroni  virus
>
> $means
>       yield      std r       LCL      UCL  Min  Max
> cc 24.40000 3.609709 3 18.086268 30.71373 21.7 28.5
> fc 12.86667 2.159475 3  6.552935 19.18040 10.6 14.9
> ff 36.33333 7.333030 3 30.019601 42.64707 28.0 41.8
> oo 36.90000 4.300000 3 30.586268 43.21373 32.1 40.4
>
> $comparison
> NULL
>
> $groups
>   trt    means  M
> 1  oo 36.90000  a
> 2  ff 36.33333  a
> 3  cc 24.40000 ab
> 4  fc 12.86667  b
>
> So you can get the LSD statistic with
>
>> out$statistics$LSD
> [1] 13.4704
>> out$statistics[4]
>       LSD
>   13.4704
>> out[[1]][4]
>       LSD
>   13.4704
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jianling Fan
> Sent: Monday, January 18, 2016 10:47 AM
> To: r-help
> Subject: [R] LSD value
>
> Hello, everyone,
>
> I am using LSD.test() from package "agricolae" to do my anova
> analysis. I know I can calculate LSD value by its equation
> t*sqrt(MSE*2/n), but I am wondering if there is a code or something
> that can give the LSD value more directly in R?
>
> Thanks!
>
> Julian
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From zadig_1 at excite.com  Tue Jan 19 02:55:14 2016
From: zadig_1 at excite.com (ce)
Date: Mon, 18 Jan 2016 20:55:14 -0500
Subject: [R] xts/zoo index problem?
Message-ID: <20160118205514.593@web004.roc2.bluetie.com>

Dear all,

I have this code :

library(xts)
a <- structure(c(1,2), class = c("xts", "zoo"), .indexCLASS = c("POSIXct", 
"POSIXt"), .indexTZ = "", tclass = c("POSIXct", "POSIXt"), tzone = "", index = structure(c(1453137885.23, 
1453149114.079), tzone = "", tclass = c("POSIXct", "POSIXt")), .Dim = c(2L, 
1L), .Dimnames = list(NULL, "value"))

a

a["2016-01-18 12:24:45.230"]
#                    value
#2016-01-18 12:24:45     1

a["2016-01-18 15:31:54.078"]
#                   value

Why second line doesn't show the value? Something to do with miliseconds ?
Thanks 
CE


From zadig_1 at excite.com  Tue Jan 19 03:31:11 2016
From: zadig_1 at excite.com (ce)
Date: Mon, 18 Jan 2016 21:31:11 -0500
Subject: [R] xts/zoo index problem?
Message-ID: <20160118213111.30423@web005.roc2.bluetie.com>


yes it shows , more interesting :

>  a["2016-01-18 15:31:54.0"]
                    value
2016-01-18 15:31:54     2

> a["2016-01-18 15:31:54.07"]
     value

> a["2016-01-18 15:31:54.079"]
                    value
2016-01-18 15:31:54     2


why it doesn't work with .07 milisecond  but work with .079  and .0 ??


-----Original Message-----
From: "Huzefa Khalil" [huzefa.khalil at umich.edu]
Date: 01/18/2016 09:26 PM
To: "ce" <zadig_1 at excite.com>
CC: r-help at r-project.org
Subject: Re: [R] xts/zoo index problem?

Try

a["2016-01-18 15:31:54.079"]

The question though is why R displays the milliseconds as "078", when
it is clearly "079"...

-h



On Mon, Jan 18, 2016 at 8:55 PM, ce <zadig_1 at excite.com> wrote:
> Dear all,
>
> I have this code :
>
> library(xts)
> a <- structure(c(1,2), class = c("xts", "zoo"), .indexCLASS = c("POSIXct",
> "POSIXt"), .indexTZ = "", tclass = c("POSIXct", "POSIXt"), tzone = "", index = structure(c(1453137885.23,
> 1453149114.079), tzone = "", tclass = c("POSIXct", "POSIXt")), .Dim = c(2L,
> 1L), .Dimnames = list(NULL, "value"))
>
> a
>
> a["2016-01-18 12:24:45.230"]
> #                    value
> #2016-01-18 12:24:45     1
>
> a["2016-01-18 15:31:54.078"]
> #                   value
>
> Why second line doesn't show the value? Something to do with miliseconds ?
> Thanks
> CE
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From josh.m.ulrich at gmail.com  Tue Jan 19 03:55:07 2016
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 18 Jan 2016 20:55:07 -0600
Subject: [R] xts/zoo index problem?
In-Reply-To: <20160118213111.30423@web005.roc2.bluetie.com>
References: <20160118213111.30423@web005.roc2.bluetie.com>
Message-ID: <CAPPM_gSup0Zm2pSZKCs6qNJsSJ2T06p0yZ3Tv3bNKr7NtVb7mA@mail.gmail.com>

On Mon, Jan 18, 2016 at 8:31 PM, ce <zadig_1 at excite.com> wrote:
>
> yes it shows , more interesting :
>
>>  a["2016-01-18 15:31:54.0"]
>                     value
> 2016-01-18 15:31:54     2
>
>> a["2016-01-18 15:31:54.07"]
>      value
>
>> a["2016-01-18 15:31:54.079"]
>                     value
> 2016-01-18 15:31:54     2
>
>
> why it doesn't work with .07 milisecond  but work with .079  and .0 ??
>
It's not surprising why it doesn't work with 0.070.  It doesn't work
because that's not what the milliseconds component is.  It's also not
surprising why it works with 0.079, because that's what the
milliseconds component is (how something prints is not what it is
equal to).  It is a bit surprising that a value is returned when the
milliseconds component is zero.

>
> -----Original Message-----
> From: "Huzefa Khalil" [huzefa.khalil at umich.edu]
> Date: 01/18/2016 09:26 PM
> To: "ce" <zadig_1 at excite.com>
> CC: r-help at r-project.org
> Subject: Re: [R] xts/zoo index problem?
>
> Try
>
> a["2016-01-18 15:31:54.079"]
>
> The question though is why R displays the milliseconds as "078", when
> it is clearly "079"...
>
See this StackOverflow question for discussion around why R prints
fractional seconds the way it does:
http://stackoverflow.com/q/7726034/271616

> -h
>
>
>
> On Mon, Jan 18, 2016 at 8:55 PM, ce <zadig_1 at excite.com> wrote:
>> Dear all,
>>
>> I have this code :
>>
>> library(xts)
>> a <- structure(c(1,2), class = c("xts", "zoo"), .indexCLASS = c("POSIXct",
>> "POSIXt"), .indexTZ = "", tclass = c("POSIXct", "POSIXt"), tzone = "", index = structure(c(1453137885.23,
>> 1453149114.079), tzone = "", tclass = c("POSIXct", "POSIXt")), .Dim = c(2L,
>> 1L), .Dimnames = list(NULL, "value"))
>>
Use a specific timezone.  Everyone does not share your local timezone,
and you don't say what it is.

library(xts)
# don't warn about object TZ vs local TZ differences
options(xts_check_TZ=FALSE, digits.secs=6)
a <- structure(1:2,
  class = c("xts", "zoo"), .indexCLASS=c("POSIXct", "POSIXt"),
  .indexTZ = "UTC", tclass = c("POSIXct", "POSIXt"), tzone = "UTC",
  index = structure(c(1453137885.23, 1453149114.079), tzone = "UTC",
  tclass = c("POSIXct", "POSIXt")), .Dim = c(2L, 1L),
  .Dimnames = list(NULL, "value"))

>> a
>>
>> a["2016-01-18 12:24:45.230"]
>> #                    value
>> #2016-01-18 12:24:45     1
>>
>> a["2016-01-18 15:31:54.078"]
>> #                   value
>>
>> Why second line doesn't show the value? Something to do with miliseconds ?
>> Thanks
>> CE
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From huzefa.khalil at umich.edu  Tue Jan 19 03:26:12 2016
From: huzefa.khalil at umich.edu (Huzefa Khalil)
Date: Mon, 18 Jan 2016 21:26:12 -0500
Subject: [R] xts/zoo index problem?
In-Reply-To: <20160118205514.593@web004.roc2.bluetie.com>
References: <20160118205514.593@web004.roc2.bluetie.com>
Message-ID: <CADsG8gOF6zuZWQCKEwj9EQXOarV3eu89FkeS=UnhfAFhswFy+w@mail.gmail.com>

Try

a["2016-01-18 15:31:54.079"]

The question though is why R displays the milliseconds as "078", when
it is clearly "079"...

-h



On Mon, Jan 18, 2016 at 8:55 PM, ce <zadig_1 at excite.com> wrote:
> Dear all,
>
> I have this code :
>
> library(xts)
> a <- structure(c(1,2), class = c("xts", "zoo"), .indexCLASS = c("POSIXct",
> "POSIXt"), .indexTZ = "", tclass = c("POSIXct", "POSIXt"), tzone = "", index = structure(c(1453137885.23,
> 1453149114.079), tzone = "", tclass = c("POSIXct", "POSIXt")), .Dim = c(2L,
> 1L), .Dimnames = list(NULL, "value"))
>
> a
>
> a["2016-01-18 12:24:45.230"]
> #                    value
> #2016-01-18 12:24:45     1
>
> a["2016-01-18 15:31:54.078"]
> #                   value
>
> Why second line doesn't show the value? Something to do with miliseconds ?
> Thanks
> CE
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From huzefa.khalil at umich.edu  Tue Jan 19 03:39:10 2016
From: huzefa.khalil at umich.edu (Huzefa Khalil)
Date: Mon, 18 Jan 2016 21:39:10 -0500
Subject: [R] xts/zoo index problem?
In-Reply-To: <20160118213111.30423@web005.roc2.bluetie.com>
References: <20160118213111.30423@web005.roc2.bluetie.com>
Message-ID: <CADsG8gPnjowJaf-icDYebRGbt4Z9jnSvaP7MnVqH_VFpCuUS=g@mail.gmail.com>

Well it works when you don't put the milliseconds because it is
matching that part of the date which is provided. Hence, the following
all work:

a["2016-01-18 15:31:54.0"]
a["2016-01-18 15:31:54"]
a["2016-01-18 15:31"]
a["2016-01-18 15"]

However, what I don't get is why it is showing 1453149114.079 as
"2016-01-18 15:31:54.078". It should be showing it as "2016-01-18
15:31:54.079".

Anybody?

On Mon, Jan 18, 2016 at 9:31 PM, ce <zadig_1 at excite.com> wrote:
>
> yes it shows , more interesting :
>
>>  a["2016-01-18 15:31:54.0"]
>                     value
> 2016-01-18 15:31:54     2
>
>> a["2016-01-18 15:31:54.07"]
>      value
>
>> a["2016-01-18 15:31:54.079"]
>                     value
> 2016-01-18 15:31:54     2
>
>
> why it doesn't work with .07 milisecond  but work with .079  and .0 ??
>
>
> -----Original Message-----
> From: "Huzefa Khalil" [huzefa.khalil at umich.edu]
> Date: 01/18/2016 09:26 PM
> To: "ce" <zadig_1 at excite.com>
> CC: r-help at r-project.org
> Subject: Re: [R] xts/zoo index problem?
>
> Try
>
> a["2016-01-18 15:31:54.079"]
>
> The question though is why R displays the milliseconds as "078", when
> it is clearly "079"...
>
> -h
>
>
>
> On Mon, Jan 18, 2016 at 8:55 PM, ce <zadig_1 at excite.com> wrote:
>> Dear all,
>>
>> I have this code :
>>
>> library(xts)
>> a <- structure(c(1,2), class = c("xts", "zoo"), .indexCLASS = c("POSIXct",
>> "POSIXt"), .indexTZ = "", tclass = c("POSIXct", "POSIXt"), tzone = "", index = structure(c(1453137885.23,
>> 1453149114.079), tzone = "", tclass = c("POSIXct", "POSIXt")), .Dim = c(2L,
>> 1L), .Dimnames = list(NULL, "value"))
>>
>> a
>>
>> a["2016-01-18 12:24:45.230"]
>> #                    value
>> #2016-01-18 12:24:45     1
>>
>> a["2016-01-18 15:31:54.078"]
>> #                   value
>>
>> Why second line doesn't show the value? Something to do with miliseconds ?
>> Thanks
>> CE
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From firoozi_maryam6858 at yahoo.com  Tue Jan 19 11:16:06 2016
From: firoozi_maryam6858 at yahoo.com (MARYAM)
Date: Tue, 19 Jan 2016 13:46:06 +0330
Subject: [R] (no subject)
Message-ID: <A43ECBD6-DDDC-4759-B41B-ECAB2D93FC2B@yahoo.com>

Dear mr/madam
I have 3 matrix with 20 rows and 3 columns like this: I want to sample randomly from three matrix and put it in a vector.How can i do that?

Young.list1<- matrix(NA,nrow= 20,ncol=3)
Young.list1[,1]<- 1:20
Young.list1[,2]<- 0.6
Young.list1[,3]<- 500
colnames(Young.list1)<- c("ID","r","EBV")
###########################################
Young.list2<- matrix(NA,nrow= 20,ncol=3)
Young.list2[,1]<- 21:40
Young.list2[,2]<- 0.7
Young.list2[,3]<- 600
colnames(Young.list2)<- c("ID","r","EBV")
###########################################
Young.list3<- matrix(NA,nrow= 20,ncol=3)
Young.list3[,1]<- 41:60
Young.list3[,2]<- 0.8
Young.list3[,3]<- 700
colnames(Young.list3)<- c("ID","r","EBV")
	[[alternative HTML version deleted]]


From raphaelfsa89 at yahoo.com.br  Tue Jan 19 12:20:58 2016
From: raphaelfsa89 at yahoo.com.br (raphael fernandes)
Date: Tue, 19 Jan 2016 11:20:58 +0000 (UTC)
Subject: [R] Adjust Richards model with nlme
References: <1633695132.7846078.1453202458211.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1633695132.7846078.1453202458211.JavaMail.yahoo@mail.yahoo.com>

Hello everyone,

I have tried to adjust the non linear Richards growth model with this script:

richards <- function(x,beta1,beta2,beta3,beta4)
????beta1*(1-beta2*exp(-x*beta3))^beta4
richards <- deriv(~beta1*(1-beta2*exp(-x*beta3))^beta4,c("beta1","beta2","beta3","beta4"),function(x,beta1,beta2,beta3,beta4){})
FF.richards <- nls(Peso~richards(Idade,beta1,beta2,beta3,beta4),data=femeas,start=c(beta1=370,beta2=9.7,beta3=272,beta4=0.00003))

but I receive the follow error:

Erro em qr.default(.swts * attr(rhs, "gradient")) :?
NA/NaN/Inf em chamada de fun??o externa (argumento 1)
Al?m disso: Mensagens de aviso perdidas:
In log(.expr5) : NaNs produzidos

What can I do to fix it?

	[[alternative HTML version deleted]]


From ludo.pagie at gmail.com  Tue Jan 19 13:50:20 2016
From: ludo.pagie at gmail.com (Ludo Pagie)
Date: Tue, 19 Jan 2016 13:50:20 +0100
Subject: [R] gap.plot fails to plot tick labels on y-axis
Message-ID: <CACpoUFUW44Fntr3_C9wgjv1NzAOHCKxXkATNbaVEFbx21TZq_A@mail.gmail.com>

Dear all,

I'm using gap.plot(..) (plotrix-3-5-11) to make a plot containing a
broken axis. It seems in some cases that gap.plot() does not plot the
proper tick labels at the y-axis. Case in question is when I specify
tick labels for all ticks, on both y-ranges (ones below and one above
the gap). When the gap is on the x-axis it works correctly.

Example code:

##############
# gap on y-axis with tick labels specified
gap.plot(x=1:7, y=c(1:3, 10:13), gap.axis='y', gap=c(4.5,9.5),
xlim=c(1,7), ylim=c(1, 13), ytics=1:13, yticlab=letters[1:13])

# same on x-axis; this works ok
gap.plot(y=1:7, x=c(1:3, 10:13), gap.axis='x', gap=c(4.5,9.5),
ylim=c(1,7), xlim=c(1, 13), xtics=1:13, xticlab=letters[1:13])


sessionInfo()
R version 3.2.1 (2015-06-18)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.1 LTS

locale:
[1] C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] plotrix_3.5-11

###############

The function gap.plot contains a specification of tick labels. The
case corresponding to the error:
show.labels <- c(ytics[littletics], yticlab[bigtics])

Other (correct?) cases use this:
show.labels <- c(yticlab[littletics], yticlab[middletics],yticlab[bigtics])
show.labels <- c(xticlab[littletics], xticlab[bigtics])
show.labels <- c(xticlab[littletics], xticlab[middletics],xticlab[bigtics])

I think the 1st case uses the 'ytics' instead of 'yticlab'. Or can
somebody point out what I do wrong?

cheers, Ludo


From bgunter.4567 at gmail.com  Tue Jan 19 16:44:24 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 19 Jan 2016 07:44:24 -0800
Subject: [R] xts/zoo index problem?
In-Reply-To: <CADsG8gOF6zuZWQCKEwj9EQXOarV3eu89FkeS=UnhfAFhswFy+w@mail.gmail.com>
References: <20160118205514.593@web004.roc2.bluetie.com>
	<CADsG8gOF6zuZWQCKEwj9EQXOarV3eu89FkeS=UnhfAFhswFy+w@mail.gmail.com>
Message-ID: <CAGxFJbQaVb8S-OyE5UHc0+w8MZSVzbBuX0ybpF79rW=pwxMyqw@mail.gmail.com>

FAQ 7.31 maybe. (Unless special software is used) Only a finite number
of numbers can be represented exactly on a computer.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Jan 18, 2016 at 6:26 PM, Huzefa Khalil <huzefa.khalil at umich.edu> wrote:
> Try
>
> a["2016-01-18 15:31:54.079"]
>
> The question though is why R displays the milliseconds as "078", when
> it is clearly "079"...
>
> -h
>
>
>
> On Mon, Jan 18, 2016 at 8:55 PM, ce <zadig_1 at excite.com> wrote:
>> Dear all,
>>
>> I have this code :
>>
>> library(xts)
>> a <- structure(c(1,2), class = c("xts", "zoo"), .indexCLASS = c("POSIXct",
>> "POSIXt"), .indexTZ = "", tclass = c("POSIXct", "POSIXt"), tzone = "", index = structure(c(1453137885.23,
>> 1453149114.079), tzone = "", tclass = c("POSIXct", "POSIXt")), .Dim = c(2L,
>> 1L), .Dimnames = list(NULL, "value"))
>>
>> a
>>
>> a["2016-01-18 12:24:45.230"]
>> #                    value
>> #2016-01-18 12:24:45     1
>>
>> a["2016-01-18 15:31:54.078"]
>> #                   value
>>
>> Why second line doesn't show the value? Something to do with miliseconds ?
>> Thanks
>> CE
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Tue Jan 19 17:01:55 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 19 Jan 2016 08:01:55 -0800
Subject: [R] xts/zoo index problem?
In-Reply-To: <CAGxFJbQaVb8S-OyE5UHc0+w8MZSVzbBuX0ybpF79rW=pwxMyqw@mail.gmail.com>
References: <20160118205514.593@web004.roc2.bluetie.com>
	<CADsG8gOF6zuZWQCKEwj9EQXOarV3eu89FkeS=UnhfAFhswFy+w@mail.gmail.com>
	<CAGxFJbQaVb8S-OyE5UHc0+w8MZSVzbBuX0ybpF79rW=pwxMyqw@mail.gmail.com>
Message-ID: <38B448DF-B9EB-4EE7-B1D9-60D1F9D08012@comcast.net>


> On Jan 19, 2016, at 7:44 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> FAQ 7.31 maybe. (Unless special software is used) Only a finite number
> of numbers can be represented exactly on a computer.

Agree that seemed the likely explanation out of the gate. Here's the evidence.

> print( as.numeric(index(a), "%OS3"), digits=20)
[1] 1453137885.2300000191 1453149114.0789999962

-- 
Best;

> 
> Cheers,
> Bert
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Mon, Jan 18, 2016 at 6:26 PM, Huzefa Khalil <huzefa.khalil at umich.edu> wrote:
>> Try
>> 
>> a["2016-01-18 15:31:54.079"]
>> 
>> The question though is why R displays the milliseconds as "078", when
>> it is clearly "079"...
>> 
>> -h
>> 
>> 
>> 
>> On Mon, Jan 18, 2016 at 8:55 PM, ce <zadig_1 at excite.com> wrote:
>>> Dear all,
>>> 
>>> I have this code :
>>> 
>>> library(xts)
>>> a <- structure(c(1,2), class = c("xts", "zoo"), .indexCLASS = c("POSIXct",
>>> "POSIXt"), .indexTZ = "", tclass = c("POSIXct", "POSIXt"), tzone = "", index = structure(c(1453137885.23,
>>> 1453149114.079), tzone = "", tclass = c("POSIXct", "POSIXt")), .Dim = c(2L,
>>> 1L), .Dimnames = list(NULL, "value"))
>>> 
>>> a
>>> 
>>> a["2016-01-18 12:24:45.230"]
>>> #                    value
>>> #2016-01-18 12:24:45     1
>>> 
>>> a["2016-01-18 15:31:54.078"]
>>> #                   value
>>> 
>>> Why second line doesn't show the value? Something to do with miliseconds ?
>>> Thanks
>>> CE
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From joeceradini at gmail.com  Tue Jan 19 18:48:13 2016
From: joeceradini at gmail.com (Joe Ceradini)
Date: Tue, 19 Jan 2016 10:48:13 -0700
Subject: [R] Survival::coxph (clogit),
	survConcordance vs. summary(fit) concordance
Message-ID: <CAKq2vL5ANQG_UD4sjHtjLtm0z-pVC7sQEvhQbW71CMcG-VaTDQ@mail.gmail.com>

Hi,

I'm running conditional logistic regression with survival::clogit. I have
"1-1 case-control" data, i.e., there is 1 case and 1 control in each strata.

Model:
fit <- clogit(resp ~ x1 + x2, strata(ID), cluster(site), method ="efron",
data = dat)
Where resp is 1's and 0's, and x1 and x2 are both continuous.

Predictors are both significant. A snippet of summary(fit):
Concordance= 0.763  (se = 0.5 )
Rsquare= 0.304   (max possible= 0.5 )
Likelihood ratio test= 27.54  on 2 df,   p=1.047e-06
Wald test            = 17.19  on 2 df,   p=0.0001853
Score (logrank) test = 17.43  on 2 df,   p=0.0001644,   Robust = 6.66
 p=0.03574

The concordance estimate seems good but the SE is HUGE.

I get a very different estimate from the survConcordance function, which I
know says computes concordance for a "single continuous covariate", but it
runs on my model with 2 continuous covariates....

survConcordance(Surv(rep(1, 76L), resp) ~ predict(fit), dat)
n= 76
Concordance= 0.9106648 se= 0.09365047
concordant  discordant   tied.risk   tied.time    std(c-d)
 1315.0000   129.0000     0.0000   703.0000   270.4626

Are both of these concordance estimates valid but providing different
information?
Is one more appropriate for measuring "performance" (in the AUC sense) of
conditional logistic models?
Is it possible that the HUGE SE estimate represents a convergence problem
(no warnings were thrown when fit the model), or is this model just useless?

Thanks!
-- 
Cooperative Fish and Wildlife Research Unit
Zoology and Physiology Dept.
University of Wyoming
JoeCeradini at gmail.com / 914.707.8506
wyocoopunit.org

	[[alternative HTML version deleted]]


From macqueen1 at llnl.gov  Tue Jan 19 19:26:12 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Tue, 19 Jan 2016 18:26:12 +0000
Subject: [R] Use SQL in R environment
In-Reply-To: <733209980.4690835.1452880779186.JavaMail.yahoo@mail.yahoo.com>
References: <733209980.4690835.1452880779186.JavaMail.yahoo.ref@mail.yahoo.com>
	<733209980.4690835.1452880779186.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <D2C3BEDE.14CF2E%macqueen1@llnl.gov>

If you're wanting to retrieve data from external databases (I vaguely
recall that is what SAS's proc sql does), then start by looking at the DBI
package, which provides support for ROracle, RMySQL, and several more.
There is also RODBC.


-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 1/15/16, 9:59 AM, "R-help on behalf of Amoy Yang via R-help"
<r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:

> Hi All, 
>I am new here and a beginner for R. Can I use SQL procedure in R
>environment as it can be done in SAS starting with PROC SQL;
>Thanks for helps!
>
>Amoy
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Tue Jan 19 20:48:10 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Tue, 19 Jan 2016 11:48:10 -0800
Subject: [R] tcltk tkwidget(..."table")
Message-ID: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>

Does anyone know a simple way to create a tcltk table with columns of
varying widths?

-Dan



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Tue Jan 19 21:35:56 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Tue, 19 Jan 2016 20:35:56 +0000
Subject: [R] (no subject)
In-Reply-To: <A43ECBD6-DDDC-4759-B41B-ECAB2D93FC2B@yahoo.com>
Message-ID: <20160119203556.Horde.9TKV3BIrowvzk6LqUOk2DKY@mail.sapo.pt>

Hello,

What do you want to sample? Rows? With or without replacement? You  
need to give us more information on what you want.

Start by seeing the help page for ?sample

Hope this helps,

Rui Barradas
?

Citando MARYAM <firoozi_maryam6858 at yahoo.com>:

> Dear mr/madam
> I have 3 matrix with 20 rows and 3 columns like this: I want to  
> sample randomly from three matrix and put it in a vector.How can i  
> do that?
>
> Young.list1<- matrix(NA,nrow= 20,ncol=3)
> Young.list1[,1]<- 1:20
> Young.list1[,2]<- 0.6
> Young.list1[,3]<- 500
> colnames(Young.list1)<- c("ID","r","EBV")
> ###########################################
> Young.list2<- matrix(NA,nrow= 20,ncol=3)
> Young.list2[,1]<- 21:40
> Young.list2[,2]<- 0.7
> Young.list2[,3]<- 600
> colnames(Young.list2)<- c("ID","r","EBV")
> ###########################################
> Young.list3<- matrix(NA,nrow= 20,ncol=3)
> Young.list3[,1]<- 41:60
> Young.list3[,2]<- 0.8
> Young.list3[,3]<- 700
> colnames(Young.list3)<- c("ID","r","EBV")
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Jan 19 21:47:53 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 19 Jan 2016 12:47:53 -0800
Subject: [R] Adjust Richards model with nlme
In-Reply-To: <1633695132.7846078.1453202458211.JavaMail.yahoo@mail.yahoo.com>
References: <1633695132.7846078.1453202458211.JavaMail.yahoo.ref@mail.yahoo.com>
	<1633695132.7846078.1453202458211.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <C4005ADC-83CC-401C-99D7-CBA9F13B6A32@comcast.net>


> On Jan 19, 2016, at 3:20 AM, raphael fernandes <raphaelfsa89 at yahoo.com.br> wrote:
> 
> Hello everyone,
> 
> I have tried to adjust the non linear Richards growth model with this script:
> 
> richards <- function(x,beta1,beta2,beta3,beta4)
>     beta1*(1-beta2*exp(-x*beta3))^beta4
> richards <- deriv(~beta1*(1-beta2*exp(-x*beta3))^beta4,c("beta1","beta2","beta3","beta4"),function(x,beta1,beta2,beta3,beta4){})
> FF.richards <- nls(Peso~richards(Idade,beta1,beta2,beta3,beta4),data=femeas,start=c(beta1=370,beta2=9.7,beta3=272,beta4=0.00003))
> 
> but I receive the follow error:
> 
> Erro em qr.default(.swts * attr(rhs, "gradient")) : 
> NA/NaN/Inf em chamada de fun??o externa (argumento 1)
> Al?m disso: Mensagens de aviso perdidas:
> In log(.expr5) : NaNs produzidos

Yes. Any interim trials for that iterative procedure that had a negative value for expr5 in the `richards` object would trigger that error:

richards
function (x, beta1, beta2, beta3, beta4) 
{
    .expr3 <- exp(-x * beta3)
    .expr5 <- 1 - beta2 * .expr3
    .expr6 <- .expr5^beta4
    .expr9 <- .expr5^(beta4 - 1)
    .value <- beta1 * .expr6
    .grad <- array(0, c(length(.value), 4L), list(NULL, c("beta1", 
        "beta2", "beta3", "beta4")))
    .grad[, "beta1"] <- .expr6
    .grad[, "beta2"] <- -(beta1 * (.expr9 * (beta4 * .expr3)))
    .grad[, "beta3"] <- beta1 * (.expr9 * (beta4 * (beta2 * (.expr3 * 
        x))))
    .grad[, "beta4"] <- beta1 * (.expr6 * log(.expr5))
    attr(.value, "gradient") <- .grad
    .value
}

> 

> What can I do to fix it?

You might try posting some data and seeing if the optimization gurus have insights. I hesitate to offer the hack I constructed to sidestep the log(neg_num) problem since I have no way to testing it for sensibility , and I'm certainly not in the guru-category.

-- 
David
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Wed Jan 20 01:14:55 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 20 Jan 2016 11:14:55 +1100
Subject: [R] gap.plot fails to plot tick labels on y-axis
In-Reply-To: <CACpoUFUW44Fntr3_C9wgjv1NzAOHCKxXkATNbaVEFbx21TZq_A@mail.gmail.com>
References: <CACpoUFUW44Fntr3_C9wgjv1NzAOHCKxXkATNbaVEFbx21TZq_A@mail.gmail.com>
Message-ID: <CA+8X3fVR4Hc5ee_JinGN7t=5dxmz1G5rgoda6TeSGG+ukNXuqg@mail.gmail.com>

Hi Ludo,
The good news is that it was easy. The bad news is that it was my fault. I
hadn't tried labeling the axes with something other than numbers. At about
line 109 in the source code, this line:

    show.labels<-c(ytics[littletics],yticlab[bigtics])

should read:

    show.labels<-c(yticlab[littletics],yticlab[bigtics])

Attached is the code that you can "source" after loading plotrix and the
fix will be in the next version. Thanks.

Jim

On Tue, Jan 19, 2016 at 11:50 PM, Ludo Pagie <ludo.pagie at gmail.com> wrote:

> Dear all,
>
> I'm using gap.plot(..) (plotrix-3-5-11) to make a plot containing a
> broken axis. It seems in some cases that gap.plot() does not plot the
> proper tick labels at the y-axis. Case in question is when I specify
> tick labels for all ticks, on both y-ranges (ones below and one above
> the gap). When the gap is on the x-axis it works correctly.
>
> Example code:
>
> ##############
> # gap on y-axis with tick labels specified
> gap.plot(x=1:7, y=c(1:3, 10:13), gap.axis='y', gap=c(4.5,9.5),
> xlim=c(1,7), ylim=c(1, 13), ytics=1:13, yticlab=letters[1:13])
>
> # same on x-axis; this works ok
> gap.plot(y=1:7, x=c(1:3, 10:13), gap.axis='x', gap=c(4.5,9.5),
> ylim=c(1,7), xlim=c(1, 13), xtics=1:13, xticlab=letters[1:13])
>
>
> sessionInfo()
> R version 3.2.1 (2015-06-18)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 14.04.1 LTS
>
> locale:
> [1] C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] plotrix_3.5-11
>
> ###############
>
> The function gap.plot contains a specification of tick labels. The
> case corresponding to the error:
> show.labels <- c(ytics[littletics], yticlab[bigtics])
>
> Other (correct?) cases use this:
> show.labels <- c(yticlab[littletics], yticlab[middletics],yticlab[bigtics])
> show.labels <- c(xticlab[littletics], xticlab[bigtics])
> show.labels <- c(xticlab[littletics], xticlab[middletics],xticlab[bigtics])
>
> I think the 1st case uses the 'ytics' instead of 'yticlab'. Or can
> somebody point out what I do wrong?
>
> cheers, Ludo
>

From drjimlemon at gmail.com  Wed Jan 20 01:21:29 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 20 Jan 2016 11:21:29 +1100
Subject: [R] (no subject)
In-Reply-To: <20160119203556.Horde.9TKV3BIrowvzk6LqUOk2DKY@mail.sapo.pt>
References: <A43ECBD6-DDDC-4759-B41B-ECAB2D93FC2B@yahoo.com>
	<20160119203556.Horde.9TKV3BIrowvzk6LqUOk2DKY@mail.sapo.pt>
Message-ID: <CA+8X3fUj903NmKQuK0eAtMkkBrVD+7acqYyFTS7hCfpbjZa=SQ@mail.gmail.com>

Hi Maryam,
Sounds like:

c(Young.list1[sample(1:20,1),],Young.list2[sample(1:20,1),],Young.list3[sample(1:20,1),])

to me.

Jim


On Wed, Jan 20, 2016 at 7:35 AM, <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> What do you want to sample? Rows? With or without replacement? You
> need to give us more information on what you want.
>
> Start by seeing the help page for ?sample
>
> Hope this helps,
>
> Rui Barradas
>
>
> Citando MARYAM <firoozi_maryam6858 at yahoo.com>:
>
> > Dear mr/madam
> > I have 3 matrix with 20 rows and 3 columns like this: I want to
> > sample randomly from three matrix and put it in a vector.How can i
> > do that?
> >
> > Young.list1<- matrix(NA,nrow= 20,ncol=3)
> > Young.list1[,1]<- 1:20
> > Young.list1[,2]<- 0.6
> > Young.list1[,3]<- 500
> > colnames(Young.list1)<- c("ID","r","EBV")
> > ###########################################
> > Young.list2<- matrix(NA,nrow= 20,ncol=3)
> > Young.list2[,1]<- 21:40
> > Young.list2[,2]<- 0.7
> > Young.list2[,3]<- 600
> > colnames(Young.list2)<- c("ID","r","EBV")
> > ###########################################
> > Young.list3<- matrix(NA,nrow= 20,ncol=3)
> > Young.list3[,1]<- 41:60
> > Young.list3[,2]<- 0.8
> > Young.list3[,3]<- 700
> > colnames(Young.list3)<- c("ID","r","EBV")
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.htmland provide commented,
> > minimal, self-contained, reproducible code.
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From reichmanj at sbcglobal.net  Wed Jan 20 01:31:10 2016
From: reichmanj at sbcglobal.net (Jeff Reichman)
Date: Tue, 19 Jan 2016 18:31:10 -0600
Subject: [R] Installing ggplot2
Message-ID: <000001d15319$dea777e0$9bf667a0$@sbcglobal.net>

R-Help

 

I'm unable to load ggplot2.

 

> install.packages("ggplot2")      #appears to run fine

> library(ggplot2)

Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
vI[[j]]) : 

  there is no package called 'Rcpp'

Error: package or namespace load failed for 'ggplot2'

> 

 

Any suggestions

 

Jeff Reichman


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jan 20 02:54:03 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 19 Jan 2016 17:54:03 -0800
Subject: [R] Installing ggplot2
In-Reply-To: <000001d15319$dea777e0$9bf667a0$@sbcglobal.net>
References: <000001d15319$dea777e0$9bf667a0$@sbcglobal.net>
Message-ID: <206F1755-35C3-48FB-A270-62B7C9AC8D2E@comcast.net>


> On Jan 19, 2016, at 4:31 PM, Jeff Reichman <reichmanj at sbcglobal.net> wrote:
> 
> R-Help
> 
> 
> 
> I'm unable to load ggplot2.
> 
> 
> 
>> install.packages("ggplot2")      #appears to run fine

Try instead:

 install.packages("ggplot2", dependencies=TRUE)

> 
>> library(ggplot2)
> 
> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
> vI[[j]]) : 
> 
>  there is no package called 'Rcpp'
> 
> Error: package or namespace load failed for 'ggplot2'
> 
>> 
> 
> 
> 


David Winsemius
Alameda, CA, USA


From kris.angelovski at solutionmetrics.com.au  Wed Jan 20 04:53:24 2016
From: kris.angelovski at solutionmetrics.com.au (Kris Angelovski)
Date: Wed, 20 Jan 2016 03:53:24 +0000
Subject: [R] R Courses: Sydney, Melbourne and Brisbane, February 2016
Message-ID: <SIXPR06MB09694F8A689F970C128E1BA5C5C20@SIXPR06MB0969.apcprd06.prod.outlook.com>



Hi, (apologies for cross-posting)

SolutionMetrics is presenting R courses in February - Sydney, Melbourne & Brisbane.

We also offer courses in-house/at your office. Convenience of staff not travelling and more economical per-user.

Getting Started with R (1 Day) - $880 (Early bird discount: $699, until 31 January 2016)

Introduction to R, Accessing Help, Objects & Classes, Import/Export, Manipulation, Data Analysis and Graphics - Histograms, Box Plots, Bar Charts, Scatter Plots; Changing symbols, colours, style of points, axes, range etc; Statistical Models - Regression. More Info<http://bit.ly/11qFxpO>

Date: 8 Feb, 2016 - Sydney (Mon)
          11 Feb, 2016 - Melbourne (Thu)
          15 Feb, 2016 - Brisbane (Mon)

Intermediate R (1 Day) - $880 (Early bird discount: $699, until 31 January 2016)

Efficient use of R language objects & functions, Big Data, Advanced Visualisations and Statistical models - Multiple Regression, Logistic Regression & Tree models. More Info<http://bit.ly/YBsT5b>

Date: 9 Feb, 2016 - Sydney (Tue)
          12 Feb, 2016 - Melbourne (Fri)
          16 Feb, 2016 - Brisbane (Tue)

To book, please email enquiries at solutionmetrics.com.au<mailto:enquiries at solutionmetrics.com.au> or call +61 2 9233 6888

Cheers
Kris Angelovski | Chief Data Scientist | SolutionMetrics
T +61 2 9233 6888 | M 0488 388 338
E kris.angelovski at solutionmetrics.com.au<mailto:kris.angelovski at solutionmetrics.com.au>
solutionmetrics.com.au<http://www.solutionmetrics.com.au/> | Suite 44, Level 9, 88 Pitt Street Sydney NSW 2000





	[[alternative HTML version deleted]]


From fsmairura at yahoo.com  Wed Jan 20 10:58:15 2016
From: fsmairura at yahoo.com (Franklin Mairura)
Date: Wed, 20 Jan 2016 09:58:15 +0000 (UTC)
Subject: [R] LSD value
In-Reply-To: <CAJ7mryJKyzrAGpafKEhE4Fec+jWJN_Lj2stC3DZeF=Q8t_mQLw@mail.gmail.com>
References: <CAJ7mryJKyzrAGpafKEhE4Fec+jWJN_Lj2stC3DZeF=Q8t_mQLw@mail.gmail.com>
Message-ID: <1688466331.8284993.1453283895671.JavaMail.yahoo@mail.yahoo.com>

How can this be achieved in 2-way or 3 way experiments? 

    On Monday, January 18, 2016 7:31 AM, Jianling Fan <fanjianling at gmail.com> wrote:
 

 Thanks David,

That's exactly what I need! Thanks very much for your example.

Best regards,

Julian






On 18 January 2016 at 13:00, David L Carlson <dcarlson at tamu.edu> wrote:
> Providing us with a reproducible example, makes it much easier to answer your question. The object returned by LSD.test() is completely different from the object described in the "Value" section of its manual page which makes things confusing. If you look at the first example on the manual page:
>
>> library(agricolae)
>> data(sweetpotato)
>> model<-aov(yield~virus, data=sweetpotato)
>> out <- LSD.test(model,"virus", p.adj="bonferroni")
>> out
> $statistics
>? ? Mean? ? ? CV? MSerror? ? LSD
>? 27.625 17.1666 22.48917 13.4704
>
> $parameters
>? Df ntr bonferroni alpha? ? ? test name.t
>? ? 8? 4? 3.478879? 0.05 bonferroni? virus
>
> $means
>? ? ? yield? ? ? std r? ? ? LCL? ? ? UCL? Min? Max
> cc 24.40000 3.609709 3 18.086268 30.71373 21.7 28.5
> fc 12.86667 2.159475 3? 6.552935 19.18040 10.6 14.9
> ff 36.33333 7.333030 3 30.019601 42.64707 28.0 41.8
> oo 36.90000 4.300000 3 30.586268 43.21373 32.1 40.4
>
> $comparison
> NULL
>
> $groups
>? trt? ? means? M
> 1? oo 36.90000? a
> 2? ff 36.33333? a
> 3? cc 24.40000 ab
> 4? fc 12.86667? b
>
> So you can get the LSD statistic with
>
>> out$statistics$LSD
> [1] 13.4704
>> out$statistics[4]
>? ? ? LSD
>? 13.4704
>> out[[1]][4]
>? ? ? LSD
>? 13.4704
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jianling Fan
> Sent: Monday, January 18, 2016 10:47 AM
> To: r-help
> Subject: [R] LSD value
>
> Hello, everyone,
>
> I am using LSD.test() from package "agricolae" to do my anova
> analysis. I know I can calculate LSD value by its equation
> t*sqrt(MSE*2/n), but I am wondering if there is a code or something
> that can give the LSD value more directly in R?
>
> Thanks!
>
> Julian
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


  
	[[alternative HTML version deleted]]


From mviljamaa at kapsi.fi  Wed Jan 20 11:34:55 2016
From: mviljamaa at kapsi.fi (mviljamaa)
Date: Wed, 20 Jan 2016 12:34:55 +0200
Subject: [R] Where to get quantiles (1st parameter q) for pnorm? (Normal
 approximation to binomial distribution)
Message-ID: <c6fce0c0a6307190e607cdad9f3db433@kapsi.fi>

I'm doing Normal approximation to binomial distribution.

My variables are generated by rbinom.

Here: 
http://msemac.redwoods.edu/~darnold/math15/spring2013/R/Activities/ApproxBinomWithNorm.html

it's claimed that normal approximation is done using the command pnorm.

Question:

Where to get quantiles (1st parameter q) for pnorm?

-Matti


From Gerrit.Eichner at math.uni-giessen.de  Wed Jan 20 11:42:13 2016
From: Gerrit.Eichner at math.uni-giessen.de (Gerrit Eichner)
Date: Wed, 20 Jan 2016 11:42:13 +0100 (MET)
Subject: [R] Where to get quantiles (1st parameter q) for
	pnorm?(Normalapproximation to binomial distribution)
In-Reply-To: <c6fce0c0a6307190e607cdad9f3db433@kapsi.fi>
References: <c6fce0c0a6307190e607cdad9f3db433@kapsi.fi>
Message-ID: <Pine.SOC.4.64.1601201141280.4151@solcom.hrz.uni-giessen.de>

Hi, Matti,

use

?pnorm

and read about qnorm.

  Hth  --  Gerrit


On Wed, 20 Jan 2016, mviljamaa wrote:

> I'm doing Normal approximation to binomial distribution.
>
> My variables are generated by rbinom.
>
> Here: 
> http://msemac.redwoods.edu/~darnold/math15/spring2013/R/Activities/ApproxBinomWithNorm.html
>
> it's claimed that normal approximation is done using the command pnorm.
>
> Question:
>
> Where to get quantiles (1st parameter q) for pnorm?
>
> -Matti
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Wed Jan 20 12:21:19 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 20 Jan 2016 12:21:19 +0100
Subject: [R] config args for Rmpfr to link to mpfr and gmp?
In-Reply-To: <1636068662.2450128.1453276865180.JavaMail.zimbra@csc.fi>
References: <1636068662.2450128.1453276865180.JavaMail.zimbra@csc.fi>
Message-ID: <22175.28079.819121.976413@stat.math.ethz.ch>

Dear Seija,

>>>>> Seija Sirki? <seija.sirkia at csc.fi>
>>>>>     on Wed, 20 Jan 2016 10:01:05 +0200 writes:

    > Hello,
    > Sorry to bother with you with this, I'll be very brief: How do I communicate to the Rmpfr package that the (correct versions of) mpfr and gmp libraries are somewhere else than its installation assumes? Pointing me to documentation is fine, I haven't been able to find any beyond the README which tells me that sudo apt-getting the libraries should do. That is not an option in my case.

Sure.

The package has a configure script (auto generated from
configure.ac by GNU autoconf).

Did you see and study the result of

  ./Rmpfr/configure --help

?
It points you (among other) to arguments

  --libdir=DIR            object code libraries [EPREFIX/lib]
  --includedir=DIR        C header files [PREFIX/include]

which I think (but did not check) should be sufficient.
I'd work for a solution by
1) manually find the correct way to call that configure script.
2) Once you found that, you should also be able to do this from R
   by correct setting of the arguments

  configure.args
  configure.vars
  INSTALL.opts

of the

   install.packages()

function.

As I am *not* an expert on the subject of  C compiler / library
/ include files (aka "headers") tweaking on all the many different platforms,
and even more importantly, as this question is of general
interest and so its answer is, too, I'm CC'ing to the R-help
mailing lsit and ask you to use 'reply to all' such that the
topic is kept and dealt with on R-help. 

I hope you'll enjoy working with the Rmpfr R package,
with best regards,

Martin Maechler
ETH Zurich


From chrisaa at med.umich.edu  Wed Jan 20 14:11:21 2016
From: chrisaa at med.umich.edu (Andrews, Chris)
Date: Wed, 20 Jan 2016 13:11:21 +0000
Subject: [R] Survival::coxph (clogit),
 survConcordance vs. summary(fit) concordance
In-Reply-To: <CAKq2vL5ANQG_UD4sjHtjLtm0z-pVC7sQEvhQbW71CMcG-VaTDQ@mail.gmail.com>
References: <CAKq2vL5ANQG_UD4sjHtjLtm0z-pVC7sQEvhQbW71CMcG-VaTDQ@mail.gmail.com>
Message-ID: <30411786F64EEF46856EFBA2CD9177992C4FEBC6@UHEXMBSPR03.umhs.med.umich.edu>

I only get the digest, sorry if this has already been answered.

When I run your code (after creating some data) I get a warning that "weights are ignored in clogit".  This is a result of miscalling the clogit function.  The first 2 commas should be +s.

library(survival)
nn <- 1000
dat <- data.frame(resp = rbinom(nn, 1, 0.5), x1=rnorm(nn), x2=rnorm(nn), ID = rep(seq(nn/2), e=2), site = rep(seq(nn/10), e=10))
fit <- clogit(resp ~ x1 + x2, strata(ID), cluster(site), method ="efron", data = dat) # warning
fit <- clogit(resp ~ x1 + x2 + strata(ID) + cluster(site), method ="efron", data = dat) # no warning
summary(fit)

Chris

-----Original Message-----
From: Joe Ceradini [mailto:joeceradini at gmail.com] 
Sent: Tuesday, January 19, 2016 12:48 PM
To: r-help at r-project.org
Subject: [R] Survival::coxph (clogit), survConcordance vs. summary(fit) concordance

Hi,

I'm running conditional logistic regression with survival::clogit. I have
"1-1 case-control" data, i.e., there is 1 case and 1 control in each strata.

Model:
fit <- clogit(resp ~ x1 + x2, strata(ID), cluster(site), method ="efron",
data = dat)
Where resp is 1's and 0's, and x1 and x2 are both continuous.

Predictors are both significant. A snippet of summary(fit):
Concordance= 0.763  (se = 0.5 )
Rsquare= 0.304   (max possible= 0.5 )
Likelihood ratio test= 27.54  on 2 df,   p=1.047e-06
Wald test            = 17.19  on 2 df,   p=0.0001853
Score (logrank) test = 17.43  on 2 df,   p=0.0001644,   Robust = 6.66
 p=0.03574

The concordance estimate seems good but the SE is HUGE.

I get a very different estimate from the survConcordance function, which I
know says computes concordance for a "single continuous covariate", but it
runs on my model with 2 continuous covariates....

survConcordance(Surv(rep(1, 76L), resp) ~ predict(fit), dat)
n= 76
Concordance= 0.9106648 se= 0.09365047
concordant  discordant   tied.risk   tied.time    std(c-d)
 1315.0000   129.0000     0.0000   703.0000   270.4626

Are both of these concordance estimates valid but providing different
information?
Is one more appropriate for measuring "performance" (in the AUC sense) of
conditional logistic models?
Is it possible that the HUGE SE estimate represents a convergence problem
(no warnings were thrown when fit the model), or is this model just useless?

Thanks!
-- 
Cooperative Fish and Wildlife Research Unit
Zoology and Physiology Dept.
University of Wyoming
JoeCeradini at gmail.com / 914.707.8506
wyocoopunit.org

	[[alternative HTML version deleted]]


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues 

From joeceradini at gmail.com  Wed Jan 20 16:00:48 2016
From: joeceradini at gmail.com (Joe Ceradini)
Date: Wed, 20 Jan 2016 08:00:48 -0700
Subject: [R] Survival::coxph (clogit),
	survConcordance vs. summary(fit) concordance
In-Reply-To: <30411786F64EEF46856EFBA2CD9177992C4FEBC6@UHEXMBSPR03.umhs.med.umich.edu>
References: <CAKq2vL5ANQG_UD4sjHtjLtm0z-pVC7sQEvhQbW71CMcG-VaTDQ@mail.gmail.com>
	<30411786F64EEF46856EFBA2CD9177992C4FEBC6@UHEXMBSPR03.umhs.med.umich.edu>
Message-ID: <CAKq2vL7ppAFhOO3mDKhCH0rfY1hcDj-dwqfidaPNY7KtB1Oqxw@mail.gmail.com>

Thanks for pointing that out, Chris. That was a thoughtless typo on my part
when I was simplifying my model for the sake of posting.

I've run a whole set of models without any problems/warning. My main
question is regarding the difference between the concordance estimate that
summary(fit) reports and the concordance estimated with survConcordance,
particularly in relation to estimating clogit model performance. Also,
whether or not I should be concerned about the giant SE estimate I get for
concordance from summary(fit). This is within the context of a 1:1
case-control study (1 case and 1 control per strata).

Corrected model:
fit <- clogit(resp ~ x1 + x2 + strata(ID) + cluster(site), method ="efron",
data = dat)
Where resp is 1's and 0's, and x1 and x2 are both continuous.

The rest of the code and output details should be in my original post.

Thanks.
Joe

On Wed, Jan 20, 2016 at 6:11 AM, Andrews, Chris <chrisaa at med.umich.edu>
wrote:

> I only get the digest, sorry if this has already been answered.
>
> When I run your code (after creating some data) I get a warning that
> "weights are ignored in clogit".  This is a result of miscalling the clogit
> function.  The first 2 commas should be +s.
>
> library(survival)
> nn <- 1000
> dat <- data.frame(resp = rbinom(nn, 1, 0.5), x1=rnorm(nn), x2=rnorm(nn),
> ID = rep(seq(nn/2), e=2), site = rep(seq(nn/10), e=10))
> fit <- clogit(resp ~ x1 + x2, strata(ID), cluster(site), method ="efron",
> data = dat) # warning
> fit <- clogit(resp ~ x1 + x2 + strata(ID) + cluster(site), method
> ="efron", data = dat) # no warning
> summary(fit)
>
> Chris
>
> -----Original Message-----
> From: Joe Ceradini [mailto:joeceradini at gmail.com]
> Sent: Tuesday, January 19, 2016 12:48 PM
> To: r-help at r-project.org
> Subject: [R] Survival::coxph (clogit), survConcordance vs. summary(fit)
> concordance
>
> Hi,
>
> I'm running conditional logistic regression with survival::clogit. I have
> "1-1 case-control" data, i.e., there is 1 case and 1 control in each
> strata.
>
> Model:
> fit <- clogit(resp ~ x1 + x2, strata(ID), cluster(site), method ="efron",
> data = dat)
> Where resp is 1's and 0's, and x1 and x2 are both continuous.
>
> Predictors are both significant. A snippet of summary(fit):
> Concordance= 0.763  (se = 0.5 )
> Rsquare= 0.304   (max possible= 0.5 )
> Likelihood ratio test= 27.54  on 2 df,   p=1.047e-06
> Wald test            = 17.19  on 2 df,   p=0.0001853
> Score (logrank) test = 17.43  on 2 df,   p=0.0001644,   Robust = 6.66
>  p=0.03574
>
> The concordance estimate seems good but the SE is HUGE.
>
> I get a very different estimate from the survConcordance function, which I
> know says computes concordance for a "single continuous covariate", but it
> runs on my model with 2 continuous covariates....
>
> survConcordance(Surv(rep(1, 76L), resp) ~ predict(fit), dat)
> n= 76
> Concordance= 0.9106648 se= 0.09365047
> concordant  discordant   tied.risk   tied.time    std(c-d)
>  1315.0000   129.0000     0.0000   703.0000   270.4626
>
> Are both of these concordance estimates valid but providing different
> information?
> Is one more appropriate for measuring "performance" (in the AUC sense) of
> conditional logistic models?
> Is it possible that the HUGE SE estimate represents a convergence problem
> (no warnings were thrown when fit the model), or is this model just
> useless?
>
> Thanks!
> --
> Cooperative Fish and Wildlife Research Unit
> Zoology and Physiology Dept.
> University of Wyoming
> JoeCeradini at gmail.com / 914.707.8506
> wyocoopunit.org
>
>         [[alternative HTML version deleted]]
>
>
> **********************************************************
> Electronic Mail is not secure, may not be read every day, and should not
> be used for urgent or sensitive issues
>



-- 
Cooperative Fish and Wildlife Research Unit
Zoology and Physiology Dept.
University of Wyoming
JoeCeradini at gmail.com / 914.707.8506
wyocoopunit.org

	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Wed Jan 20 16:32:15 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Wed, 20 Jan 2016 16:32:15 +0100
Subject: [R] Gsarima and Method
Message-ID: <20160120153215.GA5550@localhost.localdomain>

Dear All,
While tuning some time series model with gsarima (which is primarily a
wrapper for arima) from the astsa package, I encounter the following
error message

Error in stats::arima(xdata, order = c(p, d, q), seasonal = list(order
= c(P,  :
  non-stationary seasonal AR part from CSS

According to what I found here

http://bit.ly/1QmTJtu

for a case with the arima function, the solution is to add method="ML"
to the call to arima, but it is not clear at all to me how to
introduce it in the sarima function.
Any help is appreciated
Regards

Lorenzo


From emiliano.manzo at gmail.com  Wed Jan 20 10:05:21 2016
From: emiliano.manzo at gmail.com (Emiliano Manzo)
Date: Wed, 20 Jan 2016 10:05:21 +0100
Subject: [R] adehabitat and tcltk
Message-ID: <B88C31B0-FC83-448D-B4C5-48B54B9FF861@gmail.com>

Hi there,

is anybody knows how to install adehabitat on latest release? because r said that need tcltk but this package is not available for the latest version.

Can anybody help me?


Emiliano Manzo
emiliano.manzo at gmail.com


From diego.pavonjordan at gmail.com  Wed Jan 20 11:17:11 2016
From: diego.pavonjordan at gmail.com (Diego Pavon)
Date: Wed, 20 Jan 2016 12:17:11 +0200
Subject: [R] spline.correlog (ncf package) - Error message: tol must be
 strictly positive and finite
Message-ID: <CAD93_FrbrY-10CobBQyn2ddAzVCJrV-w=8nypfJwjPt4OdsH7g@mail.gmail.com>

Dear all

I am trying to run the spline.correlogram in package ncf to check for
spatial autocorrelation in the residuals of my models, but I get an error
message:

Error in smooth.spline(u, v, df = df) :
  'tol' must be strictly positive and finite


I have googled the error and I got to the website
http://www.inside-r.org/packages/cran/ncf/docs/spline.correlog

However, I could not figure out how to solve this error. My data consists
in a time series (29 years) of counts in 92 sites in Finland. I have run
the model I wanted and then I want to use the residuals to check for
spatial autocorrelation. Hence, my code looks like:

DucksCorr <- spline.correlog(x=Ducks2[, "Lon"],
                               y=Ducks2[, "Lat"],
                               z=Ducks2[, "Resid"],
                               xmax=FALSE)

When I run this code, I get the error message. I have removed NAs (although
this can deal with it) and coordinates are correct and also the residuals.
Do you have an idea or suggestions on how to deal with this error message?

Thank you very much for your time and help!

Best

Diego

-- 
*Diego Pav?n Jord?n*

*Finnish Museum of Natural History*
*PO BOX 17 *

*Helsinki. Finland*



*0445061210https://tuhat.halvi.helsinki.fi/portal/en/persons/diego-pavon-jordan%288d5db37c-eddd-4fca-92cd-9c9956a42b4a%29.html
<https://tuhat.halvi.helsinki.fi/portal/en/persons/diego-pavon-jordan%288d5db37c-eddd-4fca-92cd-9c9956a42b4a%29.html>http://www.linkedin.com/profile/view?id=170617924&trk=nav_responsive_tab_profile
<http://www.linkedin.com/profile/view?id=170617924&trk=nav_responsive_tab_profile>https://helsinki.academia.edu/DiegoPavon
<https://helsinki.academia.edu/DiegoPavon>*

	[[alternative HTML version deleted]]


From pmoniz7 at hotmail.com  Wed Jan 20 12:27:31 2016
From: pmoniz7 at hotmail.com (Paulo Moniz)
Date: Wed, 20 Jan 2016 11:27:31 +0000
Subject: [R] =?utf-8?q?Error_using_Rhipe?=
Message-ID: <BAY407-EAS233441FFE296911B35DB60F7C20@phx.gbl>

I am integrating with R Hadoop using Rhipe and my configuration is as follows:
? Ubuntu 14.


? Hadoop 1.0.3




? R 3.2.2




? Rhipe 0.73.1



? library rJava installed



When starting at the R environment  rhinit  () the following message appears:



> rhinit ()



Rhipe: Using Rhipe.jar file 

Initializing Rhipe v0.73 

Error in .jnew("org/godhuli/rhipe/PersonalServer") :
java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream





Already researched enough in other forums and so far have not got a solution.

I count on the help of lords, I thank you.
	[[alternative HTML version deleted]]


From kendejan at yahoo.fr  Wed Jan 20 13:31:33 2016
From: kendejan at yahoo.fr (kende jan)
Date: Wed, 20 Jan 2016 12:31:33 +0000 (UTC)
Subject: [R] problem of interpretation using mediation package
References: <795995316.12706284.1453293093647.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <795995316.12706284.1453293093647.JavaMail.yahoo@mail.yahoo.com>

Dear all,
I am using the package mediation in order to perform a parametric mediation
analysis on survival data. I have 8 variables:

- Mediator
- Treat
- time (days)
- death (event)
- X1-X4 (confounding variables)

I ran the following code to estimate the causal mediation effects.

med.m = lm(Mediator ~ Treat + X1 + X2 + X3 + X4)
med.y = survreg(Surv(time, death) ~ Treat + X1 + X2 + X3 + Mediator + X4)
med.out <- mediate(med.m, med.y, treat = "Treat", mediator = "Mediator")
summary(med.out)

Here are the output provided by this script:

Causal Mediation Analysis

Quasi-Bayesian Confidence Intervals

? ? ? ? ? ? ? ? ? ? ? ? ? ?Estimate 95% CI Lower 95% CI Upper p-value
ACME (control)? ? ? ? ? ?-3.68e+02? ? -1.27e+03? ? -3.34e+01? ? 0.01
ACME (treated)? ? ? ? ? ?-1.47e+02? ? -4.46e+02? ? -1.67e+01? ? 0.01
ADE (control)? ? ? ? ? ? -3.76e+03? ? -1.18e+04? ? -5.93e+02? ? 0.00
ADE (treated)? ? ? ? ? ? -3.54e+03? ? -1.14e+04? ? -5.53e+02? ? 0.00
Total Effect? ? ? ? ? ? ?-3.91e+03? ? -1.20e+04? ? -6.79e+02? ? 0.00
Prop. Mediated (control)? 9.56e-02? ? ?1.55e-02? ? ?2.36e-01? ? 0.01
Prop. Mediated (treated)? 3.82e-02? ? ?7.03e-03? ? ?1.49e-01? ? 0.01
ACME (average)? ? ? ? ? ?-2.57e+02? ? -8.36e+02? ? -2.46e+01? ? 0.01
ADE (average)? ? ? ? ? ? -3.65e+03? ? -1.16e+04? ? -5.78e+02? ? 0.00
Prop. Mediated (average)? 6.69e-02? ? ?1.17e-02? ? ?1.90e-01? ? 0.01

Sample Size Used: 713


Simulations: 1000

My problem is that I do not understand how to interpret the value of the
estimate obtained for the ACME (control) parameter.
I know that when the response variable (Y) is binary, this estimate can be
interpreted as the increase in terms of probability of the event for control
subjects.
What is the good interpretation when the response variable (Y) in the model
is a survival object ?
Does it indicates here a decrease expressed in number of days (368) ?
According to the Prop. Mediated (average) value (i.e last row of the table),
can I conclude that about 6.69% of the total effect of Treat on Y is
explained by the indirect effect of Mediator ?

Thanks for your consideration,

Best regards,

Kendejan
	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Wed Jan 20 17:36:57 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Wed, 20 Jan 2016 16:36:57 +0000
Subject: [R] (no subject)
In-Reply-To: <55C8DD35-5C21-4C38-84A9-8A23C3AB73A7@yahoo.com>
References: <20160119203556.Horde.9TKV3BIrowvzk6LqUOk2DKY@mail.sapo.pt>
	<55C8DD35-5C21-4C38-84A9-8A23C3AB73A7@yahoo.com>
Message-ID: <20160120163657.Horde.SJziznDqE7cSjngcvSc6rAs@mail.sapo.pt>

Hello,

Please respond to the list, not just to me.
If you just want the first vector ofm the matrices, try the following.

fun <- function(..., n, replace = FALSE){
?? ?m <- do.call(rbind, list(...))
?? ?idx <- sample(nrow(m), n, replace = replace)
?? ?m[idx, 1]
}

n <- 3 * nrow(Young.list1) * 0.25
fun(Young.list1, Young.list2, Young.list3, n = n, replace = FALSE)

Hope this helps,

Rui Barradas
Citando MARYAM <firoozi_maryam6858 at yahoo.com>:

> Hello, I want to select randomly to three matrix with fixed  
> number.for example i want to select 25% from three matrix  
> randomly(select first column of each matrix). On Dey 30, 1394 AP, at  
> 0:05, ruipbarradas at sapo.pt wrote:
> ?
>> ?

Hello,

What do you want to sample? Rows? With or without replacement? You  
need to give us more information on what you want.

Start by seeing the help page for ?sample

Hope this helps,

Rui Barradas
?

Citando MARYAM <firoozi_maryam6858 at yahoo.com>:

> Dear mr/madam
> I have 3 matrix with 20 rows and 3 columns like this: I want to  
> sample randomly from three matrix and put it in a vector.How can i  
> do that?
>
> Young.list1<- matrix(NA,nrow= 20,ncol=3)
> Young.list1[,1]<- 1:20
> Young.list1[,2]<- 0.6
> Young.list1[,3]<- 500
> colnames(Young.list1)<- c("ID","r","EBV")
> ###########################################
> Young.list2<- matrix(NA,nrow= 20,ncol=3)
> Young.list2[,1]<- 21:40
> Young.list2[,2]<- 0.7
> Young.list2[,3]<- 600
> colnames(Young.list2)<- c("ID","r","EBV")
> ###########################################
> Young.list3<- matrix(NA,nrow= 20,ncol=3)
> Young.list3[,1]<- 41:60
> Young.list3[,2]<- 0.8
> Young.list3[,3]<- 700
> colnames(Young.list3)<- c("ID","r","EBV")
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland[1] provide commented,  
> minimal, self-contained, reproducible code.

?

?

Liga??es:
---------
[1] http://www.r-project.org/posting-guide.htmland

	[[alternative HTML version deleted]]


From bogaso.christofer at gmail.com  Wed Jan 20 19:22:13 2016
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Wed, 20 Jan 2016 23:52:13 +0530
Subject: [R] R editor for Mac
Message-ID: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>

Hi,

Could you please suggest a good R editor for Mac OS X (10.7.5)
Previously my operating system was Windows and there I used Notepad++,
I really had very nice experience with it. However I dont see any Mac
version is available for Mac.

Appreciate your positive feedback.

Thanks and regards,


From bgunter.4567 at gmail.com  Wed Jan 20 19:26:03 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 20 Jan 2016 10:26:03 -0800
Subject: [R] Simple syntax question (I think)
Message-ID: <CAGxFJbQ4eMhmD3cZYpnrJDMBN8ihN+xcJCZqQc-FQyi4E9_Epw@mail.gmail.com>

Could someone please explain to me my mal-understanding of the
following, which I expected to give the same results without errors.

TIA.

-- Bert

> z <-  list(x=1)
> z[[2]] <- 3
> z
$x
[1] 1

[[2]]
[1] 3

> list(x = 1)[[2]] <- 3
Error in list(x = 1)[[2]] <- 3 :
  target of assignment expands to non-language object
>
> sessionInfo()

R version 3.2.3 (2015-12-10)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.11.2 (El Capitan)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] utils    datasets tools    stats    graphics splines  grid
methods  base

other attached packages:
[1] nlme_3.1-122    lattice_0.20-33

loaded via a namespace (and not attached):
[1] grDevices_3.2.3 strucplot_0.5



Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


From rshepard at appl-ecosys.com  Wed Jan 20 19:26:29 2016
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Wed, 20 Jan 2016 10:26:29 -0800 (PST)
Subject: [R] R editor for Mac
In-Reply-To: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1601201025030.18194@localhost>

On Wed, 20 Jan 2016, Christofer Bogaso wrote:

> Could you please suggest a good R editor for Mac OS X (10.7.5) Previously
> my operating system was Windows and there I used Notepad++, I really had
> very nice experience with it. However I dont see any Mac version is
> available for Mac.

   Emacs might already be available on your system. Yes, it's complex but
worth learning. (It's been called the only editor with a built-in operating
system.) It also supports ESS which is a R mode that is very helpful.

Rich


From murdoch.duncan at gmail.com  Wed Jan 20 19:28:40 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jan 2016 13:28:40 -0500
Subject: [R] R editor for Mac
In-Reply-To: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
Message-ID: <569FD1D8.4040807@gmail.com>

On 20/01/2016 1:22 PM, Christofer Bogaso wrote:
> Hi,
>
> Could you please suggest a good R editor for Mac OS X (10.7.5)
> Previously my operating system was Windows and there I used Notepad++,
> I really had very nice experience with it. However I dont see any Mac
> version is available for Mac.
>
> Appreciate your positive feedback.

RStudio is probably best on both OS X and Windows.    A nice advantage 
is that it looks the same on both, so you can move back and forth.

I only know two negatives:

  - I still don't like the tiled window.  I often work on a small 
screen, and it's not enough space.

  - The editor still changes file endings to native format whenever it 
saves.  It would be better if it handled both Windows and Unix line 
endings in both systems, and left them alone unless the user asked them 
to be changed.

The positives are too numerous to list here.

Duncan Murdoch


From roy.mendelssohn at noaa.gov  Wed Jan 20 19:30:22 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 20 Jan 2016 10:30:22 -0800
Subject: [R] R editor for Mac
In-Reply-To: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
Message-ID: <57C8D26A-E154-4564-BCEB-4C46363B35B7@noaa.gov>

Hi:

Both the default Mac OS X installation of R  (with the GUI) as well as RStudio, have very nice editors for coding in R.

-Roy

> On Jan 20, 2016, at 10:22 AM, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Hi,
> 
> Could you please suggest a good R editor for Mac OS X (10.7.5)
> Previously my operating system was Windows and there I used Notepad++,
> I really had very nice experience with it. However I dont see any Mac
> version is available for Mac.
> 
> Appreciate your positive feedback.
> 
> Thanks and regards,
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From marc_schwartz at me.com  Wed Jan 20 19:57:23 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 20 Jan 2016 12:57:23 -0600
Subject: [R] Simple syntax question (I think)
In-Reply-To: <CAGxFJbQ4eMhmD3cZYpnrJDMBN8ihN+xcJCZqQc-FQyi4E9_Epw@mail.gmail.com>
References: <CAGxFJbQ4eMhmD3cZYpnrJDMBN8ihN+xcJCZqQc-FQyi4E9_Epw@mail.gmail.com>
Message-ID: <4461E08E-0E59-4BDF-94AD-4DA9D9BB2814@me.com>


> On Jan 20, 2016, at 12:26 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> Could someone please explain to me my mal-understanding of the
> following, which I expected to give the same results without errors.
> 
> TIA.
> 
> -- Bert
> 
>> z <-  list(x=1)
>> z[[2]] <- 3
>> z
> $x
> [1] 1
> 
> [[2]]
> [1] 3
> 
>> list(x = 1)[[2]] <- 3
> Error in list(x = 1)[[2]] <- 3 :
>  target of assignment expands to non-language object


Bert,

I will take a stab at this.

In the first case, you are adding a new element to an existing list object, so works as expected:

# Create a new list 'z'
z <-  list(x = 1)

> z
$x
[1] 1


# Now, add a new unnamed element in the list
z[[2]] <- 3

> z
$x
[1] 1

[[2]]
[1] 3


In the second case, you are attempting to subset a list that does not yet exist and assign a value to an element of a non-existent object:

> list(x = 1)[[2]]
Error in list(x = 1)[[2]] : subscript out of bounds

> list(x = 1)[[2]] <- 3
Error in list(x = 1)[[2]] <- 3 : 
  target of assignment expands to non-language object


If this was to work, the parser would have to evaluate the command in a left to right fashion, first creating the list with an element 'x' and then adding the new element to it as a second step, much as you did explicitly in the first approach.

You get somewhat similar behavior with a vector, albeit the error is perhaps a bit more clear:

> Vec
Error: object 'Vec' not found

> Vec[2] <- 3
Error in Vec[2] <- 3 : object 'Vec' not found

Vec <- 1

> Vec
[1] 1

Vec[2] <- 2

> Vec
[1] 1 2


Regards,

Marc 


From bgunter.4567 at gmail.com  Wed Jan 20 20:21:38 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 20 Jan 2016 11:21:38 -0800
Subject: [R] Simple syntax question (I think)
In-Reply-To: <4461E08E-0E59-4BDF-94AD-4DA9D9BB2814@me.com>
References: <CAGxFJbQ4eMhmD3cZYpnrJDMBN8ihN+xcJCZqQc-FQyi4E9_Epw@mail.gmail.com>
	<4461E08E-0E59-4BDF-94AD-4DA9D9BB2814@me.com>
Message-ID: <CAGxFJbQiHa-1+gpSvK_g-v28dErq715j=YyJSx_G6tngveRX6g@mail.gmail.com>

Thanks Marc.

Actually, I think the cognate construction for a vector (which is what
a list is also) is:

> vector("numeric",2)[2] <-  3
Error in vector("numeric", 2)[2] <- 3 :
  target of assignment expands to non-language object

but this works:

> "[<-"(vector("numeric",2),2,3)
[1] 0 3

I would have thought the 2 versions should be identical, but as you
allude, there are apparently subtleties in the parsing/evaluation that
I do not understand, so that the explicit functional form is parsed
and evaluated differently than the implicit one. The obvious message,
though, is: don't do this!

I suspect there is a reference to this somewhere in the R Language
definition  or elsewhere, and if so, I would appreciate someone
referring me to it -- RTFM certainly applies!

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Jan 20, 2016 at 10:57 AM, Marc Schwartz <marc_schwartz at me.com> wrote:
>
>> On Jan 20, 2016, at 12:26 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> Could someone please explain to me my mal-understanding of the
>> following, which I expected to give the same results without errors.
>>
>> TIA.
>>
>> -- Bert
>>
>>> z <-  list(x=1)
>>> z[[2]] <- 3
>>> z
>> $x
>> [1] 1
>>
>> [[2]]
>> [1] 3
>>
>>> list(x = 1)[[2]] <- 3
>> Error in list(x = 1)[[2]] <- 3 :
>>  target of assignment expands to non-language object
>
>
> Bert,
>
> I will take a stab at this.
>
> In the first case, you are adding a new element to an existing list object, so works as expected:
>
> # Create a new list 'z'
> z <-  list(x = 1)
>
>> z
> $x
> [1] 1
>
>
> # Now, add a new unnamed element in the list
> z[[2]] <- 3
>
>> z
> $x
> [1] 1
>
> [[2]]
> [1] 3
>
>
> In the second case, you are attempting to subset a list that does not yet exist and assign a value to an element of a non-existent object:
>
>> list(x = 1)[[2]]
> Error in list(x = 1)[[2]] : subscript out of bounds
>
>> list(x = 1)[[2]] <- 3
> Error in list(x = 1)[[2]] <- 3 :
>   target of assignment expands to non-language object
>
>
> If this was to work, the parser would have to evaluate the command in a left to right fashion, first creating the list with an element 'x' and then adding the new element to it as a second step, much as you did explicitly in the first approach.
>
> You get somewhat similar behavior with a vector, albeit the error is perhaps a bit more clear:
>
>> Vec
> Error: object 'Vec' not found
>
>> Vec[2] <- 3
> Error in Vec[2] <- 3 : object 'Vec' not found
>
> Vec <- 1
>
>> Vec
> [1] 1
>
> Vec[2] <- 2
>
>> Vec
> [1] 1 2
>
>
> Regards,
>
> Marc
>


From bretschr at xs4all.nl  Wed Jan 20 20:22:36 2016
From: bretschr at xs4all.nl (Franklin Bretschneider)
Date: Wed, 20 Jan 2016 20:22:36 +0100
Subject: [R] R editor for Mac
In-Reply-To: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
Message-ID: <9B91D216-0AC2-43E3-8070-74B42643E74B@xs4all.nl>

Dear Christofer Bogaso,


Re:


> Could you please suggest a good R editor for Mac OS X (10.7.5)



Indeed, as Roy Mendelssohn wrote, the editor built into "R.app", the GUI program which is part of the standard R for OS X, has a beautiful editor, complete with syntax colouring and bracket balancing. And one can run only one or a few lines from a script at wish.
I couldn't wish myself more.

Success and best wishes,

Frank
--





Franklin Bretschneider
Dept of Biology
Utrecht University
bretschr at xs4all.nl


From bob at rudis.net  Wed Jan 20 20:28:36 2016
From: bob at rudis.net (boB Rudis)
Date: Wed, 20 Jan 2016 14:28:36 -0500
Subject: [R] R editor for Mac
In-Reply-To: <9B91D216-0AC2-43E3-8070-74B42643E74B@xs4all.nl>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<9B91D216-0AC2-43E3-8070-74B42643E74B@xs4all.nl>
Message-ID: <CAJ4QxaNFcSJ6Wfx-Lei-Swwe5cC6Amuz6rurQPqXiLNqvL3fEA@mail.gmail.com>

If you don't want to run RStudio, Sublime Text has both great R code
syntax highlighting/formatting and a REPL mode for an interactive
console in-editor.

Atom also has decent R support.

They both play well with "Dash" which is an alternative way (separate
app) to lookup R docs on OS X.

On Wed, Jan 20, 2016 at 2:22 PM, Franklin Bretschneider
<bretschr at xs4all.nl> wrote:
> Dear Christofer Bogaso,
>
>
> Re:
>
>
>> Could you please suggest a good R editor for Mac OS X (10.7.5)
>
>
>
> Indeed, as Roy Mendelssohn wrote, the editor built into "R.app", the GUI program which is part of the standard R for OS X, has a beautiful editor, complete with syntax colouring and bracket balancing. And one can run only one or a few lines from a script at wish.
> I couldn't wish myself more.
>
> Success and best wishes,
>
> Frank
> --
>
>
>
>
>
> Franklin Bretschneider
> Dept of Biology
> Utrecht University
> bretschr at xs4all.nl
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Wed Jan 20 20:51:59 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jan 2016 14:51:59 -0500
Subject: [R] Simple syntax question (I think)
In-Reply-To: <CAGxFJbQiHa-1+gpSvK_g-v28dErq715j=YyJSx_G6tngveRX6g@mail.gmail.com>
References: <CAGxFJbQ4eMhmD3cZYpnrJDMBN8ihN+xcJCZqQc-FQyi4E9_Epw@mail.gmail.com>
	<4461E08E-0E59-4BDF-94AD-4DA9D9BB2814@me.com>
	<CAGxFJbQiHa-1+gpSvK_g-v28dErq715j=YyJSx_G6tngveRX6g@mail.gmail.com>
Message-ID: <569FE55F.7020205@gmail.com>

On 20/01/2016 2:21 PM, Bert Gunter wrote:
> Thanks Marc.
>
> Actually, I think the cognate construction for a vector (which is what
> a list is also) is:
>
> > vector("numeric",2)[2] <-  3
> Error in vector("numeric", 2)[2] <- 3 :
>    target of assignment expands to non-language object
>
> but this works:
>
> > "[<-"(vector("numeric",2),2,3)
> [1] 0 3
>
> I would have thought the 2 versions should be identical, but as you
> allude, there are apparently subtleties in the parsing/evaluation that
> I do not understand, so that the explicit functional form is parsed
> and evaluated differently than the implicit one. The obvious message,
> though, is: don't do this!
>
> I suspect there is a reference to this somewhere in the R Language
> definition  or elsewhere, and if so, I would appreciate someone
> referring me to it -- RTFM certainly applies!

There's a detailed discussion in the Language Reference.  Search for 
"complex assignment", or look in section 3.4.4 Subset assignment. The 
big difference between the two expressions you give is in the final 
assignment of the result of the "[<-" function to the referenced 
variable.  In your first case there's no referenced variable, just an 
expression vector("numeric",2), so you get the error, just as you would with

vector("numeric",2) <- c(0, 3)

Duncan Murdoch


From wdunlap at tibco.com  Wed Jan 20 20:55:42 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 20 Jan 2016 11:55:42 -0800
Subject: [R] Simple syntax question (I think)
In-Reply-To: <CAGxFJbQiHa-1+gpSvK_g-v28dErq715j=YyJSx_G6tngveRX6g@mail.gmail.com>
References: <CAGxFJbQ4eMhmD3cZYpnrJDMBN8ihN+xcJCZqQc-FQyi4E9_Epw@mail.gmail.com>
	<4461E08E-0E59-4BDF-94AD-4DA9D9BB2814@me.com>
	<CAGxFJbQiHa-1+gpSvK_g-v28dErq715j=YyJSx_G6tngveRX6g@mail.gmail.com>
Message-ID: <CAF8bMcbO+nFenWSAADw7k11gYepHPBpBLrbG9qQvOEo2Oty1oQ@mail.gmail.com>

Note that the expression
   x[1] <- 10
is equivalent not to
   `[<-`(x, 1, value=10)
but to
   x <- `[<-`(x, 1, value=10)
so there is no conflict between your two expressions.

Saying
   c(1,2,3) <- `[<-`(c(1,2,3), 1, value=10)
is not allowed because there is no name to assign something to.

There are a few cases where an expression without a name on
the left side would make sense, as in
   environment()[["x"]] <- 12
instead of
   thisEnvir <- environment()
   thisEnvir[["x"]] <- 12
but that is not allowed (in the interests of having consistent rules).



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Jan 20, 2016 at 11:21 AM, Bert Gunter <bgunter.4567 at gmail.com>
wrote:

> Thanks Marc.
>
> Actually, I think the cognate construction for a vector (which is what
> a list is also) is:
>
> > vector("numeric",2)[2] <-  3
> Error in vector("numeric", 2)[2] <- 3 :
>   target of assignment expands to non-language object
>
> but this works:
>
> > "[<-"(vector("numeric",2),2,3)
> [1] 0 3
>
> I would have thought the 2 versions should be identical, but as you
> allude, there are apparently subtleties in the parsing/evaluation that
> I do not understand, so that the explicit functional form is parsed
> and evaluated differently than the implicit one. The obvious message,
> though, is: don't do this!
>
> I suspect there is a reference to this somewhere in the R Language
> definition  or elsewhere, and if so, I would appreciate someone
> referring me to it -- RTFM certainly applies!
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Jan 20, 2016 at 10:57 AM, Marc Schwartz <marc_schwartz at me.com>
> wrote:
> >
> >> On Jan 20, 2016, at 12:26 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >>
> >> Could someone please explain to me my mal-understanding of the
> >> following, which I expected to give the same results without errors.
> >>
> >> TIA.
> >>
> >> -- Bert
> >>
> >>> z <-  list(x=1)
> >>> z[[2]] <- 3
> >>> z
> >> $x
> >> [1] 1
> >>
> >> [[2]]
> >> [1] 3
> >>
> >>> list(x = 1)[[2]] <- 3
> >> Error in list(x = 1)[[2]] <- 3 :
> >>  target of assignment expands to non-language object
> >
> >
> > Bert,
> >
> > I will take a stab at this.
> >
> > In the first case, you are adding a new element to an existing list
> object, so works as expected:
> >
> > # Create a new list 'z'
> > z <-  list(x = 1)
> >
> >> z
> > $x
> > [1] 1
> >
> >
> > # Now, add a new unnamed element in the list
> > z[[2]] <- 3
> >
> >> z
> > $x
> > [1] 1
> >
> > [[2]]
> > [1] 3
> >
> >
> > In the second case, you are attempting to subset a list that does not
> yet exist and assign a value to an element of a non-existent object:
> >
> >> list(x = 1)[[2]]
> > Error in list(x = 1)[[2]] : subscript out of bounds
> >
> >> list(x = 1)[[2]] <- 3
> > Error in list(x = 1)[[2]] <- 3 :
> >   target of assignment expands to non-language object
> >
> >
> > If this was to work, the parser would have to evaluate the command in a
> left to right fashion, first creating the list with an element 'x' and then
> adding the new element to it as a second step, much as you did explicitly
> in the first approach.
> >
> > You get somewhat similar behavior with a vector, albeit the error is
> perhaps a bit more clear:
> >
> >> Vec
> > Error: object 'Vec' not found
> >
> >> Vec[2] <- 3
> > Error in Vec[2] <- 3 : object 'Vec' not found
> >
> > Vec <- 1
> >
> >> Vec
> > [1] 1
> >
> > Vec[2] <- 2
> >
> >> Vec
> > [1] 1 2
> >
> >
> > Regards,
> >
> > Marc
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Wed Jan 20 20:59:08 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jan 2016 14:59:08 -0500
Subject: [R] R editor for Mac
In-Reply-To: <9B91D216-0AC2-43E3-8070-74B42643E74B@xs4all.nl>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<9B91D216-0AC2-43E3-8070-74B42643E74B@xs4all.nl>
Message-ID: <569FE70C.9060308@gmail.com>

On 20/01/2016 2:22 PM, Franklin Bretschneider wrote:
> Dear Christofer Bogaso,
>
>
> Re:
>
>
> > Could you please suggest a good R editor for Mac OS X (10.7.5)
>
>
>
> Indeed, as Roy Mendelssohn wrote, the editor built into "R.app", the GUI program which is part of the standard R for OS X, has a beautiful editor, complete with syntax colouring and bracket balancing. And one can run only one or a few lines from a script at wish.
> I couldn't wish myself more.

R.app looks nicer than RStudio (no tiling), but it is missing a lot of 
functionality.

  - the debugger
  - the integration with other tools provided by RStudio, like Shiny, 
RMarkdown, htmlwidgets, package building tools, etc.
  - the integration with source code management.
  - session management (i.e. it's easier to shut down and restart R, 
which you frequently need to do when developing and testing packages)
  - syntax checking hints in the editor (not just for R, for some other 
languages too).

I think Emacs + ESS matches (exceeds if you count non-R stuff) RStudio 
in functionality, but it is much harder to learn.

Duncan Murdoch


From jafarikia at gmail.com  Wed Jan 20 21:37:15 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Wed, 20 Jan 2016 15:37:15 -0500
Subject: [R] R plot-par(mfrow=c(x,y)) on multiple pages
Message-ID: <CADs3iXnzNJ2F=w=yxa23FL7D9FezfZd_RhDGugVeHEWLdYR0gQ@mail.gmail.com>

Hello everyone:

I have 12 plots that I am using par(mfrow=c(3,2)) to have 6 of them in
a single page. R only prints the 6 first graph in a single page. Any
comments how I can ask R to print all 12 graphs in two pages.

Thanks very much,
Mohsen


From bhh at xs4all.nl  Wed Jan 20 21:39:59 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 20 Jan 2016 21:39:59 +0100
Subject: [R] R editor for Mac
In-Reply-To: <9B91D216-0AC2-43E3-8070-74B42643E74B@xs4all.nl>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<9B91D216-0AC2-43E3-8070-74B42643E74B@xs4all.nl>
Message-ID: <2206F804-AEFB-4162-8608-7BA749FFCD7F@xs4all.nl>


> On 20 Jan 2016, at 20:22, Franklin Bretschneider <bretschr at xs4all.nl> wrote:
> 
> Dear Christofer Bogaso,
> 
> 
> Re:
> 
> 
>> Could you please suggest a good R editor for Mac OS X (10.7.5)
> 
> 
> 
> Indeed, as Roy Mendelssohn wrote, the editor built into "R.app", the GUI program which is part of the standard R for OS X, has a beautiful editor, complete with syntax colouring and bracket balancing. And one can run only one or a few lines from a script at wish.
> I couldn't wish myself more.
> 

Another nice editor for R is Textmate.

Berend


From bgunter.4567 at gmail.com  Wed Jan 20 21:40:16 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 20 Jan 2016 12:40:16 -0800
Subject: [R] Simple syntax question (I think)
In-Reply-To: <569FE55F.7020205@gmail.com>
References: <CAGxFJbQ4eMhmD3cZYpnrJDMBN8ihN+xcJCZqQc-FQyi4E9_Epw@mail.gmail.com>
	<4461E08E-0E59-4BDF-94AD-4DA9D9BB2814@me.com>
	<CAGxFJbQiHa-1+gpSvK_g-v28dErq715j=YyJSx_G6tngveRX6g@mail.gmail.com>
	<569FE55F.7020205@gmail.com>
Message-ID: <CAGxFJbS2qG+rODUCNCWjSwH_OY8_=YAoUAqgaP=KXm0bML0BVQ@mail.gmail.com>

Thanks to both Bill and Duncan for their help. As I said, my
mal-understanding of the syntax.

Best,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Jan 20, 2016 at 11:51 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 20/01/2016 2:21 PM, Bert Gunter wrote:
>>
>> Thanks Marc.
>>
>> Actually, I think the cognate construction for a vector (which is what
>> a list is also) is:
>>
>> > vector("numeric",2)[2] <-  3
>> Error in vector("numeric", 2)[2] <- 3 :
>>    target of assignment expands to non-language object
>>
>> but this works:
>>
>> > "[<-"(vector("numeric",2),2,3)
>> [1] 0 3
>>
>> I would have thought the 2 versions should be identical, but as you
>> allude, there are apparently subtleties in the parsing/evaluation that
>> I do not understand, so that the explicit functional form is parsed
>> and evaluated differently than the implicit one. The obvious message,
>> though, is: don't do this!
>>
>> I suspect there is a reference to this somewhere in the R Language
>> definition  or elsewhere, and if so, I would appreciate someone
>> referring me to it -- RTFM certainly applies!
>
>
> There's a detailed discussion in the Language Reference.  Search for
> "complex assignment", or look in section 3.4.4 Subset assignment. The big
> difference between the two expressions you give is in the final assignment
> of the result of the "[<-" function to the referenced variable.  In your
> first case there's no referenced variable, just an expression
> vector("numeric",2), so you get the error, just as you would with
>
> vector("numeric",2) <- c(0, 3)
>
> Duncan Murdoch


From zilefacelvis at yahoo.com  Wed Jan 20 21:53:26 2016
From: zilefacelvis at yahoo.com (Zilefac Elvis)
Date: Wed, 20 Jan 2016 20:53:26 +0000 (UTC)
Subject: [R] Splitting strings in data files R
References: <1545301296.8308291.1453323206724.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1545301296.8308291.1453323206724.JavaMail.yahoo@mail.yahoo.com>




Please I need help processing  files with strings in R. All the files have two patterns (thus,examine separately):
Pattern 1 (see file1 below): Delete Lines 1,2 & 4 in file1. Line 3 contains the column names. Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with with NA.

File1 output format should be: Year Month Day_1 Day_2 ... Day_31  ## so all months should 31 days. Months with <31 days should have NA where appropraite (e.g. Feb 30=NA, 31=NA)

Pattern 2 (see file2 below): Delete Line 1 in file2.Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with withNA. File2 has no column names. Please do not include any. 
File2 output format: Year Month Day_1 Day_2 ... Day_31 but no column names

Here is a simple reproducible example for both files/cases: 


file1=list(df1,df1)df1=list(structure(list(X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014 =structure(c(20L,19L,21L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,16L,17L,18L),.Label =c(" 1918  7 -9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M  23.6a  25.9a  25.8a  24.9a  24.9a  29.6a  27.4a  24.5a  28.5a  28.5a  30.1a  25.3a  28.5a  19.6a  24.1a"," 1918  8   23.7a  18.6a  17.6a  19.0a  23.7a  24.7a  18.6a  22.6a  20.1a  21.4a  22.6a  24.9a  24.1a  23.2a  22.0a  17.6a  19.0a  19.0a  23.7a  24.1a  24.9a  27.9a  26.2a  22.6a  24.0a  25.4a  21.4a  24.4a  19.0a  22.6a  23.7a"," 1918  9   22.0a  22.0a  24.0a  19.0a  14.4a  11.2a  17.1a  18.1a  19.0a  12.0a  13.5a   9.6a  11.2a  10.7a  18.1a  18.1a  16.3a  14.4a  14.3a  15.9a  10.1a   9.8a  11.3a  11.4a  13.6a  14.4a   9.3a   9.6a   9.2a   8.4a-9999.9M"," 1918 10    9.3a   9.5a  11.3a  10.2a   9.9a-9999.9M   4.6a-9999.9M   9.8a  13.6a  17.0a  15.2a  15.1a  15.9a   8.1a   9.3a   8.8a   6.0a   8.7a   9.8a   9.8a  10.7a  11.3a   9.5a  10.7a-9999.9M  10.7a  16.9a  17.1a  10.7a  12.7a"," 1918 11    8.8a-9999.9M   4.0a   3.4a   4.0a   6.6a   4.0a   7.3a   8.1a   7.3a   2.5a   3.4a   7.7a   6.1a   2.2a   4.0a   4.6a   2.5a   2.2a   1.6a   2.2a   3.0a  -3.4a   2.5a   1.6a  -3.4a   2.1a   0.0a   2.6a   0.6a-9999.9M"," 1918 12  -10.1a  -8.3a  -6.3a  -5.5a  -5.1a  -7.2a-9999.9M  -3.4a  -2.2a  -5.5a  -6.0a  -3.4a   0.6a   3.0a   4.7a   0.6a  -5.0a  -6.4a  -5.9a  -2.2a   1.2a   4.0a   5.3a-9999.9M  -2.2a  -5.5a  -7.5a  -9.6a  -7.3a  -6.6a-9999.9M"," 1919  1    2.5a   0.0a  -7.3a  -6.7a  -6.6a  -9.2a  -5.9a  -0.7a  -2.9a  -13.2a  -8.0a  -17.1a  -7.4a  -4.0a  -5.5a   0.6a  -7.1a  -5.5a  -2.2a  -7.6a  -7.0a  -3.4a  -2.2a  -6.7a  -8.0a  -2.9a  -1.5a  -5.9a  -5.5a  -5.8a  -3.4a"," 1919  2 -9999.9M   0.0a  -4.0a  -3.4a  -1.5a  -2.1a  -3.4a  -7.2a  -2.8a  -5.5a  -6.7a  -5.1a  -2.1a  -2.1a   1.2a  -2.1a  -5.9a  -2.8a  -4.5a  -4.5a  -3.4a   2.1a   0.0a   0.0a   1.2a  -2.1a  -8.3a  -6.6a-9999.9M-9999.9M-9999.9M"," 1919  3 -9999.9M   0.0a   1.6a   1.7a  -5.1a  -6.7a  -5.1a  -3.4a  -2.0a   1.2a   3.4a   1.2a  -8.3a  -7.9a  -3.4a  -2.0a   1.2a   3.4a   6.6a   1.2a   6.6a   1.2a   6.6a   6.6a   3.4a  10.7a   6.6a   6.6a   0.6a   0.6a  -6.8a"," 1919  4   -5.9a  -3.4a   2.1a   3.4a   3.0a   3.0a   8.1a   8.1a   6.0a   2.1a   6.6a   8.5a   6.0a   1.7a   4.7a   2.4a   2.1a   8.5a   1.2a   1.7a   9.6a   8.6a  12.8a   9.5a  -2.8a   9.8a   4.7a  10.7a   6.6a  11.2a-9999.9M"," 1919  5   16.4a   8.5a   9.4a   6.0a  10.7a   9.8a   8.5a  13.6a  14.4a-9999.9M  16.4a  19.0a  23.2a  16.9a  17.0a  19.6a  11.3a   9.4a  12.1a  17.2a  15.2a  17.0a  15.2a  17.5a  10.2a  22.6a  14.5a  22.0a  24.9a  23.8a  19.0a"," 1919  6   17.7a  25.4a  31.2a  25.3a  26.8a  22.0a  15.8a  19.0a  12.7a  19.6a  19.0a  24.5a  25.1a  27.4a  26.8a  19.0a  20.8a  26.8a  27.9a  25.8a  20.1a  17.7a  19.0a  32.4a  30.7a  22.6a  19.0a  13.6a  17.5a  24.1a-9999.9M"," 1919  7   23.7a  24.4a  27.9a  29.6a  23.7a  21.3a  23.7a  20.1a  23.7a  21.3a  17.0a  17.8a  23.7a  27.4a  18.2a  23.2a  24.5a  26.2a  25.8a  27.9a  29.0a  25.3a  25.1a  23.9a  22.6a  23.9a  20.8a  25.8a  20.1a  23.2a  23.7a"," 1919  8   20.8a  18.2a  20.1a  20.1a  25.1a  20.8a  24.6a  18.5a  17.6a  22.0a  24.0a  23.2a  24.0a  24.0a  20.8a  24.0a  23.7a  23.8a  17.1a  23.8a  24.6a  23.8a  19.6a  24.0a  24.0a  16.9a  18.2a  18.6a  18.6a  23.2a  20.8a"," 1919  9   24.0a  21.3a  24.4a  18.1a  19.0a  19.0a  17.7a  11.4a  10.7a  12.7a  15.2a  15.2a  18.6a  12.7a  15.2a  10.1a  12.0a  12.7a  19.6a  18.5a  28.5a  28.5a  10.7a  14.5a  15.8a  11.3a  11.3a  20.8a  23.2a  11.3a-9999.9M"," 1919 10   11.3a   8.2a   8.2a  16.4a  10.7a  17.5a   7.7a   6.0a  11.3a   7.3a  12.1a   7.7a  10.2a  15.9a  18.2a   9.0a  10.7a   9.8a   8.2a   7.3a   7.7a   8.9a   9.5a  12.1a  10.2a  10.2a   4.0a  10.7a   2.9a   5.3a   3.0a"," 1919 11    8.2a   2.2a   1.2a   2.6a   1.7a   2.6a   6.1a   8.2a   7.7a   5.3a   4.7a   8.9a   4.7a   1.7a  -4.0a   2.2a   7.7a   7.7a   0.6a  -4.5a   2.6a   3.4a   2.5a  -3.4a  -5.9a  -5.1a  -5.5a  -6.4a   8.9a   3.0a-9999.9M"," 1919 12   -4.0a  -9.2a  -10.5a  -5.1a  -4.5a  -6.9a  -4.0a  -4.0a   3.0a   2.2a  -9.2a  -3.4a   5.3a  -6.4a  -6.9a  -20.4a  -20.4a  -17.6a  -10.5a  -13.8a  -8.7a  -3.4a  -2.9a  -4.5a  -5.5a  -5.5a  -2.9a  -0.8a  -10.1a  -6.9a  -5.9a"," Year Mo  Day 01  Day 02  Day 03  Day 04  Day 05  Day 06  Day 07  Day 08  Day 09  Day 10  Day 11  Day 12  Day 13  Day 14  Day 15  Day 16  Day 17  Day 18  Day 19  Day 20  Day 21  Day 22  Day 23  Day 24  Day 25  Day 26  Day 27  Day 28  Day 29  Day 30  Day 31","7011982,   DONNACONA    , QC, station jointe   , Temperature quotidienne maximale homogeneisee, Deg Celcius, Mise a jour jusqu a decembre 2014","Annee Mo Jour 01 Jour 02 Jour 03 Jour 04 Jour 05 Jour 06 Jour 07 Jour 08 Jour 09 Jour 10 Jour 11 Jour 12 Jour 13 Jour 14 Jour 15 Jour 16 Jour 17 Jour 18 Jour 19 Jour 20 Jour 21 Jour 22 Jour 23 Jour 24 Jour 25 Jour 26 Jour 27 Jour 28 Jour 29 Jour 30 Jour 31"),class ="factor")),.Names ="X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014",class ="data.frame",row.names =c(NA,-21L)))




file2=list(df2,df2)df2=list(structure(list(X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014 =structure(1:24,.Label =c("1948  1 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  2 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  3 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  4 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  5 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M   0.00    0.00    0.21T   0.21T   1.69    0.21T   0.21T   0.21T   0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    1.39    0.00    0.21T","1948  6    0.00    0.00    0.30T   3.34T   0.21T   0.00    0.00    7.19T   0.21T   1.04    0.00    4.29    1.69    0.21T   0.00    0.00    0.21T   0.65    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.21T-9999.99M","1948  7    0.21T   0.51T   0.00    2.74    0.00    0.00    0.00    0.00    0.00    1.05    0.00    0.00    1.57    1.57    2.30    0.74T   0.00    0.30T   0.74    0.53    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.53    3.34    2.30   13.43 ","1948  8    0.30T   2.61    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.53    0.21T   0.65    3.65    3.25    0.21T   3.90    0.21T   0.21T   0.21T   0.30T   0.21T   0.21T   0.21T   0.00    0.21T   0.65    1.95    0.21T   0.21T","1948  9    0.00    0.21T   0.21T   0.21T   0.21T   0.69T   7.54    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   1.04    0.00    0.00    0.00    0.00    7.28    4.68    2.34    1.95    3.90    1.30    0.21T-9999.99M","1948 10    1.04    0.00    0.00    1.69    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.65    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T","1948 11    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   1.04    0.21T   0.00    0.00    1.69    0.21T   0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1948 12    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 ","1949  1    0.00    0.00    0.00    0.00    0.00    0.39    0.65    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.65    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.00 ","1949  2    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00 -9999.99M-9999.99M-9999.99M","1949  3    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    1.69    0.00    1.04    1.69    0.65    0.21T   0.51T   0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.21T   0.21T   0.00 ","1949  4    0.00    0.00    0.39    0.21T   0.00    0.00    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.65    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1949  5    0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.39    0.39    0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.39    0.39    0.21T   1.04 ","1949  6    0.39    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.65    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.51T   0.21T   0.00    0.39    0.21T   0.00    0.21T   0.21T   0.39    0.39    0.21T   0.00    0.00    0.00    0.00 -9999.99M","1949  7    0.00    0.00    0.53    0.51T   0.51T   0.21T   0.51T   0.30T   0.00    0.00    0.00    0.00    0.00    0.00    0.30T   0.30T   0.00    0.00    0.00    0.00    0.51T   0.51T   6.25   22.63    0.00    0.51T   0.21T   0.30T   0.30T   0.00    0.00 ","1949  8    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.51T   0.51T   0.00    0.30T   0.30T   0.00    0.00    6.56    0.00    0.00    0.00    0.00    1.05    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.51T   0.00    0.30T   0.21T   0.21T","1949  9    0.30T   0.30T   0.00    0.39    0.39    0.21T   0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T-9999.99M","1949 10    0.21T   0.00    0.00    0.00    1.04    0.39    0.65    0.21T   0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.65    0.65    0.21T   0.65    0.00    0.00    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.00    0.00    0.00    0.00 ","1949 11    0.00    0.00    0.00    0.00    1.04    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T-9999.99M","1949 12    0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.21T   0.39    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.21T   0.00    0.00 "),class ="factor")),.Names ="X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014",class ="data.frame",row.names =c(NA,-24L)))


From sarah.goslee at gmail.com  Wed Jan 20 22:27:12 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 20 Jan 2016 16:27:12 -0500
Subject: [R] R plot-par(mfrow=c(x,y)) on multiple pages
In-Reply-To: <CADs3iXnzNJ2F=w=yxa23FL7D9FezfZd_RhDGugVeHEWLdYR0gQ@mail.gmail.com>
References: <CADs3iXnzNJ2F=w=yxa23FL7D9FezfZd_RhDGugVeHEWLdYR0gQ@mail.gmail.com>
Message-ID: <CAM_vju=QRNz=_ep2BxeMB9PwbL6fUG0pTZiXabB5yUqVbTpF5Q@mail.gmail.com>

Use a device like pdf() or postscript() that supports multiple pages.
Or start a new default device with dev.new() so you can see two
figures simultaneously.

Sarah

On Wed, Jan 20, 2016 at 3:37 PM, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
> Hello everyone:
>
> I have 12 plots that I am using par(mfrow=c(3,2)) to have 6 of them in
> a single page. R only prints the 6 first graph in a single page. Any
> comments how I can ask R to print all 12 graphs in two pages.
>
> Thanks very much,
> Mohsen
>


From msharp at txbiomed.org  Wed Jan 20 22:31:59 2016
From: msharp at txbiomed.org (Mark Sharp)
Date: Wed, 20 Jan 2016 21:31:59 +0000
Subject: [R] Splitting strings in data files R
In-Reply-To: <1545301296.8308291.1453323206724.JavaMail.yahoo@mail.yahoo.com>
References: <1545301296.8308291.1453323206724.JavaMail.yahoo.ref@mail.yahoo.com>
	<1545301296.8308291.1453323206724.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <7872039F-CD91-48B0-8D43-450D49F25567@TxBiomed.org>

Looks like homework.


R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org







> On Jan 20, 2016, at 2:53 PM, R. Help <r-help at r-project.org> wrote:
>
>
>
>
> Please I need help processing  files with strings in R. All the files have two patterns (thus,examine separately):
> Pattern 1 (see file1 below): Delete Lines 1,2 & 4 in file1. Line 3 contains the column names. Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with with NA.
>
> File1 output format should be: Year Month Day_1 Day_2 ... Day_31  ## so all months should 31 days. Months with <31 days should have NA where appropraite (e.g. Feb 30=NA, 31=NA)
>
> Pattern 2 (see file2 below): Delete Line 1 in file2.Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with withNA. File2 has no column names. Please do not include any.
> File2 output format: Year Month Day_1 Day_2 ... Day_31 but no column names
>
> Here is a simple reproducible example for both files/cases:
>
>
> file1=list(df1,df1)df1=list(structure(list(X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014 =structure(c(20L,19L,21L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,16L,17L,18L),.Label =c(" 1918  7 -9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M  23.6a  25.9a  25.8a  24.9a  24.9a  29.6a  27.4a  24.5a  28.5a  28.5a  30.1a  25.3a  28.5a  19.6a  24.1a"," 1918  8   23.7a  18.6a  17.6a  19.0a  23.7a  24.7a  18.6a  22.6a  20.1a  21.4a  22.6a  24.9a  24.1a  23.2a  22.0a  17.6a  19.0a  19.0a  23.7a  24.1a  24.9a  27.9a  26.2a  22.6a  24.0a  25.4a  21.4a  24.4a  19.0a  22.6a  23.7a"," 1918  9   22.0a  22.0a  24.0a  19.0a  14.4a  11.2a  17.1a  18.1a  19.0a  12.0a  13.5a   9.6a  11.2a  10.7a  18.1a  18.1a  16.3a  14.4a  14.3a  15.9a  10.1a   9.8a  11.3a  11.4a  13.6a  14.4a   9.3a   9.6a   9.2a   8.4a-9999!
> .9M"," 1918 10    9.3a   9.5a  11.3a  10.2a   9.9a-9999.9M   4.6a-9999.9M   9.8a  13.6a  17.0a  15.2a  15.1a  15.9a   8.1a   9.3a   8.8a   6.0a   8.7a   9.8a   9.8a  10.7a  11.3a   9.5a  10.7a-9999.9M  10.7a  16.9a  17.1a  10.7a  12.7a"," 1918 11    8.8a-9999.9M   4.0a   3.4a   4.0a   6.6a   4.0a   7.3a   8.1a   7.3a   2.5a   3.4a   7.7a   6.1a   2.2a   4.0a   4.6a   2.5a   2.2a   1.6a   2.2a   3.0a  -3.4a   2.5a   1.6a  -3.4a   2.1a   0.0a   2.6a   0.6a-9999.9M"," 1918 12  -10.1a  -8.3a  -6.3a  -5.5a  -5.1a  -7.2a-9999.9M  -3.4a  -2.2a  -5.5a  -6.0a  -3.4a   0.6a   3.0a   4.7a   0.6a  -5.0a  -6.4a  -5.9a  -2.2a   1.2a   4.0a   5.3a-9999.9M  -2.2a  -5.5a  -7.5a  -9.6a  -7.3a  -6.6a-9999.9M"," 1919  1    2.5a   0.0a  -7.3a  -6.7a  -6.6a  -9.2a  -5.9a  -0.7a  -2.9a  -13.2a  -8.0a  -17.1a  -7.4a  -4.0a  -5.5a   0.6a  -7.1a  -5.5a  -2.2a  -7.6a  -7.0a  -3.4a  -2.2a  -6.7a  -8.0a  -2.9a  -1.5a  -5.9a  -5.5a  -5.8a  -3.4a"," 1919  2 -9999.9M   0.0a  -4.0a  -3.4a  -1.5a  -2.1a  -3!
> .4a  -7.2a  -2.8a  -5.5a  -6.7a  -5.1a  -2.1a  -2.1a   1.2a  -2.1a  -5
> .9a  -2.8a  -4.5a  -4.5a  -3.4a   2.1a   0.0a   0.0a   1.2a  -2.1a  -8.3a  -6.6a-9999.9M-9999.9M-9999.9M"," 1919  3 -9999.9M   0.0a   1.6a   1.7a  -5.1a  -6.7a  -5.1a  -3.4a  -2.0a   1.2a   3.4a   1.2a  -8.3a  -7.9a  -3.4a  -2.0a   1.2a   3.4a   6.6a   1.2a   6.6a   1.2a   6.6a   6.6a   3.4a  10.7a   6.6a   6.6a   0.6a   0.6a  -6.8a"," 1919  4   -5.9a  -3.4a   2.1a   3.4a   3.0a   3.0a   8.1a   8.1a   6.0a   2.1a   6.6a   8.5a   6.0a   1.7a   4.7a   2.4a   2.1a   8.5a   1.2a   1.7a   9.6a   8.6a  12.8a   9.5a  -2.8a   9.8a   4.7a  10.7a   6.6a  11.2a-9999.9M"," 1919  5   16.4a   8.5a   9.4a   6.0a  10.7a   9.8a   8.5a  13.6a  14.4a-9999.9M  16.4a  19.0a  23.2a  16.9a  17.0a  19.6a  11.3a   9.4a  12.1a  17.2a  15.2a  17.0a  15.2a  17.5a  10.2a  22.6a  14.5a  22.0a  24.9a  23.8a  19.0a"," 1919  6   17.7a  25.4a  31.2a  25.3a  26.8a  22.0a  15.8a  19.0a  12.7a  19.6a  19.0a  24.5a  25.1a  27.4a  26.8a  19.0a  20.8a  26.8a  27.9a  25.8a  20.1a  17.7a  19.0a  32.4a  30.7a  22.6a !
>  19.0a  13.6a  17.5a  24.1a-9999.9M"," 1919  7   23.7a  24.4a  27.9a  29.6a  23.7a  21.3a  23.7a  20.1a  23.7a  21.3a  17.0a  17.8a  23.7a  27.4a  18.2a  23.2a  24.5a  26.2a  25.8a  27.9a  29.0a  25.3a  25.1a  23.9a  22.6a  23.9a  20.8a  25.8a  20.1a  23.2a  23.7a"," 1919  8   20.8a  18.2a  20.1a  20.1a  25.1a  20.8a  24.6a  18.5a  17.6a  22.0a  24.0a  23.2a  24.0a  24.0a  20.8a  24.0a  23.7a  23.8a  17.1a  23.8a  24.6a  23.8a  19.6a  24.0a  24.0a  16.9a  18.2a  18.6a  18.6a  23.2a  20.8a"," 1919  9   24.0a  21.3a  24.4a  18.1a  19.0a  19.0a  17.7a  11.4a  10.7a  12.7a  15.2a  15.2a  18.6a  12.7a  15.2a  10.1a  12.0a  12.7a  19.6a  18.5a  28.5a  28.5a  10.7a  14.5a  15.8a  11.3a  11.3a  20.8a  23.2a  11.3a-9999.9M"," 1919 10   11.3a   8.2a   8.2a  16.4a  10.7a  17.5a   7.7a   6.0a  11.3a   7.3a  12.1a   7.7a  10.2a  15.9a  18.2a   9.0a  10.7a   9.8a   8.2a   7.3a   7.7a   8.9a   9.5a  12.1a  10.2a  10.2a   4.0a  10.7a   2.9a   5.3a   3.0a"," 1919 11    8.2a   2.2a   1.2a   !
> 2.6a   1.7a   2.6a   6.1a   8.2a   7.7a   5.3a   4.7a   8.9a   4.7a
> 1.7a  -4.0a   2.2a   7.7a   7.7a   0.6a  -4.5a   2.6a   3.4a   2.5a  -3.4a  -5.9a  -5.1a  -5.5a  -6.4a   8.9a   3.0a-9999.9M"," 1919 12   -4.0a  -9.2a  -10.5a  -5.1a  -4.5a  -6.9a  -4.0a  -4.0a   3.0a   2.2a  -9.2a  -3.4a   5.3a  -6.4a  -6.9a  -20.4a  -20.4a  -17.6a  -10.5a  -13.8a  -8.7a  -3.4a  -2.9a  -4.5a  -5.5a  -5.5a  -2.9a  -0.8a  -10.1a  -6.9a  -5.9a"," Year Mo  Day 01  Day 02  Day 03  Day 04  Day 05  Day 06  Day 07  Day 08  Day 09  Day 10  Day 11  Day 12  Day 13  Day 14  Day 15  Day 16  Day 17  Day 18  Day 19  Day 20  Day 21  Day 22  Day 23  Day 24  Day 25  Day 26  Day 27  Day 28  Day 29  Day 30  Day 31","7011982,   DONNACONA    , QC, station jointe   , Temperature quotidienne maximale homogeneisee, Deg Celcius, Mise a jour jusqu a decembre 2014","Annee Mo Jour 01 Jour 02 Jour 03 Jour 04 Jour 05 Jour 06 Jour 07 Jour 08 Jour 09 Jour 10 Jour 11 Jour 12 Jour 13 Jour 14 Jour 15 Jour 16 Jour 17 Jour 18 Jour 19 Jour 20 Jour 21 Jour 22 Jour 23 Jour 24 Jour 25 Jour 26 Jour !
> 27 Jour 28 Jour 29 Jour 30 Jour 31"),class ="factor")),.Names ="X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014",class ="data.frame",row.names =c(NA,-21L)))
>
>
>
>
> file2=list(df2,df2)df2=list(structure(list(X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014 =structure(1:24,.Label =c("1948  1 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  2 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  3 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99!
> M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  4 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  5 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M   0.00    0.00    0.21T   0.21T   1.69    0.21T   0.21T   0.21T   0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    1.39    0.00    0.21T","1948  6    0.00    0.00    0.30T   3.34T   0.21T   0.00    0.00    7.19T   0.21T   1.04    0.00    4.29    1.69    0.21T   0.00    0.00    0.21T   0.65    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.21T-9999.99M","1948  7    0.21T   0.51T   0.00    2.74    0.00    0.00    0.00    0.00    0.00    1.05    0.00    !
> 0.00    1.57    1.57    2.30    0.74T   0.00    0.30T   0.74    0.53
>  0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.53    3.34    2.30   13.43 ","1948  8    0.30T   2.61    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.53    0.21T   0.65    3.65    3.25    0.21T   3.90    0.21T   0.21T   0.21T   0.30T   0.21T   0.21T   0.21T   0.00    0.21T   0.65    1.95    0.21T   0.21T","1948  9    0.00    0.21T   0.21T   0.21T   0.21T   0.69T   7.54    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   1.04    0.00    0.00    0.00    0.00    7.28    4.68    2.34    1.95    3.90    1.30    0.21T-9999.99M","1948 10    1.04    0.00    0.00    1.69    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.65    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T","1948 11    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   1.04    0.21T   0.00    0.00    1.69    0.21T   0.21T !
>   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1948 12    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 ","1949  1    0.00    0.00    0.00    0.00    0.00    0.39    0.65    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.65    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.00 ","1949  2    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00 -9999.99M-9999.99M-9999.99M","1949  3    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.!
> 21T   0.00    1.69    0.00    1.04    1.69    0.65    0.21T   0.51T
> 0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.21T   0.21T   0.00 ","1949  4    0.00    0.00    0.39    0.21T   0.00    0.00    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.65    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1949  5    0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.39    0.39    0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.39    0.39    0.21T   1.04 ","1949  6    0.39    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.65    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.51T   0.21T   0.00    0.39    0.21T   0.00    0.21T   0.21T   0.39    0.39    0.21T   0.00    0.00    0.00    0.00 -9999.99M","1949  7    0.00    0.00    0.53    0.51T   0.51T   0.21T   0.51T   0.30T   0.00    0.00   !
>  0.00    0.00    0.00    0.00    0.30T   0.30T   0.00    0.00    0.00    0.00    0.51T   0.51T   6.25   22.63    0.00    0.51T   0.21T   0.30T   0.30T   0.00    0.00 ","1949  8    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.51T   0.51T   0.00    0.30T   0.30T   0.00    0.00    6.56    0.00    0.00    0.00    0.00    1.05    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.51T   0.00    0.30T   0.21T   0.21T","1949  9    0.30T   0.30T   0.00    0.39    0.39    0.21T   0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T-9999.99M","1949 10    0.21T   0.00    0.00    0.00    1.04    0.39    0.65    0.21T   0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.65    0.65    0.21T   0.65    0.00    0.00    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.00    0.00    0.00    0.00 ","1949 11    0.00    0.00    0.00    0.00  !
>   1.04    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00
>    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T-9999.99M","1949 12    0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.21T   0.39    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.21T   0.00    0.00 "),class ="factor")),.Names ="X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014",class ="data.frame",row.names =c(NA,-24L)))
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

CONFIDENTIALITY NOTICE: This e-mail and any files and/or...{{dropped:10}}


From jafarikia at gmail.com  Wed Jan 20 22:56:10 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Wed, 20 Jan 2016 16:56:10 -0500
Subject: [R] R plot-par(mfrow=c(x,y)) on multiple pages
In-Reply-To: <CAM_vju=QRNz=_ep2BxeMB9PwbL6fUG0pTZiXabB5yUqVbTpF5Q@mail.gmail.com>
References: <CADs3iXnzNJ2F=w=yxa23FL7D9FezfZd_RhDGugVeHEWLdYR0gQ@mail.gmail.com>
	<CAM_vju=QRNz=_ep2BxeMB9PwbL6fUG0pTZiXabB5yUqVbTpF5Q@mail.gmail.com>
Message-ID: <CADs3iXma9YC7VFH9doP5NeWVCuVYU7__-XjzMdeM_QHaezVQ+Q@mail.gmail.com>

Thanks very much for the comment.

I was also wondering why all the y-axis on all 12 plots are similar to
the first plot. I have 12 plots and scale of the values for these
plots are different. It seems R is using the x-axis for each
individual plot correctly but y-axis is the same for all 12 graphs.

Regards,
Mohsen


On Wed, Jan 20, 2016 at 4:27 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Use a device like pdf() or postscript() that supports multiple pages.
> Or start a new default device with dev.new() so you can see two
> figures simultaneously.
>
> Sarah
>
> On Wed, Jan 20, 2016 at 3:37 PM, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
>> Hello everyone:
>>
>> I have 12 plots that I am using par(mfrow=c(3,2)) to have 6 of them in
>> a single page. R only prints the 6 first graph in a single page. Any
>> comments how I can ask R to print all 12 graphs in two pages.
>>
>> Thanks very much,
>> Mohsen
>>


From sarah.goslee at gmail.com  Wed Jan 20 23:05:20 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 20 Jan 2016 17:05:20 -0500
Subject: [R] R plot-par(mfrow=c(x,y)) on multiple pages
In-Reply-To: <CADs3iXma9YC7VFH9doP5NeWVCuVYU7__-XjzMdeM_QHaezVQ+Q@mail.gmail.com>
References: <CADs3iXnzNJ2F=w=yxa23FL7D9FezfZd_RhDGugVeHEWLdYR0gQ@mail.gmail.com>
	<CAM_vju=QRNz=_ep2BxeMB9PwbL6fUG0pTZiXabB5yUqVbTpF5Q@mail.gmail.com>
	<CADs3iXma9YC7VFH9doP5NeWVCuVYU7__-XjzMdeM_QHaezVQ+Q@mail.gmail.com>
Message-ID: <CAM_vjumtEoB=LCwJRC-AG5jgGAZBqzUtA7b+1b7kYwzY7=aLpw@mail.gmail.com>

On Wed, Jan 20, 2016 at 4:56 PM, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
> Thanks very much for the comment.
>
> I was also wondering why all the y-axis on all 12 plots are similar to
> the first plot. I have 12 plots and scale of the values for these
> plots are different. It seems R is using the x-axis for each
> individual plot correctly but y-axis is the same for all 12 graphs.

That seems highly unlikely: R should be using different axes for
independent plots.

Please provide a reproducible example, using fake data or built-in
data, or using dput()
with your own data.

The correct answer depends on what the format actually is; you need to
use dput() or some other unambiguous way of providing sample data.

Without a reproducible example that includes some sample data provided
using dput() (fake is fine), the code you used, and some clear idea of
what output you expect, it's impossible to figure out how to help you.
Here are some suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example


> Regards,
> Mohsen
>
>
> On Wed, Jan 20, 2016 at 4:27 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>> Use a device like pdf() or postscript() that supports multiple pages.
>> Or start a new default device with dev.new() so you can see two
>> figures simultaneously.
>>
>> Sarah
>>
>> On Wed, Jan 20, 2016 at 3:37 PM, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
>>> Hello everyone:
>>>
>>> I have 12 plots that I am using par(mfrow=c(3,2)) to have 6 of them in
>>> a single page. R only prints the 6 first graph in a single page. Any
>>> comments how I can ask R to print all 12 graphs in two pages.
>>>
>>> Thanks very much,
>>> Mohsen
>>>


From dwinsemius at comcast.net  Wed Jan 20 23:47:01 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 20 Jan 2016 14:47:01 -0800
Subject: [R] Splitting strings in data files R
In-Reply-To: <1545301296.8308291.1453323206724.JavaMail.yahoo@mail.yahoo.com>
References: <1545301296.8308291.1453323206724.JavaMail.yahoo.ref@mail.yahoo.com>
	<1545301296.8308291.1453323206724.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <E227A155-2813-4933-A139-55244509249F@comcast.net>


> On Jan 20, 2016, at 12:53 PM, Zilefac Elvis via R-help <r-help at r-project.org> wrote:
> 
> 
> 
> 
> Please I need help processing  files with strings in R. All the files have two patterns (thus,examine separately):

You do need help, that much is clear. But the first thing to do is retrace your initial data-entry steps. You have used the wrong read-function. The data in the input file either whitespace-separated (or fixed width format and you apparently thought it was a CSV-file.

Asking us to no work with this mess is just unreasonable. Post the original input file.

-- 
David.

> Pattern 1 (see file1 below): Delete Lines 1,2 & 4 in file1. Line 3 contains the column names. Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with with NA.
> 
> File1 output format should be: Year Month Day_1 Day_2 ... Day_31  ## so all months should 31 days. Months with <31 days should have NA where appropraite (e.g. Feb 30=NA, 31=NA)
> 
> Pattern 2 (see file2 below): Delete Line 1 in file2.Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with withNA. File2 has no column names. Please do not include any. 
> File2 output format: Year Month Day_1 Day_2 ... Day_31 but no column names
> 
> Here is a simple reproducible example for both files/cases: 
> 
> 
> file1=list(df1,df1)df1=list(structure(list(X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014 =structure(c(20L,19L,21L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,16L,17L,18L),.Label =c(" 1918  7 -9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M  23.6a  25.9a  25.8a  24.9a  24.9a  29.6a  27.4a  24.5a  28.5a  28.5a  30.1a  25.3a  28.5a  19.6a  24.1a"," 1918  8   23.7a  18.6a  17.6a  19.0a  23.7a  24.7a  18.6a  22.6a  20.1a  21.4a  22.6a  24.9a  24.1a  23.2a  22.0a  17.6a  19.0a  19.0a  23.7a  24.1a  24.9a  27.9a  26.2a  22.6a  24.0a  25.4a  21.4a  24.4a  19.0a  22.6a  23.7a"," 1918  9   22.0a  22.0a  24.0a  19.0a  14.4a  11.2a  17.1a  18.1a  19.0a  12.0a  13.5a   9.6a  11.2a  10.7a  18.1a  18.1a  16.3a  14.4a  14.3a  15.9a  10.1a   9.8a  11.3a  11.4a  13.6a  14.4a   9.3a   9.6a   9.2a   8.4a-9999!
> .9M"," 1918 10    9.3a   9.5a  11.3a  10.2a   9.9a-9999.9M   4.6a-9999.9M   9.8a  13.6a  17.0a  15.2a  15.1a  15.9a   8.1a   9.3a   8.8a   6.0a   8.7a   9.8a   9.8a  10.7a  11.3a   9.5a  10.7a-9999.9M  10.7a  16.9a  17.1a  10.7a  12.7a"," 1918 11    8.8a-9999.9M   4.0a   3.4a   4.0a   6.6a   4.0a   7.3a   8.1a   7.3a   2.5a   3.4a   7.7a   6.1a   2.2a   4.0a   4.6a   2.5a   2.2a   1.6a   2.2a   3.0a  -3.4a   2.5a   1.6a  -3.4a   2.1a   0.0a   2.6a   0.6a-9999.9M"," 1918 12  -10.1a  -8.3a  -6.3a  -5.5a  -5.1a  -7.2a-9999.9M  -3.4a  -2.2a  -5.5a  -6.0a  -3.4a   0.6a   3.0a   4.7a   0.6a  -5.0a  -6.4a  -5.9a  -2.2a   1.2a   4.0a   5.3a-9999.9M  -2.2a  -5.5a  -7.5a  -9.6a  -7.3a  -6.6a-9999.9M"," 1919  1    2.5a   0.0a  -7.3a  -6.7a  -6.6a  -9.2a  -5.9a  -0.7a  -2.9a  -13.2a  -8.0a  -17.1a  -7.4a  -4.0a  -5.5a   0.6a  -7.1a  -5.5a  -2.2a  -7.6a  -7.0a  -3.4a  -2.2a  -6.7a  -8.0a  -2.9a  -1.5a  -5.9a  -5.5a  -5.8a  -3.4a"," 1919  2 -9999.9M   0.0a  -4.0a  -3.4a  -1.5a  -2.1a  -3!
> .4a  -7.2a  -2.8a  -5.5a  -6.7a  -5.1a  -2.1a  -2.1a   1.2a  -2.1a  -5
> .9a  -2.8a  -4.5a  -4.5a  -3.4a   2.1a   0.0a   0.0a   1.2a  -2.1a  -8.3a  -6.6a-9999.9M-9999.9M-9999.9M"," 1919  3 -9999.9M   0.0a   1.6a   1.7a  -5.1a  -6.7a  -5.1a  -3.4a  -2.0a   1.2a   3.4a   1.2a  -8.3a  -7.9a  -3.4a  -2.0a   1.2a   3.4a   6.6a   1.2a   6.6a   1.2a   6.6a   6.6a   3.4a  10.7a   6.6a   6.6a   0.6a   0.6a  -6.8a"," 1919  4   -5.9a  -3.4a   2.1a   3.4a   3.0a   3.0a   8.1a   8.1a   6.0a   2.1a   6.6a   8.5a   6.0a   1.7a   4.7a   2.4a   2.1a   8.5a   1.2a   1.7a   9.6a   8.6a  12.8a   9.5a  -2.8a   9.8a   4.7a  10.7a   6.6a  11.2a-9999.9M"," 1919  5   16.4a   8.5a   9.4a   6.0a  10.7a   9.8a   8.5a  13.6a  14.4a-9999.9M  16.4a  19.0a  23.2a  16.9a  17.0a  19.6a  11.3a   9.4a  12.1a  17.2a  15.2a  17.0a  15.2a  17.5a  10.2a  22.6a  14.5a  22.0a  24.9a  23.8a  19.0a"," 1919  6   17.7a  25.4a  31.2a  25.3a  26.8a  22.0a  15.8a  19.0a  12.7a  19.6a  19.0a  24.5a  25.1a  27.4a  26.8a  19.0a  20.8a  26.8a  27.9a  25.8a  20.1a  17.7a  19.0a  32.4a  30.7a  22.6a !
>  19.0a  13.6a  17.5a  24.1a-9999.9M"," 1919  7   23.7a  24.4a  27.9a  29.6a  23.7a  21.3a  23.7a  20.1a  23.7a  21.3a  17.0a  17.8a  23.7a  27.4a  18.2a  23.2a  24.5a  26.2a  25.8a  27.9a  29.0a  25.3a  25.1a  23.9a  22.6a  23.9a  20.8a  25.8a  20.1a  23.2a  23.7a"," 1919  8   20.8a  18.2a  20.1a  20.1a  25.1a  20.8a  24.6a  18.5a  17.6a  22.0a  24.0a  23.2a  24.0a  24.0a  20.8a  24.0a  23.7a  23.8a  17.1a  23.8a  24.6a  23.8a  19.6a  24.0a  24.0a  16.9a  18.2a  18.6a  18.6a  23.2a  20.8a"," 1919  9   24.0a  21.3a  24.4a  18.1a  19.0a  19.0a  17.7a  11.4a  10.7a  12.7a  15.2a  15.2a  18.6a  12.7a  15.2a  10.1a  12.0a  12.7a  19.6a  18.5a  28.5a  28.5a  10.7a  14.5a  15.8a  11.3a  11.3a  20.8a  23.2a  11.3a-9999.9M"," 1919 10   11.3a   8.2a   8.2a  16.4a  10.7a  17.5a   7.7a   6.0a  11.3a   7.3a  12.1a   7.7a  10.2a  15.9a  18.2a   9.0a  10.7a   9.8a   8.2a   7.3a   7.7a   8.9a   9.5a  12.1a  10.2a  10.2a   4.0a  10.7a   2.9a   5.3a   3.0a"," 1919 11    8.2a   2.2a   1.2a   !
> 2.6a   1.7a   2.6a   6.1a   8.2a   7.7a   5.3a   4.7a   8.9a   4.7a   
> 1.7a  -4.0a   2.2a   7.7a   7.7a   0.6a  -4.5a   2.6a   3.4a   2.5a  -3.4a  -5.9a  -5.1a  -5.5a  -6.4a   8.9a   3.0a-9999.9M"," 1919 12   -4.0a  -9.2a  -10.5a  -5.1a  -4.5a  -6.9a  -4.0a  -4.0a   3.0a   2.2a  -9.2a  -3.4a   5.3a  -6.4a  -6.9a  -20.4a  -20.4a  -17.6a  -10.5a  -13.8a  -8.7a  -3.4a  -2.9a  -4.5a  -5.5a  -5.5a  -2.9a  -0.8a  -10.1a  -6.9a  -5.9a"," Year Mo  Day 01  Day 02  Day 03  Day 04  Day 05  Day 06  Day 07  Day 08  Day 09  Day 10  Day 11  Day 12  Day 13  Day 14  Day 15  Day 16  Day 17  Day 18  Day 19  Day 20  Day 21  Day 22  Day 23  Day 24  Day 25  Day 26  Day 27  Day 28  Day 29  Day 30  Day 31","7011982,   DONNACONA    , QC, station jointe   , Temperature quotidienne maximale homogeneisee, Deg Celcius, Mise a jour jusqu a decembre 2014","Annee Mo Jour 01 Jour 02 Jour 03 Jour 04 Jour 05 Jour 06 Jour 07 Jour 08 Jour 09 Jour 10 Jour 11 Jour 12 Jour 13 Jour 14 Jour 15 Jour 16 Jour 17 Jour 18 Jour 19 Jour 20 Jour 21 Jour 22 Jour 23 Jour 24 Jour 25 Jour 26 Jour !
> 27 Jour 28 Jour 29 Jour 30 Jour 31"),class ="factor")),.Names ="X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014",class ="data.frame",row.names =c(NA,-21L)))
> 
> 
> 
> 
> file2=list(df2,df2)df2=list(structure(list(X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014 =structure(1:24,.Label =c("1948  1 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  2 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  3 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99!
> M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  4 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  5 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M   0.00    0.00    0.21T   0.21T   1.69    0.21T   0.21T   0.21T   0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    1.39    0.00    0.21T","1948  6    0.00    0.00    0.30T   3.34T   0.21T   0.00    0.00    7.19T   0.21T   1.04    0.00    4.29    1.69    0.21T   0.00    0.00    0.21T   0.65    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.21T-9999.99M","1948  7    0.21T   0.51T   0.00    2.74    0.00    0.00    0.00    0.00    0.00    1.05    0.00    !
> 0.00    1.57    1.57    2.30    0.74T   0.00    0.30T   0.74    0.53  
>  0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.53    3.34    2.30   13.43 ","1948  8    0.30T   2.61    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.53    0.21T   0.65    3.65    3.25    0.21T   3.90    0.21T   0.21T   0.21T   0.30T   0.21T   0.21T   0.21T   0.00    0.21T   0.65    1.95    0.21T   0.21T","1948  9    0.00    0.21T   0.21T   0.21T   0.21T   0.69T   7.54    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   1.04    0.00    0.00    0.00    0.00    7.28    4.68    2.34    1.95    3.90    1.30    0.21T-9999.99M","1948 10    1.04    0.00    0.00    1.69    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.65    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T","1948 11    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   1.04    0.21T   0.00    0.00    1.69    0.21T   0.21T !
>   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1948 12    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 ","1949  1    0.00    0.00    0.00    0.00    0.00    0.39    0.65    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.65    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.00 ","1949  2    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00 -9999.99M-9999.99M-9999.99M","1949  3    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.!
> 21T   0.00    1.69    0.00    1.04    1.69    0.65    0.21T   0.51T   
> 0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.21T   0.21T   0.00 ","1949  4    0.00    0.00    0.39    0.21T   0.00    0.00    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.65    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1949  5    0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.39    0.39    0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.39    0.39    0.21T   1.04 ","1949  6    0.39    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.65    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.51T   0.21T   0.00    0.39    0.21T   0.00    0.21T   0.21T   0.39    0.39    0.21T   0.00    0.00    0.00    0.00 -9999.99M","1949  7    0.00    0.00    0.53    0.51T   0.51T   0.21T   0.51T   0.30T   0.00    0.00   !
>  0.00    0.00    0.00    0.00    0.30T   0.30T   0.00    0.00    0.00    0.00    0.51T   0.51T   6.25   22.63    0.00    0.51T   0.21T   0.30T   0.30T   0.00    0.00 ","1949  8    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.51T   0.51T   0.00    0.30T   0.30T   0.00    0.00    6.56    0.00    0.00    0.00    0.00    1.05    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.51T   0.00    0.30T   0.21T   0.21T","1949  9    0.30T   0.30T   0.00    0.39    0.39    0.21T   0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T-9999.99M","1949 10    0.21T   0.00    0.00    0.00    1.04    0.39    0.65    0.21T   0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.65    0.65    0.21T   0.65    0.00    0.00    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.00    0.00    0.00    0.00 ","1949 11    0.00    0.00    0.00    0.00  !
>   1.04    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00
>    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T-9999.99M","1949 12    0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.21T   0.39    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.21T   0.00    0.00 "),class ="factor")),.Names ="X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014",class ="data.frame",row.names =c(NA,-24L)))
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Wed Jan 20 23:59:26 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 21 Jan 2016 09:59:26 +1100
Subject: [R] (no subject)
In-Reply-To: <192D5542-943F-4EE1-870E-BF669A3F9B11@yahoo.com>
References: <A43ECBD6-DDDC-4759-B41B-ECAB2D93FC2B@yahoo.com>
	<20160119203556.Horde.9TKV3BIrowvzk6LqUOk2DKY@mail.sapo.pt>
	<CA+8X3fUj903NmKQuK0eAtMkkBrVD+7acqYyFTS7hCfpbjZa=SQ@mail.gmail.com>
	<192D5542-943F-4EE1-870E-BF669A3F9B11@yahoo.com>
Message-ID: <CA+8X3fUZt0RiNHPikC+Wdxy7bXFwmpsr09WFKbyz-SpLtAiEnA@mail.gmail.com>

Hi Maryam,

c(Young.list1[sample(1:20,5),],
 Young.list2[sample(1:20,5),],
 Young.list3[sample(1:20,5),])
# or for a more general solution
nrows<-dim(Young.list1)[1]
c(Young.list1[sample(1:nrows,nrows/4),],
 Young.list2[sample(1:nrows,nrows/4),],
 Young.list3[sample(1:nrows,nrows/4),])

Jim


On Wed, Jan 20, 2016 at 9:03 PM, maryam firoozi <
firoozi_maryam6858 at yahoo.com> wrote:

> thanks for repling.can you help me how to select randomely 25% from three
> mattix and put it in a vector.
>
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Thu Jan 21 00:07:49 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 21 Jan 2016 00:07:49 +0100
Subject: [R] tcltk tkwidget(..."table")
In-Reply-To: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>
References: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>
Message-ID: <6FEECD0A-6905-4DEF-A72B-96F2A6FDA827@gmail.com>


> On 19 Jan 2016, at 20:48 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> 
> Does anyone know a simple way to create a tcltk table with columns of
> varying widths?

Create a table, then set the width of the columns with the width subcommand?

-pd


pathName width ?col? ?value col value ...? If no col is specified, returns a list describing all cols for which a width has been set. If col is specified with no value, it prints out the width of that col in characters (positive number) or pixels (negative number). If one or more col-value pairs are specified, then it sets each col to be that width in characters (positive number) or pixels (negative number). If value is default, then the col uses the default width, specified by -colwidth.



> 
> -Dan
> 
> 
> 
> -- 
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From drjimlemon at gmail.com  Thu Jan 21 00:08:50 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 21 Jan 2016 10:08:50 +1100
Subject: [R] R plot-par(mfrow=c(x,y)) on multiple pages
In-Reply-To: <CADs3iXma9YC7VFH9doP5NeWVCuVYU7__-XjzMdeM_QHaezVQ+Q@mail.gmail.com>
References: <CADs3iXnzNJ2F=w=yxa23FL7D9FezfZd_RhDGugVeHEWLdYR0gQ@mail.gmail.com>
	<CAM_vju=QRNz=_ep2BxeMB9PwbL6fUG0pTZiXabB5yUqVbTpF5Q@mail.gmail.com>
	<CADs3iXma9YC7VFH9doP5NeWVCuVYU7__-XjzMdeM_QHaezVQ+Q@mail.gmail.com>
Message-ID: <CA+8X3fUk2oMPqBJv9E7XtvWEeapc_wRhajRb=aocyysORcdLVw@mail.gmail.com>

Hi Mohsen,
I'll have a wild guess at this. I suspect that you have either calculated a
value for the ylim= argument or used explicit values for ylim= in the first
plot, then propagated the error by copying and pasting.

Jim

On Thu, Jan 21, 2016 at 8:56 AM, Mohsen Jafarikia <jafarikia at gmail.com>
wrote:

> Thanks very much for the comment.
>
> I was also wondering why all the y-axis on all 12 plots are similar to
> the first plot. I have 12 plots and scale of the values for these
> plots are different. It seems R is using the x-axis for each
> individual plot correctly but y-axis is the same for all 12 graphs.
>
> Regards,
> Mohsen
>
>
>

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Thu Jan 21 00:25:01 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Wed, 20 Jan 2016 15:25:01 -0800
Subject: [R] tcltk tkwidget(..."table")
In-Reply-To: <6FEECD0A-6905-4DEF-A72B-96F2A6FDA827@gmail.com>
References: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>
	<6FEECD0A-6905-4DEF-A72B-96F2A6FDA827@gmail.com>
Message-ID: <CAJeYpE8Bm1QRTk6F5hTuTr24t25XrG97k8Cjz0w25FOBQFaPSQ@mail.gmail.com>

Thanks, Peter.

I'm sure that's right, but it requires knowing: (1) that there's something
called the "width subcommand", and (2) how to format the call to that
command/subcommand.

I was able to do it eventually but only after a few hours of effort
searching the web for help.

E.g. with a table (called table1) with 3 columns and want to set widths to
30, 5, and 5:

colwidths<-c(30, 5, 5)

for(i in 1:3) {
  tcl(table1, "width", i - 1, colwidths[i])
}



On Wed, Jan 20, 2016 at 3:07 PM, peter dalgaard <pdalgd at gmail.com> wrote:

>
> > On 19 Jan 2016, at 20:48 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> >
> > Does anyone know a simple way to create a tcltk table with columns of
> > varying widths?
>
> Create a table, then set the width of the columns with the width
> subcommand?
>
> -pd
>
>
> pathName width ?col? ?value col value ...? If no col is specified, returns
> a list describing all cols for which a width has been set. If col is
> specified with no value, it prints out the width of that col in characters
> (positive number) or pixels (negative number). If one or more col-value
> pairs are specified, then it sets each col to be that width in characters
> (positive number) or pixels (negative number). If value is default, then
> the col uses the default width, specified by -colwidth.
>
>
>
> >
> > -Dan
> >
> >
> >
> > --
> > Dan Dalthorp, PhD
> > USGS Forest and Rangeland Ecosystem Science Center
> > Forest Sciences Lab, Rm 189
> > 3200 SW Jefferson Way
> > Corvallis, OR 97331
> > ph: 541-750-0953
> > ddalthorp at usgs.gov
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From rmh at temple.edu  Thu Jan 21 01:46:23 2016
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 20 Jan 2016 19:46:23 -0500
Subject: [R] R editor for Mac
In-Reply-To: <569FE70C.9060308@gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<9B91D216-0AC2-43E3-8070-74B42643E74B@xs4all.nl>
	<569FE70C.9060308@gmail.com>
Message-ID: <CAGx1TMAnHPRu7ERLNErSKbnYM-d123UMGTqLZk8apPYGHFm3cQ@mail.gmail.com>

Emacs with ESS is very simple to operate when you use the menu.
See Paul Johnson's document

[Emacs has no learning curve: Emacs and ESS]
http://pj.freefaculty.org/guides/Rcourse/emacs-ess/emacs-ess.pdf

Rich

On Wed, Jan 20, 2016 at 2:59 PM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 20/01/2016 2:22 PM, Franklin Bretschneider wrote:
>>
>> Dear Christofer Bogaso,
>>
>>
>> Re:
>>
>>
>> > Could you please suggest a good R editor for Mac OS X (10.7.5)
>>
>>
>>
>> Indeed, as Roy Mendelssohn wrote, the editor built into "R.app", the GUI
>> program which is part of the standard R for OS X, has a beautiful editor,
>> complete with syntax colouring and bracket balancing. And one can run only
>> one or a few lines from a script at wish.
>> I couldn't wish myself more.
>
>
> R.app looks nicer than RStudio (no tiling), but it is missing a lot of
> functionality.
>
>  - the debugger
>  - the integration with other tools provided by RStudio, like Shiny,
> RMarkdown, htmlwidgets, package building tools, etc.
>  - the integration with source code management.
>  - session management (i.e. it's easier to shut down and restart R, which
> you frequently need to do when developing and testing packages)
>  - syntax checking hints in the editor (not just for R, for some other
> languages too).
>
> I think Emacs + ESS matches (exceeds if you count non-R stuff) RStudio in
> functionality, but it is much harder to learn.
>
> Duncan Murdoch
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wjm1 at caa.columbia.edu  Thu Jan 21 01:57:19 2016
From: wjm1 at caa.columbia.edu (William Michels)
Date: Wed, 20 Jan 2016 16:57:19 -0800
Subject: [R] R editor for Mac
In-Reply-To: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
Message-ID: <CAA99HCyOiTZWxNjCe4i7Wisjo9UrkcnzVe948AXVJ5E15OvSAQ@mail.gmail.com>

Hello Christofer!

For text-editing the R.app GUI has always been fabulous. An old
mainstay on the Mac (after Apple's TextEdit) has been TextWrangler,
and its big-brother, BBEdit. For development RStudio is quite nice,
and--based partly on RStudio's offering of a Vim-compatibility
mode--Vim has become a recent interest, although it has a steep
learning curve. Of course, Vim itself is already available at the
Terminal command line, but more Mac-like version named MacVim is
available at:

https://github.com/b4winckler/macvim/releases

Further instructions on using Vim with R are available here:
http://manuals.bioinformatics.ucr.edu/home/programming-in-r/vim-r

BTW, it looks like you're using an older version of Mac OS X (Lion,
version 10.7.5) released in 2012. So some of the text editors
mentioned by others in this thread may not be their "latest and
greatest." Also, while you can run R version 3.2.1 right now (current
R version is 3.2.3), the R Mac page says "NOTE: the binary support for
OS X before Mavericks (10.9) is being phased out, we do not expect
further releases!" See:

https://cran.r-project.org/bin/macosx/

Mac OS X is now at version 10.11.4 (El Capitan). Wikipedia says some
Macs all the way back to 2007 can run El Capitan (see
https://en.wikipedia.org/wiki/OS_X_El_Capitan), so there may be an
upgrade path for you, if not all the way to El Capitan (10.11) then
maybe up to Mavericks (10.9), to keep you from having to compile
future R-versions from source. Finally, you should consider checking
out the R-Sig-Mac mailing list for further Mac-specific info:

https://stat.ethz.ch/mailman/listinfo/r-sig-mac

HTH,

Bill

W. Michels, Ph.D.


On Wed, Jan 20, 2016 at 10:22 AM, Christofer Bogaso
<bogaso.christofer at gmail.com> wrote:


From ddalthorp at usgs.gov  Thu Jan 21 02:12:36 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Wed, 20 Jan 2016 17:12:36 -0800
Subject: [R] tcltk: write '[' and ']' in a table cell
Message-ID: <CAJeYpE-fBRuufBefF0N9WLG5iEY=19_qg+MZ1bN7WOYT-tPkRw@mail.gmail.com>

I know it should not be difficult to write the string:

i<-4
j<-17
lbl<-paste0("[", i, ", ", j, "]")

# to a table, but I'm having a devil of a time trying to figure out how to
do it.
# the following gives lbl surrounded by braces.

tt<-tktoplevel()
tfr <- tkframe(tt)
tkgrid(tfr)
junk<-tclArray()
junk[[0,0]]<-lbl
table1<-tkwidget(tfr,"table", rows=1, cols=1, variable=junk)
tkgrid(table1)


How to write without the braces?

Any quick suggestions would be very much appreciated...I've burned much
time trying to figure this should-be-simple problem out.

Thanks!



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From jafarikia at gmail.com  Thu Jan 21 02:38:58 2016
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Wed, 20 Jan 2016 20:38:58 -0500
Subject: [R] R plot-par(mfrow=c(x,y)) on multiple pages
In-Reply-To: <CA+8X3fUk2oMPqBJv9E7XtvWEeapc_wRhajRb=aocyysORcdLVw@mail.gmail.com>
References: <CADs3iXnzNJ2F=w=yxa23FL7D9FezfZd_RhDGugVeHEWLdYR0gQ@mail.gmail.com>
	<CAM_vju=QRNz=_ep2BxeMB9PwbL6fUG0pTZiXabB5yUqVbTpF5Q@mail.gmail.com>
	<CADs3iXma9YC7VFH9doP5NeWVCuVYU7__-XjzMdeM_QHaezVQ+Q@mail.gmail.com>
	<CA+8X3fUk2oMPqBJv9E7XtvWEeapc_wRhajRb=aocyysORcdLVw@mail.gmail.com>
Message-ID: <CADs3iX=AwsD1vYH3aRWpOgTEoz2zQAv4Uf=ebNo=uVFFKCm_vA@mail.gmail.com>

Dear Jim,

Your wild guess was a good guess :=)

Thanks to you and Sarah for your comments.

Regards,
Mohsen


On Wed, Jan 20, 2016 at 6:08 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi Mohsen,
> I'll have a wild guess at this. I suspect that you have either calculated a
> value for the ylim= argument or used explicit values for ylim= in the first
> plot, then propagated the error by copying and pasting.
>
> Jim
>
> On Thu, Jan 21, 2016 at 8:56 AM, Mohsen Jafarikia <jafarikia at gmail.com>
> wrote:
>>
>> Thanks very much for the comment.
>>
>> I was also wondering why all the y-axis on all 12 plots are similar to
>> the first plot. I have 12 plots and scale of the values for these
>> plots are different. It seems R is using the x-axis for each
>> individual plot correctly but y-axis is the same for all 12 graphs.
>>
>> Regards,
>> Mohsen
>>
>>
>


From sunnysingha.analytics at gmail.com  Wed Jan 20 19:26:11 2016
From: sunnysingha.analytics at gmail.com (Sandeep Rana)
Date: Wed, 20 Jan 2016 23:56:11 +0530
Subject: [R] R editor for Mac
In-Reply-To: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
Message-ID: <178DF693-DE9C-4106-A528-DD1F4D42C0E6@gmail.com>

Christopher,
Download TextWrangler from App store. I?m using it and its very convenient.

Regards,
Sunny


> On 20-Jan-2016, at 11:52 PM, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Hi,
> 
> Could you please suggest a good R editor for Mac OS X (10.7.5)
> Previously my operating system was Windows and there I used Notepad++,
> I really had very nice experience with it. However I dont see any Mac
> version is available for Mac.
> 
> Appreciate your positive feedback.
> 
> Thanks and regards,
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From firoozi_maryam6858 at yahoo.com  Wed Jan 20 22:26:38 2016
From: firoozi_maryam6858 at yahoo.com (maryam firoozi)
Date: Thu, 21 Jan 2016 00:56:38 +0330
Subject: [R] Fwd:
References: <6F174D07-953F-40F7-AB7D-4641AF45913F@yahoo.com>
Message-ID: <AFF3ABFE-9372-47CC-B4A2-3204B7D2463F@yahoo.com>

Hello,
i made a population about 4500 individual.this has two sex(female and male).they had pedigree.
i wanted to enter new indiviual but their ID of indiviual mustnot be same perivous and their ID number mustnot be bigger than 4500.
first population's ID number is 1:4500.
how can i handel it?

Sent from my iPhone

Begin forwarded message:

> From: maryam firoozi <firoozi_maryam6858 at yahoo.com>
> Date: January 18, 2016 at 1:14:56 AM GMT+3:30
> To: r-help at r-project.org
> 
> hello,
> we want to do genomic blup in r.i know that use pedigree package.
> the formule is
> gblup( P~1,data=ped[,c('ID','P')],M=M,lambda=1/h2-1)
> P:phenotype variance
> ped:pedigree
> M: matrix marker or genotype
> my ped has 4500 ID.but my M has 9000 individual.becasue i have two row for each ID in M matrix becasue each ID has two haplotype.how can i solve it.the formula didnt solve.
> sincerely
> 
> 
> Sent from my iPhone

	[[alternative HTML version deleted]]


From milena.stat at gmail.com  Wed Jan 20 22:49:01 2016
From: milena.stat at gmail.com (milena)
Date: Wed, 20 Jan 2016 22:49:01 +0100
Subject: [R] ggplot2 - curveGrob - annotation_custom
Message-ID: <CACvKwTQP0S9UVCT2O-czBFivrViKFybGiEcYv0Zzfw9dZtAizg@mail.gmail.com>

Dear R users,

I am struggling to understand *curveGrob* and *annotation_custom* command
in *ggplot*.

In brief my issue can be approximated to drawing a downward sloping arrow
curve
from point (5,5) to (10,0) but keep on getting (5,0) to (10,5) |=> see
attached arrows.png

require(grid)
g<-qplot(c(0,10),c(0,10))
myCurve<-curveGrob(0, 0, 1, 1, default.units = "npc",
                   curvature = 0.3, angle = 90, ncp = 20, shape = 1,
                   square = FALSE, squareShape = 1,
                   inflect = FALSE, arrow = arrow(), open = TRUE,
                   debug = FALSE,
                   name = NULL, gp = gpar(col="blue"), vp = NULL)

myCurve2<-curveGrob(0, 0, 1, 1, default.units = "npc",
                    curvature = -0.3, angle = 60, ncp = 10, shape = 1,
                    square = FALSE, squareShape = 1,
                    inflect = FALSE, arrow = arrow(), open = TRUE,
                    debug = FALSE,
                    name = NULL, gp = gpar(), vp = NULL)

g +
  annotation_custom(grob=myCurve,0,10,0,10) + # plot from 0,0 to 10,10
  *annotation_custom(grob=myCurve,5,10,5,0) + # !!!!!this should draw from
(5,5) to (10,0) but it does not*
  annotation_custom(grob=myCurve2,2.5,6,2.5,10)   # plot from 2.5,2.5 to 6,6


In more detail:

I am building a shiny application (like in shiny_pic.png).
after choosing country 1 and country 2, R would plot a map of Europe,
highlight the two selected countries with different colors
and draw a curved arrow from country 1 in its respective color towards
country2
and vice versa from country 2 to country 1 in the color of country 2.

size of the arrow would correspond with trade between the two countries.

since it is a shiny application I need a code that works fast.
map does not have to be beautiful, the speed of the application is a
priority.

for the curved arrows I tried geom_curve but either the arrow was not drawn
at all
or would only appear after a wait for several minutes...

I searched stackoverflow and have found a very similar problem:
http://stackoverflow.com/questions/20216179/plot-curved-lines-between-two-locations-in-ggplot2

and although it works very well for a couple of countries like Italy-Poland
(the only thing I had to change inside curveGrov2(arrow(ends="first"))
for the arrow head to appear at the beginning, not the end.

but the problem starts with country combinations like Austria-Cyprus or
France-Greece.
It seems that x-coordinates are respected, but y-coordinates inverted
to draw an *upward* looking curve.

it has been a while that I am looking at this problem so perhaps I have
lost a fresh eye.
is there some parameter I need to tweak in curveGrob function?

fyi: my map does not have to be plotted with ggplot2.
I am open to other solutions such as spplot etc.
as long as it works fast (shiny)
and draws the arrows as desired
with every country combination and code execution.

I would be grateful for any help and I am open to every feedback
if there is something I can improve in my code please fell free...

Milena

aka suzukiblue
-------------- next part --------------
A non-text attachment was scrubbed...
Name: shiny_pic.png
Type: image/png
Size: 89577 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/2bdb176e/attachment-0004.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Italy-Poland.png
Type: image/png
Size: 11819 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/2bdb176e/attachment-0005.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Austria-Cyprus.png
Type: image/png
Size: 12047 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/2bdb176e/attachment-0006.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: France-Greece.png
Type: image/png
Size: 11894 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/2bdb176e/attachment-0007.png>

From milena.stat at gmail.com  Wed Jan 20 22:55:29 2016
From: milena.stat at gmail.com (milena)
Date: Wed, 20 Jan 2016 22:55:29 +0100
Subject: [R] ggplot2 - curveGrog - annotation_custom
Message-ID: <CACvKwTQm1sOM=-8HOqRDwfOfztaNA6q1i=YBegK25fv9qq5svA@mail.gmail.com>

I repeat the question because it seems that the code did not get attached.

Dear R users,

I am struggling to understand *curveGrob* and *annotation_custom* command
in *ggplot*.

In brief my issue can be approximated to drawing a downward sloping arrow
curve
from point (5,5) to (10,0) but keep on getting (5,0) to (10,5) |=> see
attached arrows.png

require(grid)
g<-qplot(c(0,10),c(0,10))
myCurve<-curveGrob(0, 0, 1, 1, default.units = "npc",
                   curvature = 0.3, angle = 90, ncp = 20, shape = 1,
                   square = FALSE, squareShape = 1,
                   inflect = FALSE, arrow = arrow(), open = TRUE,
                   debug = FALSE,
                   name = NULL, gp = gpar(col="blue"), vp = NULL)

myCurve2<-curveGrob(0, 0, 1, 1, default.units = "npc",
                    curvature = -0.3, angle = 60, ncp = 10, shape = 1,
                    square = FALSE, squareShape = 1,
                    inflect = FALSE, arrow = arrow(), open = TRUE,
                    debug = FALSE,
                    name = NULL, gp = gpar(), vp = NULL)

g +
  annotation_custom(grob=myCurve,0,10,0,10) + # plot from 0,0 to 10,10
  *annotation_custom(grob=myCurve,5,10,5,0) + # !!!!!this should draw from
(5,5) to (10,0) but it does not*
  annotation_custom(grob=myCurve2,2.5,6,2.5,10)   # plot from 2.5,2.5 to 6,6


In more detail:

I am building a shiny application (like in shiny_pic.png).
after choosing country 1 and country 2, R would plot a map of Europe,
highlight the two selected countries with different colors
and draw a curved arrow from country 1 in its respective color towards
country2
and vice versa from country 2 to country 1 in the color of country 2.

size of the arrow would correspond with trade between the two countries.

since it is a shiny application I need a code that works fast.
map does not have to be beautiful, the speed of the application is a
priority.

for the curved arrows I tried geom_curve but either the arrow was not drawn
at all
or would only appear after a wait for several minutes...

I searched stackoverflow and have found a very similar problem:
http://stackoverflow.com/questions/20216179/plot-curved-lines-between-two-locations-in-ggplot2

and although it works very well for a couple of countries like Italy-Poland
(the only thing I had to change inside curveGrov2(arrow(ends="first"))
for the arrow head to appear at the beginning, not the end.

but the problem starts with country combinations like Austria-Cyprus or
France-Greece.
It seems that x-coordinates are respected, but y-coordinates inverted
to draw an *upward* looking curve.

it has been a while that I am looking at this problem so perhaps I have
lost a fresh eye.
is there some parameter I need to tweak in curveGrob function?

fyi: my map does not have to be plotted with ggplot2.
I am open to other solutions such as spplot etc.
as long as it works fast (shiny)
and draws the arrows as desired
with every country combination and code execution.

I would be grateful for any help and I am open to every feedback
if there is something I can improve in my code please fell free...

Milena

aka suzukiblue
-------------- next part --------------
###

rm(list = ls())
rm(list=lsf.str())

library(rgeos)
library(stringr)
library(reshape)
library(maptools)
library(ggplot2)
library(SmarterPoland)
library(sp)
library(grid)
library(rgdal)

# choose your working directory for shapefile download
setwd("C:/Users/Milena/Documents/R")
current.dir <- getwd()

#download the shapefile
download.file("http://thematicmapping.org/downloads/TM_WORLD_BORDERS-0.3.zip", 
               destfile="TM_WORLD_BORDERS-0.3.zip")

#unzip to SpatialPolygonsDataFrame
unzip("TM_WORLD_BORDERS-0.3.zip")

world <- readOGR(dsn = current.dir, layer = "TM_WORLD_BORDERS-0.3")

# extract from the world shapefile only the EU countries
c.eu <- c("AUT", "BEL", "BGR", "HRV", "CYP", "CZE", "DNK", "EST", "FIN",
           "FRA", "DEU", "GRC", "HUN", "IRL", "ITA", "LVA", "LTU", "LUX", 
           "MLT", "NLD", "POL", "PRT", "ROU", "SVK", "SVN", "ESP", "SWE", 
           "GBR")

vec.data.eu <- rep(0, times=length(c.eu))

for (i in 1:length(c.eu) ) 
{vec.data.eu[i] = which(world at data[, "ISO3"]==c.eu[i])}

world.eu1 <- world[vec.data.eu,]
world.eu <- world.eu1

# country centroids (middle points in each EU country)
LAT  <- c(47.33, 50.83, 43.00, 45.17, 35.00, 49.75, 56.00, 
          59.00, 64.00, 46.00, 51.50, 39.00, 47.00, 53.00, 
          42.83, 57.00, 56.00, 49.75, 35.92, 52.50, 52.00, 
          39.50, 46.00, 48.67, 46.25, 40.00, 62.00, 54.00)

LONG <- c(13.33,  4.00, 25.00, 15.50, 33.00, 15.00, 10.00,
          26.00, 26.00,  2.00, 10.50, 22.00, 20.00, -8.00, 
          12.83, 25.00, 24.00,  6.17, 14.43,  5.75, 20.00, 
          -8.00, 25.00, 19.50, 15.17, -4.00, 15.00, -4.00)

cent <- cbind(LAT,LONG)
rownames(cent) <- c.eu

# choose two countries and display them in two different colors (color A and color B)
# I create a vector of ones length equal to the number of countries (28)

world.eu <- world.eu1
a <- rep(1, length(c.eu))

# and overwrite the value for the 1st selected country with value 2 
# and the 2nd selected country with 3 

cor1 <<- 15 #Italy
cor2 <<- 21 #Poland

a[cor1] <- 2
a[cor2] <- 3

# combine the vector of levels with country names
# and call the factor column "score"

b <- cbind(c.eu, a)
dataframe2 <- data.frame(b)  
colnames(dataframe2) <-c("Country.Code", "score")

# merge score with the spatial points data frame 
# and pass it to ggplot for visualization

matched.indices.eu <- match(world.eu at data[, "ISO3"], dataframe2[, "Country.Code"])
world.eu at data <- data.frame(world.eu at data, dataframe2[matched.indices.eu, ])
world.f.eu <- fortify(world.eu, region = "ISO3")
world.m.eu <- merge(world.f.eu, world.eu at data, by.x = "id", by.y = "Country.Code")

# draw curved arrows from country A to country B in color A (an arrow head pointing at country B)
# and from country B to country A in color B (an arrow head pointing at country A)
# the size of the arrow should correspond with the size of trade flow 
# from country A to B and from B to A 
# (that is however not the object of that question and could be done later)

# since the ggplot will be encapsulated in a shiny application 
# code should draw correct arrows in every country A and country B combination
# and should execute really fast. 

# I made a trial with geom_curve but the arrows either did not appear at all 
# (even though no error message was displayed)
# or if appear that would be after a long wait

# I found function curveGrob (which worked quite fast on my machine)
# and annotation_custom to pass it to ggplot
# I thought that it would be a solution. 
# my problem is similar I also want to visualize export between countries

# all I had to change was the curvature of the first arrow to the -0.3
# and assign the arrow head of to the beginning not the end of the arrow

myCurve<-curveGrob(0, 0, 1, 1, default.units = "npc",
                   curvature = -0.3, angle = 60, ncp = 20, shape = 1,
                   square = FALSE, squareShape = 1,
                   inflect = FALSE, arrow = arrow(length = unit(0.15, "inches"), type="closed"), open = TRUE,
                   debug = FALSE,
                   name = NULL, gp = gpar(col="blue", lwd=10, lineend="round", fill="blue"), vp = NULL)

myCurve2<-curveGrob(0, 0, 1, 1, default.units = "npc",
                    curvature = 0.3, angle = 60, ncp = 20, shape = 1,
                    square = FALSE, squareShape = 1,
                    inflect = FALSE, arrow = arrow(length = unit(0.15, "inches"), type="closed", ends="first"), 
                    open = TRUE, debug = FALSE,
                    name = NULL, gp = gpar(col="magenta", lwd=2, lineend="round", fill="magenta"), vp = NULL)

ggplot(world.m.eu, aes(long, lat, group = group))+
geom_polygon(aes(fill = world.m.eu$score),show.legend=FALSE)+ 
geom_polygon(data = world.m.eu, aes(long,lat), 
             fill="NA", color = "white",  size=0.01) +
      coord_cartesian(xlim = c(-17, 37), ylim = c(34, 72))+ 
      scale_fill_manual(values = c("lightblue", "blue", "magenta")) +
      annotation_custom(grob=myCurve, xmin=cent[cor1,2], xmax=cent[cor2,2],
                      ymin=cent[cor1,1], ymax=cent[cor2,1]) +
      annotation_custom(grob=myCurve2,xmin=cent[cor2,2], xmax=cent[cor1,2],
                        ymin=cent[cor2,1], ymax=cent[cor1,1])
    
# however what worked for the combination Italy - Poland 
# and perhaps a few others 
# does not work for example for Austria - Cyprus
# or France - Greece. 
# it seems that regardless the assignment of xmin and xmax
# or ymin and ymax
# it looks like the ys are switched 
# for the lines to be drawed upwards


### Austria - Cyprus

world.eu <- world.eu1
a <- rep(1, length(c.eu))

cor1 <<- 1 #Austria
cor2 <<- 5 #Cyprus

a[cor1] <- 2
a[cor2] <- 3

b <- cbind(c.eu, a)
dataframe2 <- data.frame(b)  
colnames(dataframe2) <-c("Country.Code", "score")

matched.indices.eu <- match(world.eu at data[, "ISO3"], dataframe2[, "Country.Code"])
world.eu at data <- data.frame(world.eu at data, dataframe2[matched.indices.eu, ])
world.f.eu <- fortify(world.eu, region = "ISO3")
world.m.eu <- merge(world.f.eu, world.eu at data, by.x = "id", by.y = "Country.Code")

myCurve<-curveGrob(0, 0, 1, 1, default.units = "npc",
                   curvature = -0.3, angle = 60, ncp = 20, shape = 1,
                   square = FALSE, squareShape = 1,
                   inflect = FALSE, arrow = arrow(length = unit(0.15, "inches"), type="closed"), open = TRUE,
                   debug = FALSE,
                   name = NULL, gp = gpar(col="blue", lwd=10, lineend="round", fill="blue"), vp = NULL)

myCurve2<-curveGrob(0, 0, 1, 1, default.units = "npc",
                    curvature = 0.3, angle = 60, ncp = 20, shape = 1,
                    square = FALSE, squareShape = 1,
                    inflect = FALSE, arrow = arrow(length = unit(0.15, "inches"), type="closed", ends="first"), 
                    open = TRUE, debug = FALSE,
                    name = NULL, gp = gpar(col="magenta", lwd=2, lineend="round", fill="magenta"), vp = NULL)

ggplot(world.m.eu, aes(long, lat, group = group))+
  geom_polygon(aes(fill = world.m.eu$score),show.legend=FALSE)+ 
  geom_polygon(data = world.m.eu, aes(long,lat), 
               fill="NA", color = "white",  size=0.01) +
  coord_cartesian(xlim = c(-17, 37), ylim = c(34, 72))+ 
  scale_fill_manual(values = c("lightblue", "blue", "magenta")) +
  annotation_custom(grob=myCurve, xmin=cent[cor1,2], xmax=cent[cor2,2],
                    ymin=cent[cor1,1], ymax=cent[cor2,1]) +
  annotation_custom(grob=myCurve2,xmin=cent[cor2,2], xmax=cent[cor1,2],
                    ymin=cent[cor2,1], ymax=cent[cor1,1])


### France - Greece

world.eu <- world.eu1
a <- rep(1, length(c.eu))

cor1 <<-10 #France 
cor2 <<-12 #Greece

a[cor1] <- 2
a[cor2] <- 3

b <- cbind(c.eu, a)
dataframe2 <- data.frame(b)  
colnames(dataframe2) <-c("Country.Code", "score")

matched.indices.eu <- match(world.eu at data[, "ISO3"], dataframe2[, "Country.Code"])
world.eu at data <- data.frame(world.eu at data, dataframe2[matched.indices.eu, ])
world.f.eu <- fortify(world.eu, region = "ISO3")
world.m.eu <- merge(world.f.eu, world.eu at data, by.x = "id", by.y = "Country.Code")

myCurve<-curveGrob(0, 0, 1, 1, default.units = "npc",
                   curvature = -0.3, angle = 60, ncp = 20, shape = 1,
                   square = FALSE, squareShape = 1,
                   inflect = FALSE, arrow = arrow(length = unit(0.15, "inches"), type="closed"), open = TRUE,
                   debug = FALSE,
                   name = NULL, gp = gpar(col="blue", lwd=10, lineend="round", fill="blue"), vp = NULL)

myCurve2<-curveGrob(0, 0, 1, 1, default.units = "npc",
                    curvature = 0.3, angle = 60, ncp = 20, shape = 1,
                    square = FALSE, squareShape = 1,
                    inflect = FALSE, arrow = arrow(length = unit(0.15, "inches"), type="closed", ends="first"), 
                    open = TRUE, debug = FALSE,
                    name = NULL, gp = gpar(col="magenta", lwd=2, lineend="round", fill="magenta"), vp = NULL)

ggplot(world.m.eu, aes(long, lat, group = group))+
  geom_polygon(aes(fill = world.m.eu$score),show.legend=FALSE)+ 
  geom_polygon(data = world.m.eu, aes(long,lat), 
               fill="NA", color = "white",  size=0.01) +
  coord_cartesian(xlim = c(-17, 37), ylim = c(34, 72))+ 
  scale_fill_manual(values = c("lightblue", "blue", "magenta")) +
  annotation_custom(grob=myCurve, xmin=cent[cor1,2], xmax=cent[cor2,2],
                    ymin=cent[cor1,1], ymax=cent[cor2,1]) +
  annotation_custom(grob=myCurve2,xmin=cent[cor2,2], xmax=cent[cor1,2],
                    ymin=cent[cor2,1], ymax=cent[cor1,1])

###
-------------- next part --------------
A non-text attachment was scrubbed...
Name: arrows.png
Type: image/png
Size: 5692 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/e980bd99/attachment-0005.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: shiny_pic.png
Type: image/png
Size: 89577 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/e980bd99/attachment-0006.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Italy-Poland.png
Type: image/png
Size: 11819 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/e980bd99/attachment-0007.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Austria-Cyprus.png
Type: image/png
Size: 12047 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/e980bd99/attachment-0008.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: France-Greece.png
Type: image/png
Size: 11894 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/e980bd99/attachment-0009.png>

From zilefacelvis at yahoo.com  Thu Jan 21 00:33:35 2016
From: zilefacelvis at yahoo.com (Zilefac Elvis)
Date: Wed, 20 Jan 2016 23:33:35 +0000 (UTC)
Subject: [R] Splitting strings in data files R
In-Reply-To: <E227A155-2813-4933-A139-55244509249F@comcast.net>
References: <E227A155-2813-4933-A139-55244509249F@comcast.net>
Message-ID: <568044956.8908335.1453332815846.JavaMail.yahoo@mail.yahoo.com>

I did not want to include attachments but as they are requested I am attaching the original files.
File1=dx701S001

File2= dt402DAF0

I read them into R using:

temp1 = list.files(pattern="*.txt") # list all text file names in your working directory 
myfiles = lapply(temp1, read.delim)# 

The started processing them with:

res<-lapply(temp1,function(x) {con <- file(x);Lines1<- readLines(con);close(con); 
Lines2<-Lines1[-1]; 
Lines3<- str_split(Lines2,"-9999.99M")})

Thanks
AT


 



On Wednesday, January 20, 2016 4:47 PM, David Winsemius <dwinsemius at comcast.net> wrote:

> On Jan 20, 2016, at 12:53 PM, Zilefac Elvis via R-help <r-help at r-project.org> wrote:
> 
> 
> 
> 
> Please I need help processing  files with strings in R. All the files have two patterns (thus,examine separately):

You do need help, that much is clear. But the first thing to do is retrace your initial data-entry steps. You have used the wrong read-function. The data in the input file either whitespace-separated (or fixed width format and you apparently thought it was a CSV-file.

Asking us to no work with this mess is just unreasonable. Post the original input file.

-- 
David.


> Pattern 1 (see file1 below): Delete Lines 1,2 & 4 in file1. Line 3 contains the column names. Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with with NA.
> 
> File1 output format should be: Year Month Day_1 Day_2 ... Day_31  ## so all months should 31 days. Months with <31 days should have NA where appropraite (e.g. Feb 30=NA, 31=NA)
> 
> Pattern 2 (see file2 below): Delete Line 1 in file2.Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with withNA. File2 has no column names. Please do not include any. 
> File2 output format: Year Month Day_1 Day_2 ... Day_31 but no column names
> 
> Here is a simple reproducible example for both files/cases: 
> 
> 
> file1=list(df1,df1)df1=list(structure(list(X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014 =structure(c(20L,19L,21L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,16L,17L,18L),.Label =c(" 1918  7 -9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M  23.6a  25.9a  25.8a  24.9a  24.9a  29.6a  27.4a  24.5a  28.5a  28.5a  30.1a  25.3a  28.5a  19.6a  24.1a"," 1918  8   23.7a  18.6a  17.6a  19.0a  23.7a  24.7a  18.6a  22.6a  20.1a  21.4a  22.6a  24.9a  24.1a  23.2a  22.0a  17.6a  19.0a  19.0a  23.7a  24.1a  24.9a  27.9a  26.2a  22.6a  24.0a  25.4a  21.4a  24.4a  19.0a  22.6a  23.7a"," 1918  9   22.0a  22.0a  24.0a  19.0a  14.4a  11.2a  17.1a  18.1a  19.0a  12.0a  13.5a   9.6a  11.2a  10.7a  18.1a  18.1a  16.3a  14.4a  14.3a  15.9a  10.1a   9.8a  11.3a  11.4a  13.6a  14.4a   9.3a   9.6a   9.2a   8.4a-9999!
> .9M"," 1918 10    9.3a   9.5a  11.3a  10.2a   9.9a-9999.9M   4.6a-9999.9M   9.8a  13.6a  17.0a  15.2a  15.1a  15.9a   8.1a   9.3a   8.8a   6.0a   8.7a   9.8a   9.8a  10.7a  11.3a   9.5a  10.7a-9999.9M  10.7a  16.9a  17.1a  10.7a  12.7a"," 1918 11    8.8a-9999.9M   4.0a   3.4a   4.0a   6.6a   4.0a   7.3a   8.1a   7.3a   2.5a   3.4a   7.7a   6.1a   2.2a   4.0a   4.6a   2.5a   2.2a   1.6a   2.2a   3.0a  -3.4a   2.5a   1.6a  -3.4a   2.1a   0.0a   2.6a   0.6a-9999.9M"," 1918 12  -10.1a  -8.3a  -6.3a  -5.5a  -5.1a  -7.2a-9999.9M  -3.4a  -2.2a  -5.5a  -6.0a  -3.4a   0.6a   3.0a   4.7a   0.6a  -5.0a  -6.4a  -5.9a  -2.2a   1.2a   4.0a   5.3a-9999.9M  -2.2a  -5.5a  -7.5a  -9.6a  -7.3a  -6.6a-9999.9M"," 1919  1    2.5a   0.0a  -7.3a  -6.7a  -6.6a  -9.2a  -5.9a  -0.7a  -2.9a  -13.2a  -8.0a  -17.1a  -7.4a  -4.0a  -5.5a   0.6a  -7.1a  -5.5a  -2.2a  -7.6a  -7.0a  -3.4a  -2.2a  -6.7a  -8.0a  -2.9a  -1.5a  -5.9a  -5.5a  -5.8a  -3.4a"," 1919  2 -9999.9M   0.0a  -4.0a  -3.4a  -1.5a  -2.1a  -3!
> .4a  -7.2a  -2.8a  -5.5a  -6.7a  -5.1a  -2.1a  -2.1a   1.2a  -2.1a  -5
> .9a  -2.8a  -4.5a  -4.5a  -3.4a   2.1a   0.0a   0.0a   1.2a  -2.1a  -8.3a  -6.6a-9999.9M-9999.9M-9999.9M"," 1919  3 -9999.9M   0.0a   1.6a   1.7a  -5.1a  -6.7a  -5.1a  -3.4a  -2.0a   1.2a   3.4a   1.2a  -8.3a  -7.9a  -3.4a  -2.0a   1.2a   3.4a   6.6a   1.2a   6.6a   1.2a   6.6a   6.6a   3.4a  10.7a   6.6a   6.6a   0.6a   0.6a  -6.8a"," 1919  4   -5.9a  -3.4a   2.1a   3.4a   3.0a   3.0a   8.1a   8.1a   6.0a   2.1a   6.6a   8.5a   6.0a   1.7a   4.7a   2.4a   2.1a   8.5a   1.2a   1.7a   9.6a   8.6a  12.8a   9.5a  -2.8a   9.8a   4.7a  10.7a   6.6a  11.2a-9999.9M"," 1919  5   16.4a   8.5a   9.4a   6.0a  10.7a   9.8a   8.5a  13.6a  14.4a-9999.9M  16.4a  19.0a  23.2a  16.9a  17.0a  19.6a  11.3a   9.4a  12.1a  17.2a  15.2a  17.0a  15.2a  17.5a  10.2a  22.6a  14.5a  22.0a  24.9a  23.8a  19.0a"," 1919  6   17.7a  25.4a  31.2a  25.3a  26.8a  22.0a  15.8a  19.0a  12.7a  19.6a  19.0a  24.5a  25.1a  27.4a  26.8a  19.0a  20.8a  26.8a  27.9a  25.8a  20.1a  17.7a  19.0a  32.4a  30.7a  22.6a !
>  19.0a  13.6a  17.5a  24.1a-9999.9M"," 1919  7   23.7a  24.4a  27.9a  29.6a  23.7a  21.3a  23.7a  20.1a  23.7a  21.3a  17.0a  17.8a  23.7a  27.4a  18.2a  23.2a  24.5a  26.2a  25.8a  27.9a  29.0a  25.3a  25.1a  23.9a  22.6a  23.9a  20.8a  25.8a  20.1a  23.2a  23.7a"," 1919  8   20.8a  18.2a  20.1a  20.1a  25.1a  20.8a  24.6a  18.5a  17.6a  22.0a  24.0a  23.2a  24.0a  24.0a  20.8a  24.0a  23.7a  23.8a  17.1a  23.8a  24.6a  23.8a  19.6a  24.0a  24.0a  16.9a  18.2a  18.6a  18.6a  23.2a  20.8a"," 1919  9   24.0a  21.3a  24.4a  18.1a  19.0a  19.0a  17.7a  11.4a  10.7a  12.7a  15.2a  15.2a  18.6a  12.7a  15.2a  10.1a  12.0a  12.7a  19.6a  18.5a  28.5a  28.5a  10.7a  14.5a  15.8a  11.3a  11.3a  20.8a  23.2a  11.3a-9999.9M"," 1919 10   11.3a   8.2a   8.2a  16.4a  10.7a  17.5a   7.7a   6.0a  11.3a   7.3a  12.1a   7.7a  10.2a  15.9a  18.2a   9.0a  10.7a   9.8a   8.2a   7.3a   7.7a   8.9a   9.5a  12.1a  10.2a  10.2a   4.0a  10.7a   2.9a   5.3a   3.0a"," 1919 11    8.2a   2.2a   1.2a   !
> 2.6a   1.7a   2.6a   6.1a   8.2a   7.7a   5.3a   4.7a   8.9a   4.7a  
> 1.7a  -4.0a   2.2a   7.7a   7.7a   0.6a  -4.5a   2.6a   3.4a   2.5a  -3.4a  -5.9a  -5.1a  -5.5a  -6.4a   8.9a   3.0a-9999.9M"," 1919 12   -4.0a  -9.2a  -10.5a  -5.1a  -4.5a  -6.9a  -4.0a  -4.0a   3.0a   2.2a  -9.2a  -3.4a   5.3a  -6.4a  -6.9a  -20.4a  -20.4a  -17.6a  -10.5a  -13.8a  -8.7a  -3.4a  -2.9a  -4.5a  -5.5a  -5.5a  -2.9a  -0.8a  -10.1a  -6.9a  -5.9a"," Year Mo  Day 01  Day 02  Day 03  Day 04  Day 05  Day 06  Day 07  Day 08  Day 09  Day 10  Day 11  Day 12  Day 13  Day 14  Day 15  Day 16  Day 17  Day 18  Day 19  Day 20  Day 21  Day 22  Day 23  Day 24  Day 25  Day 26  Day 27  Day 28  Day 29  Day 30  Day 31","7011982,   DONNACONA    , QC, station jointe   , Temperature quotidienne maximale homogeneisee, Deg Celcius, Mise a jour jusqu a decembre 2014","Annee Mo Jour 01 Jour 02 Jour 03 Jour 04 Jour 05 Jour 06 Jour 07 Jour 08 Jour 09 Jour 10 Jour 11 Jour 12 Jour 13 Jour 14 Jour 15 Jour 16 Jour 17 Jour 18 Jour 19 Jour 20 Jour 21 Jour 22 Jour 23 Jour 24 Jour 25 Jour 26 Jour !
> 27 Jour 28 Jour 29 Jour 30 Jour 31"),class ="factor")),.Names ="X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014",class ="data.frame",row.names =c(NA,-21L)))
> 
> 
> 
> 
> file2=list(df2,df2)df2=list(structure(list(X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014 =structure(1:24,.Label =c("1948  1 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  2 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  3 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99!
> M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  4 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  5 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M   0.00    0.00    0.21T   0.21T   1.69    0.21T   0.21T   0.21T   0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    1.39    0.00    0.21T","1948  6    0.00    0.00    0.30T   3.34T   0.21T   0.00    0.00    7.19T   0.21T   1.04    0.00    4.29    1.69    0.21T   0.00    0.00    0.21T   0.65    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.21T-9999.99M","1948  7    0.21T   0.51T   0.00    2.74    0.00    0.00    0.00    0.00    0.00    1.05    0.00    !
> 0.00    1.57    1.57    2.30    0.74T   0.00    0.30T   0.74    0.53  
>  0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.53    3.34    2.30   13.43 ","1948  8    0.30T   2.61    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.53    0.21T   0.65    3.65    3.25    0.21T   3.90    0.21T   0.21T   0.21T   0.30T   0.21T   0.21T   0.21T   0.00    0.21T   0.65    1.95    0.21T   0.21T","1948  9    0.00    0.21T   0.21T   0.21T   0.21T   0.69T   7.54    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   1.04    0.00    0.00    0.00    0.00    7.28    4.68    2.34    1.95    3.90    1.30    0.21T-9999.99M","1948 10    1.04    0.00    0.00    1.69    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.65    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T","1948 11    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   1.04    0.21T   0.00    0.00    1.69    0.21T   0.21T !
>   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1948 12    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 ","1949  1    0.00    0.00    0.00    0.00    0.00    0.39    0.65    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.65    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.00 ","1949  2    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00 -9999.99M-9999.99M-9999.99M","1949  3    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.!
> 21T   0.00    1.69    0.00    1.04    1.69    0.65    0.21T   0.51T  
> 0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.21T   0.21T   0.00 ","1949  4    0.00    0.00    0.39    0.21T   0.00    0.00    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.65    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1949  5    0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.39    0.39    0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.39    0.39    0.21T   1.04 ","1949  6    0.39    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.65    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.51T   0.21T   0.00    0.39    0.21T   0.00    0.21T   0.21T   0.39    0.39    0.21T   0.00    0.00    0.00    0.00 -9999.99M","1949  7    0.00    0.00    0.53    0.51T   0.51T   0.21T   0.51T   0.30T   0.00    0.00   !
>  0.00    0.00    0.00    0.00    0.30T   0.30T   0.00    0.00    0.00    0.00    0.51T   0.51T   6.25   22.63    0.00    0.51T   0.21T   0.30T   0.30T   0.00    0.00 ","1949  8    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.51T   0.51T   0.00    0.30T   0.30T   0.00    0.00    6.56    0.00    0.00    0.00    0.00    1.05    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.51T   0.00    0.30T   0.21T   0.21T","1949  9    0.30T   0.30T   0.00    0.39    0.39    0.21T   0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T-9999.99M","1949 10    0.21T   0.00    0.00    0.00    1.04    0.39    0.65    0.21T   0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.65    0.65    0.21T   0.65    0.00    0.00    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.00    0.00    0.00    0.00 ","1949 11    0.00    0.00    0.00    0.00  !
>   1.04    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00
>    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T-9999.99M","1949 12    0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.21T   0.39    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.21T   0.00    0.00 "),class ="factor")),.Names ="X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014",class ="data.frame",row.names =c(NA,-24L)))
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: dt402DAF0.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/68fdd062/attachment-0002.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: dx701S001.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160120/68fdd062/attachment-0003.txt>

From dwinsemius at comcast.net  Thu Jan 21 03:33:55 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 20 Jan 2016 18:33:55 -0800
Subject: [R] Splitting strings in data files R
In-Reply-To: <568044956.8908335.1453332815846.JavaMail.yahoo@mail.yahoo.com>
References: <E227A155-2813-4933-A139-55244509249F@comcast.net>
	<568044956.8908335.1453332815846.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <40B650B1-9A7B-40C1-AF9F-0D2DFAE0F2E3@comcast.net>


> On Jan 20, 2016, at 3:33 PM, Zilefac Elvis <zilefacelvis at yahoo.com> wrote:
> 
> I did not want to include attachments but as they are requested I am attaching the original files.
> File1=dx701S001
> 
> File2= dt402DAF0

These are fixed width files. The "upper-left corner of the file looks like this in a text editor:

402DAF0,LEADER AIRPORT           ,SK,station joined    ,Daily adjusted precipitation, mm, Updated to December 2014
1923  1 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-
1923  2 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-
1923  3 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-
1923  4     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00  
1923  5     0.00     0.00     0.00     0.00    10.83     0.00     0.00     0.00     0.00     0.00     0.00     0.00  
1923  6     6.87     0.00     0.00     0.00     0.00     0.00     5.52     0.00     0.00     0.00     0.00     0.00  
1923  7     0.00     0.00     0.00     0.00     2.09     0.00     7.91     1.57     3.65     0.00     0.00     0.00  
1923  8     0.00     0.00     0.00     0.00     0.00     0.00     0.00     8.12     0.00     0.00     0.00     0.00  
1923  9     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00  
1923 10     0.00     0.00     0.00     0.00     0.00     0.00     0.00     4.17     0.00     0.00     0.00     0.00  
1923 11     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00  
1923 12     0.11T    0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00  
1924  1     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     6.27     0.00  
1924  2     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00  

So the convention of the file authors is to define location boundaries for numeric values (eight characters wide, after the first two) and then the interleaved columns (one character wide) are some sort of annotation. You can see two such annotation types here but I can see several others (T,A,C at the very least) in a text editor. One is clearly "M" for missing and the other that can be seen here is "T" of unknown import. Clearly -9999.99 is a missing value.

You need to use the read.fwf function in package foreign (shipped with every full copy of R). It's possible that you also want to loop through these files with readLines just to get the first line, but it's clearly not a column header line.

-- 
David.

> 
> I read them into R using:
> 
> temp1 = list.files(pattern="*.txt") # list all text file names in your working directory 
> myfiles = lapply(temp1, read.delim)# 
> 
> The started processing them with:
> 
> res<-lapply(temp1,function(x) {con <- file(x);Lines1<- readLines(con);close(con); 
> Lines2<-Lines1[-1]; 
> Lines3<- str_split(Lines2,"-9999.99M")})
> 
> Thanks
> AT
> 
> 
> 
> 
> 
> 
> On Wednesday, January 20, 2016 4:47 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
>> On Jan 20, 2016, at 12:53 PM, Zilefac Elvis via R-help <r-help at r-project.org> wrote:
>> 
>> 
>> 
>> 
>> Please I need help processing  files with strings in R. All the files have two patterns (thus,examine separately):
> 
> You do need help, that much is clear. But the first thing to do is retrace your initial data-entry steps. You have used the wrong read-function. The data in the input file either whitespace-separated (or fixed width format and you apparently thought it was a CSV-file.
> 
> Asking us to no work with this mess is just unreasonable. Post the original input file.
> 
> -- 
> David.
> 
> 
>> Pattern 1 (see file1 below): Delete Lines 1,2 & 4 in file1. Line 3 contains the column names. Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with with NA.
>> 
>> File1 output format should be: Year Month Day_1 Day_2 ... Day_31  ## so all months should 31 days. Months with <31 days should have NA where appropraite (e.g. Feb 30=NA, 31=NA)
>> 
>> Pattern 2 (see file2 below): Delete Line 1 in file2.Then find anything as.character and delete. Please do not delete any values (e.g. delete T in 0.21T). Also find -999.99M,-999.99 and replace with withNA. File2 has no column names. Please do not include any. 
>> File2 output format: Year Month Day_1 Day_2 ... Day_31 but no column names
>> 
>> Here is a simple reproducible example for both files/cases: 
>> 
>> 
>> file1=list(df1,df1)df1=list(structure(list(X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014 =structure(c(20L,19L,21L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,16L,17L,18L),.Label =c(" 1918  7 -9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M-9999.9M  23.6a  25.9a  25.8a  24.9a  24.9a  29.6a  27.4a  24.5a  28.5a  28.5a  30.1a  25.3a  28.5a  19.6a  24.1a"," 1918  8   23.7a  18.6a  17.6a  19.0a  23.7a  24.7a  18.6a  22.6a  20.1a  21.4a  22.6a  24.9a  24.1a  23.2a  22.0a  17.6a  19.0a  19.0a  23.7a  24.1a  24.9a  27.9a  26.2a  22.6a  24.0a  25.4a  21.4a  24.4a  19.0a  22.6a  23.7a"," 1918  9   22.0a  22.0a  24.0a  19.0a  14.4a  11.2a  17.1a  18.1a  19.0a  12.0a  13.5a   9.6a  11.2a  10.7a  18.1a  18.1a  16.3a  14.4a  14.3a  15.9a  10.1a   9.8a  11.3a  11.4a  13.6a  14.4a   9.3a   9.6a   9.2a   8.4a-9999!
>> .9M"," 1918 10    9.3a   9.5a  11.3a  10.2a   9.9a-9999.9M   4.6a-9999.9M   9.8a  13.6a  17.0a  15.2a  15.1a  15.9a   8.1a   9.3a   8.8a   6.0a   8.7a   9.8a   9.8a  10.7a  11.3a   9.5a  10.7a-9999.9M  10.7a  16.9a  17.1a  10.7a  12.7a"," 1918 11    8.8a-9999.9M   4.0a   3.4a   4.0a   6.6a   4.0a   7.3a   8.1a   7.3a   2.5a   3.4a   7.7a   6.1a   2.2a   4.0a   4.6a   2.5a   2.2a   1.6a   2.2a   3.0a  -3.4a   2.5a   1.6a  -3.4a   2.1a   0.0a   2.6a   0.6a-9999.9M"," 1918 12  -10.1a  -8.3a  -6.3a  -5.5a  -5.1a  -7.2a-9999.9M  -3.4a  -2.2a  -5.5a  -6.0a  -3.4a   0.6a   3.0a   4.7a   0.6a  -5.0a  -6.4a  -5.9a  -2.2a   1.2a   4.0a   5.3a-9999.9M  -2.2a  -5.5a  -7.5a  -9.6a  -7.3a  -6.6a-9999.9M"," 1919  1    2.5a   0.0a  -7.3a  -6.7a  -6.6a  -9.2a  -5.9a  -0.7a  -2.9a  -13.2a  -8.0a  -17.1a  -7.4a  -4.0a  -5.5a   0.6a  -7.1a  -5.5a  -2.2a  -7.6a  -7.0a  -3.4a  -2.2a  -6.7a  -8.0a  -2.9a  -1.5a  -5.9a  -5.5a  -5.8a  -3.4a"," 1919  2 -9999.9M   0.0a  -4.0a  -3.4a  -1.5a  -2.1a  -3!
>> .4a  -7.2a  -2.8a  -5.5a  -6.7a  -5.1a  -2.1a  -2.1a   1.2a  -2.1a  -5
>> .9a  -2.8a  -4.5a  -4.5a  -3.4a   2.1a   0.0a   0.0a   1.2a  -2.1a  -8.3a  -6.6a-9999.9M-9999.9M-9999.9M"," 1919  3 -9999.9M   0.0a   1.6a   1.7a  -5.1a  -6.7a  -5.1a  -3.4a  -2.0a   1.2a   3.4a   1.2a  -8.3a  -7.9a  -3.4a  -2.0a   1.2a   3.4a   6.6a   1.2a   6.6a   1.2a   6.6a   6.6a   3.4a  10.7a   6.6a   6.6a   0.6a   0.6a  -6.8a"," 1919  4   -5.9a  -3.4a   2.1a   3.4a   3.0a   3.0a   8.1a   8.1a   6.0a   2.1a   6.6a   8.5a   6.0a   1.7a   4.7a   2.4a   2.1a   8.5a   1.2a   1.7a   9.6a   8.6a  12.8a   9.5a  -2.8a   9.8a   4.7a  10.7a   6.6a  11.2a-9999.9M"," 1919  5   16.4a   8.5a   9.4a   6.0a  10.7a   9.8a   8.5a  13.6a  14.4a-9999.9M  16.4a  19.0a  23.2a  16.9a  17.0a  19.6a  11.3a   9.4a  12.1a  17.2a  15.2a  17.0a  15.2a  17.5a  10.2a  22.6a  14.5a  22.0a  24.9a  23.8a  19.0a"," 1919  6   17.7a  25.4a  31.2a  25.3a  26.8a  22.0a  15.8a  19.0a  12.7a  19.6a  19.0a  24.5a  25.1a  27.4a  26.8a  19.0a  20.8a  26.8a  27.9a  25.8a  20.1a  17.7a  19.0a  32.4a  30.7a  22.6a !
>> 19.0a  13.6a  17.5a  24.1a-9999.9M"," 1919  7   23.7a  24.4a  27.9a  29.6a  23.7a  21.3a  23.7a  20.1a  23.7a  21.3a  17.0a  17.8a  23.7a  27.4a  18.2a  23.2a  24.5a  26.2a  25.8a  27.9a  29.0a  25.3a  25.1a  23.9a  22.6a  23.9a  20.8a  25.8a  20.1a  23.2a  23.7a"," 1919  8   20.8a  18.2a  20.1a  20.1a  25.1a  20.8a  24.6a  18.5a  17.6a  22.0a  24.0a  23.2a  24.0a  24.0a  20.8a  24.0a  23.7a  23.8a  17.1a  23.8a  24.6a  23.8a  19.6a  24.0a  24.0a  16.9a  18.2a  18.6a  18.6a  23.2a  20.8a"," 1919  9   24.0a  21.3a  24.4a  18.1a  19.0a  19.0a  17.7a  11.4a  10.7a  12.7a  15.2a  15.2a  18.6a  12.7a  15.2a  10.1a  12.0a  12.7a  19.6a  18.5a  28.5a  28.5a  10.7a  14.5a  15.8a  11.3a  11.3a  20.8a  23.2a  11.3a-9999.9M"," 1919 10   11.3a   8.2a   8.2a  16.4a  10.7a  17.5a   7.7a   6.0a  11.3a   7.3a  12.1a   7.7a  10.2a  15.9a  18.2a   9.0a  10.7a   9.8a   8.2a   7.3a   7.7a   8.9a   9.5a  12.1a  10.2a  10.2a   4.0a  10.7a   2.9a   5.3a   3.0a"," 1919 11    8.2a   2.2a   1.2a   !
>> 2.6a   1.7a   2.6a   6.1a   8.2a   7.7a   5.3a   4.7a   8.9a   4.7a  
>> 1.7a  -4.0a   2.2a   7.7a   7.7a   0.6a  -4.5a   2.6a   3.4a   2.5a  -3.4a  -5.9a  -5.1a  -5.5a  -6.4a   8.9a   3.0a-9999.9M"," 1919 12   -4.0a  -9.2a  -10.5a  -5.1a  -4.5a  -6.9a  -4.0a  -4.0a   3.0a   2.2a  -9.2a  -3.4a   5.3a  -6.4a  -6.9a  -20.4a  -20.4a  -17.6a  -10.5a  -13.8a  -8.7a  -3.4a  -2.9a  -4.5a  -5.5a  -5.5a  -2.9a  -0.8a  -10.1a  -6.9a  -5.9a"," Year Mo  Day 01  Day 02  Day 03  Day 04  Day 05  Day 06  Day 07  Day 08  Day 09  Day 10  Day 11  Day 12  Day 13  Day 14  Day 15  Day 16  Day 17  Day 18  Day 19  Day 20  Day 21  Day 22  Day 23  Day 24  Day 25  Day 26  Day 27  Day 28  Day 29  Day 30  Day 31","7011982,   DONNACONA    , QC, station jointe   , Temperature quotidienne maximale homogeneisee, Deg Celcius, Mise a jour jusqu a decembre 2014","Annee Mo Jour 01 Jour 02 Jour 03 Jour 04 Jour 05 Jour 06 Jour 07 Jour 08 Jour 09 Jour 10 Jour 11 Jour 12 Jour 13 Jour 14 Jour 15 Jour 16 Jour 17 Jour 18 Jour 19 Jour 20 Jour 21 Jour 22 Jour 23 Jour 24 Jour 25 Jour 26 Jour !
>> 27 Jour 28 Jour 29 Jour 30 Jour 31"),class ="factor")),.Names ="X7011982.....DONNACONA........QC..station.joined......Homogenized.daily.maximum.temperature..........Deg.Celcius...........Updated.to.December.2014",class ="data.frame",row.names =c(NA,-21L)))
>> 
>> 
>> 
>> 
>> file2=list(df2,df2)df2=list(structure(list(X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014 =structure(1:24,.Label =c("1948  1 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  2 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  3 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99!
>> M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  4 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M","1948  5 -9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M-9999.99M   0.00    0.00    0.21T   0.21T   1.69    0.21T   0.21T   0.21T   0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    1.39    0.00    0.21T","1948  6    0.00    0.00    0.30T   3.34T   0.21T   0.00    0.00    7.19T   0.21T   1.04    0.00    4.29    1.69    0.21T   0.00    0.00    0.21T   0.65    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.21T-9999.99M","1948  7    0.21T   0.51T   0.00    2.74    0.00    0.00    0.00    0.00    0.00    1.05    0.00    !
>> 0.00    1.57    1.57    2.30    0.74T   0.00    0.30T   0.74    0.53  
>> 0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.53    3.34    2.30   13.43 ","1948  8    0.30T   2.61    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.53    0.21T   0.65    3.65    3.25    0.21T   3.90    0.21T   0.21T   0.21T   0.30T   0.21T   0.21T   0.21T   0.00    0.21T   0.65    1.95    0.21T   0.21T","1948  9    0.00    0.21T   0.21T   0.21T   0.21T   0.69T   7.54    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   1.04    0.00    0.00    0.00    0.00    7.28    4.68    2.34    1.95    3.90    1.30    0.21T-9999.99M","1948 10    1.04    0.00    0.00    1.69    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.65    0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T","1948 11    0.00    0.00    0.00    0.21T   0.21T   0.00    0.21T   1.04    0.21T   0.00    0.00    1.69    0.21T   0.21T !
>>  0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1948 12    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00 ","1949  1    0.00    0.00    0.00    0.00    0.00    0.39    0.65    0.00    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.65    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.00 ","1949  2    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.00 -9999.99M-9999.99M-9999.99M","1949  3    0.00    0.00    0.00    0.00    0.00    0.00    0.21T   0.!
>> 21T   0.00    1.69    0.00    1.04    1.69    0.65    0.21T   0.51T  
>> 0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.21T   0.21T   0.00 ","1949  4    0.00    0.00    0.39    0.21T   0.00    0.00    0.39    0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.65    0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.00 -9999.99M","1949  5    0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.39    0.39    0.39    0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.39    0.39    0.21T   1.04 ","1949  6    0.39    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.65    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.51T   0.21T   0.00    0.39    0.21T   0.00    0.21T   0.21T   0.39    0.39    0.21T   0.00    0.00    0.00    0.00 -9999.99M","1949  7    0.00    0.00    0.53    0.51T   0.51T   0.21T   0.51T   0.30T   0.00    0.00   !
>> 0.00    0.00    0.00    0.00    0.30T   0.30T   0.00    0.00    0.00    0.00    0.51T   0.51T   6.25   22.63    0.00    0.51T   0.21T   0.30T   0.30T   0.00    0.00 ","1949  8    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.51T   0.51T   0.00    0.30T   0.30T   0.00    0.00    6.56    0.00    0.00    0.00    0.00    1.05    0.21T   0.21T   0.21T   0.00    0.21T   0.21T   0.51T   0.00    0.30T   0.21T   0.21T","1949  9    0.30T   0.30T   0.00    0.39    0.39    0.21T   0.00    0.00    0.21T   0.21T   0.00    0.00    0.00    0.21T   0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T-9999.99M","1949 10    0.21T   0.00    0.00    0.00    1.04    0.39    0.65    0.21T   0.00    0.00    0.21T   0.21T   0.21T   0.39    0.21T   0.65    0.65    0.21T   0.65    0.00    0.00    0.21T   0.00    0.21T   0.00    0.21T   0.21T   0.00    0.00    0.00    0.00 ","1949 11    0.00    0.00    0.00    0.00  !
>>  1.04    0.21T   0.00    0.00    0.21T   0.00    0.00    0.00    0.00
>>   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.21T-9999.99M","1949 12    0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.00    0.21T   0.21T   0.00    0.00    0.21T   0.39    0.00    0.00    0.00    0.00    0.21T   0.21T   0.21T   0.21T   0.21T   0.21T   0.00    0.00    0.00    0.00    0.21T   0.00    0.00 "),class ="factor")),.Names ="X250M001.MOULD.BAY.................NT.station.joined.....Daily.adjusted.precipitation..mm..Updated.to.December.2014",class ="data.frame",row.names =c(NA,-24L)))
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> <dt402DAF0.txt><dx701S001.txt>

David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Thu Jan 21 03:58:47 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 21 Jan 2016 13:58:47 +1100
Subject: [R] Fwd:
In-Reply-To: <AFF3ABFE-9372-47CC-B4A2-3204B7D2463F@yahoo.com>
References: <6F174D07-953F-40F7-AB7D-4641AF45913F@yahoo.com>
	<AFF3ABFE-9372-47CC-B4A2-3204B7D2463F@yahoo.com>
Message-ID: <CA+8X3fXn-5-10XtY_T6i3uF5=64g3zgWvZH1wBDEA8BJbpWgMA@mail.gmail.com>

Hi maryam,
I think you have just restricted yourself to zero and negative numbers for
the new cases. Well, I suppose there are imaginary numbers...

Jim


On Thu, Jan 21, 2016 at 8:26 AM, maryam firoozi via R-help <
r-help at r-project.org> wrote:

> Hello,
> i made a population about 4500 individual.this has two sex(female and
> male).they had pedigree.
> i wanted to enter new indiviual but their ID of indiviual mustnot be same
> perivous and their ID number mustnot be bigger than 4500.
> first population's ID number is 1:4500.
> how can i handel it?
>
> Sent from my iPhone
>
> Begin forwarded message:
>
> > From: maryam firoozi <firoozi_maryam6858 at yahoo.com>
> > Date: January 18, 2016 at 1:14:56 AM GMT+3:30
> > To: r-help at r-project.org
> >
> > hello,
> > we want to do genomic blup in r.i know that use pedigree package.
> > the formule is
> > gblup( P~1,data=ped[,c('ID','P')],M=M,lambda=1/h2-1)
> > P:phenotype variance
> > ped:pedigree
> > M: matrix marker or genotype
> > my ped has 4500 ID.but my M has 9000 individual.becasue i have two row
> for each ID in M matrix becasue each ID has two haplotype.how can i solve
> it.the formula didnt solve.
> > sincerely
> >
> >
> > Sent from my iPhone
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chl948 at mail.usask.ca  Thu Jan 21 05:08:05 2016
From: chl948 at mail.usask.ca (Chel Hee Lee)
Date: Wed, 20 Jan 2016 22:08:05 -0600
Subject: [R] strange answer when using 'aggregate()' with a formula
Message-ID: <56A059A5.2010702@mail.usask.ca>

Could you kindly test the following codes?  It is because I found 
strange answer when 'aggregate()' is used with a formula.

I am trying to count how many missing data entries are in each group.  
For this exercise, I created data as below:

 > tmp <- data.frame(grp=c(2,3,2,3), y=c(NA, 0.5, 3, 0.5))
 > tmp
   grp   y
1   2  NA
2   3 0.5
3   2 3.0
4   3 0.5

I see that observations (variable y) can be grouped into two groups 
(variable grp).  For group 2, y has NA and 3.0.  For group 3, y has 0.5 
and 0.5.  Hence, the number of missing values is 1 and 0 for group 2 and 
3, respectively.   This work can be done using 'aggregate()' in the 
'stats' package as below:

 > aggregate(x=tmp$y, by=list(grp=tmp$grp), function(x) sum(is.na(x)))
   grp x
1   2 1
2   3 0

A formula can be used as below:

 > aggregate(y~grp, data=tmp, function(x) sum(is.na(x)))
   grp y
1   2 0
2   3 0

What a surprise!  Is this a bug?  I would appreciate if you share the 
results after testing the codes.   Thank you so much for your helps in 
advance!

Chel Hee Lee


From jfox at mcmaster.ca  Thu Jan 21 07:52:36 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Thu, 21 Jan 2016 06:52:36 +0000
Subject: [R] strange answer when using 'aggregate()' with a formula
In-Reply-To: <56A059A5.2010702@mail.usask.ca>
References: <56A059A5.2010702@mail.usask.ca>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F4F8BC@FHSDB2D11-2.csu.mcmaster.ca>

Dear Chel Hee Lee,

With the formula method, the default na.action is na.omit; thus,

> aggregate(y~grp, data=tmp, function(x) sum(is.na(x)), na.action=na.pass)
  grp y
1   2 1
2   3 0

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Chel Hee Lee
> Sent: January 21, 2016 5:08 AM
> To: R-help at r-project.org
> Subject: [R] strange answer when using 'aggregate()' with a formula
> 
> Could you kindly test the following codes?  It is because I found strange answer
> when 'aggregate()' is used with a formula.
> 
> I am trying to count how many missing data entries are in each group.
> For this exercise, I created data as below:
> 
>  > tmp <- data.frame(grp=c(2,3,2,3), y=c(NA, 0.5, 3, 0.5))  > tmp
>    grp   y
> 1   2  NA
> 2   3 0.5
> 3   2 3.0
> 4   3 0.5
> 
> I see that observations (variable y) can be grouped into two groups (variable
> grp).  For group 2, y has NA and 3.0.  For group 3, y has 0.5 and 0.5.  Hence, the
> number of missing values is 1 and 0 for group 2 and
> 3, respectively.   This work can be done using 'aggregate()' in the
> 'stats' package as below:
> 
>  > aggregate(x=tmp$y, by=list(grp=tmp$grp), function(x) sum(is.na(x)))
>    grp x
> 1   2 1
> 2   3 0
> 
> A formula can be used as below:
> 
>  > aggregate(y~grp, data=tmp, function(x) sum(is.na(x)))
>    grp y
> 1   2 0
> 2   3 0
> 
> What a surprise!  Is this a bug?  I would appreciate if you share the
> results after testing the codes.   Thank you so much for your helps in
> advance!
> 
> Chel Hee Lee
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mkashif at uaf.edu.pk  Thu Jan 21 10:50:30 2016
From: mkashif at uaf.edu.pk (Muhammad  Kashif)
Date: Thu, 21 Jan 2016 09:50:30 +0000
Subject: [R] trimmed mean and Winsorized mean
Message-ID: <DB4PR07MB3792BEA6B288DB51E12136B94C30@DB4PR07MB379.eurprd07.prod.outlook.com>

Dear respected group members


who we calculate trimmed and Winsorized mean of data. can we calculate directly of any latest package is available for their calculation.

	[[alternative HTML version deleted]]


From ivan.calandra at univ-reims.fr  Thu Jan 21 10:57:10 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Thu, 21 Jan 2016 10:57:10 +0100
Subject: [R] trimmed mean and Winsorized mean
In-Reply-To: <DB4PR07MB3792BEA6B288DB51E12136B94C30@DB4PR07MB379.eurprd07.prod.outlook.com>
References: <DB4PR07MB3792BEA6B288DB51E12136B94C30@DB4PR07MB379.eurprd07.prod.outlook.com>
Message-ID: <56A0AB76.2020009@univ-reims.fr>

Regarding the trimmed mean, take a look at the arguments of the function 
mean():
?mean

No idea about the Winsorized mean, though.

HTH,
Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/

Le 21/01/2016 10:50, Muhammad Kashif a ?crit :
> Dear respected group members
>
>
> who we calculate trimmed and Winsorized mean of data. can we calculate directly of any latest package is available for their calculation.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lists at dewey.myzen.co.uk  Thu Jan 21 14:33:38 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Thu, 21 Jan 2016 13:33:38 +0000
Subject: [R] Fwd:
In-Reply-To: <AFF3ABFE-9372-47CC-B4A2-3204B7D2463F@yahoo.com>
References: <6F174D07-953F-40F7-AB7D-4641AF45913F@yahoo.com>
	<AFF3ABFE-9372-47CC-B4A2-3204B7D2463F@yahoo.com>
Message-ID: <56A0DE32.6040101@dewey.myzen.co.uk>

Hello Maryam

See below

On 20/01/2016 21:26, maryam firoozi via R-help wrote:
> Hello,
> i made a population about 4500 individual.

So you mean nearly 4500, not exactly 4500?

this has two sex(female and male).they had pedigree.
> i wanted to enter new indiviual but their ID of indiviual mustnot be same perivous and their ID number mustnot be bigger than 4500.
> first population's ID number is 1:4500.

If you tabulate ID you will get a table where all the entries should be 
1 except for the ID which you have not yet used which will be zero.

?tabulate

_NOT_ ?table

All you need to do then is find them

?which

should work

> how can i handel it?
>
> Sent from my iPhone
>
> Begin forwarded message:
>
>> From: maryam firoozi <firoozi_maryam6858 at yahoo.com>
>> Date: January 18, 2016 at 1:14:56 AM GMT+3:30
>> To: r-help at r-project.org
>>
>> hello,
>> we want to do genomic blup in r.i know that use pedigree package.
>> the formule is
>> gblup( P~1,data=ped[,c('ID','P')],M=M,lambda=1/h2-1)
>> P:phenotype variance
>> ped:pedigree
>> M: matrix marker or genotype
>> my ped has 4500 ID.but my M has 9000 individual.becasue i have two row for each ID in M matrix becasue each ID has two haplotype.how can i solve it.the formula didnt solve.
>> sincerely
>>
>>
>> Sent from my iPhone
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From lists at dewey.myzen.co.uk  Thu Jan 21 14:37:48 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Thu, 21 Jan 2016 13:37:48 +0000
Subject: [R] trimmed mean and Winsorized mean
In-Reply-To: <DB4PR07MB3792BEA6B288DB51E12136B94C30@DB4PR07MB379.eurprd07.prod.outlook.com>
References: <DB4PR07MB3792BEA6B288DB51E12136B94C30@DB4PR07MB379.eurprd07.prod.outlook.com>
Message-ID: <56A0DF2C.7020705@dewey.myzen.co.uk>

Dear Muhammad

You could try an internet search which should lead you to several 
packages on CRAN which meet your need for Winsorisation.

I used winsor cran but it may depend on what Google knows about you.

On 21/01/2016 09:50, Muhammad  Kashif wrote:
> Dear respected group members
>
>
> who we calculate trimmed and Winsorized mean of data. can we calculate directly of any latest package is available for their calculation.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From petr.pikal at precheza.cz  Thu Jan 21 14:40:41 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 21 Jan 2016 13:40:41 +0000
Subject: [R] trimmed mean and Winsorized mean
In-Reply-To: <DB4PR07MB3792BEA6B288DB51E12136B94C30@DB4PR07MB379.eurprd07.prod.outlook.com>
References: <DB4PR07MB3792BEA6B288DB51E12136B94C30@DB4PR07MB379.eurprd07.prod.outlook.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C500AA17@SRVEXCHMBX.precheza.cz>

Hi

Out of curiosity I tried to find how precise is searching for such simple task on internet.

This was my question in google

winsorized mean in r


Position 6
http://finzi.psych.upenn.edu/library/SciencePo/html/winsor.mean.html

Position 5
http://www.statisticalanalysisconsulting.com/measures-of-central-tendency-the-trimmed-mean-and-median/

Position 4
https://cran.r-project.org/web/packages/robustHD/robustHD.pdf

Position 3
https://stat.ethz.ch/pipermail/r-help/2009-August/402610.html

Position 2
http://www.r-bloggers.com/winsorization/

and the winner
http://www.personality-project.org/r/html/winsor.html

Which of those 6 hits (which you could get instantly) are not compliant with your question?

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Muhammad Kashif
> Sent: Thursday, January 21, 2016 10:51 AM
> To: Group R-help
> Subject: [R] trimmed mean and Winsorized mean
>
> Dear respected group members
>
>
> who we calculate trimmed and Winsorized mean of data. can we calculate
> directly of any latest package is available for their calculation.
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From jrkrideau at inbox.com  Thu Jan 21 15:01:16 2016
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 21 Jan 2016 06:01:16 -0800
Subject: [R] trimmed mean and Winsorized mean
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C500AA17@SRVEXCHMBX.precheza.cz>
References: <db4pr07mb3792bea6b288db51e12136b94c30@db4pr07mb379.eurprd07.prod.outlook.com>
Message-ID: <2284688A1E5.00000A3Cjrkrideau@inbox.com>

I think this depends on how well google knows you. It took me about 6 months when I first starting using R just to get google to figure out what r meant.  I was getting a lot if information on how to do correlations before that. 

"winsorized mean in r statistics" sounds safer for a new R user 

John Kane
Kingston ON Canada


> -----Original Message-----
> From: petr.pikal at precheza.cz
> Sent: Thu, 21 Jan 2016 13:40:41 +0000
> To: mkashif at uaf.edu.pk, r-help at r-project.org
> Subject: Re: [R] trimmed mean and Winsorized mean
> 
> Hi
> 
> Out of curiosity I tried to find how precise is searching for such simple
> task on internet.
> 
> This was my question in google
> 
> winsorized mean in r
> 
> 
> Position 6
> http://finzi.psych.upenn.edu/library/SciencePo/html/winsor.mean.html
> 
> Position 5
> http://www.statisticalanalysisconsulting.com/measures-of-central-tendency-the-trimmed-mean-and-median/
> 
> Position 4
> https://cran.r-project.org/web/packages/robustHD/robustHD.pdf
> 
> Position 3
> https://stat.ethz.ch/pipermail/r-help/2009-August/402610.html
> 
> Position 2
> http://www.r-bloggers.com/winsorization/
> 
> and the winner
> http://www.personality-project.org/r/html/winsor.html
> 
> Which of those 6 hits (which you could get instantly) are not compliant
> with your question?
> 
> Cheers
> Petr
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>> Muhammad Kashif
>> Sent: Thursday, January 21, 2016 10:51 AM
>> To: Group R-help
>> Subject: [R] trimmed mean and Winsorized mean
>> 
>> Dear respected group members
>> 
>> 
>> who we calculate trimmed and Winsorized mean of data. can we calculate
>> directly of any latest package is available for their calculation.
>> 
>>       [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho
> kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
> 
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
> strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
> zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
> adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
> p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen?
> zn?m?.
> 
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any
> manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to
> enter into any contracts on behalf of the company except for cases in
> which he/she is expressly authorized to do so in writing, and such
> authorization or power of attorney is submitted to the recipient or the
> person represented by the recipient, or the existence of such
> authorization is known to the recipient of the person represented by the
> recipient.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From dcarlson at tamu.edu  Thu Jan 21 15:39:14 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Thu, 21 Jan 2016 14:39:14 +0000
Subject: [R] Fwd:
In-Reply-To: <56A0DE32.6040101@dewey.myzen.co.uk>
References: <6F174D07-953F-40F7-AB7D-4641AF45913F@yahoo.com>
	<AFF3ABFE-9372-47CC-B4A2-3204B7D2463F@yahoo.com>
	<56A0DE32.6040101@dewey.myzen.co.uk>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6FCDE0@mb02.ads.tamu.edu>

Assuming Michael is correct, you can use setdiff():

> set.seed(42)
> current <- sample.int(4500, 4495) # All but 5 numbers used
> setdiff(1:4500, current) # Find which numbers are left
[1]  905 1252 2508 3192 4484

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael Dewey
Sent: Thursday, January 21, 2016 7:34 AM
To: maryam firoozi; r-help at r-project.org
Subject: Re: [R] Fwd:

Hello Maryam

See below

On 20/01/2016 21:26, maryam firoozi via R-help wrote:
> Hello,
> i made a population about 4500 individual.

So you mean nearly 4500, not exactly 4500?

this has two sex(female and male).they had pedigree.
> i wanted to enter new indiviual but their ID of indiviual mustnot be same perivous and their ID number mustnot be bigger than 4500.
> first population's ID number is 1:4500.

If you tabulate ID you will get a table where all the entries should be 
1 except for the ID which you have not yet used which will be zero.

?tabulate

_NOT_ ?table

All you need to do then is find them

?which

should work

> how can i handel it?
>
> Sent from my iPhone
>
> Begin forwarded message:
>
>> From: maryam firoozi <firoozi_maryam6858 at yahoo.com>
>> Date: January 18, 2016 at 1:14:56 AM GMT+3:30
>> To: r-help at r-project.org
>>
>> hello,
>> we want to do genomic blup in r.i know that use pedigree package.
>> the formule is
>> gblup( P~1,data=ped[,c('ID','P')],M=M,lambda=1/h2-1)
>> P:phenotype variance
>> ped:pedigree
>> M: matrix marker or genotype
>> my ped has 4500 ID.but my M has 9000 individual.becasue i have two row for each ID in M matrix becasue each ID has two haplotype.how can i solve it.the formula didnt solve.
>> sincerely
>>
>>
>> Sent from my iPhone
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From therneau at mayo.edu  Thu Jan 21 16:01:51 2016
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Thu, 21 Jan 2016 09:01:51 -0600
Subject: [R] Survival::coxph (clogit),
 survConcordance vs. summary(fit) concordance
In-Reply-To: <mailman.3090.1453342151.3776.r-help@r-project.org>
References: <mailman.3090.1453342151.3776.r-help@r-project.org>
Message-ID: <7b6d90$29b7db@ironport10.mayo.edu>

I read the digest form which puts me behind, plus the last 2 days have been solid meetings 
with an external advisory group so I missed the initial query.   Three responses.

1. The clogit routine sets the data up properly and then calls a stratified Cox model.  If 
you want the survConcordance routine to give the same answer, it also needs to know about 
the strata
     survConcordance (Surv(rep(1, 76L), resp) ~ predict(fit) + strata(ID), data=dat)
I'm not surprised that you get a very different answer with/without strata.

2. I've never thought of using a robust variance for the matched case/control model.  I'm 
having a hard time wrapping my head around what you would expect that to accomplish 
(statistically).  Subjects are already matched on someone from the same site, so where 
does a per-site effect creep in?  Assuming there is a good reason and I just don't see it 
(not an unwarranted assumption), I'm not aware of any work on what an appropriate variance 
would be for the concordance in that case.

3. I need to think about the large variance issue.

Terry Therneau


On 01/20/2016 08:09 PM, r-help-request at r-project.org wrote:
> Hi,
>
> I'm running conditional logistic regression with survival::clogit. I have
> "1-1 case-control" data, i.e., there is 1 case and 1 control in each strata.
>
> Model:
> fit <- clogit(resp ~ x1 + x2, strata(ID), cluster(site), method ="efron",
> data = dat)
> Where resp is 1's and 0's, and x1 and x2 are both continuous.
>
> Predictors are both significant. A snippet of summary(fit):
> Concordance= 0.763  (se = 0.5 )
> Rsquare= 0.304   (max possible= 0.5 )
> Likelihood ratio test= 27.54  on 2 df,   p=1.047e-06
> Wald test            = 17.19  on 2 df,   p=0.0001853
> Score (logrank) test = 17.43  on 2 df,   p=0.0001644,   Robust = 6.66
>   p=0.03574
>
> The concordance estimate seems good but the SE is HUGE.
>
> I get a very different estimate from the survConcordance function, which I
> know says computes concordance for a "single continuous covariate", but it
> runs on my model with 2 continuous covariates....
>
> survConcordance(Surv(rep(1, 76L), resp) ~ predict(fit), dat)
> n= 76
> Concordance= 0.9106648 se= 0.09365047
> concordant  discordant   tied.risk   tied.time    std(c-d)
>   1315.0000   129.0000     0.0000   703.0000   270.4626
>
> Are both of these concordance estimates valid but providing different
> information?
> Is one more appropriate for measuring "performance" (in the AUC sense) of
> conditional logistic models?
> Is it possible that the HUGE SE estimate represents a convergence problem
> (no warnings were thrown when fit the model), or is this model just useless?
>
> Thanks!


From joeceradini at gmail.com  Thu Jan 21 16:29:04 2016
From: joeceradini at gmail.com (Joe Ceradini)
Date: Thu, 21 Jan 2016 08:29:04 -0700
Subject: [R] Survival::coxph (clogit),
	survConcordance vs. summary(fit) concordance
In-Reply-To: <7b6d90$29b7db@ironport10.mayo.edu>
References: <mailman.3090.1453342151.3776.r-help@r-project.org>
	<7b6d90$29b7db@ironport10.mayo.edu>
Message-ID: <CAKq2vL517w+3Sq0FucbtkR_7Bxv7_xCx6ts_jUeJwfSFia1K3A@mail.gmail.com>

Thanks Terry!

I thought that since I was providing survConcordance with the model object
that the same formula would be applied. But I was obviously wrong. I just
ran survConcordance with the addition of the strata argument, as you
suggested, and got the same answer as summary(fit)....with the same scary
SE.

This is a wildlife habitat selection analysis. Each individual animal has
habitat features that they used (1) and habitat that was available but that
they did not use (0). The habitat that is available is different for each
individual, hence the need for strata(ID of individual). However, all the
habitat data are collected from multiple discrete sites and each site has
multiple individuals on it. For all these analyses of these data, I've
assumed that individuals within a site may be more correlated than
individuals between sites, hence addition of cluster(site).

I was able recalculate the same concordance estimate as summary(fit) by
estimating predicted probabilities using:
risk <- predict(fit, type='risk')
risk / (1+risk)
And then used a probability cut-off of 0.5 for whether an observed point
was correctly classified, which returned the same 0.76 as the concordance
estimate.
So, can I just think of this concordance as a classification table (or
confusion matrix) with a 0.5 threshold (thus classification error would be
(1 - 0.76)?
Was I mistaken in thinking concordance was more akin to AUC in
unconditional logistic regression?

Thanks.
Joe


On Thu, Jan 21, 2016 at 8:01 AM, Therneau, Terry M., Ph.D. <
therneau at mayo.edu> wrote:

> I read the digest form which puts me behind, plus the last 2 days have
> been solid meetings with an external advisory group so I missed the initial
> query.   Three responses.
>
> 1. The clogit routine sets the data up properly and then calls a
> stratified Cox model.  If you want the survConcordance routine to give the
> same answer, it also needs to know about the strata
>     survConcordance (Surv(rep(1, 76L), resp) ~ predict(fit) + strata(ID),
> data=dat)
> I'm not surprised that you get a very different answer with/without strata.
>
> 2. I've never thought of using a robust variance for the matched
> case/control model.  I'm having a hard time wrapping my head around what
> you would expect that to accomplish (statistically).  Subjects are already
> matched on someone from the same site, so where does a per-site effect
> creep in?  Assuming there is a good reason and I just don't see it (not an
> unwarranted assumption), I'm not aware of any work on what an appropriate
> variance would be for the concordance in that case.
>
> 3. I need to think about the large variance issue.
>
> Terry Therneau
>
>
>
> On 01/20/2016 08:09 PM, r-help-request at r-project.org wrote:
>
>> Hi,
>>
>> I'm running conditional logistic regression with survival::clogit. I have
>> "1-1 case-control" data, i.e., there is 1 case and 1 control in each
>> strata.
>>
>> Model:
>> fit <- clogit(resp ~ x1 + x2, strata(ID), cluster(site), method ="efron",
>> data = dat)
>> Where resp is 1's and 0's, and x1 and x2 are both continuous.
>>
>> Predictors are both significant. A snippet of summary(fit):
>> Concordance= 0.763  (se = 0.5 )
>> Rsquare= 0.304   (max possible= 0.5 )
>> Likelihood ratio test= 27.54  on 2 df,   p=1.047e-06
>> Wald test            = 17.19  on 2 df,   p=0.0001853
>> Score (logrank) test = 17.43  on 2 df,   p=0.0001644,   Robust = 6.66
>>   p=0.03574
>>
>> The concordance estimate seems good but the SE is HUGE.
>>
>> I get a very different estimate from the survConcordance function, which I
>> know says computes concordance for a "single continuous covariate", but it
>> runs on my model with 2 continuous covariates....
>>
>> survConcordance(Surv(rep(1, 76L), resp) ~ predict(fit), dat)
>> n= 76
>> Concordance= 0.9106648 se= 0.09365047
>> concordant  discordant   tied.risk   tied.time    std(c-d)
>>   1315.0000   129.0000     0.0000   703.0000   270.4626
>>
>> Are both of these concordance estimates valid but providing different
>> information?
>> Is one more appropriate for measuring "performance" (in the AUC sense) of
>> conditional logistic models?
>> Is it possible that the HUGE SE estimate represents a convergence problem
>> (no warnings were thrown when fit the model), or is this model just
>> useless?
>>
>> Thanks!
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Cooperative Fish and Wildlife Research Unit
Zoology and Physiology Dept.
University of Wyoming
JoeCeradini at gmail.com / 914.707.8506
wyocoopunit.org

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Jan 21 16:45:28 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 21 Jan 2016 07:45:28 -0800
Subject: [R] trimmed mean and Winsorized mean
In-Reply-To: <2284688A1E5.00000A3Cjrkrideau@inbox.com>
References: <db4pr07mb3792bea6b288db51e12136b94c30@db4pr07mb379.eurprd07.prod.outlook.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C500AA17@SRVEXCHMBX.precheza.cz>
	<2284688A1E5.00000A3Cjrkrideau@inbox.com>
Message-ID: <CAGxFJbQZ_nsuAeu_gCALM=Zoro4vdiHvF4cfJg4DCD+JDWW7qA@mail.gmail.com>

But DO NOT DO ANY OF THIS.

See the Robust Task View on CRAN and use the functionality of the
robust or robustbase package. Trimmed/winsorized means are ancient
technology; there is much better available today (and for the last 40
or so years, in fact).

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Jan 21, 2016 at 6:01 AM, John Kane <jrkrideau at inbox.com> wrote:
> I think this depends on how well google knows you. It took me about 6 months when I first starting using R just to get google to figure out what r meant.  I was getting a lot if information on how to do correlations before that.
>
> "winsorized mean in r statistics" sounds safer for a new R user
>
> John Kane
> Kingston ON Canada
>
>
>> -----Original Message-----
>> From: petr.pikal at precheza.cz
>> Sent: Thu, 21 Jan 2016 13:40:41 +0000
>> To: mkashif at uaf.edu.pk, r-help at r-project.org
>> Subject: Re: [R] trimmed mean and Winsorized mean
>>
>> Hi
>>
>> Out of curiosity I tried to find how precise is searching for such simple
>> task on internet.
>>
>> This was my question in google
>>
>> winsorized mean in r
>>
>>
>> Position 6
>> http://finzi.psych.upenn.edu/library/SciencePo/html/winsor.mean.html
>>
>> Position 5
>> http://www.statisticalanalysisconsulting.com/measures-of-central-tendency-the-trimmed-mean-and-median/
>>
>> Position 4
>> https://cran.r-project.org/web/packages/robustHD/robustHD.pdf
>>
>> Position 3
>> https://stat.ethz.ch/pipermail/r-help/2009-August/402610.html
>>
>> Position 2
>> http://www.r-bloggers.com/winsorization/
>>
>> and the winner
>> http://www.personality-project.org/r/html/winsor.html
>>
>> Which of those 6 hits (which you could get instantly) are not compliant
>> with your question?
>>
>> Cheers
>> Petr
>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>> Muhammad Kashif
>>> Sent: Thursday, January 21, 2016 10:51 AM
>>> To: Group R-help
>>> Subject: [R] trimmed mean and Winsorized mean
>>>
>>> Dear respected group members
>>>
>>>
>>> who we calculate trimmed and Winsorized mean of data. can we calculate
>>> directly of any latest package is available for their calculation.
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>> ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho
>> kopie vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
>> ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
>> strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
>> zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
>> adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
>> p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen?
>> zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are
>> intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its
>> sender. Delete the contents of this e-mail with all attachments and its
>> copies from your system.
>> If you are not the intended recipient of this e-mail, you are not
>> authorized to use, disseminate, copy or disclose this e-mail in any
>> manner.
>> The sender of this e-mail shall not be liable for any possible damage
>> caused by modifications of the e-mail or by delay with transfer of the
>> email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a
>> contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to
>> immediately accept such offer; The sender of this e-mail (offer) excludes
>> any acceptance of the offer on the part of the recipient containing any
>> amendment or variation.
>> - the sender insists on that the respective contract is concluded only
>> upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to
>> enter into any contracts on behalf of the company except for cases in
>> which he/she is expressly authorized to do so in writing, and such
>> authorization or power of attorney is submitted to the recipient or the
>> person represented by the recipient, or the existence of such
>> authorization is known to the recipient of the person represented by the
>> recipient.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> Can't remember your password? Do you need a strong and secure password?
> Use Password manager! It stores your passwords & protects your account.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pmassicotte at hotmail.com  Thu Jan 21 08:15:52 2016
From: pmassicotte at hotmail.com (Philippe Massicotte)
Date: Thu, 21 Jan 2016 08:15:52 +0100
Subject: [R] R editor for Mac
In-Reply-To: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
Message-ID: <n7q0j9$ib1$1@ger.gmane.org>

On 01/20/2016 07:22 PM, Christofer Bogaso wrote:
> Hi,
>
> Could you please suggest a good R editor for Mac OS X (10.7.5)
> Previously my operating system was Windows and there I used Notepad++,
> I really had very nice experience with it. However I dont see any Mac
> version is available for Mac.
>
> Appreciate your positive feedback.
>
> Thanks and regards,
>
Atom seems to be a good choice also.


From istazahn at gmail.com  Thu Jan 21 18:48:38 2016
From: istazahn at gmail.com (Ista Zahn)
Date: Thu, 21 Jan 2016 12:48:38 -0500
Subject: [R] R editor for Mac
In-Reply-To: <n7q0j9$ib1$1@ger.gmane.org>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<n7q0j9$ib1$1@ger.gmane.org>
Message-ID: <CA+vqiLEzDV2A5yp+YvOx__H9xA0Prd=Vdnp-bf1U_ZiWPHxzGw@mail.gmail.com>

On Jan 21, 2016 12:01 PM, "Philippe Massicotte" <pmassicotte at hotmail.com>
wrote:
>
> On 01/20/2016 07:22 PM, Christofer Bogaso wrote:
>>
>> Hi,
>>
>> Could you please suggest a good R editor for Mac OS X (10.7.5)
>> Previously my operating system was Windows and there I used Notepad++,
>> I really had very nice experience with it. However I dont see any Mac
>> version is available for Mac.
>>
>> Appreciate your positive feedback.
>>
>> Thanks and regards,
>>
> Atom seems to be a good choice also.

Is it? Which package(s) should I install to write and run R code in Atom?
Certainly I don't see anything useful out of the box.

Best,
Ista
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bob at rudis.net  Thu Jan 21 18:54:15 2016
From: bob at rudis.net (boB Rudis)
Date: Thu, 21 Jan 2016 12:54:15 -0500
Subject: [R] R editor for Mac
In-Reply-To: <CA+vqiLEzDV2A5yp+YvOx__H9xA0Prd=Vdnp-bf1U_ZiWPHxzGw@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<n7q0j9$ib1$1@ger.gmane.org>
	<CA+vqiLEzDV2A5yp+YvOx__H9xA0Prd=Vdnp-bf1U_ZiWPHxzGw@mail.gmail.com>
Message-ID: <CAJ4QxaODEzVo7H9ue02aMbXvLwYCM2HEm6zTUA0Md36Rx44=iA@mail.gmail.com>

Here you go Ista: https://atom.io/packages/repl (Atom rly isn't bad
for general purpose data sci needs, I still think RStudio is the best
environment for working with R projects).

On Thu, Jan 21, 2016 at 12:48 PM, Ista Zahn <istazahn at gmail.com> wrote:
> On Jan 21, 2016 12:01 PM, "Philippe Massicotte" <pmassicotte at hotmail.com>
> wrote:
>>
>> On 01/20/2016 07:22 PM, Christofer Bogaso wrote:
>>>
>>> Hi,
>>>
>>> Could you please suggest a good R editor for Mac OS X (10.7.5)
>>> Previously my operating system was Windows and there I used Notepad++,
>>> I really had very nice experience with it. However I dont see any Mac
>>> version is available for Mac.
>>>
>>> Appreciate your positive feedback.
>>>
>>> Thanks and regards,
>>>
>> Atom seems to be a good choice also.
>
> Is it? Which package(s) should I install to write and run R code in Atom?
> Certainly I don't see anything useful out of the box.
>
> Best,
> Ista
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Thu Jan 21 19:06:24 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 21 Jan 2016 13:06:24 -0500
Subject: [R] R editor for Mac
In-Reply-To: <569FD1D8.4040807@gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<569FD1D8.4040807@gmail.com>
Message-ID: <56A11E20.9050601@gmail.com>

On 20/01/2016 1:28 PM, Duncan Murdoch wrote:
> On 20/01/2016 1:22 PM, Christofer Bogaso wrote:
> > Hi,
> >
> > Could you please suggest a good R editor for Mac OS X (10.7.5)
> > Previously my operating system was Windows and there I used Notepad++,
> > I really had very nice experience with it. However I dont see any Mac
> > version is available for Mac.
> >
> > Appreciate your positive feedback.
>
> RStudio is probably best on both OS X and Windows.    A nice advantage
> is that it looks the same on both, so you can move back and forth.
>
> I only know two negatives:
>
>    - I still don't like the tiled window.  I often work on a small
> screen, and it's not enough space.
>
>    - The editor still changes file endings to native format whenever it
> saves.  It would be better if it handled both Windows and Unix line
> endings in both systems, and left them alone unless the user asked them
> to be changed.

I've just heard offline from JJ Allaire that both negatives above have 
been addressed in a version soon to be released.   Excellent news!

Duncan Murdoch


From bob at rudis.net  Thu Jan 21 19:13:41 2016
From: bob at rudis.net (boB Rudis)
Date: Thu, 21 Jan 2016 13:13:41 -0500
Subject: [R] R editor for Mac
In-Reply-To: <56A11E20.9050601@gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<569FD1D8.4040807@gmail.com> <56A11E20.9050601@gmail.com>
Message-ID: <CAJ4QxaNeLyb8Mvu0P2R4xn02jEQMfOcSzhUuMxu08ER5YUsGxA@mail.gmail.com>

Aye. You can make source/editor windows consume the entire area or
have them as separate windows and can define a consistent line-ending
vs platform native (I run RStudio Preview and [sometimes] dailies and
can confirm these are in there). The addition of full R
(C/C++/HTML/javascript/etc) code diagnostics (optional) is also a
pretty compelling feature.

On Thu, Jan 21, 2016 at 1:06 PM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 20/01/2016 1:28 PM, Duncan Murdoch wrote:
>>
>> On 20/01/2016 1:22 PM, Christofer Bogaso wrote:
>> > Hi,
>> >
>> > Could you please suggest a good R editor for Mac OS X (10.7.5)
>> > Previously my operating system was Windows and there I used Notepad++,
>> > I really had very nice experience with it. However I dont see any Mac
>> > version is available for Mac.
>> >
>> > Appreciate your positive feedback.
>>
>> RStudio is probably best on both OS X and Windows.    A nice advantage
>> is that it looks the same on both, so you can move back and forth.
>>
>> I only know two negatives:
>>
>>    - I still don't like the tiled window.  I often work on a small
>> screen, and it's not enough space.
>>
>>    - The editor still changes file endings to native format whenever it
>> saves.  It would be better if it handled both Windows and Unix line
>> endings in both systems, and left them alone unless the user asked them
>> to be changed.
>
>
> I've just heard offline from JJ Allaire that both negatives above have been
> addressed in a version soon to be released.   Excellent news!
>
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From chl948 at mail.usask.ca  Thu Jan 21 16:06:57 2016
From: chl948 at mail.usask.ca (Chel Hee Lee)
Date: Thu, 21 Jan 2016 09:06:57 -0600
Subject: [R] strange answer when using 'aggregate()' with a formula
In-Reply-To: <6740a1d7b5a343eb97b15d572ab061c1@Mail06.usask.ca>
References: <56A059A5.2010702@mail.usask.ca>
	<6740a1d7b5a343eb97b15d572ab061c1@Mail06.usask.ca>
Message-ID: <56A0F411.8080608@mail.usask.ca>

I appreciate your kind guidance!  I did not read the manual carefully 
(it's my fault).

Thank you so much, Prof. John Fox!

Chel Hee Lee

On 01/21/2016 12:52 AM, Fox, John wrote:
> Dear Chel Hee Lee,
>
> With the formula method, the default na.action is na.omit; thus,
>
>> aggregate(y~grp, data=tmp, function(x) sum(is.na(x)), na.action=na.pass)
>    grp y
> 1   2 1
> 2   3 0
>
> I hope this helps,
>   John
>
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> Web: socserv.mcmaster.ca/jfox
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Chel Hee Lee
>> Sent: January 21, 2016 5:08 AM
>> To: R-help at r-project.org
>> Subject: [R] strange answer when using 'aggregate()' with a formula
>>
>> Could you kindly test the following codes?  It is because I found strange answer
>> when 'aggregate()' is used with a formula.
>>
>> I am trying to count how many missing data entries are in each group.
>> For this exercise, I created data as below:
>>
>>   > tmp <- data.frame(grp=c(2,3,2,3), y=c(NA, 0.5, 3, 0.5))  > tmp
>>     grp   y
>> 1   2  NA
>> 2   3 0.5
>> 3   2 3.0
>> 4   3 0.5
>>
>> I see that observations (variable y) can be grouped into two groups (variable
>> grp).  For group 2, y has NA and 3.0.  For group 3, y has 0.5 and 0.5.  Hence, the
>> number of missing values is 1 and 0 for group 2 and
>> 3, respectively.   This work can be done using 'aggregate()' in the
>> 'stats' package as below:
>>
>>   > aggregate(x=tmp$y, by=list(grp=tmp$grp), function(x) sum(is.na(x)))
>>     grp x
>> 1   2 1
>> 2   3 0
>>
>> A formula can be used as below:
>>
>>   > aggregate(y~grp, data=tmp, function(x) sum(is.na(x)))
>>     grp y
>> 1   2 0
>> 2   3 0
>>
>> What a surprise!  Is this a bug?  I would appreciate if you share the
>> results after testing the codes.   Thank you so much for your helps in
>> advance!
>>
>> Chel Hee Lee
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From deryassahin at gmail.com  Thu Jan 21 15:19:46 2016
From: deryassahin at gmail.com (Derya Sahin)
Date: Thu, 21 Jan 2016 09:19:46 -0500
Subject: [R] question for bayesian regression
Message-ID: <CAB50ivAyRKQ5kEpgNwjwtp84NQR+2UMWGn-isR7ic4y=_Dc9aA@mail.gmail.com>

Hello,



I don't have much knowledge about how to use JAGS to do bayesian
regression, I have seen several examples but my data is left censored and I
am not sure how to construct the likelihood function, if someone could post
a sample JAGS code for bayesian regression for left-censored data, that
would be great. for example I want to predict y and my predictors x1,x2,x3 such
that y ~a1*x1+a2*x2+a3*x3 and x3 is left censored some values are below LOD
values (LOD is also a vector, same size of x3)

for left-censored data x3, I know I can do the following,
# JAGS for left censored x3
model {
for (i in 1:N) {
above.lod[i] ~ dinterval(x3[i], llodVec[i])
x3[i] ~ dnorm(mu, tau)
}
mu ~ dnorm(0, .001)
tau <- 1/pow(sigma,2)
sigma~dt(0,1/625,1)
}

but where and how I should include the regression . In short I want to
combine the above and below code in one jags code, since I am new to JAGS
not sure what I am doing is correct. I appreciate any help and suggestions.
thanks,

#JAGS for regression
for( i in 1:N ) {
      y[i] ~ dnorm( y.hat[i] , tau )
      y.hat[i] <- a1*x1[i]+a2*x2[i]*a3*x3[i]
    }
    tau <- 1/pow(sigma,2)
    sigma ~ dunif( 0 , 10 )
    for ( j in 1:3 ) {
      a[j] ~ dnorm( 0 , 1.0E-3 )
    }
  }

regression <http://stats.stackexchange.com/questions/tagged/regression>
bayesian <http://stats.stackexchange.com/questions/tagged/bayesian> jags
<http://stats.stackexchange.com/questions/tagged/jags>

In short, I am not sure how to construct the likelihood function for this
kind of problem,
any help would  be appreciated

thanks

-- 
D

	[[alternative HTML version deleted]]


From mxfomin at gmail.com  Thu Jan 21 16:15:46 2016
From: mxfomin at gmail.com (Maxim Fomin)
Date: Thu, 21 Jan 2016 18:15:46 +0300
Subject: [R] ggplot2 - specific y axis scale for each time series in facet
	grind
Message-ID: <CALB30JAsRHmmmu8fKNi99K7WkPi6bZCFZ-G-_e8vy_-rQ5M_nQ@mail.gmail.com>

Dear R users,

I have a (melted) data frame of several metal prices from 1980. I want to
make a time series plot for each metal price. By default ggplot constructs
single plot for all prices. Adding +facet_grid(variable~.) makes for each
metal price a plot, but now there is another problem - scale of y axis.
Some metals have price $1000 and some $2. Because ggplot makes same y scale
for each metal, some metal prices printed as straight horizontal line. What
is the command to make for each plot specific y axis scale?

Currently I am trying command:

ggplot(data=melted2, aes(x=date, y=value, group=variable,
color=variable))+geom_line()+facet_grid(variable~.)


Best regards,
Maxim

P.S dput

structure(list(date = structure(c(3652, 3683, 3712, 3743, 3773,
3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740, 3652,
3683, 3712, 3743, 3773, 3804, 3834, 3865, 3896, 3926, 3957, 3987,
4018, 4049, 4077, 4108, 4138, 4169, 4199, 4230, 4261, 4291, 4322,
4352, 4383, 4414, 4442, 4473, 4503, 4534, 4564, 4595, 4626, 4656,
4687, 4717, 4748, 4779, 4807, 4838, 4868, 4899, 4929, 4960, 4991,
5021, 5052, 5082, 5113, 5144, 5173, 5204, 5234, 5265, 5295, 5326,
5357, 5387, 5418, 5448, 5479, 5510, 5538, 5569, 5599, 5630, 5660,
5691, 5722, 5752, 5783, 5813, 5844, 5875, 5903, 5934, 5964, 5995,
6025, 6056, 6087, 6117, 6148, 6178, 6209, 6240, 6268, 6299, 6329,
6360, 6390, 6421, 6452, 6482, 6513, 6543, 6574, 6605, 6634, 6665,
6695, 6726, 6756, 6787, 6818, 6848, 6879, 6909, 6940, 6971, 6999,
7030, 7060, 7091, 7121, 7152, 7183, 7213, 7244, 7274, 7305, 7336,
7364, 7395, 7425, 7456, 7486, 7517, 7548, 7578, 7609, 7639, 7670,
7701, 7729, 7760, 7790, 7821, 7851, 7882, 7913, 7943, 7974, 8004,
8035, 8066, 8095, 8126, 8156, 8187, 8217, 8248, 8279, 8309, 8340,
8370, 8401, 8432, 8460, 8491, 8521, 8552, 8582, 8613, 8644, 8674,
8705, 8735, 8766, 8797, 8825, 8856, 8886, 8917, 8947, 8978, 9009,
9039, 9070, 9100, 9131, 9162, 9190, 9221, 9251, 9282, 9312, 9343,
9374, 9404, 9435, 9465, 9496, 9527, 9556, 9587, 9617, 9648, 9678,
9709, 9740, 9770, 9801, 9831, 9862, 9893, 9921, 9952, 9982, 10013,
10043, 10074, 10105, 10135, 10166, 10196, 10227, 10258, 10286,
10317, 10347, 10378, 10408, 10439, 10470, 10500, 10531, 10561,
10592, 10623, 10651, 10682, 10712, 10743, 10773, 10804, 10835,
10865, 10896, 10926, 10957, 10988, 11017, 11048, 11078, 11109,
11139, 11170, 11201, 11231, 11262, 11292, 11323, 11354, 11382,
11413, 11443, 11474, 11504, 11535, 11566, 11596, 11627, 11657,
11688, 11719, 11747, 11778, 11808, 11839, 11869, 11900, 11931,
11961, 11992, 12022, 12053, 12084, 12112, 12143, 12173, 12204,
12234, 12265, 12296, 12326, 12357, 12387, 12418, 12449, 12478,
12509, 12539, 12570, 12600, 12631, 12662, 12692, 12723, 12753,
12784, 12815, 12843, 12874, 12904, 12935, 12965, 12996, 13027,
13057, 13088, 13118, 13149, 13180, 13208, 13239, 13269, 13300,
13330, 13361, 13392, 13422, 13453, 13483, 13514, 13545, 13573,
13604, 13634, 13665, 13695, 13726, 13757, 13787, 13818, 13848,
13879, 13910, 13939, 13970, 14000, 14031, 14061, 14092, 14123,
14153, 14184, 14214, 14245, 14276, 14304, 14335, 14365, 14396,
14426, 14457, 14488, 14518, 14549, 14579, 14610, 14641, 14669,
14700, 14730, 14761, 14791, 14822, 14853, 14883, 14914, 14944,
14975, 15006, 15034, 15065, 15095, 15126, 15156, 15187, 15218,
15248, 15279, 15309, 15340, 15371, 15400, 15431, 15461, 15492,
15522, 15553, 15584, 15614, 15645, 15675, 15706, 15737, 15765,
15796, 15826, 15857, 15887, 15918, 15949, 15979, 16010, 16040,
16071, 16102, 16130, 16161, 16191, 16222, 16252, 16283, 16314,
16344, 16375, 16405, 16436, 16467, 16495, 16526, 16556, 16587,
16617, 16648, 16679, 16709, 16740, 3652, 3683, 3712, 3743, 3773,
3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740, 3652,
3683, 3712, 3743, 3773, 3804, 3834, 3865, 3896, 3926, 3957, 3987,
4018, 4049, 4077, 4108, 4138, 4169, 4199, 4230, 4261, 4291, 4322,
4352, 4383, 4414, 4442, 4473, 4503, 4534, 4564, 4595, 4626, 4656,
4687, 4717, 4748, 4779, 4807, 4838, 4868, 4899, 4929, 4960, 4991,
5021, 5052, 5082, 5113, 5144, 5173, 5204, 5234, 5265, 5295, 5326,
5357, 5387, 5418, 5448, 5479, 5510, 5538, 5569, 5599, 5630, 5660,
5691, 5722, 5752, 5783, 5813, 5844, 5875, 5903, 5934, 5964, 5995,
6025, 6056, 6087, 6117, 6148, 6178, 6209, 6240, 6268, 6299, 6329,
6360, 6390, 6421, 6452, 6482, 6513, 6543, 6574, 6605, 6634, 6665,
6695, 6726, 6756, 6787, 6818, 6848, 6879, 6909, 6940, 6971, 6999,
7030, 7060, 7091, 7121, 7152, 7183, 7213, 7244, 7274, 7305, 7336,
7364, 7395, 7425, 7456, 7486, 7517, 7548, 7578, 7609, 7639, 7670,
7701, 7729, 7760, 7790, 7821, 7851, 7882, 7913, 7943, 7974, 8004,
8035, 8066, 8095, 8126, 8156, 8187, 8217, 8248, 8279, 8309, 8340,
8370, 8401, 8432, 8460, 8491, 8521, 8552, 8582, 8613, 8644, 8674,
8705, 8735, 8766, 8797, 8825, 8856, 8886, 8917, 8947, 8978, 9009,
9039, 9070, 9100, 9131, 9162, 9190, 9221, 9251, 9282, 9312, 9343,
9374, 9404, 9435, 9465, 9496, 9527, 9556, 9587, 9617, 9648, 9678,
9709, 9740, 9770, 9801, 9831, 9862, 9893, 9921, 9952, 9982, 10013,
10043, 10074, 10105, 10135, 10166, 10196, 10227, 10258, 10286,
10317, 10347, 10378, 10408, 10439, 10470, 10500, 10531, 10561,
10592, 10623, 10651, 10682, 10712, 10743, 10773, 10804, 10835,
10865, 10896, 10926, 10957, 10988, 11017, 11048, 11078, 11109,
11139, 11170, 11201, 11231, 11262, 11292, 11323, 11354, 11382,
11413, 11443, 11474, 11504, 11535, 11566, 11596, 11627, 11657,
11688, 11719, 11747, 11778, 11808, 11839, 11869, 11900, 11931,
11961, 11992, 12022, 12053, 12084, 12112, 12143, 12173, 12204,
12234, 12265, 12296, 12326, 12357, 12387, 12418, 12449, 12478,
12509, 12539, 12570, 12600, 12631, 12662, 12692, 12723, 12753,
12784, 12815, 12843, 12874, 12904, 12935, 12965, 12996, 13027,
13057, 13088, 13118, 13149, 13180, 13208, 13239, 13269, 13300,
13330, 13361, 13392, 13422, 13453, 13483, 13514, 13545, 13573,
13604, 13634, 13665, 13695, 13726, 13757, 13787, 13818, 13848,
13879, 13910, 13939, 13970, 14000, 14031, 14061, 14092, 14123,
14153, 14184, 14214, 14245, 14276, 14304, 14335, 14365, 14396,
14426, 14457, 14488, 14518, 14549, 14579, 14610, 14641, 14669,
14700, 14730, 14761, 14791, 14822, 14853, 14883, 14914, 14944,
14975, 15006, 15034, 15065, 15095, 15126, 15156, 15187, 15218,
15248, 15279, 15309, 15340, 15371, 15400, 15431, 15461, 15492,
15522, 15553, 15584, 15614, 15645, 15675, 15706, 15737, 15765,
15796, 15826, 15857, 15887, 15918, 15949, 15979, 16010, 16040,
16071, 16102, 16130, 16161, 16191, 16222, 16252, 16283, 16314,
16344, 16375, 16405, 16436, 16467, 16495, 16526, 16556, 16587,
16617, 16648, 16679, 16709, 16740, 3652, 3683, 3712, 3743, 3773,
3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740), class = "Date"),
    variable = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
    8L, 8L, 8L, 8L), .Label = c("aluminum", "copper", "gold",
    "ironore", "nickel", "tin", "zinc", "index"), class = "factor"),
    value = c(2054.86010742188, 2131.00854492188, 1978.37890625,
    1932.45556640625, 1775.8037109375, 1668.96044921875, 1758.07495117188,
    1783.62719726563, 1655.06567382813, 1626.15209960938, 1503.91015625,
    1430.65673828125, 1430.82958984375, 1452.38110351563, 1442.95190429688,
    1369.23461914063, 1296.99584960938, 1232.40673828125, 1175.31127929688,
    1230.82885742188, 1168.9111328125, 1140.181640625, 1081.84643554688,
    1130.88427734375, 1113.25854492188, 1087.70629882813, 1029.23657226563,
    996.20751953125, 973.294189453125, 918.849365234375, 957.847900390625,
    958.864013671875, 959.531982421875, 951.484130859375, 965.14697265625,
    987.387451171875, 1076.53076171875, 1230.45581054688, 1301.72924804688,
    1363.56713867188, 1453.43408203125, 1465.57836914063, 1520.2099609375,
    1603.66748046875, 1613.72875976563, 1566.25805664063, 1516.72778320313,
    1549.33911132813, 1548.90625, 1491.16162109375, 1455.76293945313,
    1370.35034179688, 1289.13500976563, 1275.02905273438, 1162.80615234375,
    1136.00903320313, 1012.169921875, 1027.64794921875, 1154.66040039063,
    1095.69604492188, 1075.8544921875, 1097.90087890625, 1095.25512695313,
    1108.923828125, 1104.5146484375, 1031.76220703125, 1011.92065429688,
    1018.53442382813, 985.465087890625, 970.03271484375, 950.191162109375,
    1038.3759765625, 1119.94702148438, 1115.53784179688, 1168.44848632813,
    1164.03930664063, 1164.03930664063, 1183.880859375, 1122.15161132813,
    1128.76538085938, 1205.92724609375, 1161.83471679688, 1130.96997070313,
    1130.96997070313, 1170.6533203125, 1283.0888671875, 1369.06909179688,
    1399.93359375, 1410.95678710938, 1472.68627929688, 1653.46508789063,
    1809.99291992188, 1746.05908203125, 1962.11181640625, 1679.92041015625,
    1823.220703125, 2001.794921875, 2138.4814453125, 2526.49462890625,
    2508.85766601563, 2987.26000976563, 3578.09814453125, 2581.61010742188,
    2700.65942382813, 2387.603515625, 2308.23706054688, 2380.98950195313,
    2458.1513671875, 2398.62646484375, 2175.59692382813, 2074.54736328125,
    2125.80004882813, 2259, 1914, 1756, 1798, 1718, 1820, 1736,
    1633, 1528, 1454, 1567, 1526, 1527, 1566, 1571, 1782, 2067,
    1946, 1618, 1522, 1515, 1505, 1496, 1392, 1299.94995117188,
    1283.55004882813, 1297.54663085938, 1259.693359375, 1212.09521484375,
    1152.95458984375, 1134.92504882813, 1099.5263671875, 1181.22729492188,
    1270.75, 1282.02270507813, 1317.125, 1306.71069335938, 1275.27270507813,
    1314.34790039063, 1305.55004882813, 1270.21435546875, 1180.84106445313,
    1162.21435546875, 1209.02380371094, 1208.09997558594, 1203.15002441406,
    1150.47827148438, 1110.27502441406, 1125.42102050781, 1168.09094238281,
    1203.18176269531, 1172.76196289063, 1116.90905761719, 1089.42858886719,
    1040.02270507813, 1091.34997558594, 1170, 1270.52502441406,
    1287.69567871094, 1277.28942871094, 1321.04760742188, 1402.36364746094,
    1491.95239257813, 1457.06823730469, 1571.09094238281, 1694.26196289063,
    1885.88635253906, 1878.375, 2059.35717773438, 1905.44995117188,
    1799.97827148438, 1849.44445800781, 1765.47619628906, 1775.79541015625,
    1867.27502441406, 1890.04541015625, 1761.72497558594, 1671.95458984375,
    1655.92858886719, 1657.81579589844, 1584.47619628906, 1594.57141113281,
    1614.85717773438, 1587.40002441406, 1593.85717773438, 1485.75,
    1460.02172851563, 1466.78576660156, 1405.76196289063, 1336.02172851563,
    1451.04760742188, 1500.97619628906, 1575.13635253906, 1582.05297851563,
    1631.18420410156, 1561.11364746094, 1625.44995117188, 1567.1904296875,
    1591.5869140625, 1710.02502441406, 1611.61901855469, 1607.32604980469,
    1598.32495117188, 1530.52380371094, 1485.47497558594, 1465.17504882813,
    1438.31823730469, 1417.55004882813, 1364.31579589844, 1305.52380371094,
    1312.66662597656, 1310.55004882813, 1341.88635253906, 1303.70458984375,
    1295.30004882813, 1250.39477539063, 1219.40002441406, 1186.475,
    1179.86363636364, 1277.85, 1323.13157894737, 1315.52272727273,
    1403.36363636364, 1421.69047619048, 1492.06818181818, 1473.33333333333,
    1472.43181818182, 1554.73529411765, 1679.85, 1679.425, 1579.64,
    1458.745, 1468.0652173913, 1509.37272727273, 1564.37619047619,
    1530.3347826087, 1602.30952380952, 1501.24545454545, 1475.55454545455,
    1569.055, 1619.87727272727, 1607.03, 1512.17272727273, 1499.48333333333,
    1542.39047619048, 1470.83333333333, 1418.62272727273, 1377.84545454545,
    1345.995, 1283.53043478261, 1334.45454545455, 1348.72352941176,
    1371.36363636364, 1371.08, 1404.985, 1370.38571428571, 1344.42727272727,
    1356.93333333333, 1337.8652173913, 1293.35714285714, 1301.7,
    1311.46086956522, 1373.26666666667, 1375.8619047619, 1379.29090909091,
    1421.555, 1386.54761904762, 1334.32, 1400.415, 1410.5380952381,
    1440.90434782609, 1457.24, 1416.59545454545, 1477.24782608696,
    1511.6, 1557.77857142857, 1608.89285714286, 1685.225, 1657.35434782609,
    1731.68, 1625.27368421053, 1682.04090909091, 1707.86818181818,
    1692.1, 1731.02272727273, 1830.36666666667, 1817.35, 1852.92380952381,
    1836.21, 1882.8, 1987.51904761905, 1892.00952380952, 1741.45,
    1731.94318181818, 1783.2619047619, 1871.27272727273, 1837.69318181818,
    1934.14285714286, 2056.97045454545, 2250.9, 2383.30238095238,
    2453.375, 2432.47826086957, 2623.85833333333, 2852.07142857143,
    2490.95454545455, 2511.83333333333, 2461.55227272727, 2484.38095238095,
    2657.14772727273, 2702.13636363636, 2823.67105263158, 2799.05909090909,
    2839.05, 2757.07954545455, 2817.05263157895, 2804.60476190476,
    2681.30952380952, 2738.09090909091, 2512.60227272727, 2394.9625,
    2444.53260869565, 2507.15227272727, 2382.83333333333, 2456.125,
    2784.89285714286, 3012.05263157895, 2968.03409090909, 2908.2775,
    2967.86904761905, 3067.45652173913, 2762.56, 2524.14772727273,
    2122.02826086956, 1857.13, 1504.41666666667, 1420.35952380952,
    1338.0625, 1338.07954545455, 1431.8125, 1464.41666666667,
    1586.33571428571, 1674.33181818182, 1927.6375, 1835.59772727273,
    1875.66363636364, 1956.54761904762, 2181.25714285714, 2230.15,
    2053.29, 2210.54782608696, 2314.305, 2044.73157894737, 1929.40909090909,
    1989.04545454545, 2110.44047619048, 2171.24318181818, 2342.17857142857,
    2324.04545454545, 2356.66904761905, 2439.7, 2515.26, 2555.5,
    2667.41666666667, 2587.2125, 2557.76136363636, 2525.42857142857,
    2380.975, 2293.46136363636, 2180.64761904762, 2079.98181818182,
    2024.375, 2151.33333333333, 2207.91666666667, 2184.15909090909,
    2048.50526315789, 2002.52272727273, 1885.51315789474, 1876.25,
    1843.32727272727, 2064.12, 1974.30434782609, 1948.82954545455,
    2086.76315789474, 2037.60681818182, 2053.595, 1911.2825,
    1861.02380952381, 1832.57142857143, 1814.5375, 1769.60869565217,
    1816.23571428571, 1761.30476190476, 1814.58260869565, 1747.96428571429,
    1739.81, 1727.41136363636, 1695.165, 1705.36666666667, 1810.675,
    1751.05, 1838.95238095238, 1948.30434782609, 2030.4925,
1990.43181818182,
    1946.18913043478, 2055.555, 1909.45952380952, 1814.71904761905,
    1817.82, 1773.86363636364, 1819.1875, 1804.03947368421,
1687.72727272727,
    1639.5, 1548.125, 1589.60227272727, 1516.48863636364, 1467.89285714286,
    2592.63342275848, 2916.71199088135, 2303.8279, 2074.54755455933,
    2076.75230911865, 2006.2042, 2175.96020911865, 2081.16114544067,
    2059.11494544067, 2045.88709088135, 2010.61370911865, 1878.33650911865,
    1876.13175455933, 1803.37889088135, 1816.60674544067, 1823.22100911865,
    1746.05930911865, 1699.76215455933, 1682.12479088135, 1787.94695455933,
    1706.37574544067, 1664.4881, 1651.26024544067, 1655.66975455933,
    1613.78210911865, 1596.14474544067, 1512.36945455933, 1521.1878,
    1530.00614544067, 1309.54431363983, 1439.61659088135, 1450.63969088135,
    1424.18465455933, 1463.86754544067, 1444.0261, 1474.89064544067,
    1574.09854544067, 1649.05549088135, 1598.3495, 1675.5112,
    1765.90075455933, 1701.96690911865, 1704.17099088135, 1640.23714544067,
    1560.87069088135, 1435.20775455933, 1388.9106, 1415.36630911865,
    1375.68291363983, 1428.59349088135, 1501.34635455933, 1532.2109,
    1419.77514544067, 1364.65981363983, 1331.59051363983, 1338.20435681992,
    1294.11195681992, 1272.06575681992, 1344.8182, 1320.56741363983,
    1358.04588636017, 1388.9106, 1389.35153409195, 1501.34635455933,
    1530.00614544067, 1433.003, 1474.89064544067, 1419.77514544067,
    1366.8644, 1384.50134318008, 1369.06898636017, 1391.11518636017,
    1417.57039088135, 1404.34295681992, 1444.0261, 1433.003,
    1417.57039088135, 1413.16155455933, 1344.8182, 1302.93038636017,
    1309.54431363983, 1316.15815681992, 1302.93038636017, 1331.59051363983,
    1344.8182, 1380.09208636017, 1463.86754544067, 1483.70899088135,
    1518.98304544067, 1571.89379088135, 1693.14789088135, 1754.87765455933,
    1809.99315455933, 1966.52130911865, 2519.88039088135, 2866.006,
    2660.97660911865, 2328.07902275848, 2358.9434, 2283.98662275848,
    2442.71869088135, 2539.72250911865, 2213.43817724152, 2200.21049088135,
    2433.90017724152, 2936.55410911865, 3302.52049088135, 3496.52745455933,
    3392.91004544067, 3095.28634544067, 3262.8376, 3117.33254544067,
    2738.13830911865, 2544.13117724152, 2503.78709547119, 2760.18450911865,
    2883.64269088135, 2859.39240911865, 2590.4285, 2418.46840911865,
    2365.55699088135, 2358.9434, 2625.70272275848, 2685.22689088135,
    2740.34239088135, 2583.81490911865, 2769.00302275848, 2956.39555455933,
    3040.17084544067, 2742.54697724152, 2583.81490911865, 2484.60700911865,
    2447.1282, 2447.1282, 2409.64939088135, 2471.37932275848,
    2340.4246727356, 2218.06802725677, 2231.25534581909, 2236.18677692413,
    2318.58300291138, 2360.64557547607, 2375.66037788696, 2205.91849749756,
    2150.57962901611, 2208.89225860596, 2231.78550956421, 2214.16681599274,
    2219.31505581207, 2296.44193858643, 2527.28502240143, 2513.46998458252,
    2419.61418178101, 2262.45696237183, 2161.78909368286, 2211.95028749237,
    2264.21397077332, 2212.18139313507, 2152.38020099945, 1953.62060192108,
    1799.35169844818, 1857.87070840454, 1927.33981992645, 1951.07137548676,
    1858.24999750519, 1646.99992897491, 1632.56810484467, 1723.6250736618,
    1807.07136699066, 1865.07501472778, 1913.17391866455, 1881.36846713562,
    2144.50007046051, 2363.09102303009, 2447.23803404999, 2409.74997397766,
    2504.84103139343, 2546.02375416412, 2803.54541098175, 2980.69999645081,
    3003.26189506836, 2870.45015814514, 2919.67397804871, 2894.88882296448,
    2771.57125569763, 2987.68194348755, 3076.4500581665, 3040.13619641418,
    2910.42504276428, 2810.06817431793, 2981.61903664856, 2918.21064537659,
    2604.99992943115, 2544.66672335968, 2563.34994940033, 2594.17496803894,
    2658.76193080902, 2177.59998270721, 1983.93479971771, 2017.85720584564,
    1935.07143060913, 1960.19567519531, 2215.42864607391, 2264.59527826385,
    2427.27279402924, 2406.63156159363, 2420.1842087265, 2389.31814949341,
    2513.29993523407, 2611.28570015411, 2449.19570404663, 2250.10004305115,
    2104.30955452118, 2050.6955684021, 1918.49994499359, 1761.45239143524,
    1687.60000986328, 1663.99998604431, 1747.15916341095, 1800.12507817535,
    1731.65792903595, 1656.59518632507, 1655.66672697449, 1619.92507875671,
    1646.77269191284, 1585.49992534637, 1573.72497511139, 1475.76309447174,
    1432.00002842255, 1412.95, 1378.47727272727, 1463.725, 1510.44736842105,
    1422.15909090909, 1639.18181818182, 1646.7380952381, 1749.68181818182,
    1723.28571428571, 1726.77272727273, 1764.88235294118, 1843.85,
    1807.025, 1739.8, 1681.9075, 1785.09782608696, 1752.07272727273,
    1803.14285714286, 1857.12391304348, 1961.89285714286, 1894.37272727273,
    1795.60227272727, 1852.4, 1787.05681818182, 1766.125, 1742.15909090909,
    1665.97222222222, 1684.85, 1610.46904761905, 1526.77045454545,
    1466.41136363636, 1427.69736842105, 1377.37608695652, 1434.29318181818,
    1472.84705882353, 1508.22727272727, 1561.3675, 1607.3925,
    1588.57142857143, 1597.02272727273, 1650.59444444444, 1588.2847826087,
    1482.91666666667, 1478.93333333333, 1486.17173913043, 1581.03571428571,
    1592.96428571429, 1650.31136363636, 1682.145, 1655.69285714286,
    1587.8675, 1651.1, 1685.10714285714, 1712.82608695652, 1756.725,
    1789.67045454545, 1925.58260869565, 2053.275, 2202.03571428571,
    2421.47619047619, 2751.7175, 3000.28260869565, 2926.975,
    2728.46315789474, 2689.05454545455, 2816.8, 2844.20476190476,
    2903.17272727273, 3009.40476190476, 3130.30909090909, 3139.78571428571,
    3168.1, 3247.1, 3378.90476190476, 3389.80952380952, 3241.9,
    3529.72727272727, 3608.47619047619, 3791.90909090909, 3850.65909090909,
    4056.16666666667, 4278.15909090909, 4577.025, 4743.8619047619,
    4974.975, 5123.67391304348, 6404.44444444444, 8059.19047619048,
    7222.77272727273, 7726.7380952381, 7690.25, 7622.64285714286,
    7497.40909090909, 7029.29545454546, 6680.97368421053, 5689.34090909091,
    5718.15, 6465.29545454545, 7753.34210526316, 7677.95238095238,
    7514.2380952381, 7980.93181818182, 7500.20454545455, 7671.35,
    8020.58695652174, 6957.43181818182, 6630.73611111111, 7078.90909090909,
    7941.14285714286, 8434.31578947368, 8714.18181818182, 8356.125,
    8292, 8407.02173913043, 7633.8, 6975.11363636364, 4894.89130434783,
    3729.1875, 3105.09523809524, 3260.35714285714, 3328.4125,
    3770.875, 4436.925, 4594.90277777778, 5013.29761904762,
5240.82954545455,
    6176.875, 6195.75, 6305.98863636364, 6682.44047619048, 6976.97619047619,
    7367.395, 6867.69, 7466.96086956522, 7729.855, 6843.18421052632,
    6501.5, 6750.56818181818, 7302.66666666667, 7729.59090909091,
    8289.7619047619, 8458.42045454545, 9152.85714285714, 9533.2,
    9880.9375, 9503.35869565217, 9482.75, 8931.675, 9066.85227272727,
    9650.46428571429, 8997.98863636364, 8300.13636363636, 7394.19047619048,
    7581.02272727273, 7558.875, 8061.91666666667, 8441.4880952381,
    8470.78409090909, 8285.52631578947, 7896.90909090909, 7428.28947368421,
    7584.26136363636, 7510.43181818182, 8087.7425, 8062.03260869565,
    7711.22727272727, 7966.48684210526, 8053.73863636364, 8060.925,
    7652.375, 7221.1619047619, 7248.71428571429, 7000.2375,
6906.64130434783,
    7186.25, 7159.26904761905, 7203.02173913044, 7070.65476190476,
    7214.9, 7291.46590909091, 7149.2125, 6650.03571428571, 6673.5625,
    6891.125, 6821.14285714286, 7113.38043478261, 7001.8375,
    6872.21590909091, 6737.47826086957, 6712.85, 6446.45238095238,
    5830.53571428571, 5729.275, 5939.67045454546, 6042.0875,
    6294.77631578947, 5833.01136363636, 5456.75, 5127.3, 5217.25,
    5216.09090909091, 4799.90476190476, 674.58, 665.893, 554.276,
    516.71, 514.268, 600.786, 645.75, 626.36, 673.941, 662.27,
    623.875, 596.712, 557.813, 500.8, 499.693, 496.625, 480.316,
    460.5, 409.284, 410.24, 443.773, 437.68, 413.405, 410.119,
    384.163, 374.458, 329.977, 350.335, 334.505, 314.961, 337.895,
    363.413, 438.15, 422.786, 415.114, 444.776, 481.838, 493.488,
    420.707, 433.171, 437.393, 413.148, 422.645, 416.205, 412.245,
    394.245, 381.016, 388.06, 370.735, 386.038, 394.743, 381.371,
    376.957, 378.314, 347.607, 347.677, 340.945, 340.217, 341.286,
    319.622, 302.852, 300.333, 303.205, 324.883, 316.395, 316.298,
    317.202, 330.131, 323.764, 326.093, 325.548, 321.985, 345.561,
    339.053, 346.095, 340.716, 342.325, 342.798, 348.554, 376.29,
    418.152, 423.863, 396.983, 391.595, 408.524, 401.045, 408.848,
    439.665, 461.65, 449.277, 450.33, 460.988, 460.12, 465.764,
    468.14, 487.079, 477.758, 442.124, 443.491, 451.558, 451.32,
    451.657, 437.452, 431.064, 413.439, 406.39, 419.966, 419.248,
    404.445, 387.973, 390.274, 384.72, 371.35, 367.727, 375.21,
    365.548, 361.798, 366.8, 394.361, 409.655, 410.118, 416.543,
    393.661, 374.929, 368.855, 352.657, 361.82, 394.861, 389.56,
    381.333, 381.866, 378.161, 384.591, 363.748, 363.39, 358.055,
    357.117, 366.36, 368.013, 356.721, 348.46, 358.826, 359.96,
    361.875, 354.436, 353.853, 344.641, 338.728, 337.039, 340.784,
    352.452, 343.603, 345.3, 344.277, 334.924, 334.657, 328.993,
    329.31, 329.974, 341.948, 367.045, 371.914, 392.034, 379.795,
    355.561, 364.005, 373.939, 383.243, 387.11, 381.658, 384,
    377.908, 381.343, 385.714, 385.45, 380.207, 391.348, 390.164,
    384.377, 379.48, 378.738, 376.745, 381.82, 391.339, 385.231,
    387.618, 386.138, 383.502, 382.931, 383.202, 385.209, 387.445,
    398.695, 404.919, 396.512, 392.87, 391.99, 385.245, 383.457,
    387.51, 383.29, 380.909, 377.869, 369.338, 355.025, 346.4,
    352.311, 344.707, 344.1, 340.805, 323.78, 323.998, 322.616,
    324.854, 306.345, 288.776, 289.264, 297.743, 295.87, 308.558,
    298.971, 292.223, 292.874, 284.228, 288.661, 296.595, 294.243,
    291.357, 287.333, 287.495, 286.243, 282.62, 276.932, 261.402,
    256.198, 256.936, 264.47, 311.562, 293.65, 283.743, 284.59,
    300.855, 286.704, 279.961, 275.293, 285.368, 282.152, 274.523,
    273.676, 270.405, 265.989, 271.892, 265.934, 262.018, 263.273,
    260.75, 272.057, 270.738, 267.707, 272.657, 282.478, 283.322,
    276.248, 275.992, 281.764, 295.683, 294.353, 302.862, 314.48,
    321.536, 313.567, 310.045, 318.8, 316.748, 319.255, 333.3,
    356.864, 359.575, 341.564, 328.208, 355.405, 356.912, 350.765,
    358.993, 378.859, 379.093, 390.2, 407.674, 414.495, 404.73,
    405.976, 404.85, 383.953, 391.78, 398.441, 400.133, 405.402,
    420.21, 439.059, 442.974, 424.08, 423.43, 434.355, 429.14,
    422.903, 430.302, 424.745, 437.773, 455.936, 470.107, 476.668,
    509.423, 549.433, 555.518, 557.215, 611.853, 676.769, 597.898,
    633.093, 631.557, 600.15, 586.648, 626.825, 629.513, 630.352,
    665.103, 655.891, 680.008, 668.31, 655.714, 665.266, 664.53,
    710.645, 754.48, 808.311, 803.618, 887.784, 924.283, 971.055,
    911.6, 889.125, 889.536, 941.167, 840.388, 824.92, 812.815,
    757.85, 819.94, 857.726, 939.763, 925.989, 892.663, 926.855,
    947.807, 934.272, 949.5, 996.443, 1043.511, 1126.119, 1135.012,
    1119.575, 1095.8, 1115.554, 1148.475, 1204.321, 1232.382,
    1196, 1213.464, 1271.461, 1343.19, 1371.784, 1393.512, 1360.475,
    1371.313, 1422.848, 1474.431, 1512.188, 1528.38, 1568.526,
    1759.5, 1780.648, 1667.893, 1735.977, 1652.725, 1656.095,
    1743.095, 1675.057, 1648.539, 1585.114, 1595.632, 1592.784,
    1625.682, 1741.925, 1746.348, 1724.352, 1687.342, 1671.886,
    1630.688, 1591.013, 1485.905, 1416.143, 1342.7, 1284.348,
    1345.048, 1348.464, 1314.402, 1277.417, 1221.588, 1243.068,
    1298.713, 1336.56, 1299.175, 1288.913, 1277.857, 1312.989,
    1297.005, 1241.33, 1223.565, 1176.413, 1200.44, 1249.333,
    1231.1, 1180.636, 1198.253, 1197.684, 1182.248, 1131.58,
    1117.525, 1124.905, 1157.123, 1088.388, 12.15, 12.15, 12.15,
    12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15,
    12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15,
    12.15, 12.15, 12.15, 14.05, 14.05, 14.05, 14.05, 14.05, 14.05,
    14.05, 14.05, 14.05, 14.05, 14.05, 14.05, 12.54, 12.54, 12.54,
    12.54, 12.54, 12.54, 12.54, 12.54, 12.54, 12.54, 12.54, 12.54,
    11.31, 11.31, 11.31, 11.31, 11.31, 11.31, 11.31, 11.31, 11.31,
    11.31, 11.31, 11.31, 11.49, 11.49, 11.49, 11.49, 11.49, 11.49,
    11.49, 11.49, 11.49, 11.49, 11.49, 11.49, 11.36, 11.36, 11.36,
    11.36, 11.36, 11.36, 11.36, 11.36, 11.36, 11.36, 11.36, 11.36,
    10.94, 10.94, 10.94, 10.94, 10.94, 10.94, 10.94, 10.94, 10.94,
    10.94, 10.94, 10.94, 10.51, 10.51, 10.51, 10.51, 10.51, 10.51,
    10.51, 10.51, 10.51, 10.51, 10.51, 10.51, 12.03, 12.03, 12.03,
    12.03, 12.03, 12.03, 12.03, 12.03, 12.03, 12.03, 12.03, 12.03,
    14.05, 14.05, 14.05, 14.05, 14.05, 14.05, 14.05, 14.05, 14.05,
    14.05, 14.05, 14.05, 15.03, 15.03, 15.03, 15.03, 15.03, 15.03,
    15.03, 15.03, 15.03, 15.03, 15.03, 15.03, 14.31, 14.31, 14.31,
    14.31, 14.31, 14.31, 14.31, 14.31, 14.31, 14.31, 14.31, 14.31,
    12.58, 12.58, 12.58, 12.58, 12.58, 12.58, 12.58, 12.58, 12.58,
    12.58, 12.58, 12.58, 11.45, 11.45, 11.45, 11.45, 11.45, 11.45,
    11.45, 11.45, 11.45, 11.45, 11.45, 11.45, 12.27, 12.27, 12.27,
    12.27, 12.27, 12.27, 12.27, 12.27, 12.27, 12.27, 12.27, 12.27,
    12.97, 12.97, 12.97, 12.97, 12.97, 12.97, 12.97, 12.97, 12.97,
    12.97, 12.97, 12.97, 13.04, 13.04, 13.04, 13.04, 13.04, 13.04,
    13.04, 13.04, 13.04, 13.04, 13.04, 13.04, 13.41, 13.41, 13.41,
    13.41, 13.41, 13.41, 13.41, 13.41, 13.41, 13.41, 13.41, 13.41,
    11.93, 11.93, 11.93, 11.93, 11.93, 11.93, 11.93, 11.93, 11.93,
    11.93, 11.93, 11.93, 12.45, 12.45, 12.45, 12.45, 12.45, 12.45,
    12.45, 12.45, 12.45, 12.45, 12.45, 12.45, 12.99, 12.99, 12.99,
    12.99, 12.99, 12.99, 12.99, 12.99, 12.99, 12.99, 12.99, 12.99,
    12.68, 12.68, 12.68, 12.68, 12.68, 12.68, 12.68, 12.68, 12.68,
    12.68, 12.68, 12.68, 13.82, 13.82, 13.82, 13.82, 13.82, 13.82,
    13.82, 13.82, 13.82, 13.82, 13.82, 13.82, 16.39, 16.39, 16.39,
    16.39, 16.39, 16.39, 16.39, 16.39, 16.39, 16.39, 16.39, 16.39,
    28.11, 28.11, 28.11, 28.11, 28.11, 28.11, 28.11, 28.11, 28.11,
    28.11, 28.11, 28.11, 33.45, 33.45, 33.45, 33.45, 33.45, 33.45,
    33.45, 33.45, 33.45, 33.45, 33.45, 33.45, 36.63, 36.63, 36.63,
    36.63, 36.63, 36.63, 36.63, 36.63, 36.63, 36.63, 36.63, 36.63,
    60.8, 60.8, 60.8, 60.8, 60.8, 60.8, 60.8, 60.8, 60.8, 60.8,
    60.8, 69.9826086956522, 72.5090909090909, 75.59, 64.0727272727273,
    59.7818181818182, 62.6904761904762, 71.6590909090909, 83.9521739130435,
    97.6666666666667, 80.7136363636364, 86.7863636363636, 99.2571428571429,
    105.247826086957, 125.914285714286, 127.615, 139.769565217391,
    172.468181818182, 161.347619047619, 143.631818181818, 126.363636363636,
    145.340909090909, 140.627272727273, 148.480952380952, 160.55,
    168.526086956522, 179.63, 187.18, 169.35652173913, 179.261111111111,
    177.095, 170.877272727273, 172.97619047619, 177.45, 177.227272727273,
    150.433333333333, 135.540909090909, 136.455, 140.347619047619,
    140.395238095238, 144.663636363636, 147.647368421053, 136.272727272727,
    134.621052631579, 127.936363636364, 107.795454545455, 99.47,
    113.947826086957, 120.345454545455, 128.873684210526, 150.490909090909,
    154.638888888889, 139.87, 137.390909090909, 124.009523809524,
    114.815, 127.191304347826, 137.055, 134.185714285714, 132.572727272727,
    136.32380952381, 135.790476190476, 128.119047619048, 121.37,
    111.833333333333, 114.580952380952, 100.56, 92.7428571428571,
    95.9739130434782, 92.6333333333334, 82.2681818181818, 80.0913043478261,
    73.125, 68.8, 67.3863636363636, 62.69, 56.9409090909091,
    51.15, 60.2333333333333, 62.2863636363636, 51.504347826087,
    55.3809523809524, 56.4318181818182, 52.7409090909091, 46.1619047619048,
    6584.80078125, 6978.927734375, 6733.787109375, 6233.369140625,
    6000.76953125, 6294.833984375, 6622.166015625, 6584.505859375,
    6655.357421875, 6691.947265625, 6452.666015625, 6390.912109375,
    6403.7734375, 6370.748046875, 6292.43359375, 6307.060546875,
    6352.15625, 6169.462890625, 6150.337890625, 5999.1171875,
    5579.064453125, 5216.7109375, 5100.24609375, 5496.025390625,
    5628.904296875, 5716.759765625, 5653.357421875, 5394.0546875,
    5211.837890625, 5186.697265625, 5068.580078125, 4953.02734375,
    4310.296875, 3916.5078125, 3433.65747070313, 3576.26953125,
    3796.9599609375, 4228.197265625, 4831.7734375, 4817.78515625,
    5031.533203125, 4883.158203125, 4804.037109375, 4852.7578125,
    4910.5234375, 4674.150390625, 4585.166015625, 4657, 4670.48046875,
    4639.912109375, 4780.826171875, 4911.40625, 4811.423828125,
    4773.232421875, 4640.1796875, 4736.07421875, 4705.63671875,
    4750.294921875, 4748.400390625, 4858.982421875, 4947.16796875,
    5050.78515625, 5211.72265625, 5489.50390625, 5604.14453125,
    5553.4375, 5090.466796875, 4907.484375, 4570.177734375, 4239.484375,
    4034.45458984375, 4089.57006835938, 4034.45458984375, 3988.15747070313,
    4135.8671875, 4085.16088867188, 4043.27319335938, 4087.36547851563,
    3910.99584960938, 3807.37866210938, 3723.60327148438, 3644.23681640625,
    3642.0322265625, 3562.666015625, 3525.1875, 3716.9892578125,
    3772.10473632813, 3897.76806640625, 4435.6953125, 4435.6953125,
    4753.16015625, 5306.51953125, 5332.9765625, 5692.328125,
    5937.041015625, 7661.0546875, 8073.318359375, 8666.361328125,
    15496.2734375, 18011.74609375, 17024.076171875, 15588.8671875,
    14592.37890625, 14186.73046875, 11878.4921875, 11556.6171875,
    13342.359375, 16920.458984375, 17725.14453125, 18582.474609375,
    17156.353515625, 15261.5, 13454, 12143, 12275, 12910, 11222,
    10425, 9793, 8809, 7056, 6977, 9267, 8939, 8698, 8422, 9318,
    10957, 10844, 9145, 8587, 8158, 8569, 8672, 8700, 9023, 8499.609375,
    8296.5, 8515.201171875, 8154.75, 7675.953125, 7453.86328125,
    7258.5, 7140.263671875, 7531.13671875, 7888, 7430.455078125,
    7417.951171875, 7333.947265625, 7202.36328125, 7516.173828125,
    7279.75, 6918.572265625, 6322.5, 5593.71484375, 5751.1904296875,
    5948.10009765625, 6050.75, 5974.9130859375, 5987.5, 5777.78955078125,
    5544.318359375, 5051.181640625, 4740.47607421875, 4376.04541015625,
    4464.85693359375, 4642.27294921875, 5139.7998046875, 5584.0478515625,
    5830.10009765625, 5590.4345703125, 5400.26318359375, 6082.85693359375,
    6287.04541015625, 6230.5712890625, 5856.13623046875, 6370.681640625,
    6735.85693359375, 7474.54541015625, 8540.650390625, 9596.1904296875,
    8431.25, 7522.39111328125, 7405, 7262.380859375, 7877.27294921875,
    8618, 8931.818359375, 8397.25, 8068.86376953125, 8498.095703125,
    8074.21044921875, 7870.71435546875, 8198.5, 8046.39990234375,
    8047, 8042.85693359375, 7713.15771484375, 7202.60888671875,
    7037.619140625, 7329.8095703125, 7028.69580078125, 6960.71435546875,
    6571.0478515625, 7069.13623046875, 7736.5791015625, 7893.21044921875,
    7313.13623046875, 7476, 7060, 6832.7392578125, 6764.4501953125,
    6498.28564453125, 6373.47802734375, 6137.25, 5942.380859375,
    5489, 5367.25, 5397.04541015625, 5391, 5016.84228515625,
    4456.90478515625, 4332.619140625, 4078, 4100.9091796875,
    3870.45458984375, 4117.25, 3865.76318359375, 4264, 4623.25,
    5002.95454545455, 5054.75, 5395.78947368421, 5194.77272727273,
    5696.36363636364, 6431.42857142857, 7029.09090909091, 7317.61904761905,
    7946.81818181818, 8073.23529411765, 8315.25, 9583.75, 10255.4,
    9746.5, 10122.9130434783, 8384.31818181818, 8174.80952380952,
    8038.4347826087, 8624.80952380952, 7650.63636363636, 7350.59090909091,
    7318.8, 6975.90909090909, 6546.8, 6165.18181818182, 6363.11111111111,
    7086.47619047619, 6674.33333333333, 5962.54545454546, 5525.63636363636,
    5057.65, 4830.78260869565, 5131.31818181818, 5315.82352941176,
    6061.36363636364, 6039.95, 6543.2, 6956.80952380952, 6771.18181818182,
    7147.61111111111, 7137.69565217391, 6736.85714285714, 6664.95238095238,
    6818.91304347826, 7314.80952380952, 7206.33333333333, 8032.90909090909,
    8607.95, 8339.71428571429, 7930.6, 8347.75, 8839.04761904762,
    8831.78260869565, 9359.9, 9995.81818181818, 11040.347826087,
    12052.4, 14185.2095238095, 15089.3333333333, 15099.35, 13786.5434782609,
    12725.9, 11228.6052631579, 13599.3636363636, 15020.0681818182,
    13639.619047619, 13430.3636363636, 14378.4761904762, 14089.5136363636,
    13764.9761904762, 14563.75, 15415.6, 16239.9047619048, 16138.3333333333,
    17002.25, 16113.1818181818, 14587.5952380952, 14962, 14154.5454545455,
    12431.119047619, 12235.0454545455, 13490.45, 14660.8095238095,
    14974.5, 14925.4782608696, 18028.8888888889, 21131.3333333333,
    20585.9090909091, 26185.7142857143, 30468.8636363636, 29702.619047619,
    32551.1363636364, 31891.5909090909, 34400.5263157895, 36821.5909090909,
    41078.25, 46125.2272727273, 49956.5789473684, 51783.3333333333,
    41551.6666666667, 33400.2272727273, 27649.6363636364, 29548.4,
    31156, 30505.6363636364, 26053.5555555556, 27774.7727272727,
    28064.9523809524, 31093.0526315789, 28776.8181818182, 25656.5,
    22562.5714285714, 20106.9565217391, 19111.8, 17781.8636363636,
    12144.8695652174, 10776.5, 9846.92857142857, 11562.9523809524,
    10410.75, 9710.72727272727, 11331.6, 12763.3611111111, 14961.9523809524,
    16024.5, 19375.925, 17404.6363636364, 18489.4772727273,
16911.3333333333,
    17121.6190476191, 18405.55, 19060.55, 22467.1739130435, 26028.5,
    21930, 19411.2954545455, 19548.5227272727, 21448.7857142857,
    22690.1363636364, 23793.6785714286, 22836.2272727273, 24099.5714285714,
    25621.225, 28412.175, 26710.347826087, 26332.1666666667,
    24164.55, 22420.9318181818, 23847.9523809524, 21864.6818181818,
    20377.5909090909, 19039.0476190476, 17873, 18246.0375, 19908.619047619,
    20393.6666666667, 18660.8068181818, 17892.8157894737, 16968.3181818182,
    16603.6842105263, 16128.4090909091, 15703.9886363636, 17287.9625,
    17168.7391304348, 16335.3636363636, 17448.5, 17494.0681818182,
    17690.1, 16731.7, 15629.3095238095, 14948.2261904762, 14280.275,
    13750.3152173913, 14308.2619047619, 13801.3928571429, 14117.652173913,
    13684.0119047619, 13924.55, 14101.25, 14203.55, 15678.0952380952,
    17373.6, 19401.075, 18628.8095238095, 19117.652173913, 18600.2,
    18034.7954545455, 15812.3695652174, 15807.05, 15962.0476190476,
    14849.1904761905, 14573.8375, 13755.5, 12830.925, 13511.3421052632,
    12825.2272727273, 11413.097826087, 10386, 9937.54545454545,
    10316.8295454545, 9244.33333333333, 16973.5879043457, 17090.2131635254,
    17460.5904, 17041.7126, 17180.6047364746, 17211.4672635254,
    17090.2131635254, 17008.6433, 17295.2439, 16682.3584635254,
    15599.8932729492, 14698.2004635254, 14360.8925270508, 13679.6671,
    13624.5516, 13344.5659364746, 12683.1799364746, 12583.9720364746,
    13419.5208635254, 14477.7384635254, 14969.3698, 15101.647,
    15826.9648270508, 15954.8338635254, 16124.5885270508, 15930.5862729492,
    12813.2503635254, 12628.0644364746, 12736.0886635254, 11098.0569454407,
    11316.3155364746, 12187.1404364746, 12678.7717729492, 12420.8269270508,
    11982.1097, 12041.6333635254, 12079.1108270508, 13040.3273,
    13459.2051, 13787.6913270508, 13523.1369270508, 13351.1808729492,
    13190.2425364746, 12811.0489729492, 12751.5199270508, 12747.1117635254,
    12769.1579635254, 12341.4638364746, 12112.1801270508, 12292.9632729492,
    12403.1942729492, 12440.6717364746, 12577.3571, 12658.9269635254,
    12442.8731270508, 12299.5728270508, 12087.9325364746, 11739.6015,
    11988.7246364746, 11719.7620729492, 11073.4002945129, 10946.00019729,
    11210.6010202576, 11806.4007862915, 11909.8002093018, 12385.799788855,
    12755.8002592163, 12647.2001182495, 12325.2009963135, 12391.1000807129,
    9903.15330911865, 9114.00053773193, 7910.00069096069, 7922.90022076416,
    7313.6001175354, 5686.10035791016, 5474.20003143616, 5378.30060886841,
    5386.59989170837, 5404.30016187134, 5397.20013902588, 5542.80073070679,
    6092.29997819824, 6428.10017742309, 6712.70055303955, 6688.40048432617,
    6631.50072766113, 6699.80035043945, 6747.40003927612, 6610.39980692749,
    6404.90013120117, 6556.10040924072, 6706.30023869019, 6763.80012994995,
    6927.30047599487, 6829.40049264526, 6808.10042410889, 6690.10064141235,
    6772.90037717895, 6752.60075723267, 6777.60053444214, 7035.80037026367,
    7148.80060119629, 7322.7003647644, 7408.50010070801, 7252.7005848999,
    7312.40052114258, 7337.40029835205, 7409.60079599609, 7873.70062153931,
    8760.50040474243, 10195.9005791382, 10143.7003069397, 9935.50069833374,
    9605.20061156616, 8715.6006487854, 8269.69997210693, 7922.40033286743,
    6837.30047068481, 6744.09997180176, 6592.00016466675, 6156.09993710937,
    6270.50025783081, 6390.90057890625, 6324.60050771484, 6096.00035966797,
    5924.40021498413, 5905.2006175293, 5707.00011245117, 6061.40044733276,
    5981.3006444458, 5615.20037615356, 5610.7579, 5582.09810911865,
    5515.95950911865, 5557.84715455933, 5698.72202270508, 5709.30444091186,
    5673.59979650879, 5647.72000130615, 5575.09564178467, 5550.09115499878,
    5514.45042627563, 5515.89492064209, 5483.09137531128, 5612.10012927856,
    5637.04540996094, 5836.2497276123, 6113.15801934814, 6631.99994276123,
    6980.49985119018, 6784.21076134033, 6643.33320200806, 6025.90907946167,
    5735.71439985962, 5765.80926595459, 5913.09992277222, 5792.75006144409,
    5666.30466264038, 5596.49999379883, 5505.47363722839, 5115.45443951416,
    4978.54537110901, 4819.00006005859, 4510.68192121277, 4687.23822227783,
    4643.81828251038, 4781.69987844543, 4950.71412927856, 5442.45008562927,
    5405.52162415466, 5383.05257147217, 5493.47632770691, 5497.63622927551,
    5300.95254420166, 5173.31831375122, 5318.72715839233, 5479.28570116882,
    6139.09096548462, 5935.50001380005, 6187.61911367188, 5431.00009616699,
    5516.30431739197, 5854.44416690063, 5924.52400956421, 6646.81828855591,
    6640.75033574219, 6966.13631591797, 6333.7498690979, 6209.54555587158,
    6381.6664451477, 6275.78978494263, 6257.14255352173, 6189.28563092651,
    6205.24974938354, 6467.2502133728, 6401.90484060059, 6183.7498602478,
    6245.52201011352, 6109.52357194824, 6093.71419671631, 5933.09072906494,
    5993.85704796753, 5826.23784094849, 5871.13625131226, 5874.3158881897,
    5901.31588978271, 5707.59082789307, 5704.84985441895, 5560.28570594788,
    5435.78267101746, 5423.29993872681, 5493.47632770691, 5556.52174519653,
    5650.99988488159, 5506.19050203857, 5200.49997043457, 5236.49997255859,
    5468.63633969726, 5707.00011245117, 5868.15778062744, 5967.85682216797,
    5651.19028632812, 5690.26295066528, 5479.77280592957, 5424.49987151794,
    5477.99998680725, 5262.36833021851, 5102.99996468201, 5262,
    5292.95454545455, 5381.75, 5643.15789473684, 5262.72727272727,
    5210.45454545455, 5224.28571428571, 5336.81818181818, 5425.2380952381,
    5835.22727272727, 5719.11764705882, 5926.25, 5669, 5463.8,
    5398.05, 5437.95652173913, 5461.18181818182, 5329.7619047619,
    5314.34782608696, 5471.80952380952, 5266.45454545455, 5264.81818181818,
    5227.4, 5168.5, 5121.25, 5049.63636363636, 4949.77777777778,
    4951.78947368421, 4829.90476190476, 4343.59090909091, 3929.45454545455,
    3698.36842105263, 3749.17391304348, 4059.31818181818, 4022.47058823529,
    3867.95454545455, 3724.61111111111, 3850.75, 4026.47619047619,
    4152.81818181818, 4303.33333333333, 4313.78260869565, 3832.2380952381,
    3961.7619047619, 4242.08695652174, 4228.95238095238, 4227.19047619048,
    4445.27272727273, 4572.3, 4592.66666666667, 4561.25, 4734,
    4678.04761904762, 4733.26086956522, 4802.55, 4907.27272727273,
    5227.4347826087, 5363.35, 6058.40714285714, 6432.4880952381,
    6662.7, 7602.20652173913, 8909, 9391.63157894737, 9158.63636363636,
    9021.90909090909, 9021.42857142857, 9015.68181818182, 9043.21428571429,
    9038.65, 8473.71428571429, 7705.65, 8106.325, 8442.23809523809,
    8133.80952380952, 8099.25, 7604.36363636364, 7180.92857142857,
    7228.13636363636, 6770.58181818182, 6415.47619047619, 6173.71818181818,
    6762.5, 7067.3619047619, 7788.865, 7949.02173913044, 8859.65555555556,
    8793.16666666667, 7858.77272727273, 8356.44047619048, 8436.68181818182,
    8975.02380952381, 9809.5, 10038.4090909091, 11125.9473684211,
    11331.4545454545, 12890.2, 13787.25, 13952.8421052632, 14162.1428571429,
    14078.8333333333, 14732.7272727273, 15046.9772727273, 14989.05,
    16068.347826087, 16660.7727272727, 16244.7222222222, 16310.9318181818,
    17269.8095238095, 19799.0789473684, 21646.0227272727, 23853.6,
    22133.4285714286, 22955.6304347826, 19935, 18306.5909090909,
    14423.347826087, 13674.15, 11292.1428571429, 11563.3333333333,
    11074.775, 10689.4090909091, 11830.2, 13871.8333333333,
15009.0714285714,
    13903.5909090909, 14761.65, 14936.2272727273, 15037.4090909091,
    14966.1428571429, 15588.6904761905, 17710.2, 16352.3, 17509.1304347826,
    18634.6, 17565.0263157895, 17257.7272727273, 18206.5227272727,
    20733.0714285714, 22694.3636363636, 26237.0952380952, 25403.0909090909,
    26103.8571428571, 27439.35, 31619.65, 30590.9347826087,
32347.6944444444,
    28571.4, 25519.6818181818, 27398.0952380952, 24056.7272727273,
    22526.6022727273, 21868.6428571429, 21291.7045454545, 19386.9125,
    21547.7023809524, 24293.3095238095, 22985.4318181818, 22114.5263157895,
    20295.3409090909, 19255.3947368421, 18546.0909090909, 18675.5568181818,
    20771.2625, 21233.6956521739, 20713.0681818182, 22880.8947368421,
    24598.8977272727, 24211.7375, 23302, 21589.6428571429, 20781.619047619,
    20267.4, 19563.8260869565, 21638.2142857143, 22735.0714285714,
    23101.5869565217, 22826.880952381, 22762.125, 22063.8636363636,
    22820.675, 23024.3095238095, 23405.2, 23271.25, 22762, 22424.0108695652,
    22231.05, 21090.5227272727, 19830.4130434783, 20033.475,
    19829.7142857143, 19454.119047619, 18233.9125, 17421.9090909091,
    15900.875, 15803.5921052632, 15064.9431818182, 15071.5326086957,
    15163.775, 15453.3409090909, 15794.6136363636, 14745.2857142857,
    773.821533203125, 868.620361328125, 740.75244140625, 707.68310546875,
    701.069091796875, 676.818359375, 712.09228515625, 767.207763671875,
    795.867919921875, 804.686279296875, 800.277099609375, 782.64013671875,
    776.026123046875, 731.933837890625, 756.1845703125, 824.52783203125,
    850.9833984375, 839.960205078125, 864.2109375, 954.600341796875,
    936.96337890625, 890.66650390625, 875.234130859375, 848.77880859375,
    820.11865234375, 824.52783203125, 791.45849609375, 740.75244140625,
    749.57080078125, 692.250732421875, 723.115478515625, 714.296875,
    749.57080078125, 751.775390625, 709.8876953125, 670.20458984375,
    698.864501953125, 681.2275390625, 679.02294921875, 696.659912109375,
    736.343017578125, 718.7060546875, 740.75244140625, 809.095458984375,
    837.755615234375, 857.59716796875, 859.8017578125, 857.59716796875,
    956.80517578125, 998.69287109375, 1040.58056640625, 1003.10205078125,
    1000.8974609375, 941.372802734375, 853.18798828125, 833.346435546875,
    873.029541015625, 844.369384765625, 857.59716796875, 859.8017578125,
    866.415771484375, 886.25732421875, 921.53125, 930.349609375,
    879.643310546875, 809.095458984375, 767.207763671875, 734.138427734375,
    690.046142578125, 632.725830078125, 597.451904296875, 685.63671875,
    643.7490234375, 608.47509765625, 626.112060546875, 659.181396484375,
    707.68310546875, 804.686279296875, 806.890869140625, 815.70947265625,
    870.824951171875, 886.25732421875, 820.11865234375, 798.072509765625,
    758.38916015625, 740.75244140625, 731.933837890625, 760.593994140625,
    837.755615234375, 877.438720703125, 828.93701171875, 802.481689453125,
    756.1845703125, 769.412353515625, 846.573974609375, 866.415771484375,
    877.438720703125, 875.234130859375, 981.055908203125, 1069.24072265625,
    1175.0625, 1364.65966796875, 1236.79174804688, 1307.33959960938,
    1329.3857421875, 1518.98315429688, 1556.46166992188, 1591.73559570313,
    1732.83129882813, 1929.49731445313, 1959.9072265625, 1654.39990234375,
    1627, 1537, 1614, 1726, 1626, 1583, 1435, 1450, 1294, 1394,
    1666, 1686, 1775, 1715, 1637, 1615, 1537, 1353, 1278, 1265,
    1206, 1188, 1199, 1254, 1127.73999023438, 1067.44995117188,
    1063.41333007813, 1047.73999023438, 1025.60009765625, 991.64990234375,
    1100.65014648438, 1185.0791015625, 1157.6591796875, 1135.47509765625,
    1217.77270507813, 1304.94995117188, 1377.73681640625, 1385.4091796875,
    1320.4091796875, 1363.40014648438, 1367.119140625, 1158.36376953125,
    1052.23803710938, 1061.47619628906, 1067.57495117188, 1073.625,
    996.760864257813, 1006, 979.26318359375, 928.227294921875,
    930.068176269531, 886.833312988281, 875.727294921875, 918.571411132813,
    929.386352539063, 975.534973144531, 998.59521484375, 969.75,
    937.608703613281, 924.342102050781, 956.714294433594, 966,
    962.642883300781, 946.909118652344, 993.545471191406, 1059.54760742188,
    1150.27270507813, 1112.75, 1154.8095703125, 1030.67504882813,
    1021.69567871094, 1059.13891601563, 1036.61901855469, 1007.77270507813,
    1028.05505371094, 1015.47729492188, 988.025024414063, 980.431823730469,
    1033.1904296875, 1017.15789794922, 1019.69049072266, 1035.83337402344,
    1063.25, 1043.94995117188, 1036.54760742188, 1009.40002441406,
    1000.52172851563, 1007.95239257813, 998.976196289063, 1002.71740722656,
    1044.31433105469, 1036.54760742188, 1086.13635253906, 1178.15795898438,
    1254.34216308594, 1240.02270507813, 1310.125, 1354.97619628906,
    1517.02172851563, 1650.625, 1638.28576660156, 1276.67395019531,
    1171.15002441406, 1101.26196289063, 1096.25, 1043.19995117188,
    1046.75, 1096.17504882813, 1060.36840820313, 1007.59521484375,
    1043.5, 1029.05004882813, 999.636352539063, 941.363647460938,
    965.950012207031, 961.578918457031, 931.75, 1017.6, 1028.79545454545,
    1018.175, 1039.92105263158, 999.909090909091, 1071.22727272727,
    1129.69047619048, 1192.93181818182, 1147.97619047619, 1146.38636363636,
    1185.23529411765, 1178.575, 1098.25, 1113, 1132.52, 1154.07391304348,
    1116.19090909091, 1135.91428571429, 1172.55217391304, 1221.67142857143,
    1091.61363636364, 1058.68181818182, 1059.33, 1032.7, 1021.58,
    1005.62272727273, 970.138888888889, 937.961904761905, 896.928571428571,
    853.304545454545, 830.031818181818, 799.51, 761.739130434782,
    774.345454545454, 757.970588235294, 794.195454545454, 773.47,
    819.995, 807.57619047619, 768.25, 769.422222222222, 795.013043478261,
    748.809523809524, 756.17619047619, 755.086956521739, 764.566666666666,
    794.261904761905, 782.340909090909, 785.665, 790.328571428571,
    756.75, 776.115, 790.661904761905, 828.5, 815.2, 818.940909090909,
    900.104347826087, 914.235, 976.757142857143, 1015.8880952381,
    1085.7875, 1101.79565217391, 1028.9125, 1030.97894736842,
    1018.86363636364, 988.104545454545, 976.8, 980.027272727273,
    1066.95238095238, 1100.23181818182, 1182.1380952381, 1245.55,
    1323.105, 1373.95714285714, 1297.80952380952, 1245.535,
1273.11818181818,
    1196.85714285714, 1300.75, 1396.66363636364, 1483.21904761905,
    1610.65, 1819.355, 2091.76666666667, 2219.745, 2427.65652173913,
    3068.33888888889, 3544.64285714286, 3197.59090909091, 3320.73809523809,
    3339.96590909091, 3394.05952380952, 3829.60227272727, 4378.61363636364,
    4381.44736842105, 3784.86363636364, 3321.375, 3256.18181818182,
    3566.85526315789, 3847.52380952381, 3628.65476190476, 3546.29545454545,
    3244.17045454545, 2887.6, 2979.98913043478, 2554.60227272727,
    2378.59722222222, 2364.40909090909, 2458.47619047619, 2511.18421052632,
    2278.51136363636, 2178.325, 1906.17380952381, 1856.44565217391,
    1734.65, 1744.52272727273, 1303.00434782609, 1169.3625,
1112.90476190476,
    1202.52380952381, 1118.0025, 1223.21818181818, 1388.1375,
    1491.89444444444, 1555.46428571429, 1582.85227272727, 1818.0125,
    1879.14772727273, 2070.84454545455, 2196.54761904762, 2374.0380952381,
    2414.69, 2158.82, 2277.29130434783, 2367.53, 1969.81578947368,
    1746.51136363636, 1847.02727272727, 2047.47619047619, 2150.94318181818,
    2373.5619047619, 2283.29545454545, 2287.32142857143, 2375.8125,
    2473.45, 2341.47826086957, 2371.47777777778, 2159.6, 2234.46590909091,
    2397.75476190476, 2199.23863636364, 2075.22045454545, 1871.41666666667,
    1935.31818181818, 1911.15, 1989.22619047619, 2057.78571428571,
    2035.925, 2002.68421052632, 1928.01136363636, 1855.93421052632,
    1847.75, 1816.31818181818, 2009.85, 1903.95869565217, 1912.39772727273,
    2040.42894736842, 2031.40909090909, 2128.6875, 1929.15, 1855.6,
    1831.0119047619, 1839.0125, 1837.61956521739, 1896.39285714286,
    1846.88095238095, 1884.83695652174, 1866.41666666667, 1974.975,
    2036.93181818182, 2034.525, 2007.90476190476, 2027.2125,
    2058.975, 2128.09523809524, 2310.61956521739, 2326.9875,
    2294.59090909091, 2276.82608695652, 2253.225, 2175.7619047619,
    2113.04761904762, 2097.7625, 2028.72727272727, 2212.725,
    2281.80263157895, 2082.09090909091, 2000.6847826087, 1807.6375,
    1720.22727272727, 1724.34090909091, 1583.30952380952, 84.0490040435678,
    88.34523245295, 79.7263120909169, 75.7896575197835, 72.2667483584157,
    69.6077343196229, 73.0554399445877, 72.8857593626663, 70.3062123746067,
    69.4842872070072, 66.3534646563319, 63.3659416639662, 62.8447866203785,
    62.3497532587845, 62.3821516882946, 61.3250281658416, 59.2499062965604,
    57.3935312450936, 56.458857515431, 58.874894190462, 56.700063900645,
    55.3309399096662, 54.0129823357043, 55.1970254133562, 55.2591751559872,
    54.6101575179788, 51.9562396165992, 50.5856984481846, 50.1022623380861,
    46.5344703066267, 48.2197004587016, 48.1075570319885, 47.7204634251085,
    47.5834457918367, 47.3991025311115, 48.0478564645903, 50.3549958857265,
    54.2731753471865, 55.8272239393016, 57.8973007561971, 60.5284130889203,
    60.0317340701071, 61.1746115716447, 62.5411025780342, 62.3209035757259,
    60.3668296850052, 58.837816873746, 59.5136672634343, 58.7936514922707,
    57.777245732091, 57.9592494280561, 56.6578353675137, 54.1729975833313,
    53.26389507043, 50.3845154305534, 49.8150177139051, 47.0834697736602,
    46.8294038369385, 49.8745867651861, 48.5020114100248, 48.3963672456785,
    49.0647773133828, 49.2982832362356, 50.6841081334784, 50.7860743207755,
    48.4792122992933, 47.4868844333104, 47.5950729608497, 45.3876786412611,
    45.4620062929568, 44.1658924977541, 45.540804173947, 47.6083100984259,
    47.2577931211646, 48.6249885481598, 48.2464205559558, 48.2720492656695,
    49.1181147969089, 47.2287801991246, 47.0487633277041, 48.7832346913249,
    48.0831593754822, 47.2698633806019, 47.4716297493223, 47.8939110381669,
    50.3829981022647, 52.7142510119066, 53.8185252384474, 55.287479758648,
    56.8422265947404, 61.2097846377106, 64.9936836388951, 63.9347066703767,
    69.4317321346478, 68.6790690932525, 75.2198050137077, 77.2714213747885,
    77.8020163002564, 90.5831320826795, 91.8142075850052, 101.86611157024,
    113.728793098488, 90.9250722654284, 93.0693473077664, 87.1911815059986,
    89.8101403590263, 95.1967249528679, 100.754260726341, 100.522662419456,
    95.4549627640591, 93.9658539180204, 91.4518291246892, 89.8878108648484,
    80.5698553052007, 77.653331167409, 81.0396073930035, 78.7711941920148,
    79.7608508936316, 74.7629120403507, 70.8898937966476, 67.4741755746967,
    66.4068256657509, 73.8726729330384, 72.8995135939812, 73.6119520655863,
    73.0307786078087, 74.9073835864882, 81.3266719874504, 86.709886058394,
    79.842528057733, 71.7199429847115, 68.6021819750595, 68.5126590378746,
    68.3620283665974, 67.9720774461766, 66.8217804899572, 63.1843971194637,
    61.5311268030622, 61.9472473534963, 60.8672194620405, 59.9406720983444,
    58.7736183340489, 58.8649005668352, 57.266952186169, 58.4517120537285,
    60.6078607937412, 61.0396589773427, 62.0088818730514, 62.0948258514948,
    62.1994651497547, 64.8143231900986, 64.6203619081302, 63.0498104227068,
    58.8387922851814, 56.609447947545, 57.9306117223866, 57.6419617364056,
    57.2445329684282, 55.3916926480841, 53.2713886681011, 52.0907437206014,
    52.8851909607569, 53.6172954496014, 52.800166608016, 50.6878177805333,
    48.9232092206821, 48.0183168793908, 50.349740800131, 52.401975566019,
    54.8855960582299, 55.1577221521659, 54.5033369872364, 57.9624102037809,
    61.3943122978332, 63.7197451706145, 62.3995082968793, 66.0179503595654,
    69.3121264577143, 75.9891864992355, 77.7075293556642, 82.7946755576351,
    77.2656735731521, 75.0689561416294, 76.0438523771636, 73.345902397957,
    75.564338697067, 78.5867616088899, 78.9932214085458, 74.9295896432597,
    72.2967195095255, 73.944318285506, 73.1733948419093, 69.6889286642845,
    70.444687207558, 71.0863576386183, 70.7866069485131, 71.4595795019883,
    65.4281104946757, 63.1222058117892, 63.4495098553516, 61.7409977987053,
    60.1785759589308, 64.2231633643902, 65.0821881352094, 68.2401439811424,
    68.8917203540011, 70.4137075464977, 68.099595287775, 70.4258465998338,
    69.7369731980808, 69.4963792563556, 70.767001298277, 67.6295293414232,
    65.6476463791982, 64.0929570622803, 61.0703286231864, 59.3686520631409,
    58.3630776699643, 58.5093161505154, 58.7929860972639, 56.835480201929,
    54.537189694026, 54.6812351000363, 54.0969678027438, 54.6795821814824,
    52.9230257963627, 52.8416695311856, 50.9815759183928, 49.59766359691,
    49.5514389180296, 49.6851438022816, 52.2831282198527, 53.9028893935113,
    52.6028014248924, 56.4528335291215, 57.6099394876843, 60.3888569141712,
    59.8179278981437, 60.2921786803723, 62.3650434950523, 65.7209692710653,
    65.8642632715184, 63.9104084300186, 60.8209253057962, 61.9800528232548,
    61.1407764803427, 62.5268270162097, 62.3417884790954, 65.0977255164798,
    61.3651579234905, 59.7212928236916, 61.895433972661, 62.2977233219333,
    61.6424546334369, 59.3482306525554, 58.5615073217751, 59.9908705424892,
    57.5110974779543, 55.1552605712046, 53.5267770308519, 52.1265014647535,
    50.3175993142519, 52.0664343322754, 52.6737594466944, 53.9402489307716,
    54.1459512586067, 55.740870251624, 55.1595970349781, 54.4328831959821,
    55.336713029287, 54.6343612336232, 52.3883440123195, 52.5108213824087,
    52.9132088497096, 55.2214979522273, 55.4108543704588, 57.0204277058391,
    58.5546057286081, 57.4873753104229, 55.5253397832587, 57.8437589227949,
    58.6875809545543, 59.7440897300738, 60.7061285821512, 60.7453784539216,
    64.319041006792, 66.9040718881538, 70.9307217466119, 75.8935029439414,
    80.5965028058563, 81.3882264705328, 81.3302379488912, 77.041366937834,
    79.5355726197669, 81.9314168836392, 80.7919994838746, 81.984168953944,
    85.9412130271598, 86.6013262898641, 87.4206455263252, 93.1388337803488,
    95.7597250077841, 99.7504449474684, 97.7135407153087, 94.9875272203435,
    96.4140921246732, 96.2807782175354, 100.136624665651, 99.8996266284238,
    102.8705002737, 107.366501398571, 115.681805020192, 124.330808949273,
    128.440038556541, 130.193486005171, 148.398294406592, 169.141955796001,
    153.98062101512, 163.130350638717, 165.394926654654, 166.344766096097,
    173.730546379172, 174.22843048399, 177.03984891504, 170.777464540801,
    174.006548081088, 183.763429108158, 201.950300402874, 205.377934749891,
    197.355447236955, 196.534035198473, 179.942279140524, 174.843563649836,
    179.859183834139, 172.772351861966, 162.564431936458, 178.401264381313,
    190.828357799667, 201.066818408567, 198.95230093476, 190.456582152488,
    186.272526123322, 187.96255227177, 174.990331271719, 164.101634545975,
    131.480139344084, 116.138890015906, 107.505603835611, 109.662595259862,
    107.847757051543, 105.537598100625, 112.364967583266, 118.548260822242,
    130.893363526238, 140.02479616256, 161.311322534838, 151.267211994008,
    157.472387955604, 166.651293919402, 176.782077215294, 191.183960749978,
    183.233772753085, 198.980091827993, 220.779933514144, 198.638417369001,
    182.876415419247, 179.369209053134, 198.020446566149, 202.237183322316,
    216.283061387386, 222.652156424696, 233.582525320098, 245.480797702354,
    256.236049098772, 244.209635827157, 250.084123318726, 239.463723045373,
    235.710556842909, 242.229260281278, 232.829738048472, 224.102505816299,
    200.899041579035, 193.275110602996, 192.109152264517, 202.040538241788,
    207.102537348657, 206.923334736302, 203.485453352665, 193.337002609749,
    185.689829933893, 183.151737935442, 172.493429339685, 179.919339468297,
    183.403722516244, 182.12151841937, 192.74123445816, 202.340861272633,
    205.185726249632, 190.636283596618, 183.5457173135, 176.39758892782,
    169.662797631244, 172.664608151962, 180.794650439789, 177.660023721573,
    178.870716177001, 177.93076742149, 179.13297456325, 176.404705952858,
    171.956074256366, 164.985672791525, 169.475624790374, 164.519916512457,
    161.847136849972, 168.756822724519, 168.20948231173, 161.471265038756,
    156.855090317187, 156.421102720723, 148.69522241251, 140.323143961294,
    137.29039684358, 134.60002636344, 133.79441225954, 139.572876142987,
    133.169725664993, 123.246289035037, 119.198370283234, 120.578085593469,
    117.967172695871, 109.097581873352)), row.names = c(NA, -3448L
), .Names = c("date", "variable", "value"), class = "data.frame")

	[[alternative HTML version deleted]]


From s.atasever at gmail.com  Thu Jan 21 10:16:03 2016
From: s.atasever at gmail.com (Sema Atasever)
Date: Thu, 21 Jan 2016 11:16:03 +0200
Subject: [R] Histogram with a wide range of numbers.
Message-ID: <CAAir+CrCjzMc1MgLKNS5rFbsZ+dLYNapM9fcEctk_MB7ieiBqA@mail.gmail.com>

Dear Authorized Sir / Madam,

I need your opinion on something R.
I have a sample text file that includes eValue scores (2nd column).
This column includes a wide range of numbers. For example: 1e-179 or 9.9
You can find this text file in the attachment.

How do I create a histogram in R with logarithmic scale or
which method do i use these values which includes with a wide range of
numbers.

I would appreciate if you could advise on some methods.

Thanks.
-------------- next part --------------
pident	eValue	bitScor
84.72	8e-46	130
88.57	9e-46	130
30.00	4.6	10.0
81.94	1e-43	125
45.66	3e-38	119
45.09	2e-37	117
45.09	2e-37	117
33.33	5.9	12.3
46.24	2e-39	122
45.09	2e-37	117
46.82	5e-40	124
47.49	2e-43	132
35.29	0.18	16.9
40.00	1.9	13.9
29.41	2.2	13.5
95.45	7e-177	478
94.26	2e-176	477
93.39	9e-173	468
pident	eValue	bitScore
93.80	4e-174	471
94.63	3e-175	474
97.56	0.0	492
97.15	0.0	490
95.44	7e-177	478
94.67	5e-176	476
96.34	1e-179	485
99.41	9e-128	347
98.82	5e-127	345
20.45	0.54	15.8
26.32	2.3	13.9
25.00	3.4	13.5
41.67	3.8	13.1
55.56	0.18	16.5
31.58	3.8	12.7
29.17	4.0	13.1
42.86	4.5	11.9
80.00	4.9	11.9
44.44	2.8	13.1
37.50	4.1	11.2
50.00	4.5	10.8
40.00	0.48	14.6
50.00	0.71	14.2
21.74	0.80	13.9
32.00	0.66	14.2
20.00	7.3	11.2
38.89	0.18	15.0
99.44	0.0	706
99.15	0.0	702
99.72	0.0	706
99.44	0.0	703
99.44	0.0	704
50.00	9.9	13.1
50.00	9.4	13.1
50.00	9.5	13.1
34.69	1e-04	28.9
25.00	0.69	16.9
50.00	2.1	15.4
36.00	9.7	13.1
32.26	1.1	16.2
71.43	9.0	13.1
99.62	0.0	536
99.62	0.0	536
99.25	0.0	535
99.25	0.0	535
99.25	0.0	534
99.62	0.0	536
99.62	0.0	536
53.85	0.24	17.7
36.00	8.1	13.1
53.85	0.24	17.7
36.00	8.2	13.1
55.56	3.2	13.5
29.79	7.1	12.7
97.66	8e-91	249
98.44	3e-92	253
99.22	2e-92	254
98.44	2e-91	251
97.66	1e-90	249
99.22	1e-91	251
98.44	8e-91	249
98.44	7e-92	252
98.44	6e-92	253
98.44	4e-92	253
34.18	7e-97	295
53.85	0.90	17.7
20.69	3.6	15.8
44.44	5.7	15.0
37.21	2e-105	317
37.21	1e-105	318
36.84	2e-104	315
36.84	2e-104	315
66.13	0.0	677
99.20	0.0	1019
99.40	0.0	1024
99.20	0.0	1019
57.14	0.69	15.0
62.50	3.2	13.1
57.14	3.3	13.1
57.14	0.66	15.0
55.56	2.5	13.5
57.14	3.3	13.1
19.30	7.8	11.9
66.67	0.29	16.9
77.04	2e-110	317
41.67	0.46	16.2
28.57	0.88	15.4
54.55	3.1	13.9
57.14	3.4	13.9
41.67	0.44	16.5
28.57	0.88	15.4
85.71	1.1	15.0
54.55	3.1	13.9
57.14	3.4	13.5
57.14	0.65	15.0
62.50	3.2	13.1
57.14	3.4	12.7
57.14	0.67	15.0
62.50	3.1	13.1
57.14	3.3	13.1
57.14	0.66	15.0
62.50	3.2	13.1
57.14	3.4	12.7
57.14	0.66	15.0
62.50	3.0	13.1
57.14	3.6	12.7
80.27	2e-118	328
80.63	3e-123	340
77.06	3e-111	309
81.08	5e-122	337
79.91	1e-120	333
78.18	1e-117	326
79.19	1e-118	329
83.71	5e-130	357
77.73	8e-119	329
80.65	2e-117	325
98.99	5e-72	199
97.98	2e-70	195
97.98	2e-70	195
97.98	1e-70	196
97.98	6e-71	196
97.98	1e-71	198
96.97	1e-70	196
97.98	8e-71	196
97.98	4e-70	194
98.99	4e-71	197
50.00	1.5	12.3
29.41	3.6	11.2
25.00	0.83	13.5
50.00	1.5	12.3
29.41	3.4	11.5
80.36	2e-68	191
91.07	9e-77	213
91.96	1e-75	214
81.08	9e-69	192
93.75	2e-77	219
88.39	9e-72	204
91.96	8e-76	214
77.68	1e-66	187
85.59	2e-71	199
91.07	2e-75	214
24.32	0.004	21.6
99.07	2e-163	442
98.60	2e-163	442
99.07	7e-163	440
99.07	2e-163	441
99.07	7e-164	442
99.07	6e-164	443
98.14	5e-163	441
98.60	4e-163	441
98.60	3e-163	441
99.53	3e-164	444
37.50	0.21	17.7
41.67	5.1	13.5
62.50	8.9	12.7
38.89	9.0	12.7
47.28	7e-97	280
48.67	2e-95	276
23.64	0.10	19.6
41.18	1.0	16.2
23.53	1.2	16.2
31.82	2.5	15.0
35.29	2.2	15.0
27.54	1.3	15.8
44.44	1.9	15.0
35.00	0.68	16.2
29.17	0.88	16.5
40.00	5.8	13.9
46.15	7.7	13.5
22.22	0.41	17.3
71.43	1.2	15.8
44.44	3.5	14.2
44.44	6.1	13.9
50.64	6e-104	298
43.75	0.44	17.3
28.57	2.2	15.4
43.75	0.46	17.3
54.55	2.4	15.0
43.75	0.44	17.3
28.57	2.2	15.4
43.75	0.45	17.3
54.55	2.5	15.0
43.75	0.46	17.3
54.55	2.4	15.0
48.16	3e-129	369
26.19	0.11	20.0
40.00	3.6	15.0
88.83	0.0	726
28.26	0.69	16.9
37.50	6.5	13.9
27.59	7.3	13.9
46.15	2.5	15.0
80.00	3.7	14.6
29.03	0.30	18.1
27.27	5.2	13.9
28.57	7.8	13.5
24.64	0.071	19.6
40.00	0.31	17.7
23.08	5.8	13.9
29.17	3.3	14.2
27.14	1.2	16.2
40.00	1.6	15.8
27.78	7.8	13.5
29.03	2.2	14.6
33.33	3.1	14.2
60.93	3e-92	262
71.88	2e-124	343
57.21	4e-89	253
99.56	7e-175	472
57.67	6e-90	256
58.14	6e-91	258
62.27	5e-102	287
99.56	3e-174	470
54.50	1e-76	222
99.56	6e-175	472
97.20	3e-78	216
99.07	3e-80	221
98.13	3e-79	218
98.13	3e-79	218
98.13	8e-79	217
99.07	4e-80	221
98.13	2e-79	219
98.13	1e-79	219
97.20	2e-77	214
97.20	9e-78	214
52.94	0.15	19.2
26.92	0.18	18.9
30.43	0.32	18.1
66.67	0.59	17.3
45.45	1.9	15.8
55.56	3.2	15.0
42.86	0.010	22.3
28.57	2.9	14.6
50.00	3.0	14.6
44.44	7.8	13.1
32.50	0.042	20.8
50.00	0.20	18.9
60.00	2.0	15.8
31.58	5.7	14.2
28.99	7e-04	26.6
27.50	1.5	15.8
28.89	1.4	15.0
29.41	0.81	16.9
44.44	8.6	13.9
42.86	8.9	13.5
41.94	0.47	17.7
60.00	4.9	14.2
80.00	5.7	14.2
57.14	5.8	14.2
50.00	0.49	17.3
50.00	0.47	17.3
85.79	7e-123	347
27.03	0.069	19.6
45.45	1.2	15.8
45.45	2.8	14.6
30.00	1.5	14.6
95.79	2e-136	381
77.78	0.13	18.9
50.00	3.1	14.6
81.05	5e-117	334
77.78	0.15	18.9
35.29	1.0	16.2
27.27	3.0	14.6
85.26	2e-121	344
27.03	0.072	19.6
45.45	1.2	15.8
45.45	2.8	14.6
44.44	1.1	15.0
38.10	7.5	12.3
58.33	0.068	18.5
33.33	0.44	15.4
38.46	6.7	11.9
32.14	2.6	13.9
66.67	2.2	15.4
20.00	4.6	14.2
55.56	3.5	14.6
36.84	4.1	14.2
66.67	2.1	15.4
20.00	4.4	14.2
54.29	8e-134	377
19.35	1.7	15.8
50.00	8.0	13.9
26.19	0.035	21.2
37.50	3.9	14.6
26.19	0.057	20.4
37.50	3.9	14.6
24.53	0.043	21.2
37.50	3.9	14.6
28.57	0.007	23.5
37.50	3.9	14.6
45.45	0.61	17.7
36.36	8.6	13.9
50.00	0.17	17.3
50.00	0.17	17.3
43.75	0.44	16.2
98.74	7e-119	324
39.13	0.19	16.5
33.33	1.9	13.1
31.43	4.1	11.9
42.86	0.23	16.2
37.50	0.49	15.4
27.78	1.9	13.5
44.44	0.67	14.6
28.57	0.78	14.6
28.57	0.98	14.2
44.44	0.99	14.2
45.45	0.070	17.7
66.67	2.3	13.1
42.86	4.6	12.3
50.68	1e-48	145
50.68	6e-49	145
50.64	4e-53	157
70.00	0.028	19.2
40.00	0.28	16.2
26.15	1.0	14.2
44.52	3e-46	138
28.85	0.60	15.0
42.07	3e-41	125
45.45	1.6	17.3
44.00	1.3	17.7
50.00	1.9	17.3
22.99	5.4	15.8
33.33	7.1	15.4
25.93	8.8	15.0
31.25	0.97	18.1
27.63	4.1	16.2
38.46	4.6	15.8
62.50	8.9	15.0
34.62	0.19	20.8
27.27	0.38	20.0
29.17	2.3	17.3
31.82	5.7	16.2
50.00	6.2	15.8
35.71	6.5	15.8
25.00	7.5	15.8
34.78	0.18	20.4
29.03	0.46	19.2
20.69	1.2	17.7
38.10	4.7	15.8
50.00	6.5	15.4
37.50	8.8	15.0
35.14	0.47	19.2
25.53	0.55	18.9
22.00	0.64	18.9
29.31	0.69	18.5
30.77	1.1	18.1
50.00	3.8	16.2
28.57	5.1	15.8
46.67	7.4	15.4
80.00	9.9	15.0
26.19	1.1	17.7
31.43	2.0	16.9
43.75	2.2	16.5
33.33	2.9	16.2
30.00	4.6	15.8
46.67	5.8	15.4
35.29	7.2	15.0
41.18	8.5	15.0
62.50	4.6	15.8
54.55	9.7	14.6
22.64	0.010	23.9
32.56	0.26	19.2
36.67	3.7	15.8
71.43	4.5	15.4
80.00	5.5	15.0
51.66	0.0	647
40.00	6.5	16.2
47.46	0.0	590
52.11	0.0	687
33.33	1.9	17.7
49.85	0.0	622
47.46	0.0	589
46.64	0.0	569
32.00	4.4	16.5
48.96	0.0	634
44.89	0.0	540
33.33	0.91	18.9
49.26	0.0	592
33.33	3.8	16.9
30.00	0.91	15.8
25.00	3.0	14.2
42.11	0.004	22.3
34.48	0.019	20.4
42.86	3.7	13.5
63.97	2e-119	334
35.00	0.48	16.5
66.67	4.4	13.9
35.29	0.015	20.8
30.95	0.48	17.3
31.82	1.5	14.6
18.60	1.6	14.6
35.29	0.32	15.8
35.71	8.2	11.5
44.44	2.4	13.1
35.90	0.020	18.9
41.67	0.66	15.8
38.89	0.74	15.8
35.71	8.4	12.3
25.53	1.8	14.6
66.67	5.0	13.1
29.41	0.77	14.6
28.00	1.5	13.9
25.53	1.6	14.6
66.67	5.1	13.1
33.33	3.5	11.9
25.00	0.30	15.8
22.73	0.90	14.6
46.94	8e-34	103
27.78	0.72	13.5
22.73	0.87	14.6
86.36	0.0	671
99.47	0.0	755
99.47	0.0	754
93.32	0.0	714
99.73	0.0	758
94.39	0.0	724
86.90	0.0	674
86.63	0.0	671
87.17	0.0	676
86.63	0.0	673
31.91	4e-08	30.8
34.88	5e-08	30.4
36.84	0.61	11.9
76.97	0.0	505
21.79	2.2	15.4
38.89	3.1	15.0
73.86	2e-177	487
29.73	0.087	19.6
24.14	1.3	15.8
62.50	4.8	14.2
27.27	6.7	13.9
37.50	7.0	13.5
25.00	7.8	13.5
32.18	4e-45	146
42.42	0.53	17.3
75.83	0.0	532
75.23	0.0	525
75.53	0.0	531
72.51	0.0	503
76.13	0.0	527
66.12	5e-56	164
22.73	0.79	14.6
26.67	4.7	12.3
67.23	5e-55	158
68.64	3e-54	156
68.85	2e-58	171
27.27	0.023	19.2
75.63	1e-63	180
68.91	5e-59	168
33.33	1.3	13.1
73.95	4e-61	174
35.71	1.9	12.7
72.95	7e-61	177
27.27	0.020	19.2
61.48	1e-49	144
80.83	2e-69	199
22.73	0.81	14.6
26.67	4.9	12.3
60.00	0.33	16.5
45.45	1.6	15.0
44.44	8.5	11.9
58.33	0.25	17.3
27.78	0.41	16.5
35.48	1.3	15.0
31.71	0.38	17.3
31.58	0.17	17.7
50.00	0.41	16.5
62.50	1.8	14.6
99.40	0.0	667
32.26	0.40	17.7
62.50	1.5	15.8
25.00	4.1	14.6
32.26	0.39	17.7
62.50	1.5	15.8
25.00	4.3	14.2
50.00	2.2	15.4
35.71	6.0	13.9
45.45	0.081	19.6
36.11	0.72	16.9
29.41	4.1	14.2
36.84	1.5	15.8
36.36	4.9	14.2
45.45	5.8	13.9
38.89	9.5	13.5
99.70	0.0	672
55.56	1.6	16.2
50.00	4.8	14.6
40.00	4.1	14.6
57.14	7.7	13.5
99.70	0.0	672
86.36	1e-129	357
89.69	4e-139	381
87.89	1e-134	369
80.36	5e-124	342
86.16	8e-133	365
85.27	1e-130	360
83.71	1e-128	354
86.36	2e-127	351
83.48	2e-132	364
83.48	8e-136	373
34.38	1.0	14.2
23.94	0.025	18.9
33.33	0.018	18.9
27.78	1.6	13.1
75.00	0.17	16.2
37.50	0.36	15.0
37.50	0.12	16.2
36.36	0.37	15.8
25.58	1.4	13.9
27.08	0.95	14.2
35.71	8.9	11.2
99.78	0.0	943
99.78	0.0	942
99.78	0.0	944
99.56	0.0	941
99.56	0.0	940
99.56	0.0	939
99.34	0.0	937
99.56	0.0	940
99.56	0.0	939
99.34	0.0	937
89.32	9e-68	189
95.33	1e-76	212
81.31	9e-65	182
91.59	3e-72	201
96.26	1e-77	214
97.20	1e-75	214
33.33	0.40	15.4
73.83	3e-58	165
72.90	3e-56	160
73.83	1e-57	163
85.05	1e-69	194
99.22	9e-97	265
98.44	7e-96	262
99.22	8e-97	265
99.22	1e-96	265
99.22	2e-96	264
99.22	3e-96	263
99.22	5e-97	265
99.22	3e-96	263
99.22	2e-96	264
99.22	2e-96	264
98.99	1e-69	193
97.98	3e-69	192
97.98	5e-69	192
96.97	3e-68	189
96.97	4e-68	189
97.98	3e-69	192
96.97	5e-68	189
97.98	1e-68	191
96.97	5e-68	189
96.97	2e-68	190
98.61	7e-52	145
95.83	6e-45	128
96.83	2e-43	124
97.22	6e-51	143
97.22	6e-51	143
80.00	3e-34	100
69.35	1e-28	86.3
53.33	4e-04	20.8
67.74	1e-27	83.6
67.74	1e-27	83.6
99.85	0.0	1407
99.71	0.0	1406
99.71	0.0	1404
99.71	0.0	1405
99.71	0.0	1405
99.56	0.0	1402
99.56	0.0	1404
99.56	0.0	1400
99.71	0.0	1405
99.71	0.0	1404
54.55	0.73	17.7
25.71	1.9	16.2
35.48	2.2	16.2
20.56	3.7	15.4
35.29	6.2	14.6
50.51	0.0	516
43.48	0.33	19.2
31.67	0.67	18.1
27.27	1.4	16.9
25.93	2.3	16.5
55.56	7.0	15.0
54.55	0.71	17.7
25.71	1.7	16.5
35.48	2.2	16.2
20.56	4.0	15.4
35.29	6.2	14.6
35.44	0.061	21.6
51.89	0.0	543
74.90	0.0	842
30.00	2.2	16.2
28.21	3.5	15.8
83.33	9.9	14.2
57.08	0.0	604
57.08	0.0	605
32.30	8e-57	182
29.65	3e-43	148
83.33	4.1	15.8
32.07	3e-56	181
33.33	0.99	17.3
83.33	3.9	15.4
29.41	1e-43	149
83.33	4.0	15.8
42.86	0.75	17.3
19.23	9.5	13.9
28.95	0.58	17.3
27.27	1.2	16.2
50.00	2.1	15.4
26.32	4.3	14.2
25.93	1.2	16.5
42.11	2.1	15.8
25.32	2.5	15.4
50.00	5.3	14.6
24.29	5.8	14.2
26.92	9.7	13.9
99.60	0.0	1027
99.60	0.0	1028
99.60	0.0	1027
99.80	0.0	1029
99.60	0.0	1028
96.98	0.0	1003
99.60	0.0	1028
99.60	0.0	1028
99.60	0.0	1028
99.80	0.0	1030
37.50	1.8	15.0
28.57	5.1	13.9
33.33	0.008	22.3
71.43	1.2	15.4
34.38	4.9	13.5
60.00	5.1	13.5
47.06	0.026	20.4
27.45	2.4	14.2
24.39	8.5	12.3
31.25	9.8	12.3
57.14	7e-99	280
38.46	0.44	16.2
35.71	0.16	18.1
44.44	2.7	14.2
98.63	8e-50	140
50.00	2.8	10.8
21.21	1.0	11.9
100.00	8.9	 9.2
62.50	2e-28	85.9
96.95	7e-121	329
98.78	3e-123	335
98.78	6e-124	337
98.78	3e-123	335
98.78	3e-123	335
98.78	2e-123	336
98.78	2e-123	336
98.78	2e-123	336
98.78	4e-123	335
98.78	1e-123	336
98.78	2e-122	333
98.78	4e-122	332
99.39	5e-123	335
98.78	6e-122	332
98.78	1e-122	333
99.39	5e-123	334
99.39	6e-123	334
99.39	7e-123	334
99.39	5e-123	335
99.39	4e-123	335
99.41	0.0	693
99.41	0.0	694
99.71	0.0	696
99.71	0.0	694
99.71	0.0	696
99.71	0.0	696
99.71	0.0	696
99.71	0.0	696
99.41	0.0	686
99.41	0.0	694
98.91	1e-136	372
35.71	1.8	13.9
57.38	1e-76	219
99.45	9e-137	372
35.71	1.3	14.6
51.65	1e-61	181
51.65	1e-61	181
26.09	1.4	13.9
59.67	1e-78	224
54.35	6e-65	189
54.75	2e-64	187
52.20	2e-62	183
57.14	0.84	15.0
57.14	0.90	13.5
22.22	2.6	11.9
60.00	3.1	11.9
55.56	3.5	11.9
15.79	5.2	11.2
62.50	2.8	11.9
62.50	2.9	11.9
29.63	0.56	14.6
100.00	4.9	10.8
29.63	0.59	14.6
46.15	3.6	15.0
71.43	3.0	15.4
35.71	0.35	17.7
38.46	1.9	15.4
25.00	7.9	13.5
44.44	7.9	13.5
53.33	0.018	21.6
30.61	2.2	15.0
34.78	1.9	15.4
26.32	4.4	14.2
55.56	4.6	14.2
60.00	2.2	15.0
50.00	3.0	15.0
25.00	0.048	20.8
36.36	1.3	16.5
43.75	0.22	18.5
38.10	2.4	15.0
43.75	0.23	18.5
38.10	2.5	15.0
71.04	8e-177	487
69.25	0.0	543
99.70	0.0	670
26.67	0.034	20.8
50.00	8.5	13.1
30.23	0.16	18.5
33.33	1.3	15.8
40.00	0.090	19.2
69.25	0.0	542
68.98	0.0	506
57.87	5e-169	473
23.68	1.2	16.9
44.44	2.3	15.4
84.02	7e-140	382
99.54	2e-168	454
97.26	1e-164	445
99.54	1e-168	455
88.94	8e-148	402
91.74	2e-151	411
85.84	2e-145	396
91.78	2e-152	414
89.50	3e-149	406
84.47	7e-141	385
32.80	4e-15	55.5
24.11	1e-06	32.0
30.91	0.003	21.9
99.38	8e-119	324
98.81	1e-121	331
98.81	2e-121	331
36.80	5e-22	75.1
37.39	4e-16	58.2
28.77	0.017	19.6
99.34	8e-110	300
98.70	4e-111	303
98.70	2e-111	305
98.70	5e-112	306
98.70	8e-112	305
99.35	8e-113	308
98.70	3e-111	303
98.70	1e-111	305
98.70	3e-111	304
98.70	5e-112	306
60.00	0.73	16.9
83.33	1.5	15.8
83.33	4.4	14.6
85.71	7.0	13.9
97.43	0.0	729
32.69	1.1	16.5
57.89	0.19	18.1
50.00	1.8	15.0
66.67	1.6	16.2
62.50	3.3	15.0
45.45	3.4	15.0
27.62	0.005	24.6
28.95	0.28	18.9
27.03	4.8	15.0
27.62	0.002	25.8
28.95	0.27	18.9
27.03	4.6	15.0
30.00	4.8	15.0
27.62	0.005	24.3
28.95	0.29	18.9
27.03	5.0	14.6
37.14	0.039	20.8
31.82	1.0	16.5
50.00	1.1	16.2
38.46	1.9	15.4
42.86	1.9	15.4
33.33	4.5	14.2
37.50	7.6	13.5
50.00	8.3	13.5
98.99	0.0	604
97.98	0.0	599
98.30	0.0	595
98.31	0.0	598
98.64	0.0	600
98.32	0.0	602
97.98	0.0	601
97.98	0.0	600
98.31	0.0	597
97.97	0.0	595
25.76	1.4	16.9
25.00	1.5	16.9
23.44	0.32	18.5
23.44	0.21	18.9
31.58	0.26	18.9
23.44	0.21	18.9
31.58	0.26	18.9
23.44	0.22	18.9
31.58	0.26	18.9
23.44	0.19	19.2
31.58	0.27	18.5
23.44	0.23	18.9
31.58	0.26	18.9
23.44	0.20	18.9
31.58	0.26	18.9
23.44	0.23	18.9
31.58	0.27	18.5
23.44	0.20	18.9
31.58	0.25	18.9
31.25	0.64	15.4
31.43	6.6	12.3
30.43	0.57	14.2
37.50	8.9	10.8
55.56	0.32	15.4
40.00	1.3	13.5
29.03	3.1	13.1
22.22	3.5	13.1
33.33	3.4	12.3
83.33	0.51	14.6
29.27	1.3	13.5
42.86	1.9	13.1
100.00	2e-119	330
25.47	2e-11	47.0
99.17	0.0	744
99.45	0.0	746
64.03	4e-169	469
64.31	1e-169	470
99.72	0.0	749
55.92	3e-140	396
55.65	4e-139	392
81.87	0.0	608
72.93	0.0	533
72.93	0.0	534
99.62	0.0	539
98.86	0.0	534
98.86	0.0	535
98.86	0.0	533
99.62	0.0	538
98.86	0.0	535
98.86	0.0	534
98.86	0.0	535
99.24	0.0	537
99.24	0.0	536
99.12	1e-165	448
57.83	4e-85	243
99.12	4e-165	447
99.28	3e-99	276
99.56	1e-166	450
98.67	2e-163	442
98.67	1e-163	443
98.67	2e-163	442
99.12	1e-164	445
98.67	3e-165	447
25.00	4.2	13.9
100.00	8.3	13.1
33.33	0.12	19.6
80.00	1.8	15.8
30.00	1.9	15.8
50.00	2.3	15.4
42.86	4.0	14.6
30.23	0.77	16.9
35.71	1.1	16.5
28.00	1.2	16.2
66.67	3.5	15.0
38.46	8.6	13.5
28.95	1.5	15.4
17.86	5.1	13.9
28.07	6.0	13.5
28.95	1.6	15.4
17.86	5.0	13.9
28.07	6.1	13.5
66.67	8.3	13.1
71.43	3.8	14.6
45.45	1.0	15.8
50.00	1.2	15.8
44.44	1.4	15.4
66.67	5.0	13.9
99.66	0.0	616
24.39	2.9	13.9
40.00	3.0	14.6
25.00	2.1	15.0
46.15	4.1	14.2
34.29	0.50	16.9
50.00	0.10	18.9
30.16	0.26	17.7
20.99	0.69	16.2
32.35	0.89	15.8
30.00	0.45	16.9
57.14	3.0	14.6
26.67	3.7	14.2
31.58	4.1	14.2
73.46	2e-85	239
83.95	4e-107	294
74.69	4e-95	263
74.69	3e-95	264
85.19	2e-106	292
98.15	3e-121	330
83.33	6e-103	283
82.72	1e-102	283
72.84	2e-91	254
72.05	8e-83	232
71.77	7e-67	188
57.85	9e-53	152
72.58	3e-67	189
56.78	1e-49	144
55.93	5e-49	142
70.73	2e-67	189
63.11	6e-59	168
63.11	3e-59	169
58.68	4e-53	153
67.21	5e-62	176
47.62	0.038	21.6
35.29	7.2	14.2
57.14	8.3	14.2
42.86	8.8	13.9
53.56	2e-142	404
27.59	0.075	20.4
26.53	2.3	15.8
30.43	2.7	15.8
41.67	2.8	15.4
29.17	2.9	15.4
34.62	6.8	14.2
20.83	2.5	15.8
23.81	2.6	15.8
33.33	8.7	14.2
20.00	0.13	19.6
31.25	0.77	17.3
27.78	0.43	18.5
46.15	3.4	15.8
30.00	4.3	15.4
66.67	7.4	14.6
27.78	0.44	18.5
46.15	3.4	15.8
30.00	4.4	15.4
66.67	7.5	14.6
26.92	1.8	16.2
37.50	7.7	13.9
25.81	7.3	13.9
99.04	4e-157	425
99.04	2e-156	423
99.52	1e-157	426
99.04	4e-157	425
99.04	1e-156	424
99.04	1e-157	426
99.52	9e-158	427
99.04	2e-157	426
99.52	6e-158	427
99.04	4e-157	425
42.86	5.0	11.9
38.46	8.6	11.2
46.15	1.4	14.6
97.28	9e-137	371
22.50	2.5	13.5
66.67	0.42	16.5
25.00	0.65	15.8
25.00	1.0	15.4
31.25	5.2	13.5
66.67	6.6	13.1
42.14	9e-31	100
44.66	1e-21	73.9
50.00	1e-24	82.4
85.71	0.77	15.4
34.78	3.9	13.1
31.58	6.4	12.7
40.00	5.1	12.7
31.58	9.9	11.9
27.27	1.0	15.8
55.56	4.6	13.5
40.00	0.82	15.4
57.14	0.95	15.0
33.33	0.38	16.2
33.33	2.3	13.9
36.36	2.6	13.9
50.00	3.3	13.5
45.45	4.4	13.1
94.29	3e-44	126
97.06	3e-46	131
97.06	2e-45	129
97.06	3e-46	131
89.04	2e-39	119
89.04	2e-39	119
97.06	8e-19	64.3
95.59	6e-46	131
75.00	2e-29	90.1
96.30	2e-16	53.1
75.36	8e-35	102
96.67	7e-18	57.4
98.04	4e-37	106
98.04	1e-36	105
98.04	6e-37	105
98.04	2e-36	104
98.04	4e-36	103
98.04	3e-36	103
98.04	2e-36	104
98.04	2e-36	104
98.04	2e-37	107
98.04	9e-37	105
25.93	1.2	16.2
37.50	2.9	15.0
25.00	4.8	14.2
29.31	2.1	15.0
45.45	3.7	14.2
29.31	2.1	15.0
45.45	3.7	14.2
19.61	4.0	13.9
35.00	7.1	13.1
37.84	2.1	15.0
45.45	3.7	14.2
24.56	0.027	21.2
40.00	2.2	15.0
23.81	0.34	17.7
30.00	0.37	17.3
32.50	1.4	15.8
83.33	4.5	13.9
50.00	2.5	14.6
35.71	3.5	14.2
86.76	4e-123	339
85.65	1e-123	340
99.55	6e-165	446
86.11	8e-124	341
85.65	2e-123	340
86.11	3e-124	342
91.28	2e-136	374
90.95	8e-139	380
91.86	6e-140	382
86.64	9e-128	351
25.81	0.014	21.6
45.45	3.4	13.9
30.77	4.5	13.5
25.81	0.011	21.9
45.45	3.6	13.9
30.77	4.4	13.9
33.33	2.5	13.9
33.33	0.36	16.9
36.36	9.9	12.7
21.95	1.3	15.0
24.14	0.46	16.2
57.14	0.45	16.5
29.17	0.35	16.2
57.14	3.6	13.1
25.00	9.1	11.9
27.78	0.34	16.9
18.18	4.7	12.7
24.56	1.6	13.9
35.29	3.4	12.7
23.81	6.6	11.9
20.97	0.059	18.5
33.33	8.3	12.3
33.33	5.4	12.3
58.33	0.60	15.4
66.67	5.4	12.7
28.21	0.88	14.6
28.12	0.021	20.0
28.57	0.050	18.9
38.46	0.81	13.9
53.68	2e-52	153
52.55	7e-52	152
60.00	8.1	11.2
28.72	0.021	19.2
25.88	0.45	15.4
66.67	0.42	14.6
95.45	0.002	24.6
24.66	2.2	15.0
24.66	2.3	14.6
60.00	0.44	16.9
28.95	0.79	16.2
37.50	1.9	15.0
40.00	3.5	13.9
23.33	3.6	13.9
58.33	1.9	14.6
71.43	1.1	15.0
75.00	0.70	16.5
33.33	2.6	15.0
31.58	3.3	14.6
45.45	4.3	14.2
21.05	6.8	13.5
33.33	8.3	13.5
41.67	9.8	13.1
25.00	0.004	20.4
62.50	0.70	13.9
50.00	2.9	11.9
66.67	0.072	16.5
25.00	0.004	20.4
62.50	0.70	13.9
23.44	0.028	17.7
62.50	0.70	13.9
50.00	0.32	14.6
98.91	2e-135	368
98.37	2e-134	365
98.91	8e-136	369
99.46	2e-136	370
98.91	5e-136	369
98.37	2e-135	368
97.83	6e-134	364
98.91	5e-136	369
98.91	1e-135	368
97.83	1e-134	366
99.76	0.0	850
99.76	0.0	846
99.76	0.0	846
55.86	2e-171	480
35.48	5e-73	227
38.10	0.74	17.7
34.68	2e-67	212
47.37	0.18	19.6
34.68	3e-67	212
47.37	0.19	19.6
41.18	2.0	16.2
41.67	2.2	16.2
55.56	4.7	15.0
44.44	4.9	15.0
56.63	8e-163	457
29.17	3.5	15.4
42.86	4.8	15.0
35.71	5.9	14.6
71.70	8e-115	318
40.74	0.095	18.5
83.33	0.19	16.9
40.00	6.3	12.3
34.78	0.70	15.4
28.12	2.5	13.9
40.00	6.0	12.7
45.45	5.3	12.7
100.00	2e-168	455
49.03	7e-70	204
25.00	1.3	15.0
33.33	4.3	13.5
29.55	7e-10	38.1
41.18	0.10	15.8
87.50	3e-71	198
84.55	3e-69	193
38.46	5.4	11.5
60.00	0.015	17.7
50.00	8.0	10.0
25.00	0.14	16.2
37.27	3e-22	72.8
86.49	1e-68	196
36.84	0.47	16.9
41.18	3.7	13.1
50.00	1.8	14.2
25.00	3.4	13.5
49.53	3e-71	207
21.95	0.39	16.5
34.29	0.71	15.8
26.67	3.5	13.5
66.67	0.45	15.8
31.25	0.62	15.8
41.67	5.2	13.1
24.14	0.74	16.2
42.86	3.9	13.9
66.67	7.5	13.1
47.28	5e-61	181
46.20	2e-59	177
99.35	0.0	619
99.68	0.0	621
47.77	1e-101	293
57.14	9.1	12.7
51.82	2e-119	338
56.03	5e-120	339
50.65	4e-108	309
57.33	5e-131	367
99.68	0.0	621
98.81	0.0	1025
38.10	0.021	22.3
31.82	0.48	18.1
29.27	0.56	17.7
38.46	4.6	15.0
24.24	0.80	17.7
47.37	1.4	16.9
62.50	6.4	14.6
100.00	5.2	15.0
34.62	6.6	14.6
43.24	0.002	25.8
31.82	0.50	18.1
38.46	0.63	17.7
73.43	0.0	761
73.23	0.0	758
40.54	0.037	21.6
31.82	0.50	18.1
29.27	0.54	18.1
33.33	3.1	15.4
66.67	2.1	16.5
46.15	3.5	15.8
35.29	0.38	18.5
55.56	2.5	15.8
100.00	3.6	15.4
62.50	9.4	13.9
99.44	1e-125	342
99.44	1e-125	342
99.44	1e-125	342
99.44	4e-125	341
99.44	2e-124	339
99.44	1e-125	342
99.44	1e-125	342
99.44	1e-125	342
99.44	1e-125	342
99.44	1e-125	342
33.33	0.044	19.2
71.43	0.82	15.4
57.14	1.2	15.0
33.33	2.5	13.9
26.32	0.97	15.0
66.67	2.9	13.5
50.00	0.40	15.8
35.29	2.3	13.5
31.25	2.7	13.5
33.33	6.0	12.3
36.36	6.8	12.3
27.27	5.7	12.3
42.86	8.7	11.5
30.61	0.33	16.2
80.00	0.72	15.4
66.67	4.8	13.1
28.57	0.16	17.7
22.58	0.074	18.5
21.74	5.6	13.1
50.00	4.3	13.5
17.86	0.007	22.3
26.88	0.15	18.1
41.67	3.4	13.5
26.56	5.0	13.1
34.78	5.1	13.1
57.14	9.6	12.3
18.52	5.7	13.1
35.00	4.4	13.1
43.75	1.7	15.4
47.06	0.11	19.2
20.00	0.59	16.9
57.14	0.12	19.6
40.00	0.24	18.9
42.86	0.65	17.3
25.00	4.2	14.6
33.78	0.094	20.0
40.00	0.24	18.9
42.86	0.65	17.3
25.00	4.7	14.6
55.56	5.8	14.2
40.52	3e-81	244
26.19	1.4	16.5
30.43	2.2	16.2
41.67	1.7	16.2
50.00	1.7	16.2
28.57	3.8	15.0
27.03	4.2	15.0
36.36	5.3	14.6
35.71	6.2	14.2
28.12	1.7	15.8
49.82	1e-86	252
21.88	3.9	14.2
50.00	4.5	13.9
37.67	9e-56	174
37.67	5e-56	174
36.96	5e-56	174
52.85	9e-81	237
35.71	0.21	18.1
38.89	2.8	14.6
57.14	3.9	13.9
35.47	1e-50	160
37.67	5e-56	174
37.34	2e-56	175
60.07	2e-128	360
99.44	0.0	1469
99.44	0.0	1472
99.44	0.0	1472
99.44	0.0	1471
99.30	0.0	1471
99.44	0.0	1471
99.58	0.0	1473
97.60	0.0	1457
62.50	1.8	17.3
23.40	1.9	17.3
99.44	0.0	1471
21.62	0.087	18.5
29.17	0.36	16.5
35.71	2.5	14.2
45.45	0.67	15.0
57.14	8.2	11.5
21.62	0.40	16.9
41.18	1.0	15.4
29.41	5.3	13.5
26.92	0.49	16.5
33.33	4.9	13.5
33.33	0.53	16.5
45.45	1.8	14.6
55.56	6.8	13.1
38.46	9.7	11.5
35.00	1.4	15.0
42.86	4.7	13.5
36.84	0.018	21.2
29.17	7.9	14.2
44.44	0.54	17.7
25.00	1.8	16.2
27.66	3.2	15.4
71.43	5.8	14.6
36.67	0.13	19.6
28.57	0.087	19.6
83.33	1.3	16.2
42.11	6.5	13.9
35.71	6.8	13.9
33.33	1.9	15.8
44.44	0.53	17.7
25.00	2.5	15.8
37.50	0.44	18.1
30.43	2.8	15.4
66.67	6.2	14.2
20.51	2.0	15.4
45.45	8.3	13.5
33.33	8.8	13.5
33.33	3.4	15.4
83.33	8.2	14.2
32.14	1.2	16.5
33.33	2.8	15.4
83.33	7.6	14.2
99.19	0.0	1023
99.60	0.0	1027
99.60	0.0	1026
99.60	0.0	1027
99.60	0.0	1027
99.60	0.0	1027
99.80	0.0	1028
99.80	0.0	1028
99.80	0.0	1028
99.39	0.0	1024
98.46	4e-97	266
98.46	1e-96	264
98.46	2e-97	266
98.46	3e-97	266
98.46	3e-97	266
98.46	5e-97	265
98.46	3e-97	266
98.46	7e-97	265
98.46	8e-97	265
98.46	1e-96	264
99.56	9e-169	456
37.13	3e-38	122
37.13	1e-38	123
46.15	4e-72	211
45.70	2e-71	208
44.44	0.46	16.5
40.00	0.52	16.2
26.32	4.4	13.5
35.00	0.44	16.2
57.14	3.4	13.9
99.12	2e-166	451
36.84	0.60	15.8
83.07	3e-122	335
54.55	2.4	13.5
66.67	2.9	13.1
26.32	1.4	13.9
37.50	2.2	13.5
66.67	5.5	12.3
25.93	2.7	13.5
66.67	3.2	13.5
71.43	6.5	12.3
57.14	0.88	14.6
43.75	0.12	17.3
60.00	1.5	13.9
85.71	0.36	16.2
40.00	0.64	15.4
25.00	1.1	14.6
53.85	4.2	12.7
92.11	4e-131	358
99.66	0.0	608
99.32	0.0	607
99.32	0.0	608
99.32	0.0	607
99.32	0.0	608
25.25	0.71	16.5
50.00	2.1	15.0
50.00	2.5	14.6
44.44	0.093	19.6
37.50	3.8	14.2
25.23	0.18	18.1
46.15	1.4	15.4
50.00	2.9	14.2
33.33	4.4	14.2
30.00	6.1	13.9
55.56	7.2	13.9
18.42	8.5	13.5
29.63	0.29	18.9
46.67	4.2	15.4
27.66	0.30	18.9
30.00	0.49	17.7
28.57	1.1	16.5
35.00	1.3	16.5
24.77	0.20	19.2
28.26	2.3	15.8
53.33	0.21	19.2
38.46	1.6	16.5
41.67	0.20	19.2
27.27	3.2	15.4
53.85	5.5	14.6
18.55	9.0	13.9
25.00	0.21	19.2
35.48	0.25	19.2
36.84	1.5	16.5
40.00	2.7	15.8
43.48	5.1	15.0
21.98	6.3	14.6
28.89	0.065	20.8
24.14	5.7	14.6
62.50	5.7	14.6
63.64	0.83	17.3
24.32	1.2	16.9
50.00	2.2	16.2
54.55	2.7	15.8
71.43	3.3	15.4
34.55	9e-10	35.4
36.36	3e-10	36.6
36.36	2e-10	37.4
36.36	1e-10	37.7
36.36	3e-10	37.0
36.36	2e-10	37.0
36.36	3e-11	39.7
34.55	3e-09	33.9
36.36	3e-10	37.0
36.36	3e-10	37.0
45.45	0.10	15.4
40.00	4.7	10.4
36.36	0.053	16.9
66.67	2.3	11.9
42.86	2.9	11.5
45.00	0.047	15.8
54.55	0.86	12.3
50.00	0.21	13.9
25.00	0.075	15.4
31.25	1.8	11.5
77.93	9e-113	313
80.63	6e-123	339
93.69	3e-151	411
81.08	6e-121	334
79.73	5e-118	327
83.33	2e-126	348
83.33	1e-125	346
80.54	2e-123	341
79.73	6e-122	337
81.08	4e-122	337
98.90	0.0	570
99.30	0.0	596
99.65	0.0	599
99.30	0.0	595
89.90	0.0	553
99.30	0.0	596
98.95	0.0	592
28.77	0.063	19.2
30.00	0.57	16.5
50.00	2.5	14.2
19.05	4.5	13.5
24.05	0.029	21.2
27.27	3.7	14.6
33.33	1.9	16.9
33.33	3.4	16.2
37.50	4.5	15.8
23.81	7.0	15.0
53.27	0.0	548
53.93	0.0	549
55.58	0.0	566
55.39	0.0	563
55.60	0.0	563
55.60	0.0	563
53.52	0.0	542
31.25	3.3	16.2
21.31	3.7	15.8
36.36	5.3	15.4
55.56	5.9	15.4
31.25	3.2	16.2
21.31	3.7	16.2
36.36	5.7	15.4
55.56	6.3	15.4
54.55	1.2	16.2
50.00	2.6	15.4
26.92	4.5	14.6
30.77	5.6	14.2
57.14	6.2	14.2
22.58	8.5	13.5
61.26	7e-164	457
21.43	2.2	15.4
26.09	6.3	13.9
21.43	0.51	17.7
33.33	0.84	16.9
35.00	1.3	16.5
16.13	1.7	16.2
40.00	7.7	13.9
44.44	0.005	23.1
24.00	3.5	14.6
61.26	4e-164	458
26.67	2.2	15.4
33.33	2.2	15.0
36.36	8.5	13.1
23.53	1.4	16.2
57.14	7.1	13.9
40.00	5.1	14.2
50.00	5.6	13.9
27.14	0.062	20.8
38.10	0.65	17.3
25.93	0.67	17.3
34.78	2.8	15.4
66.67	4.3	14.6
66.86	2e-174	480
52.23	3e-124	352
24.72	0.14	19.6
50.00	5.3	14.6
36.24	1e-35	122
29.35	0.079	20.4
20.54	0.33	18.5
36.24	1e-35	122
29.35	0.080	20.4
20.54	0.33	18.5
29.03	0.73	16.9
28.57	2.5	15.4
30.00	9.5	13.5
29.41	0.85	16.9
66.67	5.3	14.6
50.00	0.087	16.9
44.44	1.7	13.1
50.00	6.9	11.2
50.00	2.9	11.9
26.67	5.8	10.8
27.91	0.66	13.5
37.50	1.2	12.7
50.00	2.8	11.9
26.67	4.1	11.2
50.00	2.7	11.9
26.67	8.2	10.4
50.00	2.7	11.9
26.67	8.9	10.4
30.00	2.6	11.9
25.00	0.12	15.8
35.71	2.1	12.3
39.62	1e-11	41.2
42.86	2.3	10.8
39.22	3e-12	42.4
49.06	3e-19	61.2
32.00	0.001	19.6
44.74	2e-10	37.4
47.17	7e-15	49.7
75.00	0.98	11.5
43.80	1e-34	106
50.42	5e-37	112
43.80	4e-34	105
43.80	4e-34	105
41.67	1.2	13.1
25.93	0.015	18.5
28.00	0.64	13.9
37.50	4.6	11.5
45.45	3.1	13.5
45.45	6.5	13.5
88.49	8e-177	479
35.29	4.2	13.9
18.92	8.2	12.7
55.13	3e-90	258
22.22	2.0	15.0
46.67	4.6	13.9
44.44	6.9	13.1
57.14	0.98	15.8
24.56	4.1	13.9
34.92	0.025	20.4
26.14	0.62	16.2
35.29	1.5	15.0
57.14	2.6	14.2
98.78	2e-122	333
98.78	4e-122	332
98.78	4e-122	332
98.78	9e-122	331
98.78	9e-122	331
98.78	9e-122	331
99.39	5e-123	335
98.78	1e-122	333
98.78	9e-122	331
98.78	6e-122	332
98.17	6e-122	332
98.17	1e-121	331
98.17	1e-121	331
98.17	2e-121	330
98.17	2e-121	330
98.17	2e-121	330
98.17	2e-121	330
98.17	5e-122	332
99.39	6e-124	337
98.78	2e-122	333
52.31	3e-42	129
48.68	4e-39	121
25.00	2.8	13.1
63.29	8e-74	210
53.59	1e-56	166
46.11	6e-37	116
48.03	1e-38	120
25.00	2.7	13.1
48.03	4e-38	119
25.00	2.3	13.5
48.68	2e-39	121
25.00	2.7	13.1
48.68	1e-39	122
25.00	2.9	13.1
46.84	1e-39	122
66.47	4e-93	260
70.37	1e-90	253
56.71	2e-71	204
70.99	8e-92	256
66.86	6e-90	252
99.39	4e-124	338
70.86	4e-99	275
67.44	6e-95	265
38.46	7.9	11.9
71.26	1e-95	266
98.76	8e-122	332
50.00	1.1	15.0
50.00	9.4	12.3
27.27	0.76	15.8
30.00	3.2	13.9
42.86	3.4	13.5
35.48	0.21	17.3
21.13	0.14	18.5
30.56	4.4	13.9
27.78	5.7	12.3
21.13	0.14	18.5
30.56	4.3	13.9
21.13	0.14	18.5
30.56	4.2	13.9
37.50	4.5	15.0
55.86	2e-171	480
55.63	4e-171	479
43.60	3e-120	350
55.89	3e-170	477
55.89	4e-170	477
93.79	0.0	798
35.00	0.46	17.7
50.00	0.47	17.7
28.30	2.9	15.4
42.86	8.0	13.9
83.33	3.0	15.4
36.84	5.6	14.6
50.00	8.2	13.9
40.00	1.7	15.4
71.43	4.1	14.2
40.91	6.0	13.5
24.24	4.8	13.1
50.00	0.88	15.8
40.00	2.5	14.6
54.55	0.024	20.4
60.00	3.0	13.9
34.29	2.4	14.2
66.67	2.7	14.2
66.67	4.5	13.5
28.00	5.8	13.1
93.64	4e-144	393
92.27	2e-142	389
87.44	8e-139	380
90.99	2e-143	391
82.57	6e-128	352
67.12	4e-98	276
82.59	1e-122	338
88.99	4e-137	375
82.27	6e-126	347
87.61	2e-135	371
55.00	5e-53	156
58.09	6e-50	148
55.71	4e-54	159
52.98	4e-59	171
66.67	2.1	13.1
57.25	3e-58	169
52.87	2e-59	173
55.70	2e-63	182
62.86	1e-67	193
63.57	2e-68	194
54.14	1e-50	149
71.43	2.5	14.2
33.33	5.7	13.1
28.89	9.6	11.5
25.71	0.42	15.4
33.33	0.48	15.4
50.00	0.55	15.0
40.91	0.76	14.6
42.86	2.2	14.2
42.86	2.3	14.2
66.67	5.7	13.1
42.86	2.3	14.2
66.67	5.8	13.1
53.04	1e-120	341
53.04	2e-120	341
53.35	1e-121	343
79.23	0.0	527
79.23	0.0	528
79.42	0.0	526
73.89	3e-180	493
79.23	0.0	528
79.30	0.0	530
72.87	1e-177	488
25.71	0.23	16.2
30.00	5.7	11.9
41.18	1.1	14.2
50.00	3.3	13.1
71.43	0.43	15.4
45.45	0.79	15.8
27.27	1.6	14.6
55.56	0.13	17.7
41.67	1.0	14.6
36.36	2.8	13.5
44.44	4.7	13.1
46.15	0.30	16.5
23.08	0.13	17.3
41.18	0.22	19.2
38.89	4.1	15.0
50.00	2.6	15.8
55.56	3.9	15.0
33.33	7.9	14.2
21.77	0.27	18.5
35.71	1.4	16.2
26.92	1.9	15.8
24.24	4.6	14.6
80.00	2.8	15.4
45.45	2.8	15.4
45.45	5.6	14.6
42.86	0.65	17.3
36.36	3.9	14.6
45.45	4.9	14.6
42.52	3e-93	276
42.86	0.65	17.3
36.36	4.0	14.6
45.45	4.8	14.6
42.86	0.65	17.3
36.36	3.8	15.0
45.45	4.9	14.6
26.79	1.7	15.8
33.33	0.70	17.3
58.33	0.92	16.9
44.44	1.9	15.8
99.62	0.0	1076
99.62	0.0	1076
99.62	0.0	1075
99.62	0.0	1075
22.22	2.4	16.2
31.25	4.6	15.4
41.18	4.9	15.0
33.33	8.9	14.2
99.81	0.0	1077
99.81	0.0	1078
73.44	0.0	809
73.24	0.0	807
99.61	0.0	1072
40.00	1.4	13.1
25.00	3.8	11.9
40.00	2.3	12.3
40.00	1.6	13.1
24.24	2e-04	25.0
27.36	0.005	20.4
36.36	2.8	12.3
40.00	1.2	13.5
25.00	3.0	12.3
40.00	1.5	13.1
25.00	3.5	11.9
50.00	0.34	15.4
36.36	2.0	13.1
38.46	2.8	12.3
41.67	4.8	11.9
40.00	0.066	17.3
71.43	0.27	15.4
99.40	0.0	690
99.40	0.0	690
99.11	0.0	686
99.11	0.0	688
99.11	0.0	689
99.70	0.0	691
99.40	0.0	692
99.40	0.0	686
99.11	0.0	689
99.11	0.0	688
99.07	2e-79	219
50.00	6.0	11.2
26.92	0.73	13.5
36.79	9e-27	84.7
57.14	0.75	13.5
93.46	3e-75	209
23.46	0.95	16.9
66.67	8.2	13.9
40.53	2e-89	269
38.46	1.8	16.2
40.21	2e-88	267
33.33	1.2	16.9
38.46	1.8	16.5
40.47	1e-89	271
30.00	1.1	16.9
33.33	1.1	16.9
38.46	1.9	16.2
34.09	0.21	19.2
32.14	3.3	15.4
27.87	0.90	17.7
80.00	1.5	16.9
39.39	3.3	15.8
40.21	5e-88	266
33.33	1.2	16.9
38.46	1.8	16.5
99.58	0.0	946
26.67	0.039	21.9
28.00	3.5	15.8
28.57	5.2	15.4
50.00	6.3	15.0
21.95	8.4	14.6
83.33	9.8	14.2
97.47	0.0	920
99.50	9e-146	395
29.55	0.32	16.5
60.00	3.6	13.1
28.57	1.0	14.6
42.86	1.1	14.6
26.32	4.5	12.7
39.39	5.9	12.3
40.00	0.031	19.6
50.00	2.1	13.9
60.00	5.5	12.7
31.58	0.13	17.7
37.50	4.4	13.1
34.78	5.1	12.7
27.45	0.26	16.5
35.29	1.3	14.6
55.56	2.7	13.5
42.86	1.2	14.2
21.43	2.4	13.5
28.57	2.4	13.5
26.32	4.5	12.7
30.19	1.3	13.9
60.82	2e-73	209
57.14	5e-63	182
66.67	5e-74	210
56.55	3e-62	181
56.55	2e-62	181
55.95	9e-62	179
57.14	3e-63	182
56.55	9e-63	182
56.55	1e-62	181
67.52	2e-73	209
42.11	8.9	15.8
24.39	2.3	16.9
28.12	4.7	15.8
50.00	6.2	15.4
26.32	6.4	15.4
37.50	7.8	15.0
99.86	0.0	1487
99.72	0.0	1485
99.72	0.0	1485
18.89	1.2	18.1
21.88	2.4	16.9
41.67	1.5	17.7
39.29	5.2	15.8
26.47	1.1	17.7
36.67	1.1	17.7
23.26	5.5	15.4
46.15	5.7	15.4
62.50	8.3	15.0
45.45	8.4	15.0
55.56	1.5	17.3
22.50	0.052	22.7
45.45	1.4	18.1
33.33	1.9	17.7
57.14	3.1	14.2
25.00	7.7	13.1
27.78	5.2	13.9
99.29	0.0	575
99.65	0.0	577
99.29	0.0	574
24.00	4.4	13.9
23.08	4.9	13.9
20.95	0.007	22.3
33.33	0.77	15.8
45.45	2.8	14.2
26.79	0.26	17.3
50.00	2.4	14.2
33.33	9.7	12.3
35.71	1.9	15.8
25.81	7.4	13.9
45.45	1.1	16.5
40.00	1.3	16.2
38.46	3.4	14.6
38.46	3.5	14.6
46.15	4.4	14.2
21.43	6.6	13.9
45.45	1.1	16.5
40.00	1.3	16.2
38.46	3.4	14.6
38.46	3.5	14.6
46.15	4.3	14.6
21.43	6.6	13.9
44.44	1.6	15.4
27.27	2.4	15.0
80.00	6.2	13.5
26.98	0.018	20.8
30.77	0.10	18.1
47.06	0.27	16.9
80.00	0.78	15.4
99.70	0.0	671
30.61	0.014	22.3
32.35	1.7	15.4
44.44	0.19	17.7
21.28	2.0	14.2
25.71	0.90	15.4
24.44	0.84	15.4
26.03	0.10	17.7
24.44	0.84	15.4
24.44	0.88	15.4
30.00	0.62	16.2
41.67	0.95	15.4
62.50	5.0	13.5
38.46	0.23	18.9
45.45	4.9	14.6
48.40	7e-101	295
45.74	9e-103	301
55.56	4.1	14.2
37.50	4.3	14.2
55.56	2.7	15.8
35.00	8.5	14.2
40.33	2e-69	215
41.30	2e-89	267
34.31	3e-54	175
29.27	0.10	20.0
55.00	0.35	18.1
70.00	0.47	17.7
60.00	1.8	16.2
58.33	0.46	17.7
100.00	1.9	15.8
58.33	2.1	15.4
67.29	0.0	694
67.15	0.0	700
66.67	0.0	701
67.49	0.0	696
63.98	0.0	662
64.18	0.0	664
67.70	0.0	701
86.39	0.0	892
85.77	0.0	885
66.32	0.0	689
51.64	6e-141	397
83.06	0.0	637
51.78	2e-140	395
96.12	0.0	725
51.64	1e-138	394
96.68	0.0	734
97.51	0.0	739
95.84	0.0	724
97.78	0.0	741
96.12	0.0	727
92.25	2e-85	236
96.09	4e-94	258
92.19	1e-84	234
69.35	2e-61	175
73.39	7e-64	181
98.45	1e-97	267
99.22	1e-98	270
93.80	7e-78	217
96.90	1e-95	262
68.55	6e-61	173
98.70	9e-112	305
98.05	4e-111	303
98.05	2e-110	302
98.70	2e-111	304
98.05	1e-110	302
98.70	1e-111	305
98.05	3e-110	301
98.05	4e-111	303
98.05	4e-110	301
98.05	2e-110	301
50.00	0.019	15.8
45.45	0.64	12.7
67.42	2e-114	318
63.80	2e-108	302
79.55	3e-136	374
63.01	7e-107	299
25.00	3.3	13.9
79.09	2e-135	371
78.18	2e-133	366
79.09	2e-135	372
78.64	8e-135	369
79.55	4e-136	372
78.18	1e-134	369
97.99	5e-111	303
98.66	1e-111	304
99.33	4e-112	305
97.99	4e-111	303
99.33	6e-112	305
98.66	1e-111	304
99.33	6e-112	305
98.66	1e-111	304
99.33	7e-112	305
99.33	4e-112	305
98.16	9e-121	328
96.93	1e-118	323
97.55	1e-119	326
97.55	1e-119	326
97.55	3e-119	325
97.55	2e-119	325
97.55	2e-120	328
98.32	1e-87	243
97.54	1e-88	245
97.55	1e-119	326
75.00	0.012	19.6
27.50	0.055	17.7
43.75	0.87	14.2
25.45	4.8	11.9
26.79	0.062	18.5
60.19	4e-94	265
30.30	0.14	17.7
33.33	2.0	14.2
28.26	0.14	17.7
27.78	8.3	12.7
55.56	1.6	14.2
35.00	0.35	16.2
53.15	0.0	520
55.25	0.0	567
50.94	9e-176	495
99.79	0.0	977
49.37	2e-175	494
54.83	0.0	540
55.04	0.0	564
99.16	0.0	971
54.55	0.85	17.3
27.91	1.1	16.9
94.33	0.0	929
98.99	0.0	622
98.99	0.0	623
99.33	0.0	622
99.33	0.0	624
98.66	0.0	615
99.35	0.0	645
99.66	0.0	623
99.34	0.0	629
99.33	0.0	618
99.66	0.0	625
46.67	1.4	13.9
46.67	1.3	13.9
46.67	1.4	13.9
46.67	1.4	13.9
56.06	5e-56	162
33.33	0.19	16.5
33.33	0.20	16.5
57.14	0.023	18.9
35.71	1.00	14.2
33.33	0.57	15.8
83.33	1.8	14.2
39.29	0.063	17.7
30.43	0.22	16.2
31.25	8.4	11.5
43.33	0.066	17.7
30.43	0.21	16.2
38.46	4.6	12.3
31.25	8.0	11.5
46.67	0.025	19.2
38.46	3.5	12.7
33.33	0.46	15.4
50.00	3.0	13.5
32.00	3.9	13.1
37.50	0.37	16.2
50.00	2.1	13.9
66.67	6.5	12.3
40.00	0.16	16.9
45.45	0.23	16.5
66.67	0.79	15.0
34.15	0.69	15.4
45.45	2.5	13.5
45.45	9.9	11.9
32.73	0.003	21.9
45.45	3.4	12.7
27.59	2.2	13.9
26.32	2.1	11.9
28.57	0.68	13.9
22.86	7.4	10.8
38.46	0.87	12.7
27.27	2.6	11.5
38.46	0.92	12.7
38.46	0.85	12.7
27.27	2.6	11.5
50.00	0.061	15.4
66.67	0.48	14.2
29.23	6.2	13.1
55.56	7.4	13.1
66.67	3.6	14.6
31.43	4.5	14.2
62.50	5.1	14.2
31.25	6.5	13.9
44.44	7.8	13.5
54.55	9.4	13.1
33.33	2.0	14.6
50.00	3.8	13.9
57.14	6.8	13.1
66.67	8.4	12.7
29.23	7.3	13.1
55.56	7.4	13.1
100.00	0.60	16.2
31.58	1.6	15.0
50.00	4.1	13.9
35.71	8.9	13.1
42.86	4.6	13.9
42.86	4.5	13.9
25.00	1.7	15.0
25.40	0.71	16.5
25.93	3.3	14.2
33.33	7.7	13.1
27.78	0.75	16.5
62.50	3.9	14.2
62.50	6.2	13.9
57.14	6.7	13.5
25.00	1.3	15.8
37.50	1.6	15.4
23.81	9.6	13.1
33.33	1.3	15.4
31.43	0.28	17.3
31.43	0.28	17.3
40.00	0.99	16.2
31.25	5.4	13.9
60.00	1.5	15.4
23.53	1.5	15.4
97.37	8e-85	233
99.12	5e-87	239
89.47	6e-79	218
88.60	1e-77	215
88.60	2e-78	217
88.60	4e-78	216
89.38	4e-78	216
89.38	4e-78	216
89.38	4e-78	216
89.47	1e-78	218
38.24	0.46	16.2
25.00	1.9	14.2
95.45	2e-04	26.2
55.56	3.8	12.7
40.00	3.1	13.5
50.00	5.8	12.7
36.36	9.5	11.9
40.00	9.5	11.5
97.25	4e-125	350
32.04	4e-23	82.8
29.19	8e-17	64.7
21.82	2.9	14.2
35.29	1.6	14.6
50.00	2.2	14.2
31.25	7.0	12.7
99.26	2e-99	272
41.67	1.8	13.1
98.53	1e-98	270
99.26	1e-99	273
99.26	2e-99	272
37.50	2.1	13.5
99.57	0.0	946
99.78	0.0	947
99.57	0.0	942
99.57	0.0	943
99.78	0.0	945
20.44	0.23	19.2
25.53	0.46	18.1
40.00	1.2	16.5
35.71	6.3	14.6
54.81	0.0	508
20.44	0.26	18.9
25.53	0.42	18.1
40.00	1.3	16.5
35.71	6.3	14.6
99.78	0.0	944
100.00	0.0	931
91.92	3e-66	185
92.93	3e-67	187
92.93	4e-67	187
91.92	2e-66	185
94.95	3e-68	190
96.97	6e-69	191
91.92	1e-66	186
92.93	6e-67	186
93.94	1e-67	188
92.93	3e-66	184
43.75	0.034	16.9
38.10	9.1	10.4
44.44	0.32	17.7
26.92	0.92	16.5
40.00	4.8	14.2
40.00	8.2	13.5
71.43	1.1	15.8
25.93	4.4	13.9
19.39	0.066	20.0
46.67	0.30	18.1
62.50	1.2	16.2
36.36	1.6	15.8
61.32	3e-134	375
37.93	0.056	20.4
27.87	0.53	17.3
26.09	0.97	16.2
53.85	1.0	16.2
30.77	5.8	13.9
37.50	8.2	13.5
57.89	0.041	20.8
27.66	0.41	17.7
37.04	0.72	16.9
27.78	0.83	16.5
50.00	4.2	14.2
24.64	1.2	16.5
21.79	1.6	16.2
21.79	1.6	16.2
27.08	2.7	15.4
21.79	0.74	17.3
24.64	1.2	16.5
24.64	1.3	16.5
22.00	3.0	15.0
24.64	1.2	16.5
21.79	2.5	15.4
28.57	0.075	20.4
35.48	0.71	17.3
25.64	1.1	16.5
28.57	0.70	17.7
46.67	1.3	16.5
99.75	0.0	835
33.33	7.2	14.6
33.33	9.7	14.2
60.00	0.88	17.3
57.14	0.81	12.7
57.14	0.76	12.7
57.14	0.73	12.7
46.15	0.78	12.7
32.00	0.31	13.5
99.65	0.0	583
99.30	0.0	580
98.95	0.0	578
99.30	0.0	582
99.30	0.0	582
99.30	0.0	582
99.30	0.0	582
99.30	0.0	582
99.30	0.0	582
99.30	0.0	582
52.41	6e-62	181
37.58	5e-40	124
35.80	9e-30	97.1
44.32	9e-52	154
37.74	2e-39	122
34.50	8e-37	115
35.19	6e-29	95.1
35.80	6e-30	98.2
54.89	2e-64	188
36.97	2e-39	122
26.47	0.46	15.4
40.00	0.65	14.6
34.78	2.2	13.9
46.15	3.9	13.1
23.88	5.4	11.9
25.00	0.079	17.7
62.50	0.50	15.4
23.81	9.6	11.2
35.29	1.8	18.5
25.00	0.98	18.9
29.03	5.0	16.5
29.63	8.7	15.8
29.17	9.9	15.4
53.85	2.4	18.1
33.33	9.7	16.2
35.29	1.8	18.5
18.93	8e-12	55.5
21.17	6e-08	43.1
20.97	2e-04	31.2
30.77	1.2	19.2
55.56	5.6	16.9
54.55	7.1	16.5
24.79	0.16	21.9
27.66	1.5	18.5
46.15	2.5	17.7
37.50	3.1	17.7
55.56	3.1	17.7
22.86	0.44	20.0
46.15	2.6	18.1
46.15	2.6	18.1
23.33	0.013	22.3
46.15	1.9	15.4
31.25	2.9	15.0
31.58	6.0	13.9
23.33	0.015	22.3
46.15	1.9	15.4
31.25	2.9	15.0
31.58	6.0	13.9
23.33	0.016	21.9
46.15	1.9	15.4
31.25	2.8	15.0
31.58	6.0	13.9
23.33	0.016	21.9
46.15	1.8	15.4
31.25	2.7	15.0
31.58	5.8	13.9
22.68	0.11	19.2
37.04	1.3	15.8
31.25	2.8	15.0
31.58	6.2	13.9
30.43	0.28	17.7
55.56	2.2	15.0
23.33	0.013	22.3
46.15	1.9	15.4
31.25	2.8	15.0
31.58	5.9	13.9
23.33	0.013	22.3
46.15	1.9	15.4
31.25	2.8	15.0
31.58	6.0	13.9
99.25	0.0	834
99.25	0.0	835
99.50	0.0	836
99.50	0.0	838
99.50	0.0	838
99.50	0.0	838
99.50	0.0	837
99.75	0.0	839
99.00	0.0	833
99.50	0.0	837
38.10	1.5	16.2
33.33	1.5	16.2
20.69	2.7	15.0
30.00	7.4	13.9
29.63	1.2	16.2
25.00	2.8	15.0
36.84	1.3	16.2
25.00	4.5	14.6
30.56	2.5	15.0
26.92	7.1	13.5
66.67	8.5	13.9
35.71	1.1	16.9
27.59	2.4	15.8
26.47	2.8	14.6
26.67	5.0	13.9
26.32	9.3	13.1
25.93	0.084	20.4
55.56	2.0	16.2
62.50	4.6	15.0
25.00	0.22	18.5
71.43	1.9	15.4
31.25	4.2	14.6
30.43	5.2	14.2
26.92	0.096	20.4
44.44	2.3	15.8
33.33	4.7	15.0
35.71	1.2	16.9
26.09	2.1	15.8
33.33	4.0	15.0
50.00	0.006	23.5
45.45	3.3	14.6
66.67	7.6	13.5
38.46	3.7	14.6
44.14	2e-107	313
44.41	1e-107	313
30.30	0.55	17.7
20.63	0.76	17.3
29.41	4.9	14.6
44.44	6.2	14.2
50.00	0.93	16.5
38.46	2.4	15.4
30.30	0.55	17.7
20.63	0.79	17.3
29.41	4.9	14.6
44.44	6.2	14.2
65.67	6e-108	302
44.68	5e-73	213
29.03	0.36	16.9
44.26	1e-72	212
29.03	0.35	16.9
44.68	6e-73	213
29.03	0.35	16.9
99.59	0.0	503
43.40	9e-71	207
29.03	0.31	16.9
40.34	3e-50	154
45.45	2.5	14.2
36.36	3.3	13.9
33.33	4.1	13.9
28.57	6.6	13.1
45.45	2.5	14.2
36.36	3.2	13.9
33.33	4.2	13.5
28.57	6.4	13.1
50.00	5.9	13.1
35.04	1e-65	204
25.71	0.049	21.2
38.46	0.26	18.9
30.77	1.5	16.2
35.71	2.7	15.4
62.50	0.17	19.2
62.50	4.4	14.6
62.50	7.1	13.9
26.44	0.002	25.4
28.57	6.0	14.6
40.00	8.1	14.2
99.46	0.0	753
23.26	0.96	16.5
25.37	1.2	16.5
32.35	2.2	15.8
50.00	3.1	15.0
28.95	0.10	19.6
52.94	0.19	18.5
42.11	5.3	14.2
33.33	3.6	14.6
57.14	7.1	13.5
21.62	9.5	13.1
71.43	3.5	15.0
31.58	5.0	14.6
55.56	6.6	14.2
98.96	0.0	737
99.76	0.0	822
99.53	0.0	837
99.53	0.0	835
99.53	0.0	836
99.76	0.0	839
99.53	0.0	836
99.53	0.0	836
99.53	0.0	836
99.29	0.0	833
41.13	4e-40	120
99.29	2e-105	288
98.58	8e-105	286
99.29	1e-105	288
99.29	2e-105	288
98.58	2e-104	285
98.58	4e-105	289
40.00	0.22	15.4
28.57	0.22	15.4
47.83	2e-43	131
50.00	0.57	14.2
99.05	4e-77	216
91.67	2e-05	28.1
57.14	4.8	11.9
99.05	4e-77	216
91.67	2e-05	28.1
57.14	4.8	11.9
99.05	7e-77	216
91.67	2e-05	28.1
57.14	4.8	11.9
99.05	5e-77	216
91.67	2e-05	28.1
57.14	4.9	11.9
99.05	1e-76	215
91.67	2e-05	28.1
57.14	4.9	11.9
99.05	3e-77	216
91.67	2e-05	28.1
57.14	4.8	11.9
99.05	4e-77	216
91.67	2e-05	27.7
57.14	4.7	11.9
94.26	9e-82	228
91.67	2e-05	27.7
57.14	5.0	11.9
96.33	7e-78	218
91.67	2e-05	28.1
57.14	5.2	11.9
99.05	8e-77	215
91.67	2e-05	28.1
57.14	4.9	11.9
36.00	1.3	17.3
45.00	1.8	16.9
53.85	4.6	15.8
46.15	5.1	15.8
54.55	7.3	15.0
29.73	0.61	17.7
66.67	0.64	17.7
66.67	7.1	14.6
29.31	1.2	16.5
24.44	2.5	15.4
35.71	4.3	14.6
28.57	8.6	14.2
28.57	8.2	14.6
28.57	8.5	14.2
99.81	0.0	1022
28.57	8.8	14.2
34.62	0.25	19.2
66.67	0.68	17.7
54.55	9.6	13.9
83.33	5.2	15.4
21.43	5.1	11.5
21.43	4.8	11.5
30.43	0.60	14.6
50.00	3.7	12.3
42.86	2.2	12.3
25.00	0.11	16.5
21.43	4.7	11.9
21.43	4.7	11.9
20.69	0.79	14.6
75.41	3e-62	177
74.80	1e-65	185
75.78	7e-67	189
80.95	6e-77	214
99.21	2e-94	259
74.60	3e-64	182
98.41	2e-93	256
75.61	3e-41	123
89.19	3e-20	68.6
98.41	2e-93	256
88.62	2e-79	221
99.56	2e-177	478
49.32	3e-73	213
42.86	2.7	13.9
80.09	3e-137	377
87.17	4e-151	411
47.98	2e-71	209
42.86	2.8	13.9
48.43	1e-71	209
42.86	2.7	13.9
49.32	1e-72	211
49.77	7e-73	213
49.77	5e-71	209
50.23	5e-73	214
26.44	0.22	18.1
36.00	0.78	16.2
23.81	1.8	15.0
33.33	2.4	14.6
26.44	0.25	17.7
36.00	0.81	16.2
23.81	1.7	15.0
41.18	2.9	14.2
26.44	0.25	17.7
36.00	0.76	16.2
23.81	1.7	15.0
41.18	2.9	14.2
26.44	0.24	17.7
36.00	0.81	16.2
23.81	1.6	15.0
41.18	2.9	14.2
26.44	0.12	18.9
36.00	0.83	16.2
23.81	1.6	15.0
41.18	2.9	14.2
26.44	0.23	17.7
36.00	0.79	16.2
23.81	1.6	15.0
41.18	2.9	14.2
27.08	1.8	15.0
60.00	0.18	18.1
37.04	0.77	16.2
28.57	1.4	16.2
21.52	0.024	20.8
26.67	1.3	15.4
44.44	2.0	14.6
28.57	2.9	14.2
33.33	6.5	13.1
38.10	0.19	17.3
35.00	0.45	16.2
36.36	0.26	16.9
99.46	1e-137	374
66.67	1.3	14.6
38.89	3.2	13.5
62.50	0.001	23.9
35.29	1.5	14.2
34.78	3.0	13.5
35.29	0.37	16.2
40.74	4.5	12.7
50.00	3.1	13.9
66.67	4.3	13.5
66.67	4.2	12.7
39.13	3.1	13.5
62.50	3.5	13.1
99.39	4e-123	335
31.03	0.13	16.5
80.00	1.2	13.5
24.14	3.7	13.1
28.30	0.089	17.3
50.00	2.6	12.7
24.69	6.0	16.5
46.67	0.37	20.4
66.67	5.9	16.5
25.00	8.7	16.2
22.09	0.093	22.3
85.71	1.6	18.5
23.33	6.6	16.2
30.43	0.023	25.4
36.84	0.83	20.4
25.00	2.7	18.5
41.67	4.6	17.7
53.85	2.7	18.1
31.58	4.6	17.3
25.81	5.7	16.9
50.00	1.4	18.5
18.18	5.6	16.5
32.61	0.059	22.7
27.94	0.92	19.2
50.00	7.1	16.5
21.10	0.66	20.0
50.00	1.5	18.9
28.57	0.051	23.1
28.57	3.0	17.3
42.11	4.3	16.5
27.78	8.4	15.8
44.44	3.2	11.5
29.17	2.8	12.7
42.11	0.12	16.2
25.00	0.53	13.9
66.67	0.28	14.6
50.00	0.39	14.2
32.43	2.7	13.1
45.45	0.035	17.7
66.67	0.27	14.6
50.00	0.38	14.2
26.23	0.13	19.2
35.29	3.4	15.0
35.71	5.3	14.2
27.59	6.9	13.9
26.23	0.13	19.6
35.29	3.4	15.0
35.71	5.3	14.2
27.59	7.1	13.9
26.23	0.13	19.6
35.29	3.4	15.0
35.71	5.3	14.2
27.59	7.0	13.9
38.69	1e-89	268
42.12	1e-93	278
27.27	0.039	21.2
28.57	2.0	15.8
50.00	8.1	13.9
35.29	9.3	13.5
26.23	0.13	19.2
35.29	3.5	15.0
35.71	5.2	14.2
27.59	6.3	13.9
26.23	0.13	19.2
35.29	3.4	15.0
35.71	5.3	14.2
27.59	7.1	13.9
41.67	6.9	14.2
31.82	9.6	13.9
41.67	6.9	14.2
31.82	9.1	13.9
99.62	0.0	531
99.25	0.0	530
99.25	0.0	530
99.25	0.0	530
99.25	0.0	531
99.25	0.0	531
99.25	0.0	530
99.25	0.0	529
99.25	0.0	531
99.25	0.0	528
40.00	6.3	13.5
26.09	10.0	12.7
42.11	3.2	13.9
31.43	0.070	19.6
25.00	0.12	18.9
24.26	1.2	15.8
37.50	2.4	14.6
43.75	0.15	19.2
22.22	3.7	15.0
22.03	7.1	13.9
40.00	9.8	13.5
36.36	2.7	15.4
23.53	2.7	15.4
50.00	4.4	14.6
23.91	5.2	14.6
40.00	0.26	18.1
20.45	1.9	15.4
33.33	0.017	21.6
52.63	0.74	16.5
40.00	4.3	14.2
39.52	1e-42	132
31.21	3e-27	90.9
47.92	7e-50	151
100.00	0.002	22.3
45.45	1.0	14.6
21.62	6.0	12.3
45.45	1.0	14.6
24.53	0.80	15.0
50.00	6.7	12.3
69.77	2e-115	320
40.28	1e-48	149
49.75	2e-59	177
62.50	1.0	15.0
40.10	4e-46	143
33.33	2.7	13.9
33.33	6.9	12.7
34.04	0.004	22.7
50.00	2.9	13.9
70.23	3e-116	322
69.77	2e-115	320
19.51	1.3	14.6
29.41	3.8	13.1
72.73	1.5	15.8
31.48	5.4	14.2
58.33	6.0	14.2
38.46	6.9	13.9
23.53	4.8	13.9
57.14	5.4	13.9
50.00	7.5	13.5
28.26	0.034	20.8
41.18	0.22	18.1
40.00	5.8	13.5
40.00	0.93	16.2
71.43	2.4	15.0
58.82	0.50	17.3
42.86	2.5	15.0
57.14	6.7	13.5
28.26	0.032	20.8
41.18	0.23	18.1
40.00	5.6	13.9
36.67	1.6	15.4
38.10	2.0	15.0
80.00	9.1	13.1
25.97	0.88	16.9
26.67	3.5	15.0
27.27	0.35	17.3
80.00	1.7	13.1
30.00	2.4	12.7
36.84	2.3	13.1
99.29	1e-106	291
99.29	1e-106	291
98.58	1e-105	288
31.58	0.41	15.0
23.53	4.5	11.9
35.29	0.71	15.8
66.67	1.1	15.4
22.97	1.2	15.4
62.50	3.9	13.9
25.53	6.5	13.1
57.14	7.5	12.7
32.14	0.22	17.7
42.86	0.11	18.5
44.44	1.0	15.8
27.50	4.8	13.5
47.06	0.34	16.9
27.27	1.2	15.4
22.22	1.2	15.0
30.00	2.4	14.2
47.06	0.36	16.9
27.27	0.84	15.8
22.22	1.3	15.0
57.14	2.5	14.2
36.00	0.96	15.4
71.43	6.4	12.7
63.41	2e-75	214
59.35	6e-59	173
64.24	3e-73	209
62.05	6e-72	206
22.73	2.3	13.5
61.68	4e-67	194
65.88	1e-81	231
68.90	3e-82	232
65.66	1e-76	218
71.26	7e-89	249
68.67	1e-83	236
40.00	0.017	17.7
28.12	0.022	17.3
33.33	0.092	15.8
40.00	0.019	17.7
32.26	0.011	18.1
27.78	4.7	11.5
27.78	4.5	11.5
27.78	4.9	11.5
27.78	4.3	11.9
27.78	4.7	11.5
27.78	4.5	11.5
27.78	4.9	11.5
27.78	4.7	11.5
28.57	1.6	15.4
21.28	4.2	14.2
50.00	6.0	13.9
33.33	6.8	13.5
97.94	0.0	690
97.94	0.0	690
32.00	0.81	16.2
50.00	5.3	13.5
99.68	0.0	652
99.68	0.0	652
95.45	0.002	25.8
33.33	0.11	20.0
40.00	4.7	15.0
95.45	0.002	25.8
40.00	4.7	15.0
85.71	9e-05	29.3
38.46	0.22	18.9
40.00	4.5	14.6
96.50	0.0	630
97.71	0.0	598
97.13	0.0	616
98.09	0.0	622
97.45	0.0	617
97.77	0.0	618
97.13	0.0	614
97.45	0.0	615
97.77	0.0	619
97.45	0.0	619
97.77	0.0	621
36.36	0.19	17.7
75.00	3.5	13.9
57.14	4.3	13.5
77.86	2e-160	439
55.64	6e-102	291
63.64	0.36	17.3
25.71	1.3	15.4
52.27	1e-91	265
54.91	9e-101	288
36.36	0.19	17.7
75.00	3.4	13.9
57.14	4.5	13.5
51.62	8e-92	265
27.69	0.33	17.7
100.00	1.2	15.8
27.59	1.4	15.4
25.00	5.6	13.5
99.19	1e-94	259
45.45	0.17	15.0
44.44	2.2	12.3
28.57	8.9	10.4
98.39	4e-94	258
98.39	4e-94	258
98.39	9e-94	257
68.35	0.0	612
38.10	0.25	18.9
32.50	0.55	17.7
26.92	1.2	16.5
23.29	2.2	15.8
41.67	3.8	15.0
45.45	4.7	14.6
38.10	0.24	18.9
32.50	0.59	17.7
26.92	1.2	16.5
24.29	2.1	15.8
41.67	3.8	15.0
45.45	4.8	14.6
37.93	1.8	16.2
21.21	4.5	15.0
62.50	4.9	14.6
38.10	0.16	19.2
32.50	0.57	17.7
26.92	1.2	16.5
23.29	2.3	15.8
41.67	3.7	15.0
45.45	4.8	14.6
38.10	0.25	18.9
32.50	0.57	17.7
26.92	1.7	16.2
23.29	2.3	15.8
41.67	3.7	15.0
45.45	4.8	14.6
38.10	0.25	18.9
32.50	0.56	17.7
26.92	1.2	16.5
23.29	2.2	15.8
41.67	3.7	15.0
45.45	4.8	14.6
26.47	0.50	17.7
62.50	6.6	14.2
100.00	0.0	885
36.36	3.0	15.4
40.00	4.0	12.3
37.04	2.5	13.5
27.78	9.3	12.3
33.33	9.4	12.3
71.43	2.9	13.5
33.33	8.9	12.3
27.78	9.1	12.3
28.57	9.3	12.3
33.33	9.9	11.9
36.36	1.1	15.0
37.50	2.3	15.4
27.59	2.6	15.0
30.00	8.1	13.5
29.27	0.044	20.8
38.46	1.5	15.8
71.43	1.9	15.8
29.27	2.3	15.4
46.67	2.5	15.4
27.59	2.7	15.0
22.92	3.5	14.6
30.00	8.1	13.5
30.77	9.6	13.5
30.77	9.9	13.1
29.41	4.0	14.6
30.00	5.7	14.2
28.95	9.4	13.5
29.63	0.057	20.8
36.36	6.0	14.2
29.03	5.0	14.2
30.43	5.9	14.2
26.32	5.9	14.2
47.89	2e-98	288
42.86	1.4	16.2
62.50	1.8	15.8
38.46	2.5	15.4
71.43	9.0	13.5
66.67	4.2	14.2
29.41	5.0	13.9
30.23	0.49	16.5
34.38	1.8	15.0
30.77	3.3	14.2
29.55	0.49	16.5
34.38	2.1	14.6
30.77	3.5	13.9
30.30	0.18	18.1
26.56	0.52	16.9
40.91	2.5	14.6
34.00	2.6	14.6
50.00	2.8	14.6
50.00	0.71	16.5
23.44	1.2	15.8
20.00	4.7	13.9
30.77	0.005	22.7
22.39	3.9	13.9
31.58	0.46	16.5
39.84	2e-58	179
47.06	0.59	16.9
31.25	0.68	16.9
53.85	1.6	15.8
28.57	3.6	14.6
25.49	0.93	15.8
38.89	2.5	14.2
29.17	3.2	13.9
20.83	5.7	13.9
40.00	4.0	13.9
57.14	4.7	13.9
77.55	2e-177	484
40.00	3.6	14.2
57.14	4.5	13.9
66.67	0.15	18.1
50.00	1.9	14.6
37.50	5.9	13.1
40.00	3.5	14.2
57.14	4.6	13.9
37.50	0.22	18.1
27.27	0.91	16.2
40.91	2.8	14.2
56.92	2e-50	147
52.76	2e-48	142
76.34	9e-72	202
60.00	7e-57	164
90.23	2e-90	249
93.98	7e-92	253
93.98	2e-92	254
94.74	5e-93	256
97.74	4e-97	266
93.98	2e-92	254
71.43	3.2	13.5
25.00	1.2	14.2
38.89	0.14	17.3
28.00	2.2	13.9
30.00	6.0	12.3
66.67	0.77	15.4
25.00	1.9	13.9
12.90	1.8	14.2
12.90	1.9	14.2
71.43	0.22	16.9
30.00	3.7	13.5
12.90	1.8	14.2
33.33	1.9	11.9
40.00	0.41	15.0
46.67	0.043	17.3
63.64	3.3	11.9
25.00	0.31	15.0
46.67	0.33	15.0
35.29	2.9	12.3
27.27	2.6	11.9
40.00	1.1	13.1
21.43	2.6	12.3
24.73	0.002	23.9
21.11	0.073	19.2
22.22	0.11	18.9
38.46	1.0	15.8
50.00	0.18	18.1
62.50	4.1	13.9
39.13	1.3	15.0
25.45	1.3	15.4
19.11	1.6	15.0
99.65	0.0	590
24.07	0.17	18.5
34.21	0.32	17.7
39.13	0.53	16.9
31.25	1.00	16.2
36.36	5.5	13.9
80.00	6.5	13.5
27.96	9e-05	28.1
24.76	0.068	19.2
20.69	6.4	13.1
62.50	2.1	14.6
99.47	1e-139	379
98.98	2e-146	397
98.98	6e-146	395
99.47	3e-139	379
98.98	5e-146	395
98.98	1e-145	395
98.98	2e-145	394
98.98	3e-145	394
98.94	2e-138	376
98.94	2e-138	376
98.66	6e-111	302
99.33	2e-111	304
98.66	6e-111	302
98.66	5e-111	303
98.66	8e-111	302
99.33	2e-111	303
99.33	2e-111	304
98.66	4e-111	303
98.66	3e-111	303
98.66	4e-111	303
32.56	0.010	23.5
25.00	0.46	18.1
25.00	3.9	15.4
21.31	2.9	15.8
33.33	6.8	14.6
30.43	0.70	17.7
39.29	0.78	17.7
26.32	0.58	17.7
27.66	0.67	17.7
38.89	1.3	16.9
23.53	6.9	14.2
98.78	0.0	994
40.00	0.97	17.3
85.71	1.0	16.9
46.67	1.4	16.5
50.00	1.6	16.5
25.93	6.2	14.6
26.32	0.56	18.1
27.66	0.67	17.7
38.89	1.3	16.5
23.53	7.0	14.2
32.56	0.012	23.5
25.00	0.47	18.1
25.00	3.8	15.4
30.77	9.1	14.2
28.57	0.26	19.2
42.86	1.2	17.3
28.57	2.4	16.2
42.86	2.6	16.2
41.67	3.1	15.8
50.00	3.3	15.8
28.57	0.26	19.2
42.86	1.2	17.3
28.57	2.4	16.2
42.86	2.6	16.2
41.67	3.1	15.8
50.00	3.3	15.8
36.67	0.15	18.5
30.77	2.2	15.0
44.44	7.2	13.5
99.69	0.0	678
35.29	2.8	14.2
99.39	0.0	675
36.84	0.11	19.2
28.57	1.4	15.8
27.27	5.9	13.5
21.92	1.0	16.2
20.51	4.2	14.2
37.50	7.9	13.5
50.00	2.0	15.4
31.82	7.8	13.5
33.93	0.033	21.2
54.55	0.82	16.5
53.85	1.6	15.8
50.00	7.2	13.5
35.71	7.9	13.5
29.17	1e-43	142
54.55	0.86	16.5
40.00	1.3	15.8
28.57	0.19	20.0
26.92	3.0	16.2
41.67	3.6	15.8
21.88	8.1	14.6
55.56	8.2	14.6
29.79	1.8	16.9
62.50	9.4	14.6
25.93	2.6	16.2
24.30	1.0	17.3
42.86	0.060	21.2
26.67	1.3	16.9
62.50	4.2	15.0
19.44	0.78	17.3
50.00	3.0	15.4
42.86	6.1	14.2
19.59	1.7	16.9
28.57	1.0	17.7
22.22	1.7	16.9
30.43	6.1	15.4
83.33	8.1	15.0
28.57	1.1	17.7
22.22	1.7	16.9
30.43	6.4	15.4
83.33	8.8	14.6
33.33	1.9	14.6
66.67	4.5	14.2
50.00	4.7	14.2
40.91	5.9	13.9
66.67	7.6	13.5
40.91	0.85	15.8
27.03	1.1	15.8
42.86	0.15	18.1
40.00	0.29	17.3
28.57	1.1	15.4
25.93	6.3	13.1
38.46	2.3	15.4
53.85	3.0	15.0
50.00	1.8	15.0
27.78	6.1	13.5
22.95	0.21	18.5
26.19	9.7	13.1
66.67	2.8	14.2
32.56	2.9	14.2
38.46	2.9	13.5
20.00	8.9	11.9
60.00	0.79	15.4
25.25	0.14	17.3
28.57	2.8	13.1
37.50	0.23	16.2
50.00	1.6	13.9
29.41	5.8	11.9
27.91	6.5	11.9
42.86	7.5	11.9
50.00	1.6	13.9
28.89	2.3	13.5
80.00	1.3	13.5
25.81	0.082	18.1
71.43	3.0	15.4
21.21	3.0	15.4
99.46	0.0	750
99.73	0.0	753
54.55	0.53	17.3
33.33	1.9	15.8
55.59	1e-137	389
38.89	3.9	14.6
46.15	5.1	14.2
45.45	7.4	13.9
38.89	4.0	14.6
46.15	5.2	14.2
45.45	7.4	13.9
38.89	4.0	14.6
46.15	5.1	14.2
45.45	7.3	13.9
46.15	1.0	16.2
28.26	0.45	14.6
44.44	2.9	12.3
28.26	0.46	14.6
28.26	0.42	14.6
37.50	1.1	13.9
97.84	2e-100	275
40.00	0.12	16.9
23.53	5.2	11.9
99.20	1e-179	486
99.60	2e-180	488
99.20	2e-179	485
99.20	2e-179	485
99.20	3e-179	484
99.20	3e-179	484
99.20	1e-179	485
99.20	3e-179	484
99.20	5e-180	486
99.20	1e-179	486
98.64	0.0	599
98.98	0.0	599
97.96	0.0	593
99.67	0.0	622
99.67	0.0	624
99.34	0.0	622
95.83	0.0	608
95.51	0.0	608
99.01	0.0	619
99.34	0.0	619
35.29	0.24	19.2
42.86	1.9	16.2
31.82	4.2	15.0
99.78	0.0	940
27.45	0.046	20.8
33.33	0.93	16.9
45.45	1.2	16.5
24.72	1.4	16.2
29.41	0.066	21.6
28.57	0.17	20.4
27.27	3.9	15.8
33.33	2.0	15.8
37.04	3.3	15.4
33.33	7.9	14.2
84.82	0.0	724
84.58	0.0	723
84.58	0.0	722
84.58	0.0	723
32.14	5.8	14.6
24.00	7.2	14.2
39.85	9e-28	89.0
21.74	1.2	13.1
60.00	3.4	11.9
21.74	1.2	13.1
60.00	3.4	11.9
50.00	1e-44	132
40.30	2e-25	88.2
40.30	2e-25	88.2
40.30	2e-25	88.2
43.48	1.2	14.2
33.33	5.4	13.5
99.54	7e-164	447
25.84	4e-13	53.5
23.86	6e-06	32.3
21.71	0.72	16.5
43.75	1.7	15.4
30.77	5.1	13.9
21.71	0.83	16.5
43.75	1.7	15.4
30.77	4.9	13.9
26.87	1e-12	52.0
24.66	2e-09	42.7
27.71	5e-08	38.5
23.53	1.2	15.4
27.08	0.077	20.0
25.71	0.26	18.1
99.54	1e-165	452
26.40	2e-13	54.7
23.86	6e-06	32.3
62.50	2.4	14.6
33.33	0.66	16.9
26.42	0.88	16.5
50.00	6.4	13.9
57.14	6.5	13.9
66.67	6.6	13.9
62.50	2.5	14.6
22.50	6.4	13.5
30.43	3.2	12.7
35.29	7.6	11.5
28.57	3.6	12.3
40.00	2.8	13.1
29.41	0.93	14.2
28.21	1.0	13.9
57.14	1.4	13.5
32.26	0.25	16.2
25.71	0.40	15.8
23.08	0.15	16.9
57.14	3.8	12.7
55.56	1.3	13.5
27.78	3.2	13.9
27.78	3.3	13.9
33.33	0.42	16.2
50.00	1.0	15.4
24.00	0.42	16.9
58.33	1.9	15.0
39.29	0.085	18.5
27.40	0.27	16.5
28.57	0.51	15.4
26.83	0.96	14.6
41.18	3.6	12.7
25.68	0.25	16.5
38.46	3.8	12.7
40.00	1.8	13.9
40.00	1.8	13.9
40.00	1.8	13.9
36.00	0.032	19.2
31.03	6.3	12.3
57.14	1.1	15.8
37.50	1.5	15.8
66.67	2.6	15.4
38.46	4.8	14.2
41.67	0.17	18.9
37.50	1.9	15.4
21.74	2.1	15.4
57.14	1.1	15.8
57.14	1.2	15.8
57.14	1.1	15.8
22.22	1.1	16.2
100.00	0.12	18.9
40.00	0.33	17.7
29.63	2.6	14.6
66.67	2.3	14.6
47.37	2.7	14.2
100.00	0.42	17.3
55.56	0.93	16.2
55.56	2.8	14.6
89.09	7e-135	369
85.52	1e-125	346
90.00	3e-141	385
85.39	9e-129	354
85.91	7e-130	357
81.06	5e-125	345
86.04	2e-131	361
87.56	4e-132	362
86.18	2e-127	350
86.36	2e-129	356
33.33	0.022	21.9
21.82	0.041	20.8
25.35	0.26	18.5
24.07	1.3	16.2
29.09	0.021	21.9
40.00	0.61	17.3
43.75	3.3	15.0
30.77	7.2	13.9
99.76	0.0	844
62.50	6.9	14.6
62.50	6.8	14.6
62.50	6.9	14.6
62.50	6.6	14.6
62.50	6.6	14.6
36.11	0.14	19.6
36.11	0.50	17.7
28.00	4.5	14.6
41.67	6.8	14.2
36.11	0.13	19.6
36.11	0.44	18.1
28.00	4.5	14.6
41.67	6.3	14.2
98.55	2e-50	142
97.10	3e-49	139
98.55	3e-50	141
97.10	1e-49	140
64.18	8e-33	97.1
38.89	0.014	16.9
68.18	3e-34	100
66.67	2e-33	98.6
75.00	0.043	15.8
42.86	7.2	10.4
100.00	7.6	10.4
33.33	8.3	10.4
30.00	1.2	11.9
75.00	0.043	15.8
41.18	0.31	13.5
44.44	0.79	12.3
35.71	0.83	12.3
50.00	0.94	12.3
35.71	0.19	14.2
60.00	1.6	11.5
27.27	2.1	11.2
31.25	6.0	10.0
35.71	0.19	14.2
27.27	2.4	11.2
33.33	6.2	10.0
38.89	0.28	15.4
34.62	0.092	17.7
44.44	1.7	13.9
45.45	1.4	14.2
25.00	7.4	11.9
28.00	0.058	18.1
33.33	8e-46	148
28.57	0.54	16.5
26.53	0.96	15.8
33.73	1e-46	150
99.66	0.0	603
79.79	0.0	515
33.33	2e-46	150
99.68	0.0	637
53.74	3e-118	333
51.02	1e-119	338
79.45	0.0	511
91.67	1e-109	300
92.26	1e-111	305
92.26	2e-111	304
92.26	2e-111	305
92.90	4e-112	306
97.42	1e-115	315
98.71	2e-116	317
92.26	2e-111	304
92.26	1e-111	305
92.26	3e-111	304
95.50	9e-80	220
89.19	5e-73	203
85.59	2e-70	196
84.68	3e-69	193
84.68	2e-69	194
84.68	2e-69	194
87.39	8e-72	200
90.99	2e-74	206
92.79	1e-75	210
90.09	1e-73	204
57.71	2e-68	197
60.34	7e-71	203
34.48	0.40	14.6
71.43	1.1	13.5
37.50	3.6	11.9
34.48	0.39	14.6
37.50	3.6	11.9
79.21	3e-107	296
71.43	1.7	14.2
42.11	0.82	14.2
38.46	5.3	11.9
33.33	5.7	12.7
97.92	7e-142	385
64.89	1e-91	257
21.74	0.087	17.7
42.86	3.8	12.7
23.87	0.072	17.7
27.27	5.2	12.7
33.33	1.3	13.9
33.33	1.3	13.9
28.57	0.003	21.9
44.44	0.21	17.3
30.30	0.31	16.5
60.00	0.79	15.4
44.44	1.6	14.2
41.67	2.3	13.9
46.67	4.8	13.1
50.00	7.3	11.9
56.25	0.016	19.2
55.56	0.12	17.3
85.71	0.12	17.3
34.78	0.26	16.2
31.58	0.95	14.6
50.00	4.0	12.7
99.46	5e-139	382
99.46	5e-139	382
34.78	0.28	16.2
32.31	0.021	20.4
33.70	1e-30	100
35.00	6.4	12.3
45.45	1.0	14.6
28.00	1.4	14.2
99.45	1e-136	371
29.55	0.014	19.6
41.18	0.68	15.4
73.48	1e-104	290
94.09	4e-167	453
95.36	2e-169	458
95.78	3e-170	461
93.25	2e-165	448
94.09	1e-167	454
94.09	1e-167	454
93.25	5e-166	450
94.51	3e-168	456
93.67	3e-167	453
94.51	6e-169	457
33.33	1.9	10.8
87.23	4e-28	83.6
84.00	9e-29	85.5
70.00	1e-23	72.4
35.48	1.2	11.5
98.43	2e-93	256
98.43	5e-94	258
98.43	3e-94	258
97.64	1e-92	254
98.43	1e-93	257
99.21	3e-94	258
98.43	2e-93	256
99.14	2e-86	238
99.21	9e-95	259
98.41	7e-93	254
50.00	0.070	18.1
57.14	1.7	13.9
38.46	6.8	11.9
46.41	1e-50	152
28.12	0.31	15.8
25.81	2.4	13.1
41.67	5.9	12.3
44.44	0.080	18.5
23.53	5.0	13.1
28.57	7.1	12.7
85.71	0.82	15.4
25.71	1.1	15.4
50.00	0.13	16.9
57.14	2.6	13.1
98.08	1e-116	317
98.71	5e-118	321
98.71	1e-117	320
98.71	6e-118	321
98.71	3e-118	322
98.71	2e-118	322
99.35	4e-119	324
99.35	2e-119	325
99.35	2e-119	325
99.35	3e-119	324
53.85	0.16	18.5
36.36	6.9	13.5
80.00	8.1	13.1
41.18	0.90	16.9
33.33	4.4	14.2
28.57	7.3	13.5
50.00	10.0	13.1
33.33	4.5	14.2
28.57	7.3	13.5
50.00	9.8	13.1
54.55	0.10	19.2
46.67	0.37	17.3
46.15	3.6	14.2
60.20	8e-135	377
60.20	3e-135	378
38.10	0.009	22.7
32.35	0.86	16.5
24.10	0.55	16.9
55.56	1.8	15.0
28.95	2.1	15.0
57.14	3.5	14.2
46.15	7.0	13.5
27.78	4.4	13.9
35.00	3.6	13.9
31.25	2.5	15.0
46.15	3.4	14.2
98.59	0.0	568
31.25	2.5	15.0
46.15	3.5	14.2
32.00	0.93	15.4
20.00	5.0	13.1
43.88	4e-86	250
31.25	2.5	15.0
46.15	3.4	14.2
31.25	2.5	15.0
46.15	3.5	14.2
97.14	0.0	711
98.00	0.0	716
97.71	0.0	714
97.71	0.0	714
98.00	0.0	715
98.58	0.0	721
98.01	0.0	716
98.29	0.0	719
96.78	0.0	692
97.72	0.0	715
50.00	0.031	17.7
22.64	0.79	13.5
42.86	0.66	13.5
42.86	0.25	13.9
30.00	7.5	10.0
60.00	5.7	10.8
33.33	8.0	10.4
99.17	6e-90	247
98.35	3e-89	245
98.35	2e-89	245
50.00	0.031	17.7
22.64	0.80	13.5
44.63	3e-131	376
24.49	0.11	19.6
28.30	0.28	18.5
24.39	0.53	17.7
46.15	1.6	15.8
99.76	0.0	849
99.52	0.0	847
62.50	2.2	15.8
62.50	2.3	15.8
99.51	0.0	830
99.51	0.0	830
30.77	1.8	16.9
34.78	3.2	16.2
44.44	3.9	15.8
62.50	7.6	15.0
99.83	0.0	1211
85.96	0.0	1023
99.83	0.0	1209
99.83	0.0	1209
99.66	0.0	1208
29.63	0.094	21.2
47.62	1.4	17.7
44.44	4.3	15.8
39.13	9.6	14.6
50.00	3.6	16.5
55.56	3.9	16.5
50.00	3.6	16.5
55.56	3.9	16.5
99.66	0.0	1207
99.67	0.0	1247
99.67	0.0	1248
99.67	0.0	1247
99.67	0.0	1246
99.67	0.0	1246
99.84	0.0	1249
66.67	6.6	15.0
31.25	8.0	15.0
66.67	6.5	15.0
31.25	8.0	15.0
66.67	6.6	15.0
31.25	8.0	15.0
66.67	6.6	15.0
31.25	8.0	15.0
24.53	0.083	18.9
24.53	0.073	19.2
23.08	2.6	13.9
44.44	7.5	12.3
50.00	0.96	15.8
50.00	0.93	15.8
58.33	2.2	14.6
100.00	0.082	19.2
33.33	4.6	13.9
64.71	0.029	20.8
46.67	0.14	18.9
25.53	0.94	16.2
75.00	0.40	18.1
36.36	0.57	17.7
57.14	2.4	15.8
26.32	4.8	14.6
24.14	0.064	20.8
75.00	0.40	18.1
33.33	1.7	16.2
57.14	2.5	15.8
26.32	5.0	14.6
40.00	1.6	16.5
35.29	5.4	15.0
33.33	8.8	14.2
28.21	0.15	20.0
32.00	0.25	18.9
36.84	0.90	17.7
31.58	0.23	19.2
33.33	0.30	18.9
34.62	0.49	18.1
22.06	1.2	16.9
25.00	9.4	13.5
40.00	1.5	16.5
35.29	5.3	15.0
33.33	8.8	14.2
99.28	0.0	570
97.46	0.0	558
93.48	0.0	536
99.64	0.0	572
96.38	0.0	554
98.91	0.0	569
99.64	0.0	573
99.64	0.0	573
99.64	0.0	573
99.64	0.0	573
21.43	0.15	20.0
34.78	4.9	15.0
100.00	4.9	15.0
21.43	0.15	20.0
26.32	1.1	17.3
100.00	5.0	15.0
34.78	5.1	15.0
29.03	8.2	14.2
71.54	0.0	811
63.94	0.0	714
22.73	9.2	14.6
53.52	0.0	542
61.38	0.0	695
61.19	0.0	693
61.19	0.0	692
61.19	0.0	693
23.33	2.6	15.8
40.00	5.0	15.0
71.43	6.3	14.6
45.45	8.6	14.2
31.25	8.6	14.2
50.00	0.21	17.7
28.57	4.5	13.5
44.44	8.6	12.7
24.32	2.5	15.0
71.43	6.9	13.5
37.04	0.79	16.2
66.67	4.2	13.9
43.75	3.1	13.5
31.82	5.2	12.7
47.37	0.14	18.5
62.50	2.5	14.6
75.00	1.2	15.8
25.00	1.8	14.6
26.67	1.9	14.6
46.15	2.9	13.9
35.71	3.3	13.9
24.24	7.9	12.7
43.75	1.5	15.0
40.74	1.1	15.4
60.00	0.022	20.4
35.29	0.050	18.9
41.67	7.3	12.3
60.00	1.3	15.0
33.33	0.32	16.9
21.43	0.54	16.2
53.85	0.24	17.3
43.75	1.7	14.6
27.27	2.9	13.9
39.13	0.91	15.4
58.33	2.2	13.5
50.00	5.5	12.3
30.77	2.2	13.1
37.50	0.075	17.7
54.55	1.7	13.9
35.71	3.4	12.7
37.04	0.078	17.7
24.00	0.33	16.2
36.00	3.3	13.1
41.67	0.14	16.2
26.09	1.3	13.9
32.08	0.073	17.7
55.56	0.12	16.2
60.00	3.7	11.9
26.74	0.22	15.0
28.57	0.058	17.3
26.92	0.60	14.6
24.29	2.1	12.7
35.71	0.26	18.9
24.32	1.2	16.5
32.65	2.0	15.8
25.00	4.4	14.6
54.55	2.0	15.8
75.00	0.74	16.9
55.56	5.4	14.2
32.43	0.58	17.3
45.45	0.19	18.9
32.26	1.7	15.8
36.36	2.1	15.4
35.71	0.26	18.9
24.32	1.2	16.5
32.65	2.0	15.8
25.00	4.6	14.6
25.00	0.009	23.5
36.84	0.49	18.1
45.45	2.3	15.8
31.25	7.4	14.2
32.65	7.1	13.9
24.32	1.1	16.9
35.00	1.9	16.2
25.58	2.0	16.2
40.00	3.1	15.4
27.59	6.3	14.2
35.29	0.56	17.7
33.33	1.9	15.8
36.36	4.4	14.6
24.00	5.8	13.1
26.23	9.4	12.7
45.45	2.1	14.2
24.00	5.5	13.5
26.23	9.9	12.7
57.14	1.6	15.0
42.11	4.4	13.1
43.75	0.026	20.8
25.00	7.0	13.1
57.14	1.5	15.0
57.14	1.6	15.0
57.14	1.5	15.0
57.14	1.6	15.0
38.10	0.73	16.9
26.32	4.5	14.6
30.43	0.81	16.9
66.67	6.8	14.2
38.10	0.69	17.3
25.81	9.3	13.5
28.57	0.13	19.6
35.29	5.7	14.2
99.74	0.0	799
38.10	0.71	17.3
25.81	9.3	13.5
29.03	4.7	14.6
50.00	5.4	14.2
21.15	0.12	19.6
40.74	0.26	18.9
26.47	0.48	17.7
40.00	2.8	15.4
26.92	4.7	14.6
66.67	8.1	13.9
31.43	0.71	17.7
66.67	4.5	15.0
83.33	5.0	15.0
40.74	0.28	18.5
32.50	0.45	18.1
40.00	0.76	17.3
26.92	6.1	14.2
66.67	8.0	13.9
23.26	0.30	17.3
30.00	1.9	14.6
30.77	3.9	13.9
37.04	4.4	13.5
57.14	5.2	13.5
23.26	0.29	17.3
30.00	1.9	14.6
30.77	3.8	13.9
37.04	4.4	13.5
57.14	5.2	13.5
23.08	5.6	12.7
80.00	9.9	11.9
57.14	4.0	13.9
30.61	0.96	15.0
33.33	2.5	13.9
26.32	8.2	11.9
27.03	2.5	13.9
28.26	0.57	16.5
54.55	3.1	14.2
31.58	3.4	13.9
35.00	3.7	13.9
30.77	9.6	11.5
40.00	2.1	14.6
20.73	0.081	20.0
55.56	7.1	13.9
33.33	0.084	19.6
40.00	0.30	18.1
50.00	0.90	16.5
35.00	2.6	15.0
29.41	6.7	13.5
62.50	8.9	13.1
41.38	0.43	17.3
26.79	1.3	15.8
36.36	1.5	15.8
45.45	0.30	18.1
21.43	2.9	15.0
40.00	3.4	14.6
38.46	8.0	13.5
26.42	0.28	18.1
58.33	0.31	18.1
50.00	9.6	13.1
20.00	0.060	20.4
42.86	2.3	15.4
66.67	2.4	15.4
35.71	4.0	14.6
36.36	9.2	13.5
41.38	0.41	17.3
26.79	1.4	15.8
36.36	1.5	15.8
21.49	2.4	15.4
35.00	3.8	14.6
33.33	0.085	19.6
63.64	0.71	16.5
83.33	2.9	14.2
83.33	3.0	14.2
33.33	1.1	16.2
24.24	3.0	15.0
40.91	0.41	17.3
21.52	0.67	16.5
35.00	0.82	16.2
55.56	3.4	14.2
35.29	5.3	13.9
36.36	0.16	18.1
50.00	6.3	13.1
71.43	7.7	12.7
31.67	4.7	13.9
41.18	0.072	16.5
30.95	0.034	18.5
30.77	1.0	14.2
55.56	0.063	16.9
80.00	9.2	11.2
31.25	3.9	12.7
33.33	1.9	13.1
97.67	1e-29	87.0
66.67	3.0	10.0
99.13	1e-176	477
98.76	0.0	500
98.35	0.0	494
75.89	3e-132	365
76.99	7e-137	376
75.45	9e-132	363
88.29	3e-150	410
74.37	4e-131	362
74.58	1e-131	363
73.99	6e-127	351
28.57	0.36	17.3
26.32	4.9	13.9
66.67	0.12	19.2
43.75	1.2	16.2
46.15	0.48	16.9
83.33	0.54	16.9
42.11	1.3	15.8
57.14	6.1	13.5
23.53	7.5	13.5
46.15	0.50	16.9
83.33	0.54	16.9
42.11	1.4	15.8
57.14	6.2	13.5
32.35	1.8	15.4
28.57	5.3	13.9
42.86	6.8	13.9
31.82	2.6	15.0
28.57	4.9	14.2
42.86	6.7	13.9
57.14	8e-132	370
57.14	5.5	13.9
28.12	0.16	18.1
88.70	5e-102	291
95.80	3e-74	220
94.69	0.0	566
88.70	7e-102	291
94.96	4e-73	217
86.93	7e-100	286
93.28	5e-70	209
94.69	0.0	577
94.69	0.0	576
89.27	6e-104	296
94.96	5e-74	219
89.83	3e-104	297
95.80	7e-74	219
97.50	0.0	603
96.25	0.0	592
35.81	8e-55	173
46.15	3.8	14.6
35.81	8e-55	173
62.50	3.9	14.6
97.80	0.0	743
83.57	0.0	639
89.56	0.0	677
83.79	0.0	644
93.35	0.0	712
97.80	0.0	743
35.81	1e-54	172
54.55	1.3	16.2
100.00	0.0	746
71.43	9.5	13.9
78.51	7e-73	204
79.34	1e-73	206
81.40	7e-82	227
78.51	1e-72	203
76.86	4e-72	202
80.00	6e-77	215
78.51	1e-72	203
80.65	8e-78	217
78.51	1e-72	204
78.51	1e-72	203
47.18	2e-42	128
44.53	3e-41	125
98.63	3e-109	298
99.32	2e-110	301
99.32	2e-110	301
94.52	2e-102	280
94.52	2e-102	280
95.21	1e-103	283
41.98	7e-39	119
70.00	0.047	18.5
99.44	4e-140	379
26.09	0.51	15.4
23.08	0.25	16.5
43.75	0.29	16.5
55.56	0.43	15.8
30.00	1.6	14.2
31.58	4.4	12.7
29.41	1.6	13.9
44.44	3.5	13.1
43.82	6e-59	173
98.31	1e-136	372
66.67	1.1	14.2
60.11	6e-80	227
59.89	8e-81	229
41.67	0.46	14.6
40.74	2.1	13.1
41.67	1.4	13.5
100.00	0.24	15.4
100.00	8.5	10.8
55.56	0.13	15.8
24.32	0.86	13.5
41.18	1.4	13.1
33.33	5.9	10.8
31.58	1.0	14.2
34.78	0.39	15.8
57.14	1.2	14.2
36.84	2.8	13.1
34.88	0.22	15.4
61.06	4e-45	133
71.43	2.6	13.9
38.46	4.1	13.5
33.33	6.3	12.7
50.00	0.37	15.4
33.33	2.5	13.1
20.83	0.57	15.0
38.10	1.1	13.9
27.50	0.010	20.8
41.67	0.070	18.1
21.74	0.97	14.6
29.17	1.0	14.6
29.31	0.063	18.1
22.73	0.39	15.8
32.35	3.2	12.7
41.18	0.82	15.4
43.75	1.6	14.6
41.67	8.8	11.9
41.67	8.6	11.9
57.14	5.0	12.7
43.48	1.3	13.9
33.33	0.15	17.7
41.67	3.0	13.5
44.52	1e-48	147
46.67	2.1	13.5
36.36	2.3	13.5
30.95	0.001	22.7
27.27	0.23	15.8
27.27	0.22	15.8
70.83	4e-114	317
73.83	1e-119	330
70.37	2e-113	315
69.91	1e-112	313
70.37	4e-113	314
69.91	3e-112	311
69.91	3e-112	312
69.91	2e-112	312
70.83	9e-114	315
69.91	7e-112	311
27.03	0.24	18.1
33.33	0.62	16.9
27.59	0.18	18.9
40.00	1.3	16.2
38.89	2.0	15.8
38.10	0.74	16.9
28.21	0.79	16.9
26.67	1.0	16.5
25.00	1.1	16.5
57.14	5.7	14.2
50.00	6.0	14.2
55.56	2.1	15.4
55.56	3.5	14.6
34.29	0.23	18.9
23.40	0.30	18.5
62.50	9.4	13.5
75.00	4.1	14.6
38.46	4.2	14.6
63.64	0.31	18.5
83.33	2.9	15.4
40.00	5.9	14.2
98.63	2e-110	301
98.63	8e-110	300
97.95	3e-109	298
99.32	5e-111	302
97.26	3e-107	293
98.63	6e-110	300
97.95	3e-109	298
99.32	1e-111	304
99.32	1e-111	304
99.32	4e-111	303
96.77	1e-90	249
96.77	7e-91	249
96.77	2e-90	248
96.77	2e-90	248
96.77	8e-91	249
96.77	2e-90	248
97.58	1e-91	251
95.97	2e-83	230
95.97	3e-84	233
99.19	1e-91	251
99.72	0.0	736
99.15	0.0	736
85.67	0.0	604
87.16	0.0	601
86.83	0.0	588
81.07	0.0	587
50.00	3.3	15.0
85.47	0.0	632
84.81	0.0	625
99.40	0.0	698
99.15	0.0	736
56.19	7e-90	256
65.93	3e-113	315
61.40	2e-100	282
48.67	1e-72	212
99.12	8e-175	471
49.12	1e-73	214
31.03	3e-29	98.6
59.73	1e-94	268
48.67	8e-73	212
49.12	7e-74	215
30.30	0.086	18.9
40.00	0.090	18.9
80.00	2.1	14.6
30.43	0.59	17.3
36.36	5.0	14.6
30.43	0.59	17.3
36.36	4.9	14.6
30.43	0.58	17.3
36.36	5.0	14.2
45.45	0.16	18.9
24.32	3.0	15.0
50.00	4.9	14.2
30.30	0.038	21.2
25.76	1.4	16.2
26.32	2.3	15.4
57.14	4.2	14.6
38.10	0.74	16.9
28.21	0.79	16.9
26.67	1.0	16.5
25.00	1.1	16.5
57.14	5.7	14.2
50.00	6.0	14.2
50.00	0.76	16.9
54.55	9.6	13.5
33.33	0.060	20.4
63.64	0.78	16.9
24.32	3.4	15.0
21.88	1.9	15.8
42.86	6.0	14.2
55.56	6.2	14.2
23.81	2.6	11.9
19.35	0.14	16.5
42.86	0.17	16.5
38.10	1.3	13.5
47.06	0.31	15.4
26.67	1.4	13.5
71.43	1.4	13.5
40.00	4.2	11.5
35.00	0.36	15.8
24.00	1.0	16.5
16.22	1.2	16.2
45.45	3.0	15.0
57.14	5.1	14.2
40.00	6.3	13.9
34.78	6.8	13.9
50.00	8.5	13.5
25.81	0.23	18.9
30.00	0.99	16.9
31.25	6.5	14.2
25.00	6.6	14.2
83.33	7.8	14.2
39.29	0.052	21.2
27.27	3.4	15.4
36.84	2.7	15.4
24.00	1.0	16.5
16.22	1.2	16.2
45.45	3.0	15.0
57.14	5.1	14.2
40.00	6.3	13.9
34.78	6.8	13.9
50.00	8.5	13.5
25.40	0.32	18.9
54.55	0.93	17.3
36.36	1.0	17.3
41.67	6.7	14.6
42.86	0.67	17.7
35.29	0.81	17.3
50.00	2.6	15.8
46.15	7.6	14.2
28.57	9.7	15.0
20.17	0.16	21.6
30.00	5.6	16.5
34.78	2.5	16.9
27.27	2.7	16.9
50.00	3.8	16.2
80.00	4.3	16.2
50.00	5.3	15.8
99.70	0.0	1385
99.85	0.0	1387
23.33	0.065	21.9
33.33	0.86	18.5
28.00	0.86	18.5
35.00	7.2	15.4
23.33	0.064	21.9
33.33	0.86	18.5
28.00	0.86	18.5
35.00	7.3	15.4
42.11	1.9	17.7
30.26	2.3	17.7
29.27	4.5	16.5
31.25	7.8	15.8
40.00	8.3	15.8
42.11	1.9	17.7
30.26	2.2	17.7
29.27	4.4	16.5
31.25	7.7	15.8
40.00	8.1	15.8
53.87	0.0	730
35.00	2.4	10.8
50.00	6.3	10.8
33.33	1.7	12.3
21.21	3.4	13.9
33.33	4.0	13.9
38.89	0.12	18.1
60.00	0.93	15.8
45.45	0.62	16.2
26.67	2.0	14.6
32.68	2e-31	105
33.33	4.2	13.9
57.14	2.9	14.2
38.89	0.12	18.1
35.29	1.9	14.2
55.56	1.6	14.6
62.50	3.3	13.9
28.57	0.64	16.9
33.33	9.1	13.1
35.71	4.6	13.1
37.93	0.16	18.1
55.56	0.50	17.3
36.36	4.7	14.2
26.09	0.99	15.8
26.67	2.4	14.2
42.86	4.7	13.5
38.89	0.64	15.8
42.86	0.66	15.8
22.22	7.3	12.7
25.58	0.45	15.8
24.56	0.030	20.0
31.58	2.5	14.2
35.71	4.2	13.5
22.44	2e-11	48.5
62.50	0.94	15.8
23.53	0.13	18.5
66.67	0.25	17.3
57.14	6.9	13.1
27.78	7.4	12.7
30.00	2.9	14.2
25.00	3.2	14.2
28.57	0.51	15.8
30.00	2.8	14.6
25.00	3.2	14.2
26.47	0.022	20.4
46.15	0.32	16.9
31.58	2.2	14.2
35.71	4.3	13.5
40.00	0.14	16.2
27.27	5.5	11.2
99.72	0.0	747
99.44	0.0	744
99.44	0.0	743
99.16	0.0	741
99.72	0.0	749
99.44	0.0	744
99.72	0.0	747
99.72	0.0	747
99.72	0.0	747
99.72	0.0	747
55.56	1.3	14.2
33.33	1.4	14.2
42.86	0.096	16.5
66.67	0.65	13.9
30.00	0.25	16.5
34.38	1.1	14.6
29.27	0.53	15.4
40.00	0.60	15.4
57.14	4.6	12.7
42.86	8.1	11.9
46.15	6.6	15.8
46.15	6.8	15.4
31.25	9.1	15.0
50.00	9.7	15.0
28.36	0.37	20.0
30.00	1.2	18.1
57.14	7.1	15.8
50.00	7.8	15.4
42.29	0.0	540
33.33	1.7	18.1
39.43	1e-175	513
37.50	4.7	16.5
25.81	0.78	18.5
24.24	1.6	17.7
28.57	1.8	17.3
35.29	3.9	16.2
25.81	0.78	18.5
24.24	1.6	17.7
28.57	1.9	17.3
35.29	3.9	16.2
28.12	6.2	16.2
28.12	6.2	16.2
99.86	0.0	1516
28.36	0.45	19.6
30.00	1.1	18.5
57.14	7.4	15.8
50.00	7.8	15.4
50.00	0.92	15.0
42.86	1.2	14.2
50.00	0.95	14.6
42.86	1.3	14.2
23.08	1.0	14.6
29.03	2.6	13.5
23.08	1.0	14.6
29.03	2.6	13.5
23.08	1.1	14.6
29.03	2.5	13.5
23.08	1.1	14.6
29.03	2.5	13.5
23.08	1.1	14.6
29.03	2.5	13.5
29.63	0.71	14.6
30.00	6.9	12.7
25.00	7.8	12.3
38.46	0.36	15.8
28.57	3.3	12.7
38.46	2.6	14.6
23.40	0.002	25.0
30.43	3.9	14.6
25.00	4.2	14.6
34.00	0.40	17.7
77.78	0.90	16.5
21.99	0.011	22.3
30.43	2.3	15.0
25.00	3.6	14.6
33.33	1.4	15.8
42.86	1.5	15.0
66.67	2.4	14.6
37.04	2.8	14.2
29.41	8.5	12.7
63.64	0.98	15.8
25.35	1.4	15.4
40.00	2.8	14.6
66.67	4.1	13.9
38.89	1.5	15.8
55.56	2.4	15.0
38.89	1.5	15.8
55.56	2.4	15.0
30.53	9e-09	38.5
32.94	2e-08	37.4
21.18	2e-06	32.0
26.79	7e-05	26.9
33.33	0.001	23.1
66.67	0.082	17.7
38.46	2.3	13.9
31.25	2.5	13.9
32.50	1.3	14.2
50.00	1.9	13.9
25.00	2.7	13.5
33.33	3.6	13.1
37.84	0.003	22.3
28.95	1.8	13.9
31.82	0.51	15.4
35.29	0.79	15.0
30.30	8.4	11.9
62.50	0.75	15.4
40.00	9.6	11.9
40.00	0.20	19.6
50.00	0.28	19.2
83.33	7.5	14.6
40.00	0.21	19.6
50.00	0.28	19.2
28.57	7.4	14.6
50.00	0.073	20.8
37.50	0.20	19.6
50.00	3.5	15.4
83.33	7.6	14.6
40.00	0.20	19.6
50.00	0.27	19.2
28.57	7.3	14.6
40.00	0.20	19.6
50.00	0.27	19.2
28.57	7.2	14.6
40.00	0.21	19.6
50.00	0.28	19.2
28.57	7.3	14.6
40.00	0.20	19.6
50.00	0.28	19.2
28.57	7.4	14.6
18.75	9.9	14.2
40.00	0.20	19.6
50.00	0.28	19.2
28.57	7.4	14.6
18.75	9.9	14.2
40.00	0.20	19.6
50.00	0.45	18.5
28.57	7.4	14.6
18.75	9.9	14.2
36.00	0.65	17.7
25.71	2.2	16.2
37.50	3.6	15.4
54.55	3.6	15.4
30.77	7.4	14.2
28.12	7.9	14.2
42.31	0.26	15.0
33.33	0.27	15.0
58.33	0.18	15.8
40.00	0.040	18.5
28.00	6.1	11.9
33.33	2.4	12.3
23.26	2.1	12.3
33.33	3.0	11.2
23.26	2.1	12.7
81.48	5e-05	26.6
36.84	0.092	16.9
33.33	6.6	11.9
44.44	0.19	18.9
55.56	3.4	15.0
28.57	3.5	15.0
37.50	9.7	13.5
46.67	0.38	18.1
25.00	0.87	16.9
57.14	3.0	15.4
60.00	3.6	15.0
62.50	2.2	15.4
35.00	4.5	14.6
62.50	0.88	16.5
32.35	3.3	14.6
66.67	8.5	13.5
31.17	0.20	18.9
46.67	0.38	18.1
25.00	0.89	16.9
57.14	2.9	15.4
60.00	3.6	15.0
62.50	6.0	14.6
32.50	0.40	17.7
34.48	0.93	16.5
36.36	1.7	15.4
20.00	7.4	13.5
32.50	0.40	17.7
34.48	0.96	16.2
33.33	1.7	15.4
20.00	7.7	13.5
32.50	0.40	17.7
34.48	0.94	16.2
33.33	1.7	15.4
20.00	7.5	13.5
30.77	0.37	17.3
62.50	1.2	15.8
28.57	1.6	15.4
75.00	0.20	18.9
22.47	0.37	17.7
62.50	4.3	14.6
49.39	1e-102	296
49.09	6e-101	292
62.39	1e-141	396
29.73	2.0	15.4
25.81	3.4	14.6
36.00	4.8	14.2
43.75	5.7	13.9
28.00	7.0	13.5
60.69	2e-140	393
33.33	0.12	18.9
34.29	0.16	18.9
35.71	1.2	15.8
26.47	1.5	15.8
40.62	1e-69	211
34.09	0.20	18.5
33.33	0.91	16.5
29.63	2.2	15.4
22.06	6.3	13.9
57.14	7.4	13.5
37.50	8.9	13.5
32.08	0.008	24.6
45.00	0.31	19.6
47.37	1.2	17.3
33.33	2.0	16.5
29.03	7.4	14.6
80.00	9.2	14.2
71.43	4.4	15.4
40.00	8.9	14.2
33.33	3.4	15.8
40.00	9.1	14.2
23.64	0.35	18.9
71.43	4.3	15.4
40.00	9.0	14.2
42.86	0.21	18.9
26.80	0.35	18.1
23.08	0.60	17.7
45.45	1.9	15.8
62.50	6.7	14.2
47.37	3.7	15.4
62.50	7.8	14.2
28.12	0.85	17.7
33.33	0.013	23.1
42.11	0.039	19.2
31.82	0.13	17.7
33.33	2.9	13.5
41.38	0.28	16.5
60.00	0.75	15.4
41.38	0.28	16.5
36.11	0.26	16.9
20.00	2.6	12.7
36.11	0.20	16.9
52.80	8e-65	187
19.70	0.008	20.8
35.29	0.33	16.2
42.86	0.13	16.9
46.15	0.19	16.2
44.44	3e-51	152
24.44	0.31	15.8
43.95	2e-49	147
24.32	0.51	15.8
99.39	8e-127	344
98.77	6e-123	335
24.44	0.34	15.8
45.45	1.1	13.1
27.59	2.0	13.5
35.29	2.9	12.7
66.67	0.22	16.2
35.71	0.65	14.6
50.00	6.5	11.5
34.62	0.14	16.9
30.43	1.7	13.5
50.00	0.71	15.0
35.48	0.52	15.4
36.36	6.0	11.9
36.36	1.7	15.8
35.71	0.67	17.7
50.00	1.3	16.5
42.86	4.5	15.0
28.92	4.0	15.8
50.00	10.0	14.6
35.71	0.66	18.1
36.36	0.30	18.5
30.65	0.49	17.7
55.56	4.5	14.6
50.00	5.2	14.6
53.85	6.3	15.0
50.00	10.0	14.6
31.66	2e-27	99.8
22.58	6.7	14.6
24.42	0.41	18.5
66.67	3.7	15.4
40.00	8.9	14.2
36.36	1.7	15.8
54.55	3.2	15.4
51.90	7e-156	440
59.10	9e-177	494
58.87	3e-176	493
50.00	0.72	17.3
26.47	2.6	15.8
50.00	3.6	15.0
33.33	4.9	14.6
55.56	7.8	14.2
80.80	0.0	721
58.63	6e-176	491
58.87	3e-176	492
49.65	4e-155	439
56.13	4e-176	492
35.00	3.1	14.2
52.69	4e-98	281
34.62	1.1	15.8
47.83	4e-89	258
43.80	1e-76	227
49.80	1e-93	269
30.43	0.32	17.3
45.45	2.6	14.6
37.50	6.9	13.1
63.64	5e-132	368
50.36	3e-101	290
50.00	1.5	13.5
29.17	1.9	13.1
36.67	0.96	13.9
24.00	4.5	11.5
55.56	0.29	15.0
61.90	0.014	19.6
25.00	2.0	13.1
54.55	0.26	15.8
35.71	0.76	14.6
41.67	0.11	16.5
25.93	0.51	17.3
20.90	2.8	15.0
99.34	0.0	619
58.28	1e-132	373
58.58	3e-133	374
84.16	0.0	551
30.77	1.2	15.8
29.63	7.7	13.5
30.77	0.70	16.2
50.00	4.0	13.9
58.56	9e-131	368
50.57	3e-87	256
66.78	2e-166	458
71.67	1e-174	479
68.73	3e-173	478
35.71	6.7	13.9
69.06	2e-174	481
35.71	6.8	13.9
68.73	6e-174	480
35.71	6.7	13.9
75.26	7e-176	481
70.65	3e-180	493
73.10	4e-171	469
99.69	0.0	672
68.37	2e-162	447
33.33	1.7	14.6
29.63	1.1	14.6
46.67	0.001	23.1
29.41	5.2	12.3
37.50	0.61	15.0
37.50	0.62	15.0
50.00	2.3	14.2
26.32	0.50	15.8
18.42	5.8	12.3
28.85	0.94	15.0
50.00	2.1	14.2
35.29	3.8	13.5
24.00	2.2	14.6
28.85	2.3	14.6
28.85	2.2	14.6
24.00	2.3	14.6
31.58	3.7	13.9
33.33	0.73	16.2
33.33	0.72	16.2
33.33	0.71	16.2
33.33	0.72	16.2
35.71	0.99	15.4
55.56	2.3	14.2
31.82	0.52	15.0
79.39	2e-100	278
22.22	5.7	12.3
33.33	0.12	17.3
31.82	0.31	15.8
31.82	0.33	15.8
19.35	0.77	13.9
25.00	0.055	18.1
25.00	0.045	18.5
97.98	0.0	1014
99.19	0.0	1023
98.99	0.0	1022
98.39	0.0	1020
98.79	0.0	1021
99.19	0.0	1023
98.39	0.0	1018
98.59	0.0	1020
98.99	0.0	1022
87.10	0.0	919
96.04	0.0	1920
96.25	0.0	1924
99.48	0.0	2002
33.33	0.12	21.9
39.13	2.6	17.7
35.48	2.9	17.3
25.00	0.95	19.2
20.00	2.9	17.3
62.50	3.2	17.3
31.25	4.7	16.9
100.00	7.4	16.2
41.67	8.7	15.8
28.57	9.1	15.8
33.33	1.3	18.5
62.50	3.2	17.3
31.25	4.8	16.9
20.00	4.9	16.9
100.00	7.5	16.2
41.67	8.6	15.8
28.57	9.1	15.8
35.10	1e-167	508
98.46	0.0	1993
98.56	0.0	1993
35.91	1e-179	543
40.74	0.54	20.8
96.35	5e-101	276
99.27	1e-103	283
99.27	1e-103	283
99.27	1e-103	283
99.27	2e-103	282
99.27	3e-103	282
99.27	4e-104	284
99.27	2e-104	285
99.27	2e-103	283
98.54	1e-102	281
45.83	9e-95	276
45.54	6e-93	272
46.87	5e-104	300
46.57	4e-103	298
50.30	2e-107	309
49.70	2e-105	304
99.40	0.0	682
99.70	0.0	685
99.40	0.0	682
99.70	0.0	686
98.99	0.0	605
99.33	0.0	606
97.98	0.0	599
95.85	0.0	610
96.17	0.0	612
99.69	0.0	657
99.69	0.0	657
99.69	0.0	657
99.69	0.0	657
99.37	0.0	657
34.88	0.36	19.6
24.24	1.1	18.1
37.50	4.3	16.2
42.86	0.012	23.9
26.92	1.2	17.7
43.63	1e-174	508
42.86	0.012	23.9
28.85	0.20	20.0
99.85	0.0	1340
36.84	3.3	16.5
33.33	2.0	16.9
22.22	2.8	16.5
41.67	4.2	15.8
62.50	4.3	15.8
36.36	4.8	15.4
33.33	6.8	15.0
62.50	1.4	17.7
31.03	5.5	15.8
33.33	7.6	15.0
33.33	9.0	15.0
39.47	0.22	20.4
35.48	1.0	18.1
30.43	5.1	15.8
26.67	5.7	15.8
85.75	0.0	751
54.84	2e-162	457
55.58	2e-166	467
80.00	9.8	13.9
80.00	9.8	13.9
40.62	0.019	22.7
25.00	0.63	17.7
47.62	0.005	24.3
35.71	6.3	14.6
52.38	0.003	25.0
35.71	6.4	14.2
52.38	0.003	25.0
35.71	6.3	14.2
50.50	3e-151	429
31.03	0.094	20.0
37.50	1.6	15.8
25.58	8.7	13.5
26.09	1.6	16.2
54.55	7.5	13.9
50.00	8.9	13.9
46.15	0.69	17.3
33.33	6.9	13.9
66.67	0.90	16.9
85.71	3.1	15.4
71.43	6.2	14.2
44.44	7.3	14.2
30.77	8.9	13.9
26.19	1.4	16.2
71.43	5.7	14.2
26.09	1.4	16.2
54.55	7.5	13.9
50.00	9.0	13.9
60.00	0.006	23.9
23.89	0.12	19.6
24.24	0.58	17.7
55.56	1.5	16.2
20.83	3.2	15.0
31.25	8.3	13.9
60.00	0.006	23.9
24.64	0.096	20.0
24.24	0.59	17.3
55.56	1.5	16.2
20.83	3.2	15.0
31.25	8.5	13.9
83.33	0.83	16.9
29.41	0.43	16.9
53.33	0.80	16.2
50.00	5.7	13.5
50.00	0.59	16.5
25.00	0.15	18.5
42.31	0.55	16.5
27.27	0.68	16.5
66.67	5.4	13.5
42.86	0.20	18.5
20.00	8.3	13.1
22.73	0.45	16.9
42.31	0.54	16.5
27.27	0.64	16.5
66.67	5.3	13.5
50.00	0.71	15.8
30.30	0.79	15.4
58.33	1.1	15.8
36.36	1.8	15.0
62.50	2.1	14.6
23.75	0.083	19.2
34.48	7.8	13.1
35.29	0.17	18.5
43.48	4.4	13.9
21.43	4.1	11.9
66.67	1.1	13.5
35.71	7.3	10.8
33.33	7.9	10.8
28.57	9.0	10.8
64.75	1e-71	205
20.00	1.8	13.9
96.92	3e-92	257
20.59	0.65	15.0
25.00	9.5	11.5
41.18	1.9	12.3
81.52	4e-127	349
34.29	0.34	16.5
77.73	5e-120	331
31.43	0.24	16.9
86.32	1e-135	371
83.81	3e-133	365
87.26	5e-138	377
89.57	3e-143	390
77.99	5e-118	326
99.53	9e-158	427
86.73	3e-137	375
76.53	4e-115	319
29.67	2.1	13.9
20.45	3.2	13.5
26.67	1.4	13.9
60.00	4.7	12.3
60.00	0.027	19.2
55.56	0.37	15.4
42.11	1.6	14.2
21.80	1e-08	37.7
25.53	0.20	16.9
22.22	7.3	11.9
30.00	0.002	22.3
50.00	0.036	18.5
30.56	0.16	16.5
38.46	0.46	15.4
50.00	4.8	12.3
32.00	5.9	11.9
34.78	0.16	16.9
38.46	3.9	12.7
33.33	8.6	11.5
55.56	3.9	13.1
57.14	2.7	14.6
57.14	1.8	14.6
57.14	3.0	13.9
35.29	0.86	16.2
26.67	2.9	14.2
36.00	3.0	14.2
44.44	0.67	16.5
57.14	7.1	13.5
31.58	2.7	14.2
46.15	3.6	13.9
39.29	1.9	15.0
28.00	2.1	14.6
50.00	3.5	14.2
31.25	6.5	13.5
34.04	0.18	18.5
26.92	0.48	16.9
41.67	1.3	15.8
57.14	2.8	14.6
86.94	7e-130	358
86.94	2e-126	349
86.49	7e-126	347
85.14	2e-122	339
98.37	0.0	495
88.99	4e-133	367
86.55	1e-126	350
86.94	2e-152	415
85.59	6e-128	353
87.89	8e-130	358
50.00	0.10	14.2
50.00	0.11	14.2
21.43	0.098	14.6
60.00	0.045	16.5
53.33	0.089	19.2
44.44	3.9	14.2
60.00	0.72	16.2
25.86	1.2	15.8
40.00	2.5	14.6
45.45	1.4	15.8
31.58	6.7	13.9
53.20	6e-89	257
28.00	2.2	15.4
33.33	0.11	18.9
71.43	3.1	14.2
66.67	4.6	13.9
45.45	1.3	15.8
31.58	6.6	13.9
45.45	1.4	15.8
31.58	6.6	13.9
45.45	1.4	15.8
31.58	6.5	13.9
45.45	1.3	15.8
31.58	6.7	13.9
19.51	0.12	20.0
41.18	6.0	14.6
33.33	6.1	14.6
19.51	0.91	16.9
41.67	2.9	15.4
25.00	4.6	15.0
22.73	5.0	14.6
25.00	1.1	17.3
45.45	8.5	14.6
28.00	0.26	18.5
57.14	3.7	15.0
34.09	0.52	18.1
38.46	4.1	15.0
28.12	5.4	14.6
35.00	0.15	19.6
26.19	0.20	19.2
50.00	2.2	15.8
99.77	0.0	904
35.00	0.15	19.6
26.19	0.20	19.2
50.00	2.2	15.8
44.44	0.82	17.3
54.55	4.0	15.0
28.57	5.2	14.6
33.33	0.49	14.2
45.45	1.7	12.7
42.86	0.56	14.6
24.14	3.6	12.7
31.91	0.62	14.6
31.91	0.59	14.6
60.00	0.095	16.9
22.86	0.081	17.7
28.57	0.15	16.9
46.15	0.44	15.0
37.50	1.2	13.5
18.52	9.5	10.8
46.15	0.45	15.0
37.50	0.98	13.9
18.52	9.1	11.2
30.77	2.1	12.7
25.93	5.3	11.5
72.60	3e-121	334
21.62	4.1	13.5
36.36	6.4	12.7
35.71	3.2	14.2
25.00	4.6	13.5
44.44	4.7	13.5
35.00	8.3	12.7
35.71	3.2	14.2
25.00	4.5	13.5
44.44	4.5	13.5
35.00	8.3	12.7
35.71	3.2	14.2
25.00	4.5	13.5
44.44	4.5	13.5
35.00	8.3	12.7
99.52	4e-162	438
35.29	1.4	14.6
33.33	1.8	14.6
34.78	2.0	14.2
35.29	1.5	14.6
33.33	2.0	14.2
34.78	2.4	14.2
71.43	4.0	13.1
34.38	5.0	12.7
29.55	0.054	20.0
39.13	0.45	16.9
21.13	1.6	15.4
45.45	2.3	14.6
23.81	3.4	14.2
22.22	0.12	19.6
41.67	8.8	13.5
100.00	1.5	15.4
28.57	2.9	14.6
37.93	0.070	20.0
25.00	5.4	14.2
26.19	0.68	16.5
44.44	7.3	13.5
44.44	9.5	13.1
28.00	0.94	16.5
41.18	1.2	16.2
50.00	1.2	16.2
28.00	0.95	16.2
50.00	1.2	16.2
41.18	1.2	16.2
31.58	0.17	16.5
54.55	0.029	19.2
36.84	1.8	13.9
80.00	2.7	12.3
27.27	5.6	11.2
41.67	0.50	15.0
55.56	3.4	12.3
33.33	6.4	11.5
32.14	0.31	15.4
41.18	4.3	11.9
33.33	4.5	10.8
24.05	2.3	12.7
60.00	5.4	11.5
31.25	2.4	13.1
45.45	1.7	12.3
80.77	2e-64	181
33.33	1.4	12.7
28.21	0.67	13.9
28.21	0.67	13.9
36.36	1.1	15.8
47.06	0.38	17.7
34.62	0.44	17.3
62.50	1.4	15.8
30.00	3.1	14.6
36.36	3.5	14.6
36.36	6.3	13.9
30.77	2.8	14.6
31.03	4.8	13.9
36.36	1.1	15.8
36.36	1.0	15.8
28.57	1.1	16.5
28.57	4.5	14.6
99.66	0.0	611
99.00	0.0	612
99.00	0.0	613
44.44	0.15	20.4
56.25	3.8	15.8
45.45	4.5	15.4
36.84	6.6	15.0
41.67	9.5	14.6
29.17	9.9	14.2
27.78	0.26	19.6
33.33	1.1	17.7
41.67	2.8	16.5
66.67	9.1	14.6
28.57	0.14	20.4
31.82	5.2	15.4
32.26	6.0	15.4
34.84	4e-101	308
50.00	1.5	17.3
33.33	0.39	18.9
34.78	0.76	17.7
66.67	5.3	15.0
31.82	2.0	17.3
26.47	2.8	16.9
50.00	5.5	16.2
41.67	9.5	15.4
23.40	1.4	17.3
26.23	0.95	17.7
34.62	2.2	16.5
50.00	4.5	15.4
57.14	4.6	15.4
25.00	6.2	15.0
32.14	6.5	15.0
33.33	3.4	15.8
23.53	6.7	14.6
50.00	8.3	14.6
53.85	0.075	20.8
50.00	2.9	13.9
99.48	3e-144	391
26.92	0.28	16.5
57.14	3.5	13.1
26.09	2.4	13.9
30.43	0.016	20.4
99.48	3e-144	391
99.48	6e-144	390
26.09	1.1	15.0
41.67	2.0	15.4
44.44	6.1	13.9
71.43	1.7	15.8
58.33	2.9	15.4
66.67	7.2	13.9
99.23	0.0	793
80.00	6.8	13.9
34.48	0.027	21.2
30.77	0.26	18.1
37.50	0.66	16.9
35.29	2.0	15.4
35.29	3.9	14.2
30.00	4.8	14.2
30.43	0.69	17.3
42.86	3.3	15.0
33.33	5.2	14.6
50.00	0.21	19.6
40.00	2.0	15.8
55.56	6.6	14.2
99.44	0.0	745
99.44	0.0	747
99.72	0.0	749
99.72	0.0	748
99.72	0.0	749
99.44	0.0	748
99.72	0.0	749
99.72	0.0	748
99.72	0.0	748
99.17	0.0	746
76.89	1e-114	318
76.34	3e-114	317
77.33	4e-117	325
77.43	7e-114	317
74.45	3e-108	302
99.55	3e-167	452
76.44	7e-113	314
79.56	2e-116	323
75.66	1e-110	308
78.67	5e-117	324
41.67	2.1	13.9
57.14	0.067	16.9
23.61	0.094	17.7
57.14	2.4	13.1
57.14	2.4	13.1
57.14	2.3	13.1
36.84	0.39	15.8
46.67	0.50	15.4
100.00	5.2	12.3
30.77	0.15	17.3
50.00	0.18	16.9
28.00	0.27	17.3
25.93	0.10	18.5
83.33	1.5	15.0
33.33	1.6	13.9
60.00	0.61	15.4
39.39	3.8	13.1
31.82	2.4	14.2
50.00	3.6	13.9
50.00	3.7	13.9
89.19	4e-140	383
89.14	2e-126	348
89.73	9e-142	387
88.89	4e-140	383
86.49	2e-134	369
81.17	7e-122	337
90.27	2e-143	392
91.15	6e-147	400
90.54	4e-144	393
95.05	4e-154	419
27.59	0.31	16.5
28.21	0.47	16.2
27.27	0.75	16.5
27.27	0.83	16.2
27.27	0.78	16.5
21.88	0.60	16.9
71.43	2.9	14.2
37.50	5.3	13.5
37.50	5.3	13.5
27.27	5.1	13.9
27.27	5.0	13.9
21.88	0.62	16.9
34.22	1e-56	182
34.95	6e-58	185
34.95	4e-58	186
35.22	1e-58	187
32.79	9e-45	150
34.02	4e-42	142
47.62	2.4	15.8
45.45	3.3	15.0
99.75	0.0	822
34.14	5e-42	142
47.62	2.4	15.8
44.44	4.5	15.0
32.79	9e-45	150
99.12	8e-168	454
99.56	3e-168	455
99.12	5e-167	452
99.12	6e-167	451
99.56	3e-168	455
99.56	5e-168	454
99.12	3e-167	452
99.12	2e-167	453
99.12	3e-167	452
99.12	4e-167	452
99.75	0.0	822
99.75	0.0	819
99.75	0.0	821
99.75	0.0	822
99.49	0.0	819
99.75	0.0	819
99.75	0.0	819
99.75	0.0	819
99.75	0.0	821
100.00	0.0	820
31.58	6.0	14.2
100.00	0.63	16.9
26.92	3.8	14.2
29.17	8.5	13.1
30.30	1.3	15.4
44.44	3.0	14.2
30.30	1.3	15.4
44.44	3.1	14.2
34.78	0.13	18.9
42.86	7.0	13.5
31.08	0.053	18.9
36.36	8.4	12.3
31.08	0.076	18.9
36.36	9.7	12.3
22.22	2.2	14.6
22.73	4.7	14.2
28.57	8.8	13.5
74.83	4e-169	462
92.41	0.0	556
57.14	9.1	13.1
91.72	0.0	551
57.14	9.0	13.1
92.07	0.0	552
57.14	9.4	13.1
92.07	0.0	555
57.14	9.1	13.1
99.32	0.0	600
61.32	1e-136	380
98.63	0.0	595
66.67	2.8	14.6
80.00	6.2	13.5
69.37	1e-156	431
23.68	0.18	18.9
66.67	0.78	16.5
50.00	2.1	15.0
33.33	2.9	15.0
66.67	4.8	14.6
26.32	2.7	15.0
28.57	2.8	15.0
60.00	1.8	15.4
30.43	4.7	14.2
66.67	4.9	14.2
54.55	9.0	13.1
25.00	0.004	23.9
46.67	2.4	15.0
41.67	6.6	13.9
30.00	0.35	17.7
33.33	0.20	18.5
28.85	0.26	18.1
45.45	1.7	15.4
76.92	3e-06	27.7
76.92	3e-06	27.7
28.57	0.030	16.9
55.56	0.050	16.2
27.03	2.4	11.9
27.03	1.4	12.3
100.00	1e-60	169
36.84	0.22	19.2
58.33	0.25	19.2
20.69	2.9	15.8
33.33	4.9	15.0
25.00	7.7	14.2
36.84	0.057	21.2
34.62	0.28	18.9
30.43	0.38	18.5
36.36	2.2	15.8
37.50	9.5	13.9
46.15	2.6	15.8
71.43	4.9	15.0
46.15	2.6	15.8
71.43	4.8	15.0
23.21	0.91	16.9
33.33	1.1	16.9
85.71	1.2	16.5
80.00	3.5	15.4
31.58	6.1	14.6
25.00	7.6	14.2
69.60	0.0	616
44.44	0.70	17.3
42.86	1.8	15.8
36.36	4.1	14.6
21.36	0.87	17.3
21.36	0.84	17.3
36.00	4.4	14.6
23.26	0.098	20.4
24.14	1.3	16.9
50.00	5.0	15.0
23.53	7.6	14.2
22.73	3.0	15.4
38.46	2.5	16.2
38.46	7.7	14.6
40.00	8.0	13.9
24.07	1.4	16.2
36.00	4.4	14.6
26.47	1.0	17.3
44.44	1.6	16.5
38.89	3.5	15.4
26.47	1.0	17.3
44.44	1.6	16.5
38.89	3.5	15.4
30.00	0.48	18.5
54.55	1.4	16.9
21.43	1.5	16.5
71.43	5.7	15.0
29.03	0.20	19.6
29.51	3.8	15.4
50.00	5.2	15.0
46.15	6.3	14.6
98.36	0.0	732
97.28	0.0	719
97.81	0.0	726
98.09	0.0	729
98.07	0.0	723
97.57	0.0	740
98.65	0.0	743
98.65	0.0	743
98.36	0.0	734
99.46	0.0	753
98.25	2e-166	450
99.12	1e-167	453
98.68	6e-167	452
99.12	2e-167	452
99.56	4e-168	454
99.56	9e-168	454
98.68	1e-166	451
99.56	6e-168	454
99.12	3e-167	452
98.68	3e-166	450
73.11	0.0	514
68.98	0.0	506
72.32	0.0	516
69.51	0.0	502
21.15	0.31	18.5
75.00	0.70	17.3
25.00	4.4	14.6
72.32	0.0	516
33.33	5.2	14.6
27.78	9.0	13.9
41.67	0.35	17.7
80.00	2.7	14.6
30.56	2.2	15.4
40.00	3.2	15.0
83.33	3.5	14.6
50.00	5.6	14.2
63.96	2e-164	459
25.49	0.44	18.1
35.71	2.4	15.4
37.50	2.8	15.0
41.67	7.4	13.9
33.33	3.0	14.6
40.00	3.4	14.2
80.00	6.8	13.5
66.67	5.0	14.2
35.00	5.3	13.9
66.67	5.9	13.9
50.00	9.1	13.5
31.43	0.61	16.5
42.86	1.6	15.4
27.78	2.3	15.0
44.33	8e-85	248
44.67	7e-86	251
44.33	9e-85	248
26.92	1.1	16.2
40.00	2.3	15.4
23.08	4.2	14.2
26.47	0.050	19.6
40.00	0.41	16.9
50.00	4.0	13.9
37.50	5.4	13.5
66.67	1.9	15.4
80.00	7.9	13.1
52.94	1e-108	309
74.60	4e-61	174
72.66	2e-62	178
92.13	4e-87	241
76.38	9e-63	179
76.19	4e-64	182
76.38	2e-65	186
77.69	1e-69	196
76.34	1e-67	192
66.22	1e-63	182
75.89	4e-68	193
35.71	0.14	17.3
50.00	0.36	15.8
29.17	2.2	13.5
29.41	7.2	11.9
41.67	2.0	13.9
27.78	2.2	13.9
36.11	0.15	16.9
25.45	0.91	15.0
55.80	1e-80	229
86.89	7e-118	323
42.01	6e-38	119
71.43	0.75	14.2
45.45	1.7	13.5
34.78	3.6	12.7
42.86	8.8	11.5
40.49	6e-34	106
85.71	0.76	14.6
40.00	2.1	13.5
40.24	1e-33	105
24.49	3.5	12.7
63.64	0.039	18.1
35.71	1.0	13.9
50.00	1.2	13.9
99.46	2e-133	363
97.83	2e-131	357
51.91	9e-56	165
48.86	3e-51	153
49.43	2e-52	157
75.29	3e-91	256
98.92	1e-133	363
48.86	5e-52	155
49.43	4e-52	155
49.43	4e-52	156
33.33	0.065	20.8
35.71	3.6	15.4
80.00	4.3	15.0
40.00	0.82	17.3
32.26	1.4	16.9
19.15	3.5	15.8
37.50	0.93	16.9
23.73	2.1	15.8
32.26	1.4	16.9
19.15	3.7	15.4
45.45	5.6	15.0
50.00	0.86	17.3
34.15	1.00	16.9
23.53	3.1	15.4
35.00	4.7	14.6
32.26	1.4	16.9
19.15	3.3	15.8
53.85	1.4	16.9
100.00	2.3	16.2
41.67	3.2	15.8
37.50	3.2	15.8
30.77	6.3	14.6
32.26	1.4	16.9
19.15	3.4	15.8
56.03	5e-49	142
56.60	1e-45	133
55.93	1e-50	146
51.75	9e-47	137
31.58	0.43	14.6
56.41	2e-50	146
38.46	1.4	13.5
99.14	2e-85	235
26.67	0.70	13.9
32.14	0.058	16.2
80.00	0.57	13.5
38.46	2.7	11.5
27.27	4.5	11.2
20.00	1.6	12.7
33.33	3.9	11.9
98.32	6e-84	231
42.11	0.096	16.5
46.15	0.084	16.9
24.14	0.56	14.2
62.50	0.086	16.9
50.00	0.35	15.0
25.00	0.026	18.5
30.00	4.2	11.9
36.21	9e-59	184
78.27	0.0	543
55.56	1.7	15.4
53.85	1.8	15.4
59.94	6e-148	413
55.56	0.063	20.4
62.50	3.2	15.0
63.64	3.3	15.0
35.71	9.3	13.5
57.14	1.2	16.2
33.52	3e-45	148
25.81	0.23	17.7
100.00	1.4	15.4
41.67	7.4	13.1
30.00	8.4	13.1
38.10	0.010	22.3
37.50	0.18	18.5
40.00	0.10	19.2
28.21	0.62	16.9
29.41	1.0	16.2
98.37	0.0	729
97.82	0.0	724
96.17	0.0	706
98.09	0.0	728
97.28	0.0	719
97.80	0.0	722
97.84	0.0	739
97.84	0.0	739
98.92	0.0	743
98.37	0.0	733
55.56	0.068	16.5
99.07	2e-78	216
45.05	6e-28	87.8
55.56	0.44	13.9
99.07	7e-78	215
90.13	9e-143	392
66.67	5.4	13.1
90.13	6e-143	392
66.67	5.3	13.1
89.08	5e-137	377
66.67	5.1	13.5
91.45	7e-145	397
66.67	5.3	13.5
90.13	2e-153	419
66.67	5.5	13.1
89.70	1e-143	394
66.67	5.3	13.1
89.74	6e-143	392
66.67	5.2	13.5
88.46	2e-140	386
66.67	5.3	13.5
90.13	7e-143	392
66.67	5.3	13.1
88.65	9e-137	376
66.67	5.1	13.1
50.00	8e-83	240
54.44	7e-100	284
36.36	2.6	14.6
42.86	9.7	12.7
16.13	5.7	12.7
18.46	0.38	16.5
66.67	3.1	13.9
31.58	1.8	15.0
34.62	4.0	13.9
30.95	5e-04	26.2
39.13	1.2	15.8
30.00	5.3	13.5
42.86	6.5	13.5
31.82	0.50	16.9
50.00	1.1	15.8
93.61	7e-138	377
87.67	2e-133	366
83.26	8e-127	349
82.81	5e-126	347
79.91	1e-120	333
86.76	6e-132	362
86.30	5e-137	375
80.82	5e-120	332
86.10	6e-133	365
86.88	6e-133	365
52.27	2e-65	190
79.19	7e-105	291
53.29	2e-63	185
79.19	7e-105	292
58.47	7e-82	232
83.24	2e-110	305
99.43	3e-132	360
97.09	6e-126	344
99.45	2e-137	374
65.03	4e-76	217
35.29	0.37	17.3
25.00	1.1	16.2
25.00	0.46	17.3
23.68	1.5	15.8
23.62	9.7	13.1
37.50	8.5	12.7
46.15	9.5	12.3
27.71	0.11	19.6
34.78	1.2	16.2
23.91	0.97	15.8
31.58	0.94	16.2
32.00	0.98	16.2
24.00	2.0	15.0
29.09	0.43	18.5
66.67	0.050	21.2
99.77	0.0	907
55.56	1.9	16.2
30.43	2.0	16.2
71.43	2.2	15.8
36.36	3.5	15.4
33.33	5.8	14.6
99.55	0.0	905
99.55	0.0	907
99.77	0.0	908
40.91	1.4	16.2
44.44	0.50	17.7
19.05	4.1	14.6
33.33	9.9	13.5
25.53	0.76	17.7
58.33	1.8	16.5
71.43	7.3	14.6
99.60	0.0	1548
99.60	0.0	1548
99.60	0.0	1546
99.60	0.0	1546
99.60	0.0	1546
99.60	0.0	1547
99.60	0.0	1548
99.87	0.0	1552
99.73	0.0	1549
99.73	0.0	1550
28.21	0.14	19.2
27.27	0.67	16.9
26.09	1.5	16.2
50.00	2.9	15.0
30.00	3.4	15.4
25.00	3.6	15.4
23.08	2.6	15.4
28.12	0.12	19.6
50.00	0.75	17.3
46.15	0.84	16.9
33.33	7.8	13.9
57.14	9.9	13.5
57.14	0.73	16.5
55.56	1.7	15.8
24.79	0.99	16.2
20.99	0.46	18.1
20.00	0.61	17.7
37.93	2.6	15.8
69.83	6e-103	289
69.66	5e-109	305
72.65	3e-111	310
72.89	1e-105	296
75.22	3e-116	323
80.79	4e-132	363
77.73	1e-123	342
77.29	1e-122	340
70.17	8e-106	296
77.29	5e-121	335
30.88	0.010	26.6
25.71	2.7	18.5
28.57	1.00	20.8
40.74	2.5	19.6
46.67	4.5	18.9
30.88	0.010	26.6
25.71	2.5	18.5
20.75	2.3	18.9
40.91	2.4	18.9
25.00	2.7	18.5
32.00	4.7	17.7
58.33	6.5	17.3
30.88	0.009	26.6
25.71	2.5	18.5
33.33	0.12	23.1
40.62	0.23	21.9
26.47	3.3	18.1
28.12	8.7	16.9
33.33	0.12	22.7
40.62	0.24	21.9
26.47	3.3	18.1
28.12	8.7	16.9
99.92	0.0	2689
99.92	0.0	2689
27.45	0.71	18.5
23.81	7.2	15.0
71.43	1.3	17.3
26.42	1.3	16.9
32.25	2e-75	240
66.67	1.7	17.3
39.29	1.7	16.9
27.78	5.2	15.4
47.62	0.85	18.1
40.00	5.3	15.4
30.77	0.094	21.2
31.25	0.30	19.2
42.31	0.90	17.7
31.82	1.5	16.9
50.00	2.0	16.5
29.41	3.8	15.4
57.14	7.2	14.6
57.14	0.10	20.8
36.84	3.6	15.8
38.46	5.7	15.4
48.15	3e-43	130
44.44	0.22	15.8
26.83	0.94	13.9
40.00	0.71	14.6
33.33	1.0	13.9
50.00	1.9	13.5
30.77	0.21	15.4
41.67	5.2	11.2
50.00	3.5	12.3
33.33	4.1	11.9
69.93	3e-74	210
55.24	2e-60	174
73.02	0.0	495
73.02	0.0	495
73.02	0.0	494
72.70	4e-180	493
73.02	0.0	494
73.02	0.0	495
73.02	0.0	495
73.02	0.0	494
73.02	0.0	496
73.33	0.0	497
31.82	2.7	13.5
25.00	1.1	15.8
25.00	1.1	15.8
25.00	1.0	15.8
25.00	1.1	15.8
25.00	1.1	15.8
25.00	1.1	15.8
31.58	1.1	15.4
31.25	4.4	13.5
25.00	1.2	15.8
60.00	6.4	10.0
35.29	0.47	14.2
30.00	0.60	13.9
35.29	0.47	14.2
48.96	6e-35	105
54.55	0.76	16.2
32.14	2.5	14.6
28.57	3.2	14.2
55.56	9.7	12.7
20.69	9.3	12.3
29.27	0.13	17.7
20.00	2.7	13.9
24.66	2.7	13.9
21.43	7.8	12.3
22.73	8.7	12.3
42.15	1e-57	173
41.67	2.3	14.2
53.85	1.0	15.4
31.58	1.5	14.2
25.00	4.1	13.1
32.14	1.8	14.6
23.40	3.0	13.9
34.38	0.31	20.0
34.38	0.31	20.0
34.38	2.5	16.9
31.03	6.1	15.8
23.19	0.18	21.2
35.71	2.2	17.7
27.08	0.21	20.8
36.67	0.27	20.4
45.45	0.43	19.6
29.09	1.1	18.5
62.50	9.2	15.4
23.19	0.17	21.2
35.71	2.1	17.7
27.91	1.4	18.1
50.00	2.9	16.9
28.00	3.0	16.9
62.50	7.1	15.8
37.50	9.9	15.4
75.00	1.7	17.7
32.14	5.9	15.8
26.85	0.16	21.2
44.44	0.71	18.9
43.75	1.9	17.7
37.50	5.3	16.2
100.00	6.1	16.2
43.75	6.6	15.8
100.00	0.021	23.9
26.51	2.3	17.3
55.56	6.7	15.8
100.00	0.027	22.3
28.57	1.4	16.5
41.67	1.7	16.5
29.58	9.0	14.2
100.00	0.026	22.3
28.57	1.4	16.5
41.67	1.8	16.5
29.58	8.4	14.2
100.00	0.026	22.3
28.57	1.4	16.5
41.67	1.7	16.5
29.58	8.4	14.2
100.00	0.026	22.3
28.57	1.4	16.9
41.67	1.7	16.5
99.75	0.0	827
99.50	0.0	828
100.00	0.026	22.3
28.57	1.4	16.9
41.67	1.7	16.5
100.00	0.028	22.3
28.57	1.5	16.5
41.67	1.8	16.5
29.58	8.1	14.2
100.00	0.027	22.3
28.57	1.5	16.9
41.67	1.7	16.5
100.00	0.026	22.3
28.57	1.4	16.9
41.67	1.7	16.5
25.49	0.53	17.3
46.15	3.0	15.0
29.41	1.3	15.4
33.33	2.2	15.0
57.14	9.9	12.7
33.33	2.2	15.0
29.41	1.3	15.8
29.41	1.4	15.4
29.41	1.3	15.8
83.33	1.3	15.4
44.44	2.2	14.6
83.33	3.0	14.2
34.78	3.7	13.9
33.33	2.2	15.0
21.05	2.6	14.6
57.14	9.9	12.7
99.46	1e-132	361
98.92	2e-132	360
98.92	1e-132	361
98.92	4e-132	359
99.46	5e-133	362
98.38	1e-130	355
98.92	1e-132	361
98.92	1e-132	361
98.92	2e-132	360
98.92	2e-132	360
55.88	3e-123	348
55.88	7e-124	349
56.21	2e-124	351
79.30	0.0	530
99.68	0.0	638
100.00	0.0	640
99.68	0.0	636
75.80	0.0	506
100.00	0.0	640
75.48	0.0	505
62.50	0.50	13.5
50.00	0.47	13.5
50.00	0.42	13.5
100.00	6e-48	137
99.39	0.0	679
76.92	1e-178	491
99.40	0.0	694
85.33	0.0	560
77.57	1e-178	490
99.70	0.0	695
77.57	7e-179	491
100.00	0.0	691
100.00	0.0	696
55.46	1e-136	385
99.76	0.0	1751
99.64	0.0	1748
80.45	0.0	1406
97.27	0.0	1724
99.64	0.0	1711
99.88	0.0	1715
80.45	0.0	1405
99.88	0.0	1754
99.76	0.0	1753
47.12	0.0	733
31.03	0.47	17.7
30.43	3.9	14.6
71.43	2.8	14.6
26.92	0.070	20.0
37.50	0.39	17.7
20.00	0.43	17.7
44.44	4.2	14.6
66.67	7.6	13.9
46.15	5.6	14.6
53.85	0.57	17.7
46.15	5.6	14.6
40.00	2.2	15.4
29.41	2.9	15.0
26.67	6.9	13.9
46.15	5.2	14.6
46.15	5.1	14.6
27.42	0.10	19.6
70.00	0.22	19.2
45.45	7.7	14.2
26.67	8.1	14.2
53.33	0.22	18.5
66.67	4.7	14.2
34.78	0.82	16.5
28.57	2.1	15.0
66.67	2.2	15.0
50.00	8.4	13.1
23.81	9.0	13.1
22.22	1.3	16.2
38.46	3.2	15.0
44.44	0.58	16.9
30.00	3.2	14.6
62.50	3.4	14.6
29.63	2.0	15.8
24.75	8.5	13.9
50.00	0.62	16.9
83.33	4.5	14.2
27.78	0.88	16.5
89.91	1e-150	409
91.74	4e-155	421
89.91	4e-151	410
89.45	3e-148	403
88.79	2e-145	396
88.99	1e-148	404
88.99	1e-142	389
91.24	9e-154	417
95.39	9e-160	432
94.04	1e-157	427
43.75	3.6	16.2
27.59	5.2	15.4
25.00	1.2	17.3
33.33	3.1	16.2
35.71	4.1	15.8
31.82	5.2	15.4
25.00	1.2	17.3
33.33	3.1	16.2
35.71	4.1	15.8
31.82	5.3	15.4
72.89	0.0	510
32.35	0.56	17.7
50.00	3.9	15.0
23.68	1.8	16.5
28.57	3.0	15.8
66.67	3.4	15.8
50.00	9.4	14.2
92.96	0.0	924
23.81	0.64	18.1
26.67	0.88	17.7
30.43	6.6	15.0
36.36	0.83	16.9
25.40	6.5	14.2
50.00	6.9	13.9
26.09	1.8	16.5
71.43	2.4	16.2
31.25	2.6	16.2
68.59	0.0	698
36.00	3.4	15.0
30.00	8.7	13.5
60.61	0.28	18.5
27.91	0.67	17.3
80.00	1.1	16.2
28.57	0.12	19.2
30.00	0.18	18.9
30.77	0.95	16.5
26.09	0.53	17.3
33.33	3.6	14.6
71.43	3.7	14.6
46.15	8.4	13.5
55.56	4.6	14.6
21.88	6.6	13.9
78.26	4.1	14.6
100.00	4.5	14.6
50.00	7.7	13.9
50.00	9.2	13.5
83.33	1.4	16.2
75.00	1.4	15.8
29.55	3.2	15.0
98.37	1e-93	256
98.37	4e-93	255
98.37	4e-93	255
97.56	2e-92	253
98.37	2e-93	256
98.37	1e-93	256
99.19	4e-94	258
98.37	1e-93	256
98.37	8e-94	257
99.19	2e-94	258
33.33	0.21	16.5
99.46	7e-143	387
54.55	0.082	18.5
29.03	0.096	18.1
36.36	0.48	16.2
35.29	1.0	14.6
22.73	2.2	13.9
33.33	9.5	11.9
50.00	1.3	13.9
38.46	3.5	12.7
26.53	0.83	14.6
98.94	7e-144	389
45.00	0.11	19.6
24.07	1.4	16.2
33.33	0.83	15.8
83.33	0.85	15.8
36.36	3.7	13.9
45.45	7.9	12.7
45.00	0.11	19.6
24.07	1.4	16.2
35.29	0.005	23.1
23.38	0.054	20.0
99.72	0.0	729
99.72	0.0	729
99.72	0.0	729
95.43	1e-119	341
46.15	8.2	13.9
30.56	1.9	15.4
27.27	4.8	14.2
33.33	4.9	14.2
100.00	0.004	23.1
26.32	2.2	14.6
26.32	2.8	14.2
35.71	3.5	13.9
29.63	0.14	18.5
44.44	3.4	14.2
35.71	5.6	13.5
50.00	9.3	12.7
50.00	0.52	16.5
27.27	1.2	15.4
71.43	1.5	15.4
66.67	5.6	13.5
35.48	6.6	13.1
36.84	2.2	14.2
37.93	0.043	20.0
66.67	2.1	14.6
57.14	6.2	13.1
41.67	6.8	13.1
98.84	0.0	531
98.84	0.0	528
99.23	0.0	533
29.63	0.084	19.2
44.44	3.4	14.2
35.71	5.7	13.5
50.00	9.4	12.7
52.63	0.026	22.7
35.71	0.60	18.5
62.50	2.9	16.2
26.67	4.2	15.8
50.00	4.4	15.8
24.64	0.30	18.9
25.00	1.5	16.5
30.00	1.7	16.5
45.45	8.6	14.2
57.14	8.8	13.5
36.84	0.065	20.8
31.25	2.2	16.2
38.89	4.0	15.0
36.36	6.8	14.2
28.57	9.7	13.9
37.50	2.0	15.8
34.09	7.1	14.2
43.75	0.20	19.2
25.58	1.8	16.2
50.00	4.2	15.0
29.73	0.62	17.7
38.10	1.2	16.5
36.36	4.5	15.0
41.18	5.6	14.6
99.77	0.0	901
100.00	0.0	904
99.55	0.0	897
80.74	2e-86	239
52.80	3e-49	144
35.94	2e-25	83.2
50.00	1.2	13.5
57.14	4e-53	154
57.14	6e-53	154
56.35	2e-52	152
99.28	2e-104	285
57.14	6e-53	154
30.00	0.98	14.6
25.00	2.3	13.5
50.00	0.58	15.0
33.33	9.9	11.2
38.89	1.0	14.2
50.00	5.5	11.9
66.67	1.3	14.2
25.00	1.1	13.9
23.08	0.70	14.2
42.86	6.0	11.5
92.31	4e-101	277
99.10	6e-165	446
98.65	3e-164	444
98.65	3e-164	444
99.10	4e-165	446
99.10	2e-165	447
98.65	6e-165	446
99.10	2e-165	447
99.10	1e-164	445
99.55	2e-165	447
99.55	9e-166	448
100.00	0.0	801
99.74	0.0	797
99.22	0.0	793
99.74	0.0	798
100.00	0.0	799
100.00	0.0	800
100.00	0.0	800
100.00	0.0	801
100.00	0.0	801
100.00	0.0	800
99.52	6e-158	427
98.57	2e-159	431
98.08	5e-156	422
98.17	2e-164	445
95.83	2e-148	404
95.37	2e-149	407
95.83	4e-148	404
98.62	2e-165	447
99.52	1e-158	429
98.10	6e-158	427
60.00	0.020	17.3
39.29	0.043	16.2
33.33	0.047	16.2
44.44	0.011	18.5
41.67	0.015	18.1
31.25	0.22	15.0
54.55	1.3	15.4
30.56	3.5	13.9
38.89	3.0	14.6
38.89	0.25	18.1
24.14	1.8	15.4
35.71	6.4	13.9
35.71	8.7	13.5
28.21	0.56	16.9
29.41	4.2	14.2
37.50	8.0	13.5
16.67	8.2	13.5
25.71	9.9	13.1
99.27	0.0	564
55.56	2.8	14.6
66.67	2.8	14.6
66.67	4.9	13.9
20.29	5.4	13.9
25.00	7.9	13.1
55.56	2.8	14.6
66.67	2.8	14.6
66.67	4.9	13.9
19.72	5.5	13.9
25.00	8.1	13.1
60.00	1.1	16.2
25.00	2.7	14.2
37.50	3.8	13.9
29.41	3.9	13.9
25.00	7.3	13.1
66.67	0.24	14.2
66.67	0.25	14.2
45.45	0.25	14.2
71.43	0.67	13.9
24.07	0.28	18.1
38.89	0.59	16.9
26.19	1.9	15.4
41.18	1.2	15.8
24.10	2.0	15.0
50.00	3.6	14.2
26.47	10.0	13.1
50.78	5e-117	332
27.87	0.32	18.5
45.45	1.3	16.5
53.85	5.5	14.2
34.15	3.5	14.6
55.56	0.93	16.5
33.33	8.4	13.5
53.85	0.45	17.3
41.18	1.2	15.8
24.10	2.0	15.0
50.00	3.6	14.2
35.71	5.0	14.6
28.00	5.2	14.6
26.92	0.47	17.3
16.00	5.5	12.7
33.33	8.7	12.3
25.00	3.9	13.1
17.65	5.1	13.1
25.00	6.0	12.7
20.65	1.8	14.6
18.00	1.4	14.2
50.00	0.056	18.9
22.73	0.064	18.5
35.71	6.8	12.3
42.86	0.29	16.9
24.59	1.5	14.6
30.19	0.072	18.1
50.00	3.4	13.1
30.77	0.18	16.9
25.00	7.1	11.9
25.00	7.0	11.9
42.86	0.69	14.6
31.48	0.16	17.3
50.00	3.5	13.1
99.42	3e-128	348
50.00	1.4	14.2
36.84	3.7	13.1
31.91	0.20	19.6
31.82	0.37	18.9
25.00	3.3	15.8
42.86	6.8	15.0
31.91	0.19	20.0
31.82	0.37	18.9
25.00	3.0	16.2
42.86	6.4	15.0
31.91	0.19	20.0
31.82	0.37	18.9
25.00	3.0	16.2
42.86	6.4	15.0
33.33	0.83	17.7
30.56	1.4	16.9
58.33	4.7	15.4
55.56	4.9	15.4
29.17	9.0	14.6
27.27	0.38	18.5
21.11	1.3	16.9
62.50	7.8	14.2
27.87	0.15	19.6
36.00	1.9	16.2
26.83	3.2	15.4
44.44	4.2	15.0
28.57	9.3	13.9
25.93	1.9	16.2
45.00	1.9	16.2
21.43	7.3	14.6
54.55	0.11	20.8
75.00	0.94	17.7
33.33	9.2	14.6
35.48	0.24	19.2
35.29	1.1	16.9
30.19	1.9	16.2
35.29	0.12	17.3
37.50	0.76	15.0
26.67	2.7	12.7
40.00	0.11	15.8
38.46	1.2	12.7
27.50	7.0	11.2
99.29	5e-105	286
99.29	1e-104	286
39.13	0.63	14.6
57.14	1.4	13.5
55.46	1e-34	105
50.83	8e-32	98.6
49.17	6e-36	108
53.33	9e-43	127
53.33	5e-41	122
53.72	7e-42	124
53.33	8e-42	124
54.17	2e-41	123
53.33	2e-40	120
65.55	4e-47	137
45.65	1e-46	140
46.29	6e-45	137
46.71	1e-43	132
40.48	7e-41	126
31.40	2e-18	66.2
50.00	2.9	13.1
27.65	4e-16	59.3
42.86	4e-04	25.4
42.86	3.4	14.2
29.41	5.2	13.9
32.26	0.10	18.1
36.36	6.0	12.3
32.26	0.16	17.3
36.36	6.9	12.3
41.67	1.3	15.4
50.00	2.7	14.2
30.00	0.65	16.2
99.59	0.0	497
37.50	0.58	16.5
35.00	1.7	15.0
54.20	0.0	532
56.51	0.0	581
51.36	9e-179	503
94.33	0.0	929
52.73	0.0	518
94.12	0.0	927
49.58	1e-175	494
56.51	0.0	555
56.30	0.0	579
95.17	0.0	937
46.48	9e-43	129
27.27	5.9	12.3
40.14	3e-35	109
44.44	3.0	12.3
40.54	2e-35	111
27.27	6.1	12.3
45.95	5e-42	127
32.17	4e-20	70.1
26.09	1.2	13.5
98.65	0.0	606
99.33	0.0	611
98.67	0.0	611
99.33	0.0	617
62.33	4e-136	380
59.18	2e-126	354
99.66	0.0	613
98.66	0.0	608
99.33	0.0	612
99.66	0.0	612
42.52	2e-85	252
25.51	0.15	18.9
30.77	1.6	15.4
26.58	2.2	15.0
41.67	5.8	13.9
24.32	5.9	13.9
31.25	1.8	15.8
62.50	2.9	15.4
20.43	8.4	13.5
37.93	1.5	15.8
20.43	7.8	13.9
25.00	0.65	16.5
41.18	1.4	15.8
37.50	2.8	14.6
41.67	4.7	13.9
27.27	7.0	13.5
37.93	1.6	15.8
20.43	7.9	13.5
60.19	3e-145	405
29.63	5.6	13.5
30.43	0.15	18.9
46.15	0.48	17.3
25.23	2.9	15.0
16.13	7.3	13.5
24.00	8.2	13.5
50.00	0.51	16.2
34.04	0.58	15.8
66.67	0.75	15.4
36.36	0.77	15.4
31.43	0.034	19.2
62.50	0.75	15.4
99.52	1e-159	431
59.24	2e-89	253
99.05	5e-159	430
99.05	9e-159	429
61.90	5e-95	267
83.19	2e-128	354
77.78	3e-113	315
80.26	6e-120	332
81.86	4e-122	337
80.97	2e-124	343
83.63	2e-128	355
80.00	4e-123	340
82.74	2e-128	354
82.38	3e-127	351
82.38	3e-127	351
50.00	6.1	15.4
42.86	8.9	14.6
50.00	6.1	15.4
42.86	9.0	14.6
50.00	6.1	15.4
42.86	8.8	14.6
47.06	0.29	18.5
47.06	0.29	18.5
47.06	0.29	18.5
47.06	0.29	18.5
47.06	0.29	18.5
23.81	1.8	17.3
24.24	3.2	16.5
21.05	8.5	15.0
50.00	8.6	15.0
99.80	0.0	1002
75.65	1e-110	306
72.40	1e-101	284
70.20	9e-105	293
68.50	3e-104	292
68.50	2e-104	293
68.00	7e-103	289
68.50	2e-104	293
68.50	3e-104	292
69.15	3e-100	280
65.71	5e-87	247
78.57	5e-05	29.6
27.27	7.0	13.5
38.71	8e-04	26.6
30.77	0.97	16.2
31.25	6.3	13.9
50.00	1.5	16.2
37.50	10.0	13.5
20.83	4.1	14.2
38.46	6.6	13.5
43.33	0.39	18.1
35.29	0.96	16.9
66.67	5.2	14.6
44.83	0.13	19.6
50.00	1.4	16.2
44.83	0.13	19.2
50.00	1.4	16.2
44.83	0.13	19.6
50.00	1.4	16.2
68.09	0.0	503
54.75	2e-122	347
25.81	0.32	18.1
43.03	3e-68	206
60.00	0.65	16.5
40.00	2.5	15.0
55.56	0.18	18.9
50.00	1.5	15.8
25.93	3.2	15.0
66.67	4.6	14.2
18.63	1.2	16.2
26.09	1.5	15.8
38.46	2.1	15.4
40.00	2.2	15.0
36.36	4.1	14.2
43.54	9e-81	239
43.20	2e-79	235
51.28	6e-110	317
30.00	2.2	15.4
43.90	2e-68	206
60.00	0.65	16.5
40.00	2.6	15.0
44.44	5e-74	221
66.67	2.3	15.0
43.54	9e-81	239
50.00	2.1	13.9
43.75	0.63	16.2
26.92	2.7	14.2
41.67	5.5	13.1
66.67	6.5	13.1
27.97	2e-11	48.5
28.30	0.083	18.9
44.44	1.0	15.4
32.59	3e-29	99.0
45.62	1e-68	203
24.62	1.2	15.4
52.05	7e-80	231
26.47	4.4	13.5
45.62	1e-68	203
24.62	1.2	15.4
52.04	8e-79	228
60.00	0.30	17.7
50.00	0.61	16.5
26.32	7.0	13.1
38.46	8.3	13.1
50.00	0.66	16.2
33.33	1.4	15.0
50.00	0.69	16.2
33.33	1.6	15.0
50.00	0.18	17.7
83.33	0.75	15.8
26.32	8.1	12.7
30.00	0.74	16.2
40.00	1.2	15.8
62.50	1.5	15.4
33.33	7.3	13.1
29.41	0.98	15.4
37.50	3.1	13.9
58.33	2.8	14.6
50.00	4.1	13.9
66.67	0.007	21.2
40.91	0.013	20.4
62.50	6.1	12.3
66.67	4.0	13.9
50.00	1.4	16.5
33.33	3.3	15.4
62.50	5.4	14.6
43.17	4e-122	354
30.00	0.99	17.7
40.00	2.3	15.8
57.14	5.0	15.0
27.00	9.2	13.9
31.58	0.16	19.6
42.86	0.20	19.2
30.00	0.37	18.5
47.06	2.9	15.4
25.00	5.4	14.6
28.33	0.95	16.9
26.47	2.2	15.8
26.09	8.5	13.9
50.00	1.4	16.2
80.00	5.3	14.2
60.00	0.56	17.3
37.50	0.19	18.5
33.33	5.9	13.9
23.08	6.1	13.9
37.50	0.19	18.5
33.33	5.9	13.9
23.08	6.2	13.9
37.50	0.20	18.5
33.33	5.9	13.9
23.08	6.3	13.9
27.66	1.1	16.5
46.67	0.26	18.1
20.93	0.58	16.9
37.50	0.20	18.5
33.33	5.4	13.9
23.08	5.9	13.9
60.19	3e-145	405
37.50	0.19	18.5
36.84	5.9	13.9
23.08	6.3	13.9
37.50	0.20	18.5
33.33	6.0	13.9
23.08	6.1	13.9
37.50	0.20	18.5
36.84	5.9	13.9
23.08	6.2	13.9
31.25	3.5	11.9
98.15	8e-37	108
98.15	3e-36	107
55.56	5.7	10.8
100.00	1e-33	102
98.00	6e-33	100
98.00	6e-33	100
98.00	7e-33	100
100.00	2e-71	206
62.50	3.8	15.8
57.14	4.5	15.4
50.31	2e-157	448
29.17	1.3	16.9
35.29	3.5	15.4
46.67	8.7	14.2
98.16	0.0	985
99.80	0.0	1010
62.50	1.9	15.4
38.46	3.2	14.6
22.86	0.39	17.3
34.78	0.52	16.9
22.86	0.42	17.3
34.78	0.50	16.9
45.00	0.089	19.6
50.00	0.35	17.7
33.33	3.3	14.6
80.00	3.3	14.6
36.36	6.4	13.5
28.12	6.5	13.5
28.26	0.67	16.2
30.00	1.5	15.0
26.92	2.5	14.6
28.36	3.1	14.2
33.33	8.4	12.7
45.45	10.0	12.7
22.22	1.0	16.2
29.41	0.29	18.1
50.00	0.69	16.9
38.46	4.1	14.6
83.33	0.96	17.3
50.00	5.2	14.6
30.43	5.9	14.6
50.00	0.41	18.1
55.56	3.9	15.0
26.67	4.6	14.6
43.48	4.8	14.6
37.14	6.7	14.2
50.00	0.79	17.3
71.43	4.6	15.0
33.33	4.7	15.0
57.14	4.0	15.4
45.45	9.4	14.2
41.67	9.9	14.2
22.58	0.003	25.4
38.10	0.30	18.9
36.00	4.2	15.4
22.58	0.003	25.4
30.77	0.29	18.9
36.00	4.1	15.4
30.23	4.3	15.4
71.43	4.6	15.0
33.33	4.7	15.0
42.11	0.25	18.9
30.43	0.51	17.7
36.84	1.5	16.2
50.00	4.8	14.6
42.11	0.25	18.9
30.43	0.48	17.7
36.84	1.5	16.2
50.00	4.8	14.6
45.45	3.9	14.2
21.43	4.6	14.2
55.56	4.8	14.2
38.46	5.3	13.9
27.27	9.3	13.1
21.43	7.0	13.5
29.41	0.025	21.9
41.67	1.4	16.2
66.67	2.2	15.8
34.62	4.6	14.6
21.43	6.9	13.5
45.45	4.4	14.2
21.43	6.8	13.5
45.45	4.3	14.2
21.43	7.0	13.5
23.58	0.38	18.1
36.84	1.4	16.2
39.39	3.1	15.0
46.67	6.0	14.2
83.33	6.9	13.9
66.67	8.8	13.5
23.58	0.43	17.7
36.84	1.3	16.2
39.39	1.8	15.8
46.67	5.9	14.2
83.33	6.8	13.9
66.67	8.6	13.5
39.13	0.51	16.9
29.41	1.1	15.8
62.50	1.1	15.8
31.82	1.6	15.4
30.43	5.1	13.9
75.10	2e-149	409
50.00	3.4	14.2
29.41	4e-31	105
50.00	0.25	17.7
25.68	0.61	16.5
50.00	3.2	14.2
75.49	5e-150	411
100.00	0.0	521
66.67	3.2	14.2
36.36	4.3	13.9
76.21	1e-146	402
33.33	4.5	13.5
76.61	4e-151	414
91.59	3e-150	408
90.65	1e-147	401
88.73	2e-146	398
91.12	1e-150	409
94.81	9e-155	419
89.25	9e-148	402
80.65	2e-130	358
90.65	5e-150	407
96.26	9e-159	429
86.85	2e-140	383
56.97	2e-64	186
99.38	8e-121	329
55.29	1e-62	181
56.63	7e-62	179
65.16	1e-72	207
56.63	2e-61	178
57.58	2e-64	186
98.75	1e-118	323
99.39	3e-122	332
57.58	1e-64	186
97.64	0.0	593
98.01	0.0	611
68.81	1e-158	437
99.67	0.0	619
99.34	0.0	617
99.34	0.0	618
99.34	0.0	617
99.34	0.0	618
99.34	0.0	616
99.34	0.0	617
97.64	0.0	593
98.01	0.0	611
68.81	1e-158	437
99.67	0.0	619
99.34	0.0	617
99.34	0.0	617
99.34	0.0	618
99.34	0.0	618
99.34	0.0	616
99.34	0.0	617
58.54	1e-138	389
97.85	0.0	659
58.54	4e-139	390
99.39	0.0	679
99.39	0.0	680
99.09	0.0	677
99.70	0.0	684
99.09	0.0	677
99.09	0.0	677
56.88	6e-134	376
98.78	2e-60	169
97.56	9e-59	164
97.56	6e-59	164
97.56	4e-59	165
97.56	1e-59	166
96.34	1e-58	164
93.90	4e-57	160
97.56	1e-58	164
97.56	3e-59	165
97.56	1e-59	166
44.44	2.2	13.9
66.67	6.3	12.7
99.46	3e-139	379
50.00	2.7	13.9
43.75	1.5	14.6
66.67	1.5	14.6
35.71	4.9	13.1
33.33	5.8	12.7
28.57	7.1	12.7
34.15	3.1	13.5
34.15	3.1	13.5
100.00	0.34	16.5
57.14	2.4	13.9
40.00	6.3	12.7
75.00	0.13	18.1
31.82	4.4	13.1
66.67	7.1	12.7
45.45	9.0	12.3
45.45	2.8	13.5
22.22	6.0	12.3
31.25	0.054	20.0
83.33	1.5	15.4
36.84	4.6	13.1
46.67	0.083	19.2
37.50	0.10	18.9
50.00	0.46	16.9
37.50	0.47	16.9
80.00	1.5	15.4
35.71	4.2	13.9
45.45	2.2	14.6
40.00	3.7	15.4
29.51	0.094	20.8
24.42	0.26	19.2
50.00	0.71	18.1
25.00	0.88	17.7
83.33	8.0	14.6
39.29	0.16	19.2
41.67	3.6	15.0
39.13	3.8	15.0
30.30	6.9	14.2
29.09	0.10	20.4
30.77	1.9	16.2
37.50	2.5	15.8
29.41	3.8	15.0
100.00	5.6	14.6
43.58	5e-116	338
25.00	0.52	17.7
33.33	0.72	17.3
35.71	3.2	15.4
24.07	0.87	16.9
35.71	2.8	15.4
29.82	3.3	15.4
43.73	2e-111	327
42.86	4.5	15.0
29.41	1.0	17.7
36.00	1.1	17.7
26.67	2.9	16.2
41.18	4.6	15.4
28.12	5.3	15.4
25.00	0.57	16.9
39.13	1.1	16.2
50.00	0.54	16.9
50.00	5.5	13.9
33.33	0.020	21.6
27.59	0.23	18.1
28.57	2.1	15.0
27.59	0.26	17.7
28.57	2.2	15.0
29.73	0.41	17.3
42.86	0.58	16.9
29.17	4.5	14.2
26.92	5.9	13.9
50.00	6.4	13.5
45.45	6.5	13.5
95.17	0.0	573
95.83	4e-05	30.0
27.91	1.6	15.8
46.15	8.0	13.5
95.44	0.0	585
62.50	2.0	14.6
38.46	1.9	14.2
33.33	8.0	12.3
30.53	5e-27	93.6
30.00	3e-28	97.4
40.50	1e-53	165
98.39	3e-178	483
40.50	1e-53	165
98.47	0.0	515
99.24	0.0	522
34.21	0.22	18.5
15.38	0.30	18.1
42.86	0.60	16.9
37.50	1.9	15.4
44.44	3.5	14.6
44.44	4.7	14.2
25.00	2.3	15.0
24.29	0.32	17.3
40.91	0.94	15.8
25.00	2.4	14.6
66.67	3.0	14.2
50.00	4.2	13.9
31.25	0.27	17.7
54.55	1.3	15.4
50.00	7.5	13.1
42.11	1.0	15.4
35.71	7.6	12.7
60.87	3e-122	341
33.85	6e-46	145
42.86	0.63	15.8
50.00	5.8	12.7
31.25	0.25	17.7
54.55	1.3	15.4
41.67	6.6	13.1
98.88	0.0	549
35.71	0.54	16.2
32.96	4e-47	149
33.33	1.1	18.1
35.00	9.9	14.6
33.33	1.1	17.7
35.00	9.8	14.6
50.00	1.2	18.1
53.85	1.6	17.7
22.86	2.1	17.3
33.33	5.8	15.8
26.67	6.5	15.8
45.45	8.6	15.4
50.00	1.2	18.1
53.85	1.6	17.7
22.86	2.2	17.3
33.33	5.8	15.8
25.00	5.8	15.8
26.67	6.7	15.8
22.41	9.0	15.4
43.75	1.8	17.7
21.88	5.1	16.2
20.63	6.2	15.8
28.26	6.5	15.8
33.33	1.1	17.7
37.04	0.63	18.9
28.57	2.2	16.9
23.33	3.7	16.2
29.41	4.1	16.2
63.64	0.25	20.4
20.69	7.0	15.8
32.35	0.33	19.6
41.67	1.5	17.3
30.30	1.7	17.3
27.03	3.9	16.2
50.00	4.9	15.8
33.33	5.8	15.8
21.74	9.0	15.0
30.16	0.66	18.1
50.00	1.0	17.3
55.56	2.1	16.5
71.43	2.6	16.5
34.62	1.5	16.2
69.23	1.6	16.2
52.94	2.2	15.8
55.56	2.5	15.4
29.17	3.8	15.0
54.55	3.8	16.2
71.43	8.4	15.0
40.00	1.4	16.9
18.52	5.5	15.0
38.10	0.79	18.1
36.36	2.8	16.5
85.71	5.3	15.4
37.50	0.88	17.3
29.03	1.4	16.5
99.80	0.0	1004
75.00	0.85	16.5
33.33	0.91	16.2
26.76	1.1	16.2
40.00	5.3	13.9
66.67	8.5	13.1
34.78	0.42	17.7
42.11	1.1	16.5
42.86	1.8	16.5
50.00	3.1	15.4
28.57	3.6	15.4
30.00	4.3	15.0
32.26	5.1	15.0
50.00	0.23	18.5
26.32	2.1	15.4
30.43	2.8	15.0
28.57	5.4	13.9
83.33	6.2	13.9
47.37	0.69	17.3
75.00	2.3	15.4
36.36	4.9	14.2
35.29	1.00	16.5
66.67	3.4	15.0
41.67	4.1	14.6
23.53	6.4	14.2
100.00	0.0	709
60.00	0.70	16.9
34.38	3.6	14.6
60.00	0.74	16.9
34.38	2.4	15.0
22.73	1.1	16.5
62.50	6.9	14.2
32.00	3.0	14.6
27.27	7.0	13.5
36.36	8.5	13.1
26.32	2.0	15.0
55.56	8.7	13.1
24.74	4.5	14.6
66.67	8.6	13.5
36.36	0.81	17.3
33.33	6.5	14.2
26.67	0.37	18.1
100.00	0.41	18.1
50.00	1.7	15.8
66.67	4.9	14.6
50.00	0.36	18.5
37.50	6.7	15.0
29.73	0.25	19.2
83.33	3.7	15.4
45.45	6.0	14.6
37.50	1.0	17.3
37.50	1.1	17.3
58.28	0.0	550
29.63	1.1	16.9
32.00	3.0	15.8
33.33	3.1	15.8
21.88	4.5	15.0
66.67	0.70	17.7
32.14	0.35	18.5
45.45	1.3	16.5
99.80	0.0	1056
99.60	0.0	1053
28.07	0.005	21.9
71.43	4.6	12.7
32.00	6.8	13.5
32.00	6.7	13.5
25.71	3.3	13.9
34.62	1.2	15.4
55.56	1.4	15.4
36.84	2.5	14.6
24.44	0.97	15.4
50.00	1.1	15.4
45.45	5.1	13.1
98.79	0.0	503
38.10	1.4	15.0
30.77	1.2	18.5
56.25	3.0	17.3
28.00	5.5	16.2
26.32	1.7	17.7
38.89	3.1	16.9
71.43	4.4	16.5
55.56	6.4	15.8
32.26	0.23	20.4
36.84	1.3	18.1
38.89	1.8	17.7
50.00	5.7	15.8
71.43	6.1	15.8
27.78	8.2	15.4
22.58	9.8	15.0
31.25	2.8	16.9
85.71	5.1	16.2
27.78	6.0	15.8
27.03	8.6	15.4
63.64	1.5	18.5
39.13	2.7	17.7
31.58	8.2	16.2
66.67	3.0	17.3
36.36	0.093	21.9
54.55	3.0	17.7
53.85	5.6	16.5
33.33	6.3	16.5
31.25	0.38	20.0
36.00	0.99	18.9
23.88	5.6	16.2
42.96	2e-67	203
22.58	1.2	16.2
31.71	1.8	15.4
33.33	7.3	13.5
99.34	0.0	630
99.01	0.0	627
99.34	0.0	629
99.34	0.0	629
99.34	0.0	629
99.67	0.0	631
57.14	3.1	14.6
33.33	4.1	14.2
31.25	9.7	13.1
26.87	0.50	14.6
46.67	0.80	13.9
54.55	0.053	17.7
29.63	1.5	13.5
28.33	0.48	14.6
96.47	3e-61	172
77.38	3e-41	121
56.10	1e-25	79.7
76.19	1e-47	136
76.19	2e-47	136
75.00	1e-39	116
100.00	2e-63	176
77.38	2e-41	120
77.38	1e-41	121
77.38	3e-41	120
73.72	1e-167	462
26.97	0.034	20.8
62.50	0.84	16.5
45.45	4.1	14.2
62.50	6.0	13.9
30.56	0.20	18.9
45.45	2.3	15.4
30.56	5.0	14.2
35.71	9.0	13.5
62.50	5.4	13.1
25.93	0.55	16.9
36.84	6.1	13.5
25.00	6.1	13.5
24.53	0.094	19.2
100.00	8.1	13.1
24.53	0.13	18.9
71.43	0.28	17.7
100.00	8.0	13.1
24.53	0.088	19.6
100.00	8.1	13.1
37.84	0.29	18.5
57.14	6.2	14.2
40.00	6.5	14.2
36.73	0.11	19.6
36.00	0.91	16.9
35.71	2.1	15.8
33.33	3.6	15.0
29.27	6.0	14.2
57.14	0.92	17.3
71.43	6.0	15.0
99.57	0.0	940
36.84	0.78	17.7
46.43	0.10	20.8
75.00	3.5	15.8
43.75	1.5	16.9
34.62	2.3	16.2
71.43	3.1	15.8
71.43	4.5	15.4
35.29	0.81	17.7
71.43	2.1	16.5
34.62	2.2	16.5
41.67	5.8	15.0
41.18	9.5	14.2
38.10	0.14	20.0
42.55	0.24	19.2
20.69	5.5	15.0
36.00	1.0	17.3
35.71	2.6	16.2
29.73	3.1	15.8
50.00	4.3	15.4
99.14	0.0	932
44.44	0.018	18.9
50.00	0.13	16.5
31.48	1.5	13.1
62.50	5.0	11.9
28.57	5.5	11.5
60.00	0.008	19.6
40.74	0.056	16.9
42.11	0.86	13.1
57.14	0.037	17.7
83.33	0.75	13.9
75.00	4.6	11.2
75.00	4.9	11.2
40.00	7.1	11.2
40.00	7.1	11.2
35.00	0.13	16.5
35.29	0.50	15.0
27.78	3.6	12.3
35.29	4.5	11.9
57.14	5.6	11.5
35.00	0.12	16.5
35.29	0.47	15.0
27.78	3.0	12.3
35.29	4.1	11.9
57.14	5.5	11.5
40.00	0.30	14.6
99.30	6e-105	287
60.58	2e-61	176
33.33	0.64	14.6
98.60	9e-105	286
98.60	1e-104	286
68.81	3e-98	276
87.78	3e-138	378
87.33	2e-137	376
86.16	2e-134	369
81.94	2e-124	343
87.61	2e-135	370
86.55	1e-135	372
83.94	2e-130	358
83.49	2e-129	356
86.57	5e-132	362
99.38	1e-120	328
65.62	2e-75	213
99.38	2e-120	328
93.21	3e-114	312
25.00	8.0	11.2
81.88	1e-97	270
98.77	5e-120	327
95.06	5e-117	319
99.55	0.0	888
99.77	0.0	899
99.77	0.0	890
99.77	0.0	890
99.77	0.0	893
99.55	0.0	899
99.77	0.0	897
100.00	0.0	899
99.53	0.0	864
38.46	9.9	14.2
100.00	0.0	899
43.75	1.1	15.4
38.46	5.1	13.1
35.00	1.0	15.8
44.44	3.9	13.9
38.89	0.90	15.4
27.27	2.2	14.2
66.67	1.9	14.6
43.75	2.0	15.0
31.25	0.11	18.9
58.33	0.37	17.3
36.36	4.1	13.9
98.65	3e-112	306
31.82	0.32	15.8
98.65	2e-112	306
97.97	7e-112	305
97.97	7e-112	305
97.97	7e-112	305
99.32	2e-113	309
97.97	5e-112	305
97.28	2e-110	301
98.63	5e-111	303
98.64	2e-111	304
51.97	2e-44	131
52.76	7e-44	130
50.39	2e-43	129
52.76	3e-42	125
62.20	1e-53	155
51.59	2e-45	134
60.63	6e-50	146
100.00	2e-93	261
50.79	9e-42	124
45.67	1e-32	100
26.92	1.6	15.4
83.33	6.2	13.5
24.00	6.3	13.5
20.00	6.7	13.5
42.86	6.1	13.5
29.17	7.0	13.1
50.00	7.8	13.1
30.77	0.32	18.5
44.44	0.62	17.3
57.89	0.88	16.9
54.55	3.6	15.0
36.00	5.1	14.6
25.58	2.6	15.0
30.00	3.7	14.6
38.46	3.7	14.6
66.67	5.2	13.9
50.00	0.62	17.3
40.00	0.77	16.9
26.32	5.6	14.2
37.50	0.24	18.5
46.67	0.84	16.5
29.03	1.4	15.8
33.33	3.8	14.6
62.50	8.2	13.5
41.18	1.2	15.8
45.45	4.4	13.9
31.82	5.8	14.2
43.75	2.5	15.0
56.89	4e-142	398
35.29	0.67	17.3
100.00	2.3	15.8
33.33	0.55	17.3
38.89	1.2	16.5
40.00	4.7	15.0
29.03	0.53	17.7
35.29	2.0	15.8
30.77	6.6	14.2
45.45	4.1	14.6
37.04	0.66	16.9
62.50	1.1	16.2
22.45	6.2	13.9
30.43	6.3	13.9
29.41	7.6	13.9
34.09	1.1	16.5
50.00	2.5	15.4
43.75	0.23	18.5
40.00	0.72	16.9
34.78	0.37	15.0
45.45	0.46	14.6
31.25	0.57	14.6
50.00	1.1	13.9
50.00	4.1	11.9
57.14	0.70	14.6
29.27	4.9	11.9
57.14	0.82	14.6
83.33	2.1	13.5
25.00	4.6	12.7
40.00	5.8	12.3
36.84	8.3	11.9
33.33	0.28	16.2
36.36	1.8	13.9
40.00	1.9	13.9
31.82	3.7	12.7
34.29	0.11	17.3
85.21	6e-107	294
40.00	0.58	16.9
44.44	2.1	15.0
28.95	0.054	20.4
31.58	5.7	13.9
36.36	7.2	13.9
40.91	0.54	16.9
30.77	1.2	16.2
66.67	9.7	13.1
33.33	6.4	13.1
46.15	3.5	14.6
36.84	3.9	14.6
37.50	6.4	13.9
26.19	7.5	13.9
60.00	0.75	16.2
70.00	3.9	14.2
27.27	1.5	15.8
50.00	9.0	13.5
60.00	1.4	15.8
58.33	3.4	14.6
31.25	4.0	14.2
42.86	1.3	16.2
55.56	1.9	15.8
33.33	2.5	15.4
50.00	7.8	13.9
42.86	1.2	16.2
55.56	2.1	15.4
33.33	2.4	15.4
50.00	7.9	13.9
42.86	1.2	16.2
55.56	1.8	15.8
33.33	2.5	15.4
50.00	7.8	13.9
42.86	1.3	16.2
55.56	1.9	15.8
33.33	2.5	15.4
50.00	7.9	13.9
42.86	1.2	16.2
55.56	1.9	15.8
33.33	2.4	15.4
50.00	8.0	13.9
42.86	1.3	16.2
55.56	1.9	15.8
33.33	2.5	15.4
50.00	8.0	13.9
42.86	1.2	16.2
55.56	1.9	15.8
50.00	7.9	13.9
42.86	1.3	16.2
55.56	2.1	15.8
33.33	2.5	15.4
50.00	8.8	13.5
42.86	1.3	16.2
55.56	1.9	15.8
33.33	2.5	15.4
50.00	8.1	13.9
99.70	0.0	680
99.71	0.0	700
42.86	1.2	16.2
38.10	3.4	14.6
22.00	4.7	14.2
50.00	0.43	17.7
38.10	3.4	14.6
22.00	5.0	14.2
44.44	0.51	17.7
33.33	4.3	14.6
44.44	0.62	17.3
33.33	3.9	14.6
52.49	1e-132	375
23.72	8e-13	54.7
50.00	0.46	17.3
38.10	3.5	14.6
100.00	0.0	701
50.00	0.44	17.3
38.10	3.4	14.6
22.00	4.7	14.2
98.58	2e-102	280
98.58	5e-103	281
98.58	7e-103	281
99.29	2e-103	283
98.58	4e-103	282
97.87	1e-101	278
97.87	6e-102	279
98.58	5e-102	279
98.58	6e-102	279
100.00	6e-102	279
39.47	5e-04	28.1
38.10	5.0	15.4
57.14	5.2	15.0
40.00	6.4	15.0
44.44	8.8	14.6
40.00	0.15	20.4
66.67	6.2	15.0
32.14	7.9	14.6
27.44	1e-34	122
31.25	2.9	16.2
28.57	6.1	14.6
35.87	8e-90	276
37.82	2e-84	261
62.50	1.3	17.3
45.45	5.3	15.4
27.78	6.6	15.0
46.15	8.9	14.6
52.63	0.005	24.6
34.62	0.83	17.3
46.15	1.4	16.9
55.56	5.8	14.6
35.71	7.0	14.6
62.50	3.3	15.8
23.81	4.6	15.0
43.07	5e-132	385
63.77	0.0	542
62.72	0.0	509
62.72	0.0	510
62.72	0.0	508
59.42	2e-171	479
65.46	0.0	553
54.09	1e-160	452
61.50	0.0	516
57.95	1e-168	472
29.17	2.6	15.8
99.76	0.0	866
35.71	0.31	18.1
25.64	0.38	17.7
36.67	0.50	17.3
42.86	6.1	13.9
41.67	2.2	15.4
21.74	2.2	15.0
41.67	0.81	16.5
44.44	2.4	15.0
35.29	3.6	14.2
41.67	0.82	16.5
44.44	2.4	15.0
35.29	3.7	14.2
27.27	0.20	18.9
100.00	4.5	13.9
29.27	0.007	23.1
30.88	2.4	15.0
23.08	0.95	16.5
41.67	2.2	15.4
96.67	2e-63	176
95.56	6e-63	176
95.60	2e-63	177
96.70	5e-64	178
68.18	6e-42	122
57.78	4e-36	107
45.56	6e-25	78.6
21.05	3.3	11.2
44.44	0.46	14.2
30.56	0.56	13.9
40.00	1.6	12.7
30.77	4.8	11.2
36.67	1.5	16.2
54.55	3.1	15.0
25.00	4.5	14.2
31.91	0.47	17.7
54.55	3.1	15.0
21.05	4.3	14.6
25.00	6.0	14.2
55.56	8.4	13.5
28.57	0.27	17.7
30.77	2.0	15.0
38.46	1.6	15.8
52.92	4e-125	354
58.71	7e-133	374
58.58	5e-132	372
52.92	2e-125	355
59.00	2e-132	372
70.00	0.10	19.6
41.86	4e-91	267
70.00	0.12	19.2
75.00	3e-63	181
71.43	5e-65	186
70.55	3e-64	184
75.81	2e-63	181
73.17	1e-59	171
76.38	3e-66	188
76.56	8e-65	185
73.97	7e-73	206
75.81	5e-64	182
77.08	4e-71	201
95.68	6e-118	322
63.12	4e-72	205
94.44	7e-116	316
93.21	3e-114	312
79.38	2e-95	265
96.30	1e-118	323
71.43	2.1	13.5
95.06	5e-117	319
33.33	3.4	11.9
100.00	0.44	15.0
48.03	3e-38	116
48.82	3e-40	122
100.00	0.42	15.0
96.64	3e-111	303
97.32	7e-112	305
96.64	2e-111	304
97.99	5e-112	305
97.32	1e-111	304
97.32	7e-112	305
98.66	3e-112	306
96.64	2e-111	304
97.99	9e-112	305
97.32	8e-111	302

From ulrik.stervbo at gmail.com  Thu Jan 21 20:29:42 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Thu, 21 Jan 2016 19:29:42 +0000
Subject: [R] ggplot2 - specific y axis scale for each time series in
	facet grind
In-Reply-To: <CALB30JAsRHmmmu8fKNi99K7WkPi6bZCFZ-G-_e8vy_-rQ5M_nQ@mail.gmail.com>
References: <CALB30JAsRHmmmu8fKNi99K7WkPi6bZCFZ-G-_e8vy_-rQ5M_nQ@mail.gmail.com>
Message-ID: <CAKVAULPSErU-722vt_nBqQNEdZfDAFAQtgxn3STFdstkHmZ_=g@mail.gmail.com>

You can try facet_grid(variable~., scales = "free_y")

On Thu, 21 Jan 2016 at 19:29 Maxim Fomin <mxfomin at gmail.com> wrote:

> Dear R users,
>
> I have a (melted) data frame of several metal prices from 1980. I want to
> make a time series plot for each metal price. By default ggplot constructs
> single plot for all prices. Adding +facet_grid(variable~.) makes for each
> metal price a plot, but now there is another problem - scale of y axis.
> Some metals have price $1000 and some $2. Because ggplot makes same y scale
> for each metal, some metal prices printed as straight horizontal line. What
> is the command to make for each plot specific y axis scale?
>
> Currently I am trying command:
>
> ggplot(data=melted2, aes(x=date, y=value, group=variable,
> color=variable))+geom_line()+facet_grid(variable~.)
>
>
> Best regards,
> Maxim
>
> P.S dput
>
> structure(list(date = structure(c(3652, 3683, 3712, 3743, 3773,
> 3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
> 4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
> 4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
> 4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
> 5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
> 5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
> 5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
> 6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
> 6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
> 6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
> 7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
> 7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
> 7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
> 8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
> 8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
> 8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
> 9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
> 9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
> 9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
> 10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
> 10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
> 10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
> 10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
> 11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
> 11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
> 11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
> 12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
> 12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
> 12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
> 12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
> 13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
> 13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
> 13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
> 13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
> 14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
> 14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
> 14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
> 15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
> 15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
> 15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
> 15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
> 16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
> 16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
> 16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
> 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
> 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
> 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
> 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
> 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
> 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
> 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
> 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
> 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
> 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
> 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
> 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
> 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
> 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
> 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
> 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
> 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
> 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
> 9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
> 10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
> 10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
> 10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
> 11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
> 11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
> 11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
> 11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
> 12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
> 12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
> 12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
> 12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
> 13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
> 13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
> 13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
> 14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
> 14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
> 14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
> 14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
> 15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
> 15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
> 15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
> 15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
> 16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
> 16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740, 3652,
> 3683, 3712, 3743, 3773, 3804, 3834, 3865, 3896, 3926, 3957, 3987,
> 4018, 4049, 4077, 4108, 4138, 4169, 4199, 4230, 4261, 4291, 4322,
> 4352, 4383, 4414, 4442, 4473, 4503, 4534, 4564, 4595, 4626, 4656,
> 4687, 4717, 4748, 4779, 4807, 4838, 4868, 4899, 4929, 4960, 4991,
> 5021, 5052, 5082, 5113, 5144, 5173, 5204, 5234, 5265, 5295, 5326,
> 5357, 5387, 5418, 5448, 5479, 5510, 5538, 5569, 5599, 5630, 5660,
> 5691, 5722, 5752, 5783, 5813, 5844, 5875, 5903, 5934, 5964, 5995,
> 6025, 6056, 6087, 6117, 6148, 6178, 6209, 6240, 6268, 6299, 6329,
> 6360, 6390, 6421, 6452, 6482, 6513, 6543, 6574, 6605, 6634, 6665,
> 6695, 6726, 6756, 6787, 6818, 6848, 6879, 6909, 6940, 6971, 6999,
> 7030, 7060, 7091, 7121, 7152, 7183, 7213, 7244, 7274, 7305, 7336,
> 7364, 7395, 7425, 7456, 7486, 7517, 7548, 7578, 7609, 7639, 7670,
> 7701, 7729, 7760, 7790, 7821, 7851, 7882, 7913, 7943, 7974, 8004,
> 8035, 8066, 8095, 8126, 8156, 8187, 8217, 8248, 8279, 8309, 8340,
> 8370, 8401, 8432, 8460, 8491, 8521, 8552, 8582, 8613, 8644, 8674,
> 8705, 8735, 8766, 8797, 8825, 8856, 8886, 8917, 8947, 8978, 9009,
> 9039, 9070, 9100, 9131, 9162, 9190, 9221, 9251, 9282, 9312, 9343,
> 9374, 9404, 9435, 9465, 9496, 9527, 9556, 9587, 9617, 9648, 9678,
> 9709, 9740, 9770, 9801, 9831, 9862, 9893, 9921, 9952, 9982, 10013,
> 10043, 10074, 10105, 10135, 10166, 10196, 10227, 10258, 10286,
> 10317, 10347, 10378, 10408, 10439, 10470, 10500, 10531, 10561,
> 10592, 10623, 10651, 10682, 10712, 10743, 10773, 10804, 10835,
> 10865, 10896, 10926, 10957, 10988, 11017, 11048, 11078, 11109,
> 11139, 11170, 11201, 11231, 11262, 11292, 11323, 11354, 11382,
> 11413, 11443, 11474, 11504, 11535, 11566, 11596, 11627, 11657,
> 11688, 11719, 11747, 11778, 11808, 11839, 11869, 11900, 11931,
> 11961, 11992, 12022, 12053, 12084, 12112, 12143, 12173, 12204,
> 12234, 12265, 12296, 12326, 12357, 12387, 12418, 12449, 12478,
> 12509, 12539, 12570, 12600, 12631, 12662, 12692, 12723, 12753,
> 12784, 12815, 12843, 12874, 12904, 12935, 12965, 12996, 13027,
> 13057, 13088, 13118, 13149, 13180, 13208, 13239, 13269, 13300,
> 13330, 13361, 13392, 13422, 13453, 13483, 13514, 13545, 13573,
> 13604, 13634, 13665, 13695, 13726, 13757, 13787, 13818, 13848,
> 13879, 13910, 13939, 13970, 14000, 14031, 14061, 14092, 14123,
> 14153, 14184, 14214, 14245, 14276, 14304, 14335, 14365, 14396,
> 14426, 14457, 14488, 14518, 14549, 14579, 14610, 14641, 14669,
> 14700, 14730, 14761, 14791, 14822, 14853, 14883, 14914, 14944,
> 14975, 15006, 15034, 15065, 15095, 15126, 15156, 15187, 15218,
> 15248, 15279, 15309, 15340, 15371, 15400, 15431, 15461, 15492,
> 15522, 15553, 15584, 15614, 15645, 15675, 15706, 15737, 15765,
> 15796, 15826, 15857, 15887, 15918, 15949, 15979, 16010, 16040,
> 16071, 16102, 16130, 16161, 16191, 16222, 16252, 16283, 16314,
> 16344, 16375, 16405, 16436, 16467, 16495, 16526, 16556, 16587,
> 16617, 16648, 16679, 16709, 16740, 3652, 3683, 3712, 3743, 3773,
> 3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
> 4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
> 4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
> 4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
> 5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
> 5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
> 5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
> 6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
> 6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
> 6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
> 7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
> 7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
> 7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
> 8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
> 8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
> 8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
> 9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
> 9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
> 9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
> 10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
> 10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
> 10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
> 10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
> 11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
> 11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
> 11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
> 12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
> 12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
> 12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
> 12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
> 13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
> 13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
> 13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
> 13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
> 14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
> 14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
> 14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
> 15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
> 15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
> 15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
> 15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
> 16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
> 16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
> 16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
> 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
> 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
> 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
> 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
> 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
> 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
> 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
> 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
> 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
> 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
> 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
> 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
> 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
> 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
> 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
> 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
> 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
> 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
> 9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
> 10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
> 10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
> 10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
> 11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
> 11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
> 11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
> 11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
> 12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
> 12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
> 12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
> 12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
> 13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
> 13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
> 13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
> 14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
> 14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
> 14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
> 14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
> 15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
> 15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
> 15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
> 15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
> 16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
> 16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740, 3652,
> 3683, 3712, 3743, 3773, 3804, 3834, 3865, 3896, 3926, 3957, 3987,
> 4018, 4049, 4077, 4108, 4138, 4169, 4199, 4230, 4261, 4291, 4322,
> 4352, 4383, 4414, 4442, 4473, 4503, 4534, 4564, 4595, 4626, 4656,
> 4687, 4717, 4748, 4779, 4807, 4838, 4868, 4899, 4929, 4960, 4991,
> 5021, 5052, 5082, 5113, 5144, 5173, 5204, 5234, 5265, 5295, 5326,
> 5357, 5387, 5418, 5448, 5479, 5510, 5538, 5569, 5599, 5630, 5660,
> 5691, 5722, 5752, 5783, 5813, 5844, 5875, 5903, 5934, 5964, 5995,
> 6025, 6056, 6087, 6117, 6148, 6178, 6209, 6240, 6268, 6299, 6329,
> 6360, 6390, 6421, 6452, 6482, 6513, 6543, 6574, 6605, 6634, 6665,
> 6695, 6726, 6756, 6787, 6818, 6848, 6879, 6909, 6940, 6971, 6999,
> 7030, 7060, 7091, 7121, 7152, 7183, 7213, 7244, 7274, 7305, 7336,
> 7364, 7395, 7425, 7456, 7486, 7517, 7548, 7578, 7609, 7639, 7670,
> 7701, 7729, 7760, 7790, 7821, 7851, 7882, 7913, 7943, 7974, 8004,
> 8035, 8066, 8095, 8126, 8156, 8187, 8217, 8248, 8279, 8309, 8340,
> 8370, 8401, 8432, 8460, 8491, 8521, 8552, 8582, 8613, 8644, 8674,
> 8705, 8735, 8766, 8797, 8825, 8856, 8886, 8917, 8947, 8978, 9009,
> 9039, 9070, 9100, 9131, 9162, 9190, 9221, 9251, 9282, 9312, 9343,
> 9374, 9404, 9435, 9465, 9496, 9527, 9556, 9587, 9617, 9648, 9678,
> 9709, 9740, 9770, 9801, 9831, 9862, 9893, 9921, 9952, 9982, 10013,
> 10043, 10074, 10105, 10135, 10166, 10196, 10227, 10258, 10286,
> 10317, 10347, 10378, 10408, 10439, 10470, 10500, 10531, 10561,
> 10592, 10623, 10651, 10682, 10712, 10743, 10773, 10804, 10835,
> 10865, 10896, 10926, 10957, 10988, 11017, 11048, 11078, 11109,
> 11139, 11170, 11201, 11231, 11262, 11292, 11323, 11354, 11382,
> 11413, 11443, 11474, 11504, 11535, 11566, 11596, 11627, 11657,
> 11688, 11719, 11747, 11778, 11808, 11839, 11869, 11900, 11931,
> 11961, 11992, 12022, 12053, 12084, 12112, 12143, 12173, 12204,
> 12234, 12265, 12296, 12326, 12357, 12387, 12418, 12449, 12478,
> 12509, 12539, 12570, 12600, 12631, 12662, 12692, 12723, 12753,
> 12784, 12815, 12843, 12874, 12904, 12935, 12965, 12996, 13027,
> 13057, 13088, 13118, 13149, 13180, 13208, 13239, 13269, 13300,
> 13330, 13361, 13392, 13422, 13453, 13483, 13514, 13545, 13573,
> 13604, 13634, 13665, 13695, 13726, 13757, 13787, 13818, 13848,
> 13879, 13910, 13939, 13970, 14000, 14031, 14061, 14092, 14123,
> 14153, 14184, 14214, 14245, 14276, 14304, 14335, 14365, 14396,
> 14426, 14457, 14488, 14518, 14549, 14579, 14610, 14641, 14669,
> 14700, 14730, 14761, 14791, 14822, 14853, 14883, 14914, 14944,
> 14975, 15006, 15034, 15065, 15095, 15126, 15156, 15187, 15218,
> 15248, 15279, 15309, 15340, 15371, 15400, 15431, 15461, 15492,
> 15522, 15553, 15584, 15614, 15645, 15675, 15706, 15737, 15765,
> 15796, 15826, 15857, 15887, 15918, 15949, 15979, 16010, 16040,
> 16071, 16102, 16130, 16161, 16191, 16222, 16252, 16283, 16314,
> 16344, 16375, 16405, 16436, 16467, 16495, 16526, 16556, 16587,
> 16617, 16648, 16679, 16709, 16740, 3652, 3683, 3712, 3743, 3773,
> 3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
> 4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
> 4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
> 4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
> 5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
> 5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
> 5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
> 6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
> 6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
> 6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
> 7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
> 7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
> 7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
> 8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
> 8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
> 8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
> 9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
> 9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
> 9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
> 10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
> 10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
> 10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
> 10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
> 11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
> 11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
> 11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
> 12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
> 12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
> 12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
> 12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
> 13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
> 13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
> 13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
> 13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
> 14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
> 14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
> 14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
> 15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
> 15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
> 15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
> 15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
> 16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
> 16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
> 16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
> 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
> 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
> 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
> 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
> 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
> 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
> 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
> 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
> 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
> 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
> 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
> 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
> 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
> 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
> 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
> 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
> 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
> 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
> 9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
> 10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
> 10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
> 10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
> 11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
> 11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
> 11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
> 11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
> 12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
> 12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
> 12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
> 12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
> 13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
> 13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
> 13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
> 14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
> 14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
> 14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
> 14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
> 15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
> 15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
> 15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
> 15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
> 16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
> 16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740), class = "Date"),
>     variable = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>     4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>     5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>     8L, 8L, 8L, 8L), .Label = c("aluminum", "copper", "gold",
>     "ironore", "nickel", "tin", "zinc", "index"), class = "factor"),
>     value = c(2054.86010742188, 2131.00854492188, 1978.37890625,
>     1932.45556640625, 1775.8037109375, 1668.96044921875, 1758.07495117188,
>     1783.62719726563, 1655.06567382813, 1626.15209960938, 1503.91015625,
>     1430.65673828125, 1430.82958984375, 1452.38110351563, 1442.95190429688,
>     1369.23461914063, 1296.99584960938, 1232.40673828125, 1175.31127929688,
>     1230.82885742188, 1168.9111328125, 1140.181640625, 1081.84643554688,
>     1130.88427734375, 1113.25854492188, 1087.70629882813, 1029.23657226563,
>     996.20751953125, 973.294189453125, 918.849365234375, 957.847900390625,
>     958.864013671875, 959.531982421875, 951.484130859375, 965.14697265625,
>     987.387451171875, 1076.53076171875, 1230.45581054688, 1301.72924804688,
>     1363.56713867188, 1453.43408203125, 1465.57836914063, 1520.2099609375,
>     1603.66748046875, 1613.72875976563, 1566.25805664063, 1516.72778320313,
>     1549.33911132813, 1548.90625, 1491.16162109375, 1455.76293945313,
>     1370.35034179688, 1289.13500976563, 1275.02905273438, 1162.80615234375,
>     1136.00903320313, 1012.169921875, 1027.64794921875, 1154.66040039063,
>     1095.69604492188, 1075.8544921875, 1097.90087890625, 1095.25512695313,
>     1108.923828125, 1104.5146484375, 1031.76220703125, 1011.92065429688,
>     1018.53442382813, 985.465087890625, 970.03271484375, 950.191162109375,
>     1038.3759765625, 1119.94702148438, 1115.53784179688, 1168.44848632813,
>     1164.03930664063, 1164.03930664063, 1183.880859375, 1122.15161132813,
>     1128.76538085938, 1205.92724609375, 1161.83471679688, 1130.96997070313,
>     1130.96997070313, 1170.6533203125, 1283.0888671875, 1369.06909179688,
>     1399.93359375, 1410.95678710938, 1472.68627929688, 1653.46508789063,
>     1809.99291992188, 1746.05908203125, 1962.11181640625, 1679.92041015625,
>     1823.220703125, 2001.794921875, 2138.4814453125, 2526.49462890625,
>     2508.85766601563, 2987.26000976563, 3578.09814453125, 2581.61010742188,
>     2700.65942382813, 2387.603515625, 2308.23706054688, 2380.98950195313,
>     2458.1513671875, 2398.62646484375, 2175.59692382813, 2074.54736328125,
>     2125.80004882813, 2259, 1914, 1756, 1798, 1718, 1820, 1736,
>     1633, 1528, 1454, 1567, 1526, 1527, 1566, 1571, 1782, 2067,
>     1946, 1618, 1522, 1515, 1505, 1496, 1392, 1299.94995117188,
>     1283.55004882813, 1297.54663085938, 1259.693359375, 1212.09521484375,
>     1152.95458984375, 1134.92504882813, 1099.5263671875, 1181.22729492188,
>     1270.75, 1282.02270507813, 1317.125, 1306.71069335938,
> 1275.27270507813,
>     1314.34790039063, 1305.55004882813, 1270.21435546875, 1180.84106445313,
>     1162.21435546875, 1209.02380371094, 1208.09997558594, 1203.15002441406,
>     1150.47827148438, 1110.27502441406, 1125.42102050781, 1168.09094238281,
>     1203.18176269531, 1172.76196289063, 1116.90905761719, 1089.42858886719,
>     1040.02270507813, 1091.34997558594, 1170, 1270.52502441406,
>     1287.69567871094, 1277.28942871094, 1321.04760742188, 1402.36364746094,
>     1491.95239257813, 1457.06823730469, 1571.09094238281, 1694.26196289063,
>     1885.88635253906, 1878.375, 2059.35717773438, 1905.44995117188,
>     1799.97827148438, 1849.44445800781, 1765.47619628906, 1775.79541015625,
>     1867.27502441406, 1890.04541015625, 1761.72497558594, 1671.95458984375,
>     1655.92858886719, 1657.81579589844, 1584.47619628906, 1594.57141113281,
>     1614.85717773438, 1587.40002441406, 1593.85717773438, 1485.75,
>     1460.02172851563, 1466.78576660156, 1405.76196289063, 1336.02172851563,
>     1451.04760742188, 1500.97619628906, 1575.13635253906, 1582.05297851563,
>     1631.18420410156, 1561.11364746094, 1625.44995117188, 1567.1904296875,
>     1591.5869140625, 1710.02502441406, 1611.61901855469, 1607.32604980469,
>     1598.32495117188, 1530.52380371094, 1485.47497558594, 1465.17504882813,
>     1438.31823730469, 1417.55004882813, 1364.31579589844, 1305.52380371094,
>     1312.66662597656, 1310.55004882813, 1341.88635253906, 1303.70458984375,
>     1295.30004882813, 1250.39477539063, 1219.40002441406, 1186.475,
>     1179.86363636364, 1277.85, 1323.13157894737, 1315.52272727273,
>     1403.36363636364, 1421.69047619048, 1492.06818181818, 1473.33333333333,
>     1472.43181818182, 1554.73529411765, 1679.85, 1679.425, 1579.64,
>     1458.745, 1468.0652173913, 1509.37272727273, 1564.37619047619,
>     1530.3347826087, 1602.30952380952, 1501.24545454545, 1475.55454545455,
>     1569.055, 1619.87727272727, 1607.03, 1512.17272727273,
> 1499.48333333333,
>     1542.39047619048, 1470.83333333333, 1418.62272727273, 1377.84545454545,
>     1345.995, 1283.53043478261, 1334.45454545455, 1348.72352941176,
>     1371.36363636364, 1371.08, 1404.985, 1370.38571428571,
> 1344.42727272727,
>     1356.93333333333, 1337.8652173913, 1293.35714285714, 1301.7,
>     1311.46086956522, 1373.26666666667, 1375.8619047619, 1379.29090909091,
>     1421.555, 1386.54761904762, 1334.32, 1400.415, 1410.5380952381,
>     1440.90434782609, 1457.24, 1416.59545454545, 1477.24782608696,
>     1511.6, 1557.77857142857, 1608.89285714286, 1685.225, 1657.35434782609,
>     1731.68, 1625.27368421053, 1682.04090909091, 1707.86818181818,
>     1692.1, 1731.02272727273, 1830.36666666667, 1817.35, 1852.92380952381,
>     1836.21, 1882.8, 1987.51904761905, 1892.00952380952, 1741.45,
>     1731.94318181818, 1783.2619047619, 1871.27272727273, 1837.69318181818,
>     1934.14285714286, 2056.97045454545, 2250.9, 2383.30238095238,
>     2453.375, 2432.47826086957, 2623.85833333333, 2852.07142857143,
>     2490.95454545455, 2511.83333333333, 2461.55227272727, 2484.38095238095,
>     2657.14772727273, 2702.13636363636, 2823.67105263158, 2799.05909090909,
>     2839.05, 2757.07954545455, 2817.05263157895, 2804.60476190476,
>     2681.30952380952, 2738.09090909091, 2512.60227272727, 2394.9625,
>     2444.53260869565, 2507.15227272727, 2382.83333333333, 2456.125,
>     2784.89285714286, 3012.05263157895, 2968.03409090909, 2908.2775,
>     2967.86904761905, 3067.45652173913, 2762.56, 2524.14772727273,
>     2122.02826086956, 1857.13, 1504.41666666667, 1420.35952380952,
>     1338.0625, 1338.07954545455, 1431.8125, 1464.41666666667,
>     1586.33571428571, 1674.33181818182, 1927.6375, 1835.59772727273,
>     1875.66363636364, 1956.54761904762, 2181.25714285714, 2230.15,
>     2053.29, 2210.54782608696, 2314.305, 2044.73157894737,
> 1929.40909090909,
>     1989.04545454545, 2110.44047619048, 2171.24318181818, 2342.17857142857,
>     2324.04545454545, 2356.66904761905, 2439.7, 2515.26, 2555.5,
>     2667.41666666667, 2587.2125, 2557.76136363636, 2525.42857142857,
>     2380.975, 2293.46136363636, 2180.64761904762, 2079.98181818182,
>     2024.375, 2151.33333333333, 2207.91666666667, 2184.15909090909,
>     2048.50526315789, 2002.52272727273, 1885.51315789474, 1876.25,
>     1843.32727272727, 2064.12, 1974.30434782609, 1948.82954545455,
>     2086.76315789474, 2037.60681818182, 2053.595, 1911.2825,
>     1861.02380952381, 1832.57142857143, 1814.5375, 1769.60869565217,
>     1816.23571428571, 1761.30476190476, 1814.58260869565, 1747.96428571429,
>     1739.81, 1727.41136363636, 1695.165, 1705.36666666667, 1810.675,
>     1751.05, 1838.95238095238, 1948.30434782609, 2030.4925,
> 1990.43181818182,
>     1946.18913043478, 2055.555, 1909.45952380952, 1814.71904761905,
>     1817.82, 1773.86363636364, 1819.1875, 1804.03947368421,
> 1687.72727272727,
>     1639.5, 1548.125, 1589.60227272727, 1516.48863636364, 1467.89285714286,
>     2592.63342275848, 2916.71199088135, 2303.8279, 2074.54755455933,
>     2076.75230911865, 2006.2042, 2175.96020911865, 2081.16114544067,
>     2059.11494544067, 2045.88709088135, 2010.61370911865, 1878.33650911865,
>     1876.13175455933, 1803.37889088135, 1816.60674544067, 1823.22100911865,
>     1746.05930911865, 1699.76215455933, 1682.12479088135, 1787.94695455933,
>     1706.37574544067, 1664.4881, 1651.26024544067, 1655.66975455933,
>     1613.78210911865, 1596.14474544067, 1512.36945455933, 1521.1878,
>     1530.00614544067, 1309.54431363983, 1439.61659088135, 1450.63969088135,
>     1424.18465455933, 1463.86754544067, 1444.0261, 1474.89064544067,
>     1574.09854544067, 1649.05549088135, 1598.3495, 1675.5112,
>     1765.90075455933, 1701.96690911865, 1704.17099088135, 1640.23714544067,
>     1560.87069088135, 1435.20775455933, 1388.9106, 1415.36630911865,
>     1375.68291363983, 1428.59349088135, 1501.34635455933, 1532.2109,
>     1419.77514544067, 1364.65981363983, 1331.59051363983, 1338.20435681992,
>     1294.11195681992, 1272.06575681992, 1344.8182, 1320.56741363983,
>     1358.04588636017, 1388.9106, 1389.35153409195, 1501.34635455933,
>     1530.00614544067, 1433.003, 1474.89064544067, 1419.77514544067,
>     1366.8644, 1384.50134318008, 1369.06898636017, 1391.11518636017,
>     1417.57039088135, 1404.34295681992, 1444.0261, 1433.003,
>     1417.57039088135, 1413.16155455933, 1344.8182, 1302.93038636017,
>     1309.54431363983, 1316.15815681992, 1302.93038636017, 1331.59051363983,
>     1344.8182, 1380.09208636017, 1463.86754544067, 1483.70899088135,
>     1518.98304544067, 1571.89379088135, 1693.14789088135, 1754.87765455933,
>     1809.99315455933, 1966.52130911865, 2519.88039088135, 2866.006,
>     2660.97660911865, 2328.07902275848, 2358.9434, 2283.98662275848,
>     2442.71869088135, 2539.72250911865, 2213.43817724152, 2200.21049088135,
>     2433.90017724152, 2936.55410911865, 3302.52049088135, 3496.52745455933,
>     3392.91004544067, 3095.28634544067, 3262.8376, 3117.33254544067,
>     2738.13830911865, 2544.13117724152, 2503.78709547119, 2760.18450911865,
>     2883.64269088135, 2859.39240911865, 2590.4285, 2418.46840911865,
>     2365.55699088135, 2358.9434, 2625.70272275848, 2685.22689088135,
>     2740.34239088135, 2583.81490911865, 2769.00302275848, 2956.39555455933,
>     3040.17084544067, 2742.54697724152, 2583.81490911865, 2484.60700911865,
>     2447.1282, 2447.1282, 2409.64939088135, 2471.37932275848,
>     2340.4246727356, 2218.06802725677, 2231.25534581909, 2236.18677692413,
>     2318.58300291138, 2360.64557547607, 2375.66037788696, 2205.91849749756,
>     2150.57962901611, 2208.89225860596, 2231.78550956421, 2214.16681599274,
>     2219.31505581207, 2296.44193858643, 2527.28502240143, 2513.46998458252,
>     2419.61418178101, 2262.45696237183, 2161.78909368286, 2211.95028749237,
>     2264.21397077332, 2212.18139313507, 2152.38020099945, 1953.62060192108,
>     1799.35169844818, 1857.87070840454, 1927.33981992645, 1951.07137548676,
>     1858.24999750519, 1646.99992897491, 1632.56810484467, 1723.6250736618,
>     1807.07136699066, 1865.07501472778, 1913.17391866455, 1881.36846713562,
>     2144.50007046051, 2363.09102303009, 2447.23803404999, 2409.74997397766,
>     2504.84103139343, 2546.02375416412, 2803.54541098175, 2980.69999645081,
>     3003.26189506836, 2870.45015814514, 2919.67397804871, 2894.88882296448,
>     2771.57125569763, 2987.68194348755, 3076.4500581665, 3040.13619641418,
>     2910.42504276428, 2810.06817431793, 2981.61903664856, 2918.21064537659,
>     2604.99992943115, 2544.66672335968, 2563.34994940033, 2594.17496803894,
>     2658.76193080902, 2177.59998270721, 1983.93479971771, 2017.85720584564,
>     1935.07143060913, 1960.19567519531, 2215.42864607391, 2264.59527826385,
>     2427.27279402924, 2406.63156159363, 2420.1842087265, 2389.31814949341,
>     2513.29993523407, 2611.28570015411, 2449.19570404663, 2250.10004305115,
>     2104.30955452118, 2050.6955684021, 1918.49994499359, 1761.45239143524,
>     1687.60000986328, 1663.99998604431, 1747.15916341095, 1800.12507817535,
>     1731.65792903595, 1656.59518632507, 1655.66672697449, 1619.92507875671,
>     1646.77269191284, 1585.49992534637, 1573.72497511139, 1475.76309447174,
>     1432.00002842255, 1412.95, 1378.47727272727, 1463.725,
> 1510.44736842105,
>     1422.15909090909, 1639.18181818182, 1646.7380952381, 1749.68181818182,
>     1723.28571428571, 1726.77272727273, 1764.88235294118, 1843.85,
>     1807.025, 1739.8, 1681.9075, 1785.09782608696, 1752.07272727273,
>     1803.14285714286, 1857.12391304348, 1961.89285714286, 1894.37272727273,
>     1795.60227272727, 1852.4, 1787.05681818182, 1766.125, 1742.15909090909,
>     1665.97222222222, 1684.85, 1610.46904761905, 1526.77045454545,
>     1466.41136363636, 1427.69736842105, 1377.37608695652, 1434.29318181818,
>     1472.84705882353, 1508.22727272727, 1561.3675, 1607.3925,
>     1588.57142857143, 1597.02272727273, 1650.59444444444, 1588.2847826087,
>     1482.91666666667, 1478.93333333333, 1486.17173913043, 1581.03571428571,
>     1592.96428571429, 1650.31136363636, 1682.145, 1655.69285714286,
>     1587.8675, 1651.1, 1685.10714285714, 1712.82608695652, 1756.725,
>     1789.67045454545, 1925.58260869565, 2053.275, 2202.03571428571,
>     2421.47619047619, 2751.7175, 3000.28260869565, 2926.975,
>     2728.46315789474, 2689.05454545455, 2816.8, 2844.20476190476,
>     2903.17272727273, 3009.40476190476, 3130.30909090909, 3139.78571428571,
>     3168.1, 3247.1, 3378.90476190476, 3389.80952380952, 3241.9,
>     3529.72727272727, 3608.47619047619, 3791.90909090909, 3850.65909090909,
>     4056.16666666667, 4278.15909090909, 4577.025, 4743.8619047619,
>     4974.975, 5123.67391304348, 6404.44444444444, 8059.19047619048,
>     7222.77272727273, 7726.7380952381, 7690.25, 7622.64285714286,
>     7497.40909090909, 7029.29545454546, 6680.97368421053, 5689.34090909091,
>     5718.15, 6465.29545454545, 7753.34210526316, 7677.95238095238,
>     7514.2380952381, 7980.93181818182, 7500.20454545455, 7671.35,
>     8020.58695652174, 6957.43181818182, 6630.73611111111, 7078.90909090909,
>     7941.14285714286, 8434.31578947368, 8714.18181818182, 8356.125,
>     8292, 8407.02173913043, 7633.8, 6975.11363636364, 4894.89130434783,
>     3729.1875, 3105.09523809524, 3260.35714285714, 3328.4125,
>     3770.875, 4436.925, 4594.90277777778, 5013.29761904762,
> 5240.82954545455,
>     6176.875, 6195.75, 6305.98863636364, 6682.44047619048,
> 6976.97619047619,
>     7367.395, 6867.69, 7466.96086956522, 7729.855, 6843.18421052632,
>     6501.5, 6750.56818181818, 7302.66666666667, 7729.59090909091,
>     8289.7619047619, 8458.42045454545, 9152.85714285714, 9533.2,
>     9880.9375, 9503.35869565217, 9482.75, 8931.675, 9066.85227272727,
>     9650.46428571429, 8997.98863636364, 8300.13636363636, 7394.19047619048,
>     7581.02272727273, 7558.875, 8061.91666666667, 8441.4880952381,
>     8470.78409090909, 8285.52631578947, 7896.90909090909, 7428.28947368421,
>     7584.26136363636, 7510.43181818182, 8087.7425, 8062.03260869565,
>     7711.22727272727, 7966.48684210526, 8053.73863636364, 8060.925,
>     7652.375, 7221.1619047619, 7248.71428571429, 7000.2375,
> 6906.64130434783,
>     7186.25, 7159.26904761905, 7203.02173913044, 7070.65476190476,
>     7214.9, 7291.46590909091, 7149.2125, 6650.03571428571, 6673.5625,
>     6891.125, 6821.14285714286, 7113.38043478261, 7001.8375,
>     6872.21590909091, 6737.47826086957, 6712.85, 6446.45238095238,
>     5830.53571428571, 5729.275, 5939.67045454546, 6042.0875,
>     6294.77631578947, 5833.01136363636, 5456.75, 5127.3, 5217.25,
>     5216.09090909091, 4799.90476190476, 674.58, 665.893, 554.276,
>     516.71, 514.268, 600.786, 645.75, 626.36, 673.941, 662.27,
>     623.875, 596.712, 557.813, 500.8, 499.693, 496.625, 480.316,
>     460.5, 409.284, 410.24, 443.773, 437.68, 413.405, 410.119,
>     384.163, 374.458, 329.977, 350.335, 334.505, 314.961, 337.895,
>     363.413, 438.15, 422.786, 415.114, 444.776, 481.838, 493.488,
>     420.707, 433.171, 437.393, 413.148, 422.645, 416.205, 412.245,
>     394.245, 381.016, 388.06, 370.735, 386.038, 394.743, 381.371,
>     376.957, 378.314, 347.607, 347.677, 340.945, 340.217, 341.286,
>     319.622, 302.852, 300.333, 303.205, 324.883, 316.395, 316.298,
>     317.202, 330.131, 323.764, 326.093, 325.548, 321.985, 345.561,
>     339.053, 346.095, 340.716, 342.325, 342.798, 348.554, 376.29,
>     418.152, 423.863, 396.983, 391.595, 408.524, 401.045, 408.848,
>     439.665, 461.65, 449.277, 450.33, 460.988, 460.12, 465.764,
>     468.14, 487.079, 477.758, 442.124, 443.491, 451.558, 451.32,
>     451.657, 437.452, 431.064, 413.439, 406.39, 419.966, 419.248,
>     404.445, 387.973, 390.274, 384.72, 371.35, 367.727, 375.21,
>     365.548, 361.798, 366.8, 394.361, 409.655, 410.118, 416.543,
>     393.661, 374.929, 368.855, 352.657, 361.82, 394.861, 389.56,
>     381.333, 381.866, 378.161, 384.591, 363.748, 363.39, 358.055,
>     357.117, 366.36, 368.013, 356.721, 348.46, 358.826, 359.96,
>     361.875, 354.436, 353.853, 344.641, 338.728, 337.039, 340.784,
>     352.452, 343.603, 345.3, 344.277, 334.924, 334.657, 328.993,
>     329.31, 329.974, 341.948, 367.045, 371.914, 392.034, 379.795,
>     355.561, 364.005, 373.939, 383.243, 387.11, 381.658, 384,
>     377.908, 381.343, 385.714, 385.45, 380.207, 391.348, 390.164,
>     384.377, 379.48, 378.738, 376.745, 381.82, 391.339, 385.231,
>     387.618, 386.138, 383.502, 382.931, 383.202, 385.209, 387.445,
>     398.695, 404.919, 396.512, 392.87, 391.99, 385.245, 383.457,
>     387.51, 383.29, 380.909, 377.869, 369.338, 355.025, 346.4,
>     352.311, 344.707, 344.1, 340.805, 323.78, 323.998, 322.616,
>     324.854, 306.345, 288.776, 289.264, 297.743, 295.87, 308.558,
>     298.971, 292.223, 292.874, 284.228, 288.661, 296.595, 294.243,
>     291.357, 287.333, 287.495, 286.243, 282.62, 276.932, 261.402,
>     256.198, 256.936, 264.47, 311.562, 293.65, 283.743, 284.59,
>     300.855, 286.704, 279.961, 275.293, 285.368, 282.152, 274.523,
>     273.676, 270.405, 265.989, 271.892, 265.934, 262.018, 263.273,
>     260.75, 272.057, 270.738, 267.707, 272.657, 282.478, 283.322,
>     276.248, 275.992, 281.764, 295.683, 294.353, 302.862, 314.48,
>     321.536, 313.567, 310.045, 318.8, 316.748, 319.255, 333.3,
>     356.864, 359.575, 341.564, 328.208, 355.405, 356.912, 350.765,
>     358.993, 378.859, 379.093, 390.2, 407.674, 414.495, 404.73,
>     405.976, 404.85, 383.953, 391.78, 398.441, 400.133, 405.402,
>     420.21, 439.059, 442.974, 424.08, 423.43, 434.355, 429.14,
>     422.903, 430.302, 424.745, 437.773, 455.936, 470.107, 476.668,
>     509.423, 549.433, 555.518, 557.215, 611.853, 676.769, 597.898,
>     633.093, 631.557, 600.15, 586.648, 626.825, 629.513, 630.352,
>     665.103, 655.891, 680.008, 668.31, 655.714, 665.266, 664.53,
>     710.645, 754.48, 808.311, 803.618, 887.784, 924.283, 971.055,
>     911.6, 889.125, 889.536, 941.167, 840.388, 824.92, 812.815,
>     757.85, 819.94, 857.726, 939.763, 925.989, 892.663, 926.855,
>     947.807, 934.272, 949.5, 996.443, 1043.511, 1126.119, 1135.012,
>     1119.575, 1095.8, 1115.554, 1148.475, 1204.321, 1232.382,
>     1196, 1213.464, 1271.461, 1343.19, 1371.784, 1393.512, 1360.475,
>     1371.313, 1422.848, 1474.431, 1512.188, 1528.38, 1568.526,
>     1759.5, 1780.648, 1667.893, 1735.977, 1652.725, 1656.095,
>     1743.095, 1675.057, 1648.539, 1585.114, 1595.632, 1592.784,
>     1625.682, 1741.925, 1746.348, 1724.352, 1687.342, 1671.886,
>     1630.688, 1591.013, 1485.905, 1416.143, 1342.7, 1284.348,
>     1345.048, 1348.464, 1314.402, 1277.417, 1221.588, 1243.068,
>     1298.713, 1336.56, 1299.175, 1288.913, 1277.857, 1312.989,
>     1297.005, 1241.33, 1223.565, 1176.413, 1200.44, 1249.333,
>     1231.1, 1180.636, 1198.253, 1197.684, 1182.248, 1131.58,
>     1117.525, 1124.905, 1157.123, 1088.388, 12.15, 12.15, 12.15,
>     12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15,
>     12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15, 12.15,
>     12.15, 12.15, 12.15, 14.05, 14.05, 14.05, 14.05, 14.05, 14.05,
>     14.05, 14.05, 14.05, 14.05, 14.05, 14.05, 12.54, 12.54, 12.54,
>     12.54, 12.54, 12.54, 12.54, 12.54, 12.54, 12.54, 12.54, 12.54,
>     11.31, 11.31, 11.31, 11.31, 11.31, 11.31, 11.31, 11.31, 11.31,
>     11.31, 11.31, 11.31, 11.49, 11.49, 11.49, 11.49, 11.49, 11.49,
>     11.49, 11.49, 11.49, 11.49, 11.49, 11.49, 11.36, 11.36, 11.36,
>     11.36, 11.36, 11.36, 11.36, 11.36, 11.36, 11.36, 11.36, 11.36,
>     10.94, 10.94, 10.94, 10.94, 10.94, 10.94, 10.94, 10.94, 10.94,
>     10.94, 10.94, 10.94, 10.51, 10.51, 10.51, 10.51, 10.51, 10.51,
>     10.51, 10.51, 10.51, 10.51, 10.51, 10.51, 12.03, 12.03, 12.03,
>     12.03, 12.03, 12.03, 12.03, 12.03, 12.03, 12.03, 12.03, 12.03,
>     14.05, 14.05, 14.05, 14.05, 14.05, 14.05, 14.05, 14.05, 14.05,
>     14.05, 14.05, 14.05, 15.03, 15.03, 15.03, 15.03, 15.03, 15.03,
>     15.03, 15.03, 15.03, 15.03, 15.03, 15.03, 14.31, 14.31, 14.31,
>     14.31, 14.31, 14.31, 14.31, 14.31, 14.31, 14.31, 14.31, 14.31,
>     12.58, 12.58, 12.58, 12.58, 12.58, 12.58, 12.58, 12.58, 12.58,
>     12.58, 12.58, 12.58, 11.45, 11.45, 11.45, 11.45, 11.45, 11.45,
>     11.45, 11.45, 11.45, 11.45, 11.45, 11.45, 12.27, 12.27, 12.27,
>     12.27, 12.27, 12.27, 12.27, 12.27, 12.27, 12.27, 12.27, 12.27,
>     12.97, 12.97, 12.97, 12.97, 12.97, 12.97, 12.97, 12.97, 12.97,
>     12.97, 12.97, 12.97, 13.04, 13.04, 13.04, 13.04, 13.04, 13.04,
>     13.04, 13.04, 13.04, 13.04, 13.04, 13.04, 13.41, 13.41, 13.41,
>     13.41, 13.41, 13.41, 13.41, 13.41, 13.41, 13.41, 13.41, 13.41,
>     11.93, 11.93, 11.93, 11.93, 11.93, 11.93, 11.93, 11.93, 11.93,
>     11.93, 11.93, 11.93, 12.45, 12.45, 12.45, 12.45, 12.45, 12.45,
>     12.45, 12.45, 12.45, 12.45, 12.45, 12.45, 12.99, 12.99, 12.99,
>     12.99, 12.99, 12.99, 12.99, 12.99, 12.99, 12.99, 12.99, 12.99,
>     12.68, 12.68, 12.68, 12.68, 12.68, 12.68, 12.68, 12.68, 12.68,
>     12.68, 12.68, 12.68, 13.82, 13.82, 13.82, 13.82, 13.82, 13.82,
>     13.82, 13.82, 13.82, 13.82, 13.82, 13.82, 16.39, 16.39, 16.39,
>     16.39, 16.39, 16.39, 16.39, 16.39, 16.39, 16.39, 16.39, 16.39,
>     28.11, 28.11, 28.11, 28.11, 28.11, 28.11, 28.11, 28.11, 28.11,
>     28.11, 28.11, 28.11, 33.45, 33.45, 33.45, 33.45, 33.45, 33.45,
>     33.45, 33.45, 33.45, 33.45, 33.45, 33.45, 36.63, 36.63, 36.63,
>     36.63, 36.63, 36.63, 36.63, 36.63, 36.63, 36.63, 36.63, 36.63,
>     60.8, 60.8, 60.8, 60.8, 60.8, 60.8, 60.8, 60.8, 60.8, 60.8,
>     60.8, 69.9826086956522, 72.5090909090909, 75.59, 64.0727272727273,
>     59.7818181818182, 62.6904761904762, 71.6590909090909, 83.9521739130435,
>     97.6666666666667, 80.7136363636364, 86.7863636363636, 99.2571428571429,
>     105.247826086957, 125.914285714286, 127.615, 139.769565217391,
>     172.468181818182, 161.347619047619, 143.631818181818, 126.363636363636,
>     145.340909090909, 140.627272727273, 148.480952380952, 160.55,
>     168.526086956522, 179.63, 187.18, 169.35652173913, 179.261111111111,
>     177.095, 170.877272727273, 172.97619047619, 177.45, 177.227272727273,
>     150.433333333333, 135.540909090909, 136.455, 140.347619047619,
>     140.395238095238, 144.663636363636, 147.647368421053, 136.272727272727,
>     134.621052631579, 127.936363636364, 107.795454545455, 99.47,
>     113.947826086957, 120.345454545455, 128.873684210526, 150.490909090909,
>     154.638888888889, 139.87, 137.390909090909, 124.009523809524,
>     114.815, 127.191304347826, 137.055, 134.185714285714, 132.572727272727,
>     136.32380952381, 135.790476190476, 128.119047619048, 121.37,
>     111.833333333333, 114.580952380952, 100.56, 92.7428571428571,
>     95.9739130434782, 92.6333333333334, 82.2681818181818, 80.0913043478261,
>     73.125, 68.8, 67.3863636363636, 62.69, 56.9409090909091,
>     51.15, 60.2333333333333, 62.2863636363636, 51.504347826087,
>     55.3809523809524, 56.4318181818182, 52.7409090909091, 46.1619047619048,
>     6584.80078125, 6978.927734375, 6733.787109375, 6233.369140625,
>     6000.76953125, 6294.833984375, 6622.166015625, 6584.505859375,
>     6655.357421875, 6691.947265625, 6452.666015625, 6390.912109375,
>     6403.7734375, 6370.748046875, 6292.43359375, 6307.060546875,
>     6352.15625, 6169.462890625, 6150.337890625, 5999.1171875,
>     5579.064453125, 5216.7109375, 5100.24609375, 5496.025390625,
>     5628.904296875, 5716.759765625, 5653.357421875, 5394.0546875,
>     5211.837890625, 5186.697265625, 5068.580078125, 4953.02734375,
>     4310.296875, 3916.5078125, 3433.65747070313, 3576.26953125,
>     3796.9599609375, 4228.197265625, 4831.7734375, 4817.78515625,
>     5031.533203125, 4883.158203125, 4804.037109375, 4852.7578125,
>     4910.5234375, 4674.150390625, 4585.166015625, 4657, 4670.48046875,
>     4639.912109375, 4780.826171875, 4911.40625, 4811.423828125,
>     4773.232421875, 4640.1796875, 4736.07421875, 4705.63671875,
>     4750.294921875, 4748.400390625, 4858.982421875, 4947.16796875,
>     5050.78515625, 5211.72265625, 5489.50390625, 5604.14453125,
>     5553.4375, 5090.466796875, 4907.484375, 4570.177734375, 4239.484375,
>     4034.45458984375, 4089.57006835938, 4034.45458984375, 3988.15747070313,
>     4135.8671875, 4085.16088867188, 4043.27319335938, 4087.36547851563,
>     3910.99584960938, 3807.37866210938, 3723.60327148438, 3644.23681640625,
>     3642.0322265625, 3562.666015625, 3525.1875, 3716.9892578125,
>     3772.10473632813, 3897.76806640625, 4435.6953125, 4435.6953125,
>     4753.16015625, 5306.51953125, 5332.9765625, 5692.328125,
>     5937.041015625, 7661.0546875, 8073.318359375, 8666.361328125,
>     15496.2734375, 18011.74609375, 17024.076171875, 15588.8671875,
>     14592.37890625, 14186.73046875, 11878.4921875, 11556.6171875,
>     13342.359375, 16920.458984375, 17725.14453125, 18582.474609375,
>     17156.353515625, 15261.5, 13454, 12143, 12275, 12910, 11222,
>     10425, 9793, 8809, 7056, 6977, 9267, 8939, 8698, 8422, 9318,
>     10957, 10844, 9145, 8587, 8158, 8569, 8672, 8700, 9023, 8499.609375,
>     8296.5, 8515.201171875, 8154.75, 7675.953125, 7453.86328125,
>     7258.5, 7140.263671875, 7531.13671875, 7888, 7430.455078125,
>     7417.951171875, 7333.947265625, 7202.36328125, 7516.173828125,
>     7279.75, 6918.572265625, 6322.5, 5593.71484375, 5751.1904296875,
>     5948.10009765625, 6050.75, 5974.9130859375, 5987.5, 5777.78955078125,
>     5544.318359375, 5051.181640625, 4740.47607421875, 4376.04541015625,
>     4464.85693359375, 4642.27294921875, 5139.7998046875, 5584.0478515625,
>     5830.10009765625, 5590.4345703125, 5400.26318359375, 6082.85693359375,
>     6287.04541015625, 6230.5712890625, 5856.13623046875, 6370.681640625,
>     6735.85693359375, 7474.54541015625, 8540.650390625, 9596.1904296875,
>     8431.25, 7522.39111328125, 7405, 7262.380859375, 7877.27294921875,
>     8618, 8931.818359375, 8397.25, 8068.86376953125, 8498.095703125,
>     8074.21044921875, 7870.71435546875, 8198.5, 8046.39990234375,
>     8047, 8042.85693359375, 7713.15771484375, 7202.60888671875,
>     7037.619140625, 7329.8095703125, 7028.69580078125, 6960.71435546875,
>     6571.0478515625, 7069.13623046875, 7736.5791015625, 7893.21044921875,
>     7313.13623046875, 7476, 7060, 6832.7392578125, 6764.4501953125,
>     6498.28564453125, 6373.47802734375, 6137.25, 5942.380859375,
>     5489, 5367.25, 5397.04541015625, 5391, 5016.84228515625,
>     4456.90478515625, 4332.619140625, 4078, 4100.9091796875,
>     3870.45458984375, 4117.25, 3865.76318359375, 4264, 4623.25,
>     5002.95454545455, 5054.75, 5395.78947368421, 5194.77272727273,
>     5696.36363636364, 6431.42857142857, 7029.09090909091, 7317.61904761905,
>     7946.81818181818, 8073.23529411765, 8315.25, 9583.75, 10255.4,
>     9746.5, 10122.9130434783, 8384.31818181818, 8174.80952380952,
>     8038.4347826087, 8624.80952380952, 7650.63636363636, 7350.59090909091,
>     7318.8, 6975.90909090909, 6546.8, 6165.18181818182, 6363.11111111111,
>     7086.47619047619, 6674.33333333333, 5962.54545454546, 5525.63636363636,
>     5057.65, 4830.78260869565, 5131.31818181818, 5315.82352941176,
>     6061.36363636364, 6039.95, 6543.2, 6956.80952380952, 6771.18181818182,
>     7147.61111111111, 7137.69565217391, 6736.85714285714, 6664.95238095238,
>     6818.91304347826, 7314.80952380952, 7206.33333333333, 8032.90909090909,
>     8607.95, 8339.71428571429, 7930.6, 8347.75, 8839.04761904762,
>     8831.78260869565, 9359.9, 9995.81818181818, 11040.347826087,
>     12052.4, 14185.2095238095, 15089.3333333333, 15099.35,
> 13786.5434782609,
>     12725.9, 11228.6052631579, 13599.3636363636, 15020.0681818182,
>     13639.619047619, 13430.3636363636, 14378.4761904762, 14089.5136363636,
>     13764.9761904762, 14563.75, 15415.6, 16239.9047619048,
> 16138.3333333333,
>     17002.25, 16113.1818181818, 14587.5952380952, 14962, 14154.5454545455,
>     12431.119047619, 12235.0454545455, 13490.45, 14660.8095238095,
>     14974.5, 14925.4782608696, 18028.8888888889, 21131.3333333333,
>     20585.9090909091, 26185.7142857143, 30468.8636363636, 29702.619047619,
>     32551.1363636364, 31891.5909090909, 34400.5263157895, 36821.5909090909,
>     41078.25, 46125.2272727273, 49956.5789473684, 51783.3333333333,
>     41551.6666666667, 33400.2272727273, 27649.6363636364, 29548.4,
>     31156, 30505.6363636364, 26053.5555555556, 27774.7727272727,
>     28064.9523809524, 31093.0526315789, 28776.8181818182, 25656.5,
>     22562.5714285714, 20106.9565217391, 19111.8, 17781.8636363636,
>     12144.8695652174, 10776.5, 9846.92857142857, 11562.9523809524,
>     10410.75, 9710.72727272727, 11331.6, 12763.3611111111,
> 14961.9523809524,
>     16024.5, 19375.925, 17404.6363636364, 18489.4772727273,
> 16911.3333333333,
>     17121.6190476191, 18405.55, 19060.55, 22467.1739130435, 26028.5,
>     21930, 19411.2954545455, 19548.5227272727, 21448.7857142857,
>     22690.1363636364, 23793.6785714286, 22836.2272727273, 24099.5714285714,
>     25621.225, 28412.175, 26710.347826087, 26332.1666666667,
>     24164.55, 22420.9318181818, 23847.9523809524, 21864.6818181818,
>     20377.5909090909, 19039.0476190476, 17873, 18246.0375, 19908.619047619,
>     20393.6666666667, 18660.8068181818, 17892.8157894737, 16968.3181818182,
>     16603.6842105263, 16128.4090909091, 15703.9886363636, 17287.9625,
>     17168.7391304348, 16335.3636363636, 17448.5, 17494.0681818182,
>     17690.1, 16731.7, 15629.3095238095, 14948.2261904762, 14280.275,
>     13750.3152173913, 14308.2619047619, 13801.3928571429, 14117.652173913,
>     13684.0119047619, 13924.55, 14101.25, 14203.55, 15678.0952380952,
>     17373.6, 19401.075, 18628.8095238095, 19117.652173913, 18600.2,
>     18034.7954545455, 15812.3695652174, 15807.05, 15962.0476190476,
>     14849.1904761905, 14573.8375, 13755.5, 12830.925, 13511.3421052632,
>     12825.2272727273, 11413.097826087, 10386, 9937.54545454545,
>     10316.8295454545, 9244.33333333333, 16973.5879043457, 17090.2131635254,
>     17460.5904, 17041.7126, 17180.6047364746, 17211.4672635254,
>     17090.2131635254, 17008.6433, 17295.2439, 16682.3584635254,
>     15599.8932729492, 14698.2004635254, 14360.8925270508, 13679.6671,
>     13624.5516, 13344.5659364746, 12683.1799364746, 12583.9720364746,
>     13419.5208635254, 14477.7384635254, 14969.3698, 15101.647,
>     15826.9648270508, 15954.8338635254, 16124.5885270508, 15930.5862729492,
>     12813.2503635254, 12628.0644364746, 12736.0886635254, 11098.0569454407,
>     11316.3155364746, 12187.1404364746, 12678.7717729492, 12420.8269270508,
>     11982.1097, 12041.6333635254, 12079.1108270508, 13040.3273,
>     13459.2051, 13787.6913270508, 13523.1369270508, 13351.1808729492,
>     13190.2425364746, 12811.0489729492, 12751.5199270508, 12747.1117635254,
>     12769.1579635254, 12341.4638364746, 12112.1801270508, 12292.9632729492,
>     12403.1942729492, 12440.6717364746, 12577.3571, 12658.9269635254,
>     12442.8731270508, 12299.5728270508, 12087.9325364746, 11739.6015,
>     11988.7246364746, 11719.7620729492, 11073.4002945129, 10946.00019729,
>     11210.6010202576, 11806.4007862915, 11909.8002093018, 12385.799788855,
>     12755.8002592163, 12647.2001182495, 12325.2009963135, 12391.1000807129,
>     9903.15330911865, 9114.00053773193, 7910.00069096069, 7922.90022076416,
>     7313.6001175354, 5686.10035791016, 5474.20003143616, 5378.30060886841,
>     5386.59989170837, 5404.30016187134, 5397.20013902588, 5542.80073070679,
>     6092.29997819824, 6428.10017742309, 6712.70055303955, 6688.40048432617,
>     6631.50072766113, 6699.80035043945, 6747.40003927612, 6610.39980692749,
>     6404.90013120117, 6556.10040924072, 6706.30023869019, 6763.80012994995,
>     6927.30047599487, 6829.40049264526, 6808.10042410889, 6690.10064141235,
>     6772.90037717895, 6752.60075723267, 6777.60053444214, 7035.80037026367,
>     7148.80060119629, 7322.7003647644, 7408.50010070801, 7252.7005848999,
>     7312.40052114258, 7337.40029835205, 7409.60079599609, 7873.70062153931,
>     8760.50040474243, 10195.9005791382, 10143.7003069397, 9935.50069833374,
>     9605.20061156616, 8715.6006487854, 8269.69997210693, 7922.40033286743,
>     6837.30047068481, 6744.09997180176, 6592.00016466675, 6156.09993710937,
>     6270.50025783081, 6390.90057890625, 6324.60050771484, 6096.00035966797,
>     5924.40021498413, 5905.2006175293, 5707.00011245117, 6061.40044733276,
>     5981.3006444458, 5615.20037615356, 5610.7579, 5582.09810911865,
>     5515.95950911865, 5557.84715455933, 5698.72202270508, 5709.30444091186,
>     5673.59979650879, 5647.72000130615, 5575.09564178467, 5550.09115499878,
>     5514.45042627563, 5515.89492064209, 5483.09137531128, 5612.10012927856,
>     5637.04540996094, 5836.2497276123, 6113.15801934814, 6631.99994276123,
>     6980.49985119018, 6784.21076134033, 6643.33320200806, 6025.90907946167,
>     5735.71439985962, 5765.80926595459, 5913.09992277222, 5792.75006144409,
>     5666.30466264038, 5596.49999379883, 5505.47363722839, 5115.45443951416,
>     4978.54537110901, 4819.00006005859, 4510.68192121277, 4687.23822227783,
>     4643.81828251038, 4781.69987844543, 4950.71412927856, 5442.45008562927,
>     5405.52162415466, 5383.05257147217, 5493.47632770691, 5497.63622927551,
>     5300.95254420166, 5173.31831375122, 5318.72715839233, 5479.28570116882,
>     6139.09096548462, 5935.50001380005, 6187.61911367188, 5431.00009616699,
>     5516.30431739197, 5854.44416690063, 5924.52400956421, 6646.81828855591,
>     6640.75033574219, 6966.13631591797, 6333.7498690979, 6209.54555587158,
>     6381.6664451477, 6275.78978494263, 6257.14255352173, 6189.28563092651,
>     6205.24974938354, 6467.2502133728, 6401.90484060059, 6183.7498602478,
>     6245.52201011352, 6109.52357194824, 6093.71419671631, 5933.09072906494,
>     5993.85704796753, 5826.23784094849, 5871.13625131226, 5874.3158881897,
>     5901.31588978271, 5707.59082789307, 5704.84985441895, 5560.28570594788,
>     5435.78267101746, 5423.29993872681, 5493.47632770691, 5556.52174519653,
>     5650.99988488159, 5506.19050203857, 5200.49997043457, 5236.49997255859,
>     5468.63633969726, 5707.00011245117, 5868.15778062744, 5967.85682216797,
>     5651.19028632812, 5690.26295066528, 5479.77280592957, 5424.49987151794,
>     5477.99998680725, 5262.36833021851, 5102.99996468201, 5262,
>     5292.95454545455, 5381.75, 5643.15789473684, 5262.72727272727,
>     5210.45454545455, 5224.28571428571, 5336.81818181818, 5425.2380952381,
>     5835.22727272727, 5719.11764705882, 5926.25, 5669, 5463.8,
>     5398.05, 5437.95652173913, 5461.18181818182, 5329.7619047619,
>     5314.34782608696, 5471.80952380952, 5266.45454545455, 5264.81818181818,
>     5227.4, 5168.5, 5121.25, 5049.63636363636, 4949.77777777778,
>     4951.78947368421, 4829.90476190476, 4343.59090909091, 3929.45454545455,
>     3698.36842105263, 3749.17391304348, 4059.31818181818, 4022.47058823529,
>     3867.95454545455, 3724.61111111111, 3850.75, 4026.47619047619,
>     4152.81818181818, 4303.33333333333, 4313.78260869565, 3832.2380952381,
>     3961.7619047619, 4242.08695652174, 4228.95238095238, 4227.19047619048,
>     4445.27272727273, 4572.3, 4592.66666666667, 4561.25, 4734,
>     4678.04761904762, 4733.26086956522, 4802.55, 4907.27272727273,
>     5227.4347826087, 5363.35, 6058.40714285714, 6432.4880952381,
>     6662.7, 7602.20652173913, 8909, 9391.63157894737, 9158.63636363636,
>     9021.90909090909, 9021.42857142857, 9015.68181818182, 9043.21428571429,
>     9038.65, 8473.71428571429, 7705.65, 8106.325, 8442.23809523809,
>     8133.80952380952, 8099.25, 7604.36363636364, 7180.92857142857,
>     7228.13636363636, 6770.58181818182, 6415.47619047619, 6173.71818181818,
>     6762.5, 7067.3619047619, 7788.865, 7949.02173913044, 8859.65555555556,
>     8793.16666666667, 7858.77272727273, 8356.44047619048, 8436.68181818182,
>     8975.02380952381, 9809.5, 10038.4090909091, 11125.9473684211,
>     11331.4545454545, 12890.2, 13787.25, 13952.8421052632,
> 14162.1428571429,
>     14078.8333333333, 14732.7272727273, 15046.9772727273, 14989.05,
>     16068.347826087, 16660.7727272727, 16244.7222222222, 16310.9318181818,
>     17269.8095238095, 19799.0789473684, 21646.0227272727, 23853.6,
>     22133.4285714286, 22955.6304347826, 19935, 18306.5909090909,
>     14423.347826087, 13674.15, 11292.1428571429, 11563.3333333333,
>     11074.775, 10689.4090909091, 11830.2, 13871.8333333333,
> 15009.0714285714,
>     13903.5909090909, 14761.65, 14936.2272727273, 15037.4090909091,
>     14966.1428571429, 15588.6904761905, 17710.2, 16352.3, 17509.1304347826,
>     18634.6, 17565.0263157895, 17257.7272727273, 18206.5227272727,
>     20733.0714285714, 22694.3636363636, 26237.0952380952, 25403.0909090909,
>     26103.8571428571, 27439.35, 31619.65, 30590.9347826087,
> 32347.6944444444,
>     28571.4, 25519.6818181818, 27398.0952380952, 24056.7272727273,
>     22526.6022727273, 21868.6428571429, 21291.7045454545, 19386.9125,
>     21547.7023809524, 24293.3095238095, 22985.4318181818, 22114.5263157895,
>     20295.3409090909, 19255.3947368421, 18546.0909090909, 18675.5568181818,
>     20771.2625, 21233.6956521739, 20713.0681818182, 22880.8947368421,
>     24598.8977272727, 24211.7375, 23302, 21589.6428571429, 20781.619047619,
>     20267.4, 19563.8260869565, 21638.2142857143, 22735.0714285714,
>     23101.5869565217, 22826.880952381, 22762.125, 22063.8636363636,
>     22820.675, 23024.3095238095, 23405.2, 23271.25, 22762,
> 22424.0108695652,
>     22231.05, 21090.5227272727, 19830.4130434783, 20033.475,
>     19829.7142857143, 19454.119047619, 18233.9125, 17421.9090909091,
>     15900.875, 15803.5921052632, 15064.9431818182, 15071.5326086957,
>     15163.775, 15453.3409090909, 15794.6136363636, 14745.2857142857,
>     773.821533203125, 868.620361328125, 740.75244140625, 707.68310546875,
>     701.069091796875, 676.818359375, 712.09228515625, 767.207763671875,
>     795.867919921875, 804.686279296875, 800.277099609375, 782.64013671875,
>     776.026123046875, 731.933837890625, 756.1845703125, 824.52783203125,
>     850.9833984375, 839.960205078125, 864.2109375, 954.600341796875,
>     936.96337890625, 890.66650390625, 875.234130859375, 848.77880859375,
>     820.11865234375, 824.52783203125, 791.45849609375, 740.75244140625,
>     749.57080078125, 692.250732421875, 723.115478515625, 714.296875,
>     749.57080078125, 751.775390625, 709.8876953125, 670.20458984375,
>     698.864501953125, 681.2275390625, 679.02294921875, 696.659912109375,
>     736.343017578125, 718.7060546875, 740.75244140625, 809.095458984375,
>     837.755615234375, 857.59716796875, 859.8017578125, 857.59716796875,
>     956.80517578125, 998.69287109375, 1040.58056640625, 1003.10205078125,
>     1000.8974609375, 941.372802734375, 853.18798828125, 833.346435546875,
>     873.029541015625, 844.369384765625, 857.59716796875, 859.8017578125,
>     866.415771484375, 886.25732421875, 921.53125, 930.349609375,
>     879.643310546875, 809.095458984375, 767.207763671875, 734.138427734375,
>     690.046142578125, 632.725830078125, 597.451904296875, 685.63671875,
>     643.7490234375, 608.47509765625, 626.112060546875, 659.181396484375,
>     707.68310546875, 804.686279296875, 806.890869140625, 815.70947265625,
>     870.824951171875, 886.25732421875, 820.11865234375, 798.072509765625,
>     758.38916015625, 740.75244140625, 731.933837890625, 760.593994140625,
>     837.755615234375, 877.438720703125, 828.93701171875, 802.481689453125,
>     756.1845703125, 769.412353515625, 846.573974609375, 866.415771484375,
>     877.438720703125, 875.234130859375, 981.055908203125, 1069.24072265625,
>     1175.0625, 1364.65966796875, 1236.79174804688, 1307.33959960938,
>     1329.3857421875, 1518.98315429688, 1556.46166992188, 1591.73559570313,
>     1732.83129882813, 1929.49731445313, 1959.9072265625, 1654.39990234375,
>     1627, 1537, 1614, 1726, 1626, 1583, 1435, 1450, 1294, 1394,
>     1666, 1686, 1775, 1715, 1637, 1615, 1537, 1353, 1278, 1265,
>     1206, 1188, 1199, 1254, 1127.73999023438, 1067.44995117188,
>     1063.41333007813, 1047.73999023438, 1025.60009765625, 991.64990234375,
>     1100.65014648438, 1185.0791015625, 1157.6591796875, 1135.47509765625,
>     1217.77270507813, 1304.94995117188, 1377.73681640625, 1385.4091796875,
>     1320.4091796875, 1363.40014648438, 1367.119140625, 1158.36376953125,
>     1052.23803710938, 1061.47619628906, 1067.57495117188, 1073.625,
>     996.760864257813, 1006, 979.26318359375, 928.227294921875,
>     930.068176269531, 886.833312988281, 875.727294921875, 918.571411132813,
>     929.386352539063, 975.534973144531, 998.59521484375, 969.75,
>     937.608703613281, 924.342102050781, 956.714294433594, 966,
>     962.642883300781, 946.909118652344, 993.545471191406, 1059.54760742188,
>     1150.27270507813, 1112.75, 1154.8095703125, 1030.67504882813,
>     1021.69567871094, 1059.13891601563, 1036.61901855469, 1007.77270507813,
>     1028.05505371094, 1015.47729492188, 988.025024414063, 980.431823730469,
>     1033.1904296875, 1017.15789794922, 1019.69049072266, 1035.83337402344,
>     1063.25, 1043.94995117188, 1036.54760742188, 1009.40002441406,
>     1000.52172851563, 1007.95239257813, 998.976196289063, 1002.71740722656,
>     1044.31433105469, 1036.54760742188, 1086.13635253906, 1178.15795898438,
>     1254.34216308594, 1240.02270507813, 1310.125, 1354.97619628906,
>     1517.02172851563, 1650.625, 1638.28576660156, 1276.67395019531,
>     1171.15002441406, 1101.26196289063, 1096.25, 1043.19995117188,
>     1046.75, 1096.17504882813, 1060.36840820313, 1007.59521484375,
>     1043.5, 1029.05004882813, 999.636352539063, 941.363647460938,
>     965.950012207031, 961.578918457031, 931.75, 1017.6, 1028.79545454545,
>     1018.175, 1039.92105263158, 999.909090909091, 1071.22727272727,
>     1129.69047619048, 1192.93181818182, 1147.97619047619, 1146.38636363636,
>     1185.23529411765, 1178.575, 1098.25, 1113, 1132.52, 1154.07391304348,
>     1116.19090909091, 1135.91428571429, 1172.55217391304, 1221.67142857143,
>     1091.61363636364, 1058.68181818182, 1059.33, 1032.7, 1021.58,
>     1005.62272727273, 970.138888888889, 937.961904761905, 896.928571428571,
>     853.304545454545, 830.031818181818, 799.51, 761.739130434782,
>     774.345454545454, 757.970588235294, 794.195454545454, 773.47,
>     819.995, 807.57619047619, 768.25, 769.422222222222, 795.013043478261,
>     748.809523809524, 756.17619047619, 755.086956521739, 764.566666666666,
>     794.261904761905, 782.340909090909, 785.665, 790.328571428571,
>     756.75, 776.115, 790.661904761905, 828.5, 815.2, 818.940909090909,
>     900.104347826087, 914.235, 976.757142857143, 1015.8880952381,
>     1085.7875, 1101.79565217391, 1028.9125, 1030.97894736842,
>     1018.86363636364, 988.104545454545, 976.8, 980.027272727273,
>     1066.95238095238, 1100.23181818182, 1182.1380952381, 1245.55,
>     1323.105, 1373.95714285714, 1297.80952380952, 1245.535,
> 1273.11818181818,
>     1196.85714285714, 1300.75, 1396.66363636364, 1483.21904761905,
>     1610.65, 1819.355, 2091.76666666667, 2219.745, 2427.65652173913,
>     3068.33888888889, 3544.64285714286, 3197.59090909091, 3320.73809523809,
>     3339.96590909091, 3394.05952380952, 3829.60227272727, 4378.61363636364,
>     4381.44736842105, 3784.86363636364, 3321.375, 3256.18181818182,
>     3566.85526315789, 3847.52380952381, 3628.65476190476, 3546.29545454545,
>     3244.17045454545, 2887.6, 2979.98913043478, 2554.60227272727,
>     2378.59722222222, 2364.40909090909, 2458.47619047619, 2511.18421052632,
>     2278.51136363636, 2178.325, 1906.17380952381, 1856.44565217391,
>     1734.65, 1744.52272727273, 1303.00434782609, 1169.3625,
> 1112.90476190476,
>     1202.52380952381, 1118.0025, 1223.21818181818, 1388.1375,
>     1491.89444444444, 1555.46428571429, 1582.85227272727, 1818.0125,
>     1879.14772727273, 2070.84454545455, 2196.54761904762, 2374.0380952381,
>     2414.69, 2158.82, 2277.29130434783, 2367.53, 1969.81578947368,
>     1746.51136363636, 1847.02727272727, 2047.47619047619, 2150.94318181818,
>     2373.5619047619, 2283.29545454545, 2287.32142857143, 2375.8125,
>     2473.45, 2341.47826086957, 2371.47777777778, 2159.6, 2234.46590909091,
>     2397.75476190476, 2199.23863636364, 2075.22045454545, 1871.41666666667,
>     1935.31818181818, 1911.15, 1989.22619047619, 2057.78571428571,
>     2035.925, 2002.68421052632, 1928.01136363636, 1855.93421052632,
>     1847.75, 1816.31818181818, 2009.85, 1903.95869565217, 1912.39772727273,
>     2040.42894736842, 2031.40909090909, 2128.6875, 1929.15, 1855.6,
>     1831.0119047619, 1839.0125, 1837.61956521739, 1896.39285714286,
>     1846.88095238095, 1884.83695652174, 1866.41666666667, 1974.975,
>     2036.93181818182, 2034.525, 2007.90476190476, 2027.2125,
>     2058.975, 2128.09523809524, 2310.61956521739, 2326.9875,
>     2294.59090909091, 2276.82608695652, 2253.225, 2175.7619047619,
>     2113.04761904762, 2097.7625, 2028.72727272727, 2212.725,
>     2281.80263157895, 2082.09090909091, 2000.6847826087, 1807.6375,
>     1720.22727272727, 1724.34090909091, 1583.30952380952, 84.0490040435678,
>     88.34523245295, 79.7263120909169, 75.7896575197835, 72.2667483584157,
>     69.6077343196229, 73.0554399445877, 72.8857593626663, 70.3062123746067,
>     69.4842872070072, 66.3534646563319, 63.3659416639662, 62.8447866203785,
>     62.3497532587845, 62.3821516882946, 61.3250281658416, 59.2499062965604,
>     57.3935312450936, 56.458857515431, 58.874894190462, 56.700063900645,
>     55.3309399096662, 54.0129823357043, 55.1970254133562, 55.2591751559872,
>     54.6101575179788, 51.9562396165992, 50.5856984481846, 50.1022623380861,
>     46.5344703066267, 48.2197004587016, 48.1075570319885, 47.7204634251085,
>     47.5834457918367, 47.3991025311115, 48.0478564645903, 50.3549958857265,
>     54.2731753471865, 55.8272239393016, 57.8973007561971, 60.5284130889203,
>     60.0317340701071, 61.1746115716447, 62.5411025780342, 62.3209035757259,
>     60.3668296850052, 58.837816873746, 59.5136672634343, 58.7936514922707,
>     57.777245732091, 57.9592494280561, 56.6578353675137, 54.1729975833313,
>     53.26389507043, 50.3845154305534, 49.8150177139051, 47.0834697736602,
>     46.8294038369385, 49.8745867651861, 48.5020114100248, 48.3963672456785,
>     49.0647773133828, 49.2982832362356, 50.6841081334784, 50.7860743207755,
>     48.4792122992933, 47.4868844333104, 47.5950729608497, 45.3876786412611,
>     45.4620062929568, 44.1658924977541, 45.540804173947, 47.6083100984259,
>     47.2577931211646, 48.6249885481598, 48.2464205559558, 48.2720492656695,
>     49.1181147969089, 47.2287801991246, 47.0487633277041, 48.7832346913249,
>     48.0831593754822, 47.2698633806019, 47.4716297493223, 47.8939110381669,
>     50.3829981022647, 52.7142510119066, 53.8185252384474, 55.287479758648,
>     56.8422265947404, 61.2097846377106, 64.9936836388951, 63.9347066703767,
>     69.4317321346478, 68.6790690932525, 75.2198050137077, 77.2714213747885,
>     77.8020163002564, 90.5831320826795, 91.8142075850052, 101.86611157024,
>     113.728793098488, 90.9250722654284, 93.0693473077664, 87.1911815059986,
>     89.8101403590263, 95.1967249528679, 100.754260726341, 100.522662419456,
>     95.4549627640591, 93.9658539180204, 91.4518291246892, 89.8878108648484,
>     80.5698553052007, 77.653331167409, 81.0396073930035, 78.7711941920148,
>     79.7608508936316, 74.7629120403507, 70.8898937966476, 67.4741755746967,
>     66.4068256657509, 73.8726729330384, 72.8995135939812, 73.6119520655863,
>     73.0307786078087, 74.9073835864882, 81.3266719874504, 86.709886058394,
>     79.842528057733, 71.7199429847115, 68.6021819750595, 68.5126590378746,
>     68.3620283665974, 67.9720774461766, 66.8217804899572, 63.1843971194637,
>     61.5311268030622, 61.9472473534963, 60.8672194620405, 59.9406720983444,
>     58.7736183340489, 58.8649005668352, 57.266952186169, 58.4517120537285,
>     60.6078607937412, 61.0396589773427, 62.0088818730514, 62.0948258514948,
>     62.1994651497547, 64.8143231900986, 64.6203619081302, 63.0498104227068,
>     58.8387922851814, 56.609447947545, 57.9306117223866, 57.6419617364056,
>     57.2445329684282, 55.3916926480841, 53.2713886681011, 52.0907437206014,
>     52.8851909607569, 53.6172954496014, 52.800166608016, 50.6878177805333,
>     48.9232092206821, 48.0183168793908, 50.349740800131, 52.401975

	[[alternative HTML version deleted]]


From mxfomin at gmail.com  Thu Jan 21 20:35:23 2016
From: mxfomin at gmail.com (Maxim Fomin)
Date: Thu, 21 Jan 2016 22:35:23 +0300
Subject: [R] ggplot2 - specific y axis scale for each time series in
	facet grind
In-Reply-To: <CAKVAULPSErU-722vt_nBqQNEdZfDAFAQtgxn3STFdstkHmZ_=g@mail.gmail.com>
References: <CALB30JAsRHmmmu8fKNi99K7WkPi6bZCFZ-G-_e8vy_-rQ5M_nQ@mail.gmail.com>
	<CAKVAULPSErU-722vt_nBqQNEdZfDAFAQtgxn3STFdstkHmZ_=g@mail.gmail.com>
Message-ID: <CALB30JD6bb0TVSaVW-HD4KocdwFReRbDspSHRKp6F0=79TSRaA@mail.gmail.com>

Thanks, that worked.

Best regards,
Maxim

2016-01-21 22:29 GMT+03:00 Ulrik Stervbo <ulrik.stervbo at gmail.com>:
>
> You can try facet_grid(variable~., scales = "free_y")
>
> On Thu, 21 Jan 2016 at 19:29 Maxim Fomin <mxfomin at gmail.com> wrote:
>>
>> Dear R users,
>>
>> I have a (melted) data frame of several metal prices from 1980. I want to
>> make a time series plot for each metal price. By default ggplot constructs
>> single plot for all prices. Adding +facet_grid(variable~.) makes for each
>> metal price a plot, but now there is another problem - scale of y axis.
>> Some metals have price $1000 and some $2. Because ggplot makes same y scale
>> for each metal, some metal prices printed as straight horizontal line. What
>> is the command to make for each plot specific y axis scale?
>>
>> Currently I am trying command:
>>
>> ggplot(data=melted2, aes(x=date, y=value, group=variable,
>> color=variable))+geom_line()+facet_grid(variable~.)
>>
>>
>> Best regards,
>> Maxim
>>


From pdalgd at gmail.com  Thu Jan 21 20:46:19 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 21 Jan 2016 20:46:19 +0100
Subject: [R] tcltk tkwidget(..."table")
In-Reply-To: <CAJeYpE8Bm1QRTk6F5hTuTr24t25XrG97k8Cjz0w25FOBQFaPSQ@mail.gmail.com>
References: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>
	<6FEECD0A-6905-4DEF-A72B-96F2A6FDA827@gmail.com>
	<CAJeYpE8Bm1QRTk6F5hTuTr24t25XrG97k8Cjz0w25FOBQFaPSQ@mail.gmail.com>
Message-ID: <624FF320-0BD3-492B-9FD8-20514174E3F5@gmail.com>


> On 21 Jan 2016, at 00:25 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> 
> Thanks, Peter. 
> 
> I'm sure that's right, but it requires knowing: (1) that there's something called the "width subcommand", and (2) how to format the call to that command/subcommand.
> 

Yes, there's a fair amount of that going on with the tcltk interface. You need to both grasp the rules for passing arguments to the underlying Tcl command, and know how to find and read the Tcl/Tk documentation. Once you're up to speed on those issues it's not all that hard to find stuff in (for the present case), say, http://tktable.sourceforge.net/tktable/doc/tkTable.html

The situation may be unfortunate, but the alternative is for "someone" to sit down an convert all relevant Tcl/Tk documentation to R help files.

-pd 


> I was able to do it eventually but only after a few hours of effort searching the web for help.
> 
> E.g. with a table (called table1) with 3 columns and want to set widths to 30, 5, and 5:
> 
> colwidths<-c(30, 5, 5)
> 
> for(i in 1:3) {
>   tcl(table1, "width", i - 1, colwidths[i])
> }
> 
> 
> 
> On Wed, Jan 20, 2016 at 3:07 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> > On 19 Jan 2016, at 20:48 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> >
> > Does anyone know a simple way to create a tcltk table with columns of
> > varying widths?
> 
> Create a table, then set the width of the columns with the width subcommand?
> 
> -pd
> 
> 
> pathName width ?col? ?value col value ...? If no col is specified, returns a list describing all cols for which a width has been set. If col is specified with no value, it prints out the width of that col in characters (positive number) or pixels (negative number). If one or more col-value pairs are specified, then it sets each col to be that width in characters (positive number) or pixels (negative number). If value is default, then the col uses the default width, specified by -colwidth.
> 
> 
> 
> >
> > -Dan
> >
> >
> >
> > --
> > Dan Dalthorp, PhD
> > USGS Forest and Rangeland Ecosystem Science Center
> > Forest Sciences Lab, Rm 189
> > 3200 SW Jefferson Way
> > Corvallis, OR 97331
> > ph: 541-750-0953
> > ddalthorp at usgs.gov
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> -- 
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way 
> Corvallis, OR 97331 
> ph: 541-750-0953
> ddalthorp at usgs.gov
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From istazahn at gmail.com  Thu Jan 21 21:00:06 2016
From: istazahn at gmail.com (Ista Zahn)
Date: Thu, 21 Jan 2016 15:00:06 -0500
Subject: [R] R editor for Mac
In-Reply-To: <CAJ4QxaODEzVo7H9ue02aMbXvLwYCM2HEm6zTUA0Md36Rx44=iA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<n7q0j9$ib1$1@ger.gmane.org>
	<CA+vqiLEzDV2A5yp+YvOx__H9xA0Prd=Vdnp-bf1U_ZiWPHxzGw@mail.gmail.com>
	<CAJ4QxaODEzVo7H9ue02aMbXvLwYCM2HEm6zTUA0Md36Rx44=iA@mail.gmail.com>
Message-ID: <CA+vqiLEy5vmCOEEywZ5X4HTfS-WYQVVLY_8bmn1_rioUdinGxg@mail.gmail.com>

On Thu, Jan 21, 2016 at 12:54 PM, boB Rudis <bob at rudis.net> wrote:
> Here you go Ista: https://atom.io/packages/repl (Atom rly isn't bad
> for general purpose data sci needs, I still think RStudio is the best
> environment for working with R projects).

Thanks Bob, the Atom REPL thingy is nice. I'm sticking with Emacs with
ESS (via Spacemacs) for now, but will keep an eye on Atom.

--Ista

>
> On Thu, Jan 21, 2016 at 12:48 PM, Ista Zahn <istazahn at gmail.com> wrote:
>> On Jan 21, 2016 12:01 PM, "Philippe Massicotte" <pmassicotte at hotmail.com>
>> wrote:
>>>
>>> On 01/20/2016 07:22 PM, Christofer Bogaso wrote:
>>>>
>>>> Hi,
>>>>
>>>> Could you please suggest a good R editor for Mac OS X (10.7.5)
>>>> Previously my operating system was Windows and there I used Notepad++,
>>>> I really had very nice experience with it. However I dont see any Mac
>>>> version is available for Mac.
>>>>
>>>> Appreciate your positive feedback.
>>>>
>>>> Thanks and regards,
>>>>
>>> Atom seems to be a good choice also.
>>
>> Is it? Which package(s) should I install to write and run R code in Atom?
>> Certainly I don't see anything useful out of the box.
>>
>> Best,
>> Ista
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Thu Jan 21 21:08:14 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 21 Jan 2016 21:08:14 +0100
Subject: [R] tcltk: write '[' and ']' in a table cell
In-Reply-To: <CAJeYpE-fBRuufBefF0N9WLG5iEY=19_qg+MZ1bN7WOYT-tPkRw@mail.gmail.com>
References: <CAJeYpE-fBRuufBefF0N9WLG5iEY=19_qg+MZ1bN7WOYT-tPkRw@mail.gmail.com>
Message-ID: <F03F14A9-01E9-4872-BB1F-188CAC65AD8B@gmail.com>

?Right now I?m having amnesia and d?j? vu at the same time. I think I?ve forgotten this before.? ? Steven Wright

Check if this might be the same issue as the one of embedded spaces in

https://stat.ethz.ch/pipermail/r-help/2009-January/378558.html

-pd

> On 21 Jan 2016, at 02:12 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> 
> I know it should not be difficult to write the string:
> 
> i<-4
> j<-17
> lbl<-paste0("[", i, ", ", j, "]")
> 
> # to a table, but I'm having a devil of a time trying to figure out how to
> do it.
> # the following gives lbl surrounded by braces.
> 
> tt<-tktoplevel()
> tfr <- tkframe(tt)
> tkgrid(tfr)
> junk<-tclArray()
> junk[[0,0]]<-lbl
> table1<-tkwidget(tfr,"table", rows=1, cols=1, variable=junk)
> tkgrid(table1)
> 
> 
> How to write without the braces?
> 
> Any quick suggestions would be very much appreciated...I've burned much
> time trying to figure this should-be-simple problem out.
> 
> Thanks!
> 
> 
> 
> -- 
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From wjm1 at caa.columbia.edu  Thu Jan 21 21:09:10 2016
From: wjm1 at caa.columbia.edu (William Michels)
Date: Thu, 21 Jan 2016 12:09:10 -0800
Subject: [R] R editor for Mac
In-Reply-To: <CAJ4QxaODEzVo7H9ue02aMbXvLwYCM2HEm6zTUA0Md36Rx44=iA@mail.gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<n7q0j9$ib1$1@ger.gmane.org>
	<CA+vqiLEzDV2A5yp+YvOx__H9xA0Prd=Vdnp-bf1U_ZiWPHxzGw@mail.gmail.com>
	<CAJ4QxaODEzVo7H9ue02aMbXvLwYCM2HEm6zTUA0Md36Rx44=iA@mail.gmail.com>
Message-ID: <CAA99HCzafCy7oDeTpTXAeS9wCqjUdKJx=3s3METQ7i+u_YdgGw@mail.gmail.com>

Run Atom with the language-r and r-exec packages:

"A language description and snippets for R"
https://atom.io/packages/language-r

"Send R code to various consoles"
https://atom.io/packages/r-exec


On Thu, Jan 21, 2016 at 9:54 AM, boB Rudis <bob at rudis.net> wrote:
> Here you go Ista: https://atom.io/packages/repl (Atom rly isn't bad
> for general purpose data sci needs, I still think RStudio is the best
> environment for working with R projects).
>
> On Thu, Jan 21, 2016 at 12:48 PM, Ista Zahn <istazahn at gmail.com> wrote:
>> On Jan 21, 2016 12:01 PM, "Philippe Massicotte" <pmassicotte at hotmail.com>
>> wrote:
>>>
>>> On 01/20/2016 07:22 PM, Christofer Bogaso wrote:
>>>>
>>>> Hi,
>>>>
>>>> Could you please suggest a good R editor for Mac OS X (10.7.5)
>>>> Previously my operating system was Windows and there I used Notepad++,
>>>> I really had very nice experience with it. However I dont see any Mac
>>>> version is available for Mac.
>>>>
>>>> Appreciate your positive feedback.
>>>>
>>>> Thanks and regards,
>>>>
>>> Atom seems to be a good choice also.
>>
>> Is it? Which package(s) should I install to write and run R code in Atom?
>> Certainly I don't see anything useful out of the box.
>>
>> Best,
>> Ista
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bhh at xs4all.nl  Thu Jan 21 21:43:27 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Thu, 21 Jan 2016 21:43:27 +0100
Subject: [R] R editor for Mac
In-Reply-To: <56A11E20.9050601@gmail.com>
References: <CA+dpOJnNuMy0CtwSdn3Y0jQhjgicLS0dhndWKoR-O9JVw_=xtA@mail.gmail.com>
	<569FD1D8.4040807@gmail.com> <56A11E20.9050601@gmail.com>
Message-ID: <45AE2D64-E125-4545-A3F3-7A517164D20E@xs4all.nl>


> On 21 Jan 2016, at 19:06, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 20/01/2016 1:28 PM, Duncan Murdoch wrote:
>> On 20/01/2016 1:22 PM, Christofer Bogaso wrote:
>> > Hi,
>> >
>> > Could you please suggest a good R editor for Mac OS X (10.7.5)
>> > Previously my operating system was Windows and there I used Notepad++,
>> > I really had very nice experience with it. However I dont see any Mac
>> > version is available for Mac.
>> >
>> > Appreciate your positive feedback.
>> 
>> RStudio is probably best on both OS X and Windows.    A nice advantage
>> is that it looks the same on both, so you can move back and forth.
>> 
>> I only know two negatives:
>> 
>>   - I still don't like the tiled window.  I often work on a small
>> screen, and it's not enough space.
>> 
>>   - The editor still changes file endings to native format whenever it
>> saves.  It would be better if it handled both Windows and Unix line
>> endings in both systems, and left them alone unless the user asked them
>> to be changed.
> 
> I've just heard offline from JJ Allaire that both negatives above have been addressed in a version soon to be released.   Excellent news!
> 

The Rstudio editor has a weird difference between OS X and Linux.
It has a shortcut Ctrl+K which is supposed to Delete to Line End (and not delete the Line End).

On OS X the shortcut Ctrl+K will also delete the end of line thus joining the next line with the remainder of the current line.
On Linux that doesn't happen.

I've complained about that quite some time ago but nothing seems to be done about it.

Berend

> Duncan Murdoch
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Thu Jan 21 22:35:16 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Thu, 21 Jan 2016 13:35:16 -0800
Subject: [R] tcltk tkwidget(..."table")
In-Reply-To: <624FF320-0BD3-492B-9FD8-20514174E3F5@gmail.com>
References: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>
	<6FEECD0A-6905-4DEF-A72B-96F2A6FDA827@gmail.com>
	<CAJeYpE8Bm1QRTk6F5hTuTr24t25XrG97k8Cjz0w25FOBQFaPSQ@mail.gmail.com>
	<624FF320-0BD3-492B-9FD8-20514174E3F5@gmail.com>
Message-ID: <CAJeYpE_CAX+-gOpQ1x7FLdL74_Co_wyXscVfFokP6KFf+ZYfXw@mail.gmail.com>

>  Once you're up to speed on those issues...

Any suggestions for getting up to speed on those issues?



On Thu, Jan 21, 2016 at 11:46 AM, peter dalgaard <pdalgd at gmail.com> wrote:

>
> > On 21 Jan 2016, at 00:25 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> >
> > Thanks, Peter.
> >
> > I'm sure that's right, but it requires knowing: (1) that there's
> something called the "width subcommand", and (2) how to format the call to
> that command/subcommand.
> >
>
> Yes, there's a fair amount of that going on with the tcltk interface. You
> need to both grasp the rules for passing arguments to the underlying Tcl
> command, and know how to find and read the Tcl/Tk documentation. Once
> you're up to speed on those issues it's not all that hard to find stuff in
> (for the present case), say,
> http://tktable.sourceforge.net/tktable/doc/tkTable.html
>
> The situation may be unfortunate, but the alternative is for "someone" to
> sit down an convert all relevant Tcl/Tk documentation to R help files.
>
> -pd
>
>
> > I was able to do it eventually but only after a few hours of effort
> searching the web for help.
> >
> > E.g. with a table (called table1) with 3 columns and want to set widths
> to 30, 5, and 5:
> >
> > colwidths<-c(30, 5, 5)
> >
> > for(i in 1:3) {
> >   tcl(table1, "width", i - 1, colwidths[i])
> > }
> >
> >
> >
> > On Wed, Jan 20, 2016 at 3:07 PM, peter dalgaard <pdalgd at gmail.com>
> wrote:
> >
> > > On 19 Jan 2016, at 20:48 , Dalthorp, Daniel <ddalthorp at usgs.gov>
> wrote:
> > >
> > > Does anyone know a simple way to create a tcltk table with columns of
> > > varying widths?
> >
> > Create a table, then set the width of the columns with the width
> subcommand?
> >
> > -pd
> >
> >
> > pathName width ?col? ?value col value ...? If no col is specified,
> returns a list describing all cols for which a width has been set. If col
> is specified with no value, it prints out the width of that col in
> characters (positive number) or pixels (negative number). If one or more
> col-value pairs are specified, then it sets each col to be that width in
> characters (positive number) or pixels (negative number). If value is
> default, then the col uses the default width, specified by -colwidth.
> >
> >
> >
> > >
> > > -Dan
> > >
> > >
> > >
> > > --
> > > Dan Dalthorp, PhD
> > > USGS Forest and Rangeland Ecosystem Science Center
> > > Forest Sciences Lab, Rm 189
> > > 3200 SW Jefferson Way
> > > Corvallis, OR 97331
> > > ph: 541-750-0953
> > > ddalthorp at usgs.gov
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Peter Dalgaard, Professor,
> > Center for Statistics, Copenhagen Business School
> > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > Phone: (+45)38153501
> > Office: A 4.23
> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > --
> > Dan Dalthorp, PhD
> > USGS Forest and Rangeland Ecosystem Science Center
> > Forest Sciences Lab, Rm 189
> > 3200 SW Jefferson Way
> > Corvallis, OR 97331
> > ph: 541-750-0953
> > ddalthorp at usgs.gov
> >
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Thu Jan 21 22:44:07 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 22 Jan 2016 08:44:07 +1100
Subject: [R] Histogram with a wide range of numbers.
In-Reply-To: <CAAir+CrCjzMc1MgLKNS5rFbsZ+dLYNapM9fcEctk_MB7ieiBqA@mail.gmail.com>
References: <CAAir+CrCjzMc1MgLKNS5rFbsZ+dLYNapM9fcEctk_MB7ieiBqA@mail.gmail.com>
Message-ID: <CA+8X3fWU+=7EOi8rf0VgC-rxqSQ0hsgCQywojWs60ZOaXJ69rw@mail.gmail.com>

Hi Sema,
I trimmed your file to the first 220 lines.

ads<-read.table("all_data_scor.txt",header=TRUE,sep="\t")
ads.tab<-table(cut(all_data_scor[,2],breaks=c(0,1e-100,1e-10,1e-1,1,10)))
barplot(adt.tab)

This gives you a basic idea of what can be done. If this is not clear, ask
again.

Jim


On Thu, Jan 21, 2016 at 8:16 PM, Sema Atasever <s.atasever at gmail.com> wrote:

> Dear Authorized Sir / Madam,
>
> I need your opinion on something R.
> I have a sample text file that includes eValue scores (2nd column).
> This column includes a wide range of numbers. For example: 1e-179 or 9.9
> You can find this text file in the attachment.
>
> How do I create a histogram in R with logarithmic scale or
> which method do i use these values which includes with a wide range of
> numbers.
>
> I would appreciate if you could advise on some methods.
>
> Thanks.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Thu Jan 21 22:49:54 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Thu, 21 Jan 2016 13:49:54 -0800
Subject: [R] tcltk: write '[' and ']' in a table cell
In-Reply-To: <F03F14A9-01E9-4872-BB1F-188CAC65AD8B@gmail.com>
References: <CAJeYpE-fBRuufBefF0N9WLG5iEY=19_qg+MZ1bN7WOYT-tPkRw@mail.gmail.com>
	<F03F14A9-01E9-4872-BB1F-188CAC65AD8B@gmail.com>
Message-ID: <CAJeYpE_Z1Pbua=ZeEFqsTD3RCgyujCyt2kACuC48xtVNiwPebA@mail.gmail.com>

Many thanks, Peter.

Simple, works perfectly, and nigh on impossible to figure out without your
help.

To summarize...in my example, I need to do:

junk[[0,0]] <- as.tclObj(lbl,drop=T) # instead of junk[[0,0]]<-lbl

It works for spaces, brackets, parentheses, and all manner of special
characters.

-Dan

On Thu, Jan 21, 2016 at 12:08 PM, peter dalgaard <pdalgd at gmail.com> wrote:

> ?Right now I?m having amnesia and d?j? vu at the same time. I think I?ve
> forgotten this before.? ? Steven Wright
>
> Check if this might be the same issue as the one of embedded spaces in
>
> https://stat.ethz.ch/pipermail/r-help/2009-January/378558.html
>
> -pd
>
> > On 21 Jan 2016, at 02:12 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> >
> > I know it should not be difficult to write the string:
> >
> > i<-4
> > j<-17
> > lbl<-paste0("[", i, ", ", j, "]")
> >
> > # to a table, but I'm having a devil of a time trying to figure out how
> to
> > do it.
> > # the following gives lbl surrounded by braces.
> >
> > tt<-tktoplevel()
> > tfr <- tkframe(tt)
> > tkgrid(tfr)
> > junk<-tclArray()
> > junk[[0,0]]<-lbl
> > table1<-tkwidget(tfr,"table", rows=1, cols=1, variable=junk)
> > tkgrid(table1)
> >
> >
> > How to write without the braces?
> >
> > Any quick suggestions would be very much appreciated...I've burned much
> > time trying to figure this should-be-simple problem out.
> >
> > Thanks!
> >
> >
> >
> > --
> > Dan Dalthorp, PhD
> > USGS Forest and Rangeland Ecosystem Science Center
> > Forest Sciences Lab, Rm 189
> > 3200 SW Jefferson Way
> > Corvallis, OR 97331
> > ph: 541-750-0953
> > ddalthorp at usgs.gov
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From amoy_y at yahoo.com  Fri Jan 22 01:39:23 2016
From: amoy_y at yahoo.com (Amoy Yang)
Date: Fri, 22 Jan 2016 00:39:23 +0000 (UTC)
Subject: [R] Error opening SHP file
References: <853310330.7127045.1453423163693.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <853310330.7127045.1453423163693.JavaMail.yahoo@mail.yahoo.com>

 Any advice for the following errors?
state.map <- readShapeSpatial("maps/st24_d00.shp")
Error in getinfo.shape(fn) : Error opening SHP file

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Jan 22 02:33:36 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 21 Jan 2016 17:33:36 -0800
Subject: [R] ggplot2 - specific y axis scale for each time series in
	facet	grind
In-Reply-To: <CALB30JAsRHmmmu8fKNi99K7WkPi6bZCFZ-G-_e8vy_-rQ5M_nQ@mail.gmail.com>
References: <CALB30JAsRHmmmu8fKNi99K7WkPi6bZCFZ-G-_e8vy_-rQ5M_nQ@mail.gmail.com>
Message-ID: <CAB27FFB-8CB6-4A6C-A6B7-D54A9154FE0D@dcn.davis.ca.us>

> What is the command to make for 
> each plot specific y axis scale?

Read the documentation for ?facet_grid... in particular the scales argument. 
-- 
Sent from my phone. Please excuse my brevity.

On January 21, 2016 7:15:46 AM PST, Maxim Fomin <mxfomin at gmail.com> wrote:
>Dear R users,
>
>I have a (melted) data frame of several metal prices from 1980. I want
>to
>make a time series plot for each metal price. By default ggplot
>constructs
>single plot for all prices. Adding +facet_grid(variable~.) makes for
>each
>metal price a plot, but now there is another problem - scale of y axis.
>Some metals have price $1000 and some $2. Because ggplot makes same y
>scale
>for each metal, some metal prices printed as straight horizontal line.
>What
>is the command to make for each plot specific y axis scale?
>
>Currently I am trying command:
>
>ggplot(data=melted2, aes(x=date, y=value, group=variable,
>color=variable))+geom_line()+facet_grid(variable~.)
>
>
>Best regards,
>Maxim
>
>P.S dput
>
>structure(list(date = structure(c(3652, 3683, 3712, 3743, 3773,
>3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
>4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
>4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
>4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
>5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
>5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
>5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
>6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
>6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
>6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
>7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
>7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
>7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
>8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
>8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
>8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
>9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
>9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
>9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
>10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
>10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
>10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
>10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
>11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
>11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
>11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
>12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
>12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
>12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
>12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
>13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
>13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
>13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
>13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
>14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
>14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
>14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
>15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
>15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
>15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
>15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
>16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
>16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
>16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
>3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
>4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
>4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
>4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
>5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
>5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
>5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
>6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
>6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
>6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
>7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
>7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
>7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
>8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
>8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
>8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
>9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
>9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
>9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
>10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
>10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
>10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
>11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
>11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
>11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
>11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
>12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
>12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
>12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
>12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
>13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
>13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
>13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
>14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
>14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
>14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
>14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
>15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
>15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
>15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
>15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
>16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
>16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740, 3652,
>3683, 3712, 3743, 3773, 3804, 3834, 3865, 3896, 3926, 3957, 3987,
>4018, 4049, 4077, 4108, 4138, 4169, 4199, 4230, 4261, 4291, 4322,
>4352, 4383, 4414, 4442, 4473, 4503, 4534, 4564, 4595, 4626, 4656,
>4687, 4717, 4748, 4779, 4807, 4838, 4868, 4899, 4929, 4960, 4991,
>5021, 5052, 5082, 5113, 5144, 5173, 5204, 5234, 5265, 5295, 5326,
>5357, 5387, 5418, 5448, 5479, 5510, 5538, 5569, 5599, 5630, 5660,
>5691, 5722, 5752, 5783, 5813, 5844, 5875, 5903, 5934, 5964, 5995,
>6025, 6056, 6087, 6117, 6148, 6178, 6209, 6240, 6268, 6299, 6329,
>6360, 6390, 6421, 6452, 6482, 6513, 6543, 6574, 6605, 6634, 6665,
>6695, 6726, 6756, 6787, 6818, 6848, 6879, 6909, 6940, 6971, 6999,
>7030, 7060, 7091, 7121, 7152, 7183, 7213, 7244, 7274, 7305, 7336,
>7364, 7395, 7425, 7456, 7486, 7517, 7548, 7578, 7609, 7639, 7670,
>7701, 7729, 7760, 7790, 7821, 7851, 7882, 7913, 7943, 7974, 8004,
>8035, 8066, 8095, 8126, 8156, 8187, 8217, 8248, 8279, 8309, 8340,
>8370, 8401, 8432, 8460, 8491, 8521, 8552, 8582, 8613, 8644, 8674,
>8705, 8735, 8766, 8797, 8825, 8856, 8886, 8917, 8947, 8978, 9009,
>9039, 9070, 9100, 9131, 9162, 9190, 9221, 9251, 9282, 9312, 9343,
>9374, 9404, 9435, 9465, 9496, 9527, 9556, 9587, 9617, 9648, 9678,
>9709, 9740, 9770, 9801, 9831, 9862, 9893, 9921, 9952, 9982, 10013,
>10043, 10074, 10105, 10135, 10166, 10196, 10227, 10258, 10286,
>10317, 10347, 10378, 10408, 10439, 10470, 10500, 10531, 10561,
>10592, 10623, 10651, 10682, 10712, 10743, 10773, 10804, 10835,
>10865, 10896, 10926, 10957, 10988, 11017, 11048, 11078, 11109,
>11139, 11170, 11201, 11231, 11262, 11292, 11323, 11354, 11382,
>11413, 11443, 11474, 11504, 11535, 11566, 11596, 11627, 11657,
>11688, 11719, 11747, 11778, 11808, 11839, 11869, 11900, 11931,
>11961, 11992, 12022, 12053, 12084, 12112, 12143, 12173, 12204,
>12234, 12265, 12296, 12326, 12357, 12387, 12418, 12449, 12478,
>12509, 12539, 12570, 12600, 12631, 12662, 12692, 12723, 12753,
>12784, 12815, 12843, 12874, 12904, 12935, 12965, 12996, 13027,
>13057, 13088, 13118, 13149, 13180, 13208, 13239, 13269, 13300,
>13330, 13361, 13392, 13422, 13453, 13483, 13514, 13545, 13573,
>13604, 13634, 13665, 13695, 13726, 13757, 13787, 13818, 13848,
>13879, 13910, 13939, 13970, 14000, 14031, 14061, 14092, 14123,
>14153, 14184, 14214, 14245, 14276, 14304, 14335, 14365, 14396,
>14426, 14457, 14488, 14518, 14549, 14579, 14610, 14641, 14669,
>14700, 14730, 14761, 14791, 14822, 14853, 14883, 14914, 14944,
>14975, 15006, 15034, 15065, 15095, 15126, 15156, 15187, 15218,
>15248, 15279, 15309, 15340, 15371, 15400, 15431, 15461, 15492,
>15522, 15553, 15584, 15614, 15645, 15675, 15706, 15737, 15765,
>15796, 15826, 15857, 15887, 15918, 15949, 15979, 16010, 16040,
>16071, 16102, 16130, 16161, 16191, 16222, 16252, 16283, 16314,
>16344, 16375, 16405, 16436, 16467, 16495, 16526, 16556, 16587,
>16617, 16648, 16679, 16709, 16740, 3652, 3683, 3712, 3743, 3773,
>3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
>4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
>4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
>4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
>5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
>5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
>5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
>6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
>6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
>6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
>7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
>7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
>7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
>8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
>8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
>8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
>9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
>9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
>9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
>10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
>10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
>10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
>10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
>11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
>11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
>11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
>12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
>12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
>12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
>12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
>13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
>13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
>13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
>13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
>14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
>14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
>14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
>15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
>15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
>15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
>15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
>16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
>16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
>16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
>3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
>4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
>4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
>4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
>5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
>5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
>5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
>6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
>6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
>6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
>7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
>7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
>7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
>8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
>8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
>8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
>9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
>9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
>9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
>10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
>10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
>10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
>11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
>11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
>11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
>11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
>12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
>12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
>12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
>12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
>13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
>13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
>13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
>14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
>14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
>14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
>14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
>15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
>15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
>15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
>15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
>16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
>16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740, 3652,
>3683, 3712, 3743, 3773, 3804, 3834, 3865, 3896, 3926, 3957, 3987,
>4018, 4049, 4077, 4108, 4138, 4169, 4199, 4230, 4261, 4291, 4322,
>4352, 4383, 4414, 4442, 4473, 4503, 4534, 4564, 4595, 4626, 4656,
>4687, 4717, 4748, 4779, 4807, 4838, 4868, 4899, 4929, 4960, 4991,
>5021, 5052, 5082, 5113, 5144, 5173, 5204, 5234, 5265, 5295, 5326,
>5357, 5387, 5418, 5448, 5479, 5510, 5538, 5569, 5599, 5630, 5660,
>5691, 5722, 5752, 5783, 5813, 5844, 5875, 5903, 5934, 5964, 5995,
>6025, 6056, 6087, 6117, 6148, 6178, 6209, 6240, 6268, 6299, 6329,
>6360, 6390, 6421, 6452, 6482, 6513, 6543, 6574, 6605, 6634, 6665,
>6695, 6726, 6756, 6787, 6818, 6848, 6879, 6909, 6940, 6971, 6999,
>7030, 7060, 7091, 7121, 7152, 7183, 7213, 7244, 7274, 7305, 7336,
>7364, 7395, 7425, 7456, 7486, 7517, 7548, 7578, 7609, 7639, 7670,
>7701, 7729, 7760, 7790, 7821, 7851, 7882, 7913, 7943, 7974, 8004,
>8035, 8066, 8095, 8126, 8156, 8187, 8217, 8248, 8279, 8309, 8340,
>8370, 8401, 8432, 8460, 8491, 8521, 8552, 8582, 8613, 8644, 8674,
>8705, 8735, 8766, 8797, 8825, 8856, 8886, 8917, 8947, 8978, 9009,
>9039, 9070, 9100, 9131, 9162, 9190, 9221, 9251, 9282, 9312, 9343,
>9374, 9404, 9435, 9465, 9496, 9527, 9556, 9587, 9617, 9648, 9678,
>9709, 9740, 9770, 9801, 9831, 9862, 9893, 9921, 9952, 9982, 10013,
>10043, 10074, 10105, 10135, 10166, 10196, 10227, 10258, 10286,
>10317, 10347, 10378, 10408, 10439, 10470, 10500, 10531, 10561,
>10592, 10623, 10651, 10682, 10712, 10743, 10773, 10804, 10835,
>10865, 10896, 10926, 10957, 10988, 11017, 11048, 11078, 11109,
>11139, 11170, 11201, 11231, 11262, 11292, 11323, 11354, 11382,
>11413, 11443, 11474, 11504, 11535, 11566, 11596, 11627, 11657,
>11688, 11719, 11747, 11778, 11808, 11839, 11869, 11900, 11931,
>11961, 11992, 12022, 12053, 12084, 12112, 12143, 12173, 12204,
>12234, 12265, 12296, 12326, 12357, 12387, 12418, 12449, 12478,
>12509, 12539, 12570, 12600, 12631, 12662, 12692, 12723, 12753,
>12784, 12815, 12843, 12874, 12904, 12935, 12965, 12996, 13027,
>13057, 13088, 13118, 13149, 13180, 13208, 13239, 13269, 13300,
>13330, 13361, 13392, 13422, 13453, 13483, 13514, 13545, 13573,
>13604, 13634, 13665, 13695, 13726, 13757, 13787, 13818, 13848,
>13879, 13910, 13939, 13970, 14000, 14031, 14061, 14092, 14123,
>14153, 14184, 14214, 14245, 14276, 14304, 14335, 14365, 14396,
>14426, 14457, 14488, 14518, 14549, 14579, 14610, 14641, 14669,
>14700, 14730, 14761, 14791, 14822, 14853, 14883, 14914, 14944,
>14975, 15006, 15034, 15065, 15095, 15126, 15156, 15187, 15218,
>15248, 15279, 15309, 15340, 15371, 15400, 15431, 15461, 15492,
>15522, 15553, 15584, 15614, 15645, 15675, 15706, 15737, 15765,
>15796, 15826, 15857, 15887, 15918, 15949, 15979, 16010, 16040,
>16071, 16102, 16130, 16161, 16191, 16222, 16252, 16283, 16314,
>16344, 16375, 16405, 16436, 16467, 16495, 16526, 16556, 16587,
>16617, 16648, 16679, 16709, 16740, 3652, 3683, 3712, 3743, 3773,
>3804, 3834, 3865, 3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108,
>4138, 4169, 4199, 4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442,
>4473, 4503, 4534, 4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779,
>4807, 4838, 4868, 4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113,
>5144, 5173, 5204, 5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448,
>5479, 5510, 5538, 5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783,
>5813, 5844, 5875, 5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117,
>6148, 6178, 6209, 6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452,
>6482, 6513, 6543, 6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787,
>6818, 6848, 6879, 6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121,
>7152, 7183, 7213, 7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456,
>7486, 7517, 7548, 7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790,
>7821, 7851, 7882, 7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126,
>8156, 8187, 8217, 8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460,
>8491, 8521, 8552, 8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797,
>8825, 8856, 8886, 8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131,
>9162, 9190, 9221, 9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465,
>9496, 9527, 9556, 9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801,
>9831, 9862, 9893, 9921, 9952, 9982, 10013, 10043, 10074, 10105,
>10135, 10166, 10196, 10227, 10258, 10286, 10317, 10347, 10378,
>10408, 10439, 10470, 10500, 10531, 10561, 10592, 10623, 10651,
>10682, 10712, 10743, 10773, 10804, 10835, 10865, 10896, 10926,
>10957, 10988, 11017, 11048, 11078, 11109, 11139, 11170, 11201,
>11231, 11262, 11292, 11323, 11354, 11382, 11413, 11443, 11474,
>11504, 11535, 11566, 11596, 11627, 11657, 11688, 11719, 11747,
>11778, 11808, 11839, 11869, 11900, 11931, 11961, 11992, 12022,
>12053, 12084, 12112, 12143, 12173, 12204, 12234, 12265, 12296,
>12326, 12357, 12387, 12418, 12449, 12478, 12509, 12539, 12570,
>12600, 12631, 12662, 12692, 12723, 12753, 12784, 12815, 12843,
>12874, 12904, 12935, 12965, 12996, 13027, 13057, 13088, 13118,
>13149, 13180, 13208, 13239, 13269, 13300, 13330, 13361, 13392,
>13422, 13453, 13483, 13514, 13545, 13573, 13604, 13634, 13665,
>13695, 13726, 13757, 13787, 13818, 13848, 13879, 13910, 13939,
>13970, 14000, 14031, 14061, 14092, 14123, 14153, 14184, 14214,
>14245, 14276, 14304, 14335, 14365, 14396, 14426, 14457, 14488,
>14518, 14549, 14579, 14610, 14641, 14669, 14700, 14730, 14761,
>14791, 14822, 14853, 14883, 14914, 14944, 14975, 15006, 15034,
>15065, 15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309,
>15340, 15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584,
>15614, 15645, 15675, 15706, 15737, 15765, 15796, 15826, 15857,
>15887, 15918, 15949, 15979, 16010, 16040, 16071, 16102, 16130,
>16161, 16191, 16222, 16252, 16283, 16314, 16344, 16375, 16405,
>16436, 16467, 16495, 16526, 16556, 16587, 16617, 16648, 16679,
>16709, 16740, 3652, 3683, 3712, 3743, 3773, 3804, 3834, 3865,
>3896, 3926, 3957, 3987, 4018, 4049, 4077, 4108, 4138, 4169, 4199,
>4230, 4261, 4291, 4322, 4352, 4383, 4414, 4442, 4473, 4503, 4534,
>4564, 4595, 4626, 4656, 4687, 4717, 4748, 4779, 4807, 4838, 4868,
>4899, 4929, 4960, 4991, 5021, 5052, 5082, 5113, 5144, 5173, 5204,
>5234, 5265, 5295, 5326, 5357, 5387, 5418, 5448, 5479, 5510, 5538,
>5569, 5599, 5630, 5660, 5691, 5722, 5752, 5783, 5813, 5844, 5875,
>5903, 5934, 5964, 5995, 6025, 6056, 6087, 6117, 6148, 6178, 6209,
>6240, 6268, 6299, 6329, 6360, 6390, 6421, 6452, 6482, 6513, 6543,
>6574, 6605, 6634, 6665, 6695, 6726, 6756, 6787, 6818, 6848, 6879,
>6909, 6940, 6971, 6999, 7030, 7060, 7091, 7121, 7152, 7183, 7213,
>7244, 7274, 7305, 7336, 7364, 7395, 7425, 7456, 7486, 7517, 7548,
>7578, 7609, 7639, 7670, 7701, 7729, 7760, 7790, 7821, 7851, 7882,
>7913, 7943, 7974, 8004, 8035, 8066, 8095, 8126, 8156, 8187, 8217,
>8248, 8279, 8309, 8340, 8370, 8401, 8432, 8460, 8491, 8521, 8552,
>8582, 8613, 8644, 8674, 8705, 8735, 8766, 8797, 8825, 8856, 8886,
>8917, 8947, 8978, 9009, 9039, 9070, 9100, 9131, 9162, 9190, 9221,
>9251, 9282, 9312, 9343, 9374, 9404, 9435, 9465, 9496, 9527, 9556,
>9587, 9617, 9648, 9678, 9709, 9740, 9770, 9801, 9831, 9862, 9893,
>9921, 9952, 9982, 10013, 10043, 10074, 10105, 10135, 10166, 10196,
>10227, 10258, 10286, 10317, 10347, 10378, 10408, 10439, 10470,
>10500, 10531, 10561, 10592, 10623, 10651, 10682, 10712, 10743,
>10773, 10804, 10835, 10865, 10896, 10926, 10957, 10988, 11017,
>11048, 11078, 11109, 11139, 11170, 11201, 11231, 11262, 11292,
>11323, 11354, 11382, 11413, 11443, 11474, 11504, 11535, 11566,
>11596, 11627, 11657, 11688, 11719, 11747, 11778, 11808, 11839,
>11869, 11900, 11931, 11961, 11992, 12022, 12053, 12084, 12112,
>12143, 12173, 12204, 12234, 12265, 12296, 12326, 12357, 12387,
>12418, 12449, 12478, 12509, 12539, 12570, 12600, 12631, 12662,
>12692, 12723, 12753, 12784, 12815, 12843, 12874, 12904, 12935,
>12965, 12996, 13027, 13057, 13088, 13118, 13149, 13180, 13208,
>13239, 13269, 13300, 13330, 13361, 13392, 13422, 13453, 13483,
>13514, 13545, 13573, 13604, 13634, 13665, 13695, 13726, 13757,
>13787, 13818, 13848, 13879, 13910, 13939, 13970, 14000, 14031,
>14061, 14092, 14123, 14153, 14184, 14214, 14245, 14276, 14304,
>14335, 14365, 14396, 14426, 14457, 14488, 14518, 14549, 14579,
>14610, 14641, 14669, 14700, 14730, 14761, 14791, 14822, 14853,
>14883, 14914, 14944, 14975, 15006, 15034, 15065, 15095, 15126,
>15156, 15187, 15218, 15248, 15279, 15309, 15340, 15371, 15400,
>15431, 15461, 15492, 15522, 15553, 15584, 15614, 15645, 15675,
>15706, 15737, 15765, 15796, 15826, 15857, 15887, 15918, 15949,
>15979, 16010, 16040, 16071, 16102, 16130, 16161, 16191, 16222,
>16252, 16283, 16314, 16344, 16375, 16405, 16436, 16467, 16495,
>16526, 16556, 16587, 16617, 16648, 16679, 16709, 16740), class =
>"Date"),
>    variable = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>    1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>    4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>    5L, 5L, 5L, 5L,

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Jan 22 02:38:28 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 22 Jan 2016 12:38:28 +1100
Subject: [R] Error opening SHP file
In-Reply-To: <853310330.7127045.1453423163693.JavaMail.yahoo@mail.yahoo.com>
References: <853310330.7127045.1453423163693.JavaMail.yahoo.ref@mail.yahoo.com>
	<853310330.7127045.1453423163693.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+8X3fVn3QyAvEXPquueY33Jb=XcUTyii-SuadphvZf0LVXcHQ@mail.gmail.com>

Hi Amoy,
Another mystery question. I should have pursued parapsychology. That error
statement is usually followed by one noting that the file specified cannot
be found. Today's guess is that the R working directory:

getwd()

did not have a "maps" directory below it.

Jim


On Fri, Jan 22, 2016 at 11:39 AM, Amoy Yang via R-help <r-help at r-project.org
> wrote:

>  Any advice for the following errors?
> state.map <- readShapeSpatial("maps/st24_d00.shp")
> Error in getinfo.shape(fn) : Error opening SHP file
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jan 22 02:40:29 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 21 Jan 2016 17:40:29 -0800
Subject: [R] Error opening SHP file
In-Reply-To: <853310330.7127045.1453423163693.JavaMail.yahoo@mail.yahoo.com>
References: <853310330.7127045.1453423163693.JavaMail.yahoo.ref@mail.yahoo.com>
	<853310330.7127045.1453423163693.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1443AF37-DC8C-4EB6-9AF8-15EBE25EE77B@comcast.net>


> On Jan 21, 2016, at 4:39 PM, Amoy Yang via R-help <r-help at r-project.org> wrote:
> 
> Any advice for the following errors?
> state.map <- readShapeSpatial("maps/st24_d00.shp")
> Error in getinfo.shape(fn) : Error opening SHP file

What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?


-- 
David.

> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bob at rudis.net  Fri Jan 22 03:25:41 2016
From: bob at rudis.net (boB Rudis)
Date: Thu, 21 Jan 2016 21:25:41 -0500
Subject: [R] Error opening SHP file
In-Reply-To: <1443AF37-DC8C-4EB6-9AF8-15EBE25EE77B@comcast.net>
References: <853310330.7127045.1453423163693.JavaMail.yahoo.ref@mail.yahoo.com>
	<853310330.7127045.1453423163693.JavaMail.yahoo@mail.yahoo.com>
	<1443AF37-DC8C-4EB6-9AF8-15EBE25EE77B@comcast.net>
Message-ID: <CAJ4QxaP6dXXQ3=cne7DdsJ2kYDw9RxW_OMTeCS13czUA=joSvw@mail.gmail.com>

Agreed with the others. After finding that shapefile and getting it to
work you are definitely not in the proper working directory.

On Thu, Jan 21, 2016 at 8:40 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Jan 21, 2016, at 4:39 PM, Amoy Yang via R-help <r-help at r-project.org> wrote:
>>
>> Any advice for the following errors?
>> state.map <- readShapeSpatial("maps/st24_d00.shp")
>> Error in getinfo.shape(fn) : Error opening SHP file
>
> What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?
>
>
> --
> David.
>
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From teotjunk at hotmail.com  Fri Jan 22 08:29:26 2016
From: teotjunk at hotmail.com (TJUN KIAT TEO)
Date: Fri, 22 Jan 2016 15:29:26 +0800
Subject: [R] Lists heading in an array of lists in R
Message-ID: <SNT152-W82E0C47C543E3E6DC14E0CDFC40@phx.gbl>

I am trying to populate an array of lists in R . Here is my code

TunePar<-matrix(list(Null),2,2)
  
TunePar[1,1]=list(G=2)

But when I type TunePar[1,1,], all I get is 2. The G has disappeared. why?

If I do this

Test=list(G=2)
> Test
$G
[1] 2



 		 	   		  
	[[alternative HTML version deleted]]


From toth.denes at ttk.mta.hu  Fri Jan 22 09:45:51 2016
From: toth.denes at ttk.mta.hu (=?ISO-8859-1?Q?D=E9nes_T=F3th?=)
Date: Fri, 22 Jan 2016 09:45:51 +0100
Subject: [R] Lists heading in an array of lists in R
In-Reply-To: <SNT152-W82E0C47C543E3E6DC14E0CDFC40@phx.gbl>
References: <SNT152-W82E0C47C543E3E6DC14E0CDFC40@phx.gbl>
Message-ID: <56A1EC3F.70503@ttk.mta.hu>

Hi,

Provide a list of a list in the second assignment:

--
TunePar <- matrix(list(NULL), 2, 2)
TunePar[2,1] <- list(list(G = 2))
TunePar[2,1]
TunePar[2,1][[1]]$G
TunePar[[2]]$G
---

The point is that "[" returns the list element of the same level as the 
original object (TunePar in the present example). So in the assignment 
if you extracted one element on the LHS, it expects a one-element vector 
on the RHS, e.g., check this out:
---
# this throws an error
TunePar[1,2] <- list(H = 1:3, I = letters[1:2])

# this is fine
TunePar[1,2] <- list(list(H = 1:3, I = letters[1:2]))
TunePar[1,2]
TunePar[1,2][[1]]$I
---

HTH,
  Denes



On 01/22/2016 08:29 AM, TJUN KIAT TEO wrote:
> TunePar<-matrix(list(Null),2,2)
>
> TunePar[1,1]=list(G=2)


From murdoch.duncan at gmail.com  Fri Jan 22 09:45:56 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 22 Jan 2016 03:45:56 -0500
Subject: [R] Lists heading in an array of lists in R
In-Reply-To: <SNT152-W82E0C47C543E3E6DC14E0CDFC40@phx.gbl>
References: <SNT152-W82E0C47C543E3E6DC14E0CDFC40@phx.gbl>
Message-ID: <56A1EC44.3010808@gmail.com>

On 22/01/2016 2:29 AM, TJUN KIAT TEO wrote:
> I am trying to populate an array of lists in R . Here is my code
>
> TunePar<-matrix(list(Null),2,2)
>
> TunePar[1,1]=list(G=2)
>
> But when I type TunePar[1,1,], all I get is 2. The G has disappeared. why?
>
> If I do this
>
> Test=list(G=2)
>> Test
> $G
> [1] 2
>

Matrices generally don't keep the names of elements assigned to them. 
What you did is similar to

x <- matrix(0, 2,2)
x[1,1] <- c(G=2)
x

which loses the name.

Working with lists is different, because you need to distinguish between 
subsets and elements.  So you'd get what you want by assigning your list 
to the element using

TunePar[[1,1]] <- list(G=2)

At this point, TunePar[1,1] is a list containing list(G=2): it prints as

[[1]]
[[1]]$G
[1] 1


To get the result you want, you also need to access the element instead 
of the subset:  TunePar[[1,1]] will print

$G
[1] 1

Duncan Murdoch


From alaios at yahoo.com  Fri Jan 22 10:18:45 2016
From: alaios at yahoo.com (Alaios)
Date: Fri, 22 Jan 2016 09:18:45 +0000 (UTC)
Subject: [R] getting values from php or javascript
References: <1218519431.7336417.1453454325563.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1218519431.7336417.1453454325563.JavaMail.yahoo@mail.yahoo.com>

Dear all,I would like to execute some php or javascripts I found on the web.

see at middle of this page towards bottom
http://www.howtocreate.co.uk/php/gridref.php#samples

Is there any way I can call the php function for example directly from R?
I would like to thank you in advance for your replyRegardsAlex


	[[alternative HTML version deleted]]


From pushan.zoology at gmail.com  Fri Jan 22 11:46:02 2016
From: pushan.zoology at gmail.com (pushan chakraborty)
Date: Fri, 22 Jan 2016 16:16:02 +0530
Subject: [R] calculating functional complimentarity
Message-ID: <CAAATBeDy-ik6fMRQ9GmDm+BS-K1iZH0DhvHecoQnkh7kL5iUTQ@mail.gmail.com>

hi group

How 'functional complimentarity' of a bipartite network can be calculated
using the package 'bipartite'
-- 
Pushan Chakraborty
CSIR - SRF

Center for Pollination Studies, University of Calcutta
35, Ballyguanje Circular Road, Kolkata - 700019
          &
Wildlife Institute of India
Chandrabani, Dehradun - 248001

webpage:

http://cpscu.in/?page_id=51

Skype: cpushan

	[[alternative HTML version deleted]]


From adrian.waddell at gmail.com  Fri Jan 22 11:49:44 2016
From: adrian.waddell at gmail.com (Adrian Waddell)
Date: Fri, 22 Jan 2016 11:49:44 +0100
Subject: [R] tcltk tkwidget(..."table")
In-Reply-To: <CAJeYpE_CAX+-gOpQ1x7FLdL74_Co_wyXscVfFokP6KFf+ZYfXw@mail.gmail.com>
References: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>
	<6FEECD0A-6905-4DEF-A72B-96F2A6FDA827@gmail.com>
	<CAJeYpE8Bm1QRTk6F5hTuTr24t25XrG97k8Cjz0w25FOBQFaPSQ@mail.gmail.com>
	<624FF320-0BD3-492B-9FD8-20514174E3F5@gmail.com>
	<CAJeYpE_CAX+-gOpQ1x7FLdL74_Co_wyXscVfFokP6KFf+ZYfXw@mail.gmail.com>
Message-ID: <CACa4aQ5HUZn_7QWFP3d0v1X5DTPZtunZXyecpm-_Wkf=WL_Gqg@mail.gmail.com>

I have written some content about the R bindings to Tcl and Tk:

http://waddella.github.io/loon/learn_R_tcltk.html

And the pack geometry manager

http://adrian.waddell.ch/EssentialSoftware/Rtcltk_geometry.pdf

But if you have no experience with building graphical user interfaces then
you might want to look into a book on Tk. "Effective TCL/TK programming" by
Harrison and McLennan is a good one.

Greetings,

Adrian
On Jan 21, 2016 10:36 PM, "Dalthorp, Daniel" <ddalthorp at usgs.gov> wrote:

> >  Once you're up to speed on those issues...
>
> Any suggestions for getting up to speed on those issues?
>
>
>
> On Thu, Jan 21, 2016 at 11:46 AM, peter dalgaard <pdalgd at gmail.com> wrote:
>
> >
> > > On 21 Jan 2016, at 00:25 , Dalthorp, Daniel <ddalthorp at usgs.gov>
> wrote:
> > >
> > > Thanks, Peter.
> > >
> > > I'm sure that's right, but it requires knowing: (1) that there's
> > something called the "width subcommand", and (2) how to format the call
> to
> > that command/subcommand.
> > >
> >
> > Yes, there's a fair amount of that going on with the tcltk interface. You
> > need to both grasp the rules for passing arguments to the underlying Tcl
> > command, and know how to find and read the Tcl/Tk documentation. Once
> > you're up to speed on those issues it's not all that hard to find stuff
> in
> > (for the present case), say,
> > http://tktable.sourceforge.net/tktable/doc/tkTable.html
> >
> > The situation may be unfortunate, but the alternative is for "someone" to
> > sit down an convert all relevant Tcl/Tk documentation to R help files.
> >
> > -pd
> >
> >
> > > I was able to do it eventually but only after a few hours of effort
> > searching the web for help.
> > >
> > > E.g. with a table (called table1) with 3 columns and want to set widths
> > to 30, 5, and 5:
> > >
> > > colwidths<-c(30, 5, 5)
> > >
> > > for(i in 1:3) {
> > >   tcl(table1, "width", i - 1, colwidths[i])
> > > }
> > >
> > >
> > >
> > > On Wed, Jan 20, 2016 at 3:07 PM, peter dalgaard <pdalgd at gmail.com>
> > wrote:
> > >
> > > > On 19 Jan 2016, at 20:48 , Dalthorp, Daniel <ddalthorp at usgs.gov>
> > wrote:
> > > >
> > > > Does anyone know a simple way to create a tcltk table with columns of
> > > > varying widths?
> > >
> > > Create a table, then set the width of the columns with the width
> > subcommand?
> > >
> > > -pd
> > >
> > >
> > > pathName width ?col? ?value col value ...? If no col is specified,
> > returns a list describing all cols for which a width has been set. If col
> > is specified with no value, it prints out the width of that col in
> > characters (positive number) or pixels (negative number). If one or more
> > col-value pairs are specified, then it sets each col to be that width in
> > characters (positive number) or pixels (negative number). If value is
> > default, then the col uses the default width, specified by -colwidth.
> > >
> > >
> > >
> > > >
> > > > -Dan
> > > >
> > > >
> > > >
> > > > --
> > > > Dan Dalthorp, PhD
> > > > USGS Forest and Rangeland Ecosystem Science Center
> > > > Forest Sciences Lab, Rm 189
> > > > 3200 SW Jefferson Way
> > > > Corvallis, OR 97331
> > > > ph: 541-750-0953
> > > > ddalthorp at usgs.gov
> > > >
> > > >       [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > --
> > > Peter Dalgaard, Professor,
> > > Center for Statistics, Copenhagen Business School
> > > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > > Phone: (+45)38153501
> > > Office: A 4.23
> > > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > --
> > > Dan Dalthorp, PhD
> > > USGS Forest and Rangeland Ecosystem Science Center
> > > Forest Sciences Lab, Rm 189
> > > 3200 SW Jefferson Way
> > > Corvallis, OR 97331
> > > ph: 541-750-0953
> > > ddalthorp at usgs.gov
> > >
> >
> > --
> > Peter Dalgaard, Professor,
> > Center for Statistics, Copenhagen Business School
> > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > Phone: (+45)38153501
> > Office: A 4.23
> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
>
>
> --
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rolf.fankhauser at gepdata.ch  Fri Jan 22 11:50:54 2016
From: rolf.fankhauser at gepdata.ch (Rolf Fankhauser)
Date: Fri, 22 Jan 2016 11:50:54 +0100
Subject: [R] use gcheckbox in gWidgets to switch on/off gframe
In-Reply-To: <trinity-af63e592-8d73-4373-99a2-4cdbadd416c0-1452956034921@3capp-webde-bap32>
References: <trinity-af63e592-8d73-4373-99a2-4cdbadd416c0-1452956034921@3capp-webde-bap32>
Message-ID: <56A2098E.2020208@gepdata.ch>

Hi Tonja

Maybe I wasn't clear in my last post. This is the code:

library(gWidgetstcltk)
library(gWidgets)
options(guiToolkit="tcltk")
Population <- c("A","B","C","D","E","F")
w = gwindow("")
g1 = ggroup(horizontal = F, cont=w)
g2 = ggroup(horizontal = T, cont=g1)
glabel("Population:", cont=g2)
Station = gcombobox(Population, editable=F, cont=g2, handler=NULL)
gseparator(horizontal=T, container=g1, expand=F)
gcheckbox("checked", checked=T, container=g1, handler=function(h,...) {
enabled ( frame1 ) <- svalue(h$obj)
})
frame1 <- gframe ( "A:" , cont = g1 , horizontal=FALSE )
lyt1 <- glayout ( cont = frame1)
widget_list <- list ( )
lyt1 [1,1] <- "A1:"
lyt1 [1,2,expand = TRUE] <- (widget_list$A1 <- gedit(" ", cont=lyt1, 
handler=NULL))
lyt1 [2,1] <- "A2:"
lyt1 [2,2,expand = TRUE] <- (widget_list$A2 <- gedit(" ", cont=lyt1, 
handler=NULL))
gcheckbox("checked", checked=T, container=g1, handler=function(h,...) {
enabled ( frame2 ) <-  svalue(h$obj)
})
frame2 <- gframe ( "B:" , cont = g1 , horizontal=FALSE )
lyt2 <- glayout ( cont = frame2)
widget_list <- list ( )
lyt2 [1,1] <- "B1:"
lyt2 [1,2, expand = TRUE] <- (widget_list$B1 <- gedit(" ", cont=lyt2, 
handler=NULL))

Regards, Rolf


From pmassicotte at hotmail.com  Fri Jan 22 11:36:46 2016
From: pmassicotte at hotmail.com (Philippe Massicotte)
Date: Fri, 22 Jan 2016 11:36:46 +0100
Subject: [R] Strange behavior with S3 class
In-Reply-To: <1A8C1289955EF649A09086A153E2672403C9A5DB4C@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <COL127-W250C62A3DA01C87FADA140B3C00@phx.gbl>
	<1A8C1289955EF649A09086A153E2672403C9A5DB4C@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <n7t0nv$ntq$1@ger.gmane.org>

> That says the internal variable name is preserved but while it still has the 'eem' class the name is reported differently.
> That is presumably because you defined a 'names' function for objects of class 'eem' which returns the contents of $sample as the name, not "sample". Looks like str is using your names.eem method to extract the name of each component of your object, which is kind of what redefining 'names' asks for. And just to demonstrate that str does indeed use the current class's names method:
>
> names.eem <- function(x, ...){  "AnotherName"}
> str(test1)
> # List of 1
> #  $ AnotherName: chr "justaname"
> #  - attr(*, "class")= chr "eem"
>
> So it isn't your '<-', it's because you overrode 'names'

You are absolutely right I made a mistake describing the problem.

So my question is how I can implement the "names" method for my class 
without having this problem. The following piece of code reproduce the 
problem I am facing.

names.eem <- function(x, ...){
   x$sample
}

# First constructor
eem1 <- function(sample){
   eem <- list(sample = sample)
   class(eem) <- "eem"
   return(eem)
}

# Second constructor
eem2 <- function(sample){
   eem <- list(sample = sample)
   class(eem) <- "eem2"
   return(eem)
}

test1 <- eem1("justaname")
test2 <- eem2("justaname")

str(test1)
str(test2)

Thank you for your help,
Philippe


From X.Shi at mmu.ac.uk  Fri Jan 22 14:23:47 2016
From: X.Shi at mmu.ac.uk (Xin Shi)
Date: Fri, 22 Jan 2016 13:23:47 +0000
Subject: [R] pgmm error message,
 Error in terms.default(formula) : no terms component nor attribute
Message-ID: <B46D95999967CF47A7F0D5208680E8C20191330167@exmb1>

Dear

I try to use pgmm to estimate dynamic model for panel data. I got the message below. It is appreciate that you could help on this. data<-read.csv("G:/MMU/Research/Urbanlisation/paneldata.csv",sep=",", header=TRUE) library(plm) library(Formula) urban<-pdata.frame(data, index=c("id","year_begin")) y<-urban[,7] x1<-urban[,8] x2<-urban[,9] x3<-urban[,10] x4<-urban[,11] z1<-pgmm(formula=y~lag(y,0:1)+x2+x3+x4,data=urban, effect = "twoways", model = "twosteps") Error in terms.default(formula) : no terms component nor attribute

Xin
-------
Dr.Xin Shi
Reader in Applied Statistics
Director of DBA Programme
Room 609
Department of Marketing, Operations and Digital Business
Business School
All Saints Campus
Manchester Metropolitan University
Oxford Road
Manchester
M15 6BH
United Kingdom
E-mail: X.Shi at mmu.ac.uk<mailto:X.Shi at mmu.ac.uk>
Telephone: +44 (0)161 247 3841
Fax: +44(0)161 247 6305
Office Hours: Wed 1-3pm.
http://www.business.mmu.ac.uk/staff/staffdetails.php?uref=423
http://www.mmu.ac.uk/dba

"Before acting on this email or opening any attachments you should read the Manchester Metropolitan University email disclaimer available on its website http://www.mmu.ac.uk/emaildisclaimer "

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Fri Jan 22 16:46:40 2016
From: hannah.hlx at gmail.com (li li)
Date: Fri, 22 Jan 2016 10:46:40 -0500
Subject: [R] Logical operator in R
Message-ID: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>

Hi all,
  I encountered the following strange phenomenon.
For some reason, the obs_p[1] and res1$st_p[89] have
the same value but when I run "==", it returns FALSE.
Can anyone help give some explanation on this?
  Thanks very much!
    Hanna

> obs_p[1]
[1] 0.002201438
> res1$st_p[89]
[1] 0.002201438
> res1$st_p[89]==obs_p[1]
[1] FALSE
> res1$st_p[89]<obs_p[1]
[1] FALSE
> res1$st_p[89]>obs_p[1]
[1] TRUE

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Jan 22 16:51:37 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 22 Jan 2016 07:51:37 -0800
Subject: [R] Logical operator in R
In-Reply-To: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
References: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
Message-ID: <CAGxFJbQq5wySPgrFotBax4+6XwTYWA7Q5g+FG6247Uwe9DXQ6w@mail.gmail.com>

FAQ 7.31

-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jan 22, 2016 at 7:46 AM, li li <hannah.hlx at gmail.com> wrote:
> Hi all,
>   I encountered the following strange phenomenon.
> For some reason, the obs_p[1] and res1$st_p[89] have
> the same value but when I run "==", it returns FALSE.
> Can anyone help give some explanation on this?
>   Thanks very much!
>     Hanna
>
>> obs_p[1]
> [1] 0.002201438
>> res1$st_p[89]
> [1] 0.002201438
>> res1$st_p[89]==obs_p[1]
> [1] FALSE
>> res1$st_p[89]<obs_p[1]
> [1] FALSE
>> res1$st_p[89]>obs_p[1]
> [1] TRUE
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fmagalhaes at gmail.com  Fri Jan 22 16:53:22 2016
From: fmagalhaes at gmail.com (=?UTF-8?B?RsOhYmlvIE1hZ2FsaMOjZXM=?=)
Date: Fri, 22 Jan 2016 13:53:22 -0200
Subject: [R] Logical operator in R
In-Reply-To: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
References: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
Message-ID: <CAAu8QF7G9z8rAOxi2D+Xs-pvbO8SXDQkm0bTHYZHtpTWVG0rhQ@mail.gmail.com>

Hi,

This can get you started:
https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f


--Fabio

On Fri, Jan 22, 2016 at 1:46 PM, li li <hannah.hlx at gmail.com> wrote:

> Hi all,
>   I encountered the following strange phenomenon.
> For some reason, the obs_p[1] and res1$st_p[89] have
> the same value but when I run "==", it returns FALSE.
> Can anyone help give some explanation on this?
>   Thanks very much!
>     Hanna
>
> > obs_p[1]
> [1] 0.002201438
> > res1$st_p[89]
> [1] 0.002201438
> > res1$st_p[89]==obs_p[1]
> [1] FALSE
> > res1$st_p[89]<obs_p[1]
> [1] FALSE
> > res1$st_p[89]>obs_p[1]
> [1] TRUE
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rab45 at pitt.edu  Fri Jan 22 16:54:14 2016
From: rab45 at pitt.edu (Rick Bilonick)
Date: Fri, 22 Jan 2016 10:54:14 -0500
Subject: [R] Logical operator in R
In-Reply-To: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
References: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
Message-ID: <56A250A6.9070203@pitt.edu>

On 01/22/2016 10:46 AM, li li wrote:
> Hi all,
>    I encountered the following strange phenomenon.
> For some reason, the obs_p[1] and res1$st_p[89] have
> the same value but when I run "==", it returns FALSE.
> Can anyone help give some explanation on this?
>    Thanks very much!
>      Hanna
>
>> obs_p[1]
> [1] 0.002201438
>> res1$st_p[89]
> [1] 0.002201438
>> res1$st_p[89]==obs_p[1]
> [1] FALSE
>> res1$st_p[89]<obs_p[1]
> [1] FALSE
>> res1$st_p[89]>obs_p[1]
> [1] TRUE
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
I believe the decimal representation is only approximate. The real 
internal values in binary are different. If you want to have comparisons 
like this result in being considered equal, I think there is a way to 
use a fuzzy comparison but I don't remember the details.

Rick

-- 
Richard A. Bilonick, PhD
Assistant Professor
Dept. of Ophthalmology, School of Medicine
Dept. of Biostatistics, Graduate School of Public Health
Dept. of Orthodontics, School of Dental Medicine
University of Pittsburgh
Principal Investigator for the Pittsburgh Aerosol Research
  and Inhalation Epidemiology Study (PARIES)
412 647 5756


From rmh at temple.edu  Fri Jan 22 16:57:03 2016
From: rmh at temple.edu (Rmh)
Date: Fri, 22 Jan 2016 10:57:03 -0500
Subject: [R] Logical operator in R
In-Reply-To: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
References: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
Message-ID: <59A89205-8C37-4EB3-92B5-B9A20E34F510@temple.edu>

FAQ 7.31

in this case subtract the two numbers and see that
they differ by about 1e-16

Sent from my iPhone

> On Jan 22, 2016, at 10:46, li li <hannah.hlx at gmail.com> wrote:
> 
> Hi all,
>  I encountered the following strange phenomenon.
> For some reason, the obs_p[1] and res1$st_p[89] have
> the same value but when I run "==", it returns FALSE.
> Can anyone help give some explanation on this?
>  Thanks very much!
>    Hanna
> 
>> obs_p[1]
> [1] 0.002201438
>> res1$st_p[89]
> [1] 0.002201438
>> res1$st_p[89]==obs_p[1]
> [1] FALSE
>> res1$st_p[89]<obs_p[1]
> [1] FALSE
>> res1$st_p[89]>obs_p[1]
> [1] TRUE
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From b.rowlingson at lancaster.ac.uk  Fri Jan 22 17:03:35 2016
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 22 Jan 2016 16:03:35 +0000
Subject: [R] Error opening SHP file
In-Reply-To: <23e214142d514551ad2921274d5d2dbf@EX-1-HT0.lancs.local>
References: <853310330.7127045.1453423163693.JavaMail.yahoo.ref@mail.yahoo.com>
	<853310330.7127045.1453423163693.JavaMail.yahoo@mail.yahoo.com>
	<1443AF37-DC8C-4EB6-9AF8-15EBE25EE77B@comcast.net>
	<23e214142d514551ad2921274d5d2dbf@EX-1-HT0.lancs.local>
Message-ID: <CANVKczPXfbJFQy06JXNYJERMKLujWSLXODimw-GvYqi3K-ZV_g@mail.gmail.com>

We can duplicate the error by giving a path to a non-existent
shapefile, which is probably the original problem:

 > require(maptools)
Loading required package: maptools
Loading required package: sp
Checking rgeos availability: TRUE
 > foo=readShapeSpatial("fnord.shp")
Error in getinfo.shape(fn) : Error opening SHP file

The error message there isn't totally explicit, and might cover a
range of other possibilities such as a corrupted shapefile, or a
missing .shx component of the shapefile or whatever.

BUT you probably shouldn't be using readShapeSpatial anyway, as it has
a habit of not reading the coordinate system in the .prj file. I find
it much easier to use `raster::shapefile` which *does* read the
coordinate system *and* gives a more explicit error message for a
missing shapefile:

 > require(raster)
Loading required package: raster
 > foo=shapefile("fnord.shp")
Error in normalizePath(x, winslash = "/", mustWork = TRUE) :
  path[1]="fnord.shp": No such file or directory

"No such file or directory"

Barry



On Fri, Jan 22, 2016 at 2:25 AM, boB Rudis <bob at rudis.net> wrote:
> Agreed with the others. After finding that shapefile and getting it to
> work you are definitely not in the proper working directory.
>
> On Thu, Jan 21, 2016 at 8:40 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>> On Jan 21, 2016, at 4:39 PM, Amoy Yang via R-help <r-help at r-project.org> wrote:
>>>
>>> Any advice for the following errors?
>>> state.map <- readShapeSpatial("maps/st24_d00.shp")
>>> Error in getinfo.shape(fn) : Error opening SHP file
>>
>> What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?
>>
>>
>> --
>> David.
>>
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitri.liakhovitski at gmail.com  Fri Jan 22 17:34:54 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Fri, 22 Jan 2016 11:34:54 -0500
Subject: [R] Shift all values above certain value by 1
Message-ID: <CAN2xGJZEr9OjXniGRaYHWdvQcdanvv9XJ-g45PLnM+O6YxxmJg@mail.gmail.com>

Hello!

# I have a data frame x:
x <- data.frame(a = 1:10, b = 2:11, c = 3:12, other = rnorm(10))

# First, I need to change every value 7 in columns a:c to 4
# Then, I need to decrease by 1 all values in columns a:c that are >7

What would be the fastest way of doing it?
Thank you!

-- 
Dimitri Liakhovitski


From dimitri.liakhovitski at gmail.com  Fri Jan 22 17:38:03 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Fri, 22 Jan 2016 11:38:03 -0500
Subject: [R] Shift all values above certain value by 1
In-Reply-To: <CAN2xGJZEr9OjXniGRaYHWdvQcdanvv9XJ-g45PLnM+O6YxxmJg@mail.gmail.com>
References: <CAN2xGJZEr9OjXniGRaYHWdvQcdanvv9XJ-g45PLnM+O6YxxmJg@mail.gmail.com>
Message-ID: <CAN2xGJZvOsDBhwNZug6BmdN+XnEGNuHciNXziVxc_Usj50ejfw@mail.gmail.com>

I think I got it:

set.seed(123)
x <- data.frame(a = 1:10, b = 2:11, c = 3:12, other = rnorm(10))
x
temp <- as.matrix(x[1:3])
temp[temp %in% 7] <- 4
temp[temp > 7] <- temp[temp > 7]-1
x[1:3] <- temp
x

It works only with matrices, right? Can't do x[x>7] when x is a data frame?
Thanks!

On Fri, Jan 22, 2016 at 11:34 AM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Hello!
>
> # I have a data frame x:
> x <- data.frame(a = 1:10, b = 2:11, c = 3:12, other = rnorm(10))
>
> # First, I need to change every value 7 in columns a:c to 4
> # Then, I need to decrease by 1 all values in columns a:c that are >7
>
> What would be the fastest way of doing it?
> Thank you!
>
> --
> Dimitri Liakhovitski



-- 
Dimitri Liakhovitski


From pgilbert902 at gmail.com  Fri Jan 22 17:38:49 2016
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Fri, 22 Jan 2016 11:38:49 -0500
Subject: [R] Updating a Time Series After Forecast()
In-Reply-To: <mailman.3.1452942002.12083.r-help@r-project.org>
References: <mailman.3.1452942002.12083.r-help@r-project.org>
Message-ID: <56A25B19.5000704@gmail.com>

Lorenzo

Berend's suggestion may be the simplest if you have univariate ts style 
series. If you are writing code and you want it to work with 
multivariate series and other time representations then you might want 
to consider the splice() function in package tframe.
   library(tfplot)
   ts3 <- splice(ts2, pred2$mean)

(and tframePlus if you need zoo and other support.)

Regards,
Paul

On 01/16/2016 06:00 AM, r-help-request at r-project.org wrote:
> Date: Fri, 15 Jan 2016 13:02:58 +0100
> From: Berend Hasselman<bhh at xs4all.nl>
> To: Lorenzo Isella<lorenzo.isella at gmail.com>
> Cc:"r-help at r-project.org"  <r-help at r-project.org>
> Subject: Re: [R] Updating a Time Series After Forecast()
> Message-ID:<AC8189E0-4F20-479E-A75F-65A5CEE5F150 at xs4all.nl>
> Content-Type: text/plain; charset=us-ascii
>
>
>> >On 14 Jan 2016, at 22:36, Lorenzo Isella<lorenzo.isella at gmail.com>  wrote:
>> >
>> >Dear All,
>> >Perhaps I am drowning in a cup of water, since I am positive that the
>> >answer will be a one-liner.
>> >Consider the following short script
>> >
>> >
>> >########################################################
>> >library(forecast)
>> >
>> >ts2<-structure(c(339130, 356462, 363234, 378179, 367864, 378337, 392157,
>> >402153, 376361, 392204, 403483, 414034, 391967, 406067, 419464,
>> >434913, 410102, 424795, 437073, 448827, 415569, 430561, 444719,
>> >455764, 419892, 444190, 454648, 466312, 439922, 448963, 465153,
>> >475621, 445502, 457198, 473573, 485764, 463895, 470274, 484390,
>> >490678, 478003, 483570, 499141, 509216, 481395, 492345, 511184,
>> >513420, 483757, 490884, 514966, 515457, 497614, 510139, 523467,
>> >526406, 499784, 519033, 532009, 531260, 521539, 532590, 553118,
>> >557725, 548321, 556832, 578087, 578120, 566116, 580571, 587993,
>> >569985, 534326, 539641, 564824, 568445, 558614, 570192, 594584,
>> >598305, 593769, 598278, 620147, 615884, 611033, 609304, 630458,
>> >624325, 614356, 627192, 649324, 645988, 642965, 645125, 669471,
>> >665529, 664248, 669670, 694719), na.action = structure(1:64, class =
>> >"omit"), .Tsp = c(1991,
>> >2015.5, 4), class = "ts")
>> >
>> >fit2 <- auto.arima(ts2, approximation=FALSE,trace=FALSE)
>> >
>> >pred2 <- forecast(fit2, h=2)
>> >
>> >#######################################################
>> >
>> >So, I have an original quarterly time series ts2 and a forecast for 2
>> >quarters pred2.
>> >
>> >I would like to combine ts2 and pred2 (just the prediction) into a new
>> >time series (in other words, just stretch a bit ts2).
>> >How can I do that?
> A possible way is this
>
> ts3 <- ts(c(ts2,pred2$mean),start=start(ts2),frequency=frequency(ts2))
>
> Most  likely there are more ways of getting what you want.
>
> Berend
>
>> >Many thanks
>> >
>> >Lorenzo
>> >


From mara.pfleiderer at uni-ulm.de  Fri Jan 22 16:01:58 2016
From: mara.pfleiderer at uni-ulm.de (mara.pfleiderer at uni-ulm.de)
Date: Fri, 22 Jan 2016 16:01:58 +0100
Subject: [R] Constrained Poisson model / Bayesian Poisson model
Message-ID: <20160122160158.id4jtfzjluos08kk@imap.uni-ulm.de>

Hi all,

I am dealing with a problem about my linear Poisson regression model  
(link function=identity).

I am using the glm()-function which results in negative coefficients,  
but a negative influence of the regressors wouldn't make sense.

(i) Is there a possibility to set constraints on the regression  
parameters in glm() such that all coefficients are positive? Or is  
there another function in R for which this is possible?

(ii) Is there a Bayesian version of the glm()-function where I can  
specify the prior distribution for my regression parameters? (e.g. a  
Dirichlet prior s.t. the parameters are positive)

All this with respect to the linear Poisson model...

Thanks in advance!
Best,
Mara


From josh.m.ulrich at gmail.com  Fri Jan 22 18:23:15 2016
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Fri, 22 Jan 2016 11:23:15 -0600
Subject: [R] unexpected behaviour of an extended time series (using
 packages spuRs and xts)
In-Reply-To: <00a401d14edc$ffef75b0$ffce6110$@mines-ales.fr>
References: <00a401d14edc$ffef75b0$ffce6110$@mines-ales.fr>
Message-ID: <CAPPM_gTd6BHTpiwCHqOWVyCcDcFfHcDUYDn=aDM8S7Ka2DULdg@mail.gmail.com>

Try using the latest xts on GitHub:
https://github.com/joshuaulrich/xts

On Thu, Jan 14, 2016 at 9:05 AM, Olivier ETERRADOSSI
<olivier.eterradossi at mines-ales.fr> wrote:
> Hi list,
>
>
>
> I thought I knew how to use extended time series (package xts), but I was
> wrong  J  ?
>
>
>
> While preparing a toy example for something else, using data provided in
> R, I run into an unexpected problem and can?t figure by myself what is
> happening below, can anyone of you tell ? I searched the archives but
> didn?t locate any answer. Probably it?s trivial, so please forgive  :
>
>
>
> I?m using :
>
> R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree" / Platform:
> x86_64-w64-mingw32/x64 (64-bit)
>
> Packages are updated weekly, sometimes daily.
>
>
>
> I take some data from package spuRs :
>
>> library(spuRs)
>
>> data(kew)
>
>
>
> I turn the dataframe into time series (by combining each kew[,2:13] one
> after each other into a vector, and turning the vector into time series).
>
>
>
> One is ts :
>
>
>
>>
> kew.ts<-ts(data=stock,start=kew$year[1],end=kew$year[length(kew$year)],fre
> quency=12)
>
>
>
> And the other is xts, it looks fine at first :
>
>
>
>> kew.xts<-as.xts(kew.ts)
>
>> periodicity(kew.xts)
>
> Monthly periodicity from janv. 1697 to janv. 1999  # OK
>
>> hist(kew.xts) # OK
>
>> summary(kew.xts)
>
>      Index         kew.xts
>
>  Min.   :1697   Min.   :  0.00
>
>  1st Qu.:1772   1st Qu.: 29.70
>
>  Median :1848   Median : 47.00
>
>  Mean   :1848   Mean   : 51.14
>
>  3rd Qu.:1924   3rd Qu.: 67.60
>
>  Max.   :1999   Max.   :189.00  # OK
>
>
>
>> gdata::is.what(kew.xts)
>
> [1] "is.array"        "is.atomic"       "is.double"
> "is.index.unique"
>
> [5] "is.matrix"       "is.numeric"      "is.object"       "is.regular"
>
>
>  [9] "is.time.unique"  "is.unsorted"     "is.xts"          "is.zoo"
> # seems OK
>
>
>
>
>
> # But now, first try :
>
>> plot(kew.xts)
>
> Error in if (on == "years") { :
>
>   valeur manquante l? o? TRUE / FALSE est requis     # french for ?
> missing value where TRUE/FALSE is required ?
>
>
>
> # hmmmm, let?s try something else :
>
>> plot(kew.xts['1697-01/1979/']) # OK
>
>
>
>> plot(kew.xts['1697-01/1980/'])
>
> Error in if (on == "years") { :
>
>   valeur manquante l? o? TRUE / FALSE est requis
>
>
>
>> plot(kew.xts['1697-01/1979-12/']) # OK
>
>
>
>> plot(kew.xts['1697-01/1980-01/'])
>
> Error in if (on == "years") { :
>
>   valeur manquante l? o? TRUE / FALSE est requis
>
>
>
> # but?!  :
>
>
>
>> plot(kew.xts['1979-01/1980/']) # OK !!!!!
>
>
>
> And so are :
>
>> plot (kew.xts['1978/1980/'])
>
>> plot(kew.xts['1977/1982/'])
>
>> plot(kew.xts['1977-01/1982-12'])  # and so on?
>
>
>
> I?m puzzled ! I have probably missed a trivial point? Can someone tell ?
>
>
>
> Thanks a lot list, regards, Olivier
>
>
>
> --------------------------
>
> Olivier ETERRADOSSI
>
> Ma?tre-Assistant, HDR
>
> Ecole des Mines d?Al?s (C2MA, site de Pau)
>
> Ing?nierie de l'aspect visuel et tactile des mat?riaux
>
> P?le ? Recherche sur les Interactions des Mat?riaux avec leur
> Environnement ? (RIME)
>
> H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
>
> Tel : 05 59 30 90 35 (direct) - 05 59 30  54 25 (std)
>
> Fax : 05 59 30 63 68
>
>  <http://www.mines-ales.fr/> http://www.mines-ales.fr
>
>  <http://www.mines-telecom.fr/> http://www.mines-telecom.fr
>
>
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com
R/Finance 2016 | www.rinfinance.com


From amoy_y at yahoo.com  Fri Jan 22 18:24:27 2016
From: amoy_y at yahoo.com (Amoy Yang)
Date: Fri, 22 Jan 2016 17:24:27 +0000 (UTC)
Subject: [R] Error opening SHP file
In-Reply-To: <CANVKczPXfbJFQy06JXNYJERMKLujWSLXODimw-GvYqi3K-ZV_g@mail.gmail.com>
References: <CANVKczPXfbJFQy06JXNYJERMKLujWSLXODimw-GvYqi3K-ZV_g@mail.gmail.com>
Message-ID: <1883109390.7405356.1453483467233.JavaMail.yahoo@mail.yahoo.com>

This is the results that addresses David's advice.
> library(maptools)
> library(maps)
> state.map <- readShapeSpatial("maps/st24_d00.shp")
Error in getinfo.shape(fn) : Error opening SHP file
> # David question: What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?
> list.files('maps')
character(0)
I actually use the link below to learn how to load state/zip-data on USA map.

# http://stackoverflow.com/questions/1441717/plotting-color-map-with-zip-codes-in-r-or-python
Plotting color map with zip codes in R or Python
| ? |
| ? |  | ? | ? | ? | ? | ? |
| Plotting color map with zip codes in R or PythonI have some US demographic and firmographic data. I would like to plot zipcode areas in a state or a smaller region (e.g. city). Each area would be annotated by col... |
|  |
| View on stackoverflow.com | Preview by Yahoo |
|  |
| ? |




 

    On Friday, January 22, 2016 10:03 AM, Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
 

 We can duplicate the error by giving a path to a non-existent
shapefile, which is probably the original problem:

 > require(maptools)
Loading required package: maptools
Loading required package: sp
Checking rgeos availability: TRUE
 > foo=readShapeSpatial("fnord.shp")
Error in getinfo.shape(fn) : Error opening SHP file

The error message there isn't totally explicit, and might cover a
range of other possibilities such as a corrupted shapefile, or a
missing .shx component of the shapefile or whatever.

BUT you probably shouldn't be using readShapeSpatial anyway, as it has
a habit of not reading the coordinate system in the .prj file. I find
it much easier to use `raster::shapefile` which *does* read the
coordinate system *and* gives a more explicit error message for a
missing shapefile:

 > require(raster)
Loading required package: raster
 > foo=shapefile("fnord.shp")
Error in normalizePath(x, winslash = "/", mustWork = TRUE) :
? path[1]="fnord.shp": No such file or directory

"No such file or directory"

Barry



On Fri, Jan 22, 2016 at 2:25 AM, boB Rudis <bob at rudis.net> wrote:
> Agreed with the others. After finding that shapefile and getting it to
> work you are definitely not in the proper working directory.
>
> On Thu, Jan 21, 2016 at 8:40 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>> On Jan 21, 2016, at 4:39 PM, Amoy Yang via R-help <r-help at r-project.org> wrote:
>>>
>>> Any advice for the following errors?
>>> state.map <- readShapeSpatial("maps/st24_d00.shp")
>>> Error in getinfo.shape(fn) : Error opening SHP file
>>
>> What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?
>>
>>
>> --
>> David.
>>
>>>
>>>? ? ? [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


  
	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jan 22 18:40:53 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 22 Jan 2016 09:40:53 -0800
Subject: [R] Constrained Poisson model / Bayesian Poisson model
In-Reply-To: <20160122160158.id4jtfzjluos08kk@imap.uni-ulm.de>
References: <20160122160158.id4jtfzjluos08kk@imap.uni-ulm.de>
Message-ID: <D774FCD5-5BC7-430C-A9AB-0954B7F2D715@comcast.net>


> On Jan 22, 2016, at 7:01 AM, mara.pfleiderer at uni-ulm.de wrote:
> 
> Hi all,
> 
> I am dealing with a problem about my linear Poisson regression model (link function=identity).
> 
> I am using the glm()-function which results in negative coefficients, but a negative influence of the regressors wouldn't make sense.

Negative coefficients merely indicate a lower relative rate. You need to be more specific about the exactly data and model output before you can raise our concern to a level where further comment can be made.


> 
> (i) Is there a possibility to set constraints on the regression parameters in glm() such that all coefficients are positive? Or is there another function in R for which this is possible?
> 
> (ii) Is there a Bayesian version of the glm()-function where I can specify the prior distribution for my regression parameters? (e.g. a Dirichlet prior s.t. the parameters are positive)
> 
> All this with respect to the linear Poisson model...

As I implied above, the word "linear" means something different than "additive" when the link is log().

--

David Winsemius
Alameda, CA, USA


From hannah.hlx at gmail.com  Fri Jan 22 19:43:13 2016
From: hannah.hlx at gmail.com (li li)
Date: Fri, 22 Jan 2016 13:43:13 -0500
Subject: [R] Logical operator in R
In-Reply-To: <59A89205-8C37-4EB3-92B5-B9A20E34F510@temple.edu>
References: <CAHLnndbhzd2sdjhj7r-OsqYxs7K4EJkFdmBoyGp5mAOHBYF=Vg@mail.gmail.com>
	<59A89205-8C37-4EB3-92B5-B9A20E34F510@temple.edu>
Message-ID: <CAHLnndbhi-0VNH8r+XGnoD-usQhG209zw3QH0GH2Sx=M6uiD-A@mail.gmail.com>

I see. Thanks!

2016-01-22 10:57 GMT-05:00 Rmh <rmh at temple.edu>:

> FAQ 7.31
>
> in this case subtract the two numbers and see that
> they differ by about 1e-16
>
> Sent from my iPhone
>
> > On Jan 22, 2016, at 10:46, li li <hannah.hlx at gmail.com> wrote:
> >
> > Hi all,
> >  I encountered the following strange phenomenon.
> > For some reason, the obs_p[1] and res1$st_p[89] have
> > the same value but when I run "==", it returns FALSE.
> > Can anyone help give some explanation on this?
> >  Thanks very much!
> >    Hanna
> >
> >> obs_p[1]
> > [1] 0.002201438
> >> res1$st_p[89]
> > [1] 0.002201438
> >> res1$st_p[89]==obs_p[1]
> > [1] FALSE
> >> res1$st_p[89]<obs_p[1]
> > [1] FALSE
> >> res1$st_p[89]>obs_p[1]
> > [1] TRUE
> >
> >    [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jan 22 19:45:01 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 22 Jan 2016 10:45:01 -0800
Subject: [R] Error opening SHP file
In-Reply-To: <1883109390.7405356.1453483467233.JavaMail.yahoo@mail.yahoo.com>
References: <CANVKczPXfbJFQy06JXNYJERMKLujWSLXODimw-GvYqi3K-ZV_g@mail.gmail.com>
	<1883109390.7405356.1453483467233.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <901C57CE-5861-4AB2-AB61-B8E675DE2654@comcast.net>


> On Jan 22, 2016, at 9:24 AM, Amoy Yang via R-help <r-help at r-project.org> wrote:
> 
> This is the results that addresses David's advice.
>> library(maptools)
>> library(maps)
>> state.map <- readShapeSpatial("maps/st24_d00.shp")
> Error in getinfo.shape(fn) : Error opening SHP file
>> # David question: What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?
>> list.files('maps')
> character(0)
> I actually use the link below to learn how to load state/zip-data on USA map.

That link says:

"For example (assumes you have the maryland shapefiles in the map subdirectory):"

So it did not purport to tell you how to put shape files in that subdirectory. That statement assumed you understood basic OS path naming conventions in your unstated OS and how to move files around, which is not a topic for rhelp.

Try working through the examples in the documents that come with the `sp`-package.

-- 
David.
> 
> # http://stackoverflow.com/questions/1441717/plotting-color-map-with-zip-codes-in-r-or-python
> Plotting color map with zip codes in R or Python
> |   |
> |   |  |   |   |   |   |   |
> | Plotting color map with zip codes in R or PythonI have some US demographic and firmographic data. I would like to plot zipcode areas in a state or a smaller region (e.g. city). Each area would be annotated by col... |
> |  |
> | View on stackoverflow.com | Preview by Yahoo |
> |  |
> |   |
> 
>    On Friday, January 22, 2016 10:03 AM, Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
> 
> 
> We can duplicate the error by giving a path to a non-existent
> shapefile, which is probably the original problem:
> 
>> require(maptools)
> Loading required package: maptools
> Loading required package: sp
> Checking rgeos availability: TRUE
>> foo=readShapeSpatial("fnord.shp")
> Error in getinfo.shape(fn) : Error opening SHP file
> 
> The error message there isn't totally explicit, and might cover a
> range of other possibilities such as a corrupted shapefile, or a
> missing .shx component of the shapefile or whatever.
> 
> BUT you probably shouldn't be using readShapeSpatial anyway, as it has
> a habit of not reading the coordinate system in the .prj file. I find
> it much easier to use `raster::shapefile` which *does* read the
> coordinate system *and* gives a more explicit error message for a
> missing shapefile:
> 
>> require(raster)
> Loading required package: raster
>> foo=shapefile("fnord.shp")
> Error in normalizePath(x, winslash = "/", mustWork = TRUE) :
>   path[1]="fnord.shp": No such file or directory
> 
> "No such file or directory"
> 
> Barry
> 
> 
> 
> On Fri, Jan 22, 2016 at 2:25 AM, boB Rudis <bob at rudis.net> wrote:
>> Agreed with the others. After finding that shapefile and getting it to
>> work you are definitely not in the proper working directory.
>> 
>> On Thu, Jan 21, 2016 at 8:40 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>> 
>>>> On Jan 21, 2016, at 4:39 PM, Amoy Yang via R-help <r-help at r-project.org> wrote:
>>>> 
>>>> Any advice for the following errors?
>>>> state.map <- readShapeSpatial("maps/st24_d00.shp")
>>>> Error in getinfo.shape(fn) : Error opening SHP file
>>> 
>>> What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?
>>> 
>>> 
>>> --
>>> David.
>>> 
>>>> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.ca.us  Fri Jan 22 20:09:10 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 22 Jan 2016 11:09:10 -0800
Subject: [R] getting values from php or javascript
In-Reply-To: <1218519431.7336417.1453454325563.JavaMail.yahoo@mail.yahoo.com>
References: <1218519431.7336417.1453454325563.JavaMail.yahoo.ref@mail.yahoo.com>
	<1218519431.7336417.1453454325563.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1E9CFF48-57ED-4AB3-B310-A5E1A9FED642@dcn.davis.ca.us>

There are ways, but most likely you will not be satisfied with such a monster.  Try the maptools package or the Analysis of Spatial Data Task View [1].

[1] https://cran.r-project.org/web/views/Spatial.html
-- 
Sent from my phone. Please excuse my brevity.

On January 22, 2016 1:18:45 AM PST, Alaios via R-help <r-help at r-project.org> wrote:
>Dear all,I would like to execute some php or javascripts I found on the
>web.
>
>see at middle of this page towards bottom
>http://www.howtocreate.co.uk/php/gridref.php#samples
>
>Is there any way I can call the php function for example directly from
>R?
>I would like to thank you in advance for your replyRegardsAlex
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ebs15242 at gmail.com  Fri Jan 22 20:20:59 2016
From: ebs15242 at gmail.com (Ed Siefker)
Date: Fri, 22 Jan 2016 13:20:59 -0600
Subject: [R] aggregate and the $ operator
Message-ID: <CALRb-ofJ5JM22Puf4cbJ=SpPQDxzj14akdb44i_p0v40xXLtYw@mail.gmail.com>

Aggregate does the right thing with column names when passing it
numerical coordinates.
Given a dataframe like this:

  Nuclei Positive Nuclei Slide
1    133              96    A1
2     96              70    A1
3     62              52    A2
4     60              50    A2

I can call 'aggregate' like this:

> aggregate(example[1], by=example[3], sum)
  Slide Nuclei
1    A1    229
2    A2    122

But that means I have to keep track of which column is which number.
If I try it the
easy way, it doesn't keep track of column names and it forces me to
coerce the 'by'
to a list.

> aggregate(example$Nuclei, by=list(example$Slide), sum)
  Group.1   x
1      A1 229
2      A2 122

Is there a better way to do this?  Thanks
-Ed


From r at catwhisker.org  Fri Jan 22 20:32:19 2016
From: r at catwhisker.org (David Wolfskill)
Date: Fri, 22 Jan 2016 11:32:19 -0800
Subject: [R] aggregate and the $ operator
In-Reply-To: <CALRb-ofJ5JM22Puf4cbJ=SpPQDxzj14akdb44i_p0v40xXLtYw@mail.gmail.com>
References: <CALRb-ofJ5JM22Puf4cbJ=SpPQDxzj14akdb44i_p0v40xXLtYw@mail.gmail.com>
Message-ID: <20160122193219.GF55366@albert.catwhisker.org>

On Fri, Jan 22, 2016 at 01:20:59PM -0600, Ed Siefker wrote:
> Aggregate does the right thing with column names when passing it
> numerical coordinates.
> Given a dataframe like this:
> 
>   Nuclei Positive Nuclei Slide
> 1    133              96    A1
> 2     96              70    A1
> 3     62              52    A2
> 4     60              50    A2
> 
> I can call 'aggregate' like this:
> 
> > aggregate(example[1], by=example[3], sum)
>   Slide Nuclei
> 1    A1    229
> 2    A2    122
> 
> But that means I have to keep track of which column is which number.
> If I try it the
> easy way, it doesn't keep track of column names and it forces me to
> coerce the 'by'
> to a list.
> 
> > aggregate(example$Nuclei, by=list(example$Slide), sum)
>   Group.1   x
> 1      A1 229
> 2      A2 122
> 
> Is there a better way to do this?  Thanks
> -Ed
> ....

Something like:

> aggregate(Nuclei ~ Slide, example, sum)
  Slide Nuclei
1    A1    229
2    A2    122
> 

perhaps?

Peace,
david
-- 
David H. Wolfskill				r at catwhisker.org
Those who would murder in the name of God or prophet are blasphemous cowards.

See http://www.catwhisker.org/~david/publickey.gpg for my public key.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 603 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160122/d78698ac/attachment.bin>

From joeceradini at gmail.com  Fri Jan 22 20:32:52 2016
From: joeceradini at gmail.com (Joe Ceradini)
Date: Fri, 22 Jan 2016 12:32:52 -0700
Subject: [R] aggregate and the $ operator
In-Reply-To: <CALRb-ofJ5JM22Puf4cbJ=SpPQDxzj14akdb44i_p0v40xXLtYw@mail.gmail.com>
References: <CALRb-ofJ5JM22Puf4cbJ=SpPQDxzj14akdb44i_p0v40xXLtYw@mail.gmail.com>
Message-ID: <CAKq2vL40dRiUw-twPZVsJvR_=wGEiZC59zS=CQ_2gcHGmMRj-A@mail.gmail.com>

Does this do what you want?

aggregate(Nuclei ~ Slide, example, sum)

On Fri, Jan 22, 2016 at 12:20 PM, Ed Siefker <ebs15242 at gmail.com> wrote:

> Aggregate does the right thing with column names when passing it
> numerical coordinates.
> Given a dataframe like this:
>
>   Nuclei Positive Nuclei Slide
> 1    133              96    A1
> 2     96              70    A1
> 3     62              52    A2
> 4     60              50    A2
>
> I can call 'aggregate' like this:
>
> > aggregate(example[1], by=example[3], sum)
>   Slide Nuclei
> 1    A1    229
> 2    A2    122
>
> But that means I have to keep track of which column is which number.
> If I try it the
> easy way, it doesn't keep track of column names and it forces me to
> coerce the 'by'
> to a list.
>
> > aggregate(example$Nuclei, by=list(example$Slide), sum)
>   Group.1   x
> 1      A1 229
> 2      A2 122
>
> Is there a better way to do this?  Thanks
> -Ed
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Cooperative Fish and Wildlife Research Unit
Zoology and Physiology Dept.
University of Wyoming
JoeCeradini at gmail.com / 914.707.8506
wyocoopunit.org

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Fri Jan 22 20:45:33 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Fri, 22 Jan 2016 11:45:33 -0800
Subject: [R] tcltk tkwidget(..."table")
In-Reply-To: <CACa4aQ5HUZn_7QWFP3d0v1X5DTPZtunZXyecpm-_Wkf=WL_Gqg@mail.gmail.com>
References: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>
	<6FEECD0A-6905-4DEF-A72B-96F2A6FDA827@gmail.com>
	<CAJeYpE8Bm1QRTk6F5hTuTr24t25XrG97k8Cjz0w25FOBQFaPSQ@mail.gmail.com>
	<624FF320-0BD3-492B-9FD8-20514174E3F5@gmail.com>
	<CAJeYpE_CAX+-gOpQ1x7FLdL74_Co_wyXscVfFokP6KFf+ZYfXw@mail.gmail.com>
	<CACa4aQ5HUZn_7QWFP3d0v1X5DTPZtunZXyecpm-_Wkf=WL_Gqg@mail.gmail.com>
Message-ID: <CAJeYpE_4V93VGqwD70sdLJtN4h7WYgeZcdfN1TMoW==5zW16+w@mail.gmail.com>

Thanks, Adrian...the discussions about  R bindings to Tcl and Tk look very
helpful!

-Dan

On Fri, Jan 22, 2016 at 2:49 AM, Adrian Waddell <adrian.waddell at gmail.com>
wrote:

> I have written some content about the R bindings to Tcl and Tk:
>
> http://waddella.github.io/loon/learn_R_tcltk.html
>
> And the pack geometry manager
>
> http://adrian.waddell.ch/EssentialSoftware/Rtcltk_geometry.pdf
>
> But if you have no experience with building graphical user interfaces then
> you might want to look into a book on Tk. "Effective TCL/TK programming" by
> Harrison and McLennan is a good one.
>
> Greetings,
>
> Adrian
> On Jan 21, 2016 10:36 PM, "Dalthorp, Daniel" <ddalthorp at usgs.gov> wrote:
>
>> >  Once you're up to speed on those issues...
>>
>> Any suggestions for getting up to speed on those issues?
>>
>>
>>
>> On Thu, Jan 21, 2016 at 11:46 AM, peter dalgaard <pdalgd at gmail.com>
>> wrote:
>>
>> >
>> > > On 21 Jan 2016, at 00:25 , Dalthorp, Daniel <ddalthorp at usgs.gov>
>> wrote:
>> > >
>> > > Thanks, Peter.
>> > >
>> > > I'm sure that's right, but it requires knowing: (1) that there's
>> > something called the "width subcommand", and (2) how to format the call
>> to
>> > that command/subcommand.
>> > >
>> >
>> > Yes, there's a fair amount of that going on with the tcltk interface.
>> You
>> > need to both grasp the rules for passing arguments to the underlying Tcl
>> > command, and know how to find and read the Tcl/Tk documentation. Once
>> > you're up to speed on those issues it's not all that hard to find stuff
>> in
>> > (for the present case), say,
>> > http://tktable.sourceforge.net/tktable/doc/tkTable.html
>> >
>> > The situation may be unfortunate, but the alternative is for "someone"
>> to
>> > sit down an convert all relevant Tcl/Tk documentation to R help files.
>> >
>> > -pd
>> >
>> >
>> > > I was able to do it eventually but only after a few hours of effort
>> > searching the web for help.
>> > >
>> > > E.g. with a table (called table1) with 3 columns and want to set
>> widths
>> > to 30, 5, and 5:
>> > >
>> > > colwidths<-c(30, 5, 5)
>> > >
>> > > for(i in 1:3) {
>> > >   tcl(table1, "width", i - 1, colwidths[i])
>> > > }
>> > >
>> > >
>> > >
>> > > On Wed, Jan 20, 2016 at 3:07 PM, peter dalgaard <pdalgd at gmail.com>
>> > wrote:
>> > >
>> > > > On 19 Jan 2016, at 20:48 , Dalthorp, Daniel <ddalthorp at usgs.gov>
>> > wrote:
>> > > >
>> > > > Does anyone know a simple way to create a tcltk table with columns
>> of
>> > > > varying widths?
>> > >
>> > > Create a table, then set the width of the columns with the width
>> > subcommand?
>> > >
>> > > -pd
>> > >
>> > >
>> > > pathName width ?col? ?value col value ...? If no col is specified,
>> > returns a list describing all cols for which a width has been set. If
>> col
>> > is specified with no value, it prints out the width of that col in
>> > characters (positive number) or pixels (negative number). If one or more
>> > col-value pairs are specified, then it sets each col to be that width in
>> > characters (positive number) or pixels (negative number). If value is
>> > default, then the col uses the default width, specified by -colwidth.
>> > >
>> > >
>> > >
>> > > >
>> > > > -Dan
>> > > >
>> > > >
>> > > >
>> > > > --
>> > > > Dan Dalthorp, PhD
>> > > > USGS Forest and Rangeland Ecosystem Science Center
>> > > > Forest Sciences Lab, Rm 189
>> > > > 3200 SW Jefferson Way
>> > > > Corvallis, OR 97331
>> > > > ph: 541-750-0953
>> > > > ddalthorp at usgs.gov
>> > > >
>> > > >       [[alternative HTML version deleted]]
>> > > >
>> > > > ______________________________________________
>> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > > > and provide commented, minimal, self-contained, reproducible code.
>> > >
>> > > --
>> > > Peter Dalgaard, Professor,
>> > > Center for Statistics, Copenhagen Business School
>> > > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> > > Phone: (+45)38153501
>> > > Office: A 4.23
>> > > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > >
>> > > --
>> > > Dan Dalthorp, PhD
>> > > USGS Forest and Rangeland Ecosystem Science Center
>> > > Forest Sciences Lab, Rm 189
>> > > 3200 SW Jefferson Way
>> > > Corvallis, OR 97331
>> > > ph: 541-750-0953
>> > > ddalthorp at usgs.gov
>> > >
>> >
>> > --
>> > Peter Dalgaard, Professor,
>> > Center for Statistics, Copenhagen Business School
>> > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> > Phone: (+45)38153501
>> > Office: A 4.23
>> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>>
>>
>> --
>> Dan Dalthorp, PhD
>> USGS Forest and Rangeland Ecosystem Science Center
>> Forest Sciences Lab, Rm 189
>> 3200 SW Jefferson Way
>> Corvallis, OR 97331
>> ph: 541-750-0953
>> ddalthorp at usgs.gov
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Fri Jan 22 20:50:55 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 22 Jan 2016 20:50:55 +0100
Subject: [R] tcltk tkwidget(..."table")
In-Reply-To: <CACa4aQ5HUZn_7QWFP3d0v1X5DTPZtunZXyecpm-_Wkf=WL_Gqg@mail.gmail.com>
References: <CAJeYpE_R=9O+7Dp+8+UyUGyH=-P=1ixYqbgu0JPcreV=hOf9Nw@mail.gmail.com>
	<6FEECD0A-6905-4DEF-A72B-96F2A6FDA827@gmail.com>
	<CAJeYpE8Bm1QRTk6F5hTuTr24t25XrG97k8Cjz0w25FOBQFaPSQ@mail.gmail.com>
	<624FF320-0BD3-492B-9FD8-20514174E3F5@gmail.com>
	<CAJeYpE_CAX+-gOpQ1x7FLdL74_Co_wyXscVfFokP6KFf+ZYfXw@mail.gmail.com>
	<CACa4aQ5HUZn_7QWFP3d0v1X5DTPZtunZXyecpm-_Wkf=WL_Gqg@mail.gmail.com>
Message-ID: <5A89C4C8-3487-4869-ACB5-B290C51DB848@gmail.com>

Good pointers, although R bindings document is not quite getting into the really messy bits. 

I think Daniel may be at a point where he needs to study the code snippets that he has got to work and figure out why and how they work. The help(TclInterface) page is the most definitive documentation for this, although it is admittedly rather dense (in both senses). E.g., it is hardly obvious that it is the distinction between string objects and lists of length one that decides whether Tcl displays stuff in braces.... (That particular point is something that I probably could patch up when/if I get a spare moment.)

-pd


> On 22 Jan 2016, at 11:49 , Adrian Waddell <adrian.waddell at gmail.com> wrote:
> 
> I have written some content about the R bindings to Tcl and Tk:
> 
> http://waddella.github.io/loon/learn_R_tcltk.html
> 
> And the pack geometry manager
> 
> http://adrian.waddell.ch/EssentialSoftware/Rtcltk_geometry.pdf
> 
> But if you have no experience with building graphical user interfaces then you might want to look into a book on Tk. "Effective TCL/TK programming" by Harrison and McLennan is a good one.
> 
> Greetings,
> 
> Adrian
> 
> On Jan 21, 2016 10:36 PM, "Dalthorp, Daniel" <ddalthorp at usgs.gov> wrote:
> >  Once you're up to speed on those issues...
> 
> Any suggestions for getting up to speed on those issues?
> 
> 
> 
> On Thu, Jan 21, 2016 at 11:46 AM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> >
> > > On 21 Jan 2016, at 00:25 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> > >
> > > Thanks, Peter.
> > >
> > > I'm sure that's right, but it requires knowing: (1) that there's
> > something called the "width subcommand", and (2) how to format the call to
> > that command/subcommand.
> > >
> >
> > Yes, there's a fair amount of that going on with the tcltk interface. You
> > need to both grasp the rules for passing arguments to the underlying Tcl
> > command, and know how to find and read the Tcl/Tk documentation. Once
> > you're up to speed on those issues it's not all that hard to find stuff in
> > (for the present case), say,
> > http://tktable.sourceforge.net/tktable/doc/tkTable.html
> >
> > The situation may be unfortunate, but the alternative is for "someone" to
> > sit down an convert all relevant Tcl/Tk documentation to R help files.
> >
> > -pd
> >
> >
> > > I was able to do it eventually but only after a few hours of effort
> > searching the web for help.
> > >
> > > E.g. with a table (called table1) with 3 columns and want to set widths
> > to 30, 5, and 5:
> > >
> > > colwidths<-c(30, 5, 5)
> > >
> > > for(i in 1:3) {
> > >   tcl(table1, "width", i - 1, colwidths[i])
> > > }
> > >
> > >
> > >
> > > On Wed, Jan 20, 2016 at 3:07 PM, peter dalgaard <pdalgd at gmail.com>
> > wrote:
> > >
> > > > On 19 Jan 2016, at 20:48 , Dalthorp, Daniel <ddalthorp at usgs.gov>
> > wrote:
> > > >
> > > > Does anyone know a simple way to create a tcltk table with columns of
> > > > varying widths?
> > >
> > > Create a table, then set the width of the columns with the width
> > subcommand?
> > >
> > > -pd
> > >
> > >
> > > pathName width ?col? ?value col value ...? If no col is specified,
> > returns a list describing all cols for which a width has been set. If col
> > is specified with no value, it prints out the width of that col in
> > characters (positive number) or pixels (negative number). If one or more
> > col-value pairs are specified, then it sets each col to be that width in
> > characters (positive number) or pixels (negative number). If value is
> > default, then the col uses the default width, specified by -colwidth.
> > >
> > >
> > >
> > > >
> > > > -Dan
> > > >
> > > >
> > > >
> > > > --
> > > > Dan Dalthorp, PhD
> > > > USGS Forest and Rangeland Ecosystem Science Center
> > > > Forest Sciences Lab, Rm 189
> > > > 3200 SW Jefferson Way
> > > > Corvallis, OR 97331
> > > > ph: 541-750-0953
> > > > ddalthorp at usgs.gov
> > > >
> > > >       [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > --
> > > Peter Dalgaard, Professor,
> > > Center for Statistics, Copenhagen Business School
> > > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > > Phone: (+45)38153501
> > > Office: A 4.23
> > > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > --
> > > Dan Dalthorp, PhD
> > > USGS Forest and Rangeland Ecosystem Science Center
> > > Forest Sciences Lab, Rm 189
> > > 3200 SW Jefferson Way
> > > Corvallis, OR 97331
> > > ph: 541-750-0953
> > > ddalthorp at usgs.gov
> > >
> >
> > --
> > Peter Dalgaard, Professor,
> > Center for Statistics, Copenhagen Business School
> > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > Phone: (+45)38153501
> > Office: A 4.23
> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> 
> 
> --
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ebs15242 at gmail.com  Fri Jan 22 20:58:24 2016
From: ebs15242 at gmail.com (Ed Siefker)
Date: Fri, 22 Jan 2016 13:58:24 -0600
Subject: [R] aggregate and the $ operator
In-Reply-To: <CAKq2vL40dRiUw-twPZVsJvR_=wGEiZC59zS=CQ_2gcHGmMRj-A@mail.gmail.com>
References: <CALRb-ofJ5JM22Puf4cbJ=SpPQDxzj14akdb44i_p0v40xXLtYw@mail.gmail.com>
	<CAKq2vL40dRiUw-twPZVsJvR_=wGEiZC59zS=CQ_2gcHGmMRj-A@mail.gmail.com>
Message-ID: <CALRb-od6zYuPpOK7JfAxFoo9TdTkwG1DnJAdSy9OXW8nmx_JAA@mail.gmail.com>

So that's how that works!  Thanks.

On Fri, Jan 22, 2016 at 1:32 PM, Joe Ceradini <joeceradini at gmail.com> wrote:
> Does this do what you want?
>
> aggregate(Nuclei ~ Slide, example, sum)
>
> On Fri, Jan 22, 2016 at 12:20 PM, Ed Siefker <ebs15242 at gmail.com> wrote:
>>
>> Aggregate does the right thing with column names when passing it
>> numerical coordinates.
>> Given a dataframe like this:
>>
>>   Nuclei Positive Nuclei Slide
>> 1    133              96    A1
>> 2     96              70    A1
>> 3     62              52    A2
>> 4     60              50    A2
>>
>> I can call 'aggregate' like this:
>>
>> > aggregate(example[1], by=example[3], sum)
>>   Slide Nuclei
>> 1    A1    229
>> 2    A2    122
>>
>> But that means I have to keep track of which column is which number.
>> If I try it the
>> easy way, it doesn't keep track of column names and it forces me to
>> coerce the 'by'
>> to a list.
>>
>> > aggregate(example$Nuclei, by=list(example$Slide), sum)
>>   Group.1   x
>> 1      A1 229
>> 2      A2 122
>>
>> Is there a better way to do this?  Thanks
>> -Ed
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> Cooperative Fish and Wildlife Research Unit
> Zoology and Physiology Dept.
> University of Wyoming
> JoeCeradini at gmail.com / 914.707.8506
> wyocoopunit.org
>


From ddalthorp at usgs.gov  Fri Jan 22 21:25:49 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Fri, 22 Jan 2016 12:25:49 -0800
Subject: [R] tcltk table "validateCommand"
Message-ID: <CAJeYpE-1O4AL00pnhtX8Nx=6gTe9ha5RXKLR3Va8OkvHfDb=Lw@mail.gmail.com>

I'd like to allow users to edit data in tcltk tables and to use vcmd to
validate data entry, e.g., not allowing non-numbers to be entered in
numeric cells and not allowing '\n' to be entered in text cells.

The problem is that I can't figure out how to "see" their data entry before
it is entered, although it looks like %S can be somehow used in vcmd to get
this information.

Example: to disallow '\n' to be entered into a cell in an editable table:

require(tcltk2)
tt<-tktoplevel(); tfr<-tkframe(tt); tkgrid(tfr)
tableData<-tclArray()
tableData[[0,0]]<-"junk"

CellValidation<-function(){

## http://www.tcl.tk/community/hobbs/tcl/capp/tkTable/tkTable.html says:
## *%S* For *ValidateCommand*, it is the potential new value of the cell
being validated.
## which is exactly what I want, but I can't figure out how to do that.
## The following allows one bad character and then disallows further edits

  testval<-tclvalue(tcl(table1,"curvalue"))

  if (length(grep("\n",testval))>0)  return(tcl("expr", FALSE))  else
 return(tcl("expr", TRUE))
}

table1<<-tk2table(tfr,
  rows=1,cols=1,
  selectmode="extended",
  variable=tableData,
  validate=T,
  vcmd=CellValidation
)

tcl(table1,"tag","configure", "active", fg='black',bg=colors()[411])
tkgrid(table1)

How can I get the %S value rather than the tcl(table1,"curvalue")?

Much thanks for any help.

-Dan

	[[alternative HTML version deleted]]


From jun.shen.ut at gmail.com  Fri Jan 22 21:31:54 2016
From: jun.shen.ut at gmail.com (Jun Shen)
Date: Fri, 22 Jan 2016 15:31:54 -0500
Subject: [R] Dimension of quantile function output
Message-ID: <CAMCXXmpN7VR3+D3NPzUV3cjBKvAisHpFzyj7zjyjdrCwaWiYOg@mail.gmail.com>

Dear list,

Say I have some quantile operation like this

data.frame(ID=rep(1:10,each=10),CONC=runif(100)) -> test

lapply(c('mean','sd','quantile'), function(x)
aggregate(test['CONC'],by=test['ID'],FUN=x)) -> temp

The output temp is a list of three elements. I would like to merge the
three elements into one data frame. The first two elements (the mean and sd
output) are easy to handle that I just need to update the column names. The
problem is the third element. Although it looks like a data frame in
dimension of 10 by 6 but really is just 10 by 2. How do I coerce the third
element in 10 by 6? Thanks a lot.

Jun


> head(temp[[3]])  ID     CONC.0%    CONC.25%    CONC.50%    CONC.75%   CONC.100%
1  1 0.024352714 0.235039202 0.662819513 0.737295381 0.934520832
2  2 0.004732985 0.246812366 0.399628344 0.663579348 0.942850115
3  3 0.240309454 0.445094065 0.593619958 0.815292642 0.985080817
4  4 0.070251047 0.173052744 0.591731071 0.706560762 0.929560236

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Fri Jan 22 21:53:08 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Fri, 22 Jan 2016 20:53:08 +0000
Subject: [R] Dimension of quantile function output
In-Reply-To: <CAMCXXmpN7VR3+D3NPzUV3cjBKvAisHpFzyj7zjyjdrCwaWiYOg@mail.gmail.com>
Message-ID: <20160122205308.Horde.MYIW8R2sQv3tHiY6KEoLQ53@mail.sapo.pt>

Hello,

Maybe something like the following.

temp2 <- cbind(temp[[3]][, 1], temp[[3]][, 2])
str(temp2)

Hope this helps,

Rui Barradas
?

Citando Jun Shen <jun.shen.ut at gmail.com>:

> Dear list,
>
> Say I have some quantile operation like this
>
> data.frame(ID=rep(1:10,each=10),CONC=runif(100)) -> test
>
> lapply(c('mean','sd','quantile'), function(x)
> aggregate(test['CONC'],by=test['ID'],FUN=x)) -> temp
>
> The output temp is a list of three elements. I would like to merge the
> three elements into one data frame. The first two elements (the mean and sd
> output) are easy to handle that I just need to update the column names. The
> problem is the third element. Although it looks like a data frame in
> dimension of 10 by 6 but really is just 10 by 2. How do I coerce the third
> element in 10 by 6? Thanks a lot.
>
> Jun
> ?
>> head(temp[[3]])? ID? ? ?CONC.0%? ? CONC.25%? ? CONC.50%? ?  
>> CONC.75%? ?CONC.100%
>
> 1? 1 0.024352714 0.235039202 0.662819513 0.737295381 0.934520832
> 2? 2 0.004732985 0.246812366 0.399628344 0.663579348 0.942850115
> 3? 3 0.240309454 0.445094065 0.593619958 0.815292642 0.985080817
> 4? 4 0.070251047 0.173052744 0.591731071 0.706560762 0.929560236
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Jan 22 22:38:02 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 22 Jan 2016 13:38:02 -0800
Subject: [R] aggregate and the $ operator
In-Reply-To: <CALRb-ofJ5JM22Puf4cbJ=SpPQDxzj14akdb44i_p0v40xXLtYw@mail.gmail.com>
References: <CALRb-ofJ5JM22Puf4cbJ=SpPQDxzj14akdb44i_p0v40xXLtYw@mail.gmail.com>
Message-ID: <CAF8bMcYC4dLK6=4fxvNZAoBZ48KsjwVtafM-RkNZODa+WU67Qg@mail.gmail.com>

Using column names where you used column numbers would work:

example <- data.frame(
    check.names = FALSE,
    Nuclei = c(133L, 96L, 62L, 60L),
    `Positive Nuclei` = c(96L, 70L, 52L, 50L),
    Slide = factor(c("A1", "A1", "A2", "A2"), levels = c("A1", "A2")))
aggregate(example["Nuclei"], by=example["Slide"], sum)
#  Slide Nuclei
#1    A1    229
#2    A2    122
aggregate(example[1], by=example[3], sum)
#  Slide Nuclei
#1    A1    229
#2    A2    122

Many people find that the functions in the dplyr or plyr packages
are worth the trouble to learn about.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Jan 22, 2016 at 11:20 AM, Ed Siefker <ebs15242 at gmail.com> wrote:

> Aggregate does the right thing with column names when passing it
> numerical coordinates.
> Given a dataframe like this:
>
>   Nuclei Positive Nuclei Slide
> 1    133              96    A1
> 2     96              70    A1
> 3     62              52    A2
> 4     60              50    A2
>
> I can call 'aggregate' like this:
>
> > aggregate(example[1], by=example[3], sum)
>   Slide Nuclei
> 1    A1    229
> 2    A2    122
>
> But that means I have to keep track of which column is which number.
> If I try it the
> easy way, it doesn't keep track of column names and it forces me to
> coerce the 'by'
> to a list.
>
> > aggregate(example$Nuclei, by=list(example$Slide), sum)
>   Group.1   x
> 1      A1 229
> 2      A2 122
>
> Is there a better way to do this?  Thanks
> -Ed
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From amoy_y at yahoo.com  Fri Jan 22 23:31:45 2016
From: amoy_y at yahoo.com (Amoy Yang)
Date: Fri, 22 Jan 2016 22:31:45 +0000 (UTC)
Subject: [R] Error opening SHP file
In-Reply-To: <901C57CE-5861-4AB2-AB61-B8E675DE2654@comcast.net>
References: <901C57CE-5861-4AB2-AB61-B8E675DE2654@comcast.net>
Message-ID: <217687193.7561711.1453501905306.JavaMail.yahoo@mail.yahoo.com>

Let me just make this case simple and quick to address what I need.
Giving a data-file that includes tow columns: zip5 and population. How do I put pop (colored with different segments) by zips (with boundary) on the USA map with R?
Amoy 

    On Friday, January 22, 2016 12:45 PM, David Winsemius <dwinsemius at comcast.net> wrote:
 

 
> On Jan 22, 2016, at 9:24 AM, Amoy Yang via R-help <r-help at r-project.org> wrote:
> 
> This is the results that addresses David's advice.
>> library(maptools)
>> library(maps)
>> state.map <- readShapeSpatial("maps/st24_d00.shp")
> Error in getinfo.shape(fn) : Error opening SHP file
>> # David question: What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?
>> list.files('maps')
> character(0)
> I actually use the link below to learn how to load state/zip-data on USA map.

That link says:

"For example (assumes you have the maryland shapefiles in the map subdirectory):"

So it did not purport to tell you how to put shape files in that subdirectory. That statement assumed you understood basic OS path naming conventions in your unstated OS and how to move files around, which is not a topic for rhelp.

Try working through the examples in the documents that come with the `sp`-package.

-- 
David.
> 
> # http://stackoverflow.com/questions/1441717/plotting-color-map-with-zip-codes-in-r-or-python
> Plotting color map with zip codes in R or Python
> |? |
> |? |? |? |? |? |? |? |
> | Plotting color map with zip codes in R or PythonI have some US demographic and firmographic data. I would like to plot zipcode areas in a state or a smaller region (e.g. city). Each area would be annotated by col... |
> |? |
> | View on stackoverflow.com | Preview by Yahoo |
> |? |
> |? |
> 
>? ? On Friday, January 22, 2016 10:03 AM, Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
> 
> 
> We can duplicate the error by giving a path to a non-existent
> shapefile, which is probably the original problem:
> 
>> require(maptools)
> Loading required package: maptools
> Loading required package: sp
> Checking rgeos availability: TRUE
>> foo=readShapeSpatial("fnord.shp")
> Error in getinfo.shape(fn) : Error opening SHP file
> 
> The error message there isn't totally explicit, and might cover a
> range of other possibilities such as a corrupted shapefile, or a
> missing .shx component of the shapefile or whatever.
> 
> BUT you probably shouldn't be using readShapeSpatial anyway, as it has
> a habit of not reading the coordinate system in the .prj file. I find
> it much easier to use `raster::shapefile` which *does* read the
> coordinate system *and* gives a more explicit error message for a
> missing shapefile:
> 
>> require(raster)
> Loading required package: raster
>> foo=shapefile("fnord.shp")
> Error in normalizePath(x, winslash = "/", mustWork = TRUE) :
>? path[1]="fnord.shp": No such file or directory
> 
> "No such file or directory"
> 
> Barry
> 
> 
> 
> On Fri, Jan 22, 2016 at 2:25 AM, boB Rudis <bob at rudis.net> wrote:
>> Agreed with the others. After finding that shapefile and getting it to
>> work you are definitely not in the proper working directory.
>> 
>> On Thu, Jan 21, 2016 at 8:40 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>> 
>>>> On Jan 21, 2016, at 4:39 PM, Amoy Yang via R-help <r-help at r-project.org> wrote:
>>>> 
>>>> Any advice for the following errors?
>>>> state.map <- readShapeSpatial("maps/st24_d00.shp")
>>>> Error in getinfo.shape(fn) : Error opening SHP file
>>> 
>>> What does list.files('maps') return? Is there a 'st24_d00.shp' value in there?
>>> 
>>> 
>>> --
>>> David.
>>> 
>>>> 
> 
> 
> 
> ??? [[alternative HTML version deleted]]
? ? ? ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


  
	[[alternative HTML version deleted]]


From bob at rudis.net  Fri Jan 22 23:43:19 2016
From: bob at rudis.net (boB Rudis)
Date: Fri, 22 Jan 2016 17:43:19 -0500
Subject: [R] Error opening SHP file
In-Reply-To: <217687193.7561711.1453501905306.JavaMail.yahoo@mail.yahoo.com>
References: <901C57CE-5861-4AB2-AB61-B8E675DE2654@comcast.net>
	<217687193.7561711.1453501905306.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAJ4QxaNstuV9Ye8sHhHukhZWb551NSGySJZ22t0rT88nQn7c3g@mail.gmail.com>

Look at the choroplethr package.

On Fri, Jan 22, 2016 at 5:31 PM, Amoy Yang <amoy_y at yahoo.com> wrote:
> Let me just make this case simple and quick to address what I need.
>
> Giving a data-file that includes tow columns: zip5 and population. How do I
> put pop (colored with different segments) by zips (with boundary) on the USA
> map with R?
>
> Amoy
>
>
> On Friday, January 22, 2016 12:45 PM, David Winsemius
> <dwinsemius at comcast.net> wrote:
>
>
>
>> On Jan 22, 2016, at 9:24 AM, Amoy Yang via R-help <r-help at r-project.org>
>> wrote:
>>
>> This is the results that addresses David's advice.
>>> library(maptools)
>>> library(maps)
>>> state.map <- readShapeSpatial("maps/st24_d00.shp")
>> Error in getinfo.shape(fn) : Error opening SHP file
>>> # David question: What does list.files('maps') return? Is there a
>>> 'st24_d00.shp' value in there?
>>> list.files('maps')
>> character(0)
>> I actually use the link below to learn how to load state/zip-data on USA
>> map.
>
> That link says:
>
> "For example (assumes you have the maryland shapefiles in the map
> subdirectory):"
>
> So it did not purport to tell you how to put shape files in that
> subdirectory. That statement assumed you understood basic OS path naming
> conventions in your unstated OS and how to move files around, which is not a
> topic for rhelp.
>
> Try working through the examples in the documents that come with the
> `sp`-package.
>
>
> --
> David.
>>
>> #
>> http://stackoverflow.com/questions/1441717/plotting-color-map-with-zip-codes-in-r-or-python
>> Plotting color map with zip codes in R or Python
>> |  |
>> |  |  |  |  |  |  |  |
>> | Plotting color map with zip codes in R or PythonI have some US
>> demographic and firmographic data. I would like to plot zipcode areas in a
>> state or a smaller region (e.g. city). Each area would be annotated by
>> col... |
>> |  |
>> | View on stackoverflow.com | Preview by Yahoo |
>> |  |
>> |  |
>>
>>    On Friday, January 22, 2016 10:03 AM, Barry Rowlingson
>> <b.rowlingson at lancaster.ac.uk> wrote:
>>
>>
>> We can duplicate the error by giving a path to a non-existent
>> shapefile, which is probably the original problem:
>>
>>> require(maptools)
>> Loading required package: maptools
>> Loading required package: sp
>> Checking rgeos availability: TRUE
>>> foo=readShapeSpatial("fnord.shp")
>> Error in getinfo.shape(fn) : Error opening SHP file
>>
>> The error message there isn't totally explicit, and might cover a
>> range of other possibilities such as a corrupted shapefile, or a
>> missing .shx component of the shapefile or whatever.
>>
>> BUT you probably shouldn't be using readShapeSpatial anyway, as it has
>> a habit of not reading the coordinate system in the .prj file. I find
>> it much easier to use `raster::shapefile` which *does* read the
>> coordinate system *and* gives a more explicit error message for a
>> missing shapefile:
>>
>>> require(raster)
>> Loading required package: raster
>>> foo=shapefile("fnord.shp")
>> Error in normalizePath(x, winslash = "/", mustWork = TRUE) :
>>  path[1]="fnord.shp": No such file or directory
>>
>> "No such file or directory"
>>
>> Barry
>>
>>
>>
>> On Fri, Jan 22, 2016 at 2:25 AM, boB Rudis <bob at rudis.net> wrote:
>>> Agreed with the others. After finding that shapefile and getting it to
>>> work you are definitely not in the proper working directory.
>>>
>>> On Thu, Jan 21, 2016 at 8:40 PM, David Winsemius <dwinsemius at comcast.net>
>>> wrote:
>>>>
>>>>> On Jan 21, 2016, at 4:39 PM, Amoy Yang via R-help
>>>>> <r-help at r-project.org> wrote:
>>>>>
>>>>> Any advice for the following errors?
>>>>> state.map <- readShapeSpatial("maps/st24_d00.shp")
>>>>> Error in getinfo.shape(fn) : Error opening SHP file
>>>>
>>>> What does list.files('maps') return? Is there a 'st24_d00.shp' value in
>>>> there?
>>>>
>>>>
>>>> --
>>>> David.
>>>>
>>>>>
>>
>>
>>
>>     [[alternative HTML version deleted]]
>       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>


From rsherry8 at comcast.net  Sat Jan 23 01:21:39 2016
From: rsherry8 at comcast.net (Robert Sherry)
Date: Fri, 22 Jan 2016 19:21:39 -0500
Subject: [R] Sorting a Data Frame
Message-ID: <56A2C793.6080307@comcast.net>

In R, I run the following commands:
     df = data.frame( x=runif(10), y=runif(10) )
     df2 = df[order(x),]

The first, as I would expect, creates a data frame with two columns and 
10 rows. I expect the second to sort the data based upon
the columns x and produce a new data frame, df2, with the same size as 
df. However, the data frame is produces is much larger.
I do not understand what is going on. I am hoping somebody can help me. 
I am also wondering if I should have a comma after
order(x) in the second statement. I do not see a purpose for it but it 
was in an example on the web.

Thanks
Bob


From ligges at statistik.tu-dortmund.de  Sat Jan 23 01:28:45 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sat, 23 Jan 2016 01:28:45 +0100
Subject: [R] Sorting a Data Frame
In-Reply-To: <56A2C793.6080307@comcast.net>
References: <56A2C793.6080307@comcast.net>
Message-ID: <56A2C93D.6090008@statistik.tu-dortmund.de>



On 23.01.2016 01:21, Robert Sherry wrote:
> In R, I run the following commands:
>      df = data.frame( x=runif(10), y=runif(10) )
>      df2 = df[order(x),]


You use another x from your workspace, you actually want to


  df2 = df[order(df[,"x"]),]

Best,
Uwe Ligges


>
> The first, as I would expect, creates a data frame with two columns and
> 10 rows. I expect the second to sort the data based upon
> the columns x and produce a new data frame, df2, with the same size as
> df. However, the data frame is produces is much larger.
> I do not understand what is going on. I am hoping somebody can help me.
> I am also wondering if I should have a comma after
> order(x) in the second statement. I do not see a purpose for it but it
> was in an example on the web.
>
> Thanks
> Bob
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dupouey at nancy.inra.fr  Sat Jan 23 13:28:37 2016
From: dupouey at nancy.inra.fr (Jean-Luc Dupouey)
Date: Sat, 23 Jan 2016 13:28:37 +0100
Subject: [R] R-help mailing list activity
Message-ID: <56A371F5.5070509@nancy.inra.fr>

Dear members,

Not a technical question:

The number of threads in this mailing list, following a long period of 
increase, has been regularly and strongly decreasing since 2010, passing 
from more than 40K threads to less than 11K threads last year. The trend 
is similar for most of the "ancient" mailing lists of the R-project. I 
cannot imagine the total number of R-related inquiries on the Internet 
decreased. It means that contributors have gone elsewhere. Indeed, in 
the meantime, the number of R posts on stackoverflow passed from 2K to 
100K between 2009 and 2015. Thus my question: what are the 
specificities, the plus and minus of the R-project mailing lists, in 
comparison with other lists, and especially in comparison with 
stackoverflow? A lot of threads are duplicated on both lists, which 
seems to me a little bit counterproductive.

I hope it is the wright place to ask this question. Thanks in advance,

Jean-Luc Dupouey


From thpe at simecol.de  Sat Jan 23 13:39:18 2016
From: thpe at simecol.de (Thomas Petzoldt)
Date: Sat, 23 Jan 2016 13:39:18 +0100
Subject: [R] R-help mailing list activity
In-Reply-To: <56A371F5.5070509@nancy.inra.fr>
References: <56A371F5.5070509@nancy.inra.fr>
Message-ID: <56A37476.9030800@simecol.de>

Hi,

from my perspective as R user and package maintainer I would consider 
the normalization of the r-help mailing list a good sign. r-help is 
still a good place for general questions, while more specific 
discussions moved to the r-sig-... mailing lists.

Maybe a slight reduction can also be a motivation for more people to 
step in again answering questions.

Thomas

Am 23.01.2016 um 13:28 schrieb Jean-Luc Dupouey:
> Dear members,
>
> Not a technical question:
>
> The number of threads in this mailing list, following a long period of
> increase, has been regularly and strongly decreasing since 2010, passing
> from more than 40K threads to less than 11K threads last year. The trend
> is similar for most of the "ancient" mailing lists of the R-project. I
> cannot imagine the total number of R-related inquiries on the Internet
> decreased. It means that contributors have gone elsewhere. Indeed, in
> the meantime, the number of R posts on stackoverflow passed from 2K to
> 100K between 2009 and 2015. Thus my question: what are the
> specificities, the plus and minus of the R-project mailing lists, in
> comparison with other lists, and especially in comparison with
> stackoverflow? A lot of threads are duplicated on both lists, which
> seems to me a little bit counterproductive.
>
> I hope it is the wright place to ask this question. Thanks in advance,
>
> Jean-Luc Dupouey
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Sat Jan 23 14:33:50 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 23 Jan 2016 08:33:50 -0500
Subject: [R] R-help mailing list activity
In-Reply-To: <56A371F5.5070509@nancy.inra.fr>
References: <56A371F5.5070509@nancy.inra.fr>
Message-ID: <56A3813E.50107@gmail.com>

On 23/01/2016 7:28 AM, Jean-Luc Dupouey wrote:
> Dear members,
>
> Not a technical question:
>
> The number of threads in this mailing list, following a long period of
> increase, has been regularly and strongly decreasing since 2010, passing
> from more than 40K threads to less than 11K threads last year. The trend
> is similar for most of the "ancient" mailing lists of the R-project. I
> cannot imagine the total number of R-related inquiries on the Internet
> decreased. It means that contributors have gone elsewhere. Indeed, in
> the meantime, the number of R posts on stackoverflow passed from 2K to
> 100K between 2009 and 2015. Thus my question: what are the
> specificities, the plus and minus of the R-project mailing lists, in
> comparison with other lists, and especially in comparison with
> stackoverflow? A lot of threads are duplicated on both lists, which
> seems to me a little bit counterproductive.

I don't see duplication as counterproductive -- some people like one 
style, some like the other, both will find answers.

However, I think there is less duplication than you might think in many 
areas.  Mailing lists are preferable when the people who are good at 
answering your questions use the mailing lists; Stackoverflow is 
preferable when the good answers are there.

I generally prefer the mailing lists, though I occasionally participate 
on Stackoverflow.  The reasons I prefer them:

  1. Permanence.  If Stackoverflow shuts down tomorrow, all posts there 
will likely disappear.  There are several locations that archive the 
mailing list posts. I have local copies of a few thousand posts on my 
own laptop.

  2. Familiarity.  I've been using the mailing lists for 20 years, and 
its easier to continue than to change.  If you're more familiar with the 
Stackoverflow process, you'll probably prefer that.

  3. Simplicity.  This may be a repeat of 2, but the Stackoverflow 
distinction between answers and comments, it's gamification (badges, 
special privileges to high scorers, etc.) just seems unnecessarily ornate.

  4. Interaction.  The mailing lists are a series of conversations, 
whereas Stackoverflow is more like Wikipedia, i.e. a joint project to 
which you can contribute.  (Maybe there are conversations on 
Stackoverflow as well, but I'm not a big enough user to know about them.)

If I look at my own recent record, I tend to answer far more questions 
on the mailing lists, but ask more on Stackoverflow.  I think this is 
due to my original point:  the experts in the topics I'm asking about 
are more likely to be there than here.

Duncan Murdoch

P.S. Your statistics are a little misleading:  you counted threads in 
one R mailing list in one year, and cumulative questions in all R topics 
over 7 years in Stackoverflow, so the difference in traffic isn't as 
large as your numbers look at first glance.  However, I think it is true 
that the mailing list traffic declined and Stackoverflow increased over 
that period.


From rshepard at appl-ecosys.com  Sat Jan 23 14:53:58 2016
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Sat, 23 Jan 2016 05:53:58 -0800 (PST)
Subject: [R] R-help mailing list activity
In-Reply-To: <56A3813E.50107@gmail.com>
References: <56A371F5.5070509@nancy.inra.fr> <56A3813E.50107@gmail.com>
Message-ID: <alpine.LNX.2.11.1601230546520.17010@localhost>

On Sat, 23 Jan 2016, Duncan Murdoch wrote:

> I don't see duplication as counterproductive -- some people like one style, 
> some like the other, both will find answers.

Duncan,

   There's another factor to add to your list. Mail lists, such as r-help and
the various SIGs _push_ messages to subscribers' mail boxes. Check your mail
and the threads can be followed. From the subscriber's perspective it's
passive.

   Web fora require subscribers to _pull_ messages by pointing their browser
to that URL, logging in, finding the appropriate forum, and viewing threads.
>From the subscriber's perspective it's active.

   I'm one of the former types of participant. I subscribe to multiple mail
lists and review new messages several times a day when time permits or I
have another reason to do so. I'm rarely on a web forum because it requires
much more time away from business than does a mail list.

Just another perspective,

Rich


From jrkrideau at inbox.com  Sat Jan 23 15:12:15 2016
From: jrkrideau at inbox.com (John Kane)
Date: Sat, 23 Jan 2016 06:12:15 -0800
Subject: [R] R-help mailing list activity
In-Reply-To: <alpine.LNX.2.11.1601230546520.17010@localhost>
References: <56a371f5.5070509@nancy.inra.fr> <56a3813e.50107@gmail.com>
Message-ID: <3BC2405614B.000001BEjrkrideau@inbox.com>

While I find stackoverflow very handy of a specific question that I can google in a hurry I find that I can readily read R-help, learning things that may not be readily applicable but that I may need tomorrow or next year ... 

It also helps me keep up on new packages and besides I have an embarrassing fortune that I like to point to. 

I also agree strongly with Duncan Murdock's points 3 & 4.



John Kane
Kingston ON Canada


> -----Original Message-----
> From: rshepard at appl-ecosys.com
> Sent: Sat, 23 Jan 2016 05:53:58 -0800 (PST)
> To: r-help at r-project.org
> Subject: Re: [R] R-help mailing list activity
> 
> On Sat, 23 Jan 2016, Duncan Murdoch wrote:
> 
>> I don't see duplication as counterproductive -- some people like one
>> style,
>> some like the other, both will find answers.
> 
> Duncan,
> 
>    There's another factor to add to your list. Mail lists, such as r-help
> and
> the various SIGs _push_ messages to subscribers' mail boxes. Check your
> mail
> and the threads can be followed. From the subscriber's perspective it's
> passive.
> 
>    Web fora require subscribers to _pull_ messages by pointing their
> browser
> to that URL, logging in, finding the appropriate forum, and viewing
> threads.
> >From the subscriber's perspective it's active.
> 
>    I'm one of the former types of participant. I subscribe to multiple
> mail
> lists and review new messages several times a day when time permits or I
> have another reason to do so. I'm rarely on a web forum because it
> requires
> much more time away from business than does a mail list.
> 
> Just another perspective,
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From murdoch.duncan at gmail.com  Sat Jan 23 15:18:33 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 23 Jan 2016 09:18:33 -0500
Subject: [R] R-help mailing list activity
In-Reply-To: <56A3813E.50107@gmail.com>
References: <56A371F5.5070509@nancy.inra.fr> <56A3813E.50107@gmail.com>
Message-ID: <56A38BB9.6070804@gmail.com>

One additional point:

On 23/01/2016 8:33 AM, Duncan Murdoch wrote:

> distinction between answers and comments, it's gamification (badges,

One advantage of Stackoverflow is that you can go back and correct silly 
errors (like misspelling "its").

Duncan Murdoch


From dwinsemius at comcast.net  Sat Jan 23 18:40:33 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 23 Jan 2016 09:40:33 -0800
Subject: [R] Constrained Poisson model / Bayesian Poisson model
In-Reply-To: <20160123142400.texehipssg0c0088@imap.uni-ulm.de>
References: <20160123142400.texehipssg0c0088@imap.uni-ulm.de>
Message-ID: <248806F6-7E95-4E05-B77D-5F12C307DF1E@comcast.net>


> On Jan 23, 2016, at 5:24 AM, mara.pfleiderer at uni-ulm.de wrote:
> 
> Hi David,
> 
> I'm sorry. I'm not familiar with posting problems on helppages.
> As the data I deal with is confidential I can't provide all details,
> but I will try to be as precise as possible about my problem:
> 
> As I said I'm working on a Poisson regression model with a linear
> predictor and the identity as link function.

Then that is not what most people would call a "Poisson regression model".

In particular: I have data which consists of observations of two random variables X and Y over the period of about 3 months. I have clustered this data into hours, so I received 2088 observations of those variables and each observation represents the values of the variables in one hour. Now I assume Y to be Poisson distributed (Y_i~Poi(lambda_i)) and that the values of Y come from two different impacts, so I split Y_i into two Poisson-distributed variables Y_i1 and Y_i2. I assume that the values of X have a long-term effect (of at least one day) on the part Y_i1 and I have estimators for the parameters lambda_i2, but I have no exact values of the variables of Y_i1 and Y_i2 but only of Y_i as a whole.

> So my model looks as follows:
> fml=Y_i1 ~ b_1*X_i+....+b_n*X_(i-n+1) - lambda_i2 -1 with n>=24
> That means that the last 24 (or more) values of X influence the value of Y_i1 in an additive way and I have no intercept(b_0=0).
> Now I made a matrix whose rows represent Y_i, all of its 24 (or more) regressors for each observation of Y_i and the corresponding estimator lambda_i2.
> Then I used glm(fml, family=poisson(link="identity"), data=matrix) and tried it for different values of n (=24,48,36,...).

I worry that might not construct the model you described above. The use of `poisson` constructs Poisson distributed _errors_ with an additive link, and to my understanding does _not_ assume Poisson distributed Y values.


> But always some of the coefficients received negative values which doesnt't make sense in interpretation. (The values of X represent certain events which can only have a positive or none effect on the value of Y.)
> 
> Now I want to use the constraint b_i >=0 in my model, but I don't know how I can do this.
> 
> I also thought of analyzing this model in a Bayesian way, but yet I haven't found a Bayesian version of glm() for the Poisson distribution such that I can specify the prior for the b_i on my own. (Then I could include the positivity in the prior.) Do you have a hint for me?

The R blogoshere has been heralding the arrival of the `rstanarm` package which provides a glm-like interface to an MCMC (Bayesian) estimation engine. Version 2.90 is on CRAN. The list of authors in the description file is impressive: 

Authors at R: c(person("Jonah", "Gabry", email = "jsg2201 at columbia.edu", role = "aut"),
             person("Trustees of", "Columbia University", role = "cph"),
             person("R Core", "Deveopment Team", role = "cph", 
                     comment = "R/pp_data.R, R/stan_aov.R"),
             person("Douglas", "Bates", role = "cph", comment = "R/pp_data.R"),
             person("Martin", "Maechler", role = "cph", comment = "R/pp_data.R"),
             person("Ben", "Bolker", role = "cph", comment = "R/pp_data.R"),
             person("Steve", "Walker", role = "cph", comment = "R/pp_data.R"),
             person("Brian", "Ripley", role = "cph", 
                    comment = "R/stan_aov.R, R/stan_polr.R"),
             person("William", "Venables", role = "cph", comment = "R/stan_polr.R"),
             person("Ben", "Goodrich", email = "benjamin.goodrich at columbia.edu", 
                    role = c("cre", "aut")))

> 
> I'm sorry if I went too much into detail now...
> I hope you understand my point now and have some answers for me!
> 
> Best,
> Mara
> 
> 
> Zitat von David Winsemius <dwinsemius at comcast.net>:
> 
>> 
>>> On Jan 22, 2016, at 7:01 AM, mara.pfleiderer at uni-ulm.de wrote:
>>> 
>>> Hi all,
>>> 
>>> I am dealing with a problem about my linear Poisson regression   model (link function=identity).
>>> 
>>> I am using the glm()-function which results in negative   coefficients, but a negative influence of the regressors wouldn't   make sense.
>> 
>> Negative coefficients merely indicate a lower relative rate. You   need to be more specific about the exactly data and model output   before you can raise our concern to a level where further comment   can be made.
>> 
>> 
>>> 
>>> (i) Is there a possibility to set constraints on the regression   parameters in glm() such that all coefficients are positive? Or is   there another function in R for which this is possible?
>>> 
>>> (ii) Is there a Bayesian version of the glm()-function where I can   specify the prior distribution for my regression parameters? (e.g.   a Dirichlet prior s.t. the parameters are positive)
>>> 
>>> All this with respect to the linear Poisson model...
>> 
>> As I implied above, the word "linear" means something different than   "additive" when the link is log().
>> 
>> --
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Sat Jan 23 19:49:19 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 23 Jan 2016 10:49:19 -0800
Subject: [R] Constrained Poisson model / Bayesian Poisson model
In-Reply-To: <248806F6-7E95-4E05-B77D-5F12C307DF1E@comcast.net>
References: <20160123142400.texehipssg0c0088@imap.uni-ulm.de>
	<248806F6-7E95-4E05-B77D-5F12C307DF1E@comcast.net>
Message-ID: <5E5C6390-2546-4EF7-A983-6C7BD9DB4AA3@comcast.net>


> On Jan 23, 2016, at 9:40 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Jan 23, 2016, at 5:24 AM, mara.pfleiderer at uni-ulm.de wrote:
>> 
>> Hi David,
>> 
>> I'm sorry. I'm not familiar with posting problems on helppages.
>> As the data I deal with is confidential I can't provide all details,
>> but I will try to be as precise as possible about my problem:
>> 
>> As I said I'm working on a Poisson regression model with a linear
>> predictor and the identity as link function.
> 
> Then that is not what most people would call a "Poisson regression model".
> 
>> In particular: I have data which consists of observations of two random variables X and Y over the period of about 3 months. I have clustered this data into hours, so I received 2088 observations of those variables and each observation represents the values of the variables in one hour. Now I assume Y to be Poisson distributed (Y_i~Poi(lambda_i)) and that the values of Y come from two different impacts, so I split Y_i into two Poisson-distributed variables Y_i1 and Y_i2. I assume that the values of X have a long-term effect (of at least one day) on the part Y_i1 and I have estimators for the parameters lambda_i2, but I have no exact values of the variables of Y_i1 and Y_i2 but only of Y_i as a whole.
>> 
>> So my model looks as follows:
>> fml=Y_i1 ~ b_1*X_i+....+b_n*X_(i-n+1) - lambda_i2 -1 with n>=24
>> That means that the last 24 (or more) values of X influence the value of Y_i1 in an additive way and I have no intercept(b_0=0).
>> Now I made a matrix whose rows represent Y_i, all of its 24 (or more) regressors for each observation of Y_i and the corresponding estimator lambda_i2.
>> Then I used glm(fml, family=poisson(link="identity"), data=matrix) and tried it for different values of n (=24,48,36,...).
> 
> I worry that might not construct the model you described above. The use of `poisson` constructs Poisson distributed _errors_ with an additive link, and to my understanding does _not_ assume Poisson distributed Y values.
> 
> 
>> But always some of the coefficients received negative values which doesnt't make sense in interpretation. (The values of X represent certain events which can only have a positive or none effect on the value of Y.)

One further thought. Since your errors are being constructed from a non-negative distribution, and the Y|X_i dependence is linear, then it seems perfectly reasonable that some of the estimates (which are on the linear scale) might be forced to be negative despite this not being completely sensible in real-life. The errors would be force to be "centered" around the linear estimate and so if the "true" value were small and the errors were larger, then that would "push" the estimate down into negative territory. Just one of the dangers in choosing a non-canonical choice of link and error distribution that you as an analyst needs to accept.


>> 
>> Now I want to use the constraint b_i >=0 in my model, but I don't know how I can do this.
>> 
>> I also thought of analyzing this model in a Bayesian way, but yet I haven't found a Bayesian version of glm() for the Poisson distribution such that I can specify the prior for the b_i on my own. (Then I could include the positivity in the prior.) Do you have a hint for me?
> 
> The R blogoshere has been heralding the arrival of the `rstanarm` package which provides a glm-like interface to an MCMC (Bayesian) estimation engine. Version 2.90 is on CRAN.

The version on CRAN is actually 2.90-1 and after running the script to install from GitHub, you can have the 2.90-2 version.

https://github.com/stan-dev/rstanarm

The github route will require that you have the needed development platform for your OS.

The install log showed an error with failure to "find template reference-stan-manual" but that did not stop compilation and there seemed to be a substantial numer of vignettes to consult, including one entitled: "stan_glm: GLMs for Count Data"

Best of luck.

> The list of authors in the description file is impressive: 
> 
> Authors at R: c(person("Jonah", "Gabry", email = "jsg2201 at columbia.edu", role = "aut"),
>             person("Trustees of", "Columbia University", role = "cph"),
>             person("R Core", "Deveopment Team", role = "cph", 
>                     comment = "R/pp_data.R, R/stan_aov.R"),
>             person("Douglas", "Bates", role = "cph", comment = "R/pp_data.R"),
>             person("Martin", "Maechler", role = "cph", comment = "R/pp_data.R"),
>             person("Ben", "Bolker", role = "cph", comment = "R/pp_data.R"),
>             person("Steve", "Walker", role = "cph", comment = "R/pp_data.R"),
>             person("Brian", "Ripley", role = "cph", 
>                    comment = "R/stan_aov.R, R/stan_polr.R"),
>             person("William", "Venables", role = "cph", comment = "R/stan_polr.R"),
>             person("Ben", "Goodrich", email = "benjamin.goodrich at columbia.edu", 
>                    role = c("cre", "aut")))
> 
>> 
>> I'm sorry if I went too much into detail now...
>> I hope you understand my point now and have some answers for me!
>> 
>> Best,
>> Mara
>> 
>> 
>> Zitat von David Winsemius <dwinsemius at comcast.net>:
>> 
>>> 
>>>> On Jan 22, 2016, at 7:01 AM, mara.pfleiderer at uni-ulm.de wrote:
>>>> 
>>>> Hi all,
>>>> 
>>>> I am dealing with a problem about my linear Poisson regression   model (link function=identity).
>>>> 
>>>> I am using the glm()-function which results in negative   coefficients, but a negative influence of the regressors wouldn't   make sense.
>>> 
>>> Negative coefficients merely indicate a lower relative rate. You   need to be more specific about the exactly data and model output   before you can raise our concern to a level where further comment   can be made.
>>> 
>>> 
>>>> 
>>>> (i) Is there a possibility to set constraints on the regression   parameters in glm() such that all coefficients are positive? Or is   there another function in R for which this is possible?
>>>> 
>>>> (ii) Is there a Bayesian version of the glm()-function where I can   specify the prior distribution for my regression parameters? (e.g.   a Dirichlet prior s.t. the parameters are positive)
>>>> 
>>>> All this with respect to the linear Poisson model...
>>> 
>>> As I implied above, the word "linear" means something different than   "additive" when the link is log().
>>> 
>>> --
>>> 

David Winsemius
Alameda, CA, USA


From statistics84 at hotmail.com  Sat Jan 23 21:41:23 2016
From: statistics84 at hotmail.com (pari hesabi)
Date: Sat, 23 Jan 2016 20:41:23 +0000
Subject: [R] Logistic Regression
Message-ID: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>

Hello everybody,

I am trying to fit a logistic regression model by using glm() function in R. My response variable is a sample proportion NOT binary numbers(0,1).

Regarding glm() function, I receive this error:  non integer # successes in a binomial glm!

I would appreciate if anybody conducts me.


Regards,

Pari

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Jan 23 21:46:23 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 23 Jan 2016 12:46:23 -0800
Subject: [R] Logistic Regression
In-Reply-To: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>
References: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>
Message-ID: <9084BF7B-56C6-4531-9C58-31A50A8BEC0E@comcast.net>


> On Jan 23, 2016, at 12:41 PM, pari hesabi <statistics84 at hotmail.com> wrote:
> 
> Hello everybody,
> 
> I am trying to fit a logistic regression model by using glm() function in R. My response variable is a sample proportion NOT binary numbers(0,1).

So multiply the sample proportions (and 1-proportions) by the number of samples, round to integers, you will have an appropriate response variable and complements, and you can fit a binomial model.

> 
> Regarding glm() function, I receive this error:  non integer # successes in a binomial glm!
> 
> I would appreciate if anybody conducts me.
> 
> 
> Regards,
> 
> Pari
> 
> 	[[alternative HTML version deleted]]
-- 

David Winsemius
Alameda, CA, USA


From frainj at gmail.com  Sat Jan 23 22:08:48 2016
From: frainj at gmail.com (John C Frain)
Date: Sat, 23 Jan 2016 21:08:48 +0000
Subject: [R] Logistic Regression
In-Reply-To: <9084BF7B-56C6-4531-9C58-31A50A8BEC0E@comcast.net>
References: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>
	<9084BF7B-56C6-4531-9C58-31A50A8BEC0E@comcast.net>
Message-ID: <CAHrK516_P0tR-ss5LUCW_R9Vb4Sj4prD88jz4dT=Jmb+33EuQQ@mail.gmail.com>

Alternatively you might use log(p/1-p) as your dependent variable and use
OLS with robust standard errors. Much of your inference would be analogous
to a logistic regression

John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

On 23 January 2016 at 20:46, David Winsemius <dwinsemius at comcast.net> wrote:

>
> > On Jan 23, 2016, at 12:41 PM, pari hesabi <statistics84 at hotmail.com>
> wrote:
> >
> > Hello everybody,
> >
> > I am trying to fit a logistic regression model by using glm() function
> in R. My response variable is a sample proportion NOT binary numbers(0,1).
>
> So multiply the sample proportions (and 1-proportions) by the number of
> samples, round to integers, you will have an appropriate response variable
> and complements, and you can fit a binomial model.
>
> >
> > Regarding glm() function, I receive this error:  non integer # successes
> in a binomial glm!
> >
> > I would appreciate if anybody conducts me.
> >
> >
> > Regards,
> >
> > Pari
> >
> >       [[alternative HTML version deleted]]
> --
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Sat Jan 23 23:19:19 2016
From: jholtman at gmail.com (jim holtman)
Date: Sat, 23 Jan 2016 17:19:19 -0500
Subject: [R] Shift all values above certain value by 1
In-Reply-To: <CAN2xGJZvOsDBhwNZug6BmdN+XnEGNuHciNXziVxc_Usj50ejfw@mail.gmail.com>
References: <CAN2xGJZEr9OjXniGRaYHWdvQcdanvv9XJ-g45PLnM+O6YxxmJg@mail.gmail.com>
	<CAN2xGJZvOsDBhwNZug6BmdN+XnEGNuHciNXziVxc_Usj50ejfw@mail.gmail.com>
Message-ID: <CAAxdm-6XB+6=u5VFSqGkVVeJXCsPf2FQf3TJe8et2jq7z207iQ@mail.gmail.com>

You can keep it a dataframe as follows:

> set.seed(123)
> x <- data.frame(a = 1:10, b = 2:11, c = 3:12, other = rnorm(10))
> x
    a  b  c       other
1   1  2  3 -0.56047565
2   2  3  4 -0.23017749
3   3  4  5  1.55870831
4   4  5  6  0.07050839
5   5  6  7  0.12928774
6   6  7  8  1.71506499
7   7  8  9  0.46091621
8   8  9 10 -1.26506123
9   9 10 11 -0.68685285
10 10 11 12 -0.44566197
> # change the columns in the dataframe
> x[1:3] <- lapply(x[1:3], function(a){
+     a[a==7] <- 4  # replace values
+     a[a>7] <- a[a>7] - 1  # decrement
+     a  # return value
+ })
> x
   a  b  c       other
1  1  2  3 -0.56047565
2  2  3  4 -0.23017749
3  3  4  5  1.55870831
4  4  5  6  0.07050839
5  5  6  4  0.12928774
6  6  4  7  1.71506499
7  4  7  8  0.46091621
8  7  8  9 -1.26506123
9  8  9 10 -0.68685285
10 9 10 11 -0.44566197

?


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Fri, Jan 22, 2016 at 11:38 AM, Dimitri Liakhovitski <
dimitri.liakhovitski at gmail.com> wrote:

> I think I got it:
>
> set.seed(123)
> x <- data.frame(a = 1:10, b = 2:11, c = 3:12, other = rnorm(10))
> x
> temp <- as.matrix(x[1:3])
> temp[temp %in% 7] <- 4
> temp[temp > 7] <- temp[temp > 7]-1
> x[1:3] <- temp
> x
>
> It works only with matrices, right? Can't do x[x>7] when x is a data frame?
> Thanks!
>
> On Fri, Jan 22, 2016 at 11:34 AM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
> > Hello!
> >
> > # I have a data frame x:
> > x <- data.frame(a = 1:10, b = 2:11, c = 3:12, other = rnorm(10))
> >
> > # First, I need to change every value 7 in columns a:c to 4
> > # Then, I need to decrease by 1 all values in columns a:c that are >7
> >
> > What would be the fastest way of doing it?
> > Thank you!
> >
> > --
> > Dimitri Liakhovitski
>
>
>
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mara.pfleiderer at uni-ulm.de  Sat Jan 23 14:24:00 2016
From: mara.pfleiderer at uni-ulm.de (mara.pfleiderer at uni-ulm.de)
Date: Sat, 23 Jan 2016 14:24:00 +0100
Subject: [R] Constrained Poisson model / Bayesian Poisson model
Message-ID: <20160123142400.texehipssg0c0088@imap.uni-ulm.de>

Hi David,

I'm sorry. I'm not familiar with posting problems on helppages.
As the data I deal with is confidential I can't provide all details,
but I will try to be as precise as possible about my problem:

As I said I'm working on a Poisson regression model with a linear
predictor and the identity as link function.
In particular: I have data which consists of observations of two  
random variables X and Y over the period of about 3 months. I have  
clustered this data into hours, so I received 2088 observations of  
those variables and each observation represents the values of the  
variables in one hour. Now I assume Y to be Poisson distributed  
(Y_i~Poi(lambda_i)) and that the values of Y come from two different  
impacts, so I split Y_i into two Poisson-distributed variables Y_i1  
and Y_i2. I assume that the values of X have a long-term effect (of at  
least one day) on the part Y_i1 and I have estimators for the  
parameters lambda_i2, but I have no exact values of the variables of  
Y_i1 and Y_i2 but only of Y_i as a whole.
So my model looks as follows:
fml=Y_i1 ~ b_1*X_i+....+b_n*X_(i-n+1) - lambda_i2 -1 with n>=24
That means that the last 24 (or more) values of X influence the value  
of Y_i1 in an additive way and I have no intercept(b_0=0).
Now I made a matrix whose rows represent Y_i, all of its 24 (or more)  
regressors for each observation of Y_i and the corresponding estimator  
lambda_i2.
Then I used glm(fml, family=poisson(link="identity"), data=matrix) and  
tried it for different values of n (=24,48,36,...).
But always some of the coefficients received negative values which  
doesnt't make sense in interpretation. (The values of X represent  
certain events which can only have a positive or none effect on the  
value of Y.)

Now I want to use the constraint b_i >=0 in my model, but I don't know  
how I can do this.

I also thought of analyzing this model in a Bayesian way, but yet I  
haven't found a Bayesian version of glm() for the Poisson distribution  
such that I can specify the prior for the b_i on my own. (Then I could  
include the positivity in the prior.) Do you have a hint for me?

I'm sorry if I went too much into detail now...
I hope you understand my point now and have some answers for me!

Best,
Mara


Zitat von David Winsemius <dwinsemius at comcast.net>:

>
>> On Jan 22, 2016, at 7:01 AM, mara.pfleiderer at uni-ulm.de wrote:
>>
>> Hi all,
>>
>> I am dealing with a problem about my linear Poisson regression    
>> model (link function=identity).
>>
>> I am using the glm()-function which results in negative    
>> coefficients, but a negative influence of the regressors wouldn't    
>> make sense.
>
> Negative coefficients merely indicate a lower relative rate. You    
> need to be more specific about the exactly data and model output    
> before you can raise our concern to a level where further comment    
> can be made.
>
>
>>
>> (i) Is there a possibility to set constraints on the regression    
>> parameters in glm() such that all coefficients are positive? Or is   
>>  there another function in R for which this is possible?
>>
>> (ii) Is there a Bayesian version of the glm()-function where I can   
>>  specify the prior distribution for my regression parameters? (e.g.  
>>   a Dirichlet prior s.t. the parameters are positive)
>>
>> All this with respect to the linear Poisson model...
>
> As I implied above, the word "linear" means something different than  
>   "additive" when the link is log().
>
> --
>
> David Winsemius
> Alameda, CA, USA
>
>


From reichmanj at sbcglobal.net  Sat Jan 23 14:46:01 2016
From: reichmanj at sbcglobal.net (Jeff Reichman)
Date: Sat, 23 Jan 2016 07:46:01 -0600
Subject: [R] qplot Error Message
Message-ID: <001601d155e4$66b97670$342c6350$@sbcglobal.net>

R-Users

 

Anyone see what maybe wrong with the following command, other than R doesn't
seem to recognize the "span" parameter - it should must be my syntax.

 

> qplot(seq, count,geom=c("point","smooth"), span=0.8)

Error: Unknown parameters: span

 

Jeff


	[[alternative HTML version deleted]]


From liuwensui at gmail.com  Sat Jan 23 23:35:29 2016
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 23 Jan 2016 16:35:29 -0600
Subject: [R] Logistic Regression
In-Reply-To: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>
References: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>
Message-ID: <CAKyN3iCKRrTMePS1Krn0O-hPV1z9EPSGXvifv4OZZZB6VgJ+4g@mail.gmail.com>

with glm(), you might try the quasi binomial family

On Saturday, January 23, 2016, pari hesabi <statistics84 at hotmail.com> wrote:

> Hello everybody,
>
> I am trying to fit a logistic regression model by using glm() function in
> R. My response variable is a sample proportion NOT binary numbers(0,1).
>
> Regarding glm() function, I receive this error:  non integer # successes
> in a binomial glm!
>
> I would appreciate if anybody conducts me.
>
>
> Regards,
>
> Pari
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
https://statcompute.wordpress.com/

	[[alternative HTML version deleted]]


From bob at rudis.net  Sat Jan 23 23:56:04 2016
From: bob at rudis.net (boB Rudis)
Date: Sat, 23 Jan 2016 17:56:04 -0500
Subject: [R] qplot Error Message
In-Reply-To: <001601d155e4$66b97670$342c6350$@sbcglobal.net>
References: <001601d155e4$66b97670$342c6350$@sbcglobal.net>
Message-ID: <CAJ4QxaM2Um7d6HyZ5S-WBWk+s3poESdjjBJYVDwXnQLZK2o_cw@mail.gmail.com>

Assuming that's qplot from ggplot2, it's trying to pass span to the
Point Geom which doesn't recognize it. I highly suggest moving away
from using qplot and working with the stat_s and geom_s directly with
ggplot().

On Sat, Jan 23, 2016 at 8:46 AM, Jeff Reichman <reichmanj at sbcglobal.net> wrote:
> R-Users
>
>
>
> Anyone see what maybe wrong with the following command, other than R doesn't
> seem to recognize the "span" parameter - it should must be my syntax.
>
>
>
>> qplot(seq, count,geom=c("point","smooth"), span=0.8)
>
> Error: Unknown parameters: span
>
>
>
> Jeff
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Sun Jan 24 00:03:46 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 24 Jan 2016 10:03:46 +1100
Subject: [R] rstanarm compilation
Message-ID: <CA+8X3fXk5ac7zJbGm2im07PwsmSOqH0tTJ0VRCNBp85F9e2sKQ@mail.gmail.com>

Hi,
Seeing that the "rstanarm" package was available on CRAN in an answer to:

Constrained Poisson model / Bayesian Poisson model

I tried to install it. Took quite a while, but compilation failed at:

   ^
{standard input}: Assembler messages:
{standard input}:3144199: Warning: end of file in string; '"' inserted
g++: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://bugzilla.redhat.com/bugzilla> for instructions.
/usr/lib64/R/etc/Makeconf:141: recipe for target
'lang__grammars__var_deccls_grammar_inst.o' failed
make: *** [lang__grammars__var_deccls_grammar_inst.o] Error 4
ERROR: compilation failed for package ?rstan?
* removing ?/usr/lib64/R/library/rstan?

This may have been fixed already as it has already been reported on
bugzilla.

Jim

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Jan 24 03:19:39 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 23 Jan 2016 18:19:39 -0800
Subject: [R] rstanarm compilation
In-Reply-To: <CA+8X3fXk5ac7zJbGm2im07PwsmSOqH0tTJ0VRCNBp85F9e2sKQ@mail.gmail.com>
References: <CA+8X3fXk5ac7zJbGm2im07PwsmSOqH0tTJ0VRCNBp85F9e2sKQ@mail.gmail.com>
Message-ID: <3FE60988-CCA4-4122-B65D-6525896AB524@comcast.net>


> On Jan 23, 2016, at 3:03 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Hi,
> Seeing that the "rstanarm" package was available on CRAN in an answer to:
> 
> Constrained Poisson model / Bayesian Poisson model
> 
> I tried to install it. Took quite a while, but compilation failed at:
> 
>   ^
> {standard input}: Assembler messages:
> {standard input}:3144199: Warning: end of file in string; '"' inserted
> g++: internal compiler error: Killed (program cc1plus)
> Please submit a full bug report,
> with preprocessed source if appropriate.
> See <http://bugzilla.redhat.com/bugzilla> for instructions.
> /usr/lib64/R/etc/Makeconf:141: recipe for target
> 'lang__grammars__var_deccls_grammar_inst.o' failed
> make: *** [lang__grammars__var_deccls_grammar_inst.o] Error 4
> ERROR: compilation failed for package ?rstan?
> * removing ?/usr/lib64/R/library/rstan?
> 
> This may have been fixed already as it has already been reported on
> bugzilla.
> 

I certainly don't know, but was this the CRAN veraion 2.90-1 or the github version 2.90-2?.


> Jim
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From pgilbert902 at gmail.com  Sun Jan 24 16:45:22 2016
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Sun, 24 Jan 2016 10:45:22 -0500
Subject: [R] Estimating MA parameters through arima or through package
 "dlm"
In-Reply-To: <mailman.3.1451991601.1814.r-help@r-project.org>
References: <mailman.3.1451991601.1814.r-help@r-project.org>
Message-ID: <56A4F192.3000408@gmail.com>

(Sorry for the delay in responding to this.)

On 01/05/2016 06:00 AM, r-help-request at r-project.org wrote:
> Date: Mon, 4 Jan 2016 11:31:22 -0500
> From: Mark Leeds<markleeds2 at gmail.com>
> To: Stefano Sofia<stefano.sofia at regione.marche.it>
> Cc:"r-help at r-project.org"  <r-help at r-project.org>
> Subject: Re: [R] Estimating MA parameters through arima or through
> 	package	"dlm"
> Message-ID:
> 	<CAHz+bWYXanjp4ksu2SzMbFRLaKH8_ASwN07s_3eKYjeyWuAG1A at mail.gmail.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Hi: I don't have time to look at the details of what you're doing but the
> "equivalence"
> between state space and arima ( as paul gilbert pointed out a few weeks ago
> ) is not a true equivalence.

To state this a bit more precisely, there is a true equivalence between 
the input-output  equivalence classes of (linear, time invariant) 
state-space models with state dimension n and the input-output 
equivalence classes of (linear, time invariant) ARMA models with 
McMillan degree n. (In fact, the quotient spaces are diffeomorphic not 
just isomorphic.) This means you should be able to get exactly 
comparable results for anything that is an equivalence class invariant, 
including residuals and anything calculated from residuals, such as 
likelihood. Model roots and thus stability are also invariants, but you 
probably will not get comparable results for most other things involving 
parameters.

This is not just a "statistical equivalence" as is sometimes suggested, 
it is an algebraic equivalence between quotient spaces.

However, if you estimate a state-space model and estimate an ARMA model, 
there are several other things that come into play related to 
estimation. Comparing the two estimated models you are unlikely to find 
comparable results. (Typically ARMA estimation is more robust at finding 
the best in my experience.) Even with simulation testing of estimation 
starting with "true" models it is problematic to get estimated models 
that are equivalent.

If you really want to see the equivalence you need to do a conversion of 
a model from one form to the other. I cannot speak to dlm, but dse was 
built for studying this equivalence and the users' guide has examples. 
If you are really interested in this topic, I recommend section 5 of 
http://www.bankofcanada.ca/1993/03/working-paper-199/,
I realize this is getting a bit old, but if you find a more up-to-date 
summary I would like to hear about it. That paper also has a 
demonstration that the equivalent models give results that are 
comparable within numerical precision of computers, not just to some 
statistical significance.
>
>   if you are in an area of the parameter space that the state space
> formulation
>   can't reach, then you won't get the same parameter estimates. so, what
> you're doing
> might be okay or might not be, depending on whether the state space
> formulation
> can reach that area of the parameter space.

"Can't reach" is an estimation problem. This is typically a more serious 
problem with state-space models. The quotient spaces are diffeomorphic 
so in theory it should be possible to reach the same solution if you 
properly account for the fact that you are estimating on a smooth 
manifold and not a vector space. In practice you have also to worry 
about twisting of the parameter space and finite time for estimation, 
and gradients that may converge toward zero near boundaries of the 
manifold's charts.

> there's another state space
> formulation that is truly equivalent which is called the SSOE formulation
> or innovations representation but
> I don't know if you want to get into that. google "SSOE state space" if
> you're interested.

The quotient space of input-output equivalence classes for innovations 
form models is equivalent to the quotient space of input-output 
equivalence classes for non-innovations form models. You need more 
information to identify a non-innovations form, typically some physical 
understanding of the system. On the bases of only input-output data, and 
with no additional understanding of the physical system, there would be 
no reason to choose a non-innovations form for estimation. There is more 
discussion of this in the above mentioned summary and in the dse user's 
guide.

BTW, estimation problems tend to be much more severe with multivariate 
series than with univariate series. This is not just because of the 
usual issues. Especially in state-space representations, the twisting of 
the parameter space seems to be especially bad.

Paul

>
>
> Mark
>
>
> On Mon, Jan 4, 2016 at 9:25 AM, Stefano Sofia <
> stefano.sofia at regione.marche.it> wrote:
>
>> >Dear list users,
>> >I want to use apply a MA(2) process (x=beta1*epsilon_(t-1) +
>> >beta2*epsilon_(t-1) + epsilon_(t)) to a given time series (x), and I want
>> >to estimate the two parameters beta1, beta2 and the variance of the random
>> >variable epsilon_(t).
>> >
>> >If I use
>> >MA2_1 <- Arima(x, order=c(0,0,2))
>> >I get the following result
>> >
>> >[1] "MA2_1"
>> >Series: x
>> >ARIMA(0,0,2) with non-zero mean
>> >
>> >Coefficients:
>> >           ma1     ma2  intercept
>> >       -0.0279  0.0783     5.3737
>> >s.e.   0.0667  0.0622     0.0245
>> >
>> >sigma^2  estimated as 0.1284:  log likelihood=-92.63
>> >AIC=193.25   AICc=193.43   BIC=207.11
>> >[1] 0 2 0 0 1 0 0
>> >
>> > From this straightforward analysis V[epsilon]=0.1284, beta1=-0.0279 and
>> >beta2=0.0783.
>> >
>> >I also tried to use a DLM representation of ARIMA models and estimate the
>> >unknown parameters by maximum likelihood through the dlm package (in
>> >particular applying the example at section 3.2.6, page 115, of "Dynamic
>> >Linear Models with R" by Petris, Petrone and Campagnoli:
>> >
>> >arma_parameters <- function(x)
>> >{
>> >   buildGap <- function(u)
>> >   {
>> >     gap <- dlmModARMA(ma = u[2 : 3], sigma2 = u[1])
>> >     return(gap)
>> >    }
>> >    init <- c(0.005, 0.004, 0.003)
>> >    outMLE <- dlmMLE(x, init, buildGap)
>> >    dlmGap <- buildGap(outMLE$par)
>> >}
>> >
>> >and this gives:
>> >[1] "outMLE"
>> >$par
>> >[1] 1.00816794 0.02349296 0.02364788
>> >
>> >$value
>> >[1] 3089.196
>> >
>> >$counts
>> >function gradient
>> >       10       10
>> >
>> >$convergence
>> >[1] 0
>> >
>> >$message
>> >[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
>> >
>> >[1] "dlmGap"
>> >$FF
>> >      [,1] [,2] [,3]
>> >[1,]    1    0    0
>> >
>> >$V
>> >      [,1]
>> >[1,]    0
>> >
>> >$GG
>> >      [,1] [,2] [,3]
>> >[1,]    0    1    0
>> >[2,]    0    0    1
>> >[3,]    0    0    0
>> >
>> >$W
>> >            [,1]         [,2]         [,3]
>> >[1,] 1.00816794 0.0236848488 0.0238410337
>> >[2,] 0.02368485 0.0005564272 0.0005600964
>> >[3,] 0.02384103 0.0005600964 0.0005637899
>> >
>> >$m0
>> >[1] 0 0 0
>> >
>> >$C0
>> >       [,1]  [,2]  [,3]
>> >[1,] 1e+07 0e+00 0e+00
>> >[2,] 0e+00 1e+07 0e+00
>> >[3,] 0e+00 0e+00 1e+07
>> >
>> >In this case
>> >V[epsilon]=W[1,1]=1.00816794
>> >beta1=W[2,1]/W[1,1]=0.02349296
>> >beta2=W[3,1]/W[1,1]=0.02364788
>> >
>> >I presume that these two approaches should give comparable results, but
>> >this does not happen.
>> >Is the model that I used correct? And does it make sense to perform this
>> >kind of comparison?
>> >
>> >This is the log of a rainfall time series (which has already been
>> >deseasonalised):
>> >[1] 6.014937 4.978801 5.654592 5.616771 5.612398 5.837147 5.121580 5.832176
>> >[9] 5.205654 5.355642 5.405376 6.257859 5.516247 5.500850 4.708629 5.482304
>> >[17] 5.689684 5.727824 4.779123 5.289277 5.217107 5.976351 4.630838
>> >5.683240
>> >[25] 5.345678 5.906179 5.605434 5.497578 5.898801 5.660875 5.111988
>> >5.571013
>> >[33] 5.949340 5.374352 4.841033 5.995706 5.661223 5.458734 4.454347
>> >5.795754
>> >[41] 5.995706 5.596939 5.399971 5.908898 5.282696 5.438514 5.528635
>> >6.022721
>> >[49] 5.524257 5.519459 4.957235 5.547518 5.080783 5.411200 5.056883
>> >5.798183
>> >[57] 5.086361 5.536547 5.220356 5.141664 5.847017 5.052417 5.734635
>> >5.340419
>> >[65] 5.724238 5.634432 5.685958 5.307773 5.817706 5.134032 4.987708
>> >5.110179
>> >[73] 5.423628 5.347108 4.859037 5.556828 5.487283 5.661223 5.732370
>> >5.469325
>> >[81] 5.726848 5.419207 5.172187 5.608006 5.130490 5.586874 5.171052
>> >5.683240
>> >[89] 4.674696 5.286245 5.342813 5.370638 5.432411 5.748118 6.355239
>> >5.557986
>> >[97] 5.399067 5.222516 5.279644 5.425390 5.540871 5.917818 5.132853
>> >5.689007
>> >[105] 5.900993 5.007296 5.102911 5.778271 5.318120 5.927726 5.066385
>> >5.716699
>> >[113] 5.511815 4.714921 5.383577 5.319100 5.269403 5.354698 5.145749
>> >5.204556
>> >[121] 5.878296 5.070161 5.441552 5.213304 5.450180 5.695750 4.893352
>> >5.425390
>> >[129] 5.682559 5.487283 4.213608 5.751620 5.432411 5.379436 5.700444
>> >5.580484
>> >[137] 5.357529 5.319100 4.532599 5.603225 5.208393 5.254888 5.017280
>> >5.349961
>> >[145] 4.374498 5.187944 5.585374 5.716370 3.561046 5.119789 5.163070
>> >5.422745
>> >[153] 5.863915 5.651436 4.762174 5.655642 4.797442 5.735927 4.911183
>> >5.240688
>> >[161] 5.148076 5.477300 4.572647 5.493473 5.437644 4.854371 4.908233
>> >4.755313
>> >[169] 5.582744 5.527841 5.613128 5.211124 5.275049 5.462984 5.016617
>> >5.981919
>> >[177] 5.566817 5.094364 5.314191 5.712742 5.299317 5.452325 4.691348
>> >5.851628
>> >[185] 5.410753 5.488938 5.660179 5.900993 5.380819 5.256453 4.781641
>> >5.531807
>> >[193] 5.497578 5.274537 4.325456 5.271973 5.077047 5.258536 5.280662
>> >5.247024
>> >[201] 5.995208 4.700480 4.991113 5.457029 5.194622 5.487283 5.197391
>> >5.747161
>> >[209] 5.842094 5.372497 5.306781 5.641907 5.565286 5.259057 5.241218
>> >4.759607
>> >[217] 4.550714 5.230574 4.470495 5.664348 4.846547 5.771130 4.823502
>> >5.598422
>> >[225] 5.627621 5.547518 5.596939 5.468482 5.536940 5.606170 5.281680
>> >5.656691
>> >[233] 5.283204 5.752255 5.192401 4.550714
>> >
>> >
>> >Thank you for your attention and your help
>> >Stefano
>> >
>> >


From hannah.hlx at gmail.com  Sun Jan 24 17:46:16 2016
From: hannah.hlx at gmail.com (li li)
Date: Sun, 24 Jan 2016 11:46:16 -0500
Subject: [R] Error because of large dimension
Message-ID: <CAHLnndb=fQtZ2n9AxLHARbZLw79=bE48Q9Vm3ZCMFhG++GC_RA@mail.gmail.com>

Hi all,
  I am doing some calculation with very large dimension. I need to create a
matrix
with three columns and a very large number of rows
(3195*1290*495*35*35*35*15=1.312083e+15) i
n order to allocate calculation result from a for loop.
R does not allow me to create such a matrix because of the large dimension
(see below). Is there a way to go around this?
  Thanks very much!!
     Hanna


> matrix(0, 3195*1290*495*35*35*35*15, 3)
Error in matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
  invalid 'nrow' value (too large or NA)
In addition: Warning message:
In matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
  NAs introduced by coercion
>

	[[alternative HTML version deleted]]


From okeyes at wikimedia.org  Sun Jan 24 18:01:52 2016
From: okeyes at wikimedia.org (Oliver Keyes)
Date: Sun, 24 Jan 2016 12:01:52 -0500
Subject: [R] Error because of large dimension
In-Reply-To: <CAHLnndb=fQtZ2n9AxLHARbZLw79=bE48Q9Vm3ZCMFhG++GC_RA@mail.gmail.com>
References: <CAHLnndb=fQtZ2n9AxLHARbZLw79=bE48Q9Vm3ZCMFhG++GC_RA@mail.gmail.com>
Message-ID: <CAAUQgdA+3OpsXK6sbZ5dYR-ew9YiNpgv7D8RghhPpO2AyECnYA@mail.gmail.com>

Hey Hanna,

nrow and ncol in matrix() are integer-based, for the moment at least;
accordingly they have a maximum value. (3195*1290*495*35*35*35*15) is
actually larger than an integer can hold - you can test this with:

str((3195*1290*495*35*35*35*15))

Which shows that it's stored as a numeric value. And if you try
as.integer((3195*1290*495*35*35*35*15)) you'll get an NA - because
it's too large for an integer to hold.

You could try using the "bigmemory" package, which is designed to
handle very very large matrices (and other datatypes) but I believe
that handling is in terms of making sure you can store the thing by
storing it in a file if necessary - I'm not sure if it allows for
longs (which can store much larger values) for nrow and ncol and
indexing generally. So it may be that, for now, you're out of luck I'm
afraid :(.

On 24 January 2016 at 11:46, li li <hannah.hlx at gmail.com> wrote:
> Hi all,
>   I am doing some calculation with very large dimension. I need to create a
> matrix
> with three columns and a very large number of rows
> (3195*1290*495*35*35*35*15=1.312083e+15) i
> n order to allocate calculation result from a for loop.
> R does not allow me to create such a matrix because of the large dimension
> (see below). Is there a way to go around this?
>   Thanks very much!!
>      Hanna
>
>
>> matrix(0, 3195*1290*495*35*35*35*15, 3)
> Error in matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
>   invalid 'nrow' value (too large or NA)
> In addition: Warning message:
> In matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
>   NAs introduced by coercion
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Oliver Keyes
Count Logula
Wikimedia Foundation


From wht_crl at yahoo.com  Sun Jan 24 19:24:51 2016
From: wht_crl at yahoo.com (carol white)
Date: Sun, 24 Jan 2016 18:24:51 +0000 (UTC)
Subject: [R] logical vector of the indices of a string in a vector
References: <1699070014.413239.1453659891550.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1699070014.413239.1453659891550.JavaMail.yahoo@mail.yahoo.com>

Hi,?it might be trivial but is there any way to get the logical vector of the indices of a string in a vector? I thought that %in% would do but it doesn't. I also want to filter the empty fields.
Here I want to extract the non-empty elements containing "Yes":x =c("Yes, fsd", "", "No","","Yes, fjsdlf", "")
x[c("Yes") %in% x & x != ""]character(0)

Above, I wanted to do the 2 following operations in 1. Here with grep,? it works but %in% in above doesn't:y = x[grep("Yes", x)]
> y = y[y != ""]
> y
[1] "Yes, fsd"??? "Yes, fjsdlf"
Thanks
Carol

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Sun Jan 24 19:44:08 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Sun, 24 Jan 2016 18:44:08 +0000
Subject: [R] logical vector of the indices of a string in a vector
In-Reply-To: <1699070014.413239.1453659891550.JavaMail.yahoo@mail.yahoo.com>
References: <1699070014.413239.1453659891550.JavaMail.yahoo.ref@mail.yahoo.com>
	<1699070014.413239.1453659891550.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <20160124184408.Horde.By-QiuFWm5CyyERQi1AfDa0@mail.sapo.pt>

Hello,

Try

x[grepl("Yes", x) & x != ""]

Hope this helps,

Rui Barradas

?

Citando carol white via R-help <r-help at r-project.org>:

> Hi,?it might be trivial but is there any way to get the logical  
> vector of the indices of a string in a vector? I thought that %in%  
> would do but it doesn't. I also want to filter the empty fields.
> Here I want to extract the non-empty elements containing "Yes":x  
> =c("Yes, fsd", "", "No","","Yes, fjsdlf", "")
> x[c("Yes") %in% x & x != ""]character(0)
>
> Above, I wanted to do the 2 following operations in 1. Here with  
> grep,? it works but %in% in above doesn't:y = x[grep("Yes", x)]
>> y = y[y != ""]
>> y
>
> [1] "Yes, fsd"??? "Yes, fjsdlf"
> Thanks
> Carol
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From sowmiyan0508 at gmail.com  Sun Jan 24 18:27:01 2016
From: sowmiyan0508 at gmail.com (sowmiyan)
Date: Sun, 24 Jan 2016 22:57:01 +0530
Subject: [R] Extracting complete information from XML data file using
	R-Nested Lists
Message-ID: <CAF7+Lwm6ZcgfEKUBoSw6rG8wBYYY-URND7JkrfPMfc2O7ssNgw@mail.gmail.com>

I am working with a XML, which can be found in the link Sample XML file
<https://www.dropbox.com/s/8kn9g8xev2u5n8o/Dummy.xml?dl=0&preview=Dummy.xml>

I am trying to extract each and every fields information to a csv file. I
want my output to be as below: Required output:
*Total of 20 columns and 2 rows*
DateCreated DateModified Creator.UserAccountName Creator.PersonName
Creator..attrs.referenceNumber Modifier.UserAccountName Modifier.PersonName
Modifier..attrs.referenceNumber AdditionalEmailStr AdditionalComment
DateIssued DocumentaryInstructions NominationParcel.attr.Referencenumber
NominationParcel.SecondContractNumber
NominationParcel.Coordinator.RefernceNumber
NominationParcel.Coordinator.Username NominationParcel.Coordinator.Email
NominationParcel.Coordinator.Office.Name
NominationParcel.Coordinator.Office.Email
NominationParcel.Coordinator.Office.attrs.referenceNumber
Nomination 2007-11-25T17:01:32 2007-11-25T17:11:09 mkolker Merryn Kolker
15351 mkolker Merryn Kolker 15351 Good work   7 sam
Nomination 2007-11-25T17:18:01 2007-11-25T17:19:11 mkolker Merryn Kolker
15351 mkolker Merryn Kolker 15351 Nicely Performed   10 107 102

But I am not able to get my output in the required format. I have tried in
two different ways

1 Below is my first code, the problem with this is that my NULL fields are
not getting captured correctly and there is spillover of data. Also I am
not able to capture all the fields of nested lists in the XML

*Code 1*

  doc <- xmlParse("Dummy.xml")
  lst<-xmlToList(doc)
  f <- function(col) do.call(rbind, lapply(lst, function(x)
unlist(x[cols])));
  cols
<-c("DateCreated","DateModified","Creator","Modifier","AdditionalEmailStr","AdditionalComment","DateIssued",
"DocumentaryInstructions", "NominationParcel" );
  res <- setNames(lapply(cols, f), cols);
  list2env(res, .GlobalEnv)
*Output 1*


DateCreated DateModified Creator.UserAccountName Creator.PersonName
Creator..attrs.referenceNumber Modifier.UserAccountName Modifier.PersonName
Modifier..attrs.referenceNumber AdditionalComment
NominationParcel.Coordinator.UserAccountName
NominationParcel.Coordinator.Office..attrs.referenceNumber
NominationParcel.Coordinator..attrs.referenceNumber
NominationParcel..attrs.referenceNumber
Nomination 2007-11-25T17:01:32 2007-11-25T17:11:09 mkolker Merryn Kolker
15351 mkolker Merryn Kolker 15351 Good Work sam 7
Nomination 2007-11-25T17:18:01 2007-11-25T17:19:11 mkolker Merryn Kolker
15351 mkolker Merryn Kolker 15351 Nicely performed 102 107 10
2007-11-25T17:18:01

2 To avoid spillover of information of one cell to other because of "NULL",
I have used for loop to replace the NULL cells with NA. By using this I was
able to capture the correct data, but I could not get all the fields
information present in the XML

*Code 2*

   doc <- xmlParse("Dummy.xml")
   lstsub<-xmlToList(doc)
   for(i in 1:length(lstsub))
   {
    for(j in 1:length(lstsub[[i]]))
     {
       lstsub[[i]][[j]]=
ifelse(is.null(lstsub[[i]][[j]]),NA,lstsub[[i]][[j]])
       if(length(lstsub[[i]][[j]])>1)
       {
       for(k in 1:length(lstsub[[i]][[j]]))
       {
          lstsub[[i]][[j]][[k]]=
 ifelse(is.null(lstsub[[i]][[j]][[k]]),NA,lstsub[[i]][[j]][[k]])
         if(length(lstsub[[i]][[j]][[k]])>1)
          {
         for(l in 1:length(lstsub[[i]][[j]][[k]]))
           {
            lstsub[[i]][[j]][[k]][[l]]=
 ifelse(is.null(lstsub[[i]][[j]][[k]][[l]]),NA,lstsub[[i]][[j]][[k]][[l]])
           }
          }
        }
      }
    }
  }
   f <- function(col) do.call(rbind, lapply(lstsub, function(x)
unlist(x[cols])));
     cols <-
c("DateCreated","DateModified","Creator","Modifier","AdditionalEmailStr","AdditionalComment","DateIssued",
"DocumentaryInstructions", "NominationParcel" );
     res <- setNames(lapply(cols, f), cols);
     list2env(res, .GlobalEnv)
     write.csv(Creator,"dummy_2.csv")

*Output 2*

            DateCreated DateModified    Creator Modifier
 AdditionalEmailStr  AdditionalComment   DateIssued  DocumentaryInstructions

Nomination  2007-11-25T17:01:32 2007-11-25T17:11:09 mkolker mkolker NA
 Good Work   NA  NA
Nomination  2007-11-25T17:18:01 2007-11-25T17:19:11 mkolker mkolker NA
 Nicely performed    NA  NA

Could somebody please help me in how could I get the required output

I have posted the same question in Stackoverflow and the link is here (it
might help in giving more clear picture)

http://stackoverflow.com/questions/34963724/extracting-complete-information-from-nested-lists-in-xml-to-a-data-frame-using-r/34963821#34963821


Regards,
Sowmiyan

	[[alternative HTML version deleted]]


From okeyes at wikimedia.org  Sun Jan 24 21:19:28 2016
From: okeyes at wikimedia.org (Oliver Keyes)
Date: Sun, 24 Jan 2016 15:19:28 -0500
Subject: [R] Extracting complete information from XML data file using
 R-Nested Lists
In-Reply-To: <CAF7+Lwm6ZcgfEKUBoSw6rG8wBYYY-URND7JkrfPMfc2O7ssNgw@mail.gmail.com>
References: <CAF7+Lwm6ZcgfEKUBoSw6rG8wBYYY-URND7JkrfPMfc2O7ssNgw@mail.gmail.com>
Message-ID: <CAAUQgdBoCghLnMUK9TZhHzQ_5kE=y6EN5aM_mCNF=7g4fb-sig@mail.gmail.com>

Hey Sowmiyan,

I would recommend taking a look at the xml2, rather than xml, package
for a start. It's a lot more structured and traversing between
elements far easier :)

On 24 January 2016 at 12:27, sowmiyan <sowmiyan0508 at gmail.com> wrote:
> I am working with a XML, which can be found in the link Sample XML file
> <https://www.dropbox.com/s/8kn9g8xev2u5n8o/Dummy.xml?dl=0&preview=Dummy.xml>
>
> I am trying to extract each and every fields information to a csv file. I
> want my output to be as below: Required output:
> *Total of 20 columns and 2 rows*
> DateCreated DateModified Creator.UserAccountName Creator.PersonName
> Creator..attrs.referenceNumber Modifier.UserAccountName Modifier.PersonName
> Modifier..attrs.referenceNumber AdditionalEmailStr AdditionalComment
> DateIssued DocumentaryInstructions NominationParcel.attr.Referencenumber
> NominationParcel.SecondContractNumber
> NominationParcel.Coordinator.RefernceNumber
> NominationParcel.Coordinator.Username NominationParcel.Coordinator.Email
> NominationParcel.Coordinator.Office.Name
> NominationParcel.Coordinator.Office.Email
> NominationParcel.Coordinator.Office.attrs.referenceNumber
> Nomination 2007-11-25T17:01:32 2007-11-25T17:11:09 mkolker Merryn Kolker
> 15351 mkolker Merryn Kolker 15351 Good work   7 sam
> Nomination 2007-11-25T17:18:01 2007-11-25T17:19:11 mkolker Merryn Kolker
> 15351 mkolker Merryn Kolker 15351 Nicely Performed   10 107 102
>
> But I am not able to get my output in the required format. I have tried in
> two different ways
>
> 1 Below is my first code, the problem with this is that my NULL fields are
> not getting captured correctly and there is spillover of data. Also I am
> not able to capture all the fields of nested lists in the XML
>
> *Code 1*
>
>   doc <- xmlParse("Dummy.xml")
>   lst<-xmlToList(doc)
>   f <- function(col) do.call(rbind, lapply(lst, function(x)
> unlist(x[cols])));
>   cols
> <-c("DateCreated","DateModified","Creator","Modifier","AdditionalEmailStr","AdditionalComment","DateIssued",
> "DocumentaryInstructions", "NominationParcel" );
>   res <- setNames(lapply(cols, f), cols);
>   list2env(res, .GlobalEnv)
> *Output 1*
>
>
> DateCreated DateModified Creator.UserAccountName Creator.PersonName
> Creator..attrs.referenceNumber Modifier.UserAccountName Modifier.PersonName
> Modifier..attrs.referenceNumber AdditionalComment
> NominationParcel.Coordinator.UserAccountName
> NominationParcel.Coordinator.Office..attrs.referenceNumber
> NominationParcel.Coordinator..attrs.referenceNumber
> NominationParcel..attrs.referenceNumber
> Nomination 2007-11-25T17:01:32 2007-11-25T17:11:09 mkolker Merryn Kolker
> 15351 mkolker Merryn Kolker 15351 Good Work sam 7
> Nomination 2007-11-25T17:18:01 2007-11-25T17:19:11 mkolker Merryn Kolker
> 15351 mkolker Merryn Kolker 15351 Nicely performed 102 107 10
> 2007-11-25T17:18:01
>
> 2 To avoid spillover of information of one cell to other because of "NULL",
> I have used for loop to replace the NULL cells with NA. By using this I was
> able to capture the correct data, but I could not get all the fields
> information present in the XML
>
> *Code 2*
>
>    doc <- xmlParse("Dummy.xml")
>    lstsub<-xmlToList(doc)
>    for(i in 1:length(lstsub))
>    {
>     for(j in 1:length(lstsub[[i]]))
>      {
>        lstsub[[i]][[j]]=
> ifelse(is.null(lstsub[[i]][[j]]),NA,lstsub[[i]][[j]])
>        if(length(lstsub[[i]][[j]])>1)
>        {
>        for(k in 1:length(lstsub[[i]][[j]]))
>        {
>           lstsub[[i]][[j]][[k]]=
>  ifelse(is.null(lstsub[[i]][[j]][[k]]),NA,lstsub[[i]][[j]][[k]])
>          if(length(lstsub[[i]][[j]][[k]])>1)
>           {
>          for(l in 1:length(lstsub[[i]][[j]][[k]]))
>            {
>             lstsub[[i]][[j]][[k]][[l]]=
>  ifelse(is.null(lstsub[[i]][[j]][[k]][[l]]),NA,lstsub[[i]][[j]][[k]][[l]])
>            }
>           }
>         }
>       }
>     }
>   }
>    f <- function(col) do.call(rbind, lapply(lstsub, function(x)
> unlist(x[cols])));
>      cols <-
> c("DateCreated","DateModified","Creator","Modifier","AdditionalEmailStr","AdditionalComment","DateIssued",
> "DocumentaryInstructions", "NominationParcel" );
>      res <- setNames(lapply(cols, f), cols);
>      list2env(res, .GlobalEnv)
>      write.csv(Creator,"dummy_2.csv")
>
> *Output 2*
>
>             DateCreated DateModified    Creator Modifier
>  AdditionalEmailStr  AdditionalComment   DateIssued  DocumentaryInstructions
>
> Nomination  2007-11-25T17:01:32 2007-11-25T17:11:09 mkolker mkolker NA
>  Good Work   NA  NA
> Nomination  2007-11-25T17:18:01 2007-11-25T17:19:11 mkolker mkolker NA
>  Nicely performed    NA  NA
>
> Could somebody please help me in how could I get the required output
>
> I have posted the same question in Stackoverflow and the link is here (it
> might help in giving more clear picture)
>
> http://stackoverflow.com/questions/34963724/extracting-complete-information-from-nested-lists-in-xml-to-a-data-frame-using-r/34963821#34963821
>
>
> Regards,
> Sowmiyan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Oliver Keyes
Count Logula
Wikimedia Foundation


From henrik.bengtsson at gmail.com  Sun Jan 24 22:29:31 2016
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Sun, 24 Jan 2016 13:29:31 -0800
Subject: [R] Error because of large dimension
In-Reply-To: <CAHLnndb=fQtZ2n9AxLHARbZLw79=bE48Q9Vm3ZCMFhG++GC_RA@mail.gmail.com>
References: <CAHLnndb=fQtZ2n9AxLHARbZLw79=bE48Q9Vm3ZCMFhG++GC_RA@mail.gmail.com>
Message-ID: <CAFDcVCS2WVLhFaVEKEg=Q+Y7ThTS62x5ysnx3o_EzOO8A9rxZA@mail.gmail.com>

FYI, the matrix you tried to allocate would hold
(3195*1290*495*35*35*35*15) * 3 = 3.936248e+15 values.  Each value
would occupy 8 bytes of memory (for the double data type).  In other
words, in order to keep this data matrix in memory you would require a
computer with at least 3.148998e+16 bytes of RAM, i.e. 29327331 GiB =
28640 TiB = 28 PiB.  Storing such a large matrix even on file is not
possible.

In other words, you need to figure out how to approach your original
problem in a different way.

/Henrik

On Sun, Jan 24, 2016 at 8:46 AM, li li <hannah.hlx at gmail.com> wrote:
> Hi all,
>   I am doing some calculation with very large dimension. I need to create a
> matrix
> with three columns and a very large number of rows
> (3195*1290*495*35*35*35*15=1.312083e+15) i
> n order to allocate calculation result from a for loop.
> R does not allow me to create such a matrix because of the large dimension
> (see below). Is there a way to go around this?
>   Thanks very much!!
>      Hanna
>
>
>> matrix(0, 3195*1290*495*35*35*35*15, 3)
> Error in matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
>   invalid 'nrow' value (too large or NA)
> In addition: Warning message:
> In matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
>   NAs introduced by coercion
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From friendly at yorku.ca  Sun Jan 24 22:42:43 2016
From: friendly at yorku.ca (Michael Friendly)
Date: Sun, 24 Jan 2016 16:42:43 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A371F5.5070509@nancy.inra.fr>
References: <56A371F5.5070509@nancy.inra.fr>
Message-ID: <56A54553.60603@yorku.ca>


On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
> Dear members,
>
> Not a technical question:
But one worth raising...
>
> The number of threads in this mailing list, following a long period of
> increase, has been regularly and strongly decreasing since 2010, passing
> from more than 40K threads to less than 11K threads last year. The trend
> is similar for most of the "ancient" mailing lists of the R-project.
[snip ...]
>
> I hope it is the wright place to ask this question. Thanks in advance,
>

In addition to the other replies, there is another trend I've seen that
has actively worked to suppress discussion on R-help and move it 
elsewhere. The general things:
- R-help was too unwieldy and so it was a good idea to hive-off 
specialized topics to various sub lists, R-SIG-Mac, R-SIG-Geo,
etc.
- Many people posted badly-formed questions to R-help, and so it
was a good idea to develop and refer to the posting guide to mitigate
the number of purely junk postings.

<rant>
Yet, the trend I've seen is one of increasing **R-not-help**, in that 
there are many posts, often by new R users who get replies that not
infrequently range from just mildly off-putting to actively hostile:

- Is this homework? We don't do homework (sometimes false alarms,
where the OP has to reply to say it is not)
- Didn't you bother to do your homework, RTFM, or Google?
- This is off-topic because XXX (e.g., it is not strictly an R 
programming question).
- You asked about doing XXX, but this is a stupid thing
to want to do.
- Don't ask here; you need to talk to a statistical consultant.

I find this sad in a public mailing list sent to all R-help subscribers
and I sometimes cringe
when I read replies to people who were actually trying to get
help with some R-related problem, but expressed it badly, didn't
know exactly what to ask for, or how to format it,
or somehow motivated a frequent-replier to publicly dis the OP.

On the other hand, I still see a spirit of great generosity among some
people who frequently reply to R-help, taking a possibly badly posed
or ill-formatted question, and going to some lengths to provide a
a helpful answer of some sort.  I applaud those who take the time
and effort to do this.

I use R in a number of my courses, and used to advise students to
post to R-help for general programming questions (not just homework) 
they couldn't solve. I don't do this any more, because several of them
reported a negative experience.

In contrast, in the Stackexchange model, there are numerous sublists
cross-classified by their tags.  If I have a specific knitr, ggplot2, 
LaTeX, or statistical modeling question, I'm now more likely to post it 
there, and the worst that can happen is that no one "upvotes" it
or someone (helpfully) marks it as a duplicate of a similar question.
But comments there are not propagated to all subscribers,
and those who reply helpfully, can see their solutions accepted or not,
or commented on in that specific topic.

Perhaps one solution would be to create a new "R-not-help" list where,
as in a Monty Python skit, people could be directed there to be insulted 
and all these unhelpful replies could be sent.

A milder alternative is to encourage some R-help subscribers to click 
the "Don't send" or "Save" button and think better of their replies.
</rant>

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept. & Chair, Quantitative Methods
York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
4700 Keele Street    Web:   http://www.datavis.ca
Toronto, ONT  M3J 1P3 CANADA


From rsherry8 at comcast.net  Sun Jan 24 23:27:54 2016
From: rsherry8 at comcast.net (Robert Sherry)
Date: Sun, 24 Jan 2016 17:27:54 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A54553.60603@yorku.ca>
References: <56A371F5.5070509@nancy.inra.fr> <56A54553.60603@yorku.ca>
Message-ID: <56A54FEA.50200@comcast.net>

I think this mailing list is wonderful and it has helped me a lot. In 
fact, I am not sure I would be using R today if it was not for this
list.

Bob

On 1/24/2016 4:42 PM, Michael Friendly wrote:
>
> On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
>> Dear members,
>>
>> Not a technical question:
> But one worth raising...
>>
>> The number of threads in this mailing list, following a long period of
>> increase, has been regularly and strongly decreasing since 2010, passing
>> from more than 40K threads to less than 11K threads last year. The trend
>> is similar for most of the "ancient" mailing lists of the R-project.
> [snip ...]
>>
>> I hope it is the wright place to ask this question. Thanks in advance,
>>
>
> In addition to the other replies, there is another trend I've seen that
> has actively worked to suppress discussion on R-help and move it 
> elsewhere. The general things:
> - R-help was too unwieldy and so it was a good idea to hive-off 
> specialized topics to various sub lists, R-SIG-Mac, R-SIG-Geo,
> etc.
> - Many people posted badly-formed questions to R-help, and so it
> was a good idea to develop and refer to the posting guide to mitigate
> the number of purely junk postings.
>
> <rant>
> Yet, the trend I've seen is one of increasing **R-not-help**, in that 
> there are many posts, often by new R users who get replies that not
> infrequently range from just mildly off-putting to actively hostile:
>
> - Is this homework? We don't do homework (sometimes false alarms,
> where the OP has to reply to say it is not)
> - Didn't you bother to do your homework, RTFM, or Google?
> - This is off-topic because XXX (e.g., it is not strictly an R 
> programming question).
> - You asked about doing XXX, but this is a stupid thing
> to want to do.
> - Don't ask here; you need to talk to a statistical consultant.
>
> I find this sad in a public mailing list sent to all R-help subscribers
> and I sometimes cringe
> when I read replies to people who were actually trying to get
> help with some R-related problem, but expressed it badly, didn't
> know exactly what to ask for, or how to format it,
> or somehow motivated a frequent-replier to publicly dis the OP.
>
> On the other hand, I still see a spirit of great generosity among some
> people who frequently reply to R-help, taking a possibly badly posed
> or ill-formatted question, and going to some lengths to provide a
> a helpful answer of some sort.  I applaud those who take the time
> and effort to do this.
>
> I use R in a number of my courses, and used to advise students to
> post to R-help for general programming questions (not just homework) 
> they couldn't solve. I don't do this any more, because several of them
> reported a negative experience.
>
> In contrast, in the Stackexchange model, there are numerous sublists
> cross-classified by their tags.  If I have a specific knitr, ggplot2, 
> LaTeX, or statistical modeling question, I'm now more likely to post 
> it there, and the worst that can happen is that no one "upvotes" it
> or someone (helpfully) marks it as a duplicate of a similar question.
> But comments there are not propagated to all subscribers,
> and those who reply helpfully, can see their solutions accepted or not,
> or commented on in that specific topic.
>
> Perhaps one solution would be to create a new "R-not-help" list where,
> as in a Monty Python skit, people could be directed there to be 
> insulted and all these unhelpful replies could be sent.
>
> A milder alternative is to encourage some R-help subscribers to click 
> the "Don't send" or "Save" button and think better of their replies.
> </rant>
>


From wdunlap at tibco.com  Sun Jan 24 23:45:02 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 24 Jan 2016 14:45:02 -0800
Subject: [R] Error because of large dimension
In-Reply-To: <CAFDcVCS2WVLhFaVEKEg=Q+Y7ThTS62x5ysnx3o_EzOO8A9rxZA@mail.gmail.com>
References: <CAHLnndb=fQtZ2n9AxLHARbZLw79=bE48Q9Vm3ZCMFhG++GC_RA@mail.gmail.com>
	<CAFDcVCS2WVLhFaVEKEg=Q+Y7ThTS62x5ysnx3o_EzOO8A9rxZA@mail.gmail.com>
Message-ID: <CAF8bMcY_kKDV1Ur_=44y8sU7csiU6-dEdB0yPvVL=xUbWcSbcQ@mail.gmail.com>

> 28 PiB.  Storing such a large matrix even on file is not possible.

The ads for Amazon Red Shift say it is possible.  E.g.,
  Amazon Redshift is a fast, fully managed, petabyte-scale data
  warehouse that makes it simple and cost-effective to analyze
  all your data using your existing business intelligence tools.
  Start small for $0.25 per hour with no commitments and scale
  to petabytes for $1,000 per terabyte per year, less than a tenth
  the cost of traditional solutions. Customers typically see 3x
  compression, reducing their costs to $333 per uncompressed
  terabyte per year.

Cost may be an issue:

28 petabytes * 1024 petabytes/terabyte * $333 terabyte/year ~= $9.5
million/year
or $26 thousand/day.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sun, Jan 24, 2016 at 1:29 PM, Henrik Bengtsson <
henrik.bengtsson at gmail.com> wrote:

> FYI, the matrix you tried to allocate would hold
> (3195*1290*495*35*35*35*15) * 3 = 3.936248e+15 values.  Each value
> would occupy 8 bytes of memory (for the double data type).  In other
> words, in order to keep this data matrix in memory you would require a
> computer with at least 3.148998e+16 bytes of RAM, i.e. 29327331 GiB =
> 28640 TiB = 28 PiB.  Storing such a large matrix even on file is not
> possible.
>
> In other words, you need to figure out how to approach your original
> problem in a different way.
>
> /Henrik
>
> On Sun, Jan 24, 2016 at 8:46 AM, li li <hannah.hlx at gmail.com> wrote:
> > Hi all,
> >   I am doing some calculation with very large dimension. I need to
> create a
> > matrix
> > with three columns and a very large number of rows
> > (3195*1290*495*35*35*35*15=1.312083e+15) i
> > n order to allocate calculation result from a for loop.
> > R does not allow me to create such a matrix because of the large
> dimension
> > (see below). Is there a way to go around this?
> >   Thanks very much!!
> >      Hanna
> >
> >
> >> matrix(0, 3195*1290*495*35*35*35*15, 3)
> > Error in matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
> >   invalid 'nrow' value (too large or NA)
> > In addition: Warning message:
> > In matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
> >   NAs introduced by coercion
> >>
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Mon Jan 25 01:20:19 2016
From: hannah.hlx at gmail.com (li li)
Date: Sun, 24 Jan 2016 19:20:19 -0500
Subject: [R] Error because of large dimension
In-Reply-To: <CAF8bMcY_kKDV1Ur_=44y8sU7csiU6-dEdB0yPvVL=xUbWcSbcQ@mail.gmail.com>
References: <CAHLnndb=fQtZ2n9AxLHARbZLw79=bE48Q9Vm3ZCMFhG++GC_RA@mail.gmail.com>
	<CAFDcVCS2WVLhFaVEKEg=Q+Y7ThTS62x5ysnx3o_EzOO8A9rxZA@mail.gmail.com>
	<CAF8bMcY_kKDV1Ur_=44y8sU7csiU6-dEdB0yPvVL=xUbWcSbcQ@mail.gmail.com>
Message-ID: <CAHLnndb=0LfspwWVWHwQCyRfTF68845erSY4Fx==pHKiZHP4bg@mail.gmail.com>

Thanks all for the reply. I think I need to think of other ways to approach
the problem.
    Hanna

2016-01-24 17:45 GMT-05:00 William Dunlap <wdunlap at tibco.com>:

> > 28 PiB.  Storing such a large matrix even on file is not possible.
>
> The ads for Amazon Red Shift say it is possible.  E.g.,
>   Amazon Redshift is a fast, fully managed, petabyte-scale data
>   warehouse that makes it simple and cost-effective to analyze
>   all your data using your existing business intelligence tools.
>   Start small for $0.25 per hour with no commitments and scale
>   to petabytes for $1,000 per terabyte per year, less than a tenth
>   the cost of traditional solutions. Customers typically see 3x
>   compression, reducing their costs to $333 per uncompressed
>   terabyte per year.
>
> Cost may be an issue:
>
> 28 petabytes * 1024 petabytes/terabyte * $333 terabyte/year ~= $9.5
> million/year
> or $26 thousand/day.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Sun, Jan 24, 2016 at 1:29 PM, Henrik Bengtsson <
> henrik.bengtsson at gmail.com> wrote:
>
>> FYI, the matrix you tried to allocate would hold
>> (3195*1290*495*35*35*35*15) * 3 = 3.936248e+15 values.  Each value
>> would occupy 8 bytes of memory (for the double data type).  In other
>> words, in order to keep this data matrix in memory you would require a
>> computer with at least 3.148998e+16 bytes of RAM, i.e. 29327331 GiB =
>> 28640 TiB = 28 PiB.  Storing such a large matrix even on file is not
>> possible.
>>
>> In other words, you need to figure out how to approach your original
>> problem in a different way.
>>
>> /Henrik
>>
>> On Sun, Jan 24, 2016 at 8:46 AM, li li <hannah.hlx at gmail.com> wrote:
>> > Hi all,
>> >   I am doing some calculation with very large dimension. I need to
>> create a
>> > matrix
>> > with three columns and a very large number of rows
>> > (3195*1290*495*35*35*35*15=1.312083e+15) i
>> > n order to allocate calculation result from a for loop.
>> > R does not allow me to create such a matrix because of the large
>> dimension
>> > (see below). Is there a way to go around this?
>> >   Thanks very much!!
>> >      Hanna
>> >
>> >
>> >> matrix(0, 3195*1290*495*35*35*35*15, 3)
>> > Error in matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
>> >   invalid 'nrow' value (too large or NA)
>> > In addition: Warning message:
>> > In matrix(0, 3195 * 1290 * 495 * 35 * 35 * 35 * 15, 3) :
>> >   NAs introduced by coercion
>> >>
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From teotjunk at hotmail.com  Mon Jan 25 04:45:41 2016
From: teotjunk at hotmail.com (TJUN KIAT TEO)
Date: Mon, 25 Jan 2016 11:45:41 +0800
Subject: [R] Mixture Discriminant Analysis and  Penalized LDA
Message-ID: <SNT152-W542150431E470824022A51DFC70@phx.gbl>

Hi

I noticed we have MDA and Mclust for Mixture Discriminant Analysis and Penalized LDA. Do we have a R packages for Penalized MDA?

Tjun Kiat



 		 	   		  
	[[alternative HTML version deleted]]


From shaam249 at student.otago.ac.nz  Mon Jan 25 01:39:22 2016
From: shaam249 at student.otago.ac.nz (Amina Shahzadi Shahzadi)
Date: Mon, 25 Jan 2016 00:39:22 +0000
Subject: [R] Block Triangular Matirx
Message-ID: <SG2PR03MB09220199F2FA68ACAE90092090C70@SG2PR03MB0922.apcprd03.prod.outlook.com>

Hi


I want to create a block upper triangular square matrix.


Anybody who can help in this regard.


Thank You

	[[alternative HTML version deleted]]


From olivier.crouzet at univ-nantes.fr  Mon Jan 25 07:45:34 2016
From: olivier.crouzet at univ-nantes.fr (Olivier Crouzet)
Date: Mon, 25 Jan 2016 06:45:34 +0000
Subject: [R] Block Triangular Matirx
In-Reply-To: <SG2PR03MB09220199F2FA68ACAE90092090C70@SG2PR03MB0922.apcprd03.prod.outlook.com>
References: <SG2PR03MB09220199F2FA68ACAE90092090C70@SG2PR03MB0922.apcprd03.prod.outlook.com>
Message-ID: <983096080-1453704333-cardhu_decombobulator_blackberry.rim.net-588899922-@b17.c1.bise7.blackberry>

Hi, I think this page will help,

https://stat.ethz.ch/R-manual/R-devel/library/base/html/lower.tri.html

Olivier.

--
Olivier Crouzet
LLING - Laboratoire de Linguistique de Nantes - EA3827
Universit? de Nantes

-----Original Message-----
From: Amina Shahzadi Shahzadi <shaam249 at student.otago.ac.nz>
Sender: "R-help" <r-help-bounces at r-project.org>Date: Mon, 25 Jan 2016 00:39:22 
To: r-help at R-project.org<r-help at r-project.org>
Subject: [R] Block Triangular Matirx

Hi


I want to create a block upper triangular square matrix.


Anybody who can help in this regard.


Thank You

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From teotjunk at hotmail.com  Mon Jan 25 09:52:42 2016
From: teotjunk at hotmail.com (TJUN KIAT TEO)
Date: Mon, 25 Jan 2016 16:52:42 +0800
Subject: [R] Matrix of Lists containing numbers and characters
Message-ID: <SNT152-W40EDD76D991DE9E59D4A16DFC70@phx.gbl>

Here is my sample code

TunePar<-matrix(list(Null),2,2)

TunePar[[1,1]]=list(subclasses=3,model="gen.ridge")

tune=paste(colnames(Temp),Temp,sep="=")
tune=paste(tune,collapse=",")

However when I type tune

This is what I get

 "subclasses=3,model=1"


The text "gen.ridge has been converted to the number 1. Any idea why? Thanks



 		 	   		  
	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Jan 25 10:05:37 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 25 Jan 2016 09:05:37 +0000
Subject: [R] Matrix of Lists containing numbers and characters
In-Reply-To: <SNT152-W40EDD76D991DE9E59D4A16DFC70@phx.gbl>
References: <SNT152-W40EDD76D991DE9E59D4A16DFC70@phx.gbl>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C500B178@SRVEXCHMBX.precheza.cz>

Hi

What is Temp?

Just a guess. "model" variable is factor and it is converted to its numeric representation during paste or any other operation you made.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of TJUN
> KIAT TEO
> Sent: Monday, January 25, 2016 9:53 AM
> To: r-help at r-project.org
> Subject: [R] Matrix of Lists containing numbers and characters
>
> Here is my sample code
>
> TunePar<-matrix(list(Null),2,2)
>
> TunePar[[1,1]]=list(subclasses=3,model="gen.ridge")
>
> tune=paste(colnames(Temp),Temp,sep="=")
> tune=paste(tune,collapse=",")
>
> However when I type tune
>
> This is what I get
>
>  "subclasses=3,model=1"
>
>
> The text "gen.ridge has been converted to the number 1. Any idea why?
> Thanks
>
>
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From olivier.eterradossi at mines-ales.fr  Mon Jan 25 10:27:52 2016
From: olivier.eterradossi at mines-ales.fr (Olivier ETERRADOSSI)
Date: Mon, 25 Jan 2016 10:27:52 +0100 (CET)
Subject: [R] unexpected behaviour of an extended time series (using
	packages spuRs and xts)
In-Reply-To: <CAPPM_gTd6BHTpiwCHqOWVyCcDcFfHcDUYDn=aDM8S7Ka2DULdg@mail.gmail.com>
References: <00a401d14edc$ffef75b0$ffce6110$@mines-ales.fr>
	<CAPPM_gTd6BHTpiwCHqOWVyCcDcFfHcDUYDn=aDM8S7Ka2DULdg@mail.gmail.com>
Message-ID: <007d01d15752$9ceae6b0$d6c0b410$@mines-ales.fr>

Thank you Joshua for your valuable help,
It just works fine now !
Regards, O.

Olivier ETERRADOSSI
Ma?tre-Assistant HDR
Ing?nierie de l?aspect visuel et tactile
C2MA ? P?le R.I.M.E. (site de Pau)
Ecole des mines d?Al?s
Technopole H?lioparc
2 av. P. Angot
64053 PAU Cedex 9
France


-----Message d'origine-----
De : Joshua Ulrich [mailto:josh.m.ulrich at gmail.com]
Envoy? : vendredi 22 janvier 2016 18:23
? : Olivier ETERRADOSSI
Cc : R-Help
Objet : Re: [R] unexpected behaviour of an extended time series (using 
packages spuRs and xts)

Try using the latest xts on GitHub:
https://github.com/joshuaulrich/xts

On Thu, Jan 14, 2016 at 9:05 AM, Olivier ETERRADOSSI 
<olivier.eterradossi at mines-ales.fr> wrote:
> Hi list,
>
>
>
> I thought I knew how to use extended time series (package xts), but I
> was wrong  J  ?
>
>
>
> While preparing a toy example for something else, using data provided
> in R, I run into an unexpected problem and can?t figure by myself what
> is happening below, can anyone of you tell ? I searched the archives
> but didn?t locate any answer. Probably it?s trivial, so please forgive  :
>
>
>
> I?m using :
>
> R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree" / Platform:
> x86_64-w64-mingw32/x64 (64-bit)
>
> Packages are updated weekly, sometimes daily.
>
>
>
> I take some data from package spuRs :
>
>> library(spuRs)
>
>> data(kew)
>
>
>
> I turn the dataframe into time series (by combining each kew[,2:13]
> one after each other into a vector, and turning the vector into time 
> series).
>
>
>
> One is ts :
>
>
>
>>
> kew.ts<-ts(data=stock,start=kew$year[1],end=kew$year[length(kew$year)]
> ,fre
> quency=12)
>
>
>
> And the other is xts, it looks fine at first :
>
>
>
>> kew.xts<-as.xts(kew.ts)
>
>> periodicity(kew.xts)
>
> Monthly periodicity from janv. 1697 to janv. 1999  # OK
>
>> hist(kew.xts) # OK
>
>> summary(kew.xts)
>
>      Index         kew.xts
>
>  Min.   :1697   Min.   :  0.00
>
>  1st Qu.:1772   1st Qu.: 29.70
>
>  Median :1848   Median : 47.00
>
>  Mean   :1848   Mean   : 51.14
>
>  3rd Qu.:1924   3rd Qu.: 67.60
>
>  Max.   :1999   Max.   :189.00  # OK
>
>
>
>> gdata::is.what(kew.xts)
>
> [1] "is.array"        "is.atomic"       "is.double"
> "is.index.unique"
>
> [5] "is.matrix"       "is.numeric"      "is.object"       "is.regular"
>
>
>  [9] "is.time.unique"  "is.unsorted"     "is.xts"          "is.zoo"
> # seems OK
>
>
>
>
>
> # But now, first try :
>
>> plot(kew.xts)
>
> Error in if (on == "years") { :
>
>   valeur manquante l? o? TRUE / FALSE est requis     # french for ?
> missing value where TRUE/FALSE is required ?
>
>
>
> # hmmmm, let?s try something else :
>
>> plot(kew.xts['1697-01/1979/']) # OK
>
>
>
>> plot(kew.xts['1697-01/1980/'])
>
> Error in if (on == "years") { :
>
>   valeur manquante l? o? TRUE / FALSE est requis
>
>
>
>> plot(kew.xts['1697-01/1979-12/']) # OK
>
>
>
>> plot(kew.xts['1697-01/1980-01/'])
>
> Error in if (on == "years") { :
>
>   valeur manquante l? o? TRUE / FALSE est requis
>
>
>
> # but?!  :
>
>
>
>> plot(kew.xts['1979-01/1980/']) # OK !!!!!
>
>
>
> And so are :
>
>> plot (kew.xts['1978/1980/'])
>
>> plot(kew.xts['1977/1982/'])
>
>> plot(kew.xts['1977-01/1982-12'])  # and so on?
>
>
>
> I?m puzzled ! I have probably missed a trivial point? Can someone tell ?
>
>
>
> Thanks a lot list, regards, Olivier
>
>
>
> --------------------------
>
> Olivier ETERRADOSSI
>
> Ma?tre-Assistant, HDR
>
> Ecole des Mines d?Al?s (C2MA, site de Pau)
>
> Ing?nierie de l'aspect visuel et tactile des mat?riaux
>
> P?le ? Recherche sur les Interactions des Mat?riaux avec leur
> Environnement ? (RIME)
>
> H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
>
> Tel : 05 59 30 90 35 (direct) - 05 59 30  54 25 (std)
>
> Fax : 05 59 30 63 68
>
>  <http://www.mines-ales.fr/> http://www.mines-ales.fr
>
>  <http://www.mines-telecom.fr/> http://www.mines-telecom.fr
>
>
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com
R/Finance 2016 | www.rinfinance.com


From pdalgd at gmail.com  Mon Jan 25 11:21:50 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 25 Jan 2016 11:21:50 +0100
Subject: [R] tcltk table "validateCommand"
In-Reply-To: <CAJeYpE-1O4AL00pnhtX8Nx=6gTe9ha5RXKLR3Va8OkvHfDb=Lw@mail.gmail.com>
References: <CAJeYpE-1O4AL00pnhtX8Nx=6gTe9ha5RXKLR3Va8OkvHfDb=Lw@mail.gmail.com>
Message-ID: <3058A258-836E-4EF2-8F6D-EBF1F4270AD1@gmail.com>

It's been so long that I have forgotten how to get the package with the table widget installed on OSX, so I cannot check things for you. However, the canonical way to handle %S type arguments is to pass them as formal arguments to the callback, e.g.

> .Tcl.callback(function(x,y)x+y)
[1] "R_call 0x7f9a34806ca0 %x %y"

so I would assume that you should just define your 

CellValidation <- function(S){....}

and then just access S as a variable inside the function. 

As far as I remember, this only works at entry completion, though. That does sort of make sense since not every prefix of a valid entry is valid ("1e-2" is a double, "1e-" is not). If you want to actually disable certain keys during entry, then you have a larger task on your hand.

-pd

On 22 Jan 2016, at 21:25 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:

> I'd like to allow users to edit data in tcltk tables and to use vcmd to
> validate data entry, e.g., not allowing non-numbers to be entered in
> numeric cells and not allowing '\n' to be entered in text cells.
> 
> The problem is that I can't figure out how to "see" their data entry before
> it is entered, although it looks like %S can be somehow used in vcmd to get
> this information.
> 
> Example: to disallow '\n' to be entered into a cell in an editable table:
> 
> require(tcltk2)
> tt<-tktoplevel(); tfr<-tkframe(tt); tkgrid(tfr)
> tableData<-tclArray()
> tableData[[0,0]]<-"junk"
> 
> CellValidation<-function(){
> 
> ## http://www.tcl.tk/community/hobbs/tcl/capp/tkTable/tkTable.html says:
> ## *%S* For *ValidateCommand*, it is the potential new value of the cell
> being validated.
> ## which is exactly what I want, but I can't figure out how to do that.
> ## The following allows one bad character and then disallows further edits
> 
>  testval<-tclvalue(tcl(table1,"curvalue"))
> 
>  if (length(grep("\n",testval))>0)  return(tcl("expr", FALSE))  else
> return(tcl("expr", TRUE))
> }
> 
> table1<<-tk2table(tfr,
>  rows=1,cols=1,
>  selectmode="extended",
>  variable=tableData,
>  validate=T,
>  vcmd=CellValidation
> )
> 
> tcl(table1,"tag","configure", "active", fg='black',bg=colors()[411])
> tkgrid(table1)
> 
> How can I get the %S value rather than the tcl(table1,"curvalue")?
> 
> Much thanks for any help.
> 
> -Dan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mxkuhn at gmail.com  Mon Jan 25 16:24:41 2016
From: mxkuhn at gmail.com (Max Kuhn)
Date: Mon, 25 Jan 2016 10:24:41 -0500
Subject: [R] Mixture Discriminant Analysis and Penalized LDA
In-Reply-To: <SNT152-W542150431E470824022A51DFC70@phx.gbl>
References: <SNT152-W542150431E470824022A51DFC70@phx.gbl>
Message-ID: <CAJ9CoWkUJgVUEcazb+ttu9O+riYfad_Kj0Y1GAQip_AH9E+wPw@mail.gmail.com>

There is a function called `smda` in the sparseLDA package that implements
the model described in Clemmensen, L., Hastie, T., Witten, D. and Ersb?ll,
B. Sparse discriminant analysis, Technometrics, 53(4): 406-413, 2011

Max

On Sun, Jan 24, 2016 at 10:45 PM, TJUN KIAT TEO <teotjunk at hotmail.com>
wrote:

> Hi
>
> I noticed we have MDA and Mclust for Mixture Discriminant Analysis and
> Penalized LDA. Do we have a R packages for Penalized MDA?
>
> Tjun Kiat
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Mark.Fowler at dfo-mpo.gc.ca  Mon Jan 25 13:23:16 2016
From: Mark.Fowler at dfo-mpo.gc.ca (Fowler, Mark)
Date: Mon, 25 Jan 2016 12:23:16 +0000
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A54553.60603@yorku.ca>
References: <56A371F5.5070509@nancy.inra.fr> <56A54553.60603@yorku.ca>
Message-ID: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>

I'm glad to see the issue of negative feedback addressed. I can especially relate to the 'cringe' feeling when reading some authoritarian backhand to a new user. We do see a number of obviously inappropriate or overly lazy postings, but I encounter far more postings where I don't feel competent to judge their merit. It might be better to simply disregard a posting one does not like for some reason. It might also be worthwhile to actively counter negative feedback when we experience that 'cringing' moment. I'm not thinking to foster contention, but simply to provide some tangible reassurance to new users, and not just the ones invoking the negative feedback, that a particular respondent may not represent the perspective of the list.

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael Friendly
Sent: January 24, 2016 5:43 PM
To: Jean-Luc Dupouey; r-help at r-project.org
Subject: Re: [R] R-help mailing list activity / R-not-help?


On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
> Dear members,
>
> Not a technical question:
But one worth raising...
>
> The number of threads in this mailing list, following a long period of 
> increase, has been regularly and strongly decreasing since 2010, 
> passing from more than 40K threads to less than 11K threads last year. 
> The trend is similar for most of the "ancient" mailing lists of the R-project.
[snip ...]
>
> I hope it is the wright place to ask this question. Thanks in advance,
>

In addition to the other replies, there is another trend I've seen that has actively worked to suppress discussion on R-help and move it elsewhere. The general things:
- R-help was too unwieldy and so it was a good idea to hive-off specialized topics to various sub lists, R-SIG-Mac, R-SIG-Geo, etc.
- Many people posted badly-formed questions to R-help, and so it was a good idea to develop and refer to the posting guide to mitigate the number of purely junk postings.

<rant>
Yet, the trend I've seen is one of increasing **R-not-help**, in that there are many posts, often by new R users who get replies that not infrequently range from just mildly off-putting to actively hostile:

- Is this homework? We don't do homework (sometimes false alarms, where the OP has to reply to say it is not)
- Didn't you bother to do your homework, RTFM, or Google?
- This is off-topic because XXX (e.g., it is not strictly an R programming question).
- You asked about doing XXX, but this is a stupid thing to want to do.
- Don't ask here; you need to talk to a statistical consultant.

I find this sad in a public mailing list sent to all R-help subscribers and I sometimes cringe when I read replies to people who were actually trying to get help with some R-related problem, but expressed it badly, didn't know exactly what to ask for, or how to format it, or somehow motivated a frequent-replier to publicly dis the OP.

On the other hand, I still see a spirit of great generosity among some people who frequently reply to R-help, taking a possibly badly posed or ill-formatted question, and going to some lengths to provide a a helpful answer of some sort.  I applaud those who take the time and effort to do this.

I use R in a number of my courses, and used to advise students to post to R-help for general programming questions (not just homework) they couldn't solve. I don't do this any more, because several of them reported a negative experience.

In contrast, in the Stackexchange model, there are numerous sublists cross-classified by their tags.  If I have a specific knitr, ggplot2, LaTeX, or statistical modeling question, I'm now more likely to post it there, and the worst that can happen is that no one "upvotes" it or someone (helpfully) marks it as a duplicate of a similar question.
But comments there are not propagated to all subscribers, and those who reply helpfully, can see their solutions accepted or not, or commented on in that specific topic.

Perhaps one solution would be to create a new "R-not-help" list where, as in a Monty Python skit, people could be directed there to be insulted and all these unhelpful replies could be sent.

A milder alternative is to encourage some R-help subscribers to click the "Don't send" or "Save" button and think better of their replies.
</rant>

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept. & Chair, Quantitative Methods
York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
4700 Keele Street    Web:   http://www.datavis.ca
Toronto, ONT  M3J 1P3 CANADA

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From okeyes at wikimedia.org  Mon Jan 25 17:06:35 2016
From: okeyes at wikimedia.org (Oliver Keyes)
Date: Mon, 25 Jan 2016 11:06:35 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
References: <56A371F5.5070509@nancy.inra.fr> <56A54553.60603@yorku.ca>
	<88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
Message-ID: <CAAUQgdDUp-cT7wjE_o5jt3hq_JD6=Btbu05Aobq5Tr9WEU051Q@mail.gmail.com>

+1. And frankly I would like to suggest that there is another obvious
solution here; pairing a set of guidelines around expected user
behaviour with removing people from the mailing list, or moderating
them, if they do not think that creating a non-toxic environment is
good.

On 25 January 2016 at 07:23, Fowler, Mark <Mark.Fowler at dfo-mpo.gc.ca> wrote:
> I'm glad to see the issue of negative feedback addressed. I can especially relate to the 'cringe' feeling when reading some authoritarian backhand to a new user. We do see a number of obviously inappropriate or overly lazy postings, but I encounter far more postings where I don't feel competent to judge their merit. It might be better to simply disregard a posting one does not like for some reason. It might also be worthwhile to actively counter negative feedback when we experience that 'cringing' moment. I'm not thinking to foster contention, but simply to provide some tangible reassurance to new users, and not just the ones invoking the negative feedback, that a particular respondent may not represent the perspective of the list.
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael Friendly
> Sent: January 24, 2016 5:43 PM
> To: Jean-Luc Dupouey; r-help at r-project.org
> Subject: Re: [R] R-help mailing list activity / R-not-help?
>
>
> On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
>> Dear members,
>>
>> Not a technical question:
> But one worth raising...
>>
>> The number of threads in this mailing list, following a long period of
>> increase, has been regularly and strongly decreasing since 2010,
>> passing from more than 40K threads to less than 11K threads last year.
>> The trend is similar for most of the "ancient" mailing lists of the R-project.
> [snip ...]
>>
>> I hope it is the wright place to ask this question. Thanks in advance,
>>
>
> In addition to the other replies, there is another trend I've seen that has actively worked to suppress discussion on R-help and move it elsewhere. The general things:
> - R-help was too unwieldy and so it was a good idea to hive-off specialized topics to various sub lists, R-SIG-Mac, R-SIG-Geo, etc.
> - Many people posted badly-formed questions to R-help, and so it was a good idea to develop and refer to the posting guide to mitigate the number of purely junk postings.
>
> <rant>
> Yet, the trend I've seen is one of increasing **R-not-help**, in that there are many posts, often by new R users who get replies that not infrequently range from just mildly off-putting to actively hostile:
>
> - Is this homework? We don't do homework (sometimes false alarms, where the OP has to reply to say it is not)
> - Didn't you bother to do your homework, RTFM, or Google?
> - This is off-topic because XXX (e.g., it is not strictly an R programming question).
> - You asked about doing XXX, but this is a stupid thing to want to do.
> - Don't ask here; you need to talk to a statistical consultant.
>
> I find this sad in a public mailing list sent to all R-help subscribers and I sometimes cringe when I read replies to people who were actually trying to get help with some R-related problem, but expressed it badly, didn't know exactly what to ask for, or how to format it, or somehow motivated a frequent-replier to publicly dis the OP.
>
> On the other hand, I still see a spirit of great generosity among some people who frequently reply to R-help, taking a possibly badly posed or ill-formatted question, and going to some lengths to provide a a helpful answer of some sort.  I applaud those who take the time and effort to do this.
>
> I use R in a number of my courses, and used to advise students to post to R-help for general programming questions (not just homework) they couldn't solve. I don't do this any more, because several of them reported a negative experience.
>
> In contrast, in the Stackexchange model, there are numerous sublists cross-classified by their tags.  If I have a specific knitr, ggplot2, LaTeX, or statistical modeling question, I'm now more likely to post it there, and the worst that can happen is that no one "upvotes" it or someone (helpfully) marks it as a duplicate of a similar question.
> But comments there are not propagated to all subscribers, and those who reply helpfully, can see their solutions accepted or not, or commented on in that specific topic.
>
> Perhaps one solution would be to create a new "R-not-help" list where, as in a Monty Python skit, people could be directed there to be insulted and all these unhelpful replies could be sent.
>
> A milder alternative is to encourage some R-help subscribers to click the "Don't send" or "Save" button and think better of their replies.
> </rant>
>
> --
> Michael Friendly     Email: friendly AT yorku DOT ca
> Professor, Psychology Dept. & Chair, Quantitative Methods
> York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
> 4700 Keele Street    Web:   http://www.datavis.ca
> Toronto, ONT  M3J 1P3 CANADA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Oliver Keyes
Count Logula
Wikimedia Foundation


From Ted.Harding at wlandres.net  Mon Jan 25 18:14:10 2016
From: Ted.Harding at wlandres.net ( (Ted Harding))
Date: Mon, 25 Jan 2016 17:14:10 -0000 (GMT)
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
Message-ID: <XFMail.20160125171410.Ted.Harding@wlandres.net>

My feelings exactly! (And since quite some time ago).
Ted.

On 25-Jan-2016 12:23:16 Fowler, Mark wrote:
> I'm glad to see the issue of negative feedback addressed. I can especially
> relate to the 'cringe' feeling when reading some authoritarian backhand to a
> new user. We do see a number of obviously inappropriate or overly lazy
> postings, but I encounter far more postings where I don't feel competent to
> judge their merit. It might be better to simply disregard a posting one does
> not like for some reason. It might also be worthwhile to actively counter
> negative feedback when we experience that 'cringing' moment. I'm not thinking
> to foster contention, but simply to provide some tangible reassurance to new
> users, and not just the ones invoking the negative feedback, that a
> particular respondent may not represent the perspective of the list.
> 
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael
> Friendly
> Sent: January 24, 2016 5:43 PM
> To: Jean-Luc Dupouey; r-help at r-project.org
> Subject: Re: [R] R-help mailing list activity / R-not-help?
> 
> 
> On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
>> Dear members,
>>
>> Not a technical question:
> But one worth raising...
>>
>> The number of threads in this mailing list, following a long period of 
>> increase, has been regularly and strongly decreasing since 2010, 
>> passing from more than 40K threads to less than 11K threads last year. 
>> The trend is similar for most of the "ancient" mailing lists of the
>> R-project.
> [snip ...]
>>
>> I hope it is the wright place to ask this question. Thanks in advance,
>>
> 
> In addition to the other replies, there is another trend I've seen that has
> actively worked to suppress discussion on R-help and move it elsewhere. The
> general things:
> - R-help was too unwieldy and so it was a good idea to hive-off specialized
> topics to various sub lists, R-SIG-Mac, R-SIG-Geo, etc.
> - Many people posted badly-formed questions to R-help, and so it was a good
> idea to develop and refer to the posting guide to mitigate the number of
> purely junk postings.
> 
> <rant>
> Yet, the trend I've seen is one of increasing **R-not-help**, in that there
> are many posts, often by new R users who get replies that not infrequently
> range from just mildly off-putting to actively hostile:
> 
> - Is this homework? We don't do homework (sometimes false alarms, where the
> OP has to reply to say it is not)
> - Didn't you bother to do your homework, RTFM, or Google?
> - This is off-topic because XXX (e.g., it is not strictly an R programming
> question).
> - You asked about doing XXX, but this is a stupid thing to want to do.
> - Don't ask here; you need to talk to a statistical consultant.
> 
> I find this sad in a public mailing list sent to all R-help subscribers and I
> sometimes cringe when I read replies to people who were actually trying to
> get help with some R-related problem, but expressed it badly, didn't know
> exactly what to ask for, or how to format it, or somehow motivated a
> frequent-replier to publicly dis the OP.
> 
> On the other hand, I still see a spirit of great generosity among some people
> who frequently reply to R-help, taking a possibly badly posed or
> ill-formatted question, and going to some lengths to provide a a helpful
> answer of some sort.  I applaud those who take the time and effort to do
> this.
> 
> I use R in a number of my courses, and used to advise students to post to
> R-help for general programming questions (not just homework) they couldn't
> solve. I don't do this any more, because several of them reported a negative
> experience.
> 
> In contrast, in the Stackexchange model, there are numerous sublists
> cross-classified by their tags.  If I have a specific knitr, ggplot2, LaTeX,
> or statistical modeling question, I'm now more likely to post it there, and
> the worst that can happen is that no one "upvotes" it or someone (helpfully)
> marks it as a duplicate of a similar question.
> But comments there are not propagated to all subscribers, and those who reply
> helpfully, can see their solutions accepted or not, or commented on in that
> specific topic.
> 
> Perhaps one solution would be to create a new "R-not-help" list where, as in
> a Monty Python skit, people could be directed there to be insulted and all
> these unhelpful replies could be sent.
> 
> A milder alternative is to encourage some R-help subscribers to click the
> "Don't send" or "Save" button and think better of their replies.
> </rant>
> 
> -- 
> Michael Friendly     Email: friendly AT yorku DOT ca
> Professor, Psychology Dept. & Chair, Quantitative Methods
> York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
> 4700 Keele Street    Web:   http://www.datavis.ca
> Toronto, ONT  M3J 1P3 CANADA

-------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
Date: 25-Jan-2016  Time: 17:14:06
This message was sent by XFMail


From jsorkin at grecc.umaryland.edu  Mon Jan 25 18:35:48 2016
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 25 Jan 2016 12:35:48 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <XFMail.20160125171410.Ted.Harding@wlandres.net>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
Message-ID: <56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>

When we read acerbic replies we should remind the poster to reply in a more moderate tone. On the other hand  noting that the list is not intended to be a source of answers to home work questions is 100% appropriate. This philosophy is intended both to keep the list from being flooded with questions and to make sure that no student has an unfair advantage.
John 

> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)


> On Jan 25, 2016, at 12:17 PM, Ted Harding <Ted.Harding at wlandres.net> wrote:
> 
> My feelings exactly! (And since quite some time ago).
> Ted.
> 
>> On 25-Jan-2016 12:23:16 Fowler, Mark wrote:
>> I'm glad to see the issue of negative feedback addressed. I can especially
>> relate to the 'cringe' feeling when reading some authoritarian backhand to a
>> new user. We do see a number of obviously inappropriate or overly lazy
>> postings, but I encounter far more postings where I don't feel competent to
>> judge their merit. It might be better to simply disregard a posting one does
>> not like for some reason. It might also be worthwhile to actively counter
>> negative feedback when we experience that 'cringing' moment. I'm not thinking
>> to foster contention, but simply to provide some tangible reassurance to new
>> users, and not just the ones invoking the negative feedback, that a
>> particular respondent may not represent the perspective of the list.
>> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael
>> Friendly
>> Sent: January 24, 2016 5:43 PM
>> To: Jean-Luc Dupouey; r-help at r-project.org
>> Subject: Re: [R] R-help mailing list activity / R-not-help?
>> 
>> 
>>> On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
>>> Dear members,
>>> 
>>> Not a technical question:
>> But one worth raising...
>>> 
>>> The number of threads in this mailing list, following a long period of 
>>> increase, has been regularly and strongly decreasing since 2010, 
>>> passing from more than 40K threads to less than 11K threads last year. 
>>> The trend is similar for most of the "ancient" mailing lists of the
>>> R-project.
>> [snip ...]
>>> 
>>> I hope it is the wright place to ask this question. Thanks in advance,
>> 
>> In addition to the other replies, there is another trend I've seen that has
>> actively worked to suppress discussion on R-help and move it elsewhere. The
>> general things:
>> - R-help was too unwieldy and so it was a good idea to hive-off specialized
>> topics to various sub lists, R-SIG-Mac, R-SIG-Geo, etc.
>> - Many people posted badly-formed questions to R-help, and so it was a good
>> idea to develop and refer to the posting guide to mitigate the number of
>> purely junk postings.
>> 
>> <rant>
>> Yet, the trend I've seen is one of increasing **R-not-help**, in that there
>> are many posts, often by new R users who get replies that not infrequently
>> range from just mildly off-putting to actively hostile:
>> 
>> - Is this homework? We don't do homework (sometimes false alarms, where the
>> OP has to reply to say it is not)
>> - Didn't you bother to do your homework, RTFM, or Google?
>> - This is off-topic because XXX (e.g., it is not strictly an R programming
>> question).
>> - You asked about doing XXX, but this is a stupid thing to want to do.
>> - Don't ask here; you need to talk to a statistical consultant.
>> 
>> I find this sad in a public mailing list sent to all R-help subscribers and I
>> sometimes cringe when I read replies to people who were actually trying to
>> get help with some R-related problem, but expressed it badly, didn't know
>> exactly what to ask for, or how to format it, or somehow motivated a
>> frequent-replier to publicly dis the OP.
>> 
>> On the other hand, I still see a spirit of great generosity among some people
>> who frequently reply to R-help, taking a possibly badly posed or
>> ill-formatted question, and going to some lengths to provide a a helpful
>> answer of some sort.  I applaud those who take the time and effort to do
>> this.
>> 
>> I use R in a number of my courses, and used to advise students to post to
>> R-help for general programming questions (not just homework) they couldn't
>> solve. I don't do this any more, because several of them reported a negative
>> experience.
>> 
>> In contrast, in the Stackexchange model, there are numerous sublists
>> cross-classified by their tags.  If I have a specific knitr, ggplot2, LaTeX,
>> or statistical modeling question, I'm now more likely to post it there, and
>> the worst that can happen is that no one "upvotes" it or someone (helpfully)
>> marks it as a duplicate of a similar question.
>> But comments there are not propagated to all subscribers, and those who reply
>> helpfully, can see their solutions accepted or not, or commented on in that
>> specific topic.
>> 
>> Perhaps one solution would be to create a new "R-not-help" list where, as in
>> a Monty Python skit, people could be directed there to be insulted and all
>> these unhelpful replies could be sent.
>> 
>> A milder alternative is to encourage some R-help subscribers to click the
>> "Don't send" or "Save" button and think better of their replies.
>> </rant>
>> 
>> -- 
>> Michael Friendly     Email: friendly AT yorku DOT ca
>> Professor, Psychology Dept. & Chair, Quantitative Methods
>> York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
>> 4700 Keele Street    Web:   http://www.datavis.ca
>> Toronto, ONT  M3J 1P3 CANADA
> 
> -------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
> Date: 25-Jan-2016  Time: 17:14:06
> This message was sent by XFMail
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From murdoch.duncan at gmail.com  Mon Jan 25 18:50:15 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 25 Jan 2016 12:50:15 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
Message-ID: <56A66057.5010801@gmail.com>

On 25/01/2016 12:35 PM, John Sorkin wrote:
> When we read acerbic replies we should remind the poster to reply in a more moderate tone.
As long as you do this in private, not on the list, I wouldn't object.  
(I'd hope I wouldn't even know about it.)  Doing it on the list is more 
likely to lead to flame wars than to improved behaviour.

As others have suggested, if you think someone has been mistreated, then 
the public remedy should be to treat them well by giving a better answer 
yourself.

Duncan Murdoch

>   On the other hand  noting that the list is not intended to be a source of answers to home work questions is 100% appropriate. This philosophy is intended both to keep the list from being flooded with questions and to make sure that no student has an unfair advantage.
> John
>
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> > On Jan 25, 2016, at 12:17 PM, Ted Harding <Ted.Harding at wlandres.net> wrote:
> >
> > My feelings exactly! (And since quite some time ago).
> > Ted.
> >
> >> On 25-Jan-2016 12:23:16 Fowler, Mark wrote:
> >> I'm glad to see the issue of negative feedback addressed. I can especially
> >> relate to the 'cringe' feeling when reading some authoritarian backhand to a
> >> new user. We do see a number of obviously inappropriate or overly lazy
> >> postings, but I encounter far more postings where I don't feel competent to
> >> judge their merit. It might be better to simply disregard a posting one does
> >> not like for some reason. It might also be worthwhile to actively counter
> >> negative feedback when we experience that 'cringing' moment. I'm not thinking
> >> to foster contention, but simply to provide some tangible reassurance to new
> >> users, and not just the ones invoking the negative feedback, that a
> >> particular respondent may not represent the perspective of the list.
> >>
> >> -----Original Message-----
> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael
> >> Friendly
> >> Sent: January 24, 2016 5:43 PM
> >> To: Jean-Luc Dupouey; r-help at r-project.org
> >> Subject: Re: [R] R-help mailing list activity / R-not-help?
> >>
> >>
> >>> On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
> >>> Dear members,
> >>>
> >>> Not a technical question:
> >> But one worth raising...
> >>>
> >>> The number of threads in this mailing list, following a long period of
> >>> increase, has been regularly and strongly decreasing since 2010,
> >>> passing from more than 40K threads to less than 11K threads last year.
> >>> The trend is similar for most of the "ancient" mailing lists of the
> >>> R-project.
> >> [snip ...]
> >>>
> >>> I hope it is the wright place to ask this question. Thanks in advance,
> >>
> >> In addition to the other replies, there is another trend I've seen that has
> >> actively worked to suppress discussion on R-help and move it elsewhere. The
> >> general things:
> >> - R-help was too unwieldy and so it was a good idea to hive-off specialized
> >> topics to various sub lists, R-SIG-Mac, R-SIG-Geo, etc.
> >> - Many people posted badly-formed questions to R-help, and so it was a good
> >> idea to develop and refer to the posting guide to mitigate the number of
> >> purely junk postings.
> >>
> >> <rant>
> >> Yet, the trend I've seen is one of increasing **R-not-help**, in that there
> >> are many posts, often by new R users who get replies that not infrequently
> >> range from just mildly off-putting to actively hostile:
> >>
> >> - Is this homework? We don't do homework (sometimes false alarms, where the
> >> OP has to reply to say it is not)
> >> - Didn't you bother to do your homework, RTFM, or Google?
> >> - This is off-topic because XXX (e.g., it is not strictly an R programming
> >> question).
> >> - You asked about doing XXX, but this is a stupid thing to want to do.
> >> - Don't ask here; you need to talk to a statistical consultant.
> >>
> >> I find this sad in a public mailing list sent to all R-help subscribers and I
> >> sometimes cringe when I read replies to people who were actually trying to
> >> get help with some R-related problem, but expressed it badly, didn't know
> >> exactly what to ask for, or how to format it, or somehow motivated a
> >> frequent-replier to publicly dis the OP.
> >>
> >> On the other hand, I still see a spirit of great generosity among some people
> >> who frequently reply to R-help, taking a possibly badly posed or
> >> ill-formatted question, and going to some lengths to provide a a helpful
> >> answer of some sort.  I applaud those who take the time and effort to do
> >> this.
> >>
> >> I use R in a number of my courses, and used to advise students to post to
> >> R-help for general programming questions (not just homework) they couldn't
> >> solve. I don't do this any more, because several of them reported a negative
> >> experience.
> >>
> >> In contrast, in the Stackexchange model, there are numerous sublists
> >> cross-classified by their tags.  If I have a specific knitr, ggplot2, LaTeX,
> >> or statistical modeling question, I'm now more likely to post it there, and
> >> the worst that can happen is that no one "upvotes" it or someone (helpfully)
> >> marks it as a duplicate of a similar question.
> >> But comments there are not propagated to all subscribers, and those who reply
> >> helpfully, can see their solutions accepted or not, or commented on in that
> >> specific topic.
> >>
> >> Perhaps one solution would be to create a new "R-not-help" list where, as in
> >> a Monty Python skit, people could be directed there to be insulted and all
> >> these unhelpful replies could be sent.
> >>
> >> A milder alternative is to encourage some R-help subscribers to click the
> >> "Don't send" or "Save" button and think better of their replies.
> >> </rant>
> >>
> >> --
> >> Michael Friendly     Email: friendly AT yorku DOT ca
> >> Professor, Psychology Dept. & Chair, Quantitative Methods
> >> York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
> >> 4700 Keele Street    Web:   http://www.datavis.ca
> >> Toronto, ONT  M3J 1P3 CANADA
> >
> > -------------------------------------------------
> > E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
> > Date: 25-Jan-2016  Time: 17:14:06
> > This message was sent by XFMail
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:7}}


From Mark.Fowler at dfo-mpo.gc.ca  Mon Jan 25 19:14:27 2016
From: Mark.Fowler at dfo-mpo.gc.ca (Fowler, Mark)
Date: Mon, 25 Jan 2016 18:14:27 +0000
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
Message-ID: <88388BFE50A61F408122CBAEB917FD57367DC56D@SVNSBIOMBX01.ENT.dfo-mpo.ca>

Two concerns with implementing this philosophy.


1.       Determining whether a question is indeed seeking an answer to a homework exercise. Certainly if I think a question is short-cutting a basic homework task I ignore it. But I don't waste an email berating the alleged student.

2.       The validity of the barrier. At what point (maybe graduate levels? Nth year?) do we regard questions inspired by an educational system to be appropriate? Academia was still using mainframes when I graduated so I don't have much notion of expectations today.


I'm just musing that we might be farther ahead simply opting for no response than adding another email to the queue. It also gets around needing to feel I know the answers to 1 and 2.

From: John Sorkin [mailto:jsorkin at grecc.umaryland.edu]
Sent: January 25, 2016 1:36 PM
To: Ted.Harding at wlandres.net
Cc: Fowler, Mark; dupouey at nancy.inra.fr; r-help at r-project.org; friendly at yorku.ca
Subject: Re: [R] R-help mailing list activity / R-not-help?

When we read acerbic replies we should remind the poster to reply in a more moderate tone. On the other hand  noting that the list is not intended to be a source of answers to home work questions is 100% appropriate. This philosophy is intended both to keep the list from being flooded with questions and to make sure that no student has an unfair advantage.
John


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics

University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine

Baltimore VA Medical Center

10 North Greene Street<x-apple-data-detectors://12>

GRECC<x-apple-data-detectors://12> (BT/18/GR)

Baltimore, MD 21201-1524<x-apple-data-detectors://13/0>

(Phone) 410-605-711<tel:410-605-7119>9
(Fax) 410-605-7913<tel:410-605-7913> (Please call phone number above prior to faxing)

On Jan 25, 2016, at 12:17 PM, Ted Harding <Ted.Harding at wlandres.net<mailto:Ted.Harding at wlandres.net>> wrote:
My feelings exactly! (And since quite some time ago).
Ted.

On 25-Jan-2016 12:23:16 Fowler, Mark wrote:

I'm glad to see the issue of negative feedback addressed. I can especially
relate to the 'cringe' feeling when reading some authoritarian backhand to a
new user. We do see a number of obviously inappropriate or overly lazy
postings, but I encounter far more postings where I don't feel competent to
judge their merit. It might be better to simply disregard a posting one does
not like for some reason. It might also be worthwhile to actively counter
negative feedback when we experience that 'cringing' moment. I'm not thinking
to foster contention, but simply to provide some tangible reassurance to new
users, and not just the ones invoking the negative feedback, that a
particular respondent may not represent the perspective of the list.

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael
Friendly
Sent: January 24, 2016 5:43 PM
To: Jean-Luc Dupouey; r-help at r-project.org<mailto:r-help at r-project.org>
Subject: Re: [R] R-help mailing list activity / R-not-help?


On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
Dear members,

Not a technical question:
But one worth raising...

The number of threads in this mailing list, following a long period of
increase, has been regularly and strongly decreasing since 2010,
passing from more than 40K threads to less than 11K threads last year.
The trend is similar for most of the "ancient" mailing lists of the
R-project.
[snip ...]

I hope it is the wright place to ask this question. Thanks in advance,


In addition to the other replies, there is another trend I've seen that has
actively worked to suppress discussion on R-help and move it elsewhere. The
general things:
- R-help was too unwieldy and so it was a good idea to hive-off specialized
topics to various sub lists, R-SIG-Mac, R-SIG-Geo, etc.
- Many people posted badly-formed questions to R-help, and so it was a good
idea to develop and refer to the posting guide to mitigate the number of
purely junk postings.

<rant>
Yet, the trend I've seen is one of increasing **R-not-help**, in that there
are many posts, often by new R users who get replies that not infrequently
range from just mildly off-putting to actively hostile:

- Is this homework? We don't do homework (sometimes false alarms, where the
OP has to reply to say it is not)
- Didn't you bother to do your homework, RTFM, or Google?
- This is off-topic because XXX (e.g., it is not strictly an R programming
question).
- You asked about doing XXX, but this is a stupid thing to want to do.
- Don't ask here; you need to talk to a statistical consultant.

I find this sad in a public mailing list sent to all R-help subscribers and I
sometimes cringe when I read replies to people who were actually trying to
get help with some R-related problem, but expressed it badly, didn't know
exactly what to ask for, or how to format it, or somehow motivated a
frequent-replier to publicly dis the OP.

On the other hand, I still see a spirit of great generosity among some people
who frequently reply to R-help, taking a possibly badly posed or
ill-formatted question, and going to some lengths to provide a a helpful
answer of some sort.  I applaud those who take the time and effort to do
this.

I use R in a number of my courses, and used to advise students to post to
R-help for general programming questions (not just homework) they couldn't
solve. I don't do this any more, because several of them reported a negative
experience.

In contrast, in the Stackexchange model, there are numerous sublists
cross-classified by their tags.  If I have a specific knitr, ggplot2, LaTeX,
or statistical modeling question, I'm now more likely to post it there, and
the worst that can happen is that no one "upvotes" it or someone (helpfully)
marks it as a duplicate of a similar question.
But comments there are not propagated to all subscribers, and those who reply
helpfully, can see their solutions accepted or not, or commented on in that
specific topic.

Perhaps one solution would be to create a new "R-not-help" list where, as in
a Monty Python skit, people could be directed there to be insulted and all
these unhelpful replies could be sent.

A milder alternative is to encourage some R-help subscribers to click the
"Don't send" or "Save" button and think better of their replies.
</rant>

--
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept. & Chair, Quantitative Methods
York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
4700 Keele Street    Web:   http://www.datavis.ca
Toronto, ONT  M3J 1P3 CANADA

-------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at wlandres.net<mailto:Ted.Harding at wlandres.net>>
Date: 25-Jan-2016  Time: 17:14:06
This message was sent by XFMail

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
and provide commented, minimal, self-contained, reproducible code.


Confidentiality Statement:

This email message, including any attachments, is for th...{{dropped:9}}


From okeyes at wikimedia.org  Mon Jan 25 20:45:15 2016
From: okeyes at wikimedia.org (Oliver Keyes)
Date: Mon, 25 Jan 2016 14:45:15 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A66057.5010801@gmail.com>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
Message-ID: <CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>

I disagree, and would argue that fails to take a systemic view of this
kind of behaviour.

If individual commentators are acerbic and are only privately
reprimanded, from the perspective of everyone else it looks like the
acerbic reply was A-OK. Someone said something unnecessarily hostile
and the response was...nada. That creates an environment where there
are no clear examples of what crosses a line and no clear expectation
that moderation is even a thing that happens. Indeed, I was shocked to
discover this list _was_ moderated precisely because all I see is
people being mean and nothing much else happening.

I would much rather a system where there is some sort of public
notice. It doesn't have to be identifying. Just "after a couple of
replies that did not follow our guidelines I have put some members of
this list on moderation, meaning that they must have their posts
cleared before being sent out. A reminder that we have certain
standards here and etc etc etc"

On 25 January 2016 at 12:50, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> On 25/01/2016 12:35 PM, John Sorkin wrote:
>>
>> When we read acerbic replies we should remind the poster to reply in a
>> more moderate tone.
>
> As long as you do this in private, not on the list, I wouldn't object.  (I'd
> hope I wouldn't even know about it.)  Doing it on the list is more likely to
> lead to flame wars than to improved behaviour.
>
> As others have suggested, if you think someone has been mistreated, then the
> public remedy should be to treat them well by giving a better answer
> yourself.
>
> Duncan Murdoch
>
>>   On the other hand  noting that the list is not intended to be a source
>> of answers to home work questions is 100% appropriate. This philosophy is
>> intended both to keep the list from being flooded with questions and to make
>> sure that no student has an unfair advantage.
>> John
>>
>> > John David Sorkin M.D., Ph.D.
>> > Professor of Medicine
>> > Chief, Biostatistics and Informatics
>> > University of Maryland School of Medicine Division of Gerontology and
>> > Geriatric Medicine
>> > Baltimore VA Medical Center
>> > 10 North Greene Street
>> > GRECC (BT/18/GR)
>> > Baltimore, MD 21201-1524
>> > (Phone) 410-605-7119
>> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>
>>
>> > On Jan 25, 2016, at 12:17 PM, Ted Harding <Ted.Harding at wlandres.net>
>> > wrote:
>> >
>> > My feelings exactly! (And since quite some time ago).
>> > Ted.
>> >
>> >> On 25-Jan-2016 12:23:16 Fowler, Mark wrote:
>> >> I'm glad to see the issue of negative feedback addressed. I can
>> >> especially
>> >> relate to the 'cringe' feeling when reading some authoritarian backhand
>> >> to a
>> >> new user. We do see a number of obviously inappropriate or overly lazy
>> >> postings, but I encounter far more postings where I don't feel
>> >> competent to
>> >> judge their merit. It might be better to simply disregard a posting one
>> >> does
>> >> not like for some reason. It might also be worthwhile to actively
>> >> counter
>> >> negative feedback when we experience that 'cringing' moment. I'm not
>> >> thinking
>> >> to foster contention, but simply to provide some tangible reassurance
>> >> to new
>> >> users, and not just the ones invoking the negative feedback, that a
>> >> particular respondent may not represent the perspective of the list.
>> >>
>> >> -----Original Message-----
>> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael
>> >> Friendly
>> >> Sent: January 24, 2016 5:43 PM
>> >> To: Jean-Luc Dupouey; r-help at r-project.org
>> >> Subject: Re: [R] R-help mailing list activity / R-not-help?
>> >>
>> >>
>> >>> On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
>> >>> Dear members,
>> >>>
>> >>> Not a technical question:
>> >> But one worth raising...
>> >>>
>> >>> The number of threads in this mailing list, following a long period of
>> >>> increase, has been regularly and strongly decreasing since 2010,
>> >>> passing from more than 40K threads to less than 11K threads last year.
>> >>> The trend is similar for most of the "ancient" mailing lists of the
>> >>> R-project.
>> >> [snip ...]
>> >>>
>> >>> I hope it is the wright place to ask this question. Thanks in advance,
>> >>
>> >> In addition to the other replies, there is another trend I've seen that
>> >> has
>> >> actively worked to suppress discussion on R-help and move it elsewhere.
>> >> The
>> >> general things:
>> >> - R-help was too unwieldy and so it was a good idea to hive-off
>> >> specialized
>> >> topics to various sub lists, R-SIG-Mac, R-SIG-Geo, etc.
>> >> - Many people posted badly-formed questions to R-help, and so it was a
>> >> good
>> >> idea to develop and refer to the posting guide to mitigate the number
>> >> of
>> >> purely junk postings.
>> >>
>> >> <rant>
>> >> Yet, the trend I've seen is one of increasing **R-not-help**, in that
>> >> there
>> >> are many posts, often by new R users who get replies that not
>> >> infrequently
>> >> range from just mildly off-putting to actively hostile:
>> >>
>> >> - Is this homework? We don't do homework (sometimes false alarms, where
>> >> the
>> >> OP has to reply to say it is not)
>> >> - Didn't you bother to do your homework, RTFM, or Google?
>> >> - This is off-topic because XXX (e.g., it is not strictly an R
>> >> programming
>> >> question).
>> >> - You asked about doing XXX, but this is a stupid thing to want to do.
>> >> - Don't ask here; you need to talk to a statistical consultant.
>> >>
>> >> I find this sad in a public mailing list sent to all R-help subscribers
>> >> and I
>> >> sometimes cringe when I read replies to people who were actually trying
>> >> to
>> >> get help with some R-related problem, but expressed it badly, didn't
>> >> know
>> >> exactly what to ask for, or how to format it, or somehow motivated a
>> >> frequent-replier to publicly dis the OP.
>> >>
>> >> On the other hand, I still see a spirit of great generosity among some
>> >> people
>> >> who frequently reply to R-help, taking a possibly badly posed or
>> >> ill-formatted question, and going to some lengths to provide a a
>> >> helpful
>> >> answer of some sort.  I applaud those who take the time and effort to
>> >> do
>> >> this.
>> >>
>> >> I use R in a number of my courses, and used to advise students to post
>> >> to
>> >> R-help for general programming questions (not just homework) they
>> >> couldn't
>> >> solve. I don't do this any more, because several of them reported a
>> >> negative
>> >> experience.
>> >>
>> >> In contrast, in the Stackexchange model, there are numerous sublists
>> >> cross-classified by their tags.  If I have a specific knitr, ggplot2,
>> >> LaTeX,
>> >> or statistical modeling question, I'm now more likely to post it there,
>> >> and
>> >> the worst that can happen is that no one "upvotes" it or someone
>> >> (helpfully)
>> >> marks it as a duplicate of a similar question.
>> >> But comments there are not propagated to all subscribers, and those who
>> >> reply
>> >> helpfully, can see their solutions accepted or not, or commented on in
>> >> that
>> >> specific topic.
>> >>
>> >> Perhaps one solution would be to create a new "R-not-help" list where,
>> >> as in
>> >> a Monty Python skit, people could be directed there to be insulted and
>> >> all
>> >> these unhelpful replies could be sent.
>> >>
>> >> A milder alternative is to encourage some R-help subscribers to click
>> >> the
>> >> "Don't send" or "Save" button and think better of their replies.
>> >> </rant>
>> >>
>> >> --
>> >> Michael Friendly     Email: friendly AT yorku DOT ca
>> >> Professor, Psychology Dept. & Chair, Quantitative Methods
>> >> York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
>> >> 4700 Keele Street    Web:   http://www.datavis.ca
>> >> Toronto, ONT  M3J 1P3 CANADA
>> >
>> > -------------------------------------------------
>> > E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
>> > Date: 25-Jan-2016  Time: 17:14:06
>> > This message was sent by XFMail
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> Confidentiality Statement:
>> This email message, including any attachments, is for ...{{dropped:7}}
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Oliver Keyes
Count Logula
Wikimedia Foundation


From murdoch.duncan at gmail.com  Mon Jan 25 21:07:05 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 25 Jan 2016 15:07:05 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
	<CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
Message-ID: <56A68069.10406@gmail.com>

On 25/01/2016 2:45 PM, Oliver Keyes wrote:
> I disagree, and would argue that fails to take a systemic view of this
> kind of behaviour.
>
> If individual commentators are acerbic and are only privately
> reprimanded, from the perspective of everyone else it looks like the
> acerbic reply was A-OK. Someone said something unnecessarily hostile
> and the response was...nada. That creates an environment where there
> are no clear examples of what crosses a line and no clear expectation
> that moderation is even a thing that happens. Indeed, I was shocked to
> discover this list _was_ moderated precisely because all I see is
> people being mean and nothing much else happening.

Why would you bother to read it if that's all you see?  I think there 
are examples of posts here which are not at all helpful, and others 
which are rude, but the majority are actually helpful (even some of the 
rude ones).

Duncan Murdoch


From mkashif at uaf.edu.pk  Mon Jan 25 21:12:14 2016
From: mkashif at uaf.edu.pk (Muhammad  Kashif)
Date: Mon, 25 Jan 2016 20:12:14 +0000
Subject: [R] Division of data set with some restriction
Message-ID: <DB4PR07MB379E4A00FBCF1C3E8D1757F94C70@DB4PR07MB379.eurprd07.prod.outlook.com>


Dear Group members

Can any one help to code this situation. Suppose we have a population with some mean and a standard deviation. Then , there are n1 observations out of n  which are less than or equal to n. Also, there are n2 observations out of n which are greater than . We divide the whole data set into two parts such that we have the same mean  but different standard deviations.

for example we have 50 observations from any distribution say two parameter Weibull. Then we divide the data into two parts such that the two resulting data sets have same mean and different standard deviation.



	[[alternative HTML version deleted]]


From hasan.diwan at gmail.com  Mon Jan 25 21:33:12 2016
From: hasan.diwan at gmail.com (Hasan Diwan)
Date: Mon, 25 Jan 2016 12:33:12 -0800
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A68069.10406@gmail.com>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
	<CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
	<56A68069.10406@gmail.com>
Message-ID: <CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>

There exists a fine line between being unintentionally rude, but helpful
and purposely putting someone down. -- H

On 25 January 2016 at 12:07, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 25/01/2016 2:45 PM, Oliver Keyes wrote:
>
>> I disagree, and would argue that fails to take a systemic view of this
>> kind of behaviour.
>>
>> If individual commentators are acerbic and are only privately
>> reprimanded, from the perspective of everyone else it looks like the
>> acerbic reply was A-OK. Someone said something unnecessarily hostile
>> and the response was...nada. That creates an environment where there
>> are no clear examples of what crosses a line and no clear expectation
>> that moderation is even a thing that happens. Indeed, I was shocked to
>> discover this list _was_ moderated precisely because all I see is
>> people being mean and nothing much else happening.
>>
>
> Why would you bother to read it if that's all you see?  I think there are
> examples of posts here which are not at all helpful, and others which are
> rude, but the majority are actually helpful (even some of the rude ones).
>
> Duncan Murdoch
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
OpenPGP: https://hasan.d8u.us/gpg.key
Sent from my mobile device
Envoy? de mon portable

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Jan 25 21:59:11 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 25 Jan 2016 12:59:11 -0800
Subject: [R] Division of data set with some restriction
In-Reply-To: <DB4PR07MB379E4A00FBCF1C3E8D1757F94C70@DB4PR07MB379.eurprd07.prod.outlook.com>
References: <DB4PR07MB379E4A00FBCF1C3E8D1757F94C70@DB4PR07MB379.eurprd07.prod.outlook.com>
Message-ID: <CAGxFJbT=HTJLDr0W2_BLXodXibK=tATa=GU7OB07G6gumTZ7QA@mail.gmail.com>

Muhammed:

1. Please post in plain text, not HTML, as HTML tends to get mangled
(this is the stated policy on this list). This may have happened to
your post, so I am not entirely sure what you want.

2. However, an assumption that you appear to be making is false: it is
not in general possible to split a data set with a given mean into two
subsets each of which has the same mean, i.e. average. A simple
counterexample is the data set 1, 9,  50  which has a mean of 20 but
cannot be split into 2 subsets with that mean.

3. If this is not what you meant, than you may need to clarify, as it
is certainly unclear to me.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Jan 25, 2016 at 12:12 PM, Muhammad  Kashif <mkashif at uaf.edu.pk> wrote:
>
> Dear Group members
>
> Can any one help to code this situation. Suppose we have a population with some mean  and a standard deviation. Then  , there are n1 observations out of n  which are less than or equal to n . Also, there are n2 observations out of n which are greater than  . We divide the whole data set into two parts such that we have the same mean   but different standard deviations.
>
> for example we have 50 observations from any distribution say two parameter Weibull. Then we divide the data into two parts such that the two resulting data sets have same mean and different standard deviation.
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Mon Jan 25 22:13:50 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 25 Jan 2016 16:13:50 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
	<CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
	<56A68069.10406@gmail.com>
	<CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>
Message-ID: <56A6900E.6030608@gmail.com>

On 25/01/2016 3:33 PM, Hasan Diwan wrote:
> There exists a fine line between being unintentionally rude, but helpful
> and purposely putting someone down. -- H

I'm afraid I don't think your point is relevant.  I didn't claim all the 
people who were rude did it unintentionally.  However,  I don't know 
anyone on the list who is always rude and never helpful. Oliver claimed 
almost everyone is like that.

I actually agree with a weaker version of John's proposal (which I cut 
out of my reply to Oliver).  I can imagine a public reprimand from one 
of the moderators would be appropriate.  It would never be appropriate 
from general list members; that's what leads to flame wars.

I'm not a moderator, so I would not publicly "remind the poster to reply 
in a more moderate tone", and neither should you (unless you're a 
moderator).  It would be much better if one or both of us posted a more 
helpful response when we saw a rude, unhelpful one.

Duncan Murdoch


>
> On 25 January 2016 at 12:07, Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
> > On 25/01/2016 2:45 PM, Oliver Keyes wrote:
> >
> >> I disagree, and would argue that fails to take a systemic view of this
> >> kind of behaviour.
> >>
> >> If individual commentators are acerbic and are only privately
> >> reprimanded, from the perspective of everyone else it looks like the
> >> acerbic reply was A-OK. Someone said something unnecessarily hostile
> >> and the response was...nada. That creates an environment where there
> >> are no clear examples of what crosses a line and no clear expectation
> >> that moderation is even a thing that happens. Indeed, I was shocked to
> >> discover this list _was_ moderated precisely because all I see is
> >> people being mean and nothing much else happening.
> >>
> >
> > Why would you bother to read it if that's all you see?  I think there are
> > examples of posts here which are not at all helpful, and others which are
> > rude, but the majority are actually helpful (even some of the rude ones).
> >
> > Duncan Murdoch
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>


From 538280 at gmail.com  Mon Jan 25 22:20:11 2016
From: 538280 at gmail.com (Greg Snow)
Date: Mon, 25 Jan 2016 14:20:11 -0700
Subject: [R] Logistic Regression
In-Reply-To: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>
References: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>
Message-ID: <CAFEqCdye_CrFacy0cu7ZpOE1mMu=dq88O0zpCboXoZ9pCbXTRg@mail.gmail.com>

Do you have the sample sizes that the sample proportions were computed
from (e.g. 0.5 could be 1 out of 2 or 100 out of 200)?

If you do then you can specify the model with the proportions as the y
variable and the corresponding sample sizes as the weights argument to
glm.

If you only have proportions without an integer sample size then you
may want to switch to using beta regression instead of logistic
regression.

On Sat, Jan 23, 2016 at 1:41 PM, pari hesabi <statistics84 at hotmail.com> wrote:
> Hello everybody,
>
> I am trying to fit a logistic regression model by using glm() function in R. My response variable is a sample proportion NOT binary numbers(0,1).
>
> Regarding glm() function, I receive this error:  non integer # successes in a binomial glm!
>
> I would appreciate if anybody conducts me.
>
>
> Regards,
>
> Pari
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From hasan.diwan at gmail.com  Mon Jan 25 22:20:37 2016
From: hasan.diwan at gmail.com (Hasan Diwan)
Date: Mon, 25 Jan 2016 13:20:37 -0800
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A6900E.6030608@gmail.com>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
	<CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
	<56A68069.10406@gmail.com>
	<CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>
	<56A6900E.6030608@gmail.com>
Message-ID: <CAP+bYWDbcoCU10Hr5kxzXgmGXQYK3MdufUC-5gdGNZPGuCDLnw@mail.gmail.com>

On 25 January 2016 at 13:13, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 25/01/2016 3:33 PM, Hasan Diwan wrote:
>
>> There exists a fine line between being unintentionally rude, but helpful
>> and purposely putting someone down. -- H
>>
>
> I'm afraid I don't think your point is relevant.  I didn't claim all the
> people who were rude did it unintentionally.  However,  I don't know anyone
> on the list who is always rude and never helpful. Oliver claimed almost
> everyone is like that.


> I actually agree with a weaker version of John's proposal (which I cut out
> of my reply to Oliver).  I can imagine a public reprimand from one of the
> moderators would be appropriate.  It would never be appropriate from
> general list members; that's what leads to flame wars.
>
> I'm not a moderator, so I would not publicly "remind the poster to reply
> in a more moderate tone", and neither should you (unless you're a
> moderator).  It would be much better if one or both of us posted a more
> helpful response when we saw a rude, unhelpful one.


I'm not one to attack others in general, and have developed thick skin, so
a lot of what others find rude, I will ignore and get on with things. That
said, if someone does tell me that e.g. "Hasan is being offensive because
of $x, $y or $z", I'll apologise and get on with my life. Most of the time,
when people find me offensive, it's because I treat others how I wish to be
treated and the rhetoric just doesn't offend me. -- H

>
>
> Duncan Murdoch
>
>
>
>
>> On 25 January 2016 at 12:07, Duncan Murdoch <murdoch.duncan at gmail.com>
>> wrote:
>>
>> > On 25/01/2016 2:45 PM, Oliver Keyes wrote:
>> >
>> >> I disagree, and would argue that fails to take a systemic view of this
>> >> kind of behaviour.
>> >>
>> >> If individual commentators are acerbic and are only privately
>> >> reprimanded, from the perspective of everyone else it looks like the
>> >> acerbic reply was A-OK. Someone said something unnecessarily hostile
>> >> and the response was...nada. That creates an environment where there
>> >> are no clear examples of what crosses a line and no clear expectation
>> >> that moderation is even a thing that happens. Indeed, I was shocked to
>> >> discover this list _was_ moderated precisely because all I see is
>> >> people being mean and nothing much else happening.
>> >>
>> >
>> > Why would you bother to read it if that's all you see?  I think there
>> are
>> > examples of posts here which are not at all helpful, and others which
>> are
>> > rude, but the majority are actually helpful (even some of the rude
>> ones).
>> >
>> > Duncan Murdoch
>> >
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>
>>
>>
>


-- 
OpenPGP: https://hasan.d8u.us/gpg.key
Sent from my mobile device
Envoy? de mon portable

	[[alternative HTML version deleted]]


From okeyes at wikimedia.org  Mon Jan 25 22:22:46 2016
From: okeyes at wikimedia.org (Oliver Keyes)
Date: Mon, 25 Jan 2016 16:22:46 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A6900E.6030608@gmail.com>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
	<CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
	<56A68069.10406@gmail.com>
	<CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>
	<56A6900E.6030608@gmail.com>
Message-ID: <CAAUQgdAoqpXtkn-W+rN_=sajp=uEPsrOEf5rMv4JV3w7geMhzA@mail.gmail.com>

Sorry, poor phrasing on my part; on the occasions where someone is
rude, all I see is...

I agree the public cautioning should be done by moderators, yes.

On 25 January 2016 at 16:13, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> On 25/01/2016 3:33 PM, Hasan Diwan wrote:
>>
>> There exists a fine line between being unintentionally rude, but helpful
>> and purposely putting someone down. -- H
>
>
> I'm afraid I don't think your point is relevant.  I didn't claim all the
> people who were rude did it unintentionally.  However,  I don't know anyone
> on the list who is always rude and never helpful. Oliver claimed almost
> everyone is like that.
>
> I actually agree with a weaker version of John's proposal (which I cut out
> of my reply to Oliver).  I can imagine a public reprimand from one of the
> moderators would be appropriate.  It would never be appropriate from general
> list members; that's what leads to flame wars.
>
> I'm not a moderator, so I would not publicly "remind the poster to reply in
> a more moderate tone", and neither should you (unless you're a moderator).
> It would be much better if one or both of us posted a more helpful response
> when we saw a rude, unhelpful one.
>
> Duncan Murdoch
>
>
>>
>> On 25 January 2016 at 12:07, Duncan Murdoch <murdoch.duncan at gmail.com>
>> wrote:
>>
>> > On 25/01/2016 2:45 PM, Oliver Keyes wrote:
>> >
>> >> I disagree, and would argue that fails to take a systemic view of this
>> >> kind of behaviour.
>> >>
>> >> If individual commentators are acerbic and are only privately
>> >> reprimanded, from the perspective of everyone else it looks like the
>> >> acerbic reply was A-OK. Someone said something unnecessarily hostile
>> >> and the response was...nada. That creates an environment where there
>> >> are no clear examples of what crosses a line and no clear expectation
>> >> that moderation is even a thing that happens. Indeed, I was shocked to
>> >> discover this list _was_ moderated precisely because all I see is
>> >> people being mean and nothing much else happening.
>> >>
>> >
>> > Why would you bother to read it if that's all you see?  I think there
>> > are
>> > examples of posts here which are not at all helpful, and others which
>> > are
>> > rude, but the majority are actually helpful (even some of the rude
>> > ones).
>> >
>> > Duncan Murdoch
>> >
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Oliver Keyes
Count Logula
Wikimedia Foundation


From jwd at surewest.net  Mon Jan 25 22:56:09 2016
From: jwd at surewest.net (jwd)
Date: Mon, 25 Jan 2016 13:56:09 -0800
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
	<CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
	<56A68069.10406@gmail.com>
	<CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>
Message-ID: <20160125135609.6770c17f@Draco.site>

On Mon, 25 Jan 2016 12:33:12 -0800
Hasan Diwan <hasan.diwan at gmail.com> wrote:

> There exists a fine line between being unintentionally rude, but
> helpful and purposely putting someone down. -- H

The line is really not "fine" at all since it lies in that word
"purposely."  Also, you've associated "helpful" with unintentional
rudeness.  The distinction you've drawn is clear.

JWDougherty


From jwd at surewest.net  Mon Jan 25 23:13:43 2016
From: jwd at surewest.net (jwd)
Date: Mon, 25 Jan 2016 14:13:43 -0800
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <CAAUQgdDUp-cT7wjE_o5jt3hq_JD6=Btbu05Aobq5Tr9WEU051Q@mail.gmail.com>
References: <56A371F5.5070509@nancy.inra.fr> <56A54553.60603@yorku.ca>
	<88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<CAAUQgdDUp-cT7wjE_o5jt3hq_JD6=Btbu05Aobq5Tr9WEU051Q@mail.gmail.com>
Message-ID: <20160125141343.1e172c00@Draco.site>

On Mon, 25 Jan 2016 11:06:35 -0500
Oliver Keyes <okeyes at wikimedia.org> wrote:

> +1. And frankly I would like to suggest that there is another obvious
> solution here; pairing a set of guidelines around expected user
> behaviour with removing people from the mailing list, or moderating
> them, if they do not think that creating a non-toxic environment is
> good.
> 

The problem is defining a "toxic environment."  One person can find all
kinds of offense and rudness in something where another would
be appreciating a short, concise response, such as a suggestion to "read
the manual." "Toxic" is personal and one can find it wherever one
looks, if so minded. I suspect that if one perceives rudeness, one
ought to check and see that we haven't left our sensibilities out in
the traffic pattern where they are bound to be trampled.


From Peter.Alspach at plantandfood.co.nz  Mon Jan 25 23:32:18 2016
From: Peter.Alspach at plantandfood.co.nz (Peter Alspach)
Date: Tue, 26 Jan 2016 11:32:18 +1300
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <CAAUQgdAoqpXtkn-W+rN_=sajp=uEPsrOEf5rMv4JV3w7geMhzA@mail.gmail.com>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
	<CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
	<56A68069.10406@gmail.com>
	<CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>
	<56A6900E.6030608@gmail.com>
	<CAAUQgdAoqpXtkn-W+rN_=sajp=uEPsrOEf5rMv4JV3w7geMhzA@mail.gmail.com>
Message-ID: <E41B375B7520DE4A8C60781AC60B75452C35EEDABE@AKLEXM01.PFR.CO.NZ>

I think this would be re-defining the role of the moderators for list, which is essentially to filter out spam.  Only new members are subject to this moderation, and if a message is genuine then their moderator flag is cleared (i.e., they are no longer subject to moderation).  Thus the list isn't moderated in the 'usual' sense.

That said, I have occasionally asked a new poster to reword their question (or simply add a subject line) and explained that this helps ensure they get a good answer, and not a rude one.  Mostly people seem to appreciate that.

Peter Alspach
(one of the 'moderators')

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Oliver Keyes
Sent: Tuesday, 26 January 2016 10:23 a.m.
To: Duncan Murdoch <murdoch.duncan at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] R-help mailing list activity / R-not-help?

Sorry, poor phrasing on my part; on the occasions where someone is rude, all I see is...

I agree the public cautioning should be done by moderators, yes.

On 25 January 2016 at 16:13, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> On 25/01/2016 3:33 PM, Hasan Diwan wrote:
>>
>> There exists a fine line between being unintentionally rude, but 
>> helpful and purposely putting someone down. -- H
>
>
> I'm afraid I don't think your point is relevant.  I didn't claim all 
> the people who were rude did it unintentionally.  However,  I don't 
> know anyone on the list who is always rude and never helpful. Oliver 
> claimed almost everyone is like that.
>
> I actually agree with a weaker version of John's proposal (which I cut 
> out of my reply to Oliver).  I can imagine a public reprimand from one 
> of the moderators would be appropriate.  It would never be appropriate 
> from general list members; that's what leads to flame wars.
>
> I'm not a moderator, so I would not publicly "remind the poster to 
> reply in a more moderate tone", and neither should you (unless you're a moderator).
> It would be much better if one or both of us posted a more helpful 
> response when we saw a rude, unhelpful one.
>
> Duncan Murdoch
>
>
>>
>> On 25 January 2016 at 12:07, Duncan Murdoch 
>> <murdoch.duncan at gmail.com>
>> wrote:
>>
>> > On 25/01/2016 2:45 PM, Oliver Keyes wrote:
>> >
>> >> I disagree, and would argue that fails to take a systemic view of 
>> >> this kind of behaviour.
>> >>
>> >> If individual commentators are acerbic and are only privately 
>> >> reprimanded, from the perspective of everyone else it looks like 
>> >> the acerbic reply was A-OK. Someone said something unnecessarily 
>> >> hostile and the response was...nada. That creates an environment 
>> >> where there are no clear examples of what crosses a line and no 
>> >> clear expectation that moderation is even a thing that happens. 
>> >> Indeed, I was shocked to discover this list _was_ moderated 
>> >> precisely because all I see is people being mean and nothing much else happening.
>> >>
>> >
>> > Why would you bother to read it if that's all you see?  I think 
>> > there are examples of posts here which are not at all helpful, and 
>> > others which are rude, but the majority are actually helpful (even 
>> > some of the rude ones).
>> >
>> > Duncan Murdoch
>> >
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Oliver Keyes
Count Logula
Wikimedia Foundation

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
The contents of this e-mail are confidential and may be ...{{dropped:14}}


From jsorkin at grecc.umaryland.edu  Mon Jan 25 23:48:00 2016
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 25 Jan 2016 17:48:00 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <E41B375B7520DE4A8C60781AC60B75452C35EEDABE@AKLEXM01.PFR.CO.NZ>
References: <88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<XFMail.20160125171410.Ted.Harding@wlandres.net>
	<56A616AC020000CB00147A2F@smtp.medicine.umaryland.edu>
	<56A66057.5010801@gmail.com>
	<CAAUQgdCvNJT0tNEdyW8u63Shg8BY=STFuTreAp3PgHNbBXdHYw@mail.gmail.com>
	<56A68069.10406@gmail.com>
	<CAP+bYWDDD8FOoCMyCafN7coi2QFCjtbPd8PSjPodq-g8Rnh_jg@mail.gmail.com>
	<56A6900E.6030608@gmail.com>
	<CAAUQgdAoqpXtkn-W+rN_=sajp=uEPsrOEf5rMv4JV3w7geMhzA@mail.gmail.com>
	<E41B375B7520DE4A8C60781AC60B75452C35EEDABE@AKLEXM01.PFR.CO.NZ>
Message-ID: <56A65FD0020000CB00147AD1@smtp.medicine.umaryland.edu>

I submit it is up to list members to maintain civility. If we politely point out. off-line, to people who post questionable posts what they are doing, I am sure their behavior will quickly change.John 


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

>>> Peter Alspach <Peter.Alspach at plantandfood.co.nz> 01/25/16 5:33 PM >>>
I think this would be re-defining the role of the moderators for list, which is essentially to filter out spam.  Only new members are subject to this moderation, and if a message is genuine then their moderator flag is cleared (i.e., they are no longer subject to moderation).  Thus the list isn't moderated in the 'usual' sense.

That said, I have occasionally asked a new poster to reword their question (or simply add a subject line) and explained that this helps ensure they get a good answer, and not a rude one.  Mostly people seem to appreciate that.

Peter Alspach
(one of the 'moderators')

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Oliver Keyes
Sent: Tuesday, 26 January 2016 10:23 a.m.
To: Duncan Murdoch <murdoch.duncan at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] R-help mailing list activity / R-not-help?

Sorry, poor phrasing on my part; on the occasions where someone is rude, all I see is...

I agree the public cautioning should be done by moderators, yes.

On 25 January 2016 at 16:13, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> On 25/01/2016 3:33 PM, Hasan Diwan wrote:
>>
>> There exists a fine line between being unintentionally rude, but 
>> helpful and purposely putting someone down. -- H
>
>
> I'm afraid I don't think your point is relevant.  I didn't claim all 
> the people who were rude did it unintentionally.  However,  I don't 
> know anyone on the list who is always rude and never helpful. Oliver 
> claimed almost everyone is like that.
>
> I actually agree with a weaker version of John's proposal (which I cut 
> out of my reply to Oliver).  I can imagine a public reprimand from one 
> of the moderators would be appropriate.  It would never be appropriate 
> from general list members; that's what leads to flame wars.
>
> I'm not a moderator, so I would not publicly "remind the poster to 
> reply in a more moderate tone", and neither should you (unless you're a moderator).
> It would be much better if one or both of us posted a more helpful 
> response when we saw a rude, unhelpful one.
>
> Duncan Murdoch
>
>
>>
>> On 25 January 2016 at 12:07, Duncan Murdoch 
>> <murdoch.duncan at gmail.com>
>> wrote:
>>
>> > On 25/01/2016 2:45 PM, Oliver Keyes wrote:
>> >
>> >> I disagree, and would argue that fails to take a systemic view of 
>> >> this kind of behaviour.
>> >>
>> >> If individual commentators are acerbic and are only privately 
>> >> reprimanded, from the perspective of everyone else it looks like 
>> >> the acerbic reply was A-OK. Someone said something unnecessarily 
>> >> hostile and the response was...nada. That creates an environment 
>> >> where there are no clear examples of what crosses a line and no 
>> >> clear expectation that moderation is even a thing that happens. 
>> >> Indeed, I was shocked to discover this list _was_ moderated 
>> >> precisely because all I see is people being mean and nothing much else happening.
>> >>
>> >
>> > Why would you bother to read it if that's all you see?  I think 
>> > there are examples of posts here which are not at all helpful, and 
>> > others which are rude, but the majority are actually helpful (even 
>> > some of the rude ones).
>> >
>> > Duncan Murdoch
>> >
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Oliver Keyes
Count Logula
Wikimedia Foundation

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
The contents of this e-mail are confidential and may be ...{{dropped:14}}

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From liuwensui at gmail.com  Tue Jan 26 00:38:13 2016
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 25 Jan 2016 17:38:13 -0600
Subject: [R] Logistic Regression
In-Reply-To: <CAFEqCdye_CrFacy0cu7ZpOE1mMu=dq88O0zpCboXoZ9pCbXTRg@mail.gmail.com>
References: <AM3PR04MB41929DA5CC00ADB7069D610C6C50@AM3PR04MB419.eurprd04.prod.outlook.com>
	<CAFEqCdye_CrFacy0cu7ZpOE1mMu=dq88O0zpCboXoZ9pCbXTRg@mail.gmail.com>
Message-ID: <CAKyN3iCsJvdyrfAc3ETZBDPfVLvVLxWZ4=84gWYRofA8fXMm2Q@mail.gmail.com>

But beta can only be used to model the open interval between zero and one

On Monday, January 25, 2016, Greg Snow <538280 at gmail.com> wrote:

> Do you have the sample sizes that the sample proportions were computed
> from (e.g. 0.5 could be 1 out of 2 or 100 out of 200)?
>
> If you do then you can specify the model with the proportions as the y
> variable and the corresponding sample sizes as the weights argument to
> glm.
>
> If you only have proportions without an integer sample size then you
> may want to switch to using beta regression instead of logistic
> regression.
>
> On Sat, Jan 23, 2016 at 1:41 PM, pari hesabi <statistics84 at hotmail.com
> <javascript:;>> wrote:
> > Hello everybody,
> >
> > I am trying to fit a logistic regression model by using glm() function
> in R. My response variable is a sample proportion NOT binary numbers(0,1).
> >
> > Regarding glm() function, I receive this error:  non integer # successes
> in a binomial glm!
> >
> > I would appreciate if anybody conducts me.
> >
> >
> > Regards,
> >
> > Pari
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com <javascript:;>
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
https://statcompute.wordpress.com/

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Tue Jan 26 00:54:19 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Mon, 25 Jan 2016 15:54:19 -0800
Subject: [R] tcltk table: get celltag value
Message-ID: <CAJeYpE9HDDaJYmfR64cMCmp_szFZbbv14CN1N8cv4ki_0VYwbQ@mail.gmail.com>

I'm finding it very difficult to figure out how to read the value of
"celltag" for a given cell in a tktable.

I'm sure it's something like:

tcl(classTable, "get", "celltag", row, column)

but of the dozens of variations of names, options, args, and formats I've
tried, nothing is working. Any suggestions?

Much thanks.

-Dan

-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From kevin.thorpe at utoronto.ca  Tue Jan 26 01:08:25 2016
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Mon, 25 Jan 2016 19:08:25 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <CAAUQgdDUp-cT7wjE_o5jt3hq_JD6=Btbu05Aobq5Tr9WEU051Q@mail.gmail.com>
References: <56A371F5.5070509@nancy.inra.fr> <56A54553.60603@yorku.ca>
	<88388BFE50A61F408122CBAEB917FD57367DC488@SVNSBIOMBX01.ENT.dfo-mpo.ca>
	<CAAUQgdDUp-cT7wjE_o5jt3hq_JD6=Btbu05Aobq5Tr9WEU051Q@mail.gmail.com>
Message-ID: <56A6B8F9.4000907@utoronto.ca>

On 01/25/2016 11:06 AM, Oliver Keyes wrote:
> +1. And frankly I would like to suggest that there is another obvious
> solution here; pairing a set of guidelines around expected user
> behaviour with removing people from the mailing list, or moderating
> them, if they do not think that creating a non-toxic environment is
> good.
>

These guidelines DO exist. It is called the posting guide. 
Unfortunately, it is clear that some people cannot be bothered to read 
that. Is that an excuse to be mistreated? By no means.

If you, or anyone else has a good way to encourage new users to read and 
use the guidelines, I think we would love to hear it.

> On 25 January 2016 at 07:23, Fowler, Mark <Mark.Fowler at dfo-mpo.gc.ca> wrote:
>> I'm glad to see the issue of negative feedback addressed. I can especially relate to the 'cringe' feeling when reading some authoritarian backhand to a new user. We do see a number of obviously inappropriate or overly lazy postings, but I encounter far more postings where I don't feel competent to judge their merit. It might be better to simply disregard a posting one does not like for some reason. It might also be worthwhile to actively counter negative feedback when we experience that 'cringing' moment. I'm not thinking to foster contention, but simply to provide some tangible reassurance to new users, and not just the ones invoking the negative feedback, that a particular respondent may not represent the perspective of the list.
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael Friendly
>> Sent: January 24, 2016 5:43 PM
>> To: Jean-Luc Dupouey; r-help at r-project.org
>> Subject: Re: [R] R-help mailing list activity / R-not-help?
>>
>>
>> On 1/23/2016 7:28 AM, Jean-Luc Dupouey wrote:
>>> Dear members,
>>>
>>> Not a technical question:
>> But one worth raising...
>>>
>>> The number of threads in this mailing list, following a long period of
>>> increase, has been regularly and strongly decreasing since 2010,
>>> passing from more than 40K threads to less than 11K threads last year.
>>> The trend is similar for most of the "ancient" mailing lists of the R-project.
>> [snip ...]
>>>
>>> I hope it is the wright place to ask this question. Thanks in advance,
>>>
>>
>> In addition to the other replies, there is another trend I've seen that has actively worked to suppress discussion on R-help and move it elsewhere. The general things:
>> - R-help was too unwieldy and so it was a good idea to hive-off specialized topics to various sub lists, R-SIG-Mac, R-SIG-Geo, etc.
>> - Many people posted badly-formed questions to R-help, and so it was a good idea to develop and refer to the posting guide to mitigate the number of purely junk postings.
>>
>> <rant>
>> Yet, the trend I've seen is one of increasing **R-not-help**, in that there are many posts, often by new R users who get replies that not infrequently range from just mildly off-putting to actively hostile:
>>
>> - Is this homework? We don't do homework (sometimes false alarms, where the OP has to reply to say it is not)
>> - Didn't you bother to do your homework, RTFM, or Google?
>> - This is off-topic because XXX (e.g., it is not strictly an R programming question).
>> - You asked about doing XXX, but this is a stupid thing to want to do.
>> - Don't ask here; you need to talk to a statistical consultant.
>>
>> I find this sad in a public mailing list sent to all R-help subscribers and I sometimes cringe when I read replies to people who were actually trying to get help with some R-related problem, but expressed it badly, didn't know exactly what to ask for, or how to format it, or somehow motivated a frequent-replier to publicly dis the OP.
>>
>> On the other hand, I still see a spirit of great generosity among some people who frequently reply to R-help, taking a possibly badly posed or ill-formatted question, and going to some lengths to provide a a helpful answer of some sort.  I applaud those who take the time and effort to do this.
>>
>> I use R in a number of my courses, and used to advise students to post to R-help for general programming questions (not just homework) they couldn't solve. I don't do this any more, because several of them reported a negative experience.
>>
>> In contrast, in the Stackexchange model, there are numerous sublists cross-classified by their tags.  If I have a specific knitr, ggplot2, LaTeX, or statistical modeling question, I'm now more likely to post it there, and the worst that can happen is that no one "upvotes" it or someone (helpfully) marks it as a duplicate of a similar question.
>> But comments there are not propagated to all subscribers, and those who reply helpfully, can see their solutions accepted or not, or commented on in that specific topic.
>>
>> Perhaps one solution would be to create a new "R-not-help" list where, as in a Monty Python skit, people could be directed there to be insulted and all these unhelpful replies could be sent.
>>
>> A milder alternative is to encourage some R-help subscribers to click the "Don't send" or "Save" button and think better of their replies.
>> </rant>
>>


-- 
Kevin E. Thorpe
Head of Biostatistics,  Applied Health Research Centre (AHRC)
Li Ka Shing Knowledge Institute of St. Michael's
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016


From pai1981 at gmail.com  Tue Jan 26 01:20:26 2016
From: pai1981 at gmail.com (Debasish Pai Mazumder)
Date: Mon, 25 Jan 2016 17:20:26 -0700
Subject: [R] Problem with installing "mapview"
Message-ID: <CAM9mbiCoenyi5UF5GWCQiHTRiEU15oJekqHJK8J=K9yTm3J-Rw@mail.gmail.com>

Hi all,

I am using Rstudio version 0.99.491. I am trying install "mapview" package.
install.packages("mapview")

I receive following error message

configure: error: Cannot compile a simple JNI program. See config.log for
details.

Make sure you have Java Development Kit installed and correctly registered
in R.
If in doubt, re-run "R CMD javareconf" as root.

ERROR: configuration failed for package ?rJava?
* removing ?/Volumes/Users/debasish/Library/R/3.2/library/rJava?
Warning in install.packages :
  installation of package ?rJava? had non-zero exit status
ERROR: dependency ?rJava? is not available for package ?OpenStreetMap?
* removing ?/Volumes/Users/debasish/Library/R/3.2/library/OpenStreetMap?
Warning in install.packages :
  installation of package ?OpenStreetMap? had non-zero exit status
ERROR: dependency ?OpenStreetMap? is not available for package ?mapview?
* removing ?/Volumes/Users/debasish/Library/R/3.2/library/mapview?
Warning in install.packages :
  installation of package ?mapview? had non-zero exit status

Then I tried install rJava package
install.packages("rJava")

I got followings

configure: error: Cannot compile a simple JNI program. See config.log for
details.

Make sure you have Java Development Kit installed and correctly registered
in R.
If in doubt, re-run "R CMD javareconf" as root.

ERROR: configuration failed for package ?rJava?
* removing ?/Volumes/Users/debasish/Library/R/3.2/library/rJava?
Warning in install.packages :
  installation of package ?rJava? had non-zero exit status

Please let me know is there any other way to install mapview package

-Debasish

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Tue Jan 26 02:01:53 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 26 Jan 2016 02:01:53 +0100
Subject: [R] tcltk table: get celltag value
In-Reply-To: <CAJeYpE9HDDaJYmfR64cMCmp_szFZbbv14CN1N8cv4ki_0VYwbQ@mail.gmail.com>
References: <CAJeYpE9HDDaJYmfR64cMCmp_szFZbbv14CN1N8cv4ki_0VYwbQ@mail.gmail.com>
Message-ID: <1AA5DF90-4BC1-44E3-AC1D-F434EAFCE1A9@gmail.com>


> On 26 Jan 2016, at 00:54 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> 
> I'm finding it very difficult to figure out how to read the value of
> "celltag" for a given cell in a tktable.
> 
> I'm sure it's something like:
> 
> tcl(classTable, "get", "celltag", row, column)
> 
> but of the dozens of variations of names, options, args, and formats I've
> tried, nothing is working. Any suggestions?

Hmm, as I read the docs, tags contain cells, not the other way around. You can ask whether a tag contains a given cell by something like

.tbl tag includes mytag 12.34 

and you can get a list of tag names with

.tbl tag names

and of course a combination of the two and a loop can tell you which tags a given cell is in. Somehow it seems that you are not expected to want that...

By the way, I suppose these threads should have moved to r-devel long ago.

-pd

> 
> Much thanks.
> 
> -Dan
> 
> -- 
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From dwinsemius at comcast.net  Tue Jan 26 02:11:17 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 25 Jan 2016 17:11:17 -0800
Subject: [R] Problem with installing "mapview"
In-Reply-To: <CAM9mbiCoenyi5UF5GWCQiHTRiEU15oJekqHJK8J=K9yTm3J-Rw@mail.gmail.com>
References: <CAM9mbiCoenyi5UF5GWCQiHTRiEU15oJekqHJK8J=K9yTm3J-Rw@mail.gmail.com>
Message-ID: <7CB4E874-CAC8-43B7-8ECD-5EFDB1C90494@comcast.net>


> On Jan 25, 2016, at 4:20 PM, Debasish Pai Mazumder <pai1981 at gmail.com> wrote:
> 
> Hi all,
> 
> I am using Rstudio version 0.99.491. I am trying install "mapview" package.
> install.packages("mapview")
> 
> I receive following error message
> 
> configure: error: Cannot compile a simple JNI program. See config.log for
> details.
> 
> Make sure you have Java Development Kit installed and correctly registered
> in R.
> If in doubt, re-run "R CMD javareconf" as root.

The blog posting noted below recommends instead trying (for Mac users in a Terminal window, although this is a guess based on the directory structure in the error messages):

sudo R CMD javareconf -n

> 
> ERROR: configuration failed for package ?rJava?
> * removing ?/Volumes/Users/debasish/Library/R/3.2/library/rJava?
> Warning in install.packages :
>  installation of package ?rJava? had non-zero exit status
> ERROR: dependency ?rJava? is not available for package ?OpenStreetMap?
> * removing ?/Volumes/Users/debasish/Library/R/3.2/library/OpenStreetMap?
> Warning in install.packages :
>  installation of package ?OpenStreetMap? had non-zero exit status
> ERROR: dependency ?OpenStreetMap? is not available for package ?mapview?
> * removing ?/Volumes/Users/debasish/Library/R/3.2/library/mapview?
> Warning in install.packages :
>  installation of package ?mapview? had non-zero exit status
> 
> Then I tried install rJava package
> install.packages("rJava")
> 
> I got followings
> 
> configure: error: Cannot compile a simple JNI program. See config.log for
> details.
> 
> Make sure you have Java Development Kit installed and correctly registered
> in R.
> If in doubt, re-run "R CMD javareconf" as root.
> 
> ERROR: configuration failed for package ?rJava?
> * removing ?/Volumes/Users/debasish/Library/R/3.2/library/rJava?
> Warning in install.packages :
>  installation of package ?rJava? had non-zero exit status
> 
> Please let me know is there any other way to install mapview package

Seems pretty clear that you will need to have a version of Java installed. My memory of struggling through that process was that some packages needed to have a version less recent that the currently distributed version. Whether my memory is relevant to your situation will possibly depend on whether my guess that you are on a Mac with  Mavericks+ OS is correct. (And note you were requested very specifically in the Posting Guide to include more specifics of your setup.)

The package DESCRIPTION file say its support page is here:
http://www.rforge.net/rJava/

I _think_ that this blog post might be useful:
http://www.r-bloggers.com/getting-r-and-java-1-8-to-work-together-on-osx/

Despite following those recommendations the report on version still says : java version "1.8.0_65"

Further caveat: I'm not a regular user of RStudio, so there may be issues I have not encountered. Rstudio also has a support forum.

Nonetheless, installing mapview from source on a ElCapMac using the MacGUI Package Installer with XCode and Command Line tools in the background did just succeed. (It also installed several other r-pkg dependencies, so you definitely should read the package DESCRIPTION.)

--- from description file
Depends: R (>= 2.10), leaflet, methods
Imports: sp, raster, satellite, scales (>= 0.2.5), brew, htmlwidgets,
        htmltools, png, Rcpp (>= 0.11.3), lattice, latticeExtra, rgdal,
        gdalUtils, data.table, rasterVis, OpenStreetMap

> 
> -Debasish
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html


Do read the Posting Guide and do (re?)-read the Mailing List info page where it is explained that configuration questions for specific OSes (Mac and Debian/Ubuntu) might be more appropriate on particular OS-specific lists.


> and provide commented, minimal, self-contained, reproducible code.
-- 
David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Tue Jan 26 02:28:58 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 26 Jan 2016 12:28:58 +1100
Subject: [R] Division of data set with some restriction
In-Reply-To: <DB4PR07MB379E4A00FBCF1C3E8D1757F94C70@DB4PR07MB379.eurprd07.prod.outlook.com>
References: <DB4PR07MB379E4A00FBCF1C3E8D1757F94C70@DB4PR07MB379.eurprd07.prod.outlook.com>
Message-ID: <CA+8X3fUfdhkvhZ98jt19NCcYSJfwHcsPdD9AF_b0F3VsUtcvkg@mail.gmail.com>

Hi Muhammad,
There are a large number of approximate answers to your problem. One easy
one is to ensure that the standard deviations of the subgroups are
maximally different by dividing the observations into "mids" (observations
close to the mean) and "tails" (observations far from the mean). The
following function sorts the observations, takes half of the observations
from the "mids" and compares the mean of those observations to the mean of
the "tails". Depending upon whether the distribution is positively or
negatively skewed, it then shifts the "mids" up or down until the direction
of inequality is reversed. The two subsets of observations are returned in
a list. By printing the successive means, the user can see whether the
penultimate means were closer than the means of the subsets returned.
Perhaps it will be useful.

rw50<-rweibull(50,1)

split_on_mean<-function(x) {
 lenx<-length(x)
 sx<-sort(x)
 x4<-floor(lenx/4)
 x34<-floor(3*lenx/4)
 lowx<-1:x4
 midx<-(x4+1):x34
 highx<-(x34+1):lenx
 midmean<-mean(sx[midx])
 tailsmean<-mean(sx[c(lowx,highx)])
 if(midmean < tailsmean) {
  while(midmean < tailsmean) {
   lowx<-c(lowx,midx[1])
   midx<-c(midx[-1],highx[1])
   highx<-highx[-1]
   midmean<-mean(sx[midx])
   tailsmean<-mean(sx[c(lowx,highx)])
   cat(midmean,tailsmean,"\n")
  }
 } else {
  while(midmean > tailsmean) {
   highx<-c(highx,midx[length(midx)])
   midx<-c(midx[-length(midx)],lowx[length(lowx)])
   lowx<-lowx[-length(lowx)]
   midmean<-mean(sx[midx])
   tailsmean<-mean(sx[c(lowx,highx)])
  }
 }
 return(list(midx=sx[midx],tailsx=sx[c(lowx,highx)]))
}

Jim

On Tue, Jan 26, 2016 at 7:12 AM, Muhammad Kashif <mkashif at uaf.edu.pk> wrote:

>
> Dear Group members
>
> Can any one help to code this situation. Suppose we have a population with
> some mean  and a standard deviation. Then  , there are n1 observations out
> of n  which are less than or equal to n . Also, there are n2 observations
> out of n which are greater than  . We divide the whole data set into two
> parts such that we have the same mean   but different standard deviations.
>
> for example we have 50 observations from any distribution say two
> parameter Weibull. Then we divide the data into two parts such that the two
> resulting data sets have same mean and different standard deviation.
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Jan 26 03:13:59 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 25 Jan 2016 18:13:59 -0800
Subject: [R] Block Triangular Matirx
In-Reply-To: <983096080-1453704333-cardhu_decombobulator_blackberry.rim.net-588899922-@b17.c1.bise7.blackberry>
References: <SG2PR03MB09220199F2FA68ACAE90092090C70@SG2PR03MB0922.apcprd03.prod.outlook.com>
	<983096080-1453704333-cardhu_decombobulator_blackberry.rim.net-588899922-@b17.c1.bise7.blackberry>
Message-ID: <29B20C68-4A81-4A0C-9F14-11F0E77FB1E8@comcast.net>


> On Jan 24, 2016, at 10:45 PM, Olivier Crouzet <olivier.crouzet at univ-nantes.fr> wrote:
> 
> Hi, I think this page will help,
> 
> https://stat.ethz.ch/R-manual/R-devel/library/base/html/lower.tri.html

I agree that might be useful. Also very useful if the OP wants further advice would be for the OP to post a follow-up with R code that creates a minimal example and show what a correct answer would look like.

If each block were homogeneous, the answer might be simpler than if the values in the blocks could be arbitrary. It would be helpful to know what sort of specification for the dimensions of blocks might look like. Simpler would be square blocks.

-- 
David.
> 
> Olivier.
> 
> --
> Olivier Crouzet
> 
> -----Original Message-----
> From: Amina Shahzadi Shahzadi <shaam249 at student.otago.ac.nz>
> Sender: "R-help" <r-help-bounces at r-project.org>Date: Mon, 25 Jan 2016 00:39:22 
> To: r-help at R-project.org<r-help at r-project.org>
> Subject: [R] Block Triangular Matirx
> 
> Hi
> 
> I want to create a block upper triangular square matrix.
> 
> 
> 	[[alternative HTML version deleted]]
> 

David Winsemius
Alameda, CA, USA


From sarah.w at brightsocialsolution.com  Mon Jan 25 21:10:01 2016
From: sarah.w at brightsocialsolution.com (sarah.w at brightsocialsolution.com)
Date: Tue, 26 Jan 2016 01:40:01 +0530
Subject: [R] Drive traffic to your booth
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAGAqRBbjEXtOstu/wLeMadTCgAAAEAAAAIcG2XMiKqJAoomHAHMzARoBAAAAAA==@brightsocialsolution.com>

 

 

Hi, 

 

Greetings!!!

 

I understand your company is one of the participants in "MD&M West
2016"Would you be interested in acquiring the complete contact details of
attendees who might be interest in your products and services? We currently
compile and maintain current attendees list with their email address and
phone number who will be attending this conference. This will help providing
you with the opportunity to engage and move your customer to action.

 

Let me know your interest so that we could discuss further

 

Regards

Sarah Williams

 


	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Tue Jan 26 11:12:18 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 26 Jan 2016 11:12:18 +0100
Subject: [R] Block Triangular Matirx
In-Reply-To: <29B20C68-4A81-4A0C-9F14-11F0E77FB1E8@comcast.net>
References: <SG2PR03MB09220199F2FA68ACAE90092090C70@SG2PR03MB0922.apcprd03.prod.outlook.com>
	<983096080-1453704333-cardhu_decombobulator_blackberry.rim.net-588899922-@b17.c1.bise7.blackberry>
	<29B20C68-4A81-4A0C-9F14-11F0E77FB1E8@comcast.net>
Message-ID: <22183.18050.546129.402180@stat.math.ethz.ch>

>>>>> David Winsemius <dwinsemius at comcast.net>
>>>>>     on Mon, 25 Jan 2016 18:13:59 -0800 writes:

    >> On Jan 24, 2016, at 10:45 PM, Olivier Crouzet
    >> <olivier.crouzet at univ-nantes.fr> wrote:
    >> 
    >> Hi, I think this page will help,
    >> 
    >> https://stat.ethz.ch/R-manual/R-devel/library/base/html/lower.tri.html

    > I agree that might be useful. Also very useful if the OP
    > wants further advice would be for the OP to post a
    > follow-up with R code that creates a minimal example and
    > show what a correct answer would look like.

    > If each block were homogeneous, the answer might be
    > simpler than if the values in the blocks could be
    > arbitrary. It would be helpful to know what sort of
    > specification for the dimensions of blocks might look
    > like. Simpler would be square blocks.

Also, if the matrix is relatively large and there are relatively
many small blocks,  it may be quite advantageous to use the
Matrix package because of more efficient storage as
'sparseMatrix', and 'triangularMatrix'  typically also providing
more efficient *computation* with such matrices :


require("Matrix")

##' random (diagonal-)block-triangular matrices:
rblockTri <- function(nb, max.ni, lambda = 3) {
   .bdiag(replicate(nb, {
         n <- sample.int(max.ni, 1)
         tril(Matrix(rpois(n*n, lambda=lambda), n,n)) }))
}

set.seed(77)
TT <- rblockTri(30, 40)
image(TT) # to visualize it
tt <- as.matrix(TT)
object.size(TT) #  141832 bytes
object.size(tt) # 3145232 bytes -- factor 22.2 larger:
c(object.size(tt) / object.size(TT))

system.time(crossprod(TT))
##  user  system elapsed 
## 0.001   0.000   0.001 
system.time(crossprod(tt)) # a factor of ~100 slower :
##  user  system elapsed 
## 0.123   0.000   0.122


From mohsenhs82 at yahoo.com  Tue Jan 26 09:33:56 2016
From: mohsenhs82 at yahoo.com (mohsen hs)
Date: Tue, 26 Jan 2016 08:33:56 +0000 (UTC)
Subject: [R] [FORGED]  qqPlot vs qqcomp
In-Reply-To: <B34BF210-DC08-48C5-B01D-C953CA432F82@gmail.com>
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
	<717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
	<5678A5CD.3000302@auckland.ac.nz>
	<1819934632.1694446.1450765811908.JavaMail.yahoo@mail.yahoo.com>
	<95A9E6BE-3C6D-4225-BC0C-7C0950362A02@gmail.com>
	<1660893852.2256439.1450884141115.JavaMail.yahoo@mail.yahoo.com>
	<1790333972.2430395.1450947829967.JavaMail.yahoo@mail.yahoo.com>
	<B34BF210-DC08-48C5-B01D-C953CA432F82@gmail.com>
Message-ID: <1774275737.199895.1453797237031.JavaMail.yahoo@mail.yahoo.com>

Hi Peter and Rolf,
Hope you are doing well and thanks for your time earlierlast year. I have started working on the data again and used the first ideathat Peter proposed:

Take log of your data andcompare with normal distr.

Hence, I tried the following code:

library(EnvStats); library(fitdistrplus)

mydata <- exp(rnorm(100))

?ct=0

?arr = NULL

for(i in 0:100)

{

? arr[ct]=0

? ct=ct+1

}

arr[98]=1

arr[99]=1

p=mydata;

lp=log(p)

fitln2 <- fitdist(lp, "norm")

dev.new();qqcomp(fitln2)

?dev.new();qqPlotCensored(lp, as.logical(arr),distribution = "norm",censoring.side = "right",add.line = TRUE, main = "")

?#The above twocommands generate same plots, but they are different with the following commands:

?fitln=fitdist(p,"lnorm")

dev.new();qqcomp(fitln)



Now, I was wondering if you could kindly give me some advicewhy I can not get a similar plot to qqcomp from qqPlotCensored and how I canget a plot similar to qqcomp from qqPlotCensored?


?
Another question that I appreciate if you could kindly replyis as follows:

I get the lowest AIC from the following commands, but can I claimthat my data follow the lnorm distribution since I am not sure whether I am usinglnorm or norm distribution by the following command?


?
?lp=log(p)

fitln2 <- fitdist(lp, "norm")

dev.new();qqcomp(fitln2)



Thanks a lot for considering my questions and please forgive me if my questions might be look simple.
Many thanksMohsen 

    On Friday, December 25, 2015 1:45 AM, peter dalgaard <pdalgd at gmail.com> wrote:
 

 Two ideas:

a: Take log of your data and compare with normal distr.

b: Use log="xy" as a graphical parameter.

Otherwise, you're on your own.

-pd

> On 24 Dec 2015, at 10:03 , mohsen hs <mohsenhs82 at yahoo.com> wrote:
> 
> Hi Peter,
> 
> Thanks once again for your kind reply.
> 
> One quick question, could you please guide me and let me know how I can get the similar qq plot(log-log scale) that I get from qqcomp, from qqPlotCensored function(It is similar to qqPlot, and available in EnvStats? http://www.inside-r.org/node/218933 ).
> 
> Thanks a lot.
> 
> Cheers
> Mohsen
> 
> 
>? 
> MHS
> 
> 
> On Wednesday, December 23, 2015 6:52 PM, mohsen hs <mohsenhs82 at yahoo.com> wrote:
> 
> 
> Hi Peter and Rolf
> 
> Thank you for your time and replying me. It makes sense now. I sincerely appreciate that.
> 
> Cheers
> Mohsen
>? 
> 
> 
> 
> On Tuesday, December 22, 2015 10:08 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> 
> 
> > On 22 Dec 2015, at 07:30 , mohsen hs via R-help <r-help at r-project.org> wrote:
> > 
> > The above command gives me a differentplot. I am not sure what part I am doing wrong. I appreciate your time forconsidering my request and your feedback is highly appreciated. Please find the plots attached. The right one is from qqcomp and the left one is from qqPlot. Titles might be incorrect.
> 
> They never arrived, but your data weren't actually needed. The crucial missing information was the packages used. This will do:
> 
> > library(EnvStats); library(fitdistrplus)
> > serving <- exp(rnorm(100))
> > qqPlot ( serving, dist ="lnorm", estimate.params = TRUE, add.line = TRUE)
> 
> > 
> > fitln <- fitdist(serving,"lnorm",method="mle")
> > qqcomp(fitln)
> 
> 
> The difference is quite clearly that qqPlot is doing a QQ-plot of log(serving) vs. normal quantiles, whereas qqcomp plots serving itself against lognormal quantiles. So the former is pretty much equal to the latter on a log-log scale.
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk? Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk? Priv: PDalgd at gmail.com










  
	[[alternative HTML version deleted]]


From S.Ellison at LGCGroup.com  Tue Jan 26 12:30:30 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Tue, 26 Jan 2016 11:30:30 +0000
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <56A54553.60603@yorku.ca>
References: <56A371F5.5070509@nancy.inra.fr> <56A54553.60603@yorku.ca>
Message-ID: <1A8C1289955EF649A09086A153E2672403D0B98891@GBTEDVPEXCMB04.corp.lgc-group.com>

> Yet, the trend I've seen is one of increasing **R-not-help**, in that there are
> many posts, often by new R users who get replies that not infrequently range
> from just mildly off-putting to actively hostile:

Slightly surprised that in a debate postulated on increasing 'meanness', no-one has yet pointed to Trey Causey's analysis of R-help's alleged meanness at 

http://badhessian.org/2013/04/has-r-help-gotten-meaner-over-time-and-what-does-mancur-olson-have-to-say-about-it/

Up to 2013, it was apparently getting _less_ 'mean', not more.


S Ellison




*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From murdoch.duncan at gmail.com  Tue Jan 26 12:56:05 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 26 Jan 2016 06:56:05 -0500
Subject: [R] R-help mailing list activity / R-not-help?
In-Reply-To: <1A8C1289955EF649A09086A153E2672403D0B98891@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <56A371F5.5070509@nancy.inra.fr> <56A54553.60603@yorku.ca>
	<1A8C1289955EF649A09086A153E2672403D0B98891@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <56A75ED5.7080107@gmail.com>

On 26/01/2016 6:30 AM, S Ellison wrote:
>> Yet, the trend I've seen is one of increasing **R-not-help**, in that there are
>> many posts, often by new R users who get replies that not infrequently range
>> from just mildly off-putting to actively hostile:
>
> Slightly surprised that in a debate postulated on increasing 'meanness', no-one has yet pointed to Trey Causey's analysis of R-help's alleged meanness at
>
> http://badhessian.org/2013/04/has-r-help-gotten-meaner-over-time-and-what-does-mancur-olson-have-to-say-about-it/
>
> Up to 2013, it was apparently getting _less_ 'mean', not more.

I don't remember reading that article when it first appeared.  It's 
interesting, and mostly well done.  I'd only argue about one conclusion:

He attributes the increase of his category 2 (not a response) to 
dominance near the end of the period as due to a lot of questions going 
unanswered, but gives no apparent evidence for that.  I think anyone who 
has participated in this group for a long time would recognize that very 
few questions go unanswered; only the ones that are so badly posed that 
nobody can figure out what to say.

What is far more common is that discussion on threads goes off on a 
tangent that has nothing to do with questions or answers.  There are 
also threads like this one that contain no questions or answers, and are 
just full of hot air.

Thanks for posting the link.

Duncan Murdoch


From bogaso.christofer at gmail.com  Tue Jan 26 13:16:07 2016
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Tue, 26 Jan 2016 17:46:07 +0530
Subject: [R] Downloading Google finance data onto R
Message-ID: <CA+dpOJni4aJk7JxoC8A8rDdR=MRxk96VrQ58Zo+Vz=pcc2PR4Q@mail.gmail.com>

Hi dear,

I was trying to download a Google TS data directly onto R from this link :

https://www.google.com/finance?q=NSE%3ALIQUIDBEES&ei=xlGnVuiPJ9eDuQSZ05OICw

Used following function, however R generates error.

> library(quantmod)
> getSymbols(Symbols = "LIQUIDBEES", src = "google")
Error in download.file(paste(google.URL, "q=", Symbols.name, "&startdate=",  :
  cannot open URL
'http://finance.google.com/finance/historical?q=LIQUIDBEES&startdate=Jan+01,+2007&enddate=Jan+26,+2016&output=csv'
In addition: Warning message:
In download.file(paste(google.URL, "q=", Symbols.name, "&startdate=",  :
  cannot open: HTTP status was '404 Not Found'

Could you please guide me how can I download that data directly to R?
I have also tried with Yahoo finance data, but got the same error.

Thanks for your help.


From josh.m.ulrich at gmail.com  Tue Jan 26 13:31:47 2016
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 26 Jan 2016 06:31:47 -0600
Subject: [R] Downloading Google finance data onto R
In-Reply-To: <CA+dpOJni4aJk7JxoC8A8rDdR=MRxk96VrQ58Zo+Vz=pcc2PR4Q@mail.gmail.com>
References: <CA+dpOJni4aJk7JxoC8A8rDdR=MRxk96VrQ58Zo+Vz=pcc2PR4Q@mail.gmail.com>
Message-ID: <CAPPM_gRAxAMr13VryUkmHFotR1JeswDaR6x27APicj1zw_ugLQ@mail.gmail.com>

On Tue, Jan 26, 2016 at 6:16 AM, Christofer Bogaso
<bogaso.christofer at gmail.com> wrote:
> Hi dear,
>
> I was trying to download a Google TS data directly onto R from this link :
>
> https://www.google.com/finance?q=NSE%3ALIQUIDBEES&ei=xlGnVuiPJ9eDuQSZ05OICw
>
> Used following function, however R generates error.
>
>> library(quantmod)
>> getSymbols(Symbols = "LIQUIDBEES", src = "google")
> Error in download.file(paste(google.URL, "q=", Symbols.name, "&startdate=",  :
>   cannot open URL
> 'http://finance.google.com/finance/historical?q=LIQUIDBEES&startdate=Jan+01,+2007&enddate=Jan+26,+2016&output=csv'
> In addition: Warning message:
> In download.file(paste(google.URL, "q=", Symbols.name, "&startdate=",  :
>   cannot open: HTTP status was '404 Not Found'
>
> Could you please guide me how can I download that data directly to R?
> I have also tried with Yahoo finance data, but got the same error.
>
There's no data available for download. Compare with:
https://www.google.com/finance/historical?q=NYSEARCA%3ASPY

That page has a "Download to spreadsheet" link. The page for
LIQUIDBEES does not.  So you have to scrape the data from the HTML.

> Thanks for your help.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com
R/Finance 2016 | www.rinfinance.com


From murdoch.duncan at gmail.com  Tue Jan 26 13:36:26 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 26 Jan 2016 07:36:26 -0500
Subject: [R] Downloading Google finance data onto R
In-Reply-To: <CA+dpOJni4aJk7JxoC8A8rDdR=MRxk96VrQ58Zo+Vz=pcc2PR4Q@mail.gmail.com>
References: <CA+dpOJni4aJk7JxoC8A8rDdR=MRxk96VrQ58Zo+Vz=pcc2PR4Q@mail.gmail.com>
Message-ID: <56A7684A.9030006@gmail.com>

On 26/01/2016 7:16 AM, Christofer Bogaso wrote:
> Hi dear,
>
> I was trying to download a Google TS data directly onto R from this link :
>
> https://www.google.com/finance?q=NSE%3ALIQUIDBEES&ei=xlGnVuiPJ9eDuQSZ05OICw
>
> Used following function, however R generates error.
>
>> library(quantmod)
>> getSymbols(Symbols = "LIQUIDBEES", src = "google")
> Error in download.file(paste(google.URL, "q=", Symbols.name, "&startdate=",  :
>    cannot open URL
> 'http://finance.google.com/finance/historical?q=LIQUIDBEES&startdate=Jan+01,+2007&enddate=Jan+26,+2016&output=csv'
> In addition: Warning message:
> In download.file(paste(google.URL, "q=", Symbols.name, "&startdate=",  :
>    cannot open: HTTP status was '404 Not Found'
>
> Could you please guide me how can I download that data directly to R?
> I have also tried with Yahoo finance data, but got the same error.

That symbol should be "LIQUIDBEES.BO" on Yahoo.  Google appears to have 
changed its interface, so src = "google" doesn't work.  You will 
probably have to manually download the Google history, or debug and fix 
getSymbols.google in quantmod.

Duncan Murdoch


From josh.m.ulrich at gmail.com  Tue Jan 26 13:47:59 2016
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 26 Jan 2016 06:47:59 -0600
Subject: [R] Downloading Google finance data onto R
In-Reply-To: <56A7684A.9030006@gmail.com>
References: <CA+dpOJni4aJk7JxoC8A8rDdR=MRxk96VrQ58Zo+Vz=pcc2PR4Q@mail.gmail.com>
	<56A7684A.9030006@gmail.com>
Message-ID: <CAPPM_gSbjaEvBZukdHs3kfNBkyCfiVHYLNQZZG1knZUyGzPb=w@mail.gmail.com>

On Tue, Jan 26, 2016 at 6:36 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 26/01/2016 7:16 AM, Christofer Bogaso wrote:
>>
>> Hi dear,
>>
>> I was trying to download a Google TS data directly onto R from this link :
>>
>>
>> https://www.google.com/finance?q=NSE%3ALIQUIDBEES&ei=xlGnVuiPJ9eDuQSZ05OICw
>>
>> Used following function, however R generates error.
>>
>>> library(quantmod)
>>> getSymbols(Symbols = "LIQUIDBEES", src = "google")
>>
>> Error in download.file(paste(google.URL, "q=", Symbols.name,
>> "&startdate=",  :
>>    cannot open URL
>>
>> 'http://finance.google.com/finance/historical?q=LIQUIDBEES&startdate=Jan+01,+2007&enddate=Jan+26,+2016&output=csv'
>> In addition: Warning message:
>> In download.file(paste(google.URL, "q=", Symbols.name, "&startdate=",  :
>>    cannot open: HTTP status was '404 Not Found'
>>
>> Could you please guide me how can I download that data directly to R?
>> I have also tried with Yahoo finance data, but got the same error.
>
>
> That symbol should be "LIQUIDBEES.BO" on Yahoo.  Google appears to have
> changed its interface, so src = "google" doesn't work.  You will probably
> have to manually download the Google history, or debug and fix
> getSymbols.google in quantmod.
>
Nice job finding the correct symbol for Yahoo Finance.  That should
give Christofer what he wants.

I don't think Google changed its interface, because
quantmod::getSymbols.google *does* work for instruments where Google
provides a download link. For example:
getSymbols("SPY", src="google")

As I said in my prior response, Google does not make data for
LIQUIDBEES available for download.

> Duncan Murdoch
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com
R/Finance 2016 | www.rinfinance.com


From S.Ellison at LGCGroup.com  Tue Jan 26 14:19:31 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Tue, 26 Jan 2016 13:19:31 +0000
Subject: [R] Sorting a Data Frame
In-Reply-To: <56A2C93D.6090008@statistik.tu-dortmund.de>
References: <56A2C793.6080307@comcast.net>
	<56A2C93D.6090008@statistik.tu-dortmund.de>
Message-ID: <1A8C1289955EF649A09086A153E2672403D0B9891D@GBTEDVPEXCMB04.corp.lgc-group.com>


> On 23.01.2016 01:21, Robert Sherry wrote:
> > In R, I run the following commands:
> >      df = data.frame( x=runif(10), y=runif(10) )
> >      df2 = df[order(x),]
> 
> 
> You use another x from your workspace, you actually want to
> 
> 
>   df2 = df[order(df[,"x"]),]

or 
df[order(df$x),] 

And just to prevent yet more confusion, you might also want to avoid 'df' as a name. 'df' is the function that returns the density of the F distribution ...

S Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From g.becquembois at decision-network.eu  Tue Jan 26 14:55:09 2016
From: g.becquembois at decision-network.eu (Gregory BECQUEMBOIS (DN))
Date: Tue, 26 Jan 2016 13:55:09 +0000
Subject: [R] probability or a quantile from various distributions for
 multiple comparisons of means
Message-ID: <B76A648EFE86C943A38DECB696CAC9FB64F560@exbe-2010-a.ad.hosteam.fr>

Hi There,

I would like to know if the SAS Function PROBMC is in R. In the SAS documentation this function returns a probability or a quantile from various distributions for multiple comparisons of means. I'm not sure to understand clearly what it does but i have to reproduce it with R for one of my users (I'm a poor IT guy ;-)) ...

Best,

Greg






	[[alternative HTML version deleted]]


From JGellar at mathematica-mpr.com  Tue Jan 26 15:28:36 2016
From: JGellar at mathematica-mpr.com (Jonathan Gellar)
Date: Tue, 26 Jan 2016 14:28:36 +0000
Subject: [R] R Licensing Question
Message-ID: <5509b20702004bd99e75cd19a194fd3f@MMCULEXCH01.mathematica.net>

Hello,

I have found a list of all software licenses supported by CRAN at the following site:

https://svn.r-project.org/R/trunk/share/licenses/license.db

There is also the list of commonly used licenses here:

https://cran.r-project.org/web/licenses/

I have tried to read through some of these licenses, but I am not a lawyer and some of the legal jargon is difficult to get through. I have a simple question:

Are there any packages available on CRAN that have a license that requires that every use of a particular package (e.g. in an analysis) be made open source as well? I have never heard of this being the case, and it does not appear to be true for any of the most commonly used licenses, but from what I understand it would be possible for someone to create a license that has this requirement.

I apologize for the mass email if this is not the best forum for this question, but I could not find an answer elsewhere.

Thank you,
Jonathan

______________________________
Jonathan Gellar
Statistician
Mathematica Policy Research
1100 First Street NE, 12th Floor
Washington, DC 20002


From marc_schwartz at me.com  Tue Jan 26 16:14:36 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 26 Jan 2016 09:14:36 -0600
Subject: [R] R Licensing Question
In-Reply-To: <5509b20702004bd99e75cd19a194fd3f@MMCULEXCH01.mathematica.net>
References: <5509b20702004bd99e75cd19a194fd3f@MMCULEXCH01.mathematica.net>
Message-ID: <E3B28B98-875B-41FC-8E81-93C2A6B0A863@me.com>


> On Jan 26, 2016, at 8:28 AM, Jonathan Gellar <JGellar at mathematica-mpr.com> wrote:
> 
> Hello,
> 
> I have found a list of all software licenses supported by CRAN at the following site:
> 
> https://svn.r-project.org/R/trunk/share/licenses/license.db
> 
> There is also the list of commonly used licenses here:
> 
> https://cran.r-project.org/web/licenses/
> 
> I have tried to read through some of these licenses, but I am not a lawyer and some of the legal jargon is difficult to get through. I have a simple question:
> 
> Are there any packages available on CRAN that have a license that requires that every use of a particular package (e.g. in an analysis) be made open source as well? I have never heard of this being the case, and it does not appear to be true for any of the most commonly used licenses, but from what I understand it would be possible for someone to create a license that has this requirement.
> 
> I apologize for the mass email if this is not the best forum for this question, but I could not find an answer elsewhere.
> 
> Thank you,
> Jonathan

Hi,

With the caveat that IANAL:

There are a few packages on CRAN that would preclude commercial use. You would need to review the license status of any packages that you intend to use to check for that. These would not be, for example, GPL licensed packages, since the GPL specifically prohibits such restrictions.

That being said, to the best of my knowledge, with the exception above regarding commercial use, most common open source licenses are not relevant to use, by to copying and distribution.

Thus, if you only plan to use the package in the course of analyses (e.g. simply calling functions within the package in your code), there is no requirement that your code behind the analyses be made openly available.

However, it would be reasonable and recommended to cite said package in any relevant publications.

See this FAQ, for example:

  https://cran.r-project.org/doc/FAQ/R-FAQ.html#Can-I-use-R-for-commercial-purposes_003f


On the other hand, if you plan to copy and distribute any CRAN packages or R itself within the context of a larger product offering (e.g. bundling, etc.), then you need to evaluate the details of the licenses for those packages. That is where you will want an IP lawyer to get involved as the nature of such bundling (e.g are you actually interfacing with the package via compiled code that is linked to a binary?) will be relevant to determining if your code would need to be licensed with a compatible open source license.

Regards,

Marc Schwartz


From dcarlson at tamu.edu  Tue Jan 26 18:20:18 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 26 Jan 2016 17:20:18 +0000
Subject: [R] probability or a quantile from various distributions for
 multiple comparisons of means
In-Reply-To: <B76A648EFE86C943A38DECB696CAC9FB64F560@exbe-2010-a.ad.hosteam.fr>
References: <B76A648EFE86C943A38DECB696CAC9FB64F560@exbe-2010-a.ad.hosteam.fr>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6FDC28@mb02.ads.tamu.edu>

Quickly scanning the documentation for PROBMC suggests that it does several things, but multiple comparisons are implemented in package multcomp through the function glht(). These vignettes may help:

https://cran.r-project.org/web/packages/multcomp/vignettes/generalsiminf.pdf
https://cran.r-project.org/web/packages/multcomp/vignettes/multcomp-examples.pdf

There is also a function, p.adjust(), that adjusts p-values for multiple comparisons that is included in the stats package in R.

?p.adjust

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Gregory BECQUEMBOIS (DN)
Sent: Tuesday, January 26, 2016 7:55 AM
To: r-help at r-project.org
Subject: [R] probability or a quantile from various distributions for multiple comparisons of means

Hi There,

I would like to know if the SAS Function PROBMC is in R. In the SAS documentation this function returns a probability or a quantile from various distributions for multiple comparisons of means. I'm not sure to understand clearly what it does but i have to reproduce it with R for one of my users (I'm a poor IT guy ;-)) ...

Best,

Greg






	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From andre_mikulec at hotmail.com  Tue Jan 26 16:44:12 2016
From: andre_mikulec at hotmail.com (Andre Mikulec)
Date: Tue, 26 Jan 2016 10:44:12 -0500
Subject: [R] How do I parse embedded quotes?
Message-ID: <BLU174-W4462A0A2E13EAA0163C8869CD80@phx.gbl>

I want embedded quotes to be parsed.  I can not figure out hot to do that.   I want to this to be parsable.

"'"a"'"

In a simpler case, "'a'" is parsable.

> eval(parse(text="\"'a'\""))
[1] "'a'"


But I want to parse embedded quotes
So, I want to parse '"a"' and not just 'a'

> eval(parse(text="\"'\"a\"'\""))
Error in parse(text = "\"'\"a\"'\"") : <text>:1:4: unexpected symbol
1: "'"a


I keep getting that(above) error.
Any ideas would much be appreciated.


Thanks,
Andre Mikulec
Andre_Mikulec at Hotmail.com 		 	   		  

From elopomorph at hotmail.com  Tue Jan 26 17:48:33 2016
From: elopomorph at hotmail.com (Michael)
Date: Tue, 26 Jan 2016 16:48:33 +0000
Subject: [R] organizing my data before doing a cluster analysis
Message-ID: <CY1PR13MB008027F09116FC67FCED085DDED80@CY1PR13MB0080.namprd13.prod.outlook.com>

I have been reading the different cluster analysis methods available in R.  I have a problem getting my data in the correct format so I can use these methods.  I explain below.


I am trying to cluster different fish species to see what fish are caught with each other on a commercial fishing trips.  I gave each fish species a 1 if it was caught on a trip and a 0 if it was not.  I also have the depth where the fish were caught on each trip.  So my data looks like this:

              Depth   Species1    Species 2   Species 3
Trip A       14          1                1               0
Trip B        8           0                1               1
Trip C       22          1                0               1

I looked at the cluster analysis examples in R and they have the data in a format with variables for the columns the rows are the objects you want to be clustered.  When I transpose my data I get depth as a row.  I show an example below:

                     Trip A      Trip B      Trip C
Depth               14            8            22
Species 1           1            0             1
Species 2           1            1             0
Species 3           0            1             1

So the R cluster program will treat depth as an object that will be clustered.  I don't know how to still incorporate depth into the analysis, and also not have it be treated as an object that will be clustered.  Any help would be greatly appreciated.


Mike



	[[alternative HTML version deleted]]


From eman_ee2020 at hotmail.com  Tue Jan 26 18:16:20 2016
From: eman_ee2020 at hotmail.com (Eman M)
Date: Tue, 26 Jan 2016 20:16:20 +0300
Subject: [R] R tool downloaded
Message-ID: <DUB115-W109AF690EB5A4E064C7B53B97D80@phx.gbl>

Hi,I am a student and I want download R tool but I did not found my country (Saudi Arabia ) in the list of download, can you help me how can I download the R tool...
Best Regards,Eman 		 	   		  
	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Tue Jan 26 18:29:27 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 26 Jan 2016 12:29:27 -0500
Subject: [R] How do I parse embedded quotes?
In-Reply-To: <BLU174-W4462A0A2E13EAA0163C8869CD80@phx.gbl>
References: <BLU174-W4462A0A2E13EAA0163C8869CD80@phx.gbl>
Message-ID: <56A7ACF7.206@gmail.com>

On 26/01/2016 10:44 AM, Andre Mikulec wrote:
> I want embedded quotes to be parsed.  I can not figure out hot to do that.   I want to this to be parsable.
>
> "'"a"'"
>
> In a simpler case, "'a'" is parsable.
>
> > eval(parse(text="\"'a'\""))
> [1] "'a'"
>
>
> But I want to parse embedded quotes
> So, I want to parse '"a"' and not just 'a'
>
> > eval(parse(text="\"'\"a\"'\""))
> Error in parse(text = "\"'\"a\"'\"") : <text>:1:4: unexpected symbol
> 1: "'"a
>
>
> I keep getting that(above) error.
> Any ideas would much be appreciated.

It is usually helpful to print strings using cat(), so that the escapes 
and quotes around it don't show.   With your string that gives the 
error, I see

"'"a"'"

That's not legal R source, because the double quotes that are within the 
string are not escaped.  To be legal R  code you would need

"'\"a\"'"

and to get that you need the original string to look like

"\"'\\\"a\\\"'\""

Duncan Murdoch


From murdoch.duncan at gmail.com  Tue Jan 26 18:30:56 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 26 Jan 2016 12:30:56 -0500
Subject: [R] R tool downloaded
In-Reply-To: <DUB115-W109AF690EB5A4E064C7B53B97D80@phx.gbl>
References: <DUB115-W109AF690EB5A4E064C7B53B97D80@phx.gbl>
Message-ID: <56A7AD50.3050608@gmail.com>

On 26/01/2016 12:16 PM, Eman M wrote:
> Hi,I am a student and I want download R tool but I did not found my country (Saudi Arabia ) in the list of download, can you help me how can I download the R tool...

Choose a mirror that is near you (in a connectivity sense), it doesn't 
have to actually be local.  I don't know about your country, but in many 
places, 0 - Cloud works really well.

Duncan Murdoch


From dcarlson at tamu.edu  Tue Jan 26 18:55:37 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 26 Jan 2016 17:55:37 +0000
Subject: [R] organizing my data before doing a cluster analysis
In-Reply-To: <CY1PR13MB008027F09116FC67FCED085DDED80@CY1PR13MB0080.namprd13.prod.outlook.com>
References: <CY1PR13MB008027F09116FC67FCED085DDED80@CY1PR13MB0080.namprd13.prod.outlook.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6FDC7B@mb02.ads.tamu.edu>

Your question involves a number of basic features of R. First, don't use html formatting in your email because the r-help list strips out the formatting. Second, use the R function dput() to paste data into your email since we can transfer that to R easily. I've converted your table to an R data frame called fishing:

> dput(fishing)
structure(list(Trip = structure(1:3, .Label = c("A", "B", "C"
), class = "factor"), Depth = c(14L, 8L, 22L), Species1 = c(1L, 
0L, 1L), Species2 = c(1L, 1L, 0L), Species3 = c(0L, 1L, 1L)), .Names = c("Trip", 
"Depth", "Species1", "Species2", "Species3"), class = "data.frame", row.names = c(NA, 
-3L))
> fishing
  Trip Depth Species1 Species2 Species3
1    A    14        1        1        0
2    B     8        0        1        1
3    C    22        1        0        1

To get just the species columns and transpose:

> fishclus <- t(fishing[, 3:5])
> fishclus
         [,1] [,2] [,3]
Species1    1    0    1
Species2    1    1    0
Species3    0    1    1

Now you are ready to use cluster analysis on fishclus. Once you have decided what kind of cluster analysis, what distance measure, and how many groups to create you can compare those groups to your Depth variable using boxplots or something similar.

You would benefit by learning more about R before going much farther. Go to this webpage:

https://cran.r-project.org/other-docs.html

If you have trouble deciding which one(s), here are some suggestions:

https://cran.r-project.org/doc/contrib/usingR.pdf
https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf
https://cran.r-project.org/doc/contrib/Owen-TheRGuide.pdf

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael
Sent: Tuesday, January 26, 2016 10:49 AM
To: r-help at r-project.org
Subject: [R] organizing my data before doing a cluster analysis

I have been reading the different cluster analysis methods available in R.  I have a problem getting my data in the correct format so I can use these methods.  I explain below.


I am trying to cluster different fish species to see what fish are caught with each other on a commercial fishing trips.  I gave each fish species a 1 if it was caught on a trip and a 0 if it was not.  I also have the depth where the fish were caught on each trip.  So my data looks like this:

              Depth   Species1    Species 2   Species 3
Trip A       14          1                1               0
Trip B        8           0                1               1
Trip C       22          1                0               1

I looked at the cluster analysis examples in R and they have the data in a format with variables for the columns the rows are the objects you want to be clustered.  When I transpose my data I get depth as a row.  I show an example below:

                     Trip A      Trip B      Trip C
Depth               14            8            22
Species 1           1            0             1
Species 2           1            1             0
Species 3           0            1             1

So the R cluster program will treat depth as an object that will be clustered.  I don't know how to still incorporate depth into the analysis, and also not have it be treated as an object that will be clustered.  Any help would be greatly appreciated.


Mike



	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From lordpreetam at gmail.com  Tue Jan 26 19:26:20 2016
From: lordpreetam at gmail.com (Preetam Pal)
Date: Tue, 26 Jan 2016 23:56:20 +0530
Subject: [R] Weighted Quantile Regression as a function of Tau and Weights
	array
Message-ID: <CAHVFrXFm7eeBeTshnqxEix=Z0ejeJf-8p2AV1yk8yi5v0Fp-0g@mail.gmail.com>

I have a dataset (attached) on numeric variables y, gdp, hpa and fx. I
intend to perform weighted quantile regression on this data set (i.e. y on
the remaining variables) and extract the estimated coefficients. I want to
do this as a bivariate function of the quantile tau and the weights array.
Can you help me with how to write the function and also how to invoke it?
This might be very basic, but I have very limited exposure to coding, hence
any help would be really appreciated.
Regards,
Preetam

	[[alternative HTML version deleted]]


From lordpreetam at gmail.com  Tue Jan 26 20:45:00 2016
From: lordpreetam at gmail.com (Preetam Pal)
Date: Wed, 27 Jan 2016 01:15:00 +0530
Subject: [R] Weighted Quantile Regression as a function of Tau and
	Weights array
In-Reply-To: <CAHVFrXFm7eeBeTshnqxEix=Z0ejeJf-8p2AV1yk8yi5v0Fp-0g@mail.gmail.com>
References: <CAHVFrXFm7eeBeTshnqxEix=Z0ejeJf-8p2AV1yk8yi5v0Fp-0g@mail.gmail.com>
Message-ID: <CAHVFrXFGu6tkkbXjXTJ6rh0fputiGCGQF4L13YqvrbwCeuJ2Gg@mail.gmail.com>

Sorry, forgot to attach the data.

On Tue, Jan 26, 2016 at 11:56 PM, Preetam Pal <lordpreetam at gmail.com> wrote:

> I have a dataset (attached) on numeric variables y, gdp, hpa and fx. I
> intend to perform weighted quantile regression on this data set (i.e. y on
> the remaining variables) and extract the estimated coefficients. I want to
> do this as a bivariate function of the quantile tau and the weights array.
> Can you help me with how to write the function and also how to invoke it?
> This might be very basic, but I have very limited exposure to coding, hence
> any help would be really appreciated.
> Regards,
> Preetam
>



-- 
Preetam Pal
(+91)-9432212774
M-Stat 2nd Year,                                             Room No. N-114
Statistics Division,                                           C.V.Raman
Hall
Indian Statistical Institute,                                 B.H.O.S.
Kolkata.
-------------- next part --------------
GDP	HPA	FX	Y
0.514662421	0.635997077	1.37802145	1.773342598
0.936722	3.127683176	1.391916535	3.709809052
0.101482324	1.270555421	0.831157511	0.226267793
0.017548634	2.456061547	1.003945759	9.510258161
0.236462416	0.988324147	0.223682679	5.026671536
0.372005149	2.177631629	0.904226065	4.219235789
0.153915709	4.620341653	0.033410743	3.17396006
0.524887329	1.050861084	0.518201484	7.950098612
0.776616937	0.503349512	0.666089868	3.320938471
0.760074361	3.635853456	0.470220952	6.380945175
0.802986662	1.260738545	0.452674872	1.036040804
0.375145127	0.20035625	1.837306306	6.486871565
0.002568896	3.532359526	0.556752154	8.536594244
0.754309276	3.952381767	0.247402168	8.559081716
0.585966577	4.01463047	1.184382133	0.148121669
0.39767356	1.553753452	0.983129422	5.378373676
0.859898623	4.73191381	0.828795696	3.367809329
0.741376169	4.993350692	1.758051281	5.516460988
0.329240391	3.465836416	1.701655508	1.249497907
0.078661064	3.298298811	0.04575857	5.132921426
0.270971873	0.46627043	1.739487411	4.94697541
0.731072625	0.940642982	0.728747166	7.583041122
0.385038046	3.51048946	0.021866584	7.361148458
0.530760376	1.204422978	0.415530715	1.163503483
0.555323667	4.777712592	1.844184811	8.596644394

From rm.tech at mac.com  Tue Jan 26 21:11:58 2016
From: rm.tech at mac.com (R Martinez)
Date: Tue, 26 Jan 2016 12:11:58 -0800
Subject: [R] Confidence Interval for R-squared
Message-ID: <A4D20EBD-CE09-4658-AAF6-E346BBA50C7A@mac.com>

To the list,

Does R have a function for computing the confidence interval of R-squared (coefficient of determination)?

I checked R's Help function with ?confidence interval? as the query but none of the packages and functions that R found pertain to R-squared.

Thanks in advance for your help.

Raul Martinez

From rsherry8 at comcast.net  Tue Jan 26 22:24:27 2016
From: rsherry8 at comcast.net (Robert Sherry)
Date: Tue, 26 Jan 2016 16:24:27 -0500
Subject: [R] Sorting a Data Frame
In-Reply-To: <1A8C1289955EF649A09086A153E2672403D0B9891D@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <56A2C793.6080307@comcast.net>
	<56A2C93D.6090008@statistik.tu-dortmund.de>
	<1A8C1289955EF649A09086A153E2672403D0B9891D@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <56A7E40B.9000800@comcast.net>


Thank  you for the response. As expected, the following expression worked:
     df[order(df$x),]
I would expect the following expression to work also:
         df[order(df$x)]
However it does not. That is, the comma is needed. Please tell me why 
the comma is there.

Thanks
Bob
On 1/26/2016 8:19 AM, S Ellison wrote:
>> On 23.01.2016 01:21, Robert Sherry wrote:
>>> In R, I run the following commands:
>>>       df = data.frame( x=runif(10), y=runif(10) )
>>>       df2 = df[order(x),]
>> You use another x from your workspace, you actually want to
>>
>>
>>    df2 = df[order(df[,"x"]),]
> or
> df[order(df$x),]
>
> And just to prevent yet more confusion, you might also want to avoid 'df' as a name. 'df' is the function that returns the density of the F distribution ...
>
> S Ellison
>
>
>
> *******************************************************************
> This email and any attachments are confidential. Any use...{{dropped:8}}
>
> ______________________________________________
> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sarah.goslee at gmail.com  Tue Jan 26 22:35:56 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 26 Jan 2016 16:35:56 -0500
Subject: [R] Sorting a Data Frame
In-Reply-To: <56A7E40B.9000800@comcast.net>
References: <56A2C793.6080307@comcast.net>
	<56A2C93D.6090008@statistik.tu-dortmund.de>
	<1A8C1289955EF649A09086A153E2672403D0B9891D@GBTEDVPEXCMB04.corp.lgc-group.com>
	<56A7E40B.9000800@comcast.net>
Message-ID: <CAM_vjun5_dvBGLgR76i9awvT5qbinCMwKCCkHHyOw5eqWAdVyw@mail.gmail.com>

On Tue, Jan 26, 2016 at 4:24 PM, Robert Sherry <rsherry8 at comcast.net> wrote:
>
> Thank  you for the response. As expected, the following expression worked:
>     df[order(df$x),]

This says to sort the rows, and leave the columns alone.
Subsetting a 2-dimensional object is via
[rows, columns]

> I would expect the following expression to work also:
>         df[order(df$x)]

This does something a bit unexpected, and what it does depends on
whether you have a data frame or matrix.


> mydf <- data.frame(A=1:3, B=4:6)

> mydf[2, ] # row 2
  A B
2 2 5

> mydf[, 2] # col 2
[1] 4 5 6

> mydf[2]   # ???
  B
1 4
2 5
3 6

A data frame is "really" a list of columns, so giving a single value
returns that column.


> mymat <- as.matrix(mydf)
> mymat[2, ] # row 2
A B
2 5

> mymat[, 2] # col 2
[1] 4 5 6

> mymat[2]   # ???
[1] 2

But for a matrix, it returns that element, starting at the top left
and working down rows first.

So it's a really good idea to not subset your rectangular objects that
way, as it may eventually bite you.


> However it does not. That is, the comma is needed. Please tell me why the
> comma is there.
>
> Thanks
> Bob
> On 1/26/2016 8:19 AM, S Ellison wrote:
>>>
>>> On 23.01.2016 01:21, Robert Sherry wrote:
>>>>
>>>> In R, I run the following commands:
>>>>       df = data.frame( x=runif(10), y=runif(10) )
>>>>       df2 = df[order(x),]
>>>
>>> You use another x from your workspace, you actually want to
>>>
>>>
>>>    df2 = df[order(df[,"x"]),]
>>
>> or
>> df[order(df$x),]
>>
>> And just to prevent yet more confusion, you might also want to avoid 'df'
>> as a name. 'df' is the function that returns the density of the F
>> distribution ...
>>
>> S Ellison
>>
>>


From sarah.goslee at gmail.com  Tue Jan 26 22:38:24 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 26 Jan 2016 16:38:24 -0500
Subject: [R] Confidence Interval for R-squared
In-Reply-To: <A4D20EBD-CE09-4658-AAF6-E346BBA50C7A@mac.com>
References: <A4D20EBD-CE09-4658-AAF6-E346BBA50C7A@mac.com>
Message-ID: <CAM_vjuk5Fv6qmgn1j0Kp724ZqywfaahCw-fNCxm1xianDO97TQ@mail.gmail.com>

Hi Raul,

Searching for "confidence interval of R-squared" on rseek.org turns up
some packages that might be of use, including bootstrap and MBESS.



On Tue, Jan 26, 2016 at 3:11 PM, R Martinez <rm.tech at mac.com> wrote:
> To the list,
>
> Does R have a function for computing the confidence interval of R-squared (coefficient of determination)?
>
> I checked R's Help function with ?confidence interval? as the query but none of the packages and functions that R found pertain to R-squared.
>
> Thanks in advance for your help.
>
> Raul Martinez


From NordlDJ at dshs.wa.gov  Tue Jan 26 22:39:54 2016
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Tue, 26 Jan 2016 21:39:54 +0000
Subject: [R] Confidence Interval for R-squared
In-Reply-To: <A4D20EBD-CE09-4658-AAF6-E346BBA50C7A@mac.com>
References: <A4D20EBD-CE09-4658-AAF6-E346BBA50C7A@mac.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27662EDE86E4@WAXMXOLYMB025.WAX.wa.lcl>

A Google search suggested the use of the boot package to bootstrap a confidence interval for R-squared.

http://www.statmethods.net/advstats/bootstrapping.html

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of R Martinez
> Sent: Tuesday, January 26, 2016 12:12 PM
> To: r-help at r-project.org
> Subject: [R] Confidence Interval for R-squared
> 
> To the list,
> 
> Does R have a function for computing the confidence interval of R-squared
> (coefficient of determination)?
> 
> I checked R's Help function with ?confidence interval? as the query but none
> of the packages and functions that R found pertain to R-squared.
> 
> Thanks in advance for your help.
> 
> Raul Martinez
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From dcarlson at tamu.edu  Tue Jan 26 22:40:08 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 26 Jan 2016 21:40:08 +0000
Subject: [R] Confidence Interval for R-squared
In-Reply-To: <A4D20EBD-CE09-4658-AAF6-E346BBA50C7A@mac.com>
References: <A4D20EBD-CE09-4658-AAF6-E346BBA50C7A@mac.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6FDD6D@mb02.ads.tamu.edu>

Sometimes Google just works better. My search on "confidence interval R2 R" turned up several hits on the first two pages. 

A Quick-R page on bootstrapping with an example using R-squared
http://www.statmethods.net/advstats/bootstrapping.html

Function ci.R2() in package MBESS
http://www.inside-r.org/packages/cran/MBESS/docs/ci.R2

Functions CI.Rsq() and CI.Rsqlm() in package psychometric
http://finzi.psych.upenn.edu/library/psychometric/html/CI.Rsq.html


-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of R Martinez
Sent: Tuesday, January 26, 2016 2:12 PM
To: r-help at r-project.org
Subject: [R] Confidence Interval for R-squared

To the list,

Does R have a function for computing the confidence interval of R-squared (coefficient of determination)?

I checked R's Help function with ?confidence interval? as the query but none of the packages and functions that R found pertain to R-squared.

Thanks in advance for your help.

Raul Martinez
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From bgunter.4567 at gmail.com  Wed Jan 27 04:53:53 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 26 Jan 2016 19:53:53 -0800
Subject: [R] Sorting a Data Frame
In-Reply-To: <CAM_vjun5_dvBGLgR76i9awvT5qbinCMwKCCkHHyOw5eqWAdVyw@mail.gmail.com>
References: <56A2C793.6080307@comcast.net>
	<56A2C93D.6090008@statistik.tu-dortmund.de>
	<1A8C1289955EF649A09086A153E2672403D0B9891D@GBTEDVPEXCMB04.corp.lgc-group.com>
	<56A7E40B.9000800@comcast.net>
	<CAM_vjun5_dvBGLgR76i9awvT5qbinCMwKCCkHHyOw5eqWAdVyw@mail.gmail.com>
Message-ID: <CAGxFJbTomU9x0Ub+SWSL=SV2GEPrrgjpKUOg6=Y8kyiBOo7PYA@mail.gmail.com>

...

> mydf[2]   # ???
  B
1 4
2 5
3 6

A data frame is "really" a list of columns, so giving a single value
returns that column.

False. It returns a data frame consisting of a single column = a list
containing a single component.

mydf[[2]]
  returns a single component/column.

While these differences may seem subtle, they are essential (I have
certainly suffered bad consequences when I have been careless about
them).

The OP should study ?"[" -- or if that is too dense (it is pretty
dense!)  a suitable R tutorial. Indexing is fundamental to effective
use of R and anyone who needs to make effective use of the language
needs to put in the time to learn. I would say that this is the case
even those who prefer to use the tools provided by Hadley Wickham's
plyR packages or similar tools that may exist in others (e.g.
data.table).

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Jan 26, 2016 at 1:35 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> On Tue, Jan 26, 2016 at 4:24 PM, Robert Sherry <rsherry8 at comcast.net> wrote:
>>
>> Thank  you for the response. As expected, the following expression worked:
>>     df[order(df$x),]
>
> This says to sort the rows, and leave the columns alone.
> Subsetting a 2-dimensional object is via
> [rows, columns]
>
>> I would expect the following expression to work also:
>>         df[order(df$x)]
>
> This does something a bit unexpected, and what it does depends on
> whether you have a data frame or matrix.
>
>
>> mydf <- data.frame(A=1:3, B=4:6)
>
>> mydf[2, ] # row 2
>   A B
> 2 2 5
>
>> mydf[, 2] # col 2
> [1] 4 5 6
>
>> mydf[2]   # ???
>   B
> 1 4
> 2 5
> 3 6
>
> A data frame is "really" a list of columns, so giving a single value
> returns that column.
>
>
>> mymat <- as.matrix(mydf)
>> mymat[2, ] # row 2
> A B
> 2 5
>
>> mymat[, 2] # col 2
> [1] 4 5 6
>
>> mymat[2]   # ???
> [1] 2
>
> But for a matrix, it returns that element, starting at the top left
> and working down rows first.
>
> So it's a really good idea to not subset your rectangular objects that
> way, as it may eventually bite you.
>
>
>> However it does not. That is, the comma is needed. Please tell me why the
>> comma is there.
>>
>> Thanks
>> Bob
>> On 1/26/2016 8:19 AM, S Ellison wrote:
>>>>
>>>> On 23.01.2016 01:21, Robert Sherry wrote:
>>>>>
>>>>> In R, I run the following commands:
>>>>>       df = data.frame( x=runif(10), y=runif(10) )
>>>>>       df2 = df[order(x),]
>>>>
>>>> You use another x from your workspace, you actually want to
>>>>
>>>>
>>>>    df2 = df[order(df[,"x"]),]
>>>
>>> or
>>> df[order(df$x),]
>>>
>>> And just to prevent yet more confusion, you might also want to avoid 'df'
>>> as a name. 'df' is the function that returns the density of the F
>>> distribution ...
>>>
>>> S Ellison
>>>
>>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Wed Jan 27 04:59:50 2016
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 26 Jan 2016 19:59:50 -0800
Subject: [R] R tool downloaded
In-Reply-To: <DUB115-W109AF690EB5A4E064C7B53B97D80@phx.gbl>
Message-ID: <68B3FDF6196.000009F3jrkrideau@inbox.com>

Hi Eman,
If I understand you correctly you want to download and install R?  You can use any CRAN mirror in the list unless the Saudi government blocks the connection.

Generally the advice is to select a CRAN mirror that is near you so perhaps Algeria or Turkey?

John Kane
Kingston ON Canada


> -----Original Message-----
> From: eman_ee2020 at hotmail.com
> Sent: Tue, 26 Jan 2016 20:16:20 +0300
> To: r-help at r-project.org
> Subject: [R] R tool downloaded
> 
> Hi,I am a student and I want download R tool but I did not found my
> country (Saudi Arabia ) in the list of download, can you help me how can
> I download the R tool...
> Best Regards,Eman
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From rhuffman at iastate.edu  Tue Jan 26 22:36:46 2016
From: rhuffman at iastate.edu (Huffman, Ryan)
Date: Tue, 26 Jan 2016 15:36:46 -0600
Subject: [R] carolina function help
Message-ID: <CAFGg26jdVvpT1x_SraW0KkZHBTx2o0yhYY0QstT_fQfwXqhdbQ@mail.gmail.com>

I am looking  for advice from those who have used the carolina function,
notably carolina model=2 for a North Carolina Design II analysis. I have
having difficulty getting the function to perform correctly and would like
an example of a code that someone has gotten to work.  Thanks

-- 
Ryan Huffman
PhD Candidate
Vice President of MCDB-GSO
GPSS Senator
G426 Agronomy Hall
rhuffman at iastate.edu

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Wed Jan 27 12:00:16 2016
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 27 Jan 2016 03:00:16 -0800
Subject: [R] carolina function help
In-Reply-To: <CAFGg26jdVvpT1x_SraW0KkZHBTx2o0yhYY0QstT_fQfwXqhdbQ@mail.gmail.com>
Message-ID: <6C5FC102169.00000C63jrkrideau@inbox.com>

Hi Ryan,
No idea about the problem but have a look at these links for some suggestions on the type of detail that you might want to supply to help people understand the issue and help.

The more specific details of your problem that you can supply the easier it is to make suggestions. Usually supplying some sample data using the dput() function (See ?dput) is most helpful.

Please have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html

John Kane
Kingston ON Canada


> -----Original Message-----
> From: rhuffman at iastate.edu
> Sent: Tue, 26 Jan 2016 15:36:46 -0600
> To: r-help at r-project.org
> Subject: [R] carolina function help
> 
> I am looking  for advice from those who have used the carolina function,
> notably carolina model=2 for a North Carolina Design II analysis. I have
> having difficulty getting the function to perform correctly and would
> like
> an example of a code that someone has gotten to work.  Thanks
> 
> --
> Ryan Huffman
> PhD Candidate
> Vice President of MCDB-GSO
> GPSS Senator
> G426 Agronomy Hall
> rhuffman at iastate.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From dcarlson at tamu.edu  Wed Jan 27 16:19:18 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 27 Jan 2016 15:19:18 +0000
Subject: [R] carolina function help
In-Reply-To: <6C5FC102169.00000C63jrkrideau@inbox.com>
References: <CAFGg26jdVvpT1x_SraW0KkZHBTx2o0yhYY0QstT_fQfwXqhdbQ@mail.gmail.com>
	<6C5FC102169.00000C63jrkrideau@inbox.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6FDF91@mb02.ads.tamu.edu>

In addition to John's advice, we need to know what package you are using and what error messages you are getting since "perform correctly" is pretty vague. I'm guessing you are using package agricolae. Assuming you have installed the package and loaded it, your request for working code is included on the manual page for function carolina() as the second example:

> library(agricolae)         # load package
> ?carolina                  # to get the manual page
> example(carolina)          # to run the examples
# ============== running examples =======================
caroln> library(agricolae)

caroln> data(DC)

caroln> carolina1 <- DC$carolina1

caroln> # str(carolina1)
caroln> output<-carolina(model=1,carolina1) # <====== Model 1
Response(y):  yield 

Analysis of Variance Table

Response: y
                            Df  Sum Sq Mean Sq F value    Pr(>F)
set                          1  0.5339  0.5339  7.2120 0.0099144
set:replication              2  2.9894  1.4947 20.1914 4.335e-07
set:male                     4 22.1711  5.5428 74.8743 < 2.2e-16
set:male:female              6  4.8250  0.8042 10.8630 1.311e-07
set:replication:male:female 10  3.2072  0.3207  4.3325 0.0002462
Residuals                   48  3.5533  0.0740                  
                               
set                         ** 
set:replication             ***
set:male                    ***
set:male:female             ***
set:replication:male:female ***
Residuals                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

CV: 8.286715    Mean: 3.283333 

caroln> output[][-1]
$var.m
[1] 0.3948843

$var.f
[1] 0.08057407

$var.A
[1] 1.579537

$var.D
[1] -1.257241

caroln> carolina2 <- DC$carolina2

caroln> # str(carolina2)
caroln> majes<-subset(carolina2,carolina2[,1]==1)

caroln> majes<-majes[,c(2,5,4,3,6:8)]

caroln> output<-carolina(model=2,majes[,c(1:4,6)]) # <==== Model 2
Response(y):  yield 

Analysis of Variance Table

Response: y
                Df  Sum Sq Mean Sq F value    Pr(>F)    
set              1  847836  847836 45.6296 1.097e-09 ***
set:replication  4  144345   36086  1.9421  0.109652    
set:male         8  861053  107632  5.7926 5.032e-06 ***
set:female       8  527023   65878  3.5455  0.001227 ** 
set:male:female 32  807267   25227  1.3577  0.129527    
Residuals       96 1783762   18581                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

CV: 19.08779    Mean: 714.1301 

caroln> output[][-1]
$var.m
[1] 2746.815

$var.f
[1] 1355.024

$var.mf
[1] 2215.415

$var.Am
[1] 10987.26

$var.Af
[1] 5420.096

$var.D
[1] 8861.659

caroln> carolina3 <- DC$carolina3

caroln> # str(carolina3)
caroln> output<-carolina(model=3,carolina3)  # <==== Model 3
Response(y):  yield 

Analysis of Variance Table

Response: y
                Df Sum Sq Mean Sq F value   Pr(>F)   
set              3  2.795 0.93167  1.2784 0.300965   
set:replication  4  3.205 0.80125  1.0995 0.376215   
set:female       4  1.930 0.48250  0.6621 0.623525   
set:male        12 20.970 1.74750  2.3979 0.027770 * 
set:female:male 12 27.965 2.33042  3.1978 0.005493 **
Residuals       28 20.405 0.72875                    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

CV: 21.95932    Mean: 3.8875 

caroln> output[][-1]
$var.mi
[1] 0.8008333

$var.m
[1] 0.2546875

$var.A
[1] 1.01875

$var.D
[1] 1.601667


-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John Kane
Sent: Wednesday, January 27, 2016 5:00 AM
To: Huffman Ryan; r-help at r-project.org
Subject: Re: [R] carolina function help

Hi Ryan,
No idea about the problem but have a look at these links for some suggestions on the type of detail that you might want to supply to help people understand the issue and help.

The more specific details of your problem that you can supply the easier it is to make suggestions. Usually supplying some sample data using the dput() function (See ?dput) is most helpful.

Please have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html

John Kane
Kingston ON Canada


> -----Original Message-----
> From: rhuffman at iastate.edu
> Sent: Tue, 26 Jan 2016 15:36:46 -0600
> To: r-help at r-project.org
> Subject: [R] carolina function help
> 
> I am looking  for advice from those who have used the carolina function,
> notably carolina model=2 for a North Carolina Design II analysis. I have
> having difficulty getting the function to perform correctly and would
> like
> an example of a code that someone has gotten to work.  Thanks
> 
> --
> Ryan Huffman
> PhD Candidate
> Vice President of MCDB-GSO
> GPSS Senator
> G426 Agronomy Hall
> rhuffman at iastate.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From JSorkin at grecc.umaryland.edu  Wed Jan 27 17:51:07 2016
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 27 Jan 2016 11:51:07 -0500
Subject: [R] normalmixEM gives widely divergent results.
Message-ID: <56A8AF2B020000CB00147F82@smtp1.medicine.umaryland.edu>

I am running normalmixEM:
mixmdlscaled <- normalmixEM(data$FCWg)
summary(mixmdlscaled)
plot(mixmdlscaled,which=2)
 
If I run the program multiple times, I get widely different results:
 
> mixmdlscaled <- normalmixEM(data$FCWg)
number of iterations= 41 
> summary(mixmdlscaled)
summary of normalmixEM object:
          comp 1   comp 2
lambda 0.0818928 0.918107
mu     0.6575938 0.740870
sigma  0.0070562 0.178410
loglik at estimate:  56.87445 
> plot(mixmdlscaled,which=2)
> mixmdlscaled <- normalmixEM(data$FCWg)
number of iterations= 357 
> summary(mixmdlscaled)
summary of normalmixEM object:
         comp 1    comp 2
lambda 0.959912 0.0400879
mu     0.722022 1.0220719
sigma  0.165454 0.0131391
loglik at estimate:  53.66051 
> plot(mixmdlscaled,which=2)

 
 
I understand that when run without specifying various parameters (e.g. mu, or sigma) values are chosen randomly from a normal distribution with center(s) determined from binning the data. Despite this, would not one expect the results to be similar? If one is not to expect similar results, how can I get a solution in which I can have confidence? Should I run the program multiple times and take the average of the results? Should I look for the solution with the best log likelihood?  
 
Thank you,
John
John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From wdunlap at tibco.com  Wed Jan 27 18:36:38 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 27 Jan 2016 09:36:38 -0800
Subject: [R] normalmixEM gives widely divergent results.
In-Reply-To: <56A8AF2B020000CB00147F82@smtp1.medicine.umaryland.edu>
References: <56A8AF2B020000CB00147F82@smtp1.medicine.umaryland.edu>
Message-ID: <CAF8bMcaxvD-yELqS1po=yHh6MqejFmjGmaqXciUKDpWPT0mqBw@mail.gmail.com>

You could start by sorting the components, by lambda (size) or by mu (mean)
since, if you don't supply starting values, the order of the components is
random.  You could use the following to sort normalmixEM's output:

sort.mixEM <- function (x, decreasing = FALSE, ..., by = "lambda")
{
    stopifnot(inherits(x, "mixEM"), is.element(by, names(x)))
    o <- order(x[[by]], decreasing = decreasing)
    x$lambda <- x$lambda[o]
    x$sigma <- x$sigma[o]
    x$mu <- x$mu[o]
    x$posterior[] <- x$posterior[, o, drop = FALSE]
    x
}


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Jan 27, 2016 at 8:51 AM, John Sorkin <JSorkin at grecc.umaryland.edu>
wrote:

> I am running normalmixEM:
> mixmdlscaled <- normalmixEM(data$FCWg)
> summary(mixmdlscaled)
> plot(mixmdlscaled,which=2)
>
> If I run the program multiple times, I get widely different results:
>
> > mixmdlscaled <- normalmixEM(data$FCWg)
> number of iterations= 41
> > summary(mixmdlscaled)
> summary of normalmixEM object:
>           comp 1   comp 2
> lambda 0.0818928 0.918107
> mu     0.6575938 0.740870
> sigma  0.0070562 0.178410
> loglik at estimate:  56.87445
> > plot(mixmdlscaled,which=2)
> > mixmdlscaled <- normalmixEM(data$FCWg)
> number of iterations= 357
> > summary(mixmdlscaled)
> summary of normalmixEM object:
>          comp 1    comp 2
> lambda 0.959912 0.0400879
> mu     0.722022 1.0220719
> sigma  0.165454 0.0131391
> loglik at estimate:  53.66051
> > plot(mixmdlscaled,which=2)
>
>
>
> I understand that when run without specifying various parameters (e.g. mu,
> or sigma) values are chosen randomly from a normal distribution with
> center(s) determined from binning the data. Despite this, would not one
> expect the results to be similar? If one is not to expect similar results,
> how can I get a solution in which I can have confidence? Should I run the
> program multiple times and take the average of the results? Should I look
> for the solution with the best log likelihood?
>
> Thank you,
> John
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:16}}


From maitra.mbox.ignored at inbox.com  Wed Jan 27 19:07:01 2016
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Wed, 27 Jan 2016 12:07:01 -0600
Subject: [R] normalmixEM gives widely divergent results.
In-Reply-To: <56A8AF2B020000CB00147F82@smtp1.medicine.umaryland.edu>
References: <56A8AF2B020000CB00147F82@smtp1.medicine.umaryland.edu>
Message-ID: <20160127120701.c0f6d1877f0f8c01db59009b@inbox.com>


On Wed, 27 Jan 2016 11:51:07 -0500 John Sorkin <JSorkin at grecc.umaryland.edu> wrote:

> I am running normalmixEM:
> mixmdlscaled <- normalmixEM(data$FCWg)
> summary(mixmdlscaled)
> plot(mixmdlscaled,which=2)
>  
> If I run the program multiple times, I get widely different results:
>  
> > mixmdlscaled <- normalmixEM(data$FCWg)
> number of iterations= 41 
> > summary(mixmdlscaled)
> summary of normalmixEM object:
>           comp 1   comp 2
> lambda 0.0818928 0.918107
> mu     0.6575938 0.740870
> sigma  0.0070562 0.178410
> loglik at estimate:  56.87445 
> > plot(mixmdlscaled,which=2)
> > mixmdlscaled <- normalmixEM(data$FCWg)
> number of iterations= 357 
> > summary(mixmdlscaled)
> summary of normalmixEM object:
>          comp 1    comp 2
> lambda 0.959912 0.0400879
> mu     0.722022 1.0220719
> sigma  0.165454 0.0131391
> loglik at estimate:  53.66051 
> > plot(mixmdlscaled,which=2)
> 
>  
>  
> I understand that when run without specifying various parameters (e.g. mu, or sigma) values are chosen randomly from a normal distribution with center(s) determined from binning the data. 

I don't know what this means or what the mechanics are.

> Despite this, would not one expect the results to be similar? If one is not to expect similar results, how can I get a solution in which I can have confidence? Should I run the program multiple times and take the average of the results? Should I look for the solution with the best log likelihood?  

But if a likelihood has several local maxima wrt its parameters, isn't this what you would expect? I don't know how familiar you are with statistics so maybe I am repeating something that you already know, but a MLE (note the indefinite article) is what is found by the EM or any iterative/root-finding method in the vicinity of its initialization. 

Your best best is to use a package such as EMCluster. If you want to use the above package, you should make several runs and then choose the one which gives a stable solution and the highest loglikelihood value. EMCluster does it for you.

HTH!

Best wishes,
Ranjan



> Thank you,
> John
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:26}}


From erwfili at gmail.com  Wed Jan 27 12:56:28 2016
From: erwfili at gmail.com (Erofili Grapsa)
Date: Wed, 27 Jan 2016 13:56:28 +0200
Subject: [R] rpart and survey weights
Message-ID: <CAPHuefgVS4xZ=H75eSMHVviy9HbppnM4Bd=sFz_-3kUu8Bxjog@mail.gmail.com>

Dear R users

I have a question regarding rpart and survey weights. In the introduction
to rpart document it says "Weights are not yet supported, and will be
ignored if present", however they are somehow used as the results are
different with and without weights. Can weights now be used and if yes,
what kind of weights? Can survey weights be used safely? These are my
results with weights:
Classification tree:
rpart(formula = cl2m ~ age + day + Employed + media + geo + soclass +
    persinc + hhsizeM + nfadult + nmadult + childshM, data = tum,
    weights = tum$pweight, method = "class", control = rpart.control(xval =
10,
        minbucket = 2, cp = 0))

Variables actually used in tree construction:
[1] age      day      Employed geo      hhsizeM  media    soclass

Root node error: 11950440/16768 = 712.69

n= 16768

         CP nsplit rel error  xerror       xstd
1 0.1980770      0   1.00000 1.00000 0.00016997
2 0.1405072      1   0.80192 0.80192 0.00017852
3 0.0300841      2   0.66142 0.66142 0.00017714
4 0.0053155      3   0.63133 0.63133 0.00017604
5 0.0025728      4   0.62602 0.62819 0.00017591
6 0.0020625      6   0.62087 0.62326 0.00017570
7 0.0020000      9   0.61468 0.62233 0.00017566

and without weights:

Classification tree:
rpart(formula = cl2m ~ age + day + Employed + media + geo + soclass +
    persinc + hhsizeM + nfadult + nmadult + childshM, data = tum,
    method = "class", control = rpart.control(xval = 10, minbucket = 2,
        cp = 0))

Variables actually used in tree construction:
[1] age      day      Employed media

Root node error: 10954/16768 = 0.65327

n= 16768

        CP nsplit rel error  xerror      xstd
1 0.192624      0   1.00000 1.00000 0.0056261
2 0.157020      1   0.80738 0.80738 0.0059018
3 0.030856      2   0.65036 0.65218 0.0058457
4 0.012872      3   0.61950 0.62050 0.0058038
5 0.002000      4   0.60663 0.60809 0.0057845

Does the root node error make sense when using survey weights? How can I
interpret it?

Regards
Erofili

	[[alternative HTML version deleted]]


From kipi at kt.dtu.dk  Wed Jan 27 10:56:57 2016
From: kipi at kt.dtu.dk (Kim Pilegaard)
Date: Wed, 27 Jan 2016 09:56:57 +0000
Subject: [R] Error : there is no .Internal function 'par'
Message-ID: <D2CE5662.1C9D7%kipi@env.dtu.dk>

I have a function that calls par, it has worked nicely before. Now I get the following error:

Error in par(mar = c(5.1, 4.1, 5.1, 2.1)) :
  there is no .Internal function 'par'

Can anyone give me a hint so solve this problem?

Kim


Kim Pilegaard
Professor
Atmospheric Environment
DTU Environment
Technical University of Denmark
[http://www.dtu.dk/~/media/DTU_Generelt/Andet/DTU_email_logo_01.gif]
Department of Environmental Engineering
Milj?vej
2800 Kgs. LyngbyDenmark
Mobile: +45 4025 6839
kipi at env.dtu.dk<mailto:kipi at env.dtu.dk>
www.env.dtu.dk<http://www.env.dtu.dk/>




	[[alternative HTML version deleted]]


From ligges at statistik.tu-dortmund.de  Wed Jan 27 23:05:11 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 27 Jan 2016 23:05:11 +0100
Subject: [R] Error : there is no .Internal function 'par'
In-Reply-To: <D2CE5662.1C9D7%kipi@env.dtu.dk>
References: <D2CE5662.1C9D7%kipi@env.dtu.dk>
Message-ID: <56A93F17.8080805@statistik.tu-dortmund.de>



On 27.01.2016 10:56, Kim Pilegaard wrote:
> I have a function that calls par, it has worked nicely before. Now I get the following error:
>
> Error in par(mar = c(5.1, 4.1, 5.1, 2.1)) :
>    there is no .Internal function 'par'
>
> Can anyone give me a hint so solve this problem?


Your R is broken... Try to reinstall.

Best,
Uwe Ligges


> Kim
>
>
> Kim Pilegaard
> Professor
> Atmospheric Environment
> DTU Environment
> Technical University of Denmark
> [http://www.dtu.dk/~/media/DTU_Generelt/Andet/DTU_email_logo_01.gif]
> Department of Environmental Engineering
> Milj?vej
> 2800 Kgs. LyngbyDenmark
> Mobile: +45 4025 6839
> kipi at env.dtu.dk<mailto:kipi at env.dtu.dk>
> www.env.dtu.dk<http://www.env.dtu.dk/>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wdunlap at tibco.com  Wed Jan 27 23:08:03 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 27 Jan 2016 14:08:03 -0800
Subject: [R] Error : there is no .Internal function 'par'
In-Reply-To: <D2CE5662.1C9D7%kipi@env.dtu.dk>
References: <D2CE5662.1C9D7%kipi@env.dtu.dk>
Message-ID: <CAF8bMcbN7Mk55KeqvEsrE23X_5tMw0mNAWm3vkLaWG4wJgEv-g@mail.gmail.com>

Did you somehow get a copy of the Splus par function into your
R workspace?

Use the conflicts() function to see if you have something masking par.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Jan 27, 2016 at 1:56 AM, Kim Pilegaard <kipi at kt.dtu.dk> wrote:

> I have a function that calls par, it has worked nicely before. Now I get
> the following error:
>
> Error in par(mar = c(5.1, 4.1, 5.1, 2.1)) :
>   there is no .Internal function 'par'
>
> Can anyone give me a hint so solve this problem?
>
> Kim
>
>
> Kim Pilegaard
> Professor
> Atmospheric Environment
> DTU Environment
> Technical University of Denmark
> [http://www.dtu.dk/~/media/DTU_Generelt/Andet/DTU_email_logo_01.gif]
> Department of Environmental Engineering
> Milj?vej
> 2800 Kgs. LyngbyDenmark
> Mobile: +45 4025 6839
> kipi at env.dtu.dk<mailto:kipi at env.dtu.dk>
> www.env.dtu.dk<http://www.env.dtu.dk/>
>
>
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Wed Jan 27 23:10:41 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 27 Jan 2016 17:10:41 -0500
Subject: [R] Error : there is no .Internal function 'par'
In-Reply-To: <D2CE5662.1C9D7%kipi@env.dtu.dk>
References: <D2CE5662.1C9D7%kipi@env.dtu.dk>
Message-ID: <56A94061.9020908@gmail.com>

On 27/01/2016 4:56 AM, Kim Pilegaard wrote:
> I have a function that calls par, it has worked nicely before. Now I get the following error:
>
> Error in par(mar = c(5.1, 4.1, 5.1, 2.1)) :
>    there is no .Internal function 'par'
>
> Can anyone give me a hint so solve this problem?

You have a copy of par() from some earlier version of R.  Perhaps you 
called edit() on it and saved a copy?  The current one doesn't call 
.Internal.

Duncan Murdoch


From drjimlemon at gmail.com  Wed Jan 27 23:12:00 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 28 Jan 2016 09:12:00 +1100
Subject: [R] Error : there is no .Internal function 'par'
In-Reply-To: <D2CE5662.1C9D7%kipi@env.dtu.dk>
References: <D2CE5662.1C9D7%kipi@env.dtu.dk>
Message-ID: <CA+8X3fWC666ye07cF68TxBxXio_5nMPrGxrVy2THPTGcUv-ybA@mail.gmail.com>

Hi Kim,
The only thing that I can think of is that some packages that were
previously loaded automatically may now have to be loaded explicitly. Have
you tried adding:

require(graphics)

to your code?

Jim


On Wed, Jan 27, 2016 at 8:56 PM, Kim Pilegaard <kipi at kt.dtu.dk> wrote:

> I have a function that calls par, it has worked nicely before. Now I get
> the following error:
>
> Error in par(mar = c(5.1, 4.1, 5.1, 2.1)) :
>   there is no .Internal function 'par'
>
> Can anyone give me a hint so solve this problem?
>
> Kim
>
>
> Kim Pilegaard
> Professor
> Atmospheric Environment
> DTU Environment
> Technical University of Denmark
> [http://www.dtu.dk/~/media/DTU_Generelt/Andet/DTU_email_logo_01.gif]
> Department of Environmental Engineering
> Milj?vej
> 2800 Kgs. LyngbyDenmark
> Mobile: +45 4025 6839
> kipi at env.dtu.dk<mailto:kipi at env.dtu.dk>
> www.env.dtu.dk<http://www.env.dtu.dk/>
>
>
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kcarr2001 at gmail.com  Wed Jan 27 23:39:14 2016
From: kcarr2001 at gmail.com (Kristin Bornstein)
Date: Wed, 27 Jan 2016 17:39:14 -0500
Subject: [R] R-help with markovchain package and sensitivity analysis
Message-ID: <CAGmi8Ttgj59yuxAbNwZHxFNuMTfKBQHiVfwi1XBoj49pZv8-pw@mail.gmail.com>

Hi - I'm currently trying to work with a markov chain transition matrix
developed using the markovchain package (it's a 42x42 matrix model).  I've
got the matrix up and running smoothly with no problems.
However, I'm trying to run some sensitivity analyses and distribution
analyses on the model, and I'm having some trouble.  I thought that using
latin hypercube sampling (package 'lhs') and/or partial rank correlation
coefficients (package 'sensitivity', PRCC) would be a good route to go, but
I'm having trouble implementing them with the markov chain I already have
programmed.
Does anyone have any experience running these kinds of analyses and have
any advice?  Specifically how to implement them on a markov model?

Thanks in advance for any help.

	[[alternative HTML version deleted]]


From rvallian at umd.edu  Thu Jan 28 00:38:14 2016
From: rvallian at umd.edu (Richard L. Valliant)
Date: Wed, 27 Jan 2016 23:38:14 +0000
Subject: [R] randomForest and factor predictors--unexpected results
Message-ID: <F0662356E983264DA8105E7390D880B986ABB882@OITMX1003.AD.UMD.EDU>

I'm been experimenting with the randomForest R package (v. 4.6-12) and getting an unexpected difference between rpart and randomForest results that may have something to do with using x's that are factors.  

The same model (see code below) is used to predict a 2-value variable called "resp" that is treated as a factor.  Four x's are used that are factors.

The rpart predicted probabilities average to the same as mean(resp) when used on the full dataset.  This seems OK.  
The randomForest predicted probabilities average is quite a bit different from mean(resp).  This seems unexpected since random forests amount to repeatedly doing variations of what rpart does.

Has anyone seen anything like this or see what I am doing wrong?

(I did the same comparison using the kyphosis dataset in rpart with all continuous predictors and found consistent average predicted probabilities between rpart and randomForest.)

Here's the code ... 

require(PracTools)	# R package with dataset used
require(rpart)
require(randomForest)

data(nhis)  # dataset in PracTools
table(nhis$resp)/nrow(nhis)
#        0         1
#0.3098952 0.6901048

t1 <- rpart(resp ~ age + as.factor(hisp) + as.factor(race) + as.factor(parents_r) + as.factor(educ_r),
      method = "class",
      control = rpart.control(minbucket = 50, cp=0),
      data = nhis)
rpart.prob <- predict(object = t1, newdata = nhis, type = "prob")
apply(rpart.prob,2,mean)
#        0         1
#0.3098952 0.6901048    mean of rpart predictions same as mean(resp)

rf.nhis <- randomForest(as.factor(resp) ~ age + as.factor(hisp) + as.factor(race)
                        + as.factor(parents_r) + as.factor(educ_r),
                    importance = TRUE, na.action = na.omit, mtry=5,
                    ntree = 1000, classwt = c(0.31, 0.69),
                        # cycled through mtry =1,...,5; the lower mtry is, the worse are the predicted probs
                    data = nhis)
rfnhis.prob <- predict(object = rf.nhis, newdata = nhis, type = "prob")
apply(rfnhis.prob,2,mean)
#        0         1
#0.2485541 0.7514459    not too close to mean(resp)

R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1
randomForest_4.6-12

Thanks for any help,
Richard Valliant
Universities of Maryland and Michigan


From sairamhello0328 at gmail.com  Thu Jan 28 02:41:37 2016
From: sairamhello0328 at gmail.com (Baba s)
Date: Wed, 27 Jan 2016 20:41:37 -0500
Subject: [R] Read all the 332 files and count the number of complete rows
Message-ID: <CAO0B0A+Dt5==vrJq7yLwq4yx+1+oV6mU=RKem-w5VAg8UEOefA@mail.gmail.com>

Hi,

I want this function to read all the 332 files and count the number of
complete rows
in the data. When I run this function, I am getting only the last one.
It should give me id and rowcount for 332 files in one vector.

What am I doing wrong in this code?

 complete<-function(id=1:332){
  all_files<-list.files()
  for (i in id){
  ncases<-nrow(na.omit(read.csv(all_files[i])))
  idcase<-cbind(id,ncases)}
}

Regards,
Sai.

	[[alternative HTML version deleted]]


From andre_mikulec at hotmail.com  Thu Jan 28 02:54:34 2016
From: andre_mikulec at hotmail.com (Andre Mikulec)
Date: Wed, 27 Jan 2016 20:54:34 -0500
Subject: [R] How do I parse embedded quotes?
In-Reply-To: <56A7ACF7.206@gmail.com>
References: <BLU174-W4462A0A2E13EAA0163C8869CD80@phx.gbl>,
	<56A7ACF7.206@gmail.com>
Message-ID: <BLU174-W1836E2D7EE87C0DF9EEE99CDA0@phx.gbl>


Thanks,
That jogged my my brain.

Then, I realized that I really want to parse this.

> "\"c(list(\"c('a','b'),c('c','d'))\")\""
[1] "\"c(list(\"c('a','b'),c('c','d'))\")\""

Then, I remembered that parse() has an opposite: deparse().
So I started to work backwards.

I ended up with this.

> deparse(deparse("\"c(list(\"c('a','b'),c('c','d'))\")\""))
[1] "\"\\\"\\\\\\\"c(list(\\\\\\\"c('a','b'),c('c','d'))\\\\\\\")\\\\\\\"\\\"\""

That is too hot to handle.
I will re-start programming in a different direction.

But, again, thanks for the help.

Andre Mikulec
Andre_Mikulec at Hotmail.com




----------------------------------------
> Subject: Re: [R] How do I parse embedded quotes?
> To: andre_mikulec at hotmail.com; r-help at r-project.org
> From: murdoch.duncan at gmail.com
> Date: Tue, 26 Jan 2016 12:29:27 -0500
>
> On 26/01/2016 10:44 AM, Andre Mikulec wrote:
>> I want embedded quotes to be parsed. I can not figure out hot to do that. I want to this to be parsable.
>>
>> "'"a"'"
>>
>> In a simpler case, "'a'" is parsable.
>>
>>> eval(parse(text="\"'a'\""))
>> [1] "'a'"
>>
>>
>> But I want to parse embedded quotes
>> So, I want to parse '"a"' and not just 'a'
>>
>>> eval(parse(text="\"'\"a\"'\""))
>> Error in parse(text = "\"'\"a\"'\"") : <text>:1:4: unexpected symbol
>> 1: "'"a
>>
>>
>> I keep getting that(above) error.
>> Any ideas would much be appreciated.
>
> It is usually helpful to print strings using cat(), so that the escapes
> and quotes around it don't show. With your string that gives the
> error, I see
>
> "'"a"'"
>
> That's not legal R source, because the double quotes that are within the
> string are not escaped. To be legal R code you would need
>
> "'\"a\"'"
>
> and to get that you need the original string to look like
>
> "\"'\\\"a\\\"'\""
>
> Duncan Murdoch
 		 	   		  

From sowmiyan0508 at gmail.com  Thu Jan 28 06:13:00 2016
From: sowmiyan0508 at gmail.com (sowmiyan)
Date: Thu, 28 Jan 2016 10:43:00 +0530
Subject: [R] Any packages similar to XMLmapper interface of SAS in R??
Message-ID: <CAF7+Lw=YqyAN7gKhcYKiL3qOB0ueAxftbpkKM78nfhtXh+_JgA@mail.gmail.com>

Can anybody provide me with some pointers to some package or code snippets
similar to that of XMLmapper interface of SAS.

XMLmapper reads a xml file and derives by it own "Parent Child relation"
"Child Grandchildren relation" and so on and provide separate csv files for
each hierarchy drill down.


Thanks,
Sowmiyan

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Thu Jan 28 07:41:57 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 28 Jan 2016 06:41:57 +0000
Subject: [R] Read all the 332 files and count the number of complete rows
In-Reply-To: <CAO0B0A+Dt5==vrJq7yLwq4yx+1+oV6mU=RKem-w5VAg8UEOefA@mail.gmail.com>
References: <CAO0B0A+Dt5==vrJq7yLwq4yx+1+oV6mU=RKem-w5VAg8UEOefA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C500B835@SRVEXCHMBX.precheza.cz>

Hi

Just to clarify.
Both id and and ncases are numbers. You say you want a vector. In that case just plain

idcase<-c(idcase,ncases)

shall give you desired vector, provided you initialize idcase before cycle by

idcase <- NA

The code itself is suboptimal but if you have only couple of files it does not matter much.

If you wanted some other result please explain it by dput(expectedresult)

Cheers
Petr

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Baba s
Sent: Thursday, January 28, 2016 2:42 AM
To: r-help at r-project.org
Subject: [R] Read all the 332 files and count the number of complete rows

Hi,

I want this function to read all the 332 files and count the number of complete rows in the data. When I run this function, I am getting only the last one.
It should give me id and rowcount for 332 files in one vector.

What am I doing wrong in this code?

 complete<-function(id=1:332){
  all_files<-list.files()
  for (i in id){
  ncases<-nrow(na.omit(read.csv(all_files[i])))
  idcase<-cbind(id,ncases)}
}

Regards,
Sai.

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From dwinsemius at comcast.net  Thu Jan 28 07:58:28 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 27 Jan 2016 22:58:28 -0800
Subject: [R] Read all the 332 files and count the number of complete rows
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C500B835@SRVEXCHMBX.precheza.cz>
References: <CAO0B0A+Dt5==vrJq7yLwq4yx+1+oV6mU=RKem-w5VAg8UEOefA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C500B835@SRVEXCHMBX.precheza.cz>
Message-ID: <7D553BE6-36AF-42A7-9240-259B3C468227@comcast.net>

The number 332 suggests that this is homework from Peng's online class. Isn't there there a website  for questions regarding these exercises?

I have observed a fair number of similar question on StackOverflow over the years so you might also search there, since your error is a fairly common one. Homework on StackOverflow is not deprecated if the question demonstrates effort and a good description of the difficulty (both of which I do see.)

After completing your course, you may want to read the Posting Guide, subscribe to rhelp, but please do use plain text.

-- David.

> On Jan 27, 2016, at 10:41 PM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> 
> Hi
> 
> Just to clarify.
> Both id and and ncases are numbers. You say you want a vector. In that case just plain
> 
> idcase<-c(idcase,ncases)
> 
> shall give you desired vector, provided you initialize idcase before cycle by
> 
> idcase <- NA
> 
> The code itself is suboptimal but if you have only couple of files it does not matter much.
> 
> If you wanted some other result please explain it by dput(expectedresult)
> 
> Cheers
> Petr
> 
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Baba s
> Sent: Thursday, January 28, 2016 2:42 AM
> To: r-help at r-project.org
> Subject: [R] Read all the 332 files and count the number of complete rows
> 
> Hi,
> 
> I want this function to read all the 332 files and count the number of complete rows in the data. When I run this function, I am getting only the last one.
> It should give me id and rowcount for 332 files in one vector.
> 
> What am I doing wrong in this code?
> 
> complete<-function(id=1:332){
>  all_files<-list.files()
>  for (i in id){
>  ncases<-nrow(na.omit(read.csv(all_files[i])))
>  idcase<-cbind(id,ncases)}
> }
> 
> Regards,
> Sai.
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> 

David Winsemius
Alameda, CA, USA


From highstat at highstat.com  Thu Jan 28 11:34:30 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 28 Jan 2016 10:34:30 +0000
Subject: [R] Introduction to GAM and GAMM with R course
Message-ID: <56A9EEB6.6080300@highstat.com>

There are various remaining seats available on the following course


Course: Introduction to GAM and GAMM
Where:  University of Konstanz, Konstanz, Germany
When:   7-11 March 2016

Course website: http://www.highstat.com/statscourse.htm
Course flyer: http://highstat.com/Courses/Flyer2016_03Konstanz.pdf



Kind regards,

Alain Zuur



-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From daphnewar at gmail.com  Thu Jan 28 07:22:13 2016
From: daphnewar at gmail.com (DAPHNE WAR)
Date: Thu, 28 Jan 2016 11:52:13 +0530
Subject: [R] Error while integrating R with netbeans using rJava
Message-ID: <CAAJjxc8c_irsj2kqHiSPLkScWXPOeYuJ+3NG7J1vNyb6jbrRaQ@mail.gmail.com>

Could not find or load main class Files\R\R-3.2.3\library\rJava\jri
I encounter the above problem .Pls help


From ulrik.stervbo at gmail.com  Thu Jan 28 14:08:23 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Thu, 28 Jan 2016 13:08:23 +0000
Subject: [R] Error while integrating R with netbeans using rJava
In-Reply-To: <CAAJjxc8c_irsj2kqHiSPLkScWXPOeYuJ+3NG7J1vNyb6jbrRaQ@mail.gmail.com>
References: <CAAJjxc8c_irsj2kqHiSPLkScWXPOeYuJ+3NG7J1vNyb6jbrRaQ@mail.gmail.com>
Message-ID: <CAKVAULO9H39_EaLnKCK4QKqY6yVpwUzptq7XDRcZYUaz3quTwQ@mail.gmail.com>

Hi Daphne,

It seems Java cannot find what it is looking for.

I know next to nothing of using rJava but maybe this is helpful:
http://binfalse.de/2011/02/20/talking-r-through-java/

Best,
Ulrik

On Thu, 28 Jan 2016 at 12:15 DAPHNE WAR <daphnewar at gmail.com> wrote:

> Could not find or load main class Files\R\R-3.2.3\library\rJava\jri
> I encounter the above problem .Pls help
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From simon.urbanek at r-project.org  Thu Jan 28 17:05:00 2016
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 28 Jan 2016 11:05:00 -0500
Subject: [R] Error while integrating R with netbeans using rJava
In-Reply-To: <CAKVAULO9H39_EaLnKCK4QKqY6yVpwUzptq7XDRcZYUaz3quTwQ@mail.gmail.com>
References: <CAAJjxc8c_irsj2kqHiSPLkScWXPOeYuJ+3NG7J1vNyb6jbrRaQ@mail.gmail.com>
	<CAKVAULO9H39_EaLnKCK4QKqY6yVpwUzptq7XDRcZYUaz3quTwQ@mail.gmail.com>
Message-ID: <754A0CA3-3ACE-4405-A49F-3D710930CF1D@r-project.org>

.. this looks like a quoting problem in you classpath - you'll need to quote it properly otherwise the space in your class path gets interpreted as the next argument. 


> On Jan 28, 2016, at 8:08 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
> 
> Hi Daphne,
> 
> It seems Java cannot find what it is looking for.
> 
> I know next to nothing of using rJava but maybe this is helpful:
> http://binfalse.de/2011/02/20/talking-r-through-java/
> 
> Best,
> Ulrik
> 
> On Thu, 28 Jan 2016 at 12:15 DAPHNE WAR <daphnewar at gmail.com> wrote:
> 
>> Could not find or load main class Files\R\R-3.2.3\library\rJava\jri
>> I encounter the above problem .Pls help
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From andersalex at gmail.com  Thu Jan 28 15:48:20 2016
From: andersalex at gmail.com (Anders Alexandersson)
Date: Thu, 28 Jan 2016 09:48:20 -0500
Subject: [R] How to use compare.linkage in RecordLinkage package --
	unexpected output
Message-ID: <CAKJpBt+ruh5j_ojDgjGVy3Vu2CLq29AsgsYFekCGGaS_NoHFPA@mail.gmail.com>

I am using the compare.linkage function in the RecordLinkage package,
and getting a result I know is wrong, so I know I'm misunderstanding
something.
I am using R 3.2.3 for x64 Windows. I am very familar with Stata but not so
much with R.

I can create record pairs from the blocking fields but all pairs are
unknown status (NA).
I cannot create matches or non-matches. I want a simple working example of
how to link datasets using the RecordLinkage package. It seems that the
manual and the R Journal Vol. 2/2 only show how to de-duplicate a single
dataset using the compare.dedup function, not how to link two datasets
together using the compare.linkage function. I can reproduce the examples
in the R Journal article, so my R installation is fine.

The example dataset in the manual have 500 and 10000 observations on 7
variables, but 1 observation and 2 variables will be enough to show the
problem.
My first comparison pattern loooks like this:
  id1  id2 fname_c1 bm is_match
1  17  343        1  1       NA

Instead, I want and expect a comparison pattern that looks like this:
  id1  id2 fname_c1 bm is_match
1  17  343        1  1       1

My blocking variable is fname_c1 for first component of first name. My
matching variable is bm for birth month. My understanding is that row 1 in
my example output is the first row where fname_c1 matched in the underlying
datasets. I want and expect is_match to be 1 when the matching variable
bm=1 in both linkage datasets, as in the example.

For more details, this is what I typed and the R output:
> library(RecordLinkage)
> data(RLdata500)
> data(RLdata10000)
> RLdata500[17, ]
    fname_c1 fname_c2 lname_c1 lname_c2   by bm bd
17 ALEXANDER     <NA>  MUELLER     <NA> 1974  9  9
> RLdata10000[343, ]
     fname_c1 fname_c2 lname_c1 lname_c2   by bm bd
343 ALEXANDER     <NA>  BAUMANN     <NA> 1957  9  7
> rpairs <- compare.linkage(RLdata500,RLdata10000,blockfld=c(1),
exclude=c(2:5,7))
> rpairs$pairs[c(1:2), ] # Why is_match=NA? (should be 1)
  id1  id2 fname_c1 bm is_match
1  17  343        1  1       NA
2  17 2385        1  0       NA
> rpairs <- epiWeights(rpairs) # (Weight calculation)
> summary(rpairs) # (0 matches in Linkage Dataset)

Linkage Data Set

500 records in data set 1
10000 records in data set 2
47890 record pairs

0 matches
0 non-matches
47890 pairs with unknown status


Weight distribution:
[omitted here to save space]

References:
1. Manual for Package ?RecordLinkage?
(Available online at
https://cran.r-project.org/web/packages/RecordLinkage/RecordLinkage.pdf)
2. R Journal article Article "The RecordLinkage Package: Detecting Errors
in Data"
(Available online in PDF at
https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf
)

I saw something in the manual and R journal article about identity argument
for true match results, but I guess I only need that for reference ("gold
standard") datasets. There is a non-missing value (bm=1) for the example in
both underlying datasets, so that is not why the result is NA. What am I
missing? How does one link two simple datasets using compare.linkage?

Anders Alexandersson
andersalex at gmail.com
Florida Cancer Data System, University of Miami

	[[alternative HTML version deleted]]


From andersalex at gmail.com  Thu Jan 28 16:18:44 2016
From: andersalex at gmail.com (Anders Alexandersson)
Date: Thu, 28 Jan 2016 10:18:44 -0500
Subject: [R] How to use compare.linkage in RecordLinkage package --
	unexpected output
Message-ID: <CAKJpBtLYvb18hgPxSRewQkra2Z7a=PWo8DbaJ1VZSO6nwrnkpw@mail.gmail.com>

I am using the compare.linkage function in the RecordLinkage package,
and getting a result I know is wrong, so I know I'm misunderstanding
something.
I am using R 3.2.3 for x64 Windows. I am very familar with Stata but not so
much with R.

I can create record pairs from the blocking fields but all pairs are
unknown status (NA).
I cannot create matches or non-matches. I want a simple working example of
how to link datasets using the RecordLinkage package. It seems that the
manual and the R Journal Vol. 2/2 only show how to de-duplicate a single
dataset using the compare.dedup function, not how to link two datasets
together using the compare.linkage function. I can reproduce the examples
in the R Journal article, so my R installation is fine.

The example dataset in the manual have 500 and 10000 observations on 7
variables, but 1 observation and 2 variables will be enough to show the
problem.
My first comparison pattern loooks like this:
  id1  id2 fname_c1 bm is_match
1  17  343        1  1       NA

Instead, I want and expect a comparison pattern that looks like this:
  id1  id2 fname_c1 bm is_match
1  17  343        1  1       1

My blocking variable is fname_c1 for first component of first name. My
matching variable is bm for birth month. My understanding is that row 1 in
my example output is the first row where fname_c1 matched in the underlying
datasets. I want and expect is_match to be 1 when the matching variable
bm=1 in both linkage datasets, as in the example.

For more details, this is what I typed and the R output:
> library(RecordLinkage)
> data(RLdata500)
> data(RLdata10000)
> RLdata500[17, ]
    fname_c1 fname_c2 lname_c1 lname_c2   by bm bd
17 ALEXANDER     <NA>  MUELLER     <NA> 1974  9  9
> RLdata10000[343, ]
     fname_c1 fname_c2 lname_c1 lname_c2   by bm bd
343 ALEXANDER     <NA>  BAUMANN     <NA> 1957  9  7
> rpairs <- compare.linkage(RLdata500,RLdata10000,blockfld=c(1),
exclude=c(2:5,7))
> rpairs$pairs[c(1:2), ] # Why is_match=NA? (should be 1)
  id1  id2 fname_c1 bm is_match
1  17  343        1  1       NA
2  17 2385        1  0       NA
> rpairs <- epiWeights(rpairs) # (Weight calculation)
> summary(rpairs) # (0 matches in Linkage Dataset)

Linkage Data Set

500 records in data set 1
10000 records in data set 2
47890 record pairs

0 matches
0 non-matches
47890 pairs with unknown status


Weight distribution:
[omitted here to save space]

References:
1. Manual for Package ?RecordLinkage?
(Available online at
https://cran.r-project.org/web/packages/RecordLinkage/RecordLinkage.pdf)
2. R Journal article Article "The RecordLinkage Package: Detecting Errors
in Data"
(Available online in PDF at
https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf
)

I saw something in the manual and R journal article about identity argument
for true match results, but I guess I only need that for reference ("gold
standard") datasets. There is a non-missing value (bm=1) for my example in
both underlying datasets, so that is not why the result is NA. What am I
missing? How does one link two simple datasets using compare.linkage?

	[[alternative HTML version deleted]]


From gwennael.bataille at uclouvain.be  Thu Jan 28 18:09:53 2016
From: gwennael.bataille at uclouvain.be (=?ISO-8859-1?Q?Gwenna=EBl_Bataille?=)
Date: Thu, 28 Jan 2016 18:09:53 +0100
Subject: [R] Angle between two points with coordinates
Message-ID: <56AA4B61.1000808@uclouvain.be>

Dear all,
I'd like to calculate the angle from one point (origin) to another 
(target), whatever their coordinates.
But I encounter some problems (detailed below). The problem could be 
solved if one of you could answer positively to one of the following 
questions:

1) Is there a function in R converting angles in a standardized manner? 
(for example, converting -150 or 570 (=210+360) into 210)

2) If not, would you know a function arccos or arcsin returning two 
different angles as an output instead of one?



Details:

I'd like to calculate the angle from one point (origin) to another 
(target), whatever their coordinates.
For this, the acos and asin functions work pretty well when the end 
point is located right and above the starting point (first quarter of 
the trigonometric circle), but are problematic otherwise.

# In the following example, the origin is (0,0) and the target 
(0.8660254, 0.5) is located at an angle of 30? :
acos( (0.8660254 - 0) )*180/pi
asin( (0.5 - 0) )*180/pi
# Both acos and asin give the same answer : 30

# If now, the origin is (0.8660254, 0.5) and the target is (0,0), the 
target is located at an angle of -150? :
acos( (0 - 0.8660254) )*180/pi
asin( (0 - 0.5) )*180/pi
# Here the results are different : 150 and -30

# In fact, there are two angle solutions giving the same cosinus : 150 
and -(150)
# And for sinus as well : -30 and ( 180 - (-30) ) = 210? = -150?
-acos( (0 - 0.8660254) )*180/pi
180 - asin( (0 - 0.5) )*180/pi
# But I cannot test equality between the two :
-acos( (0 - 0.8660254) )*180/pi    ==    180 - asin( (0 - 0.5) )*180/pi
# FALSE, since 210 != -150  (it's only the case when those two are angles)


Thank you very much in advance for your answers!

Best regards,


Gwenna?l

-- 
Gwenna?l BATAILLE, PhD student - Teaching assistant

Earth and Life Institute
Universit? Catholique de Louvain
1348 Louvain-la-Neuve
BELGIUM


From jdnewmil at dcn.davis.ca.us  Thu Jan 28 19:38:01 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 28 Jan 2016 10:38:01 -0800
Subject: [R] Angle between two points with coordinates
In-Reply-To: <56AA4B61.1000808@uclouvain.be>
References: <56AA4B61.1000808@uclouvain.be>
Message-ID: <85D2EF82-8E74-43BE-8256-161B0E33A5CE@dcn.davis.ca.us>

Functions return one value. 

Look at ?atan2 to address ambiguity in identifying angles. 
-- 
Sent from my phone. Please excuse my brevity.

On January 28, 2016 9:09:53 AM PST, "Gwenna?l Bataille" <gwennael.bataille at uclouvain.be> wrote:
>Dear all,
>I'd like to calculate the angle from one point (origin) to another 
>(target), whatever their coordinates.
>But I encounter some problems (detailed below). The problem could be 
>solved if one of you could answer positively to one of the following 
>questions:
>
>1) Is there a function in R converting angles in a standardized manner?
>
>(for example, converting -150 or 570 (=210+360) into 210)
>
>2) If not, would you know a function arccos or arcsin returning two 
>different angles as an output instead of one?
>
>
>
>Details:
>
>I'd like to calculate the angle from one point (origin) to another 
>(target), whatever their coordinates.
>For this, the acos and asin functions work pretty well when the end 
>point is located right and above the starting point (first quarter of 
>the trigonometric circle), but are problematic otherwise.
>
># In the following example, the origin is (0,0) and the target 
>(0.8660254, 0.5) is located at an angle of 30? :
>acos( (0.8660254 - 0) )*180/pi
>asin( (0.5 - 0) )*180/pi
># Both acos and asin give the same answer : 30
>
># If now, the origin is (0.8660254, 0.5) and the target is (0,0), the 
>target is located at an angle of -150? :
>acos( (0 - 0.8660254) )*180/pi
>asin( (0 - 0.5) )*180/pi
># Here the results are different : 150 and -30
>
># In fact, there are two angle solutions giving the same cosinus : 150 
>and -(150)
># And for sinus as well : -30 and ( 180 - (-30) ) = 210? = -150?
>-acos( (0 - 0.8660254) )*180/pi
>180 - asin( (0 - 0.5) )*180/pi
># But I cannot test equality between the two :
>-acos( (0 - 0.8660254) )*180/pi    ==    180 - asin( (0 - 0.5) )*180/pi
># FALSE, since 210 != -150  (it's only the case when those two are
>angles)
>
>
>Thank you very much in advance for your answers!
>
>Best regards,
>
>
>Gwenna?l
>
>-- 
>Gwenna?l BATAILLE, PhD student - Teaching assistant
>
>Earth and Life Institute
>Universit? Catholique de Louvain
>1348 Louvain-la-Neuve
>BELGIUM
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Thu Jan 28 20:05:14 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Thu, 28 Jan 2016 11:05:14 -0800
Subject: [R] Angle between two points with coordinates
In-Reply-To: <85D2EF82-8E74-43BE-8256-161B0E33A5CE@dcn.davis.ca.us>
References: <56AA4B61.1000808@uclouvain.be>
	<85D2EF82-8E74-43BE-8256-161B0E33A5CE@dcn.davis.ca.us>
Message-ID: <CAJeYpE8eCfFubgJmVL3GLfQWteNgyzB9jb8bFubEoRbTsQbuEw@mail.gmail.com>

Gwenna?l,

Does the %% operator work for you?

It gives x mod y (or the remainder after dividing x into y...result is
guaranteed to be <=0 and >y)

E.g.

 -150 %% 360      # 210
570 %% 360        # 210

https://stat.ethz.ch/R-manual/R-devel/library/base/html/Arithmetic.html

-Dan

On Thu, Jan 28, 2016 at 10:38 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Functions return one value.
>
> Look at ?atan2 to address ambiguity in identifying angles.
> --
> Sent from my phone. Please excuse my brevity.
>
> On January 28, 2016 9:09:53 AM PST, "Gwenna?l Bataille" <
> gwennael.bataille at uclouvain.be> wrote:
> >Dear all,
> >I'd like to calculate the angle from one point (origin) to another
> >(target), whatever their coordinates.
> >But I encounter some problems (detailed below). The problem could be
> >solved if one of you could answer positively to one of the following
> >questions:
> >
> >1) Is there a function in R converting angles in a standardized manner?
> >
> >(for example, converting -150 or 570 (=210+360) into 210)
> >
> >2) If not, would you know a function arccos or arcsin returning two
> >different angles as an output instead of one?
> >
> >
> >
> >Details:
> >
> >I'd like to calculate the angle from one point (origin) to another
> >(target), whatever their coordinates.
> >For this, the acos and asin functions work pretty well when the end
> >point is located right and above the starting point (first quarter of
> >the trigonometric circle), but are problematic otherwise.
> >
> ># In the following example, the origin is (0,0) and the target
> >(0.8660254, 0.5) is located at an angle of 30? :
> >acos( (0.8660254 - 0) )*180/pi
> >asin( (0.5 - 0) )*180/pi
> ># Both acos and asin give the same answer : 30
> >
> ># If now, the origin is (0.8660254, 0.5) and the target is (0,0), the
> >target is located at an angle of -150? :
> >acos( (0 - 0.8660254) )*180/pi
> >asin( (0 - 0.5) )*180/pi
> ># Here the results are different : 150 and -30
> >
> ># In fact, there are two angle solutions giving the same cosinus : 150
> >and -(150)
> ># And for sinus as well : -30 and ( 180 - (-30) ) = 210? = -150?
> >-acos( (0 - 0.8660254) )*180/pi
> >180 - asin( (0 - 0.5) )*180/pi
> ># But I cannot test equality between the two :
> >-acos( (0 - 0.8660254) )*180/pi    ==    180 - asin( (0 - 0.5) )*180/pi
> ># FALSE, since 210 != -150  (it's only the case when those two are
> >angles)
> >
> >
> >Thank you very much in advance for your answers!
> >
> >Best regards,
> >
> >
> >Gwenna?l
> >
> >--
> >Gwenna?l BATAILLE, PhD student - Teaching assistant
> >
> >Earth and Life Institute
> >Universit? Catholique de Louvain
> >1348 Louvain-la-Neuve
> >BELGIUM
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Jan 28 20:39:47 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 28 Jan 2016 11:39:47 -0800
Subject: [R] Angle between two points with coordinates
In-Reply-To: <56AA4B61.1000808@uclouvain.be>
References: <56AA4B61.1000808@uclouvain.be>
Message-ID: <CAF8bMcZyWs6xg66N32R5E04djVscoAxkOcJy1oyqgWERA19AWw@mail.gmail.com>

In addition to the other fine answers, you might find it convenient
to represent the points as complex numbers and use the Arg function
to get the angle (and abs() or Mod() the distance).

  > z <- complex(real=0.8660254, imaginary=0.5)
  > Arg(z) / base::pi * 180
  [1] 30
  > Arg(-z) / base::pi * 180
  [1] -150



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jan 28, 2016 at 9:09 AM, Gwenna?l Bataille <
gwennael.bataille at uclouvain.be> wrote:

> Dear all,
> I'd like to calculate the angle from one point (origin) to another
> (target), whatever their coordinates.
> But I encounter some problems (detailed below). The problem could be
> solved if one of you could answer positively to one of the following
> questions:
>
> 1) Is there a function in R converting angles in a standardized manner?
> (for example, converting -150 or 570 (=210+360) into 210)
>
> 2) If not, would you know a function arccos or arcsin returning two
> different angles as an output instead of one?
>
>
>
> Details:
>
> I'd like to calculate the angle from one point (origin) to another
> (target), whatever their coordinates.
> For this, the acos and asin functions work pretty well when the end point
> is located right and above the starting point (first quarter of the
> trigonometric circle), but are problematic otherwise.
>
> # In the following example, the origin is (0,0) and the target (0.8660254,
> 0.5) is located at an angle of 30? :
> acos( (0.8660254 - 0) )*180/pi
> asin( (0.5 - 0) )*180/pi
> # Both acos and asin give the same answer : 30
>
> # If now, the origin is (0.8660254, 0.5) and the target is (0,0), the
> target is located at an angle of -150? :
> acos( (0 - 0.8660254) )*180/pi
> asin( (0 - 0.5) )*180/pi
> # Here the results are different : 150 and -30
>
> # In fact, there are two angle solutions giving the same cosinus : 150 and
> -(150)
> # And for sinus as well : -30 and ( 180 - (-30) ) = 210? = -150?
> -acos( (0 - 0.8660254) )*180/pi
> 180 - asin( (0 - 0.5) )*180/pi
> # But I cannot test equality between the two :
> -acos( (0 - 0.8660254) )*180/pi    ==    180 - asin( (0 - 0.5) )*180/pi
> # FALSE, since 210 != -150  (it's only the case when those two are angles)
>
>
> Thank you very much in advance for your answers!
>
> Best regards,
>
>
> Gwenna?l
>
> --
> Gwenna?l BATAILLE, PhD student - Teaching assistant
>
> Earth and Life Institute
> Universit? Catholique de Louvain
> 1348 Louvain-la-Neuve
> BELGIUM
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From andersalex at gmail.com  Thu Jan 28 21:01:52 2016
From: andersalex at gmail.com (Anders Alexandersson)
Date: Thu, 28 Jan 2016 15:01:52 -0500
Subject: [R] How to use compare.linkage in RecordLinkage package? -- more
 details but problem remains
Message-ID: <CAKJpBt+Jdebk3mtwkDC-=Sm0H8O7R36BGkA7L_UUfKZg0Jvj-A@mail.gmail.com>

How does one link two datasets using the compare.linkage function in the
RecordLinkage package? This is to follow-up on my original posting earlier
today:
https://stat.ethz.ch/pipermail/r-help/2016-January/435736.html

I suggested then that I should perhaps have added the identity argument.
But if I add the identity argument, then I unexpectedly get 5 matches,
47885 non-matches and 0 pairs with unknown status. For example, I get a
match for row 4256 which is unexpected because the matching variable bm
does not match -- is 0 in the result pair (because bm is 1 for BERND JUNG
and 4 for BERND MUELLER). Also, is_match in row 1 changes from unknown (NA)
to no match (0) which is unexpected since the matching variable bm matches
(bm=1).

Here are the major new R commands that I ran and the output:
> rpairs <- compare.linkage(RLdata500,RLdata10000,blockfld=c(1),

identity1=identity.RLdata500,identity2=identity.RLdata10000,exclude=c(2:5,7))
> subset(rpairs$pairs, is_match=="1") # Why these 5 matches?
      id1  id2 fname_c1 bm is_match
4256   59 1394        1  0        1
5811  174 3684        1  0        1
14699 139 4199        1  0        1
16453  92 4580        1  0        1
21840  73  737        1  0        1
> RLdata500[c(17, 59), ] # first obs, and first matching obs
    fname_c1 fname_c2 lname_c1 lname_c2   by bm bd
17 ALEXANDER     <NA>  MUELLER     <NA> 1974  9  9
59     BERND     <NA>     JUNG    KLEIN 1935  1 14
> RLdata10000[c(343, 1394), ] # first obs, and first matching obs
      fname_c1 fname_c2 lname_c1 lname_c2   by bm bd
343  ALEXANDER     <NA>  BAUMANN     <NA> 1957  9  7
1394     BERND     <NA>  MUELLER     <NA> 1942  4  4
> rpairs$pairs[1:2, ]; # list first 2 obs
  id1  id2 fname_c1 bm is_match
1  17  343        1  1        0
2  17 2385        1  0        0

What am I missing? How to probabilistically link two datasets using the
compare.linkage function in the RecordLinkage package?

Anders Alexandersson
andersalex at gmail.com

	[[alternative HTML version deleted]]


From manishm at dbs.com  Fri Jan 29 07:05:18 2016
From: manishm at dbs.com (Manish MAHESHWARI)
Date: Fri, 29 Jan 2016 06:05:18 +0000
Subject: [R] Redirect Output to File and Screen in Parallel
Message-ID: <8B5BC7735651764E884E3651BAA48D9A08DAFDC4@W01GMAILDAGA02.reg1.1bank.dbs.com>

Hi,

Using the sink we can redirect the output to sink files set as con.
However is there a way to do both - Have the Op printed on screen and also to the log file?

Thanks,
Manish
CONFIDENTIAL NOTE:
The information contained in this email is intended only...{{dropped:11}}


From dwinsemius at comcast.net  Fri Jan 29 08:42:55 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 28 Jan 2016 23:42:55 -0800
Subject: [R] Redirect Output to File and Screen in Parallel
In-Reply-To: <8B5BC7735651764E884E3651BAA48D9A08DAFDC4@W01GMAILDAGA02.reg1.1bank.dbs.com>
References: <8B5BC7735651764E884E3651BAA48D9A08DAFDC4@W01GMAILDAGA02.reg1.1bank.dbs.com>
Message-ID: <6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>


> On Jan 28, 2016, at 10:05 PM, Manish MAHESHWARI <manishm at dbs.com> wrote:
> 
> Hi,
> 
> Using the sink we can redirect the output to sink files set as con.
> However is there a way to do both - Have the Op printed on screen and also to the log file?

I think this has been asked and answered. Have you done any searching?

> 
> Thanks,
> Manish
> CONFIDENTIAL NOTE:
> The information contained in this email is intended only...{{dropped:11}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From andrluis at ualberta.ca  Fri Jan 29 02:42:34 2016
From: andrluis at ualberta.ca (=?UTF-8?Q?Andr=C3=A9_Luis_Neves?=)
Date: Thu, 28 Jan 2016 18:42:34 -0700
Subject: [R] Two-way cluster
Message-ID: <CAHxKz8b0Nv=QHKDr1PO8M604LmK_VQN50Wj2UaKGHZ=cb19+Vw@mail.gmail.com>

Dear everyone,

I`d like to make a two-way cluster combined with a heatmap, and was
wondering if you could give me a hand.

I have just two variables (x and y). I would like that x variable should be
placed in the x axis and y variable in the y axis. I m running lots of
examples, but unfortunately x and y variables are in the x axis and the
number of order of the samples is in the y. I have removed the number of
order column, but the problem is still there.
What I could do to solve this problem.

I`m using gplots and the function heatmap.2

Thanks,

Andre

	[[alternative HTML version deleted]]


From dmr02004 at gmail.com  Fri Jan 29 07:10:57 2016
From: dmr02004 at gmail.com (David Roy)
Date: Thu, 28 Jan 2016 22:10:57 -0800
Subject: [R] Multilevel Modeling in R
Message-ID: <CAPwiEwWWu4V2NrZ7ema-5dZ2WiiChWzWtwYpEyVrt+0F-zVdHw@mail.gmail.com>

I am conducting a multilevel regression analysis on the effect of an
intervention on student test results, and am not sure how to implement the
necessary R code to correctly capture the nested structure.



The outcome measure for the study is whether a student passed or failed a
final exam.  The structure of the data is students nested within schools,
and then schools nested within random assignment blocks.  Treatment (i.e.,
the intervention) was implemented at the school-level.  The covariates that
I am planning to use are prior year test scores (this is also a binary
variable for pass or fail), race, and gender.



My ideal output would show the impact of the treatment for each of the
random assignment blocks, and then the weighted average of the impact
across all of the random assignment blocks.



Based on my research thus far, it seems like the **lmer** function from the
**lme4** package would be the best route to go.



This is the code that I have tried:



    # Fit multilevel regression with random assignment blocks

    glmer2 <- glmer(Post_Test_Score ~ Treatment +

                                      Pre_Test_Score +

                                      (1 | School) +

                                      (1 | Random_Assignment_Block),

                    data = StudyData,

                    family = binomial("logit"))



My two questions are the following:



1.) Given the nested structure of my data, would the above regression
output the correct coefficient for the impact of treatment across all
random assignment blocks?



2.) How would I code the interaction effect between Treatment and
Random_Assignment_Block in order to generate separate impact estimates for
each of the random assignment blocks?

	[[alternative HTML version deleted]]


From manishm at dbs.com  Fri Jan 29 09:10:50 2016
From: manishm at dbs.com (Manish MAHESHWARI)
Date: Fri, 29 Jan 2016 08:10:50 +0000
Subject: [R] Redirect Output to File and Screen in Parallel
In-Reply-To: <6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>
References: <8B5BC7735651764E884E3651BAA48D9A08DAFDC4@W01GMAILDAGA02.reg1.1bank.dbs.com>
	<6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>
Message-ID: <8B5BC7735651764E884E3651BAA48D9A08DAFF3A@W01GMAILDAGA02.reg1.1bank.dbs.com>

Hi David,

I did search SO and R-Help Archive. But I haven't got the parallel option. It either prints on screen or logs to the file.

Thanks,
Manish

-----Original Message-----
From: David Winsemius [mailto:dwinsemius at comcast.net]
Sent: Friday, January 29, 2016 3:43 PM
To: Manish MAHESHWARI
Cc: r-help at r-project.org
Subject: Re: [R] Redirect Output to File and Screen in Parallel


> On Jan 28, 2016, at 10:05 PM, Manish MAHESHWARI <manishm at dbs.com> wrote:
>
> Hi,
>
> Using the sink we can redirect the output to sink files set as con.
> However is there a way to do both - Have the Op printed on screen and also to the log file?

I think this has been asked and answered. Have you done any searching?

>
> Thanks,
> Manish
> CONFIDENTIAL NOTE:
> The information contained in this email is intended
> only...{{dropped:11}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

CONFIDENTIAL NOTE:
The information contained in this email is intended only...{{dropped:8}}


From ulrik.stervbo at gmail.com  Fri Jan 29 09:15:32 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Fri, 29 Jan 2016 08:15:32 +0000
Subject: [R] Redirect Output to File and Screen in Parallel
In-Reply-To: <6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>
References: <8B5BC7735651764E884E3651BAA48D9A08DAFDC4@W01GMAILDAGA02.reg1.1bank.dbs.com>
	<6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>
Message-ID: <CAKVAULO5PHfQXX1=CsANuDc5_O+uhG+5BF4=Ns9S3bBdxbvzaw@mail.gmail.com>

?sink use the split = TRUE should do the trick

On Fri, 29 Jan 2016 at 08:44 David Winsemius <dwinsemius at comcast.net> wrote:

>
> > On Jan 28, 2016, at 10:05 PM, Manish MAHESHWARI <manishm at dbs.com> wrote:
> >
> > Hi,
> >
> > Using the sink we can redirect the output to sink files set as con.
> > However is there a way to do both - Have the Op printed on screen and
> also to the log file?
>
> I think this has been asked and answered. Have you done any searching?
>
> >
> > Thanks,
> > Manish
> > CONFIDENTIAL NOTE:
> > The information contained in this email is intended only...{{dropped:11}}
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From gwennael.bataille at uclouvain.be  Fri Jan 29 09:49:21 2016
From: gwennael.bataille at uclouvain.be (=?UTF-8?B?R3dlbm5hw6tsIEJhdGFpbGxl?=)
Date: Fri, 29 Jan 2016 09:49:21 +0100
Subject: [R] Angle between two points with coordinates
In-Reply-To: <CAF8bMcZyWs6xg66N32R5E04djVscoAxkOcJy1oyqgWERA19AWw@mail.gmail.com>
References: <56AA4B61.1000808@uclouvain.be>
	<CAF8bMcZyWs6xg66N32R5E04djVscoAxkOcJy1oyqgWERA19AWw@mail.gmail.com>
Message-ID: <56AB2791.3020300@uclouvain.be>

Thank you very much for your quick answers!
The %% operator seems the easiest way to go; it works perfectly.

Best regards,

Gwenna?l


Le 28/01/2016 20:39, William Dunlap a ?crit :
> In addition to the other fine answers, you might find it convenient
> to represent the points as complex numbers and use the Arg function
> to get the angle (and abs() or Mod() the distance).
>
>   > z <- complex(real=0.8660254, imaginary=0.5)
>   > Arg(z) / base::pi * 180
>   [1] 30
>   > Arg(-z) / base::pi * 180
>   [1] -150
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com <http://tibco.com>
>
> On Thu, Jan 28, 2016 at 9:09 AM, Gwenna?l Bataille 
> <gwennael.bataille at uclouvain.be 
> <mailto:gwennael.bataille at uclouvain.be>> wrote:
>
>     Dear all,
>     I'd like to calculate the angle from one point (origin) to another
>     (target), whatever their coordinates.
>     But I encounter some problems (detailed below). The problem could
>     be solved if one of you could answer positively to one of the
>     following questions:
>
>     1) Is there a function in R converting angles in a standardized
>     manner? (for example, converting -150 or 570 (=210+360) into 210)
>
>     2) If not, would you know a function arccos or arcsin returning
>     two different angles as an output instead of one?
>
>
>
>     Details:
>
>     I'd like to calculate the angle from one point (origin) to another
>     (target), whatever their coordinates.
>     For this, the acos and asin functions work pretty well when the
>     end point is located right and above the starting point (first
>     quarter of the trigonometric circle), but are problematic otherwise.
>
>     # In the following example, the origin is (0,0) and the target
>     (0.8660254, 0.5) is located at an angle of 30? :
>     acos( (0.8660254 - 0) )*180/pi
>     asin( (0.5 - 0) )*180/pi
>     # Both acos and asin give the same answer : 30
>
>     # If now, the origin is (0.8660254, 0.5) and the target is (0,0),
>     the target is located at an angle of -150? :
>     acos( (0 - 0.8660254) )*180/pi
>     asin( (0 - 0.5) )*180/pi
>     # Here the results are different : 150 and -30
>
>     # In fact, there are two angle solutions giving the same cosinus :
>     150 and -(150)
>     # And for sinus as well : -30 and ( 180 - (-30) ) = 210? = -150?
>     -acos( (0 - 0.8660254) )*180/pi
>     180 - asin( (0 - 0.5) )*180/pi
>     # But I cannot test equality between the two :
>     -acos( (0 - 0.8660254) )*180/pi    ==    180 - asin( (0 - 0.5)
>     )*180/pi
>     # FALSE, since 210 != -150  (it's only the case when those two are
>     angles)
>
>
>     Thank you very much in advance for your answers!
>
>     Best regards,
>
>
>     Gwenna?l
>
>     -- 
>     Gwenna?l BATAILLE, PhD student - Teaching assistant
>
>     Earth and Life Institute
>     Universit? Catholique de Louvain
>     1348 Louvain-la-Neuve
>     BELGIUM
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>

-- 
Gwenna?l BATAILLE, PhD student - Teaching assistant

Earth and Life Institute
Universit? Catholique de Louvain
SST/ELI/ELIB
B?timent Carnoy, c.145
Croix du sud 4-5, bte L7.07.04
1348 Louvain-la-Neuve
BELGIUM


	[[alternative HTML version deleted]]


From manishm at dbs.com  Fri Jan 29 09:59:21 2016
From: manishm at dbs.com (Manish MAHESHWARI)
Date: Fri, 29 Jan 2016 08:59:21 +0000
Subject: [R] Redirect Output to File and Screen in Parallel
In-Reply-To: <CAKVAULO5PHfQXX1=CsANuDc5_O+uhG+5BF4=Ns9S3bBdxbvzaw@mail.gmail.com>
References: <8B5BC7735651764E884E3651BAA48D9A08DAFDC4@W01GMAILDAGA02.reg1.1bank.dbs.com>
	<6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>
	<CAKVAULO5PHfQXX1=CsANuDc5_O+uhG+5BF4=Ns9S3bBdxbvzaw@mail.gmail.com>
Message-ID: <8B5BC7735651764E884E3651BAA48D9A08DAFF7B@W01GMAILDAGA02.reg1.1bank.dbs.com>

Thanks Ulrik. The answer was there in the forumn. Apologies.

Also got an interesting solution - http://www.r-statistics.com/2010/05/helping-the-blind-use-r-by-exporting-r-console-to-word/

Thanks,
Manish


From: Ulrik Stervbo [mailto:ulrik.stervbo at gmail.com]
Sent: Friday, January 29, 2016 4:16 PM
To: Manish MAHESHWARI
Cc: r-help at r-project.org
Subject: Re: [R] Redirect Output to File and Screen in Parallel

?sink use the split = TRUE should do the trick

On Fri, 29 Jan 2016 at 08:44 David Winsemius <dwinsemius at comcast.net<mailto:dwinsemius at comcast.net>> wrote:

> On Jan 28, 2016, at 10:05 PM, Manish MAHESHWARI <manishm at dbs.com<mailto:manishm at dbs.com>> wrote:
>
> Hi,
>
> Using the sink we can redirect the output to sink files set as con.
> However is there a way to do both - Have the Op printed on screen and also to the log file?

I think this has been asked and answered. Have you done any searching?

>
> Thanks,
> Manish
> CONFIDENTIAL NOTE:
> The information contained in this email is intended only...{{dropped:11}}
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
CONFIDENTIAL NOTE:
The information contained in this email is intended only for the use of the individual or entity named above and may contain information that is privileged, confidential and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this message in error, please immediately notify the sender and delete the mail. Thank you.

	[[alternative HTML version deleted]]


From chrysopix at gmail.com  Fri Jan 29 12:10:29 2016
From: chrysopix at gmail.com (=?UTF-8?Q?Ronaldo_Reis_J=c3=banior?=)
Date: Fri, 29 Jan 2016 09:10:29 -0200
Subject: [R] help with ggplot2
Message-ID: <56AB48A5.1090701@gmail.com>

Hi,

I try to put 4 different curves to 4 different grids using ggplot2 
package. But, I dont know how.

I have something like this:

A1B1 <- function(x) {exp(4.319735-0.098362*x)}
A1B2 <- function(x) {exp(4.350256-0.098362*x)}
A2B1 <- function(x) {exp(4.920225-0.238536*x)}
A2B2 <- function(x) {exp(5.257911-0.238536*x)}
ggplot(mean, aes(x=xvalue, y=yvalue)) +
   geom_errorbar(aes(ymin=yvalue-SE, ymax=yvalue+SE), width=.1)+
   geom_point()+
   facet_grid(A~B)

Ok, the mean+-SE graphs work. But how I can made to put each equation on 
correct grid? I try to use the stat_function(fun = A1B1), but it put the 
equation on all grids, not only on A1B1 grid.

Any idea?

Thanks
Ronaldo


-- 
1

> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/DBG/Lab. Ecologia Comportamental e Computacional
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8192 | ronaldo.reis at unimontes.br
| http://www.ppgcb.unimontes.br/lecc | LinuxUser#: 205366


	[[alternative HTML version deleted]]


From chrysopix at gmail.com  Fri Jan 29 15:11:07 2016
From: chrysopix at gmail.com (=?UTF-8?Q?Ronaldo_Reis_J=c3=banior?=)
Date: Fri, 29 Jan 2016 12:11:07 -0200
Subject: [R] help with ggplot2
In-Reply-To: <56AB48A5.1090701@gmail.com>
References: <56AB48A5.1090701@gmail.com>
Message-ID: <56AB72FB.1070702@gmail.com>

Hi,

I found a solution.

expfun <- function(x,intercept=intercept,slope=slope) 
{exp(intercept+slope*x)}

mean$treat <- as.factor(paste(mean$A,mean$B,sep=""))

ggplot(mean, aes(x=x, y=y)) +
   geom_errorbar(aes(ymin=y-SE, ymax=y+SE), width=.1)+
   geom_point()+
   stat_function(fun = 
expfun,args=list(intercept=4.319735,slope=-0.098362),data=subset(mean,mean$treat=="A1B1"))+
   stat_function(fun = 
expfun,args=list(intercept=4.350256,slope=-0.098362),data=subset(mean,mean$treat=="A1B2"))+
   stat_function(fun = 
expfun,args=list(intercept=4.920225,slope=-0.238536),data=subset(mean,mean$treat=="A2B1"))+
   stat_function(fun = 
expfun,args=list(intercept=5.257911,slope=-0.238536),data=subset(mean,mean$treat=="A2B2"))+
   facet_grid(A~B)

Its work, maybe not elegant, but work.

Inte
Ronaldo

Em 29-01-2016 09:10, Ronaldo Reis J?nior escreveu:
> Hi,
>
> I try to put 4 different curves to 4 different grids using ggplot2 
> package. But, I dont know how.
>
> I have something like this:
>
> A1B1 <- function(x) {exp(4.319735-0.098362*x)}
> A1B2 <- function(x) {exp(4.350256-0.098362*x)}
> A2B1 <- function(x) {exp(4.920225-0.238536*x)}
> A2B2 <- function(x) {exp(5.257911-0.238536*x)}
> ggplot(mean, aes(x=xvalue, y=yvalue)) +
>   geom_errorbar(aes(ymin=yvalue-SE, ymax=yvalue+SE), width=.1)+
>   geom_point()+
>   facet_grid(A~B)
>
> Ok, the mean+-SE graphs work. But how I can made to put each equation 
> on correct grid? I try to use the stat_function(fun = A1B1), but it 
> put the equation on all grids, not only on A1B1 grid.
>
> Any idea?
>
> Thanks
> Ronaldo
>
>
> -- 
> 1
>
> > Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/DBG/Lab. Ecologia Comportamental e Computacional
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
> | `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
> |   `- Fone: (38) 3229-8192 |ronaldo.reis at unimontes.br  
> |http://www.ppgcb.unimontes.br/lecc  | LinuxUser#: 205366

-- 
1

> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/DBG/Lab. Ecologia Comportamental e Computacional
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8192 | ronaldo.reis at unimontes.br
| http://www.ppgcb.unimontes.br/lecc | LinuxUser#: 205366


	[[alternative HTML version deleted]]


From andersalex at gmail.com  Fri Jan 29 16:24:40 2016
From: andersalex at gmail.com (Anders Alexandersson)
Date: Fri, 29 Jan 2016 10:24:40 -0500
Subject: [R] How to use compare.linkage in RecordLinkage package --
	unexpected output
Message-ID: <CAKJpBtK1DfZHcgko+WYxvYrC63Qkc=bfaf9iguhXVUb4UZv_xg@mail.gmail.com>

Problem resolved. I confused the true matching status and the probabilistic
record linkage results.

Anders Alexandersson
andersalex at gmail.com

On Thu, Jan 28, 2016 at 9:48 AM, Anders Alexandersson <andersalex at gmail.com>
wrote:

> I am using the compare.linkage function in the RecordLinkage package,
> and getting a result I know is wrong, so I know I'm misunderstanding
> something.
> I am using R 3.2.3 for x64 Windows. I am very familar with Stata but not
> so much with R.
> [...]
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jan 29 18:57:59 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 29 Jan 2016 09:57:59 -0800
Subject: [R] Redirect Output to File and Screen in Parallel
In-Reply-To: <8B5BC7735651764E884E3651BAA48D9A08DAFF7B@W01GMAILDAGA02.reg1.1bank.dbs.com>
References: <8B5BC7735651764E884E3651BAA48D9A08DAFDC4@W01GMAILDAGA02.reg1.1bank.dbs.com>
	<6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>
	<CAKVAULO5PHfQXX1=CsANuDc5_O+uhG+5BF4=Ns9S3bBdxbvzaw@mail.gmail.com>
	<8B5BC7735651764E884E3651BAA48D9A08DAFF7B@W01GMAILDAGA02.reg1.1bank.dbs.com>
Message-ID: <E04F23C1-8C0E-43A7-949F-AC0C983CCC89@comcast.net>


> On Jan 29, 2016, at 12:59 AM, Manish MAHESHWARI <manishm at dbs.com> wrote:
> 
> Thanks Ulrik. The answer was there in the forumn. Apologies.
> 
> Also got an interesting solution - http://www.r-statistics.com/2010/05/helping-the-blind-use-r-by-exporting-r-console-to-word/

I'm pretty sure it was Tal's request, the responses and his posting of that link on Rhelp that I was remembering. 

There is also the possibility of accessing the "system variable": .Last.value

You cannot use it with sink("file.txt") since that command returns NULL and removes the temporarily saved value you might want to put in a file, but you can use `capture.output` which I see was recently given a 'split' option. I found this in the NEWS file which on my machine is at http://127.0.0.1:21567/doc/html/NEWS.html, but on your machine may have a different access method.

capture.output(.Last.value, file="save.txt")

Should place a text copy of the screen output resulting from the last console command into a file.

Best;
David.
> 
> Thanks,
> Manish
> 
> 
> From: Ulrik Stervbo [mailto:ulrik.stervbo at gmail.com]
> Sent: Friday, January 29, 2016 4:16 PM
> To: Manish MAHESHWARI
> Cc: r-help at r-project.org
> Subject: Re: [R] Redirect Output to File and Screen in Parallel
> 
> ?sink use the split = TRUE should do the trick
> 
> On Fri, 29 Jan 2016 at 08:44 David Winsemius <dwinsemius at comcast.net<mailto:dwinsemius at comcast.net>> wrote:
> 
>> On Jan 28, 2016, at 10:05 PM, Manish MAHESHWARI <manishm at dbs.com<mailto:manishm at dbs.com>> wrote:
>> 
>> Hi,
>> 
>> Using the sink we can redirect the output to sink files set as con.
>> However is there a way to do both - Have the Op printed on screen and also to the log file?
> 
> I think this has been asked and answered. Have you done any searching?
> 
>> 
>> Thanks,
>> Manish
>> CONFIDENTIAL NOTE:
>> The information contained in this email is intended only...{{dropped:11}}
>> 
>> ______________________________________________
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> CONFIDENTIAL NOTE:
> The information contained in this email is intended on...{{dropped:21}}


From lorenzo.isella at gmail.com  Fri Jan 29 21:59:37 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 29 Jan 2016 21:59:37 +0100
Subject: [R] Time Series and Auto.arima
Message-ID: <20160129205937.GA1752@localhost.localdomain>

Dear All,
I am puzzled and probably I am misunderstanding something.
Please consider the snippet at the end of the email.
We see a time series that has clearly some pattern (essentially, it is
an account where a salary is regularly paid followed by some
expenses).
However the output of the auto.arima from the forecast function does
not seem to make any sense (at least to me).
I wonder if the problem is the fact that the time series is not
defined at regular intervals.
Any suggestions and alternative ways to fit it (e.g.: sarima from the astsa
package to account for the seasonality?) are really welcome.
Many thanks

Lorenzo



##############################################
library(forecast)

tt<-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64,
4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05,
1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36,
2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36,
821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39,
800.81, 790.81), index = structure(c(16563L, 16565L, 16570L,
16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L,
16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L,
16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L,
16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L,
16650L, 16651L, 16654L), class = "Date"), class = "zoo")

plot(tt)

fit<-auto.arima(tt)

###########################################


From lorenzo.isella at gmail.com  Fri Jan 29 23:33:31 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 29 Jan 2016 23:33:31 +0100
Subject: [R] Time Series and Auto.arima
In-Reply-To: <84B3BE17-DE3D-488E-B549-0902628721C7@comcast.net>
References: <20160129205937.GA1752@localhost.localdomain>
	<84B3BE17-DE3D-488E-B549-0902628721C7@comcast.net>
Message-ID: <20160129223331.GA3259@localhost.localdomain>

Thanks,
But something fishy is going on.
The fitted time series is full of missing values, whereas the original
tt object does not have any.
I suppose that in trying to fit the time series defined on an
irregular time grid, some problem arises inside the auto.arima
function.

Lorenzo

On Fri, Jan 29, 2016 at 02:16:27PM -0800, David Winsemius wrote:
>
>> On Jan 29, 2016, at 12:59 PM, Lorenzo Isella <lorenzo.isella at gmail.com> wrote:
>>
>> Dear All,
>> I am puzzled and probably I am misunderstanding something.
>> Please consider the snippet at the end of the email.
>> We see a time series that has clearly some pattern (essentially, it is
>> an account where a salary is regularly paid followed by some
>> expenses).
>> However the output of the auto.arima from the forecast function does
>> not seem to make any sense (at least to me).
>> I wonder if the problem is the fact that the time series is not
>> defined at regular intervals.
>> Any suggestions and alternative ways to fit it (e.g.: sarima from the astsa
>> package to account for the seasonality?) are really welcome.
>> Many thanks
>>
>> Lorenzo
>>
>>
>>
>> ##############################################
>> library(forecast)
>>
>> tt<-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64,
>> 4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05,
>> 1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36,
>> 2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36,
>> 821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39,
>> 800.81, 790.81), index = structure(c(16563L, 16565L, 16570L,
>> 16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L,
>> 16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L,
>> 16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L,
>> 16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L,
>> 16650L, 16651L, 16654L), class = "Date"), class = "zoo")
>>
>> plot(tt)
>>
>
>library(forecast)
>
>> fit<-auto.arima(tt)
>>
>> ###########################################
>
>If , after runing plot(tt), you then run:
>
> fitted(fit)
>
>Time Series:
>Start = 16563
>End = 16654
>Frequency = 1
> [1] 1448.8211        NA 1444.8612        NA        NA        NA        NA
> [8] 1398.7752        NA 1359.0350        NA        NA        NA        NA
>[15] 1309.1398        NA 1219.7420        NA        NA        NA        NA
>[22] 2302.8903 3708.1762 2713.0349 2603.0512 1968.0100 1819.1484 1725.4634
>[29]        NA 1572.6179 1593.2628        NA        NA        NA        NA
>[36]        NA 1258.3403        NA        NA        NA        NA        NA
>[43]        NA 1184.9656  955.3023  822.7394        NA        NA        NA
>[50] 1987.7634 3333.3131 2294.6941        NA        NA 1760.6351 1551.5526
>[57] 1406.6751 1309.3682 1238.1899        NA        NA        NA        NA
>[64]        NA        NA 1251.6898        NA        NA        NA        NA
>[71] 1179.9970        NA  988.3885        NA        NA  888.4533        NA
>[78]        NA        NA  889.4017        NA        NA        NA        NA
>[85] 1970.0911 3152.7668 2032.3935 1799.2350 1126.2794        NA        NA
>[92] 1088.1525
>
>
>Using that vector:
>
>lines(seq(16563 ,16654 ),fitted(fit), col="red", lwd=3)
>
>You can see that the fitted values are capturing quite a bit of the variation.
>
>
>
>I'm not a regular user of pkg:forecast, so there may be more refined methods of extracting information than using `fitted`.
>
>-- 
>
>David Winsemius
>Alameda, CA, USA
>


From bgunter.4567 at gmail.com  Fri Jan 29 23:45:47 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 29 Jan 2016 14:45:47 -0800
Subject: [R] Multilevel Modeling in R
In-Reply-To: <CAPwiEwWWu4V2NrZ7ema-5dZ2WiiChWzWtwYpEyVrt+0F-zVdHw@mail.gmail.com>
References: <CAPwiEwWWu4V2NrZ7ema-5dZ2WiiChWzWtwYpEyVrt+0F-zVdHw@mail.gmail.com>
Message-ID: <CAGxFJbSE56V2VC+S7JbtkJopYo8ZeKA=8GPurJFkiKnTX1zw=g@mail.gmail.com>

1. Please post in plain text, not HTML, which can get garbled.

2. I believe your syntax is incorrect, but I haven't used lmer in a
while, and so what I believe should be ignored anyway. HOWEVER, there
is a SIG (special interest group) for mixed models, and you have a
much better chance of getting reliable advice on such matters there.
So you should sign up and post to R-sig-mixed-models on these topics
rather than here.

Cheers,
Bert





Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Jan 28, 2016 at 10:10 PM, David Roy <dmr02004 at gmail.com> wrote:
> I am conducting a multilevel regression analysis on the effect of an
> intervention on student test results, and am not sure how to implement the
> necessary R code to correctly capture the nested structure.
>
>
>
> The outcome measure for the study is whether a student passed or failed a
> final exam.  The structure of the data is students nested within schools,
> and then schools nested within random assignment blocks.  Treatment (i.e.,
> the intervention) was implemented at the school-level.  The covariates that
> I am planning to use are prior year test scores (this is also a binary
> variable for pass or fail), race, and gender.
>
>
>
> My ideal output would show the impact of the treatment for each of the
> random assignment blocks, and then the weighted average of the impact
> across all of the random assignment blocks.
>
>
>
> Based on my research thus far, it seems like the **lmer** function from the
> **lme4** package would be the best route to go.
>
>
>
> This is the code that I have tried:
>
>
>
>     # Fit multilevel regression with random assignment blocks
>
>     glmer2 <- glmer(Post_Test_Score ~ Treatment +
>
>                                       Pre_Test_Score +
>
>                                       (1 | School) +
>
>                                       (1 | Random_Assignment_Block),
>
>                     data = StudyData,
>
>                     family = binomial("logit"))
>
>
>
> My two questions are the following:
>
>
>
> 1.) Given the nested structure of my data, would the above regression
> output the correct coefficient for the impact of treatment across all
> random assignment blocks?
>
>
>
> 2.) How would I code the interaction effect between Treatment and
> Random_Assignment_Block in order to generate separate impact estimates for
> each of the random assignment blocks?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Jan 29 23:16:27 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 29 Jan 2016 14:16:27 -0800
Subject: [R] Time Series and Auto.arima
In-Reply-To: <20160129205937.GA1752@localhost.localdomain>
References: <20160129205937.GA1752@localhost.localdomain>
Message-ID: <84B3BE17-DE3D-488E-B549-0902628721C7@comcast.net>


> On Jan 29, 2016, at 12:59 PM, Lorenzo Isella <lorenzo.isella at gmail.com> wrote:
> 
> Dear All,
> I am puzzled and probably I am misunderstanding something.
> Please consider the snippet at the end of the email.
> We see a time series that has clearly some pattern (essentially, it is
> an account where a salary is regularly paid followed by some
> expenses).
> However the output of the auto.arima from the forecast function does
> not seem to make any sense (at least to me).
> I wonder if the problem is the fact that the time series is not
> defined at regular intervals.
> Any suggestions and alternative ways to fit it (e.g.: sarima from the astsa
> package to account for the seasonality?) are really welcome.
> Many thanks
> 
> Lorenzo
> 
> 
> 
> ##############################################
> library(forecast)
> 
> tt<-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64,
> 4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05,
> 1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36,
> 2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36,
> 821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39,
> 800.81, 790.81), index = structure(c(16563L, 16565L, 16570L,
> 16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L,
> 16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L,
> 16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L,
> 16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L,
> 16650L, 16651L, 16654L), class = "Date"), class = "zoo")
> 
> plot(tt)
> 

library(forecast)

> fit<-auto.arima(tt)
> 
> ###########################################

If , after runing plot(tt), you then run:

 fitted(fit)

Time Series:
Start = 16563 
End = 16654 
Frequency = 1 
 [1] 1448.8211        NA 1444.8612        NA        NA        NA        NA
 [8] 1398.7752        NA 1359.0350        NA        NA        NA        NA
[15] 1309.1398        NA 1219.7420        NA        NA        NA        NA
[22] 2302.8903 3708.1762 2713.0349 2603.0512 1968.0100 1819.1484 1725.4634
[29]        NA 1572.6179 1593.2628        NA        NA        NA        NA
[36]        NA 1258.3403        NA        NA        NA        NA        NA
[43]        NA 1184.9656  955.3023  822.7394        NA        NA        NA
[50] 1987.7634 3333.3131 2294.6941        NA        NA 1760.6351 1551.5526
[57] 1406.6751 1309.3682 1238.1899        NA        NA        NA        NA
[64]        NA        NA 1251.6898        NA        NA        NA        NA
[71] 1179.9970        NA  988.3885        NA        NA  888.4533        NA
[78]        NA        NA  889.4017        NA        NA        NA        NA
[85] 1970.0911 3152.7668 2032.3935 1799.2350 1126.2794        NA        NA
[92] 1088.1525


Using that vector:

lines(seq(16563 ,16654 ),fitted(fit), col="red", lwd=3)

You can see that the fitted values are capturing quite a bit of the variation.



I'm not a regular user of pkg:forecast, so there may be more refined methods of extracting information than using `fitted`.

-- 

David Winsemius
Alameda, CA, USA


From bobkot at comcast.net  Fri Jan 29 23:18:27 2016
From: bobkot at comcast.net (Bob Kot)
Date: Fri, 29 Jan 2016 17:18:27 -0500
Subject: [R] Error in loading caret
Message-ID: <004201d15ae2$fab994b0$f02cbe10$@comcast.net>

After a successful install.packages("caret"), I proceed to load "caret" with
library(caret), only to obtain the following error message,

 

Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
vI[[j]]) : 

  there is no package called 'pbkrtest'

In addition: Warning message:

package 'caret' was built under R version 3.2.3 

Error: package or namespace load failed for 'caret'

 

Any thoughts on to resolve this error are greatly appreciated.

 

Best,

Bob

 


	[[alternative HTML version deleted]]


From divakarreddy.a at gmail.com  Fri Jan 29 23:24:51 2016
From: divakarreddy.a at gmail.com (Divakar Reddy)
Date: Fri, 29 Jan 2016 15:24:51 -0700
Subject: [R] help on Rprofile.site
Message-ID: <CALEm3d2yr8Jjnj=rXn+XUyp_6fJZWaPmPckVx4eQWCOoSTn=6A@mail.gmail.com>

Hi All,


Issue:
we are running issues with RStudio like crash and packages not found.

Root Cause:
Sharing a home directory between two servers( we installed RStudio on two
servers and both are sharing same home directory).

Our Solution:
Create separate home directories on both RStudio servers and ensure that
both are not sharing same profile files like .rstudio/Rhistory
Current Home directories location: /home/user/
Proposed Home directory: /home/user/Ser1 & /home/user/Ser17 for both servers

In-order to achieve this I'm trying to set new home directory on both
servers to store .rstudio files separately.

[abc at xyz etc]$ pwd
/usr/lib64/R/etc
[abc at xyz etc]$ cat Rprofile.site
.First <- function() cat("n Welcome to R!nn")
# get the System USER variable and set it to the R variable u
u <- Sys.getenv("USER")
# use file.path() to set the file path using the USER variable
u.path <- file.path("/home", u, "Ser17")
# change the working directory to the set file path
#setwd(u.path)
.Last <- function() cat("n Goodbye!nn")

Here I don't want to change the Working directory and looking to change
only home directory to store profile files which are used while starting R.

Could you please suggest me on this?

Thanks,
Divakar
Hadoop Administrator,
Phoenix,USA

	[[alternative HTML version deleted]]


From dmr02004 at gmail.com  Fri Jan 29 08:48:01 2016
From: dmr02004 at gmail.com (David Roy)
Date: Thu, 28 Jan 2016 23:48:01 -0800
Subject: [R] Redirect Output to File and Screen in Parallel
In-Reply-To: <6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>
References: <8B5BC7735651764E884E3651BAA48D9A08DAFDC4@W01GMAILDAGA02.reg1.1bank.dbs.com>
	<6F1F90DE-AF59-4890-BD1C-D16D7264E074@comcast.net>
Message-ID: <CAPwiEwXrmJr6zL-nRP0A9Hab29UM4LsMRLdEBMMgdux05ZwoHg@mail.gmail.com>

I did do some searching, but haven't been able to find an answer up to this
point.  I'll continue looking in the question archives and let you know if
I find an answer.

Thanks,

David

On Thu, Jan 28, 2016 at 11:42 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jan 28, 2016, at 10:05 PM, Manish MAHESHWARI <manishm at dbs.com> wrote:
> >
> > Hi,
> >
> > Using the sink we can redirect the output to sink files set as con.
> > However is there a way to do both - Have the Op printed on screen and
> also to the log file?
>
> I think this has been asked and answered. Have you done any searching?
>
> >
> > Thanks,
> > Manish
> > CONFIDENTIAL NOTE:
> > The information contained in this email is intended only...{{dropped:11}}
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dbastos at toledo.com  Fri Jan 29 16:35:50 2016
From: dbastos at toledo.com (Daniel Bastos)
Date: Fri, 29 Jan 2016 13:35:50 -0200
Subject: [R] on specifying an encoding for plot's main-argument
Message-ID: <0q37tgz9d5.fsf@toledo.com>

Here's how I plot a graph.

  plot(c(1,2,3), main = "graph ?")

The main-string has a UTF-8 character "?".  I believe I'm using the
windows device.  It opens up on my screen.  (The window says ``R
Graphics: Device 2 (ACTIVE)''.)  How can I tell it to use my encoding of
choice?

I looked around the web for explanations on how to properly tell the
relevant mecanisms that I'm using strings with a partcular encoding when
plotting.  I saw many with my difficulty, but no one seemed to explain
the whole issue.

At first I thought I should tell the device.  So I looked at the
documentation for various devices.  I realized only devices such as
postscript, pdf had an encoding parameter.  ``My assumptions must be
wrong'', I thought.  ``Perhaps it's not the device I must tell my
encoding.''

Then I come to you.  Can you point me towards understanding the issue?
You can tell me to read an entire book on encoding, charset and fonts.
I'd like to free myself from such difficulties.

I use R and ESS (GNU EMACS).  (My ESS console says 'U' in the EMACS
modeline.  It means I'm encoding in UTF-8.  I tried '1', ISO-8859-1,
also called Latin-1.)

Thank you.

(*) The softwares

R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: i386-w64-mingw32/i386 (32-bit)

ess-version: 15.09-2 [Released git: 01328e83039f]

GNU Emacs 24.3.1 (i386-mingw-nt6.1.7601) of 2013-03-17 on MARVIN


From glennmschultz at me.com  Fri Jan 29 18:02:08 2016
From: glennmschultz at me.com (Glenn Schultz)
Date: Fri, 29 Jan 2016 17:02:08 +0000 (GMT)
Subject: [R] String Matching
Message-ID: <a3fd27d2-8ade-45d0-a368-c519b942a049@me.com>

All,

I have a file named as so 313929BL4FNMA2432.rds ?the user may pass either the first 9 character or the last six characters. ?I need to match the remainder of the file name using either the first nine or last six. ?I have read the help files for Regular Expression as used in R and I think what I want to use is glob2rx. ?

I have worked a minimal example to test my code:

id <- "313929BL4FNMA2432.rds" 
cusip <- "313929BL4"
poolnumm <- "FNMA2432"
paste(cusip, ".*", ".rds")
glob2rx(paste(cusip, ".*", ".rds"), trim.head = TRUE, trim.tail = TRUE)

This returns false which leads me to believe that it is not working
glob2rx(paste(cusip, ".*", ".rds"), trim.head = TRUE, trim.tail = TRUE) == id

I am going to use as follows in the function below - which returns the error file not found

MBS_Test <- function(MBS.id = "character"){
MBS <- glob2rx(paste(MBS.id, ".*", "//.rds", sep = ""), trim.tail = TRUE)
MBS.Conn <- gzfile(description = paste(system.file(package = "BondLab"),
"/BondData/", MBS, sep = ""), open = "rb") 
MBS <- readRDS(MBS.Conn)
on.exit(close.connection(MBS.Conn))
return(MBS)
} ??

Any help to get me in the right direction is appreciated - Glenn

From brian at mcneilco.com  Fri Jan 29 19:11:53 2016
From: brian at mcneilco.com (Brian Bolt)
Date: Fri, 29 Jan 2016 10:11:53 -0800
Subject: [R] Reducing of car package when loading
Message-ID: <4F4F3486-5A1B-49F2-9439-CE29308DF1CD@mcneilco.com>

I have a non-CRAN package that has a large number of dependencies and as such, the memory footprint from loading my package in R is becoming larger.  I use Rapache often to pre-load my package and provide web services for my code, so the consistent memory footprint is hurting other processes on the machine.

I have created an R docker container and when I start R, the memory footprint is 27.89MB, after loading the car package, the memory footprint shoots up to 131.4MB. A difference of 103.51MB.  For comparison, loading ggplot2 only gives a difference of 9.32MB.

Is there something I can do, without removing dependencies, that could relieve some of my memory footprint?  To be clear, I am not just asking about the car package but reducing memory dependence in general.  Can I force the R package loader to only load functions from packages that I am dependent on? Is there a way to not load all of the datasets? 

Thanks,
Brian



	[[alternative HTML version deleted]]


From Matteo.Richiardi at maths.ox.ac.uk  Sat Jan 30 02:03:30 2016
From: Matteo.Richiardi at maths.ox.ac.uk (Matteo Richiardi)
Date: Sat, 30 Jan 2016 01:03:30 +0000
Subject: [R] updating elements of a list of matrixes without 'for' cycles
Message-ID: <CABSrU1LkOHUZ8M9JW1ju+neMksPriRRTD_0WzOtRLWi3z6dA9w@mail.gmail.com>

Hi, following an earlier suggestion from the list, I am storing my
data in a "cube", i.e. an array of matrixes.
Is there any smarter way of updating the elements of the cube through
a function, other than the three 'for' cycles in the example below?
(please note that the example is simplistic; in particular, my
function is more complicated).

# parameters
I <- 2L
J <- 2L
H <- 2L

# data container: an array of matrixes
mycube <- array(dim=c(I,J,H))

# initialisation
for (h in 1:H) {
  init <- matrix(c(rep(0,J)),nrow=I,ncol=J)
  mycube[,,h] <- init
}

# function
foo = function(i,j,h){
  mycube[i,j,h] <<- i*j*h
}

# update

for(h in 1:H){
  # males:
  for(i in 1:I)
    for(j in 1:J)
      foo(i,j,h)
}

Thanks a lot for your help. Matteo


From gaiusjaugustus at gmail.com  Fri Jan 29 19:52:06 2016
From: gaiusjaugustus at gmail.com (Gaius Augustus)
Date: Fri, 29 Jan 2016 11:52:06 -0700
Subject: [R] Efficient way to create new column based on comparison with
	another dataframe
Message-ID: <CACNwPfYXQN3+z+yO6DPLTEhN1c0SiwJrS==Zn0cte3P1zLay1A@mail.gmail.com>

I have two dataframes. One has chromosome arm information, and the other
has SNP position information. I am trying to assign each SNP an arm
identity.  I'd like to create this new column based on comparing it to the
reference file.

*1) Mapfile (has millions of rows)*

Name    Chr   Position
S1      1      3000
S2      1      6000
S3      1      1000

*2) Chr.Arms   file (has 39 rows)*

Chr    Arm    Start   End
1      p      0       5000
1      q      5001    10000


*R Script that works, but slow:*
Arms  <- c()
for (line in 1:nrow(Mapfile)){
      Arms[line] <- Chr.Arms$Arm[ Mapfile$Chr[line] == Chr.Arms$Chr &
 Mapfile$Position[line] > Chr.Arms$Start &  Mapfile$Position[line] <
Chr.Arms$End]}
}
Mapfile$Arm <- Arms


*Output Table:*

Name   Chr   Position   Arm
S1      1     3000      p
S2      1     6000      q
S3      1     1000      p


In words: I want each line to look up the location ( 1) find the right Chr,
2) find the line where the START < POSITION < END), then get the ARM
information and place it in a new column.

This R script works, but surely there is a more time/processing efficient
way to do it.

Thanks in advance for any help,
Gaius

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sat Jan 30 02:48:47 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 29 Jan 2016 20:48:47 -0500
Subject: [R] Error in loading caret
In-Reply-To: <004201d15ae2$fab994b0$f02cbe10$@comcast.net>
References: <004201d15ae2$fab994b0$f02cbe10$@comcast.net>
Message-ID: <56AC167F.8030209@gmail.com>

On 29/01/2016 5:18 PM, Bob Kot wrote:
> After a successful install.packages("caret"), I proceed to load "caret" with
> library(caret), only to obtain the following error message,
>
>
>
> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
> vI[[j]]) :
>
>    there is no package called 'pbkrtest'
>
> In addition: Warning message:
>
> package 'caret' was built under R version 3.2.3
>
> Error: package or namespace load failed for 'caret'
>
>
>
> Any thoughts on to resolve this error are greatly appreciated.

Do you have pbkrtest installed?

What version of caret did you install?  What version of R are you using?

Duncan Murdoch


From murdoch.duncan at gmail.com  Sat Jan 30 02:57:32 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 29 Jan 2016 20:57:32 -0500
Subject: [R] on specifying an encoding for plot's main-argument
In-Reply-To: <0q37tgz9d5.fsf@toledo.com>
References: <0q37tgz9d5.fsf@toledo.com>
Message-ID: <56AC188C.4050609@gmail.com>

On 29/01/2016 10:35 AM, Daniel Bastos wrote:
> Here's how I plot a graph.
>
>    plot(c(1,2,3), main = "graph ?")
>
> The main-string has a UTF-8 character "?".  I believe I'm using the
> windows device.  It opens up on my screen.  (The window says ``R
> Graphics: Device 2 (ACTIVE)''.)  How can I tell it to use my encoding of
> choice?

As far as I know that's impossible.  R uses the system encoding, and I 
don't think any Windows versions use UTF-8 code pages.  They use UTF-16 
for wide characters, and some 8 bit encoding for byte-sized characters. 
  R will use whatever 8 bit code page Windows chooses.
>
> I looked around the web for explanations on how to properly tell the
> relevant mecanisms that I'm using strings with a partcular encoding when
> plotting.  I saw many with my difficulty, but no one seemed to explain
> the whole issue.

If you enter the string as a literal, it is not using UTF-8 encoding, 
it's using the system's 8 bit encoding.

>
> At first I thought I should tell the device.  So I looked at the
> documentation for various devices.  I realized only devices such as
> postscript, pdf had an encoding parameter.  ``My assumptions must be
> wrong'', I thought.  ``Perhaps it's not the device I must tell my
> encoding.''
>
> Then I come to you.  Can you point me towards understanding the issue?
> You can tell me to read an entire book on encoding, charset and fonts.
> I'd like to free myself from such difficulties.
>
> I use R and ESS (GNU EMACS).  (My ESS console says 'U' in the EMACS
> modeline.  It means I'm encoding in UTF-8.  I tried '1', ISO-8859-1,
> also called Latin-1.)

Duncan Murdoch


From murdoch.duncan at gmail.com  Sat Jan 30 03:02:27 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 29 Jan 2016 21:02:27 -0500
Subject: [R] String Matching
In-Reply-To: <a3fd27d2-8ade-45d0-a368-c519b942a049@me.com>
References: <a3fd27d2-8ade-45d0-a368-c519b942a049@me.com>
Message-ID: <56AC19B3.4070402@gmail.com>

On 29/01/2016 12:02 PM, Glenn Schultz wrote:
> All,
>
> I have a file named as so 313929BL4FNMA2432.rds  the user may pass either the first 9 character or the last six characters.  I need to match the remainder of the file name using either the first nine or last six.  I have read the help files for Regular Expression as used in R and I think what I want to use is glob2rx.
>
> I have worked a minimal example to test my code:
>
> id <- "313929BL4FNMA2432.rds"
> cusip <- "313929BL4"
> poolnumm <- "FNMA2432"
> paste(cusip, ".*", ".rds")
> glob2rx(paste(cusip, ".*", ".rds"), trim.head = TRUE, trim.tail = TRUE)
>
> This returns false which leads me to believe that it is not working

No, it returns a regular expression.  You need to tell us what you 
really did if you want help fixing it.

Duncan Murdoch

> glob2rx(paste(cusip, ".*", ".rds"), trim.head = TRUE, trim.tail = TRUE) == id
>
> I am going to use as follows in the function below - which returns the error file not found
>
> MBS_Test <- function(MBS.id = "character"){
> MBS <- glob2rx(paste(MBS.id, ".*", "//.rds", sep = ""), trim.tail = TRUE)
> MBS.Conn <- gzfile(description = paste(system.file(package = "BondLab"),
> "/BondData/", MBS, sep = ""), open = "rb")
> MBS <- readRDS(MBS.Conn)
> on.exit(close.connection(MBS.Conn))
> return(MBS)
> }
>
> Any help to get me in the right direction is appreciated - Glenn
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From boris.steipe at utoronto.ca  Sat Jan 30 04:22:44 2016
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Fri, 29 Jan 2016 22:22:44 -0500
Subject: [R] help on Rprofile.site
In-Reply-To: <CALEm3d2yr8Jjnj=rXn+XUyp_6fJZWaPmPckVx4eQWCOoSTn=6A@mail.gmail.com>
References: <CALEm3d2yr8Jjnj=rXn+XUyp_6fJZWaPmPckVx4eQWCOoSTn=6A@mail.gmail.com>
Message-ID: <E5E1B08E-FA1F-4097-B710-9B55B5F90A30@utoronto.ca>

I've read this several time now but I'm still not sure I understand fully what your are trying to achieve. But in principle I would put a switch in the file ... 

if (it's one server) { PREFIX <- "A" }
if (it's the other server) { PREFIX <- "B" }  ... or whatever and then

source(paste(PREFIX, "_startupScript.R", sep=""))

or something along these lines. If that's not what you need, you'll need to explain it better.

B.



On Jan 29, 2016, at 5:24 PM, Divakar Reddy <divakarreddy.a at gmail.com> wrote:

> Hi All,
> 
> 
> Issue:
> we are running issues with RStudio like crash and packages not found.
> 
> Root Cause:
> Sharing a home directory between two servers( we installed RStudio on two
> servers and both are sharing same home directory).
> 
> Our Solution:
> Create separate home directories on both RStudio servers and ensure that
> both are not sharing same profile files like .rstudio/Rhistory
> Current Home directories location: /home/user/
> Proposed Home directory: /home/user/Ser1 & /home/user/Ser17 for both servers
> 
> In-order to achieve this I'm trying to set new home directory on both
> servers to store .rstudio files separately.
> 
> [abc at xyz etc]$ pwd
> /usr/lib64/R/etc
> [abc at xyz etc]$ cat Rprofile.site
> .First <- function() cat("n Welcome to R!nn")
> # get the System USER variable and set it to the R variable u
> u <- Sys.getenv("USER")
> # use file.path() to set the file path using the USER variable
> u.path <- file.path("/home", u, "Ser17")
> # change the working directory to the set file path
> #setwd(u.path)
> .Last <- function() cat("n Goodbye!nn")
> 
> Here I don't want to change the Working directory and looking to change
> only home directory to store profile files which are used while starting R.
> 
> Could you please suggest me on this?
> 
> Thanks,
> Divakar
> Hadoop Administrator,
> Phoenix,USA
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Jan 30 07:14:10 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 29 Jan 2016 22:14:10 -0800
Subject: [R] Reducing of car package when loading
In-Reply-To: <4F4F3486-5A1B-49F2-9439-CE29308DF1CD@mcneilco.com>
References: <4F4F3486-5A1B-49F2-9439-CE29308DF1CD@mcneilco.com>
Message-ID: <B18C046C-9D8C-44EC-AAA9-B49BF0F1631E@comcast.net>


> On Jan 29, 2016, at 10:11 AM, Brian Bolt <brian at mcneilco.com> wrote:
> 
> I have a non-CRAN package that has a large number of dependencies and as such, the memory footprint from loading my package in R is becoming larger.  I use Rapache often to pre-load my package and provide web services for my code, so the consistent memory footprint is hurting other processes on the machine.
> 
> I have created an R docker container and when I start R, the memory footprint is 27.89MB, after loading the car package, the memory footprint shoots up to 131.4MB. A difference of 103.51MB.  For comparison, loading ggplot2 only gives a difference of 9.32MB.
> 
> Is there something I can do, without removing dependencies, that could relieve some of my memory footprint?  To be clear, I am not just asking about the car package but reducing memory dependence in general.  Can I force the R package loader to only load functions from packages that I am dependent on? Is there a way to not load all of the datasets? 

You could be more specific about which functions you need.


> 
> Thanks,
> Brian
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ulrik.stervbo at gmail.com  Sat Jan 30 07:34:45 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Sat, 30 Jan 2016 06:34:45 +0000
Subject: [R] Efficient way to create new column based on comparison with
 another dataframe
In-Reply-To: <CACNwPfYXQN3+z+yO6DPLTEhN1c0SiwJrS==Zn0cte3P1zLay1A@mail.gmail.com>
References: <CACNwPfYXQN3+z+yO6DPLTEhN1c0SiwJrS==Zn0cte3P1zLay1A@mail.gmail.com>
Message-ID: <CAKVAULM2g-C887=RrBKJKKZB4RTExigS9X-kqHJjaXt5WTVMrA@mail.gmail.com>

Hi Gaius,

Could you use data.table and loop over the small Chr.arms?

library(data.table)
mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
c(3000, 6000, 1000), key = "Chr")
Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001), End
= c(5000, 10000), key = "Chr")

Arms <- data.table()
for(i in 1:nrow(Chr.Arms)){
  cur.row <- Chr.Arms[i, ]
  Arm <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
  Arm <- Arm[ , Arm:=cur.row$Arm][]
  Arms <- rbind(Arms, Arm)
}

# Or use plyr to loop over each possible arm
library(plyr)
Arms <- ddply(Chr.Arms, .variables = "Arm", function(cur.row, mapfile){
  mapfile <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
  mapfile <- mapfile[ , Arm:=cur.row$Arm][]
  return(mapfile)
}, mapfile = mapfile)

I have just started to use the data.table and I have the feeling the code
above can be greatly improved - maybe the loop can be dropped entirely?

Hope this helps
Ulrik

On Sat, 30 Jan 2016 at 03:29 Gaius Augustus <gaiusjaugustus at gmail.com>
wrote:

> I have two dataframes. One has chromosome arm information, and the other
> has SNP position information. I am trying to assign each SNP an arm
> identity.  I'd like to create this new column based on comparing it to the
> reference file.
>
> *1) Mapfile (has millions of rows)*
>
> Name    Chr   Position
> S1      1      3000
> S2      1      6000
> S3      1      1000
>
> *2) Chr.Arms   file (has 39 rows)*
>
> Chr    Arm    Start   End
> 1      p      0       5000
> 1      q      5001    10000
>
>
> *R Script that works, but slow:*
> Arms  <- c()
> for (line in 1:nrow(Mapfile)){
>       Arms[line] <- Chr.Arms$Arm[ Mapfile$Chr[line] == Chr.Arms$Chr &
>  Mapfile$Position[line] > Chr.Arms$Start &  Mapfile$Position[line] <
> Chr.Arms$End]}
> }
> Mapfile$Arm <- Arms
>
>
> *Output Table:*
>
> Name   Chr   Position   Arm
> S1      1     3000      p
> S2      1     6000      q
> S3      1     1000      p
>
>
> In words: I want each line to look up the location ( 1) find the right Chr,
> 2) find the line where the START < POSITION < END), then get the ARM
> information and place it in a new column.
>
> This R script works, but surely there is a more time/processing efficient
> way to do it.
>
> Thanks in advance for any help,
> Gaius
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Sat Jan 30 07:30:56 2016
From: chocold12 at gmail.com (lily li)
Date: Fri, 29 Jan 2016 23:30:56 -0700
Subject: [R] about change columns for specific rows
Message-ID: <CAN5afy-UKpP3Hb=sRt7weutPBtpJ31KzLJgChD5poPxuUAkJKA@mail.gmail.com>

Hi R users,

I have a data frame, and I generate a date column like this:
df$date = seq(as.Date('2012-01-01'), as.Date('2014-12-31'))

df
A  B  C
1  2   1
2  2   3
3  2   4

So the data frame has 4 columns now. But when I want to change the values
of column A for specific dates, such as 2012-01-01 to 2013-12-31, I use the
code below:
df[date >= '2012-01-01'&date <= '2013-12-31']$A =
df[date >= '2012-01-01'&date <= '2013-12-31']$A +2

But it does not work, the date I generate seems not effective. What is the
problem? Thanks for your help.

	[[alternative HTML version deleted]]


From gupta17anukriti at gmail.com  Sat Jan 30 07:44:54 2016
From: gupta17anukriti at gmail.com (Anukriti Gupta)
Date: Sat, 30 Jan 2016 12:14:54 +0530
Subject: [R] R help
Message-ID: <CABZBenYoQY3hSdNt8msjme1Zeu-J3kaB0fA9bfjMuFs80pzL9A@mail.gmail.com>

Hi

I am running a ordinal logistic regression, however its giving me an error
like

Error: cannot allocate vector of size 58.8 GbIn addition: Warning
messages:1: In rep.int(c(1, numeric(n)), n - 1L) :
  Reached total allocation of 8057Mb: see help(memory.size)2: In
rep.int(c(1, numeric(n)), n - 1L) :
  Reached total allocation of 8057Mb: see help(memory.size)3: In
rep.int(c(1, numeric(n)), n - 1L) :
  Reached total allocation of 8057Mb: see help(memory.size)4: In
rep.int(c(1, numeric(n)), n - 1L) :
  Reached total allocation of 8057Mb: see help(memory.size)


I am using a 64 bit laptop. I ma not sure what is causing this kind of issue

Regards

Anukriti Gupta
Analyst (Financial Crime Compliance), HSBC
M: +91 88820 45065
LinkedIn <https://in.linkedin.com/pub/anukriti-gupta/21/839/5a5>

	[[alternative HTML version deleted]]


From r_1470 at yahoo.co.uk  Sat Jan 30 15:43:52 2016
From: r_1470 at yahoo.co.uk (r_1470)
Date: Sat, 30 Jan 2016 14:43:52 +0000 (UTC)
Subject: [R] Use of betatree {betareg} to estimate true positives from
 multiple tests
References: <102504568.3973106.1454165032839.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <102504568.3973106.1454165032839.JavaMail.yahoo@mail.yahoo.com>

?Hi,?Doesanyone use the 'betatree' function in the betareg package to do a kind of falsediscovery rate (FDR) test for a set of many p values??I wasthinking to compare the beta parameters of the true distribution of about 1000p values with p values of permuted data, and test whether the two distributionswere significantly different, with a greater mass of low p values in the truedata. I know this is possible with betatree, I'm just not sure if it's a 'normal'method.?I knowuniform-beta mixture models are commonly used to examine FDR, and there are R packagesavailable, but my tests are non-independent so the null distribution may not beuniform and has to be found empirically. The mixture model approach assumes there are two groups: truepositives (beta distribution) and true negatives (uniform distribution). I think my approach would assume a continuous distributionof effect sizes in the true data, rather than 2 distinct groups, with a point mass at 0 effect size for thepermuted data.
I also looked at using the 'betamix' function, but this is much less accurate and takes longer to run.?Best?Richard.
	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Sat Jan 30 17:25:06 2016
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Sat, 30 Jan 2016 11:25:06 -0500
Subject: [R] R help
In-Reply-To: <CABZBenYoQY3hSdNt8msjme1Zeu-J3kaB0fA9bfjMuFs80pzL9A@mail.gmail.com>
References: <CABZBenYoQY3hSdNt8msjme1Zeu-J3kaB0fA9bfjMuFs80pzL9A@mail.gmail.com>
Message-ID: <65F5A596-5CC4-4C1A-B41C-D2D3AA615D5A@utoronto.ca>

I think the error message is pretty clear. Your calculations are attempting to allocate more memory than you have available. As to what is causing your code to do this, only someone familiar with your code could possibly tell.

B.
(Read the posting guide, please - and don't post in HTML :-)


On Jan 30, 2016, at 1:44 AM, Anukriti Gupta <gupta17anukriti at gmail.com> wrote:

> Hi
> 
> I am running a ordinal logistic regression, however its giving me an error
> like
> 
> Error: cannot allocate vector of size 58.8 GbIn addition: Warning
> messages:1: In rep.int(c(1, numeric(n)), n - 1L) :
>  Reached total allocation of 8057Mb: see help(memory.size)2: In
> rep.int(c(1, numeric(n)), n - 1L) :
>  Reached total allocation of 8057Mb: see help(memory.size)3: In
> rep.int(c(1, numeric(n)), n - 1L) :
>  Reached total allocation of 8057Mb: see help(memory.size)4: In
> rep.int(c(1, numeric(n)), n - 1L) :
>  Reached total allocation of 8057Mb: see help(memory.size)
> 
> 
> I am using a 64 bit laptop. I ma not sure what is causing this kind of issue
> 
> Regards
> 
> Anukriti Gupta
> Analyst (Financial Crime Compliance), HSBC
> M: +91 88820 45065
> LinkedIn <https://in.linkedin.com/pub/anukriti-gupta/21/839/5a5>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Sat Jan 30 19:23:06 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Sat, 30 Jan 2016 18:23:06 +0000
Subject: [R] about change columns for specific rows
In-Reply-To: <CAN5afy-UKpP3Hb=sRt7weutPBtpJ31KzLJgChD5poPxuUAkJKA@mail.gmail.com>
Message-ID: <20160130182306.Horde.uMjOI_K6nugjRv3aE4Owvrt@mail.sapo.pt>

Hello,

Try

df[df$date >= '2012-01-01'& df$date <= '2013-12-31']$A = etc

Hope this helps,

Rui Barradas
?

Citando lily li <chocold12 at gmail.com>:

> Hi R users,
>
> I have a data frame, and I generate a date column like this:
> df$date = seq(as.Date('2012-01-01'), as.Date('2014-12-31'))
>
> df
> A? B? C
> 1? 2? ?1
> 2? 2? ?3
> 3? 2? ?4
>
> So the data frame has 4 columns now. But when I want to change the values
> of column A for specific dates, such as 2012-01-01 to 2013-12-31, I use the
> code below:
> df[date >= '2012-01-01'&date <= '2013-12-31']$A =
> df[date >= '2012-01-01'&date <= '2013-12-31']$A +2
>
> But it does not work, the date I generate seems not effective. What is the
> problem? Thanks for your help.
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Jan 30 19:34:49 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 30 Jan 2016 10:34:49 -0800
Subject: [R] about change columns for specific rows
In-Reply-To: <CAN5afy-UKpP3Hb=sRt7weutPBtpJ31KzLJgChD5poPxuUAkJKA@mail.gmail.com>
References: <CAN5afy-UKpP3Hb=sRt7weutPBtpJ31KzLJgChD5poPxuUAkJKA@mail.gmail.com>
Message-ID: <356FD5E5-6938-4B69-8D14-32B6CA11B469@comcast.net>


> On Jan 29, 2016, at 10:30 PM, lily li <chocold12 at gmail.com> wrote:
> 
> Hi R users,
> 
> I have a data frame, and I generate a date column like this:
> df$date = seq(as.Date('2012-01-01'), as.Date('2014-12-31'))
> 
> df
> A  B  C
> 1  2   1
> 2  2   3
> 3  2   4
> 
> So the data frame has 4 columns now. But when I want to change the values
> of column A for specific dates, such as 2012-01-01 to 2013-12-31, I use the
> code below:
> df[date >= '2012-01-01'&date <= '2013-12-31']$A =
> df[date >= '2012-01-01'&date <= '2013-12-31']$A +2
> 
> But it does not work, the date I generate seems not effective. What is the
> problem? Thanks for your help.
> 

You are using "[" incorrectly. You should be passing that logical vector to the "row-position" of the  i,j-form of the `[<-` function. Try instead:

df[date >= '2012-01-01'&date <= '2013-12-31' , ]$A =
   df[ date >= '2012-01-01'&date <= '2013-12-31' , ]$A +2

(As it is, I believe you are selecting columns rather than rows.)

> 	[[alternative HTML version deleted]]

And study the documentation of your email client so you can learn how to send palin text emails to rhelp.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ruipbarradas at sapo.pt  Sat Jan 30 19:43:17 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Sat, 30 Jan 2016 18:43:17 +0000
Subject: [R] about change columns for specific rows
In-Reply-To: <20160130182306.Horde.uMjOI_K6nugjRv3aE4Owvrt@mail.sapo.pt>
Message-ID: <20160130184317.Horde.CMTmYBpJSaJ6E6J_olfnaMZ@mail.sapo.pt>

Sorry, there's a mistake, there's a missing comma, it should be

df[df$date >= '2012-01-01'& df$date <= '2013-12-31', ]$A

Rui Barradas
?

Citando ruipbarradas at sapo.pt:

> Hello,
>
> Try
>
> df[df$date >= '2012-01-01'& df$date <= '2013-12-31']$A = etc
>
> Hope this helps,
>
> Rui Barradas
> ?
>
> Citando lily li <chocold12 at gmail.com>:
>> Hi R users,
>>
>> I have a data frame, and I generate a date column like this:
>> df$date = seq(as.Date('2012-01-01'), as.Date('2014-12-31'))
>>
>> df
>> A? B? C
>> 1? 2? ?1
>> 2? 2? ?3
>> 3? 2? ?4
>>
>> So the data frame has 4 columns now. But when I want to change the values
>> of column A for specific dates, such as 2012-01-01 to 2013-12-31, I use the
>> code below:
>> df[date >= '2012-01-01'&date <= '2013-12-31']$A =
>> df[date >= '2012-01-01'&date <= '2013-12-31']$A +2
>>
>> But it does not work, the date I generate seems not effective. What is the
>> problem? Thanks for your help.
>>
>> ? ? ? ? [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.htmland provide commented,
>> minimal, self-contained, reproducible code.
>
> ?
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From jorfega80 at hotmail.com  Sat Jan 30 22:43:36 2016
From: jorfega80 at hotmail.com (=?iso-8859-1?Q?Jorge_Fern=E1ndez_Garc=EDa?=)
Date: Sat, 30 Jan 2016 21:43:36 +0000
Subject: [R] Problem displaying greek symbols
Message-ID: <AM4PR02MB137700BFBEEF1B0F8BE48331B5DC0@AM4PR02MB1377.eurprd02.prod.outlook.com>

Hi,


I have a problem displaying greek (and in general any special character).


I know I am using the right command as the same script works in Fedora20 but not in MAC Yosemite.


ylab=expression(delta) displays a square instead of the right symbol when I view the resulting pdf file with preview or any other tool to display pdf.


Any idea of what's going on?


Thanks in advance



	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sun Jan 31 00:24:11 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 30 Jan 2016 15:24:11 -0800
Subject: [R] Problem displaying greek symbols
In-Reply-To: <AM4PR02MB137700BFBEEF1B0F8BE48331B5DC0@AM4PR02MB1377.eurprd02.prod.outlook.com>
References: <AM4PR02MB137700BFBEEF1B0F8BE48331B5DC0@AM4PR02MB1377.eurprd02.prod.outlook.com>
Message-ID: <CAGxFJbRi0q0xa_aMQm9cM3att2VWbZP_JGzy4nE1GFijZg-W8A@mail.gmail.com>

(Ill give it a try, but more expertise than I have may be needed)

Works fine for me (on OS X).

Take a look at ?pdf . I believe the font family in use (Helvetica is
the default) needs to have the (Adobe) symbol font as font 5. What
family are you using?

To see what families are available, use:

 names(grDevices::pdfFonts())

Another possibility is that you are using the wrong encoding.
Unfortunately, this is beyond my ability to help you with, but perhaps
reading the Help on the encoding argument and related links might get
you the necessary info.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Jan 30, 2016 at 1:43 PM, Jorge Fern?ndez Garc?a
<jorfega80 at hotmail.com> wrote:
> Hi,
>
>
> I have a problem displaying greek (and in general any special character).
>
>
> I know I am using the right command as the same script works in Fedora20 but not in MAC Yosemite.
>
>
> ylab=expression(delta) displays a square instead of the right symbol when I view the resulting pdf file with preview or any other tool to display pdf.
>
>
> Any idea of what's going on?
>
>
> Thanks in advance
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sun Jan 31 02:17:12 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 30 Jan 2016 17:17:12 -0800
Subject: [R] Problem displaying greek symbols
In-Reply-To: <CAGxFJbRi0q0xa_aMQm9cM3att2VWbZP_JGzy4nE1GFijZg-W8A@mail.gmail.com>
References: <AM4PR02MB137700BFBEEF1B0F8BE48331B5DC0@AM4PR02MB1377.eurprd02.prod.outlook.com>
	<CAGxFJbRi0q0xa_aMQm9cM3att2VWbZP_JGzy4nE1GFijZg-W8A@mail.gmail.com>
Message-ID: <AA4A2B86-A789-4C8D-956D-D128A8CDDB18@comcast.net>


> On Jan 30, 2016, at 3:24 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> (Ill give it a try, but more expertise than I have may be needed)
> 
> Works fine for me (on OS X).
> 
> Take a look at ?pdf . I believe the font family in use (Helvetica is
> the default) needs to have the (Adobe) symbol font as font 5. What
> family are you using?
> 
> To see what families are available, use:
> 
> names(grDevices::pdfFonts())

That's not very informative, since the actual fonts that are going to be used are inside the 'serif', "sans", and  "mono" families. Try this instead:

> pdfFonts()$serif$metrics
[1] "Times-Roman.afm"      "Times-Bold.afm"       "Times-Italic.afm"    
[4] "Times-BoldItalic.afm" "Symbol.afm"      

> pdfFonts()$mono$metrics
[1] "Courier.afm"             "Courier-Bold.afm"       
[3] "Courier-Oblique.afm"     "Courier-BoldOblique.afm"
[5] "Symbol.afm" 

Notice the the fifth item in both is Symbol.

Which may also not be very useful either since for reasons that I have never been able to fathom, the fonts sometimes get messed up on a Mac and the way to detect and correct the problem is to use Font Book.app which I think you will find in either ~/Applications or ~/Applications/Utilities. The symptom: ... you find a font type in Font Book that has duplicate entries. Delete the corrupted one and you may find your Symbols will reappear.

(This is documented in ?quartz.)


> Another possibility is that you are using the wrong encoding.
> Unfortunately, this is beyond my ability to help you with, but perhaps
> reading the Help on the encoding argument and related links might get
> you the necessary info.
> 
> Cheers,
> Bert
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Sat, Jan 30, 2016 at 1:43 PM, Jorge Fern?ndez Garc?a
> <jorfega80 at hotmail.com> wrote:
>> Hi,
>> 
>> 
>> I have a problem displaying greek (and in general any special character).
>> 
>> 
>> I know I am using the right command as the same script works in Fedora20 but not in MAC Yosemite.
>> 
>> 
>> ylab=expression(delta) displays a square instead of the right symbol when I view the resulting pdf file with preview or any other tool to display pdf.

A full test would be:

pdf(); plot(1,1, main=expression(delta)); dev.off()

-- 
David.

>> 
>> 
>> Any idea of what's going on?
>> 
>> 
>> Thanks in advance
>> 
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From gaiusjaugustus at gmail.com  Sat Jan 30 18:50:05 2016
From: gaiusjaugustus at gmail.com (Gaius Augustus)
Date: Sat, 30 Jan 2016 10:50:05 -0700
Subject: [R] Efficient way to create new column based on comparison with
 another dataframe
In-Reply-To: <CAKVAULM2g-C887=RrBKJKKZB4RTExigS9X-kqHJjaXt5WTVMrA@mail.gmail.com>
References: <CACNwPfYXQN3+z+yO6DPLTEhN1c0SiwJrS==Zn0cte3P1zLay1A@mail.gmail.com>
	<CAKVAULM2g-C887=RrBKJKKZB4RTExigS9X-kqHJjaXt5WTVMrA@mail.gmail.com>
Message-ID: <CACNwPfZU9UTQf0Q5YwcH8xWqW+Piwg1YdFyqnsD0CgB9AhK1Tg@mail.gmail.com>

I'll look into the Intervals idea.  The data.table code posted might not
work (because I don't believe it would put the rows in the correct order if
the chromosomes are interspersed), however, it did make me think about
possibly assigning based on values...

Something like:
mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
c(3000, 6000, 1000), key = "Chr")
Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001), End
= c(5000, 10000), key = "Chr")

for(i in 1:nrow(Chr.Arms)){
  cur.row <- Chr.Arms[i, ]
  mapfile[ Chr == cur.row$Chr & Position >= cur.row$Start & Position <=
cur.row$End] <- Chr.Arms$Arm
}

This might take out the need for the intermediate table/vector.  Not sure
yet if it'll work, but we'll see.  I'm interested to know if anyone else
has any ideas, too.

Thanks,
Gaius

On Fri, Jan 29, 2016 at 11:34 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
wrote:

> Hi Gaius,
>
> Could you use data.table and loop over the small Chr.arms?
>
> library(data.table)
> mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
> c(3000, 6000, 1000), key = "Chr")
> Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001), End
> = c(5000, 10000), key = "Chr")
>
> Arms <- data.table()
> for(i in 1:nrow(Chr.Arms)){
>   cur.row <- Chr.Arms[i, ]
>   Arm <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
>   Arm <- Arm[ , Arm:=cur.row$Arm][]
>   Arms <- rbind(Arms, Arm)
> }
>
> # Or use plyr to loop over each possible arm
> library(plyr)
> Arms <- ddply(Chr.Arms, .variables = "Arm", function(cur.row, mapfile){
>   mapfile <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
>   mapfile <- mapfile[ , Arm:=cur.row$Arm][]
>   return(mapfile)
> }, mapfile = mapfile)
>
> I have just started to use the data.table and I have the feeling the code
> above can be greatly improved - maybe the loop can be dropped entirely?
>
> Hope this helps
> Ulrik
>
> On Sat, 30 Jan 2016 at 03:29 Gaius Augustus <gaiusjaugustus at gmail.com>
> wrote:
>
>> I have two dataframes. One has chromosome arm information, and the other
>> has SNP position information. I am trying to assign each SNP an arm
>> identity.  I'd like to create this new column based on comparing it to the
>> reference file.
>>
>> *1) Mapfile (has millions of rows)*
>>
>> Name    Chr   Position
>> S1      1      3000
>> S2      1      6000
>> S3      1      1000
>>
>> *2) Chr.Arms   file (has 39 rows)*
>>
>> Chr    Arm    Start   End
>> 1      p      0       5000
>> 1      q      5001    10000
>>
>>
>> *R Script that works, but slow:*
>> Arms  <- c()
>> for (line in 1:nrow(Mapfile)){
>>       Arms[line] <- Chr.Arms$Arm[ Mapfile$Chr[line] == Chr.Arms$Chr &
>>  Mapfile$Position[line] > Chr.Arms$Start &  Mapfile$Position[line] <
>> Chr.Arms$End]}
>> }
>> Mapfile$Arm <- Arms
>>
>>
>> *Output Table:*
>>
>> Name   Chr   Position   Arm
>> S1      1     3000      p
>> S2      1     6000      q
>> S3      1     1000      p
>>
>>
>> In words: I want each line to look up the location ( 1) find the right
>> Chr,
>> 2) find the line where the START < POSITION < END), then get the ARM
>> information and place it in a new column.
>>
>> This R script works, but surely there is a more time/processing efficient
>> way to do it.
>>
>> Thanks in advance for any help,
>> Gaius
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From gaiusjaugustus at gmail.com  Sat Jan 30 19:48:27 2016
From: gaiusjaugustus at gmail.com (Gaius Augustus)
Date: Sat, 30 Jan 2016 11:48:27 -0700
Subject: [R] Efficient way to create new column based on comparison with
 another dataframe
In-Reply-To: <CACNwPfZU9UTQf0Q5YwcH8xWqW+Piwg1YdFyqnsD0CgB9AhK1Tg@mail.gmail.com>
References: <CACNwPfYXQN3+z+yO6DPLTEhN1c0SiwJrS==Zn0cte3P1zLay1A@mail.gmail.com>
	<CAKVAULM2g-C887=RrBKJKKZB4RTExigS9X-kqHJjaXt5WTVMrA@mail.gmail.com>
	<CACNwPfZU9UTQf0Q5YwcH8xWqW+Piwg1YdFyqnsD0CgB9AhK1Tg@mail.gmail.com>
Message-ID: <CACNwPfaXz71gzovtQTL2CSgBBMMzLjW-4W_HBX1ivrzsc4rZhw@mail.gmail.com>

I'll look into the Intervals idea.  The data.table code posted might not
work (because I don't believe it would put the rows in the correct order if
the chromosomes are interspersed), however, it did make me think about
possibly assigning based on values...

*SOLUTION*
mapfile <- data.frame(Name = c("S1", "S2", "S3"), Chr = 1, Position =
c(3000, 6000, 1000), key = "Chr")
Chr.Arms <- data.frame(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001), End
= c(5000, 10000), key = "Chr")

for(i in 1:nrow(Chr.Arms)){
  cur.row <- Chr.Arms[i, ]
  mapfile$Arm[ mapfile$Chr == cur.row$Chr & mapfile$Position >=
cur.row$Start & mapfile$Position <= cur.row$End] <- cur.row$Arm
}

This took out the need for the intermediate table/vector.  This worked for
me, and was VERY fast.  Took <5 minutes on a dataframe with 35 million rows.

Thanks for the help,
Gaius

On Sat, Jan 30, 2016 at 10:50 AM, Gaius Augustus <gaiusjaugustus at gmail.com>
wrote:

> I'll look into the Intervals idea.  The data.table code posted might not
> work (because I don't believe it would put the rows in the correct order if
> the chromosomes are interspersed), however, it did make me think about
> possibly assigning based on values...
>
> Something like:
> mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
> c(3000, 6000, 1000), key = "Chr")
> Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001), End
> = c(5000, 10000), key = "Chr")
>
> for(i in 1:nrow(Chr.Arms)){
>   cur.row <- Chr.Arms[i, ]
>   mapfile[ Chr == cur.row$Chr & Position >= cur.row$Start & Position <=
> cur.row$End] <- Chr.Arms$Arm
> }
>
> This might take out the need for the intermediate table/vector.  Not sure
> yet if it'll work, but we'll see.  I'm interested to know if anyone else
> has any ideas, too.
>
> Thanks,
> Gaius
>
> On Fri, Jan 29, 2016 at 11:34 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> wrote:
>
>> Hi Gaius,
>>
>> Could you use data.table and loop over the small Chr.arms?
>>
>> library(data.table)
>> mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
>> c(3000, 6000, 1000), key = "Chr")
>> Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001),
>> End = c(5000, 10000), key = "Chr")
>>
>> Arms <- data.table()
>> for(i in 1:nrow(Chr.Arms)){
>>   cur.row <- Chr.Arms[i, ]
>>   Arm <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
>>   Arm <- Arm[ , Arm:=cur.row$Arm][]
>>   Arms <- rbind(Arms, Arm)
>> }
>>
>> # Or use plyr to loop over each possible arm
>> library(plyr)
>> Arms <- ddply(Chr.Arms, .variables = "Arm", function(cur.row, mapfile){
>>   mapfile <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
>>   mapfile <- mapfile[ , Arm:=cur.row$Arm][]
>>   return(mapfile)
>> }, mapfile = mapfile)
>>
>> I have just started to use the data.table and I have the feeling the code
>> above can be greatly improved - maybe the loop can be dropped entirely?
>>
>> Hope this helps
>> Ulrik
>>
>> On Sat, 30 Jan 2016 at 03:29 Gaius Augustus <gaiusjaugustus at gmail.com>
>> wrote:
>>
>>> I have two dataframes. One has chromosome arm information, and the other
>>> has SNP position information. I am trying to assign each SNP an arm
>>> identity.  I'd like to create this new column based on comparing it to
>>> the
>>> reference file.
>>>
>>> *1) Mapfile (has millions of rows)*
>>>
>>> Name    Chr   Position
>>> S1      1      3000
>>> S2      1      6000
>>> S3      1      1000
>>>
>>> *2) Chr.Arms   file (has 39 rows)*
>>>
>>> Chr    Arm    Start   End
>>> 1      p      0       5000
>>> 1      q      5001    10000
>>>
>>>
>>> *R Script that works, but slow:*
>>> Arms  <- c()
>>> for (line in 1:nrow(Mapfile)){
>>>       Arms[line] <- Chr.Arms$Arm[ Mapfile$Chr[line] == Chr.Arms$Chr &
>>>  Mapfile$Position[line] > Chr.Arms$Start &  Mapfile$Position[line] <
>>> Chr.Arms$End]}
>>> }
>>> Mapfile$Arm <- Arms
>>>
>>>
>>> *Output Table:*
>>>
>>> Name   Chr   Position   Arm
>>> S1      1     3000      p
>>> S2      1     6000      q
>>> S3      1     1000      p
>>>
>>>
>>> In words: I want each line to look up the location ( 1) find the right
>>> Chr,
>>> 2) find the line where the START < POSITION < END), then get the ARM
>>> information and place it in a new column.
>>>
>>> This R script works, but surely there is a more time/processing efficient
>>> way to do it.
>>>
>>> Thanks in advance for any help,
>>> Gaius
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>

	[[alternative HTML version deleted]]


From toth.denes at ttk.mta.hu  Sun Jan 31 10:17:50 2016
From: toth.denes at ttk.mta.hu (=?ISO-8859-1?Q?D=E9nes_T=F3th?=)
Date: Sun, 31 Jan 2016 10:17:50 +0100
Subject: [R] Efficient way to create new column based on comparison with
 another dataframe
In-Reply-To: <CACNwPfaXz71gzovtQTL2CSgBBMMzLjW-4W_HBX1ivrzsc4rZhw@mail.gmail.com>
References: <CACNwPfYXQN3+z+yO6DPLTEhN1c0SiwJrS==Zn0cte3P1zLay1A@mail.gmail.com>	<CAKVAULM2g-C887=RrBKJKKZB4RTExigS9X-kqHJjaXt5WTVMrA@mail.gmail.com>	<CACNwPfZU9UTQf0Q5YwcH8xWqW+Piwg1YdFyqnsD0CgB9AhK1Tg@mail.gmail.com>
	<CACNwPfaXz71gzovtQTL2CSgBBMMzLjW-4W_HBX1ivrzsc4rZhw@mail.gmail.com>
Message-ID: <56ADD13E.80008@ttk.mta.hu>

Hi,

I have not followed this thread from the beginning, but have you tried 
the foverlaps() function from the data.table package?

Something along the lines of:

---
# create the tables (use as.data.table() or setDT() if you
# start with a data.frame)
mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1,
                       Position = c(3000, 6000, 1000))
Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"),
                        Start = c(0, 5001), End = c(5000, 10000))

# add a dummy variable to be able to define Position as an interval
mapfile[, Position2 := Position]

# add keys
setkey(mapfile, Chr, Position, Position2)
setkey(Chr.Arms, Chr, Start, End)

# use data.table::foverlaps (see ?foverlaps)
mapfile <- foverlaps(mapfile, Chr.Arms, type = "within")

# remove the dummy variable
mapfile[, Position2 := NULL]

# recreate original order
setorder(mapfile, Chr, Name)

---

BTW, there is a typo in your *SOLUTION*. I guess you wanted to write 
data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position = c(3000, 6000, 
1000), key = "Chr") instead of data.frame(Name = c("S1", "S2", "S3"), 
Chr = 1, Position = c(3000, 6000, 1000), key = "Chr").

HTH,
   Denes



On 01/30/2016 07:48 PM, Gaius Augustus wrote:
> I'll look into the Intervals idea.  The data.table code posted might not
> work (because I don't believe it would put the rows in the correct order if
> the chromosomes are interspersed), however, it did make me think about
> possibly assigning based on values...
>
> *SOLUTION*
> mapfile <- data.frame(Name = c("S1", "S2", "S3"), Chr = 1, Position =
> c(3000, 6000, 1000), key = "Chr")
> Chr.Arms <- data.frame(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001), End
> = c(5000, 10000), key = "Chr")
>
> for(i in 1:nrow(Chr.Arms)){
>    cur.row <- Chr.Arms[i, ]
>    mapfile$Arm[ mapfile$Chr == cur.row$Chr & mapfile$Position >=
> cur.row$Start & mapfile$Position <= cur.row$End] <- cur.row$Arm
> }
>
> This took out the need for the intermediate table/vector.  This worked for
> me, and was VERY fast.  Took <5 minutes on a dataframe with 35 million rows.
>
> Thanks for the help,
> Gaius
>
> On Sat, Jan 30, 2016 at 10:50 AM, Gaius Augustus <gaiusjaugustus at gmail.com>
> wrote:
>
>> I'll look into the Intervals idea.  The data.table code posted might not
>> work (because I don't believe it would put the rows in the correct order if
>> the chromosomes are interspersed), however, it did make me think about
>> possibly assigning based on values...
>>
>> Something like:
>> mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
>> c(3000, 6000, 1000), key = "Chr")
>> Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001), End
>> = c(5000, 10000), key = "Chr")
>>
>> for(i in 1:nrow(Chr.Arms)){
>>    cur.row <- Chr.Arms[i, ]
>>    mapfile[ Chr == cur.row$Chr & Position >= cur.row$Start & Position <=
>> cur.row$End] <- Chr.Arms$Arm
>> }
>>
>> This might take out the need for the intermediate table/vector.  Not sure
>> yet if it'll work, but we'll see.  I'm interested to know if anyone else
>> has any ideas, too.
>>
>> Thanks,
>> Gaius
>>
>> On Fri, Jan 29, 2016 at 11:34 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>> wrote:
>>
>>> Hi Gaius,
>>>
>>> Could you use data.table and loop over the small Chr.arms?
>>>
>>> library(data.table)
>>> mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
>>> c(3000, 6000, 1000), key = "Chr")
>>> Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001),
>>> End = c(5000, 10000), key = "Chr")
>>>
>>> Arms <- data.table()
>>> for(i in 1:nrow(Chr.Arms)){
>>>    cur.row <- Chr.Arms[i, ]
>>>    Arm <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
>>>    Arm <- Arm[ , Arm:=cur.row$Arm][]
>>>    Arms <- rbind(Arms, Arm)
>>> }
>>>
>>> # Or use plyr to loop over each possible arm
>>> library(plyr)
>>> Arms <- ddply(Chr.Arms, .variables = "Arm", function(cur.row, mapfile){
>>>    mapfile <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
>>>    mapfile <- mapfile[ , Arm:=cur.row$Arm][]
>>>    return(mapfile)
>>> }, mapfile = mapfile)
>>>
>>> I have just started to use the data.table and I have the feeling the code
>>> above can be greatly improved - maybe the loop can be dropped entirely?
>>>
>>> Hope this helps
>>> Ulrik
>>>
>>> On Sat, 30 Jan 2016 at 03:29 Gaius Augustus <gaiusjaugustus at gmail.com>
>>> wrote:
>>>
>>>> I have two dataframes. One has chromosome arm information, and the other
>>>> has SNP position information. I am trying to assign each SNP an arm
>>>> identity.  I'd like to create this new column based on comparing it to
>>>> the
>>>> reference file.
>>>>
>>>> *1) Mapfile (has millions of rows)*
>>>>
>>>> Name    Chr   Position
>>>> S1      1      3000
>>>> S2      1      6000
>>>> S3      1      1000
>>>>
>>>> *2) Chr.Arms   file (has 39 rows)*
>>>>
>>>> Chr    Arm    Start   End
>>>> 1      p      0       5000
>>>> 1      q      5001    10000
>>>>
>>>>
>>>> *R Script that works, but slow:*
>>>> Arms  <- c()
>>>> for (line in 1:nrow(Mapfile)){
>>>>        Arms[line] <- Chr.Arms$Arm[ Mapfile$Chr[line] == Chr.Arms$Chr &
>>>>   Mapfile$Position[line] > Chr.Arms$Start &  Mapfile$Position[line] <
>>>> Chr.Arms$End]}
>>>> }
>>>> Mapfile$Arm <- Arms
>>>>
>>>>
>>>> *Output Table:*
>>>>
>>>> Name   Chr   Position   Arm
>>>> S1      1     3000      p
>>>> S2      1     6000      q
>>>> S3      1     1000      p
>>>>
>>>>
>>>> In words: I want each line to look up the location ( 1) find the right
>>>> Chr,
>>>> 2) find the line where the START < POSITION < END), then get the ARM
>>>> information and place it in a new column.
>>>>
>>>> This R script works, but surely there is a more time/processing efficient
>>>> way to do it.
>>>>
>>>> Thanks in advance for any help,
>>>> Gaius
>>>>
>>>>          [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From soni.archit1989 at gmail.com  Sun Jan 31 11:56:01 2016
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Sun, 31 Jan 2016 16:26:01 +0530
Subject: [R] R Studio error while installing twitteR package
Message-ID: <CAJ7HxBzo-Jj=fMDZkOBJuJoUVKZz-WxbX6FezXmvoq+kwMFkrQ@mail.gmail.com>

Hi All,

I am getting below error while installing package "twitteR" , but it gets
successfuly installed via R console, any ideas ?

*Error:*

*install.packages("twitteR", lib="C:/Program
Files/TIBCO/terrde40/site-library")Trying to download URL
'https://cran.rstudio.com/bin/windows/contrib/3.2/twitteR_1.1.9.zip
<https://cran.rstudio.com/bin/windows/contrib/3.2/twitteR_1.1.9.zip>' to
file
'C:/Users/Archit/AppData/Local/Temp/TERR_1ae800291/downloaded_packages/twitteR_1.1.9.zip'
Downloaded 446573 bytes* installing *binary* package twitteR from
"C:\\Users\\Archit\\AppData\\Local\\Temp\\TERR_1ae800291\\downloaded_packages\\twitteR_1.1.9.zip"
to "C:/Program Files/TIBCO/terrde40/site-library"* checking MD5
checksumsPackage "twitteR" at directory "C:/Program
Files/TIBCO/terrde40/site-library/twitteR" does not have an MD5 file, so
integrity check was not done    COULD NOT CHECK MD5 CHECKSUMS*

-- 
Regards
Archit

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sun Jan 31 13:10:24 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 31 Jan 2016 07:10:24 -0500
Subject: [R] R Studio error while installing twitteR package
In-Reply-To: <CAJ7HxBzo-Jj=fMDZkOBJuJoUVKZz-WxbX6FezXmvoq+kwMFkrQ@mail.gmail.com>
References: <CAJ7HxBzo-Jj=fMDZkOBJuJoUVKZz-WxbX6FezXmvoq+kwMFkrQ@mail.gmail.com>
Message-ID: <56ADF9B0.7000209@gmail.com>

On 31/01/2016 5:56 AM, Archit Soni wrote:
> Hi All,
>
> I am getting below error while installing package "twitteR" , but it gets
> successfuly installed via R console, any ideas ?
>
> *Error:*
>
> *install.packages("twitteR", lib="C:/Program
> Files/TIBCO/terrde40/site-library")Trying to download URL
> 'https://cran.rstudio.com/bin/windows/contrib/3.2/twitteR_1.1.9.zip
> <https://cran.rstudio.com/bin/windows/contrib/3.2/twitteR_1.1.9.zip>' to
> file
> 'C:/Users/Archit/AppData/Local/Temp/TERR_1ae800291/downloaded_packages/twitteR_1.1.9.zip'
> Downloaded 446573 bytes* installing *binary* package twitteR from
> "C:\\Users\\Archit\\AppData\\Local\\Temp\\TERR_1ae800291\\downloaded_packages\\twitteR_1.1.9.zip"
> to "C:/Program Files/TIBCO/terrde40/site-library"* checking MD5
> checksumsPackage "twitteR" at directory "C:/Program
> Files/TIBCO/terrde40/site-library/twitteR" does not have an MD5 file, so
> integrity check was not done    COULD NOT CHECK MD5 CHECKSUMS*
>

I think you'll need to contact either RStudio or Tibco support for this.

Duncan Murdoch


From soni.archit1989 at gmail.com  Sun Jan 31 13:16:41 2016
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Sun, 31 Jan 2016 17:46:41 +0530
Subject: [R] R Studio error while installing twitteR package
In-Reply-To: <56ADF9B0.7000209@gmail.com>
References: <CAJ7HxBzo-Jj=fMDZkOBJuJoUVKZz-WxbX6FezXmvoq+kwMFkrQ@mail.gmail.com>
	<56ADF9B0.7000209@gmail.com>
Message-ID: <CAJ7HxByDte=APOG1bQ9yRXLnjJBgY1bO+nf-vG4xzZ+KRhs+wQ@mail.gmail.com>

?Ya Duncan, but I searched bit more ?and got the solution to remap the
working directory and if the issue still persists then we change the CRAN
mirror. Now it is working fine.

Thanks,
Archit

On Sun, Jan 31, 2016 at 5:40 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 31/01/2016 5:56 AM, Archit Soni wrote:
>
>> Hi All,
>>
>> I am getting below error while installing package "twitteR" , but it gets
>> successfuly installed via R console, any ideas ?
>>
>> *Error:*
>>
>> *install.packages("twitteR", lib="C:/Program
>> Files/TIBCO/terrde40/site-library")Trying to download URL
>> 'https://cran.rstudio.com/bin/windows/contrib/3.2/twitteR_1.1.9.zip
>> <https://cran.rstudio.com/bin/windows/contrib/3.2/twitteR_1.1.9.zip>' to
>> file
>>
>> 'C:/Users/Archit/AppData/Local/Temp/TERR_1ae800291/downloaded_packages/twitteR_1.1.9.zip'
>> Downloaded 446573 bytes* installing *binary* package twitteR from
>>
>> "C:\\Users\\Archit\\AppData\\Local\\Temp\\TERR_1ae800291\\downloaded_packages\\twitteR_1.1.9.zip"
>> to "C:/Program Files/TIBCO/terrde40/site-library"* checking MD5
>> checksumsPackage "twitteR" at directory "C:/Program
>> Files/TIBCO/terrde40/site-library/twitteR" does not have an MD5 file, so
>> integrity check was not done    COULD NOT CHECK MD5 CHECKSUMS*
>>
>>
> I think you'll need to contact either RStudio or Tibco support for this.
>
> Duncan Murdoch
>



-- 
Regards
Archit

	[[alternative HTML version deleted]]


From dileepkunjaai at gmail.com  Sun Jan 31 18:08:21 2016
From: dileepkunjaai at gmail.com (=?UTF-8?B?4LSV4LWB4LSe4LWN4LSe4LS+4LSv4LS/IGt1bmphYWk=?=)
Date: Sun, 31 Jan 2016 22:38:21 +0530
Subject: [R] R help
In-Reply-To: <65F5A596-5CC4-4C1A-B41C-D2D3AA615D5A@utoronto.ca>
References: <CABZBenYoQY3hSdNt8msjme1Zeu-J3kaB0fA9bfjMuFs80pzL9A@mail.gmail.com>
	<65F5A596-5CC4-4C1A-B41C-D2D3AA615D5A@utoronto.ca>
Message-ID: <CALTF6smgUoV7X1Vgetn+yO2evXtj54ur62XzvMcd4hUWbxMA6w@mail.gmail.com>

Hai Anukriti Gupta,

While sending mail to mailing list, please change the subject  from "R
Help" to more specific one..(eg: R Regression error..)  Because we all can
refer your mail and the solution in future  by checking the mail subject....




On Sat, Jan 30, 2016 at 9:55 PM, Boris Steipe <boris.steipe at utoronto.ca>
 wrote:

> I think the error message is pretty clear. Your calculations are
> attempting to allocate more memory than you have available. As to what is
> causing your code to do this, only someone familiar with your code could
> possibly tell.
>
> B.
> (Read the posting guide, please - and don't post in HTML :-)
>
>
> On Jan 30, 2016, at 1:44 AM, Anukriti Gupta <gupta17anukriti at gmail.com>
> wrote:
>
> > Hi
> >
> > I am running a ordinal logistic regression, however its giving me an
> error
> > like
> >
> > Error: cannot allocate vector of size 58.8 GbIn addition: Warning
> > messages:1: In rep.int(c(1, numeric(n)), n - 1L) :
> >  Reached total allocation of 8057Mb: see help(memory.size)2: In
> > rep.int(c(1, numeric(n)), n - 1L) :
> >  Reached total allocation of 8057Mb: see help(memory.size)3: In
> > rep.int(c(1, numeric(n)), n - 1L) :
> >  Reached total allocation of 8057Mb: see help(memory.size)4: In
> > rep.int(c(1, numeric(n)), n - 1L) :
> >  Reached total allocation of 8057Mb: see help(memory.size)
> >
> >
> > I am using a 64 bit laptop. I ma not sure what is causing this kind of
> issue
> >
> > Regards
> >
> > Anukriti Gupta
> > Analyst (Financial Crime Compliance), HSBC
> > M: +91 88820 45065
> > LinkedIn <https://in.linkedin.com/pub/anukriti-gupta/21/839/5a5>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <http://www.r-project.org/posting-guide.html>
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
DILEEPKUMAR. R
J R F, IIT DELHI

On Sat, Jan 30, 2016 at 9:55 PM, Boris Steipe <boris.steipe at utoronto.ca>
wrote:

> I think the error message is pretty clear. Your calculations are
> attempting to allocate more memory than you have available. As to what is
> causing your code to do this, only someone familiar with your code could
> possibly tell.
>
> B.
> (Read the posting guide, please - and don't post in HTML :-)
>
>
> On Jan 30, 2016, at 1:44 AM, Anukriti Gupta <gupta17anukriti at gmail.com>
> wrote:
>
> > Hi
> >
> > I am running a ordinal logistic regression, however its giving me an
> error
> > like
> >
> > Error: cannot allocate vector of size 58.8 GbIn addition: Warning
> > messages:1: In rep.int(c(1, numeric(n)), n - 1L) :
> >  Reached total allocation of 8057Mb: see help(memory.size)2: In
> > rep.int(c(1, numeric(n)), n - 1L) :
> >  Reached total allocation of 8057Mb: see help(memory.size)3: In
> > rep.int(c(1, numeric(n)), n - 1L) :
> >  Reached total allocation of 8057Mb: see help(memory.size)4: In
> > rep.int(c(1, numeric(n)), n - 1L) :
> >  Reached total allocation of 8057Mb: see help(memory.size)
> >
> >
> > I am using a 64 bit laptop. I ma not sure what is causing this kind of
> issue
> >
> > Regards
> >
> > Anukriti Gupta
> > Analyst (Financial Crime Compliance), HSBC
> > M: +91 88820 45065
> > LinkedIn <https://in.linkedin.com/pub/anukriti-gupta/21/839/5a5>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
DILEEPKUMAR. R
J R F, IIT DELHI

	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Sun Jan 31 19:02:53 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Sun, 31 Jan 2016 19:02:53 +0100
Subject: [R] Modelling non-Negative Time Series
Message-ID: <20160131180253.GA2115@localhost.localdomain>

Dear All,
I am struggling to develop a model to forecast the daily expenses from
a bank account.
The daily time series consists (obviously) of non-negative numbers
which can be zero in the days when no money is taken from the bank
account.
To give you an idea of the kind of series I am dealing with, please
have a look at

myts<-structure(c(5.5, 0, 126.93, 0, 0, 0, 0, 10, 0, 135.34, 0, 0,
0, 0, 98.21, 0, 112.38, 0, 0, 0, 0, 0, 1373.77, 151.83, 26.66,
205.5, 129.33, 172.5, 0, 10, 131.09, 0, 0, 0, 0, 0, 689, 0, 0,
0, 0, 0, 0, 60.6, 183, 98.21, 0, 0, 0, 0, 1433.79, 175.89, 0,
0, 0, 200, 134.33, 98.26, 112.21, 0, 0, 0, 0, 0, 0, 112.31, 0,
0, 0, 0, 120, 0, 350, 0, 0, 98.21, 0, 0, 0, 113.24, 0, 0, 0,
0, 15, 696.65, 321.87, 929, 210.58, 0, 0, 10), .Tsp = c(16563,
16654, 1), class = "ts")

(the time origin is a bit funny, but what matters is that I have daily
data).

Do you know any R package to handle this kind of series? I think I am
outside the domain of the ARIMA approach.
I experimented with acp and tscount (to see if I could treat the
series as an autoregressive Poisson series), but I did not get very
far.
Any suggestion is appreciated.
Cheers

Lorenzo


From lorenzo.isella at gmail.com  Sun Jan 31 20:05:19 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Sun, 31 Jan 2016 20:05:19 +0100
Subject: [R] Time Series and Auto.arima
In-Reply-To: <84B3BE17-DE3D-488E-B549-0902628721C7@comcast.net>
References: <20160129205937.GA1752@localhost.localdomain>
	<84B3BE17-DE3D-488E-B549-0902628721C7@comcast.net>
Message-ID: <20160131190519.GB2115@localhost.localdomain>

Partially the trouble is that the zoo time series is then translated
into a ts object by auto.arima.
In doing so, the series along a regular time grid and some missing
data appear.
To fix this, I should replace each NA with the previous non-NA value.
This is easy enough and the series exhibits some clear cycles: roughly
every month there is a spike, followed by a decrease, then another
spike and so on.
I would like to forecast a couple of cycles (60 steps), but when I do
so with auto.arima, nothing like what I expect appears (the
seasonality is completely lost).
Any idea why?
I paste below the revised R code for reproducibility.

Lorenzo





library(forecast)

tt<-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64,
4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05,
1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36,
2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36,
821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39,
800.81, 790.81), index = structure(c(16563L, 16565L, 16570L,
16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L,
16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L,
16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L,
16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L,
16650L, 16651L, 16654L), class = "Date"), class = "zoo")

tt2<-as.ts(tt)
tt2<-na.locf(tt2)

mm<-auto.arima(tt2)


plot(forecast(mm, h=60))




On Fri, Jan 29, 2016 at 02:16:27PM -0800, David Winsemius wrote:
>
>> On Jan 29, 2016, at 12:59 PM, Lorenzo Isella <lorenzo.isella at gmail.com> wrote:
>>
>> Dear All,
>> I am puzzled and probably I am misunderstanding something.
>> Please consider the snippet at the end of the email.
>> We see a time series that has clearly some pattern (essentially, it is
>> an account where a salary is regularly paid followed by some
>> expenses).
>> However the output of the auto.arima from the forecast function does
>> not seem to make any sense (at least to me).
>> I wonder if the problem is the fact that the time series is not
>> defined at regular intervals.
>> Any suggestions and alternative ways to fit it (e.g.: sarima from the astsa
>> package to account for the seasonality?) are really welcome.
>> Many thanks
>>
>> Lorenzo
>>
>>
>>
>> ##############################################
>> library(forecast)
>>
>> tt<-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64,
>> 4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05,
>> 1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36,
>> 2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36,
>> 821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39,
>> 800.81, 790.81), index = structure(c(16563L, 16565L, 16570L,
>> 16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L,
>> 16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L,
>> 16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L,
>> 16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L,
>> 16650L, 16651L, 16654L), class = "Date"), class = "zoo")
>>
>> plot(tt)
>>
>
>library(forecast)
>
>> fit<-auto.arima(tt)
>>
>> ###########################################
>
>If , after runing plot(tt), you then run:
>
> fitted(fit)
>
>Time Series:
>Start = 16563
>End = 16654
>Frequency = 1
> [1] 1448.8211        NA 1444.8612        NA        NA        NA        NA
> [8] 1398.7752        NA 1359.0350        NA        NA        NA        NA
>[15] 1309.1398        NA 1219.7420        NA        NA        NA        NA
>[22] 2302.8903 3708.1762 2713.0349 2603.0512 1968.0100 1819.1484 1725.4634
>[29]        NA 1572.6179 1593.2628        NA        NA        NA        NA
>[36]        NA 1258.3403        NA        NA        NA        NA        NA
>[43]        NA 1184.9656  955.3023  822.7394        NA        NA        NA
>[50] 1987.7634 3333.3131 2294.6941        NA        NA 1760.6351 1551.5526
>[57] 1406.6751 1309.3682 1238.1899        NA        NA        NA        NA
>[64]        NA        NA 1251.6898        NA        NA        NA        NA
>[71] 1179.9970        NA  988.3885        NA        NA  888.4533        NA
>[78]        NA        NA  889.4017        NA        NA        NA        NA
>[85] 1970.0911 3152.7668 2032.3935 1799.2350 1126.2794        NA        NA
>[92] 1088.1525
>
>
>Using that vector:
>
>lines(seq(16563 ,16654 ),fitted(fit), col="red", lwd=3)
>
>You can see that the fitted values are capturing quite a bit of the variation.
>
>
>
>I'm not a regular user of pkg:forecast, so there may be more refined methods of extracting information than using `fitted`.
>
>-- 
>
>David Winsemius
>Alameda, CA, USA
>


From gaiusjaugustus at gmail.com  Sun Jan 31 20:17:07 2016
From: gaiusjaugustus at gmail.com (Gaius Augustus)
Date: Sun, 31 Jan 2016 12:17:07 -0700
Subject: [R] Efficient way to create new column based on comparison with
 another dataframe
In-Reply-To: <56ADD13E.80008@ttk.mta.hu>
References: <CACNwPfYXQN3+z+yO6DPLTEhN1c0SiwJrS==Zn0cte3P1zLay1A@mail.gmail.com>
	<CAKVAULM2g-C887=RrBKJKKZB4RTExigS9X-kqHJjaXt5WTVMrA@mail.gmail.com>
	<CACNwPfZU9UTQf0Q5YwcH8xWqW+Piwg1YdFyqnsD0CgB9AhK1Tg@mail.gmail.com>
	<CACNwPfaXz71gzovtQTL2CSgBBMMzLjW-4W_HBX1ivrzsc4rZhw@mail.gmail.com>
	<56ADD13E.80008@ttk.mta.hu>
Message-ID: <CACNwPfZnFw9pQeK5KCWvWjXY-7Ey1dk-LFpyUspLm2Zcw+1aEw@mail.gmail.com>

Thanks Denes,
I should have thought of foverlaps as an option.  I wonder how fast it is
compared to my solution!

My particular solution does not need data.table in order to work.  It just
loops through the ChrArms (Chromosome Arms, which always has 39 rows) and
assigns the proper arm to all rows within mapfile that lie within Start and
End on a particular Chr.  This is opposed to my first solution, where I was
trying to loop through mapfile (which could be millions of rows) and assign
each row one at a time.  That's why I used data.frame.

For some reason, yesterday, data.table was acting funny on the computer I
remote to, so I need to figure out why that is once I can get on it.  Then
I want to time my solution and one with foverlaps to see if one is faster.

Thanks,
Gaius

On Sun, Jan 31, 2016 at 2:17 AM, D?nes T?th <toth.denes at ttk.mta.hu> wrote:

> Hi,
>
> I have not followed this thread from the beginning, but have you tried the
> foverlaps() function from the data.table package?
>
> Something along the lines of:
>
> ---
> # create the tables (use as.data.table() or setDT() if you
> # start with a data.frame)
> mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1,
>                       Position = c(3000, 6000, 1000))
> Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"),
>                        Start = c(0, 5001), End = c(5000, 10000))
>
> # add a dummy variable to be able to define Position as an interval
> mapfile[, Position2 := Position]
>
> # add keys
> setkey(mapfile, Chr, Position, Position2)
> setkey(Chr.Arms, Chr, Start, End)
>
> # use data.table::foverlaps (see ?foverlaps)
> mapfile <- foverlaps(mapfile, Chr.Arms, type = "within")
>
> # remove the dummy variable
> mapfile[, Position2 := NULL]
>
> # recreate original order
> setorder(mapfile, Chr, Name)
>
> ---
>
> BTW, there is a typo in your *SOLUTION*. I guess you wanted to write
> data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position = c(3000, 6000,
> 1000), key = "Chr") instead of data.frame(Name = c("S1", "S2", "S3"), Chr =
> 1, Position = c(3000, 6000, 1000), key = "Chr").
>
> HTH,
>   Denes
>
>
>
> On 01/30/2016 07:48 PM, Gaius Augustus wrote:
>
>> I'll look into the Intervals idea.  The data.table code posted might not
>> work (because I don't believe it would put the rows in the correct order
>> if
>> the chromosomes are interspersed), however, it did make me think about
>> possibly assigning based on values...
>>
>> *SOLUTION*
>>
>> mapfile <- data.frame(Name = c("S1", "S2", "S3"), Chr = 1, Position =
>> c(3000, 6000, 1000), key = "Chr")
>> Chr.Arms <- data.frame(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001), End
>> = c(5000, 10000), key = "Chr")
>>
>> for(i in 1:nrow(Chr.Arms)){
>>    cur.row <- Chr.Arms[i, ]
>>    mapfile$Arm[ mapfile$Chr == cur.row$Chr & mapfile$Position >=
>> cur.row$Start & mapfile$Position <= cur.row$End] <- cur.row$Arm
>> }
>>
>> This took out the need for the intermediate table/vector.  This worked for
>> me, and was VERY fast.  Took <5 minutes on a dataframe with 35 million
>> rows.
>>
>> Thanks for the help,
>> Gaius
>>
>> On Sat, Jan 30, 2016 at 10:50 AM, Gaius Augustus <
>> gaiusjaugustus at gmail.com>
>> wrote:
>>
>> I'll look into the Intervals idea.  The data.table code posted might not
>>> work (because I don't believe it would put the rows in the correct order
>>> if
>>> the chromosomes are interspersed), however, it did make me think about
>>> possibly assigning based on values...
>>>
>>> Something like:
>>> mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
>>> c(3000, 6000, 1000), key = "Chr")
>>> Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001),
>>> End
>>> = c(5000, 10000), key = "Chr")
>>>
>>> for(i in 1:nrow(Chr.Arms)){
>>>    cur.row <- Chr.Arms[i, ]
>>>    mapfile[ Chr == cur.row$Chr & Position >= cur.row$Start & Position <=
>>> cur.row$End] <- Chr.Arms$Arm
>>> }
>>>
>>> This might take out the need for the intermediate table/vector.  Not sure
>>> yet if it'll work, but we'll see.  I'm interested to know if anyone else
>>> has any ideas, too.
>>>
>>> Thanks,
>>> Gaius
>>>
>>> On Fri, Jan 29, 2016 at 11:34 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com
>>> >
>>> wrote:
>>>
>>> Hi Gaius,
>>>>
>>>> Could you use data.table and loop over the small Chr.arms?
>>>>
>>>> library(data.table)
>>>> mapfile <- data.table(Name = c("S1", "S2", "S3"), Chr = 1, Position =
>>>> c(3000, 6000, 1000), key = "Chr")
>>>> Chr.Arms <- data.table(Chr = 1, Arm = c("p", "q"), Start = c(0, 5001),
>>>> End = c(5000, 10000), key = "Chr")
>>>>
>>>> Arms <- data.table()
>>>> for(i in 1:nrow(Chr.Arms)){
>>>>    cur.row <- Chr.Arms[i, ]
>>>>    Arm <- mapfile[ Position >= cur.row$Start & Position <= cur.row$End]
>>>>    Arm <- Arm[ , Arm:=cur.row$Arm][]
>>>>    Arms <- rbind(Arms, Arm)
>>>> }
>>>>
>>>> # Or use plyr to loop over each possible arm
>>>> library(plyr)
>>>> Arms <- ddply(Chr.Arms, .variables = "Arm", function(cur.row, mapfile){
>>>>    mapfile <- mapfile[ Position >= cur.row$Start & Position <=
>>>> cur.row$End]
>>>>    mapfile <- mapfile[ , Arm:=cur.row$Arm][]
>>>>    return(mapfile)
>>>> }, mapfile = mapfile)
>>>>
>>>> I have just started to use the data.table and I have the feeling the
>>>> code
>>>> above can be greatly improved - maybe the loop can be dropped entirely?
>>>>
>>>> Hope this helps
>>>> Ulrik
>>>>
>>>> On Sat, 30 Jan 2016 at 03:29 Gaius Augustus <gaiusjaugustus at gmail.com>
>>>> wrote:
>>>>
>>>> I have two dataframes. One has chromosome arm information, and the other
>>>>> has SNP position information. I am trying to assign each SNP an arm
>>>>> identity.  I'd like to create this new column based on comparing it to
>>>>> the
>>>>> reference file.
>>>>>
>>>>> *1) Mapfile (has millions of rows)*
>>>>>
>>>>> Name    Chr   Position
>>>>> S1      1      3000
>>>>> S2      1      6000
>>>>> S3      1      1000
>>>>>
>>>>> *2) Chr.Arms   file (has 39 rows)*
>>>>>
>>>>> Chr    Arm    Start   End
>>>>> 1      p      0       5000
>>>>> 1      q      5001    10000
>>>>>
>>>>>
>>>>> *R Script that works, but slow:*
>>>>> Arms  <- c()
>>>>> for (line in 1:nrow(Mapfile)){
>>>>>        Arms[line] <- Chr.Arms$Arm[ Mapfile$Chr[line] == Chr.Arms$Chr &
>>>>>   Mapfile$Position[line] > Chr.Arms$Start &  Mapfile$Position[line] <
>>>>> Chr.Arms$End]}
>>>>> }
>>>>> Mapfile$Arm <- Arms
>>>>>
>>>>>
>>>>> *Output Table:*
>>>>>
>>>>> Name   Chr   Position   Arm
>>>>> S1      1     3000      p
>>>>> S2      1     6000      q
>>>>> S3      1     1000      p
>>>>>
>>>>>
>>>>> In words: I want each line to look up the location ( 1) find the right
>>>>> Chr,
>>>>> 2) find the line where the START < POSITION < END), then get the ARM
>>>>> information and place it in a new column.
>>>>>
>>>>> This R script works, but surely there is a more time/processing
>>>>> efficient
>>>>> way to do it.
>>>>>
>>>>> Thanks in advance for any help,
>>>>> Gaius
>>>>>
>>>>>          [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>
>>>>
>>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From phill at starkingdom.co.uk  Sun Jan 31 22:36:33 2016
From: phill at starkingdom.co.uk (phill at starkingdom.co.uk)
Date: Sun, 31 Jan 2016 21:36:33 +0000
Subject: [R] [R-pkgs] New Package: backblazer
Message-ID: <81e4302a6d68ab3bae64841ed7b0b25b@starkingdom.co.uk>

 

Dear R users, 

I'm pleased to announce that my first package has been accepted in CRAN.


https://cran.r-project.org/web/packages/backblazer/ 

backblazer provides bindings to Backblaze's B2 cloud storage API. 

As it is my first package on CRAN, I would certainly appreciate feedback
and suggestions from more experienced packagers. 

Regards, 

Phill 

 
	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


